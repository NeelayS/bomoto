Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=178, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9968-10023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809142
Iteration 2/25 | Loss: 0.00140310
Iteration 3/25 | Loss: 0.00111843
Iteration 4/25 | Loss: 0.00109524
Iteration 5/25 | Loss: 0.00109267
Iteration 6/25 | Loss: 0.00109225
Iteration 7/25 | Loss: 0.00109225
Iteration 8/25 | Loss: 0.00109225
Iteration 9/25 | Loss: 0.00109225
Iteration 10/25 | Loss: 0.00109225
Iteration 11/25 | Loss: 0.00109225
Iteration 12/25 | Loss: 0.00109225
Iteration 13/25 | Loss: 0.00109225
Iteration 14/25 | Loss: 0.00109225
Iteration 15/25 | Loss: 0.00109225
Iteration 16/25 | Loss: 0.00109225
Iteration 17/25 | Loss: 0.00109225
Iteration 18/25 | Loss: 0.00109225
Iteration 19/25 | Loss: 0.00109225
Iteration 20/25 | Loss: 0.00109225
Iteration 21/25 | Loss: 0.00109225
Iteration 22/25 | Loss: 0.00109225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010922516230493784, 0.0010922516230493784, 0.0010922516230493784, 0.0010922516230493784, 0.0010922516230493784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010922516230493784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35478532
Iteration 2/25 | Loss: 0.00080651
Iteration 3/25 | Loss: 0.00080651
Iteration 4/25 | Loss: 0.00080651
Iteration 5/25 | Loss: 0.00080650
Iteration 6/25 | Loss: 0.00080650
Iteration 7/25 | Loss: 0.00080650
Iteration 8/25 | Loss: 0.00080650
Iteration 9/25 | Loss: 0.00080650
Iteration 10/25 | Loss: 0.00080650
Iteration 11/25 | Loss: 0.00080650
Iteration 12/25 | Loss: 0.00080650
Iteration 13/25 | Loss: 0.00080650
Iteration 14/25 | Loss: 0.00080650
Iteration 15/25 | Loss: 0.00080650
Iteration 16/25 | Loss: 0.00080650
Iteration 17/25 | Loss: 0.00080650
Iteration 18/25 | Loss: 0.00080650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00080650340532884, 0.00080650340532884, 0.00080650340532884, 0.00080650340532884, 0.00080650340532884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00080650340532884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080650
Iteration 2/1000 | Loss: 0.00002351
Iteration 3/1000 | Loss: 0.00001425
Iteration 4/1000 | Loss: 0.00001246
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00001107
Iteration 7/1000 | Loss: 0.00001064
Iteration 8/1000 | Loss: 0.00001038
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00001013
Iteration 11/1000 | Loss: 0.00000995
Iteration 12/1000 | Loss: 0.00000993
Iteration 13/1000 | Loss: 0.00000988
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000983
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000982
Iteration 18/1000 | Loss: 0.00000981
Iteration 19/1000 | Loss: 0.00000978
Iteration 20/1000 | Loss: 0.00000977
Iteration 21/1000 | Loss: 0.00000977
Iteration 22/1000 | Loss: 0.00000977
Iteration 23/1000 | Loss: 0.00000976
Iteration 24/1000 | Loss: 0.00000975
Iteration 25/1000 | Loss: 0.00000975
Iteration 26/1000 | Loss: 0.00000974
Iteration 27/1000 | Loss: 0.00000974
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000973
Iteration 30/1000 | Loss: 0.00000973
Iteration 31/1000 | Loss: 0.00000972
Iteration 32/1000 | Loss: 0.00000971
Iteration 33/1000 | Loss: 0.00000970
Iteration 34/1000 | Loss: 0.00000969
Iteration 35/1000 | Loss: 0.00000968
Iteration 36/1000 | Loss: 0.00000968
Iteration 37/1000 | Loss: 0.00000968
Iteration 38/1000 | Loss: 0.00000967
Iteration 39/1000 | Loss: 0.00000967
Iteration 40/1000 | Loss: 0.00000967
Iteration 41/1000 | Loss: 0.00000967
Iteration 42/1000 | Loss: 0.00000966
Iteration 43/1000 | Loss: 0.00000965
Iteration 44/1000 | Loss: 0.00000965
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000964
Iteration 47/1000 | Loss: 0.00000964
Iteration 48/1000 | Loss: 0.00000964
Iteration 49/1000 | Loss: 0.00000964
Iteration 50/1000 | Loss: 0.00000964
Iteration 51/1000 | Loss: 0.00000964
Iteration 52/1000 | Loss: 0.00000964
Iteration 53/1000 | Loss: 0.00000964
Iteration 54/1000 | Loss: 0.00000964
Iteration 55/1000 | Loss: 0.00000964
Iteration 56/1000 | Loss: 0.00000964
Iteration 57/1000 | Loss: 0.00000963
Iteration 58/1000 | Loss: 0.00000963
Iteration 59/1000 | Loss: 0.00000963
Iteration 60/1000 | Loss: 0.00000962
Iteration 61/1000 | Loss: 0.00000961
Iteration 62/1000 | Loss: 0.00000961
Iteration 63/1000 | Loss: 0.00000961
Iteration 64/1000 | Loss: 0.00000961
Iteration 65/1000 | Loss: 0.00000961
Iteration 66/1000 | Loss: 0.00000961
Iteration 67/1000 | Loss: 0.00000961
Iteration 68/1000 | Loss: 0.00000961
Iteration 69/1000 | Loss: 0.00000961
Iteration 70/1000 | Loss: 0.00000960
Iteration 71/1000 | Loss: 0.00000960
Iteration 72/1000 | Loss: 0.00000960
Iteration 73/1000 | Loss: 0.00000960
Iteration 74/1000 | Loss: 0.00000959
Iteration 75/1000 | Loss: 0.00000959
Iteration 76/1000 | Loss: 0.00000959
Iteration 77/1000 | Loss: 0.00000959
Iteration 78/1000 | Loss: 0.00000959
Iteration 79/1000 | Loss: 0.00000959
Iteration 80/1000 | Loss: 0.00000959
Iteration 81/1000 | Loss: 0.00000958
Iteration 82/1000 | Loss: 0.00000958
Iteration 83/1000 | Loss: 0.00000958
Iteration 84/1000 | Loss: 0.00000958
Iteration 85/1000 | Loss: 0.00000958
Iteration 86/1000 | Loss: 0.00000958
Iteration 87/1000 | Loss: 0.00000958
Iteration 88/1000 | Loss: 0.00000958
Iteration 89/1000 | Loss: 0.00000958
Iteration 90/1000 | Loss: 0.00000958
Iteration 91/1000 | Loss: 0.00000958
Iteration 92/1000 | Loss: 0.00000957
Iteration 93/1000 | Loss: 0.00000957
Iteration 94/1000 | Loss: 0.00000957
Iteration 95/1000 | Loss: 0.00000957
Iteration 96/1000 | Loss: 0.00000956
Iteration 97/1000 | Loss: 0.00000956
Iteration 98/1000 | Loss: 0.00000955
Iteration 99/1000 | Loss: 0.00000955
Iteration 100/1000 | Loss: 0.00000955
Iteration 101/1000 | Loss: 0.00000955
Iteration 102/1000 | Loss: 0.00000954
Iteration 103/1000 | Loss: 0.00000954
Iteration 104/1000 | Loss: 0.00000954
Iteration 105/1000 | Loss: 0.00000953
Iteration 106/1000 | Loss: 0.00000953
Iteration 107/1000 | Loss: 0.00000953
Iteration 108/1000 | Loss: 0.00000952
Iteration 109/1000 | Loss: 0.00000952
Iteration 110/1000 | Loss: 0.00000952
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000952
Iteration 113/1000 | Loss: 0.00000952
Iteration 114/1000 | Loss: 0.00000952
Iteration 115/1000 | Loss: 0.00000952
Iteration 116/1000 | Loss: 0.00000952
Iteration 117/1000 | Loss: 0.00000952
Iteration 118/1000 | Loss: 0.00000951
Iteration 119/1000 | Loss: 0.00000951
Iteration 120/1000 | Loss: 0.00000951
Iteration 121/1000 | Loss: 0.00000951
Iteration 122/1000 | Loss: 0.00000951
Iteration 123/1000 | Loss: 0.00000950
Iteration 124/1000 | Loss: 0.00000950
Iteration 125/1000 | Loss: 0.00000950
Iteration 126/1000 | Loss: 0.00000949
Iteration 127/1000 | Loss: 0.00000949
Iteration 128/1000 | Loss: 0.00000949
Iteration 129/1000 | Loss: 0.00000949
Iteration 130/1000 | Loss: 0.00000949
Iteration 131/1000 | Loss: 0.00000949
Iteration 132/1000 | Loss: 0.00000948
Iteration 133/1000 | Loss: 0.00000948
Iteration 134/1000 | Loss: 0.00000948
Iteration 135/1000 | Loss: 0.00000948
Iteration 136/1000 | Loss: 0.00000948
Iteration 137/1000 | Loss: 0.00000948
Iteration 138/1000 | Loss: 0.00000947
Iteration 139/1000 | Loss: 0.00000947
Iteration 140/1000 | Loss: 0.00000947
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000946
Iteration 146/1000 | Loss: 0.00000946
Iteration 147/1000 | Loss: 0.00000946
Iteration 148/1000 | Loss: 0.00000945
Iteration 149/1000 | Loss: 0.00000945
Iteration 150/1000 | Loss: 0.00000945
Iteration 151/1000 | Loss: 0.00000945
Iteration 152/1000 | Loss: 0.00000945
Iteration 153/1000 | Loss: 0.00000945
Iteration 154/1000 | Loss: 0.00000944
Iteration 155/1000 | Loss: 0.00000944
Iteration 156/1000 | Loss: 0.00000944
Iteration 157/1000 | Loss: 0.00000944
Iteration 158/1000 | Loss: 0.00000944
Iteration 159/1000 | Loss: 0.00000944
Iteration 160/1000 | Loss: 0.00000944
Iteration 161/1000 | Loss: 0.00000944
Iteration 162/1000 | Loss: 0.00000944
Iteration 163/1000 | Loss: 0.00000944
Iteration 164/1000 | Loss: 0.00000944
Iteration 165/1000 | Loss: 0.00000944
Iteration 166/1000 | Loss: 0.00000944
Iteration 167/1000 | Loss: 0.00000944
Iteration 168/1000 | Loss: 0.00000943
Iteration 169/1000 | Loss: 0.00000943
Iteration 170/1000 | Loss: 0.00000943
Iteration 171/1000 | Loss: 0.00000942
Iteration 172/1000 | Loss: 0.00000942
Iteration 173/1000 | Loss: 0.00000942
Iteration 174/1000 | Loss: 0.00000942
Iteration 175/1000 | Loss: 0.00000942
Iteration 176/1000 | Loss: 0.00000941
Iteration 177/1000 | Loss: 0.00000941
Iteration 178/1000 | Loss: 0.00000941
Iteration 179/1000 | Loss: 0.00000941
Iteration 180/1000 | Loss: 0.00000941
Iteration 181/1000 | Loss: 0.00000941
Iteration 182/1000 | Loss: 0.00000941
Iteration 183/1000 | Loss: 0.00000940
Iteration 184/1000 | Loss: 0.00000940
Iteration 185/1000 | Loss: 0.00000940
Iteration 186/1000 | Loss: 0.00000940
Iteration 187/1000 | Loss: 0.00000940
Iteration 188/1000 | Loss: 0.00000940
Iteration 189/1000 | Loss: 0.00000940
Iteration 190/1000 | Loss: 0.00000940
Iteration 191/1000 | Loss: 0.00000939
Iteration 192/1000 | Loss: 0.00000939
Iteration 193/1000 | Loss: 0.00000939
Iteration 194/1000 | Loss: 0.00000939
Iteration 195/1000 | Loss: 0.00000939
Iteration 196/1000 | Loss: 0.00000939
Iteration 197/1000 | Loss: 0.00000939
Iteration 198/1000 | Loss: 0.00000939
Iteration 199/1000 | Loss: 0.00000939
Iteration 200/1000 | Loss: 0.00000939
Iteration 201/1000 | Loss: 0.00000939
Iteration 202/1000 | Loss: 0.00000939
Iteration 203/1000 | Loss: 0.00000939
Iteration 204/1000 | Loss: 0.00000938
Iteration 205/1000 | Loss: 0.00000938
Iteration 206/1000 | Loss: 0.00000938
Iteration 207/1000 | Loss: 0.00000938
Iteration 208/1000 | Loss: 0.00000938
Iteration 209/1000 | Loss: 0.00000938
Iteration 210/1000 | Loss: 0.00000937
Iteration 211/1000 | Loss: 0.00000937
Iteration 212/1000 | Loss: 0.00000937
Iteration 213/1000 | Loss: 0.00000937
Iteration 214/1000 | Loss: 0.00000937
Iteration 215/1000 | Loss: 0.00000936
Iteration 216/1000 | Loss: 0.00000936
Iteration 217/1000 | Loss: 0.00000936
Iteration 218/1000 | Loss: 0.00000936
Iteration 219/1000 | Loss: 0.00000936
Iteration 220/1000 | Loss: 0.00000936
Iteration 221/1000 | Loss: 0.00000936
Iteration 222/1000 | Loss: 0.00000936
Iteration 223/1000 | Loss: 0.00000936
Iteration 224/1000 | Loss: 0.00000936
Iteration 225/1000 | Loss: 0.00000936
Iteration 226/1000 | Loss: 0.00000936
Iteration 227/1000 | Loss: 0.00000936
Iteration 228/1000 | Loss: 0.00000936
Iteration 229/1000 | Loss: 0.00000936
Iteration 230/1000 | Loss: 0.00000936
Iteration 231/1000 | Loss: 0.00000936
Iteration 232/1000 | Loss: 0.00000936
Iteration 233/1000 | Loss: 0.00000936
Iteration 234/1000 | Loss: 0.00000936
Iteration 235/1000 | Loss: 0.00000936
Iteration 236/1000 | Loss: 0.00000936
Iteration 237/1000 | Loss: 0.00000936
Iteration 238/1000 | Loss: 0.00000936
Iteration 239/1000 | Loss: 0.00000936
Iteration 240/1000 | Loss: 0.00000936
Iteration 241/1000 | Loss: 0.00000936
Iteration 242/1000 | Loss: 0.00000936
Iteration 243/1000 | Loss: 0.00000936
Iteration 244/1000 | Loss: 0.00000936
Iteration 245/1000 | Loss: 0.00000936
Iteration 246/1000 | Loss: 0.00000936
Iteration 247/1000 | Loss: 0.00000936
Iteration 248/1000 | Loss: 0.00000936
Iteration 249/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [9.356702321383636e-06, 9.356702321383636e-06, 9.356702321383636e-06, 9.356702321383636e-06, 9.356702321383636e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.356702321383636e-06

Optimization complete. Final v2v error: 2.634016752243042 mm

Highest mean error: 2.929607629776001 mm for frame 86

Lowest mean error: 2.523406744003296 mm for frame 4

Saving results

Total time: 42.339927196502686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967031
Iteration 2/25 | Loss: 0.00206308
Iteration 3/25 | Loss: 0.00135836
Iteration 4/25 | Loss: 0.00125550
Iteration 5/25 | Loss: 0.00123052
Iteration 6/25 | Loss: 0.00122098
Iteration 7/25 | Loss: 0.00125092
Iteration 8/25 | Loss: 0.00123798
Iteration 9/25 | Loss: 0.00120050
Iteration 10/25 | Loss: 0.00118468
Iteration 11/25 | Loss: 0.00117185
Iteration 12/25 | Loss: 0.00116931
Iteration 13/25 | Loss: 0.00117059
Iteration 14/25 | Loss: 0.00117222
Iteration 15/25 | Loss: 0.00116825
Iteration 16/25 | Loss: 0.00116618
Iteration 17/25 | Loss: 0.00116546
Iteration 18/25 | Loss: 0.00116523
Iteration 19/25 | Loss: 0.00116521
Iteration 20/25 | Loss: 0.00116520
Iteration 21/25 | Loss: 0.00116520
Iteration 22/25 | Loss: 0.00116520
Iteration 23/25 | Loss: 0.00116519
Iteration 24/25 | Loss: 0.00116519
Iteration 25/25 | Loss: 0.00116519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34432447
Iteration 2/25 | Loss: 0.00080850
Iteration 3/25 | Loss: 0.00080413
Iteration 4/25 | Loss: 0.00080413
Iteration 5/25 | Loss: 0.00080413
Iteration 6/25 | Loss: 0.00080413
Iteration 7/25 | Loss: 0.00080413
Iteration 8/25 | Loss: 0.00080413
Iteration 9/25 | Loss: 0.00080413
Iteration 10/25 | Loss: 0.00080413
Iteration 11/25 | Loss: 0.00080413
Iteration 12/25 | Loss: 0.00080413
Iteration 13/25 | Loss: 0.00080413
Iteration 14/25 | Loss: 0.00080413
Iteration 15/25 | Loss: 0.00080413
Iteration 16/25 | Loss: 0.00080413
Iteration 17/25 | Loss: 0.00080413
Iteration 18/25 | Loss: 0.00080413
Iteration 19/25 | Loss: 0.00080413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008041253313422203, 0.0008041253313422203, 0.0008041253313422203, 0.0008041253313422203, 0.0008041253313422203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008041253313422203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080413
Iteration 2/1000 | Loss: 0.00028062
Iteration 3/1000 | Loss: 0.00051330
Iteration 4/1000 | Loss: 0.00013052
Iteration 5/1000 | Loss: 0.00002393
Iteration 6/1000 | Loss: 0.00027270
Iteration 7/1000 | Loss: 0.00011841
Iteration 8/1000 | Loss: 0.00002172
Iteration 9/1000 | Loss: 0.00002427
Iteration 10/1000 | Loss: 0.00016276
Iteration 11/1000 | Loss: 0.00005384
Iteration 12/1000 | Loss: 0.00006319
Iteration 13/1000 | Loss: 0.00002391
Iteration 14/1000 | Loss: 0.00002375
Iteration 15/1000 | Loss: 0.00025066
Iteration 16/1000 | Loss: 0.00011592
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002030
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001689
Iteration 45/1000 | Loss: 0.00001689
Iteration 46/1000 | Loss: 0.00001689
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001688
Iteration 49/1000 | Loss: 0.00001688
Iteration 50/1000 | Loss: 0.00001688
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001684
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001836
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001679
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.6772401068010367e-05, 1.6772401068010367e-05, 1.6772401068010367e-05, 1.6772401068010367e-05, 1.6772401068010367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6772401068010367e-05

Optimization complete. Final v2v error: 3.4095025062561035 mm

Highest mean error: 5.518326282501221 mm for frame 65

Lowest mean error: 2.751176595687866 mm for frame 135

Saving results

Total time: 86.86271715164185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963162
Iteration 2/25 | Loss: 0.00229855
Iteration 3/25 | Loss: 0.00160043
Iteration 4/25 | Loss: 0.00147947
Iteration 5/25 | Loss: 0.00139524
Iteration 6/25 | Loss: 0.00135931
Iteration 7/25 | Loss: 0.00132905
Iteration 8/25 | Loss: 0.00126928
Iteration 9/25 | Loss: 0.00124230
Iteration 10/25 | Loss: 0.00123154
Iteration 11/25 | Loss: 0.00122637
Iteration 12/25 | Loss: 0.00122308
Iteration 13/25 | Loss: 0.00122237
Iteration 14/25 | Loss: 0.00122224
Iteration 15/25 | Loss: 0.00122224
Iteration 16/25 | Loss: 0.00122224
Iteration 17/25 | Loss: 0.00122223
Iteration 18/25 | Loss: 0.00122223
Iteration 19/25 | Loss: 0.00122223
Iteration 20/25 | Loss: 0.00122223
Iteration 21/25 | Loss: 0.00122223
Iteration 22/25 | Loss: 0.00122223
Iteration 23/25 | Loss: 0.00122223
Iteration 24/25 | Loss: 0.00122223
Iteration 25/25 | Loss: 0.00122223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29870510
Iteration 2/25 | Loss: 0.00083035
Iteration 3/25 | Loss: 0.00083035
Iteration 4/25 | Loss: 0.00083035
Iteration 5/25 | Loss: 0.00083035
Iteration 6/25 | Loss: 0.00083035
Iteration 7/25 | Loss: 0.00083035
Iteration 8/25 | Loss: 0.00083035
Iteration 9/25 | Loss: 0.00083035
Iteration 10/25 | Loss: 0.00083035
Iteration 11/25 | Loss: 0.00083035
Iteration 12/25 | Loss: 0.00083035
Iteration 13/25 | Loss: 0.00083035
Iteration 14/25 | Loss: 0.00083035
Iteration 15/25 | Loss: 0.00083035
Iteration 16/25 | Loss: 0.00083035
Iteration 17/25 | Loss: 0.00083035
Iteration 18/25 | Loss: 0.00083035
Iteration 19/25 | Loss: 0.00083035
Iteration 20/25 | Loss: 0.00083035
Iteration 21/25 | Loss: 0.00083035
Iteration 22/25 | Loss: 0.00083035
Iteration 23/25 | Loss: 0.00083035
Iteration 24/25 | Loss: 0.00083035
Iteration 25/25 | Loss: 0.00083035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083035
Iteration 2/1000 | Loss: 0.00006125
Iteration 3/1000 | Loss: 0.00004167
Iteration 4/1000 | Loss: 0.00029606
Iteration 5/1000 | Loss: 0.00027826
Iteration 6/1000 | Loss: 0.00024076
Iteration 7/1000 | Loss: 0.00021046
Iteration 8/1000 | Loss: 0.00003158
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002840
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002710
Iteration 13/1000 | Loss: 0.00002659
Iteration 14/1000 | Loss: 0.00002621
Iteration 15/1000 | Loss: 0.00002598
Iteration 16/1000 | Loss: 0.00002587
Iteration 17/1000 | Loss: 0.00002576
Iteration 18/1000 | Loss: 0.00002566
Iteration 19/1000 | Loss: 0.00002548
Iteration 20/1000 | Loss: 0.00002544
Iteration 21/1000 | Loss: 0.00002543
Iteration 22/1000 | Loss: 0.00002536
Iteration 23/1000 | Loss: 0.00002531
Iteration 24/1000 | Loss: 0.00002525
Iteration 25/1000 | Loss: 0.00002525
Iteration 26/1000 | Loss: 0.00002525
Iteration 27/1000 | Loss: 0.00002524
Iteration 28/1000 | Loss: 0.00002524
Iteration 29/1000 | Loss: 0.00002523
Iteration 30/1000 | Loss: 0.00002522
Iteration 31/1000 | Loss: 0.00002521
Iteration 32/1000 | Loss: 0.00002517
Iteration 33/1000 | Loss: 0.00002517
Iteration 34/1000 | Loss: 0.00002517
Iteration 35/1000 | Loss: 0.00002516
Iteration 36/1000 | Loss: 0.00002516
Iteration 37/1000 | Loss: 0.00002516
Iteration 38/1000 | Loss: 0.00002516
Iteration 39/1000 | Loss: 0.00002516
Iteration 40/1000 | Loss: 0.00002516
Iteration 41/1000 | Loss: 0.00002516
Iteration 42/1000 | Loss: 0.00002516
Iteration 43/1000 | Loss: 0.00002516
Iteration 44/1000 | Loss: 0.00002516
Iteration 45/1000 | Loss: 0.00002515
Iteration 46/1000 | Loss: 0.00002515
Iteration 47/1000 | Loss: 0.00002513
Iteration 48/1000 | Loss: 0.00002513
Iteration 49/1000 | Loss: 0.00002513
Iteration 50/1000 | Loss: 0.00002513
Iteration 51/1000 | Loss: 0.00002513
Iteration 52/1000 | Loss: 0.00002512
Iteration 53/1000 | Loss: 0.00002512
Iteration 54/1000 | Loss: 0.00002512
Iteration 55/1000 | Loss: 0.00002512
Iteration 56/1000 | Loss: 0.00002512
Iteration 57/1000 | Loss: 0.00002512
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00002512
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002512
Iteration 63/1000 | Loss: 0.00002512
Iteration 64/1000 | Loss: 0.00002512
Iteration 65/1000 | Loss: 0.00002512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.5122237275354564e-05, 2.5122237275354564e-05, 2.5122237275354564e-05, 2.5122237275354564e-05, 2.5122237275354564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5122237275354564e-05

Optimization complete. Final v2v error: 3.767320156097412 mm

Highest mean error: 11.636972427368164 mm for frame 44

Lowest mean error: 3.345670461654663 mm for frame 55

Saving results

Total time: 54.57459282875061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398531
Iteration 2/25 | Loss: 0.00130873
Iteration 3/25 | Loss: 0.00110539
Iteration 4/25 | Loss: 0.00108952
Iteration 5/25 | Loss: 0.00108734
Iteration 6/25 | Loss: 0.00108675
Iteration 7/25 | Loss: 0.00108675
Iteration 8/25 | Loss: 0.00108675
Iteration 9/25 | Loss: 0.00108675
Iteration 10/25 | Loss: 0.00108675
Iteration 11/25 | Loss: 0.00108675
Iteration 12/25 | Loss: 0.00108675
Iteration 13/25 | Loss: 0.00108675
Iteration 14/25 | Loss: 0.00108675
Iteration 15/25 | Loss: 0.00108675
Iteration 16/25 | Loss: 0.00108675
Iteration 17/25 | Loss: 0.00108675
Iteration 18/25 | Loss: 0.00108675
Iteration 19/25 | Loss: 0.00108675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010867457604035735, 0.0010867457604035735, 0.0010867457604035735, 0.0010867457604035735, 0.0010867457604035735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010867457604035735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34375644
Iteration 2/25 | Loss: 0.00062998
Iteration 3/25 | Loss: 0.00062998
Iteration 4/25 | Loss: 0.00062998
Iteration 5/25 | Loss: 0.00062998
Iteration 6/25 | Loss: 0.00062998
Iteration 7/25 | Loss: 0.00062998
Iteration 8/25 | Loss: 0.00062998
Iteration 9/25 | Loss: 0.00062998
Iteration 10/25 | Loss: 0.00062998
Iteration 11/25 | Loss: 0.00062998
Iteration 12/25 | Loss: 0.00062998
Iteration 13/25 | Loss: 0.00062998
Iteration 14/25 | Loss: 0.00062998
Iteration 15/25 | Loss: 0.00062998
Iteration 16/25 | Loss: 0.00062998
Iteration 17/25 | Loss: 0.00062998
Iteration 18/25 | Loss: 0.00062998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006299759261310101, 0.0006299759261310101, 0.0006299759261310101, 0.0006299759261310101, 0.0006299759261310101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006299759261310101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062998
Iteration 2/1000 | Loss: 0.00002422
Iteration 3/1000 | Loss: 0.00001630
Iteration 4/1000 | Loss: 0.00001439
Iteration 5/1000 | Loss: 0.00001357
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001164
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001148
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001145
Iteration 23/1000 | Loss: 0.00001145
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001145
Iteration 27/1000 | Loss: 0.00001145
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001144
Iteration 33/1000 | Loss: 0.00001144
Iteration 34/1000 | Loss: 0.00001143
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001126
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001116
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001106
Iteration 123/1000 | Loss: 0.00001106
Iteration 124/1000 | Loss: 0.00001106
Iteration 125/1000 | Loss: 0.00001106
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001106
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001104
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001104
Iteration 137/1000 | Loss: 0.00001104
Iteration 138/1000 | Loss: 0.00001104
Iteration 139/1000 | Loss: 0.00001104
Iteration 140/1000 | Loss: 0.00001104
Iteration 141/1000 | Loss: 0.00001104
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001103
Iteration 147/1000 | Loss: 0.00001103
Iteration 148/1000 | Loss: 0.00001103
Iteration 149/1000 | Loss: 0.00001103
Iteration 150/1000 | Loss: 0.00001103
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001100
Iteration 190/1000 | Loss: 0.00001100
Iteration 191/1000 | Loss: 0.00001100
Iteration 192/1000 | Loss: 0.00001100
Iteration 193/1000 | Loss: 0.00001100
Iteration 194/1000 | Loss: 0.00001100
Iteration 195/1000 | Loss: 0.00001100
Iteration 196/1000 | Loss: 0.00001100
Iteration 197/1000 | Loss: 0.00001100
Iteration 198/1000 | Loss: 0.00001100
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001100
Iteration 203/1000 | Loss: 0.00001100
Iteration 204/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0999840924341697e-05, 1.0999840924341697e-05, 1.0999840924341697e-05, 1.0999840924341697e-05, 1.0999840924341697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0999840924341697e-05

Optimization complete. Final v2v error: 2.8484597206115723 mm

Highest mean error: 2.973231554031372 mm for frame 68

Lowest mean error: 2.713744878768921 mm for frame 1

Saving results

Total time: 41.12931275367737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746415
Iteration 2/25 | Loss: 0.00119174
Iteration 3/25 | Loss: 0.00105460
Iteration 4/25 | Loss: 0.00104181
Iteration 5/25 | Loss: 0.00103855
Iteration 6/25 | Loss: 0.00103820
Iteration 7/25 | Loss: 0.00103820
Iteration 8/25 | Loss: 0.00103820
Iteration 9/25 | Loss: 0.00103820
Iteration 10/25 | Loss: 0.00103820
Iteration 11/25 | Loss: 0.00103820
Iteration 12/25 | Loss: 0.00103820
Iteration 13/25 | Loss: 0.00103820
Iteration 14/25 | Loss: 0.00103820
Iteration 15/25 | Loss: 0.00103820
Iteration 16/25 | Loss: 0.00103820
Iteration 17/25 | Loss: 0.00103820
Iteration 18/25 | Loss: 0.00103820
Iteration 19/25 | Loss: 0.00103820
Iteration 20/25 | Loss: 0.00103820
Iteration 21/25 | Loss: 0.00103820
Iteration 22/25 | Loss: 0.00103820
Iteration 23/25 | Loss: 0.00103820
Iteration 24/25 | Loss: 0.00103820
Iteration 25/25 | Loss: 0.00103820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010382028995081782, 0.0010382028995081782, 0.0010382028995081782, 0.0010382028995081782, 0.0010382028995081782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010382028995081782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31369746
Iteration 2/25 | Loss: 0.00060761
Iteration 3/25 | Loss: 0.00060758
Iteration 4/25 | Loss: 0.00060758
Iteration 5/25 | Loss: 0.00060758
Iteration 6/25 | Loss: 0.00060758
Iteration 7/25 | Loss: 0.00060758
Iteration 8/25 | Loss: 0.00060758
Iteration 9/25 | Loss: 0.00060758
Iteration 10/25 | Loss: 0.00060758
Iteration 11/25 | Loss: 0.00060758
Iteration 12/25 | Loss: 0.00060758
Iteration 13/25 | Loss: 0.00060758
Iteration 14/25 | Loss: 0.00060758
Iteration 15/25 | Loss: 0.00060758
Iteration 16/25 | Loss: 0.00060758
Iteration 17/25 | Loss: 0.00060758
Iteration 18/25 | Loss: 0.00060758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006075766286812723, 0.0006075766286812723, 0.0006075766286812723, 0.0006075766286812723, 0.0006075766286812723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006075766286812723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060758
Iteration 2/1000 | Loss: 0.00002824
Iteration 3/1000 | Loss: 0.00002093
Iteration 4/1000 | Loss: 0.00001763
Iteration 5/1000 | Loss: 0.00001621
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001179
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001163
Iteration 27/1000 | Loss: 0.00001163
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001154
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001153
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001151
Iteration 64/1000 | Loss: 0.00001151
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001150
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001145
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001144
Iteration 89/1000 | Loss: 0.00001144
Iteration 90/1000 | Loss: 0.00001144
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001141
Iteration 100/1000 | Loss: 0.00001141
Iteration 101/1000 | Loss: 0.00001141
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001138
Iteration 113/1000 | Loss: 0.00001138
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001135
Iteration 121/1000 | Loss: 0.00001135
Iteration 122/1000 | Loss: 0.00001135
Iteration 123/1000 | Loss: 0.00001135
Iteration 124/1000 | Loss: 0.00001134
Iteration 125/1000 | Loss: 0.00001134
Iteration 126/1000 | Loss: 0.00001133
Iteration 127/1000 | Loss: 0.00001133
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001132
Iteration 130/1000 | Loss: 0.00001132
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.1281210390734486e-05, 1.1281210390734486e-05, 1.1281210390734486e-05, 1.1281210390734486e-05, 1.1281210390734486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1281210390734486e-05

Optimization complete. Final v2v error: 2.87614369392395 mm

Highest mean error: 3.3104701042175293 mm for frame 193

Lowest mean error: 2.5494518280029297 mm for frame 67

Saving results

Total time: 47.72166299819946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444957
Iteration 2/25 | Loss: 0.00116444
Iteration 3/25 | Loss: 0.00110954
Iteration 4/25 | Loss: 0.00110134
Iteration 5/25 | Loss: 0.00109849
Iteration 6/25 | Loss: 0.00109842
Iteration 7/25 | Loss: 0.00109842
Iteration 8/25 | Loss: 0.00109842
Iteration 9/25 | Loss: 0.00109842
Iteration 10/25 | Loss: 0.00109842
Iteration 11/25 | Loss: 0.00109842
Iteration 12/25 | Loss: 0.00109842
Iteration 13/25 | Loss: 0.00109842
Iteration 14/25 | Loss: 0.00109842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010984200052917004, 0.0010984200052917004, 0.0010984200052917004, 0.0010984200052917004, 0.0010984200052917004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010984200052917004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36714077
Iteration 2/25 | Loss: 0.00092905
Iteration 3/25 | Loss: 0.00092905
Iteration 4/25 | Loss: 0.00092905
Iteration 5/25 | Loss: 0.00092905
Iteration 6/25 | Loss: 0.00092905
Iteration 7/25 | Loss: 0.00092905
Iteration 8/25 | Loss: 0.00092905
Iteration 9/25 | Loss: 0.00092905
Iteration 10/25 | Loss: 0.00092905
Iteration 11/25 | Loss: 0.00092905
Iteration 12/25 | Loss: 0.00092905
Iteration 13/25 | Loss: 0.00092905
Iteration 14/25 | Loss: 0.00092905
Iteration 15/25 | Loss: 0.00092905
Iteration 16/25 | Loss: 0.00092905
Iteration 17/25 | Loss: 0.00092905
Iteration 18/25 | Loss: 0.00092905
Iteration 19/25 | Loss: 0.00092905
Iteration 20/25 | Loss: 0.00092905
Iteration 21/25 | Loss: 0.00092905
Iteration 22/25 | Loss: 0.00092905
Iteration 23/25 | Loss: 0.00092905
Iteration 24/25 | Loss: 0.00092905
Iteration 25/25 | Loss: 0.00092905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092905
Iteration 2/1000 | Loss: 0.00002184
Iteration 3/1000 | Loss: 0.00001755
Iteration 4/1000 | Loss: 0.00001616
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001445
Iteration 13/1000 | Loss: 0.00001437
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001384
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001379
Iteration 62/1000 | Loss: 0.00001379
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001363
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001362
Iteration 101/1000 | Loss: 0.00001362
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001360
Iteration 105/1000 | Loss: 0.00001360
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001357
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001357
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001356
Iteration 132/1000 | Loss: 0.00001356
Iteration 133/1000 | Loss: 0.00001356
Iteration 134/1000 | Loss: 0.00001356
Iteration 135/1000 | Loss: 0.00001356
Iteration 136/1000 | Loss: 0.00001356
Iteration 137/1000 | Loss: 0.00001356
Iteration 138/1000 | Loss: 0.00001356
Iteration 139/1000 | Loss: 0.00001356
Iteration 140/1000 | Loss: 0.00001356
Iteration 141/1000 | Loss: 0.00001356
Iteration 142/1000 | Loss: 0.00001356
Iteration 143/1000 | Loss: 0.00001356
Iteration 144/1000 | Loss: 0.00001356
Iteration 145/1000 | Loss: 0.00001356
Iteration 146/1000 | Loss: 0.00001356
Iteration 147/1000 | Loss: 0.00001356
Iteration 148/1000 | Loss: 0.00001356
Iteration 149/1000 | Loss: 0.00001356
Iteration 150/1000 | Loss: 0.00001356
Iteration 151/1000 | Loss: 0.00001356
Iteration 152/1000 | Loss: 0.00001356
Iteration 153/1000 | Loss: 0.00001356
Iteration 154/1000 | Loss: 0.00001356
Iteration 155/1000 | Loss: 0.00001356
Iteration 156/1000 | Loss: 0.00001356
Iteration 157/1000 | Loss: 0.00001356
Iteration 158/1000 | Loss: 0.00001356
Iteration 159/1000 | Loss: 0.00001356
Iteration 160/1000 | Loss: 0.00001356
Iteration 161/1000 | Loss: 0.00001356
Iteration 162/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.3559862054535188e-05, 1.3559862054535188e-05, 1.3559862054535188e-05, 1.3559862054535188e-05, 1.3559862054535188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3559862054535188e-05

Optimization complete. Final v2v error: 3.0807113647460938 mm

Highest mean error: 3.614969491958618 mm for frame 131

Lowest mean error: 2.765045166015625 mm for frame 25

Saving results

Total time: 43.61692214012146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795701
Iteration 2/25 | Loss: 0.00114395
Iteration 3/25 | Loss: 0.00106457
Iteration 4/25 | Loss: 0.00105976
Iteration 5/25 | Loss: 0.00105875
Iteration 6/25 | Loss: 0.00105875
Iteration 7/25 | Loss: 0.00105875
Iteration 8/25 | Loss: 0.00105875
Iteration 9/25 | Loss: 0.00105875
Iteration 10/25 | Loss: 0.00105875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010587509023025632, 0.0010587509023025632, 0.0010587509023025632, 0.0010587509023025632, 0.0010587509023025632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010587509023025632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35585642
Iteration 2/25 | Loss: 0.00080569
Iteration 3/25 | Loss: 0.00080568
Iteration 4/25 | Loss: 0.00080568
Iteration 5/25 | Loss: 0.00080568
Iteration 6/25 | Loss: 0.00080568
Iteration 7/25 | Loss: 0.00080568
Iteration 8/25 | Loss: 0.00080568
Iteration 9/25 | Loss: 0.00080568
Iteration 10/25 | Loss: 0.00080568
Iteration 11/25 | Loss: 0.00080568
Iteration 12/25 | Loss: 0.00080568
Iteration 13/25 | Loss: 0.00080568
Iteration 14/25 | Loss: 0.00080568
Iteration 15/25 | Loss: 0.00080568
Iteration 16/25 | Loss: 0.00080568
Iteration 17/25 | Loss: 0.00080568
Iteration 18/25 | Loss: 0.00080568
Iteration 19/25 | Loss: 0.00080568
Iteration 20/25 | Loss: 0.00080568
Iteration 21/25 | Loss: 0.00080568
Iteration 22/25 | Loss: 0.00080568
Iteration 23/25 | Loss: 0.00080568
Iteration 24/25 | Loss: 0.00080568
Iteration 25/25 | Loss: 0.00080568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080568
Iteration 2/1000 | Loss: 0.00002084
Iteration 3/1000 | Loss: 0.00001341
Iteration 4/1000 | Loss: 0.00001188
Iteration 5/1000 | Loss: 0.00001113
Iteration 6/1000 | Loss: 0.00001058
Iteration 7/1000 | Loss: 0.00001018
Iteration 8/1000 | Loss: 0.00000989
Iteration 9/1000 | Loss: 0.00000968
Iteration 10/1000 | Loss: 0.00000945
Iteration 11/1000 | Loss: 0.00000940
Iteration 12/1000 | Loss: 0.00000938
Iteration 13/1000 | Loss: 0.00000937
Iteration 14/1000 | Loss: 0.00000936
Iteration 15/1000 | Loss: 0.00000929
Iteration 16/1000 | Loss: 0.00000925
Iteration 17/1000 | Loss: 0.00000921
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000915
Iteration 22/1000 | Loss: 0.00000913
Iteration 23/1000 | Loss: 0.00000912
Iteration 24/1000 | Loss: 0.00000911
Iteration 25/1000 | Loss: 0.00000910
Iteration 26/1000 | Loss: 0.00000909
Iteration 27/1000 | Loss: 0.00000908
Iteration 28/1000 | Loss: 0.00000908
Iteration 29/1000 | Loss: 0.00000908
Iteration 30/1000 | Loss: 0.00000908
Iteration 31/1000 | Loss: 0.00000908
Iteration 32/1000 | Loss: 0.00000907
Iteration 33/1000 | Loss: 0.00000907
Iteration 34/1000 | Loss: 0.00000907
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000903
Iteration 38/1000 | Loss: 0.00000903
Iteration 39/1000 | Loss: 0.00000903
Iteration 40/1000 | Loss: 0.00000902
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000900
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000899
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000898
Iteration 47/1000 | Loss: 0.00000898
Iteration 48/1000 | Loss: 0.00000898
Iteration 49/1000 | Loss: 0.00000897
Iteration 50/1000 | Loss: 0.00000897
Iteration 51/1000 | Loss: 0.00000896
Iteration 52/1000 | Loss: 0.00000895
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000894
Iteration 55/1000 | Loss: 0.00000894
Iteration 56/1000 | Loss: 0.00000894
Iteration 57/1000 | Loss: 0.00000894
Iteration 58/1000 | Loss: 0.00000894
Iteration 59/1000 | Loss: 0.00000894
Iteration 60/1000 | Loss: 0.00000893
Iteration 61/1000 | Loss: 0.00000893
Iteration 62/1000 | Loss: 0.00000893
Iteration 63/1000 | Loss: 0.00000893
Iteration 64/1000 | Loss: 0.00000892
Iteration 65/1000 | Loss: 0.00000892
Iteration 66/1000 | Loss: 0.00000890
Iteration 67/1000 | Loss: 0.00000890
Iteration 68/1000 | Loss: 0.00000890
Iteration 69/1000 | Loss: 0.00000890
Iteration 70/1000 | Loss: 0.00000890
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000889
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000887
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000885
Iteration 91/1000 | Loss: 0.00000885
Iteration 92/1000 | Loss: 0.00000885
Iteration 93/1000 | Loss: 0.00000885
Iteration 94/1000 | Loss: 0.00000884
Iteration 95/1000 | Loss: 0.00000883
Iteration 96/1000 | Loss: 0.00000883
Iteration 97/1000 | Loss: 0.00000883
Iteration 98/1000 | Loss: 0.00000882
Iteration 99/1000 | Loss: 0.00000882
Iteration 100/1000 | Loss: 0.00000882
Iteration 101/1000 | Loss: 0.00000881
Iteration 102/1000 | Loss: 0.00000881
Iteration 103/1000 | Loss: 0.00000880
Iteration 104/1000 | Loss: 0.00000880
Iteration 105/1000 | Loss: 0.00000880
Iteration 106/1000 | Loss: 0.00000880
Iteration 107/1000 | Loss: 0.00000879
Iteration 108/1000 | Loss: 0.00000879
Iteration 109/1000 | Loss: 0.00000879
Iteration 110/1000 | Loss: 0.00000879
Iteration 111/1000 | Loss: 0.00000878
Iteration 112/1000 | Loss: 0.00000878
Iteration 113/1000 | Loss: 0.00000878
Iteration 114/1000 | Loss: 0.00000877
Iteration 115/1000 | Loss: 0.00000877
Iteration 116/1000 | Loss: 0.00000877
Iteration 117/1000 | Loss: 0.00000877
Iteration 118/1000 | Loss: 0.00000877
Iteration 119/1000 | Loss: 0.00000877
Iteration 120/1000 | Loss: 0.00000877
Iteration 121/1000 | Loss: 0.00000877
Iteration 122/1000 | Loss: 0.00000876
Iteration 123/1000 | Loss: 0.00000876
Iteration 124/1000 | Loss: 0.00000876
Iteration 125/1000 | Loss: 0.00000876
Iteration 126/1000 | Loss: 0.00000876
Iteration 127/1000 | Loss: 0.00000876
Iteration 128/1000 | Loss: 0.00000876
Iteration 129/1000 | Loss: 0.00000876
Iteration 130/1000 | Loss: 0.00000876
Iteration 131/1000 | Loss: 0.00000875
Iteration 132/1000 | Loss: 0.00000874
Iteration 133/1000 | Loss: 0.00000874
Iteration 134/1000 | Loss: 0.00000874
Iteration 135/1000 | Loss: 0.00000874
Iteration 136/1000 | Loss: 0.00000874
Iteration 137/1000 | Loss: 0.00000873
Iteration 138/1000 | Loss: 0.00000873
Iteration 139/1000 | Loss: 0.00000873
Iteration 140/1000 | Loss: 0.00000873
Iteration 141/1000 | Loss: 0.00000873
Iteration 142/1000 | Loss: 0.00000873
Iteration 143/1000 | Loss: 0.00000873
Iteration 144/1000 | Loss: 0.00000873
Iteration 145/1000 | Loss: 0.00000873
Iteration 146/1000 | Loss: 0.00000873
Iteration 147/1000 | Loss: 0.00000873
Iteration 148/1000 | Loss: 0.00000872
Iteration 149/1000 | Loss: 0.00000872
Iteration 150/1000 | Loss: 0.00000872
Iteration 151/1000 | Loss: 0.00000871
Iteration 152/1000 | Loss: 0.00000871
Iteration 153/1000 | Loss: 0.00000871
Iteration 154/1000 | Loss: 0.00000871
Iteration 155/1000 | Loss: 0.00000871
Iteration 156/1000 | Loss: 0.00000871
Iteration 157/1000 | Loss: 0.00000871
Iteration 158/1000 | Loss: 0.00000871
Iteration 159/1000 | Loss: 0.00000871
Iteration 160/1000 | Loss: 0.00000870
Iteration 161/1000 | Loss: 0.00000870
Iteration 162/1000 | Loss: 0.00000870
Iteration 163/1000 | Loss: 0.00000870
Iteration 164/1000 | Loss: 0.00000870
Iteration 165/1000 | Loss: 0.00000870
Iteration 166/1000 | Loss: 0.00000870
Iteration 167/1000 | Loss: 0.00000870
Iteration 168/1000 | Loss: 0.00000869
Iteration 169/1000 | Loss: 0.00000869
Iteration 170/1000 | Loss: 0.00000869
Iteration 171/1000 | Loss: 0.00000869
Iteration 172/1000 | Loss: 0.00000869
Iteration 173/1000 | Loss: 0.00000869
Iteration 174/1000 | Loss: 0.00000868
Iteration 175/1000 | Loss: 0.00000868
Iteration 176/1000 | Loss: 0.00000868
Iteration 177/1000 | Loss: 0.00000868
Iteration 178/1000 | Loss: 0.00000868
Iteration 179/1000 | Loss: 0.00000868
Iteration 180/1000 | Loss: 0.00000868
Iteration 181/1000 | Loss: 0.00000868
Iteration 182/1000 | Loss: 0.00000868
Iteration 183/1000 | Loss: 0.00000868
Iteration 184/1000 | Loss: 0.00000868
Iteration 185/1000 | Loss: 0.00000868
Iteration 186/1000 | Loss: 0.00000868
Iteration 187/1000 | Loss: 0.00000868
Iteration 188/1000 | Loss: 0.00000868
Iteration 189/1000 | Loss: 0.00000868
Iteration 190/1000 | Loss: 0.00000868
Iteration 191/1000 | Loss: 0.00000867
Iteration 192/1000 | Loss: 0.00000867
Iteration 193/1000 | Loss: 0.00000867
Iteration 194/1000 | Loss: 0.00000867
Iteration 195/1000 | Loss: 0.00000867
Iteration 196/1000 | Loss: 0.00000867
Iteration 197/1000 | Loss: 0.00000867
Iteration 198/1000 | Loss: 0.00000867
Iteration 199/1000 | Loss: 0.00000867
Iteration 200/1000 | Loss: 0.00000867
Iteration 201/1000 | Loss: 0.00000867
Iteration 202/1000 | Loss: 0.00000867
Iteration 203/1000 | Loss: 0.00000867
Iteration 204/1000 | Loss: 0.00000867
Iteration 205/1000 | Loss: 0.00000867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [8.674746823089663e-06, 8.674746823089663e-06, 8.674746823089663e-06, 8.674746823089663e-06, 8.674746823089663e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.674746823089663e-06

Optimization complete. Final v2v error: 2.5140609741210938 mm

Highest mean error: 2.6916041374206543 mm for frame 64

Lowest mean error: 2.354541063308716 mm for frame 28

Saving results

Total time: 39.462170124053955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382898
Iteration 2/25 | Loss: 0.00121189
Iteration 3/25 | Loss: 0.00112373
Iteration 4/25 | Loss: 0.00110936
Iteration 5/25 | Loss: 0.00110334
Iteration 6/25 | Loss: 0.00110172
Iteration 7/25 | Loss: 0.00110172
Iteration 8/25 | Loss: 0.00110172
Iteration 9/25 | Loss: 0.00110172
Iteration 10/25 | Loss: 0.00110172
Iteration 11/25 | Loss: 0.00110172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011017188662663102, 0.0011017188662663102, 0.0011017188662663102, 0.0011017188662663102, 0.0011017188662663102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011017188662663102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40480518
Iteration 2/25 | Loss: 0.00095060
Iteration 3/25 | Loss: 0.00095058
Iteration 4/25 | Loss: 0.00095058
Iteration 5/25 | Loss: 0.00095058
Iteration 6/25 | Loss: 0.00095058
Iteration 7/25 | Loss: 0.00095058
Iteration 8/25 | Loss: 0.00095058
Iteration 9/25 | Loss: 0.00095058
Iteration 10/25 | Loss: 0.00095058
Iteration 11/25 | Loss: 0.00095058
Iteration 12/25 | Loss: 0.00095058
Iteration 13/25 | Loss: 0.00095058
Iteration 14/25 | Loss: 0.00095058
Iteration 15/25 | Loss: 0.00095058
Iteration 16/25 | Loss: 0.00095058
Iteration 17/25 | Loss: 0.00095058
Iteration 18/25 | Loss: 0.00095058
Iteration 19/25 | Loss: 0.00095058
Iteration 20/25 | Loss: 0.00095058
Iteration 21/25 | Loss: 0.00095058
Iteration 22/25 | Loss: 0.00095058
Iteration 23/25 | Loss: 0.00095058
Iteration 24/25 | Loss: 0.00095058
Iteration 25/25 | Loss: 0.00095058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095058
Iteration 2/1000 | Loss: 0.00002740
Iteration 3/1000 | Loss: 0.00001718
Iteration 4/1000 | Loss: 0.00001449
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001309
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001197
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001182
Iteration 67/1000 | Loss: 0.00001182
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001181
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001177
Iteration 122/1000 | Loss: 0.00001177
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001175
Iteration 137/1000 | Loss: 0.00001175
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001174
Iteration 142/1000 | Loss: 0.00001174
Iteration 143/1000 | Loss: 0.00001174
Iteration 144/1000 | Loss: 0.00001173
Iteration 145/1000 | Loss: 0.00001173
Iteration 146/1000 | Loss: 0.00001173
Iteration 147/1000 | Loss: 0.00001173
Iteration 148/1000 | Loss: 0.00001173
Iteration 149/1000 | Loss: 0.00001173
Iteration 150/1000 | Loss: 0.00001173
Iteration 151/1000 | Loss: 0.00001173
Iteration 152/1000 | Loss: 0.00001173
Iteration 153/1000 | Loss: 0.00001172
Iteration 154/1000 | Loss: 0.00001172
Iteration 155/1000 | Loss: 0.00001172
Iteration 156/1000 | Loss: 0.00001172
Iteration 157/1000 | Loss: 0.00001172
Iteration 158/1000 | Loss: 0.00001172
Iteration 159/1000 | Loss: 0.00001172
Iteration 160/1000 | Loss: 0.00001172
Iteration 161/1000 | Loss: 0.00001172
Iteration 162/1000 | Loss: 0.00001172
Iteration 163/1000 | Loss: 0.00001172
Iteration 164/1000 | Loss: 0.00001172
Iteration 165/1000 | Loss: 0.00001172
Iteration 166/1000 | Loss: 0.00001172
Iteration 167/1000 | Loss: 0.00001172
Iteration 168/1000 | Loss: 0.00001172
Iteration 169/1000 | Loss: 0.00001172
Iteration 170/1000 | Loss: 0.00001172
Iteration 171/1000 | Loss: 0.00001172
Iteration 172/1000 | Loss: 0.00001172
Iteration 173/1000 | Loss: 0.00001172
Iteration 174/1000 | Loss: 0.00001172
Iteration 175/1000 | Loss: 0.00001172
Iteration 176/1000 | Loss: 0.00001172
Iteration 177/1000 | Loss: 0.00001172
Iteration 178/1000 | Loss: 0.00001172
Iteration 179/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1716044355125632e-05, 1.1716044355125632e-05, 1.1716044355125632e-05, 1.1716044355125632e-05, 1.1716044355125632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1716044355125632e-05

Optimization complete. Final v2v error: 2.8986752033233643 mm

Highest mean error: 3.4223062992095947 mm for frame 77

Lowest mean error: 2.5325746536254883 mm for frame 53

Saving results

Total time: 36.90177583694458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461841
Iteration 2/25 | Loss: 0.00130137
Iteration 3/25 | Loss: 0.00113970
Iteration 4/25 | Loss: 0.00112156
Iteration 5/25 | Loss: 0.00111630
Iteration 6/25 | Loss: 0.00111542
Iteration 7/25 | Loss: 0.00111542
Iteration 8/25 | Loss: 0.00111542
Iteration 9/25 | Loss: 0.00111542
Iteration 10/25 | Loss: 0.00111542
Iteration 11/25 | Loss: 0.00111542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011154190870001912, 0.0011154190870001912, 0.0011154190870001912, 0.0011154190870001912, 0.0011154190870001912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011154190870001912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35708690
Iteration 2/25 | Loss: 0.00101150
Iteration 3/25 | Loss: 0.00101150
Iteration 4/25 | Loss: 0.00101150
Iteration 5/25 | Loss: 0.00101150
Iteration 6/25 | Loss: 0.00101150
Iteration 7/25 | Loss: 0.00101150
Iteration 8/25 | Loss: 0.00101150
Iteration 9/25 | Loss: 0.00101150
Iteration 10/25 | Loss: 0.00101150
Iteration 11/25 | Loss: 0.00101150
Iteration 12/25 | Loss: 0.00101150
Iteration 13/25 | Loss: 0.00101150
Iteration 14/25 | Loss: 0.00101150
Iteration 15/25 | Loss: 0.00101150
Iteration 16/25 | Loss: 0.00101150
Iteration 17/25 | Loss: 0.00101150
Iteration 18/25 | Loss: 0.00101150
Iteration 19/25 | Loss: 0.00101150
Iteration 20/25 | Loss: 0.00101150
Iteration 21/25 | Loss: 0.00101150
Iteration 22/25 | Loss: 0.00101150
Iteration 23/25 | Loss: 0.00101150
Iteration 24/25 | Loss: 0.00101150
Iteration 25/25 | Loss: 0.00101150

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101150
Iteration 2/1000 | Loss: 0.00003320
Iteration 3/1000 | Loss: 0.00002172
Iteration 4/1000 | Loss: 0.00001946
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001749
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00001611
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001600
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001587
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001573
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001570
Iteration 45/1000 | Loss: 0.00001569
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001560
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001558
Iteration 77/1000 | Loss: 0.00001558
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001558
Iteration 80/1000 | Loss: 0.00001558
Iteration 81/1000 | Loss: 0.00001558
Iteration 82/1000 | Loss: 0.00001558
Iteration 83/1000 | Loss: 0.00001558
Iteration 84/1000 | Loss: 0.00001558
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001558
Iteration 87/1000 | Loss: 0.00001558
Iteration 88/1000 | Loss: 0.00001558
Iteration 89/1000 | Loss: 0.00001558
Iteration 90/1000 | Loss: 0.00001558
Iteration 91/1000 | Loss: 0.00001558
Iteration 92/1000 | Loss: 0.00001558
Iteration 93/1000 | Loss: 0.00001558
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001558
Iteration 97/1000 | Loss: 0.00001558
Iteration 98/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.557707400934305e-05, 1.557707400934305e-05, 1.557707400934305e-05, 1.557707400934305e-05, 1.557707400934305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.557707400934305e-05

Optimization complete. Final v2v error: 3.2594780921936035 mm

Highest mean error: 4.144948959350586 mm for frame 215

Lowest mean error: 2.488030195236206 mm for frame 56

Saving results

Total time: 38.29603409767151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006787
Iteration 2/25 | Loss: 0.00221885
Iteration 3/25 | Loss: 0.00187929
Iteration 4/25 | Loss: 0.00181630
Iteration 5/25 | Loss: 0.00180874
Iteration 6/25 | Loss: 0.00177072
Iteration 7/25 | Loss: 0.00178019
Iteration 8/25 | Loss: 0.00176754
Iteration 9/25 | Loss: 0.00175467
Iteration 10/25 | Loss: 0.00175555
Iteration 11/25 | Loss: 0.00173992
Iteration 12/25 | Loss: 0.00173805
Iteration 13/25 | Loss: 0.00174006
Iteration 14/25 | Loss: 0.00174142
Iteration 15/25 | Loss: 0.00172305
Iteration 16/25 | Loss: 0.00171504
Iteration 17/25 | Loss: 0.00171248
Iteration 18/25 | Loss: 0.00171015
Iteration 19/25 | Loss: 0.00170814
Iteration 20/25 | Loss: 0.00170717
Iteration 21/25 | Loss: 0.00170671
Iteration 22/25 | Loss: 0.00170641
Iteration 23/25 | Loss: 0.00170615
Iteration 24/25 | Loss: 0.00170592
Iteration 25/25 | Loss: 0.00170572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37751067
Iteration 2/25 | Loss: 0.00437696
Iteration 3/25 | Loss: 0.00437696
Iteration 4/25 | Loss: 0.00437695
Iteration 5/25 | Loss: 0.00437695
Iteration 6/25 | Loss: 0.00437695
Iteration 7/25 | Loss: 0.00437695
Iteration 8/25 | Loss: 0.00437695
Iteration 9/25 | Loss: 0.00437695
Iteration 10/25 | Loss: 0.00437695
Iteration 11/25 | Loss: 0.00437695
Iteration 12/25 | Loss: 0.00437695
Iteration 13/25 | Loss: 0.00437695
Iteration 14/25 | Loss: 0.00437695
Iteration 15/25 | Loss: 0.00437695
Iteration 16/25 | Loss: 0.00437695
Iteration 17/25 | Loss: 0.00437695
Iteration 18/25 | Loss: 0.00437695
Iteration 19/25 | Loss: 0.00437695
Iteration 20/25 | Loss: 0.00437695
Iteration 21/25 | Loss: 0.00437695
Iteration 22/25 | Loss: 0.00437695
Iteration 23/25 | Loss: 0.00437695
Iteration 24/25 | Loss: 0.00437695
Iteration 25/25 | Loss: 0.00437695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00437695
Iteration 2/1000 | Loss: 0.01154698
Iteration 3/1000 | Loss: 0.00514223
Iteration 4/1000 | Loss: 0.00270842
Iteration 5/1000 | Loss: 0.00313605
Iteration 6/1000 | Loss: 0.00534351
Iteration 7/1000 | Loss: 0.00119747
Iteration 8/1000 | Loss: 0.00578054
Iteration 9/1000 | Loss: 0.00145743
Iteration 10/1000 | Loss: 0.00513402
Iteration 11/1000 | Loss: 0.00248044
Iteration 12/1000 | Loss: 0.00470221
Iteration 13/1000 | Loss: 0.00713209
Iteration 14/1000 | Loss: 0.00655037
Iteration 15/1000 | Loss: 0.00685056
Iteration 16/1000 | Loss: 0.00684046
Iteration 17/1000 | Loss: 0.00929516
Iteration 18/1000 | Loss: 0.01015629
Iteration 19/1000 | Loss: 0.00985476
Iteration 20/1000 | Loss: 0.00795320
Iteration 21/1000 | Loss: 0.00520111
Iteration 22/1000 | Loss: 0.00500018
Iteration 23/1000 | Loss: 0.00548126
Iteration 24/1000 | Loss: 0.00295846
Iteration 25/1000 | Loss: 0.00188147
Iteration 26/1000 | Loss: 0.00058627
Iteration 27/1000 | Loss: 0.00082134
Iteration 28/1000 | Loss: 0.00068645
Iteration 29/1000 | Loss: 0.00318237
Iteration 30/1000 | Loss: 0.00469087
Iteration 31/1000 | Loss: 0.00447231
Iteration 32/1000 | Loss: 0.00283306
Iteration 33/1000 | Loss: 0.00290484
Iteration 34/1000 | Loss: 0.00070160
Iteration 35/1000 | Loss: 0.00209199
Iteration 36/1000 | Loss: 0.00168845
Iteration 37/1000 | Loss: 0.00135574
Iteration 38/1000 | Loss: 0.00121071
Iteration 39/1000 | Loss: 0.00174609
Iteration 40/1000 | Loss: 0.00210294
Iteration 41/1000 | Loss: 0.00225569
Iteration 42/1000 | Loss: 0.00148107
Iteration 43/1000 | Loss: 0.00276062
Iteration 44/1000 | Loss: 0.00293283
Iteration 45/1000 | Loss: 0.00413368
Iteration 46/1000 | Loss: 0.00250556
Iteration 47/1000 | Loss: 0.00125412
Iteration 48/1000 | Loss: 0.00184210
Iteration 49/1000 | Loss: 0.00136351
Iteration 50/1000 | Loss: 0.00043569
Iteration 51/1000 | Loss: 0.00147744
Iteration 52/1000 | Loss: 0.00081964
Iteration 53/1000 | Loss: 0.00109819
Iteration 54/1000 | Loss: 0.00061577
Iteration 55/1000 | Loss: 0.00082931
Iteration 56/1000 | Loss: 0.00060451
Iteration 57/1000 | Loss: 0.00115980
Iteration 58/1000 | Loss: 0.00051662
Iteration 59/1000 | Loss: 0.00033390
Iteration 60/1000 | Loss: 0.00081725
Iteration 61/1000 | Loss: 0.00039885
Iteration 62/1000 | Loss: 0.00026869
Iteration 63/1000 | Loss: 0.00006865
Iteration 64/1000 | Loss: 0.00029388
Iteration 65/1000 | Loss: 0.00055700
Iteration 66/1000 | Loss: 0.00092480
Iteration 67/1000 | Loss: 0.00008831
Iteration 68/1000 | Loss: 0.00080300
Iteration 69/1000 | Loss: 0.00031603
Iteration 70/1000 | Loss: 0.00026701
Iteration 71/1000 | Loss: 0.00017639
Iteration 72/1000 | Loss: 0.00016384
Iteration 73/1000 | Loss: 0.00017598
Iteration 74/1000 | Loss: 0.00006343
Iteration 75/1000 | Loss: 0.00021683
Iteration 76/1000 | Loss: 0.00016780
Iteration 77/1000 | Loss: 0.00015562
Iteration 78/1000 | Loss: 0.00046531
Iteration 79/1000 | Loss: 0.00013438
Iteration 80/1000 | Loss: 0.00028568
Iteration 81/1000 | Loss: 0.00054568
Iteration 82/1000 | Loss: 0.00038613
Iteration 83/1000 | Loss: 0.00032323
Iteration 84/1000 | Loss: 0.00012818
Iteration 85/1000 | Loss: 0.00016242
Iteration 86/1000 | Loss: 0.00009703
Iteration 87/1000 | Loss: 0.00014178
Iteration 88/1000 | Loss: 0.00004012
Iteration 89/1000 | Loss: 0.00012340
Iteration 90/1000 | Loss: 0.00019086
Iteration 91/1000 | Loss: 0.00013594
Iteration 92/1000 | Loss: 0.00011454
Iteration 93/1000 | Loss: 0.00010872
Iteration 94/1000 | Loss: 0.00010057
Iteration 95/1000 | Loss: 0.00006759
Iteration 96/1000 | Loss: 0.00031041
Iteration 97/1000 | Loss: 0.00023574
Iteration 98/1000 | Loss: 0.00028348
Iteration 99/1000 | Loss: 0.00017724
Iteration 100/1000 | Loss: 0.00013793
Iteration 101/1000 | Loss: 0.00045516
Iteration 102/1000 | Loss: 0.00010144
Iteration 103/1000 | Loss: 0.00028700
Iteration 104/1000 | Loss: 0.00022006
Iteration 105/1000 | Loss: 0.00048448
Iteration 106/1000 | Loss: 0.00050445
Iteration 107/1000 | Loss: 0.00054986
Iteration 108/1000 | Loss: 0.00056291
Iteration 109/1000 | Loss: 0.00080851
Iteration 110/1000 | Loss: 0.00062526
Iteration 111/1000 | Loss: 0.00058974
Iteration 112/1000 | Loss: 0.00006597
Iteration 113/1000 | Loss: 0.00017154
Iteration 114/1000 | Loss: 0.00014284
Iteration 115/1000 | Loss: 0.00010947
Iteration 116/1000 | Loss: 0.00011184
Iteration 117/1000 | Loss: 0.00032298
Iteration 118/1000 | Loss: 0.00036664
Iteration 119/1000 | Loss: 0.00005506
Iteration 120/1000 | Loss: 0.00003914
Iteration 121/1000 | Loss: 0.00035686
Iteration 122/1000 | Loss: 0.00004199
Iteration 123/1000 | Loss: 0.00003430
Iteration 124/1000 | Loss: 0.00002946
Iteration 125/1000 | Loss: 0.00018412
Iteration 126/1000 | Loss: 0.00016387
Iteration 127/1000 | Loss: 0.00003703
Iteration 128/1000 | Loss: 0.00003012
Iteration 129/1000 | Loss: 0.00002763
Iteration 130/1000 | Loss: 0.00002675
Iteration 131/1000 | Loss: 0.00017733
Iteration 132/1000 | Loss: 0.00003502
Iteration 133/1000 | Loss: 0.00002913
Iteration 134/1000 | Loss: 0.00002689
Iteration 135/1000 | Loss: 0.00002581
Iteration 136/1000 | Loss: 0.00002495
Iteration 137/1000 | Loss: 0.00002423
Iteration 138/1000 | Loss: 0.00048504
Iteration 139/1000 | Loss: 0.00002490
Iteration 140/1000 | Loss: 0.00002310
Iteration 141/1000 | Loss: 0.00002236
Iteration 142/1000 | Loss: 0.00047049
Iteration 143/1000 | Loss: 0.00004332
Iteration 144/1000 | Loss: 0.00003111
Iteration 145/1000 | Loss: 0.00002386
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002212
Iteration 148/1000 | Loss: 0.00002167
Iteration 149/1000 | Loss: 0.00031571
Iteration 150/1000 | Loss: 0.00018379
Iteration 151/1000 | Loss: 0.00029410
Iteration 152/1000 | Loss: 0.00020401
Iteration 153/1000 | Loss: 0.00026860
Iteration 154/1000 | Loss: 0.00017538
Iteration 155/1000 | Loss: 0.00019715
Iteration 156/1000 | Loss: 0.00027675
Iteration 157/1000 | Loss: 0.00018476
Iteration 158/1000 | Loss: 0.00022041
Iteration 159/1000 | Loss: 0.00016927
Iteration 160/1000 | Loss: 0.00046762
Iteration 161/1000 | Loss: 0.00019290
Iteration 162/1000 | Loss: 0.00011492
Iteration 163/1000 | Loss: 0.00002204
Iteration 164/1000 | Loss: 0.00002133
Iteration 165/1000 | Loss: 0.00002111
Iteration 166/1000 | Loss: 0.00002094
Iteration 167/1000 | Loss: 0.00002090
Iteration 168/1000 | Loss: 0.00002090
Iteration 169/1000 | Loss: 0.00002089
Iteration 170/1000 | Loss: 0.00002089
Iteration 171/1000 | Loss: 0.00002088
Iteration 172/1000 | Loss: 0.00002088
Iteration 173/1000 | Loss: 0.00002087
Iteration 174/1000 | Loss: 0.00032353
Iteration 175/1000 | Loss: 0.00022975
Iteration 176/1000 | Loss: 0.00002635
Iteration 177/1000 | Loss: 0.00002170
Iteration 178/1000 | Loss: 0.00041115
Iteration 179/1000 | Loss: 0.00012115
Iteration 180/1000 | Loss: 0.00003497
Iteration 181/1000 | Loss: 0.00002452
Iteration 182/1000 | Loss: 0.00002108
Iteration 183/1000 | Loss: 0.00044078
Iteration 184/1000 | Loss: 0.00015732
Iteration 185/1000 | Loss: 0.00038408
Iteration 186/1000 | Loss: 0.00017422
Iteration 187/1000 | Loss: 0.00033033
Iteration 188/1000 | Loss: 0.00007832
Iteration 189/1000 | Loss: 0.00019949
Iteration 190/1000 | Loss: 0.00002519
Iteration 191/1000 | Loss: 0.00002247
Iteration 192/1000 | Loss: 0.00002147
Iteration 193/1000 | Loss: 0.00002117
Iteration 194/1000 | Loss: 0.00002096
Iteration 195/1000 | Loss: 0.00002093
Iteration 196/1000 | Loss: 0.00002090
Iteration 197/1000 | Loss: 0.00002078
Iteration 198/1000 | Loss: 0.00002077
Iteration 199/1000 | Loss: 0.00002077
Iteration 200/1000 | Loss: 0.00002075
Iteration 201/1000 | Loss: 0.00002074
Iteration 202/1000 | Loss: 0.00002073
Iteration 203/1000 | Loss: 0.00002072
Iteration 204/1000 | Loss: 0.00002071
Iteration 205/1000 | Loss: 0.00002071
Iteration 206/1000 | Loss: 0.00002071
Iteration 207/1000 | Loss: 0.00002070
Iteration 208/1000 | Loss: 0.00002070
Iteration 209/1000 | Loss: 0.00002070
Iteration 210/1000 | Loss: 0.00002070
Iteration 211/1000 | Loss: 0.00002070
Iteration 212/1000 | Loss: 0.00002069
Iteration 213/1000 | Loss: 0.00002069
Iteration 214/1000 | Loss: 0.00002069
Iteration 215/1000 | Loss: 0.00002068
Iteration 216/1000 | Loss: 0.00002068
Iteration 217/1000 | Loss: 0.00002067
Iteration 218/1000 | Loss: 0.00002067
Iteration 219/1000 | Loss: 0.00002067
Iteration 220/1000 | Loss: 0.00002067
Iteration 221/1000 | Loss: 0.00002067
Iteration 222/1000 | Loss: 0.00002067
Iteration 223/1000 | Loss: 0.00002067
Iteration 224/1000 | Loss: 0.00002067
Iteration 225/1000 | Loss: 0.00002067
Iteration 226/1000 | Loss: 0.00002067
Iteration 227/1000 | Loss: 0.00002066
Iteration 228/1000 | Loss: 0.00002066
Iteration 229/1000 | Loss: 0.00002066
Iteration 230/1000 | Loss: 0.00002065
Iteration 231/1000 | Loss: 0.00002065
Iteration 232/1000 | Loss: 0.00002065
Iteration 233/1000 | Loss: 0.00002064
Iteration 234/1000 | Loss: 0.00002064
Iteration 235/1000 | Loss: 0.00002063
Iteration 236/1000 | Loss: 0.00002063
Iteration 237/1000 | Loss: 0.00002062
Iteration 238/1000 | Loss: 0.00002062
Iteration 239/1000 | Loss: 0.00002062
Iteration 240/1000 | Loss: 0.00002062
Iteration 241/1000 | Loss: 0.00002062
Iteration 242/1000 | Loss: 0.00002061
Iteration 243/1000 | Loss: 0.00002061
Iteration 244/1000 | Loss: 0.00002061
Iteration 245/1000 | Loss: 0.00002061
Iteration 246/1000 | Loss: 0.00002061
Iteration 247/1000 | Loss: 0.00002061
Iteration 248/1000 | Loss: 0.00002060
Iteration 249/1000 | Loss: 0.00002060
Iteration 250/1000 | Loss: 0.00002060
Iteration 251/1000 | Loss: 0.00002060
Iteration 252/1000 | Loss: 0.00002059
Iteration 253/1000 | Loss: 0.00002059
Iteration 254/1000 | Loss: 0.00002058
Iteration 255/1000 | Loss: 0.00002058
Iteration 256/1000 | Loss: 0.00002058
Iteration 257/1000 | Loss: 0.00002057
Iteration 258/1000 | Loss: 0.00002057
Iteration 259/1000 | Loss: 0.00002057
Iteration 260/1000 | Loss: 0.00002057
Iteration 261/1000 | Loss: 0.00002057
Iteration 262/1000 | Loss: 0.00002056
Iteration 263/1000 | Loss: 0.00002056
Iteration 264/1000 | Loss: 0.00002056
Iteration 265/1000 | Loss: 0.00002056
Iteration 266/1000 | Loss: 0.00002056
Iteration 267/1000 | Loss: 0.00002056
Iteration 268/1000 | Loss: 0.00002056
Iteration 269/1000 | Loss: 0.00002055
Iteration 270/1000 | Loss: 0.00002055
Iteration 271/1000 | Loss: 0.00002055
Iteration 272/1000 | Loss: 0.00002055
Iteration 273/1000 | Loss: 0.00002055
Iteration 274/1000 | Loss: 0.00002055
Iteration 275/1000 | Loss: 0.00002054
Iteration 276/1000 | Loss: 0.00002054
Iteration 277/1000 | Loss: 0.00002054
Iteration 278/1000 | Loss: 0.00002054
Iteration 279/1000 | Loss: 0.00002053
Iteration 280/1000 | Loss: 0.00002053
Iteration 281/1000 | Loss: 0.00002053
Iteration 282/1000 | Loss: 0.00002052
Iteration 283/1000 | Loss: 0.00002052
Iteration 284/1000 | Loss: 0.00002051
Iteration 285/1000 | Loss: 0.00002051
Iteration 286/1000 | Loss: 0.00002049
Iteration 287/1000 | Loss: 0.00002048
Iteration 288/1000 | Loss: 0.00002048
Iteration 289/1000 | Loss: 0.00002047
Iteration 290/1000 | Loss: 0.00002047
Iteration 291/1000 | Loss: 0.00002046
Iteration 292/1000 | Loss: 0.00002046
Iteration 293/1000 | Loss: 0.00002046
Iteration 294/1000 | Loss: 0.00002046
Iteration 295/1000 | Loss: 0.00049988
Iteration 296/1000 | Loss: 0.00002453
Iteration 297/1000 | Loss: 0.00002045
Iteration 298/1000 | Loss: 0.00001956
Iteration 299/1000 | Loss: 0.00001906
Iteration 300/1000 | Loss: 0.00001865
Iteration 301/1000 | Loss: 0.00001848
Iteration 302/1000 | Loss: 0.00001846
Iteration 303/1000 | Loss: 0.00001845
Iteration 304/1000 | Loss: 0.00001830
Iteration 305/1000 | Loss: 0.00001829
Iteration 306/1000 | Loss: 0.00001829
Iteration 307/1000 | Loss: 0.00001828
Iteration 308/1000 | Loss: 0.00001826
Iteration 309/1000 | Loss: 0.00001825
Iteration 310/1000 | Loss: 0.00001825
Iteration 311/1000 | Loss: 0.00001824
Iteration 312/1000 | Loss: 0.00001824
Iteration 313/1000 | Loss: 0.00001824
Iteration 314/1000 | Loss: 0.00001823
Iteration 315/1000 | Loss: 0.00001823
Iteration 316/1000 | Loss: 0.00001821
Iteration 317/1000 | Loss: 0.00001820
Iteration 318/1000 | Loss: 0.00001820
Iteration 319/1000 | Loss: 0.00001818
Iteration 320/1000 | Loss: 0.00001818
Iteration 321/1000 | Loss: 0.00001815
Iteration 322/1000 | Loss: 0.00001815
Iteration 323/1000 | Loss: 0.00001813
Iteration 324/1000 | Loss: 0.00001812
Iteration 325/1000 | Loss: 0.00001812
Iteration 326/1000 | Loss: 0.00001811
Iteration 327/1000 | Loss: 0.00001811
Iteration 328/1000 | Loss: 0.00001810
Iteration 329/1000 | Loss: 0.00001809
Iteration 330/1000 | Loss: 0.00001809
Iteration 331/1000 | Loss: 0.00001808
Iteration 332/1000 | Loss: 0.00001808
Iteration 333/1000 | Loss: 0.00001807
Iteration 334/1000 | Loss: 0.00001807
Iteration 335/1000 | Loss: 0.00001804
Iteration 336/1000 | Loss: 0.00001803
Iteration 337/1000 | Loss: 0.00001803
Iteration 338/1000 | Loss: 0.00001801
Iteration 339/1000 | Loss: 0.00001801
Iteration 340/1000 | Loss: 0.00001800
Iteration 341/1000 | Loss: 0.00001800
Iteration 342/1000 | Loss: 0.00001800
Iteration 343/1000 | Loss: 0.00001800
Iteration 344/1000 | Loss: 0.00001799
Iteration 345/1000 | Loss: 0.00001799
Iteration 346/1000 | Loss: 0.00001799
Iteration 347/1000 | Loss: 0.00001799
Iteration 348/1000 | Loss: 0.00001799
Iteration 349/1000 | Loss: 0.00001799
Iteration 350/1000 | Loss: 0.00001799
Iteration 351/1000 | Loss: 0.00001799
Iteration 352/1000 | Loss: 0.00001798
Iteration 353/1000 | Loss: 0.00001798
Iteration 354/1000 | Loss: 0.00001798
Iteration 355/1000 | Loss: 0.00001798
Iteration 356/1000 | Loss: 0.00001798
Iteration 357/1000 | Loss: 0.00001798
Iteration 358/1000 | Loss: 0.00001798
Iteration 359/1000 | Loss: 0.00001798
Iteration 360/1000 | Loss: 0.00001798
Iteration 361/1000 | Loss: 0.00001797
Iteration 362/1000 | Loss: 0.00001797
Iteration 363/1000 | Loss: 0.00001797
Iteration 364/1000 | Loss: 0.00001797
Iteration 365/1000 | Loss: 0.00001797
Iteration 366/1000 | Loss: 0.00001796
Iteration 367/1000 | Loss: 0.00001796
Iteration 368/1000 | Loss: 0.00001796
Iteration 369/1000 | Loss: 0.00001796
Iteration 370/1000 | Loss: 0.00001795
Iteration 371/1000 | Loss: 0.00001795
Iteration 372/1000 | Loss: 0.00001795
Iteration 373/1000 | Loss: 0.00001795
Iteration 374/1000 | Loss: 0.00001795
Iteration 375/1000 | Loss: 0.00001795
Iteration 376/1000 | Loss: 0.00001795
Iteration 377/1000 | Loss: 0.00001795
Iteration 378/1000 | Loss: 0.00001795
Iteration 379/1000 | Loss: 0.00001795
Iteration 380/1000 | Loss: 0.00001795
Iteration 381/1000 | Loss: 0.00001794
Iteration 382/1000 | Loss: 0.00001794
Iteration 383/1000 | Loss: 0.00001794
Iteration 384/1000 | Loss: 0.00001794
Iteration 385/1000 | Loss: 0.00001794
Iteration 386/1000 | Loss: 0.00001794
Iteration 387/1000 | Loss: 0.00001794
Iteration 388/1000 | Loss: 0.00001794
Iteration 389/1000 | Loss: 0.00001794
Iteration 390/1000 | Loss: 0.00001794
Iteration 391/1000 | Loss: 0.00001794
Iteration 392/1000 | Loss: 0.00001794
Iteration 393/1000 | Loss: 0.00001794
Iteration 394/1000 | Loss: 0.00001794
Iteration 395/1000 | Loss: 0.00001794
Iteration 396/1000 | Loss: 0.00001794
Iteration 397/1000 | Loss: 0.00001794
Iteration 398/1000 | Loss: 0.00001793
Iteration 399/1000 | Loss: 0.00001793
Iteration 400/1000 | Loss: 0.00001793
Iteration 401/1000 | Loss: 0.00001793
Iteration 402/1000 | Loss: 0.00001793
Iteration 403/1000 | Loss: 0.00001793
Iteration 404/1000 | Loss: 0.00001793
Iteration 405/1000 | Loss: 0.00001793
Iteration 406/1000 | Loss: 0.00001793
Iteration 407/1000 | Loss: 0.00001793
Iteration 408/1000 | Loss: 0.00001793
Iteration 409/1000 | Loss: 0.00001793
Iteration 410/1000 | Loss: 0.00001793
Iteration 411/1000 | Loss: 0.00001793
Iteration 412/1000 | Loss: 0.00001793
Iteration 413/1000 | Loss: 0.00001793
Iteration 414/1000 | Loss: 0.00001793
Iteration 415/1000 | Loss: 0.00001793
Iteration 416/1000 | Loss: 0.00001793
Iteration 417/1000 | Loss: 0.00001792
Iteration 418/1000 | Loss: 0.00001792
Iteration 419/1000 | Loss: 0.00001792
Iteration 420/1000 | Loss: 0.00001792
Iteration 421/1000 | Loss: 0.00001792
Iteration 422/1000 | Loss: 0.00001792
Iteration 423/1000 | Loss: 0.00001792
Iteration 424/1000 | Loss: 0.00001792
Iteration 425/1000 | Loss: 0.00001792
Iteration 426/1000 | Loss: 0.00001792
Iteration 427/1000 | Loss: 0.00001792
Iteration 428/1000 | Loss: 0.00001792
Iteration 429/1000 | Loss: 0.00001792
Iteration 430/1000 | Loss: 0.00001792
Iteration 431/1000 | Loss: 0.00001792
Iteration 432/1000 | Loss: 0.00001792
Iteration 433/1000 | Loss: 0.00001792
Iteration 434/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 434. Stopping optimization.
Last 5 losses: [1.7920820027939044e-05, 1.7920820027939044e-05, 1.7920820027939044e-05, 1.7920820027939044e-05, 1.7920820027939044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7920820027939044e-05

Optimization complete. Final v2v error: 3.5598573684692383 mm

Highest mean error: 4.587918281555176 mm for frame 40

Lowest mean error: 3.263382911682129 mm for frame 126

Saving results

Total time: 354.60190653800964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00707368
Iteration 2/25 | Loss: 0.00123542
Iteration 3/25 | Loss: 0.00112127
Iteration 4/25 | Loss: 0.00109859
Iteration 5/25 | Loss: 0.00109431
Iteration 6/25 | Loss: 0.00109052
Iteration 7/25 | Loss: 0.00108373
Iteration 8/25 | Loss: 0.00108059
Iteration 9/25 | Loss: 0.00108030
Iteration 10/25 | Loss: 0.00108019
Iteration 11/25 | Loss: 0.00108015
Iteration 12/25 | Loss: 0.00108015
Iteration 13/25 | Loss: 0.00108015
Iteration 14/25 | Loss: 0.00108015
Iteration 15/25 | Loss: 0.00108015
Iteration 16/25 | Loss: 0.00108014
Iteration 17/25 | Loss: 0.00108014
Iteration 18/25 | Loss: 0.00108013
Iteration 19/25 | Loss: 0.00108013
Iteration 20/25 | Loss: 0.00108013
Iteration 21/25 | Loss: 0.00108013
Iteration 22/25 | Loss: 0.00108012
Iteration 23/25 | Loss: 0.00108012
Iteration 24/25 | Loss: 0.00108012
Iteration 25/25 | Loss: 0.00108012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97680068
Iteration 2/25 | Loss: 0.00094709
Iteration 3/25 | Loss: 0.00088826
Iteration 4/25 | Loss: 0.00088826
Iteration 5/25 | Loss: 0.00088826
Iteration 6/25 | Loss: 0.00088826
Iteration 7/25 | Loss: 0.00088825
Iteration 8/25 | Loss: 0.00088825
Iteration 9/25 | Loss: 0.00088825
Iteration 10/25 | Loss: 0.00088825
Iteration 11/25 | Loss: 0.00088825
Iteration 12/25 | Loss: 0.00088825
Iteration 13/25 | Loss: 0.00088825
Iteration 14/25 | Loss: 0.00088825
Iteration 15/25 | Loss: 0.00088825
Iteration 16/25 | Loss: 0.00088825
Iteration 17/25 | Loss: 0.00088825
Iteration 18/25 | Loss: 0.00088825
Iteration 19/25 | Loss: 0.00088825
Iteration 20/25 | Loss: 0.00088825
Iteration 21/25 | Loss: 0.00088825
Iteration 22/25 | Loss: 0.00088825
Iteration 23/25 | Loss: 0.00088825
Iteration 24/25 | Loss: 0.00088825
Iteration 25/25 | Loss: 0.00088825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088825
Iteration 2/1000 | Loss: 0.00001788
Iteration 3/1000 | Loss: 0.00008569
Iteration 4/1000 | Loss: 0.00002097
Iteration 5/1000 | Loss: 0.00001446
Iteration 6/1000 | Loss: 0.00001239
Iteration 7/1000 | Loss: 0.00001197
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00007496
Iteration 10/1000 | Loss: 0.00001116
Iteration 11/1000 | Loss: 0.00003743
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00001092
Iteration 14/1000 | Loss: 0.00004712
Iteration 15/1000 | Loss: 0.00004711
Iteration 16/1000 | Loss: 0.00011276
Iteration 17/1000 | Loss: 0.00001088
Iteration 18/1000 | Loss: 0.00005099
Iteration 19/1000 | Loss: 0.00003720
Iteration 20/1000 | Loss: 0.00003158
Iteration 21/1000 | Loss: 0.00007362
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001030
Iteration 25/1000 | Loss: 0.00001028
Iteration 26/1000 | Loss: 0.00001022
Iteration 27/1000 | Loss: 0.00001022
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001019
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00000999
Iteration 34/1000 | Loss: 0.00000999
Iteration 35/1000 | Loss: 0.00000998
Iteration 36/1000 | Loss: 0.00000997
Iteration 37/1000 | Loss: 0.00000997
Iteration 38/1000 | Loss: 0.00000996
Iteration 39/1000 | Loss: 0.00000996
Iteration 40/1000 | Loss: 0.00000996
Iteration 41/1000 | Loss: 0.00000995
Iteration 42/1000 | Loss: 0.00000994
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000993
Iteration 45/1000 | Loss: 0.00000992
Iteration 46/1000 | Loss: 0.00000992
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000986
Iteration 51/1000 | Loss: 0.00000986
Iteration 52/1000 | Loss: 0.00000986
Iteration 53/1000 | Loss: 0.00000986
Iteration 54/1000 | Loss: 0.00000986
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000986
Iteration 59/1000 | Loss: 0.00000986
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000981
Iteration 66/1000 | Loss: 0.00000981
Iteration 67/1000 | Loss: 0.00000981
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000976
Iteration 74/1000 | Loss: 0.00000975
Iteration 75/1000 | Loss: 0.00000975
Iteration 76/1000 | Loss: 0.00000975
Iteration 77/1000 | Loss: 0.00000975
Iteration 78/1000 | Loss: 0.00000975
Iteration 79/1000 | Loss: 0.00000975
Iteration 80/1000 | Loss: 0.00000973
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000972
Iteration 83/1000 | Loss: 0.00000972
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000971
Iteration 86/1000 | Loss: 0.00000971
Iteration 87/1000 | Loss: 0.00000971
Iteration 88/1000 | Loss: 0.00000970
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000969
Iteration 92/1000 | Loss: 0.00000968
Iteration 93/1000 | Loss: 0.00000968
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000967
Iteration 96/1000 | Loss: 0.00000967
Iteration 97/1000 | Loss: 0.00000967
Iteration 98/1000 | Loss: 0.00000966
Iteration 99/1000 | Loss: 0.00000966
Iteration 100/1000 | Loss: 0.00000966
Iteration 101/1000 | Loss: 0.00000966
Iteration 102/1000 | Loss: 0.00000965
Iteration 103/1000 | Loss: 0.00000965
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000963
Iteration 108/1000 | Loss: 0.00000963
Iteration 109/1000 | Loss: 0.00000963
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000962
Iteration 112/1000 | Loss: 0.00000962
Iteration 113/1000 | Loss: 0.00000962
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000960
Iteration 119/1000 | Loss: 0.00000960
Iteration 120/1000 | Loss: 0.00000960
Iteration 121/1000 | Loss: 0.00000960
Iteration 122/1000 | Loss: 0.00000960
Iteration 123/1000 | Loss: 0.00000960
Iteration 124/1000 | Loss: 0.00000960
Iteration 125/1000 | Loss: 0.00000960
Iteration 126/1000 | Loss: 0.00000960
Iteration 127/1000 | Loss: 0.00000959
Iteration 128/1000 | Loss: 0.00000959
Iteration 129/1000 | Loss: 0.00000959
Iteration 130/1000 | Loss: 0.00000959
Iteration 131/1000 | Loss: 0.00000959
Iteration 132/1000 | Loss: 0.00000959
Iteration 133/1000 | Loss: 0.00000959
Iteration 134/1000 | Loss: 0.00000959
Iteration 135/1000 | Loss: 0.00000958
Iteration 136/1000 | Loss: 0.00000958
Iteration 137/1000 | Loss: 0.00000958
Iteration 138/1000 | Loss: 0.00000958
Iteration 139/1000 | Loss: 0.00000958
Iteration 140/1000 | Loss: 0.00000958
Iteration 141/1000 | Loss: 0.00000958
Iteration 142/1000 | Loss: 0.00000958
Iteration 143/1000 | Loss: 0.00000958
Iteration 144/1000 | Loss: 0.00000958
Iteration 145/1000 | Loss: 0.00000958
Iteration 146/1000 | Loss: 0.00000958
Iteration 147/1000 | Loss: 0.00000958
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000958
Iteration 151/1000 | Loss: 0.00000958
Iteration 152/1000 | Loss: 0.00000957
Iteration 153/1000 | Loss: 0.00000957
Iteration 154/1000 | Loss: 0.00000957
Iteration 155/1000 | Loss: 0.00000957
Iteration 156/1000 | Loss: 0.00000957
Iteration 157/1000 | Loss: 0.00000957
Iteration 158/1000 | Loss: 0.00000957
Iteration 159/1000 | Loss: 0.00000957
Iteration 160/1000 | Loss: 0.00000957
Iteration 161/1000 | Loss: 0.00000957
Iteration 162/1000 | Loss: 0.00000957
Iteration 163/1000 | Loss: 0.00000957
Iteration 164/1000 | Loss: 0.00000956
Iteration 165/1000 | Loss: 0.00000956
Iteration 166/1000 | Loss: 0.00000956
Iteration 167/1000 | Loss: 0.00000956
Iteration 168/1000 | Loss: 0.00000956
Iteration 169/1000 | Loss: 0.00000956
Iteration 170/1000 | Loss: 0.00000956
Iteration 171/1000 | Loss: 0.00000956
Iteration 172/1000 | Loss: 0.00000955
Iteration 173/1000 | Loss: 0.00000955
Iteration 174/1000 | Loss: 0.00000955
Iteration 175/1000 | Loss: 0.00000955
Iteration 176/1000 | Loss: 0.00000955
Iteration 177/1000 | Loss: 0.00000955
Iteration 178/1000 | Loss: 0.00000955
Iteration 179/1000 | Loss: 0.00000955
Iteration 180/1000 | Loss: 0.00000955
Iteration 181/1000 | Loss: 0.00000955
Iteration 182/1000 | Loss: 0.00000955
Iteration 183/1000 | Loss: 0.00000955
Iteration 184/1000 | Loss: 0.00000955
Iteration 185/1000 | Loss: 0.00000955
Iteration 186/1000 | Loss: 0.00000954
Iteration 187/1000 | Loss: 0.00000954
Iteration 188/1000 | Loss: 0.00000954
Iteration 189/1000 | Loss: 0.00000954
Iteration 190/1000 | Loss: 0.00000954
Iteration 191/1000 | Loss: 0.00000954
Iteration 192/1000 | Loss: 0.00000954
Iteration 193/1000 | Loss: 0.00000954
Iteration 194/1000 | Loss: 0.00000954
Iteration 195/1000 | Loss: 0.00000954
Iteration 196/1000 | Loss: 0.00000954
Iteration 197/1000 | Loss: 0.00000954
Iteration 198/1000 | Loss: 0.00000954
Iteration 199/1000 | Loss: 0.00000953
Iteration 200/1000 | Loss: 0.00000953
Iteration 201/1000 | Loss: 0.00000953
Iteration 202/1000 | Loss: 0.00000953
Iteration 203/1000 | Loss: 0.00000953
Iteration 204/1000 | Loss: 0.00000953
Iteration 205/1000 | Loss: 0.00000953
Iteration 206/1000 | Loss: 0.00000953
Iteration 207/1000 | Loss: 0.00000953
Iteration 208/1000 | Loss: 0.00000953
Iteration 209/1000 | Loss: 0.00000952
Iteration 210/1000 | Loss: 0.00000952
Iteration 211/1000 | Loss: 0.00000952
Iteration 212/1000 | Loss: 0.00000952
Iteration 213/1000 | Loss: 0.00000952
Iteration 214/1000 | Loss: 0.00000952
Iteration 215/1000 | Loss: 0.00000952
Iteration 216/1000 | Loss: 0.00000952
Iteration 217/1000 | Loss: 0.00000952
Iteration 218/1000 | Loss: 0.00000952
Iteration 219/1000 | Loss: 0.00000952
Iteration 220/1000 | Loss: 0.00000952
Iteration 221/1000 | Loss: 0.00000952
Iteration 222/1000 | Loss: 0.00000952
Iteration 223/1000 | Loss: 0.00000952
Iteration 224/1000 | Loss: 0.00000951
Iteration 225/1000 | Loss: 0.00000951
Iteration 226/1000 | Loss: 0.00000951
Iteration 227/1000 | Loss: 0.00000951
Iteration 228/1000 | Loss: 0.00000951
Iteration 229/1000 | Loss: 0.00000951
Iteration 230/1000 | Loss: 0.00000951
Iteration 231/1000 | Loss: 0.00000951
Iteration 232/1000 | Loss: 0.00000951
Iteration 233/1000 | Loss: 0.00000951
Iteration 234/1000 | Loss: 0.00000951
Iteration 235/1000 | Loss: 0.00000951
Iteration 236/1000 | Loss: 0.00000951
Iteration 237/1000 | Loss: 0.00000951
Iteration 238/1000 | Loss: 0.00000951
Iteration 239/1000 | Loss: 0.00000951
Iteration 240/1000 | Loss: 0.00000951
Iteration 241/1000 | Loss: 0.00000951
Iteration 242/1000 | Loss: 0.00000951
Iteration 243/1000 | Loss: 0.00000951
Iteration 244/1000 | Loss: 0.00000951
Iteration 245/1000 | Loss: 0.00000951
Iteration 246/1000 | Loss: 0.00000951
Iteration 247/1000 | Loss: 0.00000951
Iteration 248/1000 | Loss: 0.00000951
Iteration 249/1000 | Loss: 0.00000951
Iteration 250/1000 | Loss: 0.00000951
Iteration 251/1000 | Loss: 0.00000951
Iteration 252/1000 | Loss: 0.00000951
Iteration 253/1000 | Loss: 0.00000951
Iteration 254/1000 | Loss: 0.00000951
Iteration 255/1000 | Loss: 0.00000951
Iteration 256/1000 | Loss: 0.00000951
Iteration 257/1000 | Loss: 0.00000951
Iteration 258/1000 | Loss: 0.00000951
Iteration 259/1000 | Loss: 0.00000951
Iteration 260/1000 | Loss: 0.00000951
Iteration 261/1000 | Loss: 0.00000951
Iteration 262/1000 | Loss: 0.00000951
Iteration 263/1000 | Loss: 0.00000951
Iteration 264/1000 | Loss: 0.00000951
Iteration 265/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [9.509430128673557e-06, 9.509430128673557e-06, 9.509430128673557e-06, 9.509430128673557e-06, 9.509430128673557e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.509430128673557e-06

Optimization complete. Final v2v error: 2.6544551849365234 mm

Highest mean error: 3.131075620651245 mm for frame 139

Lowest mean error: 2.4807369709014893 mm for frame 22

Saving results

Total time: 78.13821029663086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444668
Iteration 2/25 | Loss: 0.00137657
Iteration 3/25 | Loss: 0.00113544
Iteration 4/25 | Loss: 0.00110490
Iteration 5/25 | Loss: 0.00110028
Iteration 6/25 | Loss: 0.00109878
Iteration 7/25 | Loss: 0.00109878
Iteration 8/25 | Loss: 0.00109878
Iteration 9/25 | Loss: 0.00109878
Iteration 10/25 | Loss: 0.00109878
Iteration 11/25 | Loss: 0.00109878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010987756540998816, 0.0010987756540998816, 0.0010987756540998816, 0.0010987756540998816, 0.0010987756540998816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010987756540998816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45892560
Iteration 2/25 | Loss: 0.00073523
Iteration 3/25 | Loss: 0.00073523
Iteration 4/25 | Loss: 0.00073523
Iteration 5/25 | Loss: 0.00073523
Iteration 6/25 | Loss: 0.00073522
Iteration 7/25 | Loss: 0.00073522
Iteration 8/25 | Loss: 0.00073522
Iteration 9/25 | Loss: 0.00073522
Iteration 10/25 | Loss: 0.00073522
Iteration 11/25 | Loss: 0.00073522
Iteration 12/25 | Loss: 0.00073522
Iteration 13/25 | Loss: 0.00073522
Iteration 14/25 | Loss: 0.00073522
Iteration 15/25 | Loss: 0.00073522
Iteration 16/25 | Loss: 0.00073522
Iteration 17/25 | Loss: 0.00073522
Iteration 18/25 | Loss: 0.00073522
Iteration 19/25 | Loss: 0.00073522
Iteration 20/25 | Loss: 0.00073522
Iteration 21/25 | Loss: 0.00073522
Iteration 22/25 | Loss: 0.00073522
Iteration 23/25 | Loss: 0.00073522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007352232350967824, 0.0007352232350967824, 0.0007352232350967824, 0.0007352232350967824, 0.0007352232350967824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007352232350967824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073522
Iteration 2/1000 | Loss: 0.00002587
Iteration 3/1000 | Loss: 0.00001644
Iteration 4/1000 | Loss: 0.00001471
Iteration 5/1000 | Loss: 0.00001388
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001281
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001226
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001183
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001156
Iteration 131/1000 | Loss: 0.00001156
Iteration 132/1000 | Loss: 0.00001156
Iteration 133/1000 | Loss: 0.00001156
Iteration 134/1000 | Loss: 0.00001156
Iteration 135/1000 | Loss: 0.00001156
Iteration 136/1000 | Loss: 0.00001156
Iteration 137/1000 | Loss: 0.00001156
Iteration 138/1000 | Loss: 0.00001156
Iteration 139/1000 | Loss: 0.00001155
Iteration 140/1000 | Loss: 0.00001155
Iteration 141/1000 | Loss: 0.00001155
Iteration 142/1000 | Loss: 0.00001155
Iteration 143/1000 | Loss: 0.00001155
Iteration 144/1000 | Loss: 0.00001155
Iteration 145/1000 | Loss: 0.00001155
Iteration 146/1000 | Loss: 0.00001155
Iteration 147/1000 | Loss: 0.00001155
Iteration 148/1000 | Loss: 0.00001155
Iteration 149/1000 | Loss: 0.00001154
Iteration 150/1000 | Loss: 0.00001154
Iteration 151/1000 | Loss: 0.00001154
Iteration 152/1000 | Loss: 0.00001154
Iteration 153/1000 | Loss: 0.00001154
Iteration 154/1000 | Loss: 0.00001154
Iteration 155/1000 | Loss: 0.00001154
Iteration 156/1000 | Loss: 0.00001153
Iteration 157/1000 | Loss: 0.00001153
Iteration 158/1000 | Loss: 0.00001153
Iteration 159/1000 | Loss: 0.00001153
Iteration 160/1000 | Loss: 0.00001152
Iteration 161/1000 | Loss: 0.00001152
Iteration 162/1000 | Loss: 0.00001152
Iteration 163/1000 | Loss: 0.00001152
Iteration 164/1000 | Loss: 0.00001152
Iteration 165/1000 | Loss: 0.00001152
Iteration 166/1000 | Loss: 0.00001152
Iteration 167/1000 | Loss: 0.00001152
Iteration 168/1000 | Loss: 0.00001152
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001151
Iteration 171/1000 | Loss: 0.00001151
Iteration 172/1000 | Loss: 0.00001151
Iteration 173/1000 | Loss: 0.00001151
Iteration 174/1000 | Loss: 0.00001151
Iteration 175/1000 | Loss: 0.00001151
Iteration 176/1000 | Loss: 0.00001150
Iteration 177/1000 | Loss: 0.00001150
Iteration 178/1000 | Loss: 0.00001150
Iteration 179/1000 | Loss: 0.00001150
Iteration 180/1000 | Loss: 0.00001150
Iteration 181/1000 | Loss: 0.00001150
Iteration 182/1000 | Loss: 0.00001150
Iteration 183/1000 | Loss: 0.00001150
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001149
Iteration 187/1000 | Loss: 0.00001148
Iteration 188/1000 | Loss: 0.00001148
Iteration 189/1000 | Loss: 0.00001148
Iteration 190/1000 | Loss: 0.00001148
Iteration 191/1000 | Loss: 0.00001147
Iteration 192/1000 | Loss: 0.00001147
Iteration 193/1000 | Loss: 0.00001147
Iteration 194/1000 | Loss: 0.00001147
Iteration 195/1000 | Loss: 0.00001147
Iteration 196/1000 | Loss: 0.00001147
Iteration 197/1000 | Loss: 0.00001147
Iteration 198/1000 | Loss: 0.00001147
Iteration 199/1000 | Loss: 0.00001147
Iteration 200/1000 | Loss: 0.00001147
Iteration 201/1000 | Loss: 0.00001147
Iteration 202/1000 | Loss: 0.00001146
Iteration 203/1000 | Loss: 0.00001146
Iteration 204/1000 | Loss: 0.00001146
Iteration 205/1000 | Loss: 0.00001146
Iteration 206/1000 | Loss: 0.00001146
Iteration 207/1000 | Loss: 0.00001146
Iteration 208/1000 | Loss: 0.00001146
Iteration 209/1000 | Loss: 0.00001146
Iteration 210/1000 | Loss: 0.00001146
Iteration 211/1000 | Loss: 0.00001146
Iteration 212/1000 | Loss: 0.00001146
Iteration 213/1000 | Loss: 0.00001145
Iteration 214/1000 | Loss: 0.00001145
Iteration 215/1000 | Loss: 0.00001145
Iteration 216/1000 | Loss: 0.00001145
Iteration 217/1000 | Loss: 0.00001145
Iteration 218/1000 | Loss: 0.00001145
Iteration 219/1000 | Loss: 0.00001145
Iteration 220/1000 | Loss: 0.00001145
Iteration 221/1000 | Loss: 0.00001145
Iteration 222/1000 | Loss: 0.00001145
Iteration 223/1000 | Loss: 0.00001145
Iteration 224/1000 | Loss: 0.00001144
Iteration 225/1000 | Loss: 0.00001144
Iteration 226/1000 | Loss: 0.00001144
Iteration 227/1000 | Loss: 0.00001144
Iteration 228/1000 | Loss: 0.00001144
Iteration 229/1000 | Loss: 0.00001144
Iteration 230/1000 | Loss: 0.00001144
Iteration 231/1000 | Loss: 0.00001144
Iteration 232/1000 | Loss: 0.00001144
Iteration 233/1000 | Loss: 0.00001144
Iteration 234/1000 | Loss: 0.00001144
Iteration 235/1000 | Loss: 0.00001144
Iteration 236/1000 | Loss: 0.00001144
Iteration 237/1000 | Loss: 0.00001144
Iteration 238/1000 | Loss: 0.00001144
Iteration 239/1000 | Loss: 0.00001143
Iteration 240/1000 | Loss: 0.00001143
Iteration 241/1000 | Loss: 0.00001143
Iteration 242/1000 | Loss: 0.00001143
Iteration 243/1000 | Loss: 0.00001143
Iteration 244/1000 | Loss: 0.00001143
Iteration 245/1000 | Loss: 0.00001143
Iteration 246/1000 | Loss: 0.00001143
Iteration 247/1000 | Loss: 0.00001143
Iteration 248/1000 | Loss: 0.00001142
Iteration 249/1000 | Loss: 0.00001142
Iteration 250/1000 | Loss: 0.00001142
Iteration 251/1000 | Loss: 0.00001142
Iteration 252/1000 | Loss: 0.00001142
Iteration 253/1000 | Loss: 0.00001142
Iteration 254/1000 | Loss: 0.00001142
Iteration 255/1000 | Loss: 0.00001141
Iteration 256/1000 | Loss: 0.00001141
Iteration 257/1000 | Loss: 0.00001141
Iteration 258/1000 | Loss: 0.00001141
Iteration 259/1000 | Loss: 0.00001141
Iteration 260/1000 | Loss: 0.00001141
Iteration 261/1000 | Loss: 0.00001141
Iteration 262/1000 | Loss: 0.00001141
Iteration 263/1000 | Loss: 0.00001141
Iteration 264/1000 | Loss: 0.00001141
Iteration 265/1000 | Loss: 0.00001141
Iteration 266/1000 | Loss: 0.00001141
Iteration 267/1000 | Loss: 0.00001141
Iteration 268/1000 | Loss: 0.00001141
Iteration 269/1000 | Loss: 0.00001141
Iteration 270/1000 | Loss: 0.00001141
Iteration 271/1000 | Loss: 0.00001141
Iteration 272/1000 | Loss: 0.00001140
Iteration 273/1000 | Loss: 0.00001140
Iteration 274/1000 | Loss: 0.00001140
Iteration 275/1000 | Loss: 0.00001140
Iteration 276/1000 | Loss: 0.00001140
Iteration 277/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.1403552889532875e-05, 1.1403552889532875e-05, 1.1403552889532875e-05, 1.1403552889532875e-05, 1.1403552889532875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1403552889532875e-05

Optimization complete. Final v2v error: 2.8351922035217285 mm

Highest mean error: 4.011018753051758 mm for frame 105

Lowest mean error: 2.4386179447174072 mm for frame 158

Saving results

Total time: 49.374478578567505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045647
Iteration 2/25 | Loss: 0.00205922
Iteration 3/25 | Loss: 0.00157174
Iteration 4/25 | Loss: 0.00134920
Iteration 5/25 | Loss: 0.00144256
Iteration 6/25 | Loss: 0.00134695
Iteration 7/25 | Loss: 0.00129800
Iteration 8/25 | Loss: 0.00117067
Iteration 9/25 | Loss: 0.00125178
Iteration 10/25 | Loss: 0.00114890
Iteration 11/25 | Loss: 0.00117449
Iteration 12/25 | Loss: 0.00114663
Iteration 13/25 | Loss: 0.00114629
Iteration 14/25 | Loss: 0.00114618
Iteration 15/25 | Loss: 0.00114617
Iteration 16/25 | Loss: 0.00114617
Iteration 17/25 | Loss: 0.00114617
Iteration 18/25 | Loss: 0.00114617
Iteration 19/25 | Loss: 0.00114617
Iteration 20/25 | Loss: 0.00114616
Iteration 21/25 | Loss: 0.00114616
Iteration 22/25 | Loss: 0.00114616
Iteration 23/25 | Loss: 0.00114615
Iteration 24/25 | Loss: 0.00114614
Iteration 25/25 | Loss: 0.00114614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33221483
Iteration 2/25 | Loss: 0.00227002
Iteration 3/25 | Loss: 0.00092304
Iteration 4/25 | Loss: 0.00092304
Iteration 5/25 | Loss: 0.00092304
Iteration 6/25 | Loss: 0.00092304
Iteration 7/25 | Loss: 0.00092304
Iteration 8/25 | Loss: 0.00092304
Iteration 9/25 | Loss: 0.00092304
Iteration 10/25 | Loss: 0.00092304
Iteration 11/25 | Loss: 0.00092304
Iteration 12/25 | Loss: 0.00092304
Iteration 13/25 | Loss: 0.00092304
Iteration 14/25 | Loss: 0.00092304
Iteration 15/25 | Loss: 0.00092304
Iteration 16/25 | Loss: 0.00092304
Iteration 17/25 | Loss: 0.00092304
Iteration 18/25 | Loss: 0.00092304
Iteration 19/25 | Loss: 0.00092304
Iteration 20/25 | Loss: 0.00092304
Iteration 21/25 | Loss: 0.00092304
Iteration 22/25 | Loss: 0.00092304
Iteration 23/25 | Loss: 0.00092304
Iteration 24/25 | Loss: 0.00092304
Iteration 25/25 | Loss: 0.00092304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092304
Iteration 2/1000 | Loss: 0.00002756
Iteration 3/1000 | Loss: 0.00002126
Iteration 4/1000 | Loss: 0.00002015
Iteration 5/1000 | Loss: 0.00001952
Iteration 6/1000 | Loss: 0.00001900
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001788
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001753
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001742
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001738
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001720
Iteration 59/1000 | Loss: 0.00001720
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001716
Iteration 89/1000 | Loss: 0.00001716
Iteration 90/1000 | Loss: 0.00001716
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001715
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001715
Iteration 96/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.7154388842754997e-05, 1.7154388842754997e-05, 1.7154388842754997e-05, 1.7154388842754997e-05, 1.7154388842754997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7154388842754997e-05

Optimization complete. Final v2v error: 3.5176589488983154 mm

Highest mean error: 3.799384117126465 mm for frame 238

Lowest mean error: 3.3941617012023926 mm for frame 162

Saving results

Total time: 57.21268701553345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346028
Iteration 2/25 | Loss: 0.00113425
Iteration 3/25 | Loss: 0.00105379
Iteration 4/25 | Loss: 0.00104331
Iteration 5/25 | Loss: 0.00103954
Iteration 6/25 | Loss: 0.00103820
Iteration 7/25 | Loss: 0.00103819
Iteration 8/25 | Loss: 0.00103819
Iteration 9/25 | Loss: 0.00103819
Iteration 10/25 | Loss: 0.00103819
Iteration 11/25 | Loss: 0.00103819
Iteration 12/25 | Loss: 0.00103819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010381868341937661, 0.0010381868341937661, 0.0010381868341937661, 0.0010381868341937661, 0.0010381868341937661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010381868341937661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35905218
Iteration 2/25 | Loss: 0.00098414
Iteration 3/25 | Loss: 0.00098414
Iteration 4/25 | Loss: 0.00098414
Iteration 5/25 | Loss: 0.00098414
Iteration 6/25 | Loss: 0.00098414
Iteration 7/25 | Loss: 0.00098414
Iteration 8/25 | Loss: 0.00098413
Iteration 9/25 | Loss: 0.00098413
Iteration 10/25 | Loss: 0.00098413
Iteration 11/25 | Loss: 0.00098413
Iteration 12/25 | Loss: 0.00098413
Iteration 13/25 | Loss: 0.00098413
Iteration 14/25 | Loss: 0.00098413
Iteration 15/25 | Loss: 0.00098413
Iteration 16/25 | Loss: 0.00098413
Iteration 17/25 | Loss: 0.00098413
Iteration 18/25 | Loss: 0.00098413
Iteration 19/25 | Loss: 0.00098413
Iteration 20/25 | Loss: 0.00098413
Iteration 21/25 | Loss: 0.00098413
Iteration 22/25 | Loss: 0.00098413
Iteration 23/25 | Loss: 0.00098413
Iteration 24/25 | Loss: 0.00098413
Iteration 25/25 | Loss: 0.00098413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098413
Iteration 2/1000 | Loss: 0.00002648
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001135
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001109
Iteration 11/1000 | Loss: 0.00001096
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001088
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001084
Iteration 17/1000 | Loss: 0.00001083
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001083
Iteration 20/1000 | Loss: 0.00001080
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001071
Iteration 26/1000 | Loss: 0.00001070
Iteration 27/1000 | Loss: 0.00001068
Iteration 28/1000 | Loss: 0.00001060
Iteration 29/1000 | Loss: 0.00001060
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001058
Iteration 32/1000 | Loss: 0.00001057
Iteration 33/1000 | Loss: 0.00001056
Iteration 34/1000 | Loss: 0.00001056
Iteration 35/1000 | Loss: 0.00001055
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001053
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001051
Iteration 44/1000 | Loss: 0.00001051
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001050
Iteration 48/1000 | Loss: 0.00001050
Iteration 49/1000 | Loss: 0.00001050
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001050
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001046
Iteration 64/1000 | Loss: 0.00001046
Iteration 65/1000 | Loss: 0.00001046
Iteration 66/1000 | Loss: 0.00001046
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001045
Iteration 69/1000 | Loss: 0.00001045
Iteration 70/1000 | Loss: 0.00001045
Iteration 71/1000 | Loss: 0.00001045
Iteration 72/1000 | Loss: 0.00001045
Iteration 73/1000 | Loss: 0.00001045
Iteration 74/1000 | Loss: 0.00001044
Iteration 75/1000 | Loss: 0.00001044
Iteration 76/1000 | Loss: 0.00001043
Iteration 77/1000 | Loss: 0.00001043
Iteration 78/1000 | Loss: 0.00001043
Iteration 79/1000 | Loss: 0.00001043
Iteration 80/1000 | Loss: 0.00001043
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001043
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001041
Iteration 95/1000 | Loss: 0.00001041
Iteration 96/1000 | Loss: 0.00001041
Iteration 97/1000 | Loss: 0.00001041
Iteration 98/1000 | Loss: 0.00001041
Iteration 99/1000 | Loss: 0.00001040
Iteration 100/1000 | Loss: 0.00001040
Iteration 101/1000 | Loss: 0.00001040
Iteration 102/1000 | Loss: 0.00001040
Iteration 103/1000 | Loss: 0.00001040
Iteration 104/1000 | Loss: 0.00001040
Iteration 105/1000 | Loss: 0.00001040
Iteration 106/1000 | Loss: 0.00001040
Iteration 107/1000 | Loss: 0.00001039
Iteration 108/1000 | Loss: 0.00001039
Iteration 109/1000 | Loss: 0.00001038
Iteration 110/1000 | Loss: 0.00001038
Iteration 111/1000 | Loss: 0.00001038
Iteration 112/1000 | Loss: 0.00001038
Iteration 113/1000 | Loss: 0.00001038
Iteration 114/1000 | Loss: 0.00001038
Iteration 115/1000 | Loss: 0.00001037
Iteration 116/1000 | Loss: 0.00001037
Iteration 117/1000 | Loss: 0.00001037
Iteration 118/1000 | Loss: 0.00001037
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001034
Iteration 128/1000 | Loss: 0.00001034
Iteration 129/1000 | Loss: 0.00001034
Iteration 130/1000 | Loss: 0.00001034
Iteration 131/1000 | Loss: 0.00001033
Iteration 132/1000 | Loss: 0.00001033
Iteration 133/1000 | Loss: 0.00001033
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001032
Iteration 137/1000 | Loss: 0.00001031
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001030
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001029
Iteration 146/1000 | Loss: 0.00001029
Iteration 147/1000 | Loss: 0.00001029
Iteration 148/1000 | Loss: 0.00001029
Iteration 149/1000 | Loss: 0.00001028
Iteration 150/1000 | Loss: 0.00001028
Iteration 151/1000 | Loss: 0.00001028
Iteration 152/1000 | Loss: 0.00001028
Iteration 153/1000 | Loss: 0.00001028
Iteration 154/1000 | Loss: 0.00001028
Iteration 155/1000 | Loss: 0.00001028
Iteration 156/1000 | Loss: 0.00001027
Iteration 157/1000 | Loss: 0.00001027
Iteration 158/1000 | Loss: 0.00001027
Iteration 159/1000 | Loss: 0.00001027
Iteration 160/1000 | Loss: 0.00001027
Iteration 161/1000 | Loss: 0.00001027
Iteration 162/1000 | Loss: 0.00001027
Iteration 163/1000 | Loss: 0.00001027
Iteration 164/1000 | Loss: 0.00001027
Iteration 165/1000 | Loss: 0.00001027
Iteration 166/1000 | Loss: 0.00001027
Iteration 167/1000 | Loss: 0.00001026
Iteration 168/1000 | Loss: 0.00001026
Iteration 169/1000 | Loss: 0.00001026
Iteration 170/1000 | Loss: 0.00001026
Iteration 171/1000 | Loss: 0.00001026
Iteration 172/1000 | Loss: 0.00001025
Iteration 173/1000 | Loss: 0.00001025
Iteration 174/1000 | Loss: 0.00001025
Iteration 175/1000 | Loss: 0.00001024
Iteration 176/1000 | Loss: 0.00001024
Iteration 177/1000 | Loss: 0.00001024
Iteration 178/1000 | Loss: 0.00001024
Iteration 179/1000 | Loss: 0.00001024
Iteration 180/1000 | Loss: 0.00001024
Iteration 181/1000 | Loss: 0.00001024
Iteration 182/1000 | Loss: 0.00001024
Iteration 183/1000 | Loss: 0.00001024
Iteration 184/1000 | Loss: 0.00001024
Iteration 185/1000 | Loss: 0.00001024
Iteration 186/1000 | Loss: 0.00001024
Iteration 187/1000 | Loss: 0.00001024
Iteration 188/1000 | Loss: 0.00001024
Iteration 189/1000 | Loss: 0.00001024
Iteration 190/1000 | Loss: 0.00001023
Iteration 191/1000 | Loss: 0.00001023
Iteration 192/1000 | Loss: 0.00001023
Iteration 193/1000 | Loss: 0.00001023
Iteration 194/1000 | Loss: 0.00001023
Iteration 195/1000 | Loss: 0.00001023
Iteration 196/1000 | Loss: 0.00001023
Iteration 197/1000 | Loss: 0.00001023
Iteration 198/1000 | Loss: 0.00001023
Iteration 199/1000 | Loss: 0.00001023
Iteration 200/1000 | Loss: 0.00001023
Iteration 201/1000 | Loss: 0.00001023
Iteration 202/1000 | Loss: 0.00001022
Iteration 203/1000 | Loss: 0.00001022
Iteration 204/1000 | Loss: 0.00001022
Iteration 205/1000 | Loss: 0.00001022
Iteration 206/1000 | Loss: 0.00001022
Iteration 207/1000 | Loss: 0.00001022
Iteration 208/1000 | Loss: 0.00001022
Iteration 209/1000 | Loss: 0.00001022
Iteration 210/1000 | Loss: 0.00001022
Iteration 211/1000 | Loss: 0.00001022
Iteration 212/1000 | Loss: 0.00001021
Iteration 213/1000 | Loss: 0.00001021
Iteration 214/1000 | Loss: 0.00001021
Iteration 215/1000 | Loss: 0.00001021
Iteration 216/1000 | Loss: 0.00001021
Iteration 217/1000 | Loss: 0.00001021
Iteration 218/1000 | Loss: 0.00001021
Iteration 219/1000 | Loss: 0.00001021
Iteration 220/1000 | Loss: 0.00001021
Iteration 221/1000 | Loss: 0.00001021
Iteration 222/1000 | Loss: 0.00001021
Iteration 223/1000 | Loss: 0.00001021
Iteration 224/1000 | Loss: 0.00001021
Iteration 225/1000 | Loss: 0.00001021
Iteration 226/1000 | Loss: 0.00001021
Iteration 227/1000 | Loss: 0.00001021
Iteration 228/1000 | Loss: 0.00001021
Iteration 229/1000 | Loss: 0.00001020
Iteration 230/1000 | Loss: 0.00001020
Iteration 231/1000 | Loss: 0.00001020
Iteration 232/1000 | Loss: 0.00001020
Iteration 233/1000 | Loss: 0.00001020
Iteration 234/1000 | Loss: 0.00001020
Iteration 235/1000 | Loss: 0.00001020
Iteration 236/1000 | Loss: 0.00001020
Iteration 237/1000 | Loss: 0.00001020
Iteration 238/1000 | Loss: 0.00001020
Iteration 239/1000 | Loss: 0.00001020
Iteration 240/1000 | Loss: 0.00001020
Iteration 241/1000 | Loss: 0.00001020
Iteration 242/1000 | Loss: 0.00001020
Iteration 243/1000 | Loss: 0.00001020
Iteration 244/1000 | Loss: 0.00001020
Iteration 245/1000 | Loss: 0.00001020
Iteration 246/1000 | Loss: 0.00001020
Iteration 247/1000 | Loss: 0.00001020
Iteration 248/1000 | Loss: 0.00001020
Iteration 249/1000 | Loss: 0.00001020
Iteration 250/1000 | Loss: 0.00001020
Iteration 251/1000 | Loss: 0.00001020
Iteration 252/1000 | Loss: 0.00001020
Iteration 253/1000 | Loss: 0.00001020
Iteration 254/1000 | Loss: 0.00001020
Iteration 255/1000 | Loss: 0.00001020
Iteration 256/1000 | Loss: 0.00001020
Iteration 257/1000 | Loss: 0.00001020
Iteration 258/1000 | Loss: 0.00001020
Iteration 259/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.0196778930549044e-05, 1.0196778930549044e-05, 1.0196778930549044e-05, 1.0196778930549044e-05, 1.0196778930549044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0196778930549044e-05

Optimization complete. Final v2v error: 2.698364734649658 mm

Highest mean error: 3.561105728149414 mm for frame 10

Lowest mean error: 2.2204320430755615 mm for frame 132

Saving results

Total time: 42.65679311752319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994007
Iteration 2/25 | Loss: 0.00131977
Iteration 3/25 | Loss: 0.00116890
Iteration 4/25 | Loss: 0.00114174
Iteration 5/25 | Loss: 0.00113807
Iteration 6/25 | Loss: 0.00113376
Iteration 7/25 | Loss: 0.00112947
Iteration 8/25 | Loss: 0.00112692
Iteration 9/25 | Loss: 0.00112338
Iteration 10/25 | Loss: 0.00112221
Iteration 11/25 | Loss: 0.00112233
Iteration 12/25 | Loss: 0.00112070
Iteration 13/25 | Loss: 0.00112042
Iteration 14/25 | Loss: 0.00112036
Iteration 15/25 | Loss: 0.00112035
Iteration 16/25 | Loss: 0.00112035
Iteration 17/25 | Loss: 0.00112035
Iteration 18/25 | Loss: 0.00112035
Iteration 19/25 | Loss: 0.00112035
Iteration 20/25 | Loss: 0.00112035
Iteration 21/25 | Loss: 0.00112035
Iteration 22/25 | Loss: 0.00112035
Iteration 23/25 | Loss: 0.00112034
Iteration 24/25 | Loss: 0.00112034
Iteration 25/25 | Loss: 0.00112033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87698960
Iteration 2/25 | Loss: 0.00095716
Iteration 3/25 | Loss: 0.00095715
Iteration 4/25 | Loss: 0.00095715
Iteration 5/25 | Loss: 0.00095715
Iteration 6/25 | Loss: 0.00095715
Iteration 7/25 | Loss: 0.00095715
Iteration 8/25 | Loss: 0.00095715
Iteration 9/25 | Loss: 0.00095715
Iteration 10/25 | Loss: 0.00095715
Iteration 11/25 | Loss: 0.00095715
Iteration 12/25 | Loss: 0.00095715
Iteration 13/25 | Loss: 0.00095715
Iteration 14/25 | Loss: 0.00095715
Iteration 15/25 | Loss: 0.00095715
Iteration 16/25 | Loss: 0.00095715
Iteration 17/25 | Loss: 0.00095715
Iteration 18/25 | Loss: 0.00095715
Iteration 19/25 | Loss: 0.00095715
Iteration 20/25 | Loss: 0.00095715
Iteration 21/25 | Loss: 0.00095715
Iteration 22/25 | Loss: 0.00095715
Iteration 23/25 | Loss: 0.00095715
Iteration 24/25 | Loss: 0.00095715
Iteration 25/25 | Loss: 0.00095715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095715
Iteration 2/1000 | Loss: 0.00002581
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001449
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001291
Iteration 9/1000 | Loss: 0.00001278
Iteration 10/1000 | Loss: 0.00001266
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001238
Iteration 14/1000 | Loss: 0.00001220
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001200
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001183
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001178
Iteration 23/1000 | Loss: 0.00001177
Iteration 24/1000 | Loss: 0.00001177
Iteration 25/1000 | Loss: 0.00001176
Iteration 26/1000 | Loss: 0.00001176
Iteration 27/1000 | Loss: 0.00001176
Iteration 28/1000 | Loss: 0.00001175
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001175
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001174
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001168
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001164
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001156
Iteration 138/1000 | Loss: 0.00001156
Iteration 139/1000 | Loss: 0.00001156
Iteration 140/1000 | Loss: 0.00001156
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001156
Iteration 154/1000 | Loss: 0.00001156
Iteration 155/1000 | Loss: 0.00001156
Iteration 156/1000 | Loss: 0.00001156
Iteration 157/1000 | Loss: 0.00001156
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001156
Iteration 174/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1557428479136433e-05, 1.1557428479136433e-05, 1.1557428479136433e-05, 1.1557428479136433e-05, 1.1557428479136433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1557428479136433e-05

Optimization complete. Final v2v error: 2.901299476623535 mm

Highest mean error: 3.229358196258545 mm for frame 73

Lowest mean error: 2.5977091789245605 mm for frame 113

Saving results

Total time: 53.261308670043945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551103
Iteration 2/25 | Loss: 0.00128586
Iteration 3/25 | Loss: 0.00118847
Iteration 4/25 | Loss: 0.00117896
Iteration 5/25 | Loss: 0.00117544
Iteration 6/25 | Loss: 0.00117544
Iteration 7/25 | Loss: 0.00117544
Iteration 8/25 | Loss: 0.00117544
Iteration 9/25 | Loss: 0.00117544
Iteration 10/25 | Loss: 0.00117544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011754378210753202, 0.0011754378210753202, 0.0011754378210753202, 0.0011754378210753202, 0.0011754378210753202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011754378210753202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37245321
Iteration 2/25 | Loss: 0.00093177
Iteration 3/25 | Loss: 0.00093177
Iteration 4/25 | Loss: 0.00093177
Iteration 5/25 | Loss: 0.00093177
Iteration 6/25 | Loss: 0.00093177
Iteration 7/25 | Loss: 0.00093177
Iteration 8/25 | Loss: 0.00093177
Iteration 9/25 | Loss: 0.00093177
Iteration 10/25 | Loss: 0.00093177
Iteration 11/25 | Loss: 0.00093177
Iteration 12/25 | Loss: 0.00093177
Iteration 13/25 | Loss: 0.00093177
Iteration 14/25 | Loss: 0.00093177
Iteration 15/25 | Loss: 0.00093177
Iteration 16/25 | Loss: 0.00093177
Iteration 17/25 | Loss: 0.00093177
Iteration 18/25 | Loss: 0.00093177
Iteration 19/25 | Loss: 0.00093177
Iteration 20/25 | Loss: 0.00093177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009317673975601792, 0.0009317673975601792, 0.0009317673975601792, 0.0009317673975601792, 0.0009317673975601792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009317673975601792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093177
Iteration 2/1000 | Loss: 0.00003527
Iteration 3/1000 | Loss: 0.00002748
Iteration 4/1000 | Loss: 0.00002620
Iteration 5/1000 | Loss: 0.00002537
Iteration 6/1000 | Loss: 0.00002490
Iteration 7/1000 | Loss: 0.00002462
Iteration 8/1000 | Loss: 0.00002430
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002390
Iteration 11/1000 | Loss: 0.00002369
Iteration 12/1000 | Loss: 0.00002348
Iteration 13/1000 | Loss: 0.00002329
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002299
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002298
Iteration 18/1000 | Loss: 0.00002296
Iteration 19/1000 | Loss: 0.00002293
Iteration 20/1000 | Loss: 0.00002288
Iteration 21/1000 | Loss: 0.00002287
Iteration 22/1000 | Loss: 0.00002287
Iteration 23/1000 | Loss: 0.00002287
Iteration 24/1000 | Loss: 0.00002287
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002285
Iteration 28/1000 | Loss: 0.00002285
Iteration 29/1000 | Loss: 0.00002285
Iteration 30/1000 | Loss: 0.00002285
Iteration 31/1000 | Loss: 0.00002285
Iteration 32/1000 | Loss: 0.00002285
Iteration 33/1000 | Loss: 0.00002284
Iteration 34/1000 | Loss: 0.00002284
Iteration 35/1000 | Loss: 0.00002284
Iteration 36/1000 | Loss: 0.00002284
Iteration 37/1000 | Loss: 0.00002284
Iteration 38/1000 | Loss: 0.00002284
Iteration 39/1000 | Loss: 0.00002284
Iteration 40/1000 | Loss: 0.00002284
Iteration 41/1000 | Loss: 0.00002284
Iteration 42/1000 | Loss: 0.00002283
Iteration 43/1000 | Loss: 0.00002279
Iteration 44/1000 | Loss: 0.00002277
Iteration 45/1000 | Loss: 0.00002276
Iteration 46/1000 | Loss: 0.00002272
Iteration 47/1000 | Loss: 0.00002271
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00002271
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002270
Iteration 53/1000 | Loss: 0.00002270
Iteration 54/1000 | Loss: 0.00002269
Iteration 55/1000 | Loss: 0.00002269
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002269
Iteration 58/1000 | Loss: 0.00002269
Iteration 59/1000 | Loss: 0.00002269
Iteration 60/1000 | Loss: 0.00002268
Iteration 61/1000 | Loss: 0.00002268
Iteration 62/1000 | Loss: 0.00002268
Iteration 63/1000 | Loss: 0.00002267
Iteration 64/1000 | Loss: 0.00002267
Iteration 65/1000 | Loss: 0.00002267
Iteration 66/1000 | Loss: 0.00002267
Iteration 67/1000 | Loss: 0.00002267
Iteration 68/1000 | Loss: 0.00002267
Iteration 69/1000 | Loss: 0.00002267
Iteration 70/1000 | Loss: 0.00002267
Iteration 71/1000 | Loss: 0.00002266
Iteration 72/1000 | Loss: 0.00002266
Iteration 73/1000 | Loss: 0.00002266
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002266
Iteration 76/1000 | Loss: 0.00002265
Iteration 77/1000 | Loss: 0.00002265
Iteration 78/1000 | Loss: 0.00002265
Iteration 79/1000 | Loss: 0.00002265
Iteration 80/1000 | Loss: 0.00002265
Iteration 81/1000 | Loss: 0.00002264
Iteration 82/1000 | Loss: 0.00002264
Iteration 83/1000 | Loss: 0.00002264
Iteration 84/1000 | Loss: 0.00002264
Iteration 85/1000 | Loss: 0.00002264
Iteration 86/1000 | Loss: 0.00002264
Iteration 87/1000 | Loss: 0.00002264
Iteration 88/1000 | Loss: 0.00002264
Iteration 89/1000 | Loss: 0.00002264
Iteration 90/1000 | Loss: 0.00002264
Iteration 91/1000 | Loss: 0.00002264
Iteration 92/1000 | Loss: 0.00002264
Iteration 93/1000 | Loss: 0.00002264
Iteration 94/1000 | Loss: 0.00002263
Iteration 95/1000 | Loss: 0.00002263
Iteration 96/1000 | Loss: 0.00002263
Iteration 97/1000 | Loss: 0.00002263
Iteration 98/1000 | Loss: 0.00002263
Iteration 99/1000 | Loss: 0.00002263
Iteration 100/1000 | Loss: 0.00002263
Iteration 101/1000 | Loss: 0.00002263
Iteration 102/1000 | Loss: 0.00002263
Iteration 103/1000 | Loss: 0.00002263
Iteration 104/1000 | Loss: 0.00002262
Iteration 105/1000 | Loss: 0.00002262
Iteration 106/1000 | Loss: 0.00002262
Iteration 107/1000 | Loss: 0.00002262
Iteration 108/1000 | Loss: 0.00002262
Iteration 109/1000 | Loss: 0.00002261
Iteration 110/1000 | Loss: 0.00002261
Iteration 111/1000 | Loss: 0.00002261
Iteration 112/1000 | Loss: 0.00002261
Iteration 113/1000 | Loss: 0.00002261
Iteration 114/1000 | Loss: 0.00002261
Iteration 115/1000 | Loss: 0.00002261
Iteration 116/1000 | Loss: 0.00002261
Iteration 117/1000 | Loss: 0.00002261
Iteration 118/1000 | Loss: 0.00002261
Iteration 119/1000 | Loss: 0.00002261
Iteration 120/1000 | Loss: 0.00002261
Iteration 121/1000 | Loss: 0.00002261
Iteration 122/1000 | Loss: 0.00002261
Iteration 123/1000 | Loss: 0.00002261
Iteration 124/1000 | Loss: 0.00002261
Iteration 125/1000 | Loss: 0.00002261
Iteration 126/1000 | Loss: 0.00002261
Iteration 127/1000 | Loss: 0.00002261
Iteration 128/1000 | Loss: 0.00002261
Iteration 129/1000 | Loss: 0.00002261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.260798282804899e-05, 2.260798282804899e-05, 2.260798282804899e-05, 2.260798282804899e-05, 2.260798282804899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.260798282804899e-05

Optimization complete. Final v2v error: 3.826965093612671 mm

Highest mean error: 4.240489482879639 mm for frame 132

Lowest mean error: 3.406473159790039 mm for frame 81

Saving results

Total time: 43.309650897979736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495600
Iteration 2/25 | Loss: 0.00121748
Iteration 3/25 | Loss: 0.00112635
Iteration 4/25 | Loss: 0.00111439
Iteration 5/25 | Loss: 0.00110929
Iteration 6/25 | Loss: 0.00110881
Iteration 7/25 | Loss: 0.00110881
Iteration 8/25 | Loss: 0.00110881
Iteration 9/25 | Loss: 0.00110881
Iteration 10/25 | Loss: 0.00110881
Iteration 11/25 | Loss: 0.00110881
Iteration 12/25 | Loss: 0.00110881
Iteration 13/25 | Loss: 0.00110881
Iteration 14/25 | Loss: 0.00110881
Iteration 15/25 | Loss: 0.00110881
Iteration 16/25 | Loss: 0.00110881
Iteration 17/25 | Loss: 0.00110881
Iteration 18/25 | Loss: 0.00110881
Iteration 19/25 | Loss: 0.00110881
Iteration 20/25 | Loss: 0.00110881
Iteration 21/25 | Loss: 0.00110881
Iteration 22/25 | Loss: 0.00110881
Iteration 23/25 | Loss: 0.00110881
Iteration 24/25 | Loss: 0.00110881
Iteration 25/25 | Loss: 0.00110881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79254079
Iteration 2/25 | Loss: 0.00079477
Iteration 3/25 | Loss: 0.00079477
Iteration 4/25 | Loss: 0.00079477
Iteration 5/25 | Loss: 0.00079477
Iteration 6/25 | Loss: 0.00079476
Iteration 7/25 | Loss: 0.00079476
Iteration 8/25 | Loss: 0.00079476
Iteration 9/25 | Loss: 0.00079476
Iteration 10/25 | Loss: 0.00079476
Iteration 11/25 | Loss: 0.00079476
Iteration 12/25 | Loss: 0.00079476
Iteration 13/25 | Loss: 0.00079476
Iteration 14/25 | Loss: 0.00079476
Iteration 15/25 | Loss: 0.00079476
Iteration 16/25 | Loss: 0.00079476
Iteration 17/25 | Loss: 0.00079476
Iteration 18/25 | Loss: 0.00079476
Iteration 19/25 | Loss: 0.00079476
Iteration 20/25 | Loss: 0.00079476
Iteration 21/25 | Loss: 0.00079476
Iteration 22/25 | Loss: 0.00079476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007947639678604901, 0.0007947639678604901, 0.0007947639678604901, 0.0007947639678604901, 0.0007947639678604901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007947639678604901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079476
Iteration 2/1000 | Loss: 0.00003351
Iteration 3/1000 | Loss: 0.00002283
Iteration 4/1000 | Loss: 0.00002103
Iteration 5/1000 | Loss: 0.00002007
Iteration 6/1000 | Loss: 0.00001936
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001784
Iteration 11/1000 | Loss: 0.00001765
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001687
Iteration 19/1000 | Loss: 0.00001687
Iteration 20/1000 | Loss: 0.00001686
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001650
Iteration 32/1000 | Loss: 0.00001647
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001645
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001644
Iteration 41/1000 | Loss: 0.00001644
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001643
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001642
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001641
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001641
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001640
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001639
Iteration 58/1000 | Loss: 0.00001639
Iteration 59/1000 | Loss: 0.00001639
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001639
Iteration 62/1000 | Loss: 0.00001639
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001638
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001637
Iteration 71/1000 | Loss: 0.00001637
Iteration 72/1000 | Loss: 0.00001637
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001636
Iteration 75/1000 | Loss: 0.00001636
Iteration 76/1000 | Loss: 0.00001635
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001634
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001633
Iteration 85/1000 | Loss: 0.00001633
Iteration 86/1000 | Loss: 0.00001633
Iteration 87/1000 | Loss: 0.00001632
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001631
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00001631
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001630
Iteration 99/1000 | Loss: 0.00001630
Iteration 100/1000 | Loss: 0.00001630
Iteration 101/1000 | Loss: 0.00001630
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001629
Iteration 104/1000 | Loss: 0.00001629
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001627
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001623
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001622
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001619
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001617
Iteration 169/1000 | Loss: 0.00001617
Iteration 170/1000 | Loss: 0.00001617
Iteration 171/1000 | Loss: 0.00001617
Iteration 172/1000 | Loss: 0.00001617
Iteration 173/1000 | Loss: 0.00001617
Iteration 174/1000 | Loss: 0.00001617
Iteration 175/1000 | Loss: 0.00001617
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001617
Iteration 180/1000 | Loss: 0.00001617
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001616
Iteration 187/1000 | Loss: 0.00001616
Iteration 188/1000 | Loss: 0.00001616
Iteration 189/1000 | Loss: 0.00001616
Iteration 190/1000 | Loss: 0.00001616
Iteration 191/1000 | Loss: 0.00001616
Iteration 192/1000 | Loss: 0.00001616
Iteration 193/1000 | Loss: 0.00001616
Iteration 194/1000 | Loss: 0.00001616
Iteration 195/1000 | Loss: 0.00001616
Iteration 196/1000 | Loss: 0.00001616
Iteration 197/1000 | Loss: 0.00001616
Iteration 198/1000 | Loss: 0.00001616
Iteration 199/1000 | Loss: 0.00001616
Iteration 200/1000 | Loss: 0.00001616
Iteration 201/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.6155292541952804e-05, 1.6155292541952804e-05, 1.6155292541952804e-05, 1.6155292541952804e-05, 1.6155292541952804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6155292541952804e-05

Optimization complete. Final v2v error: 3.4095067977905273 mm

Highest mean error: 3.993626594543457 mm for frame 3

Lowest mean error: 3.2458057403564453 mm for frame 36

Saving results

Total time: 55.39044952392578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007749
Iteration 2/25 | Loss: 0.00219721
Iteration 3/25 | Loss: 0.00176409
Iteration 4/25 | Loss: 0.00144718
Iteration 5/25 | Loss: 0.00138173
Iteration 6/25 | Loss: 0.00135284
Iteration 7/25 | Loss: 0.00132132
Iteration 8/25 | Loss: 0.00130867
Iteration 9/25 | Loss: 0.00129120
Iteration 10/25 | Loss: 0.00128889
Iteration 11/25 | Loss: 0.00127594
Iteration 12/25 | Loss: 0.00128447
Iteration 13/25 | Loss: 0.00126899
Iteration 14/25 | Loss: 0.00125445
Iteration 15/25 | Loss: 0.00124322
Iteration 16/25 | Loss: 0.00124323
Iteration 17/25 | Loss: 0.00124439
Iteration 18/25 | Loss: 0.00123852
Iteration 19/25 | Loss: 0.00123549
Iteration 20/25 | Loss: 0.00124437
Iteration 21/25 | Loss: 0.00124012
Iteration 22/25 | Loss: 0.00124469
Iteration 23/25 | Loss: 0.00123893
Iteration 24/25 | Loss: 0.00123894
Iteration 25/25 | Loss: 0.00123988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34595191
Iteration 2/25 | Loss: 0.00197098
Iteration 3/25 | Loss: 0.00197088
Iteration 4/25 | Loss: 0.00197101
Iteration 5/25 | Loss: 0.00196806
Iteration 6/25 | Loss: 0.00196806
Iteration 7/25 | Loss: 0.00196806
Iteration 8/25 | Loss: 0.00196806
Iteration 9/25 | Loss: 0.00196806
Iteration 10/25 | Loss: 0.00196805
Iteration 11/25 | Loss: 0.00196805
Iteration 12/25 | Loss: 0.00196805
Iteration 13/25 | Loss: 0.00196805
Iteration 14/25 | Loss: 0.00196805
Iteration 15/25 | Loss: 0.00196805
Iteration 16/25 | Loss: 0.00196805
Iteration 17/25 | Loss: 0.00196805
Iteration 18/25 | Loss: 0.00196805
Iteration 19/25 | Loss: 0.00196805
Iteration 20/25 | Loss: 0.00196805
Iteration 21/25 | Loss: 0.00196805
Iteration 22/25 | Loss: 0.00196805
Iteration 23/25 | Loss: 0.00196805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019680538680404425, 0.0019680538680404425, 0.0019680538680404425, 0.0019680538680404425, 0.0019680538680404425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019680538680404425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196805
Iteration 2/1000 | Loss: 0.00038398
Iteration 3/1000 | Loss: 0.00035265
Iteration 4/1000 | Loss: 0.00032055
Iteration 5/1000 | Loss: 0.00036273
Iteration 6/1000 | Loss: 0.00092058
Iteration 7/1000 | Loss: 0.00054222
Iteration 8/1000 | Loss: 0.00011524
Iteration 9/1000 | Loss: 0.00045709
Iteration 10/1000 | Loss: 0.00047974
Iteration 11/1000 | Loss: 0.00044233
Iteration 12/1000 | Loss: 0.00047408
Iteration 13/1000 | Loss: 0.00058342
Iteration 14/1000 | Loss: 0.00021690
Iteration 15/1000 | Loss: 0.00048739
Iteration 16/1000 | Loss: 0.00041120
Iteration 17/1000 | Loss: 0.00088093
Iteration 18/1000 | Loss: 0.00045632
Iteration 19/1000 | Loss: 0.00092677
Iteration 20/1000 | Loss: 0.00017163
Iteration 21/1000 | Loss: 0.00010966
Iteration 22/1000 | Loss: 0.00023700
Iteration 23/1000 | Loss: 0.00008879
Iteration 24/1000 | Loss: 0.00015534
Iteration 25/1000 | Loss: 0.00011964
Iteration 26/1000 | Loss: 0.00009040
Iteration 27/1000 | Loss: 0.00011817
Iteration 28/1000 | Loss: 0.00010409
Iteration 29/1000 | Loss: 0.00009703
Iteration 30/1000 | Loss: 0.00012010
Iteration 31/1000 | Loss: 0.00011778
Iteration 32/1000 | Loss: 0.00011723
Iteration 33/1000 | Loss: 0.00009970
Iteration 34/1000 | Loss: 0.00010074
Iteration 35/1000 | Loss: 0.00011346
Iteration 36/1000 | Loss: 0.00009704
Iteration 37/1000 | Loss: 0.00011335
Iteration 38/1000 | Loss: 0.00010121
Iteration 39/1000 | Loss: 0.00010543
Iteration 40/1000 | Loss: 0.00010026
Iteration 41/1000 | Loss: 0.00009372
Iteration 42/1000 | Loss: 0.00012915
Iteration 43/1000 | Loss: 0.00011353
Iteration 44/1000 | Loss: 0.00036708
Iteration 45/1000 | Loss: 0.00028282
Iteration 46/1000 | Loss: 0.00027259
Iteration 47/1000 | Loss: 0.00012270
Iteration 48/1000 | Loss: 0.00012655
Iteration 49/1000 | Loss: 0.00011435
Iteration 50/1000 | Loss: 0.00040054
Iteration 51/1000 | Loss: 0.00013046
Iteration 52/1000 | Loss: 0.00011101
Iteration 53/1000 | Loss: 0.00010135
Iteration 54/1000 | Loss: 0.00010561
Iteration 55/1000 | Loss: 0.00064115
Iteration 56/1000 | Loss: 0.00039758
Iteration 57/1000 | Loss: 0.00035059
Iteration 58/1000 | Loss: 0.00026857
Iteration 59/1000 | Loss: 0.00019233
Iteration 60/1000 | Loss: 0.00015752
Iteration 61/1000 | Loss: 0.00007682
Iteration 62/1000 | Loss: 0.00030438
Iteration 63/1000 | Loss: 0.00010838
Iteration 64/1000 | Loss: 0.00009025
Iteration 65/1000 | Loss: 0.00091244
Iteration 66/1000 | Loss: 0.00061895
Iteration 67/1000 | Loss: 0.00017654
Iteration 68/1000 | Loss: 0.00011545
Iteration 69/1000 | Loss: 0.00007935
Iteration 70/1000 | Loss: 0.00008842
Iteration 71/1000 | Loss: 0.00008468
Iteration 72/1000 | Loss: 0.00009479
Iteration 73/1000 | Loss: 0.00009642
Iteration 74/1000 | Loss: 0.00009081
Iteration 75/1000 | Loss: 0.00081461
Iteration 76/1000 | Loss: 0.00204844
Iteration 77/1000 | Loss: 0.00241221
Iteration 78/1000 | Loss: 0.00254429
Iteration 79/1000 | Loss: 0.00078894
Iteration 80/1000 | Loss: 0.00115086
Iteration 81/1000 | Loss: 0.00198630
Iteration 82/1000 | Loss: 0.00168610
Iteration 83/1000 | Loss: 0.00095565
Iteration 84/1000 | Loss: 0.00065499
Iteration 85/1000 | Loss: 0.00018773
Iteration 86/1000 | Loss: 0.00031086
Iteration 87/1000 | Loss: 0.00006869
Iteration 88/1000 | Loss: 0.00056198
Iteration 89/1000 | Loss: 0.00008590
Iteration 90/1000 | Loss: 0.00102763
Iteration 91/1000 | Loss: 0.00019357
Iteration 92/1000 | Loss: 0.00005996
Iteration 93/1000 | Loss: 0.00003300
Iteration 94/1000 | Loss: 0.00002574
Iteration 95/1000 | Loss: 0.00032246
Iteration 96/1000 | Loss: 0.00106363
Iteration 97/1000 | Loss: 0.00162743
Iteration 98/1000 | Loss: 0.00011310
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00035681
Iteration 101/1000 | Loss: 0.00001786
Iteration 102/1000 | Loss: 0.00036902
Iteration 103/1000 | Loss: 0.00005274
Iteration 104/1000 | Loss: 0.00007997
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001460
Iteration 107/1000 | Loss: 0.00027893
Iteration 108/1000 | Loss: 0.00036541
Iteration 109/1000 | Loss: 0.00002790
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001357
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001079
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001057
Iteration 123/1000 | Loss: 0.00001057
Iteration 124/1000 | Loss: 0.00001057
Iteration 125/1000 | Loss: 0.00001056
Iteration 126/1000 | Loss: 0.00001056
Iteration 127/1000 | Loss: 0.00001056
Iteration 128/1000 | Loss: 0.00001056
Iteration 129/1000 | Loss: 0.00001055
Iteration 130/1000 | Loss: 0.00001055
Iteration 131/1000 | Loss: 0.00001055
Iteration 132/1000 | Loss: 0.00001055
Iteration 133/1000 | Loss: 0.00001054
Iteration 134/1000 | Loss: 0.00001054
Iteration 135/1000 | Loss: 0.00001054
Iteration 136/1000 | Loss: 0.00001054
Iteration 137/1000 | Loss: 0.00001054
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001053
Iteration 141/1000 | Loss: 0.00001053
Iteration 142/1000 | Loss: 0.00001053
Iteration 143/1000 | Loss: 0.00001053
Iteration 144/1000 | Loss: 0.00001053
Iteration 145/1000 | Loss: 0.00001052
Iteration 146/1000 | Loss: 0.00001052
Iteration 147/1000 | Loss: 0.00001052
Iteration 148/1000 | Loss: 0.00001052
Iteration 149/1000 | Loss: 0.00001052
Iteration 150/1000 | Loss: 0.00001051
Iteration 151/1000 | Loss: 0.00001051
Iteration 152/1000 | Loss: 0.00001051
Iteration 153/1000 | Loss: 0.00001050
Iteration 154/1000 | Loss: 0.00001050
Iteration 155/1000 | Loss: 0.00001050
Iteration 156/1000 | Loss: 0.00001050
Iteration 157/1000 | Loss: 0.00001050
Iteration 158/1000 | Loss: 0.00001049
Iteration 159/1000 | Loss: 0.00001049
Iteration 160/1000 | Loss: 0.00001049
Iteration 161/1000 | Loss: 0.00001048
Iteration 162/1000 | Loss: 0.00001048
Iteration 163/1000 | Loss: 0.00001048
Iteration 164/1000 | Loss: 0.00001048
Iteration 165/1000 | Loss: 0.00001048
Iteration 166/1000 | Loss: 0.00001048
Iteration 167/1000 | Loss: 0.00001047
Iteration 168/1000 | Loss: 0.00001047
Iteration 169/1000 | Loss: 0.00001047
Iteration 170/1000 | Loss: 0.00001047
Iteration 171/1000 | Loss: 0.00001047
Iteration 172/1000 | Loss: 0.00001047
Iteration 173/1000 | Loss: 0.00001047
Iteration 174/1000 | Loss: 0.00001047
Iteration 175/1000 | Loss: 0.00001047
Iteration 176/1000 | Loss: 0.00001046
Iteration 177/1000 | Loss: 0.00001046
Iteration 178/1000 | Loss: 0.00001046
Iteration 179/1000 | Loss: 0.00001046
Iteration 180/1000 | Loss: 0.00001046
Iteration 181/1000 | Loss: 0.00001046
Iteration 182/1000 | Loss: 0.00001046
Iteration 183/1000 | Loss: 0.00001046
Iteration 184/1000 | Loss: 0.00001045
Iteration 185/1000 | Loss: 0.00001045
Iteration 186/1000 | Loss: 0.00001045
Iteration 187/1000 | Loss: 0.00001045
Iteration 188/1000 | Loss: 0.00001045
Iteration 189/1000 | Loss: 0.00001045
Iteration 190/1000 | Loss: 0.00001045
Iteration 191/1000 | Loss: 0.00001045
Iteration 192/1000 | Loss: 0.00001045
Iteration 193/1000 | Loss: 0.00001045
Iteration 194/1000 | Loss: 0.00001044
Iteration 195/1000 | Loss: 0.00001044
Iteration 196/1000 | Loss: 0.00001044
Iteration 197/1000 | Loss: 0.00001044
Iteration 198/1000 | Loss: 0.00001044
Iteration 199/1000 | Loss: 0.00001044
Iteration 200/1000 | Loss: 0.00001044
Iteration 201/1000 | Loss: 0.00001044
Iteration 202/1000 | Loss: 0.00001044
Iteration 203/1000 | Loss: 0.00001044
Iteration 204/1000 | Loss: 0.00001044
Iteration 205/1000 | Loss: 0.00001044
Iteration 206/1000 | Loss: 0.00001044
Iteration 207/1000 | Loss: 0.00001043
Iteration 208/1000 | Loss: 0.00001043
Iteration 209/1000 | Loss: 0.00001043
Iteration 210/1000 | Loss: 0.00001043
Iteration 211/1000 | Loss: 0.00001043
Iteration 212/1000 | Loss: 0.00001043
Iteration 213/1000 | Loss: 0.00001043
Iteration 214/1000 | Loss: 0.00001043
Iteration 215/1000 | Loss: 0.00001043
Iteration 216/1000 | Loss: 0.00001043
Iteration 217/1000 | Loss: 0.00001043
Iteration 218/1000 | Loss: 0.00001043
Iteration 219/1000 | Loss: 0.00001043
Iteration 220/1000 | Loss: 0.00001043
Iteration 221/1000 | Loss: 0.00001043
Iteration 222/1000 | Loss: 0.00001043
Iteration 223/1000 | Loss: 0.00001043
Iteration 224/1000 | Loss: 0.00001043
Iteration 225/1000 | Loss: 0.00001043
Iteration 226/1000 | Loss: 0.00001043
Iteration 227/1000 | Loss: 0.00001043
Iteration 228/1000 | Loss: 0.00001043
Iteration 229/1000 | Loss: 0.00001043
Iteration 230/1000 | Loss: 0.00001043
Iteration 231/1000 | Loss: 0.00001043
Iteration 232/1000 | Loss: 0.00001043
Iteration 233/1000 | Loss: 0.00001043
Iteration 234/1000 | Loss: 0.00001043
Iteration 235/1000 | Loss: 0.00001043
Iteration 236/1000 | Loss: 0.00001043
Iteration 237/1000 | Loss: 0.00001043
Iteration 238/1000 | Loss: 0.00001043
Iteration 239/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.0427819688629825e-05, 1.0427819688629825e-05, 1.0427819688629825e-05, 1.0427819688629825e-05, 1.0427819688629825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0427819688629825e-05

Optimization complete. Final v2v error: 2.698451042175293 mm

Highest mean error: 4.763177394866943 mm for frame 56

Lowest mean error: 2.4280524253845215 mm for frame 33

Saving results

Total time: 213.5680980682373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487549
Iteration 2/25 | Loss: 0.00127203
Iteration 3/25 | Loss: 0.00116240
Iteration 4/25 | Loss: 0.00114531
Iteration 5/25 | Loss: 0.00113932
Iteration 6/25 | Loss: 0.00113894
Iteration 7/25 | Loss: 0.00113886
Iteration 8/25 | Loss: 0.00113886
Iteration 9/25 | Loss: 0.00113886
Iteration 10/25 | Loss: 0.00113886
Iteration 11/25 | Loss: 0.00113886
Iteration 12/25 | Loss: 0.00113886
Iteration 13/25 | Loss: 0.00113886
Iteration 14/25 | Loss: 0.00113886
Iteration 15/25 | Loss: 0.00113886
Iteration 16/25 | Loss: 0.00113886
Iteration 17/25 | Loss: 0.00113886
Iteration 18/25 | Loss: 0.00113886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011388577986508608, 0.0011388577986508608, 0.0011388577986508608, 0.0011388577986508608, 0.0011388577986508608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011388577986508608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82260823
Iteration 2/25 | Loss: 0.00077367
Iteration 3/25 | Loss: 0.00077367
Iteration 4/25 | Loss: 0.00077367
Iteration 5/25 | Loss: 0.00077367
Iteration 6/25 | Loss: 0.00077367
Iteration 7/25 | Loss: 0.00077367
Iteration 8/25 | Loss: 0.00077367
Iteration 9/25 | Loss: 0.00077367
Iteration 10/25 | Loss: 0.00077367
Iteration 11/25 | Loss: 0.00077367
Iteration 12/25 | Loss: 0.00077367
Iteration 13/25 | Loss: 0.00077367
Iteration 14/25 | Loss: 0.00077367
Iteration 15/25 | Loss: 0.00077367
Iteration 16/25 | Loss: 0.00077367
Iteration 17/25 | Loss: 0.00077367
Iteration 18/25 | Loss: 0.00077367
Iteration 19/25 | Loss: 0.00077367
Iteration 20/25 | Loss: 0.00077367
Iteration 21/25 | Loss: 0.00077367
Iteration 22/25 | Loss: 0.00077367
Iteration 23/25 | Loss: 0.00077367
Iteration 24/25 | Loss: 0.00077367
Iteration 25/25 | Loss: 0.00077367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077367
Iteration 2/1000 | Loss: 0.00003615
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00001967
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001723
Iteration 8/1000 | Loss: 0.00001688
Iteration 9/1000 | Loss: 0.00001659
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001608
Iteration 13/1000 | Loss: 0.00001604
Iteration 14/1000 | Loss: 0.00001587
Iteration 15/1000 | Loss: 0.00001568
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001528
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001527
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001526
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001500
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001498
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001481
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001480
Iteration 63/1000 | Loss: 0.00001480
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001478
Iteration 81/1000 | Loss: 0.00001478
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001477
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001476
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001473
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001470
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001470
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001467
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001466
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001466
Iteration 147/1000 | Loss: 0.00001465
Iteration 148/1000 | Loss: 0.00001465
Iteration 149/1000 | Loss: 0.00001465
Iteration 150/1000 | Loss: 0.00001465
Iteration 151/1000 | Loss: 0.00001465
Iteration 152/1000 | Loss: 0.00001465
Iteration 153/1000 | Loss: 0.00001465
Iteration 154/1000 | Loss: 0.00001465
Iteration 155/1000 | Loss: 0.00001465
Iteration 156/1000 | Loss: 0.00001465
Iteration 157/1000 | Loss: 0.00001465
Iteration 158/1000 | Loss: 0.00001465
Iteration 159/1000 | Loss: 0.00001465
Iteration 160/1000 | Loss: 0.00001465
Iteration 161/1000 | Loss: 0.00001464
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001464
Iteration 166/1000 | Loss: 0.00001464
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001463
Iteration 170/1000 | Loss: 0.00001463
Iteration 171/1000 | Loss: 0.00001463
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001462
Iteration 181/1000 | Loss: 0.00001462
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001461
Iteration 192/1000 | Loss: 0.00001461
Iteration 193/1000 | Loss: 0.00001461
Iteration 194/1000 | Loss: 0.00001461
Iteration 195/1000 | Loss: 0.00001461
Iteration 196/1000 | Loss: 0.00001461
Iteration 197/1000 | Loss: 0.00001461
Iteration 198/1000 | Loss: 0.00001461
Iteration 199/1000 | Loss: 0.00001461
Iteration 200/1000 | Loss: 0.00001461
Iteration 201/1000 | Loss: 0.00001461
Iteration 202/1000 | Loss: 0.00001461
Iteration 203/1000 | Loss: 0.00001461
Iteration 204/1000 | Loss: 0.00001461
Iteration 205/1000 | Loss: 0.00001461
Iteration 206/1000 | Loss: 0.00001461
Iteration 207/1000 | Loss: 0.00001461
Iteration 208/1000 | Loss: 0.00001461
Iteration 209/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4609903701057192e-05, 1.4609903701057192e-05, 1.4609903701057192e-05, 1.4609903701057192e-05, 1.4609903701057192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4609903701057192e-05

Optimization complete. Final v2v error: 3.1932835578918457 mm

Highest mean error: 3.692322254180908 mm for frame 4

Lowest mean error: 3.12270188331604 mm for frame 147

Saving results

Total time: 56.34458136558533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00656654
Iteration 2/25 | Loss: 0.00118776
Iteration 3/25 | Loss: 0.00109908
Iteration 4/25 | Loss: 0.00108782
Iteration 5/25 | Loss: 0.00108419
Iteration 6/25 | Loss: 0.00108377
Iteration 7/25 | Loss: 0.00108377
Iteration 8/25 | Loss: 0.00108377
Iteration 9/25 | Loss: 0.00108377
Iteration 10/25 | Loss: 0.00108377
Iteration 11/25 | Loss: 0.00108377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010837744921445847, 0.0010837744921445847, 0.0010837744921445847, 0.0010837744921445847, 0.0010837744921445847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010837744921445847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30613518
Iteration 2/25 | Loss: 0.00079709
Iteration 3/25 | Loss: 0.00079709
Iteration 4/25 | Loss: 0.00079709
Iteration 5/25 | Loss: 0.00079709
Iteration 6/25 | Loss: 0.00079709
Iteration 7/25 | Loss: 0.00079709
Iteration 8/25 | Loss: 0.00079709
Iteration 9/25 | Loss: 0.00079709
Iteration 10/25 | Loss: 0.00079709
Iteration 11/25 | Loss: 0.00079709
Iteration 12/25 | Loss: 0.00079709
Iteration 13/25 | Loss: 0.00079709
Iteration 14/25 | Loss: 0.00079709
Iteration 15/25 | Loss: 0.00079709
Iteration 16/25 | Loss: 0.00079709
Iteration 17/25 | Loss: 0.00079709
Iteration 18/25 | Loss: 0.00079709
Iteration 19/25 | Loss: 0.00079709
Iteration 20/25 | Loss: 0.00079709
Iteration 21/25 | Loss: 0.00079709
Iteration 22/25 | Loss: 0.00079709
Iteration 23/25 | Loss: 0.00079709
Iteration 24/25 | Loss: 0.00079709
Iteration 25/25 | Loss: 0.00079709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079709
Iteration 2/1000 | Loss: 0.00002357
Iteration 3/1000 | Loss: 0.00001548
Iteration 4/1000 | Loss: 0.00001393
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001280
Iteration 7/1000 | Loss: 0.00001240
Iteration 8/1000 | Loss: 0.00001208
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001130
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001103
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001100
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001096
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001090
Iteration 88/1000 | Loss: 0.00001090
Iteration 89/1000 | Loss: 0.00001090
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001089
Iteration 92/1000 | Loss: 0.00001089
Iteration 93/1000 | Loss: 0.00001089
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001088
Iteration 96/1000 | Loss: 0.00001088
Iteration 97/1000 | Loss: 0.00001088
Iteration 98/1000 | Loss: 0.00001088
Iteration 99/1000 | Loss: 0.00001088
Iteration 100/1000 | Loss: 0.00001087
Iteration 101/1000 | Loss: 0.00001087
Iteration 102/1000 | Loss: 0.00001087
Iteration 103/1000 | Loss: 0.00001087
Iteration 104/1000 | Loss: 0.00001087
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001087
Iteration 107/1000 | Loss: 0.00001087
Iteration 108/1000 | Loss: 0.00001087
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.0869719517359044e-05, 1.0869719517359044e-05, 1.0869719517359044e-05, 1.0869719517359044e-05, 1.0869719517359044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0869719517359044e-05

Optimization complete. Final v2v error: 2.828261613845825 mm

Highest mean error: 3.0477781295776367 mm for frame 70

Lowest mean error: 2.6332688331604004 mm for frame 10

Saving results

Total time: 33.20117425918579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888833
Iteration 2/25 | Loss: 0.00167097
Iteration 3/25 | Loss: 0.00138207
Iteration 4/25 | Loss: 0.00135779
Iteration 5/25 | Loss: 0.00135495
Iteration 6/25 | Loss: 0.00135495
Iteration 7/25 | Loss: 0.00135495
Iteration 8/25 | Loss: 0.00135495
Iteration 9/25 | Loss: 0.00135495
Iteration 10/25 | Loss: 0.00135495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013549502473324537, 0.0013549502473324537, 0.0013549502473324537, 0.0013549502473324537, 0.0013549502473324537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013549502473324537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57238126
Iteration 2/25 | Loss: 0.00092795
Iteration 3/25 | Loss: 0.00092795
Iteration 4/25 | Loss: 0.00092795
Iteration 5/25 | Loss: 0.00092795
Iteration 6/25 | Loss: 0.00092795
Iteration 7/25 | Loss: 0.00092795
Iteration 8/25 | Loss: 0.00092795
Iteration 9/25 | Loss: 0.00092795
Iteration 10/25 | Loss: 0.00092795
Iteration 11/25 | Loss: 0.00092795
Iteration 12/25 | Loss: 0.00092795
Iteration 13/25 | Loss: 0.00092795
Iteration 14/25 | Loss: 0.00092795
Iteration 15/25 | Loss: 0.00092795
Iteration 16/25 | Loss: 0.00092795
Iteration 17/25 | Loss: 0.00092795
Iteration 18/25 | Loss: 0.00092795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000927948800381273, 0.000927948800381273, 0.000927948800381273, 0.000927948800381273, 0.000927948800381273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000927948800381273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092795
Iteration 2/1000 | Loss: 0.00004723
Iteration 3/1000 | Loss: 0.00003344
Iteration 4/1000 | Loss: 0.00003081
Iteration 5/1000 | Loss: 0.00002963
Iteration 6/1000 | Loss: 0.00002894
Iteration 7/1000 | Loss: 0.00002853
Iteration 8/1000 | Loss: 0.00002798
Iteration 9/1000 | Loss: 0.00002775
Iteration 10/1000 | Loss: 0.00002749
Iteration 11/1000 | Loss: 0.00002734
Iteration 12/1000 | Loss: 0.00002731
Iteration 13/1000 | Loss: 0.00002721
Iteration 14/1000 | Loss: 0.00002719
Iteration 15/1000 | Loss: 0.00002718
Iteration 16/1000 | Loss: 0.00002718
Iteration 17/1000 | Loss: 0.00002718
Iteration 18/1000 | Loss: 0.00002718
Iteration 19/1000 | Loss: 0.00002718
Iteration 20/1000 | Loss: 0.00002718
Iteration 21/1000 | Loss: 0.00002718
Iteration 22/1000 | Loss: 0.00002718
Iteration 23/1000 | Loss: 0.00002718
Iteration 24/1000 | Loss: 0.00002713
Iteration 25/1000 | Loss: 0.00002713
Iteration 26/1000 | Loss: 0.00002713
Iteration 27/1000 | Loss: 0.00002713
Iteration 28/1000 | Loss: 0.00002713
Iteration 29/1000 | Loss: 0.00002713
Iteration 30/1000 | Loss: 0.00002713
Iteration 31/1000 | Loss: 0.00002713
Iteration 32/1000 | Loss: 0.00002713
Iteration 33/1000 | Loss: 0.00002713
Iteration 34/1000 | Loss: 0.00002713
Iteration 35/1000 | Loss: 0.00002711
Iteration 36/1000 | Loss: 0.00002711
Iteration 37/1000 | Loss: 0.00002710
Iteration 38/1000 | Loss: 0.00002710
Iteration 39/1000 | Loss: 0.00002710
Iteration 40/1000 | Loss: 0.00002709
Iteration 41/1000 | Loss: 0.00002709
Iteration 42/1000 | Loss: 0.00002709
Iteration 43/1000 | Loss: 0.00002709
Iteration 44/1000 | Loss: 0.00002709
Iteration 45/1000 | Loss: 0.00002709
Iteration 46/1000 | Loss: 0.00002709
Iteration 47/1000 | Loss: 0.00002709
Iteration 48/1000 | Loss: 0.00002708
Iteration 49/1000 | Loss: 0.00002708
Iteration 50/1000 | Loss: 0.00002708
Iteration 51/1000 | Loss: 0.00002708
Iteration 52/1000 | Loss: 0.00002708
Iteration 53/1000 | Loss: 0.00002708
Iteration 54/1000 | Loss: 0.00002707
Iteration 55/1000 | Loss: 0.00002707
Iteration 56/1000 | Loss: 0.00002707
Iteration 57/1000 | Loss: 0.00002707
Iteration 58/1000 | Loss: 0.00002707
Iteration 59/1000 | Loss: 0.00002707
Iteration 60/1000 | Loss: 0.00002707
Iteration 61/1000 | Loss: 0.00002707
Iteration 62/1000 | Loss: 0.00002707
Iteration 63/1000 | Loss: 0.00002707
Iteration 64/1000 | Loss: 0.00002707
Iteration 65/1000 | Loss: 0.00002707
Iteration 66/1000 | Loss: 0.00002707
Iteration 67/1000 | Loss: 0.00002707
Iteration 68/1000 | Loss: 0.00002707
Iteration 69/1000 | Loss: 0.00002707
Iteration 70/1000 | Loss: 0.00002707
Iteration 71/1000 | Loss: 0.00002707
Iteration 72/1000 | Loss: 0.00002707
Iteration 73/1000 | Loss: 0.00002707
Iteration 74/1000 | Loss: 0.00002707
Iteration 75/1000 | Loss: 0.00002707
Iteration 76/1000 | Loss: 0.00002707
Iteration 77/1000 | Loss: 0.00002707
Iteration 78/1000 | Loss: 0.00002707
Iteration 79/1000 | Loss: 0.00002707
Iteration 80/1000 | Loss: 0.00002707
Iteration 81/1000 | Loss: 0.00002707
Iteration 82/1000 | Loss: 0.00002707
Iteration 83/1000 | Loss: 0.00002707
Iteration 84/1000 | Loss: 0.00002707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.7071482691098936e-05, 2.7071482691098936e-05, 2.7071482691098936e-05, 2.7071482691098936e-05, 2.7071482691098936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7071482691098936e-05

Optimization complete. Final v2v error: 4.342027187347412 mm

Highest mean error: 4.638545989990234 mm for frame 98

Lowest mean error: 4.161508083343506 mm for frame 47

Saving results

Total time: 29.942561149597168
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508231
Iteration 2/25 | Loss: 0.00121220
Iteration 3/25 | Loss: 0.00112480
Iteration 4/25 | Loss: 0.00111212
Iteration 5/25 | Loss: 0.00110840
Iteration 6/25 | Loss: 0.00110728
Iteration 7/25 | Loss: 0.00110723
Iteration 8/25 | Loss: 0.00110723
Iteration 9/25 | Loss: 0.00110723
Iteration 10/25 | Loss: 0.00110723
Iteration 11/25 | Loss: 0.00110723
Iteration 12/25 | Loss: 0.00110723
Iteration 13/25 | Loss: 0.00110723
Iteration 14/25 | Loss: 0.00110723
Iteration 15/25 | Loss: 0.00110723
Iteration 16/25 | Loss: 0.00110723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011072332272306085, 0.0011072332272306085, 0.0011072332272306085, 0.0011072332272306085, 0.0011072332272306085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011072332272306085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.02847910
Iteration 2/25 | Loss: 0.00088537
Iteration 3/25 | Loss: 0.00088536
Iteration 4/25 | Loss: 0.00088536
Iteration 5/25 | Loss: 0.00088536
Iteration 6/25 | Loss: 0.00088536
Iteration 7/25 | Loss: 0.00088536
Iteration 8/25 | Loss: 0.00088536
Iteration 9/25 | Loss: 0.00088536
Iteration 10/25 | Loss: 0.00088536
Iteration 11/25 | Loss: 0.00088536
Iteration 12/25 | Loss: 0.00088536
Iteration 13/25 | Loss: 0.00088536
Iteration 14/25 | Loss: 0.00088536
Iteration 15/25 | Loss: 0.00088536
Iteration 16/25 | Loss: 0.00088536
Iteration 17/25 | Loss: 0.00088536
Iteration 18/25 | Loss: 0.00088536
Iteration 19/25 | Loss: 0.00088536
Iteration 20/25 | Loss: 0.00088536
Iteration 21/25 | Loss: 0.00088536
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008853621548041701, 0.0008853621548041701, 0.0008853621548041701, 0.0008853621548041701, 0.0008853621548041701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008853621548041701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088536
Iteration 2/1000 | Loss: 0.00003728
Iteration 3/1000 | Loss: 0.00002523
Iteration 4/1000 | Loss: 0.00002050
Iteration 5/1000 | Loss: 0.00001940
Iteration 6/1000 | Loss: 0.00001863
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001787
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001758
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001690
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001687
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001656
Iteration 24/1000 | Loss: 0.00001656
Iteration 25/1000 | Loss: 0.00001655
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001651
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001647
Iteration 33/1000 | Loss: 0.00001647
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001644
Iteration 41/1000 | Loss: 0.00001644
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001641
Iteration 57/1000 | Loss: 0.00001641
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001640
Iteration 60/1000 | Loss: 0.00001640
Iteration 61/1000 | Loss: 0.00001640
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001639
Iteration 64/1000 | Loss: 0.00001639
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001638
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001637
Iteration 74/1000 | Loss: 0.00001637
Iteration 75/1000 | Loss: 0.00001637
Iteration 76/1000 | Loss: 0.00001636
Iteration 77/1000 | Loss: 0.00001636
Iteration 78/1000 | Loss: 0.00001636
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001634
Iteration 90/1000 | Loss: 0.00001634
Iteration 91/1000 | Loss: 0.00001633
Iteration 92/1000 | Loss: 0.00001633
Iteration 93/1000 | Loss: 0.00001633
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001632
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001630
Iteration 102/1000 | Loss: 0.00001630
Iteration 103/1000 | Loss: 0.00001630
Iteration 104/1000 | Loss: 0.00001629
Iteration 105/1000 | Loss: 0.00001629
Iteration 106/1000 | Loss: 0.00001629
Iteration 107/1000 | Loss: 0.00001629
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Iteration 110/1000 | Loss: 0.00001628
Iteration 111/1000 | Loss: 0.00001628
Iteration 112/1000 | Loss: 0.00001628
Iteration 113/1000 | Loss: 0.00001627
Iteration 114/1000 | Loss: 0.00001627
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001626
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001626
Iteration 121/1000 | Loss: 0.00001625
Iteration 122/1000 | Loss: 0.00001625
Iteration 123/1000 | Loss: 0.00001625
Iteration 124/1000 | Loss: 0.00001625
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Iteration 127/1000 | Loss: 0.00001624
Iteration 128/1000 | Loss: 0.00001624
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001619
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001619
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001618
Iteration 174/1000 | Loss: 0.00001618
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001618
Iteration 177/1000 | Loss: 0.00001618
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001617
Iteration 180/1000 | Loss: 0.00001617
Iteration 181/1000 | Loss: 0.00001617
Iteration 182/1000 | Loss: 0.00001617
Iteration 183/1000 | Loss: 0.00001617
Iteration 184/1000 | Loss: 0.00001617
Iteration 185/1000 | Loss: 0.00001617
Iteration 186/1000 | Loss: 0.00001617
Iteration 187/1000 | Loss: 0.00001617
Iteration 188/1000 | Loss: 0.00001617
Iteration 189/1000 | Loss: 0.00001617
Iteration 190/1000 | Loss: 0.00001617
Iteration 191/1000 | Loss: 0.00001617
Iteration 192/1000 | Loss: 0.00001617
Iteration 193/1000 | Loss: 0.00001617
Iteration 194/1000 | Loss: 0.00001617
Iteration 195/1000 | Loss: 0.00001617
Iteration 196/1000 | Loss: 0.00001617
Iteration 197/1000 | Loss: 0.00001617
Iteration 198/1000 | Loss: 0.00001617
Iteration 199/1000 | Loss: 0.00001617
Iteration 200/1000 | Loss: 0.00001617
Iteration 201/1000 | Loss: 0.00001617
Iteration 202/1000 | Loss: 0.00001617
Iteration 203/1000 | Loss: 0.00001617
Iteration 204/1000 | Loss: 0.00001617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.616761255718302e-05, 1.616761255718302e-05, 1.616761255718302e-05, 1.616761255718302e-05, 1.616761255718302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.616761255718302e-05

Optimization complete. Final v2v error: 3.376729965209961 mm

Highest mean error: 4.489354133605957 mm for frame 68

Lowest mean error: 2.8658154010772705 mm for frame 17

Saving results

Total time: 43.58445954322815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385539
Iteration 2/25 | Loss: 0.00118339
Iteration 3/25 | Loss: 0.00111614
Iteration 4/25 | Loss: 0.00111004
Iteration 5/25 | Loss: 0.00110795
Iteration 6/25 | Loss: 0.00110727
Iteration 7/25 | Loss: 0.00110701
Iteration 8/25 | Loss: 0.00110701
Iteration 9/25 | Loss: 0.00110701
Iteration 10/25 | Loss: 0.00110701
Iteration 11/25 | Loss: 0.00110701
Iteration 12/25 | Loss: 0.00110701
Iteration 13/25 | Loss: 0.00110701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011070050531998277, 0.0011070050531998277, 0.0011070050531998277, 0.0011070050531998277, 0.0011070050531998277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011070050531998277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45935714
Iteration 2/25 | Loss: 0.00097903
Iteration 3/25 | Loss: 0.00097903
Iteration 4/25 | Loss: 0.00097902
Iteration 5/25 | Loss: 0.00097902
Iteration 6/25 | Loss: 0.00097902
Iteration 7/25 | Loss: 0.00097902
Iteration 8/25 | Loss: 0.00097902
Iteration 9/25 | Loss: 0.00097902
Iteration 10/25 | Loss: 0.00097902
Iteration 11/25 | Loss: 0.00097902
Iteration 12/25 | Loss: 0.00097902
Iteration 13/25 | Loss: 0.00097902
Iteration 14/25 | Loss: 0.00097902
Iteration 15/25 | Loss: 0.00097902
Iteration 16/25 | Loss: 0.00097902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009790229378268123, 0.0009790229378268123, 0.0009790229378268123, 0.0009790229378268123, 0.0009790229378268123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009790229378268123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097902
Iteration 2/1000 | Loss: 0.00003422
Iteration 3/1000 | Loss: 0.00001926
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001301
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001143
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001117
Iteration 17/1000 | Loss: 0.00001115
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001114
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001113
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001109
Iteration 26/1000 | Loss: 0.00001106
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001098
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001092
Iteration 58/1000 | Loss: 0.00001092
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001091
Iteration 61/1000 | Loss: 0.00001091
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001091
Iteration 64/1000 | Loss: 0.00001091
Iteration 65/1000 | Loss: 0.00001091
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001090
Iteration 70/1000 | Loss: 0.00001090
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001088
Iteration 81/1000 | Loss: 0.00001087
Iteration 82/1000 | Loss: 0.00001087
Iteration 83/1000 | Loss: 0.00001087
Iteration 84/1000 | Loss: 0.00001087
Iteration 85/1000 | Loss: 0.00001087
Iteration 86/1000 | Loss: 0.00001087
Iteration 87/1000 | Loss: 0.00001087
Iteration 88/1000 | Loss: 0.00001087
Iteration 89/1000 | Loss: 0.00001086
Iteration 90/1000 | Loss: 0.00001086
Iteration 91/1000 | Loss: 0.00001086
Iteration 92/1000 | Loss: 0.00001086
Iteration 93/1000 | Loss: 0.00001086
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001086
Iteration 96/1000 | Loss: 0.00001086
Iteration 97/1000 | Loss: 0.00001086
Iteration 98/1000 | Loss: 0.00001086
Iteration 99/1000 | Loss: 0.00001085
Iteration 100/1000 | Loss: 0.00001085
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001085
Iteration 103/1000 | Loss: 0.00001085
Iteration 104/1000 | Loss: 0.00001085
Iteration 105/1000 | Loss: 0.00001085
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001085
Iteration 113/1000 | Loss: 0.00001085
Iteration 114/1000 | Loss: 0.00001085
Iteration 115/1000 | Loss: 0.00001085
Iteration 116/1000 | Loss: 0.00001085
Iteration 117/1000 | Loss: 0.00001085
Iteration 118/1000 | Loss: 0.00001085
Iteration 119/1000 | Loss: 0.00001085
Iteration 120/1000 | Loss: 0.00001085
Iteration 121/1000 | Loss: 0.00001085
Iteration 122/1000 | Loss: 0.00001085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.0848209058167413e-05, 1.0848209058167413e-05, 1.0848209058167413e-05, 1.0848209058167413e-05, 1.0848209058167413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0848209058167413e-05

Optimization complete. Final v2v error: 2.7527623176574707 mm

Highest mean error: 4.144865989685059 mm for frame 55

Lowest mean error: 2.345541477203369 mm for frame 33

Saving results

Total time: 35.96560025215149
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405765
Iteration 2/25 | Loss: 0.00117666
Iteration 3/25 | Loss: 0.00108867
Iteration 4/25 | Loss: 0.00108016
Iteration 5/25 | Loss: 0.00107776
Iteration 6/25 | Loss: 0.00107752
Iteration 7/25 | Loss: 0.00107752
Iteration 8/25 | Loss: 0.00107752
Iteration 9/25 | Loss: 0.00107752
Iteration 10/25 | Loss: 0.00107752
Iteration 11/25 | Loss: 0.00107752
Iteration 12/25 | Loss: 0.00107752
Iteration 13/25 | Loss: 0.00107752
Iteration 14/25 | Loss: 0.00107752
Iteration 15/25 | Loss: 0.00107752
Iteration 16/25 | Loss: 0.00107752
Iteration 17/25 | Loss: 0.00107752
Iteration 18/25 | Loss: 0.00107752
Iteration 19/25 | Loss: 0.00107752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010775158880278468, 0.0010775158880278468, 0.0010775158880278468, 0.0010775158880278468, 0.0010775158880278468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010775158880278468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51544702
Iteration 2/25 | Loss: 0.00079344
Iteration 3/25 | Loss: 0.00079344
Iteration 4/25 | Loss: 0.00079344
Iteration 5/25 | Loss: 0.00079344
Iteration 6/25 | Loss: 0.00079344
Iteration 7/25 | Loss: 0.00079344
Iteration 8/25 | Loss: 0.00079344
Iteration 9/25 | Loss: 0.00079344
Iteration 10/25 | Loss: 0.00079344
Iteration 11/25 | Loss: 0.00079344
Iteration 12/25 | Loss: 0.00079344
Iteration 13/25 | Loss: 0.00079344
Iteration 14/25 | Loss: 0.00079344
Iteration 15/25 | Loss: 0.00079344
Iteration 16/25 | Loss: 0.00079344
Iteration 17/25 | Loss: 0.00079344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007934368913993239, 0.0007934368913993239, 0.0007934368913993239, 0.0007934368913993239, 0.0007934368913993239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007934368913993239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079344
Iteration 2/1000 | Loss: 0.00002751
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001201
Iteration 7/1000 | Loss: 0.00001141
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001062
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001035
Iteration 15/1000 | Loss: 0.00001034
Iteration 16/1000 | Loss: 0.00001034
Iteration 17/1000 | Loss: 0.00001033
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001026
Iteration 20/1000 | Loss: 0.00001025
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001019
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001013
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001010
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001008
Iteration 50/1000 | Loss: 0.00001007
Iteration 51/1000 | Loss: 0.00001007
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001004
Iteration 66/1000 | Loss: 0.00001004
Iteration 67/1000 | Loss: 0.00001004
Iteration 68/1000 | Loss: 0.00001004
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001002
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00001000
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00000999
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000998
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000997
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000995
Iteration 112/1000 | Loss: 0.00000995
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000994
Iteration 117/1000 | Loss: 0.00000994
Iteration 118/1000 | Loss: 0.00000994
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000992
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000991
Iteration 126/1000 | Loss: 0.00000991
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000990
Iteration 129/1000 | Loss: 0.00000990
Iteration 130/1000 | Loss: 0.00000990
Iteration 131/1000 | Loss: 0.00000990
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000989
Iteration 134/1000 | Loss: 0.00000989
Iteration 135/1000 | Loss: 0.00000989
Iteration 136/1000 | Loss: 0.00000989
Iteration 137/1000 | Loss: 0.00000989
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000988
Iteration 140/1000 | Loss: 0.00000988
Iteration 141/1000 | Loss: 0.00000988
Iteration 142/1000 | Loss: 0.00000988
Iteration 143/1000 | Loss: 0.00000988
Iteration 144/1000 | Loss: 0.00000988
Iteration 145/1000 | Loss: 0.00000988
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000987
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Iteration 154/1000 | Loss: 0.00000987
Iteration 155/1000 | Loss: 0.00000987
Iteration 156/1000 | Loss: 0.00000987
Iteration 157/1000 | Loss: 0.00000987
Iteration 158/1000 | Loss: 0.00000987
Iteration 159/1000 | Loss: 0.00000987
Iteration 160/1000 | Loss: 0.00000986
Iteration 161/1000 | Loss: 0.00000986
Iteration 162/1000 | Loss: 0.00000986
Iteration 163/1000 | Loss: 0.00000986
Iteration 164/1000 | Loss: 0.00000986
Iteration 165/1000 | Loss: 0.00000986
Iteration 166/1000 | Loss: 0.00000986
Iteration 167/1000 | Loss: 0.00000986
Iteration 168/1000 | Loss: 0.00000986
Iteration 169/1000 | Loss: 0.00000986
Iteration 170/1000 | Loss: 0.00000986
Iteration 171/1000 | Loss: 0.00000986
Iteration 172/1000 | Loss: 0.00000986
Iteration 173/1000 | Loss: 0.00000985
Iteration 174/1000 | Loss: 0.00000985
Iteration 175/1000 | Loss: 0.00000985
Iteration 176/1000 | Loss: 0.00000985
Iteration 177/1000 | Loss: 0.00000985
Iteration 178/1000 | Loss: 0.00000985
Iteration 179/1000 | Loss: 0.00000985
Iteration 180/1000 | Loss: 0.00000985
Iteration 181/1000 | Loss: 0.00000985
Iteration 182/1000 | Loss: 0.00000985
Iteration 183/1000 | Loss: 0.00000985
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000984
Iteration 187/1000 | Loss: 0.00000984
Iteration 188/1000 | Loss: 0.00000984
Iteration 189/1000 | Loss: 0.00000984
Iteration 190/1000 | Loss: 0.00000984
Iteration 191/1000 | Loss: 0.00000984
Iteration 192/1000 | Loss: 0.00000984
Iteration 193/1000 | Loss: 0.00000984
Iteration 194/1000 | Loss: 0.00000984
Iteration 195/1000 | Loss: 0.00000984
Iteration 196/1000 | Loss: 0.00000983
Iteration 197/1000 | Loss: 0.00000983
Iteration 198/1000 | Loss: 0.00000983
Iteration 199/1000 | Loss: 0.00000983
Iteration 200/1000 | Loss: 0.00000983
Iteration 201/1000 | Loss: 0.00000983
Iteration 202/1000 | Loss: 0.00000983
Iteration 203/1000 | Loss: 0.00000983
Iteration 204/1000 | Loss: 0.00000983
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000982
Iteration 210/1000 | Loss: 0.00000982
Iteration 211/1000 | Loss: 0.00000982
Iteration 212/1000 | Loss: 0.00000982
Iteration 213/1000 | Loss: 0.00000982
Iteration 214/1000 | Loss: 0.00000982
Iteration 215/1000 | Loss: 0.00000982
Iteration 216/1000 | Loss: 0.00000982
Iteration 217/1000 | Loss: 0.00000982
Iteration 218/1000 | Loss: 0.00000982
Iteration 219/1000 | Loss: 0.00000982
Iteration 220/1000 | Loss: 0.00000982
Iteration 221/1000 | Loss: 0.00000982
Iteration 222/1000 | Loss: 0.00000982
Iteration 223/1000 | Loss: 0.00000981
Iteration 224/1000 | Loss: 0.00000981
Iteration 225/1000 | Loss: 0.00000981
Iteration 226/1000 | Loss: 0.00000981
Iteration 227/1000 | Loss: 0.00000980
Iteration 228/1000 | Loss: 0.00000980
Iteration 229/1000 | Loss: 0.00000980
Iteration 230/1000 | Loss: 0.00000980
Iteration 231/1000 | Loss: 0.00000980
Iteration 232/1000 | Loss: 0.00000980
Iteration 233/1000 | Loss: 0.00000980
Iteration 234/1000 | Loss: 0.00000980
Iteration 235/1000 | Loss: 0.00000980
Iteration 236/1000 | Loss: 0.00000979
Iteration 237/1000 | Loss: 0.00000979
Iteration 238/1000 | Loss: 0.00000979
Iteration 239/1000 | Loss: 0.00000979
Iteration 240/1000 | Loss: 0.00000979
Iteration 241/1000 | Loss: 0.00000979
Iteration 242/1000 | Loss: 0.00000979
Iteration 243/1000 | Loss: 0.00000978
Iteration 244/1000 | Loss: 0.00000978
Iteration 245/1000 | Loss: 0.00000978
Iteration 246/1000 | Loss: 0.00000978
Iteration 247/1000 | Loss: 0.00000978
Iteration 248/1000 | Loss: 0.00000978
Iteration 249/1000 | Loss: 0.00000978
Iteration 250/1000 | Loss: 0.00000978
Iteration 251/1000 | Loss: 0.00000978
Iteration 252/1000 | Loss: 0.00000978
Iteration 253/1000 | Loss: 0.00000978
Iteration 254/1000 | Loss: 0.00000978
Iteration 255/1000 | Loss: 0.00000978
Iteration 256/1000 | Loss: 0.00000978
Iteration 257/1000 | Loss: 0.00000978
Iteration 258/1000 | Loss: 0.00000978
Iteration 259/1000 | Loss: 0.00000977
Iteration 260/1000 | Loss: 0.00000977
Iteration 261/1000 | Loss: 0.00000977
Iteration 262/1000 | Loss: 0.00000977
Iteration 263/1000 | Loss: 0.00000977
Iteration 264/1000 | Loss: 0.00000977
Iteration 265/1000 | Loss: 0.00000977
Iteration 266/1000 | Loss: 0.00000977
Iteration 267/1000 | Loss: 0.00000977
Iteration 268/1000 | Loss: 0.00000977
Iteration 269/1000 | Loss: 0.00000977
Iteration 270/1000 | Loss: 0.00000976
Iteration 271/1000 | Loss: 0.00000976
Iteration 272/1000 | Loss: 0.00000976
Iteration 273/1000 | Loss: 0.00000976
Iteration 274/1000 | Loss: 0.00000976
Iteration 275/1000 | Loss: 0.00000976
Iteration 276/1000 | Loss: 0.00000976
Iteration 277/1000 | Loss: 0.00000976
Iteration 278/1000 | Loss: 0.00000976
Iteration 279/1000 | Loss: 0.00000976
Iteration 280/1000 | Loss: 0.00000976
Iteration 281/1000 | Loss: 0.00000976
Iteration 282/1000 | Loss: 0.00000976
Iteration 283/1000 | Loss: 0.00000975
Iteration 284/1000 | Loss: 0.00000975
Iteration 285/1000 | Loss: 0.00000975
Iteration 286/1000 | Loss: 0.00000975
Iteration 287/1000 | Loss: 0.00000975
Iteration 288/1000 | Loss: 0.00000975
Iteration 289/1000 | Loss: 0.00000975
Iteration 290/1000 | Loss: 0.00000975
Iteration 291/1000 | Loss: 0.00000975
Iteration 292/1000 | Loss: 0.00000975
Iteration 293/1000 | Loss: 0.00000975
Iteration 294/1000 | Loss: 0.00000975
Iteration 295/1000 | Loss: 0.00000975
Iteration 296/1000 | Loss: 0.00000975
Iteration 297/1000 | Loss: 0.00000975
Iteration 298/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [9.751834113558289e-06, 9.751834113558289e-06, 9.751834113558289e-06, 9.751834113558289e-06, 9.751834113558289e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.751834113558289e-06

Optimization complete. Final v2v error: 2.6180031299591064 mm

Highest mean error: 4.006362438201904 mm for frame 168

Lowest mean error: 2.365196704864502 mm for frame 146

Saving results

Total time: 50.92641353607178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872438
Iteration 2/25 | Loss: 0.00154372
Iteration 3/25 | Loss: 0.00128372
Iteration 4/25 | Loss: 0.00123334
Iteration 5/25 | Loss: 0.00123508
Iteration 6/25 | Loss: 0.00120630
Iteration 7/25 | Loss: 0.00120964
Iteration 8/25 | Loss: 0.00120029
Iteration 9/25 | Loss: 0.00119747
Iteration 10/25 | Loss: 0.00120241
Iteration 11/25 | Loss: 0.00119667
Iteration 12/25 | Loss: 0.00119109
Iteration 13/25 | Loss: 0.00118755
Iteration 14/25 | Loss: 0.00118707
Iteration 15/25 | Loss: 0.00118694
Iteration 16/25 | Loss: 0.00118694
Iteration 17/25 | Loss: 0.00118694
Iteration 18/25 | Loss: 0.00118694
Iteration 19/25 | Loss: 0.00118694
Iteration 20/25 | Loss: 0.00118693
Iteration 21/25 | Loss: 0.00118693
Iteration 22/25 | Loss: 0.00118693
Iteration 23/25 | Loss: 0.00118693
Iteration 24/25 | Loss: 0.00118693
Iteration 25/25 | Loss: 0.00118693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42693877
Iteration 2/25 | Loss: 0.00072274
Iteration 3/25 | Loss: 0.00072270
Iteration 4/25 | Loss: 0.00072270
Iteration 5/25 | Loss: 0.00072270
Iteration 6/25 | Loss: 0.00072270
Iteration 7/25 | Loss: 0.00072270
Iteration 8/25 | Loss: 0.00072270
Iteration 9/25 | Loss: 0.00072270
Iteration 10/25 | Loss: 0.00072270
Iteration 11/25 | Loss: 0.00072270
Iteration 12/25 | Loss: 0.00072270
Iteration 13/25 | Loss: 0.00072270
Iteration 14/25 | Loss: 0.00072270
Iteration 15/25 | Loss: 0.00072270
Iteration 16/25 | Loss: 0.00072270
Iteration 17/25 | Loss: 0.00072270
Iteration 18/25 | Loss: 0.00072270
Iteration 19/25 | Loss: 0.00072270
Iteration 20/25 | Loss: 0.00072270
Iteration 21/25 | Loss: 0.00072270
Iteration 22/25 | Loss: 0.00072270
Iteration 23/25 | Loss: 0.00072270
Iteration 24/25 | Loss: 0.00072270
Iteration 25/25 | Loss: 0.00072270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072270
Iteration 2/1000 | Loss: 0.00003824
Iteration 3/1000 | Loss: 0.00002361
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00002008
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001861
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001782
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001774
Iteration 16/1000 | Loss: 0.00001772
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001760
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001759
Iteration 24/1000 | Loss: 0.00001759
Iteration 25/1000 | Loss: 0.00001759
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001757
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001755
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001755
Iteration 42/1000 | Loss: 0.00001755
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001753
Iteration 48/1000 | Loss: 0.00001753
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001751
Iteration 55/1000 | Loss: 0.00001751
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001750
Iteration 61/1000 | Loss: 0.00001750
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.7495120118837804e-05, 1.7495120118837804e-05, 1.7495120118837804e-05, 1.7495120118837804e-05, 1.7495120118837804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7495120118837804e-05

Optimization complete. Final v2v error: 3.5677714347839355 mm

Highest mean error: 4.21462869644165 mm for frame 2

Lowest mean error: 3.1159090995788574 mm for frame 225

Saving results

Total time: 55.008469343185425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821857
Iteration 2/25 | Loss: 0.00137940
Iteration 3/25 | Loss: 0.00112688
Iteration 4/25 | Loss: 0.00110793
Iteration 5/25 | Loss: 0.00110532
Iteration 6/25 | Loss: 0.00110499
Iteration 7/25 | Loss: 0.00110499
Iteration 8/25 | Loss: 0.00110499
Iteration 9/25 | Loss: 0.00110499
Iteration 10/25 | Loss: 0.00110499
Iteration 11/25 | Loss: 0.00110499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001104988856241107, 0.001104988856241107, 0.001104988856241107, 0.001104988856241107, 0.001104988856241107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001104988856241107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35547900
Iteration 2/25 | Loss: 0.00069408
Iteration 3/25 | Loss: 0.00069406
Iteration 4/25 | Loss: 0.00069406
Iteration 5/25 | Loss: 0.00069405
Iteration 6/25 | Loss: 0.00069405
Iteration 7/25 | Loss: 0.00069405
Iteration 8/25 | Loss: 0.00069405
Iteration 9/25 | Loss: 0.00069405
Iteration 10/25 | Loss: 0.00069405
Iteration 11/25 | Loss: 0.00069405
Iteration 12/25 | Loss: 0.00069405
Iteration 13/25 | Loss: 0.00069405
Iteration 14/25 | Loss: 0.00069405
Iteration 15/25 | Loss: 0.00069405
Iteration 16/25 | Loss: 0.00069405
Iteration 17/25 | Loss: 0.00069405
Iteration 18/25 | Loss: 0.00069405
Iteration 19/25 | Loss: 0.00069405
Iteration 20/25 | Loss: 0.00069405
Iteration 21/25 | Loss: 0.00069405
Iteration 22/25 | Loss: 0.00069405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000694052956532687, 0.000694052956532687, 0.000694052956532687, 0.000694052956532687, 0.000694052956532687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000694052956532687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069405
Iteration 2/1000 | Loss: 0.00002578
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001272
Iteration 5/1000 | Loss: 0.00001170
Iteration 6/1000 | Loss: 0.00001088
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001010
Iteration 9/1000 | Loss: 0.00000986
Iteration 10/1000 | Loss: 0.00000973
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000934
Iteration 14/1000 | Loss: 0.00000932
Iteration 15/1000 | Loss: 0.00000932
Iteration 16/1000 | Loss: 0.00000932
Iteration 17/1000 | Loss: 0.00000931
Iteration 18/1000 | Loss: 0.00000930
Iteration 19/1000 | Loss: 0.00000929
Iteration 20/1000 | Loss: 0.00000927
Iteration 21/1000 | Loss: 0.00000926
Iteration 22/1000 | Loss: 0.00000925
Iteration 23/1000 | Loss: 0.00000924
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000912
Iteration 26/1000 | Loss: 0.00000911
Iteration 27/1000 | Loss: 0.00000910
Iteration 28/1000 | Loss: 0.00000909
Iteration 29/1000 | Loss: 0.00000896
Iteration 30/1000 | Loss: 0.00000895
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000890
Iteration 36/1000 | Loss: 0.00000888
Iteration 37/1000 | Loss: 0.00000888
Iteration 38/1000 | Loss: 0.00000888
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000887
Iteration 41/1000 | Loss: 0.00000887
Iteration 42/1000 | Loss: 0.00000886
Iteration 43/1000 | Loss: 0.00000886
Iteration 44/1000 | Loss: 0.00000885
Iteration 45/1000 | Loss: 0.00000884
Iteration 46/1000 | Loss: 0.00000884
Iteration 47/1000 | Loss: 0.00000883
Iteration 48/1000 | Loss: 0.00000883
Iteration 49/1000 | Loss: 0.00000883
Iteration 50/1000 | Loss: 0.00000882
Iteration 51/1000 | Loss: 0.00000882
Iteration 52/1000 | Loss: 0.00000882
Iteration 53/1000 | Loss: 0.00000881
Iteration 54/1000 | Loss: 0.00000881
Iteration 55/1000 | Loss: 0.00000881
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000880
Iteration 59/1000 | Loss: 0.00000880
Iteration 60/1000 | Loss: 0.00000879
Iteration 61/1000 | Loss: 0.00000879
Iteration 62/1000 | Loss: 0.00000879
Iteration 63/1000 | Loss: 0.00000879
Iteration 64/1000 | Loss: 0.00000879
Iteration 65/1000 | Loss: 0.00000878
Iteration 66/1000 | Loss: 0.00000878
Iteration 67/1000 | Loss: 0.00000878
Iteration 68/1000 | Loss: 0.00000878
Iteration 69/1000 | Loss: 0.00000877
Iteration 70/1000 | Loss: 0.00000877
Iteration 71/1000 | Loss: 0.00000877
Iteration 72/1000 | Loss: 0.00000876
Iteration 73/1000 | Loss: 0.00000876
Iteration 74/1000 | Loss: 0.00000876
Iteration 75/1000 | Loss: 0.00000876
Iteration 76/1000 | Loss: 0.00000876
Iteration 77/1000 | Loss: 0.00000875
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000874
Iteration 81/1000 | Loss: 0.00000874
Iteration 82/1000 | Loss: 0.00000874
Iteration 83/1000 | Loss: 0.00000872
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000872
Iteration 87/1000 | Loss: 0.00000872
Iteration 88/1000 | Loss: 0.00000872
Iteration 89/1000 | Loss: 0.00000872
Iteration 90/1000 | Loss: 0.00000872
Iteration 91/1000 | Loss: 0.00000872
Iteration 92/1000 | Loss: 0.00000871
Iteration 93/1000 | Loss: 0.00000871
Iteration 94/1000 | Loss: 0.00000871
Iteration 95/1000 | Loss: 0.00000870
Iteration 96/1000 | Loss: 0.00000870
Iteration 97/1000 | Loss: 0.00000870
Iteration 98/1000 | Loss: 0.00000870
Iteration 99/1000 | Loss: 0.00000870
Iteration 100/1000 | Loss: 0.00000870
Iteration 101/1000 | Loss: 0.00000870
Iteration 102/1000 | Loss: 0.00000870
Iteration 103/1000 | Loss: 0.00000870
Iteration 104/1000 | Loss: 0.00000869
Iteration 105/1000 | Loss: 0.00000869
Iteration 106/1000 | Loss: 0.00000869
Iteration 107/1000 | Loss: 0.00000869
Iteration 108/1000 | Loss: 0.00000869
Iteration 109/1000 | Loss: 0.00000869
Iteration 110/1000 | Loss: 0.00000869
Iteration 111/1000 | Loss: 0.00000869
Iteration 112/1000 | Loss: 0.00000869
Iteration 113/1000 | Loss: 0.00000869
Iteration 114/1000 | Loss: 0.00000868
Iteration 115/1000 | Loss: 0.00000868
Iteration 116/1000 | Loss: 0.00000868
Iteration 117/1000 | Loss: 0.00000868
Iteration 118/1000 | Loss: 0.00000868
Iteration 119/1000 | Loss: 0.00000868
Iteration 120/1000 | Loss: 0.00000868
Iteration 121/1000 | Loss: 0.00000868
Iteration 122/1000 | Loss: 0.00000868
Iteration 123/1000 | Loss: 0.00000868
Iteration 124/1000 | Loss: 0.00000868
Iteration 125/1000 | Loss: 0.00000868
Iteration 126/1000 | Loss: 0.00000868
Iteration 127/1000 | Loss: 0.00000868
Iteration 128/1000 | Loss: 0.00000868
Iteration 129/1000 | Loss: 0.00000868
Iteration 130/1000 | Loss: 0.00000868
Iteration 131/1000 | Loss: 0.00000868
Iteration 132/1000 | Loss: 0.00000868
Iteration 133/1000 | Loss: 0.00000868
Iteration 134/1000 | Loss: 0.00000868
Iteration 135/1000 | Loss: 0.00000868
Iteration 136/1000 | Loss: 0.00000868
Iteration 137/1000 | Loss: 0.00000868
Iteration 138/1000 | Loss: 0.00000868
Iteration 139/1000 | Loss: 0.00000868
Iteration 140/1000 | Loss: 0.00000868
Iteration 141/1000 | Loss: 0.00000868
Iteration 142/1000 | Loss: 0.00000868
Iteration 143/1000 | Loss: 0.00000868
Iteration 144/1000 | Loss: 0.00000868
Iteration 145/1000 | Loss: 0.00000868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [8.679638085595798e-06, 8.679638085595798e-06, 8.679638085595798e-06, 8.679638085595798e-06, 8.679638085595798e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.679638085595798e-06

Optimization complete. Final v2v error: 2.5499777793884277 mm

Highest mean error: 2.9655349254608154 mm for frame 166

Lowest mean error: 2.3662188053131104 mm for frame 81

Saving results

Total time: 36.48528289794922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400775
Iteration 2/25 | Loss: 0.00117316
Iteration 3/25 | Loss: 0.00107755
Iteration 4/25 | Loss: 0.00107037
Iteration 5/25 | Loss: 0.00106880
Iteration 6/25 | Loss: 0.00106880
Iteration 7/25 | Loss: 0.00106880
Iteration 8/25 | Loss: 0.00106880
Iteration 9/25 | Loss: 0.00106880
Iteration 10/25 | Loss: 0.00106880
Iteration 11/25 | Loss: 0.00106880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010688015026971698, 0.0010688015026971698, 0.0010688015026971698, 0.0010688015026971698, 0.0010688015026971698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010688015026971698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34015095
Iteration 2/25 | Loss: 0.00070641
Iteration 3/25 | Loss: 0.00070641
Iteration 4/25 | Loss: 0.00070641
Iteration 5/25 | Loss: 0.00070641
Iteration 6/25 | Loss: 0.00070641
Iteration 7/25 | Loss: 0.00070641
Iteration 8/25 | Loss: 0.00070641
Iteration 9/25 | Loss: 0.00070641
Iteration 10/25 | Loss: 0.00070641
Iteration 11/25 | Loss: 0.00070641
Iteration 12/25 | Loss: 0.00070641
Iteration 13/25 | Loss: 0.00070641
Iteration 14/25 | Loss: 0.00070641
Iteration 15/25 | Loss: 0.00070641
Iteration 16/25 | Loss: 0.00070641
Iteration 17/25 | Loss: 0.00070641
Iteration 18/25 | Loss: 0.00070641
Iteration 19/25 | Loss: 0.00070641
Iteration 20/25 | Loss: 0.00070641
Iteration 21/25 | Loss: 0.00070641
Iteration 22/25 | Loss: 0.00070641
Iteration 23/25 | Loss: 0.00070641
Iteration 24/25 | Loss: 0.00070641
Iteration 25/25 | Loss: 0.00070641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070641
Iteration 2/1000 | Loss: 0.00002274
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001177
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001150
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001143
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001143
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001142
Iteration 158/1000 | Loss: 0.00001142
Iteration 159/1000 | Loss: 0.00001142
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001141
Iteration 163/1000 | Loss: 0.00001141
Iteration 164/1000 | Loss: 0.00001141
Iteration 165/1000 | Loss: 0.00001141
Iteration 166/1000 | Loss: 0.00001141
Iteration 167/1000 | Loss: 0.00001141
Iteration 168/1000 | Loss: 0.00001141
Iteration 169/1000 | Loss: 0.00001141
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001140
Iteration 173/1000 | Loss: 0.00001140
Iteration 174/1000 | Loss: 0.00001140
Iteration 175/1000 | Loss: 0.00001140
Iteration 176/1000 | Loss: 0.00001140
Iteration 177/1000 | Loss: 0.00001140
Iteration 178/1000 | Loss: 0.00001140
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001138
Iteration 186/1000 | Loss: 0.00001138
Iteration 187/1000 | Loss: 0.00001138
Iteration 188/1000 | Loss: 0.00001138
Iteration 189/1000 | Loss: 0.00001138
Iteration 190/1000 | Loss: 0.00001138
Iteration 191/1000 | Loss: 0.00001138
Iteration 192/1000 | Loss: 0.00001138
Iteration 193/1000 | Loss: 0.00001138
Iteration 194/1000 | Loss: 0.00001138
Iteration 195/1000 | Loss: 0.00001138
Iteration 196/1000 | Loss: 0.00001138
Iteration 197/1000 | Loss: 0.00001138
Iteration 198/1000 | Loss: 0.00001138
Iteration 199/1000 | Loss: 0.00001138
Iteration 200/1000 | Loss: 0.00001138
Iteration 201/1000 | Loss: 0.00001138
Iteration 202/1000 | Loss: 0.00001138
Iteration 203/1000 | Loss: 0.00001138
Iteration 204/1000 | Loss: 0.00001138
Iteration 205/1000 | Loss: 0.00001138
Iteration 206/1000 | Loss: 0.00001138
Iteration 207/1000 | Loss: 0.00001138
Iteration 208/1000 | Loss: 0.00001138
Iteration 209/1000 | Loss: 0.00001138
Iteration 210/1000 | Loss: 0.00001138
Iteration 211/1000 | Loss: 0.00001138
Iteration 212/1000 | Loss: 0.00001138
Iteration 213/1000 | Loss: 0.00001138
Iteration 214/1000 | Loss: 0.00001138
Iteration 215/1000 | Loss: 0.00001138
Iteration 216/1000 | Loss: 0.00001138
Iteration 217/1000 | Loss: 0.00001138
Iteration 218/1000 | Loss: 0.00001138
Iteration 219/1000 | Loss: 0.00001138
Iteration 220/1000 | Loss: 0.00001138
Iteration 221/1000 | Loss: 0.00001138
Iteration 222/1000 | Loss: 0.00001138
Iteration 223/1000 | Loss: 0.00001138
Iteration 224/1000 | Loss: 0.00001138
Iteration 225/1000 | Loss: 0.00001138
Iteration 226/1000 | Loss: 0.00001138
Iteration 227/1000 | Loss: 0.00001138
Iteration 228/1000 | Loss: 0.00001138
Iteration 229/1000 | Loss: 0.00001138
Iteration 230/1000 | Loss: 0.00001138
Iteration 231/1000 | Loss: 0.00001138
Iteration 232/1000 | Loss: 0.00001138
Iteration 233/1000 | Loss: 0.00001138
Iteration 234/1000 | Loss: 0.00001138
Iteration 235/1000 | Loss: 0.00001138
Iteration 236/1000 | Loss: 0.00001138
Iteration 237/1000 | Loss: 0.00001138
Iteration 238/1000 | Loss: 0.00001138
Iteration 239/1000 | Loss: 0.00001138
Iteration 240/1000 | Loss: 0.00001138
Iteration 241/1000 | Loss: 0.00001138
Iteration 242/1000 | Loss: 0.00001138
Iteration 243/1000 | Loss: 0.00001138
Iteration 244/1000 | Loss: 0.00001138
Iteration 245/1000 | Loss: 0.00001138
Iteration 246/1000 | Loss: 0.00001138
Iteration 247/1000 | Loss: 0.00001138
Iteration 248/1000 | Loss: 0.00001138
Iteration 249/1000 | Loss: 0.00001138
Iteration 250/1000 | Loss: 0.00001138
Iteration 251/1000 | Loss: 0.00001138
Iteration 252/1000 | Loss: 0.00001138
Iteration 253/1000 | Loss: 0.00001138
Iteration 254/1000 | Loss: 0.00001138
Iteration 255/1000 | Loss: 0.00001138
Iteration 256/1000 | Loss: 0.00001138
Iteration 257/1000 | Loss: 0.00001138
Iteration 258/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.138009156420594e-05, 1.138009156420594e-05, 1.138009156420594e-05, 1.138009156420594e-05, 1.138009156420594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.138009156420594e-05

Optimization complete. Final v2v error: 2.853132724761963 mm

Highest mean error: 3.4505348205566406 mm for frame 98

Lowest mean error: 2.4958202838897705 mm for frame 19

Saving results

Total time: 42.98467946052551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125579
Iteration 2/25 | Loss: 0.01125579
Iteration 3/25 | Loss: 0.01125579
Iteration 4/25 | Loss: 0.01125579
Iteration 5/25 | Loss: 0.01125578
Iteration 6/25 | Loss: 0.01125578
Iteration 7/25 | Loss: 0.01125578
Iteration 8/25 | Loss: 0.01125578
Iteration 9/25 | Loss: 0.01125578
Iteration 10/25 | Loss: 0.01125578
Iteration 11/25 | Loss: 0.01125578
Iteration 12/25 | Loss: 0.01125578
Iteration 13/25 | Loss: 0.01125578
Iteration 14/25 | Loss: 0.01125578
Iteration 15/25 | Loss: 0.01125578
Iteration 16/25 | Loss: 0.01125578
Iteration 17/25 | Loss: 0.01125578
Iteration 18/25 | Loss: 0.01125578
Iteration 19/25 | Loss: 0.01125578
Iteration 20/25 | Loss: 0.01125578
Iteration 21/25 | Loss: 0.01125577
Iteration 22/25 | Loss: 0.01125577
Iteration 23/25 | Loss: 0.01125577
Iteration 24/25 | Loss: 0.01125577
Iteration 25/25 | Loss: 0.01125577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.55719662
Iteration 2/25 | Loss: 0.18275073
Iteration 3/25 | Loss: 0.18173513
Iteration 4/25 | Loss: 0.18159118
Iteration 5/25 | Loss: 0.18159117
Iteration 6/25 | Loss: 0.18159117
Iteration 7/25 | Loss: 0.18159115
Iteration 8/25 | Loss: 0.18159115
Iteration 9/25 | Loss: 0.18159115
Iteration 10/25 | Loss: 0.18159115
Iteration 11/25 | Loss: 0.18159115
Iteration 12/25 | Loss: 0.18159115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.18159115314483643, 0.18159115314483643, 0.18159115314483643, 0.18159115314483643, 0.18159115314483643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18159115314483643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18159115
Iteration 2/1000 | Loss: 0.00280317
Iteration 3/1000 | Loss: 0.00079563
Iteration 4/1000 | Loss: 0.00022235
Iteration 5/1000 | Loss: 0.00009559
Iteration 6/1000 | Loss: 0.00005919
Iteration 7/1000 | Loss: 0.00003976
Iteration 8/1000 | Loss: 0.00004813
Iteration 9/1000 | Loss: 0.00004904
Iteration 10/1000 | Loss: 0.00003018
Iteration 11/1000 | Loss: 0.00002817
Iteration 12/1000 | Loss: 0.00002520
Iteration 13/1000 | Loss: 0.00004540
Iteration 14/1000 | Loss: 0.00004318
Iteration 15/1000 | Loss: 0.00004327
Iteration 16/1000 | Loss: 0.00003107
Iteration 17/1000 | Loss: 0.00002758
Iteration 18/1000 | Loss: 0.00002346
Iteration 19/1000 | Loss: 0.00002311
Iteration 20/1000 | Loss: 0.00002372
Iteration 21/1000 | Loss: 0.00003901
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00002403
Iteration 24/1000 | Loss: 0.00003009
Iteration 25/1000 | Loss: 0.00005056
Iteration 26/1000 | Loss: 0.00001762
Iteration 27/1000 | Loss: 0.00002878
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00002539
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00004139
Iteration 32/1000 | Loss: 0.00002891
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00002146
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00001984
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00002458
Iteration 40/1000 | Loss: 0.00002144
Iteration 41/1000 | Loss: 0.00001602
Iteration 42/1000 | Loss: 0.00001602
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001602
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001600
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001684
Iteration 66/1000 | Loss: 0.00001684
Iteration 67/1000 | Loss: 0.00001592
Iteration 68/1000 | Loss: 0.00001585
Iteration 69/1000 | Loss: 0.00001584
Iteration 70/1000 | Loss: 0.00001584
Iteration 71/1000 | Loss: 0.00002336
Iteration 72/1000 | Loss: 0.00002229
Iteration 73/1000 | Loss: 0.00002261
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001567
Iteration 87/1000 | Loss: 0.00001567
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001567
Iteration 91/1000 | Loss: 0.00001567
Iteration 92/1000 | Loss: 0.00001567
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001566
Iteration 95/1000 | Loss: 0.00001566
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001566
Iteration 98/1000 | Loss: 0.00001566
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00001566
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001563
Iteration 107/1000 | Loss: 0.00001563
Iteration 108/1000 | Loss: 0.00001563
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001562
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001561
Iteration 117/1000 | Loss: 0.00001561
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001560
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001560
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00002831
Iteration 145/1000 | Loss: 0.00002844
Iteration 146/1000 | Loss: 0.00002532
Iteration 147/1000 | Loss: 0.00001765
Iteration 148/1000 | Loss: 0.00001786
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00002043
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001555
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001554
Iteration 156/1000 | Loss: 0.00001554
Iteration 157/1000 | Loss: 0.00001554
Iteration 158/1000 | Loss: 0.00001554
Iteration 159/1000 | Loss: 0.00001554
Iteration 160/1000 | Loss: 0.00001554
Iteration 161/1000 | Loss: 0.00001554
Iteration 162/1000 | Loss: 0.00001554
Iteration 163/1000 | Loss: 0.00001554
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001553
Iteration 166/1000 | Loss: 0.00001553
Iteration 167/1000 | Loss: 0.00001553
Iteration 168/1000 | Loss: 0.00001553
Iteration 169/1000 | Loss: 0.00001553
Iteration 170/1000 | Loss: 0.00001553
Iteration 171/1000 | Loss: 0.00001553
Iteration 172/1000 | Loss: 0.00001553
Iteration 173/1000 | Loss: 0.00001553
Iteration 174/1000 | Loss: 0.00001553
Iteration 175/1000 | Loss: 0.00001553
Iteration 176/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.5529831216554157e-05, 1.5529831216554157e-05, 1.5529831216554157e-05, 1.5529831216554157e-05, 1.5529831216554157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5529831216554157e-05

Optimization complete. Final v2v error: 3.288699150085449 mm

Highest mean error: 3.570066213607788 mm for frame 35

Lowest mean error: 3.0751547813415527 mm for frame 0

Saving results

Total time: 102.30708122253418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380105
Iteration 2/25 | Loss: 0.00116521
Iteration 3/25 | Loss: 0.00109696
Iteration 4/25 | Loss: 0.00108845
Iteration 5/25 | Loss: 0.00108571
Iteration 6/25 | Loss: 0.00108571
Iteration 7/25 | Loss: 0.00108571
Iteration 8/25 | Loss: 0.00108571
Iteration 9/25 | Loss: 0.00108571
Iteration 10/25 | Loss: 0.00108571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010857071029022336, 0.0010857071029022336, 0.0010857071029022336, 0.0010857071029022336, 0.0010857071029022336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010857071029022336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36713040
Iteration 2/25 | Loss: 0.00088919
Iteration 3/25 | Loss: 0.00088919
Iteration 4/25 | Loss: 0.00088919
Iteration 5/25 | Loss: 0.00088919
Iteration 6/25 | Loss: 0.00088919
Iteration 7/25 | Loss: 0.00088918
Iteration 8/25 | Loss: 0.00088918
Iteration 9/25 | Loss: 0.00088918
Iteration 10/25 | Loss: 0.00088918
Iteration 11/25 | Loss: 0.00088918
Iteration 12/25 | Loss: 0.00088918
Iteration 13/25 | Loss: 0.00088918
Iteration 14/25 | Loss: 0.00088918
Iteration 15/25 | Loss: 0.00088918
Iteration 16/25 | Loss: 0.00088918
Iteration 17/25 | Loss: 0.00088918
Iteration 18/25 | Loss: 0.00088918
Iteration 19/25 | Loss: 0.00088918
Iteration 20/25 | Loss: 0.00088918
Iteration 21/25 | Loss: 0.00088918
Iteration 22/25 | Loss: 0.00088918
Iteration 23/25 | Loss: 0.00088918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008891837787814438, 0.0008891837787814438, 0.0008891837787814438, 0.0008891837787814438, 0.0008891837787814438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008891837787814438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088918
Iteration 2/1000 | Loss: 0.00001745
Iteration 3/1000 | Loss: 0.00001316
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001174
Iteration 6/1000 | Loss: 0.00001135
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001087
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001068
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001060
Iteration 19/1000 | Loss: 0.00001059
Iteration 20/1000 | Loss: 0.00001057
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001055
Iteration 25/1000 | Loss: 0.00001055
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001051
Iteration 28/1000 | Loss: 0.00001051
Iteration 29/1000 | Loss: 0.00001051
Iteration 30/1000 | Loss: 0.00001050
Iteration 31/1000 | Loss: 0.00001050
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001049
Iteration 34/1000 | Loss: 0.00001049
Iteration 35/1000 | Loss: 0.00001047
Iteration 36/1000 | Loss: 0.00001047
Iteration 37/1000 | Loss: 0.00001046
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001045
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001045
Iteration 43/1000 | Loss: 0.00001045
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001044
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001043
Iteration 49/1000 | Loss: 0.00001043
Iteration 50/1000 | Loss: 0.00001042
Iteration 51/1000 | Loss: 0.00001042
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001039
Iteration 65/1000 | Loss: 0.00001039
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001037
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001035
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001034
Iteration 88/1000 | Loss: 0.00001034
Iteration 89/1000 | Loss: 0.00001034
Iteration 90/1000 | Loss: 0.00001034
Iteration 91/1000 | Loss: 0.00001034
Iteration 92/1000 | Loss: 0.00001034
Iteration 93/1000 | Loss: 0.00001032
Iteration 94/1000 | Loss: 0.00001032
Iteration 95/1000 | Loss: 0.00001032
Iteration 96/1000 | Loss: 0.00001032
Iteration 97/1000 | Loss: 0.00001032
Iteration 98/1000 | Loss: 0.00001032
Iteration 99/1000 | Loss: 0.00001032
Iteration 100/1000 | Loss: 0.00001032
Iteration 101/1000 | Loss: 0.00001031
Iteration 102/1000 | Loss: 0.00001031
Iteration 103/1000 | Loss: 0.00001031
Iteration 104/1000 | Loss: 0.00001031
Iteration 105/1000 | Loss: 0.00001031
Iteration 106/1000 | Loss: 0.00001031
Iteration 107/1000 | Loss: 0.00001031
Iteration 108/1000 | Loss: 0.00001030
Iteration 109/1000 | Loss: 0.00001030
Iteration 110/1000 | Loss: 0.00001030
Iteration 111/1000 | Loss: 0.00001030
Iteration 112/1000 | Loss: 0.00001030
Iteration 113/1000 | Loss: 0.00001029
Iteration 114/1000 | Loss: 0.00001029
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001028
Iteration 121/1000 | Loss: 0.00001028
Iteration 122/1000 | Loss: 0.00001028
Iteration 123/1000 | Loss: 0.00001028
Iteration 124/1000 | Loss: 0.00001028
Iteration 125/1000 | Loss: 0.00001028
Iteration 126/1000 | Loss: 0.00001028
Iteration 127/1000 | Loss: 0.00001027
Iteration 128/1000 | Loss: 0.00001027
Iteration 129/1000 | Loss: 0.00001027
Iteration 130/1000 | Loss: 0.00001027
Iteration 131/1000 | Loss: 0.00001026
Iteration 132/1000 | Loss: 0.00001026
Iteration 133/1000 | Loss: 0.00001026
Iteration 134/1000 | Loss: 0.00001026
Iteration 135/1000 | Loss: 0.00001026
Iteration 136/1000 | Loss: 0.00001026
Iteration 137/1000 | Loss: 0.00001026
Iteration 138/1000 | Loss: 0.00001026
Iteration 139/1000 | Loss: 0.00001026
Iteration 140/1000 | Loss: 0.00001025
Iteration 141/1000 | Loss: 0.00001025
Iteration 142/1000 | Loss: 0.00001025
Iteration 143/1000 | Loss: 0.00001025
Iteration 144/1000 | Loss: 0.00001025
Iteration 145/1000 | Loss: 0.00001024
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001024
Iteration 151/1000 | Loss: 0.00001024
Iteration 152/1000 | Loss: 0.00001023
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001023
Iteration 159/1000 | Loss: 0.00001023
Iteration 160/1000 | Loss: 0.00001023
Iteration 161/1000 | Loss: 0.00001023
Iteration 162/1000 | Loss: 0.00001023
Iteration 163/1000 | Loss: 0.00001023
Iteration 164/1000 | Loss: 0.00001023
Iteration 165/1000 | Loss: 0.00001023
Iteration 166/1000 | Loss: 0.00001023
Iteration 167/1000 | Loss: 0.00001023
Iteration 168/1000 | Loss: 0.00001023
Iteration 169/1000 | Loss: 0.00001022
Iteration 170/1000 | Loss: 0.00001022
Iteration 171/1000 | Loss: 0.00001022
Iteration 172/1000 | Loss: 0.00001022
Iteration 173/1000 | Loss: 0.00001022
Iteration 174/1000 | Loss: 0.00001022
Iteration 175/1000 | Loss: 0.00001022
Iteration 176/1000 | Loss: 0.00001022
Iteration 177/1000 | Loss: 0.00001021
Iteration 178/1000 | Loss: 0.00001021
Iteration 179/1000 | Loss: 0.00001021
Iteration 180/1000 | Loss: 0.00001021
Iteration 181/1000 | Loss: 0.00001021
Iteration 182/1000 | Loss: 0.00001021
Iteration 183/1000 | Loss: 0.00001021
Iteration 184/1000 | Loss: 0.00001020
Iteration 185/1000 | Loss: 0.00001020
Iteration 186/1000 | Loss: 0.00001020
Iteration 187/1000 | Loss: 0.00001020
Iteration 188/1000 | Loss: 0.00001020
Iteration 189/1000 | Loss: 0.00001020
Iteration 190/1000 | Loss: 0.00001020
Iteration 191/1000 | Loss: 0.00001020
Iteration 192/1000 | Loss: 0.00001020
Iteration 193/1000 | Loss: 0.00001020
Iteration 194/1000 | Loss: 0.00001019
Iteration 195/1000 | Loss: 0.00001019
Iteration 196/1000 | Loss: 0.00001019
Iteration 197/1000 | Loss: 0.00001019
Iteration 198/1000 | Loss: 0.00001019
Iteration 199/1000 | Loss: 0.00001019
Iteration 200/1000 | Loss: 0.00001019
Iteration 201/1000 | Loss: 0.00001019
Iteration 202/1000 | Loss: 0.00001019
Iteration 203/1000 | Loss: 0.00001019
Iteration 204/1000 | Loss: 0.00001019
Iteration 205/1000 | Loss: 0.00001019
Iteration 206/1000 | Loss: 0.00001019
Iteration 207/1000 | Loss: 0.00001018
Iteration 208/1000 | Loss: 0.00001018
Iteration 209/1000 | Loss: 0.00001018
Iteration 210/1000 | Loss: 0.00001018
Iteration 211/1000 | Loss: 0.00001018
Iteration 212/1000 | Loss: 0.00001018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.0183689482801128e-05, 1.0183689482801128e-05, 1.0183689482801128e-05, 1.0183689482801128e-05, 1.0183689482801128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0183689482801128e-05

Optimization complete. Final v2v error: 2.7105114459991455 mm

Highest mean error: 2.9497339725494385 mm for frame 147

Lowest mean error: 2.515686511993408 mm for frame 0

Saving results

Total time: 36.75486207008362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865234
Iteration 2/25 | Loss: 0.00206471
Iteration 3/25 | Loss: 0.00189317
Iteration 4/25 | Loss: 0.00146539
Iteration 5/25 | Loss: 0.00147062
Iteration 6/25 | Loss: 0.00145289
Iteration 7/25 | Loss: 0.00144329
Iteration 8/25 | Loss: 0.00140064
Iteration 9/25 | Loss: 0.00137898
Iteration 10/25 | Loss: 0.00137461
Iteration 11/25 | Loss: 0.00136856
Iteration 12/25 | Loss: 0.00136862
Iteration 13/25 | Loss: 0.00137731
Iteration 14/25 | Loss: 0.00138067
Iteration 15/25 | Loss: 0.00137749
Iteration 16/25 | Loss: 0.00136825
Iteration 17/25 | Loss: 0.00135847
Iteration 18/25 | Loss: 0.00135504
Iteration 19/25 | Loss: 0.00135072
Iteration 20/25 | Loss: 0.00134736
Iteration 21/25 | Loss: 0.00134558
Iteration 22/25 | Loss: 0.00135371
Iteration 23/25 | Loss: 0.00135496
Iteration 24/25 | Loss: 0.00135391
Iteration 25/25 | Loss: 0.00135282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77720022
Iteration 2/25 | Loss: 0.00368451
Iteration 3/25 | Loss: 0.00368451
Iteration 4/25 | Loss: 0.00368451
Iteration 5/25 | Loss: 0.00368451
Iteration 6/25 | Loss: 0.00342897
Iteration 7/25 | Loss: 0.00342896
Iteration 8/25 | Loss: 0.00342896
Iteration 9/25 | Loss: 0.00342896
Iteration 10/25 | Loss: 0.00342896
Iteration 11/25 | Loss: 0.00342896
Iteration 12/25 | Loss: 0.00342896
Iteration 13/25 | Loss: 0.00342896
Iteration 14/25 | Loss: 0.00342896
Iteration 15/25 | Loss: 0.00342896
Iteration 16/25 | Loss: 0.00342896
Iteration 17/25 | Loss: 0.00342896
Iteration 18/25 | Loss: 0.00342896
Iteration 19/25 | Loss: 0.00342896
Iteration 20/25 | Loss: 0.00342896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0034289557952433825, 0.0034289557952433825, 0.0034289557952433825, 0.0034289557952433825, 0.0034289557952433825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034289557952433825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342896
Iteration 2/1000 | Loss: 0.00081602
Iteration 3/1000 | Loss: 0.00120400
Iteration 4/1000 | Loss: 0.00183329
Iteration 5/1000 | Loss: 0.00262844
Iteration 6/1000 | Loss: 0.00076780
Iteration 7/1000 | Loss: 0.00113805
Iteration 8/1000 | Loss: 0.00058203
Iteration 9/1000 | Loss: 0.00078200
Iteration 10/1000 | Loss: 0.00046236
Iteration 11/1000 | Loss: 0.00058822
Iteration 12/1000 | Loss: 0.00007235
Iteration 13/1000 | Loss: 0.00005305
Iteration 14/1000 | Loss: 0.00004298
Iteration 15/1000 | Loss: 0.00003686
Iteration 16/1000 | Loss: 0.00003298
Iteration 17/1000 | Loss: 0.00068341
Iteration 18/1000 | Loss: 0.00003406
Iteration 19/1000 | Loss: 0.00002822
Iteration 20/1000 | Loss: 0.00016394
Iteration 21/1000 | Loss: 0.00020634
Iteration 22/1000 | Loss: 0.00015358
Iteration 23/1000 | Loss: 0.00018365
Iteration 24/1000 | Loss: 0.00014752
Iteration 25/1000 | Loss: 0.00002981
Iteration 26/1000 | Loss: 0.00003251
Iteration 27/1000 | Loss: 0.00027967
Iteration 28/1000 | Loss: 0.00014177
Iteration 29/1000 | Loss: 0.00013712
Iteration 30/1000 | Loss: 0.00002950
Iteration 31/1000 | Loss: 0.00002619
Iteration 32/1000 | Loss: 0.00022004
Iteration 33/1000 | Loss: 0.00002937
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00022360
Iteration 36/1000 | Loss: 0.00019052
Iteration 37/1000 | Loss: 0.00002148
Iteration 38/1000 | Loss: 0.00021182
Iteration 39/1000 | Loss: 0.00020392
Iteration 40/1000 | Loss: 0.00020671
Iteration 41/1000 | Loss: 0.00017898
Iteration 42/1000 | Loss: 0.00002814
Iteration 43/1000 | Loss: 0.00002428
Iteration 44/1000 | Loss: 0.00002303
Iteration 45/1000 | Loss: 0.00002199
Iteration 46/1000 | Loss: 0.00002124
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002308
Iteration 51/1000 | Loss: 0.00002012
Iteration 52/1000 | Loss: 0.00001974
Iteration 53/1000 | Loss: 0.00001973
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001910
Iteration 59/1000 | Loss: 0.00001901
Iteration 60/1000 | Loss: 0.00001892
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001840
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001828
Iteration 71/1000 | Loss: 0.00015374
Iteration 72/1000 | Loss: 0.00002168
Iteration 73/1000 | Loss: 0.00023998
Iteration 74/1000 | Loss: 0.00021861
Iteration 75/1000 | Loss: 0.00002084
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00019513
Iteration 78/1000 | Loss: 0.00019176
Iteration 79/1000 | Loss: 0.00016208
Iteration 80/1000 | Loss: 0.00012101
Iteration 81/1000 | Loss: 0.00015648
Iteration 82/1000 | Loss: 0.00011649
Iteration 83/1000 | Loss: 0.00013017
Iteration 84/1000 | Loss: 0.00002142
Iteration 85/1000 | Loss: 0.00002854
Iteration 86/1000 | Loss: 0.00001951
Iteration 87/1000 | Loss: 0.00001833
Iteration 88/1000 | Loss: 0.00001792
Iteration 89/1000 | Loss: 0.00001759
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001697
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001684
Iteration 103/1000 | Loss: 0.00001683
Iteration 104/1000 | Loss: 0.00001683
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001682
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001680
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001667
Iteration 117/1000 | Loss: 0.00001667
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001662
Iteration 126/1000 | Loss: 0.00001662
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001662
Iteration 130/1000 | Loss: 0.00001662
Iteration 131/1000 | Loss: 0.00001662
Iteration 132/1000 | Loss: 0.00001662
Iteration 133/1000 | Loss: 0.00001661
Iteration 134/1000 | Loss: 0.00001661
Iteration 135/1000 | Loss: 0.00001661
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001659
Iteration 143/1000 | Loss: 0.00001659
Iteration 144/1000 | Loss: 0.00001659
Iteration 145/1000 | Loss: 0.00001659
Iteration 146/1000 | Loss: 0.00001658
Iteration 147/1000 | Loss: 0.00001658
Iteration 148/1000 | Loss: 0.00001658
Iteration 149/1000 | Loss: 0.00001658
Iteration 150/1000 | Loss: 0.00001658
Iteration 151/1000 | Loss: 0.00001658
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001657
Iteration 154/1000 | Loss: 0.00001657
Iteration 155/1000 | Loss: 0.00001657
Iteration 156/1000 | Loss: 0.00001656
Iteration 157/1000 | Loss: 0.00001656
Iteration 158/1000 | Loss: 0.00001656
Iteration 159/1000 | Loss: 0.00001656
Iteration 160/1000 | Loss: 0.00001656
Iteration 161/1000 | Loss: 0.00001655
Iteration 162/1000 | Loss: 0.00001655
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001653
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001652
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001652
Iteration 171/1000 | Loss: 0.00001652
Iteration 172/1000 | Loss: 0.00001652
Iteration 173/1000 | Loss: 0.00001652
Iteration 174/1000 | Loss: 0.00001651
Iteration 175/1000 | Loss: 0.00001651
Iteration 176/1000 | Loss: 0.00001651
Iteration 177/1000 | Loss: 0.00001650
Iteration 178/1000 | Loss: 0.00001650
Iteration 179/1000 | Loss: 0.00001650
Iteration 180/1000 | Loss: 0.00001649
Iteration 181/1000 | Loss: 0.00001649
Iteration 182/1000 | Loss: 0.00001649
Iteration 183/1000 | Loss: 0.00001649
Iteration 184/1000 | Loss: 0.00001649
Iteration 185/1000 | Loss: 0.00001649
Iteration 186/1000 | Loss: 0.00001649
Iteration 187/1000 | Loss: 0.00001649
Iteration 188/1000 | Loss: 0.00001649
Iteration 189/1000 | Loss: 0.00001649
Iteration 190/1000 | Loss: 0.00001649
Iteration 191/1000 | Loss: 0.00001649
Iteration 192/1000 | Loss: 0.00001648
Iteration 193/1000 | Loss: 0.00001648
Iteration 194/1000 | Loss: 0.00001648
Iteration 195/1000 | Loss: 0.00001648
Iteration 196/1000 | Loss: 0.00001648
Iteration 197/1000 | Loss: 0.00001648
Iteration 198/1000 | Loss: 0.00001648
Iteration 199/1000 | Loss: 0.00001648
Iteration 200/1000 | Loss: 0.00001648
Iteration 201/1000 | Loss: 0.00001648
Iteration 202/1000 | Loss: 0.00001648
Iteration 203/1000 | Loss: 0.00001648
Iteration 204/1000 | Loss: 0.00001647
Iteration 205/1000 | Loss: 0.00001647
Iteration 206/1000 | Loss: 0.00001647
Iteration 207/1000 | Loss: 0.00001647
Iteration 208/1000 | Loss: 0.00001647
Iteration 209/1000 | Loss: 0.00001647
Iteration 210/1000 | Loss: 0.00001647
Iteration 211/1000 | Loss: 0.00001647
Iteration 212/1000 | Loss: 0.00001647
Iteration 213/1000 | Loss: 0.00001647
Iteration 214/1000 | Loss: 0.00001647
Iteration 215/1000 | Loss: 0.00001647
Iteration 216/1000 | Loss: 0.00001647
Iteration 217/1000 | Loss: 0.00001647
Iteration 218/1000 | Loss: 0.00001647
Iteration 219/1000 | Loss: 0.00001647
Iteration 220/1000 | Loss: 0.00001647
Iteration 221/1000 | Loss: 0.00001647
Iteration 222/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.6470879927510396e-05, 1.6470879927510396e-05, 1.6470879927510396e-05, 1.6470879927510396e-05, 1.6470879927510396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6470879927510396e-05

Optimization complete. Final v2v error: 3.0701985359191895 mm

Highest mean error: 12.776350021362305 mm for frame 102

Lowest mean error: 2.369870901107788 mm for frame 115

Saving results

Total time: 182.94017601013184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054151
Iteration 2/25 | Loss: 0.00282865
Iteration 3/25 | Loss: 0.00185215
Iteration 4/25 | Loss: 0.00166831
Iteration 5/25 | Loss: 0.00166440
Iteration 6/25 | Loss: 0.00159093
Iteration 7/25 | Loss: 0.00156592
Iteration 8/25 | Loss: 0.00155402
Iteration 9/25 | Loss: 0.00154268
Iteration 10/25 | Loss: 0.00153485
Iteration 11/25 | Loss: 0.00153176
Iteration 12/25 | Loss: 0.00153008
Iteration 13/25 | Loss: 0.00153146
Iteration 14/25 | Loss: 0.00152946
Iteration 15/25 | Loss: 0.00152511
Iteration 16/25 | Loss: 0.00153208
Iteration 17/25 | Loss: 0.00153249
Iteration 18/25 | Loss: 0.00152553
Iteration 19/25 | Loss: 0.00152313
Iteration 20/25 | Loss: 0.00152481
Iteration 21/25 | Loss: 0.00152092
Iteration 22/25 | Loss: 0.00151946
Iteration 23/25 | Loss: 0.00151814
Iteration 24/25 | Loss: 0.00151790
Iteration 25/25 | Loss: 0.00151778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16145897
Iteration 2/25 | Loss: 0.00416018
Iteration 3/25 | Loss: 0.00416018
Iteration 4/25 | Loss: 0.00416017
Iteration 5/25 | Loss: 0.00416017
Iteration 6/25 | Loss: 0.00416017
Iteration 7/25 | Loss: 0.00416017
Iteration 8/25 | Loss: 0.00416017
Iteration 9/25 | Loss: 0.00416017
Iteration 10/25 | Loss: 0.00416017
Iteration 11/25 | Loss: 0.00416017
Iteration 12/25 | Loss: 0.00416017
Iteration 13/25 | Loss: 0.00416017
Iteration 14/25 | Loss: 0.00416017
Iteration 15/25 | Loss: 0.00416017
Iteration 16/25 | Loss: 0.00416017
Iteration 17/25 | Loss: 0.00416017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004160171374678612, 0.004160171374678612, 0.004160171374678612, 0.004160171374678612, 0.004160171374678612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004160171374678612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00416017
Iteration 2/1000 | Loss: 0.00275441
Iteration 3/1000 | Loss: 0.00273530
Iteration 4/1000 | Loss: 0.00178673
Iteration 5/1000 | Loss: 0.00290216
Iteration 6/1000 | Loss: 0.00031223
Iteration 7/1000 | Loss: 0.00247699
Iteration 8/1000 | Loss: 0.00068867
Iteration 9/1000 | Loss: 0.00022123
Iteration 10/1000 | Loss: 0.00018692
Iteration 11/1000 | Loss: 0.00026711
Iteration 12/1000 | Loss: 0.00024898
Iteration 13/1000 | Loss: 0.00013923
Iteration 14/1000 | Loss: 0.00012616
Iteration 15/1000 | Loss: 0.00011716
Iteration 16/1000 | Loss: 0.00011140
Iteration 17/1000 | Loss: 0.00031080
Iteration 18/1000 | Loss: 0.00010317
Iteration 19/1000 | Loss: 0.00009935
Iteration 20/1000 | Loss: 0.00025965
Iteration 21/1000 | Loss: 0.00083621
Iteration 22/1000 | Loss: 0.00327732
Iteration 23/1000 | Loss: 0.00077067
Iteration 24/1000 | Loss: 0.00066922
Iteration 25/1000 | Loss: 0.00013561
Iteration 26/1000 | Loss: 0.00010592
Iteration 27/1000 | Loss: 0.00008590
Iteration 28/1000 | Loss: 0.00037951
Iteration 29/1000 | Loss: 0.00006000
Iteration 30/1000 | Loss: 0.00011507
Iteration 31/1000 | Loss: 0.00013131
Iteration 32/1000 | Loss: 0.00004946
Iteration 33/1000 | Loss: 0.00004706
Iteration 34/1000 | Loss: 0.00004494
Iteration 35/1000 | Loss: 0.00004357
Iteration 36/1000 | Loss: 0.00004240
Iteration 37/1000 | Loss: 0.00004148
Iteration 38/1000 | Loss: 0.00004076
Iteration 39/1000 | Loss: 0.00021185
Iteration 40/1000 | Loss: 0.00003999
Iteration 41/1000 | Loss: 0.00003945
Iteration 42/1000 | Loss: 0.00003914
Iteration 43/1000 | Loss: 0.00003887
Iteration 44/1000 | Loss: 0.00003864
Iteration 45/1000 | Loss: 0.00003860
Iteration 46/1000 | Loss: 0.00003842
Iteration 47/1000 | Loss: 0.00003829
Iteration 48/1000 | Loss: 0.00003816
Iteration 49/1000 | Loss: 0.00003815
Iteration 50/1000 | Loss: 0.00003802
Iteration 51/1000 | Loss: 0.00020479
Iteration 52/1000 | Loss: 0.00010286
Iteration 53/1000 | Loss: 0.00003893
Iteration 54/1000 | Loss: 0.00003744
Iteration 55/1000 | Loss: 0.00003660
Iteration 56/1000 | Loss: 0.00003607
Iteration 57/1000 | Loss: 0.00003573
Iteration 58/1000 | Loss: 0.00003548
Iteration 59/1000 | Loss: 0.00003534
Iteration 60/1000 | Loss: 0.00003534
Iteration 61/1000 | Loss: 0.00003530
Iteration 62/1000 | Loss: 0.00003530
Iteration 63/1000 | Loss: 0.00003529
Iteration 64/1000 | Loss: 0.00003528
Iteration 65/1000 | Loss: 0.00003522
Iteration 66/1000 | Loss: 0.00003520
Iteration 67/1000 | Loss: 0.00003520
Iteration 68/1000 | Loss: 0.00003520
Iteration 69/1000 | Loss: 0.00003519
Iteration 70/1000 | Loss: 0.00003519
Iteration 71/1000 | Loss: 0.00003519
Iteration 72/1000 | Loss: 0.00003519
Iteration 73/1000 | Loss: 0.00003519
Iteration 74/1000 | Loss: 0.00003519
Iteration 75/1000 | Loss: 0.00003519
Iteration 76/1000 | Loss: 0.00003519
Iteration 77/1000 | Loss: 0.00003519
Iteration 78/1000 | Loss: 0.00003519
Iteration 79/1000 | Loss: 0.00003518
Iteration 80/1000 | Loss: 0.00003518
Iteration 81/1000 | Loss: 0.00003518
Iteration 82/1000 | Loss: 0.00003518
Iteration 83/1000 | Loss: 0.00003518
Iteration 84/1000 | Loss: 0.00003518
Iteration 85/1000 | Loss: 0.00003517
Iteration 86/1000 | Loss: 0.00003517
Iteration 87/1000 | Loss: 0.00003517
Iteration 88/1000 | Loss: 0.00003517
Iteration 89/1000 | Loss: 0.00003517
Iteration 90/1000 | Loss: 0.00003517
Iteration 91/1000 | Loss: 0.00003517
Iteration 92/1000 | Loss: 0.00003517
Iteration 93/1000 | Loss: 0.00003517
Iteration 94/1000 | Loss: 0.00003517
Iteration 95/1000 | Loss: 0.00003516
Iteration 96/1000 | Loss: 0.00003516
Iteration 97/1000 | Loss: 0.00003516
Iteration 98/1000 | Loss: 0.00003516
Iteration 99/1000 | Loss: 0.00003516
Iteration 100/1000 | Loss: 0.00003516
Iteration 101/1000 | Loss: 0.00003516
Iteration 102/1000 | Loss: 0.00003515
Iteration 103/1000 | Loss: 0.00003515
Iteration 104/1000 | Loss: 0.00003515
Iteration 105/1000 | Loss: 0.00003515
Iteration 106/1000 | Loss: 0.00003515
Iteration 107/1000 | Loss: 0.00003515
Iteration 108/1000 | Loss: 0.00003514
Iteration 109/1000 | Loss: 0.00003514
Iteration 110/1000 | Loss: 0.00003514
Iteration 111/1000 | Loss: 0.00003514
Iteration 112/1000 | Loss: 0.00003514
Iteration 113/1000 | Loss: 0.00003514
Iteration 114/1000 | Loss: 0.00003514
Iteration 115/1000 | Loss: 0.00003514
Iteration 116/1000 | Loss: 0.00003514
Iteration 117/1000 | Loss: 0.00003514
Iteration 118/1000 | Loss: 0.00003514
Iteration 119/1000 | Loss: 0.00003514
Iteration 120/1000 | Loss: 0.00003513
Iteration 121/1000 | Loss: 0.00003513
Iteration 122/1000 | Loss: 0.00003513
Iteration 123/1000 | Loss: 0.00003513
Iteration 124/1000 | Loss: 0.00003513
Iteration 125/1000 | Loss: 0.00003513
Iteration 126/1000 | Loss: 0.00003513
Iteration 127/1000 | Loss: 0.00003513
Iteration 128/1000 | Loss: 0.00003513
Iteration 129/1000 | Loss: 0.00003513
Iteration 130/1000 | Loss: 0.00003513
Iteration 131/1000 | Loss: 0.00003513
Iteration 132/1000 | Loss: 0.00003513
Iteration 133/1000 | Loss: 0.00003513
Iteration 134/1000 | Loss: 0.00003513
Iteration 135/1000 | Loss: 0.00003513
Iteration 136/1000 | Loss: 0.00003513
Iteration 137/1000 | Loss: 0.00003513
Iteration 138/1000 | Loss: 0.00003513
Iteration 139/1000 | Loss: 0.00003513
Iteration 140/1000 | Loss: 0.00003513
Iteration 141/1000 | Loss: 0.00003513
Iteration 142/1000 | Loss: 0.00003513
Iteration 143/1000 | Loss: 0.00003513
Iteration 144/1000 | Loss: 0.00003513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.512969124130905e-05, 3.512969124130905e-05, 3.512969124130905e-05, 3.512969124130905e-05, 3.512969124130905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.512969124130905e-05

Optimization complete. Final v2v error: 3.8541107177734375 mm

Highest mean error: 13.107604026794434 mm for frame 23

Lowest mean error: 2.759842872619629 mm for frame 139

Saving results

Total time: 126.30104732513428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528625
Iteration 2/25 | Loss: 0.00146887
Iteration 3/25 | Loss: 0.00117676
Iteration 4/25 | Loss: 0.00114551
Iteration 5/25 | Loss: 0.00113727
Iteration 6/25 | Loss: 0.00113091
Iteration 7/25 | Loss: 0.00112890
Iteration 8/25 | Loss: 0.00113689
Iteration 9/25 | Loss: 0.00113144
Iteration 10/25 | Loss: 0.00112611
Iteration 11/25 | Loss: 0.00112532
Iteration 12/25 | Loss: 0.00112513
Iteration 13/25 | Loss: 0.00112513
Iteration 14/25 | Loss: 0.00112512
Iteration 15/25 | Loss: 0.00112512
Iteration 16/25 | Loss: 0.00112512
Iteration 17/25 | Loss: 0.00112512
Iteration 18/25 | Loss: 0.00112512
Iteration 19/25 | Loss: 0.00112512
Iteration 20/25 | Loss: 0.00112512
Iteration 21/25 | Loss: 0.00112511
Iteration 22/25 | Loss: 0.00112511
Iteration 23/25 | Loss: 0.00112511
Iteration 24/25 | Loss: 0.00112511
Iteration 25/25 | Loss: 0.00112511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62875175
Iteration 2/25 | Loss: 0.00093445
Iteration 3/25 | Loss: 0.00093444
Iteration 4/25 | Loss: 0.00093444
Iteration 5/25 | Loss: 0.00093444
Iteration 6/25 | Loss: 0.00093444
Iteration 7/25 | Loss: 0.00093444
Iteration 8/25 | Loss: 0.00093444
Iteration 9/25 | Loss: 0.00093444
Iteration 10/25 | Loss: 0.00093444
Iteration 11/25 | Loss: 0.00093444
Iteration 12/25 | Loss: 0.00093444
Iteration 13/25 | Loss: 0.00093444
Iteration 14/25 | Loss: 0.00093444
Iteration 15/25 | Loss: 0.00093444
Iteration 16/25 | Loss: 0.00093444
Iteration 17/25 | Loss: 0.00093444
Iteration 18/25 | Loss: 0.00093444
Iteration 19/25 | Loss: 0.00093444
Iteration 20/25 | Loss: 0.00093444
Iteration 21/25 | Loss: 0.00093444
Iteration 22/25 | Loss: 0.00093444
Iteration 23/25 | Loss: 0.00093444
Iteration 24/25 | Loss: 0.00093444
Iteration 25/25 | Loss: 0.00093444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093444
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00020139
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001741
Iteration 8/1000 | Loss: 0.00001614
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001505
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001401
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001372
Iteration 26/1000 | Loss: 0.00001372
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001355
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001353
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Iteration 97/1000 | Loss: 0.00001350
Iteration 98/1000 | Loss: 0.00001350
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001349
Iteration 104/1000 | Loss: 0.00001349
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001349
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001348
Iteration 121/1000 | Loss: 0.00001348
Iteration 122/1000 | Loss: 0.00001348
Iteration 123/1000 | Loss: 0.00001348
Iteration 124/1000 | Loss: 0.00001348
Iteration 125/1000 | Loss: 0.00001348
Iteration 126/1000 | Loss: 0.00001348
Iteration 127/1000 | Loss: 0.00001348
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001347
Iteration 130/1000 | Loss: 0.00001347
Iteration 131/1000 | Loss: 0.00001347
Iteration 132/1000 | Loss: 0.00001347
Iteration 133/1000 | Loss: 0.00001347
Iteration 134/1000 | Loss: 0.00001347
Iteration 135/1000 | Loss: 0.00001347
Iteration 136/1000 | Loss: 0.00001346
Iteration 137/1000 | Loss: 0.00001346
Iteration 138/1000 | Loss: 0.00001346
Iteration 139/1000 | Loss: 0.00001346
Iteration 140/1000 | Loss: 0.00001346
Iteration 141/1000 | Loss: 0.00001346
Iteration 142/1000 | Loss: 0.00001346
Iteration 143/1000 | Loss: 0.00001346
Iteration 144/1000 | Loss: 0.00001346
Iteration 145/1000 | Loss: 0.00001346
Iteration 146/1000 | Loss: 0.00001346
Iteration 147/1000 | Loss: 0.00001346
Iteration 148/1000 | Loss: 0.00001345
Iteration 149/1000 | Loss: 0.00001345
Iteration 150/1000 | Loss: 0.00001345
Iteration 151/1000 | Loss: 0.00001345
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001345
Iteration 155/1000 | Loss: 0.00001345
Iteration 156/1000 | Loss: 0.00001345
Iteration 157/1000 | Loss: 0.00001345
Iteration 158/1000 | Loss: 0.00001344
Iteration 159/1000 | Loss: 0.00001344
Iteration 160/1000 | Loss: 0.00001344
Iteration 161/1000 | Loss: 0.00001344
Iteration 162/1000 | Loss: 0.00001344
Iteration 163/1000 | Loss: 0.00001344
Iteration 164/1000 | Loss: 0.00001343
Iteration 165/1000 | Loss: 0.00001343
Iteration 166/1000 | Loss: 0.00001343
Iteration 167/1000 | Loss: 0.00001343
Iteration 168/1000 | Loss: 0.00001343
Iteration 169/1000 | Loss: 0.00001343
Iteration 170/1000 | Loss: 0.00001343
Iteration 171/1000 | Loss: 0.00001343
Iteration 172/1000 | Loss: 0.00001342
Iteration 173/1000 | Loss: 0.00001342
Iteration 174/1000 | Loss: 0.00001342
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001341
Iteration 178/1000 | Loss: 0.00001341
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001339
Iteration 186/1000 | Loss: 0.00001339
Iteration 187/1000 | Loss: 0.00001339
Iteration 188/1000 | Loss: 0.00001339
Iteration 189/1000 | Loss: 0.00001339
Iteration 190/1000 | Loss: 0.00001338
Iteration 191/1000 | Loss: 0.00001338
Iteration 192/1000 | Loss: 0.00001338
Iteration 193/1000 | Loss: 0.00001338
Iteration 194/1000 | Loss: 0.00001338
Iteration 195/1000 | Loss: 0.00001338
Iteration 196/1000 | Loss: 0.00001338
Iteration 197/1000 | Loss: 0.00001338
Iteration 198/1000 | Loss: 0.00001338
Iteration 199/1000 | Loss: 0.00001337
Iteration 200/1000 | Loss: 0.00001337
Iteration 201/1000 | Loss: 0.00001337
Iteration 202/1000 | Loss: 0.00001337
Iteration 203/1000 | Loss: 0.00001336
Iteration 204/1000 | Loss: 0.00001336
Iteration 205/1000 | Loss: 0.00001336
Iteration 206/1000 | Loss: 0.00001336
Iteration 207/1000 | Loss: 0.00001336
Iteration 208/1000 | Loss: 0.00001336
Iteration 209/1000 | Loss: 0.00001336
Iteration 210/1000 | Loss: 0.00001335
Iteration 211/1000 | Loss: 0.00001335
Iteration 212/1000 | Loss: 0.00001335
Iteration 213/1000 | Loss: 0.00001335
Iteration 214/1000 | Loss: 0.00001335
Iteration 215/1000 | Loss: 0.00001335
Iteration 216/1000 | Loss: 0.00001371
Iteration 217/1000 | Loss: 0.00001335
Iteration 218/1000 | Loss: 0.00001334
Iteration 219/1000 | Loss: 0.00001334
Iteration 220/1000 | Loss: 0.00001334
Iteration 221/1000 | Loss: 0.00001334
Iteration 222/1000 | Loss: 0.00001334
Iteration 223/1000 | Loss: 0.00001334
Iteration 224/1000 | Loss: 0.00001334
Iteration 225/1000 | Loss: 0.00001334
Iteration 226/1000 | Loss: 0.00001334
Iteration 227/1000 | Loss: 0.00001334
Iteration 228/1000 | Loss: 0.00001334
Iteration 229/1000 | Loss: 0.00001334
Iteration 230/1000 | Loss: 0.00001334
Iteration 231/1000 | Loss: 0.00001334
Iteration 232/1000 | Loss: 0.00001334
Iteration 233/1000 | Loss: 0.00001334
Iteration 234/1000 | Loss: 0.00001334
Iteration 235/1000 | Loss: 0.00001334
Iteration 236/1000 | Loss: 0.00001334
Iteration 237/1000 | Loss: 0.00001334
Iteration 238/1000 | Loss: 0.00001334
Iteration 239/1000 | Loss: 0.00001334
Iteration 240/1000 | Loss: 0.00001334
Iteration 241/1000 | Loss: 0.00001334
Iteration 242/1000 | Loss: 0.00001334
Iteration 243/1000 | Loss: 0.00001334
Iteration 244/1000 | Loss: 0.00001334
Iteration 245/1000 | Loss: 0.00001334
Iteration 246/1000 | Loss: 0.00001334
Iteration 247/1000 | Loss: 0.00001334
Iteration 248/1000 | Loss: 0.00001334
Iteration 249/1000 | Loss: 0.00001334
Iteration 250/1000 | Loss: 0.00001334
Iteration 251/1000 | Loss: 0.00001334
Iteration 252/1000 | Loss: 0.00001334
Iteration 253/1000 | Loss: 0.00001334
Iteration 254/1000 | Loss: 0.00001334
Iteration 255/1000 | Loss: 0.00001334
Iteration 256/1000 | Loss: 0.00001334
Iteration 257/1000 | Loss: 0.00001334
Iteration 258/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.3341511476028245e-05, 1.3341511476028245e-05, 1.3341511476028245e-05, 1.3341511476028245e-05, 1.3341511476028245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3341511476028245e-05

Optimization complete. Final v2v error: 3.034479856491089 mm

Highest mean error: 6.370601654052734 mm for frame 131

Lowest mean error: 2.564209461212158 mm for frame 23

Saving results

Total time: 67.28252911567688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809952
Iteration 2/25 | Loss: 0.00132051
Iteration 3/25 | Loss: 0.00118399
Iteration 4/25 | Loss: 0.00116633
Iteration 5/25 | Loss: 0.00116195
Iteration 6/25 | Loss: 0.00116084
Iteration 7/25 | Loss: 0.00116061
Iteration 8/25 | Loss: 0.00116061
Iteration 9/25 | Loss: 0.00116061
Iteration 10/25 | Loss: 0.00116061
Iteration 11/25 | Loss: 0.00116061
Iteration 12/25 | Loss: 0.00116061
Iteration 13/25 | Loss: 0.00116061
Iteration 14/25 | Loss: 0.00116061
Iteration 15/25 | Loss: 0.00116061
Iteration 16/25 | Loss: 0.00116061
Iteration 17/25 | Loss: 0.00116061
Iteration 18/25 | Loss: 0.00116061
Iteration 19/25 | Loss: 0.00116061
Iteration 20/25 | Loss: 0.00116061
Iteration 21/25 | Loss: 0.00116061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001160605694167316, 0.001160605694167316, 0.001160605694167316, 0.001160605694167316, 0.001160605694167316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001160605694167316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35107017
Iteration 2/25 | Loss: 0.00193831
Iteration 3/25 | Loss: 0.00193830
Iteration 4/25 | Loss: 0.00193830
Iteration 5/25 | Loss: 0.00193830
Iteration 6/25 | Loss: 0.00193830
Iteration 7/25 | Loss: 0.00193830
Iteration 8/25 | Loss: 0.00193830
Iteration 9/25 | Loss: 0.00193830
Iteration 10/25 | Loss: 0.00193830
Iteration 11/25 | Loss: 0.00193830
Iteration 12/25 | Loss: 0.00193830
Iteration 13/25 | Loss: 0.00193830
Iteration 14/25 | Loss: 0.00193830
Iteration 15/25 | Loss: 0.00193830
Iteration 16/25 | Loss: 0.00193830
Iteration 17/25 | Loss: 0.00193830
Iteration 18/25 | Loss: 0.00193830
Iteration 19/25 | Loss: 0.00193830
Iteration 20/25 | Loss: 0.00193830
Iteration 21/25 | Loss: 0.00193830
Iteration 22/25 | Loss: 0.00193830
Iteration 23/25 | Loss: 0.00193830
Iteration 24/25 | Loss: 0.00193830
Iteration 25/25 | Loss: 0.00193830

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193830
Iteration 2/1000 | Loss: 0.00015165
Iteration 3/1000 | Loss: 0.00031186
Iteration 4/1000 | Loss: 0.00014714
Iteration 5/1000 | Loss: 0.00023733
Iteration 6/1000 | Loss: 0.00011534
Iteration 7/1000 | Loss: 0.00013606
Iteration 8/1000 | Loss: 0.00015165
Iteration 9/1000 | Loss: 0.00016914
Iteration 10/1000 | Loss: 0.00016047
Iteration 11/1000 | Loss: 0.00016888
Iteration 12/1000 | Loss: 0.00013479
Iteration 13/1000 | Loss: 0.00012755
Iteration 14/1000 | Loss: 0.00019102
Iteration 15/1000 | Loss: 0.00012080
Iteration 16/1000 | Loss: 0.00017828
Iteration 17/1000 | Loss: 0.00015724
Iteration 18/1000 | Loss: 0.00020754
Iteration 19/1000 | Loss: 0.00015814
Iteration 20/1000 | Loss: 0.00013724
Iteration 21/1000 | Loss: 0.00016577
Iteration 22/1000 | Loss: 0.00020372
Iteration 23/1000 | Loss: 0.00015803
Iteration 24/1000 | Loss: 0.00020581
Iteration 25/1000 | Loss: 0.00012794
Iteration 26/1000 | Loss: 0.00022959
Iteration 27/1000 | Loss: 0.00008078
Iteration 28/1000 | Loss: 0.00005931
Iteration 29/1000 | Loss: 0.00021958
Iteration 30/1000 | Loss: 0.00006000
Iteration 31/1000 | Loss: 0.00005720
Iteration 32/1000 | Loss: 0.00005586
Iteration 33/1000 | Loss: 0.00005494
Iteration 34/1000 | Loss: 0.00023053
Iteration 35/1000 | Loss: 0.00019149
Iteration 36/1000 | Loss: 0.00005739
Iteration 37/1000 | Loss: 0.00005505
Iteration 38/1000 | Loss: 0.00023731
Iteration 39/1000 | Loss: 0.00014700
Iteration 40/1000 | Loss: 0.00005472
Iteration 41/1000 | Loss: 0.00023454
Iteration 42/1000 | Loss: 0.00006263
Iteration 43/1000 | Loss: 0.00005584
Iteration 44/1000 | Loss: 0.00005379
Iteration 45/1000 | Loss: 0.00005322
Iteration 46/1000 | Loss: 0.00005284
Iteration 47/1000 | Loss: 0.00005238
Iteration 48/1000 | Loss: 0.00005206
Iteration 49/1000 | Loss: 0.00005194
Iteration 50/1000 | Loss: 0.00005339
Iteration 51/1000 | Loss: 0.00005185
Iteration 52/1000 | Loss: 0.00005171
Iteration 53/1000 | Loss: 0.00005137
Iteration 54/1000 | Loss: 0.00005102
Iteration 55/1000 | Loss: 0.00005057
Iteration 56/1000 | Loss: 0.00004983
Iteration 57/1000 | Loss: 0.00004943
Iteration 58/1000 | Loss: 0.00004912
Iteration 59/1000 | Loss: 0.00004891
Iteration 60/1000 | Loss: 0.00004860
Iteration 61/1000 | Loss: 0.00004831
Iteration 62/1000 | Loss: 0.00004798
Iteration 63/1000 | Loss: 0.00004773
Iteration 64/1000 | Loss: 0.00004745
Iteration 65/1000 | Loss: 0.00004720
Iteration 66/1000 | Loss: 0.00004690
Iteration 67/1000 | Loss: 0.00004664
Iteration 68/1000 | Loss: 0.00004651
Iteration 69/1000 | Loss: 0.00004627
Iteration 70/1000 | Loss: 0.00004604
Iteration 71/1000 | Loss: 0.00004585
Iteration 72/1000 | Loss: 0.00004585
Iteration 73/1000 | Loss: 0.00004584
Iteration 74/1000 | Loss: 0.00004582
Iteration 75/1000 | Loss: 0.00004580
Iteration 76/1000 | Loss: 0.00004580
Iteration 77/1000 | Loss: 0.00004577
Iteration 78/1000 | Loss: 0.00004577
Iteration 79/1000 | Loss: 0.00004576
Iteration 80/1000 | Loss: 0.00004576
Iteration 81/1000 | Loss: 0.00004572
Iteration 82/1000 | Loss: 0.00004566
Iteration 83/1000 | Loss: 0.00004553
Iteration 84/1000 | Loss: 0.00004550
Iteration 85/1000 | Loss: 0.00004545
Iteration 86/1000 | Loss: 0.00004543
Iteration 87/1000 | Loss: 0.00004542
Iteration 88/1000 | Loss: 0.00004541
Iteration 89/1000 | Loss: 0.00004541
Iteration 90/1000 | Loss: 0.00004534
Iteration 91/1000 | Loss: 0.00004533
Iteration 92/1000 | Loss: 0.00004528
Iteration 93/1000 | Loss: 0.00004527
Iteration 94/1000 | Loss: 0.00004526
Iteration 95/1000 | Loss: 0.00004526
Iteration 96/1000 | Loss: 0.00004525
Iteration 97/1000 | Loss: 0.00004525
Iteration 98/1000 | Loss: 0.00004524
Iteration 99/1000 | Loss: 0.00004524
Iteration 100/1000 | Loss: 0.00004523
Iteration 101/1000 | Loss: 0.00004523
Iteration 102/1000 | Loss: 0.00004522
Iteration 103/1000 | Loss: 0.00004522
Iteration 104/1000 | Loss: 0.00004521
Iteration 105/1000 | Loss: 0.00004520
Iteration 106/1000 | Loss: 0.00004519
Iteration 107/1000 | Loss: 0.00004516
Iteration 108/1000 | Loss: 0.00004514
Iteration 109/1000 | Loss: 0.00004514
Iteration 110/1000 | Loss: 0.00004513
Iteration 111/1000 | Loss: 0.00004513
Iteration 112/1000 | Loss: 0.00004513
Iteration 113/1000 | Loss: 0.00004513
Iteration 114/1000 | Loss: 0.00004512
Iteration 115/1000 | Loss: 0.00004512
Iteration 116/1000 | Loss: 0.00004512
Iteration 117/1000 | Loss: 0.00004512
Iteration 118/1000 | Loss: 0.00004512
Iteration 119/1000 | Loss: 0.00004512
Iteration 120/1000 | Loss: 0.00004512
Iteration 121/1000 | Loss: 0.00004512
Iteration 122/1000 | Loss: 0.00004512
Iteration 123/1000 | Loss: 0.00004512
Iteration 124/1000 | Loss: 0.00004511
Iteration 125/1000 | Loss: 0.00004511
Iteration 126/1000 | Loss: 0.00004511
Iteration 127/1000 | Loss: 0.00004510
Iteration 128/1000 | Loss: 0.00004510
Iteration 129/1000 | Loss: 0.00004510
Iteration 130/1000 | Loss: 0.00004510
Iteration 131/1000 | Loss: 0.00004510
Iteration 132/1000 | Loss: 0.00004510
Iteration 133/1000 | Loss: 0.00004510
Iteration 134/1000 | Loss: 0.00004510
Iteration 135/1000 | Loss: 0.00004509
Iteration 136/1000 | Loss: 0.00004509
Iteration 137/1000 | Loss: 0.00004509
Iteration 138/1000 | Loss: 0.00004509
Iteration 139/1000 | Loss: 0.00004509
Iteration 140/1000 | Loss: 0.00004509
Iteration 141/1000 | Loss: 0.00004509
Iteration 142/1000 | Loss: 0.00004508
Iteration 143/1000 | Loss: 0.00004508
Iteration 144/1000 | Loss: 0.00004508
Iteration 145/1000 | Loss: 0.00004507
Iteration 146/1000 | Loss: 0.00004507
Iteration 147/1000 | Loss: 0.00004507
Iteration 148/1000 | Loss: 0.00004506
Iteration 149/1000 | Loss: 0.00004506
Iteration 150/1000 | Loss: 0.00004506
Iteration 151/1000 | Loss: 0.00004506
Iteration 152/1000 | Loss: 0.00004506
Iteration 153/1000 | Loss: 0.00004506
Iteration 154/1000 | Loss: 0.00004505
Iteration 155/1000 | Loss: 0.00004505
Iteration 156/1000 | Loss: 0.00004505
Iteration 157/1000 | Loss: 0.00004505
Iteration 158/1000 | Loss: 0.00004505
Iteration 159/1000 | Loss: 0.00004505
Iteration 160/1000 | Loss: 0.00004504
Iteration 161/1000 | Loss: 0.00004504
Iteration 162/1000 | Loss: 0.00004504
Iteration 163/1000 | Loss: 0.00004504
Iteration 164/1000 | Loss: 0.00004504
Iteration 165/1000 | Loss: 0.00004504
Iteration 166/1000 | Loss: 0.00004504
Iteration 167/1000 | Loss: 0.00004503
Iteration 168/1000 | Loss: 0.00004503
Iteration 169/1000 | Loss: 0.00004503
Iteration 170/1000 | Loss: 0.00004502
Iteration 171/1000 | Loss: 0.00004502
Iteration 172/1000 | Loss: 0.00004502
Iteration 173/1000 | Loss: 0.00004502
Iteration 174/1000 | Loss: 0.00004502
Iteration 175/1000 | Loss: 0.00004501
Iteration 176/1000 | Loss: 0.00004501
Iteration 177/1000 | Loss: 0.00004501
Iteration 178/1000 | Loss: 0.00004500
Iteration 179/1000 | Loss: 0.00004500
Iteration 180/1000 | Loss: 0.00004500
Iteration 181/1000 | Loss: 0.00004499
Iteration 182/1000 | Loss: 0.00004499
Iteration 183/1000 | Loss: 0.00004499
Iteration 184/1000 | Loss: 0.00004499
Iteration 185/1000 | Loss: 0.00004498
Iteration 186/1000 | Loss: 0.00004498
Iteration 187/1000 | Loss: 0.00004498
Iteration 188/1000 | Loss: 0.00004498
Iteration 189/1000 | Loss: 0.00004497
Iteration 190/1000 | Loss: 0.00004497
Iteration 191/1000 | Loss: 0.00004497
Iteration 192/1000 | Loss: 0.00004497
Iteration 193/1000 | Loss: 0.00004497
Iteration 194/1000 | Loss: 0.00004497
Iteration 195/1000 | Loss: 0.00004497
Iteration 196/1000 | Loss: 0.00004497
Iteration 197/1000 | Loss: 0.00004496
Iteration 198/1000 | Loss: 0.00004496
Iteration 199/1000 | Loss: 0.00004496
Iteration 200/1000 | Loss: 0.00004496
Iteration 201/1000 | Loss: 0.00004495
Iteration 202/1000 | Loss: 0.00004495
Iteration 203/1000 | Loss: 0.00004495
Iteration 204/1000 | Loss: 0.00004495
Iteration 205/1000 | Loss: 0.00004495
Iteration 206/1000 | Loss: 0.00004495
Iteration 207/1000 | Loss: 0.00004494
Iteration 208/1000 | Loss: 0.00004494
Iteration 209/1000 | Loss: 0.00004494
Iteration 210/1000 | Loss: 0.00004494
Iteration 211/1000 | Loss: 0.00004494
Iteration 212/1000 | Loss: 0.00004494
Iteration 213/1000 | Loss: 0.00004493
Iteration 214/1000 | Loss: 0.00004493
Iteration 215/1000 | Loss: 0.00004493
Iteration 216/1000 | Loss: 0.00004493
Iteration 217/1000 | Loss: 0.00004493
Iteration 218/1000 | Loss: 0.00004493
Iteration 219/1000 | Loss: 0.00004493
Iteration 220/1000 | Loss: 0.00004492
Iteration 221/1000 | Loss: 0.00004492
Iteration 222/1000 | Loss: 0.00004492
Iteration 223/1000 | Loss: 0.00004492
Iteration 224/1000 | Loss: 0.00004492
Iteration 225/1000 | Loss: 0.00004492
Iteration 226/1000 | Loss: 0.00004492
Iteration 227/1000 | Loss: 0.00004492
Iteration 228/1000 | Loss: 0.00004492
Iteration 229/1000 | Loss: 0.00004492
Iteration 230/1000 | Loss: 0.00004491
Iteration 231/1000 | Loss: 0.00004491
Iteration 232/1000 | Loss: 0.00004491
Iteration 233/1000 | Loss: 0.00004491
Iteration 234/1000 | Loss: 0.00004491
Iteration 235/1000 | Loss: 0.00004491
Iteration 236/1000 | Loss: 0.00004491
Iteration 237/1000 | Loss: 0.00004491
Iteration 238/1000 | Loss: 0.00004491
Iteration 239/1000 | Loss: 0.00004491
Iteration 240/1000 | Loss: 0.00004491
Iteration 241/1000 | Loss: 0.00004491
Iteration 242/1000 | Loss: 0.00004491
Iteration 243/1000 | Loss: 0.00004491
Iteration 244/1000 | Loss: 0.00004491
Iteration 245/1000 | Loss: 0.00004491
Iteration 246/1000 | Loss: 0.00004490
Iteration 247/1000 | Loss: 0.00004490
Iteration 248/1000 | Loss: 0.00004490
Iteration 249/1000 | Loss: 0.00004490
Iteration 250/1000 | Loss: 0.00004490
Iteration 251/1000 | Loss: 0.00004490
Iteration 252/1000 | Loss: 0.00004490
Iteration 253/1000 | Loss: 0.00004490
Iteration 254/1000 | Loss: 0.00004490
Iteration 255/1000 | Loss: 0.00004490
Iteration 256/1000 | Loss: 0.00004490
Iteration 257/1000 | Loss: 0.00004490
Iteration 258/1000 | Loss: 0.00004490
Iteration 259/1000 | Loss: 0.00004490
Iteration 260/1000 | Loss: 0.00004490
Iteration 261/1000 | Loss: 0.00004490
Iteration 262/1000 | Loss: 0.00004490
Iteration 263/1000 | Loss: 0.00004490
Iteration 264/1000 | Loss: 0.00004489
Iteration 265/1000 | Loss: 0.00004489
Iteration 266/1000 | Loss: 0.00004489
Iteration 267/1000 | Loss: 0.00004489
Iteration 268/1000 | Loss: 0.00004489
Iteration 269/1000 | Loss: 0.00004489
Iteration 270/1000 | Loss: 0.00004489
Iteration 271/1000 | Loss: 0.00004489
Iteration 272/1000 | Loss: 0.00004489
Iteration 273/1000 | Loss: 0.00004489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [4.489017010200769e-05, 4.489017010200769e-05, 4.489017010200769e-05, 4.489017010200769e-05, 4.489017010200769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.489017010200769e-05

Optimization complete. Final v2v error: 3.2826385498046875 mm

Highest mean error: 11.466079711914062 mm for frame 86

Lowest mean error: 2.2387051582336426 mm for frame 42

Saving results

Total time: 136.63176274299622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798135
Iteration 2/25 | Loss: 0.00115129
Iteration 3/25 | Loss: 0.00106550
Iteration 4/25 | Loss: 0.00105708
Iteration 5/25 | Loss: 0.00105477
Iteration 6/25 | Loss: 0.00105468
Iteration 7/25 | Loss: 0.00105468
Iteration 8/25 | Loss: 0.00105468
Iteration 9/25 | Loss: 0.00105468
Iteration 10/25 | Loss: 0.00105468
Iteration 11/25 | Loss: 0.00105468
Iteration 12/25 | Loss: 0.00105468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001054679392836988, 0.001054679392836988, 0.001054679392836988, 0.001054679392836988, 0.001054679392836988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001054679392836988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36025894
Iteration 2/25 | Loss: 0.00082408
Iteration 3/25 | Loss: 0.00082408
Iteration 4/25 | Loss: 0.00082408
Iteration 5/25 | Loss: 0.00082408
Iteration 6/25 | Loss: 0.00082408
Iteration 7/25 | Loss: 0.00082408
Iteration 8/25 | Loss: 0.00082408
Iteration 9/25 | Loss: 0.00082408
Iteration 10/25 | Loss: 0.00082408
Iteration 11/25 | Loss: 0.00082407
Iteration 12/25 | Loss: 0.00082407
Iteration 13/25 | Loss: 0.00082407
Iteration 14/25 | Loss: 0.00082407
Iteration 15/25 | Loss: 0.00082407
Iteration 16/25 | Loss: 0.00082407
Iteration 17/25 | Loss: 0.00082407
Iteration 18/25 | Loss: 0.00082407
Iteration 19/25 | Loss: 0.00082407
Iteration 20/25 | Loss: 0.00082407
Iteration 21/25 | Loss: 0.00082407
Iteration 22/25 | Loss: 0.00082407
Iteration 23/25 | Loss: 0.00082407
Iteration 24/25 | Loss: 0.00082407
Iteration 25/25 | Loss: 0.00082407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082407
Iteration 2/1000 | Loss: 0.00002044
Iteration 3/1000 | Loss: 0.00001388
Iteration 4/1000 | Loss: 0.00001183
Iteration 5/1000 | Loss: 0.00001086
Iteration 6/1000 | Loss: 0.00001033
Iteration 7/1000 | Loss: 0.00000988
Iteration 8/1000 | Loss: 0.00000954
Iteration 9/1000 | Loss: 0.00000944
Iteration 10/1000 | Loss: 0.00000924
Iteration 11/1000 | Loss: 0.00000907
Iteration 12/1000 | Loss: 0.00000903
Iteration 13/1000 | Loss: 0.00000902
Iteration 14/1000 | Loss: 0.00000902
Iteration 15/1000 | Loss: 0.00000902
Iteration 16/1000 | Loss: 0.00000902
Iteration 17/1000 | Loss: 0.00000901
Iteration 18/1000 | Loss: 0.00000901
Iteration 19/1000 | Loss: 0.00000901
Iteration 20/1000 | Loss: 0.00000900
Iteration 21/1000 | Loss: 0.00000900
Iteration 22/1000 | Loss: 0.00000899
Iteration 23/1000 | Loss: 0.00000895
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000891
Iteration 26/1000 | Loss: 0.00000890
Iteration 27/1000 | Loss: 0.00000890
Iteration 28/1000 | Loss: 0.00000890
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000889
Iteration 31/1000 | Loss: 0.00000889
Iteration 32/1000 | Loss: 0.00000888
Iteration 33/1000 | Loss: 0.00000888
Iteration 34/1000 | Loss: 0.00000887
Iteration 35/1000 | Loss: 0.00000885
Iteration 36/1000 | Loss: 0.00000884
Iteration 37/1000 | Loss: 0.00000883
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000875
Iteration 40/1000 | Loss: 0.00000875
Iteration 41/1000 | Loss: 0.00000875
Iteration 42/1000 | Loss: 0.00000875
Iteration 43/1000 | Loss: 0.00000873
Iteration 44/1000 | Loss: 0.00000873
Iteration 45/1000 | Loss: 0.00000873
Iteration 46/1000 | Loss: 0.00000873
Iteration 47/1000 | Loss: 0.00000873
Iteration 48/1000 | Loss: 0.00000872
Iteration 49/1000 | Loss: 0.00000871
Iteration 50/1000 | Loss: 0.00000871
Iteration 51/1000 | Loss: 0.00000869
Iteration 52/1000 | Loss: 0.00000869
Iteration 53/1000 | Loss: 0.00000869
Iteration 54/1000 | Loss: 0.00000868
Iteration 55/1000 | Loss: 0.00000868
Iteration 56/1000 | Loss: 0.00000868
Iteration 57/1000 | Loss: 0.00000868
Iteration 58/1000 | Loss: 0.00000868
Iteration 59/1000 | Loss: 0.00000868
Iteration 60/1000 | Loss: 0.00000868
Iteration 61/1000 | Loss: 0.00000868
Iteration 62/1000 | Loss: 0.00000868
Iteration 63/1000 | Loss: 0.00000868
Iteration 64/1000 | Loss: 0.00000868
Iteration 65/1000 | Loss: 0.00000867
Iteration 66/1000 | Loss: 0.00000867
Iteration 67/1000 | Loss: 0.00000867
Iteration 68/1000 | Loss: 0.00000867
Iteration 69/1000 | Loss: 0.00000867
Iteration 70/1000 | Loss: 0.00000866
Iteration 71/1000 | Loss: 0.00000866
Iteration 72/1000 | Loss: 0.00000866
Iteration 73/1000 | Loss: 0.00000866
Iteration 74/1000 | Loss: 0.00000865
Iteration 75/1000 | Loss: 0.00000865
Iteration 76/1000 | Loss: 0.00000865
Iteration 77/1000 | Loss: 0.00000865
Iteration 78/1000 | Loss: 0.00000864
Iteration 79/1000 | Loss: 0.00000864
Iteration 80/1000 | Loss: 0.00000864
Iteration 81/1000 | Loss: 0.00000864
Iteration 82/1000 | Loss: 0.00000863
Iteration 83/1000 | Loss: 0.00000863
Iteration 84/1000 | Loss: 0.00000863
Iteration 85/1000 | Loss: 0.00000862
Iteration 86/1000 | Loss: 0.00000862
Iteration 87/1000 | Loss: 0.00000862
Iteration 88/1000 | Loss: 0.00000862
Iteration 89/1000 | Loss: 0.00000861
Iteration 90/1000 | Loss: 0.00000861
Iteration 91/1000 | Loss: 0.00000861
Iteration 92/1000 | Loss: 0.00000861
Iteration 93/1000 | Loss: 0.00000861
Iteration 94/1000 | Loss: 0.00000861
Iteration 95/1000 | Loss: 0.00000861
Iteration 96/1000 | Loss: 0.00000860
Iteration 97/1000 | Loss: 0.00000860
Iteration 98/1000 | Loss: 0.00000860
Iteration 99/1000 | Loss: 0.00000860
Iteration 100/1000 | Loss: 0.00000860
Iteration 101/1000 | Loss: 0.00000859
Iteration 102/1000 | Loss: 0.00000858
Iteration 103/1000 | Loss: 0.00000858
Iteration 104/1000 | Loss: 0.00000858
Iteration 105/1000 | Loss: 0.00000858
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00000857
Iteration 108/1000 | Loss: 0.00000857
Iteration 109/1000 | Loss: 0.00000857
Iteration 110/1000 | Loss: 0.00000857
Iteration 111/1000 | Loss: 0.00000857
Iteration 112/1000 | Loss: 0.00000856
Iteration 113/1000 | Loss: 0.00000856
Iteration 114/1000 | Loss: 0.00000855
Iteration 115/1000 | Loss: 0.00000855
Iteration 116/1000 | Loss: 0.00000855
Iteration 117/1000 | Loss: 0.00000855
Iteration 118/1000 | Loss: 0.00000855
Iteration 119/1000 | Loss: 0.00000855
Iteration 120/1000 | Loss: 0.00000854
Iteration 121/1000 | Loss: 0.00000854
Iteration 122/1000 | Loss: 0.00000854
Iteration 123/1000 | Loss: 0.00000854
Iteration 124/1000 | Loss: 0.00000854
Iteration 125/1000 | Loss: 0.00000854
Iteration 126/1000 | Loss: 0.00000854
Iteration 127/1000 | Loss: 0.00000854
Iteration 128/1000 | Loss: 0.00000854
Iteration 129/1000 | Loss: 0.00000854
Iteration 130/1000 | Loss: 0.00000854
Iteration 131/1000 | Loss: 0.00000853
Iteration 132/1000 | Loss: 0.00000853
Iteration 133/1000 | Loss: 0.00000852
Iteration 134/1000 | Loss: 0.00000851
Iteration 135/1000 | Loss: 0.00000851
Iteration 136/1000 | Loss: 0.00000851
Iteration 137/1000 | Loss: 0.00000851
Iteration 138/1000 | Loss: 0.00000851
Iteration 139/1000 | Loss: 0.00000851
Iteration 140/1000 | Loss: 0.00000851
Iteration 141/1000 | Loss: 0.00000850
Iteration 142/1000 | Loss: 0.00000850
Iteration 143/1000 | Loss: 0.00000850
Iteration 144/1000 | Loss: 0.00000850
Iteration 145/1000 | Loss: 0.00000850
Iteration 146/1000 | Loss: 0.00000849
Iteration 147/1000 | Loss: 0.00000849
Iteration 148/1000 | Loss: 0.00000848
Iteration 149/1000 | Loss: 0.00000848
Iteration 150/1000 | Loss: 0.00000847
Iteration 151/1000 | Loss: 0.00000847
Iteration 152/1000 | Loss: 0.00000847
Iteration 153/1000 | Loss: 0.00000847
Iteration 154/1000 | Loss: 0.00000847
Iteration 155/1000 | Loss: 0.00000847
Iteration 156/1000 | Loss: 0.00000846
Iteration 157/1000 | Loss: 0.00000846
Iteration 158/1000 | Loss: 0.00000846
Iteration 159/1000 | Loss: 0.00000846
Iteration 160/1000 | Loss: 0.00000846
Iteration 161/1000 | Loss: 0.00000845
Iteration 162/1000 | Loss: 0.00000845
Iteration 163/1000 | Loss: 0.00000845
Iteration 164/1000 | Loss: 0.00000844
Iteration 165/1000 | Loss: 0.00000844
Iteration 166/1000 | Loss: 0.00000844
Iteration 167/1000 | Loss: 0.00000844
Iteration 168/1000 | Loss: 0.00000844
Iteration 169/1000 | Loss: 0.00000844
Iteration 170/1000 | Loss: 0.00000844
Iteration 171/1000 | Loss: 0.00000844
Iteration 172/1000 | Loss: 0.00000844
Iteration 173/1000 | Loss: 0.00000844
Iteration 174/1000 | Loss: 0.00000844
Iteration 175/1000 | Loss: 0.00000843
Iteration 176/1000 | Loss: 0.00000843
Iteration 177/1000 | Loss: 0.00000843
Iteration 178/1000 | Loss: 0.00000843
Iteration 179/1000 | Loss: 0.00000843
Iteration 180/1000 | Loss: 0.00000843
Iteration 181/1000 | Loss: 0.00000842
Iteration 182/1000 | Loss: 0.00000842
Iteration 183/1000 | Loss: 0.00000842
Iteration 184/1000 | Loss: 0.00000842
Iteration 185/1000 | Loss: 0.00000841
Iteration 186/1000 | Loss: 0.00000841
Iteration 187/1000 | Loss: 0.00000841
Iteration 188/1000 | Loss: 0.00000841
Iteration 189/1000 | Loss: 0.00000841
Iteration 190/1000 | Loss: 0.00000841
Iteration 191/1000 | Loss: 0.00000841
Iteration 192/1000 | Loss: 0.00000841
Iteration 193/1000 | Loss: 0.00000841
Iteration 194/1000 | Loss: 0.00000841
Iteration 195/1000 | Loss: 0.00000841
Iteration 196/1000 | Loss: 0.00000841
Iteration 197/1000 | Loss: 0.00000841
Iteration 198/1000 | Loss: 0.00000841
Iteration 199/1000 | Loss: 0.00000841
Iteration 200/1000 | Loss: 0.00000841
Iteration 201/1000 | Loss: 0.00000840
Iteration 202/1000 | Loss: 0.00000840
Iteration 203/1000 | Loss: 0.00000840
Iteration 204/1000 | Loss: 0.00000840
Iteration 205/1000 | Loss: 0.00000840
Iteration 206/1000 | Loss: 0.00000840
Iteration 207/1000 | Loss: 0.00000840
Iteration 208/1000 | Loss: 0.00000840
Iteration 209/1000 | Loss: 0.00000840
Iteration 210/1000 | Loss: 0.00000840
Iteration 211/1000 | Loss: 0.00000840
Iteration 212/1000 | Loss: 0.00000840
Iteration 213/1000 | Loss: 0.00000840
Iteration 214/1000 | Loss: 0.00000840
Iteration 215/1000 | Loss: 0.00000840
Iteration 216/1000 | Loss: 0.00000840
Iteration 217/1000 | Loss: 0.00000840
Iteration 218/1000 | Loss: 0.00000840
Iteration 219/1000 | Loss: 0.00000840
Iteration 220/1000 | Loss: 0.00000840
Iteration 221/1000 | Loss: 0.00000839
Iteration 222/1000 | Loss: 0.00000839
Iteration 223/1000 | Loss: 0.00000839
Iteration 224/1000 | Loss: 0.00000839
Iteration 225/1000 | Loss: 0.00000839
Iteration 226/1000 | Loss: 0.00000839
Iteration 227/1000 | Loss: 0.00000839
Iteration 228/1000 | Loss: 0.00000839
Iteration 229/1000 | Loss: 0.00000839
Iteration 230/1000 | Loss: 0.00000838
Iteration 231/1000 | Loss: 0.00000838
Iteration 232/1000 | Loss: 0.00000838
Iteration 233/1000 | Loss: 0.00000838
Iteration 234/1000 | Loss: 0.00000838
Iteration 235/1000 | Loss: 0.00000838
Iteration 236/1000 | Loss: 0.00000838
Iteration 237/1000 | Loss: 0.00000838
Iteration 238/1000 | Loss: 0.00000838
Iteration 239/1000 | Loss: 0.00000838
Iteration 240/1000 | Loss: 0.00000838
Iteration 241/1000 | Loss: 0.00000838
Iteration 242/1000 | Loss: 0.00000838
Iteration 243/1000 | Loss: 0.00000838
Iteration 244/1000 | Loss: 0.00000838
Iteration 245/1000 | Loss: 0.00000838
Iteration 246/1000 | Loss: 0.00000838
Iteration 247/1000 | Loss: 0.00000838
Iteration 248/1000 | Loss: 0.00000838
Iteration 249/1000 | Loss: 0.00000838
Iteration 250/1000 | Loss: 0.00000838
Iteration 251/1000 | Loss: 0.00000838
Iteration 252/1000 | Loss: 0.00000838
Iteration 253/1000 | Loss: 0.00000838
Iteration 254/1000 | Loss: 0.00000838
Iteration 255/1000 | Loss: 0.00000838
Iteration 256/1000 | Loss: 0.00000838
Iteration 257/1000 | Loss: 0.00000838
Iteration 258/1000 | Loss: 0.00000838
Iteration 259/1000 | Loss: 0.00000838
Iteration 260/1000 | Loss: 0.00000838
Iteration 261/1000 | Loss: 0.00000838
Iteration 262/1000 | Loss: 0.00000838
Iteration 263/1000 | Loss: 0.00000838
Iteration 264/1000 | Loss: 0.00000838
Iteration 265/1000 | Loss: 0.00000838
Iteration 266/1000 | Loss: 0.00000838
Iteration 267/1000 | Loss: 0.00000838
Iteration 268/1000 | Loss: 0.00000838
Iteration 269/1000 | Loss: 0.00000838
Iteration 270/1000 | Loss: 0.00000838
Iteration 271/1000 | Loss: 0.00000838
Iteration 272/1000 | Loss: 0.00000838
Iteration 273/1000 | Loss: 0.00000838
Iteration 274/1000 | Loss: 0.00000838
Iteration 275/1000 | Loss: 0.00000838
Iteration 276/1000 | Loss: 0.00000838
Iteration 277/1000 | Loss: 0.00000838
Iteration 278/1000 | Loss: 0.00000838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [8.380095096072182e-06, 8.380095096072182e-06, 8.380095096072182e-06, 8.380095096072182e-06, 8.380095096072182e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.380095096072182e-06

Optimization complete. Final v2v error: 2.473994731903076 mm

Highest mean error: 2.6714541912078857 mm for frame 55

Lowest mean error: 2.3412177562713623 mm for frame 19

Saving results

Total time: 42.27441906929016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006842
Iteration 2/25 | Loss: 0.01006842
Iteration 3/25 | Loss: 0.01006842
Iteration 4/25 | Loss: 0.01006842
Iteration 5/25 | Loss: 0.01006842
Iteration 6/25 | Loss: 0.00279137
Iteration 7/25 | Loss: 0.00184764
Iteration 8/25 | Loss: 0.00167838
Iteration 9/25 | Loss: 0.00157663
Iteration 10/25 | Loss: 0.00149243
Iteration 11/25 | Loss: 0.00137794
Iteration 12/25 | Loss: 0.00134434
Iteration 13/25 | Loss: 0.00130180
Iteration 14/25 | Loss: 0.00131075
Iteration 15/25 | Loss: 0.00131224
Iteration 16/25 | Loss: 0.00130069
Iteration 17/25 | Loss: 0.00130192
Iteration 18/25 | Loss: 0.00129228
Iteration 19/25 | Loss: 0.00128707
Iteration 20/25 | Loss: 0.00128029
Iteration 21/25 | Loss: 0.00127823
Iteration 22/25 | Loss: 0.00127592
Iteration 23/25 | Loss: 0.00127492
Iteration 24/25 | Loss: 0.00127606
Iteration 25/25 | Loss: 0.00127450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31198716
Iteration 2/25 | Loss: 0.00267879
Iteration 3/25 | Loss: 0.00125324
Iteration 4/25 | Loss: 0.00125324
Iteration 5/25 | Loss: 0.00125324
Iteration 6/25 | Loss: 0.00125324
Iteration 7/25 | Loss: 0.00125324
Iteration 8/25 | Loss: 0.00125324
Iteration 9/25 | Loss: 0.00125324
Iteration 10/25 | Loss: 0.00125324
Iteration 11/25 | Loss: 0.00125324
Iteration 12/25 | Loss: 0.00125324
Iteration 13/25 | Loss: 0.00125324
Iteration 14/25 | Loss: 0.00125324
Iteration 15/25 | Loss: 0.00125324
Iteration 16/25 | Loss: 0.00125324
Iteration 17/25 | Loss: 0.00125324
Iteration 18/25 | Loss: 0.00125324
Iteration 19/25 | Loss: 0.00125324
Iteration 20/25 | Loss: 0.00125324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012532365508377552, 0.0012532365508377552, 0.0012532365508377552, 0.0012532365508377552, 0.0012532365508377552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012532365508377552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125324
Iteration 2/1000 | Loss: 0.00154327
Iteration 3/1000 | Loss: 0.00016906
Iteration 4/1000 | Loss: 0.00010236
Iteration 5/1000 | Loss: 0.00012380
Iteration 6/1000 | Loss: 0.00010182
Iteration 7/1000 | Loss: 0.00008108
Iteration 8/1000 | Loss: 0.00012952
Iteration 9/1000 | Loss: 0.00033145
Iteration 10/1000 | Loss: 0.00028296
Iteration 11/1000 | Loss: 0.00029402
Iteration 12/1000 | Loss: 0.00016031
Iteration 13/1000 | Loss: 0.00093391
Iteration 14/1000 | Loss: 0.00042030
Iteration 15/1000 | Loss: 0.00052689
Iteration 16/1000 | Loss: 0.00019340
Iteration 17/1000 | Loss: 0.00016607
Iteration 18/1000 | Loss: 0.00041224
Iteration 19/1000 | Loss: 0.00007443
Iteration 20/1000 | Loss: 0.00013340
Iteration 21/1000 | Loss: 0.00026746
Iteration 22/1000 | Loss: 0.00053097
Iteration 23/1000 | Loss: 0.00020899
Iteration 24/1000 | Loss: 0.00040314
Iteration 25/1000 | Loss: 0.00003593
Iteration 26/1000 | Loss: 0.00003114
Iteration 27/1000 | Loss: 0.00002633
Iteration 28/1000 | Loss: 0.00004305
Iteration 29/1000 | Loss: 0.00002308
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001820
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00007239
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001542
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00005884
Iteration 49/1000 | Loss: 0.00004800
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00010618
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001503
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001503
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001502
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001502
Iteration 86/1000 | Loss: 0.00001502
Iteration 87/1000 | Loss: 0.00001502
Iteration 88/1000 | Loss: 0.00001502
Iteration 89/1000 | Loss: 0.00001502
Iteration 90/1000 | Loss: 0.00001502
Iteration 91/1000 | Loss: 0.00001502
Iteration 92/1000 | Loss: 0.00001502
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.5018241356301587e-05, 1.5018241356301587e-05, 1.5018241356301587e-05, 1.5018241356301587e-05, 1.5018241356301587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5018241356301587e-05

Optimization complete. Final v2v error: 3.3249738216400146 mm

Highest mean error: 3.749798536300659 mm for frame 222

Lowest mean error: 2.972850799560547 mm for frame 232

Saving results

Total time: 119.02793502807617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760490
Iteration 2/25 | Loss: 0.00115826
Iteration 3/25 | Loss: 0.00108667
Iteration 4/25 | Loss: 0.00107489
Iteration 5/25 | Loss: 0.00107118
Iteration 6/25 | Loss: 0.00107053
Iteration 7/25 | Loss: 0.00107053
Iteration 8/25 | Loss: 0.00107053
Iteration 9/25 | Loss: 0.00107053
Iteration 10/25 | Loss: 0.00107053
Iteration 11/25 | Loss: 0.00107053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010705332970246673, 0.0010705332970246673, 0.0010705332970246673, 0.0010705332970246673, 0.0010705332970246673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010705332970246673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31730235
Iteration 2/25 | Loss: 0.00122316
Iteration 3/25 | Loss: 0.00122316
Iteration 4/25 | Loss: 0.00122315
Iteration 5/25 | Loss: 0.00122315
Iteration 6/25 | Loss: 0.00122315
Iteration 7/25 | Loss: 0.00122315
Iteration 8/25 | Loss: 0.00122315
Iteration 9/25 | Loss: 0.00122315
Iteration 10/25 | Loss: 0.00122315
Iteration 11/25 | Loss: 0.00122315
Iteration 12/25 | Loss: 0.00122315
Iteration 13/25 | Loss: 0.00122315
Iteration 14/25 | Loss: 0.00122315
Iteration 15/25 | Loss: 0.00122315
Iteration 16/25 | Loss: 0.00122315
Iteration 17/25 | Loss: 0.00122315
Iteration 18/25 | Loss: 0.00122315
Iteration 19/25 | Loss: 0.00122315
Iteration 20/25 | Loss: 0.00122315
Iteration 21/25 | Loss: 0.00122315
Iteration 22/25 | Loss: 0.00122315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012231508735567331, 0.0012231508735567331, 0.0012231508735567331, 0.0012231508735567331, 0.0012231508735567331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012231508735567331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122315
Iteration 2/1000 | Loss: 0.00003323
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001602
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001438
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001355
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001238
Iteration 16/1000 | Loss: 0.00001237
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001220
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001212
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001209
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001200
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001200
Iteration 158/1000 | Loss: 0.00001200
Iteration 159/1000 | Loss: 0.00001200
Iteration 160/1000 | Loss: 0.00001200
Iteration 161/1000 | Loss: 0.00001200
Iteration 162/1000 | Loss: 0.00001200
Iteration 163/1000 | Loss: 0.00001200
Iteration 164/1000 | Loss: 0.00001200
Iteration 165/1000 | Loss: 0.00001200
Iteration 166/1000 | Loss: 0.00001200
Iteration 167/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1996856301266234e-05, 1.1996856301266234e-05, 1.1996856301266234e-05, 1.1996856301266234e-05, 1.1996856301266234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1996856301266234e-05

Optimization complete. Final v2v error: 2.964683771133423 mm

Highest mean error: 3.262817859649658 mm for frame 96

Lowest mean error: 2.7711498737335205 mm for frame 1

Saving results

Total time: 37.452162742614746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462034
Iteration 2/25 | Loss: 0.00123672
Iteration 3/25 | Loss: 0.00111414
Iteration 4/25 | Loss: 0.00108558
Iteration 5/25 | Loss: 0.00107614
Iteration 6/25 | Loss: 0.00107412
Iteration 7/25 | Loss: 0.00107410
Iteration 8/25 | Loss: 0.00107410
Iteration 9/25 | Loss: 0.00107410
Iteration 10/25 | Loss: 0.00107410
Iteration 11/25 | Loss: 0.00107410
Iteration 12/25 | Loss: 0.00107410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010741037549450994, 0.0010741037549450994, 0.0010741037549450994, 0.0010741037549450994, 0.0010741037549450994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010741037549450994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29795694
Iteration 2/25 | Loss: 0.00095930
Iteration 3/25 | Loss: 0.00095930
Iteration 4/25 | Loss: 0.00095929
Iteration 5/25 | Loss: 0.00095929
Iteration 6/25 | Loss: 0.00095929
Iteration 7/25 | Loss: 0.00095929
Iteration 8/25 | Loss: 0.00095929
Iteration 9/25 | Loss: 0.00095929
Iteration 10/25 | Loss: 0.00095929
Iteration 11/25 | Loss: 0.00095929
Iteration 12/25 | Loss: 0.00095929
Iteration 13/25 | Loss: 0.00095929
Iteration 14/25 | Loss: 0.00095929
Iteration 15/25 | Loss: 0.00095929
Iteration 16/25 | Loss: 0.00095929
Iteration 17/25 | Loss: 0.00095929
Iteration 18/25 | Loss: 0.00095929
Iteration 19/25 | Loss: 0.00095929
Iteration 20/25 | Loss: 0.00095929
Iteration 21/25 | Loss: 0.00095929
Iteration 22/25 | Loss: 0.00095929
Iteration 23/25 | Loss: 0.00095929
Iteration 24/25 | Loss: 0.00095929
Iteration 25/25 | Loss: 0.00095929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095929
Iteration 2/1000 | Loss: 0.00005868
Iteration 3/1000 | Loss: 0.00003463
Iteration 4/1000 | Loss: 0.00002647
Iteration 5/1000 | Loss: 0.00002429
Iteration 6/1000 | Loss: 0.00002329
Iteration 7/1000 | Loss: 0.00002261
Iteration 8/1000 | Loss: 0.00002206
Iteration 9/1000 | Loss: 0.00002176
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002141
Iteration 12/1000 | Loss: 0.00002126
Iteration 13/1000 | Loss: 0.00002110
Iteration 14/1000 | Loss: 0.00002102
Iteration 15/1000 | Loss: 0.00002094
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00002088
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002086
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002082
Iteration 22/1000 | Loss: 0.00002077
Iteration 23/1000 | Loss: 0.00002070
Iteration 24/1000 | Loss: 0.00002070
Iteration 25/1000 | Loss: 0.00002068
Iteration 26/1000 | Loss: 0.00002068
Iteration 27/1000 | Loss: 0.00002065
Iteration 28/1000 | Loss: 0.00002065
Iteration 29/1000 | Loss: 0.00002065
Iteration 30/1000 | Loss: 0.00002065
Iteration 31/1000 | Loss: 0.00002065
Iteration 32/1000 | Loss: 0.00002064
Iteration 33/1000 | Loss: 0.00002064
Iteration 34/1000 | Loss: 0.00002064
Iteration 35/1000 | Loss: 0.00002064
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00002064
Iteration 38/1000 | Loss: 0.00002063
Iteration 39/1000 | Loss: 0.00002063
Iteration 40/1000 | Loss: 0.00002063
Iteration 41/1000 | Loss: 0.00002063
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00002063
Iteration 44/1000 | Loss: 0.00002063
Iteration 45/1000 | Loss: 0.00002062
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002061
Iteration 48/1000 | Loss: 0.00002060
Iteration 49/1000 | Loss: 0.00002060
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00002059
Iteration 52/1000 | Loss: 0.00002059
Iteration 53/1000 | Loss: 0.00002059
Iteration 54/1000 | Loss: 0.00002059
Iteration 55/1000 | Loss: 0.00002059
Iteration 56/1000 | Loss: 0.00002059
Iteration 57/1000 | Loss: 0.00002058
Iteration 58/1000 | Loss: 0.00002058
Iteration 59/1000 | Loss: 0.00002058
Iteration 60/1000 | Loss: 0.00002058
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002057
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002057
Iteration 65/1000 | Loss: 0.00002056
Iteration 66/1000 | Loss: 0.00002056
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002055
Iteration 69/1000 | Loss: 0.00002055
Iteration 70/1000 | Loss: 0.00002055
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002054
Iteration 75/1000 | Loss: 0.00002054
Iteration 76/1000 | Loss: 0.00002053
Iteration 77/1000 | Loss: 0.00002053
Iteration 78/1000 | Loss: 0.00002053
Iteration 79/1000 | Loss: 0.00002053
Iteration 80/1000 | Loss: 0.00002053
Iteration 81/1000 | Loss: 0.00002053
Iteration 82/1000 | Loss: 0.00002053
Iteration 83/1000 | Loss: 0.00002053
Iteration 84/1000 | Loss: 0.00002053
Iteration 85/1000 | Loss: 0.00002052
Iteration 86/1000 | Loss: 0.00002052
Iteration 87/1000 | Loss: 0.00002052
Iteration 88/1000 | Loss: 0.00002052
Iteration 89/1000 | Loss: 0.00002051
Iteration 90/1000 | Loss: 0.00002051
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002051
Iteration 94/1000 | Loss: 0.00002051
Iteration 95/1000 | Loss: 0.00002051
Iteration 96/1000 | Loss: 0.00002051
Iteration 97/1000 | Loss: 0.00002051
Iteration 98/1000 | Loss: 0.00002051
Iteration 99/1000 | Loss: 0.00002051
Iteration 100/1000 | Loss: 0.00002051
Iteration 101/1000 | Loss: 0.00002051
Iteration 102/1000 | Loss: 0.00002051
Iteration 103/1000 | Loss: 0.00002051
Iteration 104/1000 | Loss: 0.00002051
Iteration 105/1000 | Loss: 0.00002051
Iteration 106/1000 | Loss: 0.00002051
Iteration 107/1000 | Loss: 0.00002051
Iteration 108/1000 | Loss: 0.00002051
Iteration 109/1000 | Loss: 0.00002051
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00002051
Iteration 112/1000 | Loss: 0.00002051
Iteration 113/1000 | Loss: 0.00002051
Iteration 114/1000 | Loss: 0.00002051
Iteration 115/1000 | Loss: 0.00002051
Iteration 116/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.0505305656115524e-05, 2.0505305656115524e-05, 2.0505305656115524e-05, 2.0505305656115524e-05, 2.0505305656115524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0505305656115524e-05

Optimization complete. Final v2v error: 3.654893159866333 mm

Highest mean error: 4.231462001800537 mm for frame 175

Lowest mean error: 3.3308680057525635 mm for frame 166

Saving results

Total time: 38.30465006828308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036660
Iteration 2/25 | Loss: 0.00140291
Iteration 3/25 | Loss: 0.00116124
Iteration 4/25 | Loss: 0.00112100
Iteration 5/25 | Loss: 0.00111145
Iteration 6/25 | Loss: 0.00111859
Iteration 7/25 | Loss: 0.00111107
Iteration 8/25 | Loss: 0.00112283
Iteration 9/25 | Loss: 0.00113375
Iteration 10/25 | Loss: 0.00112459
Iteration 11/25 | Loss: 0.00111481
Iteration 12/25 | Loss: 0.00110678
Iteration 13/25 | Loss: 0.00110391
Iteration 14/25 | Loss: 0.00110268
Iteration 15/25 | Loss: 0.00110217
Iteration 16/25 | Loss: 0.00110092
Iteration 17/25 | Loss: 0.00110059
Iteration 18/25 | Loss: 0.00110050
Iteration 19/25 | Loss: 0.00110050
Iteration 20/25 | Loss: 0.00110050
Iteration 21/25 | Loss: 0.00110050
Iteration 22/25 | Loss: 0.00110049
Iteration 23/25 | Loss: 0.00110049
Iteration 24/25 | Loss: 0.00110049
Iteration 25/25 | Loss: 0.00110049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38801336
Iteration 2/25 | Loss: 0.00084130
Iteration 3/25 | Loss: 0.00084130
Iteration 4/25 | Loss: 0.00084130
Iteration 5/25 | Loss: 0.00084130
Iteration 6/25 | Loss: 0.00084130
Iteration 7/25 | Loss: 0.00084130
Iteration 8/25 | Loss: 0.00084130
Iteration 9/25 | Loss: 0.00084130
Iteration 10/25 | Loss: 0.00084130
Iteration 11/25 | Loss: 0.00084130
Iteration 12/25 | Loss: 0.00084130
Iteration 13/25 | Loss: 0.00084130
Iteration 14/25 | Loss: 0.00084130
Iteration 15/25 | Loss: 0.00084130
Iteration 16/25 | Loss: 0.00084130
Iteration 17/25 | Loss: 0.00084130
Iteration 18/25 | Loss: 0.00084130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008413015748374164, 0.0008413015748374164, 0.0008413015748374164, 0.0008413015748374164, 0.0008413015748374164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008413015748374164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084130
Iteration 2/1000 | Loss: 0.00002437
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001678
Iteration 5/1000 | Loss: 0.00001592
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001449
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001368
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001355
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001348
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00020513
Iteration 25/1000 | Loss: 0.00038407
Iteration 26/1000 | Loss: 0.00084626
Iteration 27/1000 | Loss: 0.00065364
Iteration 28/1000 | Loss: 0.00002554
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00001778
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00064464
Iteration 36/1000 | Loss: 0.00041293
Iteration 37/1000 | Loss: 0.00005758
Iteration 38/1000 | Loss: 0.00015944
Iteration 39/1000 | Loss: 0.00021195
Iteration 40/1000 | Loss: 0.00012481
Iteration 41/1000 | Loss: 0.00002623
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001674
Iteration 45/1000 | Loss: 0.00001519
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00022217
Iteration 52/1000 | Loss: 0.00008117
Iteration 53/1000 | Loss: 0.00019330
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001107
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001105
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001094
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001091
Iteration 116/1000 | Loss: 0.00001091
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001091
Iteration 121/1000 | Loss: 0.00001091
Iteration 122/1000 | Loss: 0.00001091
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001091
Iteration 126/1000 | Loss: 0.00001091
Iteration 127/1000 | Loss: 0.00001091
Iteration 128/1000 | Loss: 0.00001091
Iteration 129/1000 | Loss: 0.00001091
Iteration 130/1000 | Loss: 0.00001091
Iteration 131/1000 | Loss: 0.00001091
Iteration 132/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.0907549039984588e-05, 1.0907549039984588e-05, 1.0907549039984588e-05, 1.0907549039984588e-05, 1.0907549039984588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0907549039984588e-05

Optimization complete. Final v2v error: 2.755408525466919 mm

Highest mean error: 5.313416957855225 mm for frame 13

Lowest mean error: 2.5126049518585205 mm for frame 208

Saving results

Total time: 123.96990394592285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978039
Iteration 2/25 | Loss: 0.00302228
Iteration 3/25 | Loss: 0.00197431
Iteration 4/25 | Loss: 0.00162972
Iteration 5/25 | Loss: 0.00174176
Iteration 6/25 | Loss: 0.00169036
Iteration 7/25 | Loss: 0.00158466
Iteration 8/25 | Loss: 0.00141406
Iteration 9/25 | Loss: 0.00127994
Iteration 10/25 | Loss: 0.00124697
Iteration 11/25 | Loss: 0.00121769
Iteration 12/25 | Loss: 0.00119752
Iteration 13/25 | Loss: 0.00117875
Iteration 14/25 | Loss: 0.00117605
Iteration 15/25 | Loss: 0.00117681
Iteration 16/25 | Loss: 0.00115445
Iteration 17/25 | Loss: 0.00114447
Iteration 18/25 | Loss: 0.00113800
Iteration 19/25 | Loss: 0.00113474
Iteration 20/25 | Loss: 0.00113296
Iteration 21/25 | Loss: 0.00113190
Iteration 22/25 | Loss: 0.00113157
Iteration 23/25 | Loss: 0.00113116
Iteration 24/25 | Loss: 0.00113499
Iteration 25/25 | Loss: 0.00113066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38016045
Iteration 2/25 | Loss: 0.00089276
Iteration 3/25 | Loss: 0.00069595
Iteration 4/25 | Loss: 0.00069595
Iteration 5/25 | Loss: 0.00069595
Iteration 6/25 | Loss: 0.00069595
Iteration 7/25 | Loss: 0.00069595
Iteration 8/25 | Loss: 0.00069595
Iteration 9/25 | Loss: 0.00069595
Iteration 10/25 | Loss: 0.00069595
Iteration 11/25 | Loss: 0.00069595
Iteration 12/25 | Loss: 0.00069595
Iteration 13/25 | Loss: 0.00069595
Iteration 14/25 | Loss: 0.00069595
Iteration 15/25 | Loss: 0.00069595
Iteration 16/25 | Loss: 0.00069595
Iteration 17/25 | Loss: 0.00069595
Iteration 18/25 | Loss: 0.00069595
Iteration 19/25 | Loss: 0.00069595
Iteration 20/25 | Loss: 0.00069595
Iteration 21/25 | Loss: 0.00069595
Iteration 22/25 | Loss: 0.00069595
Iteration 23/25 | Loss: 0.00069595
Iteration 24/25 | Loss: 0.00069595
Iteration 25/25 | Loss: 0.00069595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069595
Iteration 2/1000 | Loss: 0.00021212
Iteration 3/1000 | Loss: 0.00046037
Iteration 4/1000 | Loss: 0.00364864
Iteration 5/1000 | Loss: 0.00018114
Iteration 6/1000 | Loss: 0.00004287
Iteration 7/1000 | Loss: 0.00010738
Iteration 8/1000 | Loss: 0.00003355
Iteration 9/1000 | Loss: 0.00038145
Iteration 10/1000 | Loss: 0.00044239
Iteration 11/1000 | Loss: 0.00003651
Iteration 12/1000 | Loss: 0.00004525
Iteration 13/1000 | Loss: 0.00004188
Iteration 14/1000 | Loss: 0.00002937
Iteration 15/1000 | Loss: 0.00002880
Iteration 16/1000 | Loss: 0.00002817
Iteration 17/1000 | Loss: 0.00002762
Iteration 18/1000 | Loss: 0.00021058
Iteration 19/1000 | Loss: 0.00003010
Iteration 20/1000 | Loss: 0.00016838
Iteration 21/1000 | Loss: 0.00002722
Iteration 22/1000 | Loss: 0.00002697
Iteration 23/1000 | Loss: 0.00002673
Iteration 24/1000 | Loss: 0.00002647
Iteration 25/1000 | Loss: 0.00002647
Iteration 26/1000 | Loss: 0.00023772
Iteration 27/1000 | Loss: 0.00002754
Iteration 28/1000 | Loss: 0.00002644
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002622
Iteration 31/1000 | Loss: 0.00002616
Iteration 32/1000 | Loss: 0.00002611
Iteration 33/1000 | Loss: 0.00002591
Iteration 34/1000 | Loss: 0.00002575
Iteration 35/1000 | Loss: 0.00002573
Iteration 36/1000 | Loss: 0.00002572
Iteration 37/1000 | Loss: 0.00002571
Iteration 38/1000 | Loss: 0.00002571
Iteration 39/1000 | Loss: 0.00002569
Iteration 40/1000 | Loss: 0.00002564
Iteration 41/1000 | Loss: 0.00002560
Iteration 42/1000 | Loss: 0.00002560
Iteration 43/1000 | Loss: 0.00002559
Iteration 44/1000 | Loss: 0.00002556
Iteration 45/1000 | Loss: 0.00002556
Iteration 46/1000 | Loss: 0.00060350
Iteration 47/1000 | Loss: 0.00105441
Iteration 48/1000 | Loss: 0.00048474
Iteration 49/1000 | Loss: 0.00003443
Iteration 50/1000 | Loss: 0.00002891
Iteration 51/1000 | Loss: 0.00002592
Iteration 52/1000 | Loss: 0.00002431
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00002311
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002237
Iteration 58/1000 | Loss: 0.00002232
Iteration 59/1000 | Loss: 0.00010097
Iteration 60/1000 | Loss: 0.00002220
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002208
Iteration 63/1000 | Loss: 0.00002208
Iteration 64/1000 | Loss: 0.00002208
Iteration 65/1000 | Loss: 0.00002207
Iteration 66/1000 | Loss: 0.00002207
Iteration 67/1000 | Loss: 0.00002207
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002207
Iteration 70/1000 | Loss: 0.00002207
Iteration 71/1000 | Loss: 0.00002207
Iteration 72/1000 | Loss: 0.00002207
Iteration 73/1000 | Loss: 0.00002207
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002205
Iteration 77/1000 | Loss: 0.00002205
Iteration 78/1000 | Loss: 0.00002205
Iteration 79/1000 | Loss: 0.00002204
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002203
Iteration 84/1000 | Loss: 0.00002203
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002201
Iteration 87/1000 | Loss: 0.00002200
Iteration 88/1000 | Loss: 0.00002200
Iteration 89/1000 | Loss: 0.00002200
Iteration 90/1000 | Loss: 0.00002199
Iteration 91/1000 | Loss: 0.00002199
Iteration 92/1000 | Loss: 0.00002199
Iteration 93/1000 | Loss: 0.00002199
Iteration 94/1000 | Loss: 0.00002199
Iteration 95/1000 | Loss: 0.00002199
Iteration 96/1000 | Loss: 0.00002199
Iteration 97/1000 | Loss: 0.00002198
Iteration 98/1000 | Loss: 0.00002198
Iteration 99/1000 | Loss: 0.00002198
Iteration 100/1000 | Loss: 0.00002198
Iteration 101/1000 | Loss: 0.00002198
Iteration 102/1000 | Loss: 0.00002198
Iteration 103/1000 | Loss: 0.00002198
Iteration 104/1000 | Loss: 0.00002198
Iteration 105/1000 | Loss: 0.00002198
Iteration 106/1000 | Loss: 0.00002198
Iteration 107/1000 | Loss: 0.00002198
Iteration 108/1000 | Loss: 0.00002197
Iteration 109/1000 | Loss: 0.00002197
Iteration 110/1000 | Loss: 0.00002197
Iteration 111/1000 | Loss: 0.00002197
Iteration 112/1000 | Loss: 0.00002197
Iteration 113/1000 | Loss: 0.00002197
Iteration 114/1000 | Loss: 0.00002196
Iteration 115/1000 | Loss: 0.00002196
Iteration 116/1000 | Loss: 0.00002196
Iteration 117/1000 | Loss: 0.00002196
Iteration 118/1000 | Loss: 0.00002196
Iteration 119/1000 | Loss: 0.00002195
Iteration 120/1000 | Loss: 0.00002195
Iteration 121/1000 | Loss: 0.00002195
Iteration 122/1000 | Loss: 0.00002195
Iteration 123/1000 | Loss: 0.00002195
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002194
Iteration 126/1000 | Loss: 0.00002194
Iteration 127/1000 | Loss: 0.00002194
Iteration 128/1000 | Loss: 0.00002194
Iteration 129/1000 | Loss: 0.00002194
Iteration 130/1000 | Loss: 0.00002194
Iteration 131/1000 | Loss: 0.00002194
Iteration 132/1000 | Loss: 0.00002194
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002194
Iteration 135/1000 | Loss: 0.00002194
Iteration 136/1000 | Loss: 0.00002194
Iteration 137/1000 | Loss: 0.00002194
Iteration 138/1000 | Loss: 0.00002194
Iteration 139/1000 | Loss: 0.00002194
Iteration 140/1000 | Loss: 0.00002194
Iteration 141/1000 | Loss: 0.00002194
Iteration 142/1000 | Loss: 0.00002194
Iteration 143/1000 | Loss: 0.00002193
Iteration 144/1000 | Loss: 0.00002193
Iteration 145/1000 | Loss: 0.00002193
Iteration 146/1000 | Loss: 0.00002193
Iteration 147/1000 | Loss: 0.00002193
Iteration 148/1000 | Loss: 0.00002193
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00002193
Iteration 151/1000 | Loss: 0.00002193
Iteration 152/1000 | Loss: 0.00002193
Iteration 153/1000 | Loss: 0.00002193
Iteration 154/1000 | Loss: 0.00002193
Iteration 155/1000 | Loss: 0.00002193
Iteration 156/1000 | Loss: 0.00002193
Iteration 157/1000 | Loss: 0.00002193
Iteration 158/1000 | Loss: 0.00002193
Iteration 159/1000 | Loss: 0.00002193
Iteration 160/1000 | Loss: 0.00002193
Iteration 161/1000 | Loss: 0.00002193
Iteration 162/1000 | Loss: 0.00002193
Iteration 163/1000 | Loss: 0.00002193
Iteration 164/1000 | Loss: 0.00002193
Iteration 165/1000 | Loss: 0.00002192
Iteration 166/1000 | Loss: 0.00002192
Iteration 167/1000 | Loss: 0.00002192
Iteration 168/1000 | Loss: 0.00002192
Iteration 169/1000 | Loss: 0.00002192
Iteration 170/1000 | Loss: 0.00002192
Iteration 171/1000 | Loss: 0.00002192
Iteration 172/1000 | Loss: 0.00002192
Iteration 173/1000 | Loss: 0.00002192
Iteration 174/1000 | Loss: 0.00002192
Iteration 175/1000 | Loss: 0.00002192
Iteration 176/1000 | Loss: 0.00002192
Iteration 177/1000 | Loss: 0.00002192
Iteration 178/1000 | Loss: 0.00002192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.1920466679148376e-05, 2.1920466679148376e-05, 2.1920466679148376e-05, 2.1920466679148376e-05, 2.1920466679148376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1920466679148376e-05

Optimization complete. Final v2v error: 4.0073466300964355 mm

Highest mean error: 4.6293559074401855 mm for frame 45

Lowest mean error: 3.286917209625244 mm for frame 96

Saving results

Total time: 115.13915205001831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00207807
Iteration 2/25 | Loss: 0.00111469
Iteration 3/25 | Loss: 0.00104587
Iteration 4/25 | Loss: 0.00102593
Iteration 5/25 | Loss: 0.00101783
Iteration 6/25 | Loss: 0.00101531
Iteration 7/25 | Loss: 0.00101421
Iteration 8/25 | Loss: 0.00101421
Iteration 9/25 | Loss: 0.00101421
Iteration 10/25 | Loss: 0.00101421
Iteration 11/25 | Loss: 0.00101421
Iteration 12/25 | Loss: 0.00101421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010142055107280612, 0.0010142055107280612, 0.0010142055107280612, 0.0010142055107280612, 0.0010142055107280612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010142055107280612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31643498
Iteration 2/25 | Loss: 0.00131630
Iteration 3/25 | Loss: 0.00131630
Iteration 4/25 | Loss: 0.00131630
Iteration 5/25 | Loss: 0.00131630
Iteration 6/25 | Loss: 0.00131630
Iteration 7/25 | Loss: 0.00131630
Iteration 8/25 | Loss: 0.00131630
Iteration 9/25 | Loss: 0.00131630
Iteration 10/25 | Loss: 0.00131630
Iteration 11/25 | Loss: 0.00131630
Iteration 12/25 | Loss: 0.00131630
Iteration 13/25 | Loss: 0.00131630
Iteration 14/25 | Loss: 0.00131630
Iteration 15/25 | Loss: 0.00131630
Iteration 16/25 | Loss: 0.00131630
Iteration 17/25 | Loss: 0.00131630
Iteration 18/25 | Loss: 0.00131630
Iteration 19/25 | Loss: 0.00131630
Iteration 20/25 | Loss: 0.00131630
Iteration 21/25 | Loss: 0.00131630
Iteration 22/25 | Loss: 0.00131630
Iteration 23/25 | Loss: 0.00131630
Iteration 24/25 | Loss: 0.00131630
Iteration 25/25 | Loss: 0.00131630

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131630
Iteration 2/1000 | Loss: 0.00003214
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001406
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001155
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001138
Iteration 18/1000 | Loss: 0.00001137
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001128
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001126
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001122
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001118
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001117
Iteration 47/1000 | Loss: 0.00001116
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001115
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001113
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001109
Iteration 77/1000 | Loss: 0.00001109
Iteration 78/1000 | Loss: 0.00001109
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001108
Iteration 82/1000 | Loss: 0.00001107
Iteration 83/1000 | Loss: 0.00001107
Iteration 84/1000 | Loss: 0.00001107
Iteration 85/1000 | Loss: 0.00001106
Iteration 86/1000 | Loss: 0.00001106
Iteration 87/1000 | Loss: 0.00001106
Iteration 88/1000 | Loss: 0.00001106
Iteration 89/1000 | Loss: 0.00001106
Iteration 90/1000 | Loss: 0.00001106
Iteration 91/1000 | Loss: 0.00001105
Iteration 92/1000 | Loss: 0.00001105
Iteration 93/1000 | Loss: 0.00001105
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001105
Iteration 96/1000 | Loss: 0.00001104
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001104
Iteration 106/1000 | Loss: 0.00001104
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001103
Iteration 112/1000 | Loss: 0.00001103
Iteration 113/1000 | Loss: 0.00001103
Iteration 114/1000 | Loss: 0.00001103
Iteration 115/1000 | Loss: 0.00001103
Iteration 116/1000 | Loss: 0.00001103
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001103
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001101
Iteration 132/1000 | Loss: 0.00001101
Iteration 133/1000 | Loss: 0.00001101
Iteration 134/1000 | Loss: 0.00001101
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001099
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001099
Iteration 147/1000 | Loss: 0.00001099
Iteration 148/1000 | Loss: 0.00001099
Iteration 149/1000 | Loss: 0.00001099
Iteration 150/1000 | Loss: 0.00001099
Iteration 151/1000 | Loss: 0.00001099
Iteration 152/1000 | Loss: 0.00001099
Iteration 153/1000 | Loss: 0.00001099
Iteration 154/1000 | Loss: 0.00001099
Iteration 155/1000 | Loss: 0.00001099
Iteration 156/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.0989970178343356e-05, 1.0989970178343356e-05, 1.0989970178343356e-05, 1.0989970178343356e-05, 1.0989970178343356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0989970178343356e-05

Optimization complete. Final v2v error: 2.8365724086761475 mm

Highest mean error: 3.1813671588897705 mm for frame 63

Lowest mean error: 2.5603227615356445 mm for frame 173

Saving results

Total time: 41.15512251853943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400982
Iteration 2/25 | Loss: 0.00126390
Iteration 3/25 | Loss: 0.00111640
Iteration 4/25 | Loss: 0.00109693
Iteration 5/25 | Loss: 0.00109221
Iteration 6/25 | Loss: 0.00109052
Iteration 7/25 | Loss: 0.00109050
Iteration 8/25 | Loss: 0.00109050
Iteration 9/25 | Loss: 0.00109050
Iteration 10/25 | Loss: 0.00109050
Iteration 11/25 | Loss: 0.00109050
Iteration 12/25 | Loss: 0.00109050
Iteration 13/25 | Loss: 0.00109050
Iteration 14/25 | Loss: 0.00109050
Iteration 15/25 | Loss: 0.00109050
Iteration 16/25 | Loss: 0.00109050
Iteration 17/25 | Loss: 0.00109050
Iteration 18/25 | Loss: 0.00109050
Iteration 19/25 | Loss: 0.00109050
Iteration 20/25 | Loss: 0.00109050
Iteration 21/25 | Loss: 0.00109050
Iteration 22/25 | Loss: 0.00109050
Iteration 23/25 | Loss: 0.00109050
Iteration 24/25 | Loss: 0.00109050
Iteration 25/25 | Loss: 0.00109050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40267587
Iteration 2/25 | Loss: 0.00097654
Iteration 3/25 | Loss: 0.00097654
Iteration 4/25 | Loss: 0.00097654
Iteration 5/25 | Loss: 0.00097653
Iteration 6/25 | Loss: 0.00097653
Iteration 7/25 | Loss: 0.00097653
Iteration 8/25 | Loss: 0.00097653
Iteration 9/25 | Loss: 0.00097653
Iteration 10/25 | Loss: 0.00097653
Iteration 11/25 | Loss: 0.00097653
Iteration 12/25 | Loss: 0.00097653
Iteration 13/25 | Loss: 0.00097653
Iteration 14/25 | Loss: 0.00097653
Iteration 15/25 | Loss: 0.00097653
Iteration 16/25 | Loss: 0.00097653
Iteration 17/25 | Loss: 0.00097653
Iteration 18/25 | Loss: 0.00097653
Iteration 19/25 | Loss: 0.00097653
Iteration 20/25 | Loss: 0.00097653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000976532930508256, 0.000976532930508256, 0.000976532930508256, 0.000976532930508256, 0.000976532930508256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000976532930508256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097653
Iteration 2/1000 | Loss: 0.00003775
Iteration 3/1000 | Loss: 0.00002389
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001212
Iteration 12/1000 | Loss: 0.00001208
Iteration 13/1000 | Loss: 0.00001203
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001193
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001158
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001156
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001147
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001133
Iteration 123/1000 | Loss: 0.00001133
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001131
Iteration 137/1000 | Loss: 0.00001131
Iteration 138/1000 | Loss: 0.00001131
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001129
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Iteration 168/1000 | Loss: 0.00001128
Iteration 169/1000 | Loss: 0.00001128
Iteration 170/1000 | Loss: 0.00001127
Iteration 171/1000 | Loss: 0.00001127
Iteration 172/1000 | Loss: 0.00001127
Iteration 173/1000 | Loss: 0.00001127
Iteration 174/1000 | Loss: 0.00001127
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001127
Iteration 177/1000 | Loss: 0.00001127
Iteration 178/1000 | Loss: 0.00001127
Iteration 179/1000 | Loss: 0.00001127
Iteration 180/1000 | Loss: 0.00001127
Iteration 181/1000 | Loss: 0.00001127
Iteration 182/1000 | Loss: 0.00001127
Iteration 183/1000 | Loss: 0.00001127
Iteration 184/1000 | Loss: 0.00001127
Iteration 185/1000 | Loss: 0.00001127
Iteration 186/1000 | Loss: 0.00001127
Iteration 187/1000 | Loss: 0.00001127
Iteration 188/1000 | Loss: 0.00001127
Iteration 189/1000 | Loss: 0.00001127
Iteration 190/1000 | Loss: 0.00001127
Iteration 191/1000 | Loss: 0.00001127
Iteration 192/1000 | Loss: 0.00001127
Iteration 193/1000 | Loss: 0.00001127
Iteration 194/1000 | Loss: 0.00001127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.1266796718700789e-05, 1.1266796718700789e-05, 1.1266796718700789e-05, 1.1266796718700789e-05, 1.1266796718700789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1266796718700789e-05

Optimization complete. Final v2v error: 2.874962091445923 mm

Highest mean error: 3.5853664875030518 mm for frame 105

Lowest mean error: 2.407541275024414 mm for frame 161

Saving results

Total time: 40.51333236694336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866959
Iteration 2/25 | Loss: 0.00310662
Iteration 3/25 | Loss: 0.00220155
Iteration 4/25 | Loss: 0.00171000
Iteration 5/25 | Loss: 0.00171229
Iteration 6/25 | Loss: 0.00165254
Iteration 7/25 | Loss: 0.00160555
Iteration 8/25 | Loss: 0.00153663
Iteration 9/25 | Loss: 0.00152251
Iteration 10/25 | Loss: 0.00144632
Iteration 11/25 | Loss: 0.00142920
Iteration 12/25 | Loss: 0.00140992
Iteration 13/25 | Loss: 0.00140493
Iteration 14/25 | Loss: 0.00145961
Iteration 15/25 | Loss: 0.00142089
Iteration 16/25 | Loss: 0.00138102
Iteration 17/25 | Loss: 0.00137819
Iteration 18/25 | Loss: 0.00137867
Iteration 19/25 | Loss: 0.00136859
Iteration 20/25 | Loss: 0.00136622
Iteration 21/25 | Loss: 0.00136573
Iteration 22/25 | Loss: 0.00136530
Iteration 23/25 | Loss: 0.00139099
Iteration 24/25 | Loss: 0.00136718
Iteration 25/25 | Loss: 0.00136289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.36684704
Iteration 2/25 | Loss: 0.00516769
Iteration 3/25 | Loss: 0.00361752
Iteration 4/25 | Loss: 0.00361752
Iteration 5/25 | Loss: 0.00361752
Iteration 6/25 | Loss: 0.00361752
Iteration 7/25 | Loss: 0.00361752
Iteration 8/25 | Loss: 0.00361752
Iteration 9/25 | Loss: 0.00361752
Iteration 10/25 | Loss: 0.00361752
Iteration 11/25 | Loss: 0.00361752
Iteration 12/25 | Loss: 0.00361752
Iteration 13/25 | Loss: 0.00361752
Iteration 14/25 | Loss: 0.00361752
Iteration 15/25 | Loss: 0.00361752
Iteration 16/25 | Loss: 0.00361752
Iteration 17/25 | Loss: 0.00361752
Iteration 18/25 | Loss: 0.00361752
Iteration 19/25 | Loss: 0.00361752
Iteration 20/25 | Loss: 0.00361752
Iteration 21/25 | Loss: 0.00361752
Iteration 22/25 | Loss: 0.00361752
Iteration 23/25 | Loss: 0.00361752
Iteration 24/25 | Loss: 0.00361752
Iteration 25/25 | Loss: 0.00361752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361752
Iteration 2/1000 | Loss: 0.00124391
Iteration 3/1000 | Loss: 0.00091293
Iteration 4/1000 | Loss: 0.00449927
Iteration 5/1000 | Loss: 0.00506536
Iteration 6/1000 | Loss: 0.00458931
Iteration 7/1000 | Loss: 0.00216577
Iteration 8/1000 | Loss: 0.00050632
Iteration 9/1000 | Loss: 0.00163012
Iteration 10/1000 | Loss: 0.00222869
Iteration 11/1000 | Loss: 0.00023146
Iteration 12/1000 | Loss: 0.00146341
Iteration 13/1000 | Loss: 0.00071400
Iteration 14/1000 | Loss: 0.00112540
Iteration 15/1000 | Loss: 0.00027922
Iteration 16/1000 | Loss: 0.00010881
Iteration 17/1000 | Loss: 0.00047711
Iteration 18/1000 | Loss: 0.00010159
Iteration 19/1000 | Loss: 0.00086616
Iteration 20/1000 | Loss: 0.00038665
Iteration 21/1000 | Loss: 0.00089901
Iteration 22/1000 | Loss: 0.00441038
Iteration 23/1000 | Loss: 0.00324887
Iteration 24/1000 | Loss: 0.00410761
Iteration 25/1000 | Loss: 0.00464233
Iteration 26/1000 | Loss: 0.00236887
Iteration 27/1000 | Loss: 0.00037559
Iteration 28/1000 | Loss: 0.00127017
Iteration 29/1000 | Loss: 0.00011225
Iteration 30/1000 | Loss: 0.00008707
Iteration 31/1000 | Loss: 0.00010415
Iteration 32/1000 | Loss: 0.00115453
Iteration 33/1000 | Loss: 0.00006014
Iteration 34/1000 | Loss: 0.00016805
Iteration 35/1000 | Loss: 0.00109139
Iteration 36/1000 | Loss: 0.00014033
Iteration 37/1000 | Loss: 0.00014062
Iteration 38/1000 | Loss: 0.00004712
Iteration 39/1000 | Loss: 0.00046624
Iteration 40/1000 | Loss: 0.00004701
Iteration 41/1000 | Loss: 0.00003826
Iteration 42/1000 | Loss: 0.00003648
Iteration 43/1000 | Loss: 0.00124168
Iteration 44/1000 | Loss: 0.00069643
Iteration 45/1000 | Loss: 0.00005740
Iteration 46/1000 | Loss: 0.00006447
Iteration 47/1000 | Loss: 0.00003529
Iteration 48/1000 | Loss: 0.00004397
Iteration 49/1000 | Loss: 0.00003106
Iteration 50/1000 | Loss: 0.00002963
Iteration 51/1000 | Loss: 0.00053523
Iteration 52/1000 | Loss: 0.00024736
Iteration 53/1000 | Loss: 0.00002948
Iteration 54/1000 | Loss: 0.00002847
Iteration 55/1000 | Loss: 0.00002797
Iteration 56/1000 | Loss: 0.00022513
Iteration 57/1000 | Loss: 0.00052440
Iteration 58/1000 | Loss: 0.00023323
Iteration 59/1000 | Loss: 0.00051161
Iteration 60/1000 | Loss: 0.00004539
Iteration 61/1000 | Loss: 0.00003451
Iteration 62/1000 | Loss: 0.00012340
Iteration 63/1000 | Loss: 0.00003740
Iteration 64/1000 | Loss: 0.00002934
Iteration 65/1000 | Loss: 0.00002804
Iteration 66/1000 | Loss: 0.00002718
Iteration 67/1000 | Loss: 0.00006559
Iteration 68/1000 | Loss: 0.00059983
Iteration 69/1000 | Loss: 0.00081692
Iteration 70/1000 | Loss: 0.00062587
Iteration 71/1000 | Loss: 0.00033624
Iteration 72/1000 | Loss: 0.00043429
Iteration 73/1000 | Loss: 0.00069839
Iteration 74/1000 | Loss: 0.00004311
Iteration 75/1000 | Loss: 0.00002963
Iteration 76/1000 | Loss: 0.00002643
Iteration 77/1000 | Loss: 0.00007428
Iteration 78/1000 | Loss: 0.00002532
Iteration 79/1000 | Loss: 0.00002412
Iteration 80/1000 | Loss: 0.00002353
Iteration 81/1000 | Loss: 0.00002293
Iteration 82/1000 | Loss: 0.00002254
Iteration 83/1000 | Loss: 0.00007187
Iteration 84/1000 | Loss: 0.00002441
Iteration 85/1000 | Loss: 0.00002591
Iteration 86/1000 | Loss: 0.00002217
Iteration 87/1000 | Loss: 0.00035046
Iteration 88/1000 | Loss: 0.00019036
Iteration 89/1000 | Loss: 0.00002316
Iteration 90/1000 | Loss: 0.00023843
Iteration 91/1000 | Loss: 0.00018139
Iteration 92/1000 | Loss: 0.00002377
Iteration 93/1000 | Loss: 0.00024275
Iteration 94/1000 | Loss: 0.00028832
Iteration 95/1000 | Loss: 0.00046754
Iteration 96/1000 | Loss: 0.00005113
Iteration 97/1000 | Loss: 0.00004408
Iteration 98/1000 | Loss: 0.00002235
Iteration 99/1000 | Loss: 0.00002087
Iteration 100/1000 | Loss: 0.00002038
Iteration 101/1000 | Loss: 0.00002000
Iteration 102/1000 | Loss: 0.00001975
Iteration 103/1000 | Loss: 0.00001971
Iteration 104/1000 | Loss: 0.00001971
Iteration 105/1000 | Loss: 0.00001971
Iteration 106/1000 | Loss: 0.00001971
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001969
Iteration 109/1000 | Loss: 0.00001966
Iteration 110/1000 | Loss: 0.00001966
Iteration 111/1000 | Loss: 0.00001965
Iteration 112/1000 | Loss: 0.00001965
Iteration 113/1000 | Loss: 0.00001964
Iteration 114/1000 | Loss: 0.00001964
Iteration 115/1000 | Loss: 0.00001964
Iteration 116/1000 | Loss: 0.00001964
Iteration 117/1000 | Loss: 0.00001964
Iteration 118/1000 | Loss: 0.00001964
Iteration 119/1000 | Loss: 0.00001964
Iteration 120/1000 | Loss: 0.00001964
Iteration 121/1000 | Loss: 0.00001964
Iteration 122/1000 | Loss: 0.00001964
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001964
Iteration 125/1000 | Loss: 0.00001963
Iteration 126/1000 | Loss: 0.00001963
Iteration 127/1000 | Loss: 0.00001963
Iteration 128/1000 | Loss: 0.00001963
Iteration 129/1000 | Loss: 0.00001962
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001961
Iteration 132/1000 | Loss: 0.00001961
Iteration 133/1000 | Loss: 0.00001960
Iteration 134/1000 | Loss: 0.00001960
Iteration 135/1000 | Loss: 0.00001959
Iteration 136/1000 | Loss: 0.00001959
Iteration 137/1000 | Loss: 0.00001959
Iteration 138/1000 | Loss: 0.00001959
Iteration 139/1000 | Loss: 0.00001959
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001958
Iteration 148/1000 | Loss: 0.00001958
Iteration 149/1000 | Loss: 0.00001958
Iteration 150/1000 | Loss: 0.00001958
Iteration 151/1000 | Loss: 0.00001958
Iteration 152/1000 | Loss: 0.00001958
Iteration 153/1000 | Loss: 0.00001957
Iteration 154/1000 | Loss: 0.00001957
Iteration 155/1000 | Loss: 0.00001957
Iteration 156/1000 | Loss: 0.00001957
Iteration 157/1000 | Loss: 0.00001957
Iteration 158/1000 | Loss: 0.00001957
Iteration 159/1000 | Loss: 0.00001957
Iteration 160/1000 | Loss: 0.00001957
Iteration 161/1000 | Loss: 0.00001957
Iteration 162/1000 | Loss: 0.00001957
Iteration 163/1000 | Loss: 0.00001956
Iteration 164/1000 | Loss: 0.00001956
Iteration 165/1000 | Loss: 0.00001956
Iteration 166/1000 | Loss: 0.00001956
Iteration 167/1000 | Loss: 0.00001956
Iteration 168/1000 | Loss: 0.00001956
Iteration 169/1000 | Loss: 0.00001956
Iteration 170/1000 | Loss: 0.00001955
Iteration 171/1000 | Loss: 0.00001955
Iteration 172/1000 | Loss: 0.00001955
Iteration 173/1000 | Loss: 0.00001955
Iteration 174/1000 | Loss: 0.00001955
Iteration 175/1000 | Loss: 0.00001955
Iteration 176/1000 | Loss: 0.00001955
Iteration 177/1000 | Loss: 0.00001955
Iteration 178/1000 | Loss: 0.00001955
Iteration 179/1000 | Loss: 0.00001955
Iteration 180/1000 | Loss: 0.00001955
Iteration 181/1000 | Loss: 0.00001955
Iteration 182/1000 | Loss: 0.00001955
Iteration 183/1000 | Loss: 0.00001955
Iteration 184/1000 | Loss: 0.00001955
Iteration 185/1000 | Loss: 0.00001955
Iteration 186/1000 | Loss: 0.00001955
Iteration 187/1000 | Loss: 0.00001955
Iteration 188/1000 | Loss: 0.00001955
Iteration 189/1000 | Loss: 0.00001955
Iteration 190/1000 | Loss: 0.00001955
Iteration 191/1000 | Loss: 0.00001955
Iteration 192/1000 | Loss: 0.00001955
Iteration 193/1000 | Loss: 0.00001955
Iteration 194/1000 | Loss: 0.00001955
Iteration 195/1000 | Loss: 0.00001955
Iteration 196/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.9548831915017217e-05, 1.9548831915017217e-05, 1.9548831915017217e-05, 1.9548831915017217e-05, 1.9548831915017217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9548831915017217e-05

Optimization complete. Final v2v error: 3.069652557373047 mm

Highest mean error: 13.000092506408691 mm for frame 74

Lowest mean error: 2.4986369609832764 mm for frame 153

Saving results

Total time: 218.59151697158813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966773
Iteration 2/25 | Loss: 0.00966773
Iteration 3/25 | Loss: 0.00966773
Iteration 4/25 | Loss: 0.00966772
Iteration 5/25 | Loss: 0.00295703
Iteration 6/25 | Loss: 0.00236739
Iteration 7/25 | Loss: 0.00244002
Iteration 8/25 | Loss: 0.00229676
Iteration 9/25 | Loss: 0.00212403
Iteration 10/25 | Loss: 0.00194866
Iteration 11/25 | Loss: 0.00193503
Iteration 12/25 | Loss: 0.00193833
Iteration 13/25 | Loss: 0.00188223
Iteration 14/25 | Loss: 0.00184042
Iteration 15/25 | Loss: 0.00183176
Iteration 16/25 | Loss: 0.00182415
Iteration 17/25 | Loss: 0.00181935
Iteration 18/25 | Loss: 0.00182092
Iteration 19/25 | Loss: 0.00181715
Iteration 20/25 | Loss: 0.00182017
Iteration 21/25 | Loss: 0.00181129
Iteration 22/25 | Loss: 0.00180912
Iteration 23/25 | Loss: 0.00180872
Iteration 24/25 | Loss: 0.00180847
Iteration 25/25 | Loss: 0.00180831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29686654
Iteration 2/25 | Loss: 0.00763730
Iteration 3/25 | Loss: 0.00411765
Iteration 4/25 | Loss: 0.00411765
Iteration 5/25 | Loss: 0.00411765
Iteration 6/25 | Loss: 0.00411764
Iteration 7/25 | Loss: 0.00411764
Iteration 8/25 | Loss: 0.00411764
Iteration 9/25 | Loss: 0.00411764
Iteration 10/25 | Loss: 0.00411764
Iteration 11/25 | Loss: 0.00411764
Iteration 12/25 | Loss: 0.00411764
Iteration 13/25 | Loss: 0.00411764
Iteration 14/25 | Loss: 0.00411764
Iteration 15/25 | Loss: 0.00411764
Iteration 16/25 | Loss: 0.00411764
Iteration 17/25 | Loss: 0.00411764
Iteration 18/25 | Loss: 0.00411764
Iteration 19/25 | Loss: 0.00411764
Iteration 20/25 | Loss: 0.00411764
Iteration 21/25 | Loss: 0.00411764
Iteration 22/25 | Loss: 0.00411764
Iteration 23/25 | Loss: 0.00411764
Iteration 24/25 | Loss: 0.00411764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0041176434606313705, 0.0041176434606313705, 0.0041176434606313705, 0.0041176434606313705, 0.0041176434606313705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0041176434606313705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00411764
Iteration 2/1000 | Loss: 0.00417503
Iteration 3/1000 | Loss: 0.00445202
Iteration 4/1000 | Loss: 0.00244990
Iteration 5/1000 | Loss: 0.00200178
Iteration 6/1000 | Loss: 0.00116181
Iteration 7/1000 | Loss: 0.00092086
Iteration 8/1000 | Loss: 0.00059644
Iteration 9/1000 | Loss: 0.00049143
Iteration 10/1000 | Loss: 0.00064127
Iteration 11/1000 | Loss: 0.00040925
Iteration 12/1000 | Loss: 0.00031301
Iteration 13/1000 | Loss: 0.00033912
Iteration 14/1000 | Loss: 0.00418116
Iteration 15/1000 | Loss: 0.01861836
Iteration 16/1000 | Loss: 0.00402361
Iteration 17/1000 | Loss: 0.00322687
Iteration 18/1000 | Loss: 0.00047674
Iteration 19/1000 | Loss: 0.00042493
Iteration 20/1000 | Loss: 0.00035645
Iteration 21/1000 | Loss: 0.00304006
Iteration 22/1000 | Loss: 0.00048145
Iteration 23/1000 | Loss: 0.00055954
Iteration 24/1000 | Loss: 0.00133307
Iteration 25/1000 | Loss: 0.00086187
Iteration 26/1000 | Loss: 0.00025518
Iteration 27/1000 | Loss: 0.00052119
Iteration 28/1000 | Loss: 0.00012629
Iteration 29/1000 | Loss: 0.00049651
Iteration 30/1000 | Loss: 0.00092461
Iteration 31/1000 | Loss: 0.00099120
Iteration 32/1000 | Loss: 0.00404544
Iteration 33/1000 | Loss: 0.00027283
Iteration 34/1000 | Loss: 0.00017446
Iteration 35/1000 | Loss: 0.00008447
Iteration 36/1000 | Loss: 0.00006360
Iteration 37/1000 | Loss: 0.00008411
Iteration 38/1000 | Loss: 0.00025591
Iteration 39/1000 | Loss: 0.00003725
Iteration 40/1000 | Loss: 0.00138842
Iteration 41/1000 | Loss: 0.00008162
Iteration 42/1000 | Loss: 0.00140569
Iteration 43/1000 | Loss: 0.00042652
Iteration 44/1000 | Loss: 0.00092904
Iteration 45/1000 | Loss: 0.00419529
Iteration 46/1000 | Loss: 0.00008706
Iteration 47/1000 | Loss: 0.00034342
Iteration 48/1000 | Loss: 0.00173336
Iteration 49/1000 | Loss: 0.00020227
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00025412
Iteration 52/1000 | Loss: 0.00035502
Iteration 53/1000 | Loss: 0.00008488
Iteration 54/1000 | Loss: 0.00004155
Iteration 55/1000 | Loss: 0.00012149
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00027024
Iteration 58/1000 | Loss: 0.00002514
Iteration 59/1000 | Loss: 0.00002617
Iteration 60/1000 | Loss: 0.00006988
Iteration 61/1000 | Loss: 0.00015369
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001332
Iteration 64/1000 | Loss: 0.00005013
Iteration 65/1000 | Loss: 0.00072968
Iteration 66/1000 | Loss: 0.00004355
Iteration 67/1000 | Loss: 0.00007550
Iteration 68/1000 | Loss: 0.00002831
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00005925
Iteration 72/1000 | Loss: 0.00011382
Iteration 73/1000 | Loss: 0.00003820
Iteration 74/1000 | Loss: 0.00002432
Iteration 75/1000 | Loss: 0.00002880
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00006823
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001269
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001268
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00006914
Iteration 89/1000 | Loss: 0.00203702
Iteration 90/1000 | Loss: 0.00003173
Iteration 91/1000 | Loss: 0.00003403
Iteration 92/1000 | Loss: 0.00002556
Iteration 93/1000 | Loss: 0.00009113
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001261
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001258
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001252
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001250
Iteration 111/1000 | Loss: 0.00001250
Iteration 112/1000 | Loss: 0.00001250
Iteration 113/1000 | Loss: 0.00001250
Iteration 114/1000 | Loss: 0.00001250
Iteration 115/1000 | Loss: 0.00001250
Iteration 116/1000 | Loss: 0.00001250
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001249
Iteration 121/1000 | Loss: 0.00001249
Iteration 122/1000 | Loss: 0.00001249
Iteration 123/1000 | Loss: 0.00001249
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001249
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Iteration 138/1000 | Loss: 0.00001248
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001248
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001248
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2481933481467422e-05, 1.2481933481467422e-05, 1.2481933481467422e-05, 1.2481933481467422e-05, 1.2481933481467422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2481933481467422e-05

Optimization complete. Final v2v error: 3.044480323791504 mm

Highest mean error: 3.5542714595794678 mm for frame 0

Lowest mean error: 2.8697798252105713 mm for frame 32

Saving results

Total time: 185.60772323608398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494698
Iteration 2/25 | Loss: 0.00129874
Iteration 3/25 | Loss: 0.00113837
Iteration 4/25 | Loss: 0.00111193
Iteration 5/25 | Loss: 0.00110594
Iteration 6/25 | Loss: 0.00110513
Iteration 7/25 | Loss: 0.00110513
Iteration 8/25 | Loss: 0.00110513
Iteration 9/25 | Loss: 0.00110513
Iteration 10/25 | Loss: 0.00110513
Iteration 11/25 | Loss: 0.00110513
Iteration 12/25 | Loss: 0.00110513
Iteration 13/25 | Loss: 0.00110513
Iteration 14/25 | Loss: 0.00110513
Iteration 15/25 | Loss: 0.00110513
Iteration 16/25 | Loss: 0.00110513
Iteration 17/25 | Loss: 0.00110513
Iteration 18/25 | Loss: 0.00110513
Iteration 19/25 | Loss: 0.00110513
Iteration 20/25 | Loss: 0.00110513
Iteration 21/25 | Loss: 0.00110513
Iteration 22/25 | Loss: 0.00110513
Iteration 23/25 | Loss: 0.00110513
Iteration 24/25 | Loss: 0.00110513
Iteration 25/25 | Loss: 0.00110513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001105127390474081, 0.001105127390474081, 0.001105127390474081, 0.001105127390474081, 0.001105127390474081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105127390474081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.57331610
Iteration 2/25 | Loss: 0.00080220
Iteration 3/25 | Loss: 0.00080217
Iteration 4/25 | Loss: 0.00080217
Iteration 5/25 | Loss: 0.00080217
Iteration 6/25 | Loss: 0.00080217
Iteration 7/25 | Loss: 0.00080217
Iteration 8/25 | Loss: 0.00080217
Iteration 9/25 | Loss: 0.00080217
Iteration 10/25 | Loss: 0.00080217
Iteration 11/25 | Loss: 0.00080217
Iteration 12/25 | Loss: 0.00080217
Iteration 13/25 | Loss: 0.00080217
Iteration 14/25 | Loss: 0.00080217
Iteration 15/25 | Loss: 0.00080217
Iteration 16/25 | Loss: 0.00080217
Iteration 17/25 | Loss: 0.00080217
Iteration 18/25 | Loss: 0.00080217
Iteration 19/25 | Loss: 0.00080217
Iteration 20/25 | Loss: 0.00080217
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008021658868528903, 0.0008021658868528903, 0.0008021658868528903, 0.0008021658868528903, 0.0008021658868528903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008021658868528903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080217
Iteration 2/1000 | Loss: 0.00002577
Iteration 3/1000 | Loss: 0.00001689
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001414
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001240
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001237
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001215
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001209
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001209
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001199
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001198
Iteration 135/1000 | Loss: 0.00001198
Iteration 136/1000 | Loss: 0.00001198
Iteration 137/1000 | Loss: 0.00001198
Iteration 138/1000 | Loss: 0.00001198
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001197
Iteration 143/1000 | Loss: 0.00001197
Iteration 144/1000 | Loss: 0.00001197
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001196
Iteration 149/1000 | Loss: 0.00001196
Iteration 150/1000 | Loss: 0.00001196
Iteration 151/1000 | Loss: 0.00001196
Iteration 152/1000 | Loss: 0.00001196
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001195
Iteration 156/1000 | Loss: 0.00001195
Iteration 157/1000 | Loss: 0.00001195
Iteration 158/1000 | Loss: 0.00001195
Iteration 159/1000 | Loss: 0.00001195
Iteration 160/1000 | Loss: 0.00001195
Iteration 161/1000 | Loss: 0.00001195
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1952582099183928e-05, 1.1952582099183928e-05, 1.1952582099183928e-05, 1.1952582099183928e-05, 1.1952582099183928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1952582099183928e-05

Optimization complete. Final v2v error: 2.9450368881225586 mm

Highest mean error: 3.480886459350586 mm for frame 94

Lowest mean error: 2.4284422397613525 mm for frame 1

Saving results

Total time: 43.85056757926941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984285
Iteration 2/25 | Loss: 0.00178960
Iteration 3/25 | Loss: 0.00139765
Iteration 4/25 | Loss: 0.00136936
Iteration 5/25 | Loss: 0.00137335
Iteration 6/25 | Loss: 0.00128626
Iteration 7/25 | Loss: 0.00125982
Iteration 8/25 | Loss: 0.00123305
Iteration 9/25 | Loss: 0.00121865
Iteration 10/25 | Loss: 0.00120702
Iteration 11/25 | Loss: 0.00119451
Iteration 12/25 | Loss: 0.00119212
Iteration 13/25 | Loss: 0.00118761
Iteration 14/25 | Loss: 0.00118351
Iteration 15/25 | Loss: 0.00117983
Iteration 16/25 | Loss: 0.00118051
Iteration 17/25 | Loss: 0.00117978
Iteration 18/25 | Loss: 0.00117847
Iteration 19/25 | Loss: 0.00117775
Iteration 20/25 | Loss: 0.00117756
Iteration 21/25 | Loss: 0.00117753
Iteration 22/25 | Loss: 0.00117753
Iteration 23/25 | Loss: 0.00117753
Iteration 24/25 | Loss: 0.00117752
Iteration 25/25 | Loss: 0.00117752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33393955
Iteration 2/25 | Loss: 0.00092433
Iteration 3/25 | Loss: 0.00090588
Iteration 4/25 | Loss: 0.00090588
Iteration 5/25 | Loss: 0.00090588
Iteration 6/25 | Loss: 0.00090588
Iteration 7/25 | Loss: 0.00090588
Iteration 8/25 | Loss: 0.00090588
Iteration 9/25 | Loss: 0.00090588
Iteration 10/25 | Loss: 0.00090588
Iteration 11/25 | Loss: 0.00090588
Iteration 12/25 | Loss: 0.00090588
Iteration 13/25 | Loss: 0.00090588
Iteration 14/25 | Loss: 0.00090588
Iteration 15/25 | Loss: 0.00090588
Iteration 16/25 | Loss: 0.00090588
Iteration 17/25 | Loss: 0.00090588
Iteration 18/25 | Loss: 0.00090588
Iteration 19/25 | Loss: 0.00090588
Iteration 20/25 | Loss: 0.00090588
Iteration 21/25 | Loss: 0.00090588
Iteration 22/25 | Loss: 0.00090588
Iteration 23/25 | Loss: 0.00090588
Iteration 24/25 | Loss: 0.00090588
Iteration 25/25 | Loss: 0.00090588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090588
Iteration 2/1000 | Loss: 0.00013983
Iteration 3/1000 | Loss: 0.00004635
Iteration 4/1000 | Loss: 0.00011473
Iteration 5/1000 | Loss: 0.00007627
Iteration 6/1000 | Loss: 0.00005724
Iteration 7/1000 | Loss: 0.00007855
Iteration 8/1000 | Loss: 0.00020109
Iteration 9/1000 | Loss: 0.00005711
Iteration 10/1000 | Loss: 0.00005873
Iteration 11/1000 | Loss: 0.00003071
Iteration 12/1000 | Loss: 0.00002991
Iteration 13/1000 | Loss: 0.00002885
Iteration 14/1000 | Loss: 0.00041179
Iteration 15/1000 | Loss: 0.00011292
Iteration 16/1000 | Loss: 0.00002869
Iteration 17/1000 | Loss: 0.00006324
Iteration 18/1000 | Loss: 0.00071932
Iteration 19/1000 | Loss: 0.00029657
Iteration 20/1000 | Loss: 0.00046785
Iteration 21/1000 | Loss: 0.00026425
Iteration 22/1000 | Loss: 0.00010447
Iteration 23/1000 | Loss: 0.00002744
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00002395
Iteration 26/1000 | Loss: 0.00007853
Iteration 27/1000 | Loss: 0.00007251
Iteration 28/1000 | Loss: 0.00002270
Iteration 29/1000 | Loss: 0.00002230
Iteration 30/1000 | Loss: 0.00002209
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00008139
Iteration 33/1000 | Loss: 0.00002186
Iteration 34/1000 | Loss: 0.00002169
Iteration 35/1000 | Loss: 0.00002169
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002161
Iteration 39/1000 | Loss: 0.00007154
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002147
Iteration 42/1000 | Loss: 0.00002147
Iteration 43/1000 | Loss: 0.00002147
Iteration 44/1000 | Loss: 0.00002146
Iteration 45/1000 | Loss: 0.00002146
Iteration 46/1000 | Loss: 0.00002146
Iteration 47/1000 | Loss: 0.00002146
Iteration 48/1000 | Loss: 0.00002145
Iteration 49/1000 | Loss: 0.00002144
Iteration 50/1000 | Loss: 0.00002144
Iteration 51/1000 | Loss: 0.00002143
Iteration 52/1000 | Loss: 0.00002143
Iteration 53/1000 | Loss: 0.00002141
Iteration 54/1000 | Loss: 0.00002140
Iteration 55/1000 | Loss: 0.00002139
Iteration 56/1000 | Loss: 0.00002139
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002131
Iteration 59/1000 | Loss: 0.00002130
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002129
Iteration 62/1000 | Loss: 0.00002129
Iteration 63/1000 | Loss: 0.00002129
Iteration 64/1000 | Loss: 0.00002128
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002127
Iteration 68/1000 | Loss: 0.00002127
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002126
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002126
Iteration 74/1000 | Loss: 0.00002126
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002125
Iteration 78/1000 | Loss: 0.00002125
Iteration 79/1000 | Loss: 0.00002124
Iteration 80/1000 | Loss: 0.00002124
Iteration 81/1000 | Loss: 0.00002123
Iteration 82/1000 | Loss: 0.00002123
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002123
Iteration 85/1000 | Loss: 0.00002123
Iteration 86/1000 | Loss: 0.00002123
Iteration 87/1000 | Loss: 0.00002122
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002121
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002120
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002119
Iteration 102/1000 | Loss: 0.00002119
Iteration 103/1000 | Loss: 0.00002119
Iteration 104/1000 | Loss: 0.00002119
Iteration 105/1000 | Loss: 0.00002119
Iteration 106/1000 | Loss: 0.00002119
Iteration 107/1000 | Loss: 0.00002119
Iteration 108/1000 | Loss: 0.00002119
Iteration 109/1000 | Loss: 0.00002119
Iteration 110/1000 | Loss: 0.00002119
Iteration 111/1000 | Loss: 0.00002119
Iteration 112/1000 | Loss: 0.00002119
Iteration 113/1000 | Loss: 0.00002119
Iteration 114/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.1188681785133667e-05, 2.1188681785133667e-05, 2.1188681785133667e-05, 2.1188681785133667e-05, 2.1188681785133667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1188681785133667e-05

Optimization complete. Final v2v error: 3.8853957653045654 mm

Highest mean error: 4.834059238433838 mm for frame 90

Lowest mean error: 3.313897132873535 mm for frame 22

Saving results

Total time: 103.61790609359741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010394
Iteration 2/25 | Loss: 0.00189670
Iteration 3/25 | Loss: 0.00146545
Iteration 4/25 | Loss: 0.00134290
Iteration 5/25 | Loss: 0.00134806
Iteration 6/25 | Loss: 0.00133475
Iteration 7/25 | Loss: 0.00128583
Iteration 8/25 | Loss: 0.00127250
Iteration 9/25 | Loss: 0.00124939
Iteration 10/25 | Loss: 0.00123986
Iteration 11/25 | Loss: 0.00122800
Iteration 12/25 | Loss: 0.00122241
Iteration 13/25 | Loss: 0.00122095
Iteration 14/25 | Loss: 0.00121550
Iteration 15/25 | Loss: 0.00120666
Iteration 16/25 | Loss: 0.00119948
Iteration 17/25 | Loss: 0.00119137
Iteration 18/25 | Loss: 0.00119839
Iteration 19/25 | Loss: 0.00119271
Iteration 20/25 | Loss: 0.00117641
Iteration 21/25 | Loss: 0.00116787
Iteration 22/25 | Loss: 0.00116678
Iteration 23/25 | Loss: 0.00116437
Iteration 24/25 | Loss: 0.00116449
Iteration 25/25 | Loss: 0.00115747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38128638
Iteration 2/25 | Loss: 0.00094126
Iteration 3/25 | Loss: 0.00094126
Iteration 4/25 | Loss: 0.00094126
Iteration 5/25 | Loss: 0.00094126
Iteration 6/25 | Loss: 0.00094126
Iteration 7/25 | Loss: 0.00094126
Iteration 8/25 | Loss: 0.00094126
Iteration 9/25 | Loss: 0.00094126
Iteration 10/25 | Loss: 0.00094126
Iteration 11/25 | Loss: 0.00094126
Iteration 12/25 | Loss: 0.00094126
Iteration 13/25 | Loss: 0.00094126
Iteration 14/25 | Loss: 0.00094126
Iteration 15/25 | Loss: 0.00094126
Iteration 16/25 | Loss: 0.00094126
Iteration 17/25 | Loss: 0.00094126
Iteration 18/25 | Loss: 0.00094126
Iteration 19/25 | Loss: 0.00094126
Iteration 20/25 | Loss: 0.00094126
Iteration 21/25 | Loss: 0.00094126
Iteration 22/25 | Loss: 0.00094126
Iteration 23/25 | Loss: 0.00094126
Iteration 24/25 | Loss: 0.00094126
Iteration 25/25 | Loss: 0.00094126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094126
Iteration 2/1000 | Loss: 0.00005534
Iteration 3/1000 | Loss: 0.00005599
Iteration 4/1000 | Loss: 0.00015255
Iteration 5/1000 | Loss: 0.00023372
Iteration 6/1000 | Loss: 0.00020722
Iteration 7/1000 | Loss: 0.00004544
Iteration 8/1000 | Loss: 0.00015928
Iteration 9/1000 | Loss: 0.00017314
Iteration 10/1000 | Loss: 0.00024499
Iteration 11/1000 | Loss: 0.00033133
Iteration 12/1000 | Loss: 0.00030986
Iteration 13/1000 | Loss: 0.00037567
Iteration 14/1000 | Loss: 0.00030735
Iteration 15/1000 | Loss: 0.00025862
Iteration 16/1000 | Loss: 0.00028171
Iteration 17/1000 | Loss: 0.00026490
Iteration 18/1000 | Loss: 0.00027929
Iteration 19/1000 | Loss: 0.00024928
Iteration 20/1000 | Loss: 0.00031671
Iteration 21/1000 | Loss: 0.00035673
Iteration 22/1000 | Loss: 0.00035978
Iteration 23/1000 | Loss: 0.00045087
Iteration 24/1000 | Loss: 0.00071951
Iteration 25/1000 | Loss: 0.00110277
Iteration 26/1000 | Loss: 0.00070153
Iteration 27/1000 | Loss: 0.00004450
Iteration 28/1000 | Loss: 0.00003270
Iteration 29/1000 | Loss: 0.00007797
Iteration 30/1000 | Loss: 0.00003180
Iteration 31/1000 | Loss: 0.00002637
Iteration 32/1000 | Loss: 0.00002371
Iteration 33/1000 | Loss: 0.00002206
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00030833
Iteration 38/1000 | Loss: 0.00023545
Iteration 39/1000 | Loss: 0.00026323
Iteration 40/1000 | Loss: 0.00027972
Iteration 41/1000 | Loss: 0.00024076
Iteration 42/1000 | Loss: 0.00024131
Iteration 43/1000 | Loss: 0.00070671
Iteration 44/1000 | Loss: 0.00005089
Iteration 45/1000 | Loss: 0.00002813
Iteration 46/1000 | Loss: 0.00002458
Iteration 47/1000 | Loss: 0.00002254
Iteration 48/1000 | Loss: 0.00002144
Iteration 49/1000 | Loss: 0.00002086
Iteration 50/1000 | Loss: 0.00001983
Iteration 51/1000 | Loss: 0.00001893
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001804
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001746
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001743
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001743
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001740
Iteration 117/1000 | Loss: 0.00001740
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001739
Iteration 125/1000 | Loss: 0.00001739
Iteration 126/1000 | Loss: 0.00001739
Iteration 127/1000 | Loss: 0.00001739
Iteration 128/1000 | Loss: 0.00001739
Iteration 129/1000 | Loss: 0.00001739
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001738
Iteration 132/1000 | Loss: 0.00001738
Iteration 133/1000 | Loss: 0.00001738
Iteration 134/1000 | Loss: 0.00001738
Iteration 135/1000 | Loss: 0.00001738
Iteration 136/1000 | Loss: 0.00001738
Iteration 137/1000 | Loss: 0.00001738
Iteration 138/1000 | Loss: 0.00001738
Iteration 139/1000 | Loss: 0.00001738
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001737
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001737
Iteration 146/1000 | Loss: 0.00001737
Iteration 147/1000 | Loss: 0.00001737
Iteration 148/1000 | Loss: 0.00001737
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001736
Iteration 157/1000 | Loss: 0.00001736
Iteration 158/1000 | Loss: 0.00001736
Iteration 159/1000 | Loss: 0.00001736
Iteration 160/1000 | Loss: 0.00001736
Iteration 161/1000 | Loss: 0.00001736
Iteration 162/1000 | Loss: 0.00001736
Iteration 163/1000 | Loss: 0.00001736
Iteration 164/1000 | Loss: 0.00001735
Iteration 165/1000 | Loss: 0.00001735
Iteration 166/1000 | Loss: 0.00001735
Iteration 167/1000 | Loss: 0.00001735
Iteration 168/1000 | Loss: 0.00001735
Iteration 169/1000 | Loss: 0.00001735
Iteration 170/1000 | Loss: 0.00001735
Iteration 171/1000 | Loss: 0.00001735
Iteration 172/1000 | Loss: 0.00001735
Iteration 173/1000 | Loss: 0.00001735
Iteration 174/1000 | Loss: 0.00001735
Iteration 175/1000 | Loss: 0.00001735
Iteration 176/1000 | Loss: 0.00001735
Iteration 177/1000 | Loss: 0.00001735
Iteration 178/1000 | Loss: 0.00001735
Iteration 179/1000 | Loss: 0.00001735
Iteration 180/1000 | Loss: 0.00001735
Iteration 181/1000 | Loss: 0.00001735
Iteration 182/1000 | Loss: 0.00001735
Iteration 183/1000 | Loss: 0.00001735
Iteration 184/1000 | Loss: 0.00001735
Iteration 185/1000 | Loss: 0.00001735
Iteration 186/1000 | Loss: 0.00001735
Iteration 187/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.734920806484297e-05, 1.734920806484297e-05, 1.734920806484297e-05, 1.734920806484297e-05, 1.734920806484297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.734920806484297e-05

Optimization complete. Final v2v error: 3.425069570541382 mm

Highest mean error: 6.5899200439453125 mm for frame 110

Lowest mean error: 2.8791117668151855 mm for frame 109

Saving results

Total time: 129.92191886901855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948044
Iteration 2/25 | Loss: 0.00303137
Iteration 3/25 | Loss: 0.00231808
Iteration 4/25 | Loss: 0.00200486
Iteration 5/25 | Loss: 0.00188495
Iteration 6/25 | Loss: 0.00179832
Iteration 7/25 | Loss: 0.00177647
Iteration 8/25 | Loss: 0.00178004
Iteration 9/25 | Loss: 0.00166619
Iteration 10/25 | Loss: 0.00162532
Iteration 11/25 | Loss: 0.00158542
Iteration 12/25 | Loss: 0.00155293
Iteration 13/25 | Loss: 0.00151985
Iteration 14/25 | Loss: 0.00151119
Iteration 15/25 | Loss: 0.00149111
Iteration 16/25 | Loss: 0.00147983
Iteration 17/25 | Loss: 0.00148428
Iteration 18/25 | Loss: 0.00147816
Iteration 19/25 | Loss: 0.00147318
Iteration 20/25 | Loss: 0.00147306
Iteration 21/25 | Loss: 0.00146794
Iteration 22/25 | Loss: 0.00147593
Iteration 23/25 | Loss: 0.00147954
Iteration 24/25 | Loss: 0.00148303
Iteration 25/25 | Loss: 0.00147225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32552493
Iteration 2/25 | Loss: 0.00350883
Iteration 3/25 | Loss: 0.00311098
Iteration 4/25 | Loss: 0.00311098
Iteration 5/25 | Loss: 0.00311098
Iteration 6/25 | Loss: 0.00311098
Iteration 7/25 | Loss: 0.00311098
Iteration 8/25 | Loss: 0.00311098
Iteration 9/25 | Loss: 0.00311098
Iteration 10/25 | Loss: 0.00311098
Iteration 11/25 | Loss: 0.00311098
Iteration 12/25 | Loss: 0.00311098
Iteration 13/25 | Loss: 0.00311098
Iteration 14/25 | Loss: 0.00311098
Iteration 15/25 | Loss: 0.00311098
Iteration 16/25 | Loss: 0.00311098
Iteration 17/25 | Loss: 0.00311098
Iteration 18/25 | Loss: 0.00311098
Iteration 19/25 | Loss: 0.00311098
Iteration 20/25 | Loss: 0.00311098
Iteration 21/25 | Loss: 0.00311098
Iteration 22/25 | Loss: 0.00311098
Iteration 23/25 | Loss: 0.00311098
Iteration 24/25 | Loss: 0.00311098
Iteration 25/25 | Loss: 0.00311098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311098
Iteration 2/1000 | Loss: 0.00084490
Iteration 3/1000 | Loss: 0.00067134
Iteration 4/1000 | Loss: 0.00082972
Iteration 5/1000 | Loss: 0.00025739
Iteration 6/1000 | Loss: 0.00024734
Iteration 7/1000 | Loss: 0.00046709
Iteration 8/1000 | Loss: 0.00067605
Iteration 9/1000 | Loss: 0.00046588
Iteration 10/1000 | Loss: 0.00153015
Iteration 11/1000 | Loss: 0.00024769
Iteration 12/1000 | Loss: 0.00022428
Iteration 13/1000 | Loss: 0.00050939
Iteration 14/1000 | Loss: 0.00079140
Iteration 15/1000 | Loss: 0.00028111
Iteration 16/1000 | Loss: 0.00030775
Iteration 17/1000 | Loss: 0.00039066
Iteration 18/1000 | Loss: 0.00050647
Iteration 19/1000 | Loss: 0.00017189
Iteration 20/1000 | Loss: 0.00013668
Iteration 21/1000 | Loss: 0.00038753
Iteration 22/1000 | Loss: 0.00013010
Iteration 23/1000 | Loss: 0.00012702
Iteration 24/1000 | Loss: 0.00038982
Iteration 25/1000 | Loss: 0.00020101
Iteration 26/1000 | Loss: 0.00012916
Iteration 27/1000 | Loss: 0.00012265
Iteration 28/1000 | Loss: 0.00017577
Iteration 29/1000 | Loss: 0.00011819
Iteration 30/1000 | Loss: 0.00014839
Iteration 31/1000 | Loss: 0.00011659
Iteration 32/1000 | Loss: 0.00011509
Iteration 33/1000 | Loss: 0.00011447
Iteration 34/1000 | Loss: 0.00028077
Iteration 35/1000 | Loss: 0.00013689
Iteration 36/1000 | Loss: 0.00011936
Iteration 37/1000 | Loss: 0.00011270
Iteration 38/1000 | Loss: 0.00011224
Iteration 39/1000 | Loss: 0.00011183
Iteration 40/1000 | Loss: 0.00053258
Iteration 41/1000 | Loss: 0.00328002
Iteration 42/1000 | Loss: 0.00189493
Iteration 43/1000 | Loss: 0.00296587
Iteration 44/1000 | Loss: 0.00629687
Iteration 45/1000 | Loss: 0.00704112
Iteration 46/1000 | Loss: 0.00131924
Iteration 47/1000 | Loss: 0.00066272
Iteration 48/1000 | Loss: 0.00152774
Iteration 49/1000 | Loss: 0.00066101
Iteration 50/1000 | Loss: 0.00018204
Iteration 51/1000 | Loss: 0.00042309
Iteration 52/1000 | Loss: 0.00033953
Iteration 53/1000 | Loss: 0.00018922
Iteration 54/1000 | Loss: 0.00020199
Iteration 55/1000 | Loss: 0.00035965
Iteration 56/1000 | Loss: 0.00055863
Iteration 57/1000 | Loss: 0.00036786
Iteration 58/1000 | Loss: 0.00010569
Iteration 59/1000 | Loss: 0.00013564
Iteration 60/1000 | Loss: 0.00009546
Iteration 61/1000 | Loss: 0.00013248
Iteration 62/1000 | Loss: 0.00005907
Iteration 63/1000 | Loss: 0.00074058
Iteration 64/1000 | Loss: 0.00011000
Iteration 65/1000 | Loss: 0.00045300
Iteration 66/1000 | Loss: 0.00060653
Iteration 67/1000 | Loss: 0.00093985
Iteration 68/1000 | Loss: 0.00028749
Iteration 69/1000 | Loss: 0.00069971
Iteration 70/1000 | Loss: 0.00023924
Iteration 71/1000 | Loss: 0.00005018
Iteration 72/1000 | Loss: 0.00004710
Iteration 73/1000 | Loss: 0.00053708
Iteration 74/1000 | Loss: 0.00027478
Iteration 75/1000 | Loss: 0.00076128
Iteration 76/1000 | Loss: 0.00033103
Iteration 77/1000 | Loss: 0.00006435
Iteration 78/1000 | Loss: 0.00033182
Iteration 79/1000 | Loss: 0.00004393
Iteration 80/1000 | Loss: 0.00004098
Iteration 81/1000 | Loss: 0.00003967
Iteration 82/1000 | Loss: 0.00003859
Iteration 83/1000 | Loss: 0.00003758
Iteration 84/1000 | Loss: 0.00003687
Iteration 85/1000 | Loss: 0.00003632
Iteration 86/1000 | Loss: 0.00023394
Iteration 87/1000 | Loss: 0.00004548
Iteration 88/1000 | Loss: 0.00003597
Iteration 89/1000 | Loss: 0.00003572
Iteration 90/1000 | Loss: 0.00003571
Iteration 91/1000 | Loss: 0.00003571
Iteration 92/1000 | Loss: 0.00003570
Iteration 93/1000 | Loss: 0.00003568
Iteration 94/1000 | Loss: 0.00003568
Iteration 95/1000 | Loss: 0.00003567
Iteration 96/1000 | Loss: 0.00003557
Iteration 97/1000 | Loss: 0.00003551
Iteration 98/1000 | Loss: 0.00003549
Iteration 99/1000 | Loss: 0.00003548
Iteration 100/1000 | Loss: 0.00016015
Iteration 101/1000 | Loss: 0.00003611
Iteration 102/1000 | Loss: 0.00003535
Iteration 103/1000 | Loss: 0.00003533
Iteration 104/1000 | Loss: 0.00003530
Iteration 105/1000 | Loss: 0.00003529
Iteration 106/1000 | Loss: 0.00003529
Iteration 107/1000 | Loss: 0.00003528
Iteration 108/1000 | Loss: 0.00003528
Iteration 109/1000 | Loss: 0.00003528
Iteration 110/1000 | Loss: 0.00003528
Iteration 111/1000 | Loss: 0.00003527
Iteration 112/1000 | Loss: 0.00003527
Iteration 113/1000 | Loss: 0.00003527
Iteration 114/1000 | Loss: 0.00003526
Iteration 115/1000 | Loss: 0.00003526
Iteration 116/1000 | Loss: 0.00003526
Iteration 117/1000 | Loss: 0.00003525
Iteration 118/1000 | Loss: 0.00003525
Iteration 119/1000 | Loss: 0.00003525
Iteration 120/1000 | Loss: 0.00003525
Iteration 121/1000 | Loss: 0.00003525
Iteration 122/1000 | Loss: 0.00003524
Iteration 123/1000 | Loss: 0.00003524
Iteration 124/1000 | Loss: 0.00003523
Iteration 125/1000 | Loss: 0.00003523
Iteration 126/1000 | Loss: 0.00003523
Iteration 127/1000 | Loss: 0.00003523
Iteration 128/1000 | Loss: 0.00003523
Iteration 129/1000 | Loss: 0.00003523
Iteration 130/1000 | Loss: 0.00003522
Iteration 131/1000 | Loss: 0.00003522
Iteration 132/1000 | Loss: 0.00003522
Iteration 133/1000 | Loss: 0.00003522
Iteration 134/1000 | Loss: 0.00003522
Iteration 135/1000 | Loss: 0.00003522
Iteration 136/1000 | Loss: 0.00003522
Iteration 137/1000 | Loss: 0.00003522
Iteration 138/1000 | Loss: 0.00003522
Iteration 139/1000 | Loss: 0.00003522
Iteration 140/1000 | Loss: 0.00003522
Iteration 141/1000 | Loss: 0.00003522
Iteration 142/1000 | Loss: 0.00003522
Iteration 143/1000 | Loss: 0.00003522
Iteration 144/1000 | Loss: 0.00003521
Iteration 145/1000 | Loss: 0.00003521
Iteration 146/1000 | Loss: 0.00003521
Iteration 147/1000 | Loss: 0.00003521
Iteration 148/1000 | Loss: 0.00003520
Iteration 149/1000 | Loss: 0.00003520
Iteration 150/1000 | Loss: 0.00003520
Iteration 151/1000 | Loss: 0.00014101
Iteration 152/1000 | Loss: 0.00014101
Iteration 153/1000 | Loss: 0.00008221
Iteration 154/1000 | Loss: 0.00003519
Iteration 155/1000 | Loss: 0.00003518
Iteration 156/1000 | Loss: 0.00003517
Iteration 157/1000 | Loss: 0.00003517
Iteration 158/1000 | Loss: 0.00003517
Iteration 159/1000 | Loss: 0.00003517
Iteration 160/1000 | Loss: 0.00003517
Iteration 161/1000 | Loss: 0.00003516
Iteration 162/1000 | Loss: 0.00003516
Iteration 163/1000 | Loss: 0.00003516
Iteration 164/1000 | Loss: 0.00003516
Iteration 165/1000 | Loss: 0.00003516
Iteration 166/1000 | Loss: 0.00003516
Iteration 167/1000 | Loss: 0.00003516
Iteration 168/1000 | Loss: 0.00003516
Iteration 169/1000 | Loss: 0.00003516
Iteration 170/1000 | Loss: 0.00003516
Iteration 171/1000 | Loss: 0.00003516
Iteration 172/1000 | Loss: 0.00003516
Iteration 173/1000 | Loss: 0.00003516
Iteration 174/1000 | Loss: 0.00003516
Iteration 175/1000 | Loss: 0.00003515
Iteration 176/1000 | Loss: 0.00003515
Iteration 177/1000 | Loss: 0.00003515
Iteration 178/1000 | Loss: 0.00003515
Iteration 179/1000 | Loss: 0.00003515
Iteration 180/1000 | Loss: 0.00003515
Iteration 181/1000 | Loss: 0.00003515
Iteration 182/1000 | Loss: 0.00003515
Iteration 183/1000 | Loss: 0.00003515
Iteration 184/1000 | Loss: 0.00003515
Iteration 185/1000 | Loss: 0.00003515
Iteration 186/1000 | Loss: 0.00003515
Iteration 187/1000 | Loss: 0.00003515
Iteration 188/1000 | Loss: 0.00003515
Iteration 189/1000 | Loss: 0.00003515
Iteration 190/1000 | Loss: 0.00003515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [3.515428397804499e-05, 3.515428397804499e-05, 3.515428397804499e-05, 3.515428397804499e-05, 3.515428397804499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.515428397804499e-05

Optimization complete. Final v2v error: 3.5967578887939453 mm

Highest mean error: 12.235610961914062 mm for frame 27

Lowest mean error: 2.671088457107544 mm for frame 14

Saving results

Total time: 183.43682670593262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883212
Iteration 2/25 | Loss: 0.00155462
Iteration 3/25 | Loss: 0.00128795
Iteration 4/25 | Loss: 0.00124603
Iteration 5/25 | Loss: 0.00124100
Iteration 6/25 | Loss: 0.00123601
Iteration 7/25 | Loss: 0.00122519
Iteration 8/25 | Loss: 0.00120884
Iteration 9/25 | Loss: 0.00119062
Iteration 10/25 | Loss: 0.00118603
Iteration 11/25 | Loss: 0.00117900
Iteration 12/25 | Loss: 0.00118036
Iteration 13/25 | Loss: 0.00118148
Iteration 14/25 | Loss: 0.00118677
Iteration 15/25 | Loss: 0.00118293
Iteration 16/25 | Loss: 0.00117707
Iteration 17/25 | Loss: 0.00117153
Iteration 18/25 | Loss: 0.00116965
Iteration 19/25 | Loss: 0.00116909
Iteration 20/25 | Loss: 0.00116893
Iteration 21/25 | Loss: 0.00116891
Iteration 22/25 | Loss: 0.00116891
Iteration 23/25 | Loss: 0.00116891
Iteration 24/25 | Loss: 0.00116891
Iteration 25/25 | Loss: 0.00116891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24689722
Iteration 2/25 | Loss: 0.00110023
Iteration 3/25 | Loss: 0.00110023
Iteration 4/25 | Loss: 0.00110023
Iteration 5/25 | Loss: 0.00110023
Iteration 6/25 | Loss: 0.00110023
Iteration 7/25 | Loss: 0.00110022
Iteration 8/25 | Loss: 0.00110022
Iteration 9/25 | Loss: 0.00110022
Iteration 10/25 | Loss: 0.00110022
Iteration 11/25 | Loss: 0.00110022
Iteration 12/25 | Loss: 0.00110022
Iteration 13/25 | Loss: 0.00110022
Iteration 14/25 | Loss: 0.00110022
Iteration 15/25 | Loss: 0.00110022
Iteration 16/25 | Loss: 0.00110022
Iteration 17/25 | Loss: 0.00110022
Iteration 18/25 | Loss: 0.00110022
Iteration 19/25 | Loss: 0.00110022
Iteration 20/25 | Loss: 0.00110022
Iteration 21/25 | Loss: 0.00110022
Iteration 22/25 | Loss: 0.00110022
Iteration 23/25 | Loss: 0.00110022
Iteration 24/25 | Loss: 0.00110022
Iteration 25/25 | Loss: 0.00110022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110022
Iteration 2/1000 | Loss: 0.00013736
Iteration 3/1000 | Loss: 0.00025786
Iteration 4/1000 | Loss: 0.00003140
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00002445
Iteration 7/1000 | Loss: 0.00002333
Iteration 8/1000 | Loss: 0.00002240
Iteration 9/1000 | Loss: 0.00002172
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002084
Iteration 12/1000 | Loss: 0.00002056
Iteration 13/1000 | Loss: 0.00002034
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001969
Iteration 18/1000 | Loss: 0.00001961
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001955
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001954
Iteration 28/1000 | Loss: 0.00001953
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001952
Iteration 32/1000 | Loss: 0.00001952
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00001951
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001950
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001946
Iteration 46/1000 | Loss: 0.00001946
Iteration 47/1000 | Loss: 0.00001946
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001943
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00001942
Iteration 61/1000 | Loss: 0.00001942
Iteration 62/1000 | Loss: 0.00001942
Iteration 63/1000 | Loss: 0.00001942
Iteration 64/1000 | Loss: 0.00001941
Iteration 65/1000 | Loss: 0.00001941
Iteration 66/1000 | Loss: 0.00001941
Iteration 67/1000 | Loss: 0.00001941
Iteration 68/1000 | Loss: 0.00001941
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001934
Iteration 97/1000 | Loss: 0.00001933
Iteration 98/1000 | Loss: 0.00001933
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001933
Iteration 102/1000 | Loss: 0.00001933
Iteration 103/1000 | Loss: 0.00001933
Iteration 104/1000 | Loss: 0.00001933
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001931
Iteration 116/1000 | Loss: 0.00001931
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001930
Iteration 121/1000 | Loss: 0.00001930
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001928
Iteration 129/1000 | Loss: 0.00001928
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001927
Iteration 134/1000 | Loss: 0.00001927
Iteration 135/1000 | Loss: 0.00001927
Iteration 136/1000 | Loss: 0.00001927
Iteration 137/1000 | Loss: 0.00001926
Iteration 138/1000 | Loss: 0.00001926
Iteration 139/1000 | Loss: 0.00001926
Iteration 140/1000 | Loss: 0.00001926
Iteration 141/1000 | Loss: 0.00001926
Iteration 142/1000 | Loss: 0.00001926
Iteration 143/1000 | Loss: 0.00001926
Iteration 144/1000 | Loss: 0.00001926
Iteration 145/1000 | Loss: 0.00001926
Iteration 146/1000 | Loss: 0.00001925
Iteration 147/1000 | Loss: 0.00001925
Iteration 148/1000 | Loss: 0.00001925
Iteration 149/1000 | Loss: 0.00001925
Iteration 150/1000 | Loss: 0.00001925
Iteration 151/1000 | Loss: 0.00001925
Iteration 152/1000 | Loss: 0.00001925
Iteration 153/1000 | Loss: 0.00001925
Iteration 154/1000 | Loss: 0.00001925
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001925
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001925
Iteration 159/1000 | Loss: 0.00001924
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001924
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001924
Iteration 167/1000 | Loss: 0.00001924
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001923
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Iteration 176/1000 | Loss: 0.00001923
Iteration 177/1000 | Loss: 0.00001923
Iteration 178/1000 | Loss: 0.00001923
Iteration 179/1000 | Loss: 0.00001923
Iteration 180/1000 | Loss: 0.00001923
Iteration 181/1000 | Loss: 0.00001923
Iteration 182/1000 | Loss: 0.00001923
Iteration 183/1000 | Loss: 0.00001923
Iteration 184/1000 | Loss: 0.00001922
Iteration 185/1000 | Loss: 0.00001922
Iteration 186/1000 | Loss: 0.00001922
Iteration 187/1000 | Loss: 0.00001922
Iteration 188/1000 | Loss: 0.00001922
Iteration 189/1000 | Loss: 0.00001922
Iteration 190/1000 | Loss: 0.00001922
Iteration 191/1000 | Loss: 0.00001922
Iteration 192/1000 | Loss: 0.00001922
Iteration 193/1000 | Loss: 0.00001922
Iteration 194/1000 | Loss: 0.00001922
Iteration 195/1000 | Loss: 0.00001922
Iteration 196/1000 | Loss: 0.00001922
Iteration 197/1000 | Loss: 0.00001922
Iteration 198/1000 | Loss: 0.00001922
Iteration 199/1000 | Loss: 0.00001922
Iteration 200/1000 | Loss: 0.00001922
Iteration 201/1000 | Loss: 0.00001922
Iteration 202/1000 | Loss: 0.00001922
Iteration 203/1000 | Loss: 0.00001922
Iteration 204/1000 | Loss: 0.00001922
Iteration 205/1000 | Loss: 0.00001922
Iteration 206/1000 | Loss: 0.00001922
Iteration 207/1000 | Loss: 0.00001922
Iteration 208/1000 | Loss: 0.00001922
Iteration 209/1000 | Loss: 0.00001922
Iteration 210/1000 | Loss: 0.00001922
Iteration 211/1000 | Loss: 0.00001922
Iteration 212/1000 | Loss: 0.00001922
Iteration 213/1000 | Loss: 0.00001922
Iteration 214/1000 | Loss: 0.00001922
Iteration 215/1000 | Loss: 0.00001922
Iteration 216/1000 | Loss: 0.00001922
Iteration 217/1000 | Loss: 0.00001922
Iteration 218/1000 | Loss: 0.00001922
Iteration 219/1000 | Loss: 0.00001922
Iteration 220/1000 | Loss: 0.00001922
Iteration 221/1000 | Loss: 0.00001922
Iteration 222/1000 | Loss: 0.00001922
Iteration 223/1000 | Loss: 0.00001922
Iteration 224/1000 | Loss: 0.00001922
Iteration 225/1000 | Loss: 0.00001922
Iteration 226/1000 | Loss: 0.00001922
Iteration 227/1000 | Loss: 0.00001922
Iteration 228/1000 | Loss: 0.00001922
Iteration 229/1000 | Loss: 0.00001922
Iteration 230/1000 | Loss: 0.00001922
Iteration 231/1000 | Loss: 0.00001922
Iteration 232/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.9217130102333613e-05, 1.9217130102333613e-05, 1.9217130102333613e-05, 1.9217130102333613e-05, 1.9217130102333613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9217130102333613e-05

Optimization complete. Final v2v error: 3.6480937004089355 mm

Highest mean error: 5.640748977661133 mm for frame 106

Lowest mean error: 3.0859858989715576 mm for frame 134

Saving results

Total time: 78.30538368225098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444088
Iteration 2/25 | Loss: 0.00118603
Iteration 3/25 | Loss: 0.00108410
Iteration 4/25 | Loss: 0.00106492
Iteration 5/25 | Loss: 0.00105864
Iteration 6/25 | Loss: 0.00105681
Iteration 7/25 | Loss: 0.00105647
Iteration 8/25 | Loss: 0.00105647
Iteration 9/25 | Loss: 0.00105647
Iteration 10/25 | Loss: 0.00105647
Iteration 11/25 | Loss: 0.00105647
Iteration 12/25 | Loss: 0.00105647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010564700933173299, 0.0010564700933173299, 0.0010564700933173299, 0.0010564700933173299, 0.0010564700933173299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010564700933173299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23404896
Iteration 2/25 | Loss: 0.00079953
Iteration 3/25 | Loss: 0.00079950
Iteration 4/25 | Loss: 0.00079950
Iteration 5/25 | Loss: 0.00079950
Iteration 6/25 | Loss: 0.00079950
Iteration 7/25 | Loss: 0.00079950
Iteration 8/25 | Loss: 0.00079950
Iteration 9/25 | Loss: 0.00079950
Iteration 10/25 | Loss: 0.00079950
Iteration 11/25 | Loss: 0.00079950
Iteration 12/25 | Loss: 0.00079950
Iteration 13/25 | Loss: 0.00079950
Iteration 14/25 | Loss: 0.00079950
Iteration 15/25 | Loss: 0.00079950
Iteration 16/25 | Loss: 0.00079950
Iteration 17/25 | Loss: 0.00079950
Iteration 18/25 | Loss: 0.00079950
Iteration 19/25 | Loss: 0.00079950
Iteration 20/25 | Loss: 0.00079950
Iteration 21/25 | Loss: 0.00079950
Iteration 22/25 | Loss: 0.00079950
Iteration 23/25 | Loss: 0.00079950
Iteration 24/25 | Loss: 0.00079950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007994956104084849, 0.0007994956104084849, 0.0007994956104084849, 0.0007994956104084849, 0.0007994956104084849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007994956104084849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079950
Iteration 2/1000 | Loss: 0.00003714
Iteration 3/1000 | Loss: 0.00002196
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001260
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001160
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001127
Iteration 17/1000 | Loss: 0.00001121
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001114
Iteration 20/1000 | Loss: 0.00001114
Iteration 21/1000 | Loss: 0.00001112
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001107
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001099
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001099
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001082
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001074
Iteration 84/1000 | Loss: 0.00001074
Iteration 85/1000 | Loss: 0.00001074
Iteration 86/1000 | Loss: 0.00001074
Iteration 87/1000 | Loss: 0.00001074
Iteration 88/1000 | Loss: 0.00001073
Iteration 89/1000 | Loss: 0.00001073
Iteration 90/1000 | Loss: 0.00001073
Iteration 91/1000 | Loss: 0.00001073
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001072
Iteration 94/1000 | Loss: 0.00001072
Iteration 95/1000 | Loss: 0.00001072
Iteration 96/1000 | Loss: 0.00001072
Iteration 97/1000 | Loss: 0.00001072
Iteration 98/1000 | Loss: 0.00001072
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001071
Iteration 101/1000 | Loss: 0.00001071
Iteration 102/1000 | Loss: 0.00001071
Iteration 103/1000 | Loss: 0.00001071
Iteration 104/1000 | Loss: 0.00001071
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001071
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Iteration 120/1000 | Loss: 0.00001070
Iteration 121/1000 | Loss: 0.00001070
Iteration 122/1000 | Loss: 0.00001070
Iteration 123/1000 | Loss: 0.00001070
Iteration 124/1000 | Loss: 0.00001070
Iteration 125/1000 | Loss: 0.00001070
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001069
Iteration 128/1000 | Loss: 0.00001069
Iteration 129/1000 | Loss: 0.00001069
Iteration 130/1000 | Loss: 0.00001069
Iteration 131/1000 | Loss: 0.00001069
Iteration 132/1000 | Loss: 0.00001069
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001068
Iteration 143/1000 | Loss: 0.00001068
Iteration 144/1000 | Loss: 0.00001068
Iteration 145/1000 | Loss: 0.00001068
Iteration 146/1000 | Loss: 0.00001068
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001068
Iteration 150/1000 | Loss: 0.00001067
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001067
Iteration 157/1000 | Loss: 0.00001066
Iteration 158/1000 | Loss: 0.00001066
Iteration 159/1000 | Loss: 0.00001066
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001066
Iteration 162/1000 | Loss: 0.00001065
Iteration 163/1000 | Loss: 0.00001065
Iteration 164/1000 | Loss: 0.00001065
Iteration 165/1000 | Loss: 0.00001065
Iteration 166/1000 | Loss: 0.00001065
Iteration 167/1000 | Loss: 0.00001065
Iteration 168/1000 | Loss: 0.00001065
Iteration 169/1000 | Loss: 0.00001065
Iteration 170/1000 | Loss: 0.00001064
Iteration 171/1000 | Loss: 0.00001064
Iteration 172/1000 | Loss: 0.00001064
Iteration 173/1000 | Loss: 0.00001064
Iteration 174/1000 | Loss: 0.00001064
Iteration 175/1000 | Loss: 0.00001064
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001063
Iteration 178/1000 | Loss: 0.00001063
Iteration 179/1000 | Loss: 0.00001063
Iteration 180/1000 | Loss: 0.00001063
Iteration 181/1000 | Loss: 0.00001063
Iteration 182/1000 | Loss: 0.00001062
Iteration 183/1000 | Loss: 0.00001062
Iteration 184/1000 | Loss: 0.00001062
Iteration 185/1000 | Loss: 0.00001062
Iteration 186/1000 | Loss: 0.00001062
Iteration 187/1000 | Loss: 0.00001062
Iteration 188/1000 | Loss: 0.00001062
Iteration 189/1000 | Loss: 0.00001062
Iteration 190/1000 | Loss: 0.00001062
Iteration 191/1000 | Loss: 0.00001062
Iteration 192/1000 | Loss: 0.00001062
Iteration 193/1000 | Loss: 0.00001062
Iteration 194/1000 | Loss: 0.00001062
Iteration 195/1000 | Loss: 0.00001062
Iteration 196/1000 | Loss: 0.00001062
Iteration 197/1000 | Loss: 0.00001062
Iteration 198/1000 | Loss: 0.00001062
Iteration 199/1000 | Loss: 0.00001062
Iteration 200/1000 | Loss: 0.00001062
Iteration 201/1000 | Loss: 0.00001062
Iteration 202/1000 | Loss: 0.00001062
Iteration 203/1000 | Loss: 0.00001062
Iteration 204/1000 | Loss: 0.00001062
Iteration 205/1000 | Loss: 0.00001062
Iteration 206/1000 | Loss: 0.00001062
Iteration 207/1000 | Loss: 0.00001062
Iteration 208/1000 | Loss: 0.00001062
Iteration 209/1000 | Loss: 0.00001062
Iteration 210/1000 | Loss: 0.00001062
Iteration 211/1000 | Loss: 0.00001062
Iteration 212/1000 | Loss: 0.00001062
Iteration 213/1000 | Loss: 0.00001062
Iteration 214/1000 | Loss: 0.00001062
Iteration 215/1000 | Loss: 0.00001062
Iteration 216/1000 | Loss: 0.00001062
Iteration 217/1000 | Loss: 0.00001062
Iteration 218/1000 | Loss: 0.00001062
Iteration 219/1000 | Loss: 0.00001062
Iteration 220/1000 | Loss: 0.00001062
Iteration 221/1000 | Loss: 0.00001062
Iteration 222/1000 | Loss: 0.00001062
Iteration 223/1000 | Loss: 0.00001062
Iteration 224/1000 | Loss: 0.00001062
Iteration 225/1000 | Loss: 0.00001062
Iteration 226/1000 | Loss: 0.00001062
Iteration 227/1000 | Loss: 0.00001062
Iteration 228/1000 | Loss: 0.00001062
Iteration 229/1000 | Loss: 0.00001062
Iteration 230/1000 | Loss: 0.00001062
Iteration 231/1000 | Loss: 0.00001062
Iteration 232/1000 | Loss: 0.00001062
Iteration 233/1000 | Loss: 0.00001062
Iteration 234/1000 | Loss: 0.00001062
Iteration 235/1000 | Loss: 0.00001062
Iteration 236/1000 | Loss: 0.00001062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.0619664863042999e-05, 1.0619664863042999e-05, 1.0619664863042999e-05, 1.0619664863042999e-05, 1.0619664863042999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0619664863042999e-05

Optimization complete. Final v2v error: 2.8089616298675537 mm

Highest mean error: 3.295487642288208 mm for frame 57

Lowest mean error: 2.5012738704681396 mm for frame 16

Saving results

Total time: 42.325408697128296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460207
Iteration 2/25 | Loss: 0.00138250
Iteration 3/25 | Loss: 0.00118336
Iteration 4/25 | Loss: 0.00115489
Iteration 5/25 | Loss: 0.00114619
Iteration 6/25 | Loss: 0.00114463
Iteration 7/25 | Loss: 0.00114406
Iteration 8/25 | Loss: 0.00114406
Iteration 9/25 | Loss: 0.00114406
Iteration 10/25 | Loss: 0.00114406
Iteration 11/25 | Loss: 0.00114406
Iteration 12/25 | Loss: 0.00114406
Iteration 13/25 | Loss: 0.00114406
Iteration 14/25 | Loss: 0.00114406
Iteration 15/25 | Loss: 0.00114406
Iteration 16/25 | Loss: 0.00114406
Iteration 17/25 | Loss: 0.00114406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001144062145613134, 0.001144062145613134, 0.001144062145613134, 0.001144062145613134, 0.001144062145613134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001144062145613134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18323410
Iteration 2/25 | Loss: 0.00098692
Iteration 3/25 | Loss: 0.00098690
Iteration 4/25 | Loss: 0.00098690
Iteration 5/25 | Loss: 0.00098690
Iteration 6/25 | Loss: 0.00098690
Iteration 7/25 | Loss: 0.00098690
Iteration 8/25 | Loss: 0.00098690
Iteration 9/25 | Loss: 0.00098690
Iteration 10/25 | Loss: 0.00098690
Iteration 11/25 | Loss: 0.00098690
Iteration 12/25 | Loss: 0.00098690
Iteration 13/25 | Loss: 0.00098690
Iteration 14/25 | Loss: 0.00098690
Iteration 15/25 | Loss: 0.00098690
Iteration 16/25 | Loss: 0.00098690
Iteration 17/25 | Loss: 0.00098690
Iteration 18/25 | Loss: 0.00098690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000986897968687117, 0.000986897968687117, 0.000986897968687117, 0.000986897968687117, 0.000986897968687117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000986897968687117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098690
Iteration 2/1000 | Loss: 0.00006343
Iteration 3/1000 | Loss: 0.00003971
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002793
Iteration 6/1000 | Loss: 0.00002672
Iteration 7/1000 | Loss: 0.00002571
Iteration 8/1000 | Loss: 0.00002499
Iteration 9/1000 | Loss: 0.00002438
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002354
Iteration 12/1000 | Loss: 0.00002333
Iteration 13/1000 | Loss: 0.00002314
Iteration 14/1000 | Loss: 0.00002312
Iteration 15/1000 | Loss: 0.00002302
Iteration 16/1000 | Loss: 0.00002295
Iteration 17/1000 | Loss: 0.00002290
Iteration 18/1000 | Loss: 0.00002285
Iteration 19/1000 | Loss: 0.00002283
Iteration 20/1000 | Loss: 0.00002282
Iteration 21/1000 | Loss: 0.00002282
Iteration 22/1000 | Loss: 0.00002279
Iteration 23/1000 | Loss: 0.00002274
Iteration 24/1000 | Loss: 0.00002274
Iteration 25/1000 | Loss: 0.00002271
Iteration 26/1000 | Loss: 0.00002269
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002260
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002259
Iteration 31/1000 | Loss: 0.00002255
Iteration 32/1000 | Loss: 0.00002252
Iteration 33/1000 | Loss: 0.00002252
Iteration 34/1000 | Loss: 0.00002252
Iteration 35/1000 | Loss: 0.00002252
Iteration 36/1000 | Loss: 0.00002252
Iteration 37/1000 | Loss: 0.00002252
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002252
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002251
Iteration 44/1000 | Loss: 0.00002251
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002251
Iteration 49/1000 | Loss: 0.00002248
Iteration 50/1000 | Loss: 0.00002248
Iteration 51/1000 | Loss: 0.00002247
Iteration 52/1000 | Loss: 0.00002247
Iteration 53/1000 | Loss: 0.00002246
Iteration 54/1000 | Loss: 0.00002246
Iteration 55/1000 | Loss: 0.00002246
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002245
Iteration 58/1000 | Loss: 0.00002245
Iteration 59/1000 | Loss: 0.00002245
Iteration 60/1000 | Loss: 0.00002245
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002242
Iteration 63/1000 | Loss: 0.00002242
Iteration 64/1000 | Loss: 0.00002240
Iteration 65/1000 | Loss: 0.00002240
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00002240
Iteration 68/1000 | Loss: 0.00002240
Iteration 69/1000 | Loss: 0.00002239
Iteration 70/1000 | Loss: 0.00002239
Iteration 71/1000 | Loss: 0.00002239
Iteration 72/1000 | Loss: 0.00002239
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002239
Iteration 75/1000 | Loss: 0.00002239
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002235
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002235
Iteration 82/1000 | Loss: 0.00002235
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00002235
Iteration 85/1000 | Loss: 0.00002235
Iteration 86/1000 | Loss: 0.00002234
Iteration 87/1000 | Loss: 0.00002234
Iteration 88/1000 | Loss: 0.00002234
Iteration 89/1000 | Loss: 0.00002234
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002233
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002232
Iteration 96/1000 | Loss: 0.00002232
Iteration 97/1000 | Loss: 0.00002232
Iteration 98/1000 | Loss: 0.00002231
Iteration 99/1000 | Loss: 0.00002231
Iteration 100/1000 | Loss: 0.00002230
Iteration 101/1000 | Loss: 0.00002230
Iteration 102/1000 | Loss: 0.00002230
Iteration 103/1000 | Loss: 0.00002229
Iteration 104/1000 | Loss: 0.00002229
Iteration 105/1000 | Loss: 0.00002229
Iteration 106/1000 | Loss: 0.00002229
Iteration 107/1000 | Loss: 0.00002229
Iteration 108/1000 | Loss: 0.00002229
Iteration 109/1000 | Loss: 0.00002229
Iteration 110/1000 | Loss: 0.00002229
Iteration 111/1000 | Loss: 0.00002229
Iteration 112/1000 | Loss: 0.00002229
Iteration 113/1000 | Loss: 0.00002229
Iteration 114/1000 | Loss: 0.00002229
Iteration 115/1000 | Loss: 0.00002229
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002228
Iteration 118/1000 | Loss: 0.00002228
Iteration 119/1000 | Loss: 0.00002228
Iteration 120/1000 | Loss: 0.00002227
Iteration 121/1000 | Loss: 0.00002227
Iteration 122/1000 | Loss: 0.00002227
Iteration 123/1000 | Loss: 0.00002227
Iteration 124/1000 | Loss: 0.00002227
Iteration 125/1000 | Loss: 0.00002226
Iteration 126/1000 | Loss: 0.00002226
Iteration 127/1000 | Loss: 0.00002226
Iteration 128/1000 | Loss: 0.00002226
Iteration 129/1000 | Loss: 0.00002226
Iteration 130/1000 | Loss: 0.00002226
Iteration 131/1000 | Loss: 0.00002226
Iteration 132/1000 | Loss: 0.00002225
Iteration 133/1000 | Loss: 0.00002225
Iteration 134/1000 | Loss: 0.00002225
Iteration 135/1000 | Loss: 0.00002225
Iteration 136/1000 | Loss: 0.00002225
Iteration 137/1000 | Loss: 0.00002225
Iteration 138/1000 | Loss: 0.00002225
Iteration 139/1000 | Loss: 0.00002225
Iteration 140/1000 | Loss: 0.00002224
Iteration 141/1000 | Loss: 0.00002224
Iteration 142/1000 | Loss: 0.00002224
Iteration 143/1000 | Loss: 0.00002224
Iteration 144/1000 | Loss: 0.00002224
Iteration 145/1000 | Loss: 0.00002224
Iteration 146/1000 | Loss: 0.00002224
Iteration 147/1000 | Loss: 0.00002224
Iteration 148/1000 | Loss: 0.00002224
Iteration 149/1000 | Loss: 0.00002223
Iteration 150/1000 | Loss: 0.00002223
Iteration 151/1000 | Loss: 0.00002223
Iteration 152/1000 | Loss: 0.00002223
Iteration 153/1000 | Loss: 0.00002223
Iteration 154/1000 | Loss: 0.00002223
Iteration 155/1000 | Loss: 0.00002223
Iteration 156/1000 | Loss: 0.00002223
Iteration 157/1000 | Loss: 0.00002223
Iteration 158/1000 | Loss: 0.00002223
Iteration 159/1000 | Loss: 0.00002223
Iteration 160/1000 | Loss: 0.00002222
Iteration 161/1000 | Loss: 0.00002222
Iteration 162/1000 | Loss: 0.00002222
Iteration 163/1000 | Loss: 0.00002222
Iteration 164/1000 | Loss: 0.00002222
Iteration 165/1000 | Loss: 0.00002222
Iteration 166/1000 | Loss: 0.00002222
Iteration 167/1000 | Loss: 0.00002222
Iteration 168/1000 | Loss: 0.00002222
Iteration 169/1000 | Loss: 0.00002222
Iteration 170/1000 | Loss: 0.00002222
Iteration 171/1000 | Loss: 0.00002222
Iteration 172/1000 | Loss: 0.00002222
Iteration 173/1000 | Loss: 0.00002222
Iteration 174/1000 | Loss: 0.00002222
Iteration 175/1000 | Loss: 0.00002221
Iteration 176/1000 | Loss: 0.00002221
Iteration 177/1000 | Loss: 0.00002221
Iteration 178/1000 | Loss: 0.00002221
Iteration 179/1000 | Loss: 0.00002221
Iteration 180/1000 | Loss: 0.00002221
Iteration 181/1000 | Loss: 0.00002221
Iteration 182/1000 | Loss: 0.00002220
Iteration 183/1000 | Loss: 0.00002220
Iteration 184/1000 | Loss: 0.00002220
Iteration 185/1000 | Loss: 0.00002220
Iteration 186/1000 | Loss: 0.00002220
Iteration 187/1000 | Loss: 0.00002220
Iteration 188/1000 | Loss: 0.00002220
Iteration 189/1000 | Loss: 0.00002220
Iteration 190/1000 | Loss: 0.00002220
Iteration 191/1000 | Loss: 0.00002220
Iteration 192/1000 | Loss: 0.00002220
Iteration 193/1000 | Loss: 0.00002220
Iteration 194/1000 | Loss: 0.00002220
Iteration 195/1000 | Loss: 0.00002220
Iteration 196/1000 | Loss: 0.00002220
Iteration 197/1000 | Loss: 0.00002220
Iteration 198/1000 | Loss: 0.00002220
Iteration 199/1000 | Loss: 0.00002219
Iteration 200/1000 | Loss: 0.00002219
Iteration 201/1000 | Loss: 0.00002219
Iteration 202/1000 | Loss: 0.00002219
Iteration 203/1000 | Loss: 0.00002219
Iteration 204/1000 | Loss: 0.00002219
Iteration 205/1000 | Loss: 0.00002219
Iteration 206/1000 | Loss: 0.00002219
Iteration 207/1000 | Loss: 0.00002219
Iteration 208/1000 | Loss: 0.00002219
Iteration 209/1000 | Loss: 0.00002219
Iteration 210/1000 | Loss: 0.00002219
Iteration 211/1000 | Loss: 0.00002219
Iteration 212/1000 | Loss: 0.00002219
Iteration 213/1000 | Loss: 0.00002219
Iteration 214/1000 | Loss: 0.00002219
Iteration 215/1000 | Loss: 0.00002219
Iteration 216/1000 | Loss: 0.00002219
Iteration 217/1000 | Loss: 0.00002218
Iteration 218/1000 | Loss: 0.00002218
Iteration 219/1000 | Loss: 0.00002218
Iteration 220/1000 | Loss: 0.00002218
Iteration 221/1000 | Loss: 0.00002218
Iteration 222/1000 | Loss: 0.00002218
Iteration 223/1000 | Loss: 0.00002218
Iteration 224/1000 | Loss: 0.00002218
Iteration 225/1000 | Loss: 0.00002218
Iteration 226/1000 | Loss: 0.00002218
Iteration 227/1000 | Loss: 0.00002218
Iteration 228/1000 | Loss: 0.00002218
Iteration 229/1000 | Loss: 0.00002218
Iteration 230/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.2182321117725223e-05, 2.2182321117725223e-05, 2.2182321117725223e-05, 2.2182321117725223e-05, 2.2182321117725223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2182321117725223e-05

Optimization complete. Final v2v error: 3.7621707916259766 mm

Highest mean error: 5.461574077606201 mm for frame 74

Lowest mean error: 2.905534029006958 mm for frame 36

Saving results

Total time: 48.84842920303345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399460
Iteration 2/25 | Loss: 0.00113397
Iteration 3/25 | Loss: 0.00106013
Iteration 4/25 | Loss: 0.00105106
Iteration 5/25 | Loss: 0.00104936
Iteration 6/25 | Loss: 0.00104933
Iteration 7/25 | Loss: 0.00104933
Iteration 8/25 | Loss: 0.00104933
Iteration 9/25 | Loss: 0.00104933
Iteration 10/25 | Loss: 0.00104933
Iteration 11/25 | Loss: 0.00104933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010493311565369368, 0.0010493311565369368, 0.0010493311565369368, 0.0010493311565369368, 0.0010493311565369368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010493311565369368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73634124
Iteration 2/25 | Loss: 0.00076868
Iteration 3/25 | Loss: 0.00076868
Iteration 4/25 | Loss: 0.00076868
Iteration 5/25 | Loss: 0.00076868
Iteration 6/25 | Loss: 0.00076868
Iteration 7/25 | Loss: 0.00076868
Iteration 8/25 | Loss: 0.00076868
Iteration 9/25 | Loss: 0.00076868
Iteration 10/25 | Loss: 0.00076867
Iteration 11/25 | Loss: 0.00076867
Iteration 12/25 | Loss: 0.00076867
Iteration 13/25 | Loss: 0.00076867
Iteration 14/25 | Loss: 0.00076867
Iteration 15/25 | Loss: 0.00076867
Iteration 16/25 | Loss: 0.00076867
Iteration 17/25 | Loss: 0.00076867
Iteration 18/25 | Loss: 0.00076867
Iteration 19/25 | Loss: 0.00076867
Iteration 20/25 | Loss: 0.00076867
Iteration 21/25 | Loss: 0.00076867
Iteration 22/25 | Loss: 0.00076867
Iteration 23/25 | Loss: 0.00076867
Iteration 24/25 | Loss: 0.00076867
Iteration 25/25 | Loss: 0.00076867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076867
Iteration 2/1000 | Loss: 0.00002066
Iteration 3/1000 | Loss: 0.00001437
Iteration 4/1000 | Loss: 0.00001258
Iteration 5/1000 | Loss: 0.00001184
Iteration 6/1000 | Loss: 0.00001118
Iteration 7/1000 | Loss: 0.00001076
Iteration 8/1000 | Loss: 0.00001053
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001007
Iteration 11/1000 | Loss: 0.00001004
Iteration 12/1000 | Loss: 0.00000995
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000992
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000985
Iteration 17/1000 | Loss: 0.00000985
Iteration 18/1000 | Loss: 0.00000984
Iteration 19/1000 | Loss: 0.00000983
Iteration 20/1000 | Loss: 0.00000982
Iteration 21/1000 | Loss: 0.00000975
Iteration 22/1000 | Loss: 0.00000974
Iteration 23/1000 | Loss: 0.00000973
Iteration 24/1000 | Loss: 0.00000970
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000968
Iteration 28/1000 | Loss: 0.00000968
Iteration 29/1000 | Loss: 0.00000959
Iteration 30/1000 | Loss: 0.00000958
Iteration 31/1000 | Loss: 0.00000955
Iteration 32/1000 | Loss: 0.00000955
Iteration 33/1000 | Loss: 0.00000954
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000950
Iteration 36/1000 | Loss: 0.00000950
Iteration 37/1000 | Loss: 0.00000949
Iteration 38/1000 | Loss: 0.00000947
Iteration 39/1000 | Loss: 0.00000946
Iteration 40/1000 | Loss: 0.00000946
Iteration 41/1000 | Loss: 0.00000946
Iteration 42/1000 | Loss: 0.00000945
Iteration 43/1000 | Loss: 0.00000944
Iteration 44/1000 | Loss: 0.00000944
Iteration 45/1000 | Loss: 0.00000943
Iteration 46/1000 | Loss: 0.00000942
Iteration 47/1000 | Loss: 0.00000941
Iteration 48/1000 | Loss: 0.00000940
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000939
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00000938
Iteration 53/1000 | Loss: 0.00000938
Iteration 54/1000 | Loss: 0.00000937
Iteration 55/1000 | Loss: 0.00000935
Iteration 56/1000 | Loss: 0.00000934
Iteration 57/1000 | Loss: 0.00000934
Iteration 58/1000 | Loss: 0.00000934
Iteration 59/1000 | Loss: 0.00000934
Iteration 60/1000 | Loss: 0.00000934
Iteration 61/1000 | Loss: 0.00000934
Iteration 62/1000 | Loss: 0.00000934
Iteration 63/1000 | Loss: 0.00000934
Iteration 64/1000 | Loss: 0.00000934
Iteration 65/1000 | Loss: 0.00000929
Iteration 66/1000 | Loss: 0.00000929
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000929
Iteration 70/1000 | Loss: 0.00000929
Iteration 71/1000 | Loss: 0.00000929
Iteration 72/1000 | Loss: 0.00000929
Iteration 73/1000 | Loss: 0.00000929
Iteration 74/1000 | Loss: 0.00000929
Iteration 75/1000 | Loss: 0.00000929
Iteration 76/1000 | Loss: 0.00000929
Iteration 77/1000 | Loss: 0.00000929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [9.285810847359244e-06, 9.285810847359244e-06, 9.285810847359244e-06, 9.285810847359244e-06, 9.285810847359244e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.285810847359244e-06

Optimization complete. Final v2v error: 2.6510138511657715 mm

Highest mean error: 2.9317407608032227 mm for frame 125

Lowest mean error: 2.507993459701538 mm for frame 181

Saving results

Total time: 31.77851963043213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360160
Iteration 2/25 | Loss: 0.00108654
Iteration 3/25 | Loss: 0.00103746
Iteration 4/25 | Loss: 0.00102962
Iteration 5/25 | Loss: 0.00102698
Iteration 6/25 | Loss: 0.00102648
Iteration 7/25 | Loss: 0.00102648
Iteration 8/25 | Loss: 0.00102648
Iteration 9/25 | Loss: 0.00102648
Iteration 10/25 | Loss: 0.00102648
Iteration 11/25 | Loss: 0.00102648
Iteration 12/25 | Loss: 0.00102648
Iteration 13/25 | Loss: 0.00102648
Iteration 14/25 | Loss: 0.00102648
Iteration 15/25 | Loss: 0.00102648
Iteration 16/25 | Loss: 0.00102648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010264799930155277, 0.0010264799930155277, 0.0010264799930155277, 0.0010264799930155277, 0.0010264799930155277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010264799930155277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43937564
Iteration 2/25 | Loss: 0.00081408
Iteration 3/25 | Loss: 0.00081408
Iteration 4/25 | Loss: 0.00081408
Iteration 5/25 | Loss: 0.00081408
Iteration 6/25 | Loss: 0.00081408
Iteration 7/25 | Loss: 0.00081408
Iteration 8/25 | Loss: 0.00081408
Iteration 9/25 | Loss: 0.00081408
Iteration 10/25 | Loss: 0.00081408
Iteration 11/25 | Loss: 0.00081408
Iteration 12/25 | Loss: 0.00081408
Iteration 13/25 | Loss: 0.00081408
Iteration 14/25 | Loss: 0.00081408
Iteration 15/25 | Loss: 0.00081408
Iteration 16/25 | Loss: 0.00081408
Iteration 17/25 | Loss: 0.00081408
Iteration 18/25 | Loss: 0.00081408
Iteration 19/25 | Loss: 0.00081408
Iteration 20/25 | Loss: 0.00081408
Iteration 21/25 | Loss: 0.00081408
Iteration 22/25 | Loss: 0.00081408
Iteration 23/25 | Loss: 0.00081408
Iteration 24/25 | Loss: 0.00081408
Iteration 25/25 | Loss: 0.00081408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081408
Iteration 2/1000 | Loss: 0.00001998
Iteration 3/1000 | Loss: 0.00001277
Iteration 4/1000 | Loss: 0.00001049
Iteration 5/1000 | Loss: 0.00000955
Iteration 6/1000 | Loss: 0.00000905
Iteration 7/1000 | Loss: 0.00000865
Iteration 8/1000 | Loss: 0.00000835
Iteration 9/1000 | Loss: 0.00000833
Iteration 10/1000 | Loss: 0.00000822
Iteration 11/1000 | Loss: 0.00000812
Iteration 12/1000 | Loss: 0.00000794
Iteration 13/1000 | Loss: 0.00000788
Iteration 14/1000 | Loss: 0.00000784
Iteration 15/1000 | Loss: 0.00000784
Iteration 16/1000 | Loss: 0.00000783
Iteration 17/1000 | Loss: 0.00000783
Iteration 18/1000 | Loss: 0.00000783
Iteration 19/1000 | Loss: 0.00000778
Iteration 20/1000 | Loss: 0.00000778
Iteration 21/1000 | Loss: 0.00000778
Iteration 22/1000 | Loss: 0.00000778
Iteration 23/1000 | Loss: 0.00000778
Iteration 24/1000 | Loss: 0.00000774
Iteration 25/1000 | Loss: 0.00000771
Iteration 26/1000 | Loss: 0.00000771
Iteration 27/1000 | Loss: 0.00000769
Iteration 28/1000 | Loss: 0.00000768
Iteration 29/1000 | Loss: 0.00000767
Iteration 30/1000 | Loss: 0.00000767
Iteration 31/1000 | Loss: 0.00000767
Iteration 32/1000 | Loss: 0.00000766
Iteration 33/1000 | Loss: 0.00000765
Iteration 34/1000 | Loss: 0.00000765
Iteration 35/1000 | Loss: 0.00000763
Iteration 36/1000 | Loss: 0.00000763
Iteration 37/1000 | Loss: 0.00000763
Iteration 38/1000 | Loss: 0.00000762
Iteration 39/1000 | Loss: 0.00000762
Iteration 40/1000 | Loss: 0.00000762
Iteration 41/1000 | Loss: 0.00000762
Iteration 42/1000 | Loss: 0.00000762
Iteration 43/1000 | Loss: 0.00000762
Iteration 44/1000 | Loss: 0.00000762
Iteration 45/1000 | Loss: 0.00000762
Iteration 46/1000 | Loss: 0.00000762
Iteration 47/1000 | Loss: 0.00000762
Iteration 48/1000 | Loss: 0.00000761
Iteration 49/1000 | Loss: 0.00000761
Iteration 50/1000 | Loss: 0.00000760
Iteration 51/1000 | Loss: 0.00000760
Iteration 52/1000 | Loss: 0.00000758
Iteration 53/1000 | Loss: 0.00000758
Iteration 54/1000 | Loss: 0.00000758
Iteration 55/1000 | Loss: 0.00000757
Iteration 56/1000 | Loss: 0.00000757
Iteration 57/1000 | Loss: 0.00000757
Iteration 58/1000 | Loss: 0.00000757
Iteration 59/1000 | Loss: 0.00000757
Iteration 60/1000 | Loss: 0.00000757
Iteration 61/1000 | Loss: 0.00000757
Iteration 62/1000 | Loss: 0.00000757
Iteration 63/1000 | Loss: 0.00000756
Iteration 64/1000 | Loss: 0.00000756
Iteration 65/1000 | Loss: 0.00000756
Iteration 66/1000 | Loss: 0.00000755
Iteration 67/1000 | Loss: 0.00000755
Iteration 68/1000 | Loss: 0.00000755
Iteration 69/1000 | Loss: 0.00000754
Iteration 70/1000 | Loss: 0.00000754
Iteration 71/1000 | Loss: 0.00000754
Iteration 72/1000 | Loss: 0.00000753
Iteration 73/1000 | Loss: 0.00000753
Iteration 74/1000 | Loss: 0.00000753
Iteration 75/1000 | Loss: 0.00000753
Iteration 76/1000 | Loss: 0.00000752
Iteration 77/1000 | Loss: 0.00000752
Iteration 78/1000 | Loss: 0.00000752
Iteration 79/1000 | Loss: 0.00000752
Iteration 80/1000 | Loss: 0.00000752
Iteration 81/1000 | Loss: 0.00000752
Iteration 82/1000 | Loss: 0.00000752
Iteration 83/1000 | Loss: 0.00000752
Iteration 84/1000 | Loss: 0.00000752
Iteration 85/1000 | Loss: 0.00000752
Iteration 86/1000 | Loss: 0.00000752
Iteration 87/1000 | Loss: 0.00000751
Iteration 88/1000 | Loss: 0.00000751
Iteration 89/1000 | Loss: 0.00000751
Iteration 90/1000 | Loss: 0.00000751
Iteration 91/1000 | Loss: 0.00000751
Iteration 92/1000 | Loss: 0.00000751
Iteration 93/1000 | Loss: 0.00000751
Iteration 94/1000 | Loss: 0.00000751
Iteration 95/1000 | Loss: 0.00000750
Iteration 96/1000 | Loss: 0.00000750
Iteration 97/1000 | Loss: 0.00000750
Iteration 98/1000 | Loss: 0.00000750
Iteration 99/1000 | Loss: 0.00000749
Iteration 100/1000 | Loss: 0.00000749
Iteration 101/1000 | Loss: 0.00000749
Iteration 102/1000 | Loss: 0.00000749
Iteration 103/1000 | Loss: 0.00000748
Iteration 104/1000 | Loss: 0.00000748
Iteration 105/1000 | Loss: 0.00000748
Iteration 106/1000 | Loss: 0.00000748
Iteration 107/1000 | Loss: 0.00000747
Iteration 108/1000 | Loss: 0.00000746
Iteration 109/1000 | Loss: 0.00000746
Iteration 110/1000 | Loss: 0.00000746
Iteration 111/1000 | Loss: 0.00000745
Iteration 112/1000 | Loss: 0.00000745
Iteration 113/1000 | Loss: 0.00000745
Iteration 114/1000 | Loss: 0.00000745
Iteration 115/1000 | Loss: 0.00000745
Iteration 116/1000 | Loss: 0.00000745
Iteration 117/1000 | Loss: 0.00000745
Iteration 118/1000 | Loss: 0.00000745
Iteration 119/1000 | Loss: 0.00000745
Iteration 120/1000 | Loss: 0.00000744
Iteration 121/1000 | Loss: 0.00000744
Iteration 122/1000 | Loss: 0.00000744
Iteration 123/1000 | Loss: 0.00000744
Iteration 124/1000 | Loss: 0.00000744
Iteration 125/1000 | Loss: 0.00000744
Iteration 126/1000 | Loss: 0.00000744
Iteration 127/1000 | Loss: 0.00000743
Iteration 128/1000 | Loss: 0.00000743
Iteration 129/1000 | Loss: 0.00000743
Iteration 130/1000 | Loss: 0.00000743
Iteration 131/1000 | Loss: 0.00000743
Iteration 132/1000 | Loss: 0.00000743
Iteration 133/1000 | Loss: 0.00000742
Iteration 134/1000 | Loss: 0.00000741
Iteration 135/1000 | Loss: 0.00000741
Iteration 136/1000 | Loss: 0.00000741
Iteration 137/1000 | Loss: 0.00000741
Iteration 138/1000 | Loss: 0.00000741
Iteration 139/1000 | Loss: 0.00000741
Iteration 140/1000 | Loss: 0.00000741
Iteration 141/1000 | Loss: 0.00000741
Iteration 142/1000 | Loss: 0.00000741
Iteration 143/1000 | Loss: 0.00000740
Iteration 144/1000 | Loss: 0.00000740
Iteration 145/1000 | Loss: 0.00000740
Iteration 146/1000 | Loss: 0.00000740
Iteration 147/1000 | Loss: 0.00000739
Iteration 148/1000 | Loss: 0.00000739
Iteration 149/1000 | Loss: 0.00000739
Iteration 150/1000 | Loss: 0.00000739
Iteration 151/1000 | Loss: 0.00000739
Iteration 152/1000 | Loss: 0.00000739
Iteration 153/1000 | Loss: 0.00000739
Iteration 154/1000 | Loss: 0.00000738
Iteration 155/1000 | Loss: 0.00000738
Iteration 156/1000 | Loss: 0.00000738
Iteration 157/1000 | Loss: 0.00000738
Iteration 158/1000 | Loss: 0.00000738
Iteration 159/1000 | Loss: 0.00000738
Iteration 160/1000 | Loss: 0.00000738
Iteration 161/1000 | Loss: 0.00000738
Iteration 162/1000 | Loss: 0.00000738
Iteration 163/1000 | Loss: 0.00000738
Iteration 164/1000 | Loss: 0.00000738
Iteration 165/1000 | Loss: 0.00000738
Iteration 166/1000 | Loss: 0.00000737
Iteration 167/1000 | Loss: 0.00000737
Iteration 168/1000 | Loss: 0.00000737
Iteration 169/1000 | Loss: 0.00000737
Iteration 170/1000 | Loss: 0.00000737
Iteration 171/1000 | Loss: 0.00000737
Iteration 172/1000 | Loss: 0.00000737
Iteration 173/1000 | Loss: 0.00000737
Iteration 174/1000 | Loss: 0.00000737
Iteration 175/1000 | Loss: 0.00000737
Iteration 176/1000 | Loss: 0.00000737
Iteration 177/1000 | Loss: 0.00000737
Iteration 178/1000 | Loss: 0.00000737
Iteration 179/1000 | Loss: 0.00000737
Iteration 180/1000 | Loss: 0.00000737
Iteration 181/1000 | Loss: 0.00000737
Iteration 182/1000 | Loss: 0.00000737
Iteration 183/1000 | Loss: 0.00000737
Iteration 184/1000 | Loss: 0.00000737
Iteration 185/1000 | Loss: 0.00000737
Iteration 186/1000 | Loss: 0.00000737
Iteration 187/1000 | Loss: 0.00000737
Iteration 188/1000 | Loss: 0.00000737
Iteration 189/1000 | Loss: 0.00000737
Iteration 190/1000 | Loss: 0.00000737
Iteration 191/1000 | Loss: 0.00000737
Iteration 192/1000 | Loss: 0.00000737
Iteration 193/1000 | Loss: 0.00000737
Iteration 194/1000 | Loss: 0.00000737
Iteration 195/1000 | Loss: 0.00000737
Iteration 196/1000 | Loss: 0.00000737
Iteration 197/1000 | Loss: 0.00000737
Iteration 198/1000 | Loss: 0.00000737
Iteration 199/1000 | Loss: 0.00000737
Iteration 200/1000 | Loss: 0.00000737
Iteration 201/1000 | Loss: 0.00000737
Iteration 202/1000 | Loss: 0.00000737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [7.373010248556966e-06, 7.373010248556966e-06, 7.373010248556966e-06, 7.373010248556966e-06, 7.373010248556966e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.373010248556966e-06

Optimization complete. Final v2v error: 2.3533847332000732 mm

Highest mean error: 2.513796806335449 mm for frame 109

Lowest mean error: 2.2987782955169678 mm for frame 14

Saving results

Total time: 37.32532238960266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429863
Iteration 2/25 | Loss: 0.00117307
Iteration 3/25 | Loss: 0.00109622
Iteration 4/25 | Loss: 0.00107978
Iteration 5/25 | Loss: 0.00107451
Iteration 6/25 | Loss: 0.00107429
Iteration 7/25 | Loss: 0.00107429
Iteration 8/25 | Loss: 0.00107429
Iteration 9/25 | Loss: 0.00107429
Iteration 10/25 | Loss: 0.00107429
Iteration 11/25 | Loss: 0.00107429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010742853628471494, 0.0010742853628471494, 0.0010742853628471494, 0.0010742853628471494, 0.0010742853628471494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010742853628471494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.22672558
Iteration 2/25 | Loss: 0.00076116
Iteration 3/25 | Loss: 0.00076116
Iteration 4/25 | Loss: 0.00076116
Iteration 5/25 | Loss: 0.00076116
Iteration 6/25 | Loss: 0.00076116
Iteration 7/25 | Loss: 0.00076115
Iteration 8/25 | Loss: 0.00076115
Iteration 9/25 | Loss: 0.00076115
Iteration 10/25 | Loss: 0.00076115
Iteration 11/25 | Loss: 0.00076115
Iteration 12/25 | Loss: 0.00076115
Iteration 13/25 | Loss: 0.00076115
Iteration 14/25 | Loss: 0.00076115
Iteration 15/25 | Loss: 0.00076115
Iteration 16/25 | Loss: 0.00076115
Iteration 17/25 | Loss: 0.00076115
Iteration 18/25 | Loss: 0.00076115
Iteration 19/25 | Loss: 0.00076115
Iteration 20/25 | Loss: 0.00076115
Iteration 21/25 | Loss: 0.00076115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007611532346345484, 0.0007611532346345484, 0.0007611532346345484, 0.0007611532346345484, 0.0007611532346345484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007611532346345484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076115
Iteration 2/1000 | Loss: 0.00001929
Iteration 3/1000 | Loss: 0.00001567
Iteration 4/1000 | Loss: 0.00001466
Iteration 5/1000 | Loss: 0.00001388
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001215
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001204
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001177
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001146
Iteration 92/1000 | Loss: 0.00001146
Iteration 93/1000 | Loss: 0.00001146
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001143
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001143
Iteration 106/1000 | Loss: 0.00001143
Iteration 107/1000 | Loss: 0.00001143
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001142
Iteration 110/1000 | Loss: 0.00001142
Iteration 111/1000 | Loss: 0.00001142
Iteration 112/1000 | Loss: 0.00001142
Iteration 113/1000 | Loss: 0.00001142
Iteration 114/1000 | Loss: 0.00001141
Iteration 115/1000 | Loss: 0.00001141
Iteration 116/1000 | Loss: 0.00001141
Iteration 117/1000 | Loss: 0.00001141
Iteration 118/1000 | Loss: 0.00001141
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001141
Iteration 121/1000 | Loss: 0.00001141
Iteration 122/1000 | Loss: 0.00001141
Iteration 123/1000 | Loss: 0.00001140
Iteration 124/1000 | Loss: 0.00001140
Iteration 125/1000 | Loss: 0.00001140
Iteration 126/1000 | Loss: 0.00001140
Iteration 127/1000 | Loss: 0.00001140
Iteration 128/1000 | Loss: 0.00001140
Iteration 129/1000 | Loss: 0.00001140
Iteration 130/1000 | Loss: 0.00001140
Iteration 131/1000 | Loss: 0.00001140
Iteration 132/1000 | Loss: 0.00001140
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001139
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1385961443011183e-05, 1.1385961443011183e-05, 1.1385961443011183e-05, 1.1385961443011183e-05, 1.1385961443011183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1385961443011183e-05

Optimization complete. Final v2v error: 2.8780436515808105 mm

Highest mean error: 3.282871961593628 mm for frame 175

Lowest mean error: 2.5842082500457764 mm for frame 210

Saving results

Total time: 43.169573068618774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724319
Iteration 2/25 | Loss: 0.00117079
Iteration 3/25 | Loss: 0.00109774
Iteration 4/25 | Loss: 0.00109305
Iteration 5/25 | Loss: 0.00109250
Iteration 6/25 | Loss: 0.00109250
Iteration 7/25 | Loss: 0.00109250
Iteration 8/25 | Loss: 0.00109250
Iteration 9/25 | Loss: 0.00109250
Iteration 10/25 | Loss: 0.00109250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010925038950517774, 0.0010925038950517774, 0.0010925038950517774, 0.0010925038950517774, 0.0010925038950517774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010925038950517774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 16.77014923
Iteration 2/25 | Loss: 0.00081243
Iteration 3/25 | Loss: 0.00081237
Iteration 4/25 | Loss: 0.00081237
Iteration 5/25 | Loss: 0.00081237
Iteration 6/25 | Loss: 0.00081237
Iteration 7/25 | Loss: 0.00081237
Iteration 8/25 | Loss: 0.00081237
Iteration 9/25 | Loss: 0.00081237
Iteration 10/25 | Loss: 0.00081237
Iteration 11/25 | Loss: 0.00081237
Iteration 12/25 | Loss: 0.00081237
Iteration 13/25 | Loss: 0.00081237
Iteration 14/25 | Loss: 0.00081237
Iteration 15/25 | Loss: 0.00081237
Iteration 16/25 | Loss: 0.00081237
Iteration 17/25 | Loss: 0.00081237
Iteration 18/25 | Loss: 0.00081237
Iteration 19/25 | Loss: 0.00081237
Iteration 20/25 | Loss: 0.00081237
Iteration 21/25 | Loss: 0.00081237
Iteration 22/25 | Loss: 0.00081237
Iteration 23/25 | Loss: 0.00081237
Iteration 24/25 | Loss: 0.00081237
Iteration 25/25 | Loss: 0.00081237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081237
Iteration 2/1000 | Loss: 0.00001739
Iteration 3/1000 | Loss: 0.00001285
Iteration 4/1000 | Loss: 0.00001171
Iteration 5/1000 | Loss: 0.00001108
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001054
Iteration 8/1000 | Loss: 0.00001024
Iteration 9/1000 | Loss: 0.00001008
Iteration 10/1000 | Loss: 0.00001006
Iteration 11/1000 | Loss: 0.00000999
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000990
Iteration 14/1000 | Loss: 0.00000982
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000979
Iteration 17/1000 | Loss: 0.00000975
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000973
Iteration 20/1000 | Loss: 0.00000972
Iteration 21/1000 | Loss: 0.00000966
Iteration 22/1000 | Loss: 0.00000961
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000959
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000958
Iteration 27/1000 | Loss: 0.00000957
Iteration 28/1000 | Loss: 0.00000956
Iteration 29/1000 | Loss: 0.00000955
Iteration 30/1000 | Loss: 0.00000955
Iteration 31/1000 | Loss: 0.00000954
Iteration 32/1000 | Loss: 0.00000954
Iteration 33/1000 | Loss: 0.00000953
Iteration 34/1000 | Loss: 0.00000953
Iteration 35/1000 | Loss: 0.00000952
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000951
Iteration 38/1000 | Loss: 0.00000950
Iteration 39/1000 | Loss: 0.00000948
Iteration 40/1000 | Loss: 0.00000948
Iteration 41/1000 | Loss: 0.00000948
Iteration 42/1000 | Loss: 0.00000948
Iteration 43/1000 | Loss: 0.00000948
Iteration 44/1000 | Loss: 0.00000948
Iteration 45/1000 | Loss: 0.00000948
Iteration 46/1000 | Loss: 0.00000948
Iteration 47/1000 | Loss: 0.00000948
Iteration 48/1000 | Loss: 0.00000948
Iteration 49/1000 | Loss: 0.00000947
Iteration 50/1000 | Loss: 0.00000946
Iteration 51/1000 | Loss: 0.00000944
Iteration 52/1000 | Loss: 0.00000944
Iteration 53/1000 | Loss: 0.00000943
Iteration 54/1000 | Loss: 0.00000943
Iteration 55/1000 | Loss: 0.00000943
Iteration 56/1000 | Loss: 0.00000943
Iteration 57/1000 | Loss: 0.00000943
Iteration 58/1000 | Loss: 0.00000943
Iteration 59/1000 | Loss: 0.00000943
Iteration 60/1000 | Loss: 0.00000943
Iteration 61/1000 | Loss: 0.00000943
Iteration 62/1000 | Loss: 0.00000943
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000942
Iteration 65/1000 | Loss: 0.00000942
Iteration 66/1000 | Loss: 0.00000942
Iteration 67/1000 | Loss: 0.00000942
Iteration 68/1000 | Loss: 0.00000942
Iteration 69/1000 | Loss: 0.00000942
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000940
Iteration 72/1000 | Loss: 0.00000940
Iteration 73/1000 | Loss: 0.00000940
Iteration 74/1000 | Loss: 0.00000940
Iteration 75/1000 | Loss: 0.00000939
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000938
Iteration 78/1000 | Loss: 0.00000938
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000937
Iteration 81/1000 | Loss: 0.00000937
Iteration 82/1000 | Loss: 0.00000937
Iteration 83/1000 | Loss: 0.00000937
Iteration 84/1000 | Loss: 0.00000937
Iteration 85/1000 | Loss: 0.00000936
Iteration 86/1000 | Loss: 0.00000936
Iteration 87/1000 | Loss: 0.00000936
Iteration 88/1000 | Loss: 0.00000936
Iteration 89/1000 | Loss: 0.00000936
Iteration 90/1000 | Loss: 0.00000935
Iteration 91/1000 | Loss: 0.00000935
Iteration 92/1000 | Loss: 0.00000934
Iteration 93/1000 | Loss: 0.00000933
Iteration 94/1000 | Loss: 0.00000933
Iteration 95/1000 | Loss: 0.00000933
Iteration 96/1000 | Loss: 0.00000933
Iteration 97/1000 | Loss: 0.00000933
Iteration 98/1000 | Loss: 0.00000933
Iteration 99/1000 | Loss: 0.00000933
Iteration 100/1000 | Loss: 0.00000932
Iteration 101/1000 | Loss: 0.00000932
Iteration 102/1000 | Loss: 0.00000932
Iteration 103/1000 | Loss: 0.00000932
Iteration 104/1000 | Loss: 0.00000932
Iteration 105/1000 | Loss: 0.00000932
Iteration 106/1000 | Loss: 0.00000932
Iteration 107/1000 | Loss: 0.00000932
Iteration 108/1000 | Loss: 0.00000931
Iteration 109/1000 | Loss: 0.00000930
Iteration 110/1000 | Loss: 0.00000930
Iteration 111/1000 | Loss: 0.00000930
Iteration 112/1000 | Loss: 0.00000930
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000929
Iteration 116/1000 | Loss: 0.00000929
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000928
Iteration 119/1000 | Loss: 0.00000928
Iteration 120/1000 | Loss: 0.00000927
Iteration 121/1000 | Loss: 0.00000927
Iteration 122/1000 | Loss: 0.00000927
Iteration 123/1000 | Loss: 0.00000927
Iteration 124/1000 | Loss: 0.00000927
Iteration 125/1000 | Loss: 0.00000927
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000926
Iteration 132/1000 | Loss: 0.00000926
Iteration 133/1000 | Loss: 0.00000926
Iteration 134/1000 | Loss: 0.00000926
Iteration 135/1000 | Loss: 0.00000925
Iteration 136/1000 | Loss: 0.00000925
Iteration 137/1000 | Loss: 0.00000925
Iteration 138/1000 | Loss: 0.00000925
Iteration 139/1000 | Loss: 0.00000925
Iteration 140/1000 | Loss: 0.00000924
Iteration 141/1000 | Loss: 0.00000924
Iteration 142/1000 | Loss: 0.00000924
Iteration 143/1000 | Loss: 0.00000923
Iteration 144/1000 | Loss: 0.00000923
Iteration 145/1000 | Loss: 0.00000923
Iteration 146/1000 | Loss: 0.00000923
Iteration 147/1000 | Loss: 0.00000923
Iteration 148/1000 | Loss: 0.00000923
Iteration 149/1000 | Loss: 0.00000922
Iteration 150/1000 | Loss: 0.00000922
Iteration 151/1000 | Loss: 0.00000922
Iteration 152/1000 | Loss: 0.00000922
Iteration 153/1000 | Loss: 0.00000922
Iteration 154/1000 | Loss: 0.00000922
Iteration 155/1000 | Loss: 0.00000922
Iteration 156/1000 | Loss: 0.00000921
Iteration 157/1000 | Loss: 0.00000921
Iteration 158/1000 | Loss: 0.00000921
Iteration 159/1000 | Loss: 0.00000921
Iteration 160/1000 | Loss: 0.00000921
Iteration 161/1000 | Loss: 0.00000921
Iteration 162/1000 | Loss: 0.00000921
Iteration 163/1000 | Loss: 0.00000921
Iteration 164/1000 | Loss: 0.00000921
Iteration 165/1000 | Loss: 0.00000921
Iteration 166/1000 | Loss: 0.00000921
Iteration 167/1000 | Loss: 0.00000921
Iteration 168/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [9.207893526763655e-06, 9.207893526763655e-06, 9.207893526763655e-06, 9.207893526763655e-06, 9.207893526763655e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.207893526763655e-06

Optimization complete. Final v2v error: 2.5912277698516846 mm

Highest mean error: 3.084196090698242 mm for frame 148

Lowest mean error: 2.32853364944458 mm for frame 57

Saving results

Total time: 41.514317989349365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997217
Iteration 2/25 | Loss: 0.00248844
Iteration 3/25 | Loss: 0.00175831
Iteration 4/25 | Loss: 0.00162000
Iteration 5/25 | Loss: 0.00170769
Iteration 6/25 | Loss: 0.00160315
Iteration 7/25 | Loss: 0.00151259
Iteration 8/25 | Loss: 0.00146228
Iteration 9/25 | Loss: 0.00136900
Iteration 10/25 | Loss: 0.00132640
Iteration 11/25 | Loss: 0.00131740
Iteration 12/25 | Loss: 0.00132061
Iteration 13/25 | Loss: 0.00130895
Iteration 14/25 | Loss: 0.00129004
Iteration 15/25 | Loss: 0.00129064
Iteration 16/25 | Loss: 0.00128853
Iteration 17/25 | Loss: 0.00127894
Iteration 18/25 | Loss: 0.00127038
Iteration 19/25 | Loss: 0.00126824
Iteration 20/25 | Loss: 0.00126682
Iteration 21/25 | Loss: 0.00126993
Iteration 22/25 | Loss: 0.00126984
Iteration 23/25 | Loss: 0.00126545
Iteration 24/25 | Loss: 0.00126771
Iteration 25/25 | Loss: 0.00126880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36327016
Iteration 2/25 | Loss: 0.00223559
Iteration 3/25 | Loss: 0.00208449
Iteration 4/25 | Loss: 0.00208449
Iteration 5/25 | Loss: 0.00208449
Iteration 6/25 | Loss: 0.00208449
Iteration 7/25 | Loss: 0.00208449
Iteration 8/25 | Loss: 0.00208449
Iteration 9/25 | Loss: 0.00208449
Iteration 10/25 | Loss: 0.00208449
Iteration 11/25 | Loss: 0.00208449
Iteration 12/25 | Loss: 0.00208449
Iteration 13/25 | Loss: 0.00208449
Iteration 14/25 | Loss: 0.00208449
Iteration 15/25 | Loss: 0.00208449
Iteration 16/25 | Loss: 0.00208449
Iteration 17/25 | Loss: 0.00208449
Iteration 18/25 | Loss: 0.00208449
Iteration 19/25 | Loss: 0.00208449
Iteration 20/25 | Loss: 0.00208449
Iteration 21/25 | Loss: 0.00208449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0020844885148108006, 0.0020844885148108006, 0.0020844885148108006, 0.0020844885148108006, 0.0020844885148108006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020844885148108006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208449
Iteration 2/1000 | Loss: 0.00106146
Iteration 3/1000 | Loss: 0.00123252
Iteration 4/1000 | Loss: 0.00107921
Iteration 5/1000 | Loss: 0.00023608
Iteration 6/1000 | Loss: 0.00049583
Iteration 7/1000 | Loss: 0.00062212
Iteration 8/1000 | Loss: 0.00117750
Iteration 9/1000 | Loss: 0.00055127
Iteration 10/1000 | Loss: 0.00021033
Iteration 11/1000 | Loss: 0.00069345
Iteration 12/1000 | Loss: 0.00047258
Iteration 13/1000 | Loss: 0.00214061
Iteration 14/1000 | Loss: 0.00122416
Iteration 15/1000 | Loss: 0.00041694
Iteration 16/1000 | Loss: 0.00040533
Iteration 17/1000 | Loss: 0.00025840
Iteration 18/1000 | Loss: 0.00066492
Iteration 19/1000 | Loss: 0.00017331
Iteration 20/1000 | Loss: 0.00038609
Iteration 21/1000 | Loss: 0.00015897
Iteration 22/1000 | Loss: 0.00075354
Iteration 23/1000 | Loss: 0.00015453
Iteration 24/1000 | Loss: 0.00046255
Iteration 25/1000 | Loss: 0.00030748
Iteration 26/1000 | Loss: 0.00036972
Iteration 27/1000 | Loss: 0.00034040
Iteration 28/1000 | Loss: 0.00009740
Iteration 29/1000 | Loss: 0.00029948
Iteration 30/1000 | Loss: 0.00087149
Iteration 31/1000 | Loss: 0.00015328
Iteration 32/1000 | Loss: 0.00010058
Iteration 33/1000 | Loss: 0.00009408
Iteration 34/1000 | Loss: 0.00016423
Iteration 35/1000 | Loss: 0.00026682
Iteration 36/1000 | Loss: 0.00021909
Iteration 37/1000 | Loss: 0.00015986
Iteration 38/1000 | Loss: 0.00011854
Iteration 39/1000 | Loss: 0.00016042
Iteration 40/1000 | Loss: 0.00013015
Iteration 41/1000 | Loss: 0.00009306
Iteration 42/1000 | Loss: 0.00009440
Iteration 43/1000 | Loss: 0.00009229
Iteration 44/1000 | Loss: 0.00055170
Iteration 45/1000 | Loss: 0.00036809
Iteration 46/1000 | Loss: 0.00021454
Iteration 47/1000 | Loss: 0.00026725
Iteration 48/1000 | Loss: 0.00050940
Iteration 49/1000 | Loss: 0.00018280
Iteration 50/1000 | Loss: 0.00038488
Iteration 51/1000 | Loss: 0.00012023
Iteration 52/1000 | Loss: 0.00008406
Iteration 53/1000 | Loss: 0.00013895
Iteration 54/1000 | Loss: 0.00029748
Iteration 55/1000 | Loss: 0.00047495
Iteration 56/1000 | Loss: 0.00012129
Iteration 57/1000 | Loss: 0.00009099
Iteration 58/1000 | Loss: 0.00008483
Iteration 59/1000 | Loss: 0.00022181
Iteration 60/1000 | Loss: 0.00061826
Iteration 61/1000 | Loss: 0.00018978
Iteration 62/1000 | Loss: 0.00024587
Iteration 63/1000 | Loss: 0.00011523
Iteration 64/1000 | Loss: 0.00014918
Iteration 65/1000 | Loss: 0.00009452
Iteration 66/1000 | Loss: 0.00034007
Iteration 67/1000 | Loss: 0.00008555
Iteration 68/1000 | Loss: 0.00016776
Iteration 69/1000 | Loss: 0.00006930
Iteration 70/1000 | Loss: 0.00018506
Iteration 71/1000 | Loss: 0.00015019
Iteration 72/1000 | Loss: 0.00013999
Iteration 73/1000 | Loss: 0.00006715
Iteration 74/1000 | Loss: 0.00007070
Iteration 75/1000 | Loss: 0.00007070
Iteration 76/1000 | Loss: 0.00023334
Iteration 77/1000 | Loss: 0.00005494
Iteration 78/1000 | Loss: 0.00009825
Iteration 79/1000 | Loss: 0.00007788
Iteration 80/1000 | Loss: 0.00007530
Iteration 81/1000 | Loss: 0.00008051
Iteration 82/1000 | Loss: 0.00030551
Iteration 83/1000 | Loss: 0.00014938
Iteration 84/1000 | Loss: 0.00007186
Iteration 85/1000 | Loss: 0.00056042
Iteration 86/1000 | Loss: 0.00004437
Iteration 87/1000 | Loss: 0.00005139
Iteration 88/1000 | Loss: 0.00005221
Iteration 89/1000 | Loss: 0.00080815
Iteration 90/1000 | Loss: 0.00005151
Iteration 91/1000 | Loss: 0.00005297
Iteration 92/1000 | Loss: 0.00033985
Iteration 93/1000 | Loss: 0.00078632
Iteration 94/1000 | Loss: 0.00020930
Iteration 95/1000 | Loss: 0.00018659
Iteration 96/1000 | Loss: 0.00004411
Iteration 97/1000 | Loss: 0.00004912
Iteration 98/1000 | Loss: 0.00046568
Iteration 99/1000 | Loss: 0.00004650
Iteration 100/1000 | Loss: 0.00022578
Iteration 101/1000 | Loss: 0.00025672
Iteration 102/1000 | Loss: 0.00004062
Iteration 103/1000 | Loss: 0.00007101
Iteration 104/1000 | Loss: 0.00021986
Iteration 105/1000 | Loss: 0.00019160
Iteration 106/1000 | Loss: 0.00014152
Iteration 107/1000 | Loss: 0.00003495
Iteration 108/1000 | Loss: 0.00003516
Iteration 109/1000 | Loss: 0.00004129
Iteration 110/1000 | Loss: 0.00003292
Iteration 111/1000 | Loss: 0.00004268
Iteration 112/1000 | Loss: 0.00003934
Iteration 113/1000 | Loss: 0.00013000
Iteration 114/1000 | Loss: 0.00015416
Iteration 115/1000 | Loss: 0.00063392
Iteration 116/1000 | Loss: 0.00015160
Iteration 117/1000 | Loss: 0.00007705
Iteration 118/1000 | Loss: 0.00020366
Iteration 119/1000 | Loss: 0.00040521
Iteration 120/1000 | Loss: 0.00018239
Iteration 121/1000 | Loss: 0.00024261
Iteration 122/1000 | Loss: 0.00028764
Iteration 123/1000 | Loss: 0.00025170
Iteration 124/1000 | Loss: 0.00019494
Iteration 125/1000 | Loss: 0.00014762
Iteration 126/1000 | Loss: 0.00010514
Iteration 127/1000 | Loss: 0.00004692
Iteration 128/1000 | Loss: 0.00003355
Iteration 129/1000 | Loss: 0.00018923
Iteration 130/1000 | Loss: 0.00003453
Iteration 131/1000 | Loss: 0.00004052
Iteration 132/1000 | Loss: 0.00003472
Iteration 133/1000 | Loss: 0.00002913
Iteration 134/1000 | Loss: 0.00002455
Iteration 135/1000 | Loss: 0.00002976
Iteration 136/1000 | Loss: 0.00003263
Iteration 137/1000 | Loss: 0.00003103
Iteration 138/1000 | Loss: 0.00003146
Iteration 139/1000 | Loss: 0.00003075
Iteration 140/1000 | Loss: 0.00049966
Iteration 141/1000 | Loss: 0.00048815
Iteration 142/1000 | Loss: 0.00028645
Iteration 143/1000 | Loss: 0.00013908
Iteration 144/1000 | Loss: 0.00006476
Iteration 145/1000 | Loss: 0.00005239
Iteration 146/1000 | Loss: 0.00003701
Iteration 147/1000 | Loss: 0.00003411
Iteration 148/1000 | Loss: 0.00002462
Iteration 149/1000 | Loss: 0.00047068
Iteration 150/1000 | Loss: 0.00103352
Iteration 151/1000 | Loss: 0.00046345
Iteration 152/1000 | Loss: 0.00028991
Iteration 153/1000 | Loss: 0.00008710
Iteration 154/1000 | Loss: 0.00002822
Iteration 155/1000 | Loss: 0.00002287
Iteration 156/1000 | Loss: 0.00002273
Iteration 157/1000 | Loss: 0.00001725
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00001562
Iteration 160/1000 | Loss: 0.00001386
Iteration 161/1000 | Loss: 0.00001334
Iteration 162/1000 | Loss: 0.00001290
Iteration 163/1000 | Loss: 0.00001250
Iteration 164/1000 | Loss: 0.00001216
Iteration 165/1000 | Loss: 0.00001188
Iteration 166/1000 | Loss: 0.00001172
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001162
Iteration 171/1000 | Loss: 0.00001154
Iteration 172/1000 | Loss: 0.00001151
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001150
Iteration 176/1000 | Loss: 0.00001149
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001147
Iteration 179/1000 | Loss: 0.00001147
Iteration 180/1000 | Loss: 0.00001147
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001145
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001144
Iteration 185/1000 | Loss: 0.00001144
Iteration 186/1000 | Loss: 0.00001144
Iteration 187/1000 | Loss: 0.00001143
Iteration 188/1000 | Loss: 0.00001143
Iteration 189/1000 | Loss: 0.00001142
Iteration 190/1000 | Loss: 0.00001142
Iteration 191/1000 | Loss: 0.00001142
Iteration 192/1000 | Loss: 0.00001142
Iteration 193/1000 | Loss: 0.00001141
Iteration 194/1000 | Loss: 0.00001141
Iteration 195/1000 | Loss: 0.00001140
Iteration 196/1000 | Loss: 0.00001140
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001138
Iteration 201/1000 | Loss: 0.00001137
Iteration 202/1000 | Loss: 0.00001137
Iteration 203/1000 | Loss: 0.00001137
Iteration 204/1000 | Loss: 0.00001137
Iteration 205/1000 | Loss: 0.00001137
Iteration 206/1000 | Loss: 0.00001137
Iteration 207/1000 | Loss: 0.00001137
Iteration 208/1000 | Loss: 0.00001136
Iteration 209/1000 | Loss: 0.00001136
Iteration 210/1000 | Loss: 0.00001136
Iteration 211/1000 | Loss: 0.00001136
Iteration 212/1000 | Loss: 0.00001136
Iteration 213/1000 | Loss: 0.00001136
Iteration 214/1000 | Loss: 0.00001136
Iteration 215/1000 | Loss: 0.00001136
Iteration 216/1000 | Loss: 0.00001136
Iteration 217/1000 | Loss: 0.00001136
Iteration 218/1000 | Loss: 0.00001135
Iteration 219/1000 | Loss: 0.00001135
Iteration 220/1000 | Loss: 0.00001135
Iteration 221/1000 | Loss: 0.00001135
Iteration 222/1000 | Loss: 0.00001135
Iteration 223/1000 | Loss: 0.00001135
Iteration 224/1000 | Loss: 0.00001135
Iteration 225/1000 | Loss: 0.00001135
Iteration 226/1000 | Loss: 0.00001134
Iteration 227/1000 | Loss: 0.00001134
Iteration 228/1000 | Loss: 0.00001134
Iteration 229/1000 | Loss: 0.00001134
Iteration 230/1000 | Loss: 0.00001134
Iteration 231/1000 | Loss: 0.00001134
Iteration 232/1000 | Loss: 0.00001134
Iteration 233/1000 | Loss: 0.00001134
Iteration 234/1000 | Loss: 0.00001134
Iteration 235/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.1339724551362451e-05, 1.1339724551362451e-05, 1.1339724551362451e-05, 1.1339724551362451e-05, 1.1339724551362451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1339724551362451e-05

Optimization complete. Final v2v error: 2.7831239700317383 mm

Highest mean error: 5.292945861816406 mm for frame 141

Lowest mean error: 2.2507364749908447 mm for frame 68

Saving results

Total time: 330.62150406837463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766263
Iteration 2/25 | Loss: 0.00118848
Iteration 3/25 | Loss: 0.00109984
Iteration 4/25 | Loss: 0.00108498
Iteration 5/25 | Loss: 0.00108042
Iteration 6/25 | Loss: 0.00107932
Iteration 7/25 | Loss: 0.00107916
Iteration 8/25 | Loss: 0.00107916
Iteration 9/25 | Loss: 0.00107916
Iteration 10/25 | Loss: 0.00107916
Iteration 11/25 | Loss: 0.00107916
Iteration 12/25 | Loss: 0.00107916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010791614186018705, 0.0010791614186018705, 0.0010791614186018705, 0.0010791614186018705, 0.0010791614186018705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010791614186018705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34741890
Iteration 2/25 | Loss: 0.00097482
Iteration 3/25 | Loss: 0.00097481
Iteration 4/25 | Loss: 0.00097481
Iteration 5/25 | Loss: 0.00097481
Iteration 6/25 | Loss: 0.00097481
Iteration 7/25 | Loss: 0.00097481
Iteration 8/25 | Loss: 0.00097481
Iteration 9/25 | Loss: 0.00097481
Iteration 10/25 | Loss: 0.00097481
Iteration 11/25 | Loss: 0.00097481
Iteration 12/25 | Loss: 0.00097481
Iteration 13/25 | Loss: 0.00097481
Iteration 14/25 | Loss: 0.00097481
Iteration 15/25 | Loss: 0.00097481
Iteration 16/25 | Loss: 0.00097481
Iteration 17/25 | Loss: 0.00097481
Iteration 18/25 | Loss: 0.00097481
Iteration 19/25 | Loss: 0.00097481
Iteration 20/25 | Loss: 0.00097481
Iteration 21/25 | Loss: 0.00097481
Iteration 22/25 | Loss: 0.00097481
Iteration 23/25 | Loss: 0.00097481
Iteration 24/25 | Loss: 0.00097481
Iteration 25/25 | Loss: 0.00097481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097481
Iteration 2/1000 | Loss: 0.00003530
Iteration 3/1000 | Loss: 0.00001964
Iteration 4/1000 | Loss: 0.00001533
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001264
Iteration 7/1000 | Loss: 0.00001208
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001144
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001109
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001074
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001066
Iteration 18/1000 | Loss: 0.00001065
Iteration 19/1000 | Loss: 0.00001065
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001064
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001063
Iteration 24/1000 | Loss: 0.00001063
Iteration 25/1000 | Loss: 0.00001062
Iteration 26/1000 | Loss: 0.00001062
Iteration 27/1000 | Loss: 0.00001061
Iteration 28/1000 | Loss: 0.00001060
Iteration 29/1000 | Loss: 0.00001060
Iteration 30/1000 | Loss: 0.00001060
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001057
Iteration 33/1000 | Loss: 0.00001056
Iteration 34/1000 | Loss: 0.00001056
Iteration 35/1000 | Loss: 0.00001056
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001053
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001052
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001051
Iteration 50/1000 | Loss: 0.00001051
Iteration 51/1000 | Loss: 0.00001051
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001050
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001049
Iteration 60/1000 | Loss: 0.00001049
Iteration 61/1000 | Loss: 0.00001049
Iteration 62/1000 | Loss: 0.00001048
Iteration 63/1000 | Loss: 0.00001048
Iteration 64/1000 | Loss: 0.00001048
Iteration 65/1000 | Loss: 0.00001048
Iteration 66/1000 | Loss: 0.00001048
Iteration 67/1000 | Loss: 0.00001048
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001047
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001046
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001046
Iteration 87/1000 | Loss: 0.00001046
Iteration 88/1000 | Loss: 0.00001046
Iteration 89/1000 | Loss: 0.00001046
Iteration 90/1000 | Loss: 0.00001045
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001045
Iteration 93/1000 | Loss: 0.00001045
Iteration 94/1000 | Loss: 0.00001045
Iteration 95/1000 | Loss: 0.00001045
Iteration 96/1000 | Loss: 0.00001045
Iteration 97/1000 | Loss: 0.00001044
Iteration 98/1000 | Loss: 0.00001044
Iteration 99/1000 | Loss: 0.00001044
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001043
Iteration 104/1000 | Loss: 0.00001042
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001041
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001041
Iteration 111/1000 | Loss: 0.00001041
Iteration 112/1000 | Loss: 0.00001041
Iteration 113/1000 | Loss: 0.00001041
Iteration 114/1000 | Loss: 0.00001041
Iteration 115/1000 | Loss: 0.00001041
Iteration 116/1000 | Loss: 0.00001040
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001040
Iteration 119/1000 | Loss: 0.00001040
Iteration 120/1000 | Loss: 0.00001040
Iteration 121/1000 | Loss: 0.00001040
Iteration 122/1000 | Loss: 0.00001040
Iteration 123/1000 | Loss: 0.00001040
Iteration 124/1000 | Loss: 0.00001040
Iteration 125/1000 | Loss: 0.00001040
Iteration 126/1000 | Loss: 0.00001040
Iteration 127/1000 | Loss: 0.00001039
Iteration 128/1000 | Loss: 0.00001039
Iteration 129/1000 | Loss: 0.00001039
Iteration 130/1000 | Loss: 0.00001039
Iteration 131/1000 | Loss: 0.00001039
Iteration 132/1000 | Loss: 0.00001039
Iteration 133/1000 | Loss: 0.00001039
Iteration 134/1000 | Loss: 0.00001038
Iteration 135/1000 | Loss: 0.00001038
Iteration 136/1000 | Loss: 0.00001038
Iteration 137/1000 | Loss: 0.00001038
Iteration 138/1000 | Loss: 0.00001038
Iteration 139/1000 | Loss: 0.00001038
Iteration 140/1000 | Loss: 0.00001038
Iteration 141/1000 | Loss: 0.00001038
Iteration 142/1000 | Loss: 0.00001038
Iteration 143/1000 | Loss: 0.00001038
Iteration 144/1000 | Loss: 0.00001038
Iteration 145/1000 | Loss: 0.00001038
Iteration 146/1000 | Loss: 0.00001038
Iteration 147/1000 | Loss: 0.00001037
Iteration 148/1000 | Loss: 0.00001037
Iteration 149/1000 | Loss: 0.00001037
Iteration 150/1000 | Loss: 0.00001037
Iteration 151/1000 | Loss: 0.00001037
Iteration 152/1000 | Loss: 0.00001037
Iteration 153/1000 | Loss: 0.00001037
Iteration 154/1000 | Loss: 0.00001037
Iteration 155/1000 | Loss: 0.00001037
Iteration 156/1000 | Loss: 0.00001037
Iteration 157/1000 | Loss: 0.00001037
Iteration 158/1000 | Loss: 0.00001037
Iteration 159/1000 | Loss: 0.00001037
Iteration 160/1000 | Loss: 0.00001037
Iteration 161/1000 | Loss: 0.00001037
Iteration 162/1000 | Loss: 0.00001037
Iteration 163/1000 | Loss: 0.00001037
Iteration 164/1000 | Loss: 0.00001036
Iteration 165/1000 | Loss: 0.00001036
Iteration 166/1000 | Loss: 0.00001036
Iteration 167/1000 | Loss: 0.00001036
Iteration 168/1000 | Loss: 0.00001036
Iteration 169/1000 | Loss: 0.00001036
Iteration 170/1000 | Loss: 0.00001036
Iteration 171/1000 | Loss: 0.00001036
Iteration 172/1000 | Loss: 0.00001036
Iteration 173/1000 | Loss: 0.00001036
Iteration 174/1000 | Loss: 0.00001036
Iteration 175/1000 | Loss: 0.00001036
Iteration 176/1000 | Loss: 0.00001036
Iteration 177/1000 | Loss: 0.00001036
Iteration 178/1000 | Loss: 0.00001036
Iteration 179/1000 | Loss: 0.00001035
Iteration 180/1000 | Loss: 0.00001035
Iteration 181/1000 | Loss: 0.00001035
Iteration 182/1000 | Loss: 0.00001035
Iteration 183/1000 | Loss: 0.00001035
Iteration 184/1000 | Loss: 0.00001035
Iteration 185/1000 | Loss: 0.00001035
Iteration 186/1000 | Loss: 0.00001035
Iteration 187/1000 | Loss: 0.00001035
Iteration 188/1000 | Loss: 0.00001035
Iteration 189/1000 | Loss: 0.00001035
Iteration 190/1000 | Loss: 0.00001035
Iteration 191/1000 | Loss: 0.00001035
Iteration 192/1000 | Loss: 0.00001035
Iteration 193/1000 | Loss: 0.00001034
Iteration 194/1000 | Loss: 0.00001034
Iteration 195/1000 | Loss: 0.00001034
Iteration 196/1000 | Loss: 0.00001034
Iteration 197/1000 | Loss: 0.00001034
Iteration 198/1000 | Loss: 0.00001034
Iteration 199/1000 | Loss: 0.00001034
Iteration 200/1000 | Loss: 0.00001034
Iteration 201/1000 | Loss: 0.00001034
Iteration 202/1000 | Loss: 0.00001034
Iteration 203/1000 | Loss: 0.00001034
Iteration 204/1000 | Loss: 0.00001034
Iteration 205/1000 | Loss: 0.00001033
Iteration 206/1000 | Loss: 0.00001033
Iteration 207/1000 | Loss: 0.00001033
Iteration 208/1000 | Loss: 0.00001033
Iteration 209/1000 | Loss: 0.00001033
Iteration 210/1000 | Loss: 0.00001033
Iteration 211/1000 | Loss: 0.00001033
Iteration 212/1000 | Loss: 0.00001033
Iteration 213/1000 | Loss: 0.00001033
Iteration 214/1000 | Loss: 0.00001033
Iteration 215/1000 | Loss: 0.00001033
Iteration 216/1000 | Loss: 0.00001033
Iteration 217/1000 | Loss: 0.00001033
Iteration 218/1000 | Loss: 0.00001033
Iteration 219/1000 | Loss: 0.00001033
Iteration 220/1000 | Loss: 0.00001033
Iteration 221/1000 | Loss: 0.00001033
Iteration 222/1000 | Loss: 0.00001032
Iteration 223/1000 | Loss: 0.00001032
Iteration 224/1000 | Loss: 0.00001032
Iteration 225/1000 | Loss: 0.00001032
Iteration 226/1000 | Loss: 0.00001032
Iteration 227/1000 | Loss: 0.00001032
Iteration 228/1000 | Loss: 0.00001032
Iteration 229/1000 | Loss: 0.00001032
Iteration 230/1000 | Loss: 0.00001032
Iteration 231/1000 | Loss: 0.00001032
Iteration 232/1000 | Loss: 0.00001032
Iteration 233/1000 | Loss: 0.00001032
Iteration 234/1000 | Loss: 0.00001032
Iteration 235/1000 | Loss: 0.00001032
Iteration 236/1000 | Loss: 0.00001032
Iteration 237/1000 | Loss: 0.00001032
Iteration 238/1000 | Loss: 0.00001032
Iteration 239/1000 | Loss: 0.00001032
Iteration 240/1000 | Loss: 0.00001032
Iteration 241/1000 | Loss: 0.00001031
Iteration 242/1000 | Loss: 0.00001031
Iteration 243/1000 | Loss: 0.00001031
Iteration 244/1000 | Loss: 0.00001031
Iteration 245/1000 | Loss: 0.00001031
Iteration 246/1000 | Loss: 0.00001031
Iteration 247/1000 | Loss: 0.00001031
Iteration 248/1000 | Loss: 0.00001031
Iteration 249/1000 | Loss: 0.00001031
Iteration 250/1000 | Loss: 0.00001031
Iteration 251/1000 | Loss: 0.00001031
Iteration 252/1000 | Loss: 0.00001031
Iteration 253/1000 | Loss: 0.00001031
Iteration 254/1000 | Loss: 0.00001031
Iteration 255/1000 | Loss: 0.00001031
Iteration 256/1000 | Loss: 0.00001031
Iteration 257/1000 | Loss: 0.00001031
Iteration 258/1000 | Loss: 0.00001031
Iteration 259/1000 | Loss: 0.00001031
Iteration 260/1000 | Loss: 0.00001031
Iteration 261/1000 | Loss: 0.00001031
Iteration 262/1000 | Loss: 0.00001031
Iteration 263/1000 | Loss: 0.00001031
Iteration 264/1000 | Loss: 0.00001031
Iteration 265/1000 | Loss: 0.00001031
Iteration 266/1000 | Loss: 0.00001031
Iteration 267/1000 | Loss: 0.00001031
Iteration 268/1000 | Loss: 0.00001031
Iteration 269/1000 | Loss: 0.00001031
Iteration 270/1000 | Loss: 0.00001031
Iteration 271/1000 | Loss: 0.00001031
Iteration 272/1000 | Loss: 0.00001031
Iteration 273/1000 | Loss: 0.00001031
Iteration 274/1000 | Loss: 0.00001031
Iteration 275/1000 | Loss: 0.00001031
Iteration 276/1000 | Loss: 0.00001031
Iteration 277/1000 | Loss: 0.00001031
Iteration 278/1000 | Loss: 0.00001031
Iteration 279/1000 | Loss: 0.00001031
Iteration 280/1000 | Loss: 0.00001031
Iteration 281/1000 | Loss: 0.00001031
Iteration 282/1000 | Loss: 0.00001031
Iteration 283/1000 | Loss: 0.00001031
Iteration 284/1000 | Loss: 0.00001031
Iteration 285/1000 | Loss: 0.00001031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.0308110176993068e-05, 1.0308110176993068e-05, 1.0308110176993068e-05, 1.0308110176993068e-05, 1.0308110176993068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0308110176993068e-05

Optimization complete. Final v2v error: 2.742032289505005 mm

Highest mean error: 3.2170820236206055 mm for frame 101

Lowest mean error: 2.443697929382324 mm for frame 31

Saving results

Total time: 45.66458296775818
