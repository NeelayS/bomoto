Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=273, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15288-15343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900592
Iteration 2/25 | Loss: 0.00130633
Iteration 3/25 | Loss: 0.00119224
Iteration 4/25 | Loss: 0.00117675
Iteration 5/25 | Loss: 0.00117279
Iteration 6/25 | Loss: 0.00117204
Iteration 7/25 | Loss: 0.00117204
Iteration 8/25 | Loss: 0.00117204
Iteration 9/25 | Loss: 0.00117204
Iteration 10/25 | Loss: 0.00117204
Iteration 11/25 | Loss: 0.00117204
Iteration 12/25 | Loss: 0.00117204
Iteration 13/25 | Loss: 0.00117204
Iteration 14/25 | Loss: 0.00117204
Iteration 15/25 | Loss: 0.00117204
Iteration 16/25 | Loss: 0.00117204
Iteration 17/25 | Loss: 0.00117204
Iteration 18/25 | Loss: 0.00117204
Iteration 19/25 | Loss: 0.00117204
Iteration 20/25 | Loss: 0.00117204
Iteration 21/25 | Loss: 0.00117204
Iteration 22/25 | Loss: 0.00117204
Iteration 23/25 | Loss: 0.00117204
Iteration 24/25 | Loss: 0.00117204
Iteration 25/25 | Loss: 0.00117204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32852376
Iteration 2/25 | Loss: 0.00057572
Iteration 3/25 | Loss: 0.00057570
Iteration 4/25 | Loss: 0.00057570
Iteration 5/25 | Loss: 0.00057570
Iteration 6/25 | Loss: 0.00057570
Iteration 7/25 | Loss: 0.00057570
Iteration 8/25 | Loss: 0.00057570
Iteration 9/25 | Loss: 0.00057570
Iteration 10/25 | Loss: 0.00057570
Iteration 11/25 | Loss: 0.00057570
Iteration 12/25 | Loss: 0.00057570
Iteration 13/25 | Loss: 0.00057570
Iteration 14/25 | Loss: 0.00057570
Iteration 15/25 | Loss: 0.00057570
Iteration 16/25 | Loss: 0.00057570
Iteration 17/25 | Loss: 0.00057570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005756961181759834, 0.0005756961181759834, 0.0005756961181759834, 0.0005756961181759834, 0.0005756961181759834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005756961181759834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057570
Iteration 2/1000 | Loss: 0.00004307
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002214
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001692
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001678
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001674
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001670
Iteration 26/1000 | Loss: 0.00001670
Iteration 27/1000 | Loss: 0.00001670
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001670
Iteration 30/1000 | Loss: 0.00001669
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001668
Iteration 33/1000 | Loss: 0.00001663
Iteration 34/1000 | Loss: 0.00001662
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001658
Iteration 37/1000 | Loss: 0.00001658
Iteration 38/1000 | Loss: 0.00001658
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001657
Iteration 49/1000 | Loss: 0.00001657
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001656
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001654
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001653
Iteration 70/1000 | Loss: 0.00001653
Iteration 71/1000 | Loss: 0.00001653
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001652
Iteration 74/1000 | Loss: 0.00001652
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001651
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001651
Iteration 85/1000 | Loss: 0.00001651
Iteration 86/1000 | Loss: 0.00001651
Iteration 87/1000 | Loss: 0.00001651
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001651
Iteration 90/1000 | Loss: 0.00001651
Iteration 91/1000 | Loss: 0.00001650
Iteration 92/1000 | Loss: 0.00001650
Iteration 93/1000 | Loss: 0.00001650
Iteration 94/1000 | Loss: 0.00001650
Iteration 95/1000 | Loss: 0.00001650
Iteration 96/1000 | Loss: 0.00001650
Iteration 97/1000 | Loss: 0.00001650
Iteration 98/1000 | Loss: 0.00001650
Iteration 99/1000 | Loss: 0.00001650
Iteration 100/1000 | Loss: 0.00001650
Iteration 101/1000 | Loss: 0.00001650
Iteration 102/1000 | Loss: 0.00001650
Iteration 103/1000 | Loss: 0.00001650
Iteration 104/1000 | Loss: 0.00001650
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001649
Iteration 115/1000 | Loss: 0.00001649
Iteration 116/1000 | Loss: 0.00001649
Iteration 117/1000 | Loss: 0.00001649
Iteration 118/1000 | Loss: 0.00001649
Iteration 119/1000 | Loss: 0.00001649
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.6489531844854355e-05, 1.6489531844854355e-05, 1.6489531844854355e-05, 1.6489531844854355e-05, 1.6489531844854355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6489531844854355e-05

Optimization complete. Final v2v error: 3.493257999420166 mm

Highest mean error: 3.6848974227905273 mm for frame 96

Lowest mean error: 3.0645666122436523 mm for frame 136

Saving results

Total time: 35.88256764411926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050033
Iteration 2/25 | Loss: 0.00252349
Iteration 3/25 | Loss: 0.00207739
Iteration 4/25 | Loss: 0.00199532
Iteration 5/25 | Loss: 0.00194725
Iteration 6/25 | Loss: 0.00191575
Iteration 7/25 | Loss: 0.00192574
Iteration 8/25 | Loss: 0.00195967
Iteration 9/25 | Loss: 0.00191229
Iteration 10/25 | Loss: 0.00191216
Iteration 11/25 | Loss: 0.00190400
Iteration 12/25 | Loss: 0.00190382
Iteration 13/25 | Loss: 0.00191024
Iteration 14/25 | Loss: 0.00190372
Iteration 15/25 | Loss: 0.00190368
Iteration 16/25 | Loss: 0.00190368
Iteration 17/25 | Loss: 0.00190368
Iteration 18/25 | Loss: 0.00190367
Iteration 19/25 | Loss: 0.00190367
Iteration 20/25 | Loss: 0.00190367
Iteration 21/25 | Loss: 0.00190367
Iteration 22/25 | Loss: 0.00190367
Iteration 23/25 | Loss: 0.00190367
Iteration 24/25 | Loss: 0.00190367
Iteration 25/25 | Loss: 0.00190367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27889705
Iteration 2/25 | Loss: 0.01591425
Iteration 3/25 | Loss: 0.00639081
Iteration 4/25 | Loss: 0.00650580
Iteration 5/25 | Loss: 0.00638998
Iteration 6/25 | Loss: 0.00628964
Iteration 7/25 | Loss: 0.00628963
Iteration 8/25 | Loss: 0.00628963
Iteration 9/25 | Loss: 0.00628963
Iteration 10/25 | Loss: 0.00628963
Iteration 11/25 | Loss: 0.00628963
Iteration 12/25 | Loss: 0.00628963
Iteration 13/25 | Loss: 0.00628963
Iteration 14/25 | Loss: 0.00628963
Iteration 15/25 | Loss: 0.00628963
Iteration 16/25 | Loss: 0.00628963
Iteration 17/25 | Loss: 0.00628963
Iteration 18/25 | Loss: 0.00628963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.006289631128311157, 0.006289631128311157, 0.006289631128311157, 0.006289631128311157, 0.006289631128311157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006289631128311157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00628963
Iteration 2/1000 | Loss: 0.00774469
Iteration 3/1000 | Loss: 0.00130871
Iteration 4/1000 | Loss: 0.00092637
Iteration 5/1000 | Loss: 0.00150864
Iteration 6/1000 | Loss: 0.01394372
Iteration 7/1000 | Loss: 0.00123843
Iteration 8/1000 | Loss: 0.00391685
Iteration 9/1000 | Loss: 0.00086306
Iteration 10/1000 | Loss: 0.00201124
Iteration 11/1000 | Loss: 0.00416575
Iteration 12/1000 | Loss: 0.00177380
Iteration 13/1000 | Loss: 0.00146124
Iteration 14/1000 | Loss: 0.00049805
Iteration 15/1000 | Loss: 0.00088193
Iteration 16/1000 | Loss: 0.00054498
Iteration 17/1000 | Loss: 0.00131780
Iteration 18/1000 | Loss: 0.00180390
Iteration 19/1000 | Loss: 0.00943988
Iteration 20/1000 | Loss: 0.02122689
Iteration 21/1000 | Loss: 0.01085131
Iteration 22/1000 | Loss: 0.00372966
Iteration 23/1000 | Loss: 0.00254346
Iteration 24/1000 | Loss: 0.00046558
Iteration 25/1000 | Loss: 0.00240577
Iteration 26/1000 | Loss: 0.00039460
Iteration 27/1000 | Loss: 0.00039761
Iteration 28/1000 | Loss: 0.00031527
Iteration 29/1000 | Loss: 0.00028009
Iteration 30/1000 | Loss: 0.00150979
Iteration 31/1000 | Loss: 0.00012160
Iteration 32/1000 | Loss: 0.00007982
Iteration 33/1000 | Loss: 0.00031470
Iteration 34/1000 | Loss: 0.00017308
Iteration 35/1000 | Loss: 0.00004265
Iteration 36/1000 | Loss: 0.00011617
Iteration 37/1000 | Loss: 0.00003447
Iteration 38/1000 | Loss: 0.00025780
Iteration 39/1000 | Loss: 0.00026387
Iteration 40/1000 | Loss: 0.00005679
Iteration 41/1000 | Loss: 0.00002541
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00018901
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00018265
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00016787
Iteration 49/1000 | Loss: 0.00029377
Iteration 50/1000 | Loss: 0.00021109
Iteration 51/1000 | Loss: 0.00001931
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001800
Iteration 54/1000 | Loss: 0.00013963
Iteration 55/1000 | Loss: 0.00025308
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001800
Iteration 58/1000 | Loss: 0.00001777
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001763
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001762
Iteration 80/1000 | Loss: 0.00017547
Iteration 81/1000 | Loss: 0.00002946
Iteration 82/1000 | Loss: 0.00002298
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001760
Iteration 88/1000 | Loss: 0.00001760
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00001758
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001758
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001752
Iteration 119/1000 | Loss: 0.00001752
Iteration 120/1000 | Loss: 0.00001752
Iteration 121/1000 | Loss: 0.00001752
Iteration 122/1000 | Loss: 0.00001752
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001751
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001751
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001750
Iteration 131/1000 | Loss: 0.00001750
Iteration 132/1000 | Loss: 0.00001750
Iteration 133/1000 | Loss: 0.00001750
Iteration 134/1000 | Loss: 0.00001750
Iteration 135/1000 | Loss: 0.00001750
Iteration 136/1000 | Loss: 0.00001750
Iteration 137/1000 | Loss: 0.00001750
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001749
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001748
Iteration 147/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.7479229427408427e-05, 1.7479229427408427e-05, 1.7479229427408427e-05, 1.7479229427408427e-05, 1.7479229427408427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7479229427408427e-05

Optimization complete. Final v2v error: 3.3680529594421387 mm

Highest mean error: 9.795382499694824 mm for frame 7

Lowest mean error: 3.1527397632598877 mm for frame 79

Saving results

Total time: 136.73448991775513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476705
Iteration 2/25 | Loss: 0.00137611
Iteration 3/25 | Loss: 0.00121816
Iteration 4/25 | Loss: 0.00119541
Iteration 5/25 | Loss: 0.00119089
Iteration 6/25 | Loss: 0.00119055
Iteration 7/25 | Loss: 0.00119055
Iteration 8/25 | Loss: 0.00119055
Iteration 9/25 | Loss: 0.00119055
Iteration 10/25 | Loss: 0.00119055
Iteration 11/25 | Loss: 0.00119055
Iteration 12/25 | Loss: 0.00119055
Iteration 13/25 | Loss: 0.00119055
Iteration 14/25 | Loss: 0.00119055
Iteration 15/25 | Loss: 0.00119055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011905456194654107, 0.0011905456194654107, 0.0011905456194654107, 0.0011905456194654107, 0.0011905456194654107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011905456194654107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29554033
Iteration 2/25 | Loss: 0.00056311
Iteration 3/25 | Loss: 0.00056311
Iteration 4/25 | Loss: 0.00056311
Iteration 5/25 | Loss: 0.00056311
Iteration 6/25 | Loss: 0.00056311
Iteration 7/25 | Loss: 0.00056311
Iteration 8/25 | Loss: 0.00056311
Iteration 9/25 | Loss: 0.00056311
Iteration 10/25 | Loss: 0.00056311
Iteration 11/25 | Loss: 0.00056311
Iteration 12/25 | Loss: 0.00056311
Iteration 13/25 | Loss: 0.00056311
Iteration 14/25 | Loss: 0.00056311
Iteration 15/25 | Loss: 0.00056311
Iteration 16/25 | Loss: 0.00056310
Iteration 17/25 | Loss: 0.00056311
Iteration 18/25 | Loss: 0.00056311
Iteration 19/25 | Loss: 0.00056310
Iteration 20/25 | Loss: 0.00056311
Iteration 21/25 | Loss: 0.00056310
Iteration 22/25 | Loss: 0.00056310
Iteration 23/25 | Loss: 0.00056310
Iteration 24/25 | Loss: 0.00056310
Iteration 25/25 | Loss: 0.00056310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056310
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00002852
Iteration 4/1000 | Loss: 0.00002640
Iteration 5/1000 | Loss: 0.00002527
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002314
Iteration 8/1000 | Loss: 0.00002248
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002181
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002174
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002156
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002148
Iteration 20/1000 | Loss: 0.00002147
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002146
Iteration 23/1000 | Loss: 0.00002144
Iteration 24/1000 | Loss: 0.00002135
Iteration 25/1000 | Loss: 0.00002135
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002134
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002132
Iteration 34/1000 | Loss: 0.00002131
Iteration 35/1000 | Loss: 0.00002131
Iteration 36/1000 | Loss: 0.00002131
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002129
Iteration 44/1000 | Loss: 0.00002129
Iteration 45/1000 | Loss: 0.00002128
Iteration 46/1000 | Loss: 0.00002128
Iteration 47/1000 | Loss: 0.00002128
Iteration 48/1000 | Loss: 0.00002128
Iteration 49/1000 | Loss: 0.00002127
Iteration 50/1000 | Loss: 0.00002127
Iteration 51/1000 | Loss: 0.00002127
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002125
Iteration 63/1000 | Loss: 0.00002125
Iteration 64/1000 | Loss: 0.00002124
Iteration 65/1000 | Loss: 0.00002124
Iteration 66/1000 | Loss: 0.00002122
Iteration 67/1000 | Loss: 0.00002122
Iteration 68/1000 | Loss: 0.00002121
Iteration 69/1000 | Loss: 0.00002121
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00002120
Iteration 72/1000 | Loss: 0.00002120
Iteration 73/1000 | Loss: 0.00002119
Iteration 74/1000 | Loss: 0.00002119
Iteration 75/1000 | Loss: 0.00002119
Iteration 76/1000 | Loss: 0.00002118
Iteration 77/1000 | Loss: 0.00002118
Iteration 78/1000 | Loss: 0.00002118
Iteration 79/1000 | Loss: 0.00002117
Iteration 80/1000 | Loss: 0.00002117
Iteration 81/1000 | Loss: 0.00002117
Iteration 82/1000 | Loss: 0.00002117
Iteration 83/1000 | Loss: 0.00002116
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002116
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00002116
Iteration 89/1000 | Loss: 0.00002116
Iteration 90/1000 | Loss: 0.00002116
Iteration 91/1000 | Loss: 0.00002116
Iteration 92/1000 | Loss: 0.00002116
Iteration 93/1000 | Loss: 0.00002116
Iteration 94/1000 | Loss: 0.00002115
Iteration 95/1000 | Loss: 0.00002115
Iteration 96/1000 | Loss: 0.00002115
Iteration 97/1000 | Loss: 0.00002115
Iteration 98/1000 | Loss: 0.00002115
Iteration 99/1000 | Loss: 0.00002115
Iteration 100/1000 | Loss: 0.00002115
Iteration 101/1000 | Loss: 0.00002115
Iteration 102/1000 | Loss: 0.00002114
Iteration 103/1000 | Loss: 0.00002114
Iteration 104/1000 | Loss: 0.00002114
Iteration 105/1000 | Loss: 0.00002114
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002113
Iteration 109/1000 | Loss: 0.00002113
Iteration 110/1000 | Loss: 0.00002113
Iteration 111/1000 | Loss: 0.00002113
Iteration 112/1000 | Loss: 0.00002113
Iteration 113/1000 | Loss: 0.00002113
Iteration 114/1000 | Loss: 0.00002112
Iteration 115/1000 | Loss: 0.00002112
Iteration 116/1000 | Loss: 0.00002112
Iteration 117/1000 | Loss: 0.00002112
Iteration 118/1000 | Loss: 0.00002112
Iteration 119/1000 | Loss: 0.00002112
Iteration 120/1000 | Loss: 0.00002112
Iteration 121/1000 | Loss: 0.00002112
Iteration 122/1000 | Loss: 0.00002112
Iteration 123/1000 | Loss: 0.00002112
Iteration 124/1000 | Loss: 0.00002112
Iteration 125/1000 | Loss: 0.00002112
Iteration 126/1000 | Loss: 0.00002112
Iteration 127/1000 | Loss: 0.00002112
Iteration 128/1000 | Loss: 0.00002112
Iteration 129/1000 | Loss: 0.00002112
Iteration 130/1000 | Loss: 0.00002112
Iteration 131/1000 | Loss: 0.00002112
Iteration 132/1000 | Loss: 0.00002112
Iteration 133/1000 | Loss: 0.00002112
Iteration 134/1000 | Loss: 0.00002112
Iteration 135/1000 | Loss: 0.00002112
Iteration 136/1000 | Loss: 0.00002112
Iteration 137/1000 | Loss: 0.00002112
Iteration 138/1000 | Loss: 0.00002112
Iteration 139/1000 | Loss: 0.00002112
Iteration 140/1000 | Loss: 0.00002112
Iteration 141/1000 | Loss: 0.00002112
Iteration 142/1000 | Loss: 0.00002112
Iteration 143/1000 | Loss: 0.00002112
Iteration 144/1000 | Loss: 0.00002112
Iteration 145/1000 | Loss: 0.00002112
Iteration 146/1000 | Loss: 0.00002112
Iteration 147/1000 | Loss: 0.00002112
Iteration 148/1000 | Loss: 0.00002112
Iteration 149/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.1123858459759504e-05, 2.1123858459759504e-05, 2.1123858459759504e-05, 2.1123858459759504e-05, 2.1123858459759504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1123858459759504e-05

Optimization complete. Final v2v error: 3.887577772140503 mm

Highest mean error: 4.115148067474365 mm for frame 215

Lowest mean error: 3.6650795936584473 mm for frame 92

Saving results

Total time: 60.24992489814758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788452
Iteration 2/25 | Loss: 0.00190607
Iteration 3/25 | Loss: 0.00151304
Iteration 4/25 | Loss: 0.00143575
Iteration 5/25 | Loss: 0.00133977
Iteration 6/25 | Loss: 0.00131336
Iteration 7/25 | Loss: 0.00129793
Iteration 8/25 | Loss: 0.00127884
Iteration 9/25 | Loss: 0.00127725
Iteration 10/25 | Loss: 0.00127670
Iteration 11/25 | Loss: 0.00127879
Iteration 12/25 | Loss: 0.00128140
Iteration 13/25 | Loss: 0.00127301
Iteration 14/25 | Loss: 0.00127165
Iteration 15/25 | Loss: 0.00127127
Iteration 16/25 | Loss: 0.00127125
Iteration 17/25 | Loss: 0.00127125
Iteration 18/25 | Loss: 0.00127124
Iteration 19/25 | Loss: 0.00127124
Iteration 20/25 | Loss: 0.00127124
Iteration 21/25 | Loss: 0.00127124
Iteration 22/25 | Loss: 0.00127124
Iteration 23/25 | Loss: 0.00127124
Iteration 24/25 | Loss: 0.00127124
Iteration 25/25 | Loss: 0.00127124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02460814
Iteration 2/25 | Loss: 0.00084340
Iteration 3/25 | Loss: 0.00084340
Iteration 4/25 | Loss: 0.00084339
Iteration 5/25 | Loss: 0.00084339
Iteration 6/25 | Loss: 0.00084339
Iteration 7/25 | Loss: 0.00084339
Iteration 8/25 | Loss: 0.00084339
Iteration 9/25 | Loss: 0.00084339
Iteration 10/25 | Loss: 0.00084339
Iteration 11/25 | Loss: 0.00084339
Iteration 12/25 | Loss: 0.00084339
Iteration 13/25 | Loss: 0.00084339
Iteration 14/25 | Loss: 0.00084339
Iteration 15/25 | Loss: 0.00084339
Iteration 16/25 | Loss: 0.00084339
Iteration 17/25 | Loss: 0.00084339
Iteration 18/25 | Loss: 0.00084339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008433924522250891, 0.0008433924522250891, 0.0008433924522250891, 0.0008433924522250891, 0.0008433924522250891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008433924522250891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084339
Iteration 2/1000 | Loss: 0.00009532
Iteration 3/1000 | Loss: 0.00004920
Iteration 4/1000 | Loss: 0.00003875
Iteration 5/1000 | Loss: 0.00003462
Iteration 6/1000 | Loss: 0.00003219
Iteration 7/1000 | Loss: 0.00003041
Iteration 8/1000 | Loss: 0.00002913
Iteration 9/1000 | Loss: 0.00002819
Iteration 10/1000 | Loss: 0.00002750
Iteration 11/1000 | Loss: 0.00002703
Iteration 12/1000 | Loss: 0.00016931
Iteration 13/1000 | Loss: 0.00016470
Iteration 14/1000 | Loss: 0.00003882
Iteration 15/1000 | Loss: 0.00003038
Iteration 16/1000 | Loss: 0.00002725
Iteration 17/1000 | Loss: 0.00002548
Iteration 18/1000 | Loss: 0.00002392
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002153
Iteration 22/1000 | Loss: 0.00002121
Iteration 23/1000 | Loss: 0.00002096
Iteration 24/1000 | Loss: 0.00002094
Iteration 25/1000 | Loss: 0.00002089
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00002083
Iteration 29/1000 | Loss: 0.00002081
Iteration 30/1000 | Loss: 0.00002077
Iteration 31/1000 | Loss: 0.00002075
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002074
Iteration 34/1000 | Loss: 0.00002074
Iteration 35/1000 | Loss: 0.00002073
Iteration 36/1000 | Loss: 0.00002073
Iteration 37/1000 | Loss: 0.00002073
Iteration 38/1000 | Loss: 0.00002072
Iteration 39/1000 | Loss: 0.00002071
Iteration 40/1000 | Loss: 0.00002070
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002068
Iteration 48/1000 | Loss: 0.00002067
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002064
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002062
Iteration 54/1000 | Loss: 0.00002061
Iteration 55/1000 | Loss: 0.00002061
Iteration 56/1000 | Loss: 0.00002060
Iteration 57/1000 | Loss: 0.00002060
Iteration 58/1000 | Loss: 0.00002060
Iteration 59/1000 | Loss: 0.00002059
Iteration 60/1000 | Loss: 0.00002059
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002057
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002057
Iteration 65/1000 | Loss: 0.00002056
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002053
Iteration 72/1000 | Loss: 0.00002053
Iteration 73/1000 | Loss: 0.00002053
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002052
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002050
Iteration 82/1000 | Loss: 0.00002050
Iteration 83/1000 | Loss: 0.00002050
Iteration 84/1000 | Loss: 0.00002050
Iteration 85/1000 | Loss: 0.00002050
Iteration 86/1000 | Loss: 0.00002049
Iteration 87/1000 | Loss: 0.00002049
Iteration 88/1000 | Loss: 0.00002049
Iteration 89/1000 | Loss: 0.00002049
Iteration 90/1000 | Loss: 0.00002049
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002048
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002044
Iteration 103/1000 | Loss: 0.00002044
Iteration 104/1000 | Loss: 0.00002043
Iteration 105/1000 | Loss: 0.00002043
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002040
Iteration 110/1000 | Loss: 0.00002040
Iteration 111/1000 | Loss: 0.00002039
Iteration 112/1000 | Loss: 0.00002039
Iteration 113/1000 | Loss: 0.00002039
Iteration 114/1000 | Loss: 0.00002038
Iteration 115/1000 | Loss: 0.00002038
Iteration 116/1000 | Loss: 0.00002038
Iteration 117/1000 | Loss: 0.00002037
Iteration 118/1000 | Loss: 0.00002037
Iteration 119/1000 | Loss: 0.00002037
Iteration 120/1000 | Loss: 0.00002036
Iteration 121/1000 | Loss: 0.00002036
Iteration 122/1000 | Loss: 0.00002036
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002034
Iteration 129/1000 | Loss: 0.00002034
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002033
Iteration 133/1000 | Loss: 0.00002033
Iteration 134/1000 | Loss: 0.00002033
Iteration 135/1000 | Loss: 0.00002032
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00002032
Iteration 140/1000 | Loss: 0.00002032
Iteration 141/1000 | Loss: 0.00002032
Iteration 142/1000 | Loss: 0.00002032
Iteration 143/1000 | Loss: 0.00002032
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002031
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002030
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002030
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002029
Iteration 157/1000 | Loss: 0.00002029
Iteration 158/1000 | Loss: 0.00002029
Iteration 159/1000 | Loss: 0.00002029
Iteration 160/1000 | Loss: 0.00002028
Iteration 161/1000 | Loss: 0.00002028
Iteration 162/1000 | Loss: 0.00002028
Iteration 163/1000 | Loss: 0.00002028
Iteration 164/1000 | Loss: 0.00002028
Iteration 165/1000 | Loss: 0.00002028
Iteration 166/1000 | Loss: 0.00002027
Iteration 167/1000 | Loss: 0.00002027
Iteration 168/1000 | Loss: 0.00002027
Iteration 169/1000 | Loss: 0.00002027
Iteration 170/1000 | Loss: 0.00002027
Iteration 171/1000 | Loss: 0.00002027
Iteration 172/1000 | Loss: 0.00002027
Iteration 173/1000 | Loss: 0.00002027
Iteration 174/1000 | Loss: 0.00002027
Iteration 175/1000 | Loss: 0.00002027
Iteration 176/1000 | Loss: 0.00002027
Iteration 177/1000 | Loss: 0.00002027
Iteration 178/1000 | Loss: 0.00002027
Iteration 179/1000 | Loss: 0.00002027
Iteration 180/1000 | Loss: 0.00002027
Iteration 181/1000 | Loss: 0.00002027
Iteration 182/1000 | Loss: 0.00002027
Iteration 183/1000 | Loss: 0.00002027
Iteration 184/1000 | Loss: 0.00002027
Iteration 185/1000 | Loss: 0.00002027
Iteration 186/1000 | Loss: 0.00002027
Iteration 187/1000 | Loss: 0.00002027
Iteration 188/1000 | Loss: 0.00002027
Iteration 189/1000 | Loss: 0.00002027
Iteration 190/1000 | Loss: 0.00002027
Iteration 191/1000 | Loss: 0.00002027
Iteration 192/1000 | Loss: 0.00002027
Iteration 193/1000 | Loss: 0.00002027
Iteration 194/1000 | Loss: 0.00002027
Iteration 195/1000 | Loss: 0.00002027
Iteration 196/1000 | Loss: 0.00002027
Iteration 197/1000 | Loss: 0.00002027
Iteration 198/1000 | Loss: 0.00002027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.0266330466256477e-05, 2.0266330466256477e-05, 2.0266330466256477e-05, 2.0266330466256477e-05, 2.0266330466256477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0266330466256477e-05

Optimization complete. Final v2v error: 3.826983690261841 mm

Highest mean error: 4.6824116706848145 mm for frame 239

Lowest mean error: 3.3351571559906006 mm for frame 46

Saving results

Total time: 90.1878068447113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899736
Iteration 2/25 | Loss: 0.00130319
Iteration 3/25 | Loss: 0.00118699
Iteration 4/25 | Loss: 0.00116902
Iteration 5/25 | Loss: 0.00116452
Iteration 6/25 | Loss: 0.00116376
Iteration 7/25 | Loss: 0.00116376
Iteration 8/25 | Loss: 0.00116376
Iteration 9/25 | Loss: 0.00116376
Iteration 10/25 | Loss: 0.00116376
Iteration 11/25 | Loss: 0.00116376
Iteration 12/25 | Loss: 0.00116376
Iteration 13/25 | Loss: 0.00116376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001163759152404964, 0.001163759152404964, 0.001163759152404964, 0.001163759152404964, 0.001163759152404964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163759152404964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33160925
Iteration 2/25 | Loss: 0.00059195
Iteration 3/25 | Loss: 0.00059193
Iteration 4/25 | Loss: 0.00059192
Iteration 5/25 | Loss: 0.00059192
Iteration 6/25 | Loss: 0.00059192
Iteration 7/25 | Loss: 0.00059192
Iteration 8/25 | Loss: 0.00059192
Iteration 9/25 | Loss: 0.00059192
Iteration 10/25 | Loss: 0.00059192
Iteration 11/25 | Loss: 0.00059192
Iteration 12/25 | Loss: 0.00059192
Iteration 13/25 | Loss: 0.00059192
Iteration 14/25 | Loss: 0.00059192
Iteration 15/25 | Loss: 0.00059192
Iteration 16/25 | Loss: 0.00059192
Iteration 17/25 | Loss: 0.00059192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005919227842241526, 0.0005919227842241526, 0.0005919227842241526, 0.0005919227842241526, 0.0005919227842241526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005919227842241526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059192
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00002846
Iteration 4/1000 | Loss: 0.00002222
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001870
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001675
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001550
Iteration 19/1000 | Loss: 0.00001550
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001550
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001546
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001546
Iteration 34/1000 | Loss: 0.00001546
Iteration 35/1000 | Loss: 0.00001546
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001545
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001544
Iteration 42/1000 | Loss: 0.00001544
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001543
Iteration 45/1000 | Loss: 0.00001543
Iteration 46/1000 | Loss: 0.00001543
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001543
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001543
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001543
Iteration 66/1000 | Loss: 0.00001543
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001543
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001543
Iteration 71/1000 | Loss: 0.00001543
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001543
Iteration 74/1000 | Loss: 0.00001543
Iteration 75/1000 | Loss: 0.00001543
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.5429524864885025e-05, 1.5429524864885025e-05, 1.5429524864885025e-05, 1.5429524864885025e-05, 1.5429524864885025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5429524864885025e-05

Optimization complete. Final v2v error: 3.326831340789795 mm

Highest mean error: 3.5232014656066895 mm for frame 76

Lowest mean error: 3.153103828430176 mm for frame 141

Saving results

Total time: 39.06685185432434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950187
Iteration 2/25 | Loss: 0.00220528
Iteration 3/25 | Loss: 0.00187094
Iteration 4/25 | Loss: 0.00161127
Iteration 5/25 | Loss: 0.00159310
Iteration 6/25 | Loss: 0.00143182
Iteration 7/25 | Loss: 0.00136630
Iteration 8/25 | Loss: 0.00132661
Iteration 9/25 | Loss: 0.00127518
Iteration 10/25 | Loss: 0.00125016
Iteration 11/25 | Loss: 0.00122982
Iteration 12/25 | Loss: 0.00122262
Iteration 13/25 | Loss: 0.00122482
Iteration 14/25 | Loss: 0.00123665
Iteration 15/25 | Loss: 0.00120607
Iteration 16/25 | Loss: 0.00120114
Iteration 17/25 | Loss: 0.00120028
Iteration 18/25 | Loss: 0.00119987
Iteration 19/25 | Loss: 0.00119966
Iteration 20/25 | Loss: 0.00119959
Iteration 21/25 | Loss: 0.00119957
Iteration 22/25 | Loss: 0.00119957
Iteration 23/25 | Loss: 0.00119957
Iteration 24/25 | Loss: 0.00119957
Iteration 25/25 | Loss: 0.00119957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41811681
Iteration 2/25 | Loss: 0.00120872
Iteration 3/25 | Loss: 0.00120872
Iteration 4/25 | Loss: 0.00120872
Iteration 5/25 | Loss: 0.00120872
Iteration 6/25 | Loss: 0.00120872
Iteration 7/25 | Loss: 0.00120872
Iteration 8/25 | Loss: 0.00120872
Iteration 9/25 | Loss: 0.00120872
Iteration 10/25 | Loss: 0.00120872
Iteration 11/25 | Loss: 0.00120872
Iteration 12/25 | Loss: 0.00120872
Iteration 13/25 | Loss: 0.00120872
Iteration 14/25 | Loss: 0.00120872
Iteration 15/25 | Loss: 0.00120872
Iteration 16/25 | Loss: 0.00120872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012087159557268023, 0.0012087159557268023, 0.0012087159557268023, 0.0012087159557268023, 0.0012087159557268023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012087159557268023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120872
Iteration 2/1000 | Loss: 0.00027242
Iteration 3/1000 | Loss: 0.00008982
Iteration 4/1000 | Loss: 0.00006737
Iteration 5/1000 | Loss: 0.00005527
Iteration 6/1000 | Loss: 0.00005119
Iteration 7/1000 | Loss: 0.00024188
Iteration 8/1000 | Loss: 0.00005853
Iteration 9/1000 | Loss: 0.00004577
Iteration 10/1000 | Loss: 0.00004335
Iteration 11/1000 | Loss: 0.00004107
Iteration 12/1000 | Loss: 0.00003957
Iteration 13/1000 | Loss: 0.00003888
Iteration 14/1000 | Loss: 0.00003827
Iteration 15/1000 | Loss: 0.00003759
Iteration 16/1000 | Loss: 0.00017848
Iteration 17/1000 | Loss: 0.00026123
Iteration 18/1000 | Loss: 0.00004836
Iteration 19/1000 | Loss: 0.00003699
Iteration 20/1000 | Loss: 0.00003334
Iteration 21/1000 | Loss: 0.00003112
Iteration 22/1000 | Loss: 0.00002954
Iteration 23/1000 | Loss: 0.00002870
Iteration 24/1000 | Loss: 0.00002824
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002768
Iteration 27/1000 | Loss: 0.00002750
Iteration 28/1000 | Loss: 0.00002741
Iteration 29/1000 | Loss: 0.00013532
Iteration 30/1000 | Loss: 0.00045551
Iteration 31/1000 | Loss: 0.00005226
Iteration 32/1000 | Loss: 0.00003588
Iteration 33/1000 | Loss: 0.00002896
Iteration 34/1000 | Loss: 0.00002400
Iteration 35/1000 | Loss: 0.00002094
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001776
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001676
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001675
Iteration 61/1000 | Loss: 0.00001675
Iteration 62/1000 | Loss: 0.00001675
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001674
Iteration 67/1000 | Loss: 0.00001674
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001674
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.67304042406613e-05, 1.67304042406613e-05, 1.67304042406613e-05, 1.67304042406613e-05, 1.67304042406613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.67304042406613e-05

Optimization complete. Final v2v error: 3.4479541778564453 mm

Highest mean error: 4.097395420074463 mm for frame 80

Lowest mean error: 3.0899312496185303 mm for frame 217

Saving results

Total time: 130.689715385437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069067
Iteration 2/25 | Loss: 0.01069067
Iteration 3/25 | Loss: 0.01069066
Iteration 4/25 | Loss: 0.01069066
Iteration 5/25 | Loss: 0.01069066
Iteration 6/25 | Loss: 0.01069066
Iteration 7/25 | Loss: 0.01069066
Iteration 8/25 | Loss: 0.01069066
Iteration 9/25 | Loss: 0.01069066
Iteration 10/25 | Loss: 0.01069065
Iteration 11/25 | Loss: 0.01069065
Iteration 12/25 | Loss: 0.01069065
Iteration 13/25 | Loss: 0.01069065
Iteration 14/25 | Loss: 0.01069065
Iteration 15/25 | Loss: 0.01069065
Iteration 16/25 | Loss: 0.01069065
Iteration 17/25 | Loss: 0.01069065
Iteration 18/25 | Loss: 0.01069064
Iteration 19/25 | Loss: 0.01069064
Iteration 20/25 | Loss: 0.01069064
Iteration 21/25 | Loss: 0.01069064
Iteration 22/25 | Loss: 0.01069064
Iteration 23/25 | Loss: 0.01069064
Iteration 24/25 | Loss: 0.01069064
Iteration 25/25 | Loss: 0.01069064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42516088
Iteration 2/25 | Loss: 0.17740560
Iteration 3/25 | Loss: 0.17449163
Iteration 4/25 | Loss: 0.17693760
Iteration 5/25 | Loss: 0.17459372
Iteration 6/25 | Loss: 0.17360550
Iteration 7/25 | Loss: 0.17284410
Iteration 8/25 | Loss: 0.17284408
Iteration 9/25 | Loss: 0.17284408
Iteration 10/25 | Loss: 0.17284405
Iteration 11/25 | Loss: 0.17284405
Iteration 12/25 | Loss: 0.17284405
Iteration 13/25 | Loss: 0.17284405
Iteration 14/25 | Loss: 0.17284404
Iteration 15/25 | Loss: 0.17284404
Iteration 16/25 | Loss: 0.17284405
Iteration 17/25 | Loss: 0.17284405
Iteration 18/25 | Loss: 0.17284405
Iteration 19/25 | Loss: 0.17284405
Iteration 20/25 | Loss: 0.17284405
Iteration 21/25 | Loss: 0.17284405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.1728440523147583, 0.1728440523147583, 0.1728440523147583, 0.1728440523147583, 0.1728440523147583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1728440523147583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17284404
Iteration 2/1000 | Loss: 0.00657747
Iteration 3/1000 | Loss: 0.00258388
Iteration 4/1000 | Loss: 0.00128081
Iteration 5/1000 | Loss: 0.00155269
Iteration 6/1000 | Loss: 0.00046011
Iteration 7/1000 | Loss: 0.00188336
Iteration 8/1000 | Loss: 0.00039488
Iteration 9/1000 | Loss: 0.00157696
Iteration 10/1000 | Loss: 0.00018358
Iteration 11/1000 | Loss: 0.00028724
Iteration 12/1000 | Loss: 0.00006133
Iteration 13/1000 | Loss: 0.00009793
Iteration 14/1000 | Loss: 0.00010083
Iteration 15/1000 | Loss: 0.00004367
Iteration 16/1000 | Loss: 0.00032457
Iteration 17/1000 | Loss: 0.00004004
Iteration 18/1000 | Loss: 0.00006243
Iteration 19/1000 | Loss: 0.00003464
Iteration 20/1000 | Loss: 0.00003518
Iteration 21/1000 | Loss: 0.00003517
Iteration 22/1000 | Loss: 0.00003292
Iteration 23/1000 | Loss: 0.00009289
Iteration 24/1000 | Loss: 0.00022071
Iteration 25/1000 | Loss: 0.00005252
Iteration 26/1000 | Loss: 0.00005476
Iteration 27/1000 | Loss: 0.00010300
Iteration 28/1000 | Loss: 0.00002707
Iteration 29/1000 | Loss: 0.00002640
Iteration 30/1000 | Loss: 0.00002582
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00008620
Iteration 33/1000 | Loss: 0.00002480
Iteration 34/1000 | Loss: 0.00002432
Iteration 35/1000 | Loss: 0.00002393
Iteration 36/1000 | Loss: 0.00002371
Iteration 37/1000 | Loss: 0.00013063
Iteration 38/1000 | Loss: 0.00005874
Iteration 39/1000 | Loss: 0.00003740
Iteration 40/1000 | Loss: 0.00013342
Iteration 41/1000 | Loss: 0.00006285
Iteration 42/1000 | Loss: 0.00005718
Iteration 43/1000 | Loss: 0.00013121
Iteration 44/1000 | Loss: 0.00007857
Iteration 45/1000 | Loss: 0.00002529
Iteration 46/1000 | Loss: 0.00012900
Iteration 47/1000 | Loss: 0.00004447
Iteration 48/1000 | Loss: 0.00002651
Iteration 49/1000 | Loss: 0.00009189
Iteration 50/1000 | Loss: 0.00003076
Iteration 51/1000 | Loss: 0.00015690
Iteration 52/1000 | Loss: 0.00005339
Iteration 53/1000 | Loss: 0.00008396
Iteration 54/1000 | Loss: 0.00003422
Iteration 55/1000 | Loss: 0.00005183
Iteration 56/1000 | Loss: 0.00006240
Iteration 57/1000 | Loss: 0.00002345
Iteration 58/1000 | Loss: 0.00015464
Iteration 59/1000 | Loss: 0.00003610
Iteration 60/1000 | Loss: 0.00005359
Iteration 61/1000 | Loss: 0.00002423
Iteration 62/1000 | Loss: 0.00014711
Iteration 63/1000 | Loss: 0.00004192
Iteration 64/1000 | Loss: 0.00002626
Iteration 65/1000 | Loss: 0.00011663
Iteration 66/1000 | Loss: 0.00002475
Iteration 67/1000 | Loss: 0.00002801
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002329
Iteration 70/1000 | Loss: 0.00002308
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00002303
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002301
Iteration 76/1000 | Loss: 0.00002299
Iteration 77/1000 | Loss: 0.00002299
Iteration 78/1000 | Loss: 0.00002299
Iteration 79/1000 | Loss: 0.00002298
Iteration 80/1000 | Loss: 0.00002298
Iteration 81/1000 | Loss: 0.00002298
Iteration 82/1000 | Loss: 0.00002298
Iteration 83/1000 | Loss: 0.00002298
Iteration 84/1000 | Loss: 0.00002298
Iteration 85/1000 | Loss: 0.00002298
Iteration 86/1000 | Loss: 0.00002297
Iteration 87/1000 | Loss: 0.00002297
Iteration 88/1000 | Loss: 0.00002296
Iteration 89/1000 | Loss: 0.00002296
Iteration 90/1000 | Loss: 0.00002296
Iteration 91/1000 | Loss: 0.00002296
Iteration 92/1000 | Loss: 0.00002296
Iteration 93/1000 | Loss: 0.00002296
Iteration 94/1000 | Loss: 0.00002295
Iteration 95/1000 | Loss: 0.00002295
Iteration 96/1000 | Loss: 0.00002295
Iteration 97/1000 | Loss: 0.00002295
Iteration 98/1000 | Loss: 0.00002295
Iteration 99/1000 | Loss: 0.00002294
Iteration 100/1000 | Loss: 0.00002294
Iteration 101/1000 | Loss: 0.00002294
Iteration 102/1000 | Loss: 0.00002294
Iteration 103/1000 | Loss: 0.00002294
Iteration 104/1000 | Loss: 0.00002294
Iteration 105/1000 | Loss: 0.00002294
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002293
Iteration 109/1000 | Loss: 0.00002293
Iteration 110/1000 | Loss: 0.00002293
Iteration 111/1000 | Loss: 0.00002292
Iteration 112/1000 | Loss: 0.00002292
Iteration 113/1000 | Loss: 0.00002292
Iteration 114/1000 | Loss: 0.00002292
Iteration 115/1000 | Loss: 0.00002292
Iteration 116/1000 | Loss: 0.00002292
Iteration 117/1000 | Loss: 0.00002291
Iteration 118/1000 | Loss: 0.00010376
Iteration 119/1000 | Loss: 0.00002327
Iteration 120/1000 | Loss: 0.00002289
Iteration 121/1000 | Loss: 0.00002288
Iteration 122/1000 | Loss: 0.00002287
Iteration 123/1000 | Loss: 0.00002287
Iteration 124/1000 | Loss: 0.00002287
Iteration 125/1000 | Loss: 0.00002287
Iteration 126/1000 | Loss: 0.00002287
Iteration 127/1000 | Loss: 0.00002287
Iteration 128/1000 | Loss: 0.00002287
Iteration 129/1000 | Loss: 0.00002287
Iteration 130/1000 | Loss: 0.00002287
Iteration 131/1000 | Loss: 0.00002287
Iteration 132/1000 | Loss: 0.00002286
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00002285
Iteration 135/1000 | Loss: 0.00002285
Iteration 136/1000 | Loss: 0.00002285
Iteration 137/1000 | Loss: 0.00002285
Iteration 138/1000 | Loss: 0.00002284
Iteration 139/1000 | Loss: 0.00002284
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002284
Iteration 142/1000 | Loss: 0.00002284
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002284
Iteration 145/1000 | Loss: 0.00002284
Iteration 146/1000 | Loss: 0.00002284
Iteration 147/1000 | Loss: 0.00002284
Iteration 148/1000 | Loss: 0.00002284
Iteration 149/1000 | Loss: 0.00002284
Iteration 150/1000 | Loss: 0.00002284
Iteration 151/1000 | Loss: 0.00002284
Iteration 152/1000 | Loss: 0.00002284
Iteration 153/1000 | Loss: 0.00002284
Iteration 154/1000 | Loss: 0.00002284
Iteration 155/1000 | Loss: 0.00002284
Iteration 156/1000 | Loss: 0.00002284
Iteration 157/1000 | Loss: 0.00002284
Iteration 158/1000 | Loss: 0.00002284
Iteration 159/1000 | Loss: 0.00002284
Iteration 160/1000 | Loss: 0.00002284
Iteration 161/1000 | Loss: 0.00002284
Iteration 162/1000 | Loss: 0.00002284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.2836737116449513e-05, 2.2836737116449513e-05, 2.2836737116449513e-05, 2.2836737116449513e-05, 2.2836737116449513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2836737116449513e-05

Optimization complete. Final v2v error: 3.576641082763672 mm

Highest mean error: 22.200746536254883 mm for frame 106

Lowest mean error: 3.228394031524658 mm for frame 142

Saving results

Total time: 150.96126651763916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136204
Iteration 2/25 | Loss: 0.00211063
Iteration 3/25 | Loss: 0.00137705
Iteration 4/25 | Loss: 0.00132242
Iteration 5/25 | Loss: 0.00122775
Iteration 6/25 | Loss: 0.00123146
Iteration 7/25 | Loss: 0.00122787
Iteration 8/25 | Loss: 0.00120108
Iteration 9/25 | Loss: 0.00119173
Iteration 10/25 | Loss: 0.00118374
Iteration 11/25 | Loss: 0.00118013
Iteration 12/25 | Loss: 0.00117888
Iteration 13/25 | Loss: 0.00117871
Iteration 14/25 | Loss: 0.00117834
Iteration 15/25 | Loss: 0.00117849
Iteration 16/25 | Loss: 0.00117838
Iteration 17/25 | Loss: 0.00117861
Iteration 18/25 | Loss: 0.00117847
Iteration 19/25 | Loss: 0.00117866
Iteration 20/25 | Loss: 0.00117856
Iteration 21/25 | Loss: 0.00117865
Iteration 22/25 | Loss: 0.00117858
Iteration 23/25 | Loss: 0.00117864
Iteration 24/25 | Loss: 0.00117854
Iteration 25/25 | Loss: 0.00117862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57701910
Iteration 2/25 | Loss: 0.00068759
Iteration 3/25 | Loss: 0.00063465
Iteration 4/25 | Loss: 0.00063465
Iteration 5/25 | Loss: 0.00063465
Iteration 6/25 | Loss: 0.00063464
Iteration 7/25 | Loss: 0.00063464
Iteration 8/25 | Loss: 0.00063464
Iteration 9/25 | Loss: 0.00063464
Iteration 10/25 | Loss: 0.00063464
Iteration 11/25 | Loss: 0.00063464
Iteration 12/25 | Loss: 0.00063464
Iteration 13/25 | Loss: 0.00063464
Iteration 14/25 | Loss: 0.00063464
Iteration 15/25 | Loss: 0.00063464
Iteration 16/25 | Loss: 0.00063464
Iteration 17/25 | Loss: 0.00063464
Iteration 18/25 | Loss: 0.00063464
Iteration 19/25 | Loss: 0.00063464
Iteration 20/25 | Loss: 0.00063464
Iteration 21/25 | Loss: 0.00063464
Iteration 22/25 | Loss: 0.00063464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006346433656290174, 0.0006346433656290174, 0.0006346433656290174, 0.0006346433656290174, 0.0006346433656290174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006346433656290174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063464
Iteration 2/1000 | Loss: 0.00012046
Iteration 3/1000 | Loss: 0.00003349
Iteration 4/1000 | Loss: 0.00003052
Iteration 5/1000 | Loss: 0.00002890
Iteration 6/1000 | Loss: 0.00002796
Iteration 7/1000 | Loss: 0.00006193
Iteration 8/1000 | Loss: 0.00002709
Iteration 9/1000 | Loss: 0.00002646
Iteration 10/1000 | Loss: 0.00035000
Iteration 11/1000 | Loss: 0.00026381
Iteration 12/1000 | Loss: 0.00013625
Iteration 13/1000 | Loss: 0.00003978
Iteration 14/1000 | Loss: 0.00003018
Iteration 15/1000 | Loss: 0.00002627
Iteration 16/1000 | Loss: 0.00002514
Iteration 17/1000 | Loss: 0.00026270
Iteration 18/1000 | Loss: 0.00003461
Iteration 19/1000 | Loss: 0.00002523
Iteration 20/1000 | Loss: 0.00003661
Iteration 21/1000 | Loss: 0.00013554
Iteration 22/1000 | Loss: 0.00010844
Iteration 23/1000 | Loss: 0.00004124
Iteration 24/1000 | Loss: 0.00004164
Iteration 25/1000 | Loss: 0.00002483
Iteration 26/1000 | Loss: 0.00002392
Iteration 27/1000 | Loss: 0.00003337
Iteration 28/1000 | Loss: 0.00002427
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00002367
Iteration 31/1000 | Loss: 0.00002426
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002410
Iteration 34/1000 | Loss: 0.00002363
Iteration 35/1000 | Loss: 0.00002357
Iteration 36/1000 | Loss: 0.00002355
Iteration 37/1000 | Loss: 0.00002355
Iteration 38/1000 | Loss: 0.00002355
Iteration 39/1000 | Loss: 0.00002355
Iteration 40/1000 | Loss: 0.00002355
Iteration 41/1000 | Loss: 0.00002355
Iteration 42/1000 | Loss: 0.00002355
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002354
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002353
Iteration 50/1000 | Loss: 0.00002353
Iteration 51/1000 | Loss: 0.00002353
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002353
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002353
Iteration 59/1000 | Loss: 0.00002353
Iteration 60/1000 | Loss: 0.00002353
Iteration 61/1000 | Loss: 0.00002353
Iteration 62/1000 | Loss: 0.00002353
Iteration 63/1000 | Loss: 0.00002353
Iteration 64/1000 | Loss: 0.00002353
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002352
Iteration 73/1000 | Loss: 0.00002352
Iteration 74/1000 | Loss: 0.00002352
Iteration 75/1000 | Loss: 0.00002352
Iteration 76/1000 | Loss: 0.00002352
Iteration 77/1000 | Loss: 0.00002352
Iteration 78/1000 | Loss: 0.00002352
Iteration 79/1000 | Loss: 0.00002352
Iteration 80/1000 | Loss: 0.00002352
Iteration 81/1000 | Loss: 0.00002352
Iteration 82/1000 | Loss: 0.00002352
Iteration 83/1000 | Loss: 0.00002351
Iteration 84/1000 | Loss: 0.00002351
Iteration 85/1000 | Loss: 0.00002351
Iteration 86/1000 | Loss: 0.00002351
Iteration 87/1000 | Loss: 0.00002351
Iteration 88/1000 | Loss: 0.00002351
Iteration 89/1000 | Loss: 0.00002351
Iteration 90/1000 | Loss: 0.00002351
Iteration 91/1000 | Loss: 0.00002351
Iteration 92/1000 | Loss: 0.00002351
Iteration 93/1000 | Loss: 0.00002351
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.3508684535045177e-05, 2.3508684535045177e-05, 2.3508684535045177e-05, 2.3508684535045177e-05, 2.3508684535045177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3508684535045177e-05

Optimization complete. Final v2v error: 4.1020827293396 mm

Highest mean error: 9.31624984741211 mm for frame 92

Lowest mean error: 3.440359115600586 mm for frame 200

Saving results

Total time: 120.9546377658844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842597
Iteration 2/25 | Loss: 0.00174593
Iteration 3/25 | Loss: 0.00136184
Iteration 4/25 | Loss: 0.00130002
Iteration 5/25 | Loss: 0.00127365
Iteration 6/25 | Loss: 0.00127737
Iteration 7/25 | Loss: 0.00127145
Iteration 8/25 | Loss: 0.00127300
Iteration 9/25 | Loss: 0.00126079
Iteration 10/25 | Loss: 0.00125143
Iteration 11/25 | Loss: 0.00124740
Iteration 12/25 | Loss: 0.00124687
Iteration 13/25 | Loss: 0.00124210
Iteration 14/25 | Loss: 0.00124278
Iteration 15/25 | Loss: 0.00124676
Iteration 16/25 | Loss: 0.00123845
Iteration 17/25 | Loss: 0.00123666
Iteration 18/25 | Loss: 0.00123999
Iteration 19/25 | Loss: 0.00123858
Iteration 20/25 | Loss: 0.00123828
Iteration 21/25 | Loss: 0.00123434
Iteration 22/25 | Loss: 0.00123214
Iteration 23/25 | Loss: 0.00123040
Iteration 24/25 | Loss: 0.00123002
Iteration 25/25 | Loss: 0.00122915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29279828
Iteration 2/25 | Loss: 0.00064394
Iteration 3/25 | Loss: 0.00064393
Iteration 4/25 | Loss: 0.00064393
Iteration 5/25 | Loss: 0.00064393
Iteration 6/25 | Loss: 0.00064393
Iteration 7/25 | Loss: 0.00064393
Iteration 8/25 | Loss: 0.00064393
Iteration 9/25 | Loss: 0.00064393
Iteration 10/25 | Loss: 0.00064393
Iteration 11/25 | Loss: 0.00064393
Iteration 12/25 | Loss: 0.00064393
Iteration 13/25 | Loss: 0.00064393
Iteration 14/25 | Loss: 0.00064393
Iteration 15/25 | Loss: 0.00064393
Iteration 16/25 | Loss: 0.00064393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006439306307584047, 0.0006439306307584047, 0.0006439306307584047, 0.0006439306307584047, 0.0006439306307584047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006439306307584047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064393
Iteration 2/1000 | Loss: 0.00006889
Iteration 3/1000 | Loss: 0.00003954
Iteration 4/1000 | Loss: 0.00003554
Iteration 5/1000 | Loss: 0.00004967
Iteration 6/1000 | Loss: 0.00003463
Iteration 7/1000 | Loss: 0.00004583
Iteration 8/1000 | Loss: 0.00003316
Iteration 9/1000 | Loss: 0.00033335
Iteration 10/1000 | Loss: 0.00025688
Iteration 11/1000 | Loss: 0.00004568
Iteration 12/1000 | Loss: 0.00004569
Iteration 13/1000 | Loss: 0.00003556
Iteration 14/1000 | Loss: 0.00004273
Iteration 15/1000 | Loss: 0.00003797
Iteration 16/1000 | Loss: 0.00003843
Iteration 17/1000 | Loss: 0.00003147
Iteration 18/1000 | Loss: 0.00002902
Iteration 19/1000 | Loss: 0.00003740
Iteration 20/1000 | Loss: 0.00003163
Iteration 21/1000 | Loss: 0.00003280
Iteration 22/1000 | Loss: 0.00003489
Iteration 23/1000 | Loss: 0.00002767
Iteration 24/1000 | Loss: 0.00003828
Iteration 25/1000 | Loss: 0.00003395
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00003154
Iteration 28/1000 | Loss: 0.00003260
Iteration 29/1000 | Loss: 0.00003251
Iteration 30/1000 | Loss: 0.00003408
Iteration 31/1000 | Loss: 0.00003594
Iteration 32/1000 | Loss: 0.00003704
Iteration 33/1000 | Loss: 0.00003587
Iteration 34/1000 | Loss: 0.00003439
Iteration 35/1000 | Loss: 0.00003393
Iteration 36/1000 | Loss: 0.00003807
Iteration 37/1000 | Loss: 0.00003354
Iteration 38/1000 | Loss: 0.00002630
Iteration 39/1000 | Loss: 0.00002563
Iteration 40/1000 | Loss: 0.00004358
Iteration 41/1000 | Loss: 0.00004740
Iteration 42/1000 | Loss: 0.00002724
Iteration 43/1000 | Loss: 0.00002554
Iteration 44/1000 | Loss: 0.00002464
Iteration 45/1000 | Loss: 0.00002408
Iteration 46/1000 | Loss: 0.00002393
Iteration 47/1000 | Loss: 0.00002381
Iteration 48/1000 | Loss: 0.00002378
Iteration 49/1000 | Loss: 0.00002378
Iteration 50/1000 | Loss: 0.00002377
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002377
Iteration 53/1000 | Loss: 0.00002377
Iteration 54/1000 | Loss: 0.00002377
Iteration 55/1000 | Loss: 0.00002377
Iteration 56/1000 | Loss: 0.00002377
Iteration 57/1000 | Loss: 0.00002372
Iteration 58/1000 | Loss: 0.00002371
Iteration 59/1000 | Loss: 0.00002370
Iteration 60/1000 | Loss: 0.00002368
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002363
Iteration 63/1000 | Loss: 0.00002363
Iteration 64/1000 | Loss: 0.00002362
Iteration 65/1000 | Loss: 0.00002362
Iteration 66/1000 | Loss: 0.00002361
Iteration 67/1000 | Loss: 0.00002358
Iteration 68/1000 | Loss: 0.00002355
Iteration 69/1000 | Loss: 0.00002354
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002350
Iteration 72/1000 | Loss: 0.00002347
Iteration 73/1000 | Loss: 0.00002346
Iteration 74/1000 | Loss: 0.00002345
Iteration 75/1000 | Loss: 0.00002344
Iteration 76/1000 | Loss: 0.00002344
Iteration 77/1000 | Loss: 0.00002344
Iteration 78/1000 | Loss: 0.00002343
Iteration 79/1000 | Loss: 0.00002343
Iteration 80/1000 | Loss: 0.00002343
Iteration 81/1000 | Loss: 0.00002342
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002341
Iteration 84/1000 | Loss: 0.00002341
Iteration 85/1000 | Loss: 0.00002341
Iteration 86/1000 | Loss: 0.00002341
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002339
Iteration 93/1000 | Loss: 0.00002339
Iteration 94/1000 | Loss: 0.00002339
Iteration 95/1000 | Loss: 0.00002339
Iteration 96/1000 | Loss: 0.00002338
Iteration 97/1000 | Loss: 0.00002338
Iteration 98/1000 | Loss: 0.00002338
Iteration 99/1000 | Loss: 0.00002338
Iteration 100/1000 | Loss: 0.00002337
Iteration 101/1000 | Loss: 0.00002337
Iteration 102/1000 | Loss: 0.00002337
Iteration 103/1000 | Loss: 0.00002337
Iteration 104/1000 | Loss: 0.00002337
Iteration 105/1000 | Loss: 0.00002337
Iteration 106/1000 | Loss: 0.00002337
Iteration 107/1000 | Loss: 0.00002337
Iteration 108/1000 | Loss: 0.00002337
Iteration 109/1000 | Loss: 0.00002337
Iteration 110/1000 | Loss: 0.00002337
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002335
Iteration 116/1000 | Loss: 0.00002334
Iteration 117/1000 | Loss: 0.00002334
Iteration 118/1000 | Loss: 0.00002334
Iteration 119/1000 | Loss: 0.00002334
Iteration 120/1000 | Loss: 0.00002334
Iteration 121/1000 | Loss: 0.00002334
Iteration 122/1000 | Loss: 0.00002334
Iteration 123/1000 | Loss: 0.00002333
Iteration 124/1000 | Loss: 0.00002333
Iteration 125/1000 | Loss: 0.00002333
Iteration 126/1000 | Loss: 0.00002333
Iteration 127/1000 | Loss: 0.00002333
Iteration 128/1000 | Loss: 0.00002333
Iteration 129/1000 | Loss: 0.00002333
Iteration 130/1000 | Loss: 0.00002333
Iteration 131/1000 | Loss: 0.00002333
Iteration 132/1000 | Loss: 0.00002333
Iteration 133/1000 | Loss: 0.00002333
Iteration 134/1000 | Loss: 0.00002333
Iteration 135/1000 | Loss: 0.00002332
Iteration 136/1000 | Loss: 0.00002332
Iteration 137/1000 | Loss: 0.00002332
Iteration 138/1000 | Loss: 0.00002332
Iteration 139/1000 | Loss: 0.00002332
Iteration 140/1000 | Loss: 0.00002332
Iteration 141/1000 | Loss: 0.00002332
Iteration 142/1000 | Loss: 0.00002332
Iteration 143/1000 | Loss: 0.00002332
Iteration 144/1000 | Loss: 0.00002332
Iteration 145/1000 | Loss: 0.00002332
Iteration 146/1000 | Loss: 0.00002331
Iteration 147/1000 | Loss: 0.00002331
Iteration 148/1000 | Loss: 0.00002331
Iteration 149/1000 | Loss: 0.00002331
Iteration 150/1000 | Loss: 0.00002331
Iteration 151/1000 | Loss: 0.00002331
Iteration 152/1000 | Loss: 0.00002331
Iteration 153/1000 | Loss: 0.00002331
Iteration 154/1000 | Loss: 0.00002331
Iteration 155/1000 | Loss: 0.00002331
Iteration 156/1000 | Loss: 0.00002331
Iteration 157/1000 | Loss: 0.00002330
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002330
Iteration 160/1000 | Loss: 0.00002330
Iteration 161/1000 | Loss: 0.00002330
Iteration 162/1000 | Loss: 0.00002329
Iteration 163/1000 | Loss: 0.00002329
Iteration 164/1000 | Loss: 0.00002329
Iteration 165/1000 | Loss: 0.00002329
Iteration 166/1000 | Loss: 0.00002329
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002328
Iteration 169/1000 | Loss: 0.00002328
Iteration 170/1000 | Loss: 0.00002328
Iteration 171/1000 | Loss: 0.00002328
Iteration 172/1000 | Loss: 0.00002328
Iteration 173/1000 | Loss: 0.00002328
Iteration 174/1000 | Loss: 0.00002328
Iteration 175/1000 | Loss: 0.00002328
Iteration 176/1000 | Loss: 0.00002328
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002327
Iteration 179/1000 | Loss: 0.00002327
Iteration 180/1000 | Loss: 0.00002327
Iteration 181/1000 | Loss: 0.00002326
Iteration 182/1000 | Loss: 0.00002326
Iteration 183/1000 | Loss: 0.00002326
Iteration 184/1000 | Loss: 0.00002326
Iteration 185/1000 | Loss: 0.00002326
Iteration 186/1000 | Loss: 0.00002326
Iteration 187/1000 | Loss: 0.00002326
Iteration 188/1000 | Loss: 0.00002326
Iteration 189/1000 | Loss: 0.00002325
Iteration 190/1000 | Loss: 0.00002325
Iteration 191/1000 | Loss: 0.00002325
Iteration 192/1000 | Loss: 0.00002325
Iteration 193/1000 | Loss: 0.00002325
Iteration 194/1000 | Loss: 0.00002325
Iteration 195/1000 | Loss: 0.00002325
Iteration 196/1000 | Loss: 0.00002325
Iteration 197/1000 | Loss: 0.00002325
Iteration 198/1000 | Loss: 0.00002325
Iteration 199/1000 | Loss: 0.00002325
Iteration 200/1000 | Loss: 0.00002325
Iteration 201/1000 | Loss: 0.00002325
Iteration 202/1000 | Loss: 0.00002325
Iteration 203/1000 | Loss: 0.00002325
Iteration 204/1000 | Loss: 0.00002325
Iteration 205/1000 | Loss: 0.00002325
Iteration 206/1000 | Loss: 0.00002325
Iteration 207/1000 | Loss: 0.00002324
Iteration 208/1000 | Loss: 0.00002324
Iteration 209/1000 | Loss: 0.00002324
Iteration 210/1000 | Loss: 0.00002324
Iteration 211/1000 | Loss: 0.00002324
Iteration 212/1000 | Loss: 0.00002324
Iteration 213/1000 | Loss: 0.00002324
Iteration 214/1000 | Loss: 0.00002324
Iteration 215/1000 | Loss: 0.00002324
Iteration 216/1000 | Loss: 0.00002324
Iteration 217/1000 | Loss: 0.00002324
Iteration 218/1000 | Loss: 0.00002324
Iteration 219/1000 | Loss: 0.00002324
Iteration 220/1000 | Loss: 0.00002323
Iteration 221/1000 | Loss: 0.00002323
Iteration 222/1000 | Loss: 0.00002323
Iteration 223/1000 | Loss: 0.00002323
Iteration 224/1000 | Loss: 0.00002323
Iteration 225/1000 | Loss: 0.00002323
Iteration 226/1000 | Loss: 0.00002323
Iteration 227/1000 | Loss: 0.00002323
Iteration 228/1000 | Loss: 0.00002323
Iteration 229/1000 | Loss: 0.00002323
Iteration 230/1000 | Loss: 0.00002323
Iteration 231/1000 | Loss: 0.00002323
Iteration 232/1000 | Loss: 0.00002323
Iteration 233/1000 | Loss: 0.00002323
Iteration 234/1000 | Loss: 0.00002323
Iteration 235/1000 | Loss: 0.00002323
Iteration 236/1000 | Loss: 0.00002323
Iteration 237/1000 | Loss: 0.00002323
Iteration 238/1000 | Loss: 0.00002323
Iteration 239/1000 | Loss: 0.00002323
Iteration 240/1000 | Loss: 0.00002323
Iteration 241/1000 | Loss: 0.00002323
Iteration 242/1000 | Loss: 0.00002323
Iteration 243/1000 | Loss: 0.00002323
Iteration 244/1000 | Loss: 0.00002322
Iteration 245/1000 | Loss: 0.00002322
Iteration 246/1000 | Loss: 0.00002322
Iteration 247/1000 | Loss: 0.00002322
Iteration 248/1000 | Loss: 0.00002322
Iteration 249/1000 | Loss: 0.00002322
Iteration 250/1000 | Loss: 0.00002322
Iteration 251/1000 | Loss: 0.00002322
Iteration 252/1000 | Loss: 0.00002322
Iteration 253/1000 | Loss: 0.00002322
Iteration 254/1000 | Loss: 0.00002322
Iteration 255/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.3224405595101416e-05, 2.3224405595101416e-05, 2.3224405595101416e-05, 2.3224405595101416e-05, 2.3224405595101416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3224405595101416e-05

Optimization complete. Final v2v error: 3.966040849685669 mm

Highest mean error: 10.697199821472168 mm for frame 112

Lowest mean error: 3.52085018157959 mm for frame 108

Saving results

Total time: 147.725932598114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083331
Iteration 2/25 | Loss: 0.00216555
Iteration 3/25 | Loss: 0.00185921
Iteration 4/25 | Loss: 0.00151963
Iteration 5/25 | Loss: 0.00136097
Iteration 6/25 | Loss: 0.00114425
Iteration 7/25 | Loss: 0.00111630
Iteration 8/25 | Loss: 0.00110901
Iteration 9/25 | Loss: 0.00110974
Iteration 10/25 | Loss: 0.00112754
Iteration 11/25 | Loss: 0.00111292
Iteration 12/25 | Loss: 0.00115654
Iteration 13/25 | Loss: 0.00114762
Iteration 14/25 | Loss: 0.00113776
Iteration 15/25 | Loss: 0.00110177
Iteration 16/25 | Loss: 0.00108695
Iteration 17/25 | Loss: 0.00107974
Iteration 18/25 | Loss: 0.00107900
Iteration 19/25 | Loss: 0.00107872
Iteration 20/25 | Loss: 0.00107855
Iteration 21/25 | Loss: 0.00107851
Iteration 22/25 | Loss: 0.00107851
Iteration 23/25 | Loss: 0.00107850
Iteration 24/25 | Loss: 0.00107848
Iteration 25/25 | Loss: 0.00107848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28362131
Iteration 2/25 | Loss: 0.00113571
Iteration 3/25 | Loss: 0.00113570
Iteration 4/25 | Loss: 0.00113570
Iteration 5/25 | Loss: 0.00113570
Iteration 6/25 | Loss: 0.00113570
Iteration 7/25 | Loss: 0.00113570
Iteration 8/25 | Loss: 0.00113569
Iteration 9/25 | Loss: 0.00113569
Iteration 10/25 | Loss: 0.00113569
Iteration 11/25 | Loss: 0.00113569
Iteration 12/25 | Loss: 0.00113569
Iteration 13/25 | Loss: 0.00113569
Iteration 14/25 | Loss: 0.00113569
Iteration 15/25 | Loss: 0.00113569
Iteration 16/25 | Loss: 0.00113569
Iteration 17/25 | Loss: 0.00113569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011356935137882829, 0.0011356935137882829, 0.0011356935137882829, 0.0011356935137882829, 0.0011356935137882829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011356935137882829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113569
Iteration 2/1000 | Loss: 0.00011419
Iteration 3/1000 | Loss: 0.00007114
Iteration 4/1000 | Loss: 0.00005844
Iteration 5/1000 | Loss: 0.00005333
Iteration 6/1000 | Loss: 0.00005053
Iteration 7/1000 | Loss: 0.00004857
Iteration 8/1000 | Loss: 0.00009480
Iteration 9/1000 | Loss: 0.00004910
Iteration 10/1000 | Loss: 0.00015906
Iteration 11/1000 | Loss: 0.00004301
Iteration 12/1000 | Loss: 0.00003559
Iteration 13/1000 | Loss: 0.00003341
Iteration 14/1000 | Loss: 0.00003222
Iteration 15/1000 | Loss: 0.00003159
Iteration 16/1000 | Loss: 0.00003116
Iteration 17/1000 | Loss: 0.00003090
Iteration 18/1000 | Loss: 0.00003069
Iteration 19/1000 | Loss: 0.00003064
Iteration 20/1000 | Loss: 0.00003048
Iteration 21/1000 | Loss: 0.00003046
Iteration 22/1000 | Loss: 0.00003046
Iteration 23/1000 | Loss: 0.00003045
Iteration 24/1000 | Loss: 0.00003045
Iteration 25/1000 | Loss: 0.00003042
Iteration 26/1000 | Loss: 0.00003042
Iteration 27/1000 | Loss: 0.00003041
Iteration 28/1000 | Loss: 0.00003040
Iteration 29/1000 | Loss: 0.00003040
Iteration 30/1000 | Loss: 0.00003039
Iteration 31/1000 | Loss: 0.00003039
Iteration 32/1000 | Loss: 0.00003036
Iteration 33/1000 | Loss: 0.00003036
Iteration 34/1000 | Loss: 0.00003035
Iteration 35/1000 | Loss: 0.00003035
Iteration 36/1000 | Loss: 0.00003031
Iteration 37/1000 | Loss: 0.00003031
Iteration 38/1000 | Loss: 0.00003030
Iteration 39/1000 | Loss: 0.00003030
Iteration 40/1000 | Loss: 0.00003030
Iteration 41/1000 | Loss: 0.00003029
Iteration 42/1000 | Loss: 0.00003029
Iteration 43/1000 | Loss: 0.00003029
Iteration 44/1000 | Loss: 0.00003027
Iteration 45/1000 | Loss: 0.00003027
Iteration 46/1000 | Loss: 0.00003027
Iteration 47/1000 | Loss: 0.00003027
Iteration 48/1000 | Loss: 0.00003026
Iteration 49/1000 | Loss: 0.00003026
Iteration 50/1000 | Loss: 0.00003026
Iteration 51/1000 | Loss: 0.00003026
Iteration 52/1000 | Loss: 0.00003025
Iteration 53/1000 | Loss: 0.00003025
Iteration 54/1000 | Loss: 0.00003025
Iteration 55/1000 | Loss: 0.00003024
Iteration 56/1000 | Loss: 0.00003024
Iteration 57/1000 | Loss: 0.00003023
Iteration 58/1000 | Loss: 0.00003023
Iteration 59/1000 | Loss: 0.00003023
Iteration 60/1000 | Loss: 0.00003022
Iteration 61/1000 | Loss: 0.00003022
Iteration 62/1000 | Loss: 0.00003022
Iteration 63/1000 | Loss: 0.00003022
Iteration 64/1000 | Loss: 0.00003022
Iteration 65/1000 | Loss: 0.00003022
Iteration 66/1000 | Loss: 0.00003022
Iteration 67/1000 | Loss: 0.00003021
Iteration 68/1000 | Loss: 0.00003021
Iteration 69/1000 | Loss: 0.00003021
Iteration 70/1000 | Loss: 0.00003021
Iteration 71/1000 | Loss: 0.00003021
Iteration 72/1000 | Loss: 0.00003021
Iteration 73/1000 | Loss: 0.00003021
Iteration 74/1000 | Loss: 0.00003020
Iteration 75/1000 | Loss: 0.00003020
Iteration 76/1000 | Loss: 0.00003020
Iteration 77/1000 | Loss: 0.00003020
Iteration 78/1000 | Loss: 0.00003020
Iteration 79/1000 | Loss: 0.00003020
Iteration 80/1000 | Loss: 0.00003020
Iteration 81/1000 | Loss: 0.00003020
Iteration 82/1000 | Loss: 0.00003020
Iteration 83/1000 | Loss: 0.00003020
Iteration 84/1000 | Loss: 0.00003020
Iteration 85/1000 | Loss: 0.00003019
Iteration 86/1000 | Loss: 0.00003019
Iteration 87/1000 | Loss: 0.00003019
Iteration 88/1000 | Loss: 0.00003019
Iteration 89/1000 | Loss: 0.00003019
Iteration 90/1000 | Loss: 0.00003019
Iteration 91/1000 | Loss: 0.00003019
Iteration 92/1000 | Loss: 0.00003019
Iteration 93/1000 | Loss: 0.00003019
Iteration 94/1000 | Loss: 0.00003019
Iteration 95/1000 | Loss: 0.00003019
Iteration 96/1000 | Loss: 0.00003019
Iteration 97/1000 | Loss: 0.00003019
Iteration 98/1000 | Loss: 0.00003019
Iteration 99/1000 | Loss: 0.00003019
Iteration 100/1000 | Loss: 0.00003019
Iteration 101/1000 | Loss: 0.00003019
Iteration 102/1000 | Loss: 0.00003019
Iteration 103/1000 | Loss: 0.00003019
Iteration 104/1000 | Loss: 0.00003019
Iteration 105/1000 | Loss: 0.00003019
Iteration 106/1000 | Loss: 0.00003019
Iteration 107/1000 | Loss: 0.00003019
Iteration 108/1000 | Loss: 0.00003019
Iteration 109/1000 | Loss: 0.00003019
Iteration 110/1000 | Loss: 0.00003019
Iteration 111/1000 | Loss: 0.00003019
Iteration 112/1000 | Loss: 0.00003019
Iteration 113/1000 | Loss: 0.00003019
Iteration 114/1000 | Loss: 0.00003019
Iteration 115/1000 | Loss: 0.00003019
Iteration 116/1000 | Loss: 0.00003019
Iteration 117/1000 | Loss: 0.00003019
Iteration 118/1000 | Loss: 0.00003019
Iteration 119/1000 | Loss: 0.00003019
Iteration 120/1000 | Loss: 0.00003019
Iteration 121/1000 | Loss: 0.00003019
Iteration 122/1000 | Loss: 0.00003019
Iteration 123/1000 | Loss: 0.00003019
Iteration 124/1000 | Loss: 0.00003019
Iteration 125/1000 | Loss: 0.00003019
Iteration 126/1000 | Loss: 0.00003019
Iteration 127/1000 | Loss: 0.00003019
Iteration 128/1000 | Loss: 0.00003019
Iteration 129/1000 | Loss: 0.00003019
Iteration 130/1000 | Loss: 0.00003019
Iteration 131/1000 | Loss: 0.00003019
Iteration 132/1000 | Loss: 0.00003019
Iteration 133/1000 | Loss: 0.00003019
Iteration 134/1000 | Loss: 0.00003019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.018712595803663e-05, 3.018712595803663e-05, 3.018712595803663e-05, 3.018712595803663e-05, 3.018712595803663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.018712595803663e-05

Optimization complete. Final v2v error: 3.698514699935913 mm

Highest mean error: 14.801207542419434 mm for frame 7

Lowest mean error: 2.9432179927825928 mm for frame 1

Saving results

Total time: 70.01972556114197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086734
Iteration 2/25 | Loss: 0.00323542
Iteration 3/25 | Loss: 0.00233484
Iteration 4/25 | Loss: 0.00209508
Iteration 5/25 | Loss: 0.00204843
Iteration 6/25 | Loss: 0.00189422
Iteration 7/25 | Loss: 0.00159384
Iteration 8/25 | Loss: 0.00145795
Iteration 9/25 | Loss: 0.00133580
Iteration 10/25 | Loss: 0.00127209
Iteration 11/25 | Loss: 0.00125582
Iteration 12/25 | Loss: 0.00125931
Iteration 13/25 | Loss: 0.00125191
Iteration 14/25 | Loss: 0.00121185
Iteration 15/25 | Loss: 0.00118597
Iteration 16/25 | Loss: 0.00117740
Iteration 17/25 | Loss: 0.00116946
Iteration 18/25 | Loss: 0.00116289
Iteration 19/25 | Loss: 0.00116489
Iteration 20/25 | Loss: 0.00116711
Iteration 21/25 | Loss: 0.00116712
Iteration 22/25 | Loss: 0.00116564
Iteration 23/25 | Loss: 0.00117259
Iteration 24/25 | Loss: 0.00116606
Iteration 25/25 | Loss: 0.00116203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30924237
Iteration 2/25 | Loss: 0.00107175
Iteration 3/25 | Loss: 0.00107175
Iteration 4/25 | Loss: 0.00107175
Iteration 5/25 | Loss: 0.00107175
Iteration 6/25 | Loss: 0.00107175
Iteration 7/25 | Loss: 0.00107174
Iteration 8/25 | Loss: 0.00107174
Iteration 9/25 | Loss: 0.00107174
Iteration 10/25 | Loss: 0.00105018
Iteration 11/25 | Loss: 0.00105017
Iteration 12/25 | Loss: 0.00104858
Iteration 13/25 | Loss: 0.00104858
Iteration 14/25 | Loss: 0.00104858
Iteration 15/25 | Loss: 0.00104858
Iteration 16/25 | Loss: 0.00104858
Iteration 17/25 | Loss: 0.00104858
Iteration 18/25 | Loss: 0.00104858
Iteration 19/25 | Loss: 0.00104858
Iteration 20/25 | Loss: 0.00104858
Iteration 21/25 | Loss: 0.00104858
Iteration 22/25 | Loss: 0.00104858
Iteration 23/25 | Loss: 0.00104858
Iteration 24/25 | Loss: 0.00104858
Iteration 25/25 | Loss: 0.00104858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104858
Iteration 2/1000 | Loss: 0.00038356
Iteration 3/1000 | Loss: 0.00033297
Iteration 4/1000 | Loss: 0.00024914
Iteration 5/1000 | Loss: 0.00014039
Iteration 6/1000 | Loss: 0.00022225
Iteration 7/1000 | Loss: 0.00025005
Iteration 8/1000 | Loss: 0.00031029
Iteration 9/1000 | Loss: 0.00043425
Iteration 10/1000 | Loss: 0.00018624
Iteration 11/1000 | Loss: 0.00020251
Iteration 12/1000 | Loss: 0.00028574
Iteration 13/1000 | Loss: 0.00022024
Iteration 14/1000 | Loss: 0.00020160
Iteration 15/1000 | Loss: 0.00016723
Iteration 16/1000 | Loss: 0.00022244
Iteration 17/1000 | Loss: 0.00039827
Iteration 18/1000 | Loss: 0.00019313
Iteration 19/1000 | Loss: 0.00019594
Iteration 20/1000 | Loss: 0.00024360
Iteration 21/1000 | Loss: 0.00010383
Iteration 22/1000 | Loss: 0.00014380
Iteration 23/1000 | Loss: 0.00017568
Iteration 24/1000 | Loss: 0.00013376
Iteration 25/1000 | Loss: 0.00016780
Iteration 26/1000 | Loss: 0.00015892
Iteration 27/1000 | Loss: 0.00036288
Iteration 28/1000 | Loss: 0.00017806
Iteration 29/1000 | Loss: 0.00015627
Iteration 30/1000 | Loss: 0.00020078
Iteration 31/1000 | Loss: 0.00014968
Iteration 32/1000 | Loss: 0.00029345
Iteration 33/1000 | Loss: 0.00018316
Iteration 34/1000 | Loss: 0.00016692
Iteration 35/1000 | Loss: 0.00015486
Iteration 36/1000 | Loss: 0.00021936
Iteration 37/1000 | Loss: 0.00023379
Iteration 38/1000 | Loss: 0.00027553
Iteration 39/1000 | Loss: 0.00010899
Iteration 40/1000 | Loss: 0.00011941
Iteration 41/1000 | Loss: 0.00006801
Iteration 42/1000 | Loss: 0.00006407
Iteration 43/1000 | Loss: 0.00011429
Iteration 44/1000 | Loss: 0.00005944
Iteration 45/1000 | Loss: 0.00007260
Iteration 46/1000 | Loss: 0.00007223
Iteration 47/1000 | Loss: 0.00006139
Iteration 48/1000 | Loss: 0.00007280
Iteration 49/1000 | Loss: 0.00079450
Iteration 50/1000 | Loss: 0.00916340
Iteration 51/1000 | Loss: 0.00496510
Iteration 52/1000 | Loss: 0.00237647
Iteration 53/1000 | Loss: 0.00408169
Iteration 54/1000 | Loss: 0.00057880
Iteration 55/1000 | Loss: 0.00099612
Iteration 56/1000 | Loss: 0.00035389
Iteration 57/1000 | Loss: 0.00222965
Iteration 58/1000 | Loss: 0.00089853
Iteration 59/1000 | Loss: 0.00172372
Iteration 60/1000 | Loss: 0.00073112
Iteration 61/1000 | Loss: 0.00106129
Iteration 62/1000 | Loss: 0.00102640
Iteration 63/1000 | Loss: 0.00086399
Iteration 64/1000 | Loss: 0.00024506
Iteration 65/1000 | Loss: 0.00029954
Iteration 66/1000 | Loss: 0.00032889
Iteration 67/1000 | Loss: 0.00040332
Iteration 68/1000 | Loss: 0.00045390
Iteration 69/1000 | Loss: 0.00062635
Iteration 70/1000 | Loss: 0.00097168
Iteration 71/1000 | Loss: 0.00043827
Iteration 72/1000 | Loss: 0.00033335
Iteration 73/1000 | Loss: 0.00060462
Iteration 74/1000 | Loss: 0.00012310
Iteration 75/1000 | Loss: 0.00007040
Iteration 76/1000 | Loss: 0.00028891
Iteration 77/1000 | Loss: 0.00044561
Iteration 78/1000 | Loss: 0.00046190
Iteration 79/1000 | Loss: 0.00028824
Iteration 80/1000 | Loss: 0.00054765
Iteration 81/1000 | Loss: 0.00047334
Iteration 82/1000 | Loss: 0.00027266
Iteration 83/1000 | Loss: 0.00078781
Iteration 84/1000 | Loss: 0.00016145
Iteration 85/1000 | Loss: 0.00037151
Iteration 86/1000 | Loss: 0.00048615
Iteration 87/1000 | Loss: 0.00045788
Iteration 88/1000 | Loss: 0.00086156
Iteration 89/1000 | Loss: 0.00113537
Iteration 90/1000 | Loss: 0.00101286
Iteration 91/1000 | Loss: 0.00013521
Iteration 92/1000 | Loss: 0.00005023
Iteration 93/1000 | Loss: 0.00008174
Iteration 94/1000 | Loss: 0.00004520
Iteration 95/1000 | Loss: 0.00004000
Iteration 96/1000 | Loss: 0.00024273
Iteration 97/1000 | Loss: 0.00027098
Iteration 98/1000 | Loss: 0.00008392
Iteration 99/1000 | Loss: 0.00013235
Iteration 100/1000 | Loss: 0.00035446
Iteration 101/1000 | Loss: 0.00026016
Iteration 102/1000 | Loss: 0.00039760
Iteration 103/1000 | Loss: 0.00009589
Iteration 104/1000 | Loss: 0.00031819
Iteration 105/1000 | Loss: 0.00045714
Iteration 106/1000 | Loss: 0.00050470
Iteration 107/1000 | Loss: 0.00039741
Iteration 108/1000 | Loss: 0.00009511
Iteration 109/1000 | Loss: 0.00048981
Iteration 110/1000 | Loss: 0.00063520
Iteration 111/1000 | Loss: 0.00053692
Iteration 112/1000 | Loss: 0.00037237
Iteration 113/1000 | Loss: 0.00051762
Iteration 114/1000 | Loss: 0.00057581
Iteration 115/1000 | Loss: 0.00026019
Iteration 116/1000 | Loss: 0.00005674
Iteration 117/1000 | Loss: 0.00005385
Iteration 118/1000 | Loss: 0.00009383
Iteration 119/1000 | Loss: 0.00003694
Iteration 120/1000 | Loss: 0.00002972
Iteration 121/1000 | Loss: 0.00008719
Iteration 122/1000 | Loss: 0.00002924
Iteration 123/1000 | Loss: 0.00003237
Iteration 124/1000 | Loss: 0.00003319
Iteration 125/1000 | Loss: 0.00003622
Iteration 126/1000 | Loss: 0.00091726
Iteration 127/1000 | Loss: 0.00036706
Iteration 128/1000 | Loss: 0.00051690
Iteration 129/1000 | Loss: 0.00041100
Iteration 130/1000 | Loss: 0.00030126
Iteration 131/1000 | Loss: 0.00005270
Iteration 132/1000 | Loss: 0.00002577
Iteration 133/1000 | Loss: 0.00005211
Iteration 134/1000 | Loss: 0.00003718
Iteration 135/1000 | Loss: 0.00004420
Iteration 136/1000 | Loss: 0.00002959
Iteration 137/1000 | Loss: 0.00002320
Iteration 138/1000 | Loss: 0.00001851
Iteration 139/1000 | Loss: 0.00004216
Iteration 140/1000 | Loss: 0.00001604
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001753
Iteration 143/1000 | Loss: 0.00003230
Iteration 144/1000 | Loss: 0.00001489
Iteration 145/1000 | Loss: 0.00003258
Iteration 146/1000 | Loss: 0.00004119
Iteration 147/1000 | Loss: 0.00001992
Iteration 148/1000 | Loss: 0.00001455
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00004996
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001761
Iteration 153/1000 | Loss: 0.00001677
Iteration 154/1000 | Loss: 0.00001418
Iteration 155/1000 | Loss: 0.00001418
Iteration 156/1000 | Loss: 0.00001418
Iteration 157/1000 | Loss: 0.00001418
Iteration 158/1000 | Loss: 0.00001417
Iteration 159/1000 | Loss: 0.00001417
Iteration 160/1000 | Loss: 0.00001417
Iteration 161/1000 | Loss: 0.00001417
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001795
Iteration 164/1000 | Loss: 0.00002539
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001410
Iteration 167/1000 | Loss: 0.00001408
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00002065
Iteration 170/1000 | Loss: 0.00001482
Iteration 171/1000 | Loss: 0.00001405
Iteration 172/1000 | Loss: 0.00001405
Iteration 173/1000 | Loss: 0.00001405
Iteration 174/1000 | Loss: 0.00001405
Iteration 175/1000 | Loss: 0.00001405
Iteration 176/1000 | Loss: 0.00001405
Iteration 177/1000 | Loss: 0.00001405
Iteration 178/1000 | Loss: 0.00001404
Iteration 179/1000 | Loss: 0.00001404
Iteration 180/1000 | Loss: 0.00001404
Iteration 181/1000 | Loss: 0.00001404
Iteration 182/1000 | Loss: 0.00001404
Iteration 183/1000 | Loss: 0.00001404
Iteration 184/1000 | Loss: 0.00001404
Iteration 185/1000 | Loss: 0.00001404
Iteration 186/1000 | Loss: 0.00001403
Iteration 187/1000 | Loss: 0.00001403
Iteration 188/1000 | Loss: 0.00001403
Iteration 189/1000 | Loss: 0.00001403
Iteration 190/1000 | Loss: 0.00001403
Iteration 191/1000 | Loss: 0.00001941
Iteration 192/1000 | Loss: 0.00001478
Iteration 193/1000 | Loss: 0.00001403
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001403
Iteration 199/1000 | Loss: 0.00001403
Iteration 200/1000 | Loss: 0.00001403
Iteration 201/1000 | Loss: 0.00001402
Iteration 202/1000 | Loss: 0.00001402
Iteration 203/1000 | Loss: 0.00001402
Iteration 204/1000 | Loss: 0.00001402
Iteration 205/1000 | Loss: 0.00001402
Iteration 206/1000 | Loss: 0.00001402
Iteration 207/1000 | Loss: 0.00001402
Iteration 208/1000 | Loss: 0.00001402
Iteration 209/1000 | Loss: 0.00001402
Iteration 210/1000 | Loss: 0.00001401
Iteration 211/1000 | Loss: 0.00001401
Iteration 212/1000 | Loss: 0.00001401
Iteration 213/1000 | Loss: 0.00001401
Iteration 214/1000 | Loss: 0.00001401
Iteration 215/1000 | Loss: 0.00001401
Iteration 216/1000 | Loss: 0.00001401
Iteration 217/1000 | Loss: 0.00001401
Iteration 218/1000 | Loss: 0.00001401
Iteration 219/1000 | Loss: 0.00001401
Iteration 220/1000 | Loss: 0.00001401
Iteration 221/1000 | Loss: 0.00001401
Iteration 222/1000 | Loss: 0.00001401
Iteration 223/1000 | Loss: 0.00001401
Iteration 224/1000 | Loss: 0.00001401
Iteration 225/1000 | Loss: 0.00001401
Iteration 226/1000 | Loss: 0.00001401
Iteration 227/1000 | Loss: 0.00001401
Iteration 228/1000 | Loss: 0.00001401
Iteration 229/1000 | Loss: 0.00001401
Iteration 230/1000 | Loss: 0.00001401
Iteration 231/1000 | Loss: 0.00001401
Iteration 232/1000 | Loss: 0.00001401
Iteration 233/1000 | Loss: 0.00001401
Iteration 234/1000 | Loss: 0.00001401
Iteration 235/1000 | Loss: 0.00001401
Iteration 236/1000 | Loss: 0.00001401
Iteration 237/1000 | Loss: 0.00001401
Iteration 238/1000 | Loss: 0.00001401
Iteration 239/1000 | Loss: 0.00001401
Iteration 240/1000 | Loss: 0.00001401
Iteration 241/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.4013227882969659e-05, 1.4013227882969659e-05, 1.4013227882969659e-05, 1.4013227882969659e-05, 1.4013227882969659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4013227882969659e-05

Optimization complete. Final v2v error: 3.0272600650787354 mm

Highest mean error: 8.729918479919434 mm for frame 135

Lowest mean error: 2.6527535915374756 mm for frame 142

Saving results

Total time: 294.2377305030823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01181102
Iteration 2/25 | Loss: 0.01181102
Iteration 3/25 | Loss: 0.01181102
Iteration 4/25 | Loss: 0.00199068
Iteration 5/25 | Loss: 0.00131976
Iteration 6/25 | Loss: 0.00123289
Iteration 7/25 | Loss: 0.00122917
Iteration 8/25 | Loss: 0.00121957
Iteration 9/25 | Loss: 0.00121129
Iteration 10/25 | Loss: 0.00120679
Iteration 11/25 | Loss: 0.00120668
Iteration 12/25 | Loss: 0.00120434
Iteration 13/25 | Loss: 0.00120284
Iteration 14/25 | Loss: 0.00120233
Iteration 15/25 | Loss: 0.00120211
Iteration 16/25 | Loss: 0.00120206
Iteration 17/25 | Loss: 0.00120206
Iteration 18/25 | Loss: 0.00120206
Iteration 19/25 | Loss: 0.00120206
Iteration 20/25 | Loss: 0.00120206
Iteration 21/25 | Loss: 0.00120206
Iteration 22/25 | Loss: 0.00120206
Iteration 23/25 | Loss: 0.00120206
Iteration 24/25 | Loss: 0.00120206
Iteration 25/25 | Loss: 0.00120206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30635071
Iteration 2/25 | Loss: 0.00057283
Iteration 3/25 | Loss: 0.00057283
Iteration 4/25 | Loss: 0.00057283
Iteration 5/25 | Loss: 0.00057283
Iteration 6/25 | Loss: 0.00057283
Iteration 7/25 | Loss: 0.00057283
Iteration 8/25 | Loss: 0.00057283
Iteration 9/25 | Loss: 0.00057283
Iteration 10/25 | Loss: 0.00057283
Iteration 11/25 | Loss: 0.00057283
Iteration 12/25 | Loss: 0.00057283
Iteration 13/25 | Loss: 0.00057283
Iteration 14/25 | Loss: 0.00057283
Iteration 15/25 | Loss: 0.00057283
Iteration 16/25 | Loss: 0.00057283
Iteration 17/25 | Loss: 0.00057283
Iteration 18/25 | Loss: 0.00057283
Iteration 19/25 | Loss: 0.00057283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005728317773900926, 0.0005728317773900926, 0.0005728317773900926, 0.0005728317773900926, 0.0005728317773900926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005728317773900926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057283
Iteration 2/1000 | Loss: 0.00005862
Iteration 3/1000 | Loss: 0.00003747
Iteration 4/1000 | Loss: 0.00003464
Iteration 5/1000 | Loss: 0.00003325
Iteration 6/1000 | Loss: 0.00003256
Iteration 7/1000 | Loss: 0.00003214
Iteration 8/1000 | Loss: 0.00003176
Iteration 9/1000 | Loss: 0.00003140
Iteration 10/1000 | Loss: 0.00003114
Iteration 11/1000 | Loss: 0.00003093
Iteration 12/1000 | Loss: 0.00003073
Iteration 13/1000 | Loss: 0.00003072
Iteration 14/1000 | Loss: 0.00003063
Iteration 15/1000 | Loss: 0.00003062
Iteration 16/1000 | Loss: 0.00003054
Iteration 17/1000 | Loss: 0.00003053
Iteration 18/1000 | Loss: 0.00003050
Iteration 19/1000 | Loss: 0.00003050
Iteration 20/1000 | Loss: 0.00003047
Iteration 21/1000 | Loss: 0.00003047
Iteration 22/1000 | Loss: 0.00003046
Iteration 23/1000 | Loss: 0.00003046
Iteration 24/1000 | Loss: 0.00003046
Iteration 25/1000 | Loss: 0.00003046
Iteration 26/1000 | Loss: 0.00003046
Iteration 27/1000 | Loss: 0.00003046
Iteration 28/1000 | Loss: 0.00003046
Iteration 29/1000 | Loss: 0.00003044
Iteration 30/1000 | Loss: 0.00003042
Iteration 31/1000 | Loss: 0.00003042
Iteration 32/1000 | Loss: 0.00003041
Iteration 33/1000 | Loss: 0.00003041
Iteration 34/1000 | Loss: 0.00003040
Iteration 35/1000 | Loss: 0.00003040
Iteration 36/1000 | Loss: 0.00003040
Iteration 37/1000 | Loss: 0.00003040
Iteration 38/1000 | Loss: 0.00003039
Iteration 39/1000 | Loss: 0.00003039
Iteration 40/1000 | Loss: 0.00003039
Iteration 41/1000 | Loss: 0.00003039
Iteration 42/1000 | Loss: 0.00003038
Iteration 43/1000 | Loss: 0.00003038
Iteration 44/1000 | Loss: 0.00003037
Iteration 45/1000 | Loss: 0.00003037
Iteration 46/1000 | Loss: 0.00003037
Iteration 47/1000 | Loss: 0.00003037
Iteration 48/1000 | Loss: 0.00003036
Iteration 49/1000 | Loss: 0.00003036
Iteration 50/1000 | Loss: 0.00003036
Iteration 51/1000 | Loss: 0.00003036
Iteration 52/1000 | Loss: 0.00003036
Iteration 53/1000 | Loss: 0.00003035
Iteration 54/1000 | Loss: 0.00003035
Iteration 55/1000 | Loss: 0.00003034
Iteration 56/1000 | Loss: 0.00003034
Iteration 57/1000 | Loss: 0.00003034
Iteration 58/1000 | Loss: 0.00003033
Iteration 59/1000 | Loss: 0.00003033
Iteration 60/1000 | Loss: 0.00003033
Iteration 61/1000 | Loss: 0.00003033
Iteration 62/1000 | Loss: 0.00003033
Iteration 63/1000 | Loss: 0.00003033
Iteration 64/1000 | Loss: 0.00003033
Iteration 65/1000 | Loss: 0.00003033
Iteration 66/1000 | Loss: 0.00003033
Iteration 67/1000 | Loss: 0.00003033
Iteration 68/1000 | Loss: 0.00003033
Iteration 69/1000 | Loss: 0.00003032
Iteration 70/1000 | Loss: 0.00003032
Iteration 71/1000 | Loss: 0.00003031
Iteration 72/1000 | Loss: 0.00003031
Iteration 73/1000 | Loss: 0.00003031
Iteration 74/1000 | Loss: 0.00003031
Iteration 75/1000 | Loss: 0.00003031
Iteration 76/1000 | Loss: 0.00003030
Iteration 77/1000 | Loss: 0.00003030
Iteration 78/1000 | Loss: 0.00003030
Iteration 79/1000 | Loss: 0.00003030
Iteration 80/1000 | Loss: 0.00003030
Iteration 81/1000 | Loss: 0.00003030
Iteration 82/1000 | Loss: 0.00003030
Iteration 83/1000 | Loss: 0.00003030
Iteration 84/1000 | Loss: 0.00003029
Iteration 85/1000 | Loss: 0.00003029
Iteration 86/1000 | Loss: 0.00003029
Iteration 87/1000 | Loss: 0.00003028
Iteration 88/1000 | Loss: 0.00003028
Iteration 89/1000 | Loss: 0.00003028
Iteration 90/1000 | Loss: 0.00003028
Iteration 91/1000 | Loss: 0.00003028
Iteration 92/1000 | Loss: 0.00003027
Iteration 93/1000 | Loss: 0.00003027
Iteration 94/1000 | Loss: 0.00003027
Iteration 95/1000 | Loss: 0.00003027
Iteration 96/1000 | Loss: 0.00003027
Iteration 97/1000 | Loss: 0.00003027
Iteration 98/1000 | Loss: 0.00003026
Iteration 99/1000 | Loss: 0.00003026
Iteration 100/1000 | Loss: 0.00003026
Iteration 101/1000 | Loss: 0.00003026
Iteration 102/1000 | Loss: 0.00003026
Iteration 103/1000 | Loss: 0.00003026
Iteration 104/1000 | Loss: 0.00003026
Iteration 105/1000 | Loss: 0.00003026
Iteration 106/1000 | Loss: 0.00003026
Iteration 107/1000 | Loss: 0.00003026
Iteration 108/1000 | Loss: 0.00003025
Iteration 109/1000 | Loss: 0.00003025
Iteration 110/1000 | Loss: 0.00003025
Iteration 111/1000 | Loss: 0.00003025
Iteration 112/1000 | Loss: 0.00003025
Iteration 113/1000 | Loss: 0.00003025
Iteration 114/1000 | Loss: 0.00003030
Iteration 115/1000 | Loss: 0.00003029
Iteration 116/1000 | Loss: 0.00003029
Iteration 117/1000 | Loss: 0.00003029
Iteration 118/1000 | Loss: 0.00003028
Iteration 119/1000 | Loss: 0.00003028
Iteration 120/1000 | Loss: 0.00003028
Iteration 121/1000 | Loss: 0.00003027
Iteration 122/1000 | Loss: 0.00003027
Iteration 123/1000 | Loss: 0.00003027
Iteration 124/1000 | Loss: 0.00003027
Iteration 125/1000 | Loss: 0.00003027
Iteration 126/1000 | Loss: 0.00003027
Iteration 127/1000 | Loss: 0.00003023
Iteration 128/1000 | Loss: 0.00003023
Iteration 129/1000 | Loss: 0.00003023
Iteration 130/1000 | Loss: 0.00003023
Iteration 131/1000 | Loss: 0.00003023
Iteration 132/1000 | Loss: 0.00003023
Iteration 133/1000 | Loss: 0.00003023
Iteration 134/1000 | Loss: 0.00003023
Iteration 135/1000 | Loss: 0.00003023
Iteration 136/1000 | Loss: 0.00003023
Iteration 137/1000 | Loss: 0.00003023
Iteration 138/1000 | Loss: 0.00003023
Iteration 139/1000 | Loss: 0.00003023
Iteration 140/1000 | Loss: 0.00003023
Iteration 141/1000 | Loss: 0.00003023
Iteration 142/1000 | Loss: 0.00003023
Iteration 143/1000 | Loss: 0.00003023
Iteration 144/1000 | Loss: 0.00003023
Iteration 145/1000 | Loss: 0.00003023
Iteration 146/1000 | Loss: 0.00003023
Iteration 147/1000 | Loss: 0.00003023
Iteration 148/1000 | Loss: 0.00003023
Iteration 149/1000 | Loss: 0.00003023
Iteration 150/1000 | Loss: 0.00003023
Iteration 151/1000 | Loss: 0.00003023
Iteration 152/1000 | Loss: 0.00003023
Iteration 153/1000 | Loss: 0.00003023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.0228537070797756e-05, 3.0228537070797756e-05, 3.0228537070797756e-05, 3.0228537070797756e-05, 3.0228537070797756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0228537070797756e-05

Optimization complete. Final v2v error: 4.225902080535889 mm

Highest mean error: 20.809297561645508 mm for frame 59

Lowest mean error: 3.5602996349334717 mm for frame 150

Saving results

Total time: 153.42789340019226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401938
Iteration 2/25 | Loss: 0.00118891
Iteration 3/25 | Loss: 0.00111221
Iteration 4/25 | Loss: 0.00110343
Iteration 5/25 | Loss: 0.00110051
Iteration 6/25 | Loss: 0.00110035
Iteration 7/25 | Loss: 0.00110035
Iteration 8/25 | Loss: 0.00110035
Iteration 9/25 | Loss: 0.00110035
Iteration 10/25 | Loss: 0.00110035
Iteration 11/25 | Loss: 0.00110035
Iteration 12/25 | Loss: 0.00110035
Iteration 13/25 | Loss: 0.00110035
Iteration 14/25 | Loss: 0.00110035
Iteration 15/25 | Loss: 0.00110035
Iteration 16/25 | Loss: 0.00110035
Iteration 17/25 | Loss: 0.00110035
Iteration 18/25 | Loss: 0.00110035
Iteration 19/25 | Loss: 0.00110035
Iteration 20/25 | Loss: 0.00110035
Iteration 21/25 | Loss: 0.00110035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011003471445292234, 0.0011003471445292234, 0.0011003471445292234, 0.0011003471445292234, 0.0011003471445292234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011003471445292234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82073808
Iteration 2/25 | Loss: 0.00150337
Iteration 3/25 | Loss: 0.00150336
Iteration 4/25 | Loss: 0.00150335
Iteration 5/25 | Loss: 0.00150335
Iteration 6/25 | Loss: 0.00150335
Iteration 7/25 | Loss: 0.00150335
Iteration 8/25 | Loss: 0.00150335
Iteration 9/25 | Loss: 0.00150335
Iteration 10/25 | Loss: 0.00150335
Iteration 11/25 | Loss: 0.00150335
Iteration 12/25 | Loss: 0.00150335
Iteration 13/25 | Loss: 0.00150335
Iteration 14/25 | Loss: 0.00150335
Iteration 15/25 | Loss: 0.00150335
Iteration 16/25 | Loss: 0.00150335
Iteration 17/25 | Loss: 0.00150335
Iteration 18/25 | Loss: 0.00150335
Iteration 19/25 | Loss: 0.00150335
Iteration 20/25 | Loss: 0.00150335
Iteration 21/25 | Loss: 0.00150335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015033517265692353, 0.0015033517265692353, 0.0015033517265692353, 0.0015033517265692353, 0.0015033517265692353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015033517265692353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150335
Iteration 2/1000 | Loss: 0.00002949
Iteration 3/1000 | Loss: 0.00002162
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001561
Iteration 6/1000 | Loss: 0.00001469
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001397
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001369
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001360
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001356
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001353
Iteration 40/1000 | Loss: 0.00001353
Iteration 41/1000 | Loss: 0.00001352
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001351
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001346
Iteration 82/1000 | Loss: 0.00001346
Iteration 83/1000 | Loss: 0.00001346
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001345
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001344
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001338
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001336
Iteration 127/1000 | Loss: 0.00001336
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001335
Iteration 131/1000 | Loss: 0.00001335
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001334
Iteration 135/1000 | Loss: 0.00001334
Iteration 136/1000 | Loss: 0.00001334
Iteration 137/1000 | Loss: 0.00001333
Iteration 138/1000 | Loss: 0.00001333
Iteration 139/1000 | Loss: 0.00001333
Iteration 140/1000 | Loss: 0.00001332
Iteration 141/1000 | Loss: 0.00001332
Iteration 142/1000 | Loss: 0.00001332
Iteration 143/1000 | Loss: 0.00001332
Iteration 144/1000 | Loss: 0.00001332
Iteration 145/1000 | Loss: 0.00001332
Iteration 146/1000 | Loss: 0.00001332
Iteration 147/1000 | Loss: 0.00001332
Iteration 148/1000 | Loss: 0.00001332
Iteration 149/1000 | Loss: 0.00001332
Iteration 150/1000 | Loss: 0.00001332
Iteration 151/1000 | Loss: 0.00001332
Iteration 152/1000 | Loss: 0.00001332
Iteration 153/1000 | Loss: 0.00001332
Iteration 154/1000 | Loss: 0.00001332
Iteration 155/1000 | Loss: 0.00001332
Iteration 156/1000 | Loss: 0.00001332
Iteration 157/1000 | Loss: 0.00001332
Iteration 158/1000 | Loss: 0.00001332
Iteration 159/1000 | Loss: 0.00001332
Iteration 160/1000 | Loss: 0.00001332
Iteration 161/1000 | Loss: 0.00001332
Iteration 162/1000 | Loss: 0.00001332
Iteration 163/1000 | Loss: 0.00001332
Iteration 164/1000 | Loss: 0.00001332
Iteration 165/1000 | Loss: 0.00001332
Iteration 166/1000 | Loss: 0.00001332
Iteration 167/1000 | Loss: 0.00001332
Iteration 168/1000 | Loss: 0.00001332
Iteration 169/1000 | Loss: 0.00001332
Iteration 170/1000 | Loss: 0.00001332
Iteration 171/1000 | Loss: 0.00001332
Iteration 172/1000 | Loss: 0.00001332
Iteration 173/1000 | Loss: 0.00001332
Iteration 174/1000 | Loss: 0.00001332
Iteration 175/1000 | Loss: 0.00001332
Iteration 176/1000 | Loss: 0.00001332
Iteration 177/1000 | Loss: 0.00001332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3319136087375227e-05, 1.3319136087375227e-05, 1.3319136087375227e-05, 1.3319136087375227e-05, 1.3319136087375227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3319136087375227e-05

Optimization complete. Final v2v error: 3.1079790592193604 mm

Highest mean error: 3.3142402172088623 mm for frame 120

Lowest mean error: 2.7761735916137695 mm for frame 197

Saving results

Total time: 34.279184341430664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065232
Iteration 2/25 | Loss: 0.01065232
Iteration 3/25 | Loss: 0.01065232
Iteration 4/25 | Loss: 0.01065232
Iteration 5/25 | Loss: 0.01065232
Iteration 6/25 | Loss: 0.01065232
Iteration 7/25 | Loss: 0.01065232
Iteration 8/25 | Loss: 0.01065231
Iteration 9/25 | Loss: 0.01065231
Iteration 10/25 | Loss: 0.01065231
Iteration 11/25 | Loss: 0.01065231
Iteration 12/25 | Loss: 0.01065231
Iteration 13/25 | Loss: 0.01065231
Iteration 14/25 | Loss: 0.01065231
Iteration 15/25 | Loss: 0.01065231
Iteration 16/25 | Loss: 0.01065231
Iteration 17/25 | Loss: 0.01065230
Iteration 18/25 | Loss: 0.01065230
Iteration 19/25 | Loss: 0.01065230
Iteration 20/25 | Loss: 0.01065230
Iteration 21/25 | Loss: 0.01065230
Iteration 22/25 | Loss: 0.01065230
Iteration 23/25 | Loss: 0.01065230
Iteration 24/25 | Loss: 0.01065230
Iteration 25/25 | Loss: 0.01065230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50948048
Iteration 2/25 | Loss: 0.09902000
Iteration 3/25 | Loss: 0.09778807
Iteration 4/25 | Loss: 0.09778798
Iteration 5/25 | Loss: 0.09694199
Iteration 6/25 | Loss: 0.09694037
Iteration 7/25 | Loss: 0.09694036
Iteration 8/25 | Loss: 0.09694036
Iteration 9/25 | Loss: 0.09694033
Iteration 10/25 | Loss: 0.09694033
Iteration 11/25 | Loss: 0.09694034
Iteration 12/25 | Loss: 0.09694034
Iteration 13/25 | Loss: 0.09694033
Iteration 14/25 | Loss: 0.09694033
Iteration 15/25 | Loss: 0.09694033
Iteration 16/25 | Loss: 0.09694033
Iteration 17/25 | Loss: 0.09694033
Iteration 18/25 | Loss: 0.09694033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.09694033116102219, 0.09694033116102219, 0.09694033116102219, 0.09694033116102219, 0.09694033116102219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09694033116102219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09694033
Iteration 2/1000 | Loss: 0.00414532
Iteration 3/1000 | Loss: 0.00130366
Iteration 4/1000 | Loss: 0.00084619
Iteration 5/1000 | Loss: 0.00035013
Iteration 6/1000 | Loss: 0.00034922
Iteration 7/1000 | Loss: 0.00019249
Iteration 8/1000 | Loss: 0.00016999
Iteration 9/1000 | Loss: 0.00010028
Iteration 10/1000 | Loss: 0.00056911
Iteration 11/1000 | Loss: 0.00006871
Iteration 12/1000 | Loss: 0.00030161
Iteration 13/1000 | Loss: 0.00008163
Iteration 14/1000 | Loss: 0.00005387
Iteration 15/1000 | Loss: 0.00034070
Iteration 16/1000 | Loss: 0.00013881
Iteration 17/1000 | Loss: 0.00011906
Iteration 18/1000 | Loss: 0.00004862
Iteration 19/1000 | Loss: 0.00014193
Iteration 20/1000 | Loss: 0.00022255
Iteration 21/1000 | Loss: 0.00014355
Iteration 22/1000 | Loss: 0.00019191
Iteration 23/1000 | Loss: 0.00082678
Iteration 24/1000 | Loss: 0.00023642
Iteration 25/1000 | Loss: 0.00017217
Iteration 26/1000 | Loss: 0.00013693
Iteration 27/1000 | Loss: 0.00004468
Iteration 28/1000 | Loss: 0.00017299
Iteration 29/1000 | Loss: 0.00015692
Iteration 30/1000 | Loss: 0.00011740
Iteration 31/1000 | Loss: 0.00008251
Iteration 32/1000 | Loss: 0.00005794
Iteration 33/1000 | Loss: 0.00045671
Iteration 34/1000 | Loss: 0.00015825
Iteration 35/1000 | Loss: 0.00004333
Iteration 36/1000 | Loss: 0.00003986
Iteration 37/1000 | Loss: 0.00004174
Iteration 38/1000 | Loss: 0.00011427
Iteration 39/1000 | Loss: 0.00003553
Iteration 40/1000 | Loss: 0.00018790
Iteration 41/1000 | Loss: 0.00044922
Iteration 42/1000 | Loss: 0.00004579
Iteration 43/1000 | Loss: 0.00017359
Iteration 44/1000 | Loss: 0.00003396
Iteration 45/1000 | Loss: 0.00003283
Iteration 46/1000 | Loss: 0.00015501
Iteration 47/1000 | Loss: 0.00021280
Iteration 48/1000 | Loss: 0.00005927
Iteration 49/1000 | Loss: 0.00005625
Iteration 50/1000 | Loss: 0.00008897
Iteration 51/1000 | Loss: 0.00003160
Iteration 52/1000 | Loss: 0.00003087
Iteration 53/1000 | Loss: 0.00003039
Iteration 54/1000 | Loss: 0.00004549
Iteration 55/1000 | Loss: 0.00024911
Iteration 56/1000 | Loss: 0.00012922
Iteration 57/1000 | Loss: 0.00003826
Iteration 58/1000 | Loss: 0.00011818
Iteration 59/1000 | Loss: 0.00005395
Iteration 60/1000 | Loss: 0.00004248
Iteration 61/1000 | Loss: 0.00005974
Iteration 62/1000 | Loss: 0.00005494
Iteration 63/1000 | Loss: 0.00006677
Iteration 64/1000 | Loss: 0.00003801
Iteration 65/1000 | Loss: 0.00011682
Iteration 66/1000 | Loss: 0.00003783
Iteration 67/1000 | Loss: 0.00003189
Iteration 68/1000 | Loss: 0.00002894
Iteration 69/1000 | Loss: 0.00015051
Iteration 70/1000 | Loss: 0.00003895
Iteration 71/1000 | Loss: 0.00003199
Iteration 72/1000 | Loss: 0.00009376
Iteration 73/1000 | Loss: 0.00014439
Iteration 74/1000 | Loss: 0.00003259
Iteration 75/1000 | Loss: 0.00002678
Iteration 76/1000 | Loss: 0.00004928
Iteration 77/1000 | Loss: 0.00008927
Iteration 78/1000 | Loss: 0.00004494
Iteration 79/1000 | Loss: 0.00008990
Iteration 80/1000 | Loss: 0.00038215
Iteration 81/1000 | Loss: 0.00002677
Iteration 82/1000 | Loss: 0.00003926
Iteration 83/1000 | Loss: 0.00010573
Iteration 84/1000 | Loss: 0.00003661
Iteration 85/1000 | Loss: 0.00002648
Iteration 86/1000 | Loss: 0.00002596
Iteration 87/1000 | Loss: 0.00002595
Iteration 88/1000 | Loss: 0.00002594
Iteration 89/1000 | Loss: 0.00002593
Iteration 90/1000 | Loss: 0.00002588
Iteration 91/1000 | Loss: 0.00002584
Iteration 92/1000 | Loss: 0.00002583
Iteration 93/1000 | Loss: 0.00002583
Iteration 94/1000 | Loss: 0.00002579
Iteration 95/1000 | Loss: 0.00010046
Iteration 96/1000 | Loss: 0.00003004
Iteration 97/1000 | Loss: 0.00004829
Iteration 98/1000 | Loss: 0.00002584
Iteration 99/1000 | Loss: 0.00002560
Iteration 100/1000 | Loss: 0.00002547
Iteration 101/1000 | Loss: 0.00002547
Iteration 102/1000 | Loss: 0.00002545
Iteration 103/1000 | Loss: 0.00002540
Iteration 104/1000 | Loss: 0.00002536
Iteration 105/1000 | Loss: 0.00002848
Iteration 106/1000 | Loss: 0.00002848
Iteration 107/1000 | Loss: 0.00014000
Iteration 108/1000 | Loss: 0.00077374
Iteration 109/1000 | Loss: 0.00002568
Iteration 110/1000 | Loss: 0.00003411
Iteration 111/1000 | Loss: 0.00002517
Iteration 112/1000 | Loss: 0.00002512
Iteration 113/1000 | Loss: 0.00002512
Iteration 114/1000 | Loss: 0.00002512
Iteration 115/1000 | Loss: 0.00002512
Iteration 116/1000 | Loss: 0.00002512
Iteration 117/1000 | Loss: 0.00002510
Iteration 118/1000 | Loss: 0.00013088
Iteration 119/1000 | Loss: 0.00005473
Iteration 120/1000 | Loss: 0.00003061
Iteration 121/1000 | Loss: 0.00003405
Iteration 122/1000 | Loss: 0.00002658
Iteration 123/1000 | Loss: 0.00002508
Iteration 124/1000 | Loss: 0.00002507
Iteration 125/1000 | Loss: 0.00002507
Iteration 126/1000 | Loss: 0.00002507
Iteration 127/1000 | Loss: 0.00002505
Iteration 128/1000 | Loss: 0.00002503
Iteration 129/1000 | Loss: 0.00002503
Iteration 130/1000 | Loss: 0.00002503
Iteration 131/1000 | Loss: 0.00002503
Iteration 132/1000 | Loss: 0.00002503
Iteration 133/1000 | Loss: 0.00002503
Iteration 134/1000 | Loss: 0.00002503
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002503
Iteration 137/1000 | Loss: 0.00002502
Iteration 138/1000 | Loss: 0.00002502
Iteration 139/1000 | Loss: 0.00002502
Iteration 140/1000 | Loss: 0.00002502
Iteration 141/1000 | Loss: 0.00002501
Iteration 142/1000 | Loss: 0.00002501
Iteration 143/1000 | Loss: 0.00002500
Iteration 144/1000 | Loss: 0.00002500
Iteration 145/1000 | Loss: 0.00002499
Iteration 146/1000 | Loss: 0.00002500
Iteration 147/1000 | Loss: 0.00002500
Iteration 148/1000 | Loss: 0.00002500
Iteration 149/1000 | Loss: 0.00002499
Iteration 150/1000 | Loss: 0.00002499
Iteration 151/1000 | Loss: 0.00002498
Iteration 152/1000 | Loss: 0.00002498
Iteration 153/1000 | Loss: 0.00002498
Iteration 154/1000 | Loss: 0.00002498
Iteration 155/1000 | Loss: 0.00002498
Iteration 156/1000 | Loss: 0.00002498
Iteration 157/1000 | Loss: 0.00002498
Iteration 158/1000 | Loss: 0.00002497
Iteration 159/1000 | Loss: 0.00002497
Iteration 160/1000 | Loss: 0.00002497
Iteration 161/1000 | Loss: 0.00002497
Iteration 162/1000 | Loss: 0.00002497
Iteration 163/1000 | Loss: 0.00002497
Iteration 164/1000 | Loss: 0.00002497
Iteration 165/1000 | Loss: 0.00002497
Iteration 166/1000 | Loss: 0.00002497
Iteration 167/1000 | Loss: 0.00002497
Iteration 168/1000 | Loss: 0.00002497
Iteration 169/1000 | Loss: 0.00002497
Iteration 170/1000 | Loss: 0.00002497
Iteration 171/1000 | Loss: 0.00002497
Iteration 172/1000 | Loss: 0.00002497
Iteration 173/1000 | Loss: 0.00002497
Iteration 174/1000 | Loss: 0.00002497
Iteration 175/1000 | Loss: 0.00002497
Iteration 176/1000 | Loss: 0.00002496
Iteration 177/1000 | Loss: 0.00002496
Iteration 178/1000 | Loss: 0.00002496
Iteration 179/1000 | Loss: 0.00002496
Iteration 180/1000 | Loss: 0.00002496
Iteration 181/1000 | Loss: 0.00002496
Iteration 182/1000 | Loss: 0.00002505
Iteration 183/1000 | Loss: 0.00002496
Iteration 184/1000 | Loss: 0.00002494
Iteration 185/1000 | Loss: 0.00002494
Iteration 186/1000 | Loss: 0.00002494
Iteration 187/1000 | Loss: 0.00002494
Iteration 188/1000 | Loss: 0.00002494
Iteration 189/1000 | Loss: 0.00002494
Iteration 190/1000 | Loss: 0.00002494
Iteration 191/1000 | Loss: 0.00002494
Iteration 192/1000 | Loss: 0.00002494
Iteration 193/1000 | Loss: 0.00002494
Iteration 194/1000 | Loss: 0.00002494
Iteration 195/1000 | Loss: 0.00002494
Iteration 196/1000 | Loss: 0.00002494
Iteration 197/1000 | Loss: 0.00002494
Iteration 198/1000 | Loss: 0.00002494
Iteration 199/1000 | Loss: 0.00002493
Iteration 200/1000 | Loss: 0.00002493
Iteration 201/1000 | Loss: 0.00002493
Iteration 202/1000 | Loss: 0.00002493
Iteration 203/1000 | Loss: 0.00002493
Iteration 204/1000 | Loss: 0.00002493
Iteration 205/1000 | Loss: 0.00002493
Iteration 206/1000 | Loss: 0.00002493
Iteration 207/1000 | Loss: 0.00002493
Iteration 208/1000 | Loss: 0.00002493
Iteration 209/1000 | Loss: 0.00002493
Iteration 210/1000 | Loss: 0.00002493
Iteration 211/1000 | Loss: 0.00002493
Iteration 212/1000 | Loss: 0.00002493
Iteration 213/1000 | Loss: 0.00002493
Iteration 214/1000 | Loss: 0.00002493
Iteration 215/1000 | Loss: 0.00002493
Iteration 216/1000 | Loss: 0.00002493
Iteration 217/1000 | Loss: 0.00002493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.4933171516750008e-05, 2.4933171516750008e-05, 2.4933171516750008e-05, 2.4933171516750008e-05, 2.4933171516750008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4933171516750008e-05

Optimization complete. Final v2v error: 3.266873836517334 mm

Highest mean error: 20.42942237854004 mm for frame 54

Lowest mean error: 2.3832106590270996 mm for frame 116

Saving results

Total time: 202.69156455993652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061849
Iteration 2/25 | Loss: 0.00378303
Iteration 3/25 | Loss: 0.00256513
Iteration 4/25 | Loss: 0.00226913
Iteration 5/25 | Loss: 0.00206697
Iteration 6/25 | Loss: 0.00184751
Iteration 7/25 | Loss: 0.00170206
Iteration 8/25 | Loss: 0.00157106
Iteration 9/25 | Loss: 0.00153590
Iteration 10/25 | Loss: 0.00150631
Iteration 11/25 | Loss: 0.00150986
Iteration 12/25 | Loss: 0.00147944
Iteration 13/25 | Loss: 0.00146685
Iteration 14/25 | Loss: 0.00145836
Iteration 15/25 | Loss: 0.00146817
Iteration 16/25 | Loss: 0.00144737
Iteration 17/25 | Loss: 0.00144940
Iteration 18/25 | Loss: 0.00144528
Iteration 19/25 | Loss: 0.00143848
Iteration 20/25 | Loss: 0.00143640
Iteration 21/25 | Loss: 0.00144235
Iteration 22/25 | Loss: 0.00143959
Iteration 23/25 | Loss: 0.00143729
Iteration 24/25 | Loss: 0.00143410
Iteration 25/25 | Loss: 0.00143340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32837272
Iteration 2/25 | Loss: 0.00375210
Iteration 3/25 | Loss: 0.00357903
Iteration 4/25 | Loss: 0.00357903
Iteration 5/25 | Loss: 0.00357903
Iteration 6/25 | Loss: 0.00357903
Iteration 7/25 | Loss: 0.00357903
Iteration 8/25 | Loss: 0.00357903
Iteration 9/25 | Loss: 0.00357903
Iteration 10/25 | Loss: 0.00357903
Iteration 11/25 | Loss: 0.00357902
Iteration 12/25 | Loss: 0.00357903
Iteration 13/25 | Loss: 0.00357903
Iteration 14/25 | Loss: 0.00357903
Iteration 15/25 | Loss: 0.00357903
Iteration 16/25 | Loss: 0.00357903
Iteration 17/25 | Loss: 0.00357903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0035790251567959785, 0.0035790251567959785, 0.0035790251567959785, 0.0035790251567959785, 0.0035790251567959785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035790251567959785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00357903
Iteration 2/1000 | Loss: 0.00069022
Iteration 3/1000 | Loss: 0.00054468
Iteration 4/1000 | Loss: 0.00200191
Iteration 5/1000 | Loss: 0.00082628
Iteration 6/1000 | Loss: 0.00065912
Iteration 7/1000 | Loss: 0.00050138
Iteration 8/1000 | Loss: 0.00060307
Iteration 9/1000 | Loss: 0.00020405
Iteration 10/1000 | Loss: 0.00051902
Iteration 11/1000 | Loss: 0.00043375
Iteration 12/1000 | Loss: 0.00021478
Iteration 13/1000 | Loss: 0.00015923
Iteration 14/1000 | Loss: 0.00035839
Iteration 15/1000 | Loss: 0.00014685
Iteration 16/1000 | Loss: 0.00014131
Iteration 17/1000 | Loss: 0.00029480
Iteration 18/1000 | Loss: 0.00089936
Iteration 19/1000 | Loss: 0.00013690
Iteration 20/1000 | Loss: 0.00013477
Iteration 21/1000 | Loss: 0.00025394
Iteration 22/1000 | Loss: 0.00013147
Iteration 23/1000 | Loss: 0.00036992
Iteration 24/1000 | Loss: 0.00013943
Iteration 25/1000 | Loss: 0.00026727
Iteration 26/1000 | Loss: 0.00134868
Iteration 27/1000 | Loss: 0.00489248
Iteration 28/1000 | Loss: 0.00186424
Iteration 29/1000 | Loss: 0.00102448
Iteration 30/1000 | Loss: 0.00037317
Iteration 31/1000 | Loss: 0.00062614
Iteration 32/1000 | Loss: 0.00045306
Iteration 33/1000 | Loss: 0.00129736
Iteration 34/1000 | Loss: 0.00018908
Iteration 35/1000 | Loss: 0.00025012
Iteration 36/1000 | Loss: 0.00023397
Iteration 37/1000 | Loss: 0.00122908
Iteration 38/1000 | Loss: 0.00010883
Iteration 39/1000 | Loss: 0.00008462
Iteration 40/1000 | Loss: 0.00049993
Iteration 41/1000 | Loss: 0.00054705
Iteration 42/1000 | Loss: 0.00017192
Iteration 43/1000 | Loss: 0.00019521
Iteration 44/1000 | Loss: 0.00050175
Iteration 45/1000 | Loss: 0.00022935
Iteration 46/1000 | Loss: 0.00006746
Iteration 47/1000 | Loss: 0.00071261
Iteration 48/1000 | Loss: 0.00077925
Iteration 49/1000 | Loss: 0.00009338
Iteration 50/1000 | Loss: 0.00008214
Iteration 51/1000 | Loss: 0.00005817
Iteration 52/1000 | Loss: 0.00007049
Iteration 53/1000 | Loss: 0.00005993
Iteration 54/1000 | Loss: 0.00005841
Iteration 55/1000 | Loss: 0.00021243
Iteration 56/1000 | Loss: 0.00082208
Iteration 57/1000 | Loss: 0.00009162
Iteration 58/1000 | Loss: 0.00013111
Iteration 59/1000 | Loss: 0.00005223
Iteration 60/1000 | Loss: 0.00005154
Iteration 61/1000 | Loss: 0.00014865
Iteration 62/1000 | Loss: 0.00009733
Iteration 63/1000 | Loss: 0.00007377
Iteration 64/1000 | Loss: 0.00005058
Iteration 65/1000 | Loss: 0.00005023
Iteration 66/1000 | Loss: 0.00008085
Iteration 67/1000 | Loss: 0.00004977
Iteration 68/1000 | Loss: 0.00004972
Iteration 69/1000 | Loss: 0.00004964
Iteration 70/1000 | Loss: 0.00004954
Iteration 71/1000 | Loss: 0.00004953
Iteration 72/1000 | Loss: 0.00007675
Iteration 73/1000 | Loss: 0.00004931
Iteration 74/1000 | Loss: 0.00004913
Iteration 75/1000 | Loss: 0.00007211
Iteration 76/1000 | Loss: 0.00005550
Iteration 77/1000 | Loss: 0.00006000
Iteration 78/1000 | Loss: 0.00004887
Iteration 79/1000 | Loss: 0.00004885
Iteration 80/1000 | Loss: 0.00004885
Iteration 81/1000 | Loss: 0.00004885
Iteration 82/1000 | Loss: 0.00004885
Iteration 83/1000 | Loss: 0.00004885
Iteration 84/1000 | Loss: 0.00004885
Iteration 85/1000 | Loss: 0.00004885
Iteration 86/1000 | Loss: 0.00004885
Iteration 87/1000 | Loss: 0.00004884
Iteration 88/1000 | Loss: 0.00004884
Iteration 89/1000 | Loss: 0.00004880
Iteration 90/1000 | Loss: 0.00004879
Iteration 91/1000 | Loss: 0.00017566
Iteration 92/1000 | Loss: 0.00004905
Iteration 93/1000 | Loss: 0.00004878
Iteration 94/1000 | Loss: 0.00004872
Iteration 95/1000 | Loss: 0.00004872
Iteration 96/1000 | Loss: 0.00004872
Iteration 97/1000 | Loss: 0.00004871
Iteration 98/1000 | Loss: 0.00004870
Iteration 99/1000 | Loss: 0.00004870
Iteration 100/1000 | Loss: 0.00004869
Iteration 101/1000 | Loss: 0.00004869
Iteration 102/1000 | Loss: 0.00004869
Iteration 103/1000 | Loss: 0.00004869
Iteration 104/1000 | Loss: 0.00004869
Iteration 105/1000 | Loss: 0.00004868
Iteration 106/1000 | Loss: 0.00004868
Iteration 107/1000 | Loss: 0.00004868
Iteration 108/1000 | Loss: 0.00004868
Iteration 109/1000 | Loss: 0.00004868
Iteration 110/1000 | Loss: 0.00004867
Iteration 111/1000 | Loss: 0.00004867
Iteration 112/1000 | Loss: 0.00004867
Iteration 113/1000 | Loss: 0.00004866
Iteration 114/1000 | Loss: 0.00004866
Iteration 115/1000 | Loss: 0.00004866
Iteration 116/1000 | Loss: 0.00004866
Iteration 117/1000 | Loss: 0.00004866
Iteration 118/1000 | Loss: 0.00004866
Iteration 119/1000 | Loss: 0.00004866
Iteration 120/1000 | Loss: 0.00004866
Iteration 121/1000 | Loss: 0.00004865
Iteration 122/1000 | Loss: 0.00004865
Iteration 123/1000 | Loss: 0.00004865
Iteration 124/1000 | Loss: 0.00004865
Iteration 125/1000 | Loss: 0.00004865
Iteration 126/1000 | Loss: 0.00004865
Iteration 127/1000 | Loss: 0.00004865
Iteration 128/1000 | Loss: 0.00004865
Iteration 129/1000 | Loss: 0.00004865
Iteration 130/1000 | Loss: 0.00004865
Iteration 131/1000 | Loss: 0.00004864
Iteration 132/1000 | Loss: 0.00004864
Iteration 133/1000 | Loss: 0.00004864
Iteration 134/1000 | Loss: 0.00004864
Iteration 135/1000 | Loss: 0.00004864
Iteration 136/1000 | Loss: 0.00004864
Iteration 137/1000 | Loss: 0.00004864
Iteration 138/1000 | Loss: 0.00004864
Iteration 139/1000 | Loss: 0.00004864
Iteration 140/1000 | Loss: 0.00004864
Iteration 141/1000 | Loss: 0.00004864
Iteration 142/1000 | Loss: 0.00004864
Iteration 143/1000 | Loss: 0.00004864
Iteration 144/1000 | Loss: 0.00004864
Iteration 145/1000 | Loss: 0.00004864
Iteration 146/1000 | Loss: 0.00004864
Iteration 147/1000 | Loss: 0.00004864
Iteration 148/1000 | Loss: 0.00004864
Iteration 149/1000 | Loss: 0.00004864
Iteration 150/1000 | Loss: 0.00004864
Iteration 151/1000 | Loss: 0.00004864
Iteration 152/1000 | Loss: 0.00004864
Iteration 153/1000 | Loss: 0.00004864
Iteration 154/1000 | Loss: 0.00004864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [4.863932917942293e-05, 4.863932917942293e-05, 4.863932917942293e-05, 4.863932917942293e-05, 4.863932917942293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.863932917942293e-05

Optimization complete. Final v2v error: 4.194011211395264 mm

Highest mean error: 12.210795402526855 mm for frame 123

Lowest mean error: 3.2360212802886963 mm for frame 115

Saving results

Total time: 200.69068503379822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805960
Iteration 2/25 | Loss: 0.00170126
Iteration 3/25 | Loss: 0.00138529
Iteration 4/25 | Loss: 0.00132764
Iteration 5/25 | Loss: 0.00134422
Iteration 6/25 | Loss: 0.00135721
Iteration 7/25 | Loss: 0.00129767
Iteration 8/25 | Loss: 0.00126840
Iteration 9/25 | Loss: 0.00126219
Iteration 10/25 | Loss: 0.00125789
Iteration 11/25 | Loss: 0.00125611
Iteration 12/25 | Loss: 0.00125722
Iteration 13/25 | Loss: 0.00125642
Iteration 14/25 | Loss: 0.00125454
Iteration 15/25 | Loss: 0.00125336
Iteration 16/25 | Loss: 0.00125291
Iteration 17/25 | Loss: 0.00125355
Iteration 18/25 | Loss: 0.00125266
Iteration 19/25 | Loss: 0.00125181
Iteration 20/25 | Loss: 0.00125024
Iteration 21/25 | Loss: 0.00124938
Iteration 22/25 | Loss: 0.00124920
Iteration 23/25 | Loss: 0.00124919
Iteration 24/25 | Loss: 0.00124918
Iteration 25/25 | Loss: 0.00124918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17399991
Iteration 2/25 | Loss: 0.00191661
Iteration 3/25 | Loss: 0.00191660
Iteration 4/25 | Loss: 0.00191660
Iteration 5/25 | Loss: 0.00191660
Iteration 6/25 | Loss: 0.00191660
Iteration 7/25 | Loss: 0.00191660
Iteration 8/25 | Loss: 0.00191659
Iteration 9/25 | Loss: 0.00191659
Iteration 10/25 | Loss: 0.00191659
Iteration 11/25 | Loss: 0.00191659
Iteration 12/25 | Loss: 0.00191659
Iteration 13/25 | Loss: 0.00191659
Iteration 14/25 | Loss: 0.00191659
Iteration 15/25 | Loss: 0.00191659
Iteration 16/25 | Loss: 0.00191659
Iteration 17/25 | Loss: 0.00191659
Iteration 18/25 | Loss: 0.00191659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019165939884260297, 0.0019165939884260297, 0.0019165939884260297, 0.0019165939884260297, 0.0019165939884260297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019165939884260297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191659
Iteration 2/1000 | Loss: 0.00008470
Iteration 3/1000 | Loss: 0.00005515
Iteration 4/1000 | Loss: 0.00004799
Iteration 5/1000 | Loss: 0.00004397
Iteration 6/1000 | Loss: 0.00004218
Iteration 7/1000 | Loss: 0.00004086
Iteration 8/1000 | Loss: 0.00004010
Iteration 9/1000 | Loss: 0.00003950
Iteration 10/1000 | Loss: 0.00003897
Iteration 11/1000 | Loss: 0.00003846
Iteration 12/1000 | Loss: 0.00003802
Iteration 13/1000 | Loss: 0.00003763
Iteration 14/1000 | Loss: 0.00003722
Iteration 15/1000 | Loss: 0.00003685
Iteration 16/1000 | Loss: 0.00031660
Iteration 17/1000 | Loss: 0.00081362
Iteration 18/1000 | Loss: 0.00033776
Iteration 19/1000 | Loss: 0.00013379
Iteration 20/1000 | Loss: 0.00005682
Iteration 21/1000 | Loss: 0.00003610
Iteration 22/1000 | Loss: 0.00003298
Iteration 23/1000 | Loss: 0.00003146
Iteration 24/1000 | Loss: 0.00003038
Iteration 25/1000 | Loss: 0.00002947
Iteration 26/1000 | Loss: 0.00002874
Iteration 27/1000 | Loss: 0.00002838
Iteration 28/1000 | Loss: 0.00002815
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00002789
Iteration 31/1000 | Loss: 0.00002783
Iteration 32/1000 | Loss: 0.00002770
Iteration 33/1000 | Loss: 0.00002767
Iteration 34/1000 | Loss: 0.00002766
Iteration 35/1000 | Loss: 0.00002766
Iteration 36/1000 | Loss: 0.00002765
Iteration 37/1000 | Loss: 0.00002764
Iteration 38/1000 | Loss: 0.00035089
Iteration 39/1000 | Loss: 0.00004088
Iteration 40/1000 | Loss: 0.00002893
Iteration 41/1000 | Loss: 0.00002720
Iteration 42/1000 | Loss: 0.00002664
Iteration 43/1000 | Loss: 0.00002631
Iteration 44/1000 | Loss: 0.00002625
Iteration 45/1000 | Loss: 0.00002619
Iteration 46/1000 | Loss: 0.00002618
Iteration 47/1000 | Loss: 0.00002617
Iteration 48/1000 | Loss: 0.00002612
Iteration 49/1000 | Loss: 0.00002593
Iteration 50/1000 | Loss: 0.00002590
Iteration 51/1000 | Loss: 0.00002589
Iteration 52/1000 | Loss: 0.00002587
Iteration 53/1000 | Loss: 0.00002586
Iteration 54/1000 | Loss: 0.00002586
Iteration 55/1000 | Loss: 0.00002585
Iteration 56/1000 | Loss: 0.00002585
Iteration 57/1000 | Loss: 0.00002584
Iteration 58/1000 | Loss: 0.00002583
Iteration 59/1000 | Loss: 0.00002583
Iteration 60/1000 | Loss: 0.00002583
Iteration 61/1000 | Loss: 0.00002582
Iteration 62/1000 | Loss: 0.00002582
Iteration 63/1000 | Loss: 0.00002581
Iteration 64/1000 | Loss: 0.00002581
Iteration 65/1000 | Loss: 0.00002581
Iteration 66/1000 | Loss: 0.00002581
Iteration 67/1000 | Loss: 0.00002581
Iteration 68/1000 | Loss: 0.00002580
Iteration 69/1000 | Loss: 0.00002580
Iteration 70/1000 | Loss: 0.00002579
Iteration 71/1000 | Loss: 0.00002579
Iteration 72/1000 | Loss: 0.00002579
Iteration 73/1000 | Loss: 0.00002578
Iteration 74/1000 | Loss: 0.00002577
Iteration 75/1000 | Loss: 0.00002577
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002574
Iteration 79/1000 | Loss: 0.00002573
Iteration 80/1000 | Loss: 0.00002573
Iteration 81/1000 | Loss: 0.00002573
Iteration 82/1000 | Loss: 0.00002572
Iteration 83/1000 | Loss: 0.00002572
Iteration 84/1000 | Loss: 0.00002571
Iteration 85/1000 | Loss: 0.00002571
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002570
Iteration 88/1000 | Loss: 0.00002570
Iteration 89/1000 | Loss: 0.00002570
Iteration 90/1000 | Loss: 0.00002570
Iteration 91/1000 | Loss: 0.00002569
Iteration 92/1000 | Loss: 0.00002569
Iteration 93/1000 | Loss: 0.00002569
Iteration 94/1000 | Loss: 0.00002569
Iteration 95/1000 | Loss: 0.00002568
Iteration 96/1000 | Loss: 0.00002568
Iteration 97/1000 | Loss: 0.00002568
Iteration 98/1000 | Loss: 0.00002568
Iteration 99/1000 | Loss: 0.00002568
Iteration 100/1000 | Loss: 0.00002567
Iteration 101/1000 | Loss: 0.00002567
Iteration 102/1000 | Loss: 0.00002567
Iteration 103/1000 | Loss: 0.00002567
Iteration 104/1000 | Loss: 0.00002567
Iteration 105/1000 | Loss: 0.00002566
Iteration 106/1000 | Loss: 0.00002566
Iteration 107/1000 | Loss: 0.00002566
Iteration 108/1000 | Loss: 0.00002566
Iteration 109/1000 | Loss: 0.00002565
Iteration 110/1000 | Loss: 0.00002565
Iteration 111/1000 | Loss: 0.00002565
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002565
Iteration 114/1000 | Loss: 0.00002565
Iteration 115/1000 | Loss: 0.00002565
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002565
Iteration 119/1000 | Loss: 0.00002565
Iteration 120/1000 | Loss: 0.00002565
Iteration 121/1000 | Loss: 0.00002565
Iteration 122/1000 | Loss: 0.00002565
Iteration 123/1000 | Loss: 0.00002564
Iteration 124/1000 | Loss: 0.00002564
Iteration 125/1000 | Loss: 0.00002564
Iteration 126/1000 | Loss: 0.00002564
Iteration 127/1000 | Loss: 0.00002564
Iteration 128/1000 | Loss: 0.00002564
Iteration 129/1000 | Loss: 0.00002564
Iteration 130/1000 | Loss: 0.00002564
Iteration 131/1000 | Loss: 0.00002564
Iteration 132/1000 | Loss: 0.00002564
Iteration 133/1000 | Loss: 0.00002564
Iteration 134/1000 | Loss: 0.00002564
Iteration 135/1000 | Loss: 0.00002564
Iteration 136/1000 | Loss: 0.00002564
Iteration 137/1000 | Loss: 0.00002564
Iteration 138/1000 | Loss: 0.00002564
Iteration 139/1000 | Loss: 0.00002564
Iteration 140/1000 | Loss: 0.00002564
Iteration 141/1000 | Loss: 0.00002564
Iteration 142/1000 | Loss: 0.00002564
Iteration 143/1000 | Loss: 0.00002564
Iteration 144/1000 | Loss: 0.00002564
Iteration 145/1000 | Loss: 0.00002564
Iteration 146/1000 | Loss: 0.00002564
Iteration 147/1000 | Loss: 0.00002564
Iteration 148/1000 | Loss: 0.00002564
Iteration 149/1000 | Loss: 0.00002564
Iteration 150/1000 | Loss: 0.00002564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.5641475076554343e-05, 2.5641475076554343e-05, 2.5641475076554343e-05, 2.5641475076554343e-05, 2.5641475076554343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5641475076554343e-05

Optimization complete. Final v2v error: 3.885969400405884 mm

Highest mean error: 11.356988906860352 mm for frame 20

Lowest mean error: 2.9907445907592773 mm for frame 208

Saving results

Total time: 125.67172288894653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803136
Iteration 2/25 | Loss: 0.00126585
Iteration 3/25 | Loss: 0.00113222
Iteration 4/25 | Loss: 0.00111570
Iteration 5/25 | Loss: 0.00111210
Iteration 6/25 | Loss: 0.00111172
Iteration 7/25 | Loss: 0.00111172
Iteration 8/25 | Loss: 0.00111172
Iteration 9/25 | Loss: 0.00111172
Iteration 10/25 | Loss: 0.00111172
Iteration 11/25 | Loss: 0.00111172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001111717545427382, 0.001111717545427382, 0.001111717545427382, 0.001111717545427382, 0.001111717545427382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001111717545427382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50457430
Iteration 2/25 | Loss: 0.00113847
Iteration 3/25 | Loss: 0.00113844
Iteration 4/25 | Loss: 0.00113844
Iteration 5/25 | Loss: 0.00113844
Iteration 6/25 | Loss: 0.00113844
Iteration 7/25 | Loss: 0.00113844
Iteration 8/25 | Loss: 0.00113844
Iteration 9/25 | Loss: 0.00113844
Iteration 10/25 | Loss: 0.00113844
Iteration 11/25 | Loss: 0.00113844
Iteration 12/25 | Loss: 0.00113844
Iteration 13/25 | Loss: 0.00113844
Iteration 14/25 | Loss: 0.00113844
Iteration 15/25 | Loss: 0.00113844
Iteration 16/25 | Loss: 0.00113844
Iteration 17/25 | Loss: 0.00113844
Iteration 18/25 | Loss: 0.00113844
Iteration 19/25 | Loss: 0.00113844
Iteration 20/25 | Loss: 0.00113844
Iteration 21/25 | Loss: 0.00113844
Iteration 22/25 | Loss: 0.00113844
Iteration 23/25 | Loss: 0.00113844
Iteration 24/25 | Loss: 0.00113844
Iteration 25/25 | Loss: 0.00113844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113844
Iteration 2/1000 | Loss: 0.00005276
Iteration 3/1000 | Loss: 0.00002860
Iteration 4/1000 | Loss: 0.00002199
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001769
Iteration 8/1000 | Loss: 0.00001708
Iteration 9/1000 | Loss: 0.00001664
Iteration 10/1000 | Loss: 0.00001641
Iteration 11/1000 | Loss: 0.00001623
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001610
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001609
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001606
Iteration 22/1000 | Loss: 0.00001605
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00001604
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001601
Iteration 30/1000 | Loss: 0.00001600
Iteration 31/1000 | Loss: 0.00001600
Iteration 32/1000 | Loss: 0.00001600
Iteration 33/1000 | Loss: 0.00001600
Iteration 34/1000 | Loss: 0.00001599
Iteration 35/1000 | Loss: 0.00001599
Iteration 36/1000 | Loss: 0.00001599
Iteration 37/1000 | Loss: 0.00001598
Iteration 38/1000 | Loss: 0.00001598
Iteration 39/1000 | Loss: 0.00001597
Iteration 40/1000 | Loss: 0.00001597
Iteration 41/1000 | Loss: 0.00001597
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001596
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001588
Iteration 55/1000 | Loss: 0.00001588
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001587
Iteration 59/1000 | Loss: 0.00001587
Iteration 60/1000 | Loss: 0.00001586
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001585
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001582
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.582083132234402e-05, 1.582083132234402e-05, 1.582083132234402e-05, 1.582083132234402e-05, 1.582083132234402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.582083132234402e-05

Optimization complete. Final v2v error: 3.401097536087036 mm

Highest mean error: 3.74456787109375 mm for frame 146

Lowest mean error: 3.1076512336730957 mm for frame 1

Saving results

Total time: 34.7504403591156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885222
Iteration 2/25 | Loss: 0.00246630
Iteration 3/25 | Loss: 0.00338653
Iteration 4/25 | Loss: 0.00162034
Iteration 5/25 | Loss: 0.00127198
Iteration 6/25 | Loss: 0.00119930
Iteration 7/25 | Loss: 0.00118998
Iteration 8/25 | Loss: 0.00118609
Iteration 9/25 | Loss: 0.00118099
Iteration 10/25 | Loss: 0.00117636
Iteration 11/25 | Loss: 0.00119780
Iteration 12/25 | Loss: 0.00117041
Iteration 13/25 | Loss: 0.00116666
Iteration 14/25 | Loss: 0.00116613
Iteration 15/25 | Loss: 0.00116600
Iteration 16/25 | Loss: 0.00116592
Iteration 17/25 | Loss: 0.00116591
Iteration 18/25 | Loss: 0.00116591
Iteration 19/25 | Loss: 0.00116591
Iteration 20/25 | Loss: 0.00116591
Iteration 21/25 | Loss: 0.00116591
Iteration 22/25 | Loss: 0.00116591
Iteration 23/25 | Loss: 0.00116591
Iteration 24/25 | Loss: 0.00116591
Iteration 25/25 | Loss: 0.00116590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20980835
Iteration 2/25 | Loss: 0.00159679
Iteration 3/25 | Loss: 0.00159679
Iteration 4/25 | Loss: 0.00159679
Iteration 5/25 | Loss: 0.00159679
Iteration 6/25 | Loss: 0.00159679
Iteration 7/25 | Loss: 0.00159679
Iteration 8/25 | Loss: 0.00159679
Iteration 9/25 | Loss: 0.00159679
Iteration 10/25 | Loss: 0.00159679
Iteration 11/25 | Loss: 0.00159679
Iteration 12/25 | Loss: 0.00159679
Iteration 13/25 | Loss: 0.00159679
Iteration 14/25 | Loss: 0.00159679
Iteration 15/25 | Loss: 0.00159679
Iteration 16/25 | Loss: 0.00159679
Iteration 17/25 | Loss: 0.00159679
Iteration 18/25 | Loss: 0.00159679
Iteration 19/25 | Loss: 0.00159679
Iteration 20/25 | Loss: 0.00159679
Iteration 21/25 | Loss: 0.00159679
Iteration 22/25 | Loss: 0.00159679
Iteration 23/25 | Loss: 0.00159679
Iteration 24/25 | Loss: 0.00159679
Iteration 25/25 | Loss: 0.00159679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159679
Iteration 2/1000 | Loss: 0.00006865
Iteration 3/1000 | Loss: 0.00004029
Iteration 4/1000 | Loss: 0.00003269
Iteration 5/1000 | Loss: 0.00002937
Iteration 6/1000 | Loss: 0.00002710
Iteration 7/1000 | Loss: 0.00002552
Iteration 8/1000 | Loss: 0.00014362
Iteration 9/1000 | Loss: 0.00014511
Iteration 10/1000 | Loss: 0.00003166
Iteration 11/1000 | Loss: 0.00002524
Iteration 12/1000 | Loss: 0.00002132
Iteration 13/1000 | Loss: 0.00001884
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001530
Iteration 18/1000 | Loss: 0.00001489
Iteration 19/1000 | Loss: 0.00001459
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001397
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001389
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001382
Iteration 31/1000 | Loss: 0.00001382
Iteration 32/1000 | Loss: 0.00001381
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001378
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001377
Iteration 42/1000 | Loss: 0.00001376
Iteration 43/1000 | Loss: 0.00001376
Iteration 44/1000 | Loss: 0.00001375
Iteration 45/1000 | Loss: 0.00001375
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001374
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001369
Iteration 108/1000 | Loss: 0.00001369
Iteration 109/1000 | Loss: 0.00001369
Iteration 110/1000 | Loss: 0.00001369
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001369
Iteration 120/1000 | Loss: 0.00001369
Iteration 121/1000 | Loss: 0.00001369
Iteration 122/1000 | Loss: 0.00001369
Iteration 123/1000 | Loss: 0.00001369
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001369
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.3691545063920785e-05, 1.3691545063920785e-05, 1.3691545063920785e-05, 1.3691545063920785e-05, 1.3691545063920785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3691545063920785e-05

Optimization complete. Final v2v error: 3.1612040996551514 mm

Highest mean error: 3.652101755142212 mm for frame 97

Lowest mean error: 2.852222442626953 mm for frame 11

Saving results

Total time: 76.47436380386353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832886
Iteration 2/25 | Loss: 0.00143496
Iteration 3/25 | Loss: 0.00126098
Iteration 4/25 | Loss: 0.00122271
Iteration 5/25 | Loss: 0.00119957
Iteration 6/25 | Loss: 0.00120420
Iteration 7/25 | Loss: 0.00118932
Iteration 8/25 | Loss: 0.00118657
Iteration 9/25 | Loss: 0.00118562
Iteration 10/25 | Loss: 0.00118532
Iteration 11/25 | Loss: 0.00118516
Iteration 12/25 | Loss: 0.00118509
Iteration 13/25 | Loss: 0.00118508
Iteration 14/25 | Loss: 0.00118507
Iteration 15/25 | Loss: 0.00118507
Iteration 16/25 | Loss: 0.00118507
Iteration 17/25 | Loss: 0.00118506
Iteration 18/25 | Loss: 0.00118506
Iteration 19/25 | Loss: 0.00118506
Iteration 20/25 | Loss: 0.00118506
Iteration 21/25 | Loss: 0.00118506
Iteration 22/25 | Loss: 0.00118506
Iteration 23/25 | Loss: 0.00118506
Iteration 24/25 | Loss: 0.00118506
Iteration 25/25 | Loss: 0.00118506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43231940
Iteration 2/25 | Loss: 0.00110685
Iteration 3/25 | Loss: 0.00110685
Iteration 4/25 | Loss: 0.00110685
Iteration 5/25 | Loss: 0.00110685
Iteration 6/25 | Loss: 0.00110685
Iteration 7/25 | Loss: 0.00110685
Iteration 8/25 | Loss: 0.00110685
Iteration 9/25 | Loss: 0.00110685
Iteration 10/25 | Loss: 0.00110685
Iteration 11/25 | Loss: 0.00110685
Iteration 12/25 | Loss: 0.00110684
Iteration 13/25 | Loss: 0.00110684
Iteration 14/25 | Loss: 0.00110684
Iteration 15/25 | Loss: 0.00110684
Iteration 16/25 | Loss: 0.00110684
Iteration 17/25 | Loss: 0.00110684
Iteration 18/25 | Loss: 0.00110684
Iteration 19/25 | Loss: 0.00110684
Iteration 20/25 | Loss: 0.00110684
Iteration 21/25 | Loss: 0.00110684
Iteration 22/25 | Loss: 0.00110684
Iteration 23/25 | Loss: 0.00110684
Iteration 24/25 | Loss: 0.00110684
Iteration 25/25 | Loss: 0.00110684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110684
Iteration 2/1000 | Loss: 0.00004822
Iteration 3/1000 | Loss: 0.00003383
Iteration 4/1000 | Loss: 0.00002857
Iteration 5/1000 | Loss: 0.00002602
Iteration 6/1000 | Loss: 0.00002471
Iteration 7/1000 | Loss: 0.00002363
Iteration 8/1000 | Loss: 0.00023082
Iteration 9/1000 | Loss: 0.00002882
Iteration 10/1000 | Loss: 0.00002412
Iteration 11/1000 | Loss: 0.00002265
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002095
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002036
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002032
Iteration 20/1000 | Loss: 0.00002021
Iteration 21/1000 | Loss: 0.00002019
Iteration 22/1000 | Loss: 0.00002018
Iteration 23/1000 | Loss: 0.00002018
Iteration 24/1000 | Loss: 0.00002018
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002017
Iteration 30/1000 | Loss: 0.00002017
Iteration 31/1000 | Loss: 0.00002016
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002015
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002014
Iteration 42/1000 | Loss: 0.00002014
Iteration 43/1000 | Loss: 0.00002014
Iteration 44/1000 | Loss: 0.00002014
Iteration 45/1000 | Loss: 0.00002014
Iteration 46/1000 | Loss: 0.00002014
Iteration 47/1000 | Loss: 0.00002013
Iteration 48/1000 | Loss: 0.00002013
Iteration 49/1000 | Loss: 0.00002013
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002012
Iteration 52/1000 | Loss: 0.00002012
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002012
Iteration 55/1000 | Loss: 0.00002011
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002011
Iteration 58/1000 | Loss: 0.00002011
Iteration 59/1000 | Loss: 0.00002011
Iteration 60/1000 | Loss: 0.00002010
Iteration 61/1000 | Loss: 0.00002010
Iteration 62/1000 | Loss: 0.00002010
Iteration 63/1000 | Loss: 0.00002009
Iteration 64/1000 | Loss: 0.00002009
Iteration 65/1000 | Loss: 0.00002008
Iteration 66/1000 | Loss: 0.00002008
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002006
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002005
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002005
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002003
Iteration 104/1000 | Loss: 0.00002003
Iteration 105/1000 | Loss: 0.00002003
Iteration 106/1000 | Loss: 0.00002003
Iteration 107/1000 | Loss: 0.00002003
Iteration 108/1000 | Loss: 0.00002002
Iteration 109/1000 | Loss: 0.00002002
Iteration 110/1000 | Loss: 0.00002002
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002002
Iteration 114/1000 | Loss: 0.00002002
Iteration 115/1000 | Loss: 0.00002002
Iteration 116/1000 | Loss: 0.00002002
Iteration 117/1000 | Loss: 0.00002002
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002002
Iteration 124/1000 | Loss: 0.00002002
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Iteration 127/1000 | Loss: 0.00002002
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002002
Iteration 134/1000 | Loss: 0.00002002
Iteration 135/1000 | Loss: 0.00002002
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002002
Iteration 143/1000 | Loss: 0.00002002
Iteration 144/1000 | Loss: 0.00002002
Iteration 145/1000 | Loss: 0.00002002
Iteration 146/1000 | Loss: 0.00002002
Iteration 147/1000 | Loss: 0.00002002
Iteration 148/1000 | Loss: 0.00002002
Iteration 149/1000 | Loss: 0.00002002
Iteration 150/1000 | Loss: 0.00002002
Iteration 151/1000 | Loss: 0.00002002
Iteration 152/1000 | Loss: 0.00002002
Iteration 153/1000 | Loss: 0.00002002
Iteration 154/1000 | Loss: 0.00002002
Iteration 155/1000 | Loss: 0.00002002
Iteration 156/1000 | Loss: 0.00002002
Iteration 157/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.001670145546086e-05, 2.001670145546086e-05, 2.001670145546086e-05, 2.001670145546086e-05, 2.001670145546086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.001670145546086e-05

Optimization complete. Final v2v error: 3.830043315887451 mm

Highest mean error: 4.535472869873047 mm for frame 58

Lowest mean error: 3.5278208255767822 mm for frame 99

Saving results

Total time: 84.06817126274109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867334
Iteration 2/25 | Loss: 0.00147325
Iteration 3/25 | Loss: 0.00122258
Iteration 4/25 | Loss: 0.00119863
Iteration 5/25 | Loss: 0.00119767
Iteration 6/25 | Loss: 0.00119767
Iteration 7/25 | Loss: 0.00119767
Iteration 8/25 | Loss: 0.00119767
Iteration 9/25 | Loss: 0.00119767
Iteration 10/25 | Loss: 0.00119767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011976733803749084, 0.0011976733803749084, 0.0011976733803749084, 0.0011976733803749084, 0.0011976733803749084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011976733803749084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85250372
Iteration 2/25 | Loss: 0.00069751
Iteration 3/25 | Loss: 0.00069750
Iteration 4/25 | Loss: 0.00069750
Iteration 5/25 | Loss: 0.00069750
Iteration 6/25 | Loss: 0.00069750
Iteration 7/25 | Loss: 0.00069750
Iteration 8/25 | Loss: 0.00069750
Iteration 9/25 | Loss: 0.00069750
Iteration 10/25 | Loss: 0.00069750
Iteration 11/25 | Loss: 0.00069750
Iteration 12/25 | Loss: 0.00069750
Iteration 13/25 | Loss: 0.00069750
Iteration 14/25 | Loss: 0.00069750
Iteration 15/25 | Loss: 0.00069750
Iteration 16/25 | Loss: 0.00069750
Iteration 17/25 | Loss: 0.00069750
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006974967545829713, 0.0006974967545829713, 0.0006974967545829713, 0.0006974967545829713, 0.0006974967545829713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006974967545829713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069750
Iteration 2/1000 | Loss: 0.00004813
Iteration 3/1000 | Loss: 0.00003109
Iteration 4/1000 | Loss: 0.00002764
Iteration 5/1000 | Loss: 0.00002552
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002340
Iteration 8/1000 | Loss: 0.00002307
Iteration 9/1000 | Loss: 0.00002281
Iteration 10/1000 | Loss: 0.00002263
Iteration 11/1000 | Loss: 0.00002244
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002228
Iteration 14/1000 | Loss: 0.00002228
Iteration 15/1000 | Loss: 0.00002226
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002223
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002222
Iteration 23/1000 | Loss: 0.00002221
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002220
Iteration 27/1000 | Loss: 0.00002220
Iteration 28/1000 | Loss: 0.00002220
Iteration 29/1000 | Loss: 0.00002219
Iteration 30/1000 | Loss: 0.00002219
Iteration 31/1000 | Loss: 0.00002219
Iteration 32/1000 | Loss: 0.00002219
Iteration 33/1000 | Loss: 0.00002219
Iteration 34/1000 | Loss: 0.00002217
Iteration 35/1000 | Loss: 0.00002217
Iteration 36/1000 | Loss: 0.00002217
Iteration 37/1000 | Loss: 0.00002217
Iteration 38/1000 | Loss: 0.00002216
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002215
Iteration 42/1000 | Loss: 0.00002214
Iteration 43/1000 | Loss: 0.00002214
Iteration 44/1000 | Loss: 0.00002214
Iteration 45/1000 | Loss: 0.00002213
Iteration 46/1000 | Loss: 0.00002213
Iteration 47/1000 | Loss: 0.00002213
Iteration 48/1000 | Loss: 0.00002213
Iteration 49/1000 | Loss: 0.00002212
Iteration 50/1000 | Loss: 0.00002212
Iteration 51/1000 | Loss: 0.00002212
Iteration 52/1000 | Loss: 0.00002212
Iteration 53/1000 | Loss: 0.00002212
Iteration 54/1000 | Loss: 0.00002212
Iteration 55/1000 | Loss: 0.00002212
Iteration 56/1000 | Loss: 0.00002211
Iteration 57/1000 | Loss: 0.00002210
Iteration 58/1000 | Loss: 0.00002210
Iteration 59/1000 | Loss: 0.00002210
Iteration 60/1000 | Loss: 0.00002210
Iteration 61/1000 | Loss: 0.00002210
Iteration 62/1000 | Loss: 0.00002210
Iteration 63/1000 | Loss: 0.00002210
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002209
Iteration 66/1000 | Loss: 0.00002209
Iteration 67/1000 | Loss: 0.00002209
Iteration 68/1000 | Loss: 0.00002209
Iteration 69/1000 | Loss: 0.00002208
Iteration 70/1000 | Loss: 0.00002208
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002208
Iteration 73/1000 | Loss: 0.00002207
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002206
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002206
Iteration 81/1000 | Loss: 0.00002205
Iteration 82/1000 | Loss: 0.00002205
Iteration 83/1000 | Loss: 0.00002205
Iteration 84/1000 | Loss: 0.00002205
Iteration 85/1000 | Loss: 0.00002205
Iteration 86/1000 | Loss: 0.00002205
Iteration 87/1000 | Loss: 0.00002205
Iteration 88/1000 | Loss: 0.00002204
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002204
Iteration 91/1000 | Loss: 0.00002204
Iteration 92/1000 | Loss: 0.00002204
Iteration 93/1000 | Loss: 0.00002204
Iteration 94/1000 | Loss: 0.00002204
Iteration 95/1000 | Loss: 0.00002204
Iteration 96/1000 | Loss: 0.00002204
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002204
Iteration 99/1000 | Loss: 0.00002204
Iteration 100/1000 | Loss: 0.00002204
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002204
Iteration 103/1000 | Loss: 0.00002204
Iteration 104/1000 | Loss: 0.00002204
Iteration 105/1000 | Loss: 0.00002204
Iteration 106/1000 | Loss: 0.00002204
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002204
Iteration 110/1000 | Loss: 0.00002204
Iteration 111/1000 | Loss: 0.00002204
Iteration 112/1000 | Loss: 0.00002204
Iteration 113/1000 | Loss: 0.00002204
Iteration 114/1000 | Loss: 0.00002204
Iteration 115/1000 | Loss: 0.00002204
Iteration 116/1000 | Loss: 0.00002204
Iteration 117/1000 | Loss: 0.00002204
Iteration 118/1000 | Loss: 0.00002204
Iteration 119/1000 | Loss: 0.00002204
Iteration 120/1000 | Loss: 0.00002204
Iteration 121/1000 | Loss: 0.00002204
Iteration 122/1000 | Loss: 0.00002204
Iteration 123/1000 | Loss: 0.00002204
Iteration 124/1000 | Loss: 0.00002204
Iteration 125/1000 | Loss: 0.00002204
Iteration 126/1000 | Loss: 0.00002204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.203859003202524e-05, 2.203859003202524e-05, 2.203859003202524e-05, 2.203859003202524e-05, 2.203859003202524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.203859003202524e-05

Optimization complete. Final v2v error: 3.958860397338867 mm

Highest mean error: 4.077167987823486 mm for frame 108

Lowest mean error: 3.791632652282715 mm for frame 72

Saving results

Total time: 31.818042755126953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859456
Iteration 2/25 | Loss: 0.00122505
Iteration 3/25 | Loss: 0.00115142
Iteration 4/25 | Loss: 0.00112284
Iteration 5/25 | Loss: 0.00111624
Iteration 6/25 | Loss: 0.00111508
Iteration 7/25 | Loss: 0.00111508
Iteration 8/25 | Loss: 0.00111508
Iteration 9/25 | Loss: 0.00111508
Iteration 10/25 | Loss: 0.00111508
Iteration 11/25 | Loss: 0.00111508
Iteration 12/25 | Loss: 0.00111508
Iteration 13/25 | Loss: 0.00111508
Iteration 14/25 | Loss: 0.00111508
Iteration 15/25 | Loss: 0.00111508
Iteration 16/25 | Loss: 0.00111508
Iteration 17/25 | Loss: 0.00111508
Iteration 18/25 | Loss: 0.00111508
Iteration 19/25 | Loss: 0.00111508
Iteration 20/25 | Loss: 0.00111508
Iteration 21/25 | Loss: 0.00111508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011150772916153073, 0.0011150772916153073, 0.0011150772916153073, 0.0011150772916153073, 0.0011150772916153073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011150772916153073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.20541906
Iteration 2/25 | Loss: 0.00220021
Iteration 3/25 | Loss: 0.00220016
Iteration 4/25 | Loss: 0.00220016
Iteration 5/25 | Loss: 0.00220016
Iteration 6/25 | Loss: 0.00220016
Iteration 7/25 | Loss: 0.00220016
Iteration 8/25 | Loss: 0.00220016
Iteration 9/25 | Loss: 0.00220016
Iteration 10/25 | Loss: 0.00220016
Iteration 11/25 | Loss: 0.00220016
Iteration 12/25 | Loss: 0.00220016
Iteration 13/25 | Loss: 0.00220016
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002200161339715123, 0.002200161339715123, 0.002200161339715123, 0.002200161339715123, 0.002200161339715123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002200161339715123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220016
Iteration 2/1000 | Loss: 0.00006531
Iteration 3/1000 | Loss: 0.00002966
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002005
Iteration 6/1000 | Loss: 0.00001874
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001773
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001690
Iteration 11/1000 | Loss: 0.00001672
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001654
Iteration 14/1000 | Loss: 0.00001654
Iteration 15/1000 | Loss: 0.00001653
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001649
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001643
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001641
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001640
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001639
Iteration 37/1000 | Loss: 0.00001639
Iteration 38/1000 | Loss: 0.00001639
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001638
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001635
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00001632
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001631
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001630
Iteration 68/1000 | Loss: 0.00001629
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001628
Iteration 75/1000 | Loss: 0.00001628
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001627
Iteration 78/1000 | Loss: 0.00001627
Iteration 79/1000 | Loss: 0.00001627
Iteration 80/1000 | Loss: 0.00001627
Iteration 81/1000 | Loss: 0.00001627
Iteration 82/1000 | Loss: 0.00001626
Iteration 83/1000 | Loss: 0.00001626
Iteration 84/1000 | Loss: 0.00001626
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001625
Iteration 87/1000 | Loss: 0.00001625
Iteration 88/1000 | Loss: 0.00001625
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001618
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Iteration 140/1000 | Loss: 0.00001618
Iteration 141/1000 | Loss: 0.00001618
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001618
Iteration 144/1000 | Loss: 0.00001618
Iteration 145/1000 | Loss: 0.00001618
Iteration 146/1000 | Loss: 0.00001618
Iteration 147/1000 | Loss: 0.00001618
Iteration 148/1000 | Loss: 0.00001618
Iteration 149/1000 | Loss: 0.00001618
Iteration 150/1000 | Loss: 0.00001618
Iteration 151/1000 | Loss: 0.00001618
Iteration 152/1000 | Loss: 0.00001618
Iteration 153/1000 | Loss: 0.00001618
Iteration 154/1000 | Loss: 0.00001618
Iteration 155/1000 | Loss: 0.00001618
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.617787711438723e-05, 1.617787711438723e-05, 1.617787711438723e-05, 1.617787711438723e-05, 1.617787711438723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.617787711438723e-05

Optimization complete. Final v2v error: 3.3587443828582764 mm

Highest mean error: 3.801394462585449 mm for frame 84

Lowest mean error: 2.965951442718506 mm for frame 220

Saving results

Total time: 67.53577923774719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850283
Iteration 2/25 | Loss: 0.00135656
Iteration 3/25 | Loss: 0.00115882
Iteration 4/25 | Loss: 0.00114238
Iteration 5/25 | Loss: 0.00114101
Iteration 6/25 | Loss: 0.00114101
Iteration 7/25 | Loss: 0.00114101
Iteration 8/25 | Loss: 0.00114101
Iteration 9/25 | Loss: 0.00114101
Iteration 10/25 | Loss: 0.00114101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011410110164433718, 0.0011410110164433718, 0.0011410110164433718, 0.0011410110164433718, 0.0011410110164433718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011410110164433718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19913495
Iteration 2/25 | Loss: 0.00162550
Iteration 3/25 | Loss: 0.00162550
Iteration 4/25 | Loss: 0.00162550
Iteration 5/25 | Loss: 0.00162550
Iteration 6/25 | Loss: 0.00162550
Iteration 7/25 | Loss: 0.00162550
Iteration 8/25 | Loss: 0.00162550
Iteration 9/25 | Loss: 0.00162550
Iteration 10/25 | Loss: 0.00162550
Iteration 11/25 | Loss: 0.00162549
Iteration 12/25 | Loss: 0.00162549
Iteration 13/25 | Loss: 0.00162549
Iteration 14/25 | Loss: 0.00162549
Iteration 15/25 | Loss: 0.00162549
Iteration 16/25 | Loss: 0.00162549
Iteration 17/25 | Loss: 0.00162549
Iteration 18/25 | Loss: 0.00162549
Iteration 19/25 | Loss: 0.00162549
Iteration 20/25 | Loss: 0.00162549
Iteration 21/25 | Loss: 0.00162549
Iteration 22/25 | Loss: 0.00162549
Iteration 23/25 | Loss: 0.00162549
Iteration 24/25 | Loss: 0.00162549
Iteration 25/25 | Loss: 0.00162549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162549
Iteration 2/1000 | Loss: 0.00005504
Iteration 3/1000 | Loss: 0.00002811
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001412
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001355
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001348
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001324
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.3231527191237547e-05, 1.3231527191237547e-05, 1.3231527191237547e-05, 1.3231527191237547e-05, 1.3231527191237547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3231527191237547e-05

Optimization complete. Final v2v error: 3.1401734352111816 mm

Highest mean error: 3.4335527420043945 mm for frame 9

Lowest mean error: 2.944470167160034 mm for frame 88

Saving results

Total time: 49.4701783657074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940954
Iteration 2/25 | Loss: 0.00132170
Iteration 3/25 | Loss: 0.00119077
Iteration 4/25 | Loss: 0.00116720
Iteration 5/25 | Loss: 0.00115758
Iteration 6/25 | Loss: 0.00115523
Iteration 7/25 | Loss: 0.00115481
Iteration 8/25 | Loss: 0.00115481
Iteration 9/25 | Loss: 0.00115481
Iteration 10/25 | Loss: 0.00115481
Iteration 11/25 | Loss: 0.00115481
Iteration 12/25 | Loss: 0.00115481
Iteration 13/25 | Loss: 0.00115481
Iteration 14/25 | Loss: 0.00115481
Iteration 15/25 | Loss: 0.00115481
Iteration 16/25 | Loss: 0.00115481
Iteration 17/25 | Loss: 0.00115481
Iteration 18/25 | Loss: 0.00115481
Iteration 19/25 | Loss: 0.00115481
Iteration 20/25 | Loss: 0.00115481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011548111215233803, 0.0011548111215233803, 0.0011548111215233803, 0.0011548111215233803, 0.0011548111215233803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011548111215233803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28741634
Iteration 2/25 | Loss: 0.00146289
Iteration 3/25 | Loss: 0.00146288
Iteration 4/25 | Loss: 0.00146288
Iteration 5/25 | Loss: 0.00146288
Iteration 6/25 | Loss: 0.00146288
Iteration 7/25 | Loss: 0.00146288
Iteration 8/25 | Loss: 0.00146287
Iteration 9/25 | Loss: 0.00146287
Iteration 10/25 | Loss: 0.00146287
Iteration 11/25 | Loss: 0.00146287
Iteration 12/25 | Loss: 0.00146287
Iteration 13/25 | Loss: 0.00146287
Iteration 14/25 | Loss: 0.00146287
Iteration 15/25 | Loss: 0.00146287
Iteration 16/25 | Loss: 0.00146287
Iteration 17/25 | Loss: 0.00146287
Iteration 18/25 | Loss: 0.00146287
Iteration 19/25 | Loss: 0.00146287
Iteration 20/25 | Loss: 0.00146287
Iteration 21/25 | Loss: 0.00146287
Iteration 22/25 | Loss: 0.00146287
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00146287411917001, 0.00146287411917001, 0.00146287411917001, 0.00146287411917001, 0.00146287411917001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00146287411917001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146287
Iteration 2/1000 | Loss: 0.00007530
Iteration 3/1000 | Loss: 0.00004438
Iteration 4/1000 | Loss: 0.00003326
Iteration 5/1000 | Loss: 0.00002776
Iteration 6/1000 | Loss: 0.00002558
Iteration 7/1000 | Loss: 0.00002384
Iteration 8/1000 | Loss: 0.00002258
Iteration 9/1000 | Loss: 0.00002185
Iteration 10/1000 | Loss: 0.00002147
Iteration 11/1000 | Loss: 0.00002120
Iteration 12/1000 | Loss: 0.00002112
Iteration 13/1000 | Loss: 0.00002098
Iteration 14/1000 | Loss: 0.00002094
Iteration 15/1000 | Loss: 0.00002084
Iteration 16/1000 | Loss: 0.00002083
Iteration 17/1000 | Loss: 0.00002082
Iteration 18/1000 | Loss: 0.00002080
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002078
Iteration 21/1000 | Loss: 0.00002077
Iteration 22/1000 | Loss: 0.00002076
Iteration 23/1000 | Loss: 0.00002075
Iteration 24/1000 | Loss: 0.00002074
Iteration 25/1000 | Loss: 0.00002073
Iteration 26/1000 | Loss: 0.00002073
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002072
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00002071
Iteration 31/1000 | Loss: 0.00002071
Iteration 32/1000 | Loss: 0.00002071
Iteration 33/1000 | Loss: 0.00002071
Iteration 34/1000 | Loss: 0.00002071
Iteration 35/1000 | Loss: 0.00002071
Iteration 36/1000 | Loss: 0.00002071
Iteration 37/1000 | Loss: 0.00002070
Iteration 38/1000 | Loss: 0.00002070
Iteration 39/1000 | Loss: 0.00002070
Iteration 40/1000 | Loss: 0.00002070
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002068
Iteration 43/1000 | Loss: 0.00002068
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002068
Iteration 48/1000 | Loss: 0.00002068
Iteration 49/1000 | Loss: 0.00002067
Iteration 50/1000 | Loss: 0.00002067
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00002067
Iteration 53/1000 | Loss: 0.00002067
Iteration 54/1000 | Loss: 0.00002067
Iteration 55/1000 | Loss: 0.00002067
Iteration 56/1000 | Loss: 0.00002066
Iteration 57/1000 | Loss: 0.00002066
Iteration 58/1000 | Loss: 0.00002066
Iteration 59/1000 | Loss: 0.00002066
Iteration 60/1000 | Loss: 0.00002066
Iteration 61/1000 | Loss: 0.00002066
Iteration 62/1000 | Loss: 0.00002066
Iteration 63/1000 | Loss: 0.00002066
Iteration 64/1000 | Loss: 0.00002065
Iteration 65/1000 | Loss: 0.00002065
Iteration 66/1000 | Loss: 0.00002065
Iteration 67/1000 | Loss: 0.00002065
Iteration 68/1000 | Loss: 0.00002065
Iteration 69/1000 | Loss: 0.00002065
Iteration 70/1000 | Loss: 0.00002065
Iteration 71/1000 | Loss: 0.00002065
Iteration 72/1000 | Loss: 0.00002065
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002065
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002064
Iteration 79/1000 | Loss: 0.00002064
Iteration 80/1000 | Loss: 0.00002064
Iteration 81/1000 | Loss: 0.00002064
Iteration 82/1000 | Loss: 0.00002064
Iteration 83/1000 | Loss: 0.00002064
Iteration 84/1000 | Loss: 0.00002064
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002064
Iteration 87/1000 | Loss: 0.00002063
Iteration 88/1000 | Loss: 0.00002063
Iteration 89/1000 | Loss: 0.00002063
Iteration 90/1000 | Loss: 0.00002063
Iteration 91/1000 | Loss: 0.00002063
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002063
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002062
Iteration 116/1000 | Loss: 0.00002062
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002062
Iteration 120/1000 | Loss: 0.00002062
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002061
Iteration 123/1000 | Loss: 0.00002061
Iteration 124/1000 | Loss: 0.00002061
Iteration 125/1000 | Loss: 0.00002061
Iteration 126/1000 | Loss: 0.00002061
Iteration 127/1000 | Loss: 0.00002061
Iteration 128/1000 | Loss: 0.00002061
Iteration 129/1000 | Loss: 0.00002061
Iteration 130/1000 | Loss: 0.00002061
Iteration 131/1000 | Loss: 0.00002061
Iteration 132/1000 | Loss: 0.00002061
Iteration 133/1000 | Loss: 0.00002061
Iteration 134/1000 | Loss: 0.00002061
Iteration 135/1000 | Loss: 0.00002061
Iteration 136/1000 | Loss: 0.00002061
Iteration 137/1000 | Loss: 0.00002060
Iteration 138/1000 | Loss: 0.00002060
Iteration 139/1000 | Loss: 0.00002060
Iteration 140/1000 | Loss: 0.00002060
Iteration 141/1000 | Loss: 0.00002060
Iteration 142/1000 | Loss: 0.00002060
Iteration 143/1000 | Loss: 0.00002060
Iteration 144/1000 | Loss: 0.00002060
Iteration 145/1000 | Loss: 0.00002060
Iteration 146/1000 | Loss: 0.00002060
Iteration 147/1000 | Loss: 0.00002060
Iteration 148/1000 | Loss: 0.00002060
Iteration 149/1000 | Loss: 0.00002060
Iteration 150/1000 | Loss: 0.00002059
Iteration 151/1000 | Loss: 0.00002059
Iteration 152/1000 | Loss: 0.00002059
Iteration 153/1000 | Loss: 0.00002059
Iteration 154/1000 | Loss: 0.00002059
Iteration 155/1000 | Loss: 0.00002059
Iteration 156/1000 | Loss: 0.00002059
Iteration 157/1000 | Loss: 0.00002059
Iteration 158/1000 | Loss: 0.00002059
Iteration 159/1000 | Loss: 0.00002059
Iteration 160/1000 | Loss: 0.00002059
Iteration 161/1000 | Loss: 0.00002059
Iteration 162/1000 | Loss: 0.00002059
Iteration 163/1000 | Loss: 0.00002059
Iteration 164/1000 | Loss: 0.00002059
Iteration 165/1000 | Loss: 0.00002058
Iteration 166/1000 | Loss: 0.00002058
Iteration 167/1000 | Loss: 0.00002058
Iteration 168/1000 | Loss: 0.00002058
Iteration 169/1000 | Loss: 0.00002058
Iteration 170/1000 | Loss: 0.00002058
Iteration 171/1000 | Loss: 0.00002058
Iteration 172/1000 | Loss: 0.00002058
Iteration 173/1000 | Loss: 0.00002058
Iteration 174/1000 | Loss: 0.00002058
Iteration 175/1000 | Loss: 0.00002058
Iteration 176/1000 | Loss: 0.00002058
Iteration 177/1000 | Loss: 0.00002058
Iteration 178/1000 | Loss: 0.00002058
Iteration 179/1000 | Loss: 0.00002057
Iteration 180/1000 | Loss: 0.00002057
Iteration 181/1000 | Loss: 0.00002057
Iteration 182/1000 | Loss: 0.00002057
Iteration 183/1000 | Loss: 0.00002057
Iteration 184/1000 | Loss: 0.00002057
Iteration 185/1000 | Loss: 0.00002057
Iteration 186/1000 | Loss: 0.00002057
Iteration 187/1000 | Loss: 0.00002057
Iteration 188/1000 | Loss: 0.00002057
Iteration 189/1000 | Loss: 0.00002057
Iteration 190/1000 | Loss: 0.00002057
Iteration 191/1000 | Loss: 0.00002057
Iteration 192/1000 | Loss: 0.00002057
Iteration 193/1000 | Loss: 0.00002057
Iteration 194/1000 | Loss: 0.00002056
Iteration 195/1000 | Loss: 0.00002056
Iteration 196/1000 | Loss: 0.00002056
Iteration 197/1000 | Loss: 0.00002056
Iteration 198/1000 | Loss: 0.00002056
Iteration 199/1000 | Loss: 0.00002056
Iteration 200/1000 | Loss: 0.00002056
Iteration 201/1000 | Loss: 0.00002056
Iteration 202/1000 | Loss: 0.00002056
Iteration 203/1000 | Loss: 0.00002056
Iteration 204/1000 | Loss: 0.00002056
Iteration 205/1000 | Loss: 0.00002056
Iteration 206/1000 | Loss: 0.00002056
Iteration 207/1000 | Loss: 0.00002056
Iteration 208/1000 | Loss: 0.00002056
Iteration 209/1000 | Loss: 0.00002056
Iteration 210/1000 | Loss: 0.00002056
Iteration 211/1000 | Loss: 0.00002056
Iteration 212/1000 | Loss: 0.00002056
Iteration 213/1000 | Loss: 0.00002056
Iteration 214/1000 | Loss: 0.00002056
Iteration 215/1000 | Loss: 0.00002056
Iteration 216/1000 | Loss: 0.00002056
Iteration 217/1000 | Loss: 0.00002056
Iteration 218/1000 | Loss: 0.00002056
Iteration 219/1000 | Loss: 0.00002056
Iteration 220/1000 | Loss: 0.00002056
Iteration 221/1000 | Loss: 0.00002056
Iteration 222/1000 | Loss: 0.00002056
Iteration 223/1000 | Loss: 0.00002056
Iteration 224/1000 | Loss: 0.00002056
Iteration 225/1000 | Loss: 0.00002056
Iteration 226/1000 | Loss: 0.00002056
Iteration 227/1000 | Loss: 0.00002056
Iteration 228/1000 | Loss: 0.00002056
Iteration 229/1000 | Loss: 0.00002056
Iteration 230/1000 | Loss: 0.00002056
Iteration 231/1000 | Loss: 0.00002056
Iteration 232/1000 | Loss: 0.00002056
Iteration 233/1000 | Loss: 0.00002056
Iteration 234/1000 | Loss: 0.00002056
Iteration 235/1000 | Loss: 0.00002056
Iteration 236/1000 | Loss: 0.00002056
Iteration 237/1000 | Loss: 0.00002056
Iteration 238/1000 | Loss: 0.00002056
Iteration 239/1000 | Loss: 0.00002056
Iteration 240/1000 | Loss: 0.00002056
Iteration 241/1000 | Loss: 0.00002056
Iteration 242/1000 | Loss: 0.00002056
Iteration 243/1000 | Loss: 0.00002056
Iteration 244/1000 | Loss: 0.00002056
Iteration 245/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [2.0559278709697537e-05, 2.0559278709697537e-05, 2.0559278709697537e-05, 2.0559278709697537e-05, 2.0559278709697537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0559278709697537e-05

Optimization complete. Final v2v error: 3.770768642425537 mm

Highest mean error: 5.619911193847656 mm for frame 70

Lowest mean error: 3.147946834564209 mm for frame 22

Saving results

Total time: 41.52324342727661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884499
Iteration 2/25 | Loss: 0.00187649
Iteration 3/25 | Loss: 0.00132241
Iteration 4/25 | Loss: 0.00127248
Iteration 5/25 | Loss: 0.00125774
Iteration 6/25 | Loss: 0.00126713
Iteration 7/25 | Loss: 0.00126154
Iteration 8/25 | Loss: 0.00125793
Iteration 9/25 | Loss: 0.00124895
Iteration 10/25 | Loss: 0.00124734
Iteration 11/25 | Loss: 0.00124971
Iteration 12/25 | Loss: 0.00124996
Iteration 13/25 | Loss: 0.00124536
Iteration 14/25 | Loss: 0.00123893
Iteration 15/25 | Loss: 0.00123857
Iteration 16/25 | Loss: 0.00123749
Iteration 17/25 | Loss: 0.00123743
Iteration 18/25 | Loss: 0.00123729
Iteration 19/25 | Loss: 0.00123674
Iteration 20/25 | Loss: 0.00123697
Iteration 21/25 | Loss: 0.00123644
Iteration 22/25 | Loss: 0.00123714
Iteration 23/25 | Loss: 0.00123696
Iteration 24/25 | Loss: 0.00123681
Iteration 25/25 | Loss: 0.00123713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46727300
Iteration 2/25 | Loss: 0.00181230
Iteration 3/25 | Loss: 0.00163936
Iteration 4/25 | Loss: 0.00163936
Iteration 5/25 | Loss: 0.00163936
Iteration 6/25 | Loss: 0.00163936
Iteration 7/25 | Loss: 0.00163936
Iteration 8/25 | Loss: 0.00163936
Iteration 9/25 | Loss: 0.00163936
Iteration 10/25 | Loss: 0.00163936
Iteration 11/25 | Loss: 0.00163936
Iteration 12/25 | Loss: 0.00163936
Iteration 13/25 | Loss: 0.00163936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0016393614932894707, 0.0016393614932894707, 0.0016393614932894707, 0.0016393614932894707, 0.0016393614932894707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016393614932894707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163936
Iteration 2/1000 | Loss: 0.00007726
Iteration 3/1000 | Loss: 0.00026742
Iteration 4/1000 | Loss: 0.00055388
Iteration 5/1000 | Loss: 0.00005228
Iteration 6/1000 | Loss: 0.00003833
Iteration 7/1000 | Loss: 0.00004642
Iteration 8/1000 | Loss: 0.00003488
Iteration 9/1000 | Loss: 0.00003954
Iteration 10/1000 | Loss: 0.00019252
Iteration 11/1000 | Loss: 0.00028875
Iteration 12/1000 | Loss: 0.00016572
Iteration 13/1000 | Loss: 0.00043011
Iteration 14/1000 | Loss: 0.00041205
Iteration 15/1000 | Loss: 0.00029783
Iteration 16/1000 | Loss: 0.00003746
Iteration 17/1000 | Loss: 0.00004156
Iteration 18/1000 | Loss: 0.00003084
Iteration 19/1000 | Loss: 0.00003269
Iteration 20/1000 | Loss: 0.00003212
Iteration 21/1000 | Loss: 0.00026251
Iteration 22/1000 | Loss: 0.00009285
Iteration 23/1000 | Loss: 0.00020184
Iteration 24/1000 | Loss: 0.00003556
Iteration 25/1000 | Loss: 0.00002826
Iteration 26/1000 | Loss: 0.00003297
Iteration 27/1000 | Loss: 0.00003248
Iteration 28/1000 | Loss: 0.00003678
Iteration 29/1000 | Loss: 0.00003600
Iteration 30/1000 | Loss: 0.00004013
Iteration 31/1000 | Loss: 0.00003456
Iteration 32/1000 | Loss: 0.00003602
Iteration 33/1000 | Loss: 0.00003318
Iteration 34/1000 | Loss: 0.00003560
Iteration 35/1000 | Loss: 0.00003303
Iteration 36/1000 | Loss: 0.00002962
Iteration 37/1000 | Loss: 0.00003533
Iteration 38/1000 | Loss: 0.00003267
Iteration 39/1000 | Loss: 0.00003578
Iteration 40/1000 | Loss: 0.00003217
Iteration 41/1000 | Loss: 0.00002885
Iteration 42/1000 | Loss: 0.00003285
Iteration 43/1000 | Loss: 0.00003467
Iteration 44/1000 | Loss: 0.00003227
Iteration 45/1000 | Loss: 0.00003549
Iteration 46/1000 | Loss: 0.00003212
Iteration 47/1000 | Loss: 0.00003570
Iteration 48/1000 | Loss: 0.00003765
Iteration 49/1000 | Loss: 0.00003670
Iteration 50/1000 | Loss: 0.00003235
Iteration 51/1000 | Loss: 0.00003616
Iteration 52/1000 | Loss: 0.00003219
Iteration 53/1000 | Loss: 0.00003633
Iteration 54/1000 | Loss: 0.00003575
Iteration 55/1000 | Loss: 0.00003568
Iteration 56/1000 | Loss: 0.00002582
Iteration 57/1000 | Loss: 0.00002343
Iteration 58/1000 | Loss: 0.00002231
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002089
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002067
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002066
Iteration 69/1000 | Loss: 0.00002066
Iteration 70/1000 | Loss: 0.00002066
Iteration 71/1000 | Loss: 0.00002066
Iteration 72/1000 | Loss: 0.00002065
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002064
Iteration 75/1000 | Loss: 0.00002063
Iteration 76/1000 | Loss: 0.00002059
Iteration 77/1000 | Loss: 0.00002057
Iteration 78/1000 | Loss: 0.00002056
Iteration 79/1000 | Loss: 0.00002056
Iteration 80/1000 | Loss: 0.00002055
Iteration 81/1000 | Loss: 0.00002055
Iteration 82/1000 | Loss: 0.00002054
Iteration 83/1000 | Loss: 0.00002054
Iteration 84/1000 | Loss: 0.00002054
Iteration 85/1000 | Loss: 0.00002054
Iteration 86/1000 | Loss: 0.00002053
Iteration 87/1000 | Loss: 0.00002053
Iteration 88/1000 | Loss: 0.00002053
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002049
Iteration 91/1000 | Loss: 0.00002049
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002047
Iteration 96/1000 | Loss: 0.00002047
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002045
Iteration 103/1000 | Loss: 0.00002045
Iteration 104/1000 | Loss: 0.00002044
Iteration 105/1000 | Loss: 0.00002044
Iteration 106/1000 | Loss: 0.00002044
Iteration 107/1000 | Loss: 0.00002044
Iteration 108/1000 | Loss: 0.00002044
Iteration 109/1000 | Loss: 0.00002044
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002043
Iteration 112/1000 | Loss: 0.00002043
Iteration 113/1000 | Loss: 0.00002043
Iteration 114/1000 | Loss: 0.00002042
Iteration 115/1000 | Loss: 0.00002042
Iteration 116/1000 | Loss: 0.00002042
Iteration 117/1000 | Loss: 0.00002042
Iteration 118/1000 | Loss: 0.00002042
Iteration 119/1000 | Loss: 0.00002042
Iteration 120/1000 | Loss: 0.00002042
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002040
Iteration 127/1000 | Loss: 0.00002040
Iteration 128/1000 | Loss: 0.00002040
Iteration 129/1000 | Loss: 0.00002040
Iteration 130/1000 | Loss: 0.00002040
Iteration 131/1000 | Loss: 0.00002040
Iteration 132/1000 | Loss: 0.00002040
Iteration 133/1000 | Loss: 0.00002040
Iteration 134/1000 | Loss: 0.00002040
Iteration 135/1000 | Loss: 0.00002039
Iteration 136/1000 | Loss: 0.00002039
Iteration 137/1000 | Loss: 0.00002039
Iteration 138/1000 | Loss: 0.00002038
Iteration 139/1000 | Loss: 0.00002038
Iteration 140/1000 | Loss: 0.00002038
Iteration 141/1000 | Loss: 0.00002038
Iteration 142/1000 | Loss: 0.00002038
Iteration 143/1000 | Loss: 0.00002037
Iteration 144/1000 | Loss: 0.00002037
Iteration 145/1000 | Loss: 0.00002037
Iteration 146/1000 | Loss: 0.00002037
Iteration 147/1000 | Loss: 0.00002037
Iteration 148/1000 | Loss: 0.00002036
Iteration 149/1000 | Loss: 0.00002036
Iteration 150/1000 | Loss: 0.00002036
Iteration 151/1000 | Loss: 0.00002035
Iteration 152/1000 | Loss: 0.00002035
Iteration 153/1000 | Loss: 0.00002035
Iteration 154/1000 | Loss: 0.00002035
Iteration 155/1000 | Loss: 0.00002034
Iteration 156/1000 | Loss: 0.00002034
Iteration 157/1000 | Loss: 0.00002034
Iteration 158/1000 | Loss: 0.00002034
Iteration 159/1000 | Loss: 0.00002034
Iteration 160/1000 | Loss: 0.00002033
Iteration 161/1000 | Loss: 0.00002033
Iteration 162/1000 | Loss: 0.00002033
Iteration 163/1000 | Loss: 0.00002033
Iteration 164/1000 | Loss: 0.00002033
Iteration 165/1000 | Loss: 0.00002032
Iteration 166/1000 | Loss: 0.00002032
Iteration 167/1000 | Loss: 0.00002032
Iteration 168/1000 | Loss: 0.00002032
Iteration 169/1000 | Loss: 0.00002032
Iteration 170/1000 | Loss: 0.00002032
Iteration 171/1000 | Loss: 0.00002032
Iteration 172/1000 | Loss: 0.00002032
Iteration 173/1000 | Loss: 0.00002031
Iteration 174/1000 | Loss: 0.00002031
Iteration 175/1000 | Loss: 0.00002031
Iteration 176/1000 | Loss: 0.00002031
Iteration 177/1000 | Loss: 0.00002030
Iteration 178/1000 | Loss: 0.00002030
Iteration 179/1000 | Loss: 0.00002030
Iteration 180/1000 | Loss: 0.00002030
Iteration 181/1000 | Loss: 0.00002030
Iteration 182/1000 | Loss: 0.00002030
Iteration 183/1000 | Loss: 0.00002030
Iteration 184/1000 | Loss: 0.00002030
Iteration 185/1000 | Loss: 0.00002029
Iteration 186/1000 | Loss: 0.00002029
Iteration 187/1000 | Loss: 0.00002029
Iteration 188/1000 | Loss: 0.00002029
Iteration 189/1000 | Loss: 0.00002029
Iteration 190/1000 | Loss: 0.00002029
Iteration 191/1000 | Loss: 0.00002029
Iteration 192/1000 | Loss: 0.00002029
Iteration 193/1000 | Loss: 0.00002029
Iteration 194/1000 | Loss: 0.00002029
Iteration 195/1000 | Loss: 0.00002028
Iteration 196/1000 | Loss: 0.00002028
Iteration 197/1000 | Loss: 0.00002028
Iteration 198/1000 | Loss: 0.00002028
Iteration 199/1000 | Loss: 0.00002028
Iteration 200/1000 | Loss: 0.00002027
Iteration 201/1000 | Loss: 0.00002027
Iteration 202/1000 | Loss: 0.00002027
Iteration 203/1000 | Loss: 0.00002027
Iteration 204/1000 | Loss: 0.00002027
Iteration 205/1000 | Loss: 0.00002027
Iteration 206/1000 | Loss: 0.00002026
Iteration 207/1000 | Loss: 0.00002026
Iteration 208/1000 | Loss: 0.00002026
Iteration 209/1000 | Loss: 0.00002025
Iteration 210/1000 | Loss: 0.00002025
Iteration 211/1000 | Loss: 0.00002025
Iteration 212/1000 | Loss: 0.00002024
Iteration 213/1000 | Loss: 0.00002024
Iteration 214/1000 | Loss: 0.00002024
Iteration 215/1000 | Loss: 0.00002024
Iteration 216/1000 | Loss: 0.00002024
Iteration 217/1000 | Loss: 0.00002024
Iteration 218/1000 | Loss: 0.00002024
Iteration 219/1000 | Loss: 0.00002024
Iteration 220/1000 | Loss: 0.00002024
Iteration 221/1000 | Loss: 0.00002024
Iteration 222/1000 | Loss: 0.00002023
Iteration 223/1000 | Loss: 0.00002023
Iteration 224/1000 | Loss: 0.00002023
Iteration 225/1000 | Loss: 0.00002023
Iteration 226/1000 | Loss: 0.00002023
Iteration 227/1000 | Loss: 0.00002023
Iteration 228/1000 | Loss: 0.00002023
Iteration 229/1000 | Loss: 0.00002023
Iteration 230/1000 | Loss: 0.00002023
Iteration 231/1000 | Loss: 0.00002023
Iteration 232/1000 | Loss: 0.00002023
Iteration 233/1000 | Loss: 0.00002023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.0232409951859154e-05, 2.0232409951859154e-05, 2.0232409951859154e-05, 2.0232409951859154e-05, 2.0232409951859154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0232409951859154e-05

Optimization complete. Final v2v error: 3.747267961502075 mm

Highest mean error: 4.486385345458984 mm for frame 17

Lowest mean error: 2.9435465335845947 mm for frame 180

Saving results

Total time: 190.27448344230652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856731
Iteration 2/25 | Loss: 0.00149229
Iteration 3/25 | Loss: 0.00121393
Iteration 4/25 | Loss: 0.00118261
Iteration 5/25 | Loss: 0.00117817
Iteration 6/25 | Loss: 0.00118555
Iteration 7/25 | Loss: 0.00118189
Iteration 8/25 | Loss: 0.00117496
Iteration 9/25 | Loss: 0.00116862
Iteration 10/25 | Loss: 0.00116699
Iteration 11/25 | Loss: 0.00116686
Iteration 12/25 | Loss: 0.00116473
Iteration 13/25 | Loss: 0.00116381
Iteration 14/25 | Loss: 0.00116360
Iteration 15/25 | Loss: 0.00116351
Iteration 16/25 | Loss: 0.00116349
Iteration 17/25 | Loss: 0.00116349
Iteration 18/25 | Loss: 0.00116349
Iteration 19/25 | Loss: 0.00116349
Iteration 20/25 | Loss: 0.00116349
Iteration 21/25 | Loss: 0.00116349
Iteration 22/25 | Loss: 0.00116349
Iteration 23/25 | Loss: 0.00116349
Iteration 24/25 | Loss: 0.00116349
Iteration 25/25 | Loss: 0.00116349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34197819
Iteration 2/25 | Loss: 0.00145055
Iteration 3/25 | Loss: 0.00144159
Iteration 4/25 | Loss: 0.00144159
Iteration 5/25 | Loss: 0.00144159
Iteration 6/25 | Loss: 0.00144159
Iteration 7/25 | Loss: 0.00144159
Iteration 8/25 | Loss: 0.00144158
Iteration 9/25 | Loss: 0.00144158
Iteration 10/25 | Loss: 0.00144158
Iteration 11/25 | Loss: 0.00144158
Iteration 12/25 | Loss: 0.00144158
Iteration 13/25 | Loss: 0.00144158
Iteration 14/25 | Loss: 0.00144158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014415844343602657, 0.0014415844343602657, 0.0014415844343602657, 0.0014415844343602657, 0.0014415844343602657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014415844343602657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144158
Iteration 2/1000 | Loss: 0.00007120
Iteration 3/1000 | Loss: 0.00003387
Iteration 4/1000 | Loss: 0.00002587
Iteration 5/1000 | Loss: 0.00002378
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00014338
Iteration 8/1000 | Loss: 0.00002375
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00001895
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001716
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001696
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001687
Iteration 20/1000 | Loss: 0.00001679
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001665
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001664
Iteration 34/1000 | Loss: 0.00001664
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001659
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001654
Iteration 50/1000 | Loss: 0.00001653
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001652
Iteration 53/1000 | Loss: 0.00001652
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001652
Iteration 56/1000 | Loss: 0.00001652
Iteration 57/1000 | Loss: 0.00001651
Iteration 58/1000 | Loss: 0.00001651
Iteration 59/1000 | Loss: 0.00001651
Iteration 60/1000 | Loss: 0.00001651
Iteration 61/1000 | Loss: 0.00001651
Iteration 62/1000 | Loss: 0.00001650
Iteration 63/1000 | Loss: 0.00001650
Iteration 64/1000 | Loss: 0.00001650
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00001650
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001648
Iteration 73/1000 | Loss: 0.00001648
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001648
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001648
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001647
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.6470208720420487e-05, 1.6470208720420487e-05, 1.6470208720420487e-05, 1.6470208720420487e-05, 1.6470208720420487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6470208720420487e-05

Optimization complete. Final v2v error: 3.49037504196167 mm

Highest mean error: 4.578220367431641 mm for frame 108

Lowest mean error: 3.060781955718994 mm for frame 70

Saving results

Total time: 69.94070363044739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496717
Iteration 2/25 | Loss: 0.00133774
Iteration 3/25 | Loss: 0.00121915
Iteration 4/25 | Loss: 0.00121060
Iteration 5/25 | Loss: 0.00120848
Iteration 6/25 | Loss: 0.00120848
Iteration 7/25 | Loss: 0.00120848
Iteration 8/25 | Loss: 0.00120848
Iteration 9/25 | Loss: 0.00120848
Iteration 10/25 | Loss: 0.00120848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012084826594218612, 0.0012084826594218612, 0.0012084826594218612, 0.0012084826594218612, 0.0012084826594218612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012084826594218612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20191276
Iteration 2/25 | Loss: 0.00150371
Iteration 3/25 | Loss: 0.00150369
Iteration 4/25 | Loss: 0.00150369
Iteration 5/25 | Loss: 0.00150369
Iteration 6/25 | Loss: 0.00150369
Iteration 7/25 | Loss: 0.00150369
Iteration 8/25 | Loss: 0.00150369
Iteration 9/25 | Loss: 0.00150369
Iteration 10/25 | Loss: 0.00150369
Iteration 11/25 | Loss: 0.00150369
Iteration 12/25 | Loss: 0.00150369
Iteration 13/25 | Loss: 0.00150369
Iteration 14/25 | Loss: 0.00150369
Iteration 15/25 | Loss: 0.00150369
Iteration 16/25 | Loss: 0.00150369
Iteration 17/25 | Loss: 0.00150369
Iteration 18/25 | Loss: 0.00150369
Iteration 19/25 | Loss: 0.00150369
Iteration 20/25 | Loss: 0.00150369
Iteration 21/25 | Loss: 0.00150369
Iteration 22/25 | Loss: 0.00150369
Iteration 23/25 | Loss: 0.00150369
Iteration 24/25 | Loss: 0.00150369
Iteration 25/25 | Loss: 0.00150369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150369
Iteration 2/1000 | Loss: 0.00005611
Iteration 3/1000 | Loss: 0.00003292
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001923
Iteration 10/1000 | Loss: 0.00001898
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001851
Iteration 15/1000 | Loss: 0.00001846
Iteration 16/1000 | Loss: 0.00001841
Iteration 17/1000 | Loss: 0.00001838
Iteration 18/1000 | Loss: 0.00001838
Iteration 19/1000 | Loss: 0.00001838
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001836
Iteration 22/1000 | Loss: 0.00001836
Iteration 23/1000 | Loss: 0.00001831
Iteration 24/1000 | Loss: 0.00001831
Iteration 25/1000 | Loss: 0.00001831
Iteration 26/1000 | Loss: 0.00001831
Iteration 27/1000 | Loss: 0.00001831
Iteration 28/1000 | Loss: 0.00001831
Iteration 29/1000 | Loss: 0.00001831
Iteration 30/1000 | Loss: 0.00001831
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001831
Iteration 33/1000 | Loss: 0.00001831
Iteration 34/1000 | Loss: 0.00001831
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001828
Iteration 37/1000 | Loss: 0.00001827
Iteration 38/1000 | Loss: 0.00001827
Iteration 39/1000 | Loss: 0.00001826
Iteration 40/1000 | Loss: 0.00001826
Iteration 41/1000 | Loss: 0.00001825
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001824
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00001816
Iteration 51/1000 | Loss: 0.00001815
Iteration 52/1000 | Loss: 0.00001814
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001812
Iteration 55/1000 | Loss: 0.00001811
Iteration 56/1000 | Loss: 0.00001811
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001809
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001807
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001803
Iteration 70/1000 | Loss: 0.00001803
Iteration 71/1000 | Loss: 0.00001802
Iteration 72/1000 | Loss: 0.00001802
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001801
Iteration 75/1000 | Loss: 0.00001801
Iteration 76/1000 | Loss: 0.00001801
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001801
Iteration 79/1000 | Loss: 0.00001801
Iteration 80/1000 | Loss: 0.00001801
Iteration 81/1000 | Loss: 0.00001800
Iteration 82/1000 | Loss: 0.00001800
Iteration 83/1000 | Loss: 0.00001800
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001798
Iteration 91/1000 | Loss: 0.00001798
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001794
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001793
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001792
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001792
Iteration 127/1000 | Loss: 0.00001792
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001791
Iteration 130/1000 | Loss: 0.00001791
Iteration 131/1000 | Loss: 0.00001791
Iteration 132/1000 | Loss: 0.00001791
Iteration 133/1000 | Loss: 0.00001791
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001791
Iteration 136/1000 | Loss: 0.00001790
Iteration 137/1000 | Loss: 0.00001790
Iteration 138/1000 | Loss: 0.00001790
Iteration 139/1000 | Loss: 0.00001790
Iteration 140/1000 | Loss: 0.00001790
Iteration 141/1000 | Loss: 0.00001790
Iteration 142/1000 | Loss: 0.00001790
Iteration 143/1000 | Loss: 0.00001790
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001790
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001790
Iteration 151/1000 | Loss: 0.00001790
Iteration 152/1000 | Loss: 0.00001790
Iteration 153/1000 | Loss: 0.00001790
Iteration 154/1000 | Loss: 0.00001790
Iteration 155/1000 | Loss: 0.00001790
Iteration 156/1000 | Loss: 0.00001790
Iteration 157/1000 | Loss: 0.00001790
Iteration 158/1000 | Loss: 0.00001790
Iteration 159/1000 | Loss: 0.00001790
Iteration 160/1000 | Loss: 0.00001790
Iteration 161/1000 | Loss: 0.00001790
Iteration 162/1000 | Loss: 0.00001790
Iteration 163/1000 | Loss: 0.00001790
Iteration 164/1000 | Loss: 0.00001790
Iteration 165/1000 | Loss: 0.00001790
Iteration 166/1000 | Loss: 0.00001790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.789563066267874e-05, 1.789563066267874e-05, 1.789563066267874e-05, 1.789563066267874e-05, 1.789563066267874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.789563066267874e-05

Optimization complete. Final v2v error: 3.5839929580688477 mm

Highest mean error: 4.184061527252197 mm for frame 173

Lowest mean error: 3.231736183166504 mm for frame 7

Saving results

Total time: 69.68775963783264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041606
Iteration 2/25 | Loss: 0.00269425
Iteration 3/25 | Loss: 0.00195710
Iteration 4/25 | Loss: 0.00173710
Iteration 5/25 | Loss: 0.00168011
Iteration 6/25 | Loss: 0.00156579
Iteration 7/25 | Loss: 0.00165547
Iteration 8/25 | Loss: 0.00159587
Iteration 9/25 | Loss: 0.00140445
Iteration 10/25 | Loss: 0.00128896
Iteration 11/25 | Loss: 0.00127549
Iteration 12/25 | Loss: 0.00126701
Iteration 13/25 | Loss: 0.00125593
Iteration 14/25 | Loss: 0.00123705
Iteration 15/25 | Loss: 0.00122736
Iteration 16/25 | Loss: 0.00121478
Iteration 17/25 | Loss: 0.00120956
Iteration 18/25 | Loss: 0.00120878
Iteration 19/25 | Loss: 0.00121240
Iteration 20/25 | Loss: 0.00120899
Iteration 21/25 | Loss: 0.00120708
Iteration 22/25 | Loss: 0.00120675
Iteration 23/25 | Loss: 0.00120667
Iteration 24/25 | Loss: 0.00120666
Iteration 25/25 | Loss: 0.00120666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22097087
Iteration 2/25 | Loss: 0.00210181
Iteration 3/25 | Loss: 0.00174686
Iteration 4/25 | Loss: 0.00174686
Iteration 5/25 | Loss: 0.00174686
Iteration 6/25 | Loss: 0.00174686
Iteration 7/25 | Loss: 0.00174686
Iteration 8/25 | Loss: 0.00174686
Iteration 9/25 | Loss: 0.00174686
Iteration 10/25 | Loss: 0.00174686
Iteration 11/25 | Loss: 0.00174686
Iteration 12/25 | Loss: 0.00174686
Iteration 13/25 | Loss: 0.00174686
Iteration 14/25 | Loss: 0.00174686
Iteration 15/25 | Loss: 0.00174686
Iteration 16/25 | Loss: 0.00174686
Iteration 17/25 | Loss: 0.00174686
Iteration 18/25 | Loss: 0.00174686
Iteration 19/25 | Loss: 0.00174686
Iteration 20/25 | Loss: 0.00174686
Iteration 21/25 | Loss: 0.00174686
Iteration 22/25 | Loss: 0.00174686
Iteration 23/25 | Loss: 0.00174686
Iteration 24/25 | Loss: 0.00174686
Iteration 25/25 | Loss: 0.00174686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174686
Iteration 2/1000 | Loss: 0.00046150
Iteration 3/1000 | Loss: 0.00024875
Iteration 4/1000 | Loss: 0.00017360
Iteration 5/1000 | Loss: 0.00004635
Iteration 6/1000 | Loss: 0.00007183
Iteration 7/1000 | Loss: 0.00011491
Iteration 8/1000 | Loss: 0.00028296
Iteration 9/1000 | Loss: 0.00018197
Iteration 10/1000 | Loss: 0.00005666
Iteration 11/1000 | Loss: 0.00003942
Iteration 12/1000 | Loss: 0.00004791
Iteration 13/1000 | Loss: 0.00021809
Iteration 14/1000 | Loss: 0.00003551
Iteration 15/1000 | Loss: 0.00044235
Iteration 16/1000 | Loss: 0.00111264
Iteration 17/1000 | Loss: 0.00079690
Iteration 18/1000 | Loss: 0.00007180
Iteration 19/1000 | Loss: 0.00017486
Iteration 20/1000 | Loss: 0.00109042
Iteration 21/1000 | Loss: 0.00008635
Iteration 22/1000 | Loss: 0.00040109
Iteration 23/1000 | Loss: 0.00002889
Iteration 24/1000 | Loss: 0.00025834
Iteration 25/1000 | Loss: 0.00038106
Iteration 26/1000 | Loss: 0.00003692
Iteration 27/1000 | Loss: 0.00003300
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00003685
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00015276
Iteration 32/1000 | Loss: 0.00020335
Iteration 33/1000 | Loss: 0.00002225
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00007398
Iteration 36/1000 | Loss: 0.00007923
Iteration 37/1000 | Loss: 0.00005116
Iteration 38/1000 | Loss: 0.00002336
Iteration 39/1000 | Loss: 0.00002152
Iteration 40/1000 | Loss: 0.00007507
Iteration 41/1000 | Loss: 0.00003285
Iteration 42/1000 | Loss: 0.00001792
Iteration 43/1000 | Loss: 0.00006625
Iteration 44/1000 | Loss: 0.00002360
Iteration 45/1000 | Loss: 0.00004890
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00003955
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001674
Iteration 52/1000 | Loss: 0.00001673
Iteration 53/1000 | Loss: 0.00001672
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001657
Iteration 59/1000 | Loss: 0.00001657
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001652
Iteration 69/1000 | Loss: 0.00001652
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001650
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001649
Iteration 83/1000 | Loss: 0.00001649
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00001648
Iteration 86/1000 | Loss: 0.00001648
Iteration 87/1000 | Loss: 0.00001648
Iteration 88/1000 | Loss: 0.00001648
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00004315
Iteration 98/1000 | Loss: 0.00004314
Iteration 99/1000 | Loss: 0.00049487
Iteration 100/1000 | Loss: 0.00002722
Iteration 101/1000 | Loss: 0.00006963
Iteration 102/1000 | Loss: 0.00002573
Iteration 103/1000 | Loss: 0.00005819
Iteration 104/1000 | Loss: 0.00002411
Iteration 105/1000 | Loss: 0.00002064
Iteration 106/1000 | Loss: 0.00004317
Iteration 107/1000 | Loss: 0.00003098
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001652
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001647
Iteration 113/1000 | Loss: 0.00001645
Iteration 114/1000 | Loss: 0.00001645
Iteration 115/1000 | Loss: 0.00004859
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00003901
Iteration 118/1000 | Loss: 0.00002391
Iteration 119/1000 | Loss: 0.00004887
Iteration 120/1000 | Loss: 0.00002029
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00004979
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001644
Iteration 126/1000 | Loss: 0.00002690
Iteration 127/1000 | Loss: 0.00001641
Iteration 128/1000 | Loss: 0.00001640
Iteration 129/1000 | Loss: 0.00001640
Iteration 130/1000 | Loss: 0.00001640
Iteration 131/1000 | Loss: 0.00001640
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001639
Iteration 138/1000 | Loss: 0.00001639
Iteration 139/1000 | Loss: 0.00001639
Iteration 140/1000 | Loss: 0.00001639
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001638
Iteration 143/1000 | Loss: 0.00001638
Iteration 144/1000 | Loss: 0.00001638
Iteration 145/1000 | Loss: 0.00001638
Iteration 146/1000 | Loss: 0.00001638
Iteration 147/1000 | Loss: 0.00001638
Iteration 148/1000 | Loss: 0.00001638
Iteration 149/1000 | Loss: 0.00001638
Iteration 150/1000 | Loss: 0.00001637
Iteration 151/1000 | Loss: 0.00001637
Iteration 152/1000 | Loss: 0.00001637
Iteration 153/1000 | Loss: 0.00001637
Iteration 154/1000 | Loss: 0.00001637
Iteration 155/1000 | Loss: 0.00001636
Iteration 156/1000 | Loss: 0.00001636
Iteration 157/1000 | Loss: 0.00001636
Iteration 158/1000 | Loss: 0.00001636
Iteration 159/1000 | Loss: 0.00001636
Iteration 160/1000 | Loss: 0.00001636
Iteration 161/1000 | Loss: 0.00001636
Iteration 162/1000 | Loss: 0.00001636
Iteration 163/1000 | Loss: 0.00001636
Iteration 164/1000 | Loss: 0.00001636
Iteration 165/1000 | Loss: 0.00001636
Iteration 166/1000 | Loss: 0.00001636
Iteration 167/1000 | Loss: 0.00001636
Iteration 168/1000 | Loss: 0.00001635
Iteration 169/1000 | Loss: 0.00001635
Iteration 170/1000 | Loss: 0.00001635
Iteration 171/1000 | Loss: 0.00001635
Iteration 172/1000 | Loss: 0.00001635
Iteration 173/1000 | Loss: 0.00001635
Iteration 174/1000 | Loss: 0.00001635
Iteration 175/1000 | Loss: 0.00001635
Iteration 176/1000 | Loss: 0.00001635
Iteration 177/1000 | Loss: 0.00001635
Iteration 178/1000 | Loss: 0.00001635
Iteration 179/1000 | Loss: 0.00001635
Iteration 180/1000 | Loss: 0.00001635
Iteration 181/1000 | Loss: 0.00001635
Iteration 182/1000 | Loss: 0.00001635
Iteration 183/1000 | Loss: 0.00001635
Iteration 184/1000 | Loss: 0.00001635
Iteration 185/1000 | Loss: 0.00001635
Iteration 186/1000 | Loss: 0.00001635
Iteration 187/1000 | Loss: 0.00001635
Iteration 188/1000 | Loss: 0.00001635
Iteration 189/1000 | Loss: 0.00001635
Iteration 190/1000 | Loss: 0.00001635
Iteration 191/1000 | Loss: 0.00001635
Iteration 192/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.634527689020615e-05, 1.634527689020615e-05, 1.634527689020615e-05, 1.634527689020615e-05, 1.634527689020615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.634527689020615e-05

Optimization complete. Final v2v error: 3.4081592559814453 mm

Highest mean error: 4.150872707366943 mm for frame 205

Lowest mean error: 3.094810962677002 mm for frame 4

Saving results

Total time: 170.02586150169373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881018
Iteration 2/25 | Loss: 0.00165995
Iteration 3/25 | Loss: 0.00126989
Iteration 4/25 | Loss: 0.00121355
Iteration 5/25 | Loss: 0.00120123
Iteration 6/25 | Loss: 0.00119847
Iteration 7/25 | Loss: 0.00119802
Iteration 8/25 | Loss: 0.00119802
Iteration 9/25 | Loss: 0.00119802
Iteration 10/25 | Loss: 0.00119802
Iteration 11/25 | Loss: 0.00119802
Iteration 12/25 | Loss: 0.00119802
Iteration 13/25 | Loss: 0.00119802
Iteration 14/25 | Loss: 0.00119802
Iteration 15/25 | Loss: 0.00119802
Iteration 16/25 | Loss: 0.00119802
Iteration 17/25 | Loss: 0.00119802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001198018086142838, 0.001198018086142838, 0.001198018086142838, 0.001198018086142838, 0.001198018086142838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001198018086142838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50866175
Iteration 2/25 | Loss: 0.00154702
Iteration 3/25 | Loss: 0.00154702
Iteration 4/25 | Loss: 0.00154702
Iteration 5/25 | Loss: 0.00154701
Iteration 6/25 | Loss: 0.00154701
Iteration 7/25 | Loss: 0.00154701
Iteration 8/25 | Loss: 0.00154701
Iteration 9/25 | Loss: 0.00154701
Iteration 10/25 | Loss: 0.00154701
Iteration 11/25 | Loss: 0.00154701
Iteration 12/25 | Loss: 0.00154701
Iteration 13/25 | Loss: 0.00154701
Iteration 14/25 | Loss: 0.00154701
Iteration 15/25 | Loss: 0.00154701
Iteration 16/25 | Loss: 0.00154701
Iteration 17/25 | Loss: 0.00154701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001547012710943818, 0.001547012710943818, 0.001547012710943818, 0.001547012710943818, 0.001547012710943818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001547012710943818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154701
Iteration 2/1000 | Loss: 0.00004950
Iteration 3/1000 | Loss: 0.00003186
Iteration 4/1000 | Loss: 0.00002576
Iteration 5/1000 | Loss: 0.00002374
Iteration 6/1000 | Loss: 0.00002250
Iteration 7/1000 | Loss: 0.00002169
Iteration 8/1000 | Loss: 0.00002101
Iteration 9/1000 | Loss: 0.00002073
Iteration 10/1000 | Loss: 0.00002051
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00002010
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001990
Iteration 19/1000 | Loss: 0.00001989
Iteration 20/1000 | Loss: 0.00001987
Iteration 21/1000 | Loss: 0.00001987
Iteration 22/1000 | Loss: 0.00001985
Iteration 23/1000 | Loss: 0.00001984
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00001983
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001981
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001980
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001976
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001966
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001963
Iteration 44/1000 | Loss: 0.00001963
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001961
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001959
Iteration 51/1000 | Loss: 0.00001959
Iteration 52/1000 | Loss: 0.00001959
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001957
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001955
Iteration 63/1000 | Loss: 0.00001955
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001954
Iteration 66/1000 | Loss: 0.00001954
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00001953
Iteration 70/1000 | Loss: 0.00001953
Iteration 71/1000 | Loss: 0.00001953
Iteration 72/1000 | Loss: 0.00001953
Iteration 73/1000 | Loss: 0.00001953
Iteration 74/1000 | Loss: 0.00001952
Iteration 75/1000 | Loss: 0.00001952
Iteration 76/1000 | Loss: 0.00001952
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001952
Iteration 79/1000 | Loss: 0.00001952
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001951
Iteration 82/1000 | Loss: 0.00001951
Iteration 83/1000 | Loss: 0.00001951
Iteration 84/1000 | Loss: 0.00001951
Iteration 85/1000 | Loss: 0.00001950
Iteration 86/1000 | Loss: 0.00001950
Iteration 87/1000 | Loss: 0.00001950
Iteration 88/1000 | Loss: 0.00001950
Iteration 89/1000 | Loss: 0.00001950
Iteration 90/1000 | Loss: 0.00001950
Iteration 91/1000 | Loss: 0.00001950
Iteration 92/1000 | Loss: 0.00001950
Iteration 93/1000 | Loss: 0.00001949
Iteration 94/1000 | Loss: 0.00001949
Iteration 95/1000 | Loss: 0.00001949
Iteration 96/1000 | Loss: 0.00001949
Iteration 97/1000 | Loss: 0.00001949
Iteration 98/1000 | Loss: 0.00001949
Iteration 99/1000 | Loss: 0.00001949
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001948
Iteration 104/1000 | Loss: 0.00001948
Iteration 105/1000 | Loss: 0.00001948
Iteration 106/1000 | Loss: 0.00001948
Iteration 107/1000 | Loss: 0.00001948
Iteration 108/1000 | Loss: 0.00001947
Iteration 109/1000 | Loss: 0.00001947
Iteration 110/1000 | Loss: 0.00001947
Iteration 111/1000 | Loss: 0.00001947
Iteration 112/1000 | Loss: 0.00001947
Iteration 113/1000 | Loss: 0.00001947
Iteration 114/1000 | Loss: 0.00001947
Iteration 115/1000 | Loss: 0.00001946
Iteration 116/1000 | Loss: 0.00001946
Iteration 117/1000 | Loss: 0.00001946
Iteration 118/1000 | Loss: 0.00001946
Iteration 119/1000 | Loss: 0.00001946
Iteration 120/1000 | Loss: 0.00001946
Iteration 121/1000 | Loss: 0.00001945
Iteration 122/1000 | Loss: 0.00001945
Iteration 123/1000 | Loss: 0.00001945
Iteration 124/1000 | Loss: 0.00001945
Iteration 125/1000 | Loss: 0.00001945
Iteration 126/1000 | Loss: 0.00001945
Iteration 127/1000 | Loss: 0.00001945
Iteration 128/1000 | Loss: 0.00001945
Iteration 129/1000 | Loss: 0.00001944
Iteration 130/1000 | Loss: 0.00001945
Iteration 131/1000 | Loss: 0.00001945
Iteration 132/1000 | Loss: 0.00001945
Iteration 133/1000 | Loss: 0.00001944
Iteration 134/1000 | Loss: 0.00001945
Iteration 135/1000 | Loss: 0.00001944
Iteration 136/1000 | Loss: 0.00001944
Iteration 137/1000 | Loss: 0.00001944
Iteration 138/1000 | Loss: 0.00001944
Iteration 139/1000 | Loss: 0.00001945
Iteration 140/1000 | Loss: 0.00001944
Iteration 141/1000 | Loss: 0.00001945
Iteration 142/1000 | Loss: 0.00001945
Iteration 143/1000 | Loss: 0.00001945
Iteration 144/1000 | Loss: 0.00001945
Iteration 145/1000 | Loss: 0.00001945
Iteration 146/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.9445000361884013e-05, 1.9445000361884013e-05, 1.9445000361884013e-05, 1.9445000361884013e-05, 1.9445000361884013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9445000361884013e-05

Optimization complete. Final v2v error: 3.6425721645355225 mm

Highest mean error: 3.904857635498047 mm for frame 30

Lowest mean error: 3.3027265071868896 mm for frame 123

Saving results

Total time: 69.93724727630615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421820
Iteration 2/25 | Loss: 0.00125213
Iteration 3/25 | Loss: 0.00115955
Iteration 4/25 | Loss: 0.00115112
Iteration 5/25 | Loss: 0.00114861
Iteration 6/25 | Loss: 0.00114829
Iteration 7/25 | Loss: 0.00114829
Iteration 8/25 | Loss: 0.00114829
Iteration 9/25 | Loss: 0.00114829
Iteration 10/25 | Loss: 0.00114829
Iteration 11/25 | Loss: 0.00114829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011482902336865664, 0.0011482902336865664, 0.0011482902336865664, 0.0011482902336865664, 0.0011482902336865664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011482902336865664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21101058
Iteration 2/25 | Loss: 0.00165396
Iteration 3/25 | Loss: 0.00165395
Iteration 4/25 | Loss: 0.00165395
Iteration 5/25 | Loss: 0.00165395
Iteration 6/25 | Loss: 0.00165395
Iteration 7/25 | Loss: 0.00165395
Iteration 8/25 | Loss: 0.00165395
Iteration 9/25 | Loss: 0.00165395
Iteration 10/25 | Loss: 0.00165395
Iteration 11/25 | Loss: 0.00165395
Iteration 12/25 | Loss: 0.00165395
Iteration 13/25 | Loss: 0.00165395
Iteration 14/25 | Loss: 0.00165395
Iteration 15/25 | Loss: 0.00165395
Iteration 16/25 | Loss: 0.00165395
Iteration 17/25 | Loss: 0.00165395
Iteration 18/25 | Loss: 0.00165395
Iteration 19/25 | Loss: 0.00165395
Iteration 20/25 | Loss: 0.00165395
Iteration 21/25 | Loss: 0.00165395
Iteration 22/25 | Loss: 0.00165395
Iteration 23/25 | Loss: 0.00165395
Iteration 24/25 | Loss: 0.00165395
Iteration 25/25 | Loss: 0.00165395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165395
Iteration 2/1000 | Loss: 0.00005016
Iteration 3/1000 | Loss: 0.00002366
Iteration 4/1000 | Loss: 0.00001849
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001608
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001538
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001473
Iteration 38/1000 | Loss: 0.00001473
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001447
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Iteration 96/1000 | Loss: 0.00001446
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001439
Iteration 136/1000 | Loss: 0.00001438
Iteration 137/1000 | Loss: 0.00001438
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001438
Iteration 143/1000 | Loss: 0.00001438
Iteration 144/1000 | Loss: 0.00001438
Iteration 145/1000 | Loss: 0.00001438
Iteration 146/1000 | Loss: 0.00001438
Iteration 147/1000 | Loss: 0.00001437
Iteration 148/1000 | Loss: 0.00001437
Iteration 149/1000 | Loss: 0.00001437
Iteration 150/1000 | Loss: 0.00001437
Iteration 151/1000 | Loss: 0.00001437
Iteration 152/1000 | Loss: 0.00001437
Iteration 153/1000 | Loss: 0.00001437
Iteration 154/1000 | Loss: 0.00001437
Iteration 155/1000 | Loss: 0.00001437
Iteration 156/1000 | Loss: 0.00001437
Iteration 157/1000 | Loss: 0.00001437
Iteration 158/1000 | Loss: 0.00001437
Iteration 159/1000 | Loss: 0.00001437
Iteration 160/1000 | Loss: 0.00001437
Iteration 161/1000 | Loss: 0.00001437
Iteration 162/1000 | Loss: 0.00001437
Iteration 163/1000 | Loss: 0.00001437
Iteration 164/1000 | Loss: 0.00001437
Iteration 165/1000 | Loss: 0.00001437
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001437
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4370086319104303e-05, 1.4370086319104303e-05, 1.4370086319104303e-05, 1.4370086319104303e-05, 1.4370086319104303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4370086319104303e-05

Optimization complete. Final v2v error: 3.2252464294433594 mm

Highest mean error: 3.829946517944336 mm for frame 227

Lowest mean error: 2.854280471801758 mm for frame 25

Saving results

Total time: 64.29283142089844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409816
Iteration 2/25 | Loss: 0.00122248
Iteration 3/25 | Loss: 0.00115917
Iteration 4/25 | Loss: 0.00114914
Iteration 5/25 | Loss: 0.00114536
Iteration 6/25 | Loss: 0.00114458
Iteration 7/25 | Loss: 0.00114458
Iteration 8/25 | Loss: 0.00114458
Iteration 9/25 | Loss: 0.00114458
Iteration 10/25 | Loss: 0.00114458
Iteration 11/25 | Loss: 0.00114458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011445825221017003, 0.0011445825221017003, 0.0011445825221017003, 0.0011445825221017003, 0.0011445825221017003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011445825221017003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63802028
Iteration 2/25 | Loss: 0.00156965
Iteration 3/25 | Loss: 0.00156964
Iteration 4/25 | Loss: 0.00156964
Iteration 5/25 | Loss: 0.00156964
Iteration 6/25 | Loss: 0.00156964
Iteration 7/25 | Loss: 0.00156964
Iteration 8/25 | Loss: 0.00156964
Iteration 9/25 | Loss: 0.00156964
Iteration 10/25 | Loss: 0.00156964
Iteration 11/25 | Loss: 0.00156964
Iteration 12/25 | Loss: 0.00156964
Iteration 13/25 | Loss: 0.00156964
Iteration 14/25 | Loss: 0.00156964
Iteration 15/25 | Loss: 0.00156964
Iteration 16/25 | Loss: 0.00156964
Iteration 17/25 | Loss: 0.00156964
Iteration 18/25 | Loss: 0.00156964
Iteration 19/25 | Loss: 0.00156964
Iteration 20/25 | Loss: 0.00156964
Iteration 21/25 | Loss: 0.00156964
Iteration 22/25 | Loss: 0.00156964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015696402406319976, 0.0015696402406319976, 0.0015696402406319976, 0.0015696402406319976, 0.0015696402406319976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015696402406319976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156964
Iteration 2/1000 | Loss: 0.00006397
Iteration 3/1000 | Loss: 0.00002875
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00001956
Iteration 6/1000 | Loss: 0.00001833
Iteration 7/1000 | Loss: 0.00001758
Iteration 8/1000 | Loss: 0.00001694
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001643
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001623
Iteration 19/1000 | Loss: 0.00001623
Iteration 20/1000 | Loss: 0.00001622
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001620
Iteration 23/1000 | Loss: 0.00001619
Iteration 24/1000 | Loss: 0.00001616
Iteration 25/1000 | Loss: 0.00001616
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00001612
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001606
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001606
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001605
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001604
Iteration 53/1000 | Loss: 0.00001604
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001604
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001603
Iteration 59/1000 | Loss: 0.00001603
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001603
Iteration 63/1000 | Loss: 0.00001603
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001603
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001602
Iteration 72/1000 | Loss: 0.00001602
Iteration 73/1000 | Loss: 0.00001602
Iteration 74/1000 | Loss: 0.00001602
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001601
Iteration 77/1000 | Loss: 0.00001601
Iteration 78/1000 | Loss: 0.00001601
Iteration 79/1000 | Loss: 0.00001600
Iteration 80/1000 | Loss: 0.00001600
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001600
Iteration 83/1000 | Loss: 0.00001600
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001600
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001600
Iteration 89/1000 | Loss: 0.00001599
Iteration 90/1000 | Loss: 0.00001599
Iteration 91/1000 | Loss: 0.00001599
Iteration 92/1000 | Loss: 0.00001599
Iteration 93/1000 | Loss: 0.00001599
Iteration 94/1000 | Loss: 0.00001599
Iteration 95/1000 | Loss: 0.00001599
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001598
Iteration 98/1000 | Loss: 0.00001598
Iteration 99/1000 | Loss: 0.00001598
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001598
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001598
Iteration 106/1000 | Loss: 0.00001598
Iteration 107/1000 | Loss: 0.00001598
Iteration 108/1000 | Loss: 0.00001598
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001597
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001597
Iteration 113/1000 | Loss: 0.00001596
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001595
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001595
Iteration 120/1000 | Loss: 0.00001595
Iteration 121/1000 | Loss: 0.00001594
Iteration 122/1000 | Loss: 0.00001594
Iteration 123/1000 | Loss: 0.00001594
Iteration 124/1000 | Loss: 0.00001593
Iteration 125/1000 | Loss: 0.00001593
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001592
Iteration 131/1000 | Loss: 0.00001592
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001590
Iteration 139/1000 | Loss: 0.00001590
Iteration 140/1000 | Loss: 0.00001590
Iteration 141/1000 | Loss: 0.00001590
Iteration 142/1000 | Loss: 0.00001590
Iteration 143/1000 | Loss: 0.00001590
Iteration 144/1000 | Loss: 0.00001589
Iteration 145/1000 | Loss: 0.00001589
Iteration 146/1000 | Loss: 0.00001589
Iteration 147/1000 | Loss: 0.00001589
Iteration 148/1000 | Loss: 0.00001589
Iteration 149/1000 | Loss: 0.00001588
Iteration 150/1000 | Loss: 0.00001588
Iteration 151/1000 | Loss: 0.00001588
Iteration 152/1000 | Loss: 0.00001588
Iteration 153/1000 | Loss: 0.00001588
Iteration 154/1000 | Loss: 0.00001588
Iteration 155/1000 | Loss: 0.00001588
Iteration 156/1000 | Loss: 0.00001588
Iteration 157/1000 | Loss: 0.00001588
Iteration 158/1000 | Loss: 0.00001587
Iteration 159/1000 | Loss: 0.00001587
Iteration 160/1000 | Loss: 0.00001587
Iteration 161/1000 | Loss: 0.00001587
Iteration 162/1000 | Loss: 0.00001587
Iteration 163/1000 | Loss: 0.00001587
Iteration 164/1000 | Loss: 0.00001587
Iteration 165/1000 | Loss: 0.00001587
Iteration 166/1000 | Loss: 0.00001587
Iteration 167/1000 | Loss: 0.00001586
Iteration 168/1000 | Loss: 0.00001586
Iteration 169/1000 | Loss: 0.00001586
Iteration 170/1000 | Loss: 0.00001586
Iteration 171/1000 | Loss: 0.00001586
Iteration 172/1000 | Loss: 0.00001586
Iteration 173/1000 | Loss: 0.00001586
Iteration 174/1000 | Loss: 0.00001586
Iteration 175/1000 | Loss: 0.00001586
Iteration 176/1000 | Loss: 0.00001586
Iteration 177/1000 | Loss: 0.00001586
Iteration 178/1000 | Loss: 0.00001586
Iteration 179/1000 | Loss: 0.00001586
Iteration 180/1000 | Loss: 0.00001585
Iteration 181/1000 | Loss: 0.00001585
Iteration 182/1000 | Loss: 0.00001585
Iteration 183/1000 | Loss: 0.00001585
Iteration 184/1000 | Loss: 0.00001585
Iteration 185/1000 | Loss: 0.00001585
Iteration 186/1000 | Loss: 0.00001585
Iteration 187/1000 | Loss: 0.00001585
Iteration 188/1000 | Loss: 0.00001585
Iteration 189/1000 | Loss: 0.00001585
Iteration 190/1000 | Loss: 0.00001585
Iteration 191/1000 | Loss: 0.00001585
Iteration 192/1000 | Loss: 0.00001585
Iteration 193/1000 | Loss: 0.00001585
Iteration 194/1000 | Loss: 0.00001585
Iteration 195/1000 | Loss: 0.00001585
Iteration 196/1000 | Loss: 0.00001584
Iteration 197/1000 | Loss: 0.00001584
Iteration 198/1000 | Loss: 0.00001584
Iteration 199/1000 | Loss: 0.00001584
Iteration 200/1000 | Loss: 0.00001584
Iteration 201/1000 | Loss: 0.00001584
Iteration 202/1000 | Loss: 0.00001584
Iteration 203/1000 | Loss: 0.00001584
Iteration 204/1000 | Loss: 0.00001584
Iteration 205/1000 | Loss: 0.00001584
Iteration 206/1000 | Loss: 0.00001584
Iteration 207/1000 | Loss: 0.00001584
Iteration 208/1000 | Loss: 0.00001584
Iteration 209/1000 | Loss: 0.00001584
Iteration 210/1000 | Loss: 0.00001584
Iteration 211/1000 | Loss: 0.00001584
Iteration 212/1000 | Loss: 0.00001584
Iteration 213/1000 | Loss: 0.00001584
Iteration 214/1000 | Loss: 0.00001583
Iteration 215/1000 | Loss: 0.00001583
Iteration 216/1000 | Loss: 0.00001583
Iteration 217/1000 | Loss: 0.00001583
Iteration 218/1000 | Loss: 0.00001583
Iteration 219/1000 | Loss: 0.00001583
Iteration 220/1000 | Loss: 0.00001583
Iteration 221/1000 | Loss: 0.00001583
Iteration 222/1000 | Loss: 0.00001583
Iteration 223/1000 | Loss: 0.00001583
Iteration 224/1000 | Loss: 0.00001583
Iteration 225/1000 | Loss: 0.00001583
Iteration 226/1000 | Loss: 0.00001583
Iteration 227/1000 | Loss: 0.00001583
Iteration 228/1000 | Loss: 0.00001583
Iteration 229/1000 | Loss: 0.00001582
Iteration 230/1000 | Loss: 0.00001582
Iteration 231/1000 | Loss: 0.00001582
Iteration 232/1000 | Loss: 0.00001582
Iteration 233/1000 | Loss: 0.00001582
Iteration 234/1000 | Loss: 0.00001582
Iteration 235/1000 | Loss: 0.00001582
Iteration 236/1000 | Loss: 0.00001582
Iteration 237/1000 | Loss: 0.00001582
Iteration 238/1000 | Loss: 0.00001582
Iteration 239/1000 | Loss: 0.00001582
Iteration 240/1000 | Loss: 0.00001582
Iteration 241/1000 | Loss: 0.00001582
Iteration 242/1000 | Loss: 0.00001582
Iteration 243/1000 | Loss: 0.00001582
Iteration 244/1000 | Loss: 0.00001582
Iteration 245/1000 | Loss: 0.00001582
Iteration 246/1000 | Loss: 0.00001582
Iteration 247/1000 | Loss: 0.00001582
Iteration 248/1000 | Loss: 0.00001582
Iteration 249/1000 | Loss: 0.00001582
Iteration 250/1000 | Loss: 0.00001582
Iteration 251/1000 | Loss: 0.00001582
Iteration 252/1000 | Loss: 0.00001582
Iteration 253/1000 | Loss: 0.00001582
Iteration 254/1000 | Loss: 0.00001582
Iteration 255/1000 | Loss: 0.00001582
Iteration 256/1000 | Loss: 0.00001582
Iteration 257/1000 | Loss: 0.00001582
Iteration 258/1000 | Loss: 0.00001582
Iteration 259/1000 | Loss: 0.00001582
Iteration 260/1000 | Loss: 0.00001582
Iteration 261/1000 | Loss: 0.00001582
Iteration 262/1000 | Loss: 0.00001582
Iteration 263/1000 | Loss: 0.00001582
Iteration 264/1000 | Loss: 0.00001582
Iteration 265/1000 | Loss: 0.00001582
Iteration 266/1000 | Loss: 0.00001582
Iteration 267/1000 | Loss: 0.00001582
Iteration 268/1000 | Loss: 0.00001582
Iteration 269/1000 | Loss: 0.00001582
Iteration 270/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.5818462998140603e-05, 1.5818462998140603e-05, 1.5818462998140603e-05, 1.5818462998140603e-05, 1.5818462998140603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5818462998140603e-05

Optimization complete. Final v2v error: 3.4223084449768066 mm

Highest mean error: 3.769474983215332 mm for frame 15

Lowest mean error: 2.949479579925537 mm for frame 51

Saving results

Total time: 54.89594769477844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107312
Iteration 2/25 | Loss: 0.01107312
Iteration 3/25 | Loss: 0.01107312
Iteration 4/25 | Loss: 0.01107311
Iteration 5/25 | Loss: 0.01107311
Iteration 6/25 | Loss: 0.01107311
Iteration 7/25 | Loss: 0.01107310
Iteration 8/25 | Loss: 0.01107310
Iteration 9/25 | Loss: 0.01107310
Iteration 10/25 | Loss: 0.01107310
Iteration 11/25 | Loss: 0.01107310
Iteration 12/25 | Loss: 0.01107310
Iteration 13/25 | Loss: 0.01107310
Iteration 14/25 | Loss: 0.01107309
Iteration 15/25 | Loss: 0.01107309
Iteration 16/25 | Loss: 0.01107309
Iteration 17/25 | Loss: 0.01107309
Iteration 18/25 | Loss: 0.01107309
Iteration 19/25 | Loss: 0.01107308
Iteration 20/25 | Loss: 0.01107308
Iteration 21/25 | Loss: 0.01107308
Iteration 22/25 | Loss: 0.01107307
Iteration 23/25 | Loss: 0.01107307
Iteration 24/25 | Loss: 0.01107307
Iteration 25/25 | Loss: 0.01107307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23554718
Iteration 2/25 | Loss: 0.19178630
Iteration 3/25 | Loss: 0.17917886
Iteration 4/25 | Loss: 0.17917846
Iteration 5/25 | Loss: 0.17900296
Iteration 6/25 | Loss: 0.17900294
Iteration 7/25 | Loss: 0.17900290
Iteration 8/25 | Loss: 0.17900290
Iteration 9/25 | Loss: 0.17900285
Iteration 10/25 | Loss: 0.17900285
Iteration 11/25 | Loss: 0.17900285
Iteration 12/25 | Loss: 0.17900285
Iteration 13/25 | Loss: 0.17900285
Iteration 14/25 | Loss: 0.17900285
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.17900285124778748, 0.17900285124778748, 0.17900285124778748, 0.17900285124778748, 0.17900285124778748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17900285124778748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17900285
Iteration 2/1000 | Loss: 0.00208434
Iteration 3/1000 | Loss: 0.00260711
Iteration 4/1000 | Loss: 0.00074745
Iteration 5/1000 | Loss: 0.00018337
Iteration 6/1000 | Loss: 0.00012107
Iteration 7/1000 | Loss: 0.00009205
Iteration 8/1000 | Loss: 0.00016748
Iteration 9/1000 | Loss: 0.00010740
Iteration 10/1000 | Loss: 0.00005520
Iteration 11/1000 | Loss: 0.00006673
Iteration 12/1000 | Loss: 0.00005622
Iteration 13/1000 | Loss: 0.00006937
Iteration 14/1000 | Loss: 0.00007959
Iteration 15/1000 | Loss: 0.00004079
Iteration 16/1000 | Loss: 0.00007112
Iteration 17/1000 | Loss: 0.00004868
Iteration 18/1000 | Loss: 0.00005360
Iteration 19/1000 | Loss: 0.00006451
Iteration 20/1000 | Loss: 0.00003399
Iteration 21/1000 | Loss: 0.00004111
Iteration 22/1000 | Loss: 0.00003306
Iteration 23/1000 | Loss: 0.00011188
Iteration 24/1000 | Loss: 0.00008637
Iteration 25/1000 | Loss: 0.00060327
Iteration 26/1000 | Loss: 0.00003992
Iteration 27/1000 | Loss: 0.00003839
Iteration 28/1000 | Loss: 0.00005901
Iteration 29/1000 | Loss: 0.00002940
Iteration 30/1000 | Loss: 0.00004566
Iteration 31/1000 | Loss: 0.00004355
Iteration 32/1000 | Loss: 0.00010886
Iteration 33/1000 | Loss: 0.00003816
Iteration 34/1000 | Loss: 0.00002952
Iteration 35/1000 | Loss: 0.00005843
Iteration 36/1000 | Loss: 0.00002767
Iteration 37/1000 | Loss: 0.00002716
Iteration 38/1000 | Loss: 0.00004248
Iteration 39/1000 | Loss: 0.00003433
Iteration 40/1000 | Loss: 0.00002728
Iteration 41/1000 | Loss: 0.00002628
Iteration 42/1000 | Loss: 0.00002828
Iteration 43/1000 | Loss: 0.00003906
Iteration 44/1000 | Loss: 0.00002570
Iteration 45/1000 | Loss: 0.00002568
Iteration 46/1000 | Loss: 0.00002567
Iteration 47/1000 | Loss: 0.00002567
Iteration 48/1000 | Loss: 0.00002647
Iteration 49/1000 | Loss: 0.00002557
Iteration 50/1000 | Loss: 0.00002548
Iteration 51/1000 | Loss: 0.00002603
Iteration 52/1000 | Loss: 0.00002557
Iteration 53/1000 | Loss: 0.00004632
Iteration 54/1000 | Loss: 0.00002626
Iteration 55/1000 | Loss: 0.00002529
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00002528
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002528
Iteration 60/1000 | Loss: 0.00002528
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002524
Iteration 63/1000 | Loss: 0.00002524
Iteration 64/1000 | Loss: 0.00002524
Iteration 65/1000 | Loss: 0.00002524
Iteration 66/1000 | Loss: 0.00002523
Iteration 67/1000 | Loss: 0.00002523
Iteration 68/1000 | Loss: 0.00002523
Iteration 69/1000 | Loss: 0.00002523
Iteration 70/1000 | Loss: 0.00003109
Iteration 71/1000 | Loss: 0.00003109
Iteration 72/1000 | Loss: 0.00002708
Iteration 73/1000 | Loss: 0.00002552
Iteration 74/1000 | Loss: 0.00002513
Iteration 75/1000 | Loss: 0.00002513
Iteration 76/1000 | Loss: 0.00002513
Iteration 77/1000 | Loss: 0.00002513
Iteration 78/1000 | Loss: 0.00002513
Iteration 79/1000 | Loss: 0.00002513
Iteration 80/1000 | Loss: 0.00002513
Iteration 81/1000 | Loss: 0.00002513
Iteration 82/1000 | Loss: 0.00002513
Iteration 83/1000 | Loss: 0.00002513
Iteration 84/1000 | Loss: 0.00002513
Iteration 85/1000 | Loss: 0.00002513
Iteration 86/1000 | Loss: 0.00002513
Iteration 87/1000 | Loss: 0.00002513
Iteration 88/1000 | Loss: 0.00002513
Iteration 89/1000 | Loss: 0.00002513
Iteration 90/1000 | Loss: 0.00002513
Iteration 91/1000 | Loss: 0.00002513
Iteration 92/1000 | Loss: 0.00002513
Iteration 93/1000 | Loss: 0.00002513
Iteration 94/1000 | Loss: 0.00002513
Iteration 95/1000 | Loss: 0.00002513
Iteration 96/1000 | Loss: 0.00002513
Iteration 97/1000 | Loss: 0.00002513
Iteration 98/1000 | Loss: 0.00002513
Iteration 99/1000 | Loss: 0.00002513
Iteration 100/1000 | Loss: 0.00002513
Iteration 101/1000 | Loss: 0.00002513
Iteration 102/1000 | Loss: 0.00002513
Iteration 103/1000 | Loss: 0.00002513
Iteration 104/1000 | Loss: 0.00002513
Iteration 105/1000 | Loss: 0.00002513
Iteration 106/1000 | Loss: 0.00002513
Iteration 107/1000 | Loss: 0.00002513
Iteration 108/1000 | Loss: 0.00002513
Iteration 109/1000 | Loss: 0.00002513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.512603168725036e-05, 2.512603168725036e-05, 2.512603168725036e-05, 2.512603168725036e-05, 2.512603168725036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.512603168725036e-05

Optimization complete. Final v2v error: 3.844534158706665 mm

Highest mean error: 21.909372329711914 mm for frame 4

Lowest mean error: 3.254131555557251 mm for frame 121

Saving results

Total time: 115.48454666137695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793121
Iteration 2/25 | Loss: 0.00179854
Iteration 3/25 | Loss: 0.00136092
Iteration 4/25 | Loss: 0.00129352
Iteration 5/25 | Loss: 0.00127594
Iteration 6/25 | Loss: 0.00127744
Iteration 7/25 | Loss: 0.00125480
Iteration 8/25 | Loss: 0.00124116
Iteration 9/25 | Loss: 0.00123937
Iteration 10/25 | Loss: 0.00123547
Iteration 11/25 | Loss: 0.00122448
Iteration 12/25 | Loss: 0.00121437
Iteration 13/25 | Loss: 0.00121565
Iteration 14/25 | Loss: 0.00121523
Iteration 15/25 | Loss: 0.00121968
Iteration 16/25 | Loss: 0.00121157
Iteration 17/25 | Loss: 0.00120664
Iteration 18/25 | Loss: 0.00120596
Iteration 19/25 | Loss: 0.00120577
Iteration 20/25 | Loss: 0.00120563
Iteration 21/25 | Loss: 0.00122095
Iteration 22/25 | Loss: 0.00120103
Iteration 23/25 | Loss: 0.00120290
Iteration 24/25 | Loss: 0.00119522
Iteration 25/25 | Loss: 0.00119361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.99696398
Iteration 2/25 | Loss: 0.00108626
Iteration 3/25 | Loss: 0.00108618
Iteration 4/25 | Loss: 0.00108618
Iteration 5/25 | Loss: 0.00108618
Iteration 6/25 | Loss: 0.00108618
Iteration 7/25 | Loss: 0.00108618
Iteration 8/25 | Loss: 0.00108618
Iteration 9/25 | Loss: 0.00108618
Iteration 10/25 | Loss: 0.00108618
Iteration 11/25 | Loss: 0.00108618
Iteration 12/25 | Loss: 0.00108618
Iteration 13/25 | Loss: 0.00108618
Iteration 14/25 | Loss: 0.00108618
Iteration 15/25 | Loss: 0.00108618
Iteration 16/25 | Loss: 0.00108618
Iteration 17/25 | Loss: 0.00108618
Iteration 18/25 | Loss: 0.00108618
Iteration 19/25 | Loss: 0.00108618
Iteration 20/25 | Loss: 0.00108618
Iteration 21/25 | Loss: 0.00108618
Iteration 22/25 | Loss: 0.00108617
Iteration 23/25 | Loss: 0.00108617
Iteration 24/25 | Loss: 0.00108617
Iteration 25/25 | Loss: 0.00108617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108617
Iteration 2/1000 | Loss: 0.00008227
Iteration 3/1000 | Loss: 0.00004271
Iteration 4/1000 | Loss: 0.00009751
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00006141
Iteration 7/1000 | Loss: 0.00002227
Iteration 8/1000 | Loss: 0.00004187
Iteration 9/1000 | Loss: 0.00002134
Iteration 10/1000 | Loss: 0.00002109
Iteration 11/1000 | Loss: 0.00003420
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002058
Iteration 14/1000 | Loss: 0.00002041
Iteration 15/1000 | Loss: 0.00054958
Iteration 16/1000 | Loss: 0.00002536
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002081
Iteration 19/1000 | Loss: 0.00005035
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00002869
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00001736
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00003392
Iteration 31/1000 | Loss: 0.00001711
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001690
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00004243
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001669
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001667
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001665
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001664
Iteration 74/1000 | Loss: 0.00001663
Iteration 75/1000 | Loss: 0.00001663
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001662
Iteration 80/1000 | Loss: 0.00001662
Iteration 81/1000 | Loss: 0.00001662
Iteration 82/1000 | Loss: 0.00001662
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001662
Iteration 87/1000 | Loss: 0.00001662
Iteration 88/1000 | Loss: 0.00001662
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001661
Iteration 93/1000 | Loss: 0.00001661
Iteration 94/1000 | Loss: 0.00001661
Iteration 95/1000 | Loss: 0.00001661
Iteration 96/1000 | Loss: 0.00001661
Iteration 97/1000 | Loss: 0.00001661
Iteration 98/1000 | Loss: 0.00001661
Iteration 99/1000 | Loss: 0.00001661
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00004156
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00002320
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001656
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001656
Iteration 127/1000 | Loss: 0.00001656
Iteration 128/1000 | Loss: 0.00001656
Iteration 129/1000 | Loss: 0.00001656
Iteration 130/1000 | Loss: 0.00001656
Iteration 131/1000 | Loss: 0.00001655
Iteration 132/1000 | Loss: 0.00001655
Iteration 133/1000 | Loss: 0.00001655
Iteration 134/1000 | Loss: 0.00002158
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001656
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.655544292589184e-05, 1.655544292589184e-05, 1.655544292589184e-05, 1.655544292589184e-05, 1.655544292589184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.655544292589184e-05

Optimization complete. Final v2v error: 3.397345781326294 mm

Highest mean error: 4.185544013977051 mm for frame 184

Lowest mean error: 3.0710151195526123 mm for frame 64

Saving results

Total time: 125.88147521018982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888098
Iteration 2/25 | Loss: 0.00127601
Iteration 3/25 | Loss: 0.00115332
Iteration 4/25 | Loss: 0.00114246
Iteration 5/25 | Loss: 0.00113952
Iteration 6/25 | Loss: 0.00113916
Iteration 7/25 | Loss: 0.00113916
Iteration 8/25 | Loss: 0.00113916
Iteration 9/25 | Loss: 0.00113916
Iteration 10/25 | Loss: 0.00113916
Iteration 11/25 | Loss: 0.00113916
Iteration 12/25 | Loss: 0.00113916
Iteration 13/25 | Loss: 0.00113916
Iteration 14/25 | Loss: 0.00113916
Iteration 15/25 | Loss: 0.00113916
Iteration 16/25 | Loss: 0.00113916
Iteration 17/25 | Loss: 0.00113916
Iteration 18/25 | Loss: 0.00113916
Iteration 19/25 | Loss: 0.00113916
Iteration 20/25 | Loss: 0.00113916
Iteration 21/25 | Loss: 0.00113916
Iteration 22/25 | Loss: 0.00113916
Iteration 23/25 | Loss: 0.00113916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011391573352739215, 0.0011391573352739215, 0.0011391573352739215, 0.0011391573352739215, 0.0011391573352739215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011391573352739215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20989585
Iteration 2/25 | Loss: 0.00140340
Iteration 3/25 | Loss: 0.00140338
Iteration 4/25 | Loss: 0.00140338
Iteration 5/25 | Loss: 0.00140338
Iteration 6/25 | Loss: 0.00140338
Iteration 7/25 | Loss: 0.00140338
Iteration 8/25 | Loss: 0.00140338
Iteration 9/25 | Loss: 0.00140338
Iteration 10/25 | Loss: 0.00140338
Iteration 11/25 | Loss: 0.00140338
Iteration 12/25 | Loss: 0.00140338
Iteration 13/25 | Loss: 0.00140338
Iteration 14/25 | Loss: 0.00140338
Iteration 15/25 | Loss: 0.00140338
Iteration 16/25 | Loss: 0.00140338
Iteration 17/25 | Loss: 0.00140338
Iteration 18/25 | Loss: 0.00140338
Iteration 19/25 | Loss: 0.00140338
Iteration 20/25 | Loss: 0.00140338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014033764600753784, 0.0014033764600753784, 0.0014033764600753784, 0.0014033764600753784, 0.0014033764600753784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014033764600753784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140338
Iteration 2/1000 | Loss: 0.00004640
Iteration 3/1000 | Loss: 0.00002386
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001364
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001343
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001323
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001317
Iteration 42/1000 | Loss: 0.00001317
Iteration 43/1000 | Loss: 0.00001316
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001315
Iteration 47/1000 | Loss: 0.00001315
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001314
Iteration 50/1000 | Loss: 0.00001314
Iteration 51/1000 | Loss: 0.00001313
Iteration 52/1000 | Loss: 0.00001313
Iteration 53/1000 | Loss: 0.00001313
Iteration 54/1000 | Loss: 0.00001312
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00001312
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001311
Iteration 61/1000 | Loss: 0.00001311
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001310
Iteration 65/1000 | Loss: 0.00001310
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001310
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001309
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001308
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001294
Iteration 91/1000 | Loss: 0.00001294
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001293
Iteration 96/1000 | Loss: 0.00001293
Iteration 97/1000 | Loss: 0.00001293
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001292
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001291
Iteration 106/1000 | Loss: 0.00001291
Iteration 107/1000 | Loss: 0.00001291
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001290
Iteration 118/1000 | Loss: 0.00001290
Iteration 119/1000 | Loss: 0.00001290
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001289
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001289
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001288
Iteration 166/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.288478870264953e-05, 1.288478870264953e-05, 1.288478870264953e-05, 1.288478870264953e-05, 1.288478870264953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.288478870264953e-05

Optimization complete. Final v2v error: 3.129181146621704 mm

Highest mean error: 3.5713918209075928 mm for frame 21

Lowest mean error: 2.9435765743255615 mm for frame 117

Saving results

Total time: 37.14780855178833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394064
Iteration 2/25 | Loss: 0.00127937
Iteration 3/25 | Loss: 0.00119767
Iteration 4/25 | Loss: 0.00118124
Iteration 5/25 | Loss: 0.00117476
Iteration 6/25 | Loss: 0.00117316
Iteration 7/25 | Loss: 0.00117292
Iteration 8/25 | Loss: 0.00117292
Iteration 9/25 | Loss: 0.00117292
Iteration 10/25 | Loss: 0.00117292
Iteration 11/25 | Loss: 0.00117292
Iteration 12/25 | Loss: 0.00117292
Iteration 13/25 | Loss: 0.00117292
Iteration 14/25 | Loss: 0.00117292
Iteration 15/25 | Loss: 0.00117292
Iteration 16/25 | Loss: 0.00117292
Iteration 17/25 | Loss: 0.00117292
Iteration 18/25 | Loss: 0.00117292
Iteration 19/25 | Loss: 0.00117292
Iteration 20/25 | Loss: 0.00117292
Iteration 21/25 | Loss: 0.00117292
Iteration 22/25 | Loss: 0.00117292
Iteration 23/25 | Loss: 0.00117292
Iteration 24/25 | Loss: 0.00117292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011729210382327437, 0.0011729210382327437, 0.0011729210382327437, 0.0011729210382327437, 0.0011729210382327437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011729210382327437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27467740
Iteration 2/25 | Loss: 0.00196639
Iteration 3/25 | Loss: 0.00196639
Iteration 4/25 | Loss: 0.00196639
Iteration 5/25 | Loss: 0.00196639
Iteration 6/25 | Loss: 0.00196639
Iteration 7/25 | Loss: 0.00196639
Iteration 8/25 | Loss: 0.00196639
Iteration 9/25 | Loss: 0.00196639
Iteration 10/25 | Loss: 0.00196639
Iteration 11/25 | Loss: 0.00196639
Iteration 12/25 | Loss: 0.00196639
Iteration 13/25 | Loss: 0.00196639
Iteration 14/25 | Loss: 0.00196639
Iteration 15/25 | Loss: 0.00196639
Iteration 16/25 | Loss: 0.00196639
Iteration 17/25 | Loss: 0.00196639
Iteration 18/25 | Loss: 0.00196639
Iteration 19/25 | Loss: 0.00196639
Iteration 20/25 | Loss: 0.00196639
Iteration 21/25 | Loss: 0.00196639
Iteration 22/25 | Loss: 0.00196639
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00196638866327703, 0.00196638866327703, 0.00196638866327703, 0.00196638866327703, 0.00196638866327703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00196638866327703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196639
Iteration 2/1000 | Loss: 0.00007374
Iteration 3/1000 | Loss: 0.00004079
Iteration 4/1000 | Loss: 0.00003194
Iteration 5/1000 | Loss: 0.00002707
Iteration 6/1000 | Loss: 0.00002536
Iteration 7/1000 | Loss: 0.00002400
Iteration 8/1000 | Loss: 0.00002326
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002199
Iteration 11/1000 | Loss: 0.00002171
Iteration 12/1000 | Loss: 0.00002167
Iteration 13/1000 | Loss: 0.00002149
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00002129
Iteration 16/1000 | Loss: 0.00002127
Iteration 17/1000 | Loss: 0.00002115
Iteration 18/1000 | Loss: 0.00002107
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002102
Iteration 21/1000 | Loss: 0.00002101
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002093
Iteration 25/1000 | Loss: 0.00002093
Iteration 26/1000 | Loss: 0.00002093
Iteration 27/1000 | Loss: 0.00002093
Iteration 28/1000 | Loss: 0.00002093
Iteration 29/1000 | Loss: 0.00002092
Iteration 30/1000 | Loss: 0.00002092
Iteration 31/1000 | Loss: 0.00002091
Iteration 32/1000 | Loss: 0.00002091
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002089
Iteration 36/1000 | Loss: 0.00002089
Iteration 37/1000 | Loss: 0.00002088
Iteration 38/1000 | Loss: 0.00002088
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002087
Iteration 41/1000 | Loss: 0.00002086
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002085
Iteration 44/1000 | Loss: 0.00002085
Iteration 45/1000 | Loss: 0.00002085
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002085
Iteration 48/1000 | Loss: 0.00002085
Iteration 49/1000 | Loss: 0.00002084
Iteration 50/1000 | Loss: 0.00002084
Iteration 51/1000 | Loss: 0.00002084
Iteration 52/1000 | Loss: 0.00002084
Iteration 53/1000 | Loss: 0.00002083
Iteration 54/1000 | Loss: 0.00002083
Iteration 55/1000 | Loss: 0.00002083
Iteration 56/1000 | Loss: 0.00002083
Iteration 57/1000 | Loss: 0.00002083
Iteration 58/1000 | Loss: 0.00002083
Iteration 59/1000 | Loss: 0.00002082
Iteration 60/1000 | Loss: 0.00002082
Iteration 61/1000 | Loss: 0.00002082
Iteration 62/1000 | Loss: 0.00002082
Iteration 63/1000 | Loss: 0.00002081
Iteration 64/1000 | Loss: 0.00002081
Iteration 65/1000 | Loss: 0.00002081
Iteration 66/1000 | Loss: 0.00002081
Iteration 67/1000 | Loss: 0.00002081
Iteration 68/1000 | Loss: 0.00002080
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002080
Iteration 71/1000 | Loss: 0.00002080
Iteration 72/1000 | Loss: 0.00002079
Iteration 73/1000 | Loss: 0.00002079
Iteration 74/1000 | Loss: 0.00002079
Iteration 75/1000 | Loss: 0.00002079
Iteration 76/1000 | Loss: 0.00002078
Iteration 77/1000 | Loss: 0.00002078
Iteration 78/1000 | Loss: 0.00002078
Iteration 79/1000 | Loss: 0.00002078
Iteration 80/1000 | Loss: 0.00002078
Iteration 81/1000 | Loss: 0.00002077
Iteration 82/1000 | Loss: 0.00002077
Iteration 83/1000 | Loss: 0.00002077
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002077
Iteration 87/1000 | Loss: 0.00002077
Iteration 88/1000 | Loss: 0.00002077
Iteration 89/1000 | Loss: 0.00002077
Iteration 90/1000 | Loss: 0.00002077
Iteration 91/1000 | Loss: 0.00002077
Iteration 92/1000 | Loss: 0.00002077
Iteration 93/1000 | Loss: 0.00002077
Iteration 94/1000 | Loss: 0.00002077
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.076532291539479e-05, 2.076532291539479e-05, 2.076532291539479e-05, 2.076532291539479e-05, 2.076532291539479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.076532291539479e-05

Optimization complete. Final v2v error: 3.800431966781616 mm

Highest mean error: 4.7719879150390625 mm for frame 75

Lowest mean error: 2.9433417320251465 mm for frame 119

Saving results

Total time: 36.702805280685425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01173829
Iteration 2/25 | Loss: 0.00205068
Iteration 3/25 | Loss: 0.00158257
Iteration 4/25 | Loss: 0.00135872
Iteration 5/25 | Loss: 0.00131144
Iteration 6/25 | Loss: 0.00128395
Iteration 7/25 | Loss: 0.00128973
Iteration 8/25 | Loss: 0.00127986
Iteration 9/25 | Loss: 0.00128971
Iteration 10/25 | Loss: 0.00127498
Iteration 11/25 | Loss: 0.00126729
Iteration 12/25 | Loss: 0.00126566
Iteration 13/25 | Loss: 0.00126508
Iteration 14/25 | Loss: 0.00126490
Iteration 15/25 | Loss: 0.00126480
Iteration 16/25 | Loss: 0.00126479
Iteration 17/25 | Loss: 0.00126479
Iteration 18/25 | Loss: 0.00126479
Iteration 19/25 | Loss: 0.00126479
Iteration 20/25 | Loss: 0.00126479
Iteration 21/25 | Loss: 0.00126479
Iteration 22/25 | Loss: 0.00126479
Iteration 23/25 | Loss: 0.00126479
Iteration 24/25 | Loss: 0.00126478
Iteration 25/25 | Loss: 0.00126478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76315236
Iteration 2/25 | Loss: 0.00106416
Iteration 3/25 | Loss: 0.00106416
Iteration 4/25 | Loss: 0.00106416
Iteration 5/25 | Loss: 0.00106416
Iteration 6/25 | Loss: 0.00106416
Iteration 7/25 | Loss: 0.00106416
Iteration 8/25 | Loss: 0.00106416
Iteration 9/25 | Loss: 0.00106416
Iteration 10/25 | Loss: 0.00106416
Iteration 11/25 | Loss: 0.00106416
Iteration 12/25 | Loss: 0.00106416
Iteration 13/25 | Loss: 0.00106416
Iteration 14/25 | Loss: 0.00106416
Iteration 15/25 | Loss: 0.00106416
Iteration 16/25 | Loss: 0.00106416
Iteration 17/25 | Loss: 0.00106416
Iteration 18/25 | Loss: 0.00106416
Iteration 19/25 | Loss: 0.00106416
Iteration 20/25 | Loss: 0.00106416
Iteration 21/25 | Loss: 0.00106416
Iteration 22/25 | Loss: 0.00106416
Iteration 23/25 | Loss: 0.00106416
Iteration 24/25 | Loss: 0.00106416
Iteration 25/25 | Loss: 0.00106416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106416
Iteration 2/1000 | Loss: 0.00010252
Iteration 3/1000 | Loss: 0.00007002
Iteration 4/1000 | Loss: 0.00005896
Iteration 5/1000 | Loss: 0.00019060
Iteration 6/1000 | Loss: 0.00005419
Iteration 7/1000 | Loss: 0.00005261
Iteration 8/1000 | Loss: 0.00051614
Iteration 9/1000 | Loss: 0.00005681
Iteration 10/1000 | Loss: 0.00005007
Iteration 11/1000 | Loss: 0.00004789
Iteration 12/1000 | Loss: 0.00004631
Iteration 13/1000 | Loss: 0.00004572
Iteration 14/1000 | Loss: 0.00017490
Iteration 15/1000 | Loss: 0.00004980
Iteration 16/1000 | Loss: 0.00004747
Iteration 17/1000 | Loss: 0.00004633
Iteration 18/1000 | Loss: 0.00004561
Iteration 19/1000 | Loss: 0.00004514
Iteration 20/1000 | Loss: 0.00004472
Iteration 21/1000 | Loss: 0.00004423
Iteration 22/1000 | Loss: 0.00004397
Iteration 23/1000 | Loss: 0.00004380
Iteration 24/1000 | Loss: 0.00004366
Iteration 25/1000 | Loss: 0.00004356
Iteration 26/1000 | Loss: 0.00004342
Iteration 27/1000 | Loss: 0.00004328
Iteration 28/1000 | Loss: 0.00004326
Iteration 29/1000 | Loss: 0.00004326
Iteration 30/1000 | Loss: 0.00004326
Iteration 31/1000 | Loss: 0.00004322
Iteration 32/1000 | Loss: 0.00004322
Iteration 33/1000 | Loss: 0.00004321
Iteration 34/1000 | Loss: 0.00004320
Iteration 35/1000 | Loss: 0.00004311
Iteration 36/1000 | Loss: 0.00004298
Iteration 37/1000 | Loss: 0.00004282
Iteration 38/1000 | Loss: 0.00004277
Iteration 39/1000 | Loss: 0.00004276
Iteration 40/1000 | Loss: 0.00004276
Iteration 41/1000 | Loss: 0.00004273
Iteration 42/1000 | Loss: 0.00004270
Iteration 43/1000 | Loss: 0.00004270
Iteration 44/1000 | Loss: 0.00004270
Iteration 45/1000 | Loss: 0.00004270
Iteration 46/1000 | Loss: 0.00004270
Iteration 47/1000 | Loss: 0.00004270
Iteration 48/1000 | Loss: 0.00004270
Iteration 49/1000 | Loss: 0.00004269
Iteration 50/1000 | Loss: 0.00004269
Iteration 51/1000 | Loss: 0.00004269
Iteration 52/1000 | Loss: 0.00004269
Iteration 53/1000 | Loss: 0.00004269
Iteration 54/1000 | Loss: 0.00004269
Iteration 55/1000 | Loss: 0.00004269
Iteration 56/1000 | Loss: 0.00004269
Iteration 57/1000 | Loss: 0.00004268
Iteration 58/1000 | Loss: 0.00004268
Iteration 59/1000 | Loss: 0.00004266
Iteration 60/1000 | Loss: 0.00004266
Iteration 61/1000 | Loss: 0.00004266
Iteration 62/1000 | Loss: 0.00004265
Iteration 63/1000 | Loss: 0.00004265
Iteration 64/1000 | Loss: 0.00004264
Iteration 65/1000 | Loss: 0.00004260
Iteration 66/1000 | Loss: 0.00004260
Iteration 67/1000 | Loss: 0.00004259
Iteration 68/1000 | Loss: 0.00004258
Iteration 69/1000 | Loss: 0.00004255
Iteration 70/1000 | Loss: 0.00004255
Iteration 71/1000 | Loss: 0.00004255
Iteration 72/1000 | Loss: 0.00004255
Iteration 73/1000 | Loss: 0.00004255
Iteration 74/1000 | Loss: 0.00004254
Iteration 75/1000 | Loss: 0.00004252
Iteration 76/1000 | Loss: 0.00004251
Iteration 77/1000 | Loss: 0.00004251
Iteration 78/1000 | Loss: 0.00004251
Iteration 79/1000 | Loss: 0.00004251
Iteration 80/1000 | Loss: 0.00004251
Iteration 81/1000 | Loss: 0.00004251
Iteration 82/1000 | Loss: 0.00004250
Iteration 83/1000 | Loss: 0.00004250
Iteration 84/1000 | Loss: 0.00004249
Iteration 85/1000 | Loss: 0.00004249
Iteration 86/1000 | Loss: 0.00004249
Iteration 87/1000 | Loss: 0.00004249
Iteration 88/1000 | Loss: 0.00004249
Iteration 89/1000 | Loss: 0.00004249
Iteration 90/1000 | Loss: 0.00004249
Iteration 91/1000 | Loss: 0.00004249
Iteration 92/1000 | Loss: 0.00004249
Iteration 93/1000 | Loss: 0.00004249
Iteration 94/1000 | Loss: 0.00004249
Iteration 95/1000 | Loss: 0.00004245
Iteration 96/1000 | Loss: 0.00004245
Iteration 97/1000 | Loss: 0.00004245
Iteration 98/1000 | Loss: 0.00004245
Iteration 99/1000 | Loss: 0.00004245
Iteration 100/1000 | Loss: 0.00004245
Iteration 101/1000 | Loss: 0.00004245
Iteration 102/1000 | Loss: 0.00004244
Iteration 103/1000 | Loss: 0.00004244
Iteration 104/1000 | Loss: 0.00004243
Iteration 105/1000 | Loss: 0.00004243
Iteration 106/1000 | Loss: 0.00004243
Iteration 107/1000 | Loss: 0.00004243
Iteration 108/1000 | Loss: 0.00004242
Iteration 109/1000 | Loss: 0.00004242
Iteration 110/1000 | Loss: 0.00004242
Iteration 111/1000 | Loss: 0.00004242
Iteration 112/1000 | Loss: 0.00004242
Iteration 113/1000 | Loss: 0.00004242
Iteration 114/1000 | Loss: 0.00004242
Iteration 115/1000 | Loss: 0.00004242
Iteration 116/1000 | Loss: 0.00004242
Iteration 117/1000 | Loss: 0.00004242
Iteration 118/1000 | Loss: 0.00004242
Iteration 119/1000 | Loss: 0.00004241
Iteration 120/1000 | Loss: 0.00004241
Iteration 121/1000 | Loss: 0.00004241
Iteration 122/1000 | Loss: 0.00004241
Iteration 123/1000 | Loss: 0.00004241
Iteration 124/1000 | Loss: 0.00004241
Iteration 125/1000 | Loss: 0.00004241
Iteration 126/1000 | Loss: 0.00004241
Iteration 127/1000 | Loss: 0.00004240
Iteration 128/1000 | Loss: 0.00004239
Iteration 129/1000 | Loss: 0.00004238
Iteration 130/1000 | Loss: 0.00004238
Iteration 131/1000 | Loss: 0.00004238
Iteration 132/1000 | Loss: 0.00004237
Iteration 133/1000 | Loss: 0.00004236
Iteration 134/1000 | Loss: 0.00004235
Iteration 135/1000 | Loss: 0.00004235
Iteration 136/1000 | Loss: 0.00004235
Iteration 137/1000 | Loss: 0.00004235
Iteration 138/1000 | Loss: 0.00004235
Iteration 139/1000 | Loss: 0.00004234
Iteration 140/1000 | Loss: 0.00004234
Iteration 141/1000 | Loss: 0.00004234
Iteration 142/1000 | Loss: 0.00004234
Iteration 143/1000 | Loss: 0.00004234
Iteration 144/1000 | Loss: 0.00004234
Iteration 145/1000 | Loss: 0.00004233
Iteration 146/1000 | Loss: 0.00004233
Iteration 147/1000 | Loss: 0.00004233
Iteration 148/1000 | Loss: 0.00004230
Iteration 149/1000 | Loss: 0.00004230
Iteration 150/1000 | Loss: 0.00004229
Iteration 151/1000 | Loss: 0.00004229
Iteration 152/1000 | Loss: 0.00004228
Iteration 153/1000 | Loss: 0.00004228
Iteration 154/1000 | Loss: 0.00004227
Iteration 155/1000 | Loss: 0.00004227
Iteration 156/1000 | Loss: 0.00004227
Iteration 157/1000 | Loss: 0.00004458
Iteration 158/1000 | Loss: 0.00004238
Iteration 159/1000 | Loss: 0.00004209
Iteration 160/1000 | Loss: 0.00004188
Iteration 161/1000 | Loss: 0.00004180
Iteration 162/1000 | Loss: 0.00004180
Iteration 163/1000 | Loss: 0.00004180
Iteration 164/1000 | Loss: 0.00004179
Iteration 165/1000 | Loss: 0.00004179
Iteration 166/1000 | Loss: 0.00004178
Iteration 167/1000 | Loss: 0.00004178
Iteration 168/1000 | Loss: 0.00004178
Iteration 169/1000 | Loss: 0.00004178
Iteration 170/1000 | Loss: 0.00004178
Iteration 171/1000 | Loss: 0.00004178
Iteration 172/1000 | Loss: 0.00004178
Iteration 173/1000 | Loss: 0.00004178
Iteration 174/1000 | Loss: 0.00004178
Iteration 175/1000 | Loss: 0.00004178
Iteration 176/1000 | Loss: 0.00004178
Iteration 177/1000 | Loss: 0.00004178
Iteration 178/1000 | Loss: 0.00004177
Iteration 179/1000 | Loss: 0.00004177
Iteration 180/1000 | Loss: 0.00004177
Iteration 181/1000 | Loss: 0.00004177
Iteration 182/1000 | Loss: 0.00004177
Iteration 183/1000 | Loss: 0.00004177
Iteration 184/1000 | Loss: 0.00004177
Iteration 185/1000 | Loss: 0.00004177
Iteration 186/1000 | Loss: 0.00004177
Iteration 187/1000 | Loss: 0.00004176
Iteration 188/1000 | Loss: 0.00004176
Iteration 189/1000 | Loss: 0.00004176
Iteration 190/1000 | Loss: 0.00004176
Iteration 191/1000 | Loss: 0.00004176
Iteration 192/1000 | Loss: 0.00004175
Iteration 193/1000 | Loss: 0.00004175
Iteration 194/1000 | Loss: 0.00004175
Iteration 195/1000 | Loss: 0.00004175
Iteration 196/1000 | Loss: 0.00004175
Iteration 197/1000 | Loss: 0.00004175
Iteration 198/1000 | Loss: 0.00004175
Iteration 199/1000 | Loss: 0.00004174
Iteration 200/1000 | Loss: 0.00004172
Iteration 201/1000 | Loss: 0.00004172
Iteration 202/1000 | Loss: 0.00004172
Iteration 203/1000 | Loss: 0.00004172
Iteration 204/1000 | Loss: 0.00004172
Iteration 205/1000 | Loss: 0.00004172
Iteration 206/1000 | Loss: 0.00004172
Iteration 207/1000 | Loss: 0.00004172
Iteration 208/1000 | Loss: 0.00004172
Iteration 209/1000 | Loss: 0.00004172
Iteration 210/1000 | Loss: 0.00004171
Iteration 211/1000 | Loss: 0.00004171
Iteration 212/1000 | Loss: 0.00004171
Iteration 213/1000 | Loss: 0.00004171
Iteration 214/1000 | Loss: 0.00004171
Iteration 215/1000 | Loss: 0.00004171
Iteration 216/1000 | Loss: 0.00004170
Iteration 217/1000 | Loss: 0.00004170
Iteration 218/1000 | Loss: 0.00004170
Iteration 219/1000 | Loss: 0.00004170
Iteration 220/1000 | Loss: 0.00004170
Iteration 221/1000 | Loss: 0.00004170
Iteration 222/1000 | Loss: 0.00004169
Iteration 223/1000 | Loss: 0.00004169
Iteration 224/1000 | Loss: 0.00004169
Iteration 225/1000 | Loss: 0.00004169
Iteration 226/1000 | Loss: 0.00004169
Iteration 227/1000 | Loss: 0.00004169
Iteration 228/1000 | Loss: 0.00004169
Iteration 229/1000 | Loss: 0.00004169
Iteration 230/1000 | Loss: 0.00004169
Iteration 231/1000 | Loss: 0.00004168
Iteration 232/1000 | Loss: 0.00004168
Iteration 233/1000 | Loss: 0.00004167
Iteration 234/1000 | Loss: 0.00004167
Iteration 235/1000 | Loss: 0.00004167
Iteration 236/1000 | Loss: 0.00004166
Iteration 237/1000 | Loss: 0.00004166
Iteration 238/1000 | Loss: 0.00004166
Iteration 239/1000 | Loss: 0.00004166
Iteration 240/1000 | Loss: 0.00004166
Iteration 241/1000 | Loss: 0.00004166
Iteration 242/1000 | Loss: 0.00004166
Iteration 243/1000 | Loss: 0.00004166
Iteration 244/1000 | Loss: 0.00004166
Iteration 245/1000 | Loss: 0.00004166
Iteration 246/1000 | Loss: 0.00004166
Iteration 247/1000 | Loss: 0.00004166
Iteration 248/1000 | Loss: 0.00004166
Iteration 249/1000 | Loss: 0.00004165
Iteration 250/1000 | Loss: 0.00004165
Iteration 251/1000 | Loss: 0.00004165
Iteration 252/1000 | Loss: 0.00004165
Iteration 253/1000 | Loss: 0.00004165
Iteration 254/1000 | Loss: 0.00004165
Iteration 255/1000 | Loss: 0.00004165
Iteration 256/1000 | Loss: 0.00004165
Iteration 257/1000 | Loss: 0.00004165
Iteration 258/1000 | Loss: 0.00004165
Iteration 259/1000 | Loss: 0.00004165
Iteration 260/1000 | Loss: 0.00004165
Iteration 261/1000 | Loss: 0.00004165
Iteration 262/1000 | Loss: 0.00004165
Iteration 263/1000 | Loss: 0.00004165
Iteration 264/1000 | Loss: 0.00004165
Iteration 265/1000 | Loss: 0.00004165
Iteration 266/1000 | Loss: 0.00004165
Iteration 267/1000 | Loss: 0.00004165
Iteration 268/1000 | Loss: 0.00004165
Iteration 269/1000 | Loss: 0.00004165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [4.1648512706160545e-05, 4.1648512706160545e-05, 4.1648512706160545e-05, 4.1648512706160545e-05, 4.1648512706160545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1648512706160545e-05

Optimization complete. Final v2v error: 5.166381359100342 mm

Highest mean error: 5.803342819213867 mm for frame 48

Lowest mean error: 4.293060779571533 mm for frame 137

Saving results

Total time: 132.38488960266113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047764
Iteration 2/25 | Loss: 0.00379549
Iteration 3/25 | Loss: 0.00215180
Iteration 4/25 | Loss: 0.00181114
Iteration 5/25 | Loss: 0.00171499
Iteration 6/25 | Loss: 0.00166907
Iteration 7/25 | Loss: 0.00159905
Iteration 8/25 | Loss: 0.00152879
Iteration 9/25 | Loss: 0.00146249
Iteration 10/25 | Loss: 0.00144953
Iteration 11/25 | Loss: 0.00141767
Iteration 12/25 | Loss: 0.00141092
Iteration 13/25 | Loss: 0.00138964
Iteration 14/25 | Loss: 0.00137651
Iteration 15/25 | Loss: 0.00136886
Iteration 16/25 | Loss: 0.00136617
Iteration 17/25 | Loss: 0.00136357
Iteration 18/25 | Loss: 0.00136191
Iteration 19/25 | Loss: 0.00136171
Iteration 20/25 | Loss: 0.00136013
Iteration 21/25 | Loss: 0.00136014
Iteration 22/25 | Loss: 0.00136073
Iteration 23/25 | Loss: 0.00136004
Iteration 24/25 | Loss: 0.00135978
Iteration 25/25 | Loss: 0.00135991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18449879
Iteration 2/25 | Loss: 0.00311603
Iteration 3/25 | Loss: 0.00287078
Iteration 4/25 | Loss: 0.00287078
Iteration 5/25 | Loss: 0.00287078
Iteration 6/25 | Loss: 0.00287078
Iteration 7/25 | Loss: 0.00287078
Iteration 8/25 | Loss: 0.00287078
Iteration 9/25 | Loss: 0.00287078
Iteration 10/25 | Loss: 0.00287078
Iteration 11/25 | Loss: 0.00287078
Iteration 12/25 | Loss: 0.00287078
Iteration 13/25 | Loss: 0.00287077
Iteration 14/25 | Loss: 0.00287077
Iteration 15/25 | Loss: 0.00287077
Iteration 16/25 | Loss: 0.00287077
Iteration 17/25 | Loss: 0.00287077
Iteration 18/25 | Loss: 0.00287077
Iteration 19/25 | Loss: 0.00287077
Iteration 20/25 | Loss: 0.00287077
Iteration 21/25 | Loss: 0.00287077
Iteration 22/25 | Loss: 0.00287077
Iteration 23/25 | Loss: 0.00287077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002870774595066905, 0.002870774595066905, 0.002870774595066905, 0.002870774595066905, 0.002870774595066905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002870774595066905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287077
Iteration 2/1000 | Loss: 0.00109733
Iteration 3/1000 | Loss: 0.00483368
Iteration 4/1000 | Loss: 0.00482702
Iteration 5/1000 | Loss: 0.00181272
Iteration 6/1000 | Loss: 0.00032701
Iteration 7/1000 | Loss: 0.00024343
Iteration 8/1000 | Loss: 0.00027142
Iteration 9/1000 | Loss: 0.00023631
Iteration 10/1000 | Loss: 0.00101514
Iteration 11/1000 | Loss: 0.00024711
Iteration 12/1000 | Loss: 0.00022814
Iteration 13/1000 | Loss: 0.00025959
Iteration 14/1000 | Loss: 0.00043687
Iteration 15/1000 | Loss: 0.00044121
Iteration 16/1000 | Loss: 0.00033105
Iteration 17/1000 | Loss: 0.00016410
Iteration 18/1000 | Loss: 0.00040072
Iteration 19/1000 | Loss: 0.00014783
Iteration 20/1000 | Loss: 0.00014628
Iteration 21/1000 | Loss: 0.00069791
Iteration 22/1000 | Loss: 0.00075239
Iteration 23/1000 | Loss: 0.00087733
Iteration 24/1000 | Loss: 0.00111325
Iteration 25/1000 | Loss: 0.00173018
Iteration 26/1000 | Loss: 0.00072408
Iteration 27/1000 | Loss: 0.00070937
Iteration 28/1000 | Loss: 0.00040668
Iteration 29/1000 | Loss: 0.00043229
Iteration 30/1000 | Loss: 0.00031520
Iteration 31/1000 | Loss: 0.00050584
Iteration 32/1000 | Loss: 0.00033399
Iteration 33/1000 | Loss: 0.00096066
Iteration 34/1000 | Loss: 0.00176304
Iteration 35/1000 | Loss: 0.00234088
Iteration 36/1000 | Loss: 0.00134970
Iteration 37/1000 | Loss: 0.00214210
Iteration 38/1000 | Loss: 0.00186131
Iteration 39/1000 | Loss: 0.00088965
Iteration 40/1000 | Loss: 0.00081564
Iteration 41/1000 | Loss: 0.00081048
Iteration 42/1000 | Loss: 0.00050927
Iteration 43/1000 | Loss: 0.00035871
Iteration 44/1000 | Loss: 0.00026325
Iteration 45/1000 | Loss: 0.00063177
Iteration 46/1000 | Loss: 0.00035297
Iteration 47/1000 | Loss: 0.00056434
Iteration 48/1000 | Loss: 0.00015724
Iteration 49/1000 | Loss: 0.00017749
Iteration 50/1000 | Loss: 0.00024911
Iteration 51/1000 | Loss: 0.00036626
Iteration 52/1000 | Loss: 0.00023530
Iteration 53/1000 | Loss: 0.00012339
Iteration 54/1000 | Loss: 0.00037250
Iteration 55/1000 | Loss: 0.00052191
Iteration 56/1000 | Loss: 0.00059773
Iteration 57/1000 | Loss: 0.00009531
Iteration 58/1000 | Loss: 0.00016079
Iteration 59/1000 | Loss: 0.00020024
Iteration 60/1000 | Loss: 0.00015971
Iteration 61/1000 | Loss: 0.00017585
Iteration 62/1000 | Loss: 0.00018926
Iteration 63/1000 | Loss: 0.00029415
Iteration 64/1000 | Loss: 0.00021984
Iteration 65/1000 | Loss: 0.00016820
Iteration 66/1000 | Loss: 0.00022226
Iteration 67/1000 | Loss: 0.00026460
Iteration 68/1000 | Loss: 0.00071384
Iteration 69/1000 | Loss: 0.00026012
Iteration 70/1000 | Loss: 0.00026058
Iteration 71/1000 | Loss: 0.00024027
Iteration 72/1000 | Loss: 0.00023890
Iteration 73/1000 | Loss: 0.00064618
Iteration 74/1000 | Loss: 0.00023911
Iteration 75/1000 | Loss: 0.00020523
Iteration 76/1000 | Loss: 0.00033208
Iteration 77/1000 | Loss: 0.00027119
Iteration 78/1000 | Loss: 0.00024898
Iteration 79/1000 | Loss: 0.00024476
Iteration 80/1000 | Loss: 0.00062214
Iteration 81/1000 | Loss: 0.00025526
Iteration 82/1000 | Loss: 0.00020151
Iteration 83/1000 | Loss: 0.00026870
Iteration 84/1000 | Loss: 0.00022342
Iteration 85/1000 | Loss: 0.00022247
Iteration 86/1000 | Loss: 0.00015010
Iteration 87/1000 | Loss: 0.00016763
Iteration 88/1000 | Loss: 0.00025275
Iteration 89/1000 | Loss: 0.00014349
Iteration 90/1000 | Loss: 0.00015716
Iteration 91/1000 | Loss: 0.00010381
Iteration 92/1000 | Loss: 0.00020809
Iteration 93/1000 | Loss: 0.00013614
Iteration 94/1000 | Loss: 0.00013324
Iteration 95/1000 | Loss: 0.00016128
Iteration 96/1000 | Loss: 0.00029466
Iteration 97/1000 | Loss: 0.00010362
Iteration 98/1000 | Loss: 0.00008452
Iteration 99/1000 | Loss: 0.00007747
Iteration 100/1000 | Loss: 0.00014577
Iteration 101/1000 | Loss: 0.00025335
Iteration 102/1000 | Loss: 0.00010774
Iteration 103/1000 | Loss: 0.00010069
Iteration 104/1000 | Loss: 0.00014943
Iteration 105/1000 | Loss: 0.00031049
Iteration 106/1000 | Loss: 0.00006712
Iteration 107/1000 | Loss: 0.00007653
Iteration 108/1000 | Loss: 0.00007641
Iteration 109/1000 | Loss: 0.00045558
Iteration 110/1000 | Loss: 0.00024042
Iteration 111/1000 | Loss: 0.00011263
Iteration 112/1000 | Loss: 0.00010560
Iteration 113/1000 | Loss: 0.00026969
Iteration 114/1000 | Loss: 0.00047554
Iteration 115/1000 | Loss: 0.00019237
Iteration 116/1000 | Loss: 0.00010539
Iteration 117/1000 | Loss: 0.00016322
Iteration 118/1000 | Loss: 0.00008578
Iteration 119/1000 | Loss: 0.00014349
Iteration 120/1000 | Loss: 0.00010147
Iteration 121/1000 | Loss: 0.00011401
Iteration 122/1000 | Loss: 0.00012301
Iteration 123/1000 | Loss: 0.00008755
Iteration 124/1000 | Loss: 0.00008412
Iteration 125/1000 | Loss: 0.00008418
Iteration 126/1000 | Loss: 0.00006158
Iteration 127/1000 | Loss: 0.00007449
Iteration 128/1000 | Loss: 0.00008919
Iteration 129/1000 | Loss: 0.00008635
Iteration 130/1000 | Loss: 0.00008202
Iteration 131/1000 | Loss: 0.00008863
Iteration 132/1000 | Loss: 0.00009311
Iteration 133/1000 | Loss: 0.00007335
Iteration 134/1000 | Loss: 0.00008257
Iteration 135/1000 | Loss: 0.00009013
Iteration 136/1000 | Loss: 0.00009778
Iteration 137/1000 | Loss: 0.00008827
Iteration 138/1000 | Loss: 0.00006931
Iteration 139/1000 | Loss: 0.00006669
Iteration 140/1000 | Loss: 0.00008394
Iteration 141/1000 | Loss: 0.00007400
Iteration 142/1000 | Loss: 0.00007418
Iteration 143/1000 | Loss: 0.00008171
Iteration 144/1000 | Loss: 0.00009746
Iteration 145/1000 | Loss: 0.00008031
Iteration 146/1000 | Loss: 0.00009028
Iteration 147/1000 | Loss: 0.00012835
Iteration 148/1000 | Loss: 0.00009302
Iteration 149/1000 | Loss: 0.00008520
Iteration 150/1000 | Loss: 0.00008946
Iteration 151/1000 | Loss: 0.00020651
Iteration 152/1000 | Loss: 0.00008874
Iteration 153/1000 | Loss: 0.00005717
Iteration 154/1000 | Loss: 0.00009697
Iteration 155/1000 | Loss: 0.00008258
Iteration 156/1000 | Loss: 0.00012157
Iteration 157/1000 | Loss: 0.00007095
Iteration 158/1000 | Loss: 0.00023150
Iteration 159/1000 | Loss: 0.00008008
Iteration 160/1000 | Loss: 0.00008854
Iteration 161/1000 | Loss: 0.00008762
Iteration 162/1000 | Loss: 0.00007528
Iteration 163/1000 | Loss: 0.00008116
Iteration 164/1000 | Loss: 0.00007596
Iteration 165/1000 | Loss: 0.00012735
Iteration 166/1000 | Loss: 0.00010360
Iteration 167/1000 | Loss: 0.00006712
Iteration 168/1000 | Loss: 0.00007045
Iteration 169/1000 | Loss: 0.00008919
Iteration 170/1000 | Loss: 0.00008696
Iteration 171/1000 | Loss: 0.00009708
Iteration 172/1000 | Loss: 0.00009659
Iteration 173/1000 | Loss: 0.00009664
Iteration 174/1000 | Loss: 0.00024204
Iteration 175/1000 | Loss: 0.00031106
Iteration 176/1000 | Loss: 0.00026871
Iteration 177/1000 | Loss: 0.00047177
Iteration 178/1000 | Loss: 0.00019532
Iteration 179/1000 | Loss: 0.00005414
Iteration 180/1000 | Loss: 0.00006498
Iteration 181/1000 | Loss: 0.00006511
Iteration 182/1000 | Loss: 0.00005820
Iteration 183/1000 | Loss: 0.00005618
Iteration 184/1000 | Loss: 0.00006377
Iteration 185/1000 | Loss: 0.00006051
Iteration 186/1000 | Loss: 0.00006306
Iteration 187/1000 | Loss: 0.00006127
Iteration 188/1000 | Loss: 0.00005535
Iteration 189/1000 | Loss: 0.00005906
Iteration 190/1000 | Loss: 0.00007676
Iteration 191/1000 | Loss: 0.00006868
Iteration 192/1000 | Loss: 0.00005179
Iteration 193/1000 | Loss: 0.00006262
Iteration 194/1000 | Loss: 0.00006748
Iteration 195/1000 | Loss: 0.00006602
Iteration 196/1000 | Loss: 0.00007049
Iteration 197/1000 | Loss: 0.00007283
Iteration 198/1000 | Loss: 0.00006897
Iteration 199/1000 | Loss: 0.00007352
Iteration 200/1000 | Loss: 0.00008312
Iteration 201/1000 | Loss: 0.00006740
Iteration 202/1000 | Loss: 0.00005950
Iteration 203/1000 | Loss: 0.00007751
Iteration 204/1000 | Loss: 0.00005287
Iteration 205/1000 | Loss: 0.00005324
Iteration 206/1000 | Loss: 0.00006319
Iteration 207/1000 | Loss: 0.00006432
Iteration 208/1000 | Loss: 0.00006536
Iteration 209/1000 | Loss: 0.00004782
Iteration 210/1000 | Loss: 0.00004879
Iteration 211/1000 | Loss: 0.00005572
Iteration 212/1000 | Loss: 0.00005622
Iteration 213/1000 | Loss: 0.00008112
Iteration 214/1000 | Loss: 0.00005957
Iteration 215/1000 | Loss: 0.00006183
Iteration 216/1000 | Loss: 0.00004423
Iteration 217/1000 | Loss: 0.00003586
Iteration 218/1000 | Loss: 0.00003305
Iteration 219/1000 | Loss: 0.00003111
Iteration 220/1000 | Loss: 0.00002965
Iteration 221/1000 | Loss: 0.00002863
Iteration 222/1000 | Loss: 0.00003036
Iteration 223/1000 | Loss: 0.00003120
Iteration 224/1000 | Loss: 0.00002641
Iteration 225/1000 | Loss: 0.00002593
Iteration 226/1000 | Loss: 0.00002559
Iteration 227/1000 | Loss: 0.00002528
Iteration 228/1000 | Loss: 0.00002504
Iteration 229/1000 | Loss: 0.00002476
Iteration 230/1000 | Loss: 0.00002464
Iteration 231/1000 | Loss: 0.00002463
Iteration 232/1000 | Loss: 0.00013371
Iteration 233/1000 | Loss: 0.00003478
Iteration 234/1000 | Loss: 0.00003117
Iteration 235/1000 | Loss: 0.00002874
Iteration 236/1000 | Loss: 0.00002785
Iteration 237/1000 | Loss: 0.00002674
Iteration 238/1000 | Loss: 0.00002610
Iteration 239/1000 | Loss: 0.00002578
Iteration 240/1000 | Loss: 0.00002566
Iteration 241/1000 | Loss: 0.00002565
Iteration 242/1000 | Loss: 0.00002565
Iteration 243/1000 | Loss: 0.00002563
Iteration 244/1000 | Loss: 0.00002562
Iteration 245/1000 | Loss: 0.00002561
Iteration 246/1000 | Loss: 0.00002556
Iteration 247/1000 | Loss: 0.00002546
Iteration 248/1000 | Loss: 0.00002545
Iteration 249/1000 | Loss: 0.00002544
Iteration 250/1000 | Loss: 0.00002544
Iteration 251/1000 | Loss: 0.00002544
Iteration 252/1000 | Loss: 0.00002543
Iteration 253/1000 | Loss: 0.00002543
Iteration 254/1000 | Loss: 0.00002538
Iteration 255/1000 | Loss: 0.00002537
Iteration 256/1000 | Loss: 0.00002536
Iteration 257/1000 | Loss: 0.00002528
Iteration 258/1000 | Loss: 0.00002528
Iteration 259/1000 | Loss: 0.00002528
Iteration 260/1000 | Loss: 0.00002527
Iteration 261/1000 | Loss: 0.00002527
Iteration 262/1000 | Loss: 0.00002526
Iteration 263/1000 | Loss: 0.00002526
Iteration 264/1000 | Loss: 0.00002526
Iteration 265/1000 | Loss: 0.00002525
Iteration 266/1000 | Loss: 0.00002524
Iteration 267/1000 | Loss: 0.00002523
Iteration 268/1000 | Loss: 0.00002523
Iteration 269/1000 | Loss: 0.00002522
Iteration 270/1000 | Loss: 0.00002520
Iteration 271/1000 | Loss: 0.00002519
Iteration 272/1000 | Loss: 0.00002519
Iteration 273/1000 | Loss: 0.00002518
Iteration 274/1000 | Loss: 0.00002518
Iteration 275/1000 | Loss: 0.00002515
Iteration 276/1000 | Loss: 0.00002507
Iteration 277/1000 | Loss: 0.00002504
Iteration 278/1000 | Loss: 0.00002503
Iteration 279/1000 | Loss: 0.00002503
Iteration 280/1000 | Loss: 0.00002503
Iteration 281/1000 | Loss: 0.00002502
Iteration 282/1000 | Loss: 0.00002501
Iteration 283/1000 | Loss: 0.00002500
Iteration 284/1000 | Loss: 0.00002500
Iteration 285/1000 | Loss: 0.00002499
Iteration 286/1000 | Loss: 0.00002499
Iteration 287/1000 | Loss: 0.00002499
Iteration 288/1000 | Loss: 0.00002499
Iteration 289/1000 | Loss: 0.00002498
Iteration 290/1000 | Loss: 0.00002497
Iteration 291/1000 | Loss: 0.00002496
Iteration 292/1000 | Loss: 0.00002494
Iteration 293/1000 | Loss: 0.00002491
Iteration 294/1000 | Loss: 0.00002567
Iteration 295/1000 | Loss: 0.00002566
Iteration 296/1000 | Loss: 0.00002545
Iteration 297/1000 | Loss: 0.00039005
Iteration 298/1000 | Loss: 0.00018687
Iteration 299/1000 | Loss: 0.00003819
Iteration 300/1000 | Loss: 0.00022815
Iteration 301/1000 | Loss: 0.00004525
Iteration 302/1000 | Loss: 0.00003395
Iteration 303/1000 | Loss: 0.00002922
Iteration 304/1000 | Loss: 0.00002736
Iteration 305/1000 | Loss: 0.00003465
Iteration 306/1000 | Loss: 0.00002639
Iteration 307/1000 | Loss: 0.00003012
Iteration 308/1000 | Loss: 0.00002560
Iteration 309/1000 | Loss: 0.00002519
Iteration 310/1000 | Loss: 0.00002579
Iteration 311/1000 | Loss: 0.00002487
Iteration 312/1000 | Loss: 0.00002471
Iteration 313/1000 | Loss: 0.00002469
Iteration 314/1000 | Loss: 0.00002468
Iteration 315/1000 | Loss: 0.00002467
Iteration 316/1000 | Loss: 0.00002466
Iteration 317/1000 | Loss: 0.00016790
Iteration 318/1000 | Loss: 0.00009434
Iteration 319/1000 | Loss: 0.00002588
Iteration 320/1000 | Loss: 0.00016725
Iteration 321/1000 | Loss: 0.00004764
Iteration 322/1000 | Loss: 0.00002970
Iteration 323/1000 | Loss: 0.00019772
Iteration 324/1000 | Loss: 0.00015205
Iteration 325/1000 | Loss: 0.00016550
Iteration 326/1000 | Loss: 0.00011753
Iteration 327/1000 | Loss: 0.00002647
Iteration 328/1000 | Loss: 0.00017177
Iteration 329/1000 | Loss: 0.00010549
Iteration 330/1000 | Loss: 0.00004447
Iteration 331/1000 | Loss: 0.00010924
Iteration 332/1000 | Loss: 0.00003512
Iteration 333/1000 | Loss: 0.00003047
Iteration 334/1000 | Loss: 0.00002634
Iteration 335/1000 | Loss: 0.00002466
Iteration 336/1000 | Loss: 0.00002551
Iteration 337/1000 | Loss: 0.00002736
Iteration 338/1000 | Loss: 0.00002399
Iteration 339/1000 | Loss: 0.00002390
Iteration 340/1000 | Loss: 0.00002954
Iteration 341/1000 | Loss: 0.00002383
Iteration 342/1000 | Loss: 0.00002382
Iteration 343/1000 | Loss: 0.00002382
Iteration 344/1000 | Loss: 0.00002382
Iteration 345/1000 | Loss: 0.00002382
Iteration 346/1000 | Loss: 0.00002381
Iteration 347/1000 | Loss: 0.00002381
Iteration 348/1000 | Loss: 0.00002381
Iteration 349/1000 | Loss: 0.00002381
Iteration 350/1000 | Loss: 0.00002381
Iteration 351/1000 | Loss: 0.00002381
Iteration 352/1000 | Loss: 0.00002381
Iteration 353/1000 | Loss: 0.00002381
Iteration 354/1000 | Loss: 0.00002381
Iteration 355/1000 | Loss: 0.00002380
Iteration 356/1000 | Loss: 0.00002380
Iteration 357/1000 | Loss: 0.00002667
Iteration 358/1000 | Loss: 0.00002620
Iteration 359/1000 | Loss: 0.00002826
Iteration 360/1000 | Loss: 0.00002387
Iteration 361/1000 | Loss: 0.00002378
Iteration 362/1000 | Loss: 0.00002378
Iteration 363/1000 | Loss: 0.00002378
Iteration 364/1000 | Loss: 0.00002378
Iteration 365/1000 | Loss: 0.00002378
Iteration 366/1000 | Loss: 0.00002377
Iteration 367/1000 | Loss: 0.00002376
Iteration 368/1000 | Loss: 0.00002376
Iteration 369/1000 | Loss: 0.00002376
Iteration 370/1000 | Loss: 0.00002374
Iteration 371/1000 | Loss: 0.00002373
Iteration 372/1000 | Loss: 0.00002373
Iteration 373/1000 | Loss: 0.00002373
Iteration 374/1000 | Loss: 0.00002480
Iteration 375/1000 | Loss: 0.00002480
Iteration 376/1000 | Loss: 0.00002380
Iteration 377/1000 | Loss: 0.00002368
Iteration 378/1000 | Loss: 0.00002368
Iteration 379/1000 | Loss: 0.00002368
Iteration 380/1000 | Loss: 0.00002368
Iteration 381/1000 | Loss: 0.00002368
Iteration 382/1000 | Loss: 0.00002368
Iteration 383/1000 | Loss: 0.00002368
Iteration 384/1000 | Loss: 0.00002368
Iteration 385/1000 | Loss: 0.00002368
Iteration 386/1000 | Loss: 0.00002368
Iteration 387/1000 | Loss: 0.00002368
Iteration 388/1000 | Loss: 0.00002367
Iteration 389/1000 | Loss: 0.00002367
Iteration 390/1000 | Loss: 0.00002367
Iteration 391/1000 | Loss: 0.00002367
Iteration 392/1000 | Loss: 0.00002367
Iteration 393/1000 | Loss: 0.00002367
Iteration 394/1000 | Loss: 0.00002367
Iteration 395/1000 | Loss: 0.00002367
Iteration 396/1000 | Loss: 0.00002367
Iteration 397/1000 | Loss: 0.00002367
Iteration 398/1000 | Loss: 0.00002367
Iteration 399/1000 | Loss: 0.00002367
Iteration 400/1000 | Loss: 0.00002367
Iteration 401/1000 | Loss: 0.00002367
Iteration 402/1000 | Loss: 0.00002367
Iteration 403/1000 | Loss: 0.00002367
Iteration 404/1000 | Loss: 0.00002367
Iteration 405/1000 | Loss: 0.00002367
Iteration 406/1000 | Loss: 0.00002367
Iteration 407/1000 | Loss: 0.00002367
Iteration 408/1000 | Loss: 0.00002367
Iteration 409/1000 | Loss: 0.00002367
Iteration 410/1000 | Loss: 0.00002367
Iteration 411/1000 | Loss: 0.00002367
Iteration 412/1000 | Loss: 0.00002367
Iteration 413/1000 | Loss: 0.00002367
Iteration 414/1000 | Loss: 0.00002367
Iteration 415/1000 | Loss: 0.00002367
Iteration 416/1000 | Loss: 0.00002367
Iteration 417/1000 | Loss: 0.00002367
Iteration 418/1000 | Loss: 0.00002367
Iteration 419/1000 | Loss: 0.00002367
Iteration 420/1000 | Loss: 0.00002367
Iteration 421/1000 | Loss: 0.00002367
Iteration 422/1000 | Loss: 0.00002367
Iteration 423/1000 | Loss: 0.00002367
Iteration 424/1000 | Loss: 0.00002367
Iteration 425/1000 | Loss: 0.00002367
Iteration 426/1000 | Loss: 0.00002367
Iteration 427/1000 | Loss: 0.00002367
Iteration 428/1000 | Loss: 0.00002367
Iteration 429/1000 | Loss: 0.00002367
Iteration 430/1000 | Loss: 0.00002367
Iteration 431/1000 | Loss: 0.00002367
Iteration 432/1000 | Loss: 0.00002367
Iteration 433/1000 | Loss: 0.00002367
Iteration 434/1000 | Loss: 0.00002367
Iteration 435/1000 | Loss: 0.00002367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 435. Stopping optimization.
Last 5 losses: [2.3666852939641103e-05, 2.3666852939641103e-05, 2.3666852939641103e-05, 2.3666852939641103e-05, 2.3666852939641103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3666852939641103e-05

Optimization complete. Final v2v error: 3.471282482147217 mm

Highest mean error: 19.764917373657227 mm for frame 195

Lowest mean error: 2.8865458965301514 mm for frame 239

Saving results

Total time: 617.7177884578705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2833/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2833/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860547
Iteration 2/25 | Loss: 0.00139540
Iteration 3/25 | Loss: 0.00125815
Iteration 4/25 | Loss: 0.00121989
Iteration 5/25 | Loss: 0.00120680
Iteration 6/25 | Loss: 0.00120371
Iteration 7/25 | Loss: 0.00120260
Iteration 8/25 | Loss: 0.00120247
Iteration 9/25 | Loss: 0.00120247
Iteration 10/25 | Loss: 0.00120247
Iteration 11/25 | Loss: 0.00120247
Iteration 12/25 | Loss: 0.00120247
Iteration 13/25 | Loss: 0.00120247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012024701572954655, 0.0012024701572954655, 0.0012024701572954655, 0.0012024701572954655, 0.0012024701572954655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012024701572954655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25391901
Iteration 2/25 | Loss: 0.00232563
Iteration 3/25 | Loss: 0.00232563
Iteration 4/25 | Loss: 0.00232563
Iteration 5/25 | Loss: 0.00232563
Iteration 6/25 | Loss: 0.00232563
Iteration 7/25 | Loss: 0.00232562
Iteration 8/25 | Loss: 0.00232562
Iteration 9/25 | Loss: 0.00232562
Iteration 10/25 | Loss: 0.00232562
Iteration 11/25 | Loss: 0.00232563
Iteration 12/25 | Loss: 0.00232562
Iteration 13/25 | Loss: 0.00232562
Iteration 14/25 | Loss: 0.00232562
Iteration 15/25 | Loss: 0.00232562
Iteration 16/25 | Loss: 0.00232562
Iteration 17/25 | Loss: 0.00232562
Iteration 18/25 | Loss: 0.00232562
Iteration 19/25 | Loss: 0.00232562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00232562399469316, 0.00232562399469316, 0.00232562399469316, 0.00232562399469316, 0.00232562399469316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00232562399469316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232562
Iteration 2/1000 | Loss: 0.00007688
Iteration 3/1000 | Loss: 0.00004071
Iteration 4/1000 | Loss: 0.00003247
Iteration 5/1000 | Loss: 0.00002881
Iteration 6/1000 | Loss: 0.00002754
Iteration 7/1000 | Loss: 0.00002659
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002516
Iteration 10/1000 | Loss: 0.00002487
Iteration 11/1000 | Loss: 0.00002450
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00002408
Iteration 14/1000 | Loss: 0.00002399
Iteration 15/1000 | Loss: 0.00002395
Iteration 16/1000 | Loss: 0.00002386
Iteration 17/1000 | Loss: 0.00002380
Iteration 18/1000 | Loss: 0.00002379
Iteration 19/1000 | Loss: 0.00002378
Iteration 20/1000 | Loss: 0.00002378
Iteration 21/1000 | Loss: 0.00002378
Iteration 22/1000 | Loss: 0.00002374
Iteration 23/1000 | Loss: 0.00002369
Iteration 24/1000 | Loss: 0.00002369
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002369
Iteration 27/1000 | Loss: 0.00002368
Iteration 28/1000 | Loss: 0.00002368
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002368
Iteration 32/1000 | Loss: 0.00002368
Iteration 33/1000 | Loss: 0.00002368
Iteration 34/1000 | Loss: 0.00002368
Iteration 35/1000 | Loss: 0.00002367
Iteration 36/1000 | Loss: 0.00002367
Iteration 37/1000 | Loss: 0.00002364
Iteration 38/1000 | Loss: 0.00002364
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002364
Iteration 43/1000 | Loss: 0.00002364
Iteration 44/1000 | Loss: 0.00002364
Iteration 45/1000 | Loss: 0.00002364
Iteration 46/1000 | Loss: 0.00002364
Iteration 47/1000 | Loss: 0.00002363
Iteration 48/1000 | Loss: 0.00002363
Iteration 49/1000 | Loss: 0.00002363
Iteration 50/1000 | Loss: 0.00002363
Iteration 51/1000 | Loss: 0.00002362
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002360
Iteration 54/1000 | Loss: 0.00002360
Iteration 55/1000 | Loss: 0.00002360
Iteration 56/1000 | Loss: 0.00002357
Iteration 57/1000 | Loss: 0.00002357
Iteration 58/1000 | Loss: 0.00002357
Iteration 59/1000 | Loss: 0.00002356
Iteration 60/1000 | Loss: 0.00002356
Iteration 61/1000 | Loss: 0.00002355
Iteration 62/1000 | Loss: 0.00002355
Iteration 63/1000 | Loss: 0.00002355
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002351
Iteration 70/1000 | Loss: 0.00002351
Iteration 71/1000 | Loss: 0.00002351
Iteration 72/1000 | Loss: 0.00002350
Iteration 73/1000 | Loss: 0.00002350
Iteration 74/1000 | Loss: 0.00002348
Iteration 75/1000 | Loss: 0.00002348
Iteration 76/1000 | Loss: 0.00002347
Iteration 77/1000 | Loss: 0.00002347
Iteration 78/1000 | Loss: 0.00002347
Iteration 79/1000 | Loss: 0.00002346
Iteration 80/1000 | Loss: 0.00002345
Iteration 81/1000 | Loss: 0.00002344
Iteration 82/1000 | Loss: 0.00002344
Iteration 83/1000 | Loss: 0.00002343
Iteration 84/1000 | Loss: 0.00002343
Iteration 85/1000 | Loss: 0.00002342
Iteration 86/1000 | Loss: 0.00002342
Iteration 87/1000 | Loss: 0.00002342
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002340
Iteration 93/1000 | Loss: 0.00002340
Iteration 94/1000 | Loss: 0.00002340
Iteration 95/1000 | Loss: 0.00002339
Iteration 96/1000 | Loss: 0.00002339
Iteration 97/1000 | Loss: 0.00002339
Iteration 98/1000 | Loss: 0.00002338
Iteration 99/1000 | Loss: 0.00002338
Iteration 100/1000 | Loss: 0.00002338
Iteration 101/1000 | Loss: 0.00002338
Iteration 102/1000 | Loss: 0.00002338
Iteration 103/1000 | Loss: 0.00002338
Iteration 104/1000 | Loss: 0.00002338
Iteration 105/1000 | Loss: 0.00002337
Iteration 106/1000 | Loss: 0.00002337
Iteration 107/1000 | Loss: 0.00002337
Iteration 108/1000 | Loss: 0.00002336
Iteration 109/1000 | Loss: 0.00002336
Iteration 110/1000 | Loss: 0.00002336
Iteration 111/1000 | Loss: 0.00002336
Iteration 112/1000 | Loss: 0.00002336
Iteration 113/1000 | Loss: 0.00002336
Iteration 114/1000 | Loss: 0.00002336
Iteration 115/1000 | Loss: 0.00002335
Iteration 116/1000 | Loss: 0.00002335
Iteration 117/1000 | Loss: 0.00002335
Iteration 118/1000 | Loss: 0.00002335
Iteration 119/1000 | Loss: 0.00002335
Iteration 120/1000 | Loss: 0.00002334
Iteration 121/1000 | Loss: 0.00002334
Iteration 122/1000 | Loss: 0.00002334
Iteration 123/1000 | Loss: 0.00002334
Iteration 124/1000 | Loss: 0.00002334
Iteration 125/1000 | Loss: 0.00002334
Iteration 126/1000 | Loss: 0.00002334
Iteration 127/1000 | Loss: 0.00002333
Iteration 128/1000 | Loss: 0.00002333
Iteration 129/1000 | Loss: 0.00002333
Iteration 130/1000 | Loss: 0.00002333
Iteration 131/1000 | Loss: 0.00002333
Iteration 132/1000 | Loss: 0.00002333
Iteration 133/1000 | Loss: 0.00002333
Iteration 134/1000 | Loss: 0.00002333
Iteration 135/1000 | Loss: 0.00002333
Iteration 136/1000 | Loss: 0.00002333
Iteration 137/1000 | Loss: 0.00002333
Iteration 138/1000 | Loss: 0.00002332
Iteration 139/1000 | Loss: 0.00002332
Iteration 140/1000 | Loss: 0.00002332
Iteration 141/1000 | Loss: 0.00002332
Iteration 142/1000 | Loss: 0.00002331
Iteration 143/1000 | Loss: 0.00002331
Iteration 144/1000 | Loss: 0.00002331
Iteration 145/1000 | Loss: 0.00002331
Iteration 146/1000 | Loss: 0.00002330
Iteration 147/1000 | Loss: 0.00002330
Iteration 148/1000 | Loss: 0.00002330
Iteration 149/1000 | Loss: 0.00002330
Iteration 150/1000 | Loss: 0.00002330
Iteration 151/1000 | Loss: 0.00002330
Iteration 152/1000 | Loss: 0.00002330
Iteration 153/1000 | Loss: 0.00002330
Iteration 154/1000 | Loss: 0.00002330
Iteration 155/1000 | Loss: 0.00002330
Iteration 156/1000 | Loss: 0.00002330
Iteration 157/1000 | Loss: 0.00002330
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002329
Iteration 160/1000 | Loss: 0.00002329
Iteration 161/1000 | Loss: 0.00002329
Iteration 162/1000 | Loss: 0.00002329
Iteration 163/1000 | Loss: 0.00002329
Iteration 164/1000 | Loss: 0.00002329
Iteration 165/1000 | Loss: 0.00002329
Iteration 166/1000 | Loss: 0.00002329
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002328
Iteration 169/1000 | Loss: 0.00002328
Iteration 170/1000 | Loss: 0.00002328
Iteration 171/1000 | Loss: 0.00002328
Iteration 172/1000 | Loss: 0.00002328
Iteration 173/1000 | Loss: 0.00002328
Iteration 174/1000 | Loss: 0.00002328
Iteration 175/1000 | Loss: 0.00002328
Iteration 176/1000 | Loss: 0.00002328
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002328
Iteration 180/1000 | Loss: 0.00002328
Iteration 181/1000 | Loss: 0.00002327
Iteration 182/1000 | Loss: 0.00002327
Iteration 183/1000 | Loss: 0.00002327
Iteration 184/1000 | Loss: 0.00002327
Iteration 185/1000 | Loss: 0.00002327
Iteration 186/1000 | Loss: 0.00002327
Iteration 187/1000 | Loss: 0.00002327
Iteration 188/1000 | Loss: 0.00002327
Iteration 189/1000 | Loss: 0.00002327
Iteration 190/1000 | Loss: 0.00002327
Iteration 191/1000 | Loss: 0.00002326
Iteration 192/1000 | Loss: 0.00002326
Iteration 193/1000 | Loss: 0.00002326
Iteration 194/1000 | Loss: 0.00002326
Iteration 195/1000 | Loss: 0.00002326
Iteration 196/1000 | Loss: 0.00002326
Iteration 197/1000 | Loss: 0.00002326
Iteration 198/1000 | Loss: 0.00002326
Iteration 199/1000 | Loss: 0.00002325
Iteration 200/1000 | Loss: 0.00002325
Iteration 201/1000 | Loss: 0.00002325
Iteration 202/1000 | Loss: 0.00002325
Iteration 203/1000 | Loss: 0.00002324
Iteration 204/1000 | Loss: 0.00002324
Iteration 205/1000 | Loss: 0.00002324
Iteration 206/1000 | Loss: 0.00002324
Iteration 207/1000 | Loss: 0.00002324
Iteration 208/1000 | Loss: 0.00002324
Iteration 209/1000 | Loss: 0.00002324
Iteration 210/1000 | Loss: 0.00002323
Iteration 211/1000 | Loss: 0.00002323
Iteration 212/1000 | Loss: 0.00002323
Iteration 213/1000 | Loss: 0.00002323
Iteration 214/1000 | Loss: 0.00002323
Iteration 215/1000 | Loss: 0.00002323
Iteration 216/1000 | Loss: 0.00002322
Iteration 217/1000 | Loss: 0.00002322
Iteration 218/1000 | Loss: 0.00002322
Iteration 219/1000 | Loss: 0.00002322
Iteration 220/1000 | Loss: 0.00002322
Iteration 221/1000 | Loss: 0.00002322
Iteration 222/1000 | Loss: 0.00002322
Iteration 223/1000 | Loss: 0.00002322
Iteration 224/1000 | Loss: 0.00002322
Iteration 225/1000 | Loss: 0.00002322
Iteration 226/1000 | Loss: 0.00002322
Iteration 227/1000 | Loss: 0.00002322
Iteration 228/1000 | Loss: 0.00002322
Iteration 229/1000 | Loss: 0.00002322
Iteration 230/1000 | Loss: 0.00002321
Iteration 231/1000 | Loss: 0.00002321
Iteration 232/1000 | Loss: 0.00002321
Iteration 233/1000 | Loss: 0.00002321
Iteration 234/1000 | Loss: 0.00002321
Iteration 235/1000 | Loss: 0.00002321
Iteration 236/1000 | Loss: 0.00002321
Iteration 237/1000 | Loss: 0.00002321
Iteration 238/1000 | Loss: 0.00002321
Iteration 239/1000 | Loss: 0.00002321
Iteration 240/1000 | Loss: 0.00002321
Iteration 241/1000 | Loss: 0.00002321
Iteration 242/1000 | Loss: 0.00002321
Iteration 243/1000 | Loss: 0.00002320
Iteration 244/1000 | Loss: 0.00002320
Iteration 245/1000 | Loss: 0.00002320
Iteration 246/1000 | Loss: 0.00002320
Iteration 247/1000 | Loss: 0.00002320
Iteration 248/1000 | Loss: 0.00002320
Iteration 249/1000 | Loss: 0.00002320
Iteration 250/1000 | Loss: 0.00002320
Iteration 251/1000 | Loss: 0.00002320
Iteration 252/1000 | Loss: 0.00002320
Iteration 253/1000 | Loss: 0.00002320
Iteration 254/1000 | Loss: 0.00002320
Iteration 255/1000 | Loss: 0.00002320
Iteration 256/1000 | Loss: 0.00002320
Iteration 257/1000 | Loss: 0.00002320
Iteration 258/1000 | Loss: 0.00002320
Iteration 259/1000 | Loss: 0.00002320
Iteration 260/1000 | Loss: 0.00002320
Iteration 261/1000 | Loss: 0.00002320
Iteration 262/1000 | Loss: 0.00002320
Iteration 263/1000 | Loss: 0.00002319
Iteration 264/1000 | Loss: 0.00002319
Iteration 265/1000 | Loss: 0.00002319
Iteration 266/1000 | Loss: 0.00002319
Iteration 267/1000 | Loss: 0.00002319
Iteration 268/1000 | Loss: 0.00002319
Iteration 269/1000 | Loss: 0.00002319
Iteration 270/1000 | Loss: 0.00002318
Iteration 271/1000 | Loss: 0.00002318
Iteration 272/1000 | Loss: 0.00002318
Iteration 273/1000 | Loss: 0.00002318
Iteration 274/1000 | Loss: 0.00002318
Iteration 275/1000 | Loss: 0.00002318
Iteration 276/1000 | Loss: 0.00002318
Iteration 277/1000 | Loss: 0.00002318
Iteration 278/1000 | Loss: 0.00002318
Iteration 279/1000 | Loss: 0.00002318
Iteration 280/1000 | Loss: 0.00002318
Iteration 281/1000 | Loss: 0.00002318
Iteration 282/1000 | Loss: 0.00002318
Iteration 283/1000 | Loss: 0.00002318
Iteration 284/1000 | Loss: 0.00002318
Iteration 285/1000 | Loss: 0.00002318
Iteration 286/1000 | Loss: 0.00002318
Iteration 287/1000 | Loss: 0.00002318
Iteration 288/1000 | Loss: 0.00002318
Iteration 289/1000 | Loss: 0.00002318
Iteration 290/1000 | Loss: 0.00002318
Iteration 291/1000 | Loss: 0.00002318
Iteration 292/1000 | Loss: 0.00002318
Iteration 293/1000 | Loss: 0.00002318
Iteration 294/1000 | Loss: 0.00002318
Iteration 295/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [2.318114275112748e-05, 2.318114275112748e-05, 2.318114275112748e-05, 2.318114275112748e-05, 2.318114275112748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.318114275112748e-05

Optimization complete. Final v2v error: 4.067569255828857 mm

Highest mean error: 4.633288860321045 mm for frame 74

Lowest mean error: 3.1333117485046387 mm for frame 0

Saving results

Total time: 50.98229455947876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030134
Iteration 2/25 | Loss: 0.00248536
Iteration 3/25 | Loss: 0.00182315
Iteration 4/25 | Loss: 0.00163794
Iteration 5/25 | Loss: 0.00138580
Iteration 6/25 | Loss: 0.00124352
Iteration 7/25 | Loss: 0.00109012
Iteration 8/25 | Loss: 0.00099218
Iteration 9/25 | Loss: 0.00093796
Iteration 10/25 | Loss: 0.00091712
Iteration 11/25 | Loss: 0.00089827
Iteration 12/25 | Loss: 0.00089459
Iteration 13/25 | Loss: 0.00088724
Iteration 14/25 | Loss: 0.00086555
Iteration 15/25 | Loss: 0.00085433
Iteration 16/25 | Loss: 0.00084605
Iteration 17/25 | Loss: 0.00083861
Iteration 18/25 | Loss: 0.00082950
Iteration 19/25 | Loss: 0.00082509
Iteration 20/25 | Loss: 0.00082281
Iteration 21/25 | Loss: 0.00082073
Iteration 22/25 | Loss: 0.00081849
Iteration 23/25 | Loss: 0.00081551
Iteration 24/25 | Loss: 0.00081429
Iteration 25/25 | Loss: 0.00081346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39234865
Iteration 2/25 | Loss: 0.00049352
Iteration 3/25 | Loss: 0.00049351
Iteration 4/25 | Loss: 0.00049351
Iteration 5/25 | Loss: 0.00049351
Iteration 6/25 | Loss: 0.00049351
Iteration 7/25 | Loss: 0.00049351
Iteration 8/25 | Loss: 0.00049351
Iteration 9/25 | Loss: 0.00049351
Iteration 10/25 | Loss: 0.00049351
Iteration 11/25 | Loss: 0.00049351
Iteration 12/25 | Loss: 0.00049351
Iteration 13/25 | Loss: 0.00049351
Iteration 14/25 | Loss: 0.00049351
Iteration 15/25 | Loss: 0.00049351
Iteration 16/25 | Loss: 0.00049351
Iteration 17/25 | Loss: 0.00049351
Iteration 18/25 | Loss: 0.00049351
Iteration 19/25 | Loss: 0.00049351
Iteration 20/25 | Loss: 0.00049351
Iteration 21/25 | Loss: 0.00049351
Iteration 22/25 | Loss: 0.00049351
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004935059114359319, 0.0004935059114359319, 0.0004935059114359319, 0.0004935059114359319, 0.0004935059114359319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004935059114359319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049351
Iteration 2/1000 | Loss: 0.00003658
Iteration 3/1000 | Loss: 0.00002519
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001739
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001669
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001656
Iteration 24/1000 | Loss: 0.00001655
Iteration 25/1000 | Loss: 0.00001655
Iteration 26/1000 | Loss: 0.00001654
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001651
Iteration 30/1000 | Loss: 0.00001651
Iteration 31/1000 | Loss: 0.00001651
Iteration 32/1000 | Loss: 0.00001650
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001650
Iteration 37/1000 | Loss: 0.00001650
Iteration 38/1000 | Loss: 0.00001650
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001649
Iteration 41/1000 | Loss: 0.00001649
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001645
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001644
Iteration 66/1000 | Loss: 0.00001644
Iteration 67/1000 | Loss: 0.00001644
Iteration 68/1000 | Loss: 0.00001644
Iteration 69/1000 | Loss: 0.00001644
Iteration 70/1000 | Loss: 0.00001644
Iteration 71/1000 | Loss: 0.00001644
Iteration 72/1000 | Loss: 0.00001644
Iteration 73/1000 | Loss: 0.00001643
Iteration 74/1000 | Loss: 0.00001643
Iteration 75/1000 | Loss: 0.00001643
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001642
Iteration 78/1000 | Loss: 0.00001642
Iteration 79/1000 | Loss: 0.00001642
Iteration 80/1000 | Loss: 0.00001642
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001641
Iteration 84/1000 | Loss: 0.00001641
Iteration 85/1000 | Loss: 0.00001641
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001640
Iteration 89/1000 | Loss: 0.00001640
Iteration 90/1000 | Loss: 0.00001640
Iteration 91/1000 | Loss: 0.00001640
Iteration 92/1000 | Loss: 0.00001640
Iteration 93/1000 | Loss: 0.00001640
Iteration 94/1000 | Loss: 0.00001639
Iteration 95/1000 | Loss: 0.00001639
Iteration 96/1000 | Loss: 0.00001639
Iteration 97/1000 | Loss: 0.00001639
Iteration 98/1000 | Loss: 0.00001639
Iteration 99/1000 | Loss: 0.00001639
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001638
Iteration 102/1000 | Loss: 0.00001638
Iteration 103/1000 | Loss: 0.00001638
Iteration 104/1000 | Loss: 0.00001638
Iteration 105/1000 | Loss: 0.00001638
Iteration 106/1000 | Loss: 0.00001638
Iteration 107/1000 | Loss: 0.00001638
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001638
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001637
Iteration 119/1000 | Loss: 0.00001637
Iteration 120/1000 | Loss: 0.00001637
Iteration 121/1000 | Loss: 0.00001637
Iteration 122/1000 | Loss: 0.00001637
Iteration 123/1000 | Loss: 0.00001637
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001637
Iteration 130/1000 | Loss: 0.00001637
Iteration 131/1000 | Loss: 0.00001637
Iteration 132/1000 | Loss: 0.00001637
Iteration 133/1000 | Loss: 0.00001637
Iteration 134/1000 | Loss: 0.00001637
Iteration 135/1000 | Loss: 0.00001637
Iteration 136/1000 | Loss: 0.00001637
Iteration 137/1000 | Loss: 0.00001637
Iteration 138/1000 | Loss: 0.00001637
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001637
Iteration 141/1000 | Loss: 0.00001637
Iteration 142/1000 | Loss: 0.00001637
Iteration 143/1000 | Loss: 0.00001637
Iteration 144/1000 | Loss: 0.00001637
Iteration 145/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.6371024685213342e-05, 1.6371024685213342e-05, 1.6371024685213342e-05, 1.6371024685213342e-05, 1.6371024685213342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6371024685213342e-05

Optimization complete. Final v2v error: 3.4278101921081543 mm

Highest mean error: 4.2575273513793945 mm for frame 11

Lowest mean error: 3.0690388679504395 mm for frame 108

Saving results

Total time: 70.1533989906311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938805
Iteration 2/25 | Loss: 0.00146710
Iteration 3/25 | Loss: 0.00128463
Iteration 4/25 | Loss: 0.00123849
Iteration 5/25 | Loss: 0.00122201
Iteration 6/25 | Loss: 0.00121938
Iteration 7/25 | Loss: 0.00121940
Iteration 8/25 | Loss: 0.00121287
Iteration 9/25 | Loss: 0.00121172
Iteration 10/25 | Loss: 0.00120899
Iteration 11/25 | Loss: 0.00120598
Iteration 12/25 | Loss: 0.00120468
Iteration 13/25 | Loss: 0.00120356
Iteration 14/25 | Loss: 0.00120331
Iteration 15/25 | Loss: 0.00120266
Iteration 16/25 | Loss: 0.00120576
Iteration 17/25 | Loss: 0.00120586
Iteration 18/25 | Loss: 0.00120275
Iteration 19/25 | Loss: 0.00120032
Iteration 20/25 | Loss: 0.00119983
Iteration 21/25 | Loss: 0.00119968
Iteration 22/25 | Loss: 0.00119965
Iteration 23/25 | Loss: 0.00119964
Iteration 24/25 | Loss: 0.00119964
Iteration 25/25 | Loss: 0.00119962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41170049
Iteration 2/25 | Loss: 0.00314000
Iteration 3/25 | Loss: 0.00313992
Iteration 4/25 | Loss: 0.00313991
Iteration 5/25 | Loss: 0.00313991
Iteration 6/25 | Loss: 0.00313991
Iteration 7/25 | Loss: 0.00313991
Iteration 8/25 | Loss: 0.00313991
Iteration 9/25 | Loss: 0.00313991
Iteration 10/25 | Loss: 0.00313991
Iteration 11/25 | Loss: 0.00313991
Iteration 12/25 | Loss: 0.00313991
Iteration 13/25 | Loss: 0.00313991
Iteration 14/25 | Loss: 0.00313991
Iteration 15/25 | Loss: 0.00313991
Iteration 16/25 | Loss: 0.00313991
Iteration 17/25 | Loss: 0.00313991
Iteration 18/25 | Loss: 0.00313991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0031399133149534464, 0.0031399133149534464, 0.0031399133149534464, 0.0031399133149534464, 0.0031399133149534464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031399133149534464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313991
Iteration 2/1000 | Loss: 0.00038949
Iteration 3/1000 | Loss: 0.00028220
Iteration 4/1000 | Loss: 0.00155272
Iteration 5/1000 | Loss: 0.00148740
Iteration 6/1000 | Loss: 0.00201191
Iteration 7/1000 | Loss: 0.00128211
Iteration 8/1000 | Loss: 0.00237109
Iteration 9/1000 | Loss: 0.00454579
Iteration 10/1000 | Loss: 0.00079321
Iteration 11/1000 | Loss: 0.00206203
Iteration 12/1000 | Loss: 0.00537935
Iteration 13/1000 | Loss: 0.00128829
Iteration 14/1000 | Loss: 0.00070430
Iteration 15/1000 | Loss: 0.00118469
Iteration 16/1000 | Loss: 0.00255135
Iteration 17/1000 | Loss: 0.00029604
Iteration 18/1000 | Loss: 0.00159130
Iteration 19/1000 | Loss: 0.00057202
Iteration 20/1000 | Loss: 0.00133108
Iteration 21/1000 | Loss: 0.00017430
Iteration 22/1000 | Loss: 0.00289155
Iteration 23/1000 | Loss: 0.00087106
Iteration 24/1000 | Loss: 0.00099681
Iteration 25/1000 | Loss: 0.00209976
Iteration 26/1000 | Loss: 0.00015542
Iteration 27/1000 | Loss: 0.00073683
Iteration 28/1000 | Loss: 0.00256470
Iteration 29/1000 | Loss: 0.00012183
Iteration 30/1000 | Loss: 0.00008256
Iteration 31/1000 | Loss: 0.00007036
Iteration 32/1000 | Loss: 0.00006213
Iteration 33/1000 | Loss: 0.00134482
Iteration 34/1000 | Loss: 0.00081672
Iteration 35/1000 | Loss: 0.00050868
Iteration 36/1000 | Loss: 0.00071777
Iteration 37/1000 | Loss: 0.00025872
Iteration 38/1000 | Loss: 0.00008409
Iteration 39/1000 | Loss: 0.00006440
Iteration 40/1000 | Loss: 0.00005601
Iteration 41/1000 | Loss: 0.00042446
Iteration 42/1000 | Loss: 0.00006086
Iteration 43/1000 | Loss: 0.00004442
Iteration 44/1000 | Loss: 0.00004173
Iteration 45/1000 | Loss: 0.00030569
Iteration 46/1000 | Loss: 0.00004879
Iteration 47/1000 | Loss: 0.00003764
Iteration 48/1000 | Loss: 0.00003549
Iteration 49/1000 | Loss: 0.00003384
Iteration 50/1000 | Loss: 0.00003261
Iteration 51/1000 | Loss: 0.00003161
Iteration 52/1000 | Loss: 0.00003097
Iteration 53/1000 | Loss: 0.00003058
Iteration 54/1000 | Loss: 0.00003024
Iteration 55/1000 | Loss: 0.00002996
Iteration 56/1000 | Loss: 0.00002976
Iteration 57/1000 | Loss: 0.00002972
Iteration 58/1000 | Loss: 0.00002962
Iteration 59/1000 | Loss: 0.00002956
Iteration 60/1000 | Loss: 0.00002953
Iteration 61/1000 | Loss: 0.00002951
Iteration 62/1000 | Loss: 0.00002950
Iteration 63/1000 | Loss: 0.00002950
Iteration 64/1000 | Loss: 0.00002949
Iteration 65/1000 | Loss: 0.00002948
Iteration 66/1000 | Loss: 0.00002944
Iteration 67/1000 | Loss: 0.00002940
Iteration 68/1000 | Loss: 0.00002939
Iteration 69/1000 | Loss: 0.00002938
Iteration 70/1000 | Loss: 0.00002938
Iteration 71/1000 | Loss: 0.00002926
Iteration 72/1000 | Loss: 0.00002924
Iteration 73/1000 | Loss: 0.00002919
Iteration 74/1000 | Loss: 0.00002918
Iteration 75/1000 | Loss: 0.00002917
Iteration 76/1000 | Loss: 0.00002917
Iteration 77/1000 | Loss: 0.00002917
Iteration 78/1000 | Loss: 0.00002917
Iteration 79/1000 | Loss: 0.00002917
Iteration 80/1000 | Loss: 0.00002915
Iteration 81/1000 | Loss: 0.00002915
Iteration 82/1000 | Loss: 0.00002914
Iteration 83/1000 | Loss: 0.00002914
Iteration 84/1000 | Loss: 0.00002913
Iteration 85/1000 | Loss: 0.00002913
Iteration 86/1000 | Loss: 0.00002912
Iteration 87/1000 | Loss: 0.00002912
Iteration 88/1000 | Loss: 0.00002911
Iteration 89/1000 | Loss: 0.00002911
Iteration 90/1000 | Loss: 0.00002911
Iteration 91/1000 | Loss: 0.00002911
Iteration 92/1000 | Loss: 0.00002911
Iteration 93/1000 | Loss: 0.00002910
Iteration 94/1000 | Loss: 0.00002910
Iteration 95/1000 | Loss: 0.00002910
Iteration 96/1000 | Loss: 0.00002910
Iteration 97/1000 | Loss: 0.00002910
Iteration 98/1000 | Loss: 0.00002909
Iteration 99/1000 | Loss: 0.00002909
Iteration 100/1000 | Loss: 0.00002909
Iteration 101/1000 | Loss: 0.00002909
Iteration 102/1000 | Loss: 0.00002909
Iteration 103/1000 | Loss: 0.00002909
Iteration 104/1000 | Loss: 0.00002909
Iteration 105/1000 | Loss: 0.00002908
Iteration 106/1000 | Loss: 0.00002908
Iteration 107/1000 | Loss: 0.00002908
Iteration 108/1000 | Loss: 0.00002907
Iteration 109/1000 | Loss: 0.00002907
Iteration 110/1000 | Loss: 0.00002907
Iteration 111/1000 | Loss: 0.00002906
Iteration 112/1000 | Loss: 0.00002906
Iteration 113/1000 | Loss: 0.00002906
Iteration 114/1000 | Loss: 0.00002905
Iteration 115/1000 | Loss: 0.00002905
Iteration 116/1000 | Loss: 0.00002905
Iteration 117/1000 | Loss: 0.00002904
Iteration 118/1000 | Loss: 0.00002904
Iteration 119/1000 | Loss: 0.00002903
Iteration 120/1000 | Loss: 0.00002903
Iteration 121/1000 | Loss: 0.00002903
Iteration 122/1000 | Loss: 0.00002903
Iteration 123/1000 | Loss: 0.00002903
Iteration 124/1000 | Loss: 0.00002903
Iteration 125/1000 | Loss: 0.00002902
Iteration 126/1000 | Loss: 0.00002902
Iteration 127/1000 | Loss: 0.00002902
Iteration 128/1000 | Loss: 0.00002901
Iteration 129/1000 | Loss: 0.00002901
Iteration 130/1000 | Loss: 0.00002901
Iteration 131/1000 | Loss: 0.00002901
Iteration 132/1000 | Loss: 0.00002900
Iteration 133/1000 | Loss: 0.00002900
Iteration 134/1000 | Loss: 0.00002900
Iteration 135/1000 | Loss: 0.00002899
Iteration 136/1000 | Loss: 0.00002899
Iteration 137/1000 | Loss: 0.00002898
Iteration 138/1000 | Loss: 0.00002898
Iteration 139/1000 | Loss: 0.00002898
Iteration 140/1000 | Loss: 0.00002897
Iteration 141/1000 | Loss: 0.00002897
Iteration 142/1000 | Loss: 0.00002897
Iteration 143/1000 | Loss: 0.00002896
Iteration 144/1000 | Loss: 0.00002896
Iteration 145/1000 | Loss: 0.00002896
Iteration 146/1000 | Loss: 0.00002895
Iteration 147/1000 | Loss: 0.00002895
Iteration 148/1000 | Loss: 0.00002895
Iteration 149/1000 | Loss: 0.00002895
Iteration 150/1000 | Loss: 0.00002895
Iteration 151/1000 | Loss: 0.00002895
Iteration 152/1000 | Loss: 0.00002895
Iteration 153/1000 | Loss: 0.00002895
Iteration 154/1000 | Loss: 0.00002895
Iteration 155/1000 | Loss: 0.00002895
Iteration 156/1000 | Loss: 0.00002894
Iteration 157/1000 | Loss: 0.00002894
Iteration 158/1000 | Loss: 0.00002894
Iteration 159/1000 | Loss: 0.00002894
Iteration 160/1000 | Loss: 0.00002894
Iteration 161/1000 | Loss: 0.00002894
Iteration 162/1000 | Loss: 0.00002894
Iteration 163/1000 | Loss: 0.00002894
Iteration 164/1000 | Loss: 0.00002894
Iteration 165/1000 | Loss: 0.00002894
Iteration 166/1000 | Loss: 0.00002894
Iteration 167/1000 | Loss: 0.00002894
Iteration 168/1000 | Loss: 0.00002894
Iteration 169/1000 | Loss: 0.00002894
Iteration 170/1000 | Loss: 0.00002894
Iteration 171/1000 | Loss: 0.00002894
Iteration 172/1000 | Loss: 0.00002894
Iteration 173/1000 | Loss: 0.00002893
Iteration 174/1000 | Loss: 0.00002893
Iteration 175/1000 | Loss: 0.00002893
Iteration 176/1000 | Loss: 0.00002893
Iteration 177/1000 | Loss: 0.00002893
Iteration 178/1000 | Loss: 0.00002893
Iteration 179/1000 | Loss: 0.00002893
Iteration 180/1000 | Loss: 0.00002893
Iteration 181/1000 | Loss: 0.00002892
Iteration 182/1000 | Loss: 0.00002892
Iteration 183/1000 | Loss: 0.00002892
Iteration 184/1000 | Loss: 0.00002892
Iteration 185/1000 | Loss: 0.00002892
Iteration 186/1000 | Loss: 0.00002892
Iteration 187/1000 | Loss: 0.00002892
Iteration 188/1000 | Loss: 0.00002891
Iteration 189/1000 | Loss: 0.00002891
Iteration 190/1000 | Loss: 0.00002891
Iteration 191/1000 | Loss: 0.00002891
Iteration 192/1000 | Loss: 0.00002891
Iteration 193/1000 | Loss: 0.00002891
Iteration 194/1000 | Loss: 0.00002891
Iteration 195/1000 | Loss: 0.00002891
Iteration 196/1000 | Loss: 0.00002891
Iteration 197/1000 | Loss: 0.00002891
Iteration 198/1000 | Loss: 0.00002891
Iteration 199/1000 | Loss: 0.00002890
Iteration 200/1000 | Loss: 0.00002890
Iteration 201/1000 | Loss: 0.00002890
Iteration 202/1000 | Loss: 0.00002890
Iteration 203/1000 | Loss: 0.00002890
Iteration 204/1000 | Loss: 0.00002890
Iteration 205/1000 | Loss: 0.00002890
Iteration 206/1000 | Loss: 0.00002890
Iteration 207/1000 | Loss: 0.00002890
Iteration 208/1000 | Loss: 0.00002890
Iteration 209/1000 | Loss: 0.00002890
Iteration 210/1000 | Loss: 0.00002890
Iteration 211/1000 | Loss: 0.00002890
Iteration 212/1000 | Loss: 0.00002890
Iteration 213/1000 | Loss: 0.00002889
Iteration 214/1000 | Loss: 0.00002889
Iteration 215/1000 | Loss: 0.00002889
Iteration 216/1000 | Loss: 0.00002889
Iteration 217/1000 | Loss: 0.00002889
Iteration 218/1000 | Loss: 0.00002889
Iteration 219/1000 | Loss: 0.00002889
Iteration 220/1000 | Loss: 0.00002888
Iteration 221/1000 | Loss: 0.00002888
Iteration 222/1000 | Loss: 0.00002888
Iteration 223/1000 | Loss: 0.00002888
Iteration 224/1000 | Loss: 0.00002888
Iteration 225/1000 | Loss: 0.00002888
Iteration 226/1000 | Loss: 0.00002887
Iteration 227/1000 | Loss: 0.00002887
Iteration 228/1000 | Loss: 0.00002887
Iteration 229/1000 | Loss: 0.00002887
Iteration 230/1000 | Loss: 0.00002887
Iteration 231/1000 | Loss: 0.00002887
Iteration 232/1000 | Loss: 0.00002887
Iteration 233/1000 | Loss: 0.00002887
Iteration 234/1000 | Loss: 0.00002887
Iteration 235/1000 | Loss: 0.00002887
Iteration 236/1000 | Loss: 0.00002887
Iteration 237/1000 | Loss: 0.00002887
Iteration 238/1000 | Loss: 0.00002887
Iteration 239/1000 | Loss: 0.00002887
Iteration 240/1000 | Loss: 0.00002887
Iteration 241/1000 | Loss: 0.00002887
Iteration 242/1000 | Loss: 0.00002887
Iteration 243/1000 | Loss: 0.00002887
Iteration 244/1000 | Loss: 0.00002887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.8866696084151044e-05, 2.8866696084151044e-05, 2.8866696084151044e-05, 2.8866696084151044e-05, 2.8866696084151044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8866696084151044e-05

Optimization complete. Final v2v error: 4.212995529174805 mm

Highest mean error: 5.652225017547607 mm for frame 175

Lowest mean error: 3.100999355316162 mm for frame 18

Saving results

Total time: 188.02792525291443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471080
Iteration 2/25 | Loss: 0.00090074
Iteration 3/25 | Loss: 0.00077944
Iteration 4/25 | Loss: 0.00074034
Iteration 5/25 | Loss: 0.00073294
Iteration 6/25 | Loss: 0.00073132
Iteration 7/25 | Loss: 0.00073086
Iteration 8/25 | Loss: 0.00073086
Iteration 9/25 | Loss: 0.00073086
Iteration 10/25 | Loss: 0.00073086
Iteration 11/25 | Loss: 0.00073086
Iteration 12/25 | Loss: 0.00073086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007308567292056978, 0.0007308567292056978, 0.0007308567292056978, 0.0007308567292056978, 0.0007308567292056978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007308567292056978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51440942
Iteration 2/25 | Loss: 0.00039963
Iteration 3/25 | Loss: 0.00039962
Iteration 4/25 | Loss: 0.00039962
Iteration 5/25 | Loss: 0.00039962
Iteration 6/25 | Loss: 0.00039962
Iteration 7/25 | Loss: 0.00039962
Iteration 8/25 | Loss: 0.00039962
Iteration 9/25 | Loss: 0.00039962
Iteration 10/25 | Loss: 0.00039962
Iteration 11/25 | Loss: 0.00039962
Iteration 12/25 | Loss: 0.00039962
Iteration 13/25 | Loss: 0.00039962
Iteration 14/25 | Loss: 0.00039962
Iteration 15/25 | Loss: 0.00039962
Iteration 16/25 | Loss: 0.00039962
Iteration 17/25 | Loss: 0.00039962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003996163432020694, 0.0003996163432020694, 0.0003996163432020694, 0.0003996163432020694, 0.0003996163432020694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003996163432020694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039962
Iteration 2/1000 | Loss: 0.00002263
Iteration 3/1000 | Loss: 0.00001819
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001468
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001456
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001444
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001442
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001441
Iteration 76/1000 | Loss: 0.00001441
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001441
Iteration 85/1000 | Loss: 0.00001441
Iteration 86/1000 | Loss: 0.00001440
Iteration 87/1000 | Loss: 0.00001440
Iteration 88/1000 | Loss: 0.00001440
Iteration 89/1000 | Loss: 0.00001440
Iteration 90/1000 | Loss: 0.00001440
Iteration 91/1000 | Loss: 0.00001440
Iteration 92/1000 | Loss: 0.00001440
Iteration 93/1000 | Loss: 0.00001440
Iteration 94/1000 | Loss: 0.00001440
Iteration 95/1000 | Loss: 0.00001440
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001439
Iteration 98/1000 | Loss: 0.00001439
Iteration 99/1000 | Loss: 0.00001439
Iteration 100/1000 | Loss: 0.00001439
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001438
Iteration 111/1000 | Loss: 0.00001438
Iteration 112/1000 | Loss: 0.00001438
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001437
Iteration 139/1000 | Loss: 0.00001437
Iteration 140/1000 | Loss: 0.00001437
Iteration 141/1000 | Loss: 0.00001437
Iteration 142/1000 | Loss: 0.00001437
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001436
Iteration 156/1000 | Loss: 0.00001436
Iteration 157/1000 | Loss: 0.00001436
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001436
Iteration 161/1000 | Loss: 0.00001436
Iteration 162/1000 | Loss: 0.00001436
Iteration 163/1000 | Loss: 0.00001436
Iteration 164/1000 | Loss: 0.00001436
Iteration 165/1000 | Loss: 0.00001436
Iteration 166/1000 | Loss: 0.00001436
Iteration 167/1000 | Loss: 0.00001436
Iteration 168/1000 | Loss: 0.00001436
Iteration 169/1000 | Loss: 0.00001436
Iteration 170/1000 | Loss: 0.00001436
Iteration 171/1000 | Loss: 0.00001436
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001436
Iteration 175/1000 | Loss: 0.00001436
Iteration 176/1000 | Loss: 0.00001436
Iteration 177/1000 | Loss: 0.00001436
Iteration 178/1000 | Loss: 0.00001436
Iteration 179/1000 | Loss: 0.00001436
Iteration 180/1000 | Loss: 0.00001436
Iteration 181/1000 | Loss: 0.00001436
Iteration 182/1000 | Loss: 0.00001436
Iteration 183/1000 | Loss: 0.00001436
Iteration 184/1000 | Loss: 0.00001436
Iteration 185/1000 | Loss: 0.00001436
Iteration 186/1000 | Loss: 0.00001436
Iteration 187/1000 | Loss: 0.00001436
Iteration 188/1000 | Loss: 0.00001436
Iteration 189/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.435606009181356e-05, 1.435606009181356e-05, 1.435606009181356e-05, 1.435606009181356e-05, 1.435606009181356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.435606009181356e-05

Optimization complete. Final v2v error: 3.2280523777008057 mm

Highest mean error: 3.615842580795288 mm for frame 141

Lowest mean error: 3.0703535079956055 mm for frame 122

Saving results

Total time: 65.07976698875427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103388
Iteration 2/25 | Loss: 0.00318710
Iteration 3/25 | Loss: 0.00132543
Iteration 4/25 | Loss: 0.00108285
Iteration 5/25 | Loss: 0.00102088
Iteration 6/25 | Loss: 0.00098437
Iteration 7/25 | Loss: 0.00096118
Iteration 8/25 | Loss: 0.00089393
Iteration 9/25 | Loss: 0.00085839
Iteration 10/25 | Loss: 0.00085336
Iteration 11/25 | Loss: 0.00084781
Iteration 12/25 | Loss: 0.00083698
Iteration 13/25 | Loss: 0.00083699
Iteration 14/25 | Loss: 0.00083486
Iteration 15/25 | Loss: 0.00082797
Iteration 16/25 | Loss: 0.00082617
Iteration 17/25 | Loss: 0.00082276
Iteration 18/25 | Loss: 0.00081902
Iteration 19/25 | Loss: 0.00081304
Iteration 20/25 | Loss: 0.00081623
Iteration 21/25 | Loss: 0.00082444
Iteration 22/25 | Loss: 0.00082740
Iteration 23/25 | Loss: 0.00081502
Iteration 24/25 | Loss: 0.00080304
Iteration 25/25 | Loss: 0.00079871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47451305
Iteration 2/25 | Loss: 0.00051730
Iteration 3/25 | Loss: 0.00051730
Iteration 4/25 | Loss: 0.00051730
Iteration 5/25 | Loss: 0.00051730
Iteration 6/25 | Loss: 0.00051730
Iteration 7/25 | Loss: 0.00051730
Iteration 8/25 | Loss: 0.00051730
Iteration 9/25 | Loss: 0.00051730
Iteration 10/25 | Loss: 0.00051730
Iteration 11/25 | Loss: 0.00051730
Iteration 12/25 | Loss: 0.00051730
Iteration 13/25 | Loss: 0.00051730
Iteration 14/25 | Loss: 0.00051730
Iteration 15/25 | Loss: 0.00051730
Iteration 16/25 | Loss: 0.00051730
Iteration 17/25 | Loss: 0.00051730
Iteration 18/25 | Loss: 0.00051730
Iteration 19/25 | Loss: 0.00051730
Iteration 20/25 | Loss: 0.00051730
Iteration 21/25 | Loss: 0.00051730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005172987002879381, 0.0005172987002879381, 0.0005172987002879381, 0.0005172987002879381, 0.0005172987002879381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005172987002879381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051730
Iteration 2/1000 | Loss: 0.00026434
Iteration 3/1000 | Loss: 0.00025759
Iteration 4/1000 | Loss: 0.00022268
Iteration 5/1000 | Loss: 0.00057884
Iteration 6/1000 | Loss: 0.00030200
Iteration 7/1000 | Loss: 0.00020180
Iteration 8/1000 | Loss: 0.00019732
Iteration 9/1000 | Loss: 0.00020693
Iteration 10/1000 | Loss: 0.00017537
Iteration 11/1000 | Loss: 0.00017732
Iteration 12/1000 | Loss: 0.00025946
Iteration 13/1000 | Loss: 0.00010779
Iteration 14/1000 | Loss: 0.00020556
Iteration 15/1000 | Loss: 0.00004019
Iteration 16/1000 | Loss: 0.00003401
Iteration 17/1000 | Loss: 0.00003076
Iteration 18/1000 | Loss: 0.00002944
Iteration 19/1000 | Loss: 0.00002829
Iteration 20/1000 | Loss: 0.00002733
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00027106
Iteration 23/1000 | Loss: 0.00043058
Iteration 24/1000 | Loss: 0.00018862
Iteration 25/1000 | Loss: 0.00025226
Iteration 26/1000 | Loss: 0.00025672
Iteration 27/1000 | Loss: 0.00026133
Iteration 28/1000 | Loss: 0.00036225
Iteration 29/1000 | Loss: 0.00039118
Iteration 30/1000 | Loss: 0.00004211
Iteration 31/1000 | Loss: 0.00014374
Iteration 32/1000 | Loss: 0.00017838
Iteration 33/1000 | Loss: 0.00003188
Iteration 34/1000 | Loss: 0.00028416
Iteration 35/1000 | Loss: 0.00025195
Iteration 36/1000 | Loss: 0.00010015
Iteration 37/1000 | Loss: 0.00035110
Iteration 38/1000 | Loss: 0.00028534
Iteration 39/1000 | Loss: 0.00025656
Iteration 40/1000 | Loss: 0.00003146
Iteration 41/1000 | Loss: 0.00024548
Iteration 42/1000 | Loss: 0.00024016
Iteration 43/1000 | Loss: 0.00004810
Iteration 44/1000 | Loss: 0.00003710
Iteration 45/1000 | Loss: 0.00003347
Iteration 46/1000 | Loss: 0.00074211
Iteration 47/1000 | Loss: 0.00002985
Iteration 48/1000 | Loss: 0.00002701
Iteration 49/1000 | Loss: 0.00002644
Iteration 50/1000 | Loss: 0.00002598
Iteration 51/1000 | Loss: 0.00002551
Iteration 52/1000 | Loss: 0.00023877
Iteration 53/1000 | Loss: 0.00024776
Iteration 54/1000 | Loss: 0.00026555
Iteration 55/1000 | Loss: 0.00019640
Iteration 56/1000 | Loss: 0.00028284
Iteration 57/1000 | Loss: 0.00036779
Iteration 58/1000 | Loss: 0.00035147
Iteration 59/1000 | Loss: 0.00004509
Iteration 60/1000 | Loss: 0.00003733
Iteration 61/1000 | Loss: 0.00003481
Iteration 62/1000 | Loss: 0.00003299
Iteration 63/1000 | Loss: 0.00003132
Iteration 64/1000 | Loss: 0.00002944
Iteration 65/1000 | Loss: 0.00002824
Iteration 66/1000 | Loss: 0.00002768
Iteration 67/1000 | Loss: 0.00022742
Iteration 68/1000 | Loss: 0.00008241
Iteration 69/1000 | Loss: 0.00018140
Iteration 70/1000 | Loss: 0.00023323
Iteration 71/1000 | Loss: 0.00002745
Iteration 72/1000 | Loss: 0.00002674
Iteration 73/1000 | Loss: 0.00025031
Iteration 74/1000 | Loss: 0.00023979
Iteration 75/1000 | Loss: 0.00002761
Iteration 76/1000 | Loss: 0.00026378
Iteration 77/1000 | Loss: 0.00022221
Iteration 78/1000 | Loss: 0.00002692
Iteration 79/1000 | Loss: 0.00002659
Iteration 80/1000 | Loss: 0.00002467
Iteration 81/1000 | Loss: 0.00002512
Iteration 82/1000 | Loss: 0.00002385
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002312
Iteration 85/1000 | Loss: 0.00002306
Iteration 86/1000 | Loss: 0.00002287
Iteration 87/1000 | Loss: 0.00002275
Iteration 88/1000 | Loss: 0.00002272
Iteration 89/1000 | Loss: 0.00024704
Iteration 90/1000 | Loss: 0.00048356
Iteration 91/1000 | Loss: 0.00038308
Iteration 92/1000 | Loss: 0.00041867
Iteration 93/1000 | Loss: 0.00004849
Iteration 94/1000 | Loss: 0.00003544
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00021169
Iteration 97/1000 | Loss: 0.00015308
Iteration 98/1000 | Loss: 0.00015138
Iteration 99/1000 | Loss: 0.00008320
Iteration 100/1000 | Loss: 0.00002913
Iteration 101/1000 | Loss: 0.00002752
Iteration 102/1000 | Loss: 0.00002667
Iteration 103/1000 | Loss: 0.00023184
Iteration 104/1000 | Loss: 0.00044704
Iteration 105/1000 | Loss: 0.00017256
Iteration 106/1000 | Loss: 0.00003112
Iteration 107/1000 | Loss: 0.00041311
Iteration 108/1000 | Loss: 0.00042190
Iteration 109/1000 | Loss: 0.00030481
Iteration 110/1000 | Loss: 0.00032272
Iteration 111/1000 | Loss: 0.00004714
Iteration 112/1000 | Loss: 0.00004701
Iteration 113/1000 | Loss: 0.00021068
Iteration 114/1000 | Loss: 0.00012629
Iteration 115/1000 | Loss: 0.00012393
Iteration 116/1000 | Loss: 0.00027405
Iteration 117/1000 | Loss: 0.00024687
Iteration 118/1000 | Loss: 0.00011239
Iteration 119/1000 | Loss: 0.00018170
Iteration 120/1000 | Loss: 0.00013075
Iteration 121/1000 | Loss: 0.00018640
Iteration 122/1000 | Loss: 0.00029284
Iteration 123/1000 | Loss: 0.00010596
Iteration 124/1000 | Loss: 0.00025127
Iteration 125/1000 | Loss: 0.00012279
Iteration 126/1000 | Loss: 0.00003453
Iteration 127/1000 | Loss: 0.00022919
Iteration 128/1000 | Loss: 0.00052037
Iteration 129/1000 | Loss: 0.00053273
Iteration 130/1000 | Loss: 0.00051136
Iteration 131/1000 | Loss: 0.00015226
Iteration 132/1000 | Loss: 0.00019146
Iteration 133/1000 | Loss: 0.00036194
Iteration 134/1000 | Loss: 0.00032696
Iteration 135/1000 | Loss: 0.00037784
Iteration 136/1000 | Loss: 0.00041080
Iteration 137/1000 | Loss: 0.00004073
Iteration 138/1000 | Loss: 0.00003221
Iteration 139/1000 | Loss: 0.00002858
Iteration 140/1000 | Loss: 0.00002392
Iteration 141/1000 | Loss: 0.00002240
Iteration 142/1000 | Loss: 0.00002180
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002051
Iteration 146/1000 | Loss: 0.00002021
Iteration 147/1000 | Loss: 0.00002006
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00001992
Iteration 150/1000 | Loss: 0.00001992
Iteration 151/1000 | Loss: 0.00001982
Iteration 152/1000 | Loss: 0.00001979
Iteration 153/1000 | Loss: 0.00001975
Iteration 154/1000 | Loss: 0.00001974
Iteration 155/1000 | Loss: 0.00001973
Iteration 156/1000 | Loss: 0.00001973
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00001972
Iteration 159/1000 | Loss: 0.00001971
Iteration 160/1000 | Loss: 0.00001971
Iteration 161/1000 | Loss: 0.00001970
Iteration 162/1000 | Loss: 0.00001970
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001968
Iteration 167/1000 | Loss: 0.00001968
Iteration 168/1000 | Loss: 0.00001968
Iteration 169/1000 | Loss: 0.00001967
Iteration 170/1000 | Loss: 0.00001967
Iteration 171/1000 | Loss: 0.00001967
Iteration 172/1000 | Loss: 0.00001966
Iteration 173/1000 | Loss: 0.00001966
Iteration 174/1000 | Loss: 0.00001966
Iteration 175/1000 | Loss: 0.00001965
Iteration 176/1000 | Loss: 0.00001965
Iteration 177/1000 | Loss: 0.00001965
Iteration 178/1000 | Loss: 0.00001965
Iteration 179/1000 | Loss: 0.00001965
Iteration 180/1000 | Loss: 0.00001965
Iteration 181/1000 | Loss: 0.00001965
Iteration 182/1000 | Loss: 0.00001965
Iteration 183/1000 | Loss: 0.00001964
Iteration 184/1000 | Loss: 0.00001964
Iteration 185/1000 | Loss: 0.00001964
Iteration 186/1000 | Loss: 0.00001964
Iteration 187/1000 | Loss: 0.00001964
Iteration 188/1000 | Loss: 0.00001964
Iteration 189/1000 | Loss: 0.00001964
Iteration 190/1000 | Loss: 0.00001964
Iteration 191/1000 | Loss: 0.00001964
Iteration 192/1000 | Loss: 0.00001964
Iteration 193/1000 | Loss: 0.00001964
Iteration 194/1000 | Loss: 0.00001963
Iteration 195/1000 | Loss: 0.00001963
Iteration 196/1000 | Loss: 0.00001963
Iteration 197/1000 | Loss: 0.00001963
Iteration 198/1000 | Loss: 0.00001963
Iteration 199/1000 | Loss: 0.00001963
Iteration 200/1000 | Loss: 0.00001963
Iteration 201/1000 | Loss: 0.00001963
Iteration 202/1000 | Loss: 0.00001963
Iteration 203/1000 | Loss: 0.00001963
Iteration 204/1000 | Loss: 0.00001962
Iteration 205/1000 | Loss: 0.00001962
Iteration 206/1000 | Loss: 0.00001962
Iteration 207/1000 | Loss: 0.00001962
Iteration 208/1000 | Loss: 0.00001962
Iteration 209/1000 | Loss: 0.00001962
Iteration 210/1000 | Loss: 0.00001962
Iteration 211/1000 | Loss: 0.00001962
Iteration 212/1000 | Loss: 0.00001962
Iteration 213/1000 | Loss: 0.00001961
Iteration 214/1000 | Loss: 0.00001961
Iteration 215/1000 | Loss: 0.00001961
Iteration 216/1000 | Loss: 0.00001961
Iteration 217/1000 | Loss: 0.00001961
Iteration 218/1000 | Loss: 0.00001961
Iteration 219/1000 | Loss: 0.00001961
Iteration 220/1000 | Loss: 0.00001961
Iteration 221/1000 | Loss: 0.00001961
Iteration 222/1000 | Loss: 0.00001961
Iteration 223/1000 | Loss: 0.00001961
Iteration 224/1000 | Loss: 0.00001961
Iteration 225/1000 | Loss: 0.00001961
Iteration 226/1000 | Loss: 0.00001961
Iteration 227/1000 | Loss: 0.00001961
Iteration 228/1000 | Loss: 0.00001961
Iteration 229/1000 | Loss: 0.00001961
Iteration 230/1000 | Loss: 0.00001961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.9606295609264635e-05, 1.9606295609264635e-05, 1.9606295609264635e-05, 1.9606295609264635e-05, 1.9606295609264635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9606295609264635e-05

Optimization complete. Final v2v error: 3.664715528488159 mm

Highest mean error: 6.694736957550049 mm for frame 88

Lowest mean error: 3.3796513080596924 mm for frame 17

Saving results

Total time: 314.2282962799072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811294
Iteration 2/25 | Loss: 0.00156133
Iteration 3/25 | Loss: 0.00110029
Iteration 4/25 | Loss: 0.00103630
Iteration 5/25 | Loss: 0.00102076
Iteration 6/25 | Loss: 0.00101649
Iteration 7/25 | Loss: 0.00101540
Iteration 8/25 | Loss: 0.00101535
Iteration 9/25 | Loss: 0.00101535
Iteration 10/25 | Loss: 0.00101535
Iteration 11/25 | Loss: 0.00101535
Iteration 12/25 | Loss: 0.00101535
Iteration 13/25 | Loss: 0.00101535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001015345798805356, 0.001015345798805356, 0.001015345798805356, 0.001015345798805356, 0.001015345798805356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001015345798805356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.26294482
Iteration 2/25 | Loss: 0.00065521
Iteration 3/25 | Loss: 0.00065521
Iteration 4/25 | Loss: 0.00065520
Iteration 5/25 | Loss: 0.00065520
Iteration 6/25 | Loss: 0.00065520
Iteration 7/25 | Loss: 0.00065520
Iteration 8/25 | Loss: 0.00065520
Iteration 9/25 | Loss: 0.00065520
Iteration 10/25 | Loss: 0.00065520
Iteration 11/25 | Loss: 0.00065520
Iteration 12/25 | Loss: 0.00065520
Iteration 13/25 | Loss: 0.00065520
Iteration 14/25 | Loss: 0.00065520
Iteration 15/25 | Loss: 0.00065520
Iteration 16/25 | Loss: 0.00065520
Iteration 17/25 | Loss: 0.00065520
Iteration 18/25 | Loss: 0.00065520
Iteration 19/25 | Loss: 0.00065520
Iteration 20/25 | Loss: 0.00065520
Iteration 21/25 | Loss: 0.00065520
Iteration 22/25 | Loss: 0.00065520
Iteration 23/25 | Loss: 0.00065520
Iteration 24/25 | Loss: 0.00065520
Iteration 25/25 | Loss: 0.00065520

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065520
Iteration 2/1000 | Loss: 0.00006981
Iteration 3/1000 | Loss: 0.00004560
Iteration 4/1000 | Loss: 0.00003933
Iteration 5/1000 | Loss: 0.00003641
Iteration 6/1000 | Loss: 0.00003440
Iteration 7/1000 | Loss: 0.00003317
Iteration 8/1000 | Loss: 0.00003254
Iteration 9/1000 | Loss: 0.00003213
Iteration 10/1000 | Loss: 0.00003184
Iteration 11/1000 | Loss: 0.00003153
Iteration 12/1000 | Loss: 0.00003127
Iteration 13/1000 | Loss: 0.00003112
Iteration 14/1000 | Loss: 0.00003110
Iteration 15/1000 | Loss: 0.00003099
Iteration 16/1000 | Loss: 0.00003094
Iteration 17/1000 | Loss: 0.00003090
Iteration 18/1000 | Loss: 0.00003090
Iteration 19/1000 | Loss: 0.00003089
Iteration 20/1000 | Loss: 0.00003088
Iteration 21/1000 | Loss: 0.00003087
Iteration 22/1000 | Loss: 0.00003086
Iteration 23/1000 | Loss: 0.00003085
Iteration 24/1000 | Loss: 0.00003085
Iteration 25/1000 | Loss: 0.00003084
Iteration 26/1000 | Loss: 0.00003084
Iteration 27/1000 | Loss: 0.00003083
Iteration 28/1000 | Loss: 0.00003083
Iteration 29/1000 | Loss: 0.00003083
Iteration 30/1000 | Loss: 0.00003083
Iteration 31/1000 | Loss: 0.00003083
Iteration 32/1000 | Loss: 0.00003083
Iteration 33/1000 | Loss: 0.00003083
Iteration 34/1000 | Loss: 0.00003082
Iteration 35/1000 | Loss: 0.00003082
Iteration 36/1000 | Loss: 0.00003082
Iteration 37/1000 | Loss: 0.00003082
Iteration 38/1000 | Loss: 0.00003082
Iteration 39/1000 | Loss: 0.00003081
Iteration 40/1000 | Loss: 0.00003079
Iteration 41/1000 | Loss: 0.00003078
Iteration 42/1000 | Loss: 0.00003078
Iteration 43/1000 | Loss: 0.00003078
Iteration 44/1000 | Loss: 0.00003078
Iteration 45/1000 | Loss: 0.00003077
Iteration 46/1000 | Loss: 0.00003077
Iteration 47/1000 | Loss: 0.00003076
Iteration 48/1000 | Loss: 0.00003076
Iteration 49/1000 | Loss: 0.00003076
Iteration 50/1000 | Loss: 0.00003075
Iteration 51/1000 | Loss: 0.00003075
Iteration 52/1000 | Loss: 0.00003075
Iteration 53/1000 | Loss: 0.00003075
Iteration 54/1000 | Loss: 0.00003074
Iteration 55/1000 | Loss: 0.00003074
Iteration 56/1000 | Loss: 0.00003074
Iteration 57/1000 | Loss: 0.00003074
Iteration 58/1000 | Loss: 0.00003074
Iteration 59/1000 | Loss: 0.00003073
Iteration 60/1000 | Loss: 0.00003073
Iteration 61/1000 | Loss: 0.00003073
Iteration 62/1000 | Loss: 0.00003072
Iteration 63/1000 | Loss: 0.00003072
Iteration 64/1000 | Loss: 0.00003072
Iteration 65/1000 | Loss: 0.00003071
Iteration 66/1000 | Loss: 0.00003071
Iteration 67/1000 | Loss: 0.00003071
Iteration 68/1000 | Loss: 0.00003071
Iteration 69/1000 | Loss: 0.00003071
Iteration 70/1000 | Loss: 0.00003070
Iteration 71/1000 | Loss: 0.00003070
Iteration 72/1000 | Loss: 0.00003070
Iteration 73/1000 | Loss: 0.00003070
Iteration 74/1000 | Loss: 0.00003069
Iteration 75/1000 | Loss: 0.00003069
Iteration 76/1000 | Loss: 0.00003069
Iteration 77/1000 | Loss: 0.00003069
Iteration 78/1000 | Loss: 0.00003069
Iteration 79/1000 | Loss: 0.00003069
Iteration 80/1000 | Loss: 0.00003069
Iteration 81/1000 | Loss: 0.00003069
Iteration 82/1000 | Loss: 0.00003068
Iteration 83/1000 | Loss: 0.00003068
Iteration 84/1000 | Loss: 0.00003068
Iteration 85/1000 | Loss: 0.00003068
Iteration 86/1000 | Loss: 0.00003067
Iteration 87/1000 | Loss: 0.00003067
Iteration 88/1000 | Loss: 0.00003067
Iteration 89/1000 | Loss: 0.00003067
Iteration 90/1000 | Loss: 0.00003067
Iteration 91/1000 | Loss: 0.00003067
Iteration 92/1000 | Loss: 0.00003067
Iteration 93/1000 | Loss: 0.00003067
Iteration 94/1000 | Loss: 0.00003067
Iteration 95/1000 | Loss: 0.00003066
Iteration 96/1000 | Loss: 0.00003066
Iteration 97/1000 | Loss: 0.00003066
Iteration 98/1000 | Loss: 0.00003066
Iteration 99/1000 | Loss: 0.00003066
Iteration 100/1000 | Loss: 0.00003066
Iteration 101/1000 | Loss: 0.00003066
Iteration 102/1000 | Loss: 0.00003066
Iteration 103/1000 | Loss: 0.00003066
Iteration 104/1000 | Loss: 0.00003065
Iteration 105/1000 | Loss: 0.00003065
Iteration 106/1000 | Loss: 0.00003065
Iteration 107/1000 | Loss: 0.00003065
Iteration 108/1000 | Loss: 0.00003064
Iteration 109/1000 | Loss: 0.00003064
Iteration 110/1000 | Loss: 0.00003064
Iteration 111/1000 | Loss: 0.00003064
Iteration 112/1000 | Loss: 0.00003064
Iteration 113/1000 | Loss: 0.00003064
Iteration 114/1000 | Loss: 0.00003064
Iteration 115/1000 | Loss: 0.00003064
Iteration 116/1000 | Loss: 0.00003063
Iteration 117/1000 | Loss: 0.00003063
Iteration 118/1000 | Loss: 0.00003063
Iteration 119/1000 | Loss: 0.00003063
Iteration 120/1000 | Loss: 0.00003063
Iteration 121/1000 | Loss: 0.00003063
Iteration 122/1000 | Loss: 0.00003063
Iteration 123/1000 | Loss: 0.00003063
Iteration 124/1000 | Loss: 0.00003063
Iteration 125/1000 | Loss: 0.00003063
Iteration 126/1000 | Loss: 0.00003063
Iteration 127/1000 | Loss: 0.00003063
Iteration 128/1000 | Loss: 0.00003063
Iteration 129/1000 | Loss: 0.00003063
Iteration 130/1000 | Loss: 0.00003063
Iteration 131/1000 | Loss: 0.00003063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.062721953028813e-05, 3.062721953028813e-05, 3.062721953028813e-05, 3.062721953028813e-05, 3.062721953028813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.062721953028813e-05

Optimization complete. Final v2v error: 4.2837138175964355 mm

Highest mean error: 5.23518180847168 mm for frame 8

Lowest mean error: 3.4053781032562256 mm for frame 155

Saving results

Total time: 44.784204721450806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780973
Iteration 2/25 | Loss: 0.00140169
Iteration 3/25 | Loss: 0.00085976
Iteration 4/25 | Loss: 0.00079331
Iteration 5/25 | Loss: 0.00077964
Iteration 6/25 | Loss: 0.00077701
Iteration 7/25 | Loss: 0.00077692
Iteration 8/25 | Loss: 0.00077692
Iteration 9/25 | Loss: 0.00077692
Iteration 10/25 | Loss: 0.00077692
Iteration 11/25 | Loss: 0.00077692
Iteration 12/25 | Loss: 0.00077692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007769193616695702, 0.0007769193616695702, 0.0007769193616695702, 0.0007769193616695702, 0.0007769193616695702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007769193616695702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49230039
Iteration 2/25 | Loss: 0.00049809
Iteration 3/25 | Loss: 0.00049807
Iteration 4/25 | Loss: 0.00049807
Iteration 5/25 | Loss: 0.00049807
Iteration 6/25 | Loss: 0.00049807
Iteration 7/25 | Loss: 0.00049807
Iteration 8/25 | Loss: 0.00049807
Iteration 9/25 | Loss: 0.00049807
Iteration 10/25 | Loss: 0.00049807
Iteration 11/25 | Loss: 0.00049807
Iteration 12/25 | Loss: 0.00049807
Iteration 13/25 | Loss: 0.00049807
Iteration 14/25 | Loss: 0.00049807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0004980682278983295, 0.0004980682278983295, 0.0004980682278983295, 0.0004980682278983295, 0.0004980682278983295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004980682278983295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049807
Iteration 2/1000 | Loss: 0.00002189
Iteration 3/1000 | Loss: 0.00001704
Iteration 4/1000 | Loss: 0.00001559
Iteration 5/1000 | Loss: 0.00001488
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001369
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001336
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001330
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001326
Iteration 20/1000 | Loss: 0.00001326
Iteration 21/1000 | Loss: 0.00001325
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001309
Iteration 40/1000 | Loss: 0.00001309
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001306
Iteration 65/1000 | Loss: 0.00001306
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001306
Iteration 68/1000 | Loss: 0.00001306
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001306
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001305
Iteration 73/1000 | Loss: 0.00001305
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001304
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001303
Iteration 90/1000 | Loss: 0.00001303
Iteration 91/1000 | Loss: 0.00001302
Iteration 92/1000 | Loss: 0.00001302
Iteration 93/1000 | Loss: 0.00001302
Iteration 94/1000 | Loss: 0.00001302
Iteration 95/1000 | Loss: 0.00001302
Iteration 96/1000 | Loss: 0.00001302
Iteration 97/1000 | Loss: 0.00001302
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001301
Iteration 100/1000 | Loss: 0.00001301
Iteration 101/1000 | Loss: 0.00001301
Iteration 102/1000 | Loss: 0.00001301
Iteration 103/1000 | Loss: 0.00001301
Iteration 104/1000 | Loss: 0.00001301
Iteration 105/1000 | Loss: 0.00001301
Iteration 106/1000 | Loss: 0.00001301
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001301
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001301
Iteration 114/1000 | Loss: 0.00001301
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.3006366316403728e-05, 1.3006366316403728e-05, 1.3006366316403728e-05, 1.3006366316403728e-05, 1.3006366316403728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3006366316403728e-05

Optimization complete. Final v2v error: 3.087002754211426 mm

Highest mean error: 3.3241496086120605 mm for frame 0

Lowest mean error: 2.9676551818847656 mm for frame 173

Saving results

Total time: 36.23997688293457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465203
Iteration 2/25 | Loss: 0.00098942
Iteration 3/25 | Loss: 0.00085270
Iteration 4/25 | Loss: 0.00081948
Iteration 5/25 | Loss: 0.00081365
Iteration 6/25 | Loss: 0.00081128
Iteration 7/25 | Loss: 0.00081080
Iteration 8/25 | Loss: 0.00081080
Iteration 9/25 | Loss: 0.00081080
Iteration 10/25 | Loss: 0.00081080
Iteration 11/25 | Loss: 0.00081080
Iteration 12/25 | Loss: 0.00081080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008108023903332651, 0.0008108023903332651, 0.0008108023903332651, 0.0008108023903332651, 0.0008108023903332651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008108023903332651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88935375
Iteration 2/25 | Loss: 0.00047982
Iteration 3/25 | Loss: 0.00047981
Iteration 4/25 | Loss: 0.00047981
Iteration 5/25 | Loss: 0.00047981
Iteration 6/25 | Loss: 0.00047981
Iteration 7/25 | Loss: 0.00047981
Iteration 8/25 | Loss: 0.00047981
Iteration 9/25 | Loss: 0.00047981
Iteration 10/25 | Loss: 0.00047981
Iteration 11/25 | Loss: 0.00047981
Iteration 12/25 | Loss: 0.00047981
Iteration 13/25 | Loss: 0.00047981
Iteration 14/25 | Loss: 0.00047981
Iteration 15/25 | Loss: 0.00047981
Iteration 16/25 | Loss: 0.00047981
Iteration 17/25 | Loss: 0.00047981
Iteration 18/25 | Loss: 0.00047981
Iteration 19/25 | Loss: 0.00047981
Iteration 20/25 | Loss: 0.00047981
Iteration 21/25 | Loss: 0.00047981
Iteration 22/25 | Loss: 0.00047981
Iteration 23/25 | Loss: 0.00047981
Iteration 24/25 | Loss: 0.00047981
Iteration 25/25 | Loss: 0.00047981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047981
Iteration 2/1000 | Loss: 0.00003056
Iteration 3/1000 | Loss: 0.00002579
Iteration 4/1000 | Loss: 0.00002462
Iteration 5/1000 | Loss: 0.00002367
Iteration 6/1000 | Loss: 0.00002310
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002240
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002195
Iteration 11/1000 | Loss: 0.00002185
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002180
Iteration 14/1000 | Loss: 0.00002175
Iteration 15/1000 | Loss: 0.00002174
Iteration 16/1000 | Loss: 0.00002174
Iteration 17/1000 | Loss: 0.00002173
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002170
Iteration 20/1000 | Loss: 0.00002170
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002166
Iteration 24/1000 | Loss: 0.00002166
Iteration 25/1000 | Loss: 0.00002165
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002164
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002160
Iteration 33/1000 | Loss: 0.00002160
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002160
Iteration 36/1000 | Loss: 0.00002160
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002159
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002157
Iteration 41/1000 | Loss: 0.00002157
Iteration 42/1000 | Loss: 0.00002156
Iteration 43/1000 | Loss: 0.00002156
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002155
Iteration 46/1000 | Loss: 0.00002154
Iteration 47/1000 | Loss: 0.00002153
Iteration 48/1000 | Loss: 0.00002153
Iteration 49/1000 | Loss: 0.00002153
Iteration 50/1000 | Loss: 0.00002153
Iteration 51/1000 | Loss: 0.00002153
Iteration 52/1000 | Loss: 0.00002153
Iteration 53/1000 | Loss: 0.00002153
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002153
Iteration 56/1000 | Loss: 0.00002152
Iteration 57/1000 | Loss: 0.00002152
Iteration 58/1000 | Loss: 0.00002151
Iteration 59/1000 | Loss: 0.00002151
Iteration 60/1000 | Loss: 0.00002151
Iteration 61/1000 | Loss: 0.00002151
Iteration 62/1000 | Loss: 0.00002151
Iteration 63/1000 | Loss: 0.00002150
Iteration 64/1000 | Loss: 0.00002150
Iteration 65/1000 | Loss: 0.00002150
Iteration 66/1000 | Loss: 0.00002150
Iteration 67/1000 | Loss: 0.00002150
Iteration 68/1000 | Loss: 0.00002150
Iteration 69/1000 | Loss: 0.00002149
Iteration 70/1000 | Loss: 0.00002149
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002148
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002148
Iteration 81/1000 | Loss: 0.00002148
Iteration 82/1000 | Loss: 0.00002148
Iteration 83/1000 | Loss: 0.00002148
Iteration 84/1000 | Loss: 0.00002148
Iteration 85/1000 | Loss: 0.00002147
Iteration 86/1000 | Loss: 0.00002147
Iteration 87/1000 | Loss: 0.00002147
Iteration 88/1000 | Loss: 0.00002147
Iteration 89/1000 | Loss: 0.00002147
Iteration 90/1000 | Loss: 0.00002147
Iteration 91/1000 | Loss: 0.00002147
Iteration 92/1000 | Loss: 0.00002147
Iteration 93/1000 | Loss: 0.00002147
Iteration 94/1000 | Loss: 0.00002147
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002146
Iteration 97/1000 | Loss: 0.00002146
Iteration 98/1000 | Loss: 0.00002146
Iteration 99/1000 | Loss: 0.00002146
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002144
Iteration 105/1000 | Loss: 0.00002144
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002144
Iteration 110/1000 | Loss: 0.00002144
Iteration 111/1000 | Loss: 0.00002144
Iteration 112/1000 | Loss: 0.00002144
Iteration 113/1000 | Loss: 0.00002144
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Iteration 116/1000 | Loss: 0.00002143
Iteration 117/1000 | Loss: 0.00002143
Iteration 118/1000 | Loss: 0.00002143
Iteration 119/1000 | Loss: 0.00002143
Iteration 120/1000 | Loss: 0.00002143
Iteration 121/1000 | Loss: 0.00002142
Iteration 122/1000 | Loss: 0.00002142
Iteration 123/1000 | Loss: 0.00002142
Iteration 124/1000 | Loss: 0.00002142
Iteration 125/1000 | Loss: 0.00002142
Iteration 126/1000 | Loss: 0.00002141
Iteration 127/1000 | Loss: 0.00002141
Iteration 128/1000 | Loss: 0.00002141
Iteration 129/1000 | Loss: 0.00002141
Iteration 130/1000 | Loss: 0.00002141
Iteration 131/1000 | Loss: 0.00002141
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002141
Iteration 134/1000 | Loss: 0.00002141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.1410140107036568e-05, 2.1410140107036568e-05, 2.1410140107036568e-05, 2.1410140107036568e-05, 2.1410140107036568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1410140107036568e-05

Optimization complete. Final v2v error: 3.8653478622436523 mm

Highest mean error: 4.369308948516846 mm for frame 227

Lowest mean error: 3.476773500442505 mm for frame 72

Saving results

Total time: 42.370317459106445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813638
Iteration 2/25 | Loss: 0.00115946
Iteration 3/25 | Loss: 0.00086073
Iteration 4/25 | Loss: 0.00080439
Iteration 5/25 | Loss: 0.00078541
Iteration 6/25 | Loss: 0.00078233
Iteration 7/25 | Loss: 0.00078170
Iteration 8/25 | Loss: 0.00078170
Iteration 9/25 | Loss: 0.00078170
Iteration 10/25 | Loss: 0.00078170
Iteration 11/25 | Loss: 0.00078170
Iteration 12/25 | Loss: 0.00078170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007816951256245375, 0.0007816951256245375, 0.0007816951256245375, 0.0007816951256245375, 0.0007816951256245375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007816951256245375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47253084
Iteration 2/25 | Loss: 0.00054416
Iteration 3/25 | Loss: 0.00054415
Iteration 4/25 | Loss: 0.00054415
Iteration 5/25 | Loss: 0.00054415
Iteration 6/25 | Loss: 0.00054415
Iteration 7/25 | Loss: 0.00054415
Iteration 8/25 | Loss: 0.00054415
Iteration 9/25 | Loss: 0.00054415
Iteration 10/25 | Loss: 0.00054415
Iteration 11/25 | Loss: 0.00054415
Iteration 12/25 | Loss: 0.00054415
Iteration 13/25 | Loss: 0.00054415
Iteration 14/25 | Loss: 0.00054415
Iteration 15/25 | Loss: 0.00054415
Iteration 16/25 | Loss: 0.00054415
Iteration 17/25 | Loss: 0.00054415
Iteration 18/25 | Loss: 0.00054415
Iteration 19/25 | Loss: 0.00054415
Iteration 20/25 | Loss: 0.00054415
Iteration 21/25 | Loss: 0.00054415
Iteration 22/25 | Loss: 0.00054415
Iteration 23/25 | Loss: 0.00054415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005441458197310567, 0.0005441458197310567, 0.0005441458197310567, 0.0005441458197310567, 0.0005441458197310567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005441458197310567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054415
Iteration 2/1000 | Loss: 0.00003568
Iteration 3/1000 | Loss: 0.00002690
Iteration 4/1000 | Loss: 0.00002323
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002095
Iteration 7/1000 | Loss: 0.00002033
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001882
Iteration 17/1000 | Loss: 0.00001865
Iteration 18/1000 | Loss: 0.00001857
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001854
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001839
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001836
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001832
Iteration 59/1000 | Loss: 0.00001832
Iteration 60/1000 | Loss: 0.00001832
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001830
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001827
Iteration 86/1000 | Loss: 0.00001827
Iteration 87/1000 | Loss: 0.00001827
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001826
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001825
Iteration 105/1000 | Loss: 0.00001825
Iteration 106/1000 | Loss: 0.00001825
Iteration 107/1000 | Loss: 0.00001825
Iteration 108/1000 | Loss: 0.00001825
Iteration 109/1000 | Loss: 0.00001825
Iteration 110/1000 | Loss: 0.00001825
Iteration 111/1000 | Loss: 0.00001825
Iteration 112/1000 | Loss: 0.00001824
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001824
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00001824
Iteration 118/1000 | Loss: 0.00001824
Iteration 119/1000 | Loss: 0.00001824
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001823
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001823
Iteration 125/1000 | Loss: 0.00001823
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001823
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001823
Iteration 143/1000 | Loss: 0.00001823
Iteration 144/1000 | Loss: 0.00001823
Iteration 145/1000 | Loss: 0.00001823
Iteration 146/1000 | Loss: 0.00001823
Iteration 147/1000 | Loss: 0.00001823
Iteration 148/1000 | Loss: 0.00001823
Iteration 149/1000 | Loss: 0.00001823
Iteration 150/1000 | Loss: 0.00001823
Iteration 151/1000 | Loss: 0.00001823
Iteration 152/1000 | Loss: 0.00001823
Iteration 153/1000 | Loss: 0.00001823
Iteration 154/1000 | Loss: 0.00001823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.8227758118882775e-05, 1.8227758118882775e-05, 1.8227758118882775e-05, 1.8227758118882775e-05, 1.8227758118882775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8227758118882775e-05

Optimization complete. Final v2v error: 3.588879346847534 mm

Highest mean error: 4.760881423950195 mm for frame 164

Lowest mean error: 3.0477707386016846 mm for frame 239

Saving results

Total time: 64.94367861747742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378838
Iteration 2/25 | Loss: 0.00080856
Iteration 3/25 | Loss: 0.00073484
Iteration 4/25 | Loss: 0.00072168
Iteration 5/25 | Loss: 0.00071852
Iteration 6/25 | Loss: 0.00071778
Iteration 7/25 | Loss: 0.00071778
Iteration 8/25 | Loss: 0.00071778
Iteration 9/25 | Loss: 0.00071778
Iteration 10/25 | Loss: 0.00071778
Iteration 11/25 | Loss: 0.00071778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007177788065746427, 0.0007177788065746427, 0.0007177788065746427, 0.0007177788065746427, 0.0007177788065746427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007177788065746427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50404644
Iteration 2/25 | Loss: 0.00045827
Iteration 3/25 | Loss: 0.00045827
Iteration 4/25 | Loss: 0.00045827
Iteration 5/25 | Loss: 0.00045827
Iteration 6/25 | Loss: 0.00045827
Iteration 7/25 | Loss: 0.00045827
Iteration 8/25 | Loss: 0.00045827
Iteration 9/25 | Loss: 0.00045827
Iteration 10/25 | Loss: 0.00045827
Iteration 11/25 | Loss: 0.00045827
Iteration 12/25 | Loss: 0.00045827
Iteration 13/25 | Loss: 0.00045827
Iteration 14/25 | Loss: 0.00045827
Iteration 15/25 | Loss: 0.00045827
Iteration 16/25 | Loss: 0.00045827
Iteration 17/25 | Loss: 0.00045827
Iteration 18/25 | Loss: 0.00045827
Iteration 19/25 | Loss: 0.00045827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004582662368193269, 0.0004582662368193269, 0.0004582662368193269, 0.0004582662368193269, 0.0004582662368193269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004582662368193269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045827
Iteration 2/1000 | Loss: 0.00002284
Iteration 3/1000 | Loss: 0.00001495
Iteration 4/1000 | Loss: 0.00001411
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001267
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001181
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001166
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001165
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.1653808542178012e-05, 1.1653808542178012e-05, 1.1653808542178012e-05, 1.1653808542178012e-05, 1.1653808542178012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1653808542178012e-05

Optimization complete. Final v2v error: 2.9172275066375732 mm

Highest mean error: 2.9706039428710938 mm for frame 181

Lowest mean error: 2.842355251312256 mm for frame 191

Saving results

Total time: 39.50764274597168
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079268
Iteration 2/25 | Loss: 0.01079268
Iteration 3/25 | Loss: 0.01079268
Iteration 4/25 | Loss: 0.01079268
Iteration 5/25 | Loss: 0.01079268
Iteration 6/25 | Loss: 0.01079267
Iteration 7/25 | Loss: 0.01079267
Iteration 8/25 | Loss: 0.01079267
Iteration 9/25 | Loss: 0.01079267
Iteration 10/25 | Loss: 0.01079267
Iteration 11/25 | Loss: 0.01079267
Iteration 12/25 | Loss: 0.01079267
Iteration 13/25 | Loss: 0.01079267
Iteration 14/25 | Loss: 0.01079267
Iteration 15/25 | Loss: 0.01079267
Iteration 16/25 | Loss: 0.01079267
Iteration 17/25 | Loss: 0.01079267
Iteration 18/25 | Loss: 0.01079267
Iteration 19/25 | Loss: 0.01079267
Iteration 20/25 | Loss: 0.01079267
Iteration 21/25 | Loss: 0.01079266
Iteration 22/25 | Loss: 0.01079266
Iteration 23/25 | Loss: 0.01079266
Iteration 24/25 | Loss: 0.01079266
Iteration 25/25 | Loss: 0.01079266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61255980
Iteration 2/25 | Loss: 0.12016613
Iteration 3/25 | Loss: 0.12007881
Iteration 4/25 | Loss: 0.11980828
Iteration 5/25 | Loss: 0.11980825
Iteration 6/25 | Loss: 0.11980824
Iteration 7/25 | Loss: 0.11980822
Iteration 8/25 | Loss: 0.11980822
Iteration 9/25 | Loss: 0.11980821
Iteration 10/25 | Loss: 0.11980822
Iteration 11/25 | Loss: 0.11980822
Iteration 12/25 | Loss: 0.11980821
Iteration 13/25 | Loss: 0.11980821
Iteration 14/25 | Loss: 0.11980821
Iteration 15/25 | Loss: 0.11980821
Iteration 16/25 | Loss: 0.11980821
Iteration 17/25 | Loss: 0.11980821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.11980821192264557, 0.11980821192264557, 0.11980821192264557, 0.11980821192264557, 0.11980821192264557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11980821192264557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11980821
Iteration 2/1000 | Loss: 0.00142939
Iteration 3/1000 | Loss: 0.00088044
Iteration 4/1000 | Loss: 0.00026086
Iteration 5/1000 | Loss: 0.00027603
Iteration 6/1000 | Loss: 0.00023975
Iteration 7/1000 | Loss: 0.00004176
Iteration 8/1000 | Loss: 0.00015143
Iteration 9/1000 | Loss: 0.00137331
Iteration 10/1000 | Loss: 0.00004321
Iteration 11/1000 | Loss: 0.00005397
Iteration 12/1000 | Loss: 0.00012320
Iteration 13/1000 | Loss: 0.00107047
Iteration 14/1000 | Loss: 0.00005203
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00006724
Iteration 17/1000 | Loss: 0.00004990
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00006314
Iteration 20/1000 | Loss: 0.00002467
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00002864
Iteration 23/1000 | Loss: 0.00002963
Iteration 24/1000 | Loss: 0.00003089
Iteration 25/1000 | Loss: 0.00001884
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00002654
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00008406
Iteration 30/1000 | Loss: 0.00004831
Iteration 31/1000 | Loss: 0.00002782
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001171
Iteration 34/1000 | Loss: 0.00002018
Iteration 35/1000 | Loss: 0.00024305
Iteration 36/1000 | Loss: 0.00002950
Iteration 37/1000 | Loss: 0.00002518
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00001100
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001099
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00006738
Iteration 53/1000 | Loss: 0.00007328
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00004989
Iteration 56/1000 | Loss: 0.00001071
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001069
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001066
Iteration 63/1000 | Loss: 0.00001066
Iteration 64/1000 | Loss: 0.00001065
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00002475
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001879
Iteration 71/1000 | Loss: 0.00005251
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001056
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001054
Iteration 79/1000 | Loss: 0.00001054
Iteration 80/1000 | Loss: 0.00001054
Iteration 81/1000 | Loss: 0.00001054
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001053
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001053
Iteration 94/1000 | Loss: 0.00001053
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001052
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001052
Iteration 102/1000 | Loss: 0.00001052
Iteration 103/1000 | Loss: 0.00001052
Iteration 104/1000 | Loss: 0.00001052
Iteration 105/1000 | Loss: 0.00001052
Iteration 106/1000 | Loss: 0.00001052
Iteration 107/1000 | Loss: 0.00001051
Iteration 108/1000 | Loss: 0.00001051
Iteration 109/1000 | Loss: 0.00001051
Iteration 110/1000 | Loss: 0.00001051
Iteration 111/1000 | Loss: 0.00002109
Iteration 112/1000 | Loss: 0.00011182
Iteration 113/1000 | Loss: 0.00001606
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001061
Iteration 118/1000 | Loss: 0.00001048
Iteration 119/1000 | Loss: 0.00001048
Iteration 120/1000 | Loss: 0.00001048
Iteration 121/1000 | Loss: 0.00001048
Iteration 122/1000 | Loss: 0.00001048
Iteration 123/1000 | Loss: 0.00001048
Iteration 124/1000 | Loss: 0.00001048
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001060
Iteration 127/1000 | Loss: 0.00001047
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001047
Iteration 132/1000 | Loss: 0.00001047
Iteration 133/1000 | Loss: 0.00001047
Iteration 134/1000 | Loss: 0.00001047
Iteration 135/1000 | Loss: 0.00001047
Iteration 136/1000 | Loss: 0.00001047
Iteration 137/1000 | Loss: 0.00001047
Iteration 138/1000 | Loss: 0.00001047
Iteration 139/1000 | Loss: 0.00001047
Iteration 140/1000 | Loss: 0.00001047
Iteration 141/1000 | Loss: 0.00001047
Iteration 142/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.0465420018590521e-05, 1.0465420018590521e-05, 1.0465420018590521e-05, 1.0465420018590521e-05, 1.0465420018590521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0465420018590521e-05

Optimization complete. Final v2v error: 2.7230098247528076 mm

Highest mean error: 9.38223934173584 mm for frame 145

Lowest mean error: 2.439504384994507 mm for frame 144

Saving results

Total time: 102.83363771438599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512165
Iteration 2/25 | Loss: 0.00125054
Iteration 3/25 | Loss: 0.00097171
Iteration 4/25 | Loss: 0.00092810
Iteration 5/25 | Loss: 0.00091910
Iteration 6/25 | Loss: 0.00091815
Iteration 7/25 | Loss: 0.00091815
Iteration 8/25 | Loss: 0.00091815
Iteration 9/25 | Loss: 0.00091815
Iteration 10/25 | Loss: 0.00091815
Iteration 11/25 | Loss: 0.00091815
Iteration 12/25 | Loss: 0.00091815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000918149424251169, 0.000918149424251169, 0.000918149424251169, 0.000918149424251169, 0.000918149424251169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000918149424251169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52419233
Iteration 2/25 | Loss: 0.00055316
Iteration 3/25 | Loss: 0.00055316
Iteration 4/25 | Loss: 0.00055316
Iteration 5/25 | Loss: 0.00055316
Iteration 6/25 | Loss: 0.00055315
Iteration 7/25 | Loss: 0.00055315
Iteration 8/25 | Loss: 0.00055315
Iteration 9/25 | Loss: 0.00055315
Iteration 10/25 | Loss: 0.00055315
Iteration 11/25 | Loss: 0.00055315
Iteration 12/25 | Loss: 0.00055315
Iteration 13/25 | Loss: 0.00055315
Iteration 14/25 | Loss: 0.00055315
Iteration 15/25 | Loss: 0.00055315
Iteration 16/25 | Loss: 0.00055315
Iteration 17/25 | Loss: 0.00055315
Iteration 18/25 | Loss: 0.00055315
Iteration 19/25 | Loss: 0.00055315
Iteration 20/25 | Loss: 0.00055315
Iteration 21/25 | Loss: 0.00055315
Iteration 22/25 | Loss: 0.00055315
Iteration 23/25 | Loss: 0.00055315
Iteration 24/25 | Loss: 0.00055315
Iteration 25/25 | Loss: 0.00055315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055315
Iteration 2/1000 | Loss: 0.00005005
Iteration 3/1000 | Loss: 0.00003335
Iteration 4/1000 | Loss: 0.00003071
Iteration 5/1000 | Loss: 0.00002956
Iteration 6/1000 | Loss: 0.00002869
Iteration 7/1000 | Loss: 0.00002810
Iteration 8/1000 | Loss: 0.00002769
Iteration 9/1000 | Loss: 0.00002737
Iteration 10/1000 | Loss: 0.00002719
Iteration 11/1000 | Loss: 0.00002704
Iteration 12/1000 | Loss: 0.00002701
Iteration 13/1000 | Loss: 0.00002700
Iteration 14/1000 | Loss: 0.00002700
Iteration 15/1000 | Loss: 0.00002699
Iteration 16/1000 | Loss: 0.00002698
Iteration 17/1000 | Loss: 0.00002697
Iteration 18/1000 | Loss: 0.00002697
Iteration 19/1000 | Loss: 0.00002696
Iteration 20/1000 | Loss: 0.00002693
Iteration 21/1000 | Loss: 0.00002693
Iteration 22/1000 | Loss: 0.00002692
Iteration 23/1000 | Loss: 0.00002692
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00002691
Iteration 26/1000 | Loss: 0.00002690
Iteration 27/1000 | Loss: 0.00002690
Iteration 28/1000 | Loss: 0.00002690
Iteration 29/1000 | Loss: 0.00002690
Iteration 30/1000 | Loss: 0.00002690
Iteration 31/1000 | Loss: 0.00002690
Iteration 32/1000 | Loss: 0.00002690
Iteration 33/1000 | Loss: 0.00002690
Iteration 34/1000 | Loss: 0.00002690
Iteration 35/1000 | Loss: 0.00002689
Iteration 36/1000 | Loss: 0.00002689
Iteration 37/1000 | Loss: 0.00002689
Iteration 38/1000 | Loss: 0.00002689
Iteration 39/1000 | Loss: 0.00002689
Iteration 40/1000 | Loss: 0.00002689
Iteration 41/1000 | Loss: 0.00002688
Iteration 42/1000 | Loss: 0.00002688
Iteration 43/1000 | Loss: 0.00002688
Iteration 44/1000 | Loss: 0.00002688
Iteration 45/1000 | Loss: 0.00002688
Iteration 46/1000 | Loss: 0.00002688
Iteration 47/1000 | Loss: 0.00002687
Iteration 48/1000 | Loss: 0.00002687
Iteration 49/1000 | Loss: 0.00002687
Iteration 50/1000 | Loss: 0.00002687
Iteration 51/1000 | Loss: 0.00002687
Iteration 52/1000 | Loss: 0.00002687
Iteration 53/1000 | Loss: 0.00002687
Iteration 54/1000 | Loss: 0.00002687
Iteration 55/1000 | Loss: 0.00002686
Iteration 56/1000 | Loss: 0.00002686
Iteration 57/1000 | Loss: 0.00002686
Iteration 58/1000 | Loss: 0.00002686
Iteration 59/1000 | Loss: 0.00002686
Iteration 60/1000 | Loss: 0.00002686
Iteration 61/1000 | Loss: 0.00002686
Iteration 62/1000 | Loss: 0.00002686
Iteration 63/1000 | Loss: 0.00002685
Iteration 64/1000 | Loss: 0.00002685
Iteration 65/1000 | Loss: 0.00002685
Iteration 66/1000 | Loss: 0.00002685
Iteration 67/1000 | Loss: 0.00002685
Iteration 68/1000 | Loss: 0.00002685
Iteration 69/1000 | Loss: 0.00002685
Iteration 70/1000 | Loss: 0.00002685
Iteration 71/1000 | Loss: 0.00002685
Iteration 72/1000 | Loss: 0.00002685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.6847947083297186e-05, 2.6847947083297186e-05, 2.6847947083297186e-05, 2.6847947083297186e-05, 2.6847947083297186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6847947083297186e-05

Optimization complete. Final v2v error: 4.291534900665283 mm

Highest mean error: 5.077690124511719 mm for frame 172

Lowest mean error: 3.735895872116089 mm for frame 0

Saving results

Total time: 59.40232491493225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029684
Iteration 2/25 | Loss: 0.00291234
Iteration 3/25 | Loss: 0.00175395
Iteration 4/25 | Loss: 0.00154573
Iteration 5/25 | Loss: 0.00142212
Iteration 6/25 | Loss: 0.00147210
Iteration 7/25 | Loss: 0.00152149
Iteration 8/25 | Loss: 0.00145014
Iteration 9/25 | Loss: 0.00128547
Iteration 10/25 | Loss: 0.00122517
Iteration 11/25 | Loss: 0.00120522
Iteration 12/25 | Loss: 0.00116097
Iteration 13/25 | Loss: 0.00112939
Iteration 14/25 | Loss: 0.00111679
Iteration 15/25 | Loss: 0.00109261
Iteration 16/25 | Loss: 0.00109210
Iteration 17/25 | Loss: 0.00108123
Iteration 18/25 | Loss: 0.00107973
Iteration 19/25 | Loss: 0.00107149
Iteration 20/25 | Loss: 0.00105683
Iteration 21/25 | Loss: 0.00105377
Iteration 22/25 | Loss: 0.00105475
Iteration 23/25 | Loss: 0.00104869
Iteration 24/25 | Loss: 0.00104426
Iteration 25/25 | Loss: 0.00103917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56409895
Iteration 2/25 | Loss: 0.00332283
Iteration 3/25 | Loss: 0.00316430
Iteration 4/25 | Loss: 0.00316430
Iteration 5/25 | Loss: 0.00316430
Iteration 6/25 | Loss: 0.00316430
Iteration 7/25 | Loss: 0.00316430
Iteration 8/25 | Loss: 0.00316430
Iteration 9/25 | Loss: 0.00316430
Iteration 10/25 | Loss: 0.00316430
Iteration 11/25 | Loss: 0.00316430
Iteration 12/25 | Loss: 0.00316430
Iteration 13/25 | Loss: 0.00316430
Iteration 14/25 | Loss: 0.00316430
Iteration 15/25 | Loss: 0.00316430
Iteration 16/25 | Loss: 0.00316430
Iteration 17/25 | Loss: 0.00316430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0031643006950616837, 0.0031643006950616837, 0.0031643006950616837, 0.0031643006950616837, 0.0031643006950616837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031643006950616837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316430
Iteration 2/1000 | Loss: 0.00313741
Iteration 3/1000 | Loss: 0.00129871
Iteration 4/1000 | Loss: 0.00072164
Iteration 5/1000 | Loss: 0.00102424
Iteration 6/1000 | Loss: 0.00188608
Iteration 7/1000 | Loss: 0.00123364
Iteration 8/1000 | Loss: 0.00297068
Iteration 9/1000 | Loss: 0.00354448
Iteration 10/1000 | Loss: 0.00116652
Iteration 11/1000 | Loss: 0.00164533
Iteration 12/1000 | Loss: 0.00281234
Iteration 13/1000 | Loss: 0.00290766
Iteration 14/1000 | Loss: 0.00123798
Iteration 15/1000 | Loss: 0.00036641
Iteration 16/1000 | Loss: 0.00087725
Iteration 17/1000 | Loss: 0.00145358
Iteration 18/1000 | Loss: 0.00114738
Iteration 19/1000 | Loss: 0.00081139
Iteration 20/1000 | Loss: 0.00295372
Iteration 21/1000 | Loss: 0.00277901
Iteration 22/1000 | Loss: 0.00317636
Iteration 23/1000 | Loss: 0.00218190
Iteration 24/1000 | Loss: 0.00277947
Iteration 25/1000 | Loss: 0.00144286
Iteration 26/1000 | Loss: 0.00120479
Iteration 27/1000 | Loss: 0.00135927
Iteration 28/1000 | Loss: 0.00057473
Iteration 29/1000 | Loss: 0.00098225
Iteration 30/1000 | Loss: 0.00094323
Iteration 31/1000 | Loss: 0.00067870
Iteration 32/1000 | Loss: 0.00040599
Iteration 33/1000 | Loss: 0.00066165
Iteration 34/1000 | Loss: 0.00041344
Iteration 35/1000 | Loss: 0.00079515
Iteration 36/1000 | Loss: 0.00110948
Iteration 37/1000 | Loss: 0.00160795
Iteration 38/1000 | Loss: 0.00227525
Iteration 39/1000 | Loss: 0.00135551
Iteration 40/1000 | Loss: 0.00071083
Iteration 41/1000 | Loss: 0.00057401
Iteration 42/1000 | Loss: 0.00135638
Iteration 43/1000 | Loss: 0.00057272
Iteration 44/1000 | Loss: 0.00079817
Iteration 45/1000 | Loss: 0.00178391
Iteration 46/1000 | Loss: 0.00108817
Iteration 47/1000 | Loss: 0.00036611
Iteration 48/1000 | Loss: 0.00022258
Iteration 49/1000 | Loss: 0.00146645
Iteration 50/1000 | Loss: 0.00064451
Iteration 51/1000 | Loss: 0.00060891
Iteration 52/1000 | Loss: 0.00044431
Iteration 53/1000 | Loss: 0.00030882
Iteration 54/1000 | Loss: 0.00023365
Iteration 55/1000 | Loss: 0.00081195
Iteration 56/1000 | Loss: 0.00169960
Iteration 57/1000 | Loss: 0.00244642
Iteration 58/1000 | Loss: 0.00171131
Iteration 59/1000 | Loss: 0.00082212
Iteration 60/1000 | Loss: 0.00050364
Iteration 61/1000 | Loss: 0.00120281
Iteration 62/1000 | Loss: 0.00081722
Iteration 63/1000 | Loss: 0.00024798
Iteration 64/1000 | Loss: 0.00046511
Iteration 65/1000 | Loss: 0.00096483
Iteration 66/1000 | Loss: 0.00056705
Iteration 67/1000 | Loss: 0.00093995
Iteration 68/1000 | Loss: 0.00040062
Iteration 69/1000 | Loss: 0.00012241
Iteration 70/1000 | Loss: 0.00040107
Iteration 71/1000 | Loss: 0.00092175
Iteration 72/1000 | Loss: 0.00015961
Iteration 73/1000 | Loss: 0.00012440
Iteration 74/1000 | Loss: 0.00028829
Iteration 75/1000 | Loss: 0.00039384
Iteration 76/1000 | Loss: 0.00042866
Iteration 77/1000 | Loss: 0.00022399
Iteration 78/1000 | Loss: 0.00042888
Iteration 79/1000 | Loss: 0.00028846
Iteration 80/1000 | Loss: 0.00050285
Iteration 81/1000 | Loss: 0.00021124
Iteration 82/1000 | Loss: 0.00026286
Iteration 83/1000 | Loss: 0.00092467
Iteration 84/1000 | Loss: 0.00085072
Iteration 85/1000 | Loss: 0.00087753
Iteration 86/1000 | Loss: 0.00086715
Iteration 87/1000 | Loss: 0.00024637
Iteration 88/1000 | Loss: 0.00015966
Iteration 89/1000 | Loss: 0.00034946
Iteration 90/1000 | Loss: 0.00045957
Iteration 91/1000 | Loss: 0.00046523
Iteration 92/1000 | Loss: 0.00014853
Iteration 93/1000 | Loss: 0.00044777
Iteration 94/1000 | Loss: 0.00037025
Iteration 95/1000 | Loss: 0.00062258
Iteration 96/1000 | Loss: 0.00012851
Iteration 97/1000 | Loss: 0.00058490
Iteration 98/1000 | Loss: 0.00070393
Iteration 99/1000 | Loss: 0.00016871
Iteration 100/1000 | Loss: 0.00018838
Iteration 101/1000 | Loss: 0.00062710
Iteration 102/1000 | Loss: 0.00063533
Iteration 103/1000 | Loss: 0.00061501
Iteration 104/1000 | Loss: 0.00098130
Iteration 105/1000 | Loss: 0.00018072
Iteration 106/1000 | Loss: 0.00037544
Iteration 107/1000 | Loss: 0.00043373
Iteration 108/1000 | Loss: 0.00098741
Iteration 109/1000 | Loss: 0.00049844
Iteration 110/1000 | Loss: 0.00071145
Iteration 111/1000 | Loss: 0.00059380
Iteration 112/1000 | Loss: 0.00006793
Iteration 113/1000 | Loss: 0.00005351
Iteration 114/1000 | Loss: 0.00010568
Iteration 115/1000 | Loss: 0.00009685
Iteration 116/1000 | Loss: 0.00011672
Iteration 117/1000 | Loss: 0.00006210
Iteration 118/1000 | Loss: 0.00004335
Iteration 119/1000 | Loss: 0.00006018
Iteration 120/1000 | Loss: 0.00028049
Iteration 121/1000 | Loss: 0.00016996
Iteration 122/1000 | Loss: 0.00004604
Iteration 123/1000 | Loss: 0.00004335
Iteration 124/1000 | Loss: 0.00004353
Iteration 125/1000 | Loss: 0.00004209
Iteration 126/1000 | Loss: 0.00003844
Iteration 127/1000 | Loss: 0.00002821
Iteration 128/1000 | Loss: 0.00004570
Iteration 129/1000 | Loss: 0.00004312
Iteration 130/1000 | Loss: 0.00027194
Iteration 131/1000 | Loss: 0.00016585
Iteration 132/1000 | Loss: 0.00016687
Iteration 133/1000 | Loss: 0.00005192
Iteration 134/1000 | Loss: 0.00025794
Iteration 135/1000 | Loss: 0.00005100
Iteration 136/1000 | Loss: 0.00004918
Iteration 137/1000 | Loss: 0.00005446
Iteration 138/1000 | Loss: 0.00003247
Iteration 139/1000 | Loss: 0.00005910
Iteration 140/1000 | Loss: 0.00004881
Iteration 141/1000 | Loss: 0.00005264
Iteration 142/1000 | Loss: 0.00005153
Iteration 143/1000 | Loss: 0.00004935
Iteration 144/1000 | Loss: 0.00004800
Iteration 145/1000 | Loss: 0.00005251
Iteration 146/1000 | Loss: 0.00005272
Iteration 147/1000 | Loss: 0.00004148
Iteration 148/1000 | Loss: 0.00005547
Iteration 149/1000 | Loss: 0.00004790
Iteration 150/1000 | Loss: 0.00003940
Iteration 151/1000 | Loss: 0.00026656
Iteration 152/1000 | Loss: 0.00008060
Iteration 153/1000 | Loss: 0.00006562
Iteration 154/1000 | Loss: 0.00003452
Iteration 155/1000 | Loss: 0.00002663
Iteration 156/1000 | Loss: 0.00002299
Iteration 157/1000 | Loss: 0.00002116
Iteration 158/1000 | Loss: 0.00001996
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001886
Iteration 161/1000 | Loss: 0.00070167
Iteration 162/1000 | Loss: 0.00003344
Iteration 163/1000 | Loss: 0.00002096
Iteration 164/1000 | Loss: 0.00001899
Iteration 165/1000 | Loss: 0.00001813
Iteration 166/1000 | Loss: 0.00001730
Iteration 167/1000 | Loss: 0.00001640
Iteration 168/1000 | Loss: 0.00001593
Iteration 169/1000 | Loss: 0.00001574
Iteration 170/1000 | Loss: 0.00001572
Iteration 171/1000 | Loss: 0.00001561
Iteration 172/1000 | Loss: 0.00001559
Iteration 173/1000 | Loss: 0.00001550
Iteration 174/1000 | Loss: 0.00001542
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001538
Iteration 177/1000 | Loss: 0.00001537
Iteration 178/1000 | Loss: 0.00001533
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001521
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00001515
Iteration 183/1000 | Loss: 0.00001511
Iteration 184/1000 | Loss: 0.00001508
Iteration 185/1000 | Loss: 0.00001508
Iteration 186/1000 | Loss: 0.00001508
Iteration 187/1000 | Loss: 0.00001508
Iteration 188/1000 | Loss: 0.00001507
Iteration 189/1000 | Loss: 0.00001507
Iteration 190/1000 | Loss: 0.00001507
Iteration 191/1000 | Loss: 0.00001507
Iteration 192/1000 | Loss: 0.00001507
Iteration 193/1000 | Loss: 0.00001507
Iteration 194/1000 | Loss: 0.00001507
Iteration 195/1000 | Loss: 0.00001507
Iteration 196/1000 | Loss: 0.00001506
Iteration 197/1000 | Loss: 0.00001506
Iteration 198/1000 | Loss: 0.00001504
Iteration 199/1000 | Loss: 0.00001504
Iteration 200/1000 | Loss: 0.00001503
Iteration 201/1000 | Loss: 0.00001503
Iteration 202/1000 | Loss: 0.00001503
Iteration 203/1000 | Loss: 0.00001503
Iteration 204/1000 | Loss: 0.00001503
Iteration 205/1000 | Loss: 0.00001502
Iteration 206/1000 | Loss: 0.00001502
Iteration 207/1000 | Loss: 0.00001502
Iteration 208/1000 | Loss: 0.00001502
Iteration 209/1000 | Loss: 0.00001502
Iteration 210/1000 | Loss: 0.00001501
Iteration 211/1000 | Loss: 0.00001501
Iteration 212/1000 | Loss: 0.00001501
Iteration 213/1000 | Loss: 0.00001500
Iteration 214/1000 | Loss: 0.00001500
Iteration 215/1000 | Loss: 0.00001500
Iteration 216/1000 | Loss: 0.00001500
Iteration 217/1000 | Loss: 0.00001500
Iteration 218/1000 | Loss: 0.00001499
Iteration 219/1000 | Loss: 0.00001499
Iteration 220/1000 | Loss: 0.00001499
Iteration 221/1000 | Loss: 0.00001499
Iteration 222/1000 | Loss: 0.00001499
Iteration 223/1000 | Loss: 0.00001499
Iteration 224/1000 | Loss: 0.00001499
Iteration 225/1000 | Loss: 0.00001498
Iteration 226/1000 | Loss: 0.00001498
Iteration 227/1000 | Loss: 0.00001498
Iteration 228/1000 | Loss: 0.00001498
Iteration 229/1000 | Loss: 0.00001498
Iteration 230/1000 | Loss: 0.00001498
Iteration 231/1000 | Loss: 0.00001498
Iteration 232/1000 | Loss: 0.00001498
Iteration 233/1000 | Loss: 0.00001498
Iteration 234/1000 | Loss: 0.00001498
Iteration 235/1000 | Loss: 0.00001498
Iteration 236/1000 | Loss: 0.00001498
Iteration 237/1000 | Loss: 0.00001498
Iteration 238/1000 | Loss: 0.00001498
Iteration 239/1000 | Loss: 0.00001498
Iteration 240/1000 | Loss: 0.00001498
Iteration 241/1000 | Loss: 0.00001498
Iteration 242/1000 | Loss: 0.00001498
Iteration 243/1000 | Loss: 0.00001498
Iteration 244/1000 | Loss: 0.00001497
Iteration 245/1000 | Loss: 0.00001497
Iteration 246/1000 | Loss: 0.00001497
Iteration 247/1000 | Loss: 0.00001497
Iteration 248/1000 | Loss: 0.00001497
Iteration 249/1000 | Loss: 0.00001497
Iteration 250/1000 | Loss: 0.00001497
Iteration 251/1000 | Loss: 0.00001497
Iteration 252/1000 | Loss: 0.00001497
Iteration 253/1000 | Loss: 0.00001497
Iteration 254/1000 | Loss: 0.00001497
Iteration 255/1000 | Loss: 0.00001497
Iteration 256/1000 | Loss: 0.00001497
Iteration 257/1000 | Loss: 0.00001497
Iteration 258/1000 | Loss: 0.00001497
Iteration 259/1000 | Loss: 0.00001497
Iteration 260/1000 | Loss: 0.00001497
Iteration 261/1000 | Loss: 0.00001497
Iteration 262/1000 | Loss: 0.00001497
Iteration 263/1000 | Loss: 0.00001497
Iteration 264/1000 | Loss: 0.00001497
Iteration 265/1000 | Loss: 0.00001497
Iteration 266/1000 | Loss: 0.00001497
Iteration 267/1000 | Loss: 0.00001497
Iteration 268/1000 | Loss: 0.00001497
Iteration 269/1000 | Loss: 0.00001496
Iteration 270/1000 | Loss: 0.00001496
Iteration 271/1000 | Loss: 0.00001496
Iteration 272/1000 | Loss: 0.00001496
Iteration 273/1000 | Loss: 0.00001496
Iteration 274/1000 | Loss: 0.00001496
Iteration 275/1000 | Loss: 0.00001496
Iteration 276/1000 | Loss: 0.00001496
Iteration 277/1000 | Loss: 0.00001496
Iteration 278/1000 | Loss: 0.00001496
Iteration 279/1000 | Loss: 0.00001496
Iteration 280/1000 | Loss: 0.00001496
Iteration 281/1000 | Loss: 0.00001496
Iteration 282/1000 | Loss: 0.00001496
Iteration 283/1000 | Loss: 0.00001496
Iteration 284/1000 | Loss: 0.00001496
Iteration 285/1000 | Loss: 0.00001496
Iteration 286/1000 | Loss: 0.00001496
Iteration 287/1000 | Loss: 0.00001496
Iteration 288/1000 | Loss: 0.00001496
Iteration 289/1000 | Loss: 0.00001496
Iteration 290/1000 | Loss: 0.00001496
Iteration 291/1000 | Loss: 0.00001495
Iteration 292/1000 | Loss: 0.00001495
Iteration 293/1000 | Loss: 0.00001495
Iteration 294/1000 | Loss: 0.00001495
Iteration 295/1000 | Loss: 0.00001495
Iteration 296/1000 | Loss: 0.00001495
Iteration 297/1000 | Loss: 0.00001495
Iteration 298/1000 | Loss: 0.00001495
Iteration 299/1000 | Loss: 0.00001495
Iteration 300/1000 | Loss: 0.00001495
Iteration 301/1000 | Loss: 0.00001495
Iteration 302/1000 | Loss: 0.00001495
Iteration 303/1000 | Loss: 0.00001495
Iteration 304/1000 | Loss: 0.00001495
Iteration 305/1000 | Loss: 0.00001495
Iteration 306/1000 | Loss: 0.00001495
Iteration 307/1000 | Loss: 0.00001495
Iteration 308/1000 | Loss: 0.00001495
Iteration 309/1000 | Loss: 0.00001495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.495380638516508e-05, 1.495380638516508e-05, 1.495380638516508e-05, 1.495380638516508e-05, 1.495380638516508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.495380638516508e-05

Optimization complete. Final v2v error: 3.2205348014831543 mm

Highest mean error: 4.714015483856201 mm for frame 108

Lowest mean error: 2.5583198070526123 mm for frame 172

Saving results

Total time: 339.53244733810425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747241
Iteration 2/25 | Loss: 0.00146175
Iteration 3/25 | Loss: 0.00107215
Iteration 4/25 | Loss: 0.00089980
Iteration 5/25 | Loss: 0.00083885
Iteration 6/25 | Loss: 0.00081946
Iteration 7/25 | Loss: 0.00081605
Iteration 8/25 | Loss: 0.00081018
Iteration 9/25 | Loss: 0.00080861
Iteration 10/25 | Loss: 0.00080689
Iteration 11/25 | Loss: 0.00080404
Iteration 12/25 | Loss: 0.00080239
Iteration 13/25 | Loss: 0.00080165
Iteration 14/25 | Loss: 0.00080132
Iteration 15/25 | Loss: 0.00080120
Iteration 16/25 | Loss: 0.00080120
Iteration 17/25 | Loss: 0.00080120
Iteration 18/25 | Loss: 0.00080120
Iteration 19/25 | Loss: 0.00080120
Iteration 20/25 | Loss: 0.00080120
Iteration 21/25 | Loss: 0.00080120
Iteration 22/25 | Loss: 0.00080120
Iteration 23/25 | Loss: 0.00080119
Iteration 24/25 | Loss: 0.00080119
Iteration 25/25 | Loss: 0.00080119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94226968
Iteration 2/25 | Loss: 0.00049390
Iteration 3/25 | Loss: 0.00049390
Iteration 4/25 | Loss: 0.00049390
Iteration 5/25 | Loss: 0.00049390
Iteration 6/25 | Loss: 0.00049389
Iteration 7/25 | Loss: 0.00049389
Iteration 8/25 | Loss: 0.00049389
Iteration 9/25 | Loss: 0.00049389
Iteration 10/25 | Loss: 0.00049389
Iteration 11/25 | Loss: 0.00049389
Iteration 12/25 | Loss: 0.00049389
Iteration 13/25 | Loss: 0.00049389
Iteration 14/25 | Loss: 0.00049389
Iteration 15/25 | Loss: 0.00049389
Iteration 16/25 | Loss: 0.00049389
Iteration 17/25 | Loss: 0.00049389
Iteration 18/25 | Loss: 0.00049389
Iteration 19/25 | Loss: 0.00049389
Iteration 20/25 | Loss: 0.00049389
Iteration 21/25 | Loss: 0.00049389
Iteration 22/25 | Loss: 0.00049389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004938935744576156, 0.0004938935744576156, 0.0004938935744576156, 0.0004938935744576156, 0.0004938935744576156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004938935744576156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049389
Iteration 2/1000 | Loss: 0.00003791
Iteration 3/1000 | Loss: 0.00002915
Iteration 4/1000 | Loss: 0.00002730
Iteration 5/1000 | Loss: 0.00002592
Iteration 6/1000 | Loss: 0.00002518
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002403
Iteration 9/1000 | Loss: 0.00002375
Iteration 10/1000 | Loss: 0.00002350
Iteration 11/1000 | Loss: 0.00002336
Iteration 12/1000 | Loss: 0.00002329
Iteration 13/1000 | Loss: 0.00002327
Iteration 14/1000 | Loss: 0.00002319
Iteration 15/1000 | Loss: 0.00002318
Iteration 16/1000 | Loss: 0.00002313
Iteration 17/1000 | Loss: 0.00002313
Iteration 18/1000 | Loss: 0.00002313
Iteration 19/1000 | Loss: 0.00002312
Iteration 20/1000 | Loss: 0.00002312
Iteration 21/1000 | Loss: 0.00002308
Iteration 22/1000 | Loss: 0.00002308
Iteration 23/1000 | Loss: 0.00002304
Iteration 24/1000 | Loss: 0.00002300
Iteration 25/1000 | Loss: 0.00002299
Iteration 26/1000 | Loss: 0.00002294
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002293
Iteration 29/1000 | Loss: 0.00002292
Iteration 30/1000 | Loss: 0.00002292
Iteration 31/1000 | Loss: 0.00002291
Iteration 32/1000 | Loss: 0.00002291
Iteration 33/1000 | Loss: 0.00002291
Iteration 34/1000 | Loss: 0.00002290
Iteration 35/1000 | Loss: 0.00002290
Iteration 36/1000 | Loss: 0.00002290
Iteration 37/1000 | Loss: 0.00002289
Iteration 38/1000 | Loss: 0.00002289
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00002289
Iteration 41/1000 | Loss: 0.00002288
Iteration 42/1000 | Loss: 0.00002288
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002287
Iteration 47/1000 | Loss: 0.00002287
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002287
Iteration 50/1000 | Loss: 0.00002286
Iteration 51/1000 | Loss: 0.00002286
Iteration 52/1000 | Loss: 0.00002286
Iteration 53/1000 | Loss: 0.00002286
Iteration 54/1000 | Loss: 0.00002286
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002286
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002286
Iteration 59/1000 | Loss: 0.00002286
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002286
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002285
Iteration 65/1000 | Loss: 0.00002285
Iteration 66/1000 | Loss: 0.00002285
Iteration 67/1000 | Loss: 0.00002285
Iteration 68/1000 | Loss: 0.00002285
Iteration 69/1000 | Loss: 0.00002284
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002284
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002284
Iteration 76/1000 | Loss: 0.00002284
Iteration 77/1000 | Loss: 0.00002284
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002283
Iteration 80/1000 | Loss: 0.00002283
Iteration 81/1000 | Loss: 0.00002283
Iteration 82/1000 | Loss: 0.00002283
Iteration 83/1000 | Loss: 0.00002283
Iteration 84/1000 | Loss: 0.00002283
Iteration 85/1000 | Loss: 0.00002282
Iteration 86/1000 | Loss: 0.00002282
Iteration 87/1000 | Loss: 0.00002282
Iteration 88/1000 | Loss: 0.00002282
Iteration 89/1000 | Loss: 0.00002282
Iteration 90/1000 | Loss: 0.00002282
Iteration 91/1000 | Loss: 0.00002281
Iteration 92/1000 | Loss: 0.00002281
Iteration 93/1000 | Loss: 0.00002281
Iteration 94/1000 | Loss: 0.00002281
Iteration 95/1000 | Loss: 0.00002281
Iteration 96/1000 | Loss: 0.00002280
Iteration 97/1000 | Loss: 0.00002280
Iteration 98/1000 | Loss: 0.00002280
Iteration 99/1000 | Loss: 0.00002280
Iteration 100/1000 | Loss: 0.00002279
Iteration 101/1000 | Loss: 0.00002279
Iteration 102/1000 | Loss: 0.00002279
Iteration 103/1000 | Loss: 0.00002279
Iteration 104/1000 | Loss: 0.00002279
Iteration 105/1000 | Loss: 0.00002279
Iteration 106/1000 | Loss: 0.00002279
Iteration 107/1000 | Loss: 0.00002278
Iteration 108/1000 | Loss: 0.00002278
Iteration 109/1000 | Loss: 0.00002278
Iteration 110/1000 | Loss: 0.00002278
Iteration 111/1000 | Loss: 0.00002278
Iteration 112/1000 | Loss: 0.00002278
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00002278
Iteration 115/1000 | Loss: 0.00002278
Iteration 116/1000 | Loss: 0.00002278
Iteration 117/1000 | Loss: 0.00002278
Iteration 118/1000 | Loss: 0.00002278
Iteration 119/1000 | Loss: 0.00002278
Iteration 120/1000 | Loss: 0.00002278
Iteration 121/1000 | Loss: 0.00002278
Iteration 122/1000 | Loss: 0.00002278
Iteration 123/1000 | Loss: 0.00002278
Iteration 124/1000 | Loss: 0.00002278
Iteration 125/1000 | Loss: 0.00002278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.278236934216693e-05, 2.278236934216693e-05, 2.278236934216693e-05, 2.278236934216693e-05, 2.278236934216693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.278236934216693e-05

Optimization complete. Final v2v error: 4.000947952270508 mm

Highest mean error: 4.3585968017578125 mm for frame 101

Lowest mean error: 3.6249895095825195 mm for frame 53

Saving results

Total time: 60.70459842681885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373393
Iteration 2/25 | Loss: 0.00086286
Iteration 3/25 | Loss: 0.00076455
Iteration 4/25 | Loss: 0.00074851
Iteration 5/25 | Loss: 0.00074327
Iteration 6/25 | Loss: 0.00074196
Iteration 7/25 | Loss: 0.00074179
Iteration 8/25 | Loss: 0.00074179
Iteration 9/25 | Loss: 0.00074179
Iteration 10/25 | Loss: 0.00074179
Iteration 11/25 | Loss: 0.00074179
Iteration 12/25 | Loss: 0.00074179
Iteration 13/25 | Loss: 0.00074179
Iteration 14/25 | Loss: 0.00074179
Iteration 15/25 | Loss: 0.00074179
Iteration 16/25 | Loss: 0.00074179
Iteration 17/25 | Loss: 0.00074179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007417860324494541, 0.0007417860324494541, 0.0007417860324494541, 0.0007417860324494541, 0.0007417860324494541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007417860324494541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51730537
Iteration 2/25 | Loss: 0.00055302
Iteration 3/25 | Loss: 0.00055302
Iteration 4/25 | Loss: 0.00055302
Iteration 5/25 | Loss: 0.00055301
Iteration 6/25 | Loss: 0.00055301
Iteration 7/25 | Loss: 0.00055301
Iteration 8/25 | Loss: 0.00055301
Iteration 9/25 | Loss: 0.00055301
Iteration 10/25 | Loss: 0.00055301
Iteration 11/25 | Loss: 0.00055301
Iteration 12/25 | Loss: 0.00055301
Iteration 13/25 | Loss: 0.00055301
Iteration 14/25 | Loss: 0.00055301
Iteration 15/25 | Loss: 0.00055301
Iteration 16/25 | Loss: 0.00055301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005530136986635625, 0.0005530136986635625, 0.0005530136986635625, 0.0005530136986635625, 0.0005530136986635625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005530136986635625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055301
Iteration 2/1000 | Loss: 0.00002077
Iteration 3/1000 | Loss: 0.00001418
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001260
Iteration 6/1000 | Loss: 0.00001228
Iteration 7/1000 | Loss: 0.00001227
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001181
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001177
Iteration 21/1000 | Loss: 0.00001176
Iteration 22/1000 | Loss: 0.00001176
Iteration 23/1000 | Loss: 0.00001175
Iteration 24/1000 | Loss: 0.00001167
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001163
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001162
Iteration 31/1000 | Loss: 0.00001162
Iteration 32/1000 | Loss: 0.00001162
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001161
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001155
Iteration 73/1000 | Loss: 0.00001155
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001155
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001155
Iteration 89/1000 | Loss: 0.00001155
Iteration 90/1000 | Loss: 0.00001155
Iteration 91/1000 | Loss: 0.00001155
Iteration 92/1000 | Loss: 0.00001155
Iteration 93/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.1553996955626644e-05, 1.1553996955626644e-05, 1.1553996955626644e-05, 1.1553996955626644e-05, 1.1553996955626644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1553996955626644e-05

Optimization complete. Final v2v error: 2.8519392013549805 mm

Highest mean error: 3.0599334239959717 mm for frame 10

Lowest mean error: 2.6037909984588623 mm for frame 98

Saving results

Total time: 29.57476544380188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760764
Iteration 2/25 | Loss: 0.00132594
Iteration 3/25 | Loss: 0.00091606
Iteration 4/25 | Loss: 0.00084416
Iteration 5/25 | Loss: 0.00082375
Iteration 6/25 | Loss: 0.00082204
Iteration 7/25 | Loss: 0.00082204
Iteration 8/25 | Loss: 0.00082204
Iteration 9/25 | Loss: 0.00082204
Iteration 10/25 | Loss: 0.00082204
Iteration 11/25 | Loss: 0.00082204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008220387971960008, 0.0008220387971960008, 0.0008220387971960008, 0.0008220387971960008, 0.0008220387971960008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008220387971960008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49039066
Iteration 2/25 | Loss: 0.00046475
Iteration 3/25 | Loss: 0.00046475
Iteration 4/25 | Loss: 0.00046475
Iteration 5/25 | Loss: 0.00046475
Iteration 6/25 | Loss: 0.00046474
Iteration 7/25 | Loss: 0.00046474
Iteration 8/25 | Loss: 0.00046474
Iteration 9/25 | Loss: 0.00046474
Iteration 10/25 | Loss: 0.00046474
Iteration 11/25 | Loss: 0.00046474
Iteration 12/25 | Loss: 0.00046474
Iteration 13/25 | Loss: 0.00046474
Iteration 14/25 | Loss: 0.00046474
Iteration 15/25 | Loss: 0.00046474
Iteration 16/25 | Loss: 0.00046474
Iteration 17/25 | Loss: 0.00046474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004647439927794039, 0.0004647439927794039, 0.0004647439927794039, 0.0004647439927794039, 0.0004647439927794039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004647439927794039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046474
Iteration 2/1000 | Loss: 0.00003639
Iteration 3/1000 | Loss: 0.00002744
Iteration 4/1000 | Loss: 0.00002519
Iteration 5/1000 | Loss: 0.00002366
Iteration 6/1000 | Loss: 0.00002250
Iteration 7/1000 | Loss: 0.00002171
Iteration 8/1000 | Loss: 0.00002101
Iteration 9/1000 | Loss: 0.00002063
Iteration 10/1000 | Loss: 0.00002033
Iteration 11/1000 | Loss: 0.00002011
Iteration 12/1000 | Loss: 0.00001998
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001984
Iteration 16/1000 | Loss: 0.00001984
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001980
Iteration 22/1000 | Loss: 0.00001980
Iteration 23/1000 | Loss: 0.00001980
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001979
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001978
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001977
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001977
Iteration 36/1000 | Loss: 0.00001977
Iteration 37/1000 | Loss: 0.00001976
Iteration 38/1000 | Loss: 0.00001976
Iteration 39/1000 | Loss: 0.00001976
Iteration 40/1000 | Loss: 0.00001976
Iteration 41/1000 | Loss: 0.00001976
Iteration 42/1000 | Loss: 0.00001976
Iteration 43/1000 | Loss: 0.00001976
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001976
Iteration 46/1000 | Loss: 0.00001976
Iteration 47/1000 | Loss: 0.00001976
Iteration 48/1000 | Loss: 0.00001976
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001976
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.9759079805226065e-05, 1.9759079805226065e-05, 1.9759079805226065e-05, 1.9759079805226065e-05, 1.9759079805226065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9759079805226065e-05

Optimization complete. Final v2v error: 3.753420829772949 mm

Highest mean error: 4.075565814971924 mm for frame 70

Lowest mean error: 3.4694321155548096 mm for frame 183

Saving results

Total time: 33.60430121421814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451541
Iteration 2/25 | Loss: 0.00095068
Iteration 3/25 | Loss: 0.00080433
Iteration 4/25 | Loss: 0.00078897
Iteration 5/25 | Loss: 0.00078582
Iteration 6/25 | Loss: 0.00078503
Iteration 7/25 | Loss: 0.00078503
Iteration 8/25 | Loss: 0.00078503
Iteration 9/25 | Loss: 0.00078503
Iteration 10/25 | Loss: 0.00078503
Iteration 11/25 | Loss: 0.00078503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007850277470424771, 0.0007850277470424771, 0.0007850277470424771, 0.0007850277470424771, 0.0007850277470424771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007850277470424771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49771786
Iteration 2/25 | Loss: 0.00048675
Iteration 3/25 | Loss: 0.00048675
Iteration 4/25 | Loss: 0.00048675
Iteration 5/25 | Loss: 0.00048675
Iteration 6/25 | Loss: 0.00048675
Iteration 7/25 | Loss: 0.00048675
Iteration 8/25 | Loss: 0.00048675
Iteration 9/25 | Loss: 0.00048674
Iteration 10/25 | Loss: 0.00048674
Iteration 11/25 | Loss: 0.00048674
Iteration 12/25 | Loss: 0.00048674
Iteration 13/25 | Loss: 0.00048674
Iteration 14/25 | Loss: 0.00048674
Iteration 15/25 | Loss: 0.00048674
Iteration 16/25 | Loss: 0.00048674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004867445968557149, 0.0004867445968557149, 0.0004867445968557149, 0.0004867445968557149, 0.0004867445968557149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004867445968557149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048674
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00001854
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001567
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00001495
Iteration 8/1000 | Loss: 0.00001479
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001477
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001475
Iteration 13/1000 | Loss: 0.00001474
Iteration 14/1000 | Loss: 0.00001473
Iteration 15/1000 | Loss: 0.00001472
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001453
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001431
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001425
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001421
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001420
Iteration 167/1000 | Loss: 0.00001420
Iteration 168/1000 | Loss: 0.00001420
Iteration 169/1000 | Loss: 0.00001420
Iteration 170/1000 | Loss: 0.00001420
Iteration 171/1000 | Loss: 0.00001420
Iteration 172/1000 | Loss: 0.00001420
Iteration 173/1000 | Loss: 0.00001420
Iteration 174/1000 | Loss: 0.00001420
Iteration 175/1000 | Loss: 0.00001420
Iteration 176/1000 | Loss: 0.00001420
Iteration 177/1000 | Loss: 0.00001420
Iteration 178/1000 | Loss: 0.00001420
Iteration 179/1000 | Loss: 0.00001420
Iteration 180/1000 | Loss: 0.00001420
Iteration 181/1000 | Loss: 0.00001420
Iteration 182/1000 | Loss: 0.00001420
Iteration 183/1000 | Loss: 0.00001420
Iteration 184/1000 | Loss: 0.00001420
Iteration 185/1000 | Loss: 0.00001420
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001419
Iteration 188/1000 | Loss: 0.00001419
Iteration 189/1000 | Loss: 0.00001419
Iteration 190/1000 | Loss: 0.00001419
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001419
Iteration 193/1000 | Loss: 0.00001419
Iteration 194/1000 | Loss: 0.00001419
Iteration 195/1000 | Loss: 0.00001419
Iteration 196/1000 | Loss: 0.00001419
Iteration 197/1000 | Loss: 0.00001419
Iteration 198/1000 | Loss: 0.00001419
Iteration 199/1000 | Loss: 0.00001419
Iteration 200/1000 | Loss: 0.00001419
Iteration 201/1000 | Loss: 0.00001419
Iteration 202/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.4192162780091166e-05, 1.4192162780091166e-05, 1.4192162780091166e-05, 1.4192162780091166e-05, 1.4192162780091166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4192162780091166e-05

Optimization complete. Final v2v error: 3.091552257537842 mm

Highest mean error: 3.4952609539031982 mm for frame 37

Lowest mean error: 2.709115505218506 mm for frame 5

Saving results

Total time: 35.809002161026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455452
Iteration 2/25 | Loss: 0.00102682
Iteration 3/25 | Loss: 0.00081995
Iteration 4/25 | Loss: 0.00079373
Iteration 5/25 | Loss: 0.00078350
Iteration 6/25 | Loss: 0.00078170
Iteration 7/25 | Loss: 0.00078152
Iteration 8/25 | Loss: 0.00078152
Iteration 9/25 | Loss: 0.00078152
Iteration 10/25 | Loss: 0.00078152
Iteration 11/25 | Loss: 0.00078152
Iteration 12/25 | Loss: 0.00078152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007815182907506824, 0.0007815182907506824, 0.0007815182907506824, 0.0007815182907506824, 0.0007815182907506824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007815182907506824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49551761
Iteration 2/25 | Loss: 0.00046756
Iteration 3/25 | Loss: 0.00046756
Iteration 4/25 | Loss: 0.00046756
Iteration 5/25 | Loss: 0.00046756
Iteration 6/25 | Loss: 0.00046756
Iteration 7/25 | Loss: 0.00046756
Iteration 8/25 | Loss: 0.00046755
Iteration 9/25 | Loss: 0.00046755
Iteration 10/25 | Loss: 0.00046755
Iteration 11/25 | Loss: 0.00046755
Iteration 12/25 | Loss: 0.00046755
Iteration 13/25 | Loss: 0.00046755
Iteration 14/25 | Loss: 0.00046755
Iteration 15/25 | Loss: 0.00046755
Iteration 16/25 | Loss: 0.00046755
Iteration 17/25 | Loss: 0.00046755
Iteration 18/25 | Loss: 0.00046755
Iteration 19/25 | Loss: 0.00046755
Iteration 20/25 | Loss: 0.00046755
Iteration 21/25 | Loss: 0.00046755
Iteration 22/25 | Loss: 0.00046755
Iteration 23/25 | Loss: 0.00046755
Iteration 24/25 | Loss: 0.00046755
Iteration 25/25 | Loss: 0.00046755
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000467554695205763, 0.000467554695205763, 0.000467554695205763, 0.000467554695205763, 0.000467554695205763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000467554695205763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046755
Iteration 2/1000 | Loss: 0.00002764
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00002006
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00001792
Iteration 9/1000 | Loss: 0.00001772
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001755
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00001742
Iteration 23/1000 | Loss: 0.00001741
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001738
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001738
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001738
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001735
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001733
Iteration 52/1000 | Loss: 0.00001733
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001731
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001730
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001727
Iteration 71/1000 | Loss: 0.00001727
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001725
Iteration 77/1000 | Loss: 0.00001725
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001721
Iteration 91/1000 | Loss: 0.00001721
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001720
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001719
Iteration 121/1000 | Loss: 0.00001719
Iteration 122/1000 | Loss: 0.00001719
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00001719
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.7188020137837157e-05, 1.7188020137837157e-05, 1.7188020137837157e-05, 1.7188020137837157e-05, 1.7188020137837157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7188020137837157e-05

Optimization complete. Final v2v error: 3.4869306087493896 mm

Highest mean error: 3.9139161109924316 mm for frame 24

Lowest mean error: 3.1699657440185547 mm for frame 108

Saving results

Total time: 38.45189619064331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000874
Iteration 2/25 | Loss: 0.01000874
Iteration 3/25 | Loss: 0.01000874
Iteration 4/25 | Loss: 0.01000873
Iteration 5/25 | Loss: 0.01000873
Iteration 6/25 | Loss: 0.01000873
Iteration 7/25 | Loss: 0.01000873
Iteration 8/25 | Loss: 0.01000873
Iteration 9/25 | Loss: 0.01000873
Iteration 10/25 | Loss: 0.01000873
Iteration 11/25 | Loss: 0.01000873
Iteration 12/25 | Loss: 0.01000873
Iteration 13/25 | Loss: 0.01000873
Iteration 14/25 | Loss: 0.01000872
Iteration 15/25 | Loss: 0.01000872
Iteration 16/25 | Loss: 0.01000872
Iteration 17/25 | Loss: 0.01000872
Iteration 18/25 | Loss: 0.01000872
Iteration 19/25 | Loss: 0.01000872
Iteration 20/25 | Loss: 0.01000872
Iteration 21/25 | Loss: 0.01000872
Iteration 22/25 | Loss: 0.01000872
Iteration 23/25 | Loss: 0.01000872
Iteration 24/25 | Loss: 0.01000871
Iteration 25/25 | Loss: 0.01000871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90075123
Iteration 2/25 | Loss: 0.15104517
Iteration 3/25 | Loss: 0.14225733
Iteration 4/25 | Loss: 0.13843153
Iteration 5/25 | Loss: 0.13776037
Iteration 6/25 | Loss: 0.13799255
Iteration 7/25 | Loss: 0.13783999
Iteration 8/25 | Loss: 0.13775761
Iteration 9/25 | Loss: 0.13768494
Iteration 10/25 | Loss: 0.13768491
Iteration 11/25 | Loss: 0.13768491
Iteration 12/25 | Loss: 0.13768491
Iteration 13/25 | Loss: 0.13768491
Iteration 14/25 | Loss: 0.13768490
Iteration 15/25 | Loss: 0.13768490
Iteration 16/25 | Loss: 0.13768490
Iteration 17/25 | Loss: 0.13768490
Iteration 18/25 | Loss: 0.13768490
Iteration 19/25 | Loss: 0.13768488
Iteration 20/25 | Loss: 0.13768488
Iteration 21/25 | Loss: 0.13768488
Iteration 22/25 | Loss: 0.13768488
Iteration 23/25 | Loss: 0.13768488
Iteration 24/25 | Loss: 0.13768488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.1376848816871643, 0.1376848816871643, 0.1376848816871643, 0.1376848816871643, 0.1376848816871643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1376848816871643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13768488
Iteration 2/1000 | Loss: 0.01130301
Iteration 3/1000 | Loss: 0.00335258
Iteration 4/1000 | Loss: 0.00134343
Iteration 5/1000 | Loss: 0.00147098
Iteration 6/1000 | Loss: 0.00153027
Iteration 7/1000 | Loss: 0.00159383
Iteration 8/1000 | Loss: 0.00062868
Iteration 9/1000 | Loss: 0.00017614
Iteration 10/1000 | Loss: 0.00075144
Iteration 11/1000 | Loss: 0.00032287
Iteration 12/1000 | Loss: 0.00015434
Iteration 13/1000 | Loss: 0.00060422
Iteration 14/1000 | Loss: 0.00027419
Iteration 15/1000 | Loss: 0.00045340
Iteration 16/1000 | Loss: 0.00016527
Iteration 17/1000 | Loss: 0.00116232
Iteration 18/1000 | Loss: 0.00025245
Iteration 19/1000 | Loss: 0.00005359
Iteration 20/1000 | Loss: 0.00068168
Iteration 21/1000 | Loss: 0.00230525
Iteration 22/1000 | Loss: 0.00007594
Iteration 23/1000 | Loss: 0.00030898
Iteration 24/1000 | Loss: 0.00004727
Iteration 25/1000 | Loss: 0.00012553
Iteration 26/1000 | Loss: 0.00015131
Iteration 27/1000 | Loss: 0.00003365
Iteration 28/1000 | Loss: 0.00034829
Iteration 29/1000 | Loss: 0.00022397
Iteration 30/1000 | Loss: 0.00009246
Iteration 31/1000 | Loss: 0.00013075
Iteration 32/1000 | Loss: 0.00004389
Iteration 33/1000 | Loss: 0.00019550
Iteration 34/1000 | Loss: 0.00003741
Iteration 35/1000 | Loss: 0.00034687
Iteration 36/1000 | Loss: 0.00002722
Iteration 37/1000 | Loss: 0.00022810
Iteration 38/1000 | Loss: 0.00062050
Iteration 39/1000 | Loss: 0.00010544
Iteration 40/1000 | Loss: 0.00002394
Iteration 41/1000 | Loss: 0.00021095
Iteration 42/1000 | Loss: 0.00012857
Iteration 43/1000 | Loss: 0.00002785
Iteration 44/1000 | Loss: 0.00005547
Iteration 45/1000 | Loss: 0.00013335
Iteration 46/1000 | Loss: 0.00002270
Iteration 47/1000 | Loss: 0.00005727
Iteration 48/1000 | Loss: 0.00002242
Iteration 49/1000 | Loss: 0.00018748
Iteration 50/1000 | Loss: 0.00003857
Iteration 51/1000 | Loss: 0.00003332
Iteration 52/1000 | Loss: 0.00029038
Iteration 53/1000 | Loss: 0.00005966
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00004460
Iteration 56/1000 | Loss: 0.00002175
Iteration 57/1000 | Loss: 0.00006971
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00007689
Iteration 61/1000 | Loss: 0.00018354
Iteration 62/1000 | Loss: 0.00095425
Iteration 63/1000 | Loss: 0.00003805
Iteration 64/1000 | Loss: 0.00004271
Iteration 65/1000 | Loss: 0.00004794
Iteration 66/1000 | Loss: 0.00002182
Iteration 67/1000 | Loss: 0.00007508
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00007586
Iteration 70/1000 | Loss: 0.00002143
Iteration 71/1000 | Loss: 0.00005668
Iteration 72/1000 | Loss: 0.00002137
Iteration 73/1000 | Loss: 0.00002123
Iteration 74/1000 | Loss: 0.00005617
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002119
Iteration 77/1000 | Loss: 0.00009761
Iteration 78/1000 | Loss: 0.00002119
Iteration 79/1000 | Loss: 0.00002110
Iteration 80/1000 | Loss: 0.00002107
Iteration 81/1000 | Loss: 0.00002107
Iteration 82/1000 | Loss: 0.00002107
Iteration 83/1000 | Loss: 0.00002107
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002107
Iteration 86/1000 | Loss: 0.00002107
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002107
Iteration 89/1000 | Loss: 0.00002107
Iteration 90/1000 | Loss: 0.00002107
Iteration 91/1000 | Loss: 0.00002107
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00002106
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002106
Iteration 99/1000 | Loss: 0.00002106
Iteration 100/1000 | Loss: 0.00002106
Iteration 101/1000 | Loss: 0.00002105
Iteration 102/1000 | Loss: 0.00002105
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00002105
Iteration 105/1000 | Loss: 0.00002105
Iteration 106/1000 | Loss: 0.00002105
Iteration 107/1000 | Loss: 0.00002105
Iteration 108/1000 | Loss: 0.00002105
Iteration 109/1000 | Loss: 0.00002105
Iteration 110/1000 | Loss: 0.00002104
Iteration 111/1000 | Loss: 0.00002104
Iteration 112/1000 | Loss: 0.00002104
Iteration 113/1000 | Loss: 0.00002104
Iteration 114/1000 | Loss: 0.00002103
Iteration 115/1000 | Loss: 0.00002103
Iteration 116/1000 | Loss: 0.00002103
Iteration 117/1000 | Loss: 0.00002103
Iteration 118/1000 | Loss: 0.00002103
Iteration 119/1000 | Loss: 0.00002103
Iteration 120/1000 | Loss: 0.00002103
Iteration 121/1000 | Loss: 0.00002103
Iteration 122/1000 | Loss: 0.00002103
Iteration 123/1000 | Loss: 0.00002103
Iteration 124/1000 | Loss: 0.00002102
Iteration 125/1000 | Loss: 0.00002102
Iteration 126/1000 | Loss: 0.00002102
Iteration 127/1000 | Loss: 0.00002102
Iteration 128/1000 | Loss: 0.00002102
Iteration 129/1000 | Loss: 0.00002102
Iteration 130/1000 | Loss: 0.00002102
Iteration 131/1000 | Loss: 0.00002101
Iteration 132/1000 | Loss: 0.00002101
Iteration 133/1000 | Loss: 0.00002101
Iteration 134/1000 | Loss: 0.00002101
Iteration 135/1000 | Loss: 0.00002101
Iteration 136/1000 | Loss: 0.00002101
Iteration 137/1000 | Loss: 0.00002101
Iteration 138/1000 | Loss: 0.00002101
Iteration 139/1000 | Loss: 0.00002100
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002100
Iteration 142/1000 | Loss: 0.00002100
Iteration 143/1000 | Loss: 0.00002100
Iteration 144/1000 | Loss: 0.00002100
Iteration 145/1000 | Loss: 0.00002100
Iteration 146/1000 | Loss: 0.00016762
Iteration 147/1000 | Loss: 0.00002120
Iteration 148/1000 | Loss: 0.00002098
Iteration 149/1000 | Loss: 0.00002096
Iteration 150/1000 | Loss: 0.00002096
Iteration 151/1000 | Loss: 0.00002096
Iteration 152/1000 | Loss: 0.00002096
Iteration 153/1000 | Loss: 0.00002096
Iteration 154/1000 | Loss: 0.00002096
Iteration 155/1000 | Loss: 0.00002096
Iteration 156/1000 | Loss: 0.00002096
Iteration 157/1000 | Loss: 0.00002096
Iteration 158/1000 | Loss: 0.00002096
Iteration 159/1000 | Loss: 0.00002096
Iteration 160/1000 | Loss: 0.00002096
Iteration 161/1000 | Loss: 0.00002095
Iteration 162/1000 | Loss: 0.00002095
Iteration 163/1000 | Loss: 0.00002095
Iteration 164/1000 | Loss: 0.00002095
Iteration 165/1000 | Loss: 0.00002095
Iteration 166/1000 | Loss: 0.00002095
Iteration 167/1000 | Loss: 0.00002095
Iteration 168/1000 | Loss: 0.00002095
Iteration 169/1000 | Loss: 0.00002095
Iteration 170/1000 | Loss: 0.00002095
Iteration 171/1000 | Loss: 0.00002095
Iteration 172/1000 | Loss: 0.00002094
Iteration 173/1000 | Loss: 0.00002094
Iteration 174/1000 | Loss: 0.00002094
Iteration 175/1000 | Loss: 0.00002094
Iteration 176/1000 | Loss: 0.00002094
Iteration 177/1000 | Loss: 0.00002094
Iteration 178/1000 | Loss: 0.00002094
Iteration 179/1000 | Loss: 0.00002094
Iteration 180/1000 | Loss: 0.00002094
Iteration 181/1000 | Loss: 0.00002094
Iteration 182/1000 | Loss: 0.00002094
Iteration 183/1000 | Loss: 0.00002094
Iteration 184/1000 | Loss: 0.00002094
Iteration 185/1000 | Loss: 0.00002093
Iteration 186/1000 | Loss: 0.00002093
Iteration 187/1000 | Loss: 0.00002093
Iteration 188/1000 | Loss: 0.00002093
Iteration 189/1000 | Loss: 0.00002093
Iteration 190/1000 | Loss: 0.00002093
Iteration 191/1000 | Loss: 0.00002093
Iteration 192/1000 | Loss: 0.00002093
Iteration 193/1000 | Loss: 0.00002093
Iteration 194/1000 | Loss: 0.00002093
Iteration 195/1000 | Loss: 0.00002093
Iteration 196/1000 | Loss: 0.00002093
Iteration 197/1000 | Loss: 0.00002093
Iteration 198/1000 | Loss: 0.00002093
Iteration 199/1000 | Loss: 0.00002093
Iteration 200/1000 | Loss: 0.00002093
Iteration 201/1000 | Loss: 0.00002093
Iteration 202/1000 | Loss: 0.00002093
Iteration 203/1000 | Loss: 0.00002093
Iteration 204/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.0930789105477743e-05, 2.0930789105477743e-05, 2.0930789105477743e-05, 2.0930789105477743e-05, 2.0930789105477743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0930789105477743e-05

Optimization complete. Final v2v error: 3.893644332885742 mm

Highest mean error: 4.291083335876465 mm for frame 226

Lowest mean error: 3.6141138076782227 mm for frame 181

Saving results

Total time: 162.24796271324158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_033/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_033/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081419
Iteration 2/25 | Loss: 0.00275639
Iteration 3/25 | Loss: 0.00161636
Iteration 4/25 | Loss: 0.00167815
Iteration 5/25 | Loss: 0.00138420
Iteration 6/25 | Loss: 0.00133941
Iteration 7/25 | Loss: 0.00126384
Iteration 8/25 | Loss: 0.00124248
Iteration 9/25 | Loss: 0.00120910
Iteration 10/25 | Loss: 0.00120649
Iteration 11/25 | Loss: 0.00116741
Iteration 12/25 | Loss: 0.00116118
Iteration 13/25 | Loss: 0.00115461
Iteration 14/25 | Loss: 0.00114213
Iteration 15/25 | Loss: 0.00113097
Iteration 16/25 | Loss: 0.00112149
Iteration 17/25 | Loss: 0.00111793
Iteration 18/25 | Loss: 0.00111843
Iteration 19/25 | Loss: 0.00111494
Iteration 20/25 | Loss: 0.00110563
Iteration 21/25 | Loss: 0.00109591
Iteration 22/25 | Loss: 0.00109492
Iteration 23/25 | Loss: 0.00109350
Iteration 24/25 | Loss: 0.00108976
Iteration 25/25 | Loss: 0.00108937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43185246
Iteration 2/25 | Loss: 0.00528762
Iteration 3/25 | Loss: 0.00334870
Iteration 4/25 | Loss: 0.00334870
Iteration 5/25 | Loss: 0.00334870
Iteration 6/25 | Loss: 0.00334870
Iteration 7/25 | Loss: 0.00334870
Iteration 8/25 | Loss: 0.00334870
Iteration 9/25 | Loss: 0.00334870
Iteration 10/25 | Loss: 0.00334870
Iteration 11/25 | Loss: 0.00334870
Iteration 12/25 | Loss: 0.00334870
Iteration 13/25 | Loss: 0.00334870
Iteration 14/25 | Loss: 0.00334870
Iteration 15/25 | Loss: 0.00334870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0033486997708678246, 0.0033486997708678246, 0.0033486997708678246, 0.0033486997708678246, 0.0033486997708678246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033486997708678246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00334870
Iteration 2/1000 | Loss: 0.00443991
Iteration 3/1000 | Loss: 0.00407869
Iteration 4/1000 | Loss: 0.00144261
Iteration 5/1000 | Loss: 0.00380084
Iteration 6/1000 | Loss: 0.00258996
Iteration 7/1000 | Loss: 0.00337149
Iteration 8/1000 | Loss: 0.00335368
Iteration 9/1000 | Loss: 0.00397149
Iteration 10/1000 | Loss: 0.00158753
Iteration 11/1000 | Loss: 0.00179551
Iteration 12/1000 | Loss: 0.00127085
Iteration 13/1000 | Loss: 0.00092208
Iteration 14/1000 | Loss: 0.00066424
Iteration 15/1000 | Loss: 0.00118142
Iteration 16/1000 | Loss: 0.00044227
Iteration 17/1000 | Loss: 0.00208395
Iteration 18/1000 | Loss: 0.00096420
Iteration 19/1000 | Loss: 0.00057453
Iteration 20/1000 | Loss: 0.00197541
Iteration 21/1000 | Loss: 0.00378246
Iteration 22/1000 | Loss: 0.00277419
Iteration 23/1000 | Loss: 0.00480289
Iteration 24/1000 | Loss: 0.00499992
Iteration 25/1000 | Loss: 0.00243188
Iteration 26/1000 | Loss: 0.00351762
Iteration 27/1000 | Loss: 0.00160601
Iteration 28/1000 | Loss: 0.00253192
Iteration 29/1000 | Loss: 0.00079863
Iteration 30/1000 | Loss: 0.00063711
Iteration 31/1000 | Loss: 0.00044533
Iteration 32/1000 | Loss: 0.00075020
Iteration 33/1000 | Loss: 0.00190558
Iteration 34/1000 | Loss: 0.00092674
Iteration 35/1000 | Loss: 0.00096396
Iteration 36/1000 | Loss: 0.00097604
Iteration 37/1000 | Loss: 0.00064932
Iteration 38/1000 | Loss: 0.00116464
Iteration 39/1000 | Loss: 0.00172751
Iteration 40/1000 | Loss: 0.00123711
Iteration 41/1000 | Loss: 0.00184499
Iteration 42/1000 | Loss: 0.00131562
Iteration 43/1000 | Loss: 0.00222735
Iteration 44/1000 | Loss: 0.00144151
Iteration 45/1000 | Loss: 0.00121964
Iteration 46/1000 | Loss: 0.00103382
Iteration 47/1000 | Loss: 0.00153542
Iteration 48/1000 | Loss: 0.00039062
Iteration 49/1000 | Loss: 0.00074405
Iteration 50/1000 | Loss: 0.00085957
Iteration 51/1000 | Loss: 0.00085611
Iteration 52/1000 | Loss: 0.00058699
Iteration 53/1000 | Loss: 0.00057331
Iteration 54/1000 | Loss: 0.00033296
Iteration 55/1000 | Loss: 0.00019462
Iteration 56/1000 | Loss: 0.00006785
Iteration 57/1000 | Loss: 0.00117640
Iteration 58/1000 | Loss: 0.00027655
Iteration 59/1000 | Loss: 0.00029496
Iteration 60/1000 | Loss: 0.00007947
Iteration 61/1000 | Loss: 0.00008701
Iteration 62/1000 | Loss: 0.00017040
Iteration 63/1000 | Loss: 0.00007113
Iteration 64/1000 | Loss: 0.00006201
Iteration 65/1000 | Loss: 0.00035581
Iteration 66/1000 | Loss: 0.00055507
Iteration 67/1000 | Loss: 0.00041427
Iteration 68/1000 | Loss: 0.00047953
Iteration 69/1000 | Loss: 0.00025332
Iteration 70/1000 | Loss: 0.00023707
Iteration 71/1000 | Loss: 0.00023186
Iteration 72/1000 | Loss: 0.00020089
Iteration 73/1000 | Loss: 0.00011094
Iteration 74/1000 | Loss: 0.00015479
Iteration 75/1000 | Loss: 0.00016470
Iteration 76/1000 | Loss: 0.00070304
Iteration 77/1000 | Loss: 0.00034394
Iteration 78/1000 | Loss: 0.00015898
Iteration 79/1000 | Loss: 0.00009848
Iteration 80/1000 | Loss: 0.00014592
Iteration 81/1000 | Loss: 0.00005735
Iteration 82/1000 | Loss: 0.00009013
Iteration 83/1000 | Loss: 0.00004306
Iteration 84/1000 | Loss: 0.00005286
Iteration 85/1000 | Loss: 0.00043774
Iteration 86/1000 | Loss: 0.00076316
Iteration 87/1000 | Loss: 0.00054331
Iteration 88/1000 | Loss: 0.00074335
Iteration 89/1000 | Loss: 0.00138550
Iteration 90/1000 | Loss: 0.00067871
Iteration 91/1000 | Loss: 0.00024228
Iteration 92/1000 | Loss: 0.00049261
Iteration 93/1000 | Loss: 0.00024800
Iteration 94/1000 | Loss: 0.00090310
Iteration 95/1000 | Loss: 0.00039275
Iteration 96/1000 | Loss: 0.00032230
Iteration 97/1000 | Loss: 0.00030033
Iteration 98/1000 | Loss: 0.00046797
Iteration 99/1000 | Loss: 0.00026887
Iteration 100/1000 | Loss: 0.00014205
Iteration 101/1000 | Loss: 0.00028135
Iteration 102/1000 | Loss: 0.00023316
Iteration 103/1000 | Loss: 0.00016798
Iteration 104/1000 | Loss: 0.00004411
Iteration 105/1000 | Loss: 0.00007673
Iteration 106/1000 | Loss: 0.00003030
Iteration 107/1000 | Loss: 0.00004778
Iteration 108/1000 | Loss: 0.00005277
Iteration 109/1000 | Loss: 0.00003607
Iteration 110/1000 | Loss: 0.00004061
Iteration 111/1000 | Loss: 0.00004627
Iteration 112/1000 | Loss: 0.00003964
Iteration 113/1000 | Loss: 0.00003963
Iteration 114/1000 | Loss: 0.00004000
Iteration 115/1000 | Loss: 0.00003955
Iteration 116/1000 | Loss: 0.00007394
Iteration 117/1000 | Loss: 0.00006618
Iteration 118/1000 | Loss: 0.00004406
Iteration 119/1000 | Loss: 0.00004206
Iteration 120/1000 | Loss: 0.00007644
Iteration 121/1000 | Loss: 0.00003320
Iteration 122/1000 | Loss: 0.00002852
Iteration 123/1000 | Loss: 0.00002710
Iteration 124/1000 | Loss: 0.00009566
Iteration 125/1000 | Loss: 0.00002597
Iteration 126/1000 | Loss: 0.00003191
Iteration 127/1000 | Loss: 0.00003720
Iteration 128/1000 | Loss: 0.00002463
Iteration 129/1000 | Loss: 0.00003522
Iteration 130/1000 | Loss: 0.00002422
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00003360
Iteration 133/1000 | Loss: 0.00002394
Iteration 134/1000 | Loss: 0.00002398
Iteration 135/1000 | Loss: 0.00018054
Iteration 136/1000 | Loss: 0.00016254
Iteration 137/1000 | Loss: 0.00011124
Iteration 138/1000 | Loss: 0.00023091
Iteration 139/1000 | Loss: 0.00003325
Iteration 140/1000 | Loss: 0.00003551
Iteration 141/1000 | Loss: 0.00002998
Iteration 142/1000 | Loss: 0.00002586
Iteration 143/1000 | Loss: 0.00003507
Iteration 144/1000 | Loss: 0.00002410
Iteration 145/1000 | Loss: 0.00003443
Iteration 146/1000 | Loss: 0.00002898
Iteration 147/1000 | Loss: 0.00002302
Iteration 148/1000 | Loss: 0.00002591
Iteration 149/1000 | Loss: 0.00002591
Iteration 150/1000 | Loss: 0.00002276
Iteration 151/1000 | Loss: 0.00002275
Iteration 152/1000 | Loss: 0.00002275
Iteration 153/1000 | Loss: 0.00002275
Iteration 154/1000 | Loss: 0.00002275
Iteration 155/1000 | Loss: 0.00002275
Iteration 156/1000 | Loss: 0.00002275
Iteration 157/1000 | Loss: 0.00002275
Iteration 158/1000 | Loss: 0.00002275
Iteration 159/1000 | Loss: 0.00002275
Iteration 160/1000 | Loss: 0.00002275
Iteration 161/1000 | Loss: 0.00002275
Iteration 162/1000 | Loss: 0.00002275
Iteration 163/1000 | Loss: 0.00002275
Iteration 164/1000 | Loss: 0.00002274
Iteration 165/1000 | Loss: 0.00002274
Iteration 166/1000 | Loss: 0.00002273
Iteration 167/1000 | Loss: 0.00002273
Iteration 168/1000 | Loss: 0.00002272
Iteration 169/1000 | Loss: 0.00002757
Iteration 170/1000 | Loss: 0.00002366
Iteration 171/1000 | Loss: 0.00002268
Iteration 172/1000 | Loss: 0.00002254
Iteration 173/1000 | Loss: 0.00002254
Iteration 174/1000 | Loss: 0.00002254
Iteration 175/1000 | Loss: 0.00002254
Iteration 176/1000 | Loss: 0.00002254
Iteration 177/1000 | Loss: 0.00002254
Iteration 178/1000 | Loss: 0.00002254
Iteration 179/1000 | Loss: 0.00002254
Iteration 180/1000 | Loss: 0.00002254
Iteration 181/1000 | Loss: 0.00002254
Iteration 182/1000 | Loss: 0.00002253
Iteration 183/1000 | Loss: 0.00002253
Iteration 184/1000 | Loss: 0.00002253
Iteration 185/1000 | Loss: 0.00002253
Iteration 186/1000 | Loss: 0.00002253
Iteration 187/1000 | Loss: 0.00002253
Iteration 188/1000 | Loss: 0.00002253
Iteration 189/1000 | Loss: 0.00002253
Iteration 190/1000 | Loss: 0.00002253
Iteration 191/1000 | Loss: 0.00002253
Iteration 192/1000 | Loss: 0.00002253
Iteration 193/1000 | Loss: 0.00002253
Iteration 194/1000 | Loss: 0.00002253
Iteration 195/1000 | Loss: 0.00002253
Iteration 196/1000 | Loss: 0.00002253
Iteration 197/1000 | Loss: 0.00002253
Iteration 198/1000 | Loss: 0.00002253
Iteration 199/1000 | Loss: 0.00002253
Iteration 200/1000 | Loss: 0.00002253
Iteration 201/1000 | Loss: 0.00002253
Iteration 202/1000 | Loss: 0.00002253
Iteration 203/1000 | Loss: 0.00002253
Iteration 204/1000 | Loss: 0.00002253
Iteration 205/1000 | Loss: 0.00002253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.2527754481416196e-05, 2.2527754481416196e-05, 2.2527754481416196e-05, 2.2527754481416196e-05, 2.2527754481416196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2527754481416196e-05

Optimization complete. Final v2v error: 3.872951030731201 mm

Highest mean error: 6.037538528442383 mm for frame 34

Lowest mean error: 3.230628728866577 mm for frame 52

Saving results

Total time: 305.720064163208
