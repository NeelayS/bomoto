Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=134, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7504-7559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042881
Iteration 2/25 | Loss: 0.00178637
Iteration 3/25 | Loss: 0.00120187
Iteration 4/25 | Loss: 0.00105682
Iteration 5/25 | Loss: 0.00099452
Iteration 6/25 | Loss: 0.00108048
Iteration 7/25 | Loss: 0.00109888
Iteration 8/25 | Loss: 0.00095426
Iteration 9/25 | Loss: 0.00090411
Iteration 10/25 | Loss: 0.00087321
Iteration 11/25 | Loss: 0.00086841
Iteration 12/25 | Loss: 0.00083800
Iteration 13/25 | Loss: 0.00082847
Iteration 14/25 | Loss: 0.00082324
Iteration 15/25 | Loss: 0.00081729
Iteration 16/25 | Loss: 0.00081159
Iteration 17/25 | Loss: 0.00081339
Iteration 18/25 | Loss: 0.00080651
Iteration 19/25 | Loss: 0.00080382
Iteration 20/25 | Loss: 0.00080324
Iteration 21/25 | Loss: 0.00081132
Iteration 22/25 | Loss: 0.00081134
Iteration 23/25 | Loss: 0.00080586
Iteration 24/25 | Loss: 0.00079530
Iteration 25/25 | Loss: 0.00079524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56217539
Iteration 2/25 | Loss: 0.00154661
Iteration 3/25 | Loss: 0.00130652
Iteration 4/25 | Loss: 0.00130651
Iteration 5/25 | Loss: 0.00130651
Iteration 6/25 | Loss: 0.00130651
Iteration 7/25 | Loss: 0.00130651
Iteration 8/25 | Loss: 0.00130651
Iteration 9/25 | Loss: 0.00130651
Iteration 10/25 | Loss: 0.00130651
Iteration 11/25 | Loss: 0.00130651
Iteration 12/25 | Loss: 0.00130651
Iteration 13/25 | Loss: 0.00130651
Iteration 14/25 | Loss: 0.00130651
Iteration 15/25 | Loss: 0.00130651
Iteration 16/25 | Loss: 0.00130651
Iteration 17/25 | Loss: 0.00130651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001306508551351726, 0.001306508551351726, 0.001306508551351726, 0.001306508551351726, 0.001306508551351726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001306508551351726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130651
Iteration 2/1000 | Loss: 0.00024660
Iteration 3/1000 | Loss: 0.00032384
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001869
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00006346
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001662
Iteration 19/1000 | Loss: 0.00001661
Iteration 20/1000 | Loss: 0.00005287
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001644
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001644
Iteration 28/1000 | Loss: 0.00001644
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001640
Iteration 41/1000 | Loss: 0.00001640
Iteration 42/1000 | Loss: 0.00001639
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001631
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00035733
Iteration 70/1000 | Loss: 0.00007547
Iteration 71/1000 | Loss: 0.00005209
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00007140
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001646
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001645
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001644
Iteration 92/1000 | Loss: 0.00003905
Iteration 93/1000 | Loss: 0.00001648
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001624
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00005490
Iteration 100/1000 | Loss: 0.00029501
Iteration 101/1000 | Loss: 0.00015589
Iteration 102/1000 | Loss: 0.00002668
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00002628
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00027123
Iteration 107/1000 | Loss: 0.00012062
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001625
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001620
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00027100
Iteration 127/1000 | Loss: 0.00013981
Iteration 128/1000 | Loss: 0.00043966
Iteration 129/1000 | Loss: 0.00029696
Iteration 130/1000 | Loss: 0.00015640
Iteration 131/1000 | Loss: 0.00032678
Iteration 132/1000 | Loss: 0.00012706
Iteration 133/1000 | Loss: 0.00023918
Iteration 134/1000 | Loss: 0.00012187
Iteration 135/1000 | Loss: 0.00004029
Iteration 136/1000 | Loss: 0.00001655
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00026155
Iteration 141/1000 | Loss: 0.00032713
Iteration 142/1000 | Loss: 0.00023249
Iteration 143/1000 | Loss: 0.00022314
Iteration 144/1000 | Loss: 0.00011169
Iteration 145/1000 | Loss: 0.00004410
Iteration 146/1000 | Loss: 0.00018085
Iteration 147/1000 | Loss: 0.00020949
Iteration 148/1000 | Loss: 0.00015770
Iteration 149/1000 | Loss: 0.00032072
Iteration 150/1000 | Loss: 0.00003305
Iteration 151/1000 | Loss: 0.00026631
Iteration 152/1000 | Loss: 0.00009641
Iteration 153/1000 | Loss: 0.00002649
Iteration 154/1000 | Loss: 0.00087213
Iteration 155/1000 | Loss: 0.00020047
Iteration 156/1000 | Loss: 0.00017870
Iteration 157/1000 | Loss: 0.00005803
Iteration 158/1000 | Loss: 0.00057441
Iteration 159/1000 | Loss: 0.00029771
Iteration 160/1000 | Loss: 0.00042655
Iteration 161/1000 | Loss: 0.00044727
Iteration 162/1000 | Loss: 0.00034933
Iteration 163/1000 | Loss: 0.00012872
Iteration 164/1000 | Loss: 0.00044533
Iteration 165/1000 | Loss: 0.00011960
Iteration 166/1000 | Loss: 0.00033773
Iteration 167/1000 | Loss: 0.00002227
Iteration 168/1000 | Loss: 0.00017022
Iteration 169/1000 | Loss: 0.00001836
Iteration 170/1000 | Loss: 0.00004069
Iteration 171/1000 | Loss: 0.00001667
Iteration 172/1000 | Loss: 0.00004034
Iteration 173/1000 | Loss: 0.00001610
Iteration 174/1000 | Loss: 0.00001576
Iteration 175/1000 | Loss: 0.00001546
Iteration 176/1000 | Loss: 0.00004623
Iteration 177/1000 | Loss: 0.00001553
Iteration 178/1000 | Loss: 0.00001516
Iteration 179/1000 | Loss: 0.00001513
Iteration 180/1000 | Loss: 0.00001512
Iteration 181/1000 | Loss: 0.00001512
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001512
Iteration 184/1000 | Loss: 0.00001511
Iteration 185/1000 | Loss: 0.00001511
Iteration 186/1000 | Loss: 0.00001511
Iteration 187/1000 | Loss: 0.00001511
Iteration 188/1000 | Loss: 0.00001510
Iteration 189/1000 | Loss: 0.00001510
Iteration 190/1000 | Loss: 0.00001510
Iteration 191/1000 | Loss: 0.00001510
Iteration 192/1000 | Loss: 0.00001510
Iteration 193/1000 | Loss: 0.00001510
Iteration 194/1000 | Loss: 0.00001510
Iteration 195/1000 | Loss: 0.00001510
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001510
Iteration 201/1000 | Loss: 0.00001510
Iteration 202/1000 | Loss: 0.00001510
Iteration 203/1000 | Loss: 0.00001509
Iteration 204/1000 | Loss: 0.00001509
Iteration 205/1000 | Loss: 0.00001509
Iteration 206/1000 | Loss: 0.00001509
Iteration 207/1000 | Loss: 0.00001509
Iteration 208/1000 | Loss: 0.00001509
Iteration 209/1000 | Loss: 0.00001509
Iteration 210/1000 | Loss: 0.00001509
Iteration 211/1000 | Loss: 0.00001509
Iteration 212/1000 | Loss: 0.00001509
Iteration 213/1000 | Loss: 0.00001509
Iteration 214/1000 | Loss: 0.00001509
Iteration 215/1000 | Loss: 0.00001509
Iteration 216/1000 | Loss: 0.00001509
Iteration 217/1000 | Loss: 0.00001509
Iteration 218/1000 | Loss: 0.00001509
Iteration 219/1000 | Loss: 0.00001509
Iteration 220/1000 | Loss: 0.00001508
Iteration 221/1000 | Loss: 0.00001508
Iteration 222/1000 | Loss: 0.00001508
Iteration 223/1000 | Loss: 0.00001508
Iteration 224/1000 | Loss: 0.00001508
Iteration 225/1000 | Loss: 0.00001508
Iteration 226/1000 | Loss: 0.00001508
Iteration 227/1000 | Loss: 0.00001508
Iteration 228/1000 | Loss: 0.00001508
Iteration 229/1000 | Loss: 0.00001507
Iteration 230/1000 | Loss: 0.00001507
Iteration 231/1000 | Loss: 0.00001507
Iteration 232/1000 | Loss: 0.00001507
Iteration 233/1000 | Loss: 0.00001507
Iteration 234/1000 | Loss: 0.00003733
Iteration 235/1000 | Loss: 0.00001932
Iteration 236/1000 | Loss: 0.00002203
Iteration 237/1000 | Loss: 0.00001509
Iteration 238/1000 | Loss: 0.00001508
Iteration 239/1000 | Loss: 0.00001508
Iteration 240/1000 | Loss: 0.00001508
Iteration 241/1000 | Loss: 0.00001508
Iteration 242/1000 | Loss: 0.00001508
Iteration 243/1000 | Loss: 0.00001508
Iteration 244/1000 | Loss: 0.00001508
Iteration 245/1000 | Loss: 0.00004960
Iteration 246/1000 | Loss: 0.00010079
Iteration 247/1000 | Loss: 0.00002729
Iteration 248/1000 | Loss: 0.00001509
Iteration 249/1000 | Loss: 0.00001498
Iteration 250/1000 | Loss: 0.00001498
Iteration 251/1000 | Loss: 0.00001498
Iteration 252/1000 | Loss: 0.00001498
Iteration 253/1000 | Loss: 0.00001498
Iteration 254/1000 | Loss: 0.00001498
Iteration 255/1000 | Loss: 0.00001498
Iteration 256/1000 | Loss: 0.00001498
Iteration 257/1000 | Loss: 0.00001498
Iteration 258/1000 | Loss: 0.00001497
Iteration 259/1000 | Loss: 0.00001497
Iteration 260/1000 | Loss: 0.00001497
Iteration 261/1000 | Loss: 0.00001497
Iteration 262/1000 | Loss: 0.00001497
Iteration 263/1000 | Loss: 0.00001497
Iteration 264/1000 | Loss: 0.00001497
Iteration 265/1000 | Loss: 0.00001497
Iteration 266/1000 | Loss: 0.00001497
Iteration 267/1000 | Loss: 0.00001497
Iteration 268/1000 | Loss: 0.00001497
Iteration 269/1000 | Loss: 0.00001497
Iteration 270/1000 | Loss: 0.00001497
Iteration 271/1000 | Loss: 0.00001496
Iteration 272/1000 | Loss: 0.00001496
Iteration 273/1000 | Loss: 0.00001496
Iteration 274/1000 | Loss: 0.00001496
Iteration 275/1000 | Loss: 0.00001496
Iteration 276/1000 | Loss: 0.00001496
Iteration 277/1000 | Loss: 0.00001496
Iteration 278/1000 | Loss: 0.00001496
Iteration 279/1000 | Loss: 0.00001496
Iteration 280/1000 | Loss: 0.00001496
Iteration 281/1000 | Loss: 0.00001496
Iteration 282/1000 | Loss: 0.00001496
Iteration 283/1000 | Loss: 0.00001496
Iteration 284/1000 | Loss: 0.00001496
Iteration 285/1000 | Loss: 0.00001496
Iteration 286/1000 | Loss: 0.00001496
Iteration 287/1000 | Loss: 0.00001496
Iteration 288/1000 | Loss: 0.00001496
Iteration 289/1000 | Loss: 0.00001496
Iteration 290/1000 | Loss: 0.00001496
Iteration 291/1000 | Loss: 0.00001496
Iteration 292/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.4960297448851634e-05, 1.4960297448851634e-05, 1.4960297448851634e-05, 1.4960297448851634e-05, 1.4960297448851634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4960297448851634e-05

Optimization complete. Final v2v error: 3.2116713523864746 mm

Highest mean error: 4.611088752746582 mm for frame 68

Lowest mean error: 2.8546841144561768 mm for frame 21

Saving results

Total time: 193.1055839061737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876882
Iteration 2/25 | Loss: 0.00133494
Iteration 3/25 | Loss: 0.00096003
Iteration 4/25 | Loss: 0.00090290
Iteration 5/25 | Loss: 0.00089001
Iteration 6/25 | Loss: 0.00088672
Iteration 7/25 | Loss: 0.00088615
Iteration 8/25 | Loss: 0.00088615
Iteration 9/25 | Loss: 0.00088615
Iteration 10/25 | Loss: 0.00088615
Iteration 11/25 | Loss: 0.00088615
Iteration 12/25 | Loss: 0.00088615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008861457463353872, 0.0008861457463353872, 0.0008861457463353872, 0.0008861457463353872, 0.0008861457463353872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008861457463353872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58613086
Iteration 2/25 | Loss: 0.00120846
Iteration 3/25 | Loss: 0.00120842
Iteration 4/25 | Loss: 0.00120842
Iteration 5/25 | Loss: 0.00120842
Iteration 6/25 | Loss: 0.00120842
Iteration 7/25 | Loss: 0.00120842
Iteration 8/25 | Loss: 0.00120842
Iteration 9/25 | Loss: 0.00120842
Iteration 10/25 | Loss: 0.00120842
Iteration 11/25 | Loss: 0.00120842
Iteration 12/25 | Loss: 0.00120842
Iteration 13/25 | Loss: 0.00120842
Iteration 14/25 | Loss: 0.00120842
Iteration 15/25 | Loss: 0.00120842
Iteration 16/25 | Loss: 0.00120842
Iteration 17/25 | Loss: 0.00120842
Iteration 18/25 | Loss: 0.00120842
Iteration 19/25 | Loss: 0.00120842
Iteration 20/25 | Loss: 0.00120842
Iteration 21/25 | Loss: 0.00120842
Iteration 22/25 | Loss: 0.00120842
Iteration 23/25 | Loss: 0.00120842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012084159534424543, 0.0012084159534424543, 0.0012084159534424543, 0.0012084159534424543, 0.0012084159534424543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012084159534424543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120842
Iteration 2/1000 | Loss: 0.00004711
Iteration 3/1000 | Loss: 0.00003571
Iteration 4/1000 | Loss: 0.00003191
Iteration 5/1000 | Loss: 0.00002996
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002752
Iteration 8/1000 | Loss: 0.00002689
Iteration 9/1000 | Loss: 0.00002646
Iteration 10/1000 | Loss: 0.00002618
Iteration 11/1000 | Loss: 0.00002592
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002550
Iteration 15/1000 | Loss: 0.00002546
Iteration 16/1000 | Loss: 0.00002545
Iteration 17/1000 | Loss: 0.00002544
Iteration 18/1000 | Loss: 0.00002540
Iteration 19/1000 | Loss: 0.00002539
Iteration 20/1000 | Loss: 0.00002538
Iteration 21/1000 | Loss: 0.00002537
Iteration 22/1000 | Loss: 0.00002536
Iteration 23/1000 | Loss: 0.00002536
Iteration 24/1000 | Loss: 0.00002535
Iteration 25/1000 | Loss: 0.00002535
Iteration 26/1000 | Loss: 0.00002535
Iteration 27/1000 | Loss: 0.00002534
Iteration 28/1000 | Loss: 0.00002534
Iteration 29/1000 | Loss: 0.00002533
Iteration 30/1000 | Loss: 0.00002533
Iteration 31/1000 | Loss: 0.00002533
Iteration 32/1000 | Loss: 0.00002533
Iteration 33/1000 | Loss: 0.00002532
Iteration 34/1000 | Loss: 0.00002532
Iteration 35/1000 | Loss: 0.00002531
Iteration 36/1000 | Loss: 0.00002531
Iteration 37/1000 | Loss: 0.00002531
Iteration 38/1000 | Loss: 0.00002531
Iteration 39/1000 | Loss: 0.00002531
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002531
Iteration 42/1000 | Loss: 0.00002530
Iteration 43/1000 | Loss: 0.00002530
Iteration 44/1000 | Loss: 0.00002530
Iteration 45/1000 | Loss: 0.00002529
Iteration 46/1000 | Loss: 0.00002529
Iteration 47/1000 | Loss: 0.00002529
Iteration 48/1000 | Loss: 0.00002528
Iteration 49/1000 | Loss: 0.00002528
Iteration 50/1000 | Loss: 0.00002527
Iteration 51/1000 | Loss: 0.00002526
Iteration 52/1000 | Loss: 0.00002526
Iteration 53/1000 | Loss: 0.00002526
Iteration 54/1000 | Loss: 0.00002525
Iteration 55/1000 | Loss: 0.00002525
Iteration 56/1000 | Loss: 0.00002524
Iteration 57/1000 | Loss: 0.00002523
Iteration 58/1000 | Loss: 0.00002523
Iteration 59/1000 | Loss: 0.00002522
Iteration 60/1000 | Loss: 0.00002522
Iteration 61/1000 | Loss: 0.00002521
Iteration 62/1000 | Loss: 0.00002521
Iteration 63/1000 | Loss: 0.00002521
Iteration 64/1000 | Loss: 0.00002521
Iteration 65/1000 | Loss: 0.00002521
Iteration 66/1000 | Loss: 0.00002521
Iteration 67/1000 | Loss: 0.00002520
Iteration 68/1000 | Loss: 0.00002520
Iteration 69/1000 | Loss: 0.00002520
Iteration 70/1000 | Loss: 0.00002519
Iteration 71/1000 | Loss: 0.00002519
Iteration 72/1000 | Loss: 0.00002519
Iteration 73/1000 | Loss: 0.00002519
Iteration 74/1000 | Loss: 0.00002519
Iteration 75/1000 | Loss: 0.00002518
Iteration 76/1000 | Loss: 0.00002518
Iteration 77/1000 | Loss: 0.00002517
Iteration 78/1000 | Loss: 0.00002517
Iteration 79/1000 | Loss: 0.00002517
Iteration 80/1000 | Loss: 0.00002517
Iteration 81/1000 | Loss: 0.00002517
Iteration 82/1000 | Loss: 0.00002517
Iteration 83/1000 | Loss: 0.00002517
Iteration 84/1000 | Loss: 0.00002517
Iteration 85/1000 | Loss: 0.00002516
Iteration 86/1000 | Loss: 0.00002516
Iteration 87/1000 | Loss: 0.00002516
Iteration 88/1000 | Loss: 0.00002516
Iteration 89/1000 | Loss: 0.00002516
Iteration 90/1000 | Loss: 0.00002516
Iteration 91/1000 | Loss: 0.00002515
Iteration 92/1000 | Loss: 0.00002515
Iteration 93/1000 | Loss: 0.00002515
Iteration 94/1000 | Loss: 0.00002515
Iteration 95/1000 | Loss: 0.00002514
Iteration 96/1000 | Loss: 0.00002514
Iteration 97/1000 | Loss: 0.00002514
Iteration 98/1000 | Loss: 0.00002514
Iteration 99/1000 | Loss: 0.00002514
Iteration 100/1000 | Loss: 0.00002514
Iteration 101/1000 | Loss: 0.00002513
Iteration 102/1000 | Loss: 0.00002513
Iteration 103/1000 | Loss: 0.00002513
Iteration 104/1000 | Loss: 0.00002512
Iteration 105/1000 | Loss: 0.00002512
Iteration 106/1000 | Loss: 0.00002512
Iteration 107/1000 | Loss: 0.00002512
Iteration 108/1000 | Loss: 0.00002512
Iteration 109/1000 | Loss: 0.00002511
Iteration 110/1000 | Loss: 0.00002511
Iteration 111/1000 | Loss: 0.00002511
Iteration 112/1000 | Loss: 0.00002511
Iteration 113/1000 | Loss: 0.00002510
Iteration 114/1000 | Loss: 0.00002510
Iteration 115/1000 | Loss: 0.00002510
Iteration 116/1000 | Loss: 0.00002510
Iteration 117/1000 | Loss: 0.00002510
Iteration 118/1000 | Loss: 0.00002510
Iteration 119/1000 | Loss: 0.00002510
Iteration 120/1000 | Loss: 0.00002509
Iteration 121/1000 | Loss: 0.00002509
Iteration 122/1000 | Loss: 0.00002509
Iteration 123/1000 | Loss: 0.00002509
Iteration 124/1000 | Loss: 0.00002509
Iteration 125/1000 | Loss: 0.00002508
Iteration 126/1000 | Loss: 0.00002508
Iteration 127/1000 | Loss: 0.00002508
Iteration 128/1000 | Loss: 0.00002508
Iteration 129/1000 | Loss: 0.00002508
Iteration 130/1000 | Loss: 0.00002508
Iteration 131/1000 | Loss: 0.00002508
Iteration 132/1000 | Loss: 0.00002508
Iteration 133/1000 | Loss: 0.00002508
Iteration 134/1000 | Loss: 0.00002507
Iteration 135/1000 | Loss: 0.00002507
Iteration 136/1000 | Loss: 0.00002507
Iteration 137/1000 | Loss: 0.00002507
Iteration 138/1000 | Loss: 0.00002507
Iteration 139/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.5073961296584457e-05, 2.5073961296584457e-05, 2.5073961296584457e-05, 2.5073961296584457e-05, 2.5073961296584457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5073961296584457e-05

Optimization complete. Final v2v error: 4.10258150100708 mm

Highest mean error: 4.873259544372559 mm for frame 32

Lowest mean error: 3.433063507080078 mm for frame 155

Saving results

Total time: 45.596299171447754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403040
Iteration 2/25 | Loss: 0.00093059
Iteration 3/25 | Loss: 0.00076227
Iteration 4/25 | Loss: 0.00073314
Iteration 5/25 | Loss: 0.00072605
Iteration 6/25 | Loss: 0.00072389
Iteration 7/25 | Loss: 0.00072315
Iteration 8/25 | Loss: 0.00072308
Iteration 9/25 | Loss: 0.00072308
Iteration 10/25 | Loss: 0.00072308
Iteration 11/25 | Loss: 0.00072308
Iteration 12/25 | Loss: 0.00072308
Iteration 13/25 | Loss: 0.00072308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007230840856209397, 0.0007230840856209397, 0.0007230840856209397, 0.0007230840856209397, 0.0007230840856209397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007230840856209397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60150731
Iteration 2/25 | Loss: 0.00106631
Iteration 3/25 | Loss: 0.00106630
Iteration 4/25 | Loss: 0.00106630
Iteration 5/25 | Loss: 0.00106630
Iteration 6/25 | Loss: 0.00106630
Iteration 7/25 | Loss: 0.00106630
Iteration 8/25 | Loss: 0.00106630
Iteration 9/25 | Loss: 0.00106630
Iteration 10/25 | Loss: 0.00106630
Iteration 11/25 | Loss: 0.00106630
Iteration 12/25 | Loss: 0.00106630
Iteration 13/25 | Loss: 0.00106630
Iteration 14/25 | Loss: 0.00106630
Iteration 15/25 | Loss: 0.00106630
Iteration 16/25 | Loss: 0.00106630
Iteration 17/25 | Loss: 0.00106630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010662964778020978, 0.0010662964778020978, 0.0010662964778020978, 0.0010662964778020978, 0.0010662964778020978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010662964778020978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106630
Iteration 2/1000 | Loss: 0.00002176
Iteration 3/1000 | Loss: 0.00001378
Iteration 4/1000 | Loss: 0.00001241
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001121
Iteration 7/1000 | Loss: 0.00001096
Iteration 8/1000 | Loss: 0.00001080
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001070
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001066
Iteration 14/1000 | Loss: 0.00001065
Iteration 15/1000 | Loss: 0.00001064
Iteration 16/1000 | Loss: 0.00001062
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001059
Iteration 19/1000 | Loss: 0.00001059
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001056
Iteration 25/1000 | Loss: 0.00001053
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001051
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001049
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001049
Iteration 38/1000 | Loss: 0.00001048
Iteration 39/1000 | Loss: 0.00001048
Iteration 40/1000 | Loss: 0.00001048
Iteration 41/1000 | Loss: 0.00001047
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001046
Iteration 44/1000 | Loss: 0.00001046
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001045
Iteration 47/1000 | Loss: 0.00001045
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001044
Iteration 50/1000 | Loss: 0.00001043
Iteration 51/1000 | Loss: 0.00001042
Iteration 52/1000 | Loss: 0.00001042
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001037
Iteration 63/1000 | Loss: 0.00001037
Iteration 64/1000 | Loss: 0.00001037
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001035
Iteration 68/1000 | Loss: 0.00001035
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001035
Iteration 72/1000 | Loss: 0.00001035
Iteration 73/1000 | Loss: 0.00001035
Iteration 74/1000 | Loss: 0.00001035
Iteration 75/1000 | Loss: 0.00001035
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001034
Iteration 78/1000 | Loss: 0.00001034
Iteration 79/1000 | Loss: 0.00001034
Iteration 80/1000 | Loss: 0.00001034
Iteration 81/1000 | Loss: 0.00001034
Iteration 82/1000 | Loss: 0.00001034
Iteration 83/1000 | Loss: 0.00001034
Iteration 84/1000 | Loss: 0.00001034
Iteration 85/1000 | Loss: 0.00001034
Iteration 86/1000 | Loss: 0.00001033
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001032
Iteration 92/1000 | Loss: 0.00001032
Iteration 93/1000 | Loss: 0.00001032
Iteration 94/1000 | Loss: 0.00001032
Iteration 95/1000 | Loss: 0.00001032
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001031
Iteration 98/1000 | Loss: 0.00001031
Iteration 99/1000 | Loss: 0.00001031
Iteration 100/1000 | Loss: 0.00001030
Iteration 101/1000 | Loss: 0.00001030
Iteration 102/1000 | Loss: 0.00001030
Iteration 103/1000 | Loss: 0.00001030
Iteration 104/1000 | Loss: 0.00001030
Iteration 105/1000 | Loss: 0.00001030
Iteration 106/1000 | Loss: 0.00001029
Iteration 107/1000 | Loss: 0.00001029
Iteration 108/1000 | Loss: 0.00001029
Iteration 109/1000 | Loss: 0.00001029
Iteration 110/1000 | Loss: 0.00001029
Iteration 111/1000 | Loss: 0.00001029
Iteration 112/1000 | Loss: 0.00001029
Iteration 113/1000 | Loss: 0.00001029
Iteration 114/1000 | Loss: 0.00001029
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001025
Iteration 126/1000 | Loss: 0.00001025
Iteration 127/1000 | Loss: 0.00001025
Iteration 128/1000 | Loss: 0.00001024
Iteration 129/1000 | Loss: 0.00001024
Iteration 130/1000 | Loss: 0.00001024
Iteration 131/1000 | Loss: 0.00001023
Iteration 132/1000 | Loss: 0.00001023
Iteration 133/1000 | Loss: 0.00001023
Iteration 134/1000 | Loss: 0.00001023
Iteration 135/1000 | Loss: 0.00001023
Iteration 136/1000 | Loss: 0.00001023
Iteration 137/1000 | Loss: 0.00001022
Iteration 138/1000 | Loss: 0.00001022
Iteration 139/1000 | Loss: 0.00001022
Iteration 140/1000 | Loss: 0.00001022
Iteration 141/1000 | Loss: 0.00001022
Iteration 142/1000 | Loss: 0.00001022
Iteration 143/1000 | Loss: 0.00001022
Iteration 144/1000 | Loss: 0.00001022
Iteration 145/1000 | Loss: 0.00001022
Iteration 146/1000 | Loss: 0.00001021
Iteration 147/1000 | Loss: 0.00001021
Iteration 148/1000 | Loss: 0.00001021
Iteration 149/1000 | Loss: 0.00001020
Iteration 150/1000 | Loss: 0.00001020
Iteration 151/1000 | Loss: 0.00001020
Iteration 152/1000 | Loss: 0.00001020
Iteration 153/1000 | Loss: 0.00001020
Iteration 154/1000 | Loss: 0.00001020
Iteration 155/1000 | Loss: 0.00001020
Iteration 156/1000 | Loss: 0.00001020
Iteration 157/1000 | Loss: 0.00001020
Iteration 158/1000 | Loss: 0.00001020
Iteration 159/1000 | Loss: 0.00001020
Iteration 160/1000 | Loss: 0.00001020
Iteration 161/1000 | Loss: 0.00001020
Iteration 162/1000 | Loss: 0.00001020
Iteration 163/1000 | Loss: 0.00001020
Iteration 164/1000 | Loss: 0.00001020
Iteration 165/1000 | Loss: 0.00001020
Iteration 166/1000 | Loss: 0.00001020
Iteration 167/1000 | Loss: 0.00001020
Iteration 168/1000 | Loss: 0.00001020
Iteration 169/1000 | Loss: 0.00001020
Iteration 170/1000 | Loss: 0.00001020
Iteration 171/1000 | Loss: 0.00001020
Iteration 172/1000 | Loss: 0.00001020
Iteration 173/1000 | Loss: 0.00001020
Iteration 174/1000 | Loss: 0.00001020
Iteration 175/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.0201058103120886e-05, 1.0201058103120886e-05, 1.0201058103120886e-05, 1.0201058103120886e-05, 1.0201058103120886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0201058103120886e-05

Optimization complete. Final v2v error: 2.718336582183838 mm

Highest mean error: 3.514103412628174 mm for frame 65

Lowest mean error: 2.583038806915283 mm for frame 103

Saving results

Total time: 36.846962690353394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859971
Iteration 2/25 | Loss: 0.00103938
Iteration 3/25 | Loss: 0.00085014
Iteration 4/25 | Loss: 0.00080788
Iteration 5/25 | Loss: 0.00080243
Iteration 6/25 | Loss: 0.00080080
Iteration 7/25 | Loss: 0.00080036
Iteration 8/25 | Loss: 0.00080036
Iteration 9/25 | Loss: 0.00080036
Iteration 10/25 | Loss: 0.00080036
Iteration 11/25 | Loss: 0.00080036
Iteration 12/25 | Loss: 0.00080036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000800357898697257, 0.000800357898697257, 0.000800357898697257, 0.000800357898697257, 0.000800357898697257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000800357898697257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90044069
Iteration 2/25 | Loss: 0.00114737
Iteration 3/25 | Loss: 0.00114734
Iteration 4/25 | Loss: 0.00114733
Iteration 5/25 | Loss: 0.00114733
Iteration 6/25 | Loss: 0.00114733
Iteration 7/25 | Loss: 0.00114733
Iteration 8/25 | Loss: 0.00114733
Iteration 9/25 | Loss: 0.00114733
Iteration 10/25 | Loss: 0.00114733
Iteration 11/25 | Loss: 0.00114733
Iteration 12/25 | Loss: 0.00114733
Iteration 13/25 | Loss: 0.00114733
Iteration 14/25 | Loss: 0.00114733
Iteration 15/25 | Loss: 0.00114733
Iteration 16/25 | Loss: 0.00114733
Iteration 17/25 | Loss: 0.00114733
Iteration 18/25 | Loss: 0.00114733
Iteration 19/25 | Loss: 0.00114733
Iteration 20/25 | Loss: 0.00114733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011473321355879307, 0.0011473321355879307, 0.0011473321355879307, 0.0011473321355879307, 0.0011473321355879307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011473321355879307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114733
Iteration 2/1000 | Loss: 0.00004123
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002127
Iteration 6/1000 | Loss: 0.00002066
Iteration 7/1000 | Loss: 0.00002014
Iteration 8/1000 | Loss: 0.00001981
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001916
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001915
Iteration 15/1000 | Loss: 0.00001909
Iteration 16/1000 | Loss: 0.00001908
Iteration 17/1000 | Loss: 0.00001905
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001893
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001891
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001883
Iteration 32/1000 | Loss: 0.00001883
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001880
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001874
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001873
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001871
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001871
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001870
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001870
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001869
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.8693854144657962e-05, 1.8693854144657962e-05, 1.8693854144657962e-05, 1.8693854144657962e-05, 1.8693854144657962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8693854144657962e-05

Optimization complete. Final v2v error: 3.663525104522705 mm

Highest mean error: 3.9756200313568115 mm for frame 89

Lowest mean error: 3.3280787467956543 mm for frame 0

Saving results

Total time: 35.43440079689026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795840
Iteration 2/25 | Loss: 0.00104333
Iteration 3/25 | Loss: 0.00085711
Iteration 4/25 | Loss: 0.00082118
Iteration 5/25 | Loss: 0.00081489
Iteration 6/25 | Loss: 0.00081303
Iteration 7/25 | Loss: 0.00081288
Iteration 8/25 | Loss: 0.00081288
Iteration 9/25 | Loss: 0.00081288
Iteration 10/25 | Loss: 0.00081288
Iteration 11/25 | Loss: 0.00081288
Iteration 12/25 | Loss: 0.00081288
Iteration 13/25 | Loss: 0.00081288
Iteration 14/25 | Loss: 0.00081288
Iteration 15/25 | Loss: 0.00081288
Iteration 16/25 | Loss: 0.00081288
Iteration 17/25 | Loss: 0.00081288
Iteration 18/25 | Loss: 0.00081288
Iteration 19/25 | Loss: 0.00081288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008128757472150028, 0.0008128757472150028, 0.0008128757472150028, 0.0008128757472150028, 0.0008128757472150028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008128757472150028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.84124184
Iteration 2/25 | Loss: 0.00126698
Iteration 3/25 | Loss: 0.00126694
Iteration 4/25 | Loss: 0.00126694
Iteration 5/25 | Loss: 0.00126694
Iteration 6/25 | Loss: 0.00126694
Iteration 7/25 | Loss: 0.00126694
Iteration 8/25 | Loss: 0.00126694
Iteration 9/25 | Loss: 0.00126694
Iteration 10/25 | Loss: 0.00126694
Iteration 11/25 | Loss: 0.00126694
Iteration 12/25 | Loss: 0.00126694
Iteration 13/25 | Loss: 0.00126694
Iteration 14/25 | Loss: 0.00126694
Iteration 15/25 | Loss: 0.00126694
Iteration 16/25 | Loss: 0.00126694
Iteration 17/25 | Loss: 0.00126694
Iteration 18/25 | Loss: 0.00126694
Iteration 19/25 | Loss: 0.00126694
Iteration 20/25 | Loss: 0.00126694
Iteration 21/25 | Loss: 0.00126694
Iteration 22/25 | Loss: 0.00126694
Iteration 23/25 | Loss: 0.00126694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012669399147853255, 0.0012669399147853255, 0.0012669399147853255, 0.0012669399147853255, 0.0012669399147853255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012669399147853255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126694
Iteration 2/1000 | Loss: 0.00002536
Iteration 3/1000 | Loss: 0.00001878
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001576
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001535
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001529
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001528
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001523
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001521
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001520
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001515
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001511
Iteration 35/1000 | Loss: 0.00001505
Iteration 36/1000 | Loss: 0.00001505
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001502
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001501
Iteration 56/1000 | Loss: 0.00001501
Iteration 57/1000 | Loss: 0.00001501
Iteration 58/1000 | Loss: 0.00001501
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001500
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001499
Iteration 72/1000 | Loss: 0.00001499
Iteration 73/1000 | Loss: 0.00001499
Iteration 74/1000 | Loss: 0.00001499
Iteration 75/1000 | Loss: 0.00001498
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001498
Iteration 80/1000 | Loss: 0.00001498
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001497
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001495
Iteration 97/1000 | Loss: 0.00001495
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001494
Iteration 102/1000 | Loss: 0.00001494
Iteration 103/1000 | Loss: 0.00001494
Iteration 104/1000 | Loss: 0.00001494
Iteration 105/1000 | Loss: 0.00001494
Iteration 106/1000 | Loss: 0.00001494
Iteration 107/1000 | Loss: 0.00001494
Iteration 108/1000 | Loss: 0.00001494
Iteration 109/1000 | Loss: 0.00001494
Iteration 110/1000 | Loss: 0.00001494
Iteration 111/1000 | Loss: 0.00001493
Iteration 112/1000 | Loss: 0.00001493
Iteration 113/1000 | Loss: 0.00001493
Iteration 114/1000 | Loss: 0.00001493
Iteration 115/1000 | Loss: 0.00001493
Iteration 116/1000 | Loss: 0.00001493
Iteration 117/1000 | Loss: 0.00001493
Iteration 118/1000 | Loss: 0.00001493
Iteration 119/1000 | Loss: 0.00001493
Iteration 120/1000 | Loss: 0.00001493
Iteration 121/1000 | Loss: 0.00001493
Iteration 122/1000 | Loss: 0.00001493
Iteration 123/1000 | Loss: 0.00001492
Iteration 124/1000 | Loss: 0.00001492
Iteration 125/1000 | Loss: 0.00001492
Iteration 126/1000 | Loss: 0.00001491
Iteration 127/1000 | Loss: 0.00001491
Iteration 128/1000 | Loss: 0.00001491
Iteration 129/1000 | Loss: 0.00001491
Iteration 130/1000 | Loss: 0.00001490
Iteration 131/1000 | Loss: 0.00001490
Iteration 132/1000 | Loss: 0.00001490
Iteration 133/1000 | Loss: 0.00001490
Iteration 134/1000 | Loss: 0.00001490
Iteration 135/1000 | Loss: 0.00001490
Iteration 136/1000 | Loss: 0.00001490
Iteration 137/1000 | Loss: 0.00001489
Iteration 138/1000 | Loss: 0.00001489
Iteration 139/1000 | Loss: 0.00001489
Iteration 140/1000 | Loss: 0.00001489
Iteration 141/1000 | Loss: 0.00001489
Iteration 142/1000 | Loss: 0.00001489
Iteration 143/1000 | Loss: 0.00001489
Iteration 144/1000 | Loss: 0.00001489
Iteration 145/1000 | Loss: 0.00001489
Iteration 146/1000 | Loss: 0.00001488
Iteration 147/1000 | Loss: 0.00001488
Iteration 148/1000 | Loss: 0.00001488
Iteration 149/1000 | Loss: 0.00001488
Iteration 150/1000 | Loss: 0.00001488
Iteration 151/1000 | Loss: 0.00001488
Iteration 152/1000 | Loss: 0.00001488
Iteration 153/1000 | Loss: 0.00001488
Iteration 154/1000 | Loss: 0.00001487
Iteration 155/1000 | Loss: 0.00001487
Iteration 156/1000 | Loss: 0.00001487
Iteration 157/1000 | Loss: 0.00001487
Iteration 158/1000 | Loss: 0.00001487
Iteration 159/1000 | Loss: 0.00001487
Iteration 160/1000 | Loss: 0.00001486
Iteration 161/1000 | Loss: 0.00001486
Iteration 162/1000 | Loss: 0.00001486
Iteration 163/1000 | Loss: 0.00001486
Iteration 164/1000 | Loss: 0.00001486
Iteration 165/1000 | Loss: 0.00001486
Iteration 166/1000 | Loss: 0.00001486
Iteration 167/1000 | Loss: 0.00001486
Iteration 168/1000 | Loss: 0.00001486
Iteration 169/1000 | Loss: 0.00001486
Iteration 170/1000 | Loss: 0.00001486
Iteration 171/1000 | Loss: 0.00001486
Iteration 172/1000 | Loss: 0.00001485
Iteration 173/1000 | Loss: 0.00001485
Iteration 174/1000 | Loss: 0.00001485
Iteration 175/1000 | Loss: 0.00001485
Iteration 176/1000 | Loss: 0.00001485
Iteration 177/1000 | Loss: 0.00001485
Iteration 178/1000 | Loss: 0.00001485
Iteration 179/1000 | Loss: 0.00001485
Iteration 180/1000 | Loss: 0.00001485
Iteration 181/1000 | Loss: 0.00001485
Iteration 182/1000 | Loss: 0.00001485
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001485
Iteration 185/1000 | Loss: 0.00001485
Iteration 186/1000 | Loss: 0.00001485
Iteration 187/1000 | Loss: 0.00001485
Iteration 188/1000 | Loss: 0.00001485
Iteration 189/1000 | Loss: 0.00001485
Iteration 190/1000 | Loss: 0.00001485
Iteration 191/1000 | Loss: 0.00001485
Iteration 192/1000 | Loss: 0.00001485
Iteration 193/1000 | Loss: 0.00001485
Iteration 194/1000 | Loss: 0.00001485
Iteration 195/1000 | Loss: 0.00001485
Iteration 196/1000 | Loss: 0.00001485
Iteration 197/1000 | Loss: 0.00001485
Iteration 198/1000 | Loss: 0.00001485
Iteration 199/1000 | Loss: 0.00001485
Iteration 200/1000 | Loss: 0.00001485
Iteration 201/1000 | Loss: 0.00001485
Iteration 202/1000 | Loss: 0.00001485
Iteration 203/1000 | Loss: 0.00001485
Iteration 204/1000 | Loss: 0.00001485
Iteration 205/1000 | Loss: 0.00001485
Iteration 206/1000 | Loss: 0.00001485
Iteration 207/1000 | Loss: 0.00001485
Iteration 208/1000 | Loss: 0.00001485
Iteration 209/1000 | Loss: 0.00001485
Iteration 210/1000 | Loss: 0.00001485
Iteration 211/1000 | Loss: 0.00001485
Iteration 212/1000 | Loss: 0.00001485
Iteration 213/1000 | Loss: 0.00001485
Iteration 214/1000 | Loss: 0.00001485
Iteration 215/1000 | Loss: 0.00001485
Iteration 216/1000 | Loss: 0.00001485
Iteration 217/1000 | Loss: 0.00001485
Iteration 218/1000 | Loss: 0.00001485
Iteration 219/1000 | Loss: 0.00001485
Iteration 220/1000 | Loss: 0.00001485
Iteration 221/1000 | Loss: 0.00001485
Iteration 222/1000 | Loss: 0.00001485
Iteration 223/1000 | Loss: 0.00001485
Iteration 224/1000 | Loss: 0.00001485
Iteration 225/1000 | Loss: 0.00001485
Iteration 226/1000 | Loss: 0.00001485
Iteration 227/1000 | Loss: 0.00001485
Iteration 228/1000 | Loss: 0.00001485
Iteration 229/1000 | Loss: 0.00001485
Iteration 230/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.484841232013423e-05, 1.484841232013423e-05, 1.484841232013423e-05, 1.484841232013423e-05, 1.484841232013423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.484841232013423e-05

Optimization complete. Final v2v error: 3.273442506790161 mm

Highest mean error: 3.6226279735565186 mm for frame 101

Lowest mean error: 2.975041389465332 mm for frame 207

Saving results

Total time: 42.19424653053284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026453
Iteration 2/25 | Loss: 0.00337111
Iteration 3/25 | Loss: 0.00238207
Iteration 4/25 | Loss: 0.00199887
Iteration 5/25 | Loss: 0.00197636
Iteration 6/25 | Loss: 0.00195489
Iteration 7/25 | Loss: 0.00179748
Iteration 8/25 | Loss: 0.00143033
Iteration 9/25 | Loss: 0.00130132
Iteration 10/25 | Loss: 0.00121947
Iteration 11/25 | Loss: 0.00118544
Iteration 12/25 | Loss: 0.00114627
Iteration 13/25 | Loss: 0.00111378
Iteration 14/25 | Loss: 0.00109324
Iteration 15/25 | Loss: 0.00106285
Iteration 16/25 | Loss: 0.00105542
Iteration 17/25 | Loss: 0.00105154
Iteration 18/25 | Loss: 0.00104890
Iteration 19/25 | Loss: 0.00105898
Iteration 20/25 | Loss: 0.00105579
Iteration 21/25 | Loss: 0.00105101
Iteration 22/25 | Loss: 0.00104149
Iteration 23/25 | Loss: 0.00103921
Iteration 24/25 | Loss: 0.00105420
Iteration 25/25 | Loss: 0.00103287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56879139
Iteration 2/25 | Loss: 0.00295264
Iteration 3/25 | Loss: 0.00267561
Iteration 4/25 | Loss: 0.00267561
Iteration 5/25 | Loss: 0.00267561
Iteration 6/25 | Loss: 0.00267561
Iteration 7/25 | Loss: 0.00267561
Iteration 8/25 | Loss: 0.00267560
Iteration 9/25 | Loss: 0.00267560
Iteration 10/25 | Loss: 0.00267560
Iteration 11/25 | Loss: 0.00267560
Iteration 12/25 | Loss: 0.00267560
Iteration 13/25 | Loss: 0.00267560
Iteration 14/25 | Loss: 0.00267560
Iteration 15/25 | Loss: 0.00267560
Iteration 16/25 | Loss: 0.00267560
Iteration 17/25 | Loss: 0.00267560
Iteration 18/25 | Loss: 0.00267560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0026756038423627615, 0.0026756038423627615, 0.0026756038423627615, 0.0026756038423627615, 0.0026756038423627615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026756038423627615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267560
Iteration 2/1000 | Loss: 0.00117495
Iteration 3/1000 | Loss: 0.00041498
Iteration 4/1000 | Loss: 0.00034649
Iteration 5/1000 | Loss: 0.00018661
Iteration 6/1000 | Loss: 0.00020864
Iteration 7/1000 | Loss: 0.00017293
Iteration 8/1000 | Loss: 0.00021371
Iteration 9/1000 | Loss: 0.00081413
Iteration 10/1000 | Loss: 0.00057741
Iteration 11/1000 | Loss: 0.00088196
Iteration 12/1000 | Loss: 0.00072259
Iteration 13/1000 | Loss: 0.00070381
Iteration 14/1000 | Loss: 0.00052757
Iteration 15/1000 | Loss: 0.00047712
Iteration 16/1000 | Loss: 0.00032060
Iteration 17/1000 | Loss: 0.00043156
Iteration 18/1000 | Loss: 0.00053715
Iteration 19/1000 | Loss: 0.00159118
Iteration 20/1000 | Loss: 0.00152694
Iteration 21/1000 | Loss: 0.00092978
Iteration 22/1000 | Loss: 0.00082423
Iteration 23/1000 | Loss: 0.00057918
Iteration 24/1000 | Loss: 0.00040160
Iteration 25/1000 | Loss: 0.00095204
Iteration 26/1000 | Loss: 0.00106457
Iteration 27/1000 | Loss: 0.00019184
Iteration 28/1000 | Loss: 0.00028587
Iteration 29/1000 | Loss: 0.00036085
Iteration 30/1000 | Loss: 0.00028257
Iteration 31/1000 | Loss: 0.00023677
Iteration 32/1000 | Loss: 0.00030558
Iteration 33/1000 | Loss: 0.00062295
Iteration 34/1000 | Loss: 0.00030061
Iteration 35/1000 | Loss: 0.00013056
Iteration 36/1000 | Loss: 0.00037441
Iteration 37/1000 | Loss: 0.00027997
Iteration 38/1000 | Loss: 0.00021197
Iteration 39/1000 | Loss: 0.00021347
Iteration 40/1000 | Loss: 0.00029047
Iteration 41/1000 | Loss: 0.00033238
Iteration 42/1000 | Loss: 0.00027166
Iteration 43/1000 | Loss: 0.00028847
Iteration 44/1000 | Loss: 0.00026010
Iteration 45/1000 | Loss: 0.00049234
Iteration 46/1000 | Loss: 0.00024152
Iteration 47/1000 | Loss: 0.00047397
Iteration 48/1000 | Loss: 0.00031766
Iteration 49/1000 | Loss: 0.00070111
Iteration 50/1000 | Loss: 0.00057249
Iteration 51/1000 | Loss: 0.00056098
Iteration 52/1000 | Loss: 0.00033423
Iteration 53/1000 | Loss: 0.00026657
Iteration 54/1000 | Loss: 0.00028770
Iteration 55/1000 | Loss: 0.00037426
Iteration 56/1000 | Loss: 0.00026016
Iteration 57/1000 | Loss: 0.00030773
Iteration 58/1000 | Loss: 0.00032336
Iteration 59/1000 | Loss: 0.00042670
Iteration 60/1000 | Loss: 0.00054572
Iteration 61/1000 | Loss: 0.00040052
Iteration 62/1000 | Loss: 0.00041920
Iteration 63/1000 | Loss: 0.00031080
Iteration 64/1000 | Loss: 0.00053281
Iteration 65/1000 | Loss: 0.00026374
Iteration 66/1000 | Loss: 0.00062131
Iteration 67/1000 | Loss: 0.00040447
Iteration 68/1000 | Loss: 0.00019195
Iteration 69/1000 | Loss: 0.00018066
Iteration 70/1000 | Loss: 0.00018481
Iteration 71/1000 | Loss: 0.00020802
Iteration 72/1000 | Loss: 0.00026211
Iteration 73/1000 | Loss: 0.00024701
Iteration 74/1000 | Loss: 0.00014772
Iteration 75/1000 | Loss: 0.00029753
Iteration 76/1000 | Loss: 0.00022093
Iteration 77/1000 | Loss: 0.00029687
Iteration 78/1000 | Loss: 0.00030419
Iteration 79/1000 | Loss: 0.00034621
Iteration 80/1000 | Loss: 0.00033570
Iteration 81/1000 | Loss: 0.00031863
Iteration 82/1000 | Loss: 0.00047539
Iteration 83/1000 | Loss: 0.00133085
Iteration 84/1000 | Loss: 0.00169024
Iteration 85/1000 | Loss: 0.00115236
Iteration 86/1000 | Loss: 0.00201141
Iteration 87/1000 | Loss: 0.00133251
Iteration 88/1000 | Loss: 0.00281915
Iteration 89/1000 | Loss: 0.00089414
Iteration 90/1000 | Loss: 0.00017981
Iteration 91/1000 | Loss: 0.00022377
Iteration 92/1000 | Loss: 0.00061412
Iteration 93/1000 | Loss: 0.00119608
Iteration 94/1000 | Loss: 0.00079976
Iteration 95/1000 | Loss: 0.00046221
Iteration 96/1000 | Loss: 0.00011458
Iteration 97/1000 | Loss: 0.00036757
Iteration 98/1000 | Loss: 0.00060648
Iteration 99/1000 | Loss: 0.00060947
Iteration 100/1000 | Loss: 0.00019888
Iteration 101/1000 | Loss: 0.00019195
Iteration 102/1000 | Loss: 0.00045392
Iteration 103/1000 | Loss: 0.00019496
Iteration 104/1000 | Loss: 0.00023750
Iteration 105/1000 | Loss: 0.00017640
Iteration 106/1000 | Loss: 0.00020278
Iteration 107/1000 | Loss: 0.00041392
Iteration 108/1000 | Loss: 0.00055329
Iteration 109/1000 | Loss: 0.00017266
Iteration 110/1000 | Loss: 0.00017336
Iteration 111/1000 | Loss: 0.00029183
Iteration 112/1000 | Loss: 0.00011048
Iteration 113/1000 | Loss: 0.00009921
Iteration 114/1000 | Loss: 0.00014264
Iteration 115/1000 | Loss: 0.00013846
Iteration 116/1000 | Loss: 0.00023735
Iteration 117/1000 | Loss: 0.00014973
Iteration 118/1000 | Loss: 0.00013681
Iteration 119/1000 | Loss: 0.00014211
Iteration 120/1000 | Loss: 0.00014902
Iteration 121/1000 | Loss: 0.00025356
Iteration 122/1000 | Loss: 0.00024543
Iteration 123/1000 | Loss: 0.00031927
Iteration 124/1000 | Loss: 0.00019482
Iteration 125/1000 | Loss: 0.00012962
Iteration 126/1000 | Loss: 0.00014774
Iteration 127/1000 | Loss: 0.00021612
Iteration 128/1000 | Loss: 0.00017340
Iteration 129/1000 | Loss: 0.00012180
Iteration 130/1000 | Loss: 0.00015060
Iteration 131/1000 | Loss: 0.00023202
Iteration 132/1000 | Loss: 0.00018416
Iteration 133/1000 | Loss: 0.00012524
Iteration 134/1000 | Loss: 0.00057019
Iteration 135/1000 | Loss: 0.00023209
Iteration 136/1000 | Loss: 0.00025919
Iteration 137/1000 | Loss: 0.00022519
Iteration 138/1000 | Loss: 0.00025583
Iteration 139/1000 | Loss: 0.00077608
Iteration 140/1000 | Loss: 0.00017800
Iteration 141/1000 | Loss: 0.00037731
Iteration 142/1000 | Loss: 0.00020489
Iteration 143/1000 | Loss: 0.00022276
Iteration 144/1000 | Loss: 0.00011250
Iteration 145/1000 | Loss: 0.00009548
Iteration 146/1000 | Loss: 0.00011480
Iteration 147/1000 | Loss: 0.00011152
Iteration 148/1000 | Loss: 0.00013101
Iteration 149/1000 | Loss: 0.00011202
Iteration 150/1000 | Loss: 0.00014294
Iteration 151/1000 | Loss: 0.00018388
Iteration 152/1000 | Loss: 0.00013038
Iteration 153/1000 | Loss: 0.00013266
Iteration 154/1000 | Loss: 0.00012645
Iteration 155/1000 | Loss: 0.00011890
Iteration 156/1000 | Loss: 0.00011513
Iteration 157/1000 | Loss: 0.00011246
Iteration 158/1000 | Loss: 0.00012112
Iteration 159/1000 | Loss: 0.00012174
Iteration 160/1000 | Loss: 0.00025194
Iteration 161/1000 | Loss: 0.00021585
Iteration 162/1000 | Loss: 0.00023626
Iteration 163/1000 | Loss: 0.00022532
Iteration 164/1000 | Loss: 0.00023297
Iteration 165/1000 | Loss: 0.00042195
Iteration 166/1000 | Loss: 0.00024879
Iteration 167/1000 | Loss: 0.00022385
Iteration 168/1000 | Loss: 0.00020402
Iteration 169/1000 | Loss: 0.00017010
Iteration 170/1000 | Loss: 0.00022501
Iteration 171/1000 | Loss: 0.00018398
Iteration 172/1000 | Loss: 0.00020371
Iteration 173/1000 | Loss: 0.00017662
Iteration 174/1000 | Loss: 0.00020582
Iteration 175/1000 | Loss: 0.00021917
Iteration 176/1000 | Loss: 0.00023598
Iteration 177/1000 | Loss: 0.00019900
Iteration 178/1000 | Loss: 0.00024893
Iteration 179/1000 | Loss: 0.00018041
Iteration 180/1000 | Loss: 0.00017891
Iteration 181/1000 | Loss: 0.00018636
Iteration 182/1000 | Loss: 0.00032364
Iteration 183/1000 | Loss: 0.00025120
Iteration 184/1000 | Loss: 0.00029430
Iteration 185/1000 | Loss: 0.00022870
Iteration 186/1000 | Loss: 0.00041195
Iteration 187/1000 | Loss: 0.00031600
Iteration 188/1000 | Loss: 0.00042790
Iteration 189/1000 | Loss: 0.00030887
Iteration 190/1000 | Loss: 0.00028001
Iteration 191/1000 | Loss: 0.00024956
Iteration 192/1000 | Loss: 0.00036993
Iteration 193/1000 | Loss: 0.00028989
Iteration 194/1000 | Loss: 0.00023025
Iteration 195/1000 | Loss: 0.00027703
Iteration 196/1000 | Loss: 0.00024256
Iteration 197/1000 | Loss: 0.00019549
Iteration 198/1000 | Loss: 0.00029717
Iteration 199/1000 | Loss: 0.00025470
Iteration 200/1000 | Loss: 0.00027247
Iteration 201/1000 | Loss: 0.00025028
Iteration 202/1000 | Loss: 0.00028362
Iteration 203/1000 | Loss: 0.00026237
Iteration 204/1000 | Loss: 0.00034644
Iteration 205/1000 | Loss: 0.00030166
Iteration 206/1000 | Loss: 0.00038382
Iteration 207/1000 | Loss: 0.00027208
Iteration 208/1000 | Loss: 0.00038586
Iteration 209/1000 | Loss: 0.00029297
Iteration 210/1000 | Loss: 0.00030367
Iteration 211/1000 | Loss: 0.00022309
Iteration 212/1000 | Loss: 0.00027284
Iteration 213/1000 | Loss: 0.00030114
Iteration 214/1000 | Loss: 0.00028974
Iteration 215/1000 | Loss: 0.00030366
Iteration 216/1000 | Loss: 0.00023104
Iteration 217/1000 | Loss: 0.00028645
Iteration 218/1000 | Loss: 0.00022193
Iteration 219/1000 | Loss: 0.00027581
Iteration 220/1000 | Loss: 0.00014162
Iteration 221/1000 | Loss: 0.00012080
Iteration 222/1000 | Loss: 0.00021475
Iteration 223/1000 | Loss: 0.00023811
Iteration 224/1000 | Loss: 0.00016053
Iteration 225/1000 | Loss: 0.00019377
Iteration 226/1000 | Loss: 0.00014469
Iteration 227/1000 | Loss: 0.00016196
Iteration 228/1000 | Loss: 0.00018465
Iteration 229/1000 | Loss: 0.00022827
Iteration 230/1000 | Loss: 0.00017817
Iteration 231/1000 | Loss: 0.00026442
Iteration 232/1000 | Loss: 0.00027457
Iteration 233/1000 | Loss: 0.00027745
Iteration 234/1000 | Loss: 0.00024225
Iteration 235/1000 | Loss: 0.00018953
Iteration 236/1000 | Loss: 0.00011652
Iteration 237/1000 | Loss: 0.00012631
Iteration 238/1000 | Loss: 0.00016279
Iteration 239/1000 | Loss: 0.00024671
Iteration 240/1000 | Loss: 0.00029418
Iteration 241/1000 | Loss: 0.00020101
Iteration 242/1000 | Loss: 0.00023101
Iteration 243/1000 | Loss: 0.00018267
Iteration 244/1000 | Loss: 0.00017763
Iteration 245/1000 | Loss: 0.00022166
Iteration 246/1000 | Loss: 0.00018523
Iteration 247/1000 | Loss: 0.00020819
Iteration 248/1000 | Loss: 0.00010821
Iteration 249/1000 | Loss: 0.00010755
Iteration 250/1000 | Loss: 0.00017702
Iteration 251/1000 | Loss: 0.00020608
Iteration 252/1000 | Loss: 0.00013300
Iteration 253/1000 | Loss: 0.00011651
Iteration 254/1000 | Loss: 0.00012009
Iteration 255/1000 | Loss: 0.00012046
Iteration 256/1000 | Loss: 0.00010600
Iteration 257/1000 | Loss: 0.00024374
Iteration 258/1000 | Loss: 0.00011800
Iteration 259/1000 | Loss: 0.00026834
Iteration 260/1000 | Loss: 0.00012981
Iteration 261/1000 | Loss: 0.00017906
Iteration 262/1000 | Loss: 0.00014870
Iteration 263/1000 | Loss: 0.00012109
Iteration 264/1000 | Loss: 0.00013862
Iteration 265/1000 | Loss: 0.00014287
Iteration 266/1000 | Loss: 0.00013411
Iteration 267/1000 | Loss: 0.00011445
Iteration 268/1000 | Loss: 0.00011568
Iteration 269/1000 | Loss: 0.00011532
Iteration 270/1000 | Loss: 0.00031825
Iteration 271/1000 | Loss: 0.00012806
Iteration 272/1000 | Loss: 0.00014464
Iteration 273/1000 | Loss: 0.00013997
Iteration 274/1000 | Loss: 0.00013341
Iteration 275/1000 | Loss: 0.00009292
Iteration 276/1000 | Loss: 0.00009637
Iteration 277/1000 | Loss: 0.00011665
Iteration 278/1000 | Loss: 0.00011921
Iteration 279/1000 | Loss: 0.00012067
Iteration 280/1000 | Loss: 0.00016530
Iteration 281/1000 | Loss: 0.00013642
Iteration 282/1000 | Loss: 0.00022478
Iteration 283/1000 | Loss: 0.00010264
Iteration 284/1000 | Loss: 0.00007930
Iteration 285/1000 | Loss: 0.00005822
Iteration 286/1000 | Loss: 0.00006033
Iteration 287/1000 | Loss: 0.00007102
Iteration 288/1000 | Loss: 0.00008159
Iteration 289/1000 | Loss: 0.00007135
Iteration 290/1000 | Loss: 0.00008226
Iteration 291/1000 | Loss: 0.00018619
Iteration 292/1000 | Loss: 0.00009844
Iteration 293/1000 | Loss: 0.00007086
Iteration 294/1000 | Loss: 0.00008696
Iteration 295/1000 | Loss: 0.00010074
Iteration 296/1000 | Loss: 0.00011326
Iteration 297/1000 | Loss: 0.00008914
Iteration 298/1000 | Loss: 0.00010921
Iteration 299/1000 | Loss: 0.00012886
Iteration 300/1000 | Loss: 0.00008680
Iteration 301/1000 | Loss: 0.00009415
Iteration 302/1000 | Loss: 0.00008602
Iteration 303/1000 | Loss: 0.00006980
Iteration 304/1000 | Loss: 0.00006582
Iteration 305/1000 | Loss: 0.00008910
Iteration 306/1000 | Loss: 0.00007569
Iteration 307/1000 | Loss: 0.00006985
Iteration 308/1000 | Loss: 0.00008782
Iteration 309/1000 | Loss: 0.00007316
Iteration 310/1000 | Loss: 0.00008827
Iteration 311/1000 | Loss: 0.00008494
Iteration 312/1000 | Loss: 0.00007638
Iteration 313/1000 | Loss: 0.00010168
Iteration 314/1000 | Loss: 0.00008494
Iteration 315/1000 | Loss: 0.00007948
Iteration 316/1000 | Loss: 0.00007243
Iteration 317/1000 | Loss: 0.00008128
Iteration 318/1000 | Loss: 0.00008493
Iteration 319/1000 | Loss: 0.00007119
Iteration 320/1000 | Loss: 0.00007070
Iteration 321/1000 | Loss: 0.00006896
Iteration 322/1000 | Loss: 0.00007022
Iteration 323/1000 | Loss: 0.00006359
Iteration 324/1000 | Loss: 0.00016149
Iteration 325/1000 | Loss: 0.00008477
Iteration 326/1000 | Loss: 0.00007791
Iteration 327/1000 | Loss: 0.00007240
Iteration 328/1000 | Loss: 0.00007504
Iteration 329/1000 | Loss: 0.00007270
Iteration 330/1000 | Loss: 0.00016725
Iteration 331/1000 | Loss: 0.00007739
Iteration 332/1000 | Loss: 0.00007529
Iteration 333/1000 | Loss: 0.00007348
Iteration 334/1000 | Loss: 0.00008202
Iteration 335/1000 | Loss: 0.00006993
Iteration 336/1000 | Loss: 0.00016051
Iteration 337/1000 | Loss: 0.00009435
Iteration 338/1000 | Loss: 0.00021700
Iteration 339/1000 | Loss: 0.00011658
Iteration 340/1000 | Loss: 0.00011995
Iteration 341/1000 | Loss: 0.00010375
Iteration 342/1000 | Loss: 0.00009995
Iteration 343/1000 | Loss: 0.00006584
Iteration 344/1000 | Loss: 0.00008545
Iteration 345/1000 | Loss: 0.00006624
Iteration 346/1000 | Loss: 0.00007663
Iteration 347/1000 | Loss: 0.00007565
Iteration 348/1000 | Loss: 0.00007781
Iteration 349/1000 | Loss: 0.00007421
Iteration 350/1000 | Loss: 0.00006377
Iteration 351/1000 | Loss: 0.00006986
Iteration 352/1000 | Loss: 0.00005929
Iteration 353/1000 | Loss: 0.00005971
Iteration 354/1000 | Loss: 0.00009477
Iteration 355/1000 | Loss: 0.00006703
Iteration 356/1000 | Loss: 0.00007455
Iteration 357/1000 | Loss: 0.00007167
Iteration 358/1000 | Loss: 0.00006618
Iteration 359/1000 | Loss: 0.00006771
Iteration 360/1000 | Loss: 0.00007174
Iteration 361/1000 | Loss: 0.00007137
Iteration 362/1000 | Loss: 0.00006791
Iteration 363/1000 | Loss: 0.00007188
Iteration 364/1000 | Loss: 0.00006254
Iteration 365/1000 | Loss: 0.00006978
Iteration 366/1000 | Loss: 0.00006978
Iteration 367/1000 | Loss: 0.00007102
Iteration 368/1000 | Loss: 0.00006783
Iteration 369/1000 | Loss: 0.00007109
Iteration 370/1000 | Loss: 0.00006454
Iteration 371/1000 | Loss: 0.00007243
Iteration 372/1000 | Loss: 0.00009202
Iteration 373/1000 | Loss: 0.00007690
Iteration 374/1000 | Loss: 0.00008596
Iteration 375/1000 | Loss: 0.00008251
Iteration 376/1000 | Loss: 0.00008555
Iteration 377/1000 | Loss: 0.00008468
Iteration 378/1000 | Loss: 0.00008316
Iteration 379/1000 | Loss: 0.00009889
Iteration 380/1000 | Loss: 0.00007521
Iteration 381/1000 | Loss: 0.00008557
Iteration 382/1000 | Loss: 0.00006814
Iteration 383/1000 | Loss: 0.00006771
Iteration 384/1000 | Loss: 0.00006742
Iteration 385/1000 | Loss: 0.00007076
Iteration 386/1000 | Loss: 0.00007106
Iteration 387/1000 | Loss: 0.00009923
Iteration 388/1000 | Loss: 0.00007816
Iteration 389/1000 | Loss: 0.00007864
Iteration 390/1000 | Loss: 0.00006167
Iteration 391/1000 | Loss: 0.00006844
Iteration 392/1000 | Loss: 0.00005997
Iteration 393/1000 | Loss: 0.00006473
Iteration 394/1000 | Loss: 0.00007172
Iteration 395/1000 | Loss: 0.00008185
Iteration 396/1000 | Loss: 0.00009369
Iteration 397/1000 | Loss: 0.00008504
Iteration 398/1000 | Loss: 0.00008957
Iteration 399/1000 | Loss: 0.00007456
Iteration 400/1000 | Loss: 0.00009261
Iteration 401/1000 | Loss: 0.00007319
Iteration 402/1000 | Loss: 0.00006761
Iteration 403/1000 | Loss: 0.00005360
Iteration 404/1000 | Loss: 0.00006620
Iteration 405/1000 | Loss: 0.00009004
Iteration 406/1000 | Loss: 0.00008533
Iteration 407/1000 | Loss: 0.00008917
Iteration 408/1000 | Loss: 0.00007211
Iteration 409/1000 | Loss: 0.00009272
Iteration 410/1000 | Loss: 0.00005773
Iteration 411/1000 | Loss: 0.00006351
Iteration 412/1000 | Loss: 0.00007491
Iteration 413/1000 | Loss: 0.00006821
Iteration 414/1000 | Loss: 0.00008458
Iteration 415/1000 | Loss: 0.00006107
Iteration 416/1000 | Loss: 0.00009755
Iteration 417/1000 | Loss: 0.00006732
Iteration 418/1000 | Loss: 0.00006172
Iteration 419/1000 | Loss: 0.00005521
Iteration 420/1000 | Loss: 0.00005982
Iteration 421/1000 | Loss: 0.00010282
Iteration 422/1000 | Loss: 0.00011997
Iteration 423/1000 | Loss: 0.00017212
Iteration 424/1000 | Loss: 0.00008777
Iteration 425/1000 | Loss: 0.00005668
Iteration 426/1000 | Loss: 0.00005233
Iteration 427/1000 | Loss: 0.00004968
Iteration 428/1000 | Loss: 0.00004836
Iteration 429/1000 | Loss: 0.00004774
Iteration 430/1000 | Loss: 0.00020237
Iteration 431/1000 | Loss: 0.00005057
Iteration 432/1000 | Loss: 0.00004827
Iteration 433/1000 | Loss: 0.00004685
Iteration 434/1000 | Loss: 0.00004583
Iteration 435/1000 | Loss: 0.00019479
Iteration 436/1000 | Loss: 0.00005024
Iteration 437/1000 | Loss: 0.00004694
Iteration 438/1000 | Loss: 0.00004566
Iteration 439/1000 | Loss: 0.00004458
Iteration 440/1000 | Loss: 0.00004376
Iteration 441/1000 | Loss: 0.00004335
Iteration 442/1000 | Loss: 0.00004321
Iteration 443/1000 | Loss: 0.00004311
Iteration 444/1000 | Loss: 0.00004311
Iteration 445/1000 | Loss: 0.00004311
Iteration 446/1000 | Loss: 0.00004308
Iteration 447/1000 | Loss: 0.00004307
Iteration 448/1000 | Loss: 0.00004306
Iteration 449/1000 | Loss: 0.00004303
Iteration 450/1000 | Loss: 0.00004303
Iteration 451/1000 | Loss: 0.00004302
Iteration 452/1000 | Loss: 0.00004300
Iteration 453/1000 | Loss: 0.00004291
Iteration 454/1000 | Loss: 0.00004290
Iteration 455/1000 | Loss: 0.00004289
Iteration 456/1000 | Loss: 0.00004289
Iteration 457/1000 | Loss: 0.00004289
Iteration 458/1000 | Loss: 0.00004289
Iteration 459/1000 | Loss: 0.00004288
Iteration 460/1000 | Loss: 0.00004288
Iteration 461/1000 | Loss: 0.00004287
Iteration 462/1000 | Loss: 0.00004287
Iteration 463/1000 | Loss: 0.00004287
Iteration 464/1000 | Loss: 0.00004286
Iteration 465/1000 | Loss: 0.00004286
Iteration 466/1000 | Loss: 0.00004286
Iteration 467/1000 | Loss: 0.00004286
Iteration 468/1000 | Loss: 0.00004286
Iteration 469/1000 | Loss: 0.00004286
Iteration 470/1000 | Loss: 0.00004286
Iteration 471/1000 | Loss: 0.00004286
Iteration 472/1000 | Loss: 0.00004286
Iteration 473/1000 | Loss: 0.00004286
Iteration 474/1000 | Loss: 0.00004286
Iteration 475/1000 | Loss: 0.00004286
Iteration 476/1000 | Loss: 0.00004284
Iteration 477/1000 | Loss: 0.00004284
Iteration 478/1000 | Loss: 0.00004284
Iteration 479/1000 | Loss: 0.00004284
Iteration 480/1000 | Loss: 0.00004284
Iteration 481/1000 | Loss: 0.00004284
Iteration 482/1000 | Loss: 0.00004284
Iteration 483/1000 | Loss: 0.00004284
Iteration 484/1000 | Loss: 0.00004284
Iteration 485/1000 | Loss: 0.00004284
Iteration 486/1000 | Loss: 0.00004284
Iteration 487/1000 | Loss: 0.00004284
Iteration 488/1000 | Loss: 0.00004283
Iteration 489/1000 | Loss: 0.00004283
Iteration 490/1000 | Loss: 0.00004283
Iteration 491/1000 | Loss: 0.00004283
Iteration 492/1000 | Loss: 0.00004283
Iteration 493/1000 | Loss: 0.00004283
Iteration 494/1000 | Loss: 0.00004283
Iteration 495/1000 | Loss: 0.00004282
Iteration 496/1000 | Loss: 0.00004282
Iteration 497/1000 | Loss: 0.00004282
Iteration 498/1000 | Loss: 0.00004282
Iteration 499/1000 | Loss: 0.00004282
Iteration 500/1000 | Loss: 0.00004282
Iteration 501/1000 | Loss: 0.00004282
Iteration 502/1000 | Loss: 0.00004282
Iteration 503/1000 | Loss: 0.00004281
Iteration 504/1000 | Loss: 0.00004281
Iteration 505/1000 | Loss: 0.00004281
Iteration 506/1000 | Loss: 0.00004281
Iteration 507/1000 | Loss: 0.00004281
Iteration 508/1000 | Loss: 0.00004281
Iteration 509/1000 | Loss: 0.00004281
Iteration 510/1000 | Loss: 0.00004281
Iteration 511/1000 | Loss: 0.00004281
Iteration 512/1000 | Loss: 0.00004281
Iteration 513/1000 | Loss: 0.00004281
Iteration 514/1000 | Loss: 0.00004281
Iteration 515/1000 | Loss: 0.00004281
Iteration 516/1000 | Loss: 0.00004281
Iteration 517/1000 | Loss: 0.00004280
Iteration 518/1000 | Loss: 0.00004280
Iteration 519/1000 | Loss: 0.00004280
Iteration 520/1000 | Loss: 0.00004280
Iteration 521/1000 | Loss: 0.00004280
Iteration 522/1000 | Loss: 0.00004280
Iteration 523/1000 | Loss: 0.00004280
Iteration 524/1000 | Loss: 0.00004280
Iteration 525/1000 | Loss: 0.00004280
Iteration 526/1000 | Loss: 0.00004280
Iteration 527/1000 | Loss: 0.00004280
Iteration 528/1000 | Loss: 0.00004280
Iteration 529/1000 | Loss: 0.00004280
Iteration 530/1000 | Loss: 0.00004280
Iteration 531/1000 | Loss: 0.00004280
Iteration 532/1000 | Loss: 0.00004280
Iteration 533/1000 | Loss: 0.00004280
Iteration 534/1000 | Loss: 0.00004280
Iteration 535/1000 | Loss: 0.00004280
Iteration 536/1000 | Loss: 0.00004280
Iteration 537/1000 | Loss: 0.00004280
Iteration 538/1000 | Loss: 0.00004280
Iteration 539/1000 | Loss: 0.00004280
Iteration 540/1000 | Loss: 0.00004280
Iteration 541/1000 | Loss: 0.00004280
Iteration 542/1000 | Loss: 0.00004280
Iteration 543/1000 | Loss: 0.00004280
Iteration 544/1000 | Loss: 0.00004280
Iteration 545/1000 | Loss: 0.00004280
Iteration 546/1000 | Loss: 0.00004280
Iteration 547/1000 | Loss: 0.00004280
Iteration 548/1000 | Loss: 0.00004280
Iteration 549/1000 | Loss: 0.00004280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 549. Stopping optimization.
Last 5 losses: [4.279807399143465e-05, 4.279807399143465e-05, 4.279807399143465e-05, 4.279807399143465e-05, 4.279807399143465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.279807399143465e-05

Optimization complete. Final v2v error: 4.345155715942383 mm

Highest mean error: 11.049832344055176 mm for frame 199

Lowest mean error: 3.6912682056427 mm for frame 11

Saving results

Total time: 774.755234003067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782323
Iteration 2/25 | Loss: 0.00109616
Iteration 3/25 | Loss: 0.00087721
Iteration 4/25 | Loss: 0.00083956
Iteration 5/25 | Loss: 0.00083383
Iteration 6/25 | Loss: 0.00083350
Iteration 7/25 | Loss: 0.00083350
Iteration 8/25 | Loss: 0.00083350
Iteration 9/25 | Loss: 0.00083350
Iteration 10/25 | Loss: 0.00083350
Iteration 11/25 | Loss: 0.00083350
Iteration 12/25 | Loss: 0.00083350
Iteration 13/25 | Loss: 0.00083350
Iteration 14/25 | Loss: 0.00083350
Iteration 15/25 | Loss: 0.00083350
Iteration 16/25 | Loss: 0.00083350
Iteration 17/25 | Loss: 0.00083350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008335030870512128, 0.0008335030870512128, 0.0008335030870512128, 0.0008335030870512128, 0.0008335030870512128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008335030870512128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59193182
Iteration 2/25 | Loss: 0.00141532
Iteration 3/25 | Loss: 0.00141528
Iteration 4/25 | Loss: 0.00141528
Iteration 5/25 | Loss: 0.00141528
Iteration 6/25 | Loss: 0.00141528
Iteration 7/25 | Loss: 0.00141528
Iteration 8/25 | Loss: 0.00141527
Iteration 9/25 | Loss: 0.00141527
Iteration 10/25 | Loss: 0.00141527
Iteration 11/25 | Loss: 0.00141527
Iteration 12/25 | Loss: 0.00141527
Iteration 13/25 | Loss: 0.00141527
Iteration 14/25 | Loss: 0.00141527
Iteration 15/25 | Loss: 0.00141527
Iteration 16/25 | Loss: 0.00141527
Iteration 17/25 | Loss: 0.00141527
Iteration 18/25 | Loss: 0.00141527
Iteration 19/25 | Loss: 0.00141527
Iteration 20/25 | Loss: 0.00141527
Iteration 21/25 | Loss: 0.00141527
Iteration 22/25 | Loss: 0.00141527
Iteration 23/25 | Loss: 0.00141527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014152738731354475, 0.0014152738731354475, 0.0014152738731354475, 0.0014152738731354475, 0.0014152738731354475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014152738731354475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141527
Iteration 2/1000 | Loss: 0.00005633
Iteration 3/1000 | Loss: 0.00003944
Iteration 4/1000 | Loss: 0.00003106
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002686
Iteration 7/1000 | Loss: 0.00002579
Iteration 8/1000 | Loss: 0.00002496
Iteration 9/1000 | Loss: 0.00002463
Iteration 10/1000 | Loss: 0.00002432
Iteration 11/1000 | Loss: 0.00002405
Iteration 12/1000 | Loss: 0.00002378
Iteration 13/1000 | Loss: 0.00002375
Iteration 14/1000 | Loss: 0.00002360
Iteration 15/1000 | Loss: 0.00002353
Iteration 16/1000 | Loss: 0.00002348
Iteration 17/1000 | Loss: 0.00002348
Iteration 18/1000 | Loss: 0.00002347
Iteration 19/1000 | Loss: 0.00002345
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002344
Iteration 22/1000 | Loss: 0.00002344
Iteration 23/1000 | Loss: 0.00002342
Iteration 24/1000 | Loss: 0.00002342
Iteration 25/1000 | Loss: 0.00002342
Iteration 26/1000 | Loss: 0.00002342
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002342
Iteration 30/1000 | Loss: 0.00002342
Iteration 31/1000 | Loss: 0.00002342
Iteration 32/1000 | Loss: 0.00002341
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002341
Iteration 35/1000 | Loss: 0.00002341
Iteration 36/1000 | Loss: 0.00002341
Iteration 37/1000 | Loss: 0.00002341
Iteration 38/1000 | Loss: 0.00002341
Iteration 39/1000 | Loss: 0.00002340
Iteration 40/1000 | Loss: 0.00002340
Iteration 41/1000 | Loss: 0.00002340
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002338
Iteration 45/1000 | Loss: 0.00002338
Iteration 46/1000 | Loss: 0.00002338
Iteration 47/1000 | Loss: 0.00002338
Iteration 48/1000 | Loss: 0.00002338
Iteration 49/1000 | Loss: 0.00002338
Iteration 50/1000 | Loss: 0.00002338
Iteration 51/1000 | Loss: 0.00002337
Iteration 52/1000 | Loss: 0.00002337
Iteration 53/1000 | Loss: 0.00002337
Iteration 54/1000 | Loss: 0.00002337
Iteration 55/1000 | Loss: 0.00002337
Iteration 56/1000 | Loss: 0.00002337
Iteration 57/1000 | Loss: 0.00002337
Iteration 58/1000 | Loss: 0.00002337
Iteration 59/1000 | Loss: 0.00002336
Iteration 60/1000 | Loss: 0.00002336
Iteration 61/1000 | Loss: 0.00002336
Iteration 62/1000 | Loss: 0.00002336
Iteration 63/1000 | Loss: 0.00002336
Iteration 64/1000 | Loss: 0.00002336
Iteration 65/1000 | Loss: 0.00002336
Iteration 66/1000 | Loss: 0.00002336
Iteration 67/1000 | Loss: 0.00002335
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00002335
Iteration 70/1000 | Loss: 0.00002335
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002335
Iteration 74/1000 | Loss: 0.00002335
Iteration 75/1000 | Loss: 0.00002335
Iteration 76/1000 | Loss: 0.00002335
Iteration 77/1000 | Loss: 0.00002335
Iteration 78/1000 | Loss: 0.00002335
Iteration 79/1000 | Loss: 0.00002335
Iteration 80/1000 | Loss: 0.00002334
Iteration 81/1000 | Loss: 0.00002334
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002334
Iteration 84/1000 | Loss: 0.00002334
Iteration 85/1000 | Loss: 0.00002334
Iteration 86/1000 | Loss: 0.00002334
Iteration 87/1000 | Loss: 0.00002334
Iteration 88/1000 | Loss: 0.00002334
Iteration 89/1000 | Loss: 0.00002333
Iteration 90/1000 | Loss: 0.00002333
Iteration 91/1000 | Loss: 0.00002333
Iteration 92/1000 | Loss: 0.00002333
Iteration 93/1000 | Loss: 0.00002333
Iteration 94/1000 | Loss: 0.00002333
Iteration 95/1000 | Loss: 0.00002333
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00002333
Iteration 98/1000 | Loss: 0.00002333
Iteration 99/1000 | Loss: 0.00002333
Iteration 100/1000 | Loss: 0.00002333
Iteration 101/1000 | Loss: 0.00002333
Iteration 102/1000 | Loss: 0.00002333
Iteration 103/1000 | Loss: 0.00002332
Iteration 104/1000 | Loss: 0.00002332
Iteration 105/1000 | Loss: 0.00002332
Iteration 106/1000 | Loss: 0.00002332
Iteration 107/1000 | Loss: 0.00002332
Iteration 108/1000 | Loss: 0.00002332
Iteration 109/1000 | Loss: 0.00002331
Iteration 110/1000 | Loss: 0.00002331
Iteration 111/1000 | Loss: 0.00002331
Iteration 112/1000 | Loss: 0.00002331
Iteration 113/1000 | Loss: 0.00002331
Iteration 114/1000 | Loss: 0.00002331
Iteration 115/1000 | Loss: 0.00002331
Iteration 116/1000 | Loss: 0.00002331
Iteration 117/1000 | Loss: 0.00002331
Iteration 118/1000 | Loss: 0.00002331
Iteration 119/1000 | Loss: 0.00002331
Iteration 120/1000 | Loss: 0.00002331
Iteration 121/1000 | Loss: 0.00002331
Iteration 122/1000 | Loss: 0.00002331
Iteration 123/1000 | Loss: 0.00002331
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002330
Iteration 126/1000 | Loss: 0.00002330
Iteration 127/1000 | Loss: 0.00002330
Iteration 128/1000 | Loss: 0.00002330
Iteration 129/1000 | Loss: 0.00002330
Iteration 130/1000 | Loss: 0.00002330
Iteration 131/1000 | Loss: 0.00002330
Iteration 132/1000 | Loss: 0.00002329
Iteration 133/1000 | Loss: 0.00002329
Iteration 134/1000 | Loss: 0.00002329
Iteration 135/1000 | Loss: 0.00002329
Iteration 136/1000 | Loss: 0.00002329
Iteration 137/1000 | Loss: 0.00002329
Iteration 138/1000 | Loss: 0.00002329
Iteration 139/1000 | Loss: 0.00002329
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00002329
Iteration 142/1000 | Loss: 0.00002329
Iteration 143/1000 | Loss: 0.00002329
Iteration 144/1000 | Loss: 0.00002329
Iteration 145/1000 | Loss: 0.00002329
Iteration 146/1000 | Loss: 0.00002329
Iteration 147/1000 | Loss: 0.00002329
Iteration 148/1000 | Loss: 0.00002329
Iteration 149/1000 | Loss: 0.00002329
Iteration 150/1000 | Loss: 0.00002329
Iteration 151/1000 | Loss: 0.00002329
Iteration 152/1000 | Loss: 0.00002329
Iteration 153/1000 | Loss: 0.00002329
Iteration 154/1000 | Loss: 0.00002329
Iteration 155/1000 | Loss: 0.00002329
Iteration 156/1000 | Loss: 0.00002329
Iteration 157/1000 | Loss: 0.00002329
Iteration 158/1000 | Loss: 0.00002329
Iteration 159/1000 | Loss: 0.00002329
Iteration 160/1000 | Loss: 0.00002329
Iteration 161/1000 | Loss: 0.00002329
Iteration 162/1000 | Loss: 0.00002329
Iteration 163/1000 | Loss: 0.00002329
Iteration 164/1000 | Loss: 0.00002329
Iteration 165/1000 | Loss: 0.00002329
Iteration 166/1000 | Loss: 0.00002329
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002329
Iteration 169/1000 | Loss: 0.00002329
Iteration 170/1000 | Loss: 0.00002329
Iteration 171/1000 | Loss: 0.00002329
Iteration 172/1000 | Loss: 0.00002329
Iteration 173/1000 | Loss: 0.00002329
Iteration 174/1000 | Loss: 0.00002329
Iteration 175/1000 | Loss: 0.00002329
Iteration 176/1000 | Loss: 0.00002329
Iteration 177/1000 | Loss: 0.00002329
Iteration 178/1000 | Loss: 0.00002329
Iteration 179/1000 | Loss: 0.00002329
Iteration 180/1000 | Loss: 0.00002329
Iteration 181/1000 | Loss: 0.00002329
Iteration 182/1000 | Loss: 0.00002329
Iteration 183/1000 | Loss: 0.00002329
Iteration 184/1000 | Loss: 0.00002329
Iteration 185/1000 | Loss: 0.00002329
Iteration 186/1000 | Loss: 0.00002329
Iteration 187/1000 | Loss: 0.00002329
Iteration 188/1000 | Loss: 0.00002329
Iteration 189/1000 | Loss: 0.00002329
Iteration 190/1000 | Loss: 0.00002329
Iteration 191/1000 | Loss: 0.00002329
Iteration 192/1000 | Loss: 0.00002329
Iteration 193/1000 | Loss: 0.00002329
Iteration 194/1000 | Loss: 0.00002329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.3292910555028357e-05, 2.3292910555028357e-05, 2.3292910555028357e-05, 2.3292910555028357e-05, 2.3292910555028357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3292910555028357e-05

Optimization complete. Final v2v error: 3.9971513748168945 mm

Highest mean error: 4.303267955780029 mm for frame 155

Lowest mean error: 3.6752989292144775 mm for frame 238

Saving results

Total time: 43.23070502281189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458126
Iteration 2/25 | Loss: 0.00098345
Iteration 3/25 | Loss: 0.00083409
Iteration 4/25 | Loss: 0.00079302
Iteration 5/25 | Loss: 0.00078780
Iteration 6/25 | Loss: 0.00078649
Iteration 7/25 | Loss: 0.00078602
Iteration 8/25 | Loss: 0.00078602
Iteration 9/25 | Loss: 0.00078602
Iteration 10/25 | Loss: 0.00078602
Iteration 11/25 | Loss: 0.00078602
Iteration 12/25 | Loss: 0.00078602
Iteration 13/25 | Loss: 0.00078602
Iteration 14/25 | Loss: 0.00078602
Iteration 15/25 | Loss: 0.00078602
Iteration 16/25 | Loss: 0.00078602
Iteration 17/25 | Loss: 0.00078602
Iteration 18/25 | Loss: 0.00078602
Iteration 19/25 | Loss: 0.00078602
Iteration 20/25 | Loss: 0.00078602
Iteration 21/25 | Loss: 0.00078602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007860197219997644, 0.0007860197219997644, 0.0007860197219997644, 0.0007860197219997644, 0.0007860197219997644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007860197219997644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46472633
Iteration 2/25 | Loss: 0.00144497
Iteration 3/25 | Loss: 0.00144493
Iteration 4/25 | Loss: 0.00144493
Iteration 5/25 | Loss: 0.00144493
Iteration 6/25 | Loss: 0.00144493
Iteration 7/25 | Loss: 0.00144493
Iteration 8/25 | Loss: 0.00144493
Iteration 9/25 | Loss: 0.00144493
Iteration 10/25 | Loss: 0.00144493
Iteration 11/25 | Loss: 0.00144493
Iteration 12/25 | Loss: 0.00144493
Iteration 13/25 | Loss: 0.00144493
Iteration 14/25 | Loss: 0.00144493
Iteration 15/25 | Loss: 0.00144493
Iteration 16/25 | Loss: 0.00144493
Iteration 17/25 | Loss: 0.00144493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014449268346652389, 0.0014449268346652389, 0.0014449268346652389, 0.0014449268346652389, 0.0014449268346652389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014449268346652389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144493
Iteration 2/1000 | Loss: 0.00003955
Iteration 3/1000 | Loss: 0.00003001
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00001974
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001705
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001689
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001686
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001683
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001681
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001681
Iteration 39/1000 | Loss: 0.00001681
Iteration 40/1000 | Loss: 0.00001681
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001677
Iteration 48/1000 | Loss: 0.00001677
Iteration 49/1000 | Loss: 0.00001677
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001676
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001672
Iteration 64/1000 | Loss: 0.00001672
Iteration 65/1000 | Loss: 0.00001672
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001671
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001671
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001671
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001669
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001668
Iteration 90/1000 | Loss: 0.00001668
Iteration 91/1000 | Loss: 0.00001668
Iteration 92/1000 | Loss: 0.00001668
Iteration 93/1000 | Loss: 0.00001668
Iteration 94/1000 | Loss: 0.00001668
Iteration 95/1000 | Loss: 0.00001668
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001668
Iteration 101/1000 | Loss: 0.00001668
Iteration 102/1000 | Loss: 0.00001668
Iteration 103/1000 | Loss: 0.00001668
Iteration 104/1000 | Loss: 0.00001668
Iteration 105/1000 | Loss: 0.00001668
Iteration 106/1000 | Loss: 0.00001668
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6677951862220652e-05, 1.6677951862220652e-05, 1.6677951862220652e-05, 1.6677951862220652e-05, 1.6677951862220652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6677951862220652e-05

Optimization complete. Final v2v error: 3.4546897411346436 mm

Highest mean error: 3.7201571464538574 mm for frame 87

Lowest mean error: 3.278292655944824 mm for frame 83

Saving results

Total time: 37.36336874961853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864977
Iteration 2/25 | Loss: 0.00129514
Iteration 3/25 | Loss: 0.00094723
Iteration 4/25 | Loss: 0.00089302
Iteration 5/25 | Loss: 0.00088732
Iteration 6/25 | Loss: 0.00088664
Iteration 7/25 | Loss: 0.00088664
Iteration 8/25 | Loss: 0.00088664
Iteration 9/25 | Loss: 0.00088664
Iteration 10/25 | Loss: 0.00088664
Iteration 11/25 | Loss: 0.00088664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000886644353158772, 0.000886644353158772, 0.000886644353158772, 0.000886644353158772, 0.000886644353158772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000886644353158772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15375829
Iteration 2/25 | Loss: 0.00105524
Iteration 3/25 | Loss: 0.00105524
Iteration 4/25 | Loss: 0.00105523
Iteration 5/25 | Loss: 0.00105523
Iteration 6/25 | Loss: 0.00105523
Iteration 7/25 | Loss: 0.00105523
Iteration 8/25 | Loss: 0.00105523
Iteration 9/25 | Loss: 0.00105523
Iteration 10/25 | Loss: 0.00105523
Iteration 11/25 | Loss: 0.00105523
Iteration 12/25 | Loss: 0.00105523
Iteration 13/25 | Loss: 0.00105523
Iteration 14/25 | Loss: 0.00105523
Iteration 15/25 | Loss: 0.00105523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001055233646184206, 0.001055233646184206, 0.001055233646184206, 0.001055233646184206, 0.001055233646184206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001055233646184206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105523
Iteration 2/1000 | Loss: 0.00003829
Iteration 3/1000 | Loss: 0.00003105
Iteration 4/1000 | Loss: 0.00002887
Iteration 5/1000 | Loss: 0.00002744
Iteration 6/1000 | Loss: 0.00002665
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00002590
Iteration 9/1000 | Loss: 0.00002584
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002546
Iteration 12/1000 | Loss: 0.00002532
Iteration 13/1000 | Loss: 0.00002516
Iteration 14/1000 | Loss: 0.00002505
Iteration 15/1000 | Loss: 0.00002502
Iteration 16/1000 | Loss: 0.00002501
Iteration 17/1000 | Loss: 0.00002501
Iteration 18/1000 | Loss: 0.00002501
Iteration 19/1000 | Loss: 0.00002501
Iteration 20/1000 | Loss: 0.00002501
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00002501
Iteration 23/1000 | Loss: 0.00002501
Iteration 24/1000 | Loss: 0.00002500
Iteration 25/1000 | Loss: 0.00002495
Iteration 26/1000 | Loss: 0.00002494
Iteration 27/1000 | Loss: 0.00002494
Iteration 28/1000 | Loss: 0.00002494
Iteration 29/1000 | Loss: 0.00002494
Iteration 30/1000 | Loss: 0.00002493
Iteration 31/1000 | Loss: 0.00002493
Iteration 32/1000 | Loss: 0.00002493
Iteration 33/1000 | Loss: 0.00002492
Iteration 34/1000 | Loss: 0.00002492
Iteration 35/1000 | Loss: 0.00002492
Iteration 36/1000 | Loss: 0.00002491
Iteration 37/1000 | Loss: 0.00002491
Iteration 38/1000 | Loss: 0.00002490
Iteration 39/1000 | Loss: 0.00002490
Iteration 40/1000 | Loss: 0.00002489
Iteration 41/1000 | Loss: 0.00002489
Iteration 42/1000 | Loss: 0.00002489
Iteration 43/1000 | Loss: 0.00002488
Iteration 44/1000 | Loss: 0.00002487
Iteration 45/1000 | Loss: 0.00002487
Iteration 46/1000 | Loss: 0.00002487
Iteration 47/1000 | Loss: 0.00002487
Iteration 48/1000 | Loss: 0.00002487
Iteration 49/1000 | Loss: 0.00002486
Iteration 50/1000 | Loss: 0.00002486
Iteration 51/1000 | Loss: 0.00002486
Iteration 52/1000 | Loss: 0.00002486
Iteration 53/1000 | Loss: 0.00002486
Iteration 54/1000 | Loss: 0.00002486
Iteration 55/1000 | Loss: 0.00002486
Iteration 56/1000 | Loss: 0.00002486
Iteration 57/1000 | Loss: 0.00002486
Iteration 58/1000 | Loss: 0.00002485
Iteration 59/1000 | Loss: 0.00002485
Iteration 60/1000 | Loss: 0.00002485
Iteration 61/1000 | Loss: 0.00002484
Iteration 62/1000 | Loss: 0.00002484
Iteration 63/1000 | Loss: 0.00002483
Iteration 64/1000 | Loss: 0.00002483
Iteration 65/1000 | Loss: 0.00002483
Iteration 66/1000 | Loss: 0.00002483
Iteration 67/1000 | Loss: 0.00002483
Iteration 68/1000 | Loss: 0.00002483
Iteration 69/1000 | Loss: 0.00002483
Iteration 70/1000 | Loss: 0.00002483
Iteration 71/1000 | Loss: 0.00002483
Iteration 72/1000 | Loss: 0.00002482
Iteration 73/1000 | Loss: 0.00002482
Iteration 74/1000 | Loss: 0.00002481
Iteration 75/1000 | Loss: 0.00002480
Iteration 76/1000 | Loss: 0.00002479
Iteration 77/1000 | Loss: 0.00002479
Iteration 78/1000 | Loss: 0.00002479
Iteration 79/1000 | Loss: 0.00002478
Iteration 80/1000 | Loss: 0.00002478
Iteration 81/1000 | Loss: 0.00002478
Iteration 82/1000 | Loss: 0.00002477
Iteration 83/1000 | Loss: 0.00002477
Iteration 84/1000 | Loss: 0.00002477
Iteration 85/1000 | Loss: 0.00002477
Iteration 86/1000 | Loss: 0.00002476
Iteration 87/1000 | Loss: 0.00002476
Iteration 88/1000 | Loss: 0.00002476
Iteration 89/1000 | Loss: 0.00002476
Iteration 90/1000 | Loss: 0.00002476
Iteration 91/1000 | Loss: 0.00002476
Iteration 92/1000 | Loss: 0.00002476
Iteration 93/1000 | Loss: 0.00002476
Iteration 94/1000 | Loss: 0.00002476
Iteration 95/1000 | Loss: 0.00002475
Iteration 96/1000 | Loss: 0.00002475
Iteration 97/1000 | Loss: 0.00002475
Iteration 98/1000 | Loss: 0.00002475
Iteration 99/1000 | Loss: 0.00002475
Iteration 100/1000 | Loss: 0.00002475
Iteration 101/1000 | Loss: 0.00002475
Iteration 102/1000 | Loss: 0.00002474
Iteration 103/1000 | Loss: 0.00002474
Iteration 104/1000 | Loss: 0.00002474
Iteration 105/1000 | Loss: 0.00002473
Iteration 106/1000 | Loss: 0.00002473
Iteration 107/1000 | Loss: 0.00002473
Iteration 108/1000 | Loss: 0.00002473
Iteration 109/1000 | Loss: 0.00002473
Iteration 110/1000 | Loss: 0.00002473
Iteration 111/1000 | Loss: 0.00002473
Iteration 112/1000 | Loss: 0.00002473
Iteration 113/1000 | Loss: 0.00002473
Iteration 114/1000 | Loss: 0.00002473
Iteration 115/1000 | Loss: 0.00002473
Iteration 116/1000 | Loss: 0.00002473
Iteration 117/1000 | Loss: 0.00002472
Iteration 118/1000 | Loss: 0.00002472
Iteration 119/1000 | Loss: 0.00002472
Iteration 120/1000 | Loss: 0.00002472
Iteration 121/1000 | Loss: 0.00002472
Iteration 122/1000 | Loss: 0.00002472
Iteration 123/1000 | Loss: 0.00002472
Iteration 124/1000 | Loss: 0.00002472
Iteration 125/1000 | Loss: 0.00002472
Iteration 126/1000 | Loss: 0.00002472
Iteration 127/1000 | Loss: 0.00002472
Iteration 128/1000 | Loss: 0.00002472
Iteration 129/1000 | Loss: 0.00002472
Iteration 130/1000 | Loss: 0.00002472
Iteration 131/1000 | Loss: 0.00002472
Iteration 132/1000 | Loss: 0.00002471
Iteration 133/1000 | Loss: 0.00002471
Iteration 134/1000 | Loss: 0.00002471
Iteration 135/1000 | Loss: 0.00002471
Iteration 136/1000 | Loss: 0.00002471
Iteration 137/1000 | Loss: 0.00002471
Iteration 138/1000 | Loss: 0.00002471
Iteration 139/1000 | Loss: 0.00002471
Iteration 140/1000 | Loss: 0.00002471
Iteration 141/1000 | Loss: 0.00002471
Iteration 142/1000 | Loss: 0.00002471
Iteration 143/1000 | Loss: 0.00002471
Iteration 144/1000 | Loss: 0.00002471
Iteration 145/1000 | Loss: 0.00002471
Iteration 146/1000 | Loss: 0.00002471
Iteration 147/1000 | Loss: 0.00002471
Iteration 148/1000 | Loss: 0.00002471
Iteration 149/1000 | Loss: 0.00002471
Iteration 150/1000 | Loss: 0.00002471
Iteration 151/1000 | Loss: 0.00002471
Iteration 152/1000 | Loss: 0.00002471
Iteration 153/1000 | Loss: 0.00002471
Iteration 154/1000 | Loss: 0.00002471
Iteration 155/1000 | Loss: 0.00002471
Iteration 156/1000 | Loss: 0.00002471
Iteration 157/1000 | Loss: 0.00002471
Iteration 158/1000 | Loss: 0.00002471
Iteration 159/1000 | Loss: 0.00002471
Iteration 160/1000 | Loss: 0.00002471
Iteration 161/1000 | Loss: 0.00002471
Iteration 162/1000 | Loss: 0.00002471
Iteration 163/1000 | Loss: 0.00002471
Iteration 164/1000 | Loss: 0.00002471
Iteration 165/1000 | Loss: 0.00002471
Iteration 166/1000 | Loss: 0.00002471
Iteration 167/1000 | Loss: 0.00002471
Iteration 168/1000 | Loss: 0.00002471
Iteration 169/1000 | Loss: 0.00002471
Iteration 170/1000 | Loss: 0.00002471
Iteration 171/1000 | Loss: 0.00002471
Iteration 172/1000 | Loss: 0.00002471
Iteration 173/1000 | Loss: 0.00002471
Iteration 174/1000 | Loss: 0.00002471
Iteration 175/1000 | Loss: 0.00002471
Iteration 176/1000 | Loss: 0.00002471
Iteration 177/1000 | Loss: 0.00002471
Iteration 178/1000 | Loss: 0.00002471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.4710932848392986e-05, 2.4710932848392986e-05, 2.4710932848392986e-05, 2.4710932848392986e-05, 2.4710932848392986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4710932848392986e-05

Optimization complete. Final v2v error: 4.118412017822266 mm

Highest mean error: 4.168575763702393 mm for frame 115

Lowest mean error: 4.0692138671875 mm for frame 5

Saving results

Total time: 35.24079203605652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804865
Iteration 2/25 | Loss: 0.00135417
Iteration 3/25 | Loss: 0.00107367
Iteration 4/25 | Loss: 0.00094660
Iteration 5/25 | Loss: 0.00090697
Iteration 6/25 | Loss: 0.00089610
Iteration 7/25 | Loss: 0.00088817
Iteration 8/25 | Loss: 0.00088574
Iteration 9/25 | Loss: 0.00088366
Iteration 10/25 | Loss: 0.00088225
Iteration 11/25 | Loss: 0.00088357
Iteration 12/25 | Loss: 0.00088205
Iteration 13/25 | Loss: 0.00088204
Iteration 14/25 | Loss: 0.00088204
Iteration 15/25 | Loss: 0.00088204
Iteration 16/25 | Loss: 0.00088203
Iteration 17/25 | Loss: 0.00088203
Iteration 18/25 | Loss: 0.00088261
Iteration 19/25 | Loss: 0.00088205
Iteration 20/25 | Loss: 0.00088205
Iteration 21/25 | Loss: 0.00088204
Iteration 22/25 | Loss: 0.00088204
Iteration 23/25 | Loss: 0.00088204
Iteration 24/25 | Loss: 0.00088204
Iteration 25/25 | Loss: 0.00088204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.30845451
Iteration 2/25 | Loss: 0.00127427
Iteration 3/25 | Loss: 0.00127426
Iteration 4/25 | Loss: 0.00127426
Iteration 5/25 | Loss: 0.00127426
Iteration 6/25 | Loss: 0.00127425
Iteration 7/25 | Loss: 0.00127425
Iteration 8/25 | Loss: 0.00127425
Iteration 9/25 | Loss: 0.00127425
Iteration 10/25 | Loss: 0.00127425
Iteration 11/25 | Loss: 0.00127425
Iteration 12/25 | Loss: 0.00127425
Iteration 13/25 | Loss: 0.00127425
Iteration 14/25 | Loss: 0.00127425
Iteration 15/25 | Loss: 0.00127425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012742537073791027, 0.0012742537073791027, 0.0012742537073791027, 0.0012742537073791027, 0.0012742537073791027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012742537073791027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127425
Iteration 2/1000 | Loss: 0.00004290
Iteration 3/1000 | Loss: 0.00003275
Iteration 4/1000 | Loss: 0.00003091
Iteration 5/1000 | Loss: 0.00002991
Iteration 6/1000 | Loss: 0.00002916
Iteration 7/1000 | Loss: 0.00002866
Iteration 8/1000 | Loss: 0.00002821
Iteration 9/1000 | Loss: 0.00002788
Iteration 10/1000 | Loss: 0.00002765
Iteration 11/1000 | Loss: 0.00002754
Iteration 12/1000 | Loss: 0.00002736
Iteration 13/1000 | Loss: 0.00002724
Iteration 14/1000 | Loss: 0.00002721
Iteration 15/1000 | Loss: 0.00002720
Iteration 16/1000 | Loss: 0.00002717
Iteration 17/1000 | Loss: 0.00002715
Iteration 18/1000 | Loss: 0.00002714
Iteration 19/1000 | Loss: 0.00002712
Iteration 20/1000 | Loss: 0.00002710
Iteration 21/1000 | Loss: 0.00002707
Iteration 22/1000 | Loss: 0.00002706
Iteration 23/1000 | Loss: 0.00002705
Iteration 24/1000 | Loss: 0.00002704
Iteration 25/1000 | Loss: 0.00002703
Iteration 26/1000 | Loss: 0.00002703
Iteration 27/1000 | Loss: 0.00002700
Iteration 28/1000 | Loss: 0.00002699
Iteration 29/1000 | Loss: 0.00002699
Iteration 30/1000 | Loss: 0.00002699
Iteration 31/1000 | Loss: 0.00002698
Iteration 32/1000 | Loss: 0.00002698
Iteration 33/1000 | Loss: 0.00002696
Iteration 34/1000 | Loss: 0.00002696
Iteration 35/1000 | Loss: 0.00002696
Iteration 36/1000 | Loss: 0.00002696
Iteration 37/1000 | Loss: 0.00002696
Iteration 38/1000 | Loss: 0.00002696
Iteration 39/1000 | Loss: 0.00002696
Iteration 40/1000 | Loss: 0.00002696
Iteration 41/1000 | Loss: 0.00002696
Iteration 42/1000 | Loss: 0.00002694
Iteration 43/1000 | Loss: 0.00002694
Iteration 44/1000 | Loss: 0.00002694
Iteration 45/1000 | Loss: 0.00002693
Iteration 46/1000 | Loss: 0.00002693
Iteration 47/1000 | Loss: 0.00002693
Iteration 48/1000 | Loss: 0.00002693
Iteration 49/1000 | Loss: 0.00002693
Iteration 50/1000 | Loss: 0.00002693
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00002692
Iteration 53/1000 | Loss: 0.00002692
Iteration 54/1000 | Loss: 0.00002692
Iteration 55/1000 | Loss: 0.00002691
Iteration 56/1000 | Loss: 0.00002691
Iteration 57/1000 | Loss: 0.00002691
Iteration 58/1000 | Loss: 0.00002691
Iteration 59/1000 | Loss: 0.00002691
Iteration 60/1000 | Loss: 0.00002691
Iteration 61/1000 | Loss: 0.00002691
Iteration 62/1000 | Loss: 0.00002691
Iteration 63/1000 | Loss: 0.00002691
Iteration 64/1000 | Loss: 0.00002691
Iteration 65/1000 | Loss: 0.00002690
Iteration 66/1000 | Loss: 0.00002690
Iteration 67/1000 | Loss: 0.00002690
Iteration 68/1000 | Loss: 0.00002690
Iteration 69/1000 | Loss: 0.00002690
Iteration 70/1000 | Loss: 0.00002689
Iteration 71/1000 | Loss: 0.00002689
Iteration 72/1000 | Loss: 0.00002689
Iteration 73/1000 | Loss: 0.00002689
Iteration 74/1000 | Loss: 0.00002688
Iteration 75/1000 | Loss: 0.00002688
Iteration 76/1000 | Loss: 0.00002688
Iteration 77/1000 | Loss: 0.00002688
Iteration 78/1000 | Loss: 0.00002688
Iteration 79/1000 | Loss: 0.00002687
Iteration 80/1000 | Loss: 0.00002687
Iteration 81/1000 | Loss: 0.00002687
Iteration 82/1000 | Loss: 0.00002686
Iteration 83/1000 | Loss: 0.00002686
Iteration 84/1000 | Loss: 0.00002686
Iteration 85/1000 | Loss: 0.00002686
Iteration 86/1000 | Loss: 0.00002686
Iteration 87/1000 | Loss: 0.00002686
Iteration 88/1000 | Loss: 0.00002686
Iteration 89/1000 | Loss: 0.00002686
Iteration 90/1000 | Loss: 0.00002686
Iteration 91/1000 | Loss: 0.00002685
Iteration 92/1000 | Loss: 0.00002685
Iteration 93/1000 | Loss: 0.00002685
Iteration 94/1000 | Loss: 0.00002685
Iteration 95/1000 | Loss: 0.00002685
Iteration 96/1000 | Loss: 0.00002685
Iteration 97/1000 | Loss: 0.00002685
Iteration 98/1000 | Loss: 0.00002685
Iteration 99/1000 | Loss: 0.00002685
Iteration 100/1000 | Loss: 0.00002685
Iteration 101/1000 | Loss: 0.00002685
Iteration 102/1000 | Loss: 0.00002684
Iteration 103/1000 | Loss: 0.00002684
Iteration 104/1000 | Loss: 0.00002684
Iteration 105/1000 | Loss: 0.00002684
Iteration 106/1000 | Loss: 0.00002684
Iteration 107/1000 | Loss: 0.00002684
Iteration 108/1000 | Loss: 0.00002684
Iteration 109/1000 | Loss: 0.00002684
Iteration 110/1000 | Loss: 0.00002684
Iteration 111/1000 | Loss: 0.00002683
Iteration 112/1000 | Loss: 0.00002683
Iteration 113/1000 | Loss: 0.00002683
Iteration 114/1000 | Loss: 0.00002683
Iteration 115/1000 | Loss: 0.00002683
Iteration 116/1000 | Loss: 0.00002683
Iteration 117/1000 | Loss: 0.00002683
Iteration 118/1000 | Loss: 0.00002683
Iteration 119/1000 | Loss: 0.00002683
Iteration 120/1000 | Loss: 0.00002683
Iteration 121/1000 | Loss: 0.00002683
Iteration 122/1000 | Loss: 0.00002683
Iteration 123/1000 | Loss: 0.00002683
Iteration 124/1000 | Loss: 0.00002683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.6826410248759203e-05, 2.6826410248759203e-05, 2.6826410248759203e-05, 2.6826410248759203e-05, 2.6826410248759203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6826410248759203e-05

Optimization complete. Final v2v error: 4.358791828155518 mm

Highest mean error: 5.045691013336182 mm for frame 118

Lowest mean error: 3.409620523452759 mm for frame 193

Saving results

Total time: 56.306777477264404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398460
Iteration 2/25 | Loss: 0.00087535
Iteration 3/25 | Loss: 0.00076344
Iteration 4/25 | Loss: 0.00074106
Iteration 5/25 | Loss: 0.00073699
Iteration 6/25 | Loss: 0.00073644
Iteration 7/25 | Loss: 0.00073644
Iteration 8/25 | Loss: 0.00073644
Iteration 9/25 | Loss: 0.00073644
Iteration 10/25 | Loss: 0.00073644
Iteration 11/25 | Loss: 0.00073644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007364429184235632, 0.0007364429184235632, 0.0007364429184235632, 0.0007364429184235632, 0.0007364429184235632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007364429184235632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59034777
Iteration 2/25 | Loss: 0.00116768
Iteration 3/25 | Loss: 0.00116768
Iteration 4/25 | Loss: 0.00116768
Iteration 5/25 | Loss: 0.00116768
Iteration 6/25 | Loss: 0.00116768
Iteration 7/25 | Loss: 0.00116768
Iteration 8/25 | Loss: 0.00116768
Iteration 9/25 | Loss: 0.00116768
Iteration 10/25 | Loss: 0.00116768
Iteration 11/25 | Loss: 0.00116768
Iteration 12/25 | Loss: 0.00116768
Iteration 13/25 | Loss: 0.00116768
Iteration 14/25 | Loss: 0.00116768
Iteration 15/25 | Loss: 0.00116768
Iteration 16/25 | Loss: 0.00116768
Iteration 17/25 | Loss: 0.00116768
Iteration 18/25 | Loss: 0.00116768
Iteration 19/25 | Loss: 0.00116768
Iteration 20/25 | Loss: 0.00116768
Iteration 21/25 | Loss: 0.00116768
Iteration 22/25 | Loss: 0.00116768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011676775757223368, 0.0011676775757223368, 0.0011676775757223368, 0.0011676775757223368, 0.0011676775757223368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011676775757223368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116768
Iteration 2/1000 | Loss: 0.00002107
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001492
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001307
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001299
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001293
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001288
Iteration 31/1000 | Loss: 0.00001288
Iteration 32/1000 | Loss: 0.00001287
Iteration 33/1000 | Loss: 0.00001287
Iteration 34/1000 | Loss: 0.00001287
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001286
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001285
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001282
Iteration 53/1000 | Loss: 0.00001282
Iteration 54/1000 | Loss: 0.00001282
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001279
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001274
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001272
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001272
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001268
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001263
Iteration 127/1000 | Loss: 0.00001263
Iteration 128/1000 | Loss: 0.00001263
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001262
Iteration 134/1000 | Loss: 0.00001262
Iteration 135/1000 | Loss: 0.00001262
Iteration 136/1000 | Loss: 0.00001262
Iteration 137/1000 | Loss: 0.00001262
Iteration 138/1000 | Loss: 0.00001262
Iteration 139/1000 | Loss: 0.00001262
Iteration 140/1000 | Loss: 0.00001262
Iteration 141/1000 | Loss: 0.00001262
Iteration 142/1000 | Loss: 0.00001262
Iteration 143/1000 | Loss: 0.00001262
Iteration 144/1000 | Loss: 0.00001262
Iteration 145/1000 | Loss: 0.00001262
Iteration 146/1000 | Loss: 0.00001262
Iteration 147/1000 | Loss: 0.00001261
Iteration 148/1000 | Loss: 0.00001261
Iteration 149/1000 | Loss: 0.00001261
Iteration 150/1000 | Loss: 0.00001261
Iteration 151/1000 | Loss: 0.00001261
Iteration 152/1000 | Loss: 0.00001261
Iteration 153/1000 | Loss: 0.00001261
Iteration 154/1000 | Loss: 0.00001261
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001261
Iteration 157/1000 | Loss: 0.00001261
Iteration 158/1000 | Loss: 0.00001261
Iteration 159/1000 | Loss: 0.00001261
Iteration 160/1000 | Loss: 0.00001261
Iteration 161/1000 | Loss: 0.00001261
Iteration 162/1000 | Loss: 0.00001261
Iteration 163/1000 | Loss: 0.00001261
Iteration 164/1000 | Loss: 0.00001261
Iteration 165/1000 | Loss: 0.00001261
Iteration 166/1000 | Loss: 0.00001261
Iteration 167/1000 | Loss: 0.00001261
Iteration 168/1000 | Loss: 0.00001260
Iteration 169/1000 | Loss: 0.00001260
Iteration 170/1000 | Loss: 0.00001260
Iteration 171/1000 | Loss: 0.00001260
Iteration 172/1000 | Loss: 0.00001260
Iteration 173/1000 | Loss: 0.00001260
Iteration 174/1000 | Loss: 0.00001260
Iteration 175/1000 | Loss: 0.00001260
Iteration 176/1000 | Loss: 0.00001260
Iteration 177/1000 | Loss: 0.00001260
Iteration 178/1000 | Loss: 0.00001260
Iteration 179/1000 | Loss: 0.00001260
Iteration 180/1000 | Loss: 0.00001260
Iteration 181/1000 | Loss: 0.00001260
Iteration 182/1000 | Loss: 0.00001260
Iteration 183/1000 | Loss: 0.00001260
Iteration 184/1000 | Loss: 0.00001260
Iteration 185/1000 | Loss: 0.00001260
Iteration 186/1000 | Loss: 0.00001260
Iteration 187/1000 | Loss: 0.00001260
Iteration 188/1000 | Loss: 0.00001260
Iteration 189/1000 | Loss: 0.00001260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2601873095263727e-05, 1.2601873095263727e-05, 1.2601873095263727e-05, 1.2601873095263727e-05, 1.2601873095263727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2601873095263727e-05

Optimization complete. Final v2v error: 3.0176641941070557 mm

Highest mean error: 3.1816720962524414 mm for frame 49

Lowest mean error: 2.8708808422088623 mm for frame 169

Saving results

Total time: 37.04718542098999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486684
Iteration 2/25 | Loss: 0.00086245
Iteration 3/25 | Loss: 0.00075953
Iteration 4/25 | Loss: 0.00074736
Iteration 5/25 | Loss: 0.00074399
Iteration 6/25 | Loss: 0.00074301
Iteration 7/25 | Loss: 0.00074283
Iteration 8/25 | Loss: 0.00074283
Iteration 9/25 | Loss: 0.00074283
Iteration 10/25 | Loss: 0.00074283
Iteration 11/25 | Loss: 0.00074283
Iteration 12/25 | Loss: 0.00074283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000742826669011265, 0.000742826669011265, 0.000742826669011265, 0.000742826669011265, 0.000742826669011265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000742826669011265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.30429649
Iteration 2/25 | Loss: 0.00117472
Iteration 3/25 | Loss: 0.00117471
Iteration 4/25 | Loss: 0.00117471
Iteration 5/25 | Loss: 0.00117471
Iteration 6/25 | Loss: 0.00117471
Iteration 7/25 | Loss: 0.00117471
Iteration 8/25 | Loss: 0.00117471
Iteration 9/25 | Loss: 0.00117471
Iteration 10/25 | Loss: 0.00117471
Iteration 11/25 | Loss: 0.00117471
Iteration 12/25 | Loss: 0.00117471
Iteration 13/25 | Loss: 0.00117471
Iteration 14/25 | Loss: 0.00117471
Iteration 15/25 | Loss: 0.00117471
Iteration 16/25 | Loss: 0.00117471
Iteration 17/25 | Loss: 0.00117471
Iteration 18/25 | Loss: 0.00117471
Iteration 19/25 | Loss: 0.00117471
Iteration 20/25 | Loss: 0.00117471
Iteration 21/25 | Loss: 0.00117471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011747080134227872, 0.0011747080134227872, 0.0011747080134227872, 0.0011747080134227872, 0.0011747080134227872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011747080134227872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117471
Iteration 2/1000 | Loss: 0.00002707
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001574
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001416
Iteration 7/1000 | Loss: 0.00001381
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001326
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001312
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001290
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001283
Iteration 87/1000 | Loss: 0.00001283
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001282
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001280
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001278
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001278
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001277
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001277
Iteration 152/1000 | Loss: 0.00001277
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Iteration 163/1000 | Loss: 0.00001277
Iteration 164/1000 | Loss: 0.00001277
Iteration 165/1000 | Loss: 0.00001277
Iteration 166/1000 | Loss: 0.00001277
Iteration 167/1000 | Loss: 0.00001277
Iteration 168/1000 | Loss: 0.00001277
Iteration 169/1000 | Loss: 0.00001277
Iteration 170/1000 | Loss: 0.00001277
Iteration 171/1000 | Loss: 0.00001277
Iteration 172/1000 | Loss: 0.00001277
Iteration 173/1000 | Loss: 0.00001277
Iteration 174/1000 | Loss: 0.00001277
Iteration 175/1000 | Loss: 0.00001277
Iteration 176/1000 | Loss: 0.00001277
Iteration 177/1000 | Loss: 0.00001277
Iteration 178/1000 | Loss: 0.00001277
Iteration 179/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.2766319741785992e-05, 1.2766319741785992e-05, 1.2766319741785992e-05, 1.2766319741785992e-05, 1.2766319741785992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2766319741785992e-05

Optimization complete. Final v2v error: 3.063312292098999 mm

Highest mean error: 3.488670587539673 mm for frame 67

Lowest mean error: 2.8382718563079834 mm for frame 3

Saving results

Total time: 36.52349901199341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870226
Iteration 2/25 | Loss: 0.00122041
Iteration 3/25 | Loss: 0.00101014
Iteration 4/25 | Loss: 0.00091783
Iteration 5/25 | Loss: 0.00090857
Iteration 6/25 | Loss: 0.00090765
Iteration 7/25 | Loss: 0.00090765
Iteration 8/25 | Loss: 0.00090765
Iteration 9/25 | Loss: 0.00090765
Iteration 10/25 | Loss: 0.00090765
Iteration 11/25 | Loss: 0.00090765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000907649751752615, 0.000907649751752615, 0.000907649751752615, 0.000907649751752615, 0.000907649751752615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000907649751752615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56888711
Iteration 2/25 | Loss: 0.00133821
Iteration 3/25 | Loss: 0.00133820
Iteration 4/25 | Loss: 0.00133820
Iteration 5/25 | Loss: 0.00133820
Iteration 6/25 | Loss: 0.00133820
Iteration 7/25 | Loss: 0.00133820
Iteration 8/25 | Loss: 0.00133820
Iteration 9/25 | Loss: 0.00133820
Iteration 10/25 | Loss: 0.00133820
Iteration 11/25 | Loss: 0.00133820
Iteration 12/25 | Loss: 0.00133820
Iteration 13/25 | Loss: 0.00133820
Iteration 14/25 | Loss: 0.00133820
Iteration 15/25 | Loss: 0.00133820
Iteration 16/25 | Loss: 0.00133820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013382023898884654, 0.0013382023898884654, 0.0013382023898884654, 0.0013382023898884654, 0.0013382023898884654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013382023898884654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133820
Iteration 2/1000 | Loss: 0.00005045
Iteration 3/1000 | Loss: 0.00003103
Iteration 4/1000 | Loss: 0.00002785
Iteration 5/1000 | Loss: 0.00002645
Iteration 6/1000 | Loss: 0.00002547
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00002432
Iteration 9/1000 | Loss: 0.00002408
Iteration 10/1000 | Loss: 0.00002392
Iteration 11/1000 | Loss: 0.00002375
Iteration 12/1000 | Loss: 0.00002367
Iteration 13/1000 | Loss: 0.00002367
Iteration 14/1000 | Loss: 0.00002365
Iteration 15/1000 | Loss: 0.00002357
Iteration 16/1000 | Loss: 0.00002355
Iteration 17/1000 | Loss: 0.00002355
Iteration 18/1000 | Loss: 0.00002354
Iteration 19/1000 | Loss: 0.00002354
Iteration 20/1000 | Loss: 0.00002354
Iteration 21/1000 | Loss: 0.00002353
Iteration 22/1000 | Loss: 0.00002353
Iteration 23/1000 | Loss: 0.00002353
Iteration 24/1000 | Loss: 0.00002352
Iteration 25/1000 | Loss: 0.00002352
Iteration 26/1000 | Loss: 0.00002348
Iteration 27/1000 | Loss: 0.00002348
Iteration 28/1000 | Loss: 0.00002347
Iteration 29/1000 | Loss: 0.00002346
Iteration 30/1000 | Loss: 0.00002346
Iteration 31/1000 | Loss: 0.00002345
Iteration 32/1000 | Loss: 0.00002345
Iteration 33/1000 | Loss: 0.00002345
Iteration 34/1000 | Loss: 0.00002345
Iteration 35/1000 | Loss: 0.00002344
Iteration 36/1000 | Loss: 0.00002344
Iteration 37/1000 | Loss: 0.00002344
Iteration 38/1000 | Loss: 0.00002343
Iteration 39/1000 | Loss: 0.00002343
Iteration 40/1000 | Loss: 0.00002343
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002342
Iteration 44/1000 | Loss: 0.00002341
Iteration 45/1000 | Loss: 0.00002341
Iteration 46/1000 | Loss: 0.00002341
Iteration 47/1000 | Loss: 0.00002340
Iteration 48/1000 | Loss: 0.00002340
Iteration 49/1000 | Loss: 0.00002339
Iteration 50/1000 | Loss: 0.00002339
Iteration 51/1000 | Loss: 0.00002339
Iteration 52/1000 | Loss: 0.00002338
Iteration 53/1000 | Loss: 0.00002338
Iteration 54/1000 | Loss: 0.00002338
Iteration 55/1000 | Loss: 0.00002338
Iteration 56/1000 | Loss: 0.00002338
Iteration 57/1000 | Loss: 0.00002338
Iteration 58/1000 | Loss: 0.00002337
Iteration 59/1000 | Loss: 0.00002337
Iteration 60/1000 | Loss: 0.00002336
Iteration 61/1000 | Loss: 0.00002336
Iteration 62/1000 | Loss: 0.00002336
Iteration 63/1000 | Loss: 0.00002336
Iteration 64/1000 | Loss: 0.00002336
Iteration 65/1000 | Loss: 0.00002336
Iteration 66/1000 | Loss: 0.00002335
Iteration 67/1000 | Loss: 0.00002335
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00002335
Iteration 70/1000 | Loss: 0.00002334
Iteration 71/1000 | Loss: 0.00002334
Iteration 72/1000 | Loss: 0.00002334
Iteration 73/1000 | Loss: 0.00002334
Iteration 74/1000 | Loss: 0.00002334
Iteration 75/1000 | Loss: 0.00002334
Iteration 76/1000 | Loss: 0.00002334
Iteration 77/1000 | Loss: 0.00002333
Iteration 78/1000 | Loss: 0.00002333
Iteration 79/1000 | Loss: 0.00002333
Iteration 80/1000 | Loss: 0.00002333
Iteration 81/1000 | Loss: 0.00002333
Iteration 82/1000 | Loss: 0.00002333
Iteration 83/1000 | Loss: 0.00002333
Iteration 84/1000 | Loss: 0.00002333
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002333
Iteration 88/1000 | Loss: 0.00002333
Iteration 89/1000 | Loss: 0.00002332
Iteration 90/1000 | Loss: 0.00002332
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002332
Iteration 93/1000 | Loss: 0.00002332
Iteration 94/1000 | Loss: 0.00002332
Iteration 95/1000 | Loss: 0.00002332
Iteration 96/1000 | Loss: 0.00002332
Iteration 97/1000 | Loss: 0.00002332
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002332
Iteration 100/1000 | Loss: 0.00002332
Iteration 101/1000 | Loss: 0.00002331
Iteration 102/1000 | Loss: 0.00002331
Iteration 103/1000 | Loss: 0.00002331
Iteration 104/1000 | Loss: 0.00002331
Iteration 105/1000 | Loss: 0.00002331
Iteration 106/1000 | Loss: 0.00002331
Iteration 107/1000 | Loss: 0.00002331
Iteration 108/1000 | Loss: 0.00002331
Iteration 109/1000 | Loss: 0.00002330
Iteration 110/1000 | Loss: 0.00002330
Iteration 111/1000 | Loss: 0.00002330
Iteration 112/1000 | Loss: 0.00002330
Iteration 113/1000 | Loss: 0.00002330
Iteration 114/1000 | Loss: 0.00002330
Iteration 115/1000 | Loss: 0.00002330
Iteration 116/1000 | Loss: 0.00002330
Iteration 117/1000 | Loss: 0.00002329
Iteration 118/1000 | Loss: 0.00002329
Iteration 119/1000 | Loss: 0.00002329
Iteration 120/1000 | Loss: 0.00002329
Iteration 121/1000 | Loss: 0.00002329
Iteration 122/1000 | Loss: 0.00002329
Iteration 123/1000 | Loss: 0.00002329
Iteration 124/1000 | Loss: 0.00002329
Iteration 125/1000 | Loss: 0.00002329
Iteration 126/1000 | Loss: 0.00002329
Iteration 127/1000 | Loss: 0.00002329
Iteration 128/1000 | Loss: 0.00002329
Iteration 129/1000 | Loss: 0.00002329
Iteration 130/1000 | Loss: 0.00002329
Iteration 131/1000 | Loss: 0.00002329
Iteration 132/1000 | Loss: 0.00002329
Iteration 133/1000 | Loss: 0.00002329
Iteration 134/1000 | Loss: 0.00002329
Iteration 135/1000 | Loss: 0.00002329
Iteration 136/1000 | Loss: 0.00002329
Iteration 137/1000 | Loss: 0.00002329
Iteration 138/1000 | Loss: 0.00002329
Iteration 139/1000 | Loss: 0.00002329
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00002329
Iteration 142/1000 | Loss: 0.00002329
Iteration 143/1000 | Loss: 0.00002329
Iteration 144/1000 | Loss: 0.00002329
Iteration 145/1000 | Loss: 0.00002329
Iteration 146/1000 | Loss: 0.00002329
Iteration 147/1000 | Loss: 0.00002329
Iteration 148/1000 | Loss: 0.00002329
Iteration 149/1000 | Loss: 0.00002329
Iteration 150/1000 | Loss: 0.00002329
Iteration 151/1000 | Loss: 0.00002329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.3286640498554334e-05, 2.3286640498554334e-05, 2.3286640498554334e-05, 2.3286640498554334e-05, 2.3286640498554334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3286640498554334e-05

Optimization complete. Final v2v error: 4.025358200073242 mm

Highest mean error: 4.306170463562012 mm for frame 144

Lowest mean error: 3.910964250564575 mm for frame 36

Saving results

Total time: 35.344279527664185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053134
Iteration 2/25 | Loss: 0.00162983
Iteration 3/25 | Loss: 0.00099494
Iteration 4/25 | Loss: 0.00092570
Iteration 5/25 | Loss: 0.00091056
Iteration 6/25 | Loss: 0.00090628
Iteration 7/25 | Loss: 0.00090506
Iteration 8/25 | Loss: 0.00090492
Iteration 9/25 | Loss: 0.00090492
Iteration 10/25 | Loss: 0.00090492
Iteration 11/25 | Loss: 0.00090492
Iteration 12/25 | Loss: 0.00090492
Iteration 13/25 | Loss: 0.00090492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009049195214174688, 0.0009049195214174688, 0.0009049195214174688, 0.0009049195214174688, 0.0009049195214174688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009049195214174688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48222721
Iteration 2/25 | Loss: 0.00111965
Iteration 3/25 | Loss: 0.00111965
Iteration 4/25 | Loss: 0.00111965
Iteration 5/25 | Loss: 0.00111965
Iteration 6/25 | Loss: 0.00111965
Iteration 7/25 | Loss: 0.00111965
Iteration 8/25 | Loss: 0.00111965
Iteration 9/25 | Loss: 0.00111965
Iteration 10/25 | Loss: 0.00111965
Iteration 11/25 | Loss: 0.00111965
Iteration 12/25 | Loss: 0.00111965
Iteration 13/25 | Loss: 0.00111965
Iteration 14/25 | Loss: 0.00111965
Iteration 15/25 | Loss: 0.00111965
Iteration 16/25 | Loss: 0.00111965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011196464765816927, 0.0011196464765816927, 0.0011196464765816927, 0.0011196464765816927, 0.0011196464765816927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011196464765816927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111965
Iteration 2/1000 | Loss: 0.00005108
Iteration 3/1000 | Loss: 0.00003495
Iteration 4/1000 | Loss: 0.00003130
Iteration 5/1000 | Loss: 0.00002852
Iteration 6/1000 | Loss: 0.00002705
Iteration 7/1000 | Loss: 0.00002589
Iteration 8/1000 | Loss: 0.00002499
Iteration 9/1000 | Loss: 0.00002439
Iteration 10/1000 | Loss: 0.00002402
Iteration 11/1000 | Loss: 0.00002376
Iteration 12/1000 | Loss: 0.00002351
Iteration 13/1000 | Loss: 0.00002332
Iteration 14/1000 | Loss: 0.00002317
Iteration 15/1000 | Loss: 0.00002314
Iteration 16/1000 | Loss: 0.00002306
Iteration 17/1000 | Loss: 0.00002303
Iteration 18/1000 | Loss: 0.00002303
Iteration 19/1000 | Loss: 0.00002302
Iteration 20/1000 | Loss: 0.00002298
Iteration 21/1000 | Loss: 0.00002298
Iteration 22/1000 | Loss: 0.00002297
Iteration 23/1000 | Loss: 0.00002296
Iteration 24/1000 | Loss: 0.00002295
Iteration 25/1000 | Loss: 0.00002294
Iteration 26/1000 | Loss: 0.00002294
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002294
Iteration 29/1000 | Loss: 0.00002291
Iteration 30/1000 | Loss: 0.00002291
Iteration 31/1000 | Loss: 0.00002291
Iteration 32/1000 | Loss: 0.00002291
Iteration 33/1000 | Loss: 0.00002291
Iteration 34/1000 | Loss: 0.00002291
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002291
Iteration 38/1000 | Loss: 0.00002291
Iteration 39/1000 | Loss: 0.00002290
Iteration 40/1000 | Loss: 0.00002290
Iteration 41/1000 | Loss: 0.00002289
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002289
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002287
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002286
Iteration 50/1000 | Loss: 0.00002286
Iteration 51/1000 | Loss: 0.00002286
Iteration 52/1000 | Loss: 0.00002286
Iteration 53/1000 | Loss: 0.00002286
Iteration 54/1000 | Loss: 0.00002285
Iteration 55/1000 | Loss: 0.00002285
Iteration 56/1000 | Loss: 0.00002285
Iteration 57/1000 | Loss: 0.00002285
Iteration 58/1000 | Loss: 0.00002285
Iteration 59/1000 | Loss: 0.00002285
Iteration 60/1000 | Loss: 0.00002284
Iteration 61/1000 | Loss: 0.00002284
Iteration 62/1000 | Loss: 0.00002284
Iteration 63/1000 | Loss: 0.00002283
Iteration 64/1000 | Loss: 0.00002283
Iteration 65/1000 | Loss: 0.00002283
Iteration 66/1000 | Loss: 0.00002283
Iteration 67/1000 | Loss: 0.00002283
Iteration 68/1000 | Loss: 0.00002283
Iteration 69/1000 | Loss: 0.00002283
Iteration 70/1000 | Loss: 0.00002283
Iteration 71/1000 | Loss: 0.00002283
Iteration 72/1000 | Loss: 0.00002283
Iteration 73/1000 | Loss: 0.00002283
Iteration 74/1000 | Loss: 0.00002283
Iteration 75/1000 | Loss: 0.00002283
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002282
Iteration 83/1000 | Loss: 0.00002282
Iteration 84/1000 | Loss: 0.00002282
Iteration 85/1000 | Loss: 0.00002282
Iteration 86/1000 | Loss: 0.00002282
Iteration 87/1000 | Loss: 0.00002282
Iteration 88/1000 | Loss: 0.00002282
Iteration 89/1000 | Loss: 0.00002282
Iteration 90/1000 | Loss: 0.00002281
Iteration 91/1000 | Loss: 0.00002281
Iteration 92/1000 | Loss: 0.00002281
Iteration 93/1000 | Loss: 0.00002281
Iteration 94/1000 | Loss: 0.00002281
Iteration 95/1000 | Loss: 0.00002281
Iteration 96/1000 | Loss: 0.00002281
Iteration 97/1000 | Loss: 0.00002281
Iteration 98/1000 | Loss: 0.00002281
Iteration 99/1000 | Loss: 0.00002281
Iteration 100/1000 | Loss: 0.00002281
Iteration 101/1000 | Loss: 0.00002281
Iteration 102/1000 | Loss: 0.00002281
Iteration 103/1000 | Loss: 0.00002281
Iteration 104/1000 | Loss: 0.00002281
Iteration 105/1000 | Loss: 0.00002281
Iteration 106/1000 | Loss: 0.00002281
Iteration 107/1000 | Loss: 0.00002281
Iteration 108/1000 | Loss: 0.00002281
Iteration 109/1000 | Loss: 0.00002281
Iteration 110/1000 | Loss: 0.00002281
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002281
Iteration 116/1000 | Loss: 0.00002281
Iteration 117/1000 | Loss: 0.00002281
Iteration 118/1000 | Loss: 0.00002281
Iteration 119/1000 | Loss: 0.00002281
Iteration 120/1000 | Loss: 0.00002281
Iteration 121/1000 | Loss: 0.00002281
Iteration 122/1000 | Loss: 0.00002281
Iteration 123/1000 | Loss: 0.00002281
Iteration 124/1000 | Loss: 0.00002281
Iteration 125/1000 | Loss: 0.00002281
Iteration 126/1000 | Loss: 0.00002281
Iteration 127/1000 | Loss: 0.00002281
Iteration 128/1000 | Loss: 0.00002281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.2814852854935452e-05, 2.2814852854935452e-05, 2.2814852854935452e-05, 2.2814852854935452e-05, 2.2814852854935452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2814852854935452e-05

Optimization complete. Final v2v error: 3.8372268676757812 mm

Highest mean error: 4.951072692871094 mm for frame 30

Lowest mean error: 3.0480825901031494 mm for frame 9

Saving results

Total time: 44.96522307395935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874181
Iteration 2/25 | Loss: 0.00127108
Iteration 3/25 | Loss: 0.00089338
Iteration 4/25 | Loss: 0.00081051
Iteration 5/25 | Loss: 0.00079350
Iteration 6/25 | Loss: 0.00078727
Iteration 7/25 | Loss: 0.00078590
Iteration 8/25 | Loss: 0.00078590
Iteration 9/25 | Loss: 0.00078590
Iteration 10/25 | Loss: 0.00078590
Iteration 11/25 | Loss: 0.00078590
Iteration 12/25 | Loss: 0.00078590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007859027828089893, 0.0007859027828089893, 0.0007859027828089893, 0.0007859027828089893, 0.0007859027828089893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007859027828089893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58722150
Iteration 2/25 | Loss: 0.00119549
Iteration 3/25 | Loss: 0.00119547
Iteration 4/25 | Loss: 0.00119547
Iteration 5/25 | Loss: 0.00119547
Iteration 6/25 | Loss: 0.00119547
Iteration 7/25 | Loss: 0.00119547
Iteration 8/25 | Loss: 0.00119547
Iteration 9/25 | Loss: 0.00119547
Iteration 10/25 | Loss: 0.00119547
Iteration 11/25 | Loss: 0.00119547
Iteration 12/25 | Loss: 0.00119547
Iteration 13/25 | Loss: 0.00119547
Iteration 14/25 | Loss: 0.00119547
Iteration 15/25 | Loss: 0.00119547
Iteration 16/25 | Loss: 0.00119547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011954701039940119, 0.0011954701039940119, 0.0011954701039940119, 0.0011954701039940119, 0.0011954701039940119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011954701039940119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119547
Iteration 2/1000 | Loss: 0.00001875
Iteration 3/1000 | Loss: 0.00001610
Iteration 4/1000 | Loss: 0.00001517
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001344
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001337
Iteration 22/1000 | Loss: 0.00001331
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001323
Iteration 41/1000 | Loss: 0.00001323
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001314
Iteration 83/1000 | Loss: 0.00001314
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001309
Iteration 131/1000 | Loss: 0.00001309
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001309
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001309
Iteration 151/1000 | Loss: 0.00001309
Iteration 152/1000 | Loss: 0.00001309
Iteration 153/1000 | Loss: 0.00001309
Iteration 154/1000 | Loss: 0.00001309
Iteration 155/1000 | Loss: 0.00001309
Iteration 156/1000 | Loss: 0.00001309
Iteration 157/1000 | Loss: 0.00001309
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001309
Iteration 164/1000 | Loss: 0.00001309
Iteration 165/1000 | Loss: 0.00001309
Iteration 166/1000 | Loss: 0.00001309
Iteration 167/1000 | Loss: 0.00001309
Iteration 168/1000 | Loss: 0.00001309
Iteration 169/1000 | Loss: 0.00001309
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.3088216292089783e-05, 1.3088216292089783e-05, 1.3088216292089783e-05, 1.3088216292089783e-05, 1.3088216292089783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3088216292089783e-05

Optimization complete. Final v2v error: 3.060154914855957 mm

Highest mean error: 3.2649030685424805 mm for frame 210

Lowest mean error: 2.7175681591033936 mm for frame 10

Saving results

Total time: 40.19072246551514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065223
Iteration 2/25 | Loss: 0.00198205
Iteration 3/25 | Loss: 0.00144167
Iteration 4/25 | Loss: 0.00110782
Iteration 5/25 | Loss: 0.00096294
Iteration 6/25 | Loss: 0.00090813
Iteration 7/25 | Loss: 0.00092042
Iteration 8/25 | Loss: 0.00092522
Iteration 9/25 | Loss: 0.00090337
Iteration 10/25 | Loss: 0.00088991
Iteration 11/25 | Loss: 0.00088236
Iteration 12/25 | Loss: 0.00087848
Iteration 13/25 | Loss: 0.00088014
Iteration 14/25 | Loss: 0.00087996
Iteration 15/25 | Loss: 0.00087700
Iteration 16/25 | Loss: 0.00087515
Iteration 17/25 | Loss: 0.00087399
Iteration 18/25 | Loss: 0.00087358
Iteration 19/25 | Loss: 0.00087357
Iteration 20/25 | Loss: 0.00087356
Iteration 21/25 | Loss: 0.00087356
Iteration 22/25 | Loss: 0.00087356
Iteration 23/25 | Loss: 0.00087356
Iteration 24/25 | Loss: 0.00087356
Iteration 25/25 | Loss: 0.00087356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56488490
Iteration 2/25 | Loss: 0.00152094
Iteration 3/25 | Loss: 0.00152094
Iteration 4/25 | Loss: 0.00152094
Iteration 5/25 | Loss: 0.00152094
Iteration 6/25 | Loss: 0.00152094
Iteration 7/25 | Loss: 0.00152094
Iteration 8/25 | Loss: 0.00152094
Iteration 9/25 | Loss: 0.00152094
Iteration 10/25 | Loss: 0.00152094
Iteration 11/25 | Loss: 0.00152094
Iteration 12/25 | Loss: 0.00152094
Iteration 13/25 | Loss: 0.00152094
Iteration 14/25 | Loss: 0.00152094
Iteration 15/25 | Loss: 0.00152094
Iteration 16/25 | Loss: 0.00152094
Iteration 17/25 | Loss: 0.00152094
Iteration 18/25 | Loss: 0.00152094
Iteration 19/25 | Loss: 0.00152094
Iteration 20/25 | Loss: 0.00152094
Iteration 21/25 | Loss: 0.00152094
Iteration 22/25 | Loss: 0.00152094
Iteration 23/25 | Loss: 0.00152094
Iteration 24/25 | Loss: 0.00152094
Iteration 25/25 | Loss: 0.00152094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152094
Iteration 2/1000 | Loss: 0.00003579
Iteration 3/1000 | Loss: 0.00002446
Iteration 4/1000 | Loss: 0.00002247
Iteration 5/1000 | Loss: 0.00002123
Iteration 6/1000 | Loss: 0.00002047
Iteration 7/1000 | Loss: 0.00001988
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001908
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001892
Iteration 13/1000 | Loss: 0.00001883
Iteration 14/1000 | Loss: 0.00001876
Iteration 15/1000 | Loss: 0.00001875
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001874
Iteration 18/1000 | Loss: 0.00001873
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001872
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001870
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001868
Iteration 32/1000 | Loss: 0.00001867
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001867
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001866
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001863
Iteration 62/1000 | Loss: 0.00001863
Iteration 63/1000 | Loss: 0.00001863
Iteration 64/1000 | Loss: 0.00001862
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00001860
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001860
Iteration 71/1000 | Loss: 0.00001860
Iteration 72/1000 | Loss: 0.00001860
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001859
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001857
Iteration 90/1000 | Loss: 0.00001857
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001857
Iteration 93/1000 | Loss: 0.00001857
Iteration 94/1000 | Loss: 0.00001857
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Iteration 100/1000 | Loss: 0.00001855
Iteration 101/1000 | Loss: 0.00001855
Iteration 102/1000 | Loss: 0.00001855
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001853
Iteration 116/1000 | Loss: 0.00001853
Iteration 117/1000 | Loss: 0.00001853
Iteration 118/1000 | Loss: 0.00001853
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001849
Iteration 149/1000 | Loss: 0.00001849
Iteration 150/1000 | Loss: 0.00001849
Iteration 151/1000 | Loss: 0.00001849
Iteration 152/1000 | Loss: 0.00001849
Iteration 153/1000 | Loss: 0.00001849
Iteration 154/1000 | Loss: 0.00001849
Iteration 155/1000 | Loss: 0.00001849
Iteration 156/1000 | Loss: 0.00001849
Iteration 157/1000 | Loss: 0.00001849
Iteration 158/1000 | Loss: 0.00001848
Iteration 159/1000 | Loss: 0.00001848
Iteration 160/1000 | Loss: 0.00001848
Iteration 161/1000 | Loss: 0.00001848
Iteration 162/1000 | Loss: 0.00001848
Iteration 163/1000 | Loss: 0.00001848
Iteration 164/1000 | Loss: 0.00001848
Iteration 165/1000 | Loss: 0.00001848
Iteration 166/1000 | Loss: 0.00001848
Iteration 167/1000 | Loss: 0.00001848
Iteration 168/1000 | Loss: 0.00001848
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.8478307538316585e-05, 1.8478307538316585e-05, 1.8478307538316585e-05, 1.8478307538316585e-05, 1.8478307538316585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8478307538316585e-05

Optimization complete. Final v2v error: 3.526646614074707 mm

Highest mean error: 3.7834153175354004 mm for frame 1

Lowest mean error: 3.380133867263794 mm for frame 132

Saving results

Total time: 59.40983581542969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00636908
Iteration 2/25 | Loss: 0.00132365
Iteration 3/25 | Loss: 0.00107763
Iteration 4/25 | Loss: 0.00100315
Iteration 5/25 | Loss: 0.00099297
Iteration 6/25 | Loss: 0.00099280
Iteration 7/25 | Loss: 0.00095618
Iteration 8/25 | Loss: 0.00092321
Iteration 9/25 | Loss: 0.00091250
Iteration 10/25 | Loss: 0.00090671
Iteration 11/25 | Loss: 0.00090406
Iteration 12/25 | Loss: 0.00090278
Iteration 13/25 | Loss: 0.00090258
Iteration 14/25 | Loss: 0.00090253
Iteration 15/25 | Loss: 0.00090253
Iteration 16/25 | Loss: 0.00090252
Iteration 17/25 | Loss: 0.00090252
Iteration 18/25 | Loss: 0.00090252
Iteration 19/25 | Loss: 0.00090252
Iteration 20/25 | Loss: 0.00090252
Iteration 21/25 | Loss: 0.00090252
Iteration 22/25 | Loss: 0.00090252
Iteration 23/25 | Loss: 0.00090252
Iteration 24/25 | Loss: 0.00090252
Iteration 25/25 | Loss: 0.00090252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47040308
Iteration 2/25 | Loss: 0.00136689
Iteration 3/25 | Loss: 0.00136680
Iteration 4/25 | Loss: 0.00136680
Iteration 5/25 | Loss: 0.00136679
Iteration 6/25 | Loss: 0.00136679
Iteration 7/25 | Loss: 0.00136679
Iteration 8/25 | Loss: 0.00136679
Iteration 9/25 | Loss: 0.00136679
Iteration 10/25 | Loss: 0.00136679
Iteration 11/25 | Loss: 0.00136679
Iteration 12/25 | Loss: 0.00136679
Iteration 13/25 | Loss: 0.00136679
Iteration 14/25 | Loss: 0.00136679
Iteration 15/25 | Loss: 0.00136679
Iteration 16/25 | Loss: 0.00136679
Iteration 17/25 | Loss: 0.00136679
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013667929451912642, 0.0013667929451912642, 0.0013667929451912642, 0.0013667929451912642, 0.0013667929451912642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013667929451912642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136679
Iteration 2/1000 | Loss: 0.00012330
Iteration 3/1000 | Loss: 0.00006196
Iteration 4/1000 | Loss: 0.00004082
Iteration 5/1000 | Loss: 0.00003366
Iteration 6/1000 | Loss: 0.00003066
Iteration 7/1000 | Loss: 0.00002972
Iteration 8/1000 | Loss: 0.00002888
Iteration 9/1000 | Loss: 0.00002802
Iteration 10/1000 | Loss: 0.00037653
Iteration 11/1000 | Loss: 0.00009436
Iteration 12/1000 | Loss: 0.00003796
Iteration 13/1000 | Loss: 0.00003073
Iteration 14/1000 | Loss: 0.00002761
Iteration 15/1000 | Loss: 0.00002680
Iteration 16/1000 | Loss: 0.00002648
Iteration 17/1000 | Loss: 0.00002617
Iteration 18/1000 | Loss: 0.00002604
Iteration 19/1000 | Loss: 0.00002585
Iteration 20/1000 | Loss: 0.00002565
Iteration 21/1000 | Loss: 0.00002560
Iteration 22/1000 | Loss: 0.00002558
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002551
Iteration 25/1000 | Loss: 0.00002550
Iteration 26/1000 | Loss: 0.00002550
Iteration 27/1000 | Loss: 0.00002550
Iteration 28/1000 | Loss: 0.00002550
Iteration 29/1000 | Loss: 0.00002549
Iteration 30/1000 | Loss: 0.00002549
Iteration 31/1000 | Loss: 0.00002549
Iteration 32/1000 | Loss: 0.00002549
Iteration 33/1000 | Loss: 0.00002549
Iteration 34/1000 | Loss: 0.00002547
Iteration 35/1000 | Loss: 0.00002543
Iteration 36/1000 | Loss: 0.00002543
Iteration 37/1000 | Loss: 0.00002542
Iteration 38/1000 | Loss: 0.00002541
Iteration 39/1000 | Loss: 0.00002540
Iteration 40/1000 | Loss: 0.00002539
Iteration 41/1000 | Loss: 0.00002539
Iteration 42/1000 | Loss: 0.00002538
Iteration 43/1000 | Loss: 0.00002538
Iteration 44/1000 | Loss: 0.00002537
Iteration 45/1000 | Loss: 0.00002531
Iteration 46/1000 | Loss: 0.00002531
Iteration 47/1000 | Loss: 0.00002530
Iteration 48/1000 | Loss: 0.00002530
Iteration 49/1000 | Loss: 0.00002529
Iteration 50/1000 | Loss: 0.00002528
Iteration 51/1000 | Loss: 0.00002526
Iteration 52/1000 | Loss: 0.00002522
Iteration 53/1000 | Loss: 0.00002522
Iteration 54/1000 | Loss: 0.00002521
Iteration 55/1000 | Loss: 0.00002521
Iteration 56/1000 | Loss: 0.00002520
Iteration 57/1000 | Loss: 0.00002520
Iteration 58/1000 | Loss: 0.00002519
Iteration 59/1000 | Loss: 0.00002519
Iteration 60/1000 | Loss: 0.00002519
Iteration 61/1000 | Loss: 0.00002518
Iteration 62/1000 | Loss: 0.00002518
Iteration 63/1000 | Loss: 0.00002518
Iteration 64/1000 | Loss: 0.00002517
Iteration 65/1000 | Loss: 0.00002517
Iteration 66/1000 | Loss: 0.00002517
Iteration 67/1000 | Loss: 0.00002516
Iteration 68/1000 | Loss: 0.00002516
Iteration 69/1000 | Loss: 0.00002516
Iteration 70/1000 | Loss: 0.00002515
Iteration 71/1000 | Loss: 0.00002515
Iteration 72/1000 | Loss: 0.00002514
Iteration 73/1000 | Loss: 0.00002514
Iteration 74/1000 | Loss: 0.00002514
Iteration 75/1000 | Loss: 0.00002513
Iteration 76/1000 | Loss: 0.00002513
Iteration 77/1000 | Loss: 0.00002512
Iteration 78/1000 | Loss: 0.00002512
Iteration 79/1000 | Loss: 0.00002512
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002509
Iteration 83/1000 | Loss: 0.00002509
Iteration 84/1000 | Loss: 0.00002509
Iteration 85/1000 | Loss: 0.00002509
Iteration 86/1000 | Loss: 0.00002508
Iteration 87/1000 | Loss: 0.00002508
Iteration 88/1000 | Loss: 0.00002508
Iteration 89/1000 | Loss: 0.00002508
Iteration 90/1000 | Loss: 0.00002508
Iteration 91/1000 | Loss: 0.00002508
Iteration 92/1000 | Loss: 0.00002508
Iteration 93/1000 | Loss: 0.00002508
Iteration 94/1000 | Loss: 0.00002507
Iteration 95/1000 | Loss: 0.00002507
Iteration 96/1000 | Loss: 0.00002507
Iteration 97/1000 | Loss: 0.00002507
Iteration 98/1000 | Loss: 0.00002507
Iteration 99/1000 | Loss: 0.00002507
Iteration 100/1000 | Loss: 0.00002507
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002506
Iteration 103/1000 | Loss: 0.00002506
Iteration 104/1000 | Loss: 0.00002506
Iteration 105/1000 | Loss: 0.00002506
Iteration 106/1000 | Loss: 0.00002506
Iteration 107/1000 | Loss: 0.00002506
Iteration 108/1000 | Loss: 0.00002506
Iteration 109/1000 | Loss: 0.00002506
Iteration 110/1000 | Loss: 0.00002506
Iteration 111/1000 | Loss: 0.00002506
Iteration 112/1000 | Loss: 0.00002506
Iteration 113/1000 | Loss: 0.00002506
Iteration 114/1000 | Loss: 0.00002506
Iteration 115/1000 | Loss: 0.00002506
Iteration 116/1000 | Loss: 0.00002505
Iteration 117/1000 | Loss: 0.00002505
Iteration 118/1000 | Loss: 0.00002505
Iteration 119/1000 | Loss: 0.00002505
Iteration 120/1000 | Loss: 0.00002505
Iteration 121/1000 | Loss: 0.00002505
Iteration 122/1000 | Loss: 0.00002505
Iteration 123/1000 | Loss: 0.00002505
Iteration 124/1000 | Loss: 0.00002505
Iteration 125/1000 | Loss: 0.00002505
Iteration 126/1000 | Loss: 0.00002505
Iteration 127/1000 | Loss: 0.00002505
Iteration 128/1000 | Loss: 0.00002504
Iteration 129/1000 | Loss: 0.00002504
Iteration 130/1000 | Loss: 0.00002504
Iteration 131/1000 | Loss: 0.00002504
Iteration 132/1000 | Loss: 0.00002504
Iteration 133/1000 | Loss: 0.00002504
Iteration 134/1000 | Loss: 0.00002503
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002503
Iteration 137/1000 | Loss: 0.00002503
Iteration 138/1000 | Loss: 0.00002503
Iteration 139/1000 | Loss: 0.00002503
Iteration 140/1000 | Loss: 0.00002502
Iteration 141/1000 | Loss: 0.00002502
Iteration 142/1000 | Loss: 0.00002502
Iteration 143/1000 | Loss: 0.00002502
Iteration 144/1000 | Loss: 0.00002502
Iteration 145/1000 | Loss: 0.00002502
Iteration 146/1000 | Loss: 0.00002502
Iteration 147/1000 | Loss: 0.00002502
Iteration 148/1000 | Loss: 0.00002502
Iteration 149/1000 | Loss: 0.00002502
Iteration 150/1000 | Loss: 0.00002502
Iteration 151/1000 | Loss: 0.00002502
Iteration 152/1000 | Loss: 0.00002502
Iteration 153/1000 | Loss: 0.00002502
Iteration 154/1000 | Loss: 0.00002502
Iteration 155/1000 | Loss: 0.00002502
Iteration 156/1000 | Loss: 0.00002502
Iteration 157/1000 | Loss: 0.00002502
Iteration 158/1000 | Loss: 0.00002502
Iteration 159/1000 | Loss: 0.00002502
Iteration 160/1000 | Loss: 0.00002502
Iteration 161/1000 | Loss: 0.00002502
Iteration 162/1000 | Loss: 0.00002502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.501809285604395e-05, 2.501809285604395e-05, 2.501809285604395e-05, 2.501809285604395e-05, 2.501809285604395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.501809285604395e-05

Optimization complete. Final v2v error: 3.817777395248413 mm

Highest mean error: 5.791852951049805 mm for frame 118

Lowest mean error: 2.8842289447784424 mm for frame 135

Saving results

Total time: 67.34793162345886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712218
Iteration 2/25 | Loss: 0.00092141
Iteration 3/25 | Loss: 0.00079376
Iteration 4/25 | Loss: 0.00076924
Iteration 5/25 | Loss: 0.00075844
Iteration 6/25 | Loss: 0.00075462
Iteration 7/25 | Loss: 0.00075352
Iteration 8/25 | Loss: 0.00075352
Iteration 9/25 | Loss: 0.00075352
Iteration 10/25 | Loss: 0.00075352
Iteration 11/25 | Loss: 0.00075352
Iteration 12/25 | Loss: 0.00075352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007535186596214771, 0.0007535186596214771, 0.0007535186596214771, 0.0007535186596214771, 0.0007535186596214771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007535186596214771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.94569492
Iteration 2/25 | Loss: 0.00110470
Iteration 3/25 | Loss: 0.00110468
Iteration 4/25 | Loss: 0.00110468
Iteration 5/25 | Loss: 0.00110468
Iteration 6/25 | Loss: 0.00110468
Iteration 7/25 | Loss: 0.00110468
Iteration 8/25 | Loss: 0.00110468
Iteration 9/25 | Loss: 0.00110468
Iteration 10/25 | Loss: 0.00110468
Iteration 11/25 | Loss: 0.00110468
Iteration 12/25 | Loss: 0.00110468
Iteration 13/25 | Loss: 0.00110468
Iteration 14/25 | Loss: 0.00110468
Iteration 15/25 | Loss: 0.00110468
Iteration 16/25 | Loss: 0.00110468
Iteration 17/25 | Loss: 0.00110468
Iteration 18/25 | Loss: 0.00110468
Iteration 19/25 | Loss: 0.00110468
Iteration 20/25 | Loss: 0.00110468
Iteration 21/25 | Loss: 0.00110468
Iteration 22/25 | Loss: 0.00110468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011046812869608402, 0.0011046812869608402, 0.0011046812869608402, 0.0011046812869608402, 0.0011046812869608402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011046812869608402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110468
Iteration 2/1000 | Loss: 0.00003194
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001576
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001495
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001486
Iteration 21/1000 | Loss: 0.00001486
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001486
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001481
Iteration 43/1000 | Loss: 0.00001480
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001472
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001471
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001469
Iteration 86/1000 | Loss: 0.00001469
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001468
Iteration 89/1000 | Loss: 0.00001468
Iteration 90/1000 | Loss: 0.00001468
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001465
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001465
Iteration 107/1000 | Loss: 0.00001465
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001465
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Iteration 118/1000 | Loss: 0.00001464
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001463
Iteration 135/1000 | Loss: 0.00001463
Iteration 136/1000 | Loss: 0.00001463
Iteration 137/1000 | Loss: 0.00001463
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.4624458344769664e-05, 1.4624458344769664e-05, 1.4624458344769664e-05, 1.4624458344769664e-05, 1.4624458344769664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4624458344769664e-05

Optimization complete. Final v2v error: 3.285337209701538 mm

Highest mean error: 3.5529282093048096 mm for frame 155

Lowest mean error: 3.0114288330078125 mm for frame 33

Saving results

Total time: 38.05311036109924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464593
Iteration 2/25 | Loss: 0.00091029
Iteration 3/25 | Loss: 0.00079524
Iteration 4/25 | Loss: 0.00077682
Iteration 5/25 | Loss: 0.00077051
Iteration 6/25 | Loss: 0.00076900
Iteration 7/25 | Loss: 0.00076851
Iteration 8/25 | Loss: 0.00076851
Iteration 9/25 | Loss: 0.00076851
Iteration 10/25 | Loss: 0.00076851
Iteration 11/25 | Loss: 0.00076851
Iteration 12/25 | Loss: 0.00076851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007685074815526605, 0.0007685074815526605, 0.0007685074815526605, 0.0007685074815526605, 0.0007685074815526605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007685074815526605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.25342512
Iteration 2/25 | Loss: 0.00110643
Iteration 3/25 | Loss: 0.00110642
Iteration 4/25 | Loss: 0.00110642
Iteration 5/25 | Loss: 0.00110642
Iteration 6/25 | Loss: 0.00110642
Iteration 7/25 | Loss: 0.00110642
Iteration 8/25 | Loss: 0.00110642
Iteration 9/25 | Loss: 0.00110642
Iteration 10/25 | Loss: 0.00110642
Iteration 11/25 | Loss: 0.00110642
Iteration 12/25 | Loss: 0.00110642
Iteration 13/25 | Loss: 0.00110642
Iteration 14/25 | Loss: 0.00110642
Iteration 15/25 | Loss: 0.00110642
Iteration 16/25 | Loss: 0.00110642
Iteration 17/25 | Loss: 0.00110642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001106420997530222, 0.001106420997530222, 0.001106420997530222, 0.001106420997530222, 0.001106420997530222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001106420997530222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110642
Iteration 2/1000 | Loss: 0.00002747
Iteration 3/1000 | Loss: 0.00001935
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001660
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001572
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001564
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001557
Iteration 25/1000 | Loss: 0.00001557
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001554
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001547
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001546
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001546
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001540
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001540
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Iteration 140/1000 | Loss: 0.00001539
Iteration 141/1000 | Loss: 0.00001539
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001539
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001538
Iteration 146/1000 | Loss: 0.00001538
Iteration 147/1000 | Loss: 0.00001538
Iteration 148/1000 | Loss: 0.00001538
Iteration 149/1000 | Loss: 0.00001538
Iteration 150/1000 | Loss: 0.00001538
Iteration 151/1000 | Loss: 0.00001538
Iteration 152/1000 | Loss: 0.00001538
Iteration 153/1000 | Loss: 0.00001538
Iteration 154/1000 | Loss: 0.00001538
Iteration 155/1000 | Loss: 0.00001538
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Iteration 164/1000 | Loss: 0.00001537
Iteration 165/1000 | Loss: 0.00001537
Iteration 166/1000 | Loss: 0.00001537
Iteration 167/1000 | Loss: 0.00001537
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001536
Iteration 171/1000 | Loss: 0.00001536
Iteration 172/1000 | Loss: 0.00001536
Iteration 173/1000 | Loss: 0.00001536
Iteration 174/1000 | Loss: 0.00001536
Iteration 175/1000 | Loss: 0.00001536
Iteration 176/1000 | Loss: 0.00001536
Iteration 177/1000 | Loss: 0.00001536
Iteration 178/1000 | Loss: 0.00001536
Iteration 179/1000 | Loss: 0.00001535
Iteration 180/1000 | Loss: 0.00001535
Iteration 181/1000 | Loss: 0.00001535
Iteration 182/1000 | Loss: 0.00001535
Iteration 183/1000 | Loss: 0.00001535
Iteration 184/1000 | Loss: 0.00001535
Iteration 185/1000 | Loss: 0.00001535
Iteration 186/1000 | Loss: 0.00001535
Iteration 187/1000 | Loss: 0.00001535
Iteration 188/1000 | Loss: 0.00001535
Iteration 189/1000 | Loss: 0.00001535
Iteration 190/1000 | Loss: 0.00001534
Iteration 191/1000 | Loss: 0.00001534
Iteration 192/1000 | Loss: 0.00001534
Iteration 193/1000 | Loss: 0.00001534
Iteration 194/1000 | Loss: 0.00001534
Iteration 195/1000 | Loss: 0.00001534
Iteration 196/1000 | Loss: 0.00001534
Iteration 197/1000 | Loss: 0.00001534
Iteration 198/1000 | Loss: 0.00001534
Iteration 199/1000 | Loss: 0.00001534
Iteration 200/1000 | Loss: 0.00001534
Iteration 201/1000 | Loss: 0.00001534
Iteration 202/1000 | Loss: 0.00001534
Iteration 203/1000 | Loss: 0.00001533
Iteration 204/1000 | Loss: 0.00001533
Iteration 205/1000 | Loss: 0.00001533
Iteration 206/1000 | Loss: 0.00001533
Iteration 207/1000 | Loss: 0.00001533
Iteration 208/1000 | Loss: 0.00001533
Iteration 209/1000 | Loss: 0.00001533
Iteration 210/1000 | Loss: 0.00001533
Iteration 211/1000 | Loss: 0.00001533
Iteration 212/1000 | Loss: 0.00001533
Iteration 213/1000 | Loss: 0.00001533
Iteration 214/1000 | Loss: 0.00001533
Iteration 215/1000 | Loss: 0.00001532
Iteration 216/1000 | Loss: 0.00001532
Iteration 217/1000 | Loss: 0.00001532
Iteration 218/1000 | Loss: 0.00001532
Iteration 219/1000 | Loss: 0.00001532
Iteration 220/1000 | Loss: 0.00001532
Iteration 221/1000 | Loss: 0.00001532
Iteration 222/1000 | Loss: 0.00001532
Iteration 223/1000 | Loss: 0.00001532
Iteration 224/1000 | Loss: 0.00001532
Iteration 225/1000 | Loss: 0.00001532
Iteration 226/1000 | Loss: 0.00001532
Iteration 227/1000 | Loss: 0.00001532
Iteration 228/1000 | Loss: 0.00001532
Iteration 229/1000 | Loss: 0.00001532
Iteration 230/1000 | Loss: 0.00001531
Iteration 231/1000 | Loss: 0.00001531
Iteration 232/1000 | Loss: 0.00001531
Iteration 233/1000 | Loss: 0.00001531
Iteration 234/1000 | Loss: 0.00001531
Iteration 235/1000 | Loss: 0.00001531
Iteration 236/1000 | Loss: 0.00001531
Iteration 237/1000 | Loss: 0.00001531
Iteration 238/1000 | Loss: 0.00001531
Iteration 239/1000 | Loss: 0.00001531
Iteration 240/1000 | Loss: 0.00001531
Iteration 241/1000 | Loss: 0.00001531
Iteration 242/1000 | Loss: 0.00001531
Iteration 243/1000 | Loss: 0.00001531
Iteration 244/1000 | Loss: 0.00001531
Iteration 245/1000 | Loss: 0.00001531
Iteration 246/1000 | Loss: 0.00001531
Iteration 247/1000 | Loss: 0.00001531
Iteration 248/1000 | Loss: 0.00001531
Iteration 249/1000 | Loss: 0.00001531
Iteration 250/1000 | Loss: 0.00001531
Iteration 251/1000 | Loss: 0.00001531
Iteration 252/1000 | Loss: 0.00001531
Iteration 253/1000 | Loss: 0.00001531
Iteration 254/1000 | Loss: 0.00001531
Iteration 255/1000 | Loss: 0.00001531
Iteration 256/1000 | Loss: 0.00001531
Iteration 257/1000 | Loss: 0.00001531
Iteration 258/1000 | Loss: 0.00001531
Iteration 259/1000 | Loss: 0.00001531
Iteration 260/1000 | Loss: 0.00001531
Iteration 261/1000 | Loss: 0.00001531
Iteration 262/1000 | Loss: 0.00001531
Iteration 263/1000 | Loss: 0.00001531
Iteration 264/1000 | Loss: 0.00001531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.530898043711204e-05, 1.530898043711204e-05, 1.530898043711204e-05, 1.530898043711204e-05, 1.530898043711204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530898043711204e-05

Optimization complete. Final v2v error: 3.2998948097229004 mm

Highest mean error: 3.6631104946136475 mm for frame 61

Lowest mean error: 3.0020899772644043 mm for frame 98

Saving results

Total time: 41.392218589782715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395731
Iteration 2/25 | Loss: 0.00086493
Iteration 3/25 | Loss: 0.00076374
Iteration 4/25 | Loss: 0.00073831
Iteration 5/25 | Loss: 0.00073410
Iteration 6/25 | Loss: 0.00073291
Iteration 7/25 | Loss: 0.00073291
Iteration 8/25 | Loss: 0.00073291
Iteration 9/25 | Loss: 0.00073291
Iteration 10/25 | Loss: 0.00073291
Iteration 11/25 | Loss: 0.00073291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007329120999202132, 0.0007329120999202132, 0.0007329120999202132, 0.0007329120999202132, 0.0007329120999202132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007329120999202132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06089401
Iteration 2/25 | Loss: 0.00119548
Iteration 3/25 | Loss: 0.00119548
Iteration 4/25 | Loss: 0.00119548
Iteration 5/25 | Loss: 0.00119548
Iteration 6/25 | Loss: 0.00119547
Iteration 7/25 | Loss: 0.00119547
Iteration 8/25 | Loss: 0.00119547
Iteration 9/25 | Loss: 0.00119547
Iteration 10/25 | Loss: 0.00119547
Iteration 11/25 | Loss: 0.00119547
Iteration 12/25 | Loss: 0.00119547
Iteration 13/25 | Loss: 0.00119547
Iteration 14/25 | Loss: 0.00119547
Iteration 15/25 | Loss: 0.00119547
Iteration 16/25 | Loss: 0.00119547
Iteration 17/25 | Loss: 0.00119547
Iteration 18/25 | Loss: 0.00119547
Iteration 19/25 | Loss: 0.00119547
Iteration 20/25 | Loss: 0.00119547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011954738292843103, 0.0011954738292843103, 0.0011954738292843103, 0.0011954738292843103, 0.0011954738292843103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011954738292843103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119547
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001535
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001414
Iteration 7/1000 | Loss: 0.00001388
Iteration 8/1000 | Loss: 0.00001366
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001343
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001341
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001339
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001337
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001333
Iteration 25/1000 | Loss: 0.00001333
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001309
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001306
Iteration 77/1000 | Loss: 0.00001306
Iteration 78/1000 | Loss: 0.00001306
Iteration 79/1000 | Loss: 0.00001306
Iteration 80/1000 | Loss: 0.00001306
Iteration 81/1000 | Loss: 0.00001306
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001305
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001303
Iteration 92/1000 | Loss: 0.00001303
Iteration 93/1000 | Loss: 0.00001302
Iteration 94/1000 | Loss: 0.00001302
Iteration 95/1000 | Loss: 0.00001302
Iteration 96/1000 | Loss: 0.00001302
Iteration 97/1000 | Loss: 0.00001302
Iteration 98/1000 | Loss: 0.00001301
Iteration 99/1000 | Loss: 0.00001301
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001300
Iteration 104/1000 | Loss: 0.00001300
Iteration 105/1000 | Loss: 0.00001300
Iteration 106/1000 | Loss: 0.00001300
Iteration 107/1000 | Loss: 0.00001299
Iteration 108/1000 | Loss: 0.00001299
Iteration 109/1000 | Loss: 0.00001299
Iteration 110/1000 | Loss: 0.00001299
Iteration 111/1000 | Loss: 0.00001299
Iteration 112/1000 | Loss: 0.00001299
Iteration 113/1000 | Loss: 0.00001299
Iteration 114/1000 | Loss: 0.00001299
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001299
Iteration 119/1000 | Loss: 0.00001299
Iteration 120/1000 | Loss: 0.00001299
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001299
Iteration 124/1000 | Loss: 0.00001299
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001298
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001297
Iteration 145/1000 | Loss: 0.00001297
Iteration 146/1000 | Loss: 0.00001297
Iteration 147/1000 | Loss: 0.00001297
Iteration 148/1000 | Loss: 0.00001297
Iteration 149/1000 | Loss: 0.00001297
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001296
Iteration 153/1000 | Loss: 0.00001296
Iteration 154/1000 | Loss: 0.00001296
Iteration 155/1000 | Loss: 0.00001296
Iteration 156/1000 | Loss: 0.00001296
Iteration 157/1000 | Loss: 0.00001296
Iteration 158/1000 | Loss: 0.00001296
Iteration 159/1000 | Loss: 0.00001296
Iteration 160/1000 | Loss: 0.00001295
Iteration 161/1000 | Loss: 0.00001295
Iteration 162/1000 | Loss: 0.00001295
Iteration 163/1000 | Loss: 0.00001295
Iteration 164/1000 | Loss: 0.00001295
Iteration 165/1000 | Loss: 0.00001295
Iteration 166/1000 | Loss: 0.00001295
Iteration 167/1000 | Loss: 0.00001295
Iteration 168/1000 | Loss: 0.00001295
Iteration 169/1000 | Loss: 0.00001295
Iteration 170/1000 | Loss: 0.00001295
Iteration 171/1000 | Loss: 0.00001295
Iteration 172/1000 | Loss: 0.00001295
Iteration 173/1000 | Loss: 0.00001295
Iteration 174/1000 | Loss: 0.00001295
Iteration 175/1000 | Loss: 0.00001294
Iteration 176/1000 | Loss: 0.00001294
Iteration 177/1000 | Loss: 0.00001294
Iteration 178/1000 | Loss: 0.00001294
Iteration 179/1000 | Loss: 0.00001294
Iteration 180/1000 | Loss: 0.00001294
Iteration 181/1000 | Loss: 0.00001294
Iteration 182/1000 | Loss: 0.00001294
Iteration 183/1000 | Loss: 0.00001294
Iteration 184/1000 | Loss: 0.00001294
Iteration 185/1000 | Loss: 0.00001294
Iteration 186/1000 | Loss: 0.00001294
Iteration 187/1000 | Loss: 0.00001294
Iteration 188/1000 | Loss: 0.00001293
Iteration 189/1000 | Loss: 0.00001293
Iteration 190/1000 | Loss: 0.00001293
Iteration 191/1000 | Loss: 0.00001293
Iteration 192/1000 | Loss: 0.00001293
Iteration 193/1000 | Loss: 0.00001293
Iteration 194/1000 | Loss: 0.00001293
Iteration 195/1000 | Loss: 0.00001293
Iteration 196/1000 | Loss: 0.00001293
Iteration 197/1000 | Loss: 0.00001293
Iteration 198/1000 | Loss: 0.00001293
Iteration 199/1000 | Loss: 0.00001293
Iteration 200/1000 | Loss: 0.00001293
Iteration 201/1000 | Loss: 0.00001293
Iteration 202/1000 | Loss: 0.00001293
Iteration 203/1000 | Loss: 0.00001293
Iteration 204/1000 | Loss: 0.00001293
Iteration 205/1000 | Loss: 0.00001293
Iteration 206/1000 | Loss: 0.00001293
Iteration 207/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.2932492609252222e-05, 1.2932492609252222e-05, 1.2932492609252222e-05, 1.2932492609252222e-05, 1.2932492609252222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2932492609252222e-05

Optimization complete. Final v2v error: 3.079629898071289 mm

Highest mean error: 3.276276111602783 mm for frame 114

Lowest mean error: 2.9403176307678223 mm for frame 141

Saving results

Total time: 41.055954933166504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708220
Iteration 2/25 | Loss: 0.00159338
Iteration 3/25 | Loss: 0.00110235
Iteration 4/25 | Loss: 0.00094732
Iteration 5/25 | Loss: 0.00088639
Iteration 6/25 | Loss: 0.00087779
Iteration 7/25 | Loss: 0.00086585
Iteration 8/25 | Loss: 0.00086329
Iteration 9/25 | Loss: 0.00086849
Iteration 10/25 | Loss: 0.00086806
Iteration 11/25 | Loss: 0.00086291
Iteration 12/25 | Loss: 0.00086229
Iteration 13/25 | Loss: 0.00086197
Iteration 14/25 | Loss: 0.00086048
Iteration 15/25 | Loss: 0.00085491
Iteration 16/25 | Loss: 0.00085000
Iteration 17/25 | Loss: 0.00084870
Iteration 18/25 | Loss: 0.00084798
Iteration 19/25 | Loss: 0.00084772
Iteration 20/25 | Loss: 0.00084772
Iteration 21/25 | Loss: 0.00084772
Iteration 22/25 | Loss: 0.00084772
Iteration 23/25 | Loss: 0.00084772
Iteration 24/25 | Loss: 0.00084772
Iteration 25/25 | Loss: 0.00084772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47010756
Iteration 2/25 | Loss: 0.00124722
Iteration 3/25 | Loss: 0.00124717
Iteration 4/25 | Loss: 0.00124717
Iteration 5/25 | Loss: 0.00124717
Iteration 6/25 | Loss: 0.00124717
Iteration 7/25 | Loss: 0.00124717
Iteration 8/25 | Loss: 0.00124717
Iteration 9/25 | Loss: 0.00124717
Iteration 10/25 | Loss: 0.00124717
Iteration 11/25 | Loss: 0.00124717
Iteration 12/25 | Loss: 0.00124717
Iteration 13/25 | Loss: 0.00124717
Iteration 14/25 | Loss: 0.00124717
Iteration 15/25 | Loss: 0.00124717
Iteration 16/25 | Loss: 0.00124717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012471693335101008, 0.0012471693335101008, 0.0012471693335101008, 0.0012471693335101008, 0.0012471693335101008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012471693335101008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124717
Iteration 2/1000 | Loss: 0.00003838
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00002369
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002098
Iteration 7/1000 | Loss: 0.00002035
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001953
Iteration 10/1000 | Loss: 0.00014347
Iteration 11/1000 | Loss: 0.00013839
Iteration 12/1000 | Loss: 0.00011090
Iteration 13/1000 | Loss: 0.00013296
Iteration 14/1000 | Loss: 0.00009303
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001862
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001723
Iteration 20/1000 | Loss: 0.00001705
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001699
Iteration 23/1000 | Loss: 0.00001695
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00001681
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001676
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001673
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001669
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001667
Iteration 59/1000 | Loss: 0.00001667
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001666
Iteration 62/1000 | Loss: 0.00001666
Iteration 63/1000 | Loss: 0.00001666
Iteration 64/1000 | Loss: 0.00001666
Iteration 65/1000 | Loss: 0.00001666
Iteration 66/1000 | Loss: 0.00001665
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001665
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001665
Iteration 71/1000 | Loss: 0.00001665
Iteration 72/1000 | Loss: 0.00001665
Iteration 73/1000 | Loss: 0.00001665
Iteration 74/1000 | Loss: 0.00001665
Iteration 75/1000 | Loss: 0.00001665
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001665
Iteration 85/1000 | Loss: 0.00001665
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.66471963893855e-05, 1.66471963893855e-05, 1.66471963893855e-05, 1.66471963893855e-05, 1.66471963893855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.66471963893855e-05

Optimization complete. Final v2v error: 3.465651512145996 mm

Highest mean error: 4.215455532073975 mm for frame 18

Lowest mean error: 2.9809553623199463 mm for frame 132

Saving results

Total time: 70.72503924369812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924892
Iteration 2/25 | Loss: 0.00402701
Iteration 3/25 | Loss: 0.00271956
Iteration 4/25 | Loss: 0.00245784
Iteration 5/25 | Loss: 0.00212103
Iteration 6/25 | Loss: 0.00187518
Iteration 7/25 | Loss: 0.00176851
Iteration 8/25 | Loss: 0.00168480
Iteration 9/25 | Loss: 0.00164409
Iteration 10/25 | Loss: 0.00165726
Iteration 11/25 | Loss: 0.00161581
Iteration 12/25 | Loss: 0.00171693
Iteration 13/25 | Loss: 0.00176839
Iteration 14/25 | Loss: 0.00126090
Iteration 15/25 | Loss: 0.00098356
Iteration 16/25 | Loss: 0.00091197
Iteration 17/25 | Loss: 0.00088410
Iteration 18/25 | Loss: 0.00086396
Iteration 19/25 | Loss: 0.00083741
Iteration 20/25 | Loss: 0.00083889
Iteration 21/25 | Loss: 0.00082949
Iteration 22/25 | Loss: 0.00082526
Iteration 23/25 | Loss: 0.00082260
Iteration 24/25 | Loss: 0.00081979
Iteration 25/25 | Loss: 0.00082024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59101701
Iteration 2/25 | Loss: 0.00122980
Iteration 3/25 | Loss: 0.00122980
Iteration 4/25 | Loss: 0.00122980
Iteration 5/25 | Loss: 0.00122980
Iteration 6/25 | Loss: 0.00123378
Iteration 7/25 | Loss: 0.00123378
Iteration 8/25 | Loss: 0.00123378
Iteration 9/25 | Loss: 0.00123378
Iteration 10/25 | Loss: 0.00122980
Iteration 11/25 | Loss: 0.00122980
Iteration 12/25 | Loss: 0.00122980
Iteration 13/25 | Loss: 0.00122980
Iteration 14/25 | Loss: 0.00122980
Iteration 15/25 | Loss: 0.00122980
Iteration 16/25 | Loss: 0.00122980
Iteration 17/25 | Loss: 0.00122980
Iteration 18/25 | Loss: 0.00122980
Iteration 19/25 | Loss: 0.00122980
Iteration 20/25 | Loss: 0.00122980
Iteration 21/25 | Loss: 0.00122980
Iteration 22/25 | Loss: 0.00122980
Iteration 23/25 | Loss: 0.00122980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012297985376790166, 0.0012297985376790166, 0.0012297985376790166, 0.0012297985376790166, 0.0012297985376790166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012297985376790166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122980
Iteration 2/1000 | Loss: 0.00007927
Iteration 3/1000 | Loss: 0.00018001
Iteration 4/1000 | Loss: 0.00030111
Iteration 5/1000 | Loss: 0.00008332
Iteration 6/1000 | Loss: 0.00026520
Iteration 7/1000 | Loss: 0.00015242
Iteration 8/1000 | Loss: 0.00008030
Iteration 9/1000 | Loss: 0.00005817
Iteration 10/1000 | Loss: 0.00019021
Iteration 11/1000 | Loss: 0.00005417
Iteration 12/1000 | Loss: 0.00003772
Iteration 13/1000 | Loss: 0.00015221
Iteration 14/1000 | Loss: 0.00005814
Iteration 15/1000 | Loss: 0.00003553
Iteration 16/1000 | Loss: 0.00002705
Iteration 17/1000 | Loss: 0.00003648
Iteration 18/1000 | Loss: 0.00002091
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001905
Iteration 22/1000 | Loss: 0.00006348
Iteration 23/1000 | Loss: 0.00004608
Iteration 24/1000 | Loss: 0.00002148
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00002253
Iteration 27/1000 | Loss: 0.00001809
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001763
Iteration 34/1000 | Loss: 0.00003317
Iteration 35/1000 | Loss: 0.00003317
Iteration 36/1000 | Loss: 0.00005689
Iteration 37/1000 | Loss: 0.00003252
Iteration 38/1000 | Loss: 0.00007605
Iteration 39/1000 | Loss: 0.00020220
Iteration 40/1000 | Loss: 0.00005232
Iteration 41/1000 | Loss: 0.00002107
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001752
Iteration 52/1000 | Loss: 0.00001752
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001753
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001749
Iteration 65/1000 | Loss: 0.00001749
Iteration 66/1000 | Loss: 0.00001749
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001747
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001746
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001743
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001748
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001750
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001745
Iteration 103/1000 | Loss: 0.00001745
Iteration 104/1000 | Loss: 0.00001745
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001741
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001748
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001744
Iteration 116/1000 | Loss: 0.00001746
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001746
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001743
Iteration 122/1000 | Loss: 0.00001743
Iteration 123/1000 | Loss: 0.00001742
Iteration 124/1000 | Loss: 0.00001742
Iteration 125/1000 | Loss: 0.00001739
Iteration 126/1000 | Loss: 0.00001739
Iteration 127/1000 | Loss: 0.00001741
Iteration 128/1000 | Loss: 0.00001741
Iteration 129/1000 | Loss: 0.00001738
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001738
Iteration 137/1000 | Loss: 0.00001738
Iteration 138/1000 | Loss: 0.00001738
Iteration 139/1000 | Loss: 0.00001738
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001738
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001737
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001736
Iteration 157/1000 | Loss: 0.00001736
Iteration 158/1000 | Loss: 0.00001736
Iteration 159/1000 | Loss: 0.00001736
Iteration 160/1000 | Loss: 0.00001736
Iteration 161/1000 | Loss: 0.00001735
Iteration 162/1000 | Loss: 0.00001735
Iteration 163/1000 | Loss: 0.00001735
Iteration 164/1000 | Loss: 0.00001735
Iteration 165/1000 | Loss: 0.00001735
Iteration 166/1000 | Loss: 0.00001748
Iteration 167/1000 | Loss: 0.00001748
Iteration 168/1000 | Loss: 0.00001748
Iteration 169/1000 | Loss: 0.00001748
Iteration 170/1000 | Loss: 0.00001748
Iteration 171/1000 | Loss: 0.00001747
Iteration 172/1000 | Loss: 0.00001747
Iteration 173/1000 | Loss: 0.00001747
Iteration 174/1000 | Loss: 0.00001746
Iteration 175/1000 | Loss: 0.00001746
Iteration 176/1000 | Loss: 0.00001746
Iteration 177/1000 | Loss: 0.00001746
Iteration 178/1000 | Loss: 0.00001746
Iteration 179/1000 | Loss: 0.00001746
Iteration 180/1000 | Loss: 0.00001745
Iteration 181/1000 | Loss: 0.00001744
Iteration 182/1000 | Loss: 0.00001744
Iteration 183/1000 | Loss: 0.00001748
Iteration 184/1000 | Loss: 0.00001748
Iteration 185/1000 | Loss: 0.00001737
Iteration 186/1000 | Loss: 0.00001734
Iteration 187/1000 | Loss: 0.00001733
Iteration 188/1000 | Loss: 0.00001733
Iteration 189/1000 | Loss: 0.00001735
Iteration 190/1000 | Loss: 0.00001733
Iteration 191/1000 | Loss: 0.00001733
Iteration 192/1000 | Loss: 0.00001733
Iteration 193/1000 | Loss: 0.00001733
Iteration 194/1000 | Loss: 0.00001733
Iteration 195/1000 | Loss: 0.00001733
Iteration 196/1000 | Loss: 0.00001733
Iteration 197/1000 | Loss: 0.00001733
Iteration 198/1000 | Loss: 0.00001732
Iteration 199/1000 | Loss: 0.00001732
Iteration 200/1000 | Loss: 0.00001733
Iteration 201/1000 | Loss: 0.00001733
Iteration 202/1000 | Loss: 0.00001733
Iteration 203/1000 | Loss: 0.00001733
Iteration 204/1000 | Loss: 0.00001733
Iteration 205/1000 | Loss: 0.00001733
Iteration 206/1000 | Loss: 0.00001733
Iteration 207/1000 | Loss: 0.00001732
Iteration 208/1000 | Loss: 0.00001732
Iteration 209/1000 | Loss: 0.00001732
Iteration 210/1000 | Loss: 0.00001732
Iteration 211/1000 | Loss: 0.00001731
Iteration 212/1000 | Loss: 0.00001731
Iteration 213/1000 | Loss: 0.00001731
Iteration 214/1000 | Loss: 0.00001731
Iteration 215/1000 | Loss: 0.00001731
Iteration 216/1000 | Loss: 0.00001731
Iteration 217/1000 | Loss: 0.00001731
Iteration 218/1000 | Loss: 0.00001731
Iteration 219/1000 | Loss: 0.00001730
Iteration 220/1000 | Loss: 0.00001731
Iteration 221/1000 | Loss: 0.00001731
Iteration 222/1000 | Loss: 0.00001731
Iteration 223/1000 | Loss: 0.00001731
Iteration 224/1000 | Loss: 0.00001731
Iteration 225/1000 | Loss: 0.00001731
Iteration 226/1000 | Loss: 0.00001731
Iteration 227/1000 | Loss: 0.00001731
Iteration 228/1000 | Loss: 0.00001731
Iteration 229/1000 | Loss: 0.00001731
Iteration 230/1000 | Loss: 0.00001731
Iteration 231/1000 | Loss: 0.00001731
Iteration 232/1000 | Loss: 0.00001731
Iteration 233/1000 | Loss: 0.00001731
Iteration 234/1000 | Loss: 0.00001743
Iteration 235/1000 | Loss: 0.00001743
Iteration 236/1000 | Loss: 0.00001743
Iteration 237/1000 | Loss: 0.00001743
Iteration 238/1000 | Loss: 0.00001743
Iteration 239/1000 | Loss: 0.00001743
Iteration 240/1000 | Loss: 0.00001742
Iteration 241/1000 | Loss: 0.00001742
Iteration 242/1000 | Loss: 0.00001742
Iteration 243/1000 | Loss: 0.00001741
Iteration 244/1000 | Loss: 0.00001741
Iteration 245/1000 | Loss: 0.00001741
Iteration 246/1000 | Loss: 0.00001740
Iteration 247/1000 | Loss: 0.00001739
Iteration 248/1000 | Loss: 0.00001738
Iteration 249/1000 | Loss: 0.00001737
Iteration 250/1000 | Loss: 0.00001733
Iteration 251/1000 | Loss: 0.00001733
Iteration 252/1000 | Loss: 0.00001731
Iteration 253/1000 | Loss: 0.00001731
Iteration 254/1000 | Loss: 0.00001742
Iteration 255/1000 | Loss: 0.00001736
Iteration 256/1000 | Loss: 0.00001738
Iteration 257/1000 | Loss: 0.00001734
Iteration 258/1000 | Loss: 0.00001729
Iteration 259/1000 | Loss: 0.00001729
Iteration 260/1000 | Loss: 0.00001728
Iteration 261/1000 | Loss: 0.00001728
Iteration 262/1000 | Loss: 0.00001728
Iteration 263/1000 | Loss: 0.00001728
Iteration 264/1000 | Loss: 0.00001728
Iteration 265/1000 | Loss: 0.00001728
Iteration 266/1000 | Loss: 0.00001728
Iteration 267/1000 | Loss: 0.00001728
Iteration 268/1000 | Loss: 0.00001727
Iteration 269/1000 | Loss: 0.00001727
Iteration 270/1000 | Loss: 0.00001727
Iteration 271/1000 | Loss: 0.00001738
Iteration 272/1000 | Loss: 0.00001738
Iteration 273/1000 | Loss: 0.00001734
Iteration 274/1000 | Loss: 0.00001728
Iteration 275/1000 | Loss: 0.00001728
Iteration 276/1000 | Loss: 0.00001739
Iteration 277/1000 | Loss: 0.00001738
Iteration 278/1000 | Loss: 0.00001740
Iteration 279/1000 | Loss: 0.00001737
Iteration 280/1000 | Loss: 0.00001746
Iteration 281/1000 | Loss: 0.00001744
Iteration 282/1000 | Loss: 0.00001754
Iteration 283/1000 | Loss: 0.00001754
Iteration 284/1000 | Loss: 0.00001731
Iteration 285/1000 | Loss: 0.00001731
Iteration 286/1000 | Loss: 0.00001731
Iteration 287/1000 | Loss: 0.00001731
Iteration 288/1000 | Loss: 0.00001731
Iteration 289/1000 | Loss: 0.00001731
Iteration 290/1000 | Loss: 0.00001731
Iteration 291/1000 | Loss: 0.00001730
Iteration 292/1000 | Loss: 0.00001730
Iteration 293/1000 | Loss: 0.00001730
Iteration 294/1000 | Loss: 0.00001730
Iteration 295/1000 | Loss: 0.00001730
Iteration 296/1000 | Loss: 0.00001729
Iteration 297/1000 | Loss: 0.00001729
Iteration 298/1000 | Loss: 0.00001729
Iteration 299/1000 | Loss: 0.00001729
Iteration 300/1000 | Loss: 0.00001729
Iteration 301/1000 | Loss: 0.00001729
Iteration 302/1000 | Loss: 0.00001729
Iteration 303/1000 | Loss: 0.00001729
Iteration 304/1000 | Loss: 0.00001729
Iteration 305/1000 | Loss: 0.00001729
Iteration 306/1000 | Loss: 0.00001729
Iteration 307/1000 | Loss: 0.00001729
Iteration 308/1000 | Loss: 0.00001729
Iteration 309/1000 | Loss: 0.00001729
Iteration 310/1000 | Loss: 0.00001729
Iteration 311/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.7288741219090298e-05, 1.7288741219090298e-05, 1.7288741219090298e-05, 1.7288741219090298e-05, 1.7288741219090298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7288741219090298e-05

Optimization complete. Final v2v error: 3.527742862701416 mm

Highest mean error: 4.365097999572754 mm for frame 52

Lowest mean error: 3.435310125350952 mm for frame 10

Saving results

Total time: 143.1282331943512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887774
Iteration 2/25 | Loss: 0.00116706
Iteration 3/25 | Loss: 0.00087421
Iteration 4/25 | Loss: 0.00083109
Iteration 5/25 | Loss: 0.00081350
Iteration 6/25 | Loss: 0.00080772
Iteration 7/25 | Loss: 0.00080552
Iteration 8/25 | Loss: 0.00080452
Iteration 9/25 | Loss: 0.00080426
Iteration 10/25 | Loss: 0.00080426
Iteration 11/25 | Loss: 0.00080426
Iteration 12/25 | Loss: 0.00080426
Iteration 13/25 | Loss: 0.00080426
Iteration 14/25 | Loss: 0.00080426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008042597328312695, 0.0008042597328312695, 0.0008042597328312695, 0.0008042597328312695, 0.0008042597328312695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008042597328312695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61860251
Iteration 2/25 | Loss: 0.00113111
Iteration 3/25 | Loss: 0.00113110
Iteration 4/25 | Loss: 0.00113110
Iteration 5/25 | Loss: 0.00113110
Iteration 6/25 | Loss: 0.00113110
Iteration 7/25 | Loss: 0.00113110
Iteration 8/25 | Loss: 0.00113110
Iteration 9/25 | Loss: 0.00113110
Iteration 10/25 | Loss: 0.00113110
Iteration 11/25 | Loss: 0.00113110
Iteration 12/25 | Loss: 0.00113110
Iteration 13/25 | Loss: 0.00113110
Iteration 14/25 | Loss: 0.00113110
Iteration 15/25 | Loss: 0.00113110
Iteration 16/25 | Loss: 0.00113110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011311027919873595, 0.0011311027919873595, 0.0011311027919873595, 0.0011311027919873595, 0.0011311027919873595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011311027919873595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113110
Iteration 2/1000 | Loss: 0.00004728
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002401
Iteration 5/1000 | Loss: 0.00002249
Iteration 6/1000 | Loss: 0.00002156
Iteration 7/1000 | Loss: 0.00002095
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001947
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001919
Iteration 14/1000 | Loss: 0.00001917
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001884
Iteration 18/1000 | Loss: 0.00001883
Iteration 19/1000 | Loss: 0.00001882
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001880
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001879
Iteration 25/1000 | Loss: 0.00001878
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001863
Iteration 60/1000 | Loss: 0.00001863
Iteration 61/1000 | Loss: 0.00001863
Iteration 62/1000 | Loss: 0.00001863
Iteration 63/1000 | Loss: 0.00001862
Iteration 64/1000 | Loss: 0.00001862
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001862
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001860
Iteration 78/1000 | Loss: 0.00001860
Iteration 79/1000 | Loss: 0.00001860
Iteration 80/1000 | Loss: 0.00001860
Iteration 81/1000 | Loss: 0.00001860
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001854
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001854
Iteration 129/1000 | Loss: 0.00001854
Iteration 130/1000 | Loss: 0.00001854
Iteration 131/1000 | Loss: 0.00001854
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001853
Iteration 141/1000 | Loss: 0.00001853
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001851
Iteration 155/1000 | Loss: 0.00001851
Iteration 156/1000 | Loss: 0.00001851
Iteration 157/1000 | Loss: 0.00001851
Iteration 158/1000 | Loss: 0.00001851
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.85080170922447e-05, 1.85080170922447e-05, 1.85080170922447e-05, 1.85080170922447e-05, 1.85080170922447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.85080170922447e-05

Optimization complete. Final v2v error: 3.544203042984009 mm

Highest mean error: 5.966065883636475 mm for frame 70

Lowest mean error: 2.6883976459503174 mm for frame 128

Saving results

Total time: 46.16814422607422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004439
Iteration 2/25 | Loss: 0.00302145
Iteration 3/25 | Loss: 0.00125727
Iteration 4/25 | Loss: 0.00110215
Iteration 5/25 | Loss: 0.00095138
Iteration 6/25 | Loss: 0.00087848
Iteration 7/25 | Loss: 0.00088473
Iteration 8/25 | Loss: 0.00087223
Iteration 9/25 | Loss: 0.00083789
Iteration 10/25 | Loss: 0.00082958
Iteration 11/25 | Loss: 0.00083069
Iteration 12/25 | Loss: 0.00082167
Iteration 13/25 | Loss: 0.00081500
Iteration 14/25 | Loss: 0.00081529
Iteration 15/25 | Loss: 0.00081853
Iteration 16/25 | Loss: 0.00081258
Iteration 17/25 | Loss: 0.00081075
Iteration 18/25 | Loss: 0.00080911
Iteration 19/25 | Loss: 0.00081178
Iteration 20/25 | Loss: 0.00080838
Iteration 21/25 | Loss: 0.00080518
Iteration 22/25 | Loss: 0.00080480
Iteration 23/25 | Loss: 0.00080467
Iteration 24/25 | Loss: 0.00080467
Iteration 25/25 | Loss: 0.00080467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08263588
Iteration 2/25 | Loss: 0.00124311
Iteration 3/25 | Loss: 0.00120606
Iteration 4/25 | Loss: 0.00120606
Iteration 5/25 | Loss: 0.00120606
Iteration 6/25 | Loss: 0.00120606
Iteration 7/25 | Loss: 0.00120606
Iteration 8/25 | Loss: 0.00120606
Iteration 9/25 | Loss: 0.00120606
Iteration 10/25 | Loss: 0.00120606
Iteration 11/25 | Loss: 0.00120606
Iteration 12/25 | Loss: 0.00120606
Iteration 13/25 | Loss: 0.00120606
Iteration 14/25 | Loss: 0.00120606
Iteration 15/25 | Loss: 0.00120606
Iteration 16/25 | Loss: 0.00120606
Iteration 17/25 | Loss: 0.00120606
Iteration 18/25 | Loss: 0.00120606
Iteration 19/25 | Loss: 0.00120606
Iteration 20/25 | Loss: 0.00120606
Iteration 21/25 | Loss: 0.00120606
Iteration 22/25 | Loss: 0.00120606
Iteration 23/25 | Loss: 0.00120606
Iteration 24/25 | Loss: 0.00120606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012060599401593208, 0.0012060599401593208, 0.0012060599401593208, 0.0012060599401593208, 0.0012060599401593208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012060599401593208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120606
Iteration 2/1000 | Loss: 0.00003845
Iteration 3/1000 | Loss: 0.00007068
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00004998
Iteration 6/1000 | Loss: 0.00002244
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00005690
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001959
Iteration 12/1000 | Loss: 0.00009117
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001897
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001889
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00006981
Iteration 22/1000 | Loss: 0.00002888
Iteration 23/1000 | Loss: 0.00003262
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001876
Iteration 28/1000 | Loss: 0.00001876
Iteration 29/1000 | Loss: 0.00001876
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00001874
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001871
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00006296
Iteration 71/1000 | Loss: 0.00007410
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00003583
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001873
Iteration 76/1000 | Loss: 0.00001873
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001872
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001870
Iteration 85/1000 | Loss: 0.00001870
Iteration 86/1000 | Loss: 0.00001869
Iteration 87/1000 | Loss: 0.00001869
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001868
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00001868
Iteration 93/1000 | Loss: 0.00001868
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00001867
Iteration 97/1000 | Loss: 0.00002156
Iteration 98/1000 | Loss: 0.00001913
Iteration 99/1000 | Loss: 0.00001868
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001870
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001869
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001868
Iteration 107/1000 | Loss: 0.00001867
Iteration 108/1000 | Loss: 0.00001867
Iteration 109/1000 | Loss: 0.00001867
Iteration 110/1000 | Loss: 0.00001867
Iteration 111/1000 | Loss: 0.00001867
Iteration 112/1000 | Loss: 0.00001867
Iteration 113/1000 | Loss: 0.00001867
Iteration 114/1000 | Loss: 0.00001866
Iteration 115/1000 | Loss: 0.00001866
Iteration 116/1000 | Loss: 0.00001865
Iteration 117/1000 | Loss: 0.00002402
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001865
Iteration 122/1000 | Loss: 0.00001865
Iteration 123/1000 | Loss: 0.00001865
Iteration 124/1000 | Loss: 0.00001865
Iteration 125/1000 | Loss: 0.00001865
Iteration 126/1000 | Loss: 0.00001864
Iteration 127/1000 | Loss: 0.00001864
Iteration 128/1000 | Loss: 0.00001863
Iteration 129/1000 | Loss: 0.00001863
Iteration 130/1000 | Loss: 0.00001863
Iteration 131/1000 | Loss: 0.00001863
Iteration 132/1000 | Loss: 0.00001863
Iteration 133/1000 | Loss: 0.00001863
Iteration 134/1000 | Loss: 0.00001862
Iteration 135/1000 | Loss: 0.00001862
Iteration 136/1000 | Loss: 0.00001862
Iteration 137/1000 | Loss: 0.00001862
Iteration 138/1000 | Loss: 0.00001862
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001861
Iteration 143/1000 | Loss: 0.00001861
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001861
Iteration 147/1000 | Loss: 0.00001860
Iteration 148/1000 | Loss: 0.00001860
Iteration 149/1000 | Loss: 0.00001860
Iteration 150/1000 | Loss: 0.00001860
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001859
Iteration 153/1000 | Loss: 0.00001859
Iteration 154/1000 | Loss: 0.00001859
Iteration 155/1000 | Loss: 0.00001859
Iteration 156/1000 | Loss: 0.00001858
Iteration 157/1000 | Loss: 0.00001858
Iteration 158/1000 | Loss: 0.00001858
Iteration 159/1000 | Loss: 0.00003220
Iteration 160/1000 | Loss: 0.00001903
Iteration 161/1000 | Loss: 0.00001861
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001860
Iteration 165/1000 | Loss: 0.00001860
Iteration 166/1000 | Loss: 0.00001860
Iteration 167/1000 | Loss: 0.00001860
Iteration 168/1000 | Loss: 0.00001860
Iteration 169/1000 | Loss: 0.00001859
Iteration 170/1000 | Loss: 0.00001858
Iteration 171/1000 | Loss: 0.00001858
Iteration 172/1000 | Loss: 0.00001857
Iteration 173/1000 | Loss: 0.00001857
Iteration 174/1000 | Loss: 0.00001857
Iteration 175/1000 | Loss: 0.00001856
Iteration 176/1000 | Loss: 0.00003401
Iteration 177/1000 | Loss: 0.00001870
Iteration 178/1000 | Loss: 0.00001870
Iteration 179/1000 | Loss: 0.00001869
Iteration 180/1000 | Loss: 0.00001865
Iteration 181/1000 | Loss: 0.00001864
Iteration 182/1000 | Loss: 0.00001863
Iteration 183/1000 | Loss: 0.00001863
Iteration 184/1000 | Loss: 0.00001862
Iteration 185/1000 | Loss: 0.00001862
Iteration 186/1000 | Loss: 0.00001862
Iteration 187/1000 | Loss: 0.00001862
Iteration 188/1000 | Loss: 0.00001861
Iteration 189/1000 | Loss: 0.00001861
Iteration 190/1000 | Loss: 0.00001861
Iteration 191/1000 | Loss: 0.00001860
Iteration 192/1000 | Loss: 0.00001860
Iteration 193/1000 | Loss: 0.00001859
Iteration 194/1000 | Loss: 0.00001856
Iteration 195/1000 | Loss: 0.00001855
Iteration 196/1000 | Loss: 0.00001855
Iteration 197/1000 | Loss: 0.00001855
Iteration 198/1000 | Loss: 0.00001855
Iteration 199/1000 | Loss: 0.00001855
Iteration 200/1000 | Loss: 0.00001855
Iteration 201/1000 | Loss: 0.00001855
Iteration 202/1000 | Loss: 0.00001854
Iteration 203/1000 | Loss: 0.00001854
Iteration 204/1000 | Loss: 0.00001854
Iteration 205/1000 | Loss: 0.00001854
Iteration 206/1000 | Loss: 0.00001854
Iteration 207/1000 | Loss: 0.00001854
Iteration 208/1000 | Loss: 0.00001854
Iteration 209/1000 | Loss: 0.00001853
Iteration 210/1000 | Loss: 0.00001853
Iteration 211/1000 | Loss: 0.00001853
Iteration 212/1000 | Loss: 0.00001853
Iteration 213/1000 | Loss: 0.00001853
Iteration 214/1000 | Loss: 0.00001853
Iteration 215/1000 | Loss: 0.00001853
Iteration 216/1000 | Loss: 0.00001853
Iteration 217/1000 | Loss: 0.00001853
Iteration 218/1000 | Loss: 0.00001853
Iteration 219/1000 | Loss: 0.00001853
Iteration 220/1000 | Loss: 0.00001853
Iteration 221/1000 | Loss: 0.00001853
Iteration 222/1000 | Loss: 0.00001853
Iteration 223/1000 | Loss: 0.00001853
Iteration 224/1000 | Loss: 0.00001853
Iteration 225/1000 | Loss: 0.00001853
Iteration 226/1000 | Loss: 0.00001853
Iteration 227/1000 | Loss: 0.00001853
Iteration 228/1000 | Loss: 0.00001853
Iteration 229/1000 | Loss: 0.00001853
Iteration 230/1000 | Loss: 0.00001853
Iteration 231/1000 | Loss: 0.00001853
Iteration 232/1000 | Loss: 0.00001853
Iteration 233/1000 | Loss: 0.00001853
Iteration 234/1000 | Loss: 0.00001853
Iteration 235/1000 | Loss: 0.00001853
Iteration 236/1000 | Loss: 0.00001853
Iteration 237/1000 | Loss: 0.00001853
Iteration 238/1000 | Loss: 0.00001853
Iteration 239/1000 | Loss: 0.00001853
Iteration 240/1000 | Loss: 0.00001853
Iteration 241/1000 | Loss: 0.00001853
Iteration 242/1000 | Loss: 0.00001853
Iteration 243/1000 | Loss: 0.00001853
Iteration 244/1000 | Loss: 0.00001853
Iteration 245/1000 | Loss: 0.00001853
Iteration 246/1000 | Loss: 0.00001853
Iteration 247/1000 | Loss: 0.00001853
Iteration 248/1000 | Loss: 0.00001853
Iteration 249/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.8529663066146895e-05, 1.8529663066146895e-05, 1.8529663066146895e-05, 1.8529663066146895e-05, 1.8529663066146895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8529663066146895e-05

Optimization complete. Final v2v error: 3.675283193588257 mm

Highest mean error: 4.254528999328613 mm for frame 211

Lowest mean error: 3.2171859741210938 mm for frame 104

Saving results

Total time: 109.59578323364258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055711
Iteration 2/25 | Loss: 0.00199804
Iteration 3/25 | Loss: 0.00120161
Iteration 4/25 | Loss: 0.00113374
Iteration 5/25 | Loss: 0.00098601
Iteration 6/25 | Loss: 0.00094625
Iteration 7/25 | Loss: 0.00093676
Iteration 8/25 | Loss: 0.00092854
Iteration 9/25 | Loss: 0.00089786
Iteration 10/25 | Loss: 0.00085478
Iteration 11/25 | Loss: 0.00083405
Iteration 12/25 | Loss: 0.00080816
Iteration 13/25 | Loss: 0.00079628
Iteration 14/25 | Loss: 0.00080763
Iteration 15/25 | Loss: 0.00080173
Iteration 16/25 | Loss: 0.00079848
Iteration 17/25 | Loss: 0.00079570
Iteration 18/25 | Loss: 0.00079044
Iteration 19/25 | Loss: 0.00078403
Iteration 20/25 | Loss: 0.00078157
Iteration 21/25 | Loss: 0.00078490
Iteration 22/25 | Loss: 0.00078563
Iteration 23/25 | Loss: 0.00078685
Iteration 24/25 | Loss: 0.00078620
Iteration 25/25 | Loss: 0.00079263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62213278
Iteration 2/25 | Loss: 0.00143778
Iteration 3/25 | Loss: 0.00143778
Iteration 4/25 | Loss: 0.00143778
Iteration 5/25 | Loss: 0.00143778
Iteration 6/25 | Loss: 0.00143778
Iteration 7/25 | Loss: 0.00143778
Iteration 8/25 | Loss: 0.00143778
Iteration 9/25 | Loss: 0.00143778
Iteration 10/25 | Loss: 0.00143778
Iteration 11/25 | Loss: 0.00143778
Iteration 12/25 | Loss: 0.00143778
Iteration 13/25 | Loss: 0.00143778
Iteration 14/25 | Loss: 0.00143778
Iteration 15/25 | Loss: 0.00143778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014377761399373412, 0.0014377761399373412, 0.0014377761399373412, 0.0014377761399373412, 0.0014377761399373412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014377761399373412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143778
Iteration 2/1000 | Loss: 0.00021298
Iteration 3/1000 | Loss: 0.00010602
Iteration 4/1000 | Loss: 0.00057380
Iteration 5/1000 | Loss: 0.00023871
Iteration 6/1000 | Loss: 0.00025164
Iteration 7/1000 | Loss: 0.00044264
Iteration 8/1000 | Loss: 0.00056852
Iteration 9/1000 | Loss: 0.00007047
Iteration 10/1000 | Loss: 0.00058843
Iteration 11/1000 | Loss: 0.00049095
Iteration 12/1000 | Loss: 0.00049309
Iteration 13/1000 | Loss: 0.00046291
Iteration 14/1000 | Loss: 0.00049266
Iteration 15/1000 | Loss: 0.00030428
Iteration 16/1000 | Loss: 0.00046873
Iteration 17/1000 | Loss: 0.00030185
Iteration 18/1000 | Loss: 0.00040398
Iteration 19/1000 | Loss: 0.00037392
Iteration 20/1000 | Loss: 0.00013580
Iteration 21/1000 | Loss: 0.00003803
Iteration 22/1000 | Loss: 0.00023446
Iteration 23/1000 | Loss: 0.00015973
Iteration 24/1000 | Loss: 0.00002978
Iteration 25/1000 | Loss: 0.00022815
Iteration 26/1000 | Loss: 0.00027705
Iteration 27/1000 | Loss: 0.00045618
Iteration 28/1000 | Loss: 0.00034009
Iteration 29/1000 | Loss: 0.00024667
Iteration 30/1000 | Loss: 0.00037324
Iteration 31/1000 | Loss: 0.00051614
Iteration 32/1000 | Loss: 0.00004450
Iteration 33/1000 | Loss: 0.00061039
Iteration 34/1000 | Loss: 0.00051266
Iteration 35/1000 | Loss: 0.00044956
Iteration 36/1000 | Loss: 0.00049012
Iteration 37/1000 | Loss: 0.00033836
Iteration 38/1000 | Loss: 0.00023644
Iteration 39/1000 | Loss: 0.00022697
Iteration 40/1000 | Loss: 0.00022029
Iteration 41/1000 | Loss: 0.00031971
Iteration 42/1000 | Loss: 0.00045178
Iteration 43/1000 | Loss: 0.00010921
Iteration 44/1000 | Loss: 0.00016524
Iteration 45/1000 | Loss: 0.00019321
Iteration 46/1000 | Loss: 0.00050896
Iteration 47/1000 | Loss: 0.00149920
Iteration 48/1000 | Loss: 0.00066403
Iteration 49/1000 | Loss: 0.00017479
Iteration 50/1000 | Loss: 0.00087211
Iteration 51/1000 | Loss: 0.00009920
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00008555
Iteration 54/1000 | Loss: 0.00006601
Iteration 55/1000 | Loss: 0.00015168
Iteration 56/1000 | Loss: 0.00017918
Iteration 57/1000 | Loss: 0.00047248
Iteration 58/1000 | Loss: 0.00002636
Iteration 59/1000 | Loss: 0.00012253
Iteration 60/1000 | Loss: 0.00007896
Iteration 61/1000 | Loss: 0.00006844
Iteration 62/1000 | Loss: 0.00006582
Iteration 63/1000 | Loss: 0.00005428
Iteration 64/1000 | Loss: 0.00006164
Iteration 65/1000 | Loss: 0.00002227
Iteration 66/1000 | Loss: 0.00009786
Iteration 67/1000 | Loss: 0.00011981
Iteration 68/1000 | Loss: 0.00002814
Iteration 69/1000 | Loss: 0.00016761
Iteration 70/1000 | Loss: 0.00037627
Iteration 71/1000 | Loss: 0.00024548
Iteration 72/1000 | Loss: 0.00005191
Iteration 73/1000 | Loss: 0.00015294
Iteration 74/1000 | Loss: 0.00021983
Iteration 75/1000 | Loss: 0.00021239
Iteration 76/1000 | Loss: 0.00020356
Iteration 77/1000 | Loss: 0.00021987
Iteration 78/1000 | Loss: 0.00036893
Iteration 79/1000 | Loss: 0.00006493
Iteration 80/1000 | Loss: 0.00006182
Iteration 81/1000 | Loss: 0.00004777
Iteration 82/1000 | Loss: 0.00078309
Iteration 83/1000 | Loss: 0.00091094
Iteration 84/1000 | Loss: 0.00036142
Iteration 85/1000 | Loss: 0.00004801
Iteration 86/1000 | Loss: 0.00002869
Iteration 87/1000 | Loss: 0.00004587
Iteration 88/1000 | Loss: 0.00008888
Iteration 89/1000 | Loss: 0.00002998
Iteration 90/1000 | Loss: 0.00005401
Iteration 91/1000 | Loss: 0.00002474
Iteration 92/1000 | Loss: 0.00005102
Iteration 93/1000 | Loss: 0.00005438
Iteration 94/1000 | Loss: 0.00005184
Iteration 95/1000 | Loss: 0.00005512
Iteration 96/1000 | Loss: 0.00005422
Iteration 97/1000 | Loss: 0.00005494
Iteration 98/1000 | Loss: 0.00007385
Iteration 99/1000 | Loss: 0.00002834
Iteration 100/1000 | Loss: 0.00002655
Iteration 101/1000 | Loss: 0.00002585
Iteration 102/1000 | Loss: 0.00002497
Iteration 103/1000 | Loss: 0.00086207
Iteration 104/1000 | Loss: 0.00129846
Iteration 105/1000 | Loss: 0.00047748
Iteration 106/1000 | Loss: 0.00112696
Iteration 107/1000 | Loss: 0.00100833
Iteration 108/1000 | Loss: 0.00061038
Iteration 109/1000 | Loss: 0.00138739
Iteration 110/1000 | Loss: 0.00112020
Iteration 111/1000 | Loss: 0.00079618
Iteration 112/1000 | Loss: 0.00101925
Iteration 113/1000 | Loss: 0.00048415
Iteration 114/1000 | Loss: 0.00101481
Iteration 115/1000 | Loss: 0.00012985
Iteration 116/1000 | Loss: 0.00064053
Iteration 117/1000 | Loss: 0.00019537
Iteration 118/1000 | Loss: 0.00095545
Iteration 119/1000 | Loss: 0.00014592
Iteration 120/1000 | Loss: 0.00005030
Iteration 121/1000 | Loss: 0.00010520
Iteration 122/1000 | Loss: 0.00006867
Iteration 123/1000 | Loss: 0.00009526
Iteration 124/1000 | Loss: 0.00005739
Iteration 125/1000 | Loss: 0.00007376
Iteration 126/1000 | Loss: 0.00057289
Iteration 127/1000 | Loss: 0.00017281
Iteration 128/1000 | Loss: 0.00013284
Iteration 129/1000 | Loss: 0.00010667
Iteration 130/1000 | Loss: 0.00008815
Iteration 131/1000 | Loss: 0.00009216
Iteration 132/1000 | Loss: 0.00009300
Iteration 133/1000 | Loss: 0.00006151
Iteration 134/1000 | Loss: 0.00009214
Iteration 135/1000 | Loss: 0.00025891
Iteration 136/1000 | Loss: 0.00007856
Iteration 137/1000 | Loss: 0.00005854
Iteration 138/1000 | Loss: 0.00008250
Iteration 139/1000 | Loss: 0.00005099
Iteration 140/1000 | Loss: 0.00006780
Iteration 141/1000 | Loss: 0.00002145
Iteration 142/1000 | Loss: 0.00001657
Iteration 143/1000 | Loss: 0.00007988
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00004403
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001408
Iteration 149/1000 | Loss: 0.00001408
Iteration 150/1000 | Loss: 0.00003569
Iteration 151/1000 | Loss: 0.00002121
Iteration 152/1000 | Loss: 0.00001555
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001388
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001385
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00009195
Iteration 165/1000 | Loss: 0.00001401
Iteration 166/1000 | Loss: 0.00001369
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001364
Iteration 170/1000 | Loss: 0.00001364
Iteration 171/1000 | Loss: 0.00001364
Iteration 172/1000 | Loss: 0.00001364
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001363
Iteration 180/1000 | Loss: 0.00001363
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001362
Iteration 185/1000 | Loss: 0.00001362
Iteration 186/1000 | Loss: 0.00001362
Iteration 187/1000 | Loss: 0.00001362
Iteration 188/1000 | Loss: 0.00001361
Iteration 189/1000 | Loss: 0.00001361
Iteration 190/1000 | Loss: 0.00001361
Iteration 191/1000 | Loss: 0.00001361
Iteration 192/1000 | Loss: 0.00001361
Iteration 193/1000 | Loss: 0.00001361
Iteration 194/1000 | Loss: 0.00001361
Iteration 195/1000 | Loss: 0.00001361
Iteration 196/1000 | Loss: 0.00001361
Iteration 197/1000 | Loss: 0.00001361
Iteration 198/1000 | Loss: 0.00001361
Iteration 199/1000 | Loss: 0.00005816
Iteration 200/1000 | Loss: 0.00005816
Iteration 201/1000 | Loss: 0.00001763
Iteration 202/1000 | Loss: 0.00001517
Iteration 203/1000 | Loss: 0.00001369
Iteration 204/1000 | Loss: 0.00003760
Iteration 205/1000 | Loss: 0.00001371
Iteration 206/1000 | Loss: 0.00001361
Iteration 207/1000 | Loss: 0.00001361
Iteration 208/1000 | Loss: 0.00002036
Iteration 209/1000 | Loss: 0.00001629
Iteration 210/1000 | Loss: 0.00001361
Iteration 211/1000 | Loss: 0.00001361
Iteration 212/1000 | Loss: 0.00001361
Iteration 213/1000 | Loss: 0.00001361
Iteration 214/1000 | Loss: 0.00001361
Iteration 215/1000 | Loss: 0.00001361
Iteration 216/1000 | Loss: 0.00001361
Iteration 217/1000 | Loss: 0.00001361
Iteration 218/1000 | Loss: 0.00001361
Iteration 219/1000 | Loss: 0.00001361
Iteration 220/1000 | Loss: 0.00001361
Iteration 221/1000 | Loss: 0.00001361
Iteration 222/1000 | Loss: 0.00001361
Iteration 223/1000 | Loss: 0.00001361
Iteration 224/1000 | Loss: 0.00001361
Iteration 225/1000 | Loss: 0.00001361
Iteration 226/1000 | Loss: 0.00001361
Iteration 227/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.3605937056127004e-05, 1.3605937056127004e-05, 1.3605937056127004e-05, 1.3605937056127004e-05, 1.3605937056127004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3605937056127004e-05

Optimization complete. Final v2v error: 3.0936968326568604 mm

Highest mean error: 4.083279132843018 mm for frame 10

Lowest mean error: 2.77636981010437 mm for frame 128

Saving results

Total time: 271.6569809913635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920771
Iteration 2/25 | Loss: 0.00091911
Iteration 3/25 | Loss: 0.00076772
Iteration 4/25 | Loss: 0.00073594
Iteration 5/25 | Loss: 0.00072703
Iteration 6/25 | Loss: 0.00072509
Iteration 7/25 | Loss: 0.00072475
Iteration 8/25 | Loss: 0.00072475
Iteration 9/25 | Loss: 0.00072475
Iteration 10/25 | Loss: 0.00072475
Iteration 11/25 | Loss: 0.00072475
Iteration 12/25 | Loss: 0.00072475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007247502217069268, 0.0007247502217069268, 0.0007247502217069268, 0.0007247502217069268, 0.0007247502217069268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007247502217069268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.99717999
Iteration 2/25 | Loss: 0.00105177
Iteration 3/25 | Loss: 0.00105177
Iteration 4/25 | Loss: 0.00105177
Iteration 5/25 | Loss: 0.00105177
Iteration 6/25 | Loss: 0.00105177
Iteration 7/25 | Loss: 0.00105177
Iteration 8/25 | Loss: 0.00105177
Iteration 9/25 | Loss: 0.00105176
Iteration 10/25 | Loss: 0.00105176
Iteration 11/25 | Loss: 0.00105176
Iteration 12/25 | Loss: 0.00105176
Iteration 13/25 | Loss: 0.00105176
Iteration 14/25 | Loss: 0.00105176
Iteration 15/25 | Loss: 0.00105176
Iteration 16/25 | Loss: 0.00105176
Iteration 17/25 | Loss: 0.00105176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010517645860090852, 0.0010517645860090852, 0.0010517645860090852, 0.0010517645860090852, 0.0010517645860090852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010517645860090852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105176
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001497
Iteration 5/1000 | Loss: 0.00001403
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001326
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001276
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001260
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001256
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001252
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001251
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001247
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001244
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001239
Iteration 64/1000 | Loss: 0.00001239
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001229
Iteration 85/1000 | Loss: 0.00001229
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001227
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001226
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001225
Iteration 103/1000 | Loss: 0.00001225
Iteration 104/1000 | Loss: 0.00001225
Iteration 105/1000 | Loss: 0.00001225
Iteration 106/1000 | Loss: 0.00001225
Iteration 107/1000 | Loss: 0.00001225
Iteration 108/1000 | Loss: 0.00001225
Iteration 109/1000 | Loss: 0.00001225
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001225
Iteration 112/1000 | Loss: 0.00001225
Iteration 113/1000 | Loss: 0.00001225
Iteration 114/1000 | Loss: 0.00001225
Iteration 115/1000 | Loss: 0.00001225
Iteration 116/1000 | Loss: 0.00001225
Iteration 117/1000 | Loss: 0.00001225
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.2249328392499592e-05, 1.2249328392499592e-05, 1.2249328392499592e-05, 1.2249328392499592e-05, 1.2249328392499592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2249328392499592e-05

Optimization complete. Final v2v error: 2.947511672973633 mm

Highest mean error: 3.089385986328125 mm for frame 109

Lowest mean error: 2.8040902614593506 mm for frame 32

Saving results

Total time: 36.50475478172302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987326
Iteration 2/25 | Loss: 0.00190340
Iteration 3/25 | Loss: 0.00121855
Iteration 4/25 | Loss: 0.00105961
Iteration 5/25 | Loss: 0.00100671
Iteration 6/25 | Loss: 0.00099152
Iteration 7/25 | Loss: 0.00098183
Iteration 8/25 | Loss: 0.00095990
Iteration 9/25 | Loss: 0.00094409
Iteration 10/25 | Loss: 0.00092243
Iteration 11/25 | Loss: 0.00091760
Iteration 12/25 | Loss: 0.00090895
Iteration 13/25 | Loss: 0.00089924
Iteration 14/25 | Loss: 0.00089631
Iteration 15/25 | Loss: 0.00089215
Iteration 16/25 | Loss: 0.00089007
Iteration 17/25 | Loss: 0.00088952
Iteration 18/25 | Loss: 0.00088697
Iteration 19/25 | Loss: 0.00088475
Iteration 20/25 | Loss: 0.00088440
Iteration 21/25 | Loss: 0.00088354
Iteration 22/25 | Loss: 0.00088349
Iteration 23/25 | Loss: 0.00088354
Iteration 24/25 | Loss: 0.00088254
Iteration 25/25 | Loss: 0.00088354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63305271
Iteration 2/25 | Loss: 0.00152736
Iteration 3/25 | Loss: 0.00152735
Iteration 4/25 | Loss: 0.00152735
Iteration 5/25 | Loss: 0.00152735
Iteration 6/25 | Loss: 0.00152735
Iteration 7/25 | Loss: 0.00152735
Iteration 8/25 | Loss: 0.00152735
Iteration 9/25 | Loss: 0.00152735
Iteration 10/25 | Loss: 0.00152735
Iteration 11/25 | Loss: 0.00152735
Iteration 12/25 | Loss: 0.00152735
Iteration 13/25 | Loss: 0.00152735
Iteration 14/25 | Loss: 0.00152735
Iteration 15/25 | Loss: 0.00152735
Iteration 16/25 | Loss: 0.00152735
Iteration 17/25 | Loss: 0.00152735
Iteration 18/25 | Loss: 0.00152735
Iteration 19/25 | Loss: 0.00152735
Iteration 20/25 | Loss: 0.00152735
Iteration 21/25 | Loss: 0.00152735
Iteration 22/25 | Loss: 0.00152735
Iteration 23/25 | Loss: 0.00152735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015273470198735595, 0.0015273470198735595, 0.0015273470198735595, 0.0015273470198735595, 0.0015273470198735595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015273470198735595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152735
Iteration 2/1000 | Loss: 0.00144405
Iteration 3/1000 | Loss: 0.00071451
Iteration 4/1000 | Loss: 0.00069672
Iteration 5/1000 | Loss: 0.00075418
Iteration 6/1000 | Loss: 0.00012101
Iteration 7/1000 | Loss: 0.00009235
Iteration 8/1000 | Loss: 0.00007753
Iteration 9/1000 | Loss: 0.00006921
Iteration 10/1000 | Loss: 0.00006782
Iteration 11/1000 | Loss: 0.00038682
Iteration 12/1000 | Loss: 0.00101502
Iteration 13/1000 | Loss: 0.00060866
Iteration 14/1000 | Loss: 0.00110668
Iteration 15/1000 | Loss: 0.00232778
Iteration 16/1000 | Loss: 0.00057060
Iteration 17/1000 | Loss: 0.00037419
Iteration 18/1000 | Loss: 0.00009599
Iteration 19/1000 | Loss: 0.00006618
Iteration 20/1000 | Loss: 0.00007615
Iteration 21/1000 | Loss: 0.00013646
Iteration 22/1000 | Loss: 0.00027605
Iteration 23/1000 | Loss: 0.00006048
Iteration 24/1000 | Loss: 0.00004402
Iteration 25/1000 | Loss: 0.00004244
Iteration 26/1000 | Loss: 0.00018569
Iteration 27/1000 | Loss: 0.00021893
Iteration 28/1000 | Loss: 0.00015206
Iteration 29/1000 | Loss: 0.00004744
Iteration 30/1000 | Loss: 0.00004743
Iteration 31/1000 | Loss: 0.00004762
Iteration 32/1000 | Loss: 0.00003762
Iteration 33/1000 | Loss: 0.00016801
Iteration 34/1000 | Loss: 0.00015007
Iteration 35/1000 | Loss: 0.00004210
Iteration 36/1000 | Loss: 0.00005174
Iteration 37/1000 | Loss: 0.00006355
Iteration 38/1000 | Loss: 0.00005026
Iteration 39/1000 | Loss: 0.00004660
Iteration 40/1000 | Loss: 0.00005622
Iteration 41/1000 | Loss: 0.00005016
Iteration 42/1000 | Loss: 0.00004405
Iteration 43/1000 | Loss: 0.00005033
Iteration 44/1000 | Loss: 0.00005126
Iteration 45/1000 | Loss: 0.00005169
Iteration 46/1000 | Loss: 0.00004991
Iteration 47/1000 | Loss: 0.00005210
Iteration 48/1000 | Loss: 0.00004016
Iteration 49/1000 | Loss: 0.00004763
Iteration 50/1000 | Loss: 0.00004972
Iteration 51/1000 | Loss: 0.00003949
Iteration 52/1000 | Loss: 0.00003134
Iteration 53/1000 | Loss: 0.00004494
Iteration 54/1000 | Loss: 0.00004398
Iteration 55/1000 | Loss: 0.00004815
Iteration 56/1000 | Loss: 0.00005261
Iteration 57/1000 | Loss: 0.00004942
Iteration 58/1000 | Loss: 0.00005044
Iteration 59/1000 | Loss: 0.00005681
Iteration 60/1000 | Loss: 0.00004547
Iteration 61/1000 | Loss: 0.00005697
Iteration 62/1000 | Loss: 0.00004730
Iteration 63/1000 | Loss: 0.00004447
Iteration 64/1000 | Loss: 0.00004423
Iteration 65/1000 | Loss: 0.00005400
Iteration 66/1000 | Loss: 0.00004428
Iteration 67/1000 | Loss: 0.00005210
Iteration 68/1000 | Loss: 0.00003747
Iteration 69/1000 | Loss: 0.00002738
Iteration 70/1000 | Loss: 0.00003306
Iteration 71/1000 | Loss: 0.00003627
Iteration 72/1000 | Loss: 0.00003666
Iteration 73/1000 | Loss: 0.00003531
Iteration 74/1000 | Loss: 0.00004705
Iteration 75/1000 | Loss: 0.00003953
Iteration 76/1000 | Loss: 0.00002993
Iteration 77/1000 | Loss: 0.00003051
Iteration 78/1000 | Loss: 0.00003357
Iteration 79/1000 | Loss: 0.00003476
Iteration 80/1000 | Loss: 0.00003818
Iteration 81/1000 | Loss: 0.00004604
Iteration 82/1000 | Loss: 0.00003335
Iteration 83/1000 | Loss: 0.00002867
Iteration 84/1000 | Loss: 0.00003724
Iteration 85/1000 | Loss: 0.00003913
Iteration 86/1000 | Loss: 0.00004517
Iteration 87/1000 | Loss: 0.00003011
Iteration 88/1000 | Loss: 0.00002861
Iteration 89/1000 | Loss: 0.00002718
Iteration 90/1000 | Loss: 0.00002818
Iteration 91/1000 | Loss: 0.00003537
Iteration 92/1000 | Loss: 0.00003343
Iteration 93/1000 | Loss: 0.00002923
Iteration 94/1000 | Loss: 0.00003260
Iteration 95/1000 | Loss: 0.00003348
Iteration 96/1000 | Loss: 0.00003693
Iteration 97/1000 | Loss: 0.00003626
Iteration 98/1000 | Loss: 0.00003145
Iteration 99/1000 | Loss: 0.00004446
Iteration 100/1000 | Loss: 0.00002908
Iteration 101/1000 | Loss: 0.00003229
Iteration 102/1000 | Loss: 0.00002443
Iteration 103/1000 | Loss: 0.00002775
Iteration 104/1000 | Loss: 0.00002775
Iteration 105/1000 | Loss: 0.00003738
Iteration 106/1000 | Loss: 0.00003196
Iteration 107/1000 | Loss: 0.00002936
Iteration 108/1000 | Loss: 0.00003488
Iteration 109/1000 | Loss: 0.00003430
Iteration 110/1000 | Loss: 0.00003459
Iteration 111/1000 | Loss: 0.00003422
Iteration 112/1000 | Loss: 0.00003371
Iteration 113/1000 | Loss: 0.00003385
Iteration 114/1000 | Loss: 0.00003367
Iteration 115/1000 | Loss: 0.00003296
Iteration 116/1000 | Loss: 0.00003279
Iteration 117/1000 | Loss: 0.00003265
Iteration 118/1000 | Loss: 0.00003265
Iteration 119/1000 | Loss: 0.00003827
Iteration 120/1000 | Loss: 0.00003449
Iteration 121/1000 | Loss: 0.00003224
Iteration 122/1000 | Loss: 0.00003129
Iteration 123/1000 | Loss: 0.00003131
Iteration 124/1000 | Loss: 0.00003198
Iteration 125/1000 | Loss: 0.00003162
Iteration 126/1000 | Loss: 0.00002972
Iteration 127/1000 | Loss: 0.00003609
Iteration 128/1000 | Loss: 0.00003226
Iteration 129/1000 | Loss: 0.00002636
Iteration 130/1000 | Loss: 0.00003736
Iteration 131/1000 | Loss: 0.00003149
Iteration 132/1000 | Loss: 0.00003660
Iteration 133/1000 | Loss: 0.00003216
Iteration 134/1000 | Loss: 0.00003848
Iteration 135/1000 | Loss: 0.00003186
Iteration 136/1000 | Loss: 0.00004100
Iteration 137/1000 | Loss: 0.00003652
Iteration 138/1000 | Loss: 0.00003442
Iteration 139/1000 | Loss: 0.00003201
Iteration 140/1000 | Loss: 0.00003326
Iteration 141/1000 | Loss: 0.00003262
Iteration 142/1000 | Loss: 0.00003025
Iteration 143/1000 | Loss: 0.00003102
Iteration 144/1000 | Loss: 0.00004769
Iteration 145/1000 | Loss: 0.00003296
Iteration 146/1000 | Loss: 0.00003610
Iteration 147/1000 | Loss: 0.00002400
Iteration 148/1000 | Loss: 0.00002227
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002104
Iteration 151/1000 | Loss: 0.00002073
Iteration 152/1000 | Loss: 0.00002043
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002014
Iteration 155/1000 | Loss: 0.00002009
Iteration 156/1000 | Loss: 0.00002008
Iteration 157/1000 | Loss: 0.00002008
Iteration 158/1000 | Loss: 0.00002007
Iteration 159/1000 | Loss: 0.00002006
Iteration 160/1000 | Loss: 0.00002006
Iteration 161/1000 | Loss: 0.00002005
Iteration 162/1000 | Loss: 0.00002004
Iteration 163/1000 | Loss: 0.00002003
Iteration 164/1000 | Loss: 0.00002003
Iteration 165/1000 | Loss: 0.00002003
Iteration 166/1000 | Loss: 0.00002003
Iteration 167/1000 | Loss: 0.00002003
Iteration 168/1000 | Loss: 0.00002003
Iteration 169/1000 | Loss: 0.00002003
Iteration 170/1000 | Loss: 0.00002003
Iteration 171/1000 | Loss: 0.00002003
Iteration 172/1000 | Loss: 0.00002003
Iteration 173/1000 | Loss: 0.00002002
Iteration 174/1000 | Loss: 0.00001999
Iteration 175/1000 | Loss: 0.00001999
Iteration 176/1000 | Loss: 0.00001999
Iteration 177/1000 | Loss: 0.00001998
Iteration 178/1000 | Loss: 0.00001998
Iteration 179/1000 | Loss: 0.00001998
Iteration 180/1000 | Loss: 0.00001998
Iteration 181/1000 | Loss: 0.00001998
Iteration 182/1000 | Loss: 0.00001998
Iteration 183/1000 | Loss: 0.00001998
Iteration 184/1000 | Loss: 0.00001998
Iteration 185/1000 | Loss: 0.00001998
Iteration 186/1000 | Loss: 0.00001998
Iteration 187/1000 | Loss: 0.00001997
Iteration 188/1000 | Loss: 0.00001997
Iteration 189/1000 | Loss: 0.00001997
Iteration 190/1000 | Loss: 0.00001997
Iteration 191/1000 | Loss: 0.00001996
Iteration 192/1000 | Loss: 0.00001996
Iteration 193/1000 | Loss: 0.00001996
Iteration 194/1000 | Loss: 0.00001995
Iteration 195/1000 | Loss: 0.00001995
Iteration 196/1000 | Loss: 0.00001995
Iteration 197/1000 | Loss: 0.00001995
Iteration 198/1000 | Loss: 0.00001994
Iteration 199/1000 | Loss: 0.00001994
Iteration 200/1000 | Loss: 0.00001994
Iteration 201/1000 | Loss: 0.00001994
Iteration 202/1000 | Loss: 0.00001993
Iteration 203/1000 | Loss: 0.00001993
Iteration 204/1000 | Loss: 0.00001993
Iteration 205/1000 | Loss: 0.00001993
Iteration 206/1000 | Loss: 0.00001993
Iteration 207/1000 | Loss: 0.00001992
Iteration 208/1000 | Loss: 0.00001992
Iteration 209/1000 | Loss: 0.00001991
Iteration 210/1000 | Loss: 0.00001990
Iteration 211/1000 | Loss: 0.00001990
Iteration 212/1000 | Loss: 0.00001989
Iteration 213/1000 | Loss: 0.00001989
Iteration 214/1000 | Loss: 0.00001988
Iteration 215/1000 | Loss: 0.00001988
Iteration 216/1000 | Loss: 0.00001988
Iteration 217/1000 | Loss: 0.00001988
Iteration 218/1000 | Loss: 0.00001988
Iteration 219/1000 | Loss: 0.00001988
Iteration 220/1000 | Loss: 0.00001988
Iteration 221/1000 | Loss: 0.00001987
Iteration 222/1000 | Loss: 0.00001987
Iteration 223/1000 | Loss: 0.00001987
Iteration 224/1000 | Loss: 0.00001987
Iteration 225/1000 | Loss: 0.00001987
Iteration 226/1000 | Loss: 0.00001987
Iteration 227/1000 | Loss: 0.00001987
Iteration 228/1000 | Loss: 0.00001987
Iteration 229/1000 | Loss: 0.00001987
Iteration 230/1000 | Loss: 0.00001987
Iteration 231/1000 | Loss: 0.00001987
Iteration 232/1000 | Loss: 0.00001987
Iteration 233/1000 | Loss: 0.00001987
Iteration 234/1000 | Loss: 0.00001986
Iteration 235/1000 | Loss: 0.00001986
Iteration 236/1000 | Loss: 0.00001986
Iteration 237/1000 | Loss: 0.00001986
Iteration 238/1000 | Loss: 0.00001986
Iteration 239/1000 | Loss: 0.00001986
Iteration 240/1000 | Loss: 0.00001986
Iteration 241/1000 | Loss: 0.00001986
Iteration 242/1000 | Loss: 0.00001985
Iteration 243/1000 | Loss: 0.00001985
Iteration 244/1000 | Loss: 0.00001985
Iteration 245/1000 | Loss: 0.00001985
Iteration 246/1000 | Loss: 0.00001985
Iteration 247/1000 | Loss: 0.00001985
Iteration 248/1000 | Loss: 0.00001985
Iteration 249/1000 | Loss: 0.00001985
Iteration 250/1000 | Loss: 0.00001985
Iteration 251/1000 | Loss: 0.00001985
Iteration 252/1000 | Loss: 0.00001985
Iteration 253/1000 | Loss: 0.00001985
Iteration 254/1000 | Loss: 0.00001985
Iteration 255/1000 | Loss: 0.00001985
Iteration 256/1000 | Loss: 0.00001985
Iteration 257/1000 | Loss: 0.00001985
Iteration 258/1000 | Loss: 0.00001985
Iteration 259/1000 | Loss: 0.00001985
Iteration 260/1000 | Loss: 0.00001985
Iteration 261/1000 | Loss: 0.00001985
Iteration 262/1000 | Loss: 0.00001985
Iteration 263/1000 | Loss: 0.00001985
Iteration 264/1000 | Loss: 0.00001985
Iteration 265/1000 | Loss: 0.00001985
Iteration 266/1000 | Loss: 0.00001985
Iteration 267/1000 | Loss: 0.00001985
Iteration 268/1000 | Loss: 0.00001985
Iteration 269/1000 | Loss: 0.00001985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.984602931770496e-05, 1.984602931770496e-05, 1.984602931770496e-05, 1.984602931770496e-05, 1.984602931770496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.984602931770496e-05

Optimization complete. Final v2v error: 3.4752237796783447 mm

Highest mean error: 5.567451477050781 mm for frame 119

Lowest mean error: 2.960559844970703 mm for frame 35

Saving results

Total time: 300.8912045955658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772624
Iteration 2/25 | Loss: 0.00155737
Iteration 3/25 | Loss: 0.00104350
Iteration 4/25 | Loss: 0.00089934
Iteration 5/25 | Loss: 0.00087446
Iteration 6/25 | Loss: 0.00087163
Iteration 7/25 | Loss: 0.00087142
Iteration 8/25 | Loss: 0.00087142
Iteration 9/25 | Loss: 0.00087142
Iteration 10/25 | Loss: 0.00087142
Iteration 11/25 | Loss: 0.00087142
Iteration 12/25 | Loss: 0.00087142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008714231662452221, 0.0008714231662452221, 0.0008714231662452221, 0.0008714231662452221, 0.0008714231662452221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008714231662452221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96335816
Iteration 2/25 | Loss: 0.00132860
Iteration 3/25 | Loss: 0.00132856
Iteration 4/25 | Loss: 0.00132855
Iteration 5/25 | Loss: 0.00132855
Iteration 6/25 | Loss: 0.00132855
Iteration 7/25 | Loss: 0.00132855
Iteration 8/25 | Loss: 0.00132855
Iteration 9/25 | Loss: 0.00132855
Iteration 10/25 | Loss: 0.00132855
Iteration 11/25 | Loss: 0.00132855
Iteration 12/25 | Loss: 0.00132855
Iteration 13/25 | Loss: 0.00132855
Iteration 14/25 | Loss: 0.00132855
Iteration 15/25 | Loss: 0.00132855
Iteration 16/25 | Loss: 0.00132855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013285528402775526, 0.0013285528402775526, 0.0013285528402775526, 0.0013285528402775526, 0.0013285528402775526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013285528402775526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132855
Iteration 2/1000 | Loss: 0.00003869
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002294
Iteration 6/1000 | Loss: 0.00002239
Iteration 7/1000 | Loss: 0.00002183
Iteration 8/1000 | Loss: 0.00002139
Iteration 9/1000 | Loss: 0.00002108
Iteration 10/1000 | Loss: 0.00002085
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002049
Iteration 13/1000 | Loss: 0.00002042
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002038
Iteration 16/1000 | Loss: 0.00002036
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002028
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002025
Iteration 23/1000 | Loss: 0.00002024
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002023
Iteration 27/1000 | Loss: 0.00002023
Iteration 28/1000 | Loss: 0.00002023
Iteration 29/1000 | Loss: 0.00002023
Iteration 30/1000 | Loss: 0.00002023
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002022
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002022
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00002022
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002021
Iteration 42/1000 | Loss: 0.00002020
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002019
Iteration 46/1000 | Loss: 0.00002019
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002018
Iteration 49/1000 | Loss: 0.00002018
Iteration 50/1000 | Loss: 0.00002018
Iteration 51/1000 | Loss: 0.00002018
Iteration 52/1000 | Loss: 0.00002018
Iteration 53/1000 | Loss: 0.00002018
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002018
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002017
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002017
Iteration 62/1000 | Loss: 0.00002016
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002016
Iteration 65/1000 | Loss: 0.00002016
Iteration 66/1000 | Loss: 0.00002016
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002015
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002014
Iteration 73/1000 | Loss: 0.00002014
Iteration 74/1000 | Loss: 0.00002014
Iteration 75/1000 | Loss: 0.00002013
Iteration 76/1000 | Loss: 0.00002013
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002013
Iteration 79/1000 | Loss: 0.00002013
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002013
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002012
Iteration 90/1000 | Loss: 0.00002012
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002011
Iteration 96/1000 | Loss: 0.00002011
Iteration 97/1000 | Loss: 0.00002011
Iteration 98/1000 | Loss: 0.00002011
Iteration 99/1000 | Loss: 0.00002011
Iteration 100/1000 | Loss: 0.00002011
Iteration 101/1000 | Loss: 0.00002011
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002011
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002011
Iteration 106/1000 | Loss: 0.00002011
Iteration 107/1000 | Loss: 0.00002011
Iteration 108/1000 | Loss: 0.00002011
Iteration 109/1000 | Loss: 0.00002011
Iteration 110/1000 | Loss: 0.00002010
Iteration 111/1000 | Loss: 0.00002010
Iteration 112/1000 | Loss: 0.00002010
Iteration 113/1000 | Loss: 0.00002010
Iteration 114/1000 | Loss: 0.00002010
Iteration 115/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.010484240599908e-05, 2.010484240599908e-05, 2.010484240599908e-05, 2.010484240599908e-05, 2.010484240599908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.010484240599908e-05

Optimization complete. Final v2v error: 3.6959128379821777 mm

Highest mean error: 4.170400142669678 mm for frame 234

Lowest mean error: 3.4778993129730225 mm for frame 157

Saving results

Total time: 39.71087074279785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00204123
Iteration 2/25 | Loss: 0.00092128
Iteration 3/25 | Loss: 0.00082234
Iteration 4/25 | Loss: 0.00077585
Iteration 5/25 | Loss: 0.00075959
Iteration 6/25 | Loss: 0.00075621
Iteration 7/25 | Loss: 0.00075459
Iteration 8/25 | Loss: 0.00075430
Iteration 9/25 | Loss: 0.00075430
Iteration 10/25 | Loss: 0.00075430
Iteration 11/25 | Loss: 0.00075430
Iteration 12/25 | Loss: 0.00075430
Iteration 13/25 | Loss: 0.00075430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007542994571849704, 0.0007542994571849704, 0.0007542994571849704, 0.0007542994571849704, 0.0007542994571849704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007542994571849704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62166214
Iteration 2/25 | Loss: 0.00161969
Iteration 3/25 | Loss: 0.00161969
Iteration 4/25 | Loss: 0.00161969
Iteration 5/25 | Loss: 0.00161969
Iteration 6/25 | Loss: 0.00161969
Iteration 7/25 | Loss: 0.00161969
Iteration 8/25 | Loss: 0.00161969
Iteration 9/25 | Loss: 0.00161969
Iteration 10/25 | Loss: 0.00161969
Iteration 11/25 | Loss: 0.00161969
Iteration 12/25 | Loss: 0.00161969
Iteration 13/25 | Loss: 0.00161969
Iteration 14/25 | Loss: 0.00161969
Iteration 15/25 | Loss: 0.00161969
Iteration 16/25 | Loss: 0.00161969
Iteration 17/25 | Loss: 0.00161969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016196880023926497, 0.0016196880023926497, 0.0016196880023926497, 0.0016196880023926497, 0.0016196880023926497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016196880023926497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161969
Iteration 2/1000 | Loss: 0.00004152
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002014
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001704
Iteration 8/1000 | Loss: 0.00001661
Iteration 9/1000 | Loss: 0.00001632
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001598
Iteration 12/1000 | Loss: 0.00001597
Iteration 13/1000 | Loss: 0.00001587
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00001577
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001557
Iteration 24/1000 | Loss: 0.00001555
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001551
Iteration 29/1000 | Loss: 0.00001551
Iteration 30/1000 | Loss: 0.00001550
Iteration 31/1000 | Loss: 0.00001550
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001544
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001539
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001539
Iteration 58/1000 | Loss: 0.00001539
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001537
Iteration 71/1000 | Loss: 0.00001537
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001537
Iteration 76/1000 | Loss: 0.00001537
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001535
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001535
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001534
Iteration 110/1000 | Loss: 0.00001534
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001533
Iteration 120/1000 | Loss: 0.00001533
Iteration 121/1000 | Loss: 0.00001533
Iteration 122/1000 | Loss: 0.00001533
Iteration 123/1000 | Loss: 0.00001532
Iteration 124/1000 | Loss: 0.00001532
Iteration 125/1000 | Loss: 0.00001532
Iteration 126/1000 | Loss: 0.00001532
Iteration 127/1000 | Loss: 0.00001532
Iteration 128/1000 | Loss: 0.00001532
Iteration 129/1000 | Loss: 0.00001532
Iteration 130/1000 | Loss: 0.00001532
Iteration 131/1000 | Loss: 0.00001532
Iteration 132/1000 | Loss: 0.00001532
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001532
Iteration 135/1000 | Loss: 0.00001532
Iteration 136/1000 | Loss: 0.00001532
Iteration 137/1000 | Loss: 0.00001532
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001532
Iteration 140/1000 | Loss: 0.00001531
Iteration 141/1000 | Loss: 0.00001531
Iteration 142/1000 | Loss: 0.00001531
Iteration 143/1000 | Loss: 0.00001531
Iteration 144/1000 | Loss: 0.00001531
Iteration 145/1000 | Loss: 0.00001530
Iteration 146/1000 | Loss: 0.00001530
Iteration 147/1000 | Loss: 0.00001530
Iteration 148/1000 | Loss: 0.00001530
Iteration 149/1000 | Loss: 0.00001530
Iteration 150/1000 | Loss: 0.00001530
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001530
Iteration 154/1000 | Loss: 0.00001530
Iteration 155/1000 | Loss: 0.00001530
Iteration 156/1000 | Loss: 0.00001530
Iteration 157/1000 | Loss: 0.00001530
Iteration 158/1000 | Loss: 0.00001530
Iteration 159/1000 | Loss: 0.00001530
Iteration 160/1000 | Loss: 0.00001530
Iteration 161/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.5296471246983856e-05, 1.5296471246983856e-05, 1.5296471246983856e-05, 1.5296471246983856e-05, 1.5296471246983856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5296471246983856e-05

Optimization complete. Final v2v error: 3.315518856048584 mm

Highest mean error: 3.5589983463287354 mm for frame 62

Lowest mean error: 3.1194417476654053 mm for frame 86

Saving results

Total time: 41.05831336975098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896411
Iteration 2/25 | Loss: 0.00094419
Iteration 3/25 | Loss: 0.00078002
Iteration 4/25 | Loss: 0.00075660
Iteration 5/25 | Loss: 0.00074865
Iteration 6/25 | Loss: 0.00074707
Iteration 7/25 | Loss: 0.00074691
Iteration 8/25 | Loss: 0.00074691
Iteration 9/25 | Loss: 0.00074691
Iteration 10/25 | Loss: 0.00074691
Iteration 11/25 | Loss: 0.00074691
Iteration 12/25 | Loss: 0.00074691
Iteration 13/25 | Loss: 0.00074691
Iteration 14/25 | Loss: 0.00074691
Iteration 15/25 | Loss: 0.00074691
Iteration 16/25 | Loss: 0.00074691
Iteration 17/25 | Loss: 0.00074691
Iteration 18/25 | Loss: 0.00074691
Iteration 19/25 | Loss: 0.00074691
Iteration 20/25 | Loss: 0.00074691
Iteration 21/25 | Loss: 0.00074691
Iteration 22/25 | Loss: 0.00074691
Iteration 23/25 | Loss: 0.00074691
Iteration 24/25 | Loss: 0.00074691
Iteration 25/25 | Loss: 0.00074691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98386192
Iteration 2/25 | Loss: 0.00117136
Iteration 3/25 | Loss: 0.00117136
Iteration 4/25 | Loss: 0.00117136
Iteration 5/25 | Loss: 0.00117136
Iteration 6/25 | Loss: 0.00117136
Iteration 7/25 | Loss: 0.00117136
Iteration 8/25 | Loss: 0.00117136
Iteration 9/25 | Loss: 0.00117136
Iteration 10/25 | Loss: 0.00117136
Iteration 11/25 | Loss: 0.00117135
Iteration 12/25 | Loss: 0.00117135
Iteration 13/25 | Loss: 0.00117135
Iteration 14/25 | Loss: 0.00117135
Iteration 15/25 | Loss: 0.00117135
Iteration 16/25 | Loss: 0.00117135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011713546700775623, 0.0011713546700775623, 0.0011713546700775623, 0.0011713546700775623, 0.0011713546700775623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011713546700775623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117135
Iteration 2/1000 | Loss: 0.00002343
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001367
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001262
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001242
Iteration 56/1000 | Loss: 0.00001242
Iteration 57/1000 | Loss: 0.00001242
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001222
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001222
Iteration 114/1000 | Loss: 0.00001222
Iteration 115/1000 | Loss: 0.00001222
Iteration 116/1000 | Loss: 0.00001222
Iteration 117/1000 | Loss: 0.00001222
Iteration 118/1000 | Loss: 0.00001222
Iteration 119/1000 | Loss: 0.00001222
Iteration 120/1000 | Loss: 0.00001222
Iteration 121/1000 | Loss: 0.00001222
Iteration 122/1000 | Loss: 0.00001222
Iteration 123/1000 | Loss: 0.00001222
Iteration 124/1000 | Loss: 0.00001222
Iteration 125/1000 | Loss: 0.00001222
Iteration 126/1000 | Loss: 0.00001222
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001222
Iteration 132/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2217330549901817e-05, 1.2217330549901817e-05, 1.2217330549901817e-05, 1.2217330549901817e-05, 1.2217330549901817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2217330549901817e-05

Optimization complete. Final v2v error: 2.9596712589263916 mm

Highest mean error: 3.454726457595825 mm for frame 157

Lowest mean error: 2.7524044513702393 mm for frame 217

Saving results

Total time: 37.114407539367676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920709
Iteration 2/25 | Loss: 0.00147090
Iteration 3/25 | Loss: 0.00088235
Iteration 4/25 | Loss: 0.00083792
Iteration 5/25 | Loss: 0.00081861
Iteration 6/25 | Loss: 0.00080231
Iteration 7/25 | Loss: 0.00079262
Iteration 8/25 | Loss: 0.00079180
Iteration 9/25 | Loss: 0.00079288
Iteration 10/25 | Loss: 0.00078803
Iteration 11/25 | Loss: 0.00078859
Iteration 12/25 | Loss: 0.00078790
Iteration 13/25 | Loss: 0.00078926
Iteration 14/25 | Loss: 0.00078591
Iteration 15/25 | Loss: 0.00078368
Iteration 16/25 | Loss: 0.00078322
Iteration 17/25 | Loss: 0.00078296
Iteration 18/25 | Loss: 0.00078296
Iteration 19/25 | Loss: 0.00078296
Iteration 20/25 | Loss: 0.00078296
Iteration 21/25 | Loss: 0.00078296
Iteration 22/25 | Loss: 0.00078296
Iteration 23/25 | Loss: 0.00078296
Iteration 24/25 | Loss: 0.00078296
Iteration 25/25 | Loss: 0.00078296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27602482
Iteration 2/25 | Loss: 0.00134521
Iteration 3/25 | Loss: 0.00132047
Iteration 4/25 | Loss: 0.00132047
Iteration 5/25 | Loss: 0.00132047
Iteration 6/25 | Loss: 0.00132047
Iteration 7/25 | Loss: 0.00132046
Iteration 8/25 | Loss: 0.00132046
Iteration 9/25 | Loss: 0.00132046
Iteration 10/25 | Loss: 0.00132046
Iteration 11/25 | Loss: 0.00132046
Iteration 12/25 | Loss: 0.00132046
Iteration 13/25 | Loss: 0.00132046
Iteration 14/25 | Loss: 0.00132046
Iteration 15/25 | Loss: 0.00132046
Iteration 16/25 | Loss: 0.00132046
Iteration 17/25 | Loss: 0.00132046
Iteration 18/25 | Loss: 0.00132046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013204632559791207, 0.0013204632559791207, 0.0013204632559791207, 0.0013204632559791207, 0.0013204632559791207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013204632559791207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132046
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00005233
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00005403
Iteration 7/1000 | Loss: 0.00001957
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001557
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001543
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001543
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001541
Iteration 42/1000 | Loss: 0.00001540
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001540
Iteration 46/1000 | Loss: 0.00001540
Iteration 47/1000 | Loss: 0.00001540
Iteration 48/1000 | Loss: 0.00001540
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001539
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001537
Iteration 60/1000 | Loss: 0.00001537
Iteration 61/1000 | Loss: 0.00001537
Iteration 62/1000 | Loss: 0.00001537
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001536
Iteration 72/1000 | Loss: 0.00001536
Iteration 73/1000 | Loss: 0.00001536
Iteration 74/1000 | Loss: 0.00001536
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001535
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001534
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001534
Iteration 88/1000 | Loss: 0.00001534
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.5337915101554245e-05, 1.5337915101554245e-05, 1.5337915101554245e-05, 1.5337915101554245e-05, 1.5337915101554245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5337915101554245e-05

Optimization complete. Final v2v error: 3.3113350868225098 mm

Highest mean error: 3.695721387863159 mm for frame 40

Lowest mean error: 3.0939791202545166 mm for frame 224

Saving results

Total time: 60.024335861206055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678142
Iteration 2/25 | Loss: 0.00101485
Iteration 3/25 | Loss: 0.00084402
Iteration 4/25 | Loss: 0.00081717
Iteration 5/25 | Loss: 0.00080855
Iteration 6/25 | Loss: 0.00080725
Iteration 7/25 | Loss: 0.00080718
Iteration 8/25 | Loss: 0.00080718
Iteration 9/25 | Loss: 0.00080717
Iteration 10/25 | Loss: 0.00080717
Iteration 11/25 | Loss: 0.00080717
Iteration 12/25 | Loss: 0.00080717
Iteration 13/25 | Loss: 0.00080717
Iteration 14/25 | Loss: 0.00080717
Iteration 15/25 | Loss: 0.00080717
Iteration 16/25 | Loss: 0.00080717
Iteration 17/25 | Loss: 0.00080717
Iteration 18/25 | Loss: 0.00080717
Iteration 19/25 | Loss: 0.00080717
Iteration 20/25 | Loss: 0.00080717
Iteration 21/25 | Loss: 0.00080717
Iteration 22/25 | Loss: 0.00080717
Iteration 23/25 | Loss: 0.00080717
Iteration 24/25 | Loss: 0.00080717
Iteration 25/25 | Loss: 0.00080717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.34239483
Iteration 2/25 | Loss: 0.00118312
Iteration 3/25 | Loss: 0.00118311
Iteration 4/25 | Loss: 0.00118311
Iteration 5/25 | Loss: 0.00118311
Iteration 6/25 | Loss: 0.00118311
Iteration 7/25 | Loss: 0.00118311
Iteration 8/25 | Loss: 0.00118311
Iteration 9/25 | Loss: 0.00118311
Iteration 10/25 | Loss: 0.00118311
Iteration 11/25 | Loss: 0.00118311
Iteration 12/25 | Loss: 0.00118310
Iteration 13/25 | Loss: 0.00118310
Iteration 14/25 | Loss: 0.00118310
Iteration 15/25 | Loss: 0.00118310
Iteration 16/25 | Loss: 0.00118310
Iteration 17/25 | Loss: 0.00118311
Iteration 18/25 | Loss: 0.00118310
Iteration 19/25 | Loss: 0.00118310
Iteration 20/25 | Loss: 0.00118311
Iteration 21/25 | Loss: 0.00118310
Iteration 22/25 | Loss: 0.00118310
Iteration 23/25 | Loss: 0.00118310
Iteration 24/25 | Loss: 0.00118310
Iteration 25/25 | Loss: 0.00118310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118310
Iteration 2/1000 | Loss: 0.00003469
Iteration 3/1000 | Loss: 0.00002516
Iteration 4/1000 | Loss: 0.00002293
Iteration 5/1000 | Loss: 0.00002164
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00002004
Iteration 8/1000 | Loss: 0.00001951
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001851
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001813
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001793
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001782
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001781
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001780
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001777
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001774
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001773
Iteration 41/1000 | Loss: 0.00001773
Iteration 42/1000 | Loss: 0.00001772
Iteration 43/1000 | Loss: 0.00001772
Iteration 44/1000 | Loss: 0.00001772
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001770
Iteration 48/1000 | Loss: 0.00001770
Iteration 49/1000 | Loss: 0.00001770
Iteration 50/1000 | Loss: 0.00001767
Iteration 51/1000 | Loss: 0.00001767
Iteration 52/1000 | Loss: 0.00001766
Iteration 53/1000 | Loss: 0.00001766
Iteration 54/1000 | Loss: 0.00001765
Iteration 55/1000 | Loss: 0.00001765
Iteration 56/1000 | Loss: 0.00001765
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.7618165657040663e-05, 1.7618165657040663e-05, 1.7618165657040663e-05, 1.7618165657040663e-05, 1.7618165657040663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7618165657040663e-05

Optimization complete. Final v2v error: 3.5661115646362305 mm

Highest mean error: 4.137304782867432 mm for frame 37

Lowest mean error: 3.181915521621704 mm for frame 239

Saving results

Total time: 40.65291213989258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858071
Iteration 2/25 | Loss: 0.00121830
Iteration 3/25 | Loss: 0.00091267
Iteration 4/25 | Loss: 0.00087615
Iteration 5/25 | Loss: 0.00086953
Iteration 6/25 | Loss: 0.00086762
Iteration 7/25 | Loss: 0.00086750
Iteration 8/25 | Loss: 0.00086750
Iteration 9/25 | Loss: 0.00086750
Iteration 10/25 | Loss: 0.00086750
Iteration 11/25 | Loss: 0.00086750
Iteration 12/25 | Loss: 0.00086750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008674979326315224, 0.0008674979326315224, 0.0008674979326315224, 0.0008674979326315224, 0.0008674979326315224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008674979326315224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13959348
Iteration 2/25 | Loss: 0.00104110
Iteration 3/25 | Loss: 0.00104110
Iteration 4/25 | Loss: 0.00104110
Iteration 5/25 | Loss: 0.00104110
Iteration 6/25 | Loss: 0.00104110
Iteration 7/25 | Loss: 0.00104110
Iteration 8/25 | Loss: 0.00104110
Iteration 9/25 | Loss: 0.00104110
Iteration 10/25 | Loss: 0.00104110
Iteration 11/25 | Loss: 0.00104109
Iteration 12/25 | Loss: 0.00104109
Iteration 13/25 | Loss: 0.00104110
Iteration 14/25 | Loss: 0.00104110
Iteration 15/25 | Loss: 0.00104110
Iteration 16/25 | Loss: 0.00104110
Iteration 17/25 | Loss: 0.00104110
Iteration 18/25 | Loss: 0.00104110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010410950053483248, 0.0010410950053483248, 0.0010410950053483248, 0.0010410950053483248, 0.0010410950053483248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010410950053483248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104110
Iteration 2/1000 | Loss: 0.00003178
Iteration 3/1000 | Loss: 0.00002539
Iteration 4/1000 | Loss: 0.00002394
Iteration 5/1000 | Loss: 0.00002271
Iteration 6/1000 | Loss: 0.00002212
Iteration 7/1000 | Loss: 0.00002163
Iteration 8/1000 | Loss: 0.00002148
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002126
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002107
Iteration 13/1000 | Loss: 0.00002100
Iteration 14/1000 | Loss: 0.00002099
Iteration 15/1000 | Loss: 0.00002097
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002096
Iteration 18/1000 | Loss: 0.00002096
Iteration 19/1000 | Loss: 0.00002096
Iteration 20/1000 | Loss: 0.00002096
Iteration 21/1000 | Loss: 0.00002096
Iteration 22/1000 | Loss: 0.00002096
Iteration 23/1000 | Loss: 0.00002096
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002096
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00002095
Iteration 30/1000 | Loss: 0.00002095
Iteration 31/1000 | Loss: 0.00002095
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002093
Iteration 34/1000 | Loss: 0.00002093
Iteration 35/1000 | Loss: 0.00002093
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002092
Iteration 39/1000 | Loss: 0.00002092
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002092
Iteration 42/1000 | Loss: 0.00002092
Iteration 43/1000 | Loss: 0.00002092
Iteration 44/1000 | Loss: 0.00002092
Iteration 45/1000 | Loss: 0.00002092
Iteration 46/1000 | Loss: 0.00002092
Iteration 47/1000 | Loss: 0.00002091
Iteration 48/1000 | Loss: 0.00002090
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002090
Iteration 51/1000 | Loss: 0.00002089
Iteration 52/1000 | Loss: 0.00002089
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002088
Iteration 56/1000 | Loss: 0.00002088
Iteration 57/1000 | Loss: 0.00002088
Iteration 58/1000 | Loss: 0.00002088
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002085
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002085
Iteration 71/1000 | Loss: 0.00002085
Iteration 72/1000 | Loss: 0.00002084
Iteration 73/1000 | Loss: 0.00002084
Iteration 74/1000 | Loss: 0.00002083
Iteration 75/1000 | Loss: 0.00002083
Iteration 76/1000 | Loss: 0.00002083
Iteration 77/1000 | Loss: 0.00002083
Iteration 78/1000 | Loss: 0.00002083
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00002083
Iteration 81/1000 | Loss: 0.00002083
Iteration 82/1000 | Loss: 0.00002083
Iteration 83/1000 | Loss: 0.00002083
Iteration 84/1000 | Loss: 0.00002083
Iteration 85/1000 | Loss: 0.00002082
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002082
Iteration 88/1000 | Loss: 0.00002082
Iteration 89/1000 | Loss: 0.00002082
Iteration 90/1000 | Loss: 0.00002082
Iteration 91/1000 | Loss: 0.00002082
Iteration 92/1000 | Loss: 0.00002082
Iteration 93/1000 | Loss: 0.00002082
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00002082
Iteration 96/1000 | Loss: 0.00002082
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002081
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002080
Iteration 112/1000 | Loss: 0.00002080
Iteration 113/1000 | Loss: 0.00002080
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002079
Iteration 117/1000 | Loss: 0.00002079
Iteration 118/1000 | Loss: 0.00002079
Iteration 119/1000 | Loss: 0.00002079
Iteration 120/1000 | Loss: 0.00002079
Iteration 121/1000 | Loss: 0.00002079
Iteration 122/1000 | Loss: 0.00002079
Iteration 123/1000 | Loss: 0.00002078
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002078
Iteration 128/1000 | Loss: 0.00002078
Iteration 129/1000 | Loss: 0.00002078
Iteration 130/1000 | Loss: 0.00002078
Iteration 131/1000 | Loss: 0.00002078
Iteration 132/1000 | Loss: 0.00002078
Iteration 133/1000 | Loss: 0.00002078
Iteration 134/1000 | Loss: 0.00002078
Iteration 135/1000 | Loss: 0.00002078
Iteration 136/1000 | Loss: 0.00002078
Iteration 137/1000 | Loss: 0.00002078
Iteration 138/1000 | Loss: 0.00002078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.0778503312612884e-05, 2.0778503312612884e-05, 2.0778503312612884e-05, 2.0778503312612884e-05, 2.0778503312612884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0778503312612884e-05

Optimization complete. Final v2v error: 3.8560972213745117 mm

Highest mean error: 4.140714168548584 mm for frame 118

Lowest mean error: 3.5596489906311035 mm for frame 49

Saving results

Total time: 31.76424503326416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921834
Iteration 2/25 | Loss: 0.00101112
Iteration 3/25 | Loss: 0.00087684
Iteration 4/25 | Loss: 0.00083742
Iteration 5/25 | Loss: 0.00083121
Iteration 6/25 | Loss: 0.00082989
Iteration 7/25 | Loss: 0.00082951
Iteration 8/25 | Loss: 0.00082951
Iteration 9/25 | Loss: 0.00082951
Iteration 10/25 | Loss: 0.00082951
Iteration 11/25 | Loss: 0.00082951
Iteration 12/25 | Loss: 0.00082951
Iteration 13/25 | Loss: 0.00082951
Iteration 14/25 | Loss: 0.00082951
Iteration 15/25 | Loss: 0.00082951
Iteration 16/25 | Loss: 0.00082951
Iteration 17/25 | Loss: 0.00082951
Iteration 18/25 | Loss: 0.00082951
Iteration 19/25 | Loss: 0.00082951
Iteration 20/25 | Loss: 0.00082951
Iteration 21/25 | Loss: 0.00082951
Iteration 22/25 | Loss: 0.00082951
Iteration 23/25 | Loss: 0.00082951
Iteration 24/25 | Loss: 0.00082951
Iteration 25/25 | Loss: 0.00082951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59420311
Iteration 2/25 | Loss: 0.00124459
Iteration 3/25 | Loss: 0.00124459
Iteration 4/25 | Loss: 0.00124459
Iteration 5/25 | Loss: 0.00124459
Iteration 6/25 | Loss: 0.00124459
Iteration 7/25 | Loss: 0.00124459
Iteration 8/25 | Loss: 0.00124459
Iteration 9/25 | Loss: 0.00124459
Iteration 10/25 | Loss: 0.00124459
Iteration 11/25 | Loss: 0.00124459
Iteration 12/25 | Loss: 0.00124459
Iteration 13/25 | Loss: 0.00124459
Iteration 14/25 | Loss: 0.00124459
Iteration 15/25 | Loss: 0.00124459
Iteration 16/25 | Loss: 0.00124459
Iteration 17/25 | Loss: 0.00124459
Iteration 18/25 | Loss: 0.00124459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012445864267647266, 0.0012445864267647266, 0.0012445864267647266, 0.0012445864267647266, 0.0012445864267647266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012445864267647266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124459
Iteration 2/1000 | Loss: 0.00003534
Iteration 3/1000 | Loss: 0.00002574
Iteration 4/1000 | Loss: 0.00002397
Iteration 5/1000 | Loss: 0.00002289
Iteration 6/1000 | Loss: 0.00002225
Iteration 7/1000 | Loss: 0.00002178
Iteration 8/1000 | Loss: 0.00002160
Iteration 9/1000 | Loss: 0.00002148
Iteration 10/1000 | Loss: 0.00002140
Iteration 11/1000 | Loss: 0.00002140
Iteration 12/1000 | Loss: 0.00002139
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00002138
Iteration 15/1000 | Loss: 0.00002138
Iteration 16/1000 | Loss: 0.00002138
Iteration 17/1000 | Loss: 0.00002137
Iteration 18/1000 | Loss: 0.00002137
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002135
Iteration 21/1000 | Loss: 0.00002134
Iteration 22/1000 | Loss: 0.00002134
Iteration 23/1000 | Loss: 0.00002133
Iteration 24/1000 | Loss: 0.00002133
Iteration 25/1000 | Loss: 0.00002133
Iteration 26/1000 | Loss: 0.00002132
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002130
Iteration 31/1000 | Loss: 0.00002130
Iteration 32/1000 | Loss: 0.00002130
Iteration 33/1000 | Loss: 0.00002130
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002130
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002128
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002128
Iteration 46/1000 | Loss: 0.00002128
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002127
Iteration 49/1000 | Loss: 0.00002127
Iteration 50/1000 | Loss: 0.00002127
Iteration 51/1000 | Loss: 0.00002127
Iteration 52/1000 | Loss: 0.00002126
Iteration 53/1000 | Loss: 0.00002126
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002125
Iteration 63/1000 | Loss: 0.00002125
Iteration 64/1000 | Loss: 0.00002125
Iteration 65/1000 | Loss: 0.00002125
Iteration 66/1000 | Loss: 0.00002125
Iteration 67/1000 | Loss: 0.00002125
Iteration 68/1000 | Loss: 0.00002125
Iteration 69/1000 | Loss: 0.00002124
Iteration 70/1000 | Loss: 0.00002124
Iteration 71/1000 | Loss: 0.00002124
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00002124
Iteration 76/1000 | Loss: 0.00002124
Iteration 77/1000 | Loss: 0.00002124
Iteration 78/1000 | Loss: 0.00002124
Iteration 79/1000 | Loss: 0.00002123
Iteration 80/1000 | Loss: 0.00002123
Iteration 81/1000 | Loss: 0.00002123
Iteration 82/1000 | Loss: 0.00002123
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002123
Iteration 85/1000 | Loss: 0.00002123
Iteration 86/1000 | Loss: 0.00002123
Iteration 87/1000 | Loss: 0.00002123
Iteration 88/1000 | Loss: 0.00002123
Iteration 89/1000 | Loss: 0.00002123
Iteration 90/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.123309786838945e-05, 2.123309786838945e-05, 2.123309786838945e-05, 2.123309786838945e-05, 2.123309786838945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.123309786838945e-05

Optimization complete. Final v2v error: 3.8519744873046875 mm

Highest mean error: 4.187697410583496 mm for frame 5

Lowest mean error: 3.6122028827667236 mm for frame 35

Saving results

Total time: 27.500417947769165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457951
Iteration 2/25 | Loss: 0.00098866
Iteration 3/25 | Loss: 0.00081678
Iteration 4/25 | Loss: 0.00079518
Iteration 5/25 | Loss: 0.00078773
Iteration 6/25 | Loss: 0.00078554
Iteration 7/25 | Loss: 0.00078509
Iteration 8/25 | Loss: 0.00078509
Iteration 9/25 | Loss: 0.00078509
Iteration 10/25 | Loss: 0.00078509
Iteration 11/25 | Loss: 0.00078509
Iteration 12/25 | Loss: 0.00078509
Iteration 13/25 | Loss: 0.00078509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007850860711187124, 0.0007850860711187124, 0.0007850860711187124, 0.0007850860711187124, 0.0007850860711187124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007850860711187124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57409000
Iteration 2/25 | Loss: 0.00129374
Iteration 3/25 | Loss: 0.00129374
Iteration 4/25 | Loss: 0.00129374
Iteration 5/25 | Loss: 0.00129374
Iteration 6/25 | Loss: 0.00129374
Iteration 7/25 | Loss: 0.00129374
Iteration 8/25 | Loss: 0.00129374
Iteration 9/25 | Loss: 0.00129374
Iteration 10/25 | Loss: 0.00129374
Iteration 11/25 | Loss: 0.00129374
Iteration 12/25 | Loss: 0.00129374
Iteration 13/25 | Loss: 0.00129374
Iteration 14/25 | Loss: 0.00129374
Iteration 15/25 | Loss: 0.00129374
Iteration 16/25 | Loss: 0.00129374
Iteration 17/25 | Loss: 0.00129374
Iteration 18/25 | Loss: 0.00129374
Iteration 19/25 | Loss: 0.00129374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012937396531924605, 0.0012937396531924605, 0.0012937396531924605, 0.0012937396531924605, 0.0012937396531924605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012937396531924605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129374
Iteration 2/1000 | Loss: 0.00002368
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001278
Iteration 6/1000 | Loss: 0.00001253
Iteration 7/1000 | Loss: 0.00001226
Iteration 8/1000 | Loss: 0.00001204
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001190
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001180
Iteration 19/1000 | Loss: 0.00001178
Iteration 20/1000 | Loss: 0.00001177
Iteration 21/1000 | Loss: 0.00001177
Iteration 22/1000 | Loss: 0.00001176
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001176
Iteration 25/1000 | Loss: 0.00001175
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001171
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001165
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001162
Iteration 90/1000 | Loss: 0.00001162
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001161
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001157
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001157
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001156
Iteration 125/1000 | Loss: 0.00001156
Iteration 126/1000 | Loss: 0.00001156
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001156
Iteration 131/1000 | Loss: 0.00001155
Iteration 132/1000 | Loss: 0.00001155
Iteration 133/1000 | Loss: 0.00001155
Iteration 134/1000 | Loss: 0.00001155
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001155
Iteration 137/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.1554207048902754e-05, 1.1554207048902754e-05, 1.1554207048902754e-05, 1.1554207048902754e-05, 1.1554207048902754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1554207048902754e-05

Optimization complete. Final v2v error: 2.8555080890655518 mm

Highest mean error: 3.041841745376587 mm for frame 214

Lowest mean error: 2.6839194297790527 mm for frame 0

Saving results

Total time: 36.23909378051758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407098
Iteration 2/25 | Loss: 0.00088829
Iteration 3/25 | Loss: 0.00075258
Iteration 4/25 | Loss: 0.00072360
Iteration 5/25 | Loss: 0.00071937
Iteration 6/25 | Loss: 0.00071776
Iteration 7/25 | Loss: 0.00071734
Iteration 8/25 | Loss: 0.00071734
Iteration 9/25 | Loss: 0.00071734
Iteration 10/25 | Loss: 0.00071734
Iteration 11/25 | Loss: 0.00071734
Iteration 12/25 | Loss: 0.00071734
Iteration 13/25 | Loss: 0.00071734
Iteration 14/25 | Loss: 0.00071734
Iteration 15/25 | Loss: 0.00071734
Iteration 16/25 | Loss: 0.00071734
Iteration 17/25 | Loss: 0.00071734
Iteration 18/25 | Loss: 0.00071734
Iteration 19/25 | Loss: 0.00071734
Iteration 20/25 | Loss: 0.00071734
Iteration 21/25 | Loss: 0.00071734
Iteration 22/25 | Loss: 0.00071734
Iteration 23/25 | Loss: 0.00071734
Iteration 24/25 | Loss: 0.00071734
Iteration 25/25 | Loss: 0.00071734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56363583
Iteration 2/25 | Loss: 0.00109636
Iteration 3/25 | Loss: 0.00109632
Iteration 4/25 | Loss: 0.00109632
Iteration 5/25 | Loss: 0.00109632
Iteration 6/25 | Loss: 0.00109632
Iteration 7/25 | Loss: 0.00109632
Iteration 8/25 | Loss: 0.00109632
Iteration 9/25 | Loss: 0.00109632
Iteration 10/25 | Loss: 0.00109632
Iteration 11/25 | Loss: 0.00109632
Iteration 12/25 | Loss: 0.00109632
Iteration 13/25 | Loss: 0.00109632
Iteration 14/25 | Loss: 0.00109632
Iteration 15/25 | Loss: 0.00109632
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010963199893012643, 0.0010963199893012643, 0.0010963199893012643, 0.0010963199893012643, 0.0010963199893012643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010963199893012643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109632
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00001683
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001282
Iteration 6/1000 | Loss: 0.00001214
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001173
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001126
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001106
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001099
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001094
Iteration 17/1000 | Loss: 0.00001094
Iteration 18/1000 | Loss: 0.00001094
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001090
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001080
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001079
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001078
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001075
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001074
Iteration 69/1000 | Loss: 0.00001074
Iteration 70/1000 | Loss: 0.00001074
Iteration 71/1000 | Loss: 0.00001074
Iteration 72/1000 | Loss: 0.00001073
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001073
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001072
Iteration 81/1000 | Loss: 0.00001072
Iteration 82/1000 | Loss: 0.00001072
Iteration 83/1000 | Loss: 0.00001072
Iteration 84/1000 | Loss: 0.00001072
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001070
Iteration 95/1000 | Loss: 0.00001070
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001070
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001069
Iteration 110/1000 | Loss: 0.00001069
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001068
Iteration 113/1000 | Loss: 0.00001068
Iteration 114/1000 | Loss: 0.00001068
Iteration 115/1000 | Loss: 0.00001068
Iteration 116/1000 | Loss: 0.00001068
Iteration 117/1000 | Loss: 0.00001068
Iteration 118/1000 | Loss: 0.00001068
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001067
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001066
Iteration 126/1000 | Loss: 0.00001066
Iteration 127/1000 | Loss: 0.00001066
Iteration 128/1000 | Loss: 0.00001066
Iteration 129/1000 | Loss: 0.00001066
Iteration 130/1000 | Loss: 0.00001066
Iteration 131/1000 | Loss: 0.00001066
Iteration 132/1000 | Loss: 0.00001066
Iteration 133/1000 | Loss: 0.00001066
Iteration 134/1000 | Loss: 0.00001066
Iteration 135/1000 | Loss: 0.00001066
Iteration 136/1000 | Loss: 0.00001065
Iteration 137/1000 | Loss: 0.00001065
Iteration 138/1000 | Loss: 0.00001065
Iteration 139/1000 | Loss: 0.00001065
Iteration 140/1000 | Loss: 0.00001065
Iteration 141/1000 | Loss: 0.00001065
Iteration 142/1000 | Loss: 0.00001065
Iteration 143/1000 | Loss: 0.00001065
Iteration 144/1000 | Loss: 0.00001065
Iteration 145/1000 | Loss: 0.00001065
Iteration 146/1000 | Loss: 0.00001065
Iteration 147/1000 | Loss: 0.00001065
Iteration 148/1000 | Loss: 0.00001064
Iteration 149/1000 | Loss: 0.00001064
Iteration 150/1000 | Loss: 0.00001064
Iteration 151/1000 | Loss: 0.00001064
Iteration 152/1000 | Loss: 0.00001064
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001063
Iteration 158/1000 | Loss: 0.00001063
Iteration 159/1000 | Loss: 0.00001063
Iteration 160/1000 | Loss: 0.00001063
Iteration 161/1000 | Loss: 0.00001063
Iteration 162/1000 | Loss: 0.00001063
Iteration 163/1000 | Loss: 0.00001063
Iteration 164/1000 | Loss: 0.00001063
Iteration 165/1000 | Loss: 0.00001063
Iteration 166/1000 | Loss: 0.00001063
Iteration 167/1000 | Loss: 0.00001063
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001062
Iteration 172/1000 | Loss: 0.00001062
Iteration 173/1000 | Loss: 0.00001062
Iteration 174/1000 | Loss: 0.00001062
Iteration 175/1000 | Loss: 0.00001062
Iteration 176/1000 | Loss: 0.00001062
Iteration 177/1000 | Loss: 0.00001062
Iteration 178/1000 | Loss: 0.00001062
Iteration 179/1000 | Loss: 0.00001062
Iteration 180/1000 | Loss: 0.00001062
Iteration 181/1000 | Loss: 0.00001062
Iteration 182/1000 | Loss: 0.00001062
Iteration 183/1000 | Loss: 0.00001062
Iteration 184/1000 | Loss: 0.00001062
Iteration 185/1000 | Loss: 0.00001062
Iteration 186/1000 | Loss: 0.00001062
Iteration 187/1000 | Loss: 0.00001062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0620638931868598e-05, 1.0620638931868598e-05, 1.0620638931868598e-05, 1.0620638931868598e-05, 1.0620638931868598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0620638931868598e-05

Optimization complete. Final v2v error: 2.809906482696533 mm

Highest mean error: 3.126612663269043 mm for frame 70

Lowest mean error: 2.6702659130096436 mm for frame 3

Saving results

Total time: 37.1147825717926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551708
Iteration 2/25 | Loss: 0.00112100
Iteration 3/25 | Loss: 0.00085141
Iteration 4/25 | Loss: 0.00080602
Iteration 5/25 | Loss: 0.00079525
Iteration 6/25 | Loss: 0.00079244
Iteration 7/25 | Loss: 0.00079185
Iteration 8/25 | Loss: 0.00079179
Iteration 9/25 | Loss: 0.00079179
Iteration 10/25 | Loss: 0.00079179
Iteration 11/25 | Loss: 0.00079179
Iteration 12/25 | Loss: 0.00079179
Iteration 13/25 | Loss: 0.00079179
Iteration 14/25 | Loss: 0.00079179
Iteration 15/25 | Loss: 0.00079179
Iteration 16/25 | Loss: 0.00079179
Iteration 17/25 | Loss: 0.00079179
Iteration 18/25 | Loss: 0.00079179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007917947368696332, 0.0007917947368696332, 0.0007917947368696332, 0.0007917947368696332, 0.0007917947368696332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007917947368696332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65822554
Iteration 2/25 | Loss: 0.00118308
Iteration 3/25 | Loss: 0.00118307
Iteration 4/25 | Loss: 0.00118307
Iteration 5/25 | Loss: 0.00118307
Iteration 6/25 | Loss: 0.00118307
Iteration 7/25 | Loss: 0.00118307
Iteration 8/25 | Loss: 0.00118307
Iteration 9/25 | Loss: 0.00118307
Iteration 10/25 | Loss: 0.00118307
Iteration 11/25 | Loss: 0.00118307
Iteration 12/25 | Loss: 0.00118307
Iteration 13/25 | Loss: 0.00118307
Iteration 14/25 | Loss: 0.00118307
Iteration 15/25 | Loss: 0.00118307
Iteration 16/25 | Loss: 0.00118307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011830666335299611, 0.0011830666335299611, 0.0011830666335299611, 0.0011830666335299611, 0.0011830666335299611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011830666335299611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118307
Iteration 2/1000 | Loss: 0.00003492
Iteration 3/1000 | Loss: 0.00002414
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001864
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001799
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001782
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001732
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001719
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001717
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001713
Iteration 82/1000 | Loss: 0.00001713
Iteration 83/1000 | Loss: 0.00001713
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001712
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001711
Iteration 90/1000 | Loss: 0.00001711
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001710
Iteration 99/1000 | Loss: 0.00001710
Iteration 100/1000 | Loss: 0.00001710
Iteration 101/1000 | Loss: 0.00001710
Iteration 102/1000 | Loss: 0.00001710
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001710
Iteration 106/1000 | Loss: 0.00001710
Iteration 107/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.710092146822717e-05, 1.710092146822717e-05, 1.710092146822717e-05, 1.710092146822717e-05, 1.710092146822717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.710092146822717e-05

Optimization complete. Final v2v error: 3.5003302097320557 mm

Highest mean error: 3.8504838943481445 mm for frame 44

Lowest mean error: 3.0475382804870605 mm for frame 93

Saving results

Total time: 36.63259434700012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005081
Iteration 2/25 | Loss: 0.00447057
Iteration 3/25 | Loss: 0.00267500
Iteration 4/25 | Loss: 0.00232352
Iteration 5/25 | Loss: 0.00200582
Iteration 6/25 | Loss: 0.00187943
Iteration 7/25 | Loss: 0.00180735
Iteration 8/25 | Loss: 0.00178436
Iteration 9/25 | Loss: 0.00165808
Iteration 10/25 | Loss: 0.00165178
Iteration 11/25 | Loss: 0.00161572
Iteration 12/25 | Loss: 0.00161277
Iteration 13/25 | Loss: 0.00160598
Iteration 14/25 | Loss: 0.00157081
Iteration 15/25 | Loss: 0.00154571
Iteration 16/25 | Loss: 0.00155559
Iteration 17/25 | Loss: 0.00153807
Iteration 18/25 | Loss: 0.00153666
Iteration 19/25 | Loss: 0.00154530
Iteration 20/25 | Loss: 0.00152778
Iteration 21/25 | Loss: 0.00152900
Iteration 22/25 | Loss: 0.00152748
Iteration 23/25 | Loss: 0.00152435
Iteration 24/25 | Loss: 0.00152435
Iteration 25/25 | Loss: 0.00152434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55859780
Iteration 2/25 | Loss: 0.00574660
Iteration 3/25 | Loss: 0.00482189
Iteration 4/25 | Loss: 0.00481266
Iteration 5/25 | Loss: 0.00481266
Iteration 6/25 | Loss: 0.00481266
Iteration 7/25 | Loss: 0.00481266
Iteration 8/25 | Loss: 0.00481266
Iteration 9/25 | Loss: 0.00481266
Iteration 10/25 | Loss: 0.00481266
Iteration 11/25 | Loss: 0.00481266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0048126638866961, 0.0048126638866961, 0.0048126638866961, 0.0048126638866961, 0.0048126638866961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0048126638866961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00481266
Iteration 2/1000 | Loss: 0.00138345
Iteration 3/1000 | Loss: 0.00129127
Iteration 4/1000 | Loss: 0.00101428
Iteration 5/1000 | Loss: 0.00180891
Iteration 6/1000 | Loss: 0.00136121
Iteration 7/1000 | Loss: 0.00059191
Iteration 8/1000 | Loss: 0.00037468
Iteration 9/1000 | Loss: 0.00062482
Iteration 10/1000 | Loss: 0.00649073
Iteration 11/1000 | Loss: 0.01109985
Iteration 12/1000 | Loss: 0.00677758
Iteration 13/1000 | Loss: 0.00077151
Iteration 14/1000 | Loss: 0.00348217
Iteration 15/1000 | Loss: 0.00029864
Iteration 16/1000 | Loss: 0.00344471
Iteration 17/1000 | Loss: 0.00067348
Iteration 18/1000 | Loss: 0.00090580
Iteration 19/1000 | Loss: 0.00090923
Iteration 20/1000 | Loss: 0.00078459
Iteration 21/1000 | Loss: 0.00178420
Iteration 22/1000 | Loss: 0.00073975
Iteration 23/1000 | Loss: 0.00085438
Iteration 24/1000 | Loss: 0.00050783
Iteration 25/1000 | Loss: 0.00129496
Iteration 26/1000 | Loss: 0.00083763
Iteration 27/1000 | Loss: 0.00003523
Iteration 28/1000 | Loss: 0.00019041
Iteration 29/1000 | Loss: 0.00120249
Iteration 30/1000 | Loss: 0.00013472
Iteration 31/1000 | Loss: 0.00144548
Iteration 32/1000 | Loss: 0.00163707
Iteration 33/1000 | Loss: 0.00015999
Iteration 34/1000 | Loss: 0.00002213
Iteration 35/1000 | Loss: 0.00002062
Iteration 36/1000 | Loss: 0.00012362
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00020156
Iteration 39/1000 | Loss: 0.00133741
Iteration 40/1000 | Loss: 0.00039919
Iteration 41/1000 | Loss: 0.00009427
Iteration 42/1000 | Loss: 0.00038034
Iteration 43/1000 | Loss: 0.00028829
Iteration 44/1000 | Loss: 0.00010586
Iteration 45/1000 | Loss: 0.00004625
Iteration 46/1000 | Loss: 0.00008270
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001652
Iteration 53/1000 | Loss: 0.00001652
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001652
Iteration 56/1000 | Loss: 0.00001652
Iteration 57/1000 | Loss: 0.00001651
Iteration 58/1000 | Loss: 0.00001651
Iteration 59/1000 | Loss: 0.00006814
Iteration 60/1000 | Loss: 0.00006336
Iteration 61/1000 | Loss: 0.00001650
Iteration 62/1000 | Loss: 0.00001639
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00006746
Iteration 72/1000 | Loss: 0.00007541
Iteration 73/1000 | Loss: 0.00069933
Iteration 74/1000 | Loss: 0.00015876
Iteration 75/1000 | Loss: 0.00003259
Iteration 76/1000 | Loss: 0.00019324
Iteration 77/1000 | Loss: 0.00003577
Iteration 78/1000 | Loss: 0.00002752
Iteration 79/1000 | Loss: 0.00009184
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001633
Iteration 82/1000 | Loss: 0.00011605
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001617
Iteration 86/1000 | Loss: 0.00001617
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001616
Iteration 90/1000 | Loss: 0.00001616
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001614
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001612
Iteration 103/1000 | Loss: 0.00001612
Iteration 104/1000 | Loss: 0.00001612
Iteration 105/1000 | Loss: 0.00001612
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.6108990166685544e-05, 1.6108990166685544e-05, 1.6108990166685544e-05, 1.6108990166685544e-05, 1.6108990166685544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6108990166685544e-05

Optimization complete. Final v2v error: 3.416386842727661 mm

Highest mean error: 3.685333490371704 mm for frame 139

Lowest mean error: 3.1174159049987793 mm for frame 47

Saving results

Total time: 133.15245270729065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035527
Iteration 2/25 | Loss: 0.00312564
Iteration 3/25 | Loss: 0.00153831
Iteration 4/25 | Loss: 0.00131881
Iteration 5/25 | Loss: 0.00120706
Iteration 6/25 | Loss: 0.00137021
Iteration 7/25 | Loss: 0.00122663
Iteration 8/25 | Loss: 0.00106628
Iteration 9/25 | Loss: 0.00093491
Iteration 10/25 | Loss: 0.00092858
Iteration 11/25 | Loss: 0.00091580
Iteration 12/25 | Loss: 0.00087558
Iteration 13/25 | Loss: 0.00090520
Iteration 14/25 | Loss: 0.00084657
Iteration 15/25 | Loss: 0.00084085
Iteration 16/25 | Loss: 0.00083678
Iteration 17/25 | Loss: 0.00082180
Iteration 18/25 | Loss: 0.00081415
Iteration 19/25 | Loss: 0.00081019
Iteration 20/25 | Loss: 0.00080948
Iteration 21/25 | Loss: 0.00080927
Iteration 22/25 | Loss: 0.00080923
Iteration 23/25 | Loss: 0.00080922
Iteration 24/25 | Loss: 0.00080922
Iteration 25/25 | Loss: 0.00080922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64692581
Iteration 2/25 | Loss: 0.00136851
Iteration 3/25 | Loss: 0.00136851
Iteration 4/25 | Loss: 0.00136851
Iteration 5/25 | Loss: 0.00136851
Iteration 6/25 | Loss: 0.00136851
Iteration 7/25 | Loss: 0.00136851
Iteration 8/25 | Loss: 0.00136851
Iteration 9/25 | Loss: 0.00136851
Iteration 10/25 | Loss: 0.00136851
Iteration 11/25 | Loss: 0.00136851
Iteration 12/25 | Loss: 0.00136851
Iteration 13/25 | Loss: 0.00136851
Iteration 14/25 | Loss: 0.00136851
Iteration 15/25 | Loss: 0.00136851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013685086742043495, 0.0013685086742043495, 0.0013685086742043495, 0.0013685086742043495, 0.0013685086742043495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013685086742043495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136851
Iteration 2/1000 | Loss: 0.00204189
Iteration 3/1000 | Loss: 0.00112256
Iteration 4/1000 | Loss: 0.00302299
Iteration 5/1000 | Loss: 0.00187869
Iteration 6/1000 | Loss: 0.00195461
Iteration 7/1000 | Loss: 0.00222598
Iteration 8/1000 | Loss: 0.00045673
Iteration 9/1000 | Loss: 0.00192080
Iteration 10/1000 | Loss: 0.00191018
Iteration 11/1000 | Loss: 0.00137994
Iteration 12/1000 | Loss: 0.00187547
Iteration 13/1000 | Loss: 0.00195726
Iteration 14/1000 | Loss: 0.00011444
Iteration 15/1000 | Loss: 0.00151644
Iteration 16/1000 | Loss: 0.00146405
Iteration 17/1000 | Loss: 0.00017507
Iteration 18/1000 | Loss: 0.00006459
Iteration 19/1000 | Loss: 0.00174479
Iteration 20/1000 | Loss: 0.00162750
Iteration 21/1000 | Loss: 0.00188711
Iteration 22/1000 | Loss: 0.00257265
Iteration 23/1000 | Loss: 0.00165824
Iteration 24/1000 | Loss: 0.00305060
Iteration 25/1000 | Loss: 0.00226968
Iteration 26/1000 | Loss: 0.00224624
Iteration 27/1000 | Loss: 0.00433017
Iteration 28/1000 | Loss: 0.00397957
Iteration 29/1000 | Loss: 0.00209714
Iteration 30/1000 | Loss: 0.00211738
Iteration 31/1000 | Loss: 0.00233427
Iteration 32/1000 | Loss: 0.00156592
Iteration 33/1000 | Loss: 0.00312871
Iteration 34/1000 | Loss: 0.00306325
Iteration 35/1000 | Loss: 0.00039067
Iteration 36/1000 | Loss: 0.00093206
Iteration 37/1000 | Loss: 0.00007940
Iteration 38/1000 | Loss: 0.00187153
Iteration 39/1000 | Loss: 0.00007556
Iteration 40/1000 | Loss: 0.00006194
Iteration 41/1000 | Loss: 0.00212150
Iteration 42/1000 | Loss: 0.00374052
Iteration 43/1000 | Loss: 0.00105614
Iteration 44/1000 | Loss: 0.00153235
Iteration 45/1000 | Loss: 0.00088069
Iteration 46/1000 | Loss: 0.00054056
Iteration 47/1000 | Loss: 0.00208293
Iteration 48/1000 | Loss: 0.00213896
Iteration 49/1000 | Loss: 0.00268354
Iteration 50/1000 | Loss: 0.00148964
Iteration 51/1000 | Loss: 0.00215600
Iteration 52/1000 | Loss: 0.00174122
Iteration 53/1000 | Loss: 0.00172908
Iteration 54/1000 | Loss: 0.00162115
Iteration 55/1000 | Loss: 0.00393052
Iteration 56/1000 | Loss: 0.00212972
Iteration 57/1000 | Loss: 0.00225888
Iteration 58/1000 | Loss: 0.00067166
Iteration 59/1000 | Loss: 0.00124513
Iteration 60/1000 | Loss: 0.00138770
Iteration 61/1000 | Loss: 0.00016964
Iteration 62/1000 | Loss: 0.00235977
Iteration 63/1000 | Loss: 0.00170475
Iteration 64/1000 | Loss: 0.00106824
Iteration 65/1000 | Loss: 0.00390606
Iteration 66/1000 | Loss: 0.00270520
Iteration 67/1000 | Loss: 0.00189777
Iteration 68/1000 | Loss: 0.00099360
Iteration 69/1000 | Loss: 0.00119959
Iteration 70/1000 | Loss: 0.00012055
Iteration 71/1000 | Loss: 0.00149415
Iteration 72/1000 | Loss: 0.00321471
Iteration 73/1000 | Loss: 0.00258871
Iteration 74/1000 | Loss: 0.00232712
Iteration 75/1000 | Loss: 0.00142731
Iteration 76/1000 | Loss: 0.00096055
Iteration 77/1000 | Loss: 0.00242964
Iteration 78/1000 | Loss: 0.00170057
Iteration 79/1000 | Loss: 0.00115785
Iteration 80/1000 | Loss: 0.00062923
Iteration 81/1000 | Loss: 0.00151053
Iteration 82/1000 | Loss: 0.00077394
Iteration 83/1000 | Loss: 0.00158245
Iteration 84/1000 | Loss: 0.00107253
Iteration 85/1000 | Loss: 0.00162051
Iteration 86/1000 | Loss: 0.00076667
Iteration 87/1000 | Loss: 0.00062867
Iteration 88/1000 | Loss: 0.00175806
Iteration 89/1000 | Loss: 0.00227848
Iteration 90/1000 | Loss: 0.00157722
Iteration 91/1000 | Loss: 0.00106933
Iteration 92/1000 | Loss: 0.00142089
Iteration 93/1000 | Loss: 0.00112432
Iteration 94/1000 | Loss: 0.00053582
Iteration 95/1000 | Loss: 0.00244409
Iteration 96/1000 | Loss: 0.00065943
Iteration 97/1000 | Loss: 0.00092084
Iteration 98/1000 | Loss: 0.00142135
Iteration 99/1000 | Loss: 0.00120338
Iteration 100/1000 | Loss: 0.00080131
Iteration 101/1000 | Loss: 0.00123041
Iteration 102/1000 | Loss: 0.00162840
Iteration 103/1000 | Loss: 0.00100817
Iteration 104/1000 | Loss: 0.00070055
Iteration 105/1000 | Loss: 0.00310084
Iteration 106/1000 | Loss: 0.00263656
Iteration 107/1000 | Loss: 0.00109798
Iteration 108/1000 | Loss: 0.00035511
Iteration 109/1000 | Loss: 0.00016412
Iteration 110/1000 | Loss: 0.00011453
Iteration 111/1000 | Loss: 0.00010742
Iteration 112/1000 | Loss: 0.00005433
Iteration 113/1000 | Loss: 0.00006256
Iteration 114/1000 | Loss: 0.00057140
Iteration 115/1000 | Loss: 0.00008723
Iteration 116/1000 | Loss: 0.00004980
Iteration 117/1000 | Loss: 0.00004065
Iteration 118/1000 | Loss: 0.00066428
Iteration 119/1000 | Loss: 0.00009849
Iteration 120/1000 | Loss: 0.00088383
Iteration 121/1000 | Loss: 0.00042217
Iteration 122/1000 | Loss: 0.00006691
Iteration 123/1000 | Loss: 0.00003814
Iteration 124/1000 | Loss: 0.00069348
Iteration 125/1000 | Loss: 0.00003269
Iteration 126/1000 | Loss: 0.00003187
Iteration 127/1000 | Loss: 0.00041393
Iteration 128/1000 | Loss: 0.00030637
Iteration 129/1000 | Loss: 0.00005812
Iteration 130/1000 | Loss: 0.00004558
Iteration 131/1000 | Loss: 0.00002939
Iteration 132/1000 | Loss: 0.00002806
Iteration 133/1000 | Loss: 0.00043446
Iteration 134/1000 | Loss: 0.00003792
Iteration 135/1000 | Loss: 0.00006396
Iteration 136/1000 | Loss: 0.00005344
Iteration 137/1000 | Loss: 0.00024438
Iteration 138/1000 | Loss: 0.00083279
Iteration 139/1000 | Loss: 0.00053254
Iteration 140/1000 | Loss: 0.00069696
Iteration 141/1000 | Loss: 0.00052062
Iteration 142/1000 | Loss: 0.00026305
Iteration 143/1000 | Loss: 0.00017858
Iteration 144/1000 | Loss: 0.00072785
Iteration 145/1000 | Loss: 0.00038761
Iteration 146/1000 | Loss: 0.00003368
Iteration 147/1000 | Loss: 0.00003076
Iteration 148/1000 | Loss: 0.00002911
Iteration 149/1000 | Loss: 0.00005348
Iteration 150/1000 | Loss: 0.00085804
Iteration 151/1000 | Loss: 0.00022869
Iteration 152/1000 | Loss: 0.00004698
Iteration 153/1000 | Loss: 0.00046764
Iteration 154/1000 | Loss: 0.00032840
Iteration 155/1000 | Loss: 0.00107868
Iteration 156/1000 | Loss: 0.00144099
Iteration 157/1000 | Loss: 0.00035622
Iteration 158/1000 | Loss: 0.00088597
Iteration 159/1000 | Loss: 0.00082635
Iteration 160/1000 | Loss: 0.00032243
Iteration 161/1000 | Loss: 0.00009954
Iteration 162/1000 | Loss: 0.00008974
Iteration 163/1000 | Loss: 0.00100454
Iteration 164/1000 | Loss: 0.00009199
Iteration 165/1000 | Loss: 0.00008249
Iteration 166/1000 | Loss: 0.00007300
Iteration 167/1000 | Loss: 0.00002892
Iteration 168/1000 | Loss: 0.00002548
Iteration 169/1000 | Loss: 0.00053427
Iteration 170/1000 | Loss: 0.00058763
Iteration 171/1000 | Loss: 0.00008889
Iteration 172/1000 | Loss: 0.00036517
Iteration 173/1000 | Loss: 0.00015405
Iteration 174/1000 | Loss: 0.00003057
Iteration 175/1000 | Loss: 0.00003657
Iteration 176/1000 | Loss: 0.00002444
Iteration 177/1000 | Loss: 0.00002173
Iteration 178/1000 | Loss: 0.00001967
Iteration 179/1000 | Loss: 0.00001937
Iteration 180/1000 | Loss: 0.00001785
Iteration 181/1000 | Loss: 0.00001733
Iteration 182/1000 | Loss: 0.00001686
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001626
Iteration 185/1000 | Loss: 0.00001606
Iteration 186/1000 | Loss: 0.00001599
Iteration 187/1000 | Loss: 0.00001581
Iteration 188/1000 | Loss: 0.00001559
Iteration 189/1000 | Loss: 0.00001558
Iteration 190/1000 | Loss: 0.00001556
Iteration 191/1000 | Loss: 0.00001542
Iteration 192/1000 | Loss: 0.00001534
Iteration 193/1000 | Loss: 0.00001534
Iteration 194/1000 | Loss: 0.00001534
Iteration 195/1000 | Loss: 0.00001534
Iteration 196/1000 | Loss: 0.00001534
Iteration 197/1000 | Loss: 0.00001534
Iteration 198/1000 | Loss: 0.00001534
Iteration 199/1000 | Loss: 0.00001533
Iteration 200/1000 | Loss: 0.00001533
Iteration 201/1000 | Loss: 0.00001531
Iteration 202/1000 | Loss: 0.00001531
Iteration 203/1000 | Loss: 0.00001531
Iteration 204/1000 | Loss: 0.00001531
Iteration 205/1000 | Loss: 0.00001531
Iteration 206/1000 | Loss: 0.00001531
Iteration 207/1000 | Loss: 0.00001531
Iteration 208/1000 | Loss: 0.00001531
Iteration 209/1000 | Loss: 0.00001531
Iteration 210/1000 | Loss: 0.00001531
Iteration 211/1000 | Loss: 0.00001531
Iteration 212/1000 | Loss: 0.00001530
Iteration 213/1000 | Loss: 0.00001530
Iteration 214/1000 | Loss: 0.00001530
Iteration 215/1000 | Loss: 0.00001529
Iteration 216/1000 | Loss: 0.00001529
Iteration 217/1000 | Loss: 0.00001528
Iteration 218/1000 | Loss: 0.00001528
Iteration 219/1000 | Loss: 0.00001528
Iteration 220/1000 | Loss: 0.00001527
Iteration 221/1000 | Loss: 0.00001527
Iteration 222/1000 | Loss: 0.00001527
Iteration 223/1000 | Loss: 0.00001527
Iteration 224/1000 | Loss: 0.00001527
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001527
Iteration 227/1000 | Loss: 0.00001527
Iteration 228/1000 | Loss: 0.00001526
Iteration 229/1000 | Loss: 0.00001526
Iteration 230/1000 | Loss: 0.00001526
Iteration 231/1000 | Loss: 0.00001526
Iteration 232/1000 | Loss: 0.00001526
Iteration 233/1000 | Loss: 0.00001526
Iteration 234/1000 | Loss: 0.00001526
Iteration 235/1000 | Loss: 0.00001526
Iteration 236/1000 | Loss: 0.00001526
Iteration 237/1000 | Loss: 0.00001526
Iteration 238/1000 | Loss: 0.00001526
Iteration 239/1000 | Loss: 0.00001526
Iteration 240/1000 | Loss: 0.00001525
Iteration 241/1000 | Loss: 0.00001525
Iteration 242/1000 | Loss: 0.00001525
Iteration 243/1000 | Loss: 0.00001525
Iteration 244/1000 | Loss: 0.00001525
Iteration 245/1000 | Loss: 0.00001525
Iteration 246/1000 | Loss: 0.00001525
Iteration 247/1000 | Loss: 0.00001525
Iteration 248/1000 | Loss: 0.00001525
Iteration 249/1000 | Loss: 0.00001525
Iteration 250/1000 | Loss: 0.00001525
Iteration 251/1000 | Loss: 0.00001525
Iteration 252/1000 | Loss: 0.00001524
Iteration 253/1000 | Loss: 0.00001524
Iteration 254/1000 | Loss: 0.00001524
Iteration 255/1000 | Loss: 0.00001524
Iteration 256/1000 | Loss: 0.00001524
Iteration 257/1000 | Loss: 0.00001524
Iteration 258/1000 | Loss: 0.00001524
Iteration 259/1000 | Loss: 0.00001523
Iteration 260/1000 | Loss: 0.00001523
Iteration 261/1000 | Loss: 0.00001523
Iteration 262/1000 | Loss: 0.00001523
Iteration 263/1000 | Loss: 0.00001523
Iteration 264/1000 | Loss: 0.00001523
Iteration 265/1000 | Loss: 0.00001523
Iteration 266/1000 | Loss: 0.00001523
Iteration 267/1000 | Loss: 0.00001523
Iteration 268/1000 | Loss: 0.00001522
Iteration 269/1000 | Loss: 0.00001522
Iteration 270/1000 | Loss: 0.00001522
Iteration 271/1000 | Loss: 0.00001522
Iteration 272/1000 | Loss: 0.00001522
Iteration 273/1000 | Loss: 0.00001522
Iteration 274/1000 | Loss: 0.00001522
Iteration 275/1000 | Loss: 0.00001522
Iteration 276/1000 | Loss: 0.00001522
Iteration 277/1000 | Loss: 0.00001522
Iteration 278/1000 | Loss: 0.00001522
Iteration 279/1000 | Loss: 0.00001522
Iteration 280/1000 | Loss: 0.00001522
Iteration 281/1000 | Loss: 0.00001522
Iteration 282/1000 | Loss: 0.00001521
Iteration 283/1000 | Loss: 0.00001521
Iteration 284/1000 | Loss: 0.00001521
Iteration 285/1000 | Loss: 0.00001521
Iteration 286/1000 | Loss: 0.00001521
Iteration 287/1000 | Loss: 0.00001521
Iteration 288/1000 | Loss: 0.00001521
Iteration 289/1000 | Loss: 0.00001521
Iteration 290/1000 | Loss: 0.00001521
Iteration 291/1000 | Loss: 0.00001521
Iteration 292/1000 | Loss: 0.00001521
Iteration 293/1000 | Loss: 0.00001521
Iteration 294/1000 | Loss: 0.00001521
Iteration 295/1000 | Loss: 0.00001521
Iteration 296/1000 | Loss: 0.00001521
Iteration 297/1000 | Loss: 0.00001521
Iteration 298/1000 | Loss: 0.00001521
Iteration 299/1000 | Loss: 0.00001521
Iteration 300/1000 | Loss: 0.00001521
Iteration 301/1000 | Loss: 0.00001521
Iteration 302/1000 | Loss: 0.00001521
Iteration 303/1000 | Loss: 0.00001521
Iteration 304/1000 | Loss: 0.00001521
Iteration 305/1000 | Loss: 0.00001521
Iteration 306/1000 | Loss: 0.00001521
Iteration 307/1000 | Loss: 0.00001521
Iteration 308/1000 | Loss: 0.00001521
Iteration 309/1000 | Loss: 0.00001521
Iteration 310/1000 | Loss: 0.00001521
Iteration 311/1000 | Loss: 0.00001521
Iteration 312/1000 | Loss: 0.00001521
Iteration 313/1000 | Loss: 0.00001521
Iteration 314/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [1.5213799088087399e-05, 1.5213799088087399e-05, 1.5213799088087399e-05, 1.5213799088087399e-05, 1.5213799088087399e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5213799088087399e-05

Optimization complete. Final v2v error: 3.25641131401062 mm

Highest mean error: 4.827668190002441 mm for frame 79

Lowest mean error: 2.8804845809936523 mm for frame 136

Saving results

Total time: 301.94012093544006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390970
Iteration 2/25 | Loss: 0.00108504
Iteration 3/25 | Loss: 0.00089923
Iteration 4/25 | Loss: 0.00086612
Iteration 5/25 | Loss: 0.00085604
Iteration 6/25 | Loss: 0.00085271
Iteration 7/25 | Loss: 0.00085150
Iteration 8/25 | Loss: 0.00085116
Iteration 9/25 | Loss: 0.00085116
Iteration 10/25 | Loss: 0.00085116
Iteration 11/25 | Loss: 0.00085116
Iteration 12/25 | Loss: 0.00085116
Iteration 13/25 | Loss: 0.00085116
Iteration 14/25 | Loss: 0.00085116
Iteration 15/25 | Loss: 0.00085116
Iteration 16/25 | Loss: 0.00085116
Iteration 17/25 | Loss: 0.00085116
Iteration 18/25 | Loss: 0.00085116
Iteration 19/25 | Loss: 0.00085116
Iteration 20/25 | Loss: 0.00085116
Iteration 21/25 | Loss: 0.00085116
Iteration 22/25 | Loss: 0.00085116
Iteration 23/25 | Loss: 0.00085116
Iteration 24/25 | Loss: 0.00085116
Iteration 25/25 | Loss: 0.00085116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72741508
Iteration 2/25 | Loss: 0.00191456
Iteration 3/25 | Loss: 0.00191456
Iteration 4/25 | Loss: 0.00191456
Iteration 5/25 | Loss: 0.00191456
Iteration 6/25 | Loss: 0.00191456
Iteration 7/25 | Loss: 0.00191456
Iteration 8/25 | Loss: 0.00191456
Iteration 9/25 | Loss: 0.00191456
Iteration 10/25 | Loss: 0.00191456
Iteration 11/25 | Loss: 0.00191456
Iteration 12/25 | Loss: 0.00191456
Iteration 13/25 | Loss: 0.00191456
Iteration 14/25 | Loss: 0.00191456
Iteration 15/25 | Loss: 0.00191456
Iteration 16/25 | Loss: 0.00191456
Iteration 17/25 | Loss: 0.00191456
Iteration 18/25 | Loss: 0.00191456
Iteration 19/25 | Loss: 0.00191456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019145599799230695, 0.0019145599799230695, 0.0019145599799230695, 0.0019145599799230695, 0.0019145599799230695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019145599799230695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191456
Iteration 2/1000 | Loss: 0.00005146
Iteration 3/1000 | Loss: 0.00004069
Iteration 4/1000 | Loss: 0.00003409
Iteration 5/1000 | Loss: 0.00003087
Iteration 6/1000 | Loss: 0.00002948
Iteration 7/1000 | Loss: 0.00002840
Iteration 8/1000 | Loss: 0.00002749
Iteration 9/1000 | Loss: 0.00002676
Iteration 10/1000 | Loss: 0.00002625
Iteration 11/1000 | Loss: 0.00002595
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00002544
Iteration 14/1000 | Loss: 0.00002523
Iteration 15/1000 | Loss: 0.00002514
Iteration 16/1000 | Loss: 0.00002506
Iteration 17/1000 | Loss: 0.00002489
Iteration 18/1000 | Loss: 0.00002485
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002483
Iteration 21/1000 | Loss: 0.00002481
Iteration 22/1000 | Loss: 0.00002478
Iteration 23/1000 | Loss: 0.00002478
Iteration 24/1000 | Loss: 0.00002477
Iteration 25/1000 | Loss: 0.00002477
Iteration 26/1000 | Loss: 0.00002476
Iteration 27/1000 | Loss: 0.00002476
Iteration 28/1000 | Loss: 0.00002475
Iteration 29/1000 | Loss: 0.00002475
Iteration 30/1000 | Loss: 0.00002474
Iteration 31/1000 | Loss: 0.00002473
Iteration 32/1000 | Loss: 0.00002473
Iteration 33/1000 | Loss: 0.00002472
Iteration 34/1000 | Loss: 0.00002472
Iteration 35/1000 | Loss: 0.00002471
Iteration 36/1000 | Loss: 0.00002471
Iteration 37/1000 | Loss: 0.00002471
Iteration 38/1000 | Loss: 0.00002470
Iteration 39/1000 | Loss: 0.00002469
Iteration 40/1000 | Loss: 0.00002469
Iteration 41/1000 | Loss: 0.00002468
Iteration 42/1000 | Loss: 0.00002468
Iteration 43/1000 | Loss: 0.00002468
Iteration 44/1000 | Loss: 0.00002467
Iteration 45/1000 | Loss: 0.00002467
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002466
Iteration 48/1000 | Loss: 0.00002465
Iteration 49/1000 | Loss: 0.00002465
Iteration 50/1000 | Loss: 0.00002464
Iteration 51/1000 | Loss: 0.00002464
Iteration 52/1000 | Loss: 0.00002464
Iteration 53/1000 | Loss: 0.00002463
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002463
Iteration 56/1000 | Loss: 0.00002463
Iteration 57/1000 | Loss: 0.00002463
Iteration 58/1000 | Loss: 0.00002462
Iteration 59/1000 | Loss: 0.00002462
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002462
Iteration 62/1000 | Loss: 0.00002462
Iteration 63/1000 | Loss: 0.00002462
Iteration 64/1000 | Loss: 0.00002462
Iteration 65/1000 | Loss: 0.00002462
Iteration 66/1000 | Loss: 0.00002461
Iteration 67/1000 | Loss: 0.00002461
Iteration 68/1000 | Loss: 0.00002461
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002461
Iteration 72/1000 | Loss: 0.00002460
Iteration 73/1000 | Loss: 0.00002460
Iteration 74/1000 | Loss: 0.00002460
Iteration 75/1000 | Loss: 0.00002460
Iteration 76/1000 | Loss: 0.00002460
Iteration 77/1000 | Loss: 0.00002460
Iteration 78/1000 | Loss: 0.00002460
Iteration 79/1000 | Loss: 0.00002459
Iteration 80/1000 | Loss: 0.00002459
Iteration 81/1000 | Loss: 0.00002459
Iteration 82/1000 | Loss: 0.00002459
Iteration 83/1000 | Loss: 0.00002459
Iteration 84/1000 | Loss: 0.00002459
Iteration 85/1000 | Loss: 0.00002458
Iteration 86/1000 | Loss: 0.00002458
Iteration 87/1000 | Loss: 0.00002458
Iteration 88/1000 | Loss: 0.00002458
Iteration 89/1000 | Loss: 0.00002458
Iteration 90/1000 | Loss: 0.00002458
Iteration 91/1000 | Loss: 0.00002458
Iteration 92/1000 | Loss: 0.00002458
Iteration 93/1000 | Loss: 0.00002458
Iteration 94/1000 | Loss: 0.00002458
Iteration 95/1000 | Loss: 0.00002458
Iteration 96/1000 | Loss: 0.00002457
Iteration 97/1000 | Loss: 0.00002457
Iteration 98/1000 | Loss: 0.00002457
Iteration 99/1000 | Loss: 0.00002457
Iteration 100/1000 | Loss: 0.00002457
Iteration 101/1000 | Loss: 0.00002457
Iteration 102/1000 | Loss: 0.00002457
Iteration 103/1000 | Loss: 0.00002457
Iteration 104/1000 | Loss: 0.00002457
Iteration 105/1000 | Loss: 0.00002457
Iteration 106/1000 | Loss: 0.00002457
Iteration 107/1000 | Loss: 0.00002457
Iteration 108/1000 | Loss: 0.00002457
Iteration 109/1000 | Loss: 0.00002456
Iteration 110/1000 | Loss: 0.00002456
Iteration 111/1000 | Loss: 0.00002456
Iteration 112/1000 | Loss: 0.00002456
Iteration 113/1000 | Loss: 0.00002456
Iteration 114/1000 | Loss: 0.00002456
Iteration 115/1000 | Loss: 0.00002456
Iteration 116/1000 | Loss: 0.00002456
Iteration 117/1000 | Loss: 0.00002456
Iteration 118/1000 | Loss: 0.00002456
Iteration 119/1000 | Loss: 0.00002456
Iteration 120/1000 | Loss: 0.00002456
Iteration 121/1000 | Loss: 0.00002456
Iteration 122/1000 | Loss: 0.00002456
Iteration 123/1000 | Loss: 0.00002455
Iteration 124/1000 | Loss: 0.00002455
Iteration 125/1000 | Loss: 0.00002455
Iteration 126/1000 | Loss: 0.00002455
Iteration 127/1000 | Loss: 0.00002455
Iteration 128/1000 | Loss: 0.00002455
Iteration 129/1000 | Loss: 0.00002455
Iteration 130/1000 | Loss: 0.00002455
Iteration 131/1000 | Loss: 0.00002455
Iteration 132/1000 | Loss: 0.00002455
Iteration 133/1000 | Loss: 0.00002455
Iteration 134/1000 | Loss: 0.00002455
Iteration 135/1000 | Loss: 0.00002455
Iteration 136/1000 | Loss: 0.00002454
Iteration 137/1000 | Loss: 0.00002454
Iteration 138/1000 | Loss: 0.00002454
Iteration 139/1000 | Loss: 0.00002454
Iteration 140/1000 | Loss: 0.00002454
Iteration 141/1000 | Loss: 0.00002454
Iteration 142/1000 | Loss: 0.00002454
Iteration 143/1000 | Loss: 0.00002454
Iteration 144/1000 | Loss: 0.00002454
Iteration 145/1000 | Loss: 0.00002454
Iteration 146/1000 | Loss: 0.00002454
Iteration 147/1000 | Loss: 0.00002454
Iteration 148/1000 | Loss: 0.00002454
Iteration 149/1000 | Loss: 0.00002454
Iteration 150/1000 | Loss: 0.00002454
Iteration 151/1000 | Loss: 0.00002454
Iteration 152/1000 | Loss: 0.00002454
Iteration 153/1000 | Loss: 0.00002454
Iteration 154/1000 | Loss: 0.00002454
Iteration 155/1000 | Loss: 0.00002454
Iteration 156/1000 | Loss: 0.00002454
Iteration 157/1000 | Loss: 0.00002454
Iteration 158/1000 | Loss: 0.00002454
Iteration 159/1000 | Loss: 0.00002454
Iteration 160/1000 | Loss: 0.00002454
Iteration 161/1000 | Loss: 0.00002453
Iteration 162/1000 | Loss: 0.00002453
Iteration 163/1000 | Loss: 0.00002453
Iteration 164/1000 | Loss: 0.00002453
Iteration 165/1000 | Loss: 0.00002453
Iteration 166/1000 | Loss: 0.00002453
Iteration 167/1000 | Loss: 0.00002453
Iteration 168/1000 | Loss: 0.00002453
Iteration 169/1000 | Loss: 0.00002453
Iteration 170/1000 | Loss: 0.00002453
Iteration 171/1000 | Loss: 0.00002453
Iteration 172/1000 | Loss: 0.00002453
Iteration 173/1000 | Loss: 0.00002453
Iteration 174/1000 | Loss: 0.00002453
Iteration 175/1000 | Loss: 0.00002453
Iteration 176/1000 | Loss: 0.00002453
Iteration 177/1000 | Loss: 0.00002453
Iteration 178/1000 | Loss: 0.00002453
Iteration 179/1000 | Loss: 0.00002453
Iteration 180/1000 | Loss: 0.00002453
Iteration 181/1000 | Loss: 0.00002453
Iteration 182/1000 | Loss: 0.00002453
Iteration 183/1000 | Loss: 0.00002453
Iteration 184/1000 | Loss: 0.00002453
Iteration 185/1000 | Loss: 0.00002453
Iteration 186/1000 | Loss: 0.00002453
Iteration 187/1000 | Loss: 0.00002453
Iteration 188/1000 | Loss: 0.00002453
Iteration 189/1000 | Loss: 0.00002453
Iteration 190/1000 | Loss: 0.00002453
Iteration 191/1000 | Loss: 0.00002453
Iteration 192/1000 | Loss: 0.00002453
Iteration 193/1000 | Loss: 0.00002453
Iteration 194/1000 | Loss: 0.00002453
Iteration 195/1000 | Loss: 0.00002453
Iteration 196/1000 | Loss: 0.00002453
Iteration 197/1000 | Loss: 0.00002453
Iteration 198/1000 | Loss: 0.00002453
Iteration 199/1000 | Loss: 0.00002453
Iteration 200/1000 | Loss: 0.00002453
Iteration 201/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.4534654585295357e-05, 2.4534654585295357e-05, 2.4534654585295357e-05, 2.4534654585295357e-05, 2.4534654585295357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4534654585295357e-05

Optimization complete. Final v2v error: 3.966189384460449 mm

Highest mean error: 5.746677875518799 mm for frame 76

Lowest mean error: 2.8193607330322266 mm for frame 34

Saving results

Total time: 45.108041763305664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580260
Iteration 2/25 | Loss: 0.00088094
Iteration 3/25 | Loss: 0.00075300
Iteration 4/25 | Loss: 0.00073661
Iteration 5/25 | Loss: 0.00073286
Iteration 6/25 | Loss: 0.00073172
Iteration 7/25 | Loss: 0.00073152
Iteration 8/25 | Loss: 0.00073152
Iteration 9/25 | Loss: 0.00073152
Iteration 10/25 | Loss: 0.00073152
Iteration 11/25 | Loss: 0.00073152
Iteration 12/25 | Loss: 0.00073152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007315228576771915, 0.0007315228576771915, 0.0007315228576771915, 0.0007315228576771915, 0.0007315228576771915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007315228576771915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06860971
Iteration 2/25 | Loss: 0.00123947
Iteration 3/25 | Loss: 0.00123946
Iteration 4/25 | Loss: 0.00123946
Iteration 5/25 | Loss: 0.00123946
Iteration 6/25 | Loss: 0.00123946
Iteration 7/25 | Loss: 0.00123946
Iteration 8/25 | Loss: 0.00123946
Iteration 9/25 | Loss: 0.00123946
Iteration 10/25 | Loss: 0.00123946
Iteration 11/25 | Loss: 0.00123946
Iteration 12/25 | Loss: 0.00123946
Iteration 13/25 | Loss: 0.00123946
Iteration 14/25 | Loss: 0.00123946
Iteration 15/25 | Loss: 0.00123946
Iteration 16/25 | Loss: 0.00123946
Iteration 17/25 | Loss: 0.00123946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012394610093906522, 0.0012394610093906522, 0.0012394610093906522, 0.0012394610093906522, 0.0012394610093906522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012394610093906522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123946
Iteration 2/1000 | Loss: 0.00002056
Iteration 3/1000 | Loss: 0.00001386
Iteration 4/1000 | Loss: 0.00001318
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001178
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001161
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001159
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001144
Iteration 54/1000 | Loss: 0.00001144
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001144
Iteration 57/1000 | Loss: 0.00001143
Iteration 58/1000 | Loss: 0.00001143
Iteration 59/1000 | Loss: 0.00001143
Iteration 60/1000 | Loss: 0.00001143
Iteration 61/1000 | Loss: 0.00001143
Iteration 62/1000 | Loss: 0.00001143
Iteration 63/1000 | Loss: 0.00001143
Iteration 64/1000 | Loss: 0.00001142
Iteration 65/1000 | Loss: 0.00001142
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001141
Iteration 70/1000 | Loss: 0.00001141
Iteration 71/1000 | Loss: 0.00001141
Iteration 72/1000 | Loss: 0.00001141
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001138
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.130383043346228e-05, 1.130383043346228e-05, 1.130383043346228e-05, 1.130383043346228e-05, 1.130383043346228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.130383043346228e-05

Optimization complete. Final v2v error: 2.8785293102264404 mm

Highest mean error: 3.2331082820892334 mm for frame 118

Lowest mean error: 2.756147861480713 mm for frame 149

Saving results

Total time: 33.66441202163696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038211
Iteration 2/25 | Loss: 0.00167186
Iteration 3/25 | Loss: 0.00120398
Iteration 4/25 | Loss: 0.00111758
Iteration 5/25 | Loss: 0.00105579
Iteration 6/25 | Loss: 0.00103842
Iteration 7/25 | Loss: 0.00102685
Iteration 8/25 | Loss: 0.00102283
Iteration 9/25 | Loss: 0.00103567
Iteration 10/25 | Loss: 0.00104483
Iteration 11/25 | Loss: 0.00103856
Iteration 12/25 | Loss: 0.00102437
Iteration 13/25 | Loss: 0.00101943
Iteration 14/25 | Loss: 0.00100978
Iteration 15/25 | Loss: 0.00100644
Iteration 16/25 | Loss: 0.00100151
Iteration 17/25 | Loss: 0.00100364
Iteration 18/25 | Loss: 0.00099940
Iteration 19/25 | Loss: 0.00099017
Iteration 20/25 | Loss: 0.00099249
Iteration 21/25 | Loss: 0.00098953
Iteration 22/25 | Loss: 0.00098921
Iteration 23/25 | Loss: 0.00098872
Iteration 24/25 | Loss: 0.00098519
Iteration 25/25 | Loss: 0.00099033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98009837
Iteration 2/25 | Loss: 0.00287746
Iteration 3/25 | Loss: 0.00274379
Iteration 4/25 | Loss: 0.00274379
Iteration 5/25 | Loss: 0.00274379
Iteration 6/25 | Loss: 0.00274379
Iteration 7/25 | Loss: 0.00274379
Iteration 8/25 | Loss: 0.00274379
Iteration 9/25 | Loss: 0.00274379
Iteration 10/25 | Loss: 0.00274379
Iteration 11/25 | Loss: 0.00274379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0027437934186309576, 0.0027437934186309576, 0.0027437934186309576, 0.0027437934186309576, 0.0027437934186309576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027437934186309576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00274379
Iteration 2/1000 | Loss: 0.00040034
Iteration 3/1000 | Loss: 0.00051637
Iteration 4/1000 | Loss: 0.00058092
Iteration 5/1000 | Loss: 0.00075538
Iteration 6/1000 | Loss: 0.00029072
Iteration 7/1000 | Loss: 0.00071241
Iteration 8/1000 | Loss: 0.00139003
Iteration 9/1000 | Loss: 0.00119693
Iteration 10/1000 | Loss: 0.00098777
Iteration 11/1000 | Loss: 0.00084748
Iteration 12/1000 | Loss: 0.00126882
Iteration 13/1000 | Loss: 0.00103169
Iteration 14/1000 | Loss: 0.00104776
Iteration 15/1000 | Loss: 0.00128820
Iteration 16/1000 | Loss: 0.00110583
Iteration 17/1000 | Loss: 0.00088394
Iteration 18/1000 | Loss: 0.00116612
Iteration 19/1000 | Loss: 0.00083295
Iteration 20/1000 | Loss: 0.00090151
Iteration 21/1000 | Loss: 0.00102125
Iteration 22/1000 | Loss: 0.00034479
Iteration 23/1000 | Loss: 0.00190714
Iteration 24/1000 | Loss: 0.00094446
Iteration 25/1000 | Loss: 0.00056973
Iteration 26/1000 | Loss: 0.00102055
Iteration 27/1000 | Loss: 0.00037185
Iteration 28/1000 | Loss: 0.00141255
Iteration 29/1000 | Loss: 0.00075674
Iteration 30/1000 | Loss: 0.00098131
Iteration 31/1000 | Loss: 0.00037800
Iteration 32/1000 | Loss: 0.00063112
Iteration 33/1000 | Loss: 0.00095244
Iteration 34/1000 | Loss: 0.00078675
Iteration 35/1000 | Loss: 0.00050768
Iteration 36/1000 | Loss: 0.00077025
Iteration 37/1000 | Loss: 0.00085521
Iteration 38/1000 | Loss: 0.00061941
Iteration 39/1000 | Loss: 0.00033604
Iteration 40/1000 | Loss: 0.00196336
Iteration 41/1000 | Loss: 0.00037043
Iteration 42/1000 | Loss: 0.00053511
Iteration 43/1000 | Loss: 0.00022340
Iteration 44/1000 | Loss: 0.00015453
Iteration 45/1000 | Loss: 0.00009794
Iteration 46/1000 | Loss: 0.00023924
Iteration 47/1000 | Loss: 0.00008933
Iteration 48/1000 | Loss: 0.00008090
Iteration 49/1000 | Loss: 0.00043411
Iteration 50/1000 | Loss: 0.00009562
Iteration 51/1000 | Loss: 0.00068073
Iteration 52/1000 | Loss: 0.00108667
Iteration 53/1000 | Loss: 0.00069293
Iteration 54/1000 | Loss: 0.00046046
Iteration 55/1000 | Loss: 0.00008311
Iteration 56/1000 | Loss: 0.00069802
Iteration 57/1000 | Loss: 0.00136968
Iteration 58/1000 | Loss: 0.00099783
Iteration 59/1000 | Loss: 0.00182911
Iteration 60/1000 | Loss: 0.00023512
Iteration 61/1000 | Loss: 0.00067427
Iteration 62/1000 | Loss: 0.00047270
Iteration 63/1000 | Loss: 0.00061535
Iteration 64/1000 | Loss: 0.00051516
Iteration 65/1000 | Loss: 0.00090993
Iteration 66/1000 | Loss: 0.00014933
Iteration 67/1000 | Loss: 0.00088126
Iteration 68/1000 | Loss: 0.00013577
Iteration 69/1000 | Loss: 0.00011907
Iteration 70/1000 | Loss: 0.00016802
Iteration 71/1000 | Loss: 0.00024789
Iteration 72/1000 | Loss: 0.00008105
Iteration 73/1000 | Loss: 0.00055107
Iteration 74/1000 | Loss: 0.00055880
Iteration 75/1000 | Loss: 0.00041595
Iteration 76/1000 | Loss: 0.00032915
Iteration 77/1000 | Loss: 0.00005417
Iteration 78/1000 | Loss: 0.00005012
Iteration 79/1000 | Loss: 0.00004711
Iteration 80/1000 | Loss: 0.00081425
Iteration 81/1000 | Loss: 0.00062727
Iteration 82/1000 | Loss: 0.00030000
Iteration 83/1000 | Loss: 0.00033356
Iteration 84/1000 | Loss: 0.00008629
Iteration 85/1000 | Loss: 0.00019851
Iteration 86/1000 | Loss: 0.00036547
Iteration 87/1000 | Loss: 0.00040416
Iteration 88/1000 | Loss: 0.00013947
Iteration 89/1000 | Loss: 0.00004287
Iteration 90/1000 | Loss: 0.00006670
Iteration 91/1000 | Loss: 0.00003944
Iteration 92/1000 | Loss: 0.00003822
Iteration 93/1000 | Loss: 0.00018951
Iteration 94/1000 | Loss: 0.00035299
Iteration 95/1000 | Loss: 0.00019877
Iteration 96/1000 | Loss: 0.00027633
Iteration 97/1000 | Loss: 0.00018689
Iteration 98/1000 | Loss: 0.00004435
Iteration 99/1000 | Loss: 0.00029840
Iteration 100/1000 | Loss: 0.00040796
Iteration 101/1000 | Loss: 0.00025944
Iteration 102/1000 | Loss: 0.00014619
Iteration 103/1000 | Loss: 0.00025378
Iteration 104/1000 | Loss: 0.00022697
Iteration 105/1000 | Loss: 0.00026354
Iteration 106/1000 | Loss: 0.00017241
Iteration 107/1000 | Loss: 0.00033372
Iteration 108/1000 | Loss: 0.00023294
Iteration 109/1000 | Loss: 0.00013702
Iteration 110/1000 | Loss: 0.00003822
Iteration 111/1000 | Loss: 0.00033079
Iteration 112/1000 | Loss: 0.00019045
Iteration 113/1000 | Loss: 0.00033881
Iteration 114/1000 | Loss: 0.00025519
Iteration 115/1000 | Loss: 0.00036091
Iteration 116/1000 | Loss: 0.00004590
Iteration 117/1000 | Loss: 0.00032774
Iteration 118/1000 | Loss: 0.00014848
Iteration 119/1000 | Loss: 0.00020518
Iteration 120/1000 | Loss: 0.00004241
Iteration 121/1000 | Loss: 0.00003665
Iteration 122/1000 | Loss: 0.00003488
Iteration 123/1000 | Loss: 0.00003363
Iteration 124/1000 | Loss: 0.00003317
Iteration 125/1000 | Loss: 0.00041635
Iteration 126/1000 | Loss: 0.00021114
Iteration 127/1000 | Loss: 0.00021654
Iteration 128/1000 | Loss: 0.00017391
Iteration 129/1000 | Loss: 0.00022327
Iteration 130/1000 | Loss: 0.00004117
Iteration 131/1000 | Loss: 0.00003514
Iteration 132/1000 | Loss: 0.00003185
Iteration 133/1000 | Loss: 0.00003107
Iteration 134/1000 | Loss: 0.00003041
Iteration 135/1000 | Loss: 0.00003007
Iteration 136/1000 | Loss: 0.00024508
Iteration 137/1000 | Loss: 0.00020771
Iteration 138/1000 | Loss: 0.00022946
Iteration 139/1000 | Loss: 0.00025354
Iteration 140/1000 | Loss: 0.00038912
Iteration 141/1000 | Loss: 0.00024602
Iteration 142/1000 | Loss: 0.00023448
Iteration 143/1000 | Loss: 0.00003596
Iteration 144/1000 | Loss: 0.00003316
Iteration 145/1000 | Loss: 0.00003150
Iteration 146/1000 | Loss: 0.00003093
Iteration 147/1000 | Loss: 0.00003049
Iteration 148/1000 | Loss: 0.00003026
Iteration 149/1000 | Loss: 0.00003007
Iteration 150/1000 | Loss: 0.00028983
Iteration 151/1000 | Loss: 0.00009100
Iteration 152/1000 | Loss: 0.00003795
Iteration 153/1000 | Loss: 0.00008363
Iteration 154/1000 | Loss: 0.00003238
Iteration 155/1000 | Loss: 0.00003096
Iteration 156/1000 | Loss: 0.00002998
Iteration 157/1000 | Loss: 0.00002949
Iteration 158/1000 | Loss: 0.00002920
Iteration 159/1000 | Loss: 0.00002884
Iteration 160/1000 | Loss: 0.00021305
Iteration 161/1000 | Loss: 0.00011467
Iteration 162/1000 | Loss: 0.00003375
Iteration 163/1000 | Loss: 0.00002974
Iteration 164/1000 | Loss: 0.00002889
Iteration 165/1000 | Loss: 0.00002876
Iteration 166/1000 | Loss: 0.00002871
Iteration 167/1000 | Loss: 0.00002870
Iteration 168/1000 | Loss: 0.00020806
Iteration 169/1000 | Loss: 0.00006088
Iteration 170/1000 | Loss: 0.00002911
Iteration 171/1000 | Loss: 0.00019568
Iteration 172/1000 | Loss: 0.00004345
Iteration 173/1000 | Loss: 0.00011171
Iteration 174/1000 | Loss: 0.00015124
Iteration 175/1000 | Loss: 0.00014570
Iteration 176/1000 | Loss: 0.00003805
Iteration 177/1000 | Loss: 0.00016746
Iteration 178/1000 | Loss: 0.00004318
Iteration 179/1000 | Loss: 0.00005009
Iteration 180/1000 | Loss: 0.00007710
Iteration 181/1000 | Loss: 0.00006986
Iteration 182/1000 | Loss: 0.00009589
Iteration 183/1000 | Loss: 0.00006855
Iteration 184/1000 | Loss: 0.00014742
Iteration 185/1000 | Loss: 0.00010055
Iteration 186/1000 | Loss: 0.00010871
Iteration 187/1000 | Loss: 0.00013498
Iteration 188/1000 | Loss: 0.00010359
Iteration 189/1000 | Loss: 0.00013353
Iteration 190/1000 | Loss: 0.00015566
Iteration 191/1000 | Loss: 0.00004319
Iteration 192/1000 | Loss: 0.00019057
Iteration 193/1000 | Loss: 0.00005057
Iteration 194/1000 | Loss: 0.00005178
Iteration 195/1000 | Loss: 0.00004309
Iteration 196/1000 | Loss: 0.00004666
Iteration 197/1000 | Loss: 0.00003466
Iteration 198/1000 | Loss: 0.00059121
Iteration 199/1000 | Loss: 0.00011975
Iteration 200/1000 | Loss: 0.00018636
Iteration 201/1000 | Loss: 0.00014036
Iteration 202/1000 | Loss: 0.00019318
Iteration 203/1000 | Loss: 0.00013430
Iteration 204/1000 | Loss: 0.00023184
Iteration 205/1000 | Loss: 0.00013835
Iteration 206/1000 | Loss: 0.00023592
Iteration 207/1000 | Loss: 0.00013230
Iteration 208/1000 | Loss: 0.00023112
Iteration 209/1000 | Loss: 0.00013144
Iteration 210/1000 | Loss: 0.00025433
Iteration 211/1000 | Loss: 0.00014516
Iteration 212/1000 | Loss: 0.00015295
Iteration 213/1000 | Loss: 0.00045107
Iteration 214/1000 | Loss: 0.00017469
Iteration 215/1000 | Loss: 0.00003681
Iteration 216/1000 | Loss: 0.00003448
Iteration 217/1000 | Loss: 0.00003394
Iteration 218/1000 | Loss: 0.00003321
Iteration 219/1000 | Loss: 0.00003233
Iteration 220/1000 | Loss: 0.00003163
Iteration 221/1000 | Loss: 0.00065839
Iteration 222/1000 | Loss: 0.00004167
Iteration 223/1000 | Loss: 0.00003147
Iteration 224/1000 | Loss: 0.00003000
Iteration 225/1000 | Loss: 0.00002935
Iteration 226/1000 | Loss: 0.00002893
Iteration 227/1000 | Loss: 0.00002873
Iteration 228/1000 | Loss: 0.00002850
Iteration 229/1000 | Loss: 0.00002847
Iteration 230/1000 | Loss: 0.00002841
Iteration 231/1000 | Loss: 0.00002839
Iteration 232/1000 | Loss: 0.00002839
Iteration 233/1000 | Loss: 0.00002839
Iteration 234/1000 | Loss: 0.00002839
Iteration 235/1000 | Loss: 0.00002839
Iteration 236/1000 | Loss: 0.00002839
Iteration 237/1000 | Loss: 0.00002838
Iteration 238/1000 | Loss: 0.00002838
Iteration 239/1000 | Loss: 0.00002838
Iteration 240/1000 | Loss: 0.00002838
Iteration 241/1000 | Loss: 0.00002838
Iteration 242/1000 | Loss: 0.00002836
Iteration 243/1000 | Loss: 0.00002836
Iteration 244/1000 | Loss: 0.00002836
Iteration 245/1000 | Loss: 0.00002835
Iteration 246/1000 | Loss: 0.00002835
Iteration 247/1000 | Loss: 0.00002835
Iteration 248/1000 | Loss: 0.00002835
Iteration 249/1000 | Loss: 0.00002834
Iteration 250/1000 | Loss: 0.00002834
Iteration 251/1000 | Loss: 0.00002834
Iteration 252/1000 | Loss: 0.00002834
Iteration 253/1000 | Loss: 0.00002834
Iteration 254/1000 | Loss: 0.00002834
Iteration 255/1000 | Loss: 0.00002834
Iteration 256/1000 | Loss: 0.00002834
Iteration 257/1000 | Loss: 0.00002834
Iteration 258/1000 | Loss: 0.00002834
Iteration 259/1000 | Loss: 0.00002833
Iteration 260/1000 | Loss: 0.00002833
Iteration 261/1000 | Loss: 0.00002833
Iteration 262/1000 | Loss: 0.00002833
Iteration 263/1000 | Loss: 0.00002833
Iteration 264/1000 | Loss: 0.00002832
Iteration 265/1000 | Loss: 0.00002832
Iteration 266/1000 | Loss: 0.00002831
Iteration 267/1000 | Loss: 0.00002831
Iteration 268/1000 | Loss: 0.00002831
Iteration 269/1000 | Loss: 0.00002831
Iteration 270/1000 | Loss: 0.00002830
Iteration 271/1000 | Loss: 0.00002830
Iteration 272/1000 | Loss: 0.00002830
Iteration 273/1000 | Loss: 0.00002829
Iteration 274/1000 | Loss: 0.00002828
Iteration 275/1000 | Loss: 0.00002828
Iteration 276/1000 | Loss: 0.00002828
Iteration 277/1000 | Loss: 0.00002913
Iteration 278/1000 | Loss: 0.00002823
Iteration 279/1000 | Loss: 0.00002800
Iteration 280/1000 | Loss: 0.00002798
Iteration 281/1000 | Loss: 0.00002796
Iteration 282/1000 | Loss: 0.00002796
Iteration 283/1000 | Loss: 0.00002796
Iteration 284/1000 | Loss: 0.00002795
Iteration 285/1000 | Loss: 0.00002795
Iteration 286/1000 | Loss: 0.00002794
Iteration 287/1000 | Loss: 0.00002793
Iteration 288/1000 | Loss: 0.00002793
Iteration 289/1000 | Loss: 0.00002793
Iteration 290/1000 | Loss: 0.00002792
Iteration 291/1000 | Loss: 0.00002792
Iteration 292/1000 | Loss: 0.00002792
Iteration 293/1000 | Loss: 0.00002791
Iteration 294/1000 | Loss: 0.00002791
Iteration 295/1000 | Loss: 0.00002791
Iteration 296/1000 | Loss: 0.00002790
Iteration 297/1000 | Loss: 0.00002790
Iteration 298/1000 | Loss: 0.00002790
Iteration 299/1000 | Loss: 0.00002790
Iteration 300/1000 | Loss: 0.00002790
Iteration 301/1000 | Loss: 0.00002790
Iteration 302/1000 | Loss: 0.00002790
Iteration 303/1000 | Loss: 0.00002789
Iteration 304/1000 | Loss: 0.00002789
Iteration 305/1000 | Loss: 0.00002789
Iteration 306/1000 | Loss: 0.00002788
Iteration 307/1000 | Loss: 0.00002788
Iteration 308/1000 | Loss: 0.00002788
Iteration 309/1000 | Loss: 0.00002788
Iteration 310/1000 | Loss: 0.00002787
Iteration 311/1000 | Loss: 0.00002786
Iteration 312/1000 | Loss: 0.00002786
Iteration 313/1000 | Loss: 0.00002786
Iteration 314/1000 | Loss: 0.00002785
Iteration 315/1000 | Loss: 0.00002785
Iteration 316/1000 | Loss: 0.00002785
Iteration 317/1000 | Loss: 0.00002784
Iteration 318/1000 | Loss: 0.00002784
Iteration 319/1000 | Loss: 0.00002784
Iteration 320/1000 | Loss: 0.00002784
Iteration 321/1000 | Loss: 0.00002784
Iteration 322/1000 | Loss: 0.00002784
Iteration 323/1000 | Loss: 0.00002784
Iteration 324/1000 | Loss: 0.00002784
Iteration 325/1000 | Loss: 0.00002784
Iteration 326/1000 | Loss: 0.00002783
Iteration 327/1000 | Loss: 0.00002783
Iteration 328/1000 | Loss: 0.00002783
Iteration 329/1000 | Loss: 0.00002783
Iteration 330/1000 | Loss: 0.00002783
Iteration 331/1000 | Loss: 0.00002783
Iteration 332/1000 | Loss: 0.00002782
Iteration 333/1000 | Loss: 0.00002782
Iteration 334/1000 | Loss: 0.00002782
Iteration 335/1000 | Loss: 0.00002781
Iteration 336/1000 | Loss: 0.00002781
Iteration 337/1000 | Loss: 0.00002781
Iteration 338/1000 | Loss: 0.00002781
Iteration 339/1000 | Loss: 0.00002780
Iteration 340/1000 | Loss: 0.00002780
Iteration 341/1000 | Loss: 0.00002780
Iteration 342/1000 | Loss: 0.00002780
Iteration 343/1000 | Loss: 0.00002780
Iteration 344/1000 | Loss: 0.00002780
Iteration 345/1000 | Loss: 0.00002780
Iteration 346/1000 | Loss: 0.00002780
Iteration 347/1000 | Loss: 0.00002779
Iteration 348/1000 | Loss: 0.00002779
Iteration 349/1000 | Loss: 0.00002779
Iteration 350/1000 | Loss: 0.00002779
Iteration 351/1000 | Loss: 0.00002779
Iteration 352/1000 | Loss: 0.00002779
Iteration 353/1000 | Loss: 0.00002779
Iteration 354/1000 | Loss: 0.00002779
Iteration 355/1000 | Loss: 0.00002779
Iteration 356/1000 | Loss: 0.00002779
Iteration 357/1000 | Loss: 0.00002779
Iteration 358/1000 | Loss: 0.00002779
Iteration 359/1000 | Loss: 0.00002779
Iteration 360/1000 | Loss: 0.00002779
Iteration 361/1000 | Loss: 0.00002778
Iteration 362/1000 | Loss: 0.00002778
Iteration 363/1000 | Loss: 0.00002778
Iteration 364/1000 | Loss: 0.00002778
Iteration 365/1000 | Loss: 0.00002778
Iteration 366/1000 | Loss: 0.00002778
Iteration 367/1000 | Loss: 0.00002778
Iteration 368/1000 | Loss: 0.00002778
Iteration 369/1000 | Loss: 0.00002778
Iteration 370/1000 | Loss: 0.00002778
Iteration 371/1000 | Loss: 0.00002778
Iteration 372/1000 | Loss: 0.00002778
Iteration 373/1000 | Loss: 0.00002778
Iteration 374/1000 | Loss: 0.00002778
Iteration 375/1000 | Loss: 0.00002778
Iteration 376/1000 | Loss: 0.00002778
Iteration 377/1000 | Loss: 0.00002778
Iteration 378/1000 | Loss: 0.00002778
Iteration 379/1000 | Loss: 0.00002778
Iteration 380/1000 | Loss: 0.00002778
Iteration 381/1000 | Loss: 0.00002778
Iteration 382/1000 | Loss: 0.00002778
Iteration 383/1000 | Loss: 0.00002778
Iteration 384/1000 | Loss: 0.00002778
Iteration 385/1000 | Loss: 0.00002778
Iteration 386/1000 | Loss: 0.00002778
Iteration 387/1000 | Loss: 0.00002778
Iteration 388/1000 | Loss: 0.00002778
Iteration 389/1000 | Loss: 0.00002778
Iteration 390/1000 | Loss: 0.00002778
Iteration 391/1000 | Loss: 0.00002778
Iteration 392/1000 | Loss: 0.00002778
Iteration 393/1000 | Loss: 0.00002778
Iteration 394/1000 | Loss: 0.00002778
Iteration 395/1000 | Loss: 0.00002778
Iteration 396/1000 | Loss: 0.00002778
Iteration 397/1000 | Loss: 0.00002778
Iteration 398/1000 | Loss: 0.00002778
Iteration 399/1000 | Loss: 0.00002778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 399. Stopping optimization.
Last 5 losses: [2.777750341920182e-05, 2.777750341920182e-05, 2.777750341920182e-05, 2.777750341920182e-05, 2.777750341920182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.777750341920182e-05

Optimization complete. Final v2v error: 4.390750885009766 mm

Highest mean error: 5.736135959625244 mm for frame 29

Lowest mean error: 3.7709062099456787 mm for frame 186

Saving results

Total time: 429.69716238975525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01208307
Iteration 2/25 | Loss: 0.00317079
Iteration 3/25 | Loss: 0.00208698
Iteration 4/25 | Loss: 0.00196224
Iteration 5/25 | Loss: 0.00173628
Iteration 6/25 | Loss: 0.00172502
Iteration 7/25 | Loss: 0.00169312
Iteration 8/25 | Loss: 0.00184274
Iteration 9/25 | Loss: 0.00175146
Iteration 10/25 | Loss: 0.00161911
Iteration 11/25 | Loss: 0.00150945
Iteration 12/25 | Loss: 0.00146021
Iteration 13/25 | Loss: 0.00142631
Iteration 14/25 | Loss: 0.00142304
Iteration 15/25 | Loss: 0.00139916
Iteration 16/25 | Loss: 0.00137015
Iteration 17/25 | Loss: 0.00134796
Iteration 18/25 | Loss: 0.00134278
Iteration 19/25 | Loss: 0.00133598
Iteration 20/25 | Loss: 0.00133406
Iteration 21/25 | Loss: 0.00133309
Iteration 22/25 | Loss: 0.00133289
Iteration 23/25 | Loss: 0.00133279
Iteration 24/25 | Loss: 0.00133271
Iteration 25/25 | Loss: 0.00133270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39057270
Iteration 2/25 | Loss: 0.00089562
Iteration 3/25 | Loss: 0.00089562
Iteration 4/25 | Loss: 0.00089562
Iteration 5/25 | Loss: 0.00089562
Iteration 6/25 | Loss: 0.00089562
Iteration 7/25 | Loss: 0.00089562
Iteration 8/25 | Loss: 0.00089562
Iteration 9/25 | Loss: 0.00089562
Iteration 10/25 | Loss: 0.00089562
Iteration 11/25 | Loss: 0.00089562
Iteration 12/25 | Loss: 0.00089562
Iteration 13/25 | Loss: 0.00089562
Iteration 14/25 | Loss: 0.00089562
Iteration 15/25 | Loss: 0.00089562
Iteration 16/25 | Loss: 0.00089562
Iteration 17/25 | Loss: 0.00089562
Iteration 18/25 | Loss: 0.00089562
Iteration 19/25 | Loss: 0.00089562
Iteration 20/25 | Loss: 0.00089562
Iteration 21/25 | Loss: 0.00089562
Iteration 22/25 | Loss: 0.00089562
Iteration 23/25 | Loss: 0.00089562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008956159581430256, 0.0008956159581430256, 0.0008956159581430256, 0.0008956159581430256, 0.0008956159581430256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008956159581430256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089562
Iteration 2/1000 | Loss: 0.00009519
Iteration 3/1000 | Loss: 0.00007064
Iteration 4/1000 | Loss: 0.00006195
Iteration 5/1000 | Loss: 0.00005997
Iteration 6/1000 | Loss: 0.00005863
Iteration 7/1000 | Loss: 0.00005729
Iteration 8/1000 | Loss: 0.00005612
Iteration 9/1000 | Loss: 0.00005553
Iteration 10/1000 | Loss: 0.00005493
Iteration 11/1000 | Loss: 0.00005432
Iteration 12/1000 | Loss: 0.00005394
Iteration 13/1000 | Loss: 0.00005356
Iteration 14/1000 | Loss: 0.00005324
Iteration 15/1000 | Loss: 0.00005320
Iteration 16/1000 | Loss: 0.00030622
Iteration 17/1000 | Loss: 0.00005964
Iteration 18/1000 | Loss: 0.00005630
Iteration 19/1000 | Loss: 0.00005453
Iteration 20/1000 | Loss: 0.00005387
Iteration 21/1000 | Loss: 0.00005355
Iteration 22/1000 | Loss: 0.00005324
Iteration 23/1000 | Loss: 0.00005320
Iteration 24/1000 | Loss: 0.00005308
Iteration 25/1000 | Loss: 0.00034897
Iteration 26/1000 | Loss: 0.00011607
Iteration 27/1000 | Loss: 0.00034316
Iteration 28/1000 | Loss: 0.00013356
Iteration 29/1000 | Loss: 0.00006380
Iteration 30/1000 | Loss: 0.00005581
Iteration 31/1000 | Loss: 0.00005339
Iteration 32/1000 | Loss: 0.00034175
Iteration 33/1000 | Loss: 0.00005813
Iteration 34/1000 | Loss: 0.00005468
Iteration 35/1000 | Loss: 0.00005261
Iteration 36/1000 | Loss: 0.00005225
Iteration 37/1000 | Loss: 0.00005201
Iteration 38/1000 | Loss: 0.00005187
Iteration 39/1000 | Loss: 0.00005186
Iteration 40/1000 | Loss: 0.00005186
Iteration 41/1000 | Loss: 0.00005185
Iteration 42/1000 | Loss: 0.00005181
Iteration 43/1000 | Loss: 0.00005178
Iteration 44/1000 | Loss: 0.00005177
Iteration 45/1000 | Loss: 0.00005176
Iteration 46/1000 | Loss: 0.00005176
Iteration 47/1000 | Loss: 0.00005176
Iteration 48/1000 | Loss: 0.00005175
Iteration 49/1000 | Loss: 0.00005175
Iteration 50/1000 | Loss: 0.00005175
Iteration 51/1000 | Loss: 0.00005175
Iteration 52/1000 | Loss: 0.00005175
Iteration 53/1000 | Loss: 0.00005175
Iteration 54/1000 | Loss: 0.00005175
Iteration 55/1000 | Loss: 0.00005175
Iteration 56/1000 | Loss: 0.00005175
Iteration 57/1000 | Loss: 0.00005175
Iteration 58/1000 | Loss: 0.00005175
Iteration 59/1000 | Loss: 0.00005175
Iteration 60/1000 | Loss: 0.00005175
Iteration 61/1000 | Loss: 0.00005175
Iteration 62/1000 | Loss: 0.00005174
Iteration 63/1000 | Loss: 0.00005174
Iteration 64/1000 | Loss: 0.00005174
Iteration 65/1000 | Loss: 0.00005174
Iteration 66/1000 | Loss: 0.00005174
Iteration 67/1000 | Loss: 0.00005174
Iteration 68/1000 | Loss: 0.00005174
Iteration 69/1000 | Loss: 0.00005174
Iteration 70/1000 | Loss: 0.00005174
Iteration 71/1000 | Loss: 0.00005173
Iteration 72/1000 | Loss: 0.00005173
Iteration 73/1000 | Loss: 0.00005173
Iteration 74/1000 | Loss: 0.00005173
Iteration 75/1000 | Loss: 0.00005172
Iteration 76/1000 | Loss: 0.00005172
Iteration 77/1000 | Loss: 0.00005171
Iteration 78/1000 | Loss: 0.00005170
Iteration 79/1000 | Loss: 0.00005170
Iteration 80/1000 | Loss: 0.00005170
Iteration 81/1000 | Loss: 0.00005169
Iteration 82/1000 | Loss: 0.00005169
Iteration 83/1000 | Loss: 0.00005169
Iteration 84/1000 | Loss: 0.00005169
Iteration 85/1000 | Loss: 0.00005168
Iteration 86/1000 | Loss: 0.00005168
Iteration 87/1000 | Loss: 0.00005168
Iteration 88/1000 | Loss: 0.00005168
Iteration 89/1000 | Loss: 0.00005168
Iteration 90/1000 | Loss: 0.00005168
Iteration 91/1000 | Loss: 0.00005167
Iteration 92/1000 | Loss: 0.00005167
Iteration 93/1000 | Loss: 0.00005167
Iteration 94/1000 | Loss: 0.00005167
Iteration 95/1000 | Loss: 0.00005167
Iteration 96/1000 | Loss: 0.00005167
Iteration 97/1000 | Loss: 0.00005167
Iteration 98/1000 | Loss: 0.00005167
Iteration 99/1000 | Loss: 0.00005167
Iteration 100/1000 | Loss: 0.00005167
Iteration 101/1000 | Loss: 0.00005167
Iteration 102/1000 | Loss: 0.00005167
Iteration 103/1000 | Loss: 0.00005167
Iteration 104/1000 | Loss: 0.00005167
Iteration 105/1000 | Loss: 0.00005167
Iteration 106/1000 | Loss: 0.00005167
Iteration 107/1000 | Loss: 0.00005167
Iteration 108/1000 | Loss: 0.00005167
Iteration 109/1000 | Loss: 0.00005167
Iteration 110/1000 | Loss: 0.00005167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [5.166795017430559e-05, 5.166795017430559e-05, 5.166795017430559e-05, 5.166795017430559e-05, 5.166795017430559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.166795017430559e-05

Optimization complete. Final v2v error: 5.792848587036133 mm

Highest mean error: 7.519965171813965 mm for frame 4

Lowest mean error: 5.587533473968506 mm for frame 143

Saving results

Total time: 113.07171869277954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386600
Iteration 2/25 | Loss: 0.00096444
Iteration 3/25 | Loss: 0.00078258
Iteration 4/25 | Loss: 0.00076622
Iteration 5/25 | Loss: 0.00076039
Iteration 6/25 | Loss: 0.00075759
Iteration 7/25 | Loss: 0.00075683
Iteration 8/25 | Loss: 0.00075683
Iteration 9/25 | Loss: 0.00075683
Iteration 10/25 | Loss: 0.00075683
Iteration 11/25 | Loss: 0.00075683
Iteration 12/25 | Loss: 0.00075683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007568266009911895, 0.0007568266009911895, 0.0007568266009911895, 0.0007568266009911895, 0.0007568266009911895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007568266009911895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72636223
Iteration 2/25 | Loss: 0.00132752
Iteration 3/25 | Loss: 0.00132752
Iteration 4/25 | Loss: 0.00132752
Iteration 5/25 | Loss: 0.00132752
Iteration 6/25 | Loss: 0.00132752
Iteration 7/25 | Loss: 0.00132752
Iteration 8/25 | Loss: 0.00132752
Iteration 9/25 | Loss: 0.00132752
Iteration 10/25 | Loss: 0.00132752
Iteration 11/25 | Loss: 0.00132752
Iteration 12/25 | Loss: 0.00132752
Iteration 13/25 | Loss: 0.00132752
Iteration 14/25 | Loss: 0.00132752
Iteration 15/25 | Loss: 0.00132752
Iteration 16/25 | Loss: 0.00132752
Iteration 17/25 | Loss: 0.00132752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001327520003542304, 0.001327520003542304, 0.001327520003542304, 0.001327520003542304, 0.001327520003542304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001327520003542304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132752
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001555
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001246
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001113
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001111
Iteration 29/1000 | Loss: 0.00001108
Iteration 30/1000 | Loss: 0.00001108
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001105
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001102
Iteration 36/1000 | Loss: 0.00001102
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001100
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001099
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001097
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001090
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001086
Iteration 126/1000 | Loss: 0.00001086
Iteration 127/1000 | Loss: 0.00001086
Iteration 128/1000 | Loss: 0.00001086
Iteration 129/1000 | Loss: 0.00001086
Iteration 130/1000 | Loss: 0.00001086
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.0862142516998574e-05, 1.0862142516998574e-05, 1.0862142516998574e-05, 1.0862142516998574e-05, 1.0862142516998574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0862142516998574e-05

Optimization complete. Final v2v error: 2.8191182613372803 mm

Highest mean error: 3.112354278564453 mm for frame 145

Lowest mean error: 2.534755229949951 mm for frame 230

Saving results

Total time: 41.66590762138367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855745
Iteration 2/25 | Loss: 0.00123014
Iteration 3/25 | Loss: 0.00096002
Iteration 4/25 | Loss: 0.00085238
Iteration 5/25 | Loss: 0.00084532
Iteration 6/25 | Loss: 0.00082734
Iteration 7/25 | Loss: 0.00082502
Iteration 8/25 | Loss: 0.00082443
Iteration 9/25 | Loss: 0.00082428
Iteration 10/25 | Loss: 0.00082424
Iteration 11/25 | Loss: 0.00082424
Iteration 12/25 | Loss: 0.00082424
Iteration 13/25 | Loss: 0.00082424
Iteration 14/25 | Loss: 0.00082422
Iteration 15/25 | Loss: 0.00082422
Iteration 16/25 | Loss: 0.00082422
Iteration 17/25 | Loss: 0.00082422
Iteration 18/25 | Loss: 0.00082422
Iteration 19/25 | Loss: 0.00082422
Iteration 20/25 | Loss: 0.00082422
Iteration 21/25 | Loss: 0.00082422
Iteration 22/25 | Loss: 0.00082422
Iteration 23/25 | Loss: 0.00082422
Iteration 24/25 | Loss: 0.00082421
Iteration 25/25 | Loss: 0.00082421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.16382980
Iteration 2/25 | Loss: 0.00132343
Iteration 3/25 | Loss: 0.00132337
Iteration 4/25 | Loss: 0.00132337
Iteration 5/25 | Loss: 0.00132336
Iteration 6/25 | Loss: 0.00132336
Iteration 7/25 | Loss: 0.00132336
Iteration 8/25 | Loss: 0.00132336
Iteration 9/25 | Loss: 0.00132336
Iteration 10/25 | Loss: 0.00132336
Iteration 11/25 | Loss: 0.00132336
Iteration 12/25 | Loss: 0.00132336
Iteration 13/25 | Loss: 0.00132336
Iteration 14/25 | Loss: 0.00132336
Iteration 15/25 | Loss: 0.00132336
Iteration 16/25 | Loss: 0.00132336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013233635108917952, 0.0013233635108917952, 0.0013233635108917952, 0.0013233635108917952, 0.0013233635108917952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013233635108917952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132336
Iteration 2/1000 | Loss: 0.00004068
Iteration 3/1000 | Loss: 0.00002790
Iteration 4/1000 | Loss: 0.00002285
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002028
Iteration 8/1000 | Loss: 0.00001994
Iteration 9/1000 | Loss: 0.00001966
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001921
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001910
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001896
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001882
Iteration 20/1000 | Loss: 0.00001880
Iteration 21/1000 | Loss: 0.00001878
Iteration 22/1000 | Loss: 0.00001877
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001876
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001864
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001864
Iteration 75/1000 | Loss: 0.00001864
Iteration 76/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.863672150648199e-05, 1.863672150648199e-05, 1.863672150648199e-05, 1.863672150648199e-05, 1.863672150648199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.863672150648199e-05

Optimization complete. Final v2v error: 3.5980477333068848 mm

Highest mean error: 4.292783737182617 mm for frame 57

Lowest mean error: 3.045438051223755 mm for frame 112

Saving results

Total time: 39.33263611793518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_018/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_018/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449933
Iteration 2/25 | Loss: 0.00104261
Iteration 3/25 | Loss: 0.00086149
Iteration 4/25 | Loss: 0.00082461
Iteration 5/25 | Loss: 0.00080886
Iteration 6/25 | Loss: 0.00080383
Iteration 7/25 | Loss: 0.00080232
Iteration 8/25 | Loss: 0.00080225
Iteration 9/25 | Loss: 0.00080225
Iteration 10/25 | Loss: 0.00080225
Iteration 11/25 | Loss: 0.00080225
Iteration 12/25 | Loss: 0.00080225
Iteration 13/25 | Loss: 0.00080225
Iteration 14/25 | Loss: 0.00080225
Iteration 15/25 | Loss: 0.00080225
Iteration 16/25 | Loss: 0.00080225
Iteration 17/25 | Loss: 0.00080225
Iteration 18/25 | Loss: 0.00080225
Iteration 19/25 | Loss: 0.00080225
Iteration 20/25 | Loss: 0.00080225
Iteration 21/25 | Loss: 0.00080225
Iteration 22/25 | Loss: 0.00080225
Iteration 23/25 | Loss: 0.00080225
Iteration 24/25 | Loss: 0.00080225
Iteration 25/25 | Loss: 0.00080225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74115801
Iteration 2/25 | Loss: 0.00165996
Iteration 3/25 | Loss: 0.00165996
Iteration 4/25 | Loss: 0.00165996
Iteration 5/25 | Loss: 0.00165996
Iteration 6/25 | Loss: 0.00165996
Iteration 7/25 | Loss: 0.00165996
Iteration 8/25 | Loss: 0.00165996
Iteration 9/25 | Loss: 0.00165996
Iteration 10/25 | Loss: 0.00165996
Iteration 11/25 | Loss: 0.00165996
Iteration 12/25 | Loss: 0.00165996
Iteration 13/25 | Loss: 0.00165995
Iteration 14/25 | Loss: 0.00165995
Iteration 15/25 | Loss: 0.00165995
Iteration 16/25 | Loss: 0.00165995
Iteration 17/25 | Loss: 0.00165995
Iteration 18/25 | Loss: 0.00165995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016599547816440463, 0.0016599547816440463, 0.0016599547816440463, 0.0016599547816440463, 0.0016599547816440463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016599547816440463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165995
Iteration 2/1000 | Loss: 0.00004773
Iteration 3/1000 | Loss: 0.00003591
Iteration 4/1000 | Loss: 0.00002902
Iteration 5/1000 | Loss: 0.00002692
Iteration 6/1000 | Loss: 0.00002542
Iteration 7/1000 | Loss: 0.00002430
Iteration 8/1000 | Loss: 0.00002358
Iteration 9/1000 | Loss: 0.00002293
Iteration 10/1000 | Loss: 0.00002227
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002160
Iteration 13/1000 | Loss: 0.00002131
Iteration 14/1000 | Loss: 0.00002111
Iteration 15/1000 | Loss: 0.00002091
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002080
Iteration 19/1000 | Loss: 0.00002075
Iteration 20/1000 | Loss: 0.00002066
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002061
Iteration 23/1000 | Loss: 0.00002060
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002059
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002058
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002057
Iteration 32/1000 | Loss: 0.00002057
Iteration 33/1000 | Loss: 0.00002056
Iteration 34/1000 | Loss: 0.00002056
Iteration 35/1000 | Loss: 0.00002056
Iteration 36/1000 | Loss: 0.00002055
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002055
Iteration 39/1000 | Loss: 0.00002054
Iteration 40/1000 | Loss: 0.00002054
Iteration 41/1000 | Loss: 0.00002053
Iteration 42/1000 | Loss: 0.00002053
Iteration 43/1000 | Loss: 0.00002053
Iteration 44/1000 | Loss: 0.00002053
Iteration 45/1000 | Loss: 0.00002053
Iteration 46/1000 | Loss: 0.00002053
Iteration 47/1000 | Loss: 0.00002053
Iteration 48/1000 | Loss: 0.00002053
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002052
Iteration 51/1000 | Loss: 0.00002052
Iteration 52/1000 | Loss: 0.00002052
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00002051
Iteration 55/1000 | Loss: 0.00002051
Iteration 56/1000 | Loss: 0.00002050
Iteration 57/1000 | Loss: 0.00002050
Iteration 58/1000 | Loss: 0.00002050
Iteration 59/1000 | Loss: 0.00002050
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00002049
Iteration 62/1000 | Loss: 0.00002049
Iteration 63/1000 | Loss: 0.00002049
Iteration 64/1000 | Loss: 0.00002049
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00002048
Iteration 68/1000 | Loss: 0.00002048
Iteration 69/1000 | Loss: 0.00002048
Iteration 70/1000 | Loss: 0.00002048
Iteration 71/1000 | Loss: 0.00002048
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002046
Iteration 79/1000 | Loss: 0.00002046
Iteration 80/1000 | Loss: 0.00002046
Iteration 81/1000 | Loss: 0.00002046
Iteration 82/1000 | Loss: 0.00002045
Iteration 83/1000 | Loss: 0.00002045
Iteration 84/1000 | Loss: 0.00002045
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002045
Iteration 87/1000 | Loss: 0.00002045
Iteration 88/1000 | Loss: 0.00002044
Iteration 89/1000 | Loss: 0.00002044
Iteration 90/1000 | Loss: 0.00002044
Iteration 91/1000 | Loss: 0.00002044
Iteration 92/1000 | Loss: 0.00002043
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002042
Iteration 98/1000 | Loss: 0.00002042
Iteration 99/1000 | Loss: 0.00002042
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002041
Iteration 107/1000 | Loss: 0.00002041
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002040
Iteration 113/1000 | Loss: 0.00002040
Iteration 114/1000 | Loss: 0.00002040
Iteration 115/1000 | Loss: 0.00002040
Iteration 116/1000 | Loss: 0.00002040
Iteration 117/1000 | Loss: 0.00002040
Iteration 118/1000 | Loss: 0.00002040
Iteration 119/1000 | Loss: 0.00002040
Iteration 120/1000 | Loss: 0.00002040
Iteration 121/1000 | Loss: 0.00002039
Iteration 122/1000 | Loss: 0.00002039
Iteration 123/1000 | Loss: 0.00002039
Iteration 124/1000 | Loss: 0.00002039
Iteration 125/1000 | Loss: 0.00002039
Iteration 126/1000 | Loss: 0.00002039
Iteration 127/1000 | Loss: 0.00002039
Iteration 128/1000 | Loss: 0.00002039
Iteration 129/1000 | Loss: 0.00002039
Iteration 130/1000 | Loss: 0.00002039
Iteration 131/1000 | Loss: 0.00002039
Iteration 132/1000 | Loss: 0.00002038
Iteration 133/1000 | Loss: 0.00002038
Iteration 134/1000 | Loss: 0.00002038
Iteration 135/1000 | Loss: 0.00002038
Iteration 136/1000 | Loss: 0.00002038
Iteration 137/1000 | Loss: 0.00002038
Iteration 138/1000 | Loss: 0.00002038
Iteration 139/1000 | Loss: 0.00002038
Iteration 140/1000 | Loss: 0.00002038
Iteration 141/1000 | Loss: 0.00002038
Iteration 142/1000 | Loss: 0.00002038
Iteration 143/1000 | Loss: 0.00002038
Iteration 144/1000 | Loss: 0.00002038
Iteration 145/1000 | Loss: 0.00002038
Iteration 146/1000 | Loss: 0.00002038
Iteration 147/1000 | Loss: 0.00002037
Iteration 148/1000 | Loss: 0.00002037
Iteration 149/1000 | Loss: 0.00002037
Iteration 150/1000 | Loss: 0.00002037
Iteration 151/1000 | Loss: 0.00002037
Iteration 152/1000 | Loss: 0.00002037
Iteration 153/1000 | Loss: 0.00002037
Iteration 154/1000 | Loss: 0.00002037
Iteration 155/1000 | Loss: 0.00002037
Iteration 156/1000 | Loss: 0.00002037
Iteration 157/1000 | Loss: 0.00002037
Iteration 158/1000 | Loss: 0.00002037
Iteration 159/1000 | Loss: 0.00002037
Iteration 160/1000 | Loss: 0.00002037
Iteration 161/1000 | Loss: 0.00002037
Iteration 162/1000 | Loss: 0.00002037
Iteration 163/1000 | Loss: 0.00002037
Iteration 164/1000 | Loss: 0.00002036
Iteration 165/1000 | Loss: 0.00002036
Iteration 166/1000 | Loss: 0.00002036
Iteration 167/1000 | Loss: 0.00002036
Iteration 168/1000 | Loss: 0.00002036
Iteration 169/1000 | Loss: 0.00002036
Iteration 170/1000 | Loss: 0.00002036
Iteration 171/1000 | Loss: 0.00002036
Iteration 172/1000 | Loss: 0.00002036
Iteration 173/1000 | Loss: 0.00002036
Iteration 174/1000 | Loss: 0.00002036
Iteration 175/1000 | Loss: 0.00002036
Iteration 176/1000 | Loss: 0.00002036
Iteration 177/1000 | Loss: 0.00002036
Iteration 178/1000 | Loss: 0.00002036
Iteration 179/1000 | Loss: 0.00002036
Iteration 180/1000 | Loss: 0.00002036
Iteration 181/1000 | Loss: 0.00002036
Iteration 182/1000 | Loss: 0.00002035
Iteration 183/1000 | Loss: 0.00002035
Iteration 184/1000 | Loss: 0.00002035
Iteration 185/1000 | Loss: 0.00002035
Iteration 186/1000 | Loss: 0.00002035
Iteration 187/1000 | Loss: 0.00002035
Iteration 188/1000 | Loss: 0.00002035
Iteration 189/1000 | Loss: 0.00002035
Iteration 190/1000 | Loss: 0.00002035
Iteration 191/1000 | Loss: 0.00002035
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Iteration 196/1000 | Loss: 0.00002035
Iteration 197/1000 | Loss: 0.00002035
Iteration 198/1000 | Loss: 0.00002035
Iteration 199/1000 | Loss: 0.00002035
Iteration 200/1000 | Loss: 0.00002035
Iteration 201/1000 | Loss: 0.00002035
Iteration 202/1000 | Loss: 0.00002035
Iteration 203/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.0348530597402714e-05, 2.0348530597402714e-05, 2.0348530597402714e-05, 2.0348530597402714e-05, 2.0348530597402714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0348530597402714e-05

Optimization complete. Final v2v error: 3.6649558544158936 mm

Highest mean error: 6.313398838043213 mm for frame 65

Lowest mean error: 2.7790348529815674 mm for frame 52

Saving results

Total time: 53.021775007247925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433381
Iteration 2/25 | Loss: 0.00142009
Iteration 3/25 | Loss: 0.00134564
Iteration 4/25 | Loss: 0.00133691
Iteration 5/25 | Loss: 0.00133410
Iteration 6/25 | Loss: 0.00133370
Iteration 7/25 | Loss: 0.00133370
Iteration 8/25 | Loss: 0.00133370
Iteration 9/25 | Loss: 0.00133370
Iteration 10/25 | Loss: 0.00133370
Iteration 11/25 | Loss: 0.00133370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013337039854377508, 0.0013337039854377508, 0.0013337039854377508, 0.0013337039854377508, 0.0013337039854377508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013337039854377508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39689004
Iteration 2/25 | Loss: 0.00098444
Iteration 3/25 | Loss: 0.00098444
Iteration 4/25 | Loss: 0.00098444
Iteration 5/25 | Loss: 0.00098444
Iteration 6/25 | Loss: 0.00098444
Iteration 7/25 | Loss: 0.00098444
Iteration 8/25 | Loss: 0.00098444
Iteration 9/25 | Loss: 0.00098444
Iteration 10/25 | Loss: 0.00098444
Iteration 11/25 | Loss: 0.00098444
Iteration 12/25 | Loss: 0.00098444
Iteration 13/25 | Loss: 0.00098444
Iteration 14/25 | Loss: 0.00098444
Iteration 15/25 | Loss: 0.00098444
Iteration 16/25 | Loss: 0.00098444
Iteration 17/25 | Loss: 0.00098444
Iteration 18/25 | Loss: 0.00098444
Iteration 19/25 | Loss: 0.00098444
Iteration 20/25 | Loss: 0.00098444
Iteration 21/25 | Loss: 0.00098444
Iteration 22/25 | Loss: 0.00098444
Iteration 23/25 | Loss: 0.00098444
Iteration 24/25 | Loss: 0.00098444
Iteration 25/25 | Loss: 0.00098444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009844406740739942, 0.0009844406740739942, 0.0009844406740739942, 0.0009844406740739942, 0.0009844406740739942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009844406740739942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098444
Iteration 2/1000 | Loss: 0.00003858
Iteration 3/1000 | Loss: 0.00002297
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001832
Iteration 7/1000 | Loss: 0.00001782
Iteration 8/1000 | Loss: 0.00001754
Iteration 9/1000 | Loss: 0.00001732
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001667
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001648
Iteration 23/1000 | Loss: 0.00001648
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001647
Iteration 26/1000 | Loss: 0.00001647
Iteration 27/1000 | Loss: 0.00001647
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001643
Iteration 58/1000 | Loss: 0.00001643
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001642
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001637
Iteration 92/1000 | Loss: 0.00001637
Iteration 93/1000 | Loss: 0.00001637
Iteration 94/1000 | Loss: 0.00001637
Iteration 95/1000 | Loss: 0.00001637
Iteration 96/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.637291097722482e-05, 1.637291097722482e-05, 1.637291097722482e-05, 1.637291097722482e-05, 1.637291097722482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.637291097722482e-05

Optimization complete. Final v2v error: 3.4994993209838867 mm

Highest mean error: 3.7392148971557617 mm for frame 29

Lowest mean error: 3.334567070007324 mm for frame 6

Saving results

Total time: 31.24663257598877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819065
Iteration 2/25 | Loss: 0.00204463
Iteration 3/25 | Loss: 0.00150924
Iteration 4/25 | Loss: 0.00139932
Iteration 5/25 | Loss: 0.00138878
Iteration 6/25 | Loss: 0.00142449
Iteration 7/25 | Loss: 0.00140998
Iteration 8/25 | Loss: 0.00137054
Iteration 9/25 | Loss: 0.00135880
Iteration 10/25 | Loss: 0.00135171
Iteration 11/25 | Loss: 0.00134696
Iteration 12/25 | Loss: 0.00134213
Iteration 13/25 | Loss: 0.00133973
Iteration 14/25 | Loss: 0.00133899
Iteration 15/25 | Loss: 0.00133851
Iteration 16/25 | Loss: 0.00134388
Iteration 17/25 | Loss: 0.00134668
Iteration 18/25 | Loss: 0.00133846
Iteration 19/25 | Loss: 0.00133366
Iteration 20/25 | Loss: 0.00133215
Iteration 21/25 | Loss: 0.00133174
Iteration 22/25 | Loss: 0.00133163
Iteration 23/25 | Loss: 0.00133163
Iteration 24/25 | Loss: 0.00133163
Iteration 25/25 | Loss: 0.00133163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.28348446
Iteration 2/25 | Loss: 0.00095724
Iteration 3/25 | Loss: 0.00089869
Iteration 4/25 | Loss: 0.00089869
Iteration 5/25 | Loss: 0.00089868
Iteration 6/25 | Loss: 0.00089868
Iteration 7/25 | Loss: 0.00089868
Iteration 8/25 | Loss: 0.00089868
Iteration 9/25 | Loss: 0.00089868
Iteration 10/25 | Loss: 0.00089868
Iteration 11/25 | Loss: 0.00089868
Iteration 12/25 | Loss: 0.00089868
Iteration 13/25 | Loss: 0.00089868
Iteration 14/25 | Loss: 0.00089868
Iteration 15/25 | Loss: 0.00089868
Iteration 16/25 | Loss: 0.00089868
Iteration 17/25 | Loss: 0.00089868
Iteration 18/25 | Loss: 0.00089868
Iteration 19/25 | Loss: 0.00089868
Iteration 20/25 | Loss: 0.00089868
Iteration 21/25 | Loss: 0.00089868
Iteration 22/25 | Loss: 0.00089868
Iteration 23/25 | Loss: 0.00089868
Iteration 24/25 | Loss: 0.00089868
Iteration 25/25 | Loss: 0.00089868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089868
Iteration 2/1000 | Loss: 0.00039856
Iteration 3/1000 | Loss: 0.00008998
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00007995
Iteration 6/1000 | Loss: 0.00010371
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00056084
Iteration 9/1000 | Loss: 0.00001784
Iteration 10/1000 | Loss: 0.00001725
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001668
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001623
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001611
Iteration 19/1000 | Loss: 0.00001610
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001608
Iteration 22/1000 | Loss: 0.00001608
Iteration 23/1000 | Loss: 0.00001608
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001607
Iteration 27/1000 | Loss: 0.00001607
Iteration 28/1000 | Loss: 0.00001607
Iteration 29/1000 | Loss: 0.00001606
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001602
Iteration 32/1000 | Loss: 0.00001602
Iteration 33/1000 | Loss: 0.00001599
Iteration 34/1000 | Loss: 0.00001599
Iteration 35/1000 | Loss: 0.00001599
Iteration 36/1000 | Loss: 0.00001598
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001584
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001581
Iteration 57/1000 | Loss: 0.00001580
Iteration 58/1000 | Loss: 0.00001578
Iteration 59/1000 | Loss: 0.00001578
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001570
Iteration 80/1000 | Loss: 0.00001570
Iteration 81/1000 | Loss: 0.00001570
Iteration 82/1000 | Loss: 0.00001570
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001568
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001563
Iteration 99/1000 | Loss: 0.00001563
Iteration 100/1000 | Loss: 0.00001563
Iteration 101/1000 | Loss: 0.00001562
Iteration 102/1000 | Loss: 0.00001562
Iteration 103/1000 | Loss: 0.00001562
Iteration 104/1000 | Loss: 0.00001562
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001560
Iteration 109/1000 | Loss: 0.00001560
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001556
Iteration 133/1000 | Loss: 0.00001556
Iteration 134/1000 | Loss: 0.00001556
Iteration 135/1000 | Loss: 0.00001556
Iteration 136/1000 | Loss: 0.00001556
Iteration 137/1000 | Loss: 0.00001556
Iteration 138/1000 | Loss: 0.00001555
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001555
Iteration 142/1000 | Loss: 0.00001555
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00001554
Iteration 145/1000 | Loss: 0.00001554
Iteration 146/1000 | Loss: 0.00001554
Iteration 147/1000 | Loss: 0.00001554
Iteration 148/1000 | Loss: 0.00001554
Iteration 149/1000 | Loss: 0.00001554
Iteration 150/1000 | Loss: 0.00001553
Iteration 151/1000 | Loss: 0.00001553
Iteration 152/1000 | Loss: 0.00001553
Iteration 153/1000 | Loss: 0.00001553
Iteration 154/1000 | Loss: 0.00001553
Iteration 155/1000 | Loss: 0.00001553
Iteration 156/1000 | Loss: 0.00001553
Iteration 157/1000 | Loss: 0.00001553
Iteration 158/1000 | Loss: 0.00001553
Iteration 159/1000 | Loss: 0.00001553
Iteration 160/1000 | Loss: 0.00001553
Iteration 161/1000 | Loss: 0.00001553
Iteration 162/1000 | Loss: 0.00001553
Iteration 163/1000 | Loss: 0.00001553
Iteration 164/1000 | Loss: 0.00001553
Iteration 165/1000 | Loss: 0.00001553
Iteration 166/1000 | Loss: 0.00001553
Iteration 167/1000 | Loss: 0.00001553
Iteration 168/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.552728281239979e-05, 1.552728281239979e-05, 1.552728281239979e-05, 1.552728281239979e-05, 1.552728281239979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.552728281239979e-05

Optimization complete. Final v2v error: 3.332632541656494 mm

Highest mean error: 4.066030502319336 mm for frame 166

Lowest mean error: 2.9220950603485107 mm for frame 205

Saving results

Total time: 84.32557463645935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484201
Iteration 2/25 | Loss: 0.00160121
Iteration 3/25 | Loss: 0.00135343
Iteration 4/25 | Loss: 0.00132952
Iteration 5/25 | Loss: 0.00132599
Iteration 6/25 | Loss: 0.00132508
Iteration 7/25 | Loss: 0.00132508
Iteration 8/25 | Loss: 0.00132508
Iteration 9/25 | Loss: 0.00132508
Iteration 10/25 | Loss: 0.00132508
Iteration 11/25 | Loss: 0.00132508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013250798219814897, 0.0013250798219814897, 0.0013250798219814897, 0.0013250798219814897, 0.0013250798219814897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013250798219814897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41459024
Iteration 2/25 | Loss: 0.00084279
Iteration 3/25 | Loss: 0.00084278
Iteration 4/25 | Loss: 0.00084278
Iteration 5/25 | Loss: 0.00084278
Iteration 6/25 | Loss: 0.00084278
Iteration 7/25 | Loss: 0.00084278
Iteration 8/25 | Loss: 0.00084278
Iteration 9/25 | Loss: 0.00084278
Iteration 10/25 | Loss: 0.00084278
Iteration 11/25 | Loss: 0.00084278
Iteration 12/25 | Loss: 0.00084278
Iteration 13/25 | Loss: 0.00084278
Iteration 14/25 | Loss: 0.00084278
Iteration 15/25 | Loss: 0.00084278
Iteration 16/25 | Loss: 0.00084278
Iteration 17/25 | Loss: 0.00084278
Iteration 18/25 | Loss: 0.00084278
Iteration 19/25 | Loss: 0.00084278
Iteration 20/25 | Loss: 0.00084278
Iteration 21/25 | Loss: 0.00084278
Iteration 22/25 | Loss: 0.00084278
Iteration 23/25 | Loss: 0.00084278
Iteration 24/25 | Loss: 0.00084278
Iteration 25/25 | Loss: 0.00084278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084278
Iteration 2/1000 | Loss: 0.00004508
Iteration 3/1000 | Loss: 0.00002591
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001815
Iteration 12/1000 | Loss: 0.00001793
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001774
Iteration 16/1000 | Loss: 0.00001769
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001745
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001741
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001741
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001740
Iteration 35/1000 | Loss: 0.00001740
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001737
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001727
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001725
Iteration 61/1000 | Loss: 0.00001724
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001717
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001707
Iteration 113/1000 | Loss: 0.00001707
Iteration 114/1000 | Loss: 0.00001707
Iteration 115/1000 | Loss: 0.00001707
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001706
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001701
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001701
Iteration 151/1000 | Loss: 0.00001701
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001699
Iteration 164/1000 | Loss: 0.00001699
Iteration 165/1000 | Loss: 0.00001699
Iteration 166/1000 | Loss: 0.00001699
Iteration 167/1000 | Loss: 0.00001699
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001698
Iteration 170/1000 | Loss: 0.00001698
Iteration 171/1000 | Loss: 0.00001698
Iteration 172/1000 | Loss: 0.00001698
Iteration 173/1000 | Loss: 0.00001698
Iteration 174/1000 | Loss: 0.00001698
Iteration 175/1000 | Loss: 0.00001698
Iteration 176/1000 | Loss: 0.00001698
Iteration 177/1000 | Loss: 0.00001698
Iteration 178/1000 | Loss: 0.00001698
Iteration 179/1000 | Loss: 0.00001698
Iteration 180/1000 | Loss: 0.00001697
Iteration 181/1000 | Loss: 0.00001697
Iteration 182/1000 | Loss: 0.00001697
Iteration 183/1000 | Loss: 0.00001697
Iteration 184/1000 | Loss: 0.00001697
Iteration 185/1000 | Loss: 0.00001697
Iteration 186/1000 | Loss: 0.00001697
Iteration 187/1000 | Loss: 0.00001697
Iteration 188/1000 | Loss: 0.00001697
Iteration 189/1000 | Loss: 0.00001696
Iteration 190/1000 | Loss: 0.00001696
Iteration 191/1000 | Loss: 0.00001696
Iteration 192/1000 | Loss: 0.00001695
Iteration 193/1000 | Loss: 0.00001695
Iteration 194/1000 | Loss: 0.00001695
Iteration 195/1000 | Loss: 0.00001695
Iteration 196/1000 | Loss: 0.00001694
Iteration 197/1000 | Loss: 0.00001694
Iteration 198/1000 | Loss: 0.00001694
Iteration 199/1000 | Loss: 0.00001694
Iteration 200/1000 | Loss: 0.00001693
Iteration 201/1000 | Loss: 0.00001693
Iteration 202/1000 | Loss: 0.00001693
Iteration 203/1000 | Loss: 0.00001692
Iteration 204/1000 | Loss: 0.00001692
Iteration 205/1000 | Loss: 0.00001692
Iteration 206/1000 | Loss: 0.00001692
Iteration 207/1000 | Loss: 0.00001692
Iteration 208/1000 | Loss: 0.00001692
Iteration 209/1000 | Loss: 0.00001692
Iteration 210/1000 | Loss: 0.00001692
Iteration 211/1000 | Loss: 0.00001692
Iteration 212/1000 | Loss: 0.00001692
Iteration 213/1000 | Loss: 0.00001692
Iteration 214/1000 | Loss: 0.00001692
Iteration 215/1000 | Loss: 0.00001691
Iteration 216/1000 | Loss: 0.00001691
Iteration 217/1000 | Loss: 0.00001691
Iteration 218/1000 | Loss: 0.00001691
Iteration 219/1000 | Loss: 0.00001691
Iteration 220/1000 | Loss: 0.00001691
Iteration 221/1000 | Loss: 0.00001691
Iteration 222/1000 | Loss: 0.00001691
Iteration 223/1000 | Loss: 0.00001690
Iteration 224/1000 | Loss: 0.00001690
Iteration 225/1000 | Loss: 0.00001690
Iteration 226/1000 | Loss: 0.00001690
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001690
Iteration 229/1000 | Loss: 0.00001690
Iteration 230/1000 | Loss: 0.00001690
Iteration 231/1000 | Loss: 0.00001690
Iteration 232/1000 | Loss: 0.00001690
Iteration 233/1000 | Loss: 0.00001690
Iteration 234/1000 | Loss: 0.00001690
Iteration 235/1000 | Loss: 0.00001690
Iteration 236/1000 | Loss: 0.00001690
Iteration 237/1000 | Loss: 0.00001690
Iteration 238/1000 | Loss: 0.00001690
Iteration 239/1000 | Loss: 0.00001690
Iteration 240/1000 | Loss: 0.00001690
Iteration 241/1000 | Loss: 0.00001690
Iteration 242/1000 | Loss: 0.00001690
Iteration 243/1000 | Loss: 0.00001690
Iteration 244/1000 | Loss: 0.00001690
Iteration 245/1000 | Loss: 0.00001690
Iteration 246/1000 | Loss: 0.00001690
Iteration 247/1000 | Loss: 0.00001690
Iteration 248/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.69015365827363e-05, 1.69015365827363e-05, 1.69015365827363e-05, 1.69015365827363e-05, 1.69015365827363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.69015365827363e-05

Optimization complete. Final v2v error: 3.365621328353882 mm

Highest mean error: 4.750849723815918 mm for frame 72

Lowest mean error: 2.945667028427124 mm for frame 161

Saving results

Total time: 45.59144592285156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047764
Iteration 2/25 | Loss: 0.01047764
Iteration 3/25 | Loss: 0.01047764
Iteration 4/25 | Loss: 0.01047764
Iteration 5/25 | Loss: 0.01047764
Iteration 6/25 | Loss: 0.01047764
Iteration 7/25 | Loss: 0.01047764
Iteration 8/25 | Loss: 0.01047764
Iteration 9/25 | Loss: 0.01047764
Iteration 10/25 | Loss: 0.01047763
Iteration 11/25 | Loss: 0.01047763
Iteration 12/25 | Loss: 0.01047763
Iteration 13/25 | Loss: 0.01047763
Iteration 14/25 | Loss: 0.01047763
Iteration 15/25 | Loss: 0.01047763
Iteration 16/25 | Loss: 0.01047763
Iteration 17/25 | Loss: 0.01047763
Iteration 18/25 | Loss: 0.01047763
Iteration 19/25 | Loss: 0.01047763
Iteration 20/25 | Loss: 0.01047763
Iteration 21/25 | Loss: 0.01047763
Iteration 22/25 | Loss: 0.01047763
Iteration 23/25 | Loss: 0.01047763
Iteration 24/25 | Loss: 0.01047763
Iteration 25/25 | Loss: 0.01047763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78250623
Iteration 2/25 | Loss: 0.06090179
Iteration 3/25 | Loss: 0.06059530
Iteration 4/25 | Loss: 0.05993130
Iteration 5/25 | Loss: 0.05993129
Iteration 6/25 | Loss: 0.05993129
Iteration 7/25 | Loss: 0.05993129
Iteration 8/25 | Loss: 0.05993129
Iteration 9/25 | Loss: 0.05993129
Iteration 10/25 | Loss: 0.05993128
Iteration 11/25 | Loss: 0.05993128
Iteration 12/25 | Loss: 0.05993127
Iteration 13/25 | Loss: 0.05993127
Iteration 14/25 | Loss: 0.05993127
Iteration 15/25 | Loss: 0.05993127
Iteration 16/25 | Loss: 0.05993127
Iteration 17/25 | Loss: 0.05993127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.05993127450346947, 0.05993127450346947, 0.05993127450346947, 0.05993127450346947, 0.05993127450346947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05993127450346947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05993127
Iteration 2/1000 | Loss: 0.00263434
Iteration 3/1000 | Loss: 0.00087999
Iteration 4/1000 | Loss: 0.00049510
Iteration 5/1000 | Loss: 0.00022629
Iteration 6/1000 | Loss: 0.00033111
Iteration 7/1000 | Loss: 0.00288648
Iteration 8/1000 | Loss: 0.00016913
Iteration 9/1000 | Loss: 0.00018316
Iteration 10/1000 | Loss: 0.00013344
Iteration 11/1000 | Loss: 0.00013373
Iteration 12/1000 | Loss: 0.00017516
Iteration 13/1000 | Loss: 0.00008674
Iteration 14/1000 | Loss: 0.00006472
Iteration 15/1000 | Loss: 0.00005955
Iteration 16/1000 | Loss: 0.00015342
Iteration 17/1000 | Loss: 0.00009438
Iteration 18/1000 | Loss: 0.00003262
Iteration 19/1000 | Loss: 0.00003118
Iteration 20/1000 | Loss: 0.00007642
Iteration 21/1000 | Loss: 0.00005741
Iteration 22/1000 | Loss: 0.00011752
Iteration 23/1000 | Loss: 0.00003578
Iteration 24/1000 | Loss: 0.00015211
Iteration 25/1000 | Loss: 0.00015691
Iteration 26/1000 | Loss: 0.00005855
Iteration 27/1000 | Loss: 0.00002538
Iteration 28/1000 | Loss: 0.00010607
Iteration 29/1000 | Loss: 0.00002534
Iteration 30/1000 | Loss: 0.00002618
Iteration 31/1000 | Loss: 0.00005432
Iteration 32/1000 | Loss: 0.00002399
Iteration 33/1000 | Loss: 0.00007721
Iteration 34/1000 | Loss: 0.00030753
Iteration 35/1000 | Loss: 0.00002408
Iteration 36/1000 | Loss: 0.00007187
Iteration 37/1000 | Loss: 0.00004176
Iteration 38/1000 | Loss: 0.00002286
Iteration 39/1000 | Loss: 0.00002264
Iteration 40/1000 | Loss: 0.00008858
Iteration 41/1000 | Loss: 0.00002277
Iteration 42/1000 | Loss: 0.00002640
Iteration 43/1000 | Loss: 0.00002398
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002229
Iteration 46/1000 | Loss: 0.00002229
Iteration 47/1000 | Loss: 0.00002229
Iteration 48/1000 | Loss: 0.00002229
Iteration 49/1000 | Loss: 0.00002229
Iteration 50/1000 | Loss: 0.00002229
Iteration 51/1000 | Loss: 0.00002229
Iteration 52/1000 | Loss: 0.00002228
Iteration 53/1000 | Loss: 0.00007957
Iteration 54/1000 | Loss: 0.00008144
Iteration 55/1000 | Loss: 0.00043146
Iteration 56/1000 | Loss: 0.00010950
Iteration 57/1000 | Loss: 0.00010863
Iteration 58/1000 | Loss: 0.00002236
Iteration 59/1000 | Loss: 0.00002546
Iteration 60/1000 | Loss: 0.00005363
Iteration 61/1000 | Loss: 0.00002225
Iteration 62/1000 | Loss: 0.00002962
Iteration 63/1000 | Loss: 0.00002962
Iteration 64/1000 | Loss: 0.00003195
Iteration 65/1000 | Loss: 0.00005330
Iteration 66/1000 | Loss: 0.00002495
Iteration 67/1000 | Loss: 0.00002220
Iteration 68/1000 | Loss: 0.00002218
Iteration 69/1000 | Loss: 0.00002218
Iteration 70/1000 | Loss: 0.00002218
Iteration 71/1000 | Loss: 0.00002218
Iteration 72/1000 | Loss: 0.00002216
Iteration 73/1000 | Loss: 0.00002216
Iteration 74/1000 | Loss: 0.00002214
Iteration 75/1000 | Loss: 0.00002214
Iteration 76/1000 | Loss: 0.00002214
Iteration 77/1000 | Loss: 0.00002214
Iteration 78/1000 | Loss: 0.00002214
Iteration 79/1000 | Loss: 0.00002214
Iteration 80/1000 | Loss: 0.00002214
Iteration 81/1000 | Loss: 0.00002214
Iteration 82/1000 | Loss: 0.00002214
Iteration 83/1000 | Loss: 0.00002214
Iteration 84/1000 | Loss: 0.00002214
Iteration 85/1000 | Loss: 0.00002213
Iteration 86/1000 | Loss: 0.00002213
Iteration 87/1000 | Loss: 0.00002211
Iteration 88/1000 | Loss: 0.00002211
Iteration 89/1000 | Loss: 0.00002211
Iteration 90/1000 | Loss: 0.00002211
Iteration 91/1000 | Loss: 0.00002211
Iteration 92/1000 | Loss: 0.00002211
Iteration 93/1000 | Loss: 0.00002210
Iteration 94/1000 | Loss: 0.00002210
Iteration 95/1000 | Loss: 0.00002210
Iteration 96/1000 | Loss: 0.00002209
Iteration 97/1000 | Loss: 0.00002209
Iteration 98/1000 | Loss: 0.00002208
Iteration 99/1000 | Loss: 0.00002208
Iteration 100/1000 | Loss: 0.00002208
Iteration 101/1000 | Loss: 0.00002207
Iteration 102/1000 | Loss: 0.00002207
Iteration 103/1000 | Loss: 0.00002207
Iteration 104/1000 | Loss: 0.00002207
Iteration 105/1000 | Loss: 0.00002207
Iteration 106/1000 | Loss: 0.00002207
Iteration 107/1000 | Loss: 0.00002207
Iteration 108/1000 | Loss: 0.00002207
Iteration 109/1000 | Loss: 0.00002207
Iteration 110/1000 | Loss: 0.00002207
Iteration 111/1000 | Loss: 0.00002207
Iteration 112/1000 | Loss: 0.00002206
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002205
Iteration 115/1000 | Loss: 0.00002204
Iteration 116/1000 | Loss: 0.00002204
Iteration 117/1000 | Loss: 0.00002204
Iteration 118/1000 | Loss: 0.00002204
Iteration 119/1000 | Loss: 0.00002203
Iteration 120/1000 | Loss: 0.00002203
Iteration 121/1000 | Loss: 0.00002203
Iteration 122/1000 | Loss: 0.00002203
Iteration 123/1000 | Loss: 0.00002203
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002203
Iteration 128/1000 | Loss: 0.00002203
Iteration 129/1000 | Loss: 0.00002202
Iteration 130/1000 | Loss: 0.00002202
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002200
Iteration 138/1000 | Loss: 0.00002200
Iteration 139/1000 | Loss: 0.00002200
Iteration 140/1000 | Loss: 0.00002200
Iteration 141/1000 | Loss: 0.00002199
Iteration 142/1000 | Loss: 0.00002198
Iteration 143/1000 | Loss: 0.00002198
Iteration 144/1000 | Loss: 0.00002198
Iteration 145/1000 | Loss: 0.00002197
Iteration 146/1000 | Loss: 0.00002197
Iteration 147/1000 | Loss: 0.00002197
Iteration 148/1000 | Loss: 0.00002197
Iteration 149/1000 | Loss: 0.00002197
Iteration 150/1000 | Loss: 0.00002197
Iteration 151/1000 | Loss: 0.00002197
Iteration 152/1000 | Loss: 0.00002196
Iteration 153/1000 | Loss: 0.00002196
Iteration 154/1000 | Loss: 0.00002196
Iteration 155/1000 | Loss: 0.00002195
Iteration 156/1000 | Loss: 0.00002195
Iteration 157/1000 | Loss: 0.00002195
Iteration 158/1000 | Loss: 0.00002195
Iteration 159/1000 | Loss: 0.00002195
Iteration 160/1000 | Loss: 0.00002195
Iteration 161/1000 | Loss: 0.00002195
Iteration 162/1000 | Loss: 0.00002195
Iteration 163/1000 | Loss: 0.00002195
Iteration 164/1000 | Loss: 0.00002194
Iteration 165/1000 | Loss: 0.00002194
Iteration 166/1000 | Loss: 0.00002194
Iteration 167/1000 | Loss: 0.00002194
Iteration 168/1000 | Loss: 0.00002194
Iteration 169/1000 | Loss: 0.00002194
Iteration 170/1000 | Loss: 0.00003732
Iteration 171/1000 | Loss: 0.00009774
Iteration 172/1000 | Loss: 0.00005655
Iteration 173/1000 | Loss: 0.00002411
Iteration 174/1000 | Loss: 0.00008407
Iteration 175/1000 | Loss: 0.00002735
Iteration 176/1000 | Loss: 0.00002576
Iteration 177/1000 | Loss: 0.00002412
Iteration 178/1000 | Loss: 0.00003685
Iteration 179/1000 | Loss: 0.00010842
Iteration 180/1000 | Loss: 0.00025197
Iteration 181/1000 | Loss: 0.00035038
Iteration 182/1000 | Loss: 0.00012713
Iteration 183/1000 | Loss: 0.00006861
Iteration 184/1000 | Loss: 0.00002939
Iteration 185/1000 | Loss: 0.00002288
Iteration 186/1000 | Loss: 0.00002211
Iteration 187/1000 | Loss: 0.00006360
Iteration 188/1000 | Loss: 0.00046434
Iteration 189/1000 | Loss: 0.00002193
Iteration 190/1000 | Loss: 0.00002198
Iteration 191/1000 | Loss: 0.00002187
Iteration 192/1000 | Loss: 0.00002187
Iteration 193/1000 | Loss: 0.00002188
Iteration 194/1000 | Loss: 0.00002187
Iteration 195/1000 | Loss: 0.00002187
Iteration 196/1000 | Loss: 0.00002187
Iteration 197/1000 | Loss: 0.00002187
Iteration 198/1000 | Loss: 0.00002187
Iteration 199/1000 | Loss: 0.00002187
Iteration 200/1000 | Loss: 0.00002187
Iteration 201/1000 | Loss: 0.00002187
Iteration 202/1000 | Loss: 0.00002187
Iteration 203/1000 | Loss: 0.00002187
Iteration 204/1000 | Loss: 0.00002187
Iteration 205/1000 | Loss: 0.00002187
Iteration 206/1000 | Loss: 0.00002187
Iteration 207/1000 | Loss: 0.00002187
Iteration 208/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.187170321121812e-05, 2.187170321121812e-05, 2.187170321121812e-05, 2.187170321121812e-05, 2.187170321121812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.187170321121812e-05

Optimization complete. Final v2v error: 4.007072925567627 mm

Highest mean error: 4.585009574890137 mm for frame 120

Lowest mean error: 3.6603753566741943 mm for frame 92

Saving results

Total time: 132.05302548408508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444816
Iteration 2/25 | Loss: 0.00136851
Iteration 3/25 | Loss: 0.00132034
Iteration 4/25 | Loss: 0.00131426
Iteration 5/25 | Loss: 0.00131297
Iteration 6/25 | Loss: 0.00131297
Iteration 7/25 | Loss: 0.00131297
Iteration 8/25 | Loss: 0.00131297
Iteration 9/25 | Loss: 0.00131297
Iteration 10/25 | Loss: 0.00131297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013129736762493849, 0.0013129736762493849, 0.0013129736762493849, 0.0013129736762493849, 0.0013129736762493849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013129736762493849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03583920
Iteration 2/25 | Loss: 0.00066364
Iteration 3/25 | Loss: 0.00066364
Iteration 4/25 | Loss: 0.00066364
Iteration 5/25 | Loss: 0.00066364
Iteration 6/25 | Loss: 0.00066364
Iteration 7/25 | Loss: 0.00066364
Iteration 8/25 | Loss: 0.00066364
Iteration 9/25 | Loss: 0.00066364
Iteration 10/25 | Loss: 0.00066364
Iteration 11/25 | Loss: 0.00066364
Iteration 12/25 | Loss: 0.00066364
Iteration 13/25 | Loss: 0.00066364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006636353791691363, 0.0006636353791691363, 0.0006636353791691363, 0.0006636353791691363, 0.0006636353791691363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006636353791691363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066364
Iteration 2/1000 | Loss: 0.00004647
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002271
Iteration 5/1000 | Loss: 0.00002136
Iteration 6/1000 | Loss: 0.00002025
Iteration 7/1000 | Loss: 0.00001914
Iteration 8/1000 | Loss: 0.00001867
Iteration 9/1000 | Loss: 0.00001826
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001722
Iteration 14/1000 | Loss: 0.00001697
Iteration 15/1000 | Loss: 0.00001686
Iteration 16/1000 | Loss: 0.00001684
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001682
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001674
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001674
Iteration 29/1000 | Loss: 0.00001674
Iteration 30/1000 | Loss: 0.00001673
Iteration 31/1000 | Loss: 0.00001672
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001668
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001666
Iteration 36/1000 | Loss: 0.00001666
Iteration 37/1000 | Loss: 0.00001665
Iteration 38/1000 | Loss: 0.00001665
Iteration 39/1000 | Loss: 0.00001665
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001662
Iteration 45/1000 | Loss: 0.00001662
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001658
Iteration 48/1000 | Loss: 0.00001658
Iteration 49/1000 | Loss: 0.00001657
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001652
Iteration 62/1000 | Loss: 0.00001652
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001652
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001652
Iteration 69/1000 | Loss: 0.00001652
Iteration 70/1000 | Loss: 0.00001652
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00001651
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001648
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001647
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001646
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001644
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001642
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001642
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001642
Iteration 118/1000 | Loss: 0.00001642
Iteration 119/1000 | Loss: 0.00001642
Iteration 120/1000 | Loss: 0.00001642
Iteration 121/1000 | Loss: 0.00001642
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001641
Iteration 130/1000 | Loss: 0.00001641
Iteration 131/1000 | Loss: 0.00001641
Iteration 132/1000 | Loss: 0.00001641
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.6402605979237705e-05, 1.6402605979237705e-05, 1.6402605979237705e-05, 1.6402605979237705e-05, 1.6402605979237705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6402605979237705e-05

Optimization complete. Final v2v error: 3.4799115657806396 mm

Highest mean error: 3.509448528289795 mm for frame 105

Lowest mean error: 3.4368362426757812 mm for frame 49

Saving results

Total time: 35.24285912513733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842545
Iteration 2/25 | Loss: 0.00157953
Iteration 3/25 | Loss: 0.00143451
Iteration 4/25 | Loss: 0.00142513
Iteration 5/25 | Loss: 0.00142339
Iteration 6/25 | Loss: 0.00142339
Iteration 7/25 | Loss: 0.00142339
Iteration 8/25 | Loss: 0.00142339
Iteration 9/25 | Loss: 0.00142339
Iteration 10/25 | Loss: 0.00142339
Iteration 11/25 | Loss: 0.00142339
Iteration 12/25 | Loss: 0.00142339
Iteration 13/25 | Loss: 0.00142339
Iteration 14/25 | Loss: 0.00142339
Iteration 15/25 | Loss: 0.00142339
Iteration 16/25 | Loss: 0.00142339
Iteration 17/25 | Loss: 0.00142339
Iteration 18/25 | Loss: 0.00142339
Iteration 19/25 | Loss: 0.00142339
Iteration 20/25 | Loss: 0.00142339
Iteration 21/25 | Loss: 0.00142339
Iteration 22/25 | Loss: 0.00142339
Iteration 23/25 | Loss: 0.00142339
Iteration 24/25 | Loss: 0.00142339
Iteration 25/25 | Loss: 0.00142339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33390641
Iteration 2/25 | Loss: 0.00093527
Iteration 3/25 | Loss: 0.00093523
Iteration 4/25 | Loss: 0.00093523
Iteration 5/25 | Loss: 0.00093523
Iteration 6/25 | Loss: 0.00093523
Iteration 7/25 | Loss: 0.00093523
Iteration 8/25 | Loss: 0.00093523
Iteration 9/25 | Loss: 0.00093523
Iteration 10/25 | Loss: 0.00093523
Iteration 11/25 | Loss: 0.00093523
Iteration 12/25 | Loss: 0.00093523
Iteration 13/25 | Loss: 0.00093523
Iteration 14/25 | Loss: 0.00093523
Iteration 15/25 | Loss: 0.00093523
Iteration 16/25 | Loss: 0.00093523
Iteration 17/25 | Loss: 0.00093523
Iteration 18/25 | Loss: 0.00093523
Iteration 19/25 | Loss: 0.00093523
Iteration 20/25 | Loss: 0.00093523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009352302877232432, 0.0009352302877232432, 0.0009352302877232432, 0.0009352302877232432, 0.0009352302877232432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009352302877232432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093523
Iteration 2/1000 | Loss: 0.00008366
Iteration 3/1000 | Loss: 0.00004858
Iteration 4/1000 | Loss: 0.00003875
Iteration 5/1000 | Loss: 0.00003502
Iteration 6/1000 | Loss: 0.00003315
Iteration 7/1000 | Loss: 0.00003205
Iteration 8/1000 | Loss: 0.00003107
Iteration 9/1000 | Loss: 0.00003034
Iteration 10/1000 | Loss: 0.00002997
Iteration 11/1000 | Loss: 0.00002967
Iteration 12/1000 | Loss: 0.00002942
Iteration 13/1000 | Loss: 0.00002923
Iteration 14/1000 | Loss: 0.00002918
Iteration 15/1000 | Loss: 0.00002917
Iteration 16/1000 | Loss: 0.00002904
Iteration 17/1000 | Loss: 0.00002890
Iteration 18/1000 | Loss: 0.00002888
Iteration 19/1000 | Loss: 0.00002886
Iteration 20/1000 | Loss: 0.00002879
Iteration 21/1000 | Loss: 0.00002874
Iteration 22/1000 | Loss: 0.00002873
Iteration 23/1000 | Loss: 0.00002872
Iteration 24/1000 | Loss: 0.00002872
Iteration 25/1000 | Loss: 0.00002871
Iteration 26/1000 | Loss: 0.00002871
Iteration 27/1000 | Loss: 0.00002870
Iteration 28/1000 | Loss: 0.00002869
Iteration 29/1000 | Loss: 0.00002869
Iteration 30/1000 | Loss: 0.00002867
Iteration 31/1000 | Loss: 0.00002865
Iteration 32/1000 | Loss: 0.00002864
Iteration 33/1000 | Loss: 0.00002864
Iteration 34/1000 | Loss: 0.00002864
Iteration 35/1000 | Loss: 0.00002863
Iteration 36/1000 | Loss: 0.00002863
Iteration 37/1000 | Loss: 0.00002863
Iteration 38/1000 | Loss: 0.00002862
Iteration 39/1000 | Loss: 0.00002862
Iteration 40/1000 | Loss: 0.00002861
Iteration 41/1000 | Loss: 0.00002861
Iteration 42/1000 | Loss: 0.00002861
Iteration 43/1000 | Loss: 0.00002860
Iteration 44/1000 | Loss: 0.00002860
Iteration 45/1000 | Loss: 0.00002860
Iteration 46/1000 | Loss: 0.00002860
Iteration 47/1000 | Loss: 0.00002860
Iteration 48/1000 | Loss: 0.00002859
Iteration 49/1000 | Loss: 0.00002859
Iteration 50/1000 | Loss: 0.00002859
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002858
Iteration 54/1000 | Loss: 0.00002858
Iteration 55/1000 | Loss: 0.00002857
Iteration 56/1000 | Loss: 0.00002857
Iteration 57/1000 | Loss: 0.00002857
Iteration 58/1000 | Loss: 0.00002856
Iteration 59/1000 | Loss: 0.00002856
Iteration 60/1000 | Loss: 0.00002856
Iteration 61/1000 | Loss: 0.00002855
Iteration 62/1000 | Loss: 0.00002855
Iteration 63/1000 | Loss: 0.00002855
Iteration 64/1000 | Loss: 0.00002855
Iteration 65/1000 | Loss: 0.00002854
Iteration 66/1000 | Loss: 0.00002854
Iteration 67/1000 | Loss: 0.00002854
Iteration 68/1000 | Loss: 0.00002853
Iteration 69/1000 | Loss: 0.00002853
Iteration 70/1000 | Loss: 0.00002853
Iteration 71/1000 | Loss: 0.00002853
Iteration 72/1000 | Loss: 0.00002853
Iteration 73/1000 | Loss: 0.00002853
Iteration 74/1000 | Loss: 0.00002853
Iteration 75/1000 | Loss: 0.00002852
Iteration 76/1000 | Loss: 0.00002852
Iteration 77/1000 | Loss: 0.00002852
Iteration 78/1000 | Loss: 0.00002852
Iteration 79/1000 | Loss: 0.00002852
Iteration 80/1000 | Loss: 0.00002852
Iteration 81/1000 | Loss: 0.00002852
Iteration 82/1000 | Loss: 0.00002851
Iteration 83/1000 | Loss: 0.00002851
Iteration 84/1000 | Loss: 0.00002851
Iteration 85/1000 | Loss: 0.00002851
Iteration 86/1000 | Loss: 0.00002851
Iteration 87/1000 | Loss: 0.00002850
Iteration 88/1000 | Loss: 0.00002850
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00002850
Iteration 91/1000 | Loss: 0.00002850
Iteration 92/1000 | Loss: 0.00002850
Iteration 93/1000 | Loss: 0.00002849
Iteration 94/1000 | Loss: 0.00002849
Iteration 95/1000 | Loss: 0.00002849
Iteration 96/1000 | Loss: 0.00002849
Iteration 97/1000 | Loss: 0.00002849
Iteration 98/1000 | Loss: 0.00002849
Iteration 99/1000 | Loss: 0.00002849
Iteration 100/1000 | Loss: 0.00002849
Iteration 101/1000 | Loss: 0.00002849
Iteration 102/1000 | Loss: 0.00002849
Iteration 103/1000 | Loss: 0.00002848
Iteration 104/1000 | Loss: 0.00002848
Iteration 105/1000 | Loss: 0.00002848
Iteration 106/1000 | Loss: 0.00002847
Iteration 107/1000 | Loss: 0.00002847
Iteration 108/1000 | Loss: 0.00002847
Iteration 109/1000 | Loss: 0.00002847
Iteration 110/1000 | Loss: 0.00002847
Iteration 111/1000 | Loss: 0.00002846
Iteration 112/1000 | Loss: 0.00002846
Iteration 113/1000 | Loss: 0.00002846
Iteration 114/1000 | Loss: 0.00002846
Iteration 115/1000 | Loss: 0.00002846
Iteration 116/1000 | Loss: 0.00002845
Iteration 117/1000 | Loss: 0.00002845
Iteration 118/1000 | Loss: 0.00002845
Iteration 119/1000 | Loss: 0.00002845
Iteration 120/1000 | Loss: 0.00002845
Iteration 121/1000 | Loss: 0.00002844
Iteration 122/1000 | Loss: 0.00002844
Iteration 123/1000 | Loss: 0.00002844
Iteration 124/1000 | Loss: 0.00002844
Iteration 125/1000 | Loss: 0.00002844
Iteration 126/1000 | Loss: 0.00002843
Iteration 127/1000 | Loss: 0.00002843
Iteration 128/1000 | Loss: 0.00002843
Iteration 129/1000 | Loss: 0.00002843
Iteration 130/1000 | Loss: 0.00002843
Iteration 131/1000 | Loss: 0.00002843
Iteration 132/1000 | Loss: 0.00002843
Iteration 133/1000 | Loss: 0.00002843
Iteration 134/1000 | Loss: 0.00002843
Iteration 135/1000 | Loss: 0.00002843
Iteration 136/1000 | Loss: 0.00002843
Iteration 137/1000 | Loss: 0.00002842
Iteration 138/1000 | Loss: 0.00002842
Iteration 139/1000 | Loss: 0.00002842
Iteration 140/1000 | Loss: 0.00002841
Iteration 141/1000 | Loss: 0.00002841
Iteration 142/1000 | Loss: 0.00002841
Iteration 143/1000 | Loss: 0.00002841
Iteration 144/1000 | Loss: 0.00002840
Iteration 145/1000 | Loss: 0.00002840
Iteration 146/1000 | Loss: 0.00002840
Iteration 147/1000 | Loss: 0.00002840
Iteration 148/1000 | Loss: 0.00002840
Iteration 149/1000 | Loss: 0.00002840
Iteration 150/1000 | Loss: 0.00002840
Iteration 151/1000 | Loss: 0.00002840
Iteration 152/1000 | Loss: 0.00002840
Iteration 153/1000 | Loss: 0.00002840
Iteration 154/1000 | Loss: 0.00002840
Iteration 155/1000 | Loss: 0.00002840
Iteration 156/1000 | Loss: 0.00002840
Iteration 157/1000 | Loss: 0.00002840
Iteration 158/1000 | Loss: 0.00002840
Iteration 159/1000 | Loss: 0.00002839
Iteration 160/1000 | Loss: 0.00002839
Iteration 161/1000 | Loss: 0.00002839
Iteration 162/1000 | Loss: 0.00002839
Iteration 163/1000 | Loss: 0.00002839
Iteration 164/1000 | Loss: 0.00002839
Iteration 165/1000 | Loss: 0.00002839
Iteration 166/1000 | Loss: 0.00002839
Iteration 167/1000 | Loss: 0.00002839
Iteration 168/1000 | Loss: 0.00002839
Iteration 169/1000 | Loss: 0.00002838
Iteration 170/1000 | Loss: 0.00002838
Iteration 171/1000 | Loss: 0.00002838
Iteration 172/1000 | Loss: 0.00002838
Iteration 173/1000 | Loss: 0.00002838
Iteration 174/1000 | Loss: 0.00002838
Iteration 175/1000 | Loss: 0.00002838
Iteration 176/1000 | Loss: 0.00002838
Iteration 177/1000 | Loss: 0.00002838
Iteration 178/1000 | Loss: 0.00002838
Iteration 179/1000 | Loss: 0.00002838
Iteration 180/1000 | Loss: 0.00002838
Iteration 181/1000 | Loss: 0.00002838
Iteration 182/1000 | Loss: 0.00002838
Iteration 183/1000 | Loss: 0.00002838
Iteration 184/1000 | Loss: 0.00002838
Iteration 185/1000 | Loss: 0.00002838
Iteration 186/1000 | Loss: 0.00002838
Iteration 187/1000 | Loss: 0.00002838
Iteration 188/1000 | Loss: 0.00002838
Iteration 189/1000 | Loss: 0.00002838
Iteration 190/1000 | Loss: 0.00002838
Iteration 191/1000 | Loss: 0.00002838
Iteration 192/1000 | Loss: 0.00002838
Iteration 193/1000 | Loss: 0.00002838
Iteration 194/1000 | Loss: 0.00002838
Iteration 195/1000 | Loss: 0.00002838
Iteration 196/1000 | Loss: 0.00002838
Iteration 197/1000 | Loss: 0.00002838
Iteration 198/1000 | Loss: 0.00002838
Iteration 199/1000 | Loss: 0.00002838
Iteration 200/1000 | Loss: 0.00002838
Iteration 201/1000 | Loss: 0.00002838
Iteration 202/1000 | Loss: 0.00002838
Iteration 203/1000 | Loss: 0.00002838
Iteration 204/1000 | Loss: 0.00002838
Iteration 205/1000 | Loss: 0.00002838
Iteration 206/1000 | Loss: 0.00002838
Iteration 207/1000 | Loss: 0.00002838
Iteration 208/1000 | Loss: 0.00002838
Iteration 209/1000 | Loss: 0.00002838
Iteration 210/1000 | Loss: 0.00002838
Iteration 211/1000 | Loss: 0.00002838
Iteration 212/1000 | Loss: 0.00002838
Iteration 213/1000 | Loss: 0.00002838
Iteration 214/1000 | Loss: 0.00002838
Iteration 215/1000 | Loss: 0.00002838
Iteration 216/1000 | Loss: 0.00002838
Iteration 217/1000 | Loss: 0.00002838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.838308319041971e-05, 2.838308319041971e-05, 2.838308319041971e-05, 2.838308319041971e-05, 2.838308319041971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.838308319041971e-05

Optimization complete. Final v2v error: 4.237959861755371 mm

Highest mean error: 5.816646099090576 mm for frame 160

Lowest mean error: 3.272294282913208 mm for frame 141

Saving results

Total time: 47.75552487373352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830900
Iteration 2/25 | Loss: 0.00157590
Iteration 3/25 | Loss: 0.00134477
Iteration 4/25 | Loss: 0.00131393
Iteration 5/25 | Loss: 0.00130955
Iteration 6/25 | Loss: 0.00130921
Iteration 7/25 | Loss: 0.00130921
Iteration 8/25 | Loss: 0.00130921
Iteration 9/25 | Loss: 0.00130921
Iteration 10/25 | Loss: 0.00130921
Iteration 11/25 | Loss: 0.00130921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001309206010773778, 0.001309206010773778, 0.001309206010773778, 0.001309206010773778, 0.001309206010773778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001309206010773778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40105283
Iteration 2/25 | Loss: 0.00085423
Iteration 3/25 | Loss: 0.00085423
Iteration 4/25 | Loss: 0.00085423
Iteration 5/25 | Loss: 0.00085423
Iteration 6/25 | Loss: 0.00085423
Iteration 7/25 | Loss: 0.00085423
Iteration 8/25 | Loss: 0.00085423
Iteration 9/25 | Loss: 0.00085423
Iteration 10/25 | Loss: 0.00085423
Iteration 11/25 | Loss: 0.00085423
Iteration 12/25 | Loss: 0.00085423
Iteration 13/25 | Loss: 0.00085423
Iteration 14/25 | Loss: 0.00085423
Iteration 15/25 | Loss: 0.00085423
Iteration 16/25 | Loss: 0.00085423
Iteration 17/25 | Loss: 0.00085423
Iteration 18/25 | Loss: 0.00085423
Iteration 19/25 | Loss: 0.00085423
Iteration 20/25 | Loss: 0.00085423
Iteration 21/25 | Loss: 0.00085423
Iteration 22/25 | Loss: 0.00085423
Iteration 23/25 | Loss: 0.00085423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008542297873646021, 0.0008542297873646021, 0.0008542297873646021, 0.0008542297873646021, 0.0008542297873646021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008542297873646021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085423
Iteration 2/1000 | Loss: 0.00003349
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00001959
Iteration 5/1000 | Loss: 0.00001823
Iteration 6/1000 | Loss: 0.00001707
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001616
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001545
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001527
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001505
Iteration 18/1000 | Loss: 0.00001504
Iteration 19/1000 | Loss: 0.00001503
Iteration 20/1000 | Loss: 0.00001495
Iteration 21/1000 | Loss: 0.00001494
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001488
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001483
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001482
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001477
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001452
Iteration 58/1000 | Loss: 0.00001452
Iteration 59/1000 | Loss: 0.00001452
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001451
Iteration 62/1000 | Loss: 0.00001451
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001441
Iteration 89/1000 | Loss: 0.00001441
Iteration 90/1000 | Loss: 0.00001440
Iteration 91/1000 | Loss: 0.00001440
Iteration 92/1000 | Loss: 0.00001440
Iteration 93/1000 | Loss: 0.00001440
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001439
Iteration 98/1000 | Loss: 0.00001439
Iteration 99/1000 | Loss: 0.00001439
Iteration 100/1000 | Loss: 0.00001439
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00001437
Iteration 103/1000 | Loss: 0.00001437
Iteration 104/1000 | Loss: 0.00001437
Iteration 105/1000 | Loss: 0.00001437
Iteration 106/1000 | Loss: 0.00001437
Iteration 107/1000 | Loss: 0.00001437
Iteration 108/1000 | Loss: 0.00001437
Iteration 109/1000 | Loss: 0.00001436
Iteration 110/1000 | Loss: 0.00001436
Iteration 111/1000 | Loss: 0.00001436
Iteration 112/1000 | Loss: 0.00001436
Iteration 113/1000 | Loss: 0.00001436
Iteration 114/1000 | Loss: 0.00001436
Iteration 115/1000 | Loss: 0.00001436
Iteration 116/1000 | Loss: 0.00001436
Iteration 117/1000 | Loss: 0.00001436
Iteration 118/1000 | Loss: 0.00001436
Iteration 119/1000 | Loss: 0.00001436
Iteration 120/1000 | Loss: 0.00001436
Iteration 121/1000 | Loss: 0.00001436
Iteration 122/1000 | Loss: 0.00001435
Iteration 123/1000 | Loss: 0.00001435
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001435
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001434
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001434
Iteration 132/1000 | Loss: 0.00001434
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001434
Iteration 135/1000 | Loss: 0.00001434
Iteration 136/1000 | Loss: 0.00001434
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001433
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001433
Iteration 149/1000 | Loss: 0.00001433
Iteration 150/1000 | Loss: 0.00001433
Iteration 151/1000 | Loss: 0.00001433
Iteration 152/1000 | Loss: 0.00001433
Iteration 153/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.4329970326798502e-05, 1.4329970326798502e-05, 1.4329970326798502e-05, 1.4329970326798502e-05, 1.4329970326798502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4329970326798502e-05

Optimization complete. Final v2v error: 3.246709108352661 mm

Highest mean error: 3.8314919471740723 mm for frame 133

Lowest mean error: 2.898188352584839 mm for frame 33

Saving results

Total time: 45.81635928153992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423843
Iteration 2/25 | Loss: 0.00137549
Iteration 3/25 | Loss: 0.00132066
Iteration 4/25 | Loss: 0.00131238
Iteration 5/25 | Loss: 0.00131098
Iteration 6/25 | Loss: 0.00131084
Iteration 7/25 | Loss: 0.00131084
Iteration 8/25 | Loss: 0.00131084
Iteration 9/25 | Loss: 0.00131084
Iteration 10/25 | Loss: 0.00131084
Iteration 11/25 | Loss: 0.00131084
Iteration 12/25 | Loss: 0.00131084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013108380371704698, 0.0013108380371704698, 0.0013108380371704698, 0.0013108380371704698, 0.0013108380371704698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013108380371704698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43637884
Iteration 2/25 | Loss: 0.00088469
Iteration 3/25 | Loss: 0.00088469
Iteration 4/25 | Loss: 0.00088469
Iteration 5/25 | Loss: 0.00088468
Iteration 6/25 | Loss: 0.00088468
Iteration 7/25 | Loss: 0.00088468
Iteration 8/25 | Loss: 0.00088468
Iteration 9/25 | Loss: 0.00088468
Iteration 10/25 | Loss: 0.00088468
Iteration 11/25 | Loss: 0.00088468
Iteration 12/25 | Loss: 0.00088468
Iteration 13/25 | Loss: 0.00088468
Iteration 14/25 | Loss: 0.00088468
Iteration 15/25 | Loss: 0.00088468
Iteration 16/25 | Loss: 0.00088468
Iteration 17/25 | Loss: 0.00088468
Iteration 18/25 | Loss: 0.00088468
Iteration 19/25 | Loss: 0.00088468
Iteration 20/25 | Loss: 0.00088468
Iteration 21/25 | Loss: 0.00088468
Iteration 22/25 | Loss: 0.00088468
Iteration 23/25 | Loss: 0.00088468
Iteration 24/25 | Loss: 0.00088468
Iteration 25/25 | Loss: 0.00088468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088468
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00002445
Iteration 4/1000 | Loss: 0.00002273
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00002131
Iteration 7/1000 | Loss: 0.00002076
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002005
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001915
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001911
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001903
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001899
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001894
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001880
Iteration 32/1000 | Loss: 0.00001879
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001875
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001874
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001873
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001868
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001867
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001865
Iteration 65/1000 | Loss: 0.00001865
Iteration 66/1000 | Loss: 0.00001865
Iteration 67/1000 | Loss: 0.00001865
Iteration 68/1000 | Loss: 0.00001865
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001856
Iteration 117/1000 | Loss: 0.00001856
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001856
Iteration 120/1000 | Loss: 0.00001856
Iteration 121/1000 | Loss: 0.00001856
Iteration 122/1000 | Loss: 0.00001856
Iteration 123/1000 | Loss: 0.00001856
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001855
Iteration 126/1000 | Loss: 0.00001855
Iteration 127/1000 | Loss: 0.00001855
Iteration 128/1000 | Loss: 0.00001855
Iteration 129/1000 | Loss: 0.00001855
Iteration 130/1000 | Loss: 0.00001855
Iteration 131/1000 | Loss: 0.00001855
Iteration 132/1000 | Loss: 0.00001855
Iteration 133/1000 | Loss: 0.00001855
Iteration 134/1000 | Loss: 0.00001855
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.8549950254964642e-05, 1.8549950254964642e-05, 1.8549950254964642e-05, 1.8549950254964642e-05, 1.8549950254964642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8549950254964642e-05

Optimization complete. Final v2v error: 3.6706502437591553 mm

Highest mean error: 4.102048397064209 mm for frame 25

Lowest mean error: 3.4415533542633057 mm for frame 46

Saving results

Total time: 37.927263259887695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423915
Iteration 2/25 | Loss: 0.00135436
Iteration 3/25 | Loss: 0.00129587
Iteration 4/25 | Loss: 0.00128704
Iteration 5/25 | Loss: 0.00128379
Iteration 6/25 | Loss: 0.00128373
Iteration 7/25 | Loss: 0.00128373
Iteration 8/25 | Loss: 0.00128373
Iteration 9/25 | Loss: 0.00128373
Iteration 10/25 | Loss: 0.00128373
Iteration 11/25 | Loss: 0.00128373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012837315443903208, 0.0012837315443903208, 0.0012837315443903208, 0.0012837315443903208, 0.0012837315443903208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012837315443903208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48040295
Iteration 2/25 | Loss: 0.00085940
Iteration 3/25 | Loss: 0.00085939
Iteration 4/25 | Loss: 0.00085939
Iteration 5/25 | Loss: 0.00085939
Iteration 6/25 | Loss: 0.00085939
Iteration 7/25 | Loss: 0.00085939
Iteration 8/25 | Loss: 0.00085939
Iteration 9/25 | Loss: 0.00085939
Iteration 10/25 | Loss: 0.00085939
Iteration 11/25 | Loss: 0.00085939
Iteration 12/25 | Loss: 0.00085939
Iteration 13/25 | Loss: 0.00085939
Iteration 14/25 | Loss: 0.00085939
Iteration 15/25 | Loss: 0.00085939
Iteration 16/25 | Loss: 0.00085939
Iteration 17/25 | Loss: 0.00085939
Iteration 18/25 | Loss: 0.00085939
Iteration 19/25 | Loss: 0.00085939
Iteration 20/25 | Loss: 0.00085939
Iteration 21/25 | Loss: 0.00085939
Iteration 22/25 | Loss: 0.00085939
Iteration 23/25 | Loss: 0.00085939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008593907696194947, 0.0008593907696194947, 0.0008593907696194947, 0.0008593907696194947, 0.0008593907696194947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008593907696194947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085939
Iteration 2/1000 | Loss: 0.00002284
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001363
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001339
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001332
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001326
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001317
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001316
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001315
Iteration 45/1000 | Loss: 0.00001315
Iteration 46/1000 | Loss: 0.00001314
Iteration 47/1000 | Loss: 0.00001314
Iteration 48/1000 | Loss: 0.00001313
Iteration 49/1000 | Loss: 0.00001313
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001312
Iteration 53/1000 | Loss: 0.00001312
Iteration 54/1000 | Loss: 0.00001312
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001311
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001310
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001308
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001305
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001304
Iteration 76/1000 | Loss: 0.00001304
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001300
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001290
Iteration 115/1000 | Loss: 0.00001290
Iteration 116/1000 | Loss: 0.00001290
Iteration 117/1000 | Loss: 0.00001290
Iteration 118/1000 | Loss: 0.00001290
Iteration 119/1000 | Loss: 0.00001290
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001289
Iteration 125/1000 | Loss: 0.00001289
Iteration 126/1000 | Loss: 0.00001289
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001288
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001287
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001287
Iteration 163/1000 | Loss: 0.00001287
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001286
Iteration 166/1000 | Loss: 0.00001286
Iteration 167/1000 | Loss: 0.00001286
Iteration 168/1000 | Loss: 0.00001286
Iteration 169/1000 | Loss: 0.00001286
Iteration 170/1000 | Loss: 0.00001286
Iteration 171/1000 | Loss: 0.00001286
Iteration 172/1000 | Loss: 0.00001286
Iteration 173/1000 | Loss: 0.00001286
Iteration 174/1000 | Loss: 0.00001286
Iteration 175/1000 | Loss: 0.00001286
Iteration 176/1000 | Loss: 0.00001286
Iteration 177/1000 | Loss: 0.00001286
Iteration 178/1000 | Loss: 0.00001286
Iteration 179/1000 | Loss: 0.00001286
Iteration 180/1000 | Loss: 0.00001286
Iteration 181/1000 | Loss: 0.00001285
Iteration 182/1000 | Loss: 0.00001285
Iteration 183/1000 | Loss: 0.00001285
Iteration 184/1000 | Loss: 0.00001285
Iteration 185/1000 | Loss: 0.00001285
Iteration 186/1000 | Loss: 0.00001285
Iteration 187/1000 | Loss: 0.00001285
Iteration 188/1000 | Loss: 0.00001285
Iteration 189/1000 | Loss: 0.00001285
Iteration 190/1000 | Loss: 0.00001285
Iteration 191/1000 | Loss: 0.00001285
Iteration 192/1000 | Loss: 0.00001285
Iteration 193/1000 | Loss: 0.00001285
Iteration 194/1000 | Loss: 0.00001285
Iteration 195/1000 | Loss: 0.00001285
Iteration 196/1000 | Loss: 0.00001284
Iteration 197/1000 | Loss: 0.00001284
Iteration 198/1000 | Loss: 0.00001284
Iteration 199/1000 | Loss: 0.00001284
Iteration 200/1000 | Loss: 0.00001284
Iteration 201/1000 | Loss: 0.00001284
Iteration 202/1000 | Loss: 0.00001284
Iteration 203/1000 | Loss: 0.00001284
Iteration 204/1000 | Loss: 0.00001284
Iteration 205/1000 | Loss: 0.00001284
Iteration 206/1000 | Loss: 0.00001284
Iteration 207/1000 | Loss: 0.00001284
Iteration 208/1000 | Loss: 0.00001284
Iteration 209/1000 | Loss: 0.00001284
Iteration 210/1000 | Loss: 0.00001284
Iteration 211/1000 | Loss: 0.00001284
Iteration 212/1000 | Loss: 0.00001284
Iteration 213/1000 | Loss: 0.00001284
Iteration 214/1000 | Loss: 0.00001284
Iteration 215/1000 | Loss: 0.00001284
Iteration 216/1000 | Loss: 0.00001284
Iteration 217/1000 | Loss: 0.00001284
Iteration 218/1000 | Loss: 0.00001284
Iteration 219/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.28368601508555e-05, 1.28368601508555e-05, 1.28368601508555e-05, 1.28368601508555e-05, 1.28368601508555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.28368601508555e-05

Optimization complete. Final v2v error: 3.033822774887085 mm

Highest mean error: 3.402651786804199 mm for frame 156

Lowest mean error: 2.788281202316284 mm for frame 46

Saving results

Total time: 45.90278506278992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047491
Iteration 2/25 | Loss: 0.00320649
Iteration 3/25 | Loss: 0.00248808
Iteration 4/25 | Loss: 0.00208724
Iteration 5/25 | Loss: 0.00178236
Iteration 6/25 | Loss: 0.00157785
Iteration 7/25 | Loss: 0.00146227
Iteration 8/25 | Loss: 0.00141068
Iteration 9/25 | Loss: 0.00140056
Iteration 10/25 | Loss: 0.00132736
Iteration 11/25 | Loss: 0.00131913
Iteration 12/25 | Loss: 0.00131444
Iteration 13/25 | Loss: 0.00131539
Iteration 14/25 | Loss: 0.00131375
Iteration 15/25 | Loss: 0.00131426
Iteration 16/25 | Loss: 0.00131638
Iteration 17/25 | Loss: 0.00131676
Iteration 18/25 | Loss: 0.00131470
Iteration 19/25 | Loss: 0.00131327
Iteration 20/25 | Loss: 0.00131490
Iteration 21/25 | Loss: 0.00131530
Iteration 22/25 | Loss: 0.00131578
Iteration 23/25 | Loss: 0.00131509
Iteration 24/25 | Loss: 0.00131540
Iteration 25/25 | Loss: 0.00131534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35124516
Iteration 2/25 | Loss: 0.00109392
Iteration 3/25 | Loss: 0.00109392
Iteration 4/25 | Loss: 0.00109392
Iteration 5/25 | Loss: 0.00109392
Iteration 6/25 | Loss: 0.00109392
Iteration 7/25 | Loss: 0.00109392
Iteration 8/25 | Loss: 0.00109392
Iteration 9/25 | Loss: 0.00109392
Iteration 10/25 | Loss: 0.00109392
Iteration 11/25 | Loss: 0.00109392
Iteration 12/25 | Loss: 0.00109392
Iteration 13/25 | Loss: 0.00109392
Iteration 14/25 | Loss: 0.00109392
Iteration 15/25 | Loss: 0.00109392
Iteration 16/25 | Loss: 0.00109392
Iteration 17/25 | Loss: 0.00109392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010939165949821472, 0.0010939165949821472, 0.0010939165949821472, 0.0010939165949821472, 0.0010939165949821472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010939165949821472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109392
Iteration 2/1000 | Loss: 0.00009647
Iteration 3/1000 | Loss: 0.00004538
Iteration 4/1000 | Loss: 0.00007341
Iteration 5/1000 | Loss: 0.00005782
Iteration 6/1000 | Loss: 0.00005914
Iteration 7/1000 | Loss: 0.00007919
Iteration 8/1000 | Loss: 0.00005680
Iteration 9/1000 | Loss: 0.00006744
Iteration 10/1000 | Loss: 0.00007581
Iteration 11/1000 | Loss: 0.00007876
Iteration 12/1000 | Loss: 0.00008118
Iteration 13/1000 | Loss: 0.00007686
Iteration 14/1000 | Loss: 0.00007720
Iteration 15/1000 | Loss: 0.00007714
Iteration 16/1000 | Loss: 0.00006478
Iteration 17/1000 | Loss: 0.00006515
Iteration 18/1000 | Loss: 0.00005645
Iteration 19/1000 | Loss: 0.00006202
Iteration 20/1000 | Loss: 0.00007867
Iteration 21/1000 | Loss: 0.00007432
Iteration 22/1000 | Loss: 0.00007703
Iteration 23/1000 | Loss: 0.00007368
Iteration 24/1000 | Loss: 0.00007660
Iteration 25/1000 | Loss: 0.00007655
Iteration 26/1000 | Loss: 0.00007533
Iteration 27/1000 | Loss: 0.00007847
Iteration 28/1000 | Loss: 0.00007475
Iteration 29/1000 | Loss: 0.00007583
Iteration 30/1000 | Loss: 0.00007582
Iteration 31/1000 | Loss: 0.00007437
Iteration 32/1000 | Loss: 0.00008864
Iteration 33/1000 | Loss: 0.00007303
Iteration 34/1000 | Loss: 0.00008540
Iteration 35/1000 | Loss: 0.00007353
Iteration 36/1000 | Loss: 0.00008462
Iteration 37/1000 | Loss: 0.00007772
Iteration 38/1000 | Loss: 0.00008500
Iteration 39/1000 | Loss: 0.00007684
Iteration 40/1000 | Loss: 0.00008352
Iteration 41/1000 | Loss: 0.00007135
Iteration 42/1000 | Loss: 0.00003694
Iteration 43/1000 | Loss: 0.00007537
Iteration 44/1000 | Loss: 0.00007704
Iteration 45/1000 | Loss: 0.00007545
Iteration 46/1000 | Loss: 0.00007800
Iteration 47/1000 | Loss: 0.00007197
Iteration 48/1000 | Loss: 0.00007773
Iteration 49/1000 | Loss: 0.00007437
Iteration 50/1000 | Loss: 0.00007695
Iteration 51/1000 | Loss: 0.00007688
Iteration 52/1000 | Loss: 0.00007790
Iteration 53/1000 | Loss: 0.00007720
Iteration 54/1000 | Loss: 0.00009984
Iteration 55/1000 | Loss: 0.00008777
Iteration 56/1000 | Loss: 0.00008043
Iteration 57/1000 | Loss: 0.00007514
Iteration 58/1000 | Loss: 0.00007799
Iteration 59/1000 | Loss: 0.00007599
Iteration 60/1000 | Loss: 0.00004266
Iteration 61/1000 | Loss: 0.00007686
Iteration 62/1000 | Loss: 0.00007223
Iteration 63/1000 | Loss: 0.00007199
Iteration 64/1000 | Loss: 0.00007609
Iteration 65/1000 | Loss: 0.00008877
Iteration 66/1000 | Loss: 0.00007274
Iteration 67/1000 | Loss: 0.00007260
Iteration 68/1000 | Loss: 0.00006041
Iteration 69/1000 | Loss: 0.00006228
Iteration 70/1000 | Loss: 0.00007200
Iteration 71/1000 | Loss: 0.00007261
Iteration 72/1000 | Loss: 0.00007018
Iteration 73/1000 | Loss: 0.00007660
Iteration 74/1000 | Loss: 0.00006993
Iteration 75/1000 | Loss: 0.00007565
Iteration 76/1000 | Loss: 0.00007662
Iteration 77/1000 | Loss: 0.00007928
Iteration 78/1000 | Loss: 0.00007516
Iteration 79/1000 | Loss: 0.00009267
Iteration 80/1000 | Loss: 0.00007208
Iteration 81/1000 | Loss: 0.00007338
Iteration 82/1000 | Loss: 0.00007481
Iteration 83/1000 | Loss: 0.00007699
Iteration 84/1000 | Loss: 0.00006534
Iteration 85/1000 | Loss: 0.00007478
Iteration 86/1000 | Loss: 0.00006413
Iteration 87/1000 | Loss: 0.00006482
Iteration 88/1000 | Loss: 0.00007166
Iteration 89/1000 | Loss: 0.00007570
Iteration 90/1000 | Loss: 0.00008092
Iteration 91/1000 | Loss: 0.00007504
Iteration 92/1000 | Loss: 0.00007448
Iteration 93/1000 | Loss: 0.00008251
Iteration 94/1000 | Loss: 0.00005360
Iteration 95/1000 | Loss: 0.00006879
Iteration 96/1000 | Loss: 0.00006818
Iteration 97/1000 | Loss: 0.00007155
Iteration 98/1000 | Loss: 0.00007394
Iteration 99/1000 | Loss: 0.00007351
Iteration 100/1000 | Loss: 0.00007176
Iteration 101/1000 | Loss: 0.00007745
Iteration 102/1000 | Loss: 0.00008837
Iteration 103/1000 | Loss: 0.00007525
Iteration 104/1000 | Loss: 0.00004888
Iteration 105/1000 | Loss: 0.00005687
Iteration 106/1000 | Loss: 0.00004598
Iteration 107/1000 | Loss: 0.00006678
Iteration 108/1000 | Loss: 0.00008113
Iteration 109/1000 | Loss: 0.00007586
Iteration 110/1000 | Loss: 0.00007129
Iteration 111/1000 | Loss: 0.00007065
Iteration 112/1000 | Loss: 0.00006606
Iteration 113/1000 | Loss: 0.00005645
Iteration 114/1000 | Loss: 0.00004972
Iteration 115/1000 | Loss: 0.00006841
Iteration 116/1000 | Loss: 0.00007184
Iteration 117/1000 | Loss: 0.00007359
Iteration 118/1000 | Loss: 0.00007305
Iteration 119/1000 | Loss: 0.00007802
Iteration 120/1000 | Loss: 0.00008443
Iteration 121/1000 | Loss: 0.00008151
Iteration 122/1000 | Loss: 0.00008007
Iteration 123/1000 | Loss: 0.00008089
Iteration 124/1000 | Loss: 0.00007757
Iteration 125/1000 | Loss: 0.00009110
Iteration 126/1000 | Loss: 0.00006233
Iteration 127/1000 | Loss: 0.00003677
Iteration 128/1000 | Loss: 0.00007253
Iteration 129/1000 | Loss: 0.00008027
Iteration 130/1000 | Loss: 0.00009093
Iteration 131/1000 | Loss: 0.00008631
Iteration 132/1000 | Loss: 0.00008581
Iteration 133/1000 | Loss: 0.00006968
Iteration 134/1000 | Loss: 0.00007002
Iteration 135/1000 | Loss: 0.00006538
Iteration 136/1000 | Loss: 0.00006420
Iteration 137/1000 | Loss: 0.00007134
Iteration 138/1000 | Loss: 0.00007986
Iteration 139/1000 | Loss: 0.00007284
Iteration 140/1000 | Loss: 0.00007476
Iteration 141/1000 | Loss: 0.00007035
Iteration 142/1000 | Loss: 0.00008132
Iteration 143/1000 | Loss: 0.00007096
Iteration 144/1000 | Loss: 0.00008752
Iteration 145/1000 | Loss: 0.00007733
Iteration 146/1000 | Loss: 0.00007234
Iteration 147/1000 | Loss: 0.00007429
Iteration 148/1000 | Loss: 0.00008253
Iteration 149/1000 | Loss: 0.00006916
Iteration 150/1000 | Loss: 0.00008840
Iteration 151/1000 | Loss: 0.00009456
Iteration 152/1000 | Loss: 0.00004406
Iteration 153/1000 | Loss: 0.00006615
Iteration 154/1000 | Loss: 0.00006263
Iteration 155/1000 | Loss: 0.00005417
Iteration 156/1000 | Loss: 0.00007746
Iteration 157/1000 | Loss: 0.00004624
Iteration 158/1000 | Loss: 0.00005540
Iteration 159/1000 | Loss: 0.00008139
Iteration 160/1000 | Loss: 0.00004997
Iteration 161/1000 | Loss: 0.00006955
Iteration 162/1000 | Loss: 0.00005831
Iteration 163/1000 | Loss: 0.00004638
Iteration 164/1000 | Loss: 0.00004790
Iteration 165/1000 | Loss: 0.00005411
Iteration 166/1000 | Loss: 0.00003168
Iteration 167/1000 | Loss: 0.00002681
Iteration 168/1000 | Loss: 0.00002489
Iteration 169/1000 | Loss: 0.00002291
Iteration 170/1000 | Loss: 0.00002212
Iteration 171/1000 | Loss: 0.00002153
Iteration 172/1000 | Loss: 0.00002108
Iteration 173/1000 | Loss: 0.00002064
Iteration 174/1000 | Loss: 0.00002028
Iteration 175/1000 | Loss: 0.00002011
Iteration 176/1000 | Loss: 0.00002003
Iteration 177/1000 | Loss: 0.00062903
Iteration 178/1000 | Loss: 0.00035713
Iteration 179/1000 | Loss: 0.00024793
Iteration 180/1000 | Loss: 0.00029707
Iteration 181/1000 | Loss: 0.00015032
Iteration 182/1000 | Loss: 0.00002558
Iteration 183/1000 | Loss: 0.00010114
Iteration 184/1000 | Loss: 0.00040954
Iteration 185/1000 | Loss: 0.00002580
Iteration 186/1000 | Loss: 0.00002098
Iteration 187/1000 | Loss: 0.00002000
Iteration 188/1000 | Loss: 0.00001933
Iteration 189/1000 | Loss: 0.00001844
Iteration 190/1000 | Loss: 0.00001816
Iteration 191/1000 | Loss: 0.00001803
Iteration 192/1000 | Loss: 0.00001801
Iteration 193/1000 | Loss: 0.00001800
Iteration 194/1000 | Loss: 0.00001798
Iteration 195/1000 | Loss: 0.00001798
Iteration 196/1000 | Loss: 0.00001795
Iteration 197/1000 | Loss: 0.00001780
Iteration 198/1000 | Loss: 0.00001764
Iteration 199/1000 | Loss: 0.00001762
Iteration 200/1000 | Loss: 0.00001758
Iteration 201/1000 | Loss: 0.00001755
Iteration 202/1000 | Loss: 0.00001755
Iteration 203/1000 | Loss: 0.00001751
Iteration 204/1000 | Loss: 0.00001751
Iteration 205/1000 | Loss: 0.00001746
Iteration 206/1000 | Loss: 0.00001743
Iteration 207/1000 | Loss: 0.00001742
Iteration 208/1000 | Loss: 0.00001742
Iteration 209/1000 | Loss: 0.00001741
Iteration 210/1000 | Loss: 0.00001740
Iteration 211/1000 | Loss: 0.00001740
Iteration 212/1000 | Loss: 0.00001739
Iteration 213/1000 | Loss: 0.00001739
Iteration 214/1000 | Loss: 0.00001737
Iteration 215/1000 | Loss: 0.00001736
Iteration 216/1000 | Loss: 0.00001735
Iteration 217/1000 | Loss: 0.00001735
Iteration 218/1000 | Loss: 0.00001734
Iteration 219/1000 | Loss: 0.00001734
Iteration 220/1000 | Loss: 0.00001734
Iteration 221/1000 | Loss: 0.00001733
Iteration 222/1000 | Loss: 0.00001733
Iteration 223/1000 | Loss: 0.00001733
Iteration 224/1000 | Loss: 0.00001733
Iteration 225/1000 | Loss: 0.00001733
Iteration 226/1000 | Loss: 0.00001733
Iteration 227/1000 | Loss: 0.00001732
Iteration 228/1000 | Loss: 0.00001732
Iteration 229/1000 | Loss: 0.00001732
Iteration 230/1000 | Loss: 0.00001732
Iteration 231/1000 | Loss: 0.00001732
Iteration 232/1000 | Loss: 0.00001732
Iteration 233/1000 | Loss: 0.00001732
Iteration 234/1000 | Loss: 0.00001732
Iteration 235/1000 | Loss: 0.00001731
Iteration 236/1000 | Loss: 0.00001731
Iteration 237/1000 | Loss: 0.00001731
Iteration 238/1000 | Loss: 0.00001731
Iteration 239/1000 | Loss: 0.00001731
Iteration 240/1000 | Loss: 0.00001731
Iteration 241/1000 | Loss: 0.00001731
Iteration 242/1000 | Loss: 0.00001731
Iteration 243/1000 | Loss: 0.00001730
Iteration 244/1000 | Loss: 0.00001730
Iteration 245/1000 | Loss: 0.00001730
Iteration 246/1000 | Loss: 0.00001730
Iteration 247/1000 | Loss: 0.00001730
Iteration 248/1000 | Loss: 0.00001730
Iteration 249/1000 | Loss: 0.00001729
Iteration 250/1000 | Loss: 0.00001729
Iteration 251/1000 | Loss: 0.00001729
Iteration 252/1000 | Loss: 0.00001729
Iteration 253/1000 | Loss: 0.00001728
Iteration 254/1000 | Loss: 0.00001728
Iteration 255/1000 | Loss: 0.00001728
Iteration 256/1000 | Loss: 0.00001728
Iteration 257/1000 | Loss: 0.00001728
Iteration 258/1000 | Loss: 0.00001727
Iteration 259/1000 | Loss: 0.00001727
Iteration 260/1000 | Loss: 0.00001727
Iteration 261/1000 | Loss: 0.00001727
Iteration 262/1000 | Loss: 0.00001727
Iteration 263/1000 | Loss: 0.00001727
Iteration 264/1000 | Loss: 0.00001727
Iteration 265/1000 | Loss: 0.00001727
Iteration 266/1000 | Loss: 0.00001727
Iteration 267/1000 | Loss: 0.00001727
Iteration 268/1000 | Loss: 0.00001727
Iteration 269/1000 | Loss: 0.00001727
Iteration 270/1000 | Loss: 0.00001726
Iteration 271/1000 | Loss: 0.00001726
Iteration 272/1000 | Loss: 0.00001726
Iteration 273/1000 | Loss: 0.00001726
Iteration 274/1000 | Loss: 0.00001726
Iteration 275/1000 | Loss: 0.00001726
Iteration 276/1000 | Loss: 0.00001726
Iteration 277/1000 | Loss: 0.00001726
Iteration 278/1000 | Loss: 0.00001725
Iteration 279/1000 | Loss: 0.00001725
Iteration 280/1000 | Loss: 0.00001725
Iteration 281/1000 | Loss: 0.00001725
Iteration 282/1000 | Loss: 0.00001725
Iteration 283/1000 | Loss: 0.00001725
Iteration 284/1000 | Loss: 0.00001725
Iteration 285/1000 | Loss: 0.00001724
Iteration 286/1000 | Loss: 0.00001724
Iteration 287/1000 | Loss: 0.00001724
Iteration 288/1000 | Loss: 0.00001724
Iteration 289/1000 | Loss: 0.00001724
Iteration 290/1000 | Loss: 0.00001724
Iteration 291/1000 | Loss: 0.00001724
Iteration 292/1000 | Loss: 0.00001724
Iteration 293/1000 | Loss: 0.00001724
Iteration 294/1000 | Loss: 0.00001724
Iteration 295/1000 | Loss: 0.00001723
Iteration 296/1000 | Loss: 0.00001723
Iteration 297/1000 | Loss: 0.00001723
Iteration 298/1000 | Loss: 0.00001723
Iteration 299/1000 | Loss: 0.00001723
Iteration 300/1000 | Loss: 0.00001723
Iteration 301/1000 | Loss: 0.00001723
Iteration 302/1000 | Loss: 0.00001723
Iteration 303/1000 | Loss: 0.00001723
Iteration 304/1000 | Loss: 0.00001723
Iteration 305/1000 | Loss: 0.00001723
Iteration 306/1000 | Loss: 0.00001723
Iteration 307/1000 | Loss: 0.00001723
Iteration 308/1000 | Loss: 0.00001723
Iteration 309/1000 | Loss: 0.00001723
Iteration 310/1000 | Loss: 0.00001723
Iteration 311/1000 | Loss: 0.00001723
Iteration 312/1000 | Loss: 0.00001722
Iteration 313/1000 | Loss: 0.00001722
Iteration 314/1000 | Loss: 0.00001722
Iteration 315/1000 | Loss: 0.00001722
Iteration 316/1000 | Loss: 0.00001722
Iteration 317/1000 | Loss: 0.00001722
Iteration 318/1000 | Loss: 0.00001722
Iteration 319/1000 | Loss: 0.00001722
Iteration 320/1000 | Loss: 0.00001722
Iteration 321/1000 | Loss: 0.00001722
Iteration 322/1000 | Loss: 0.00001722
Iteration 323/1000 | Loss: 0.00001722
Iteration 324/1000 | Loss: 0.00001722
Iteration 325/1000 | Loss: 0.00001722
Iteration 326/1000 | Loss: 0.00001722
Iteration 327/1000 | Loss: 0.00001722
Iteration 328/1000 | Loss: 0.00001722
Iteration 329/1000 | Loss: 0.00001722
Iteration 330/1000 | Loss: 0.00001722
Iteration 331/1000 | Loss: 0.00001722
Iteration 332/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [1.721714943414554e-05, 1.721714943414554e-05, 1.721714943414554e-05, 1.721714943414554e-05, 1.721714943414554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.721714943414554e-05

Optimization complete. Final v2v error: 3.5020856857299805 mm

Highest mean error: 4.147684574127197 mm for frame 61

Lowest mean error: 3.3078348636627197 mm for frame 158

Saving results

Total time: 328.61631536483765
