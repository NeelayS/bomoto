Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=79, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4424-4479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816093
Iteration 2/25 | Loss: 0.00098605
Iteration 3/25 | Loss: 0.00072696
Iteration 4/25 | Loss: 0.00067824
Iteration 5/25 | Loss: 0.00066592
Iteration 6/25 | Loss: 0.00066282
Iteration 7/25 | Loss: 0.00066215
Iteration 8/25 | Loss: 0.00066215
Iteration 9/25 | Loss: 0.00066215
Iteration 10/25 | Loss: 0.00066215
Iteration 11/25 | Loss: 0.00066215
Iteration 12/25 | Loss: 0.00066215
Iteration 13/25 | Loss: 0.00066215
Iteration 14/25 | Loss: 0.00066215
Iteration 15/25 | Loss: 0.00066215
Iteration 16/25 | Loss: 0.00066215
Iteration 17/25 | Loss: 0.00066215
Iteration 18/25 | Loss: 0.00066215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000662150967400521, 0.000662150967400521, 0.000662150967400521, 0.000662150967400521, 0.000662150967400521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000662150967400521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39714873
Iteration 2/25 | Loss: 0.00026625
Iteration 3/25 | Loss: 0.00026624
Iteration 4/25 | Loss: 0.00026624
Iteration 5/25 | Loss: 0.00026624
Iteration 6/25 | Loss: 0.00026624
Iteration 7/25 | Loss: 0.00026624
Iteration 8/25 | Loss: 0.00026624
Iteration 9/25 | Loss: 0.00026624
Iteration 10/25 | Loss: 0.00026624
Iteration 11/25 | Loss: 0.00026624
Iteration 12/25 | Loss: 0.00026624
Iteration 13/25 | Loss: 0.00026624
Iteration 14/25 | Loss: 0.00026624
Iteration 15/25 | Loss: 0.00026624
Iteration 16/25 | Loss: 0.00026624
Iteration 17/25 | Loss: 0.00026624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002662395709194243, 0.0002662395709194243, 0.0002662395709194243, 0.0002662395709194243, 0.0002662395709194243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002662395709194243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026624
Iteration 2/1000 | Loss: 0.00003249
Iteration 3/1000 | Loss: 0.00002367
Iteration 4/1000 | Loss: 0.00002223
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001932
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001874
Iteration 11/1000 | Loss: 0.00001856
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001848
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001842
Iteration 19/1000 | Loss: 0.00001842
Iteration 20/1000 | Loss: 0.00001839
Iteration 21/1000 | Loss: 0.00001838
Iteration 22/1000 | Loss: 0.00001838
Iteration 23/1000 | Loss: 0.00001837
Iteration 24/1000 | Loss: 0.00001837
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001836
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001835
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001835
Iteration 32/1000 | Loss: 0.00001834
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001833
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001832
Iteration 45/1000 | Loss: 0.00001832
Iteration 46/1000 | Loss: 0.00001832
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001832
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001831
Iteration 58/1000 | Loss: 0.00001831
Iteration 59/1000 | Loss: 0.00001831
Iteration 60/1000 | Loss: 0.00001831
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.8309032384422608e-05, 1.8309032384422608e-05, 1.8309032384422608e-05, 1.8309032384422608e-05, 1.8309032384422608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8309032384422608e-05

Optimization complete. Final v2v error: 3.610424757003784 mm

Highest mean error: 4.7252278327941895 mm for frame 202

Lowest mean error: 3.162297487258911 mm for frame 160

Saving results

Total time: 36.28680968284607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01219985
Iteration 2/25 | Loss: 0.00401675
Iteration 3/25 | Loss: 0.00221451
Iteration 4/25 | Loss: 0.00193442
Iteration 5/25 | Loss: 0.00135206
Iteration 6/25 | Loss: 0.00123903
Iteration 7/25 | Loss: 0.00118182
Iteration 8/25 | Loss: 0.00113711
Iteration 9/25 | Loss: 0.00111217
Iteration 10/25 | Loss: 0.00109767
Iteration 11/25 | Loss: 0.00109367
Iteration 12/25 | Loss: 0.00109189
Iteration 13/25 | Loss: 0.00109605
Iteration 14/25 | Loss: 0.00110012
Iteration 15/25 | Loss: 0.00109631
Iteration 16/25 | Loss: 0.00109401
Iteration 17/25 | Loss: 0.00109612
Iteration 18/25 | Loss: 0.00109277
Iteration 19/25 | Loss: 0.00109114
Iteration 20/25 | Loss: 0.00108956
Iteration 21/25 | Loss: 0.00109348
Iteration 22/25 | Loss: 0.00109081
Iteration 23/25 | Loss: 0.00109327
Iteration 24/25 | Loss: 0.00109627
Iteration 25/25 | Loss: 0.00108833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62316298
Iteration 2/25 | Loss: 0.00102451
Iteration 3/25 | Loss: 0.00102450
Iteration 4/25 | Loss: 0.00102450
Iteration 5/25 | Loss: 0.00102450
Iteration 6/25 | Loss: 0.00102450
Iteration 7/25 | Loss: 0.00102450
Iteration 8/25 | Loss: 0.00102450
Iteration 9/25 | Loss: 0.00102450
Iteration 10/25 | Loss: 0.00102450
Iteration 11/25 | Loss: 0.00102450
Iteration 12/25 | Loss: 0.00102450
Iteration 13/25 | Loss: 0.00102450
Iteration 14/25 | Loss: 0.00102450
Iteration 15/25 | Loss: 0.00102450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010245012817904353, 0.0010245012817904353, 0.0010245012817904353, 0.0010245012817904353, 0.0010245012817904353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010245012817904353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102450
Iteration 2/1000 | Loss: 0.00010836
Iteration 3/1000 | Loss: 0.00007466
Iteration 4/1000 | Loss: 0.00006319
Iteration 5/1000 | Loss: 0.00005615
Iteration 6/1000 | Loss: 0.00005310
Iteration 7/1000 | Loss: 0.00005065
Iteration 8/1000 | Loss: 0.00004951
Iteration 9/1000 | Loss: 0.00004861
Iteration 10/1000 | Loss: 0.00004795
Iteration 11/1000 | Loss: 0.00004739
Iteration 12/1000 | Loss: 0.00004686
Iteration 13/1000 | Loss: 0.00004656
Iteration 14/1000 | Loss: 0.00004636
Iteration 15/1000 | Loss: 0.00004613
Iteration 16/1000 | Loss: 0.00004601
Iteration 17/1000 | Loss: 0.00004593
Iteration 18/1000 | Loss: 0.00004592
Iteration 19/1000 | Loss: 0.00004591
Iteration 20/1000 | Loss: 0.00004591
Iteration 21/1000 | Loss: 0.00004590
Iteration 22/1000 | Loss: 0.00004590
Iteration 23/1000 | Loss: 0.00004589
Iteration 24/1000 | Loss: 0.00004589
Iteration 25/1000 | Loss: 0.00004588
Iteration 26/1000 | Loss: 0.00004588
Iteration 27/1000 | Loss: 0.00004588
Iteration 28/1000 | Loss: 0.00004587
Iteration 29/1000 | Loss: 0.00004587
Iteration 30/1000 | Loss: 0.00004586
Iteration 31/1000 | Loss: 0.00004586
Iteration 32/1000 | Loss: 0.00004586
Iteration 33/1000 | Loss: 0.00004586
Iteration 34/1000 | Loss: 0.00004586
Iteration 35/1000 | Loss: 0.00004586
Iteration 36/1000 | Loss: 0.00004585
Iteration 37/1000 | Loss: 0.00004585
Iteration 38/1000 | Loss: 0.00004585
Iteration 39/1000 | Loss: 0.00004585
Iteration 40/1000 | Loss: 0.00004585
Iteration 41/1000 | Loss: 0.00004585
Iteration 42/1000 | Loss: 0.00004585
Iteration 43/1000 | Loss: 0.00004585
Iteration 44/1000 | Loss: 0.00004585
Iteration 45/1000 | Loss: 0.00004585
Iteration 46/1000 | Loss: 0.00004584
Iteration 47/1000 | Loss: 0.00004583
Iteration 48/1000 | Loss: 0.00004583
Iteration 49/1000 | Loss: 0.00004583
Iteration 50/1000 | Loss: 0.00004583
Iteration 51/1000 | Loss: 0.00004582
Iteration 52/1000 | Loss: 0.00004582
Iteration 53/1000 | Loss: 0.00004582
Iteration 54/1000 | Loss: 0.00004582
Iteration 55/1000 | Loss: 0.00004582
Iteration 56/1000 | Loss: 0.00004582
Iteration 57/1000 | Loss: 0.00004582
Iteration 58/1000 | Loss: 0.00004582
Iteration 59/1000 | Loss: 0.00004582
Iteration 60/1000 | Loss: 0.00004582
Iteration 61/1000 | Loss: 0.00004582
Iteration 62/1000 | Loss: 0.00004582
Iteration 63/1000 | Loss: 0.00004582
Iteration 64/1000 | Loss: 0.00004581
Iteration 65/1000 | Loss: 0.00004581
Iteration 66/1000 | Loss: 0.00004581
Iteration 67/1000 | Loss: 0.00004581
Iteration 68/1000 | Loss: 0.00004581
Iteration 69/1000 | Loss: 0.00004581
Iteration 70/1000 | Loss: 0.00004580
Iteration 71/1000 | Loss: 0.00004580
Iteration 72/1000 | Loss: 0.00004580
Iteration 73/1000 | Loss: 0.00004579
Iteration 74/1000 | Loss: 0.00004579
Iteration 75/1000 | Loss: 0.00004578
Iteration 76/1000 | Loss: 0.00004578
Iteration 77/1000 | Loss: 0.00004578
Iteration 78/1000 | Loss: 0.00004578
Iteration 79/1000 | Loss: 0.00004578
Iteration 80/1000 | Loss: 0.00004578
Iteration 81/1000 | Loss: 0.00004578
Iteration 82/1000 | Loss: 0.00004578
Iteration 83/1000 | Loss: 0.00004577
Iteration 84/1000 | Loss: 0.00004577
Iteration 85/1000 | Loss: 0.00004577
Iteration 86/1000 | Loss: 0.00004577
Iteration 87/1000 | Loss: 0.00004577
Iteration 88/1000 | Loss: 0.00004577
Iteration 89/1000 | Loss: 0.00004577
Iteration 90/1000 | Loss: 0.00004577
Iteration 91/1000 | Loss: 0.00004577
Iteration 92/1000 | Loss: 0.00004577
Iteration 93/1000 | Loss: 0.00004577
Iteration 94/1000 | Loss: 0.00004577
Iteration 95/1000 | Loss: 0.00004577
Iteration 96/1000 | Loss: 0.00004576
Iteration 97/1000 | Loss: 0.00004576
Iteration 98/1000 | Loss: 0.00004576
Iteration 99/1000 | Loss: 0.00004576
Iteration 100/1000 | Loss: 0.00004576
Iteration 101/1000 | Loss: 0.00004575
Iteration 102/1000 | Loss: 0.00004575
Iteration 103/1000 | Loss: 0.00004575
Iteration 104/1000 | Loss: 0.00004574
Iteration 105/1000 | Loss: 0.00004573
Iteration 106/1000 | Loss: 0.00004573
Iteration 107/1000 | Loss: 0.00004573
Iteration 108/1000 | Loss: 0.00004573
Iteration 109/1000 | Loss: 0.00004572
Iteration 110/1000 | Loss: 0.00004572
Iteration 111/1000 | Loss: 0.00004572
Iteration 112/1000 | Loss: 0.00004572
Iteration 113/1000 | Loss: 0.00004572
Iteration 114/1000 | Loss: 0.00004572
Iteration 115/1000 | Loss: 0.00004572
Iteration 116/1000 | Loss: 0.00004572
Iteration 117/1000 | Loss: 0.00004572
Iteration 118/1000 | Loss: 0.00004571
Iteration 119/1000 | Loss: 0.00004571
Iteration 120/1000 | Loss: 0.00004571
Iteration 121/1000 | Loss: 0.00004571
Iteration 122/1000 | Loss: 0.00004571
Iteration 123/1000 | Loss: 0.00004571
Iteration 124/1000 | Loss: 0.00004571
Iteration 125/1000 | Loss: 0.00004571
Iteration 126/1000 | Loss: 0.00004570
Iteration 127/1000 | Loss: 0.00004570
Iteration 128/1000 | Loss: 0.00004570
Iteration 129/1000 | Loss: 0.00004570
Iteration 130/1000 | Loss: 0.00004570
Iteration 131/1000 | Loss: 0.00004570
Iteration 132/1000 | Loss: 0.00004570
Iteration 133/1000 | Loss: 0.00004570
Iteration 134/1000 | Loss: 0.00004570
Iteration 135/1000 | Loss: 0.00004570
Iteration 136/1000 | Loss: 0.00004569
Iteration 137/1000 | Loss: 0.00004569
Iteration 138/1000 | Loss: 0.00004569
Iteration 139/1000 | Loss: 0.00004569
Iteration 140/1000 | Loss: 0.00004569
Iteration 141/1000 | Loss: 0.00004569
Iteration 142/1000 | Loss: 0.00004569
Iteration 143/1000 | Loss: 0.00004569
Iteration 144/1000 | Loss: 0.00004569
Iteration 145/1000 | Loss: 0.00004568
Iteration 146/1000 | Loss: 0.00004568
Iteration 147/1000 | Loss: 0.00004568
Iteration 148/1000 | Loss: 0.00004568
Iteration 149/1000 | Loss: 0.00004568
Iteration 150/1000 | Loss: 0.00004568
Iteration 151/1000 | Loss: 0.00004568
Iteration 152/1000 | Loss: 0.00004568
Iteration 153/1000 | Loss: 0.00004568
Iteration 154/1000 | Loss: 0.00004567
Iteration 155/1000 | Loss: 0.00004567
Iteration 156/1000 | Loss: 0.00004567
Iteration 157/1000 | Loss: 0.00004567
Iteration 158/1000 | Loss: 0.00004567
Iteration 159/1000 | Loss: 0.00004567
Iteration 160/1000 | Loss: 0.00004567
Iteration 161/1000 | Loss: 0.00004567
Iteration 162/1000 | Loss: 0.00004567
Iteration 163/1000 | Loss: 0.00004567
Iteration 164/1000 | Loss: 0.00004567
Iteration 165/1000 | Loss: 0.00004567
Iteration 166/1000 | Loss: 0.00004567
Iteration 167/1000 | Loss: 0.00004567
Iteration 168/1000 | Loss: 0.00004567
Iteration 169/1000 | Loss: 0.00004567
Iteration 170/1000 | Loss: 0.00004567
Iteration 171/1000 | Loss: 0.00004567
Iteration 172/1000 | Loss: 0.00004567
Iteration 173/1000 | Loss: 0.00004567
Iteration 174/1000 | Loss: 0.00004567
Iteration 175/1000 | Loss: 0.00004567
Iteration 176/1000 | Loss: 0.00004567
Iteration 177/1000 | Loss: 0.00004567
Iteration 178/1000 | Loss: 0.00004567
Iteration 179/1000 | Loss: 0.00004567
Iteration 180/1000 | Loss: 0.00004567
Iteration 181/1000 | Loss: 0.00004567
Iteration 182/1000 | Loss: 0.00004567
Iteration 183/1000 | Loss: 0.00004567
Iteration 184/1000 | Loss: 0.00004567
Iteration 185/1000 | Loss: 0.00004567
Iteration 186/1000 | Loss: 0.00004567
Iteration 187/1000 | Loss: 0.00004567
Iteration 188/1000 | Loss: 0.00004567
Iteration 189/1000 | Loss: 0.00004567
Iteration 190/1000 | Loss: 0.00004567
Iteration 191/1000 | Loss: 0.00004567
Iteration 192/1000 | Loss: 0.00004567
Iteration 193/1000 | Loss: 0.00004567
Iteration 194/1000 | Loss: 0.00004567
Iteration 195/1000 | Loss: 0.00004567
Iteration 196/1000 | Loss: 0.00004567
Iteration 197/1000 | Loss: 0.00004567
Iteration 198/1000 | Loss: 0.00004567
Iteration 199/1000 | Loss: 0.00004567
Iteration 200/1000 | Loss: 0.00004567
Iteration 201/1000 | Loss: 0.00004567
Iteration 202/1000 | Loss: 0.00004567
Iteration 203/1000 | Loss: 0.00004567
Iteration 204/1000 | Loss: 0.00004567
Iteration 205/1000 | Loss: 0.00004567
Iteration 206/1000 | Loss: 0.00004567
Iteration 207/1000 | Loss: 0.00004567
Iteration 208/1000 | Loss: 0.00004567
Iteration 209/1000 | Loss: 0.00004567
Iteration 210/1000 | Loss: 0.00004567
Iteration 211/1000 | Loss: 0.00004567
Iteration 212/1000 | Loss: 0.00004567
Iteration 213/1000 | Loss: 0.00004567
Iteration 214/1000 | Loss: 0.00004567
Iteration 215/1000 | Loss: 0.00004567
Iteration 216/1000 | Loss: 0.00004567
Iteration 217/1000 | Loss: 0.00004567
Iteration 218/1000 | Loss: 0.00004567
Iteration 219/1000 | Loss: 0.00004567
Iteration 220/1000 | Loss: 0.00004567
Iteration 221/1000 | Loss: 0.00004567
Iteration 222/1000 | Loss: 0.00004567
Iteration 223/1000 | Loss: 0.00004567
Iteration 224/1000 | Loss: 0.00004567
Iteration 225/1000 | Loss: 0.00004567
Iteration 226/1000 | Loss: 0.00004567
Iteration 227/1000 | Loss: 0.00004567
Iteration 228/1000 | Loss: 0.00004567
Iteration 229/1000 | Loss: 0.00004567
Iteration 230/1000 | Loss: 0.00004567
Iteration 231/1000 | Loss: 0.00004567
Iteration 232/1000 | Loss: 0.00004567
Iteration 233/1000 | Loss: 0.00004567
Iteration 234/1000 | Loss: 0.00004567
Iteration 235/1000 | Loss: 0.00004567
Iteration 236/1000 | Loss: 0.00004567
Iteration 237/1000 | Loss: 0.00004567
Iteration 238/1000 | Loss: 0.00004567
Iteration 239/1000 | Loss: 0.00004567
Iteration 240/1000 | Loss: 0.00004567
Iteration 241/1000 | Loss: 0.00004567
Iteration 242/1000 | Loss: 0.00004567
Iteration 243/1000 | Loss: 0.00004567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [4.566732241073623e-05, 4.566732241073623e-05, 4.566732241073623e-05, 4.566732241073623e-05, 4.566732241073623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.566732241073623e-05

Optimization complete. Final v2v error: 5.508594512939453 mm

Highest mean error: 6.043919563293457 mm for frame 8

Lowest mean error: 4.980749130249023 mm for frame 18

Saving results

Total time: 78.82163453102112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105919
Iteration 2/25 | Loss: 0.00174794
Iteration 3/25 | Loss: 0.00111810
Iteration 4/25 | Loss: 0.00104372
Iteration 5/25 | Loss: 0.00100075
Iteration 6/25 | Loss: 0.00095368
Iteration 7/25 | Loss: 0.00092895
Iteration 8/25 | Loss: 0.00087672
Iteration 9/25 | Loss: 0.00084272
Iteration 10/25 | Loss: 0.00081269
Iteration 11/25 | Loss: 0.00080479
Iteration 12/25 | Loss: 0.00080227
Iteration 13/25 | Loss: 0.00080127
Iteration 14/25 | Loss: 0.00080347
Iteration 15/25 | Loss: 0.00079636
Iteration 16/25 | Loss: 0.00079435
Iteration 17/25 | Loss: 0.00079332
Iteration 18/25 | Loss: 0.00079700
Iteration 19/25 | Loss: 0.00079804
Iteration 20/25 | Loss: 0.00079651
Iteration 21/25 | Loss: 0.00081025
Iteration 22/25 | Loss: 0.00079703
Iteration 23/25 | Loss: 0.00081899
Iteration 24/25 | Loss: 0.00079816
Iteration 25/25 | Loss: 0.00079180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.28829908
Iteration 2/25 | Loss: 0.00084296
Iteration 3/25 | Loss: 0.00069273
Iteration 4/25 | Loss: 0.00069273
Iteration 5/25 | Loss: 0.00069273
Iteration 6/25 | Loss: 0.00069273
Iteration 7/25 | Loss: 0.00069273
Iteration 8/25 | Loss: 0.00069273
Iteration 9/25 | Loss: 0.00069273
Iteration 10/25 | Loss: 0.00069273
Iteration 11/25 | Loss: 0.00069273
Iteration 12/25 | Loss: 0.00069273
Iteration 13/25 | Loss: 0.00069273
Iteration 14/25 | Loss: 0.00069273
Iteration 15/25 | Loss: 0.00069273
Iteration 16/25 | Loss: 0.00069273
Iteration 17/25 | Loss: 0.00069273
Iteration 18/25 | Loss: 0.00069273
Iteration 19/25 | Loss: 0.00069273
Iteration 20/25 | Loss: 0.00069273
Iteration 21/25 | Loss: 0.00069273
Iteration 22/25 | Loss: 0.00069273
Iteration 23/25 | Loss: 0.00069273
Iteration 24/25 | Loss: 0.00069273
Iteration 25/25 | Loss: 0.00069273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006927295471541584, 0.0006927295471541584, 0.0006927295471541584, 0.0006927295471541584, 0.0006927295471541584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006927295471541584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069273
Iteration 2/1000 | Loss: 0.00025068
Iteration 3/1000 | Loss: 0.00004720
Iteration 4/1000 | Loss: 0.00003498
Iteration 5/1000 | Loss: 0.00003150
Iteration 6/1000 | Loss: 0.00002931
Iteration 7/1000 | Loss: 0.00002836
Iteration 8/1000 | Loss: 0.00002750
Iteration 9/1000 | Loss: 0.00002680
Iteration 10/1000 | Loss: 0.00002616
Iteration 11/1000 | Loss: 0.00002564
Iteration 12/1000 | Loss: 0.00002522
Iteration 13/1000 | Loss: 0.00023792
Iteration 14/1000 | Loss: 0.00018395
Iteration 15/1000 | Loss: 0.00003954
Iteration 16/1000 | Loss: 0.00003272
Iteration 17/1000 | Loss: 0.00002823
Iteration 18/1000 | Loss: 0.00002607
Iteration 19/1000 | Loss: 0.00002515
Iteration 20/1000 | Loss: 0.00024704
Iteration 21/1000 | Loss: 0.00005788
Iteration 22/1000 | Loss: 0.00002728
Iteration 23/1000 | Loss: 0.00002515
Iteration 24/1000 | Loss: 0.00026943
Iteration 25/1000 | Loss: 0.00003649
Iteration 26/1000 | Loss: 0.00002528
Iteration 27/1000 | Loss: 0.00028425
Iteration 28/1000 | Loss: 0.00003825
Iteration 29/1000 | Loss: 0.00002556
Iteration 30/1000 | Loss: 0.00028722
Iteration 31/1000 | Loss: 0.00004023
Iteration 32/1000 | Loss: 0.00002472
Iteration 33/1000 | Loss: 0.00029228
Iteration 34/1000 | Loss: 0.00004779
Iteration 35/1000 | Loss: 0.00028722
Iteration 36/1000 | Loss: 0.00004529
Iteration 37/1000 | Loss: 0.00002576
Iteration 38/1000 | Loss: 0.00029522
Iteration 39/1000 | Loss: 0.00005046
Iteration 40/1000 | Loss: 0.00004312
Iteration 41/1000 | Loss: 0.00003651
Iteration 42/1000 | Loss: 0.00003071
Iteration 43/1000 | Loss: 0.00002635
Iteration 44/1000 | Loss: 0.00005562
Iteration 45/1000 | Loss: 0.00002444
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002283
Iteration 49/1000 | Loss: 0.00002273
Iteration 50/1000 | Loss: 0.00002273
Iteration 51/1000 | Loss: 0.00002272
Iteration 52/1000 | Loss: 0.00002272
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002271
Iteration 55/1000 | Loss: 0.00002270
Iteration 56/1000 | Loss: 0.00002270
Iteration 57/1000 | Loss: 0.00002270
Iteration 58/1000 | Loss: 0.00002269
Iteration 59/1000 | Loss: 0.00002269
Iteration 60/1000 | Loss: 0.00002268
Iteration 61/1000 | Loss: 0.00002268
Iteration 62/1000 | Loss: 0.00002267
Iteration 63/1000 | Loss: 0.00002266
Iteration 64/1000 | Loss: 0.00002266
Iteration 65/1000 | Loss: 0.00002266
Iteration 66/1000 | Loss: 0.00002266
Iteration 67/1000 | Loss: 0.00002266
Iteration 68/1000 | Loss: 0.00002266
Iteration 69/1000 | Loss: 0.00002266
Iteration 70/1000 | Loss: 0.00002266
Iteration 71/1000 | Loss: 0.00002265
Iteration 72/1000 | Loss: 0.00002265
Iteration 73/1000 | Loss: 0.00002264
Iteration 74/1000 | Loss: 0.00002264
Iteration 75/1000 | Loss: 0.00002264
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002264
Iteration 78/1000 | Loss: 0.00002264
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002263
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002263
Iteration 83/1000 | Loss: 0.00002263
Iteration 84/1000 | Loss: 0.00002263
Iteration 85/1000 | Loss: 0.00002263
Iteration 86/1000 | Loss: 0.00002263
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002261
Iteration 92/1000 | Loss: 0.00002260
Iteration 93/1000 | Loss: 0.00002259
Iteration 94/1000 | Loss: 0.00002259
Iteration 95/1000 | Loss: 0.00002259
Iteration 96/1000 | Loss: 0.00002258
Iteration 97/1000 | Loss: 0.00002258
Iteration 98/1000 | Loss: 0.00002258
Iteration 99/1000 | Loss: 0.00002258
Iteration 100/1000 | Loss: 0.00002258
Iteration 101/1000 | Loss: 0.00002258
Iteration 102/1000 | Loss: 0.00002257
Iteration 103/1000 | Loss: 0.00002257
Iteration 104/1000 | Loss: 0.00002257
Iteration 105/1000 | Loss: 0.00002257
Iteration 106/1000 | Loss: 0.00002256
Iteration 107/1000 | Loss: 0.00002256
Iteration 108/1000 | Loss: 0.00002256
Iteration 109/1000 | Loss: 0.00002255
Iteration 110/1000 | Loss: 0.00002255
Iteration 111/1000 | Loss: 0.00002255
Iteration 112/1000 | Loss: 0.00002255
Iteration 113/1000 | Loss: 0.00002254
Iteration 114/1000 | Loss: 0.00002254
Iteration 115/1000 | Loss: 0.00002253
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Iteration 118/1000 | Loss: 0.00002253
Iteration 119/1000 | Loss: 0.00002253
Iteration 120/1000 | Loss: 0.00002253
Iteration 121/1000 | Loss: 0.00002253
Iteration 122/1000 | Loss: 0.00002253
Iteration 123/1000 | Loss: 0.00002253
Iteration 124/1000 | Loss: 0.00002253
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002250
Iteration 139/1000 | Loss: 0.00002250
Iteration 140/1000 | Loss: 0.00002250
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002250
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002250
Iteration 146/1000 | Loss: 0.00002250
Iteration 147/1000 | Loss: 0.00002250
Iteration 148/1000 | Loss: 0.00002250
Iteration 149/1000 | Loss: 0.00002250
Iteration 150/1000 | Loss: 0.00002250
Iteration 151/1000 | Loss: 0.00002250
Iteration 152/1000 | Loss: 0.00002250
Iteration 153/1000 | Loss: 0.00002250
Iteration 154/1000 | Loss: 0.00002250
Iteration 155/1000 | Loss: 0.00002250
Iteration 156/1000 | Loss: 0.00002250
Iteration 157/1000 | Loss: 0.00002250
Iteration 158/1000 | Loss: 0.00002250
Iteration 159/1000 | Loss: 0.00002250
Iteration 160/1000 | Loss: 0.00002250
Iteration 161/1000 | Loss: 0.00002250
Iteration 162/1000 | Loss: 0.00002250
Iteration 163/1000 | Loss: 0.00002250
Iteration 164/1000 | Loss: 0.00002250
Iteration 165/1000 | Loss: 0.00002250
Iteration 166/1000 | Loss: 0.00002250
Iteration 167/1000 | Loss: 0.00002250
Iteration 168/1000 | Loss: 0.00002250
Iteration 169/1000 | Loss: 0.00002250
Iteration 170/1000 | Loss: 0.00002250
Iteration 171/1000 | Loss: 0.00002250
Iteration 172/1000 | Loss: 0.00002250
Iteration 173/1000 | Loss: 0.00002250
Iteration 174/1000 | Loss: 0.00002250
Iteration 175/1000 | Loss: 0.00002250
Iteration 176/1000 | Loss: 0.00002250
Iteration 177/1000 | Loss: 0.00002250
Iteration 178/1000 | Loss: 0.00002250
Iteration 179/1000 | Loss: 0.00002250
Iteration 180/1000 | Loss: 0.00002250
Iteration 181/1000 | Loss: 0.00002250
Iteration 182/1000 | Loss: 0.00002250
Iteration 183/1000 | Loss: 0.00002250
Iteration 184/1000 | Loss: 0.00002250
Iteration 185/1000 | Loss: 0.00002250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.2499261831399053e-05, 2.2499261831399053e-05, 2.2499261831399053e-05, 2.2499261831399053e-05, 2.2499261831399053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2499261831399053e-05

Optimization complete. Final v2v error: 4.065062522888184 mm

Highest mean error: 5.151580333709717 mm for frame 0

Lowest mean error: 3.536684989929199 mm for frame 188

Saving results

Total time: 128.32447981834412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870051
Iteration 2/25 | Loss: 0.00084714
Iteration 3/25 | Loss: 0.00067858
Iteration 4/25 | Loss: 0.00065619
Iteration 5/25 | Loss: 0.00064839
Iteration 6/25 | Loss: 0.00064567
Iteration 7/25 | Loss: 0.00064529
Iteration 8/25 | Loss: 0.00064529
Iteration 9/25 | Loss: 0.00064529
Iteration 10/25 | Loss: 0.00064529
Iteration 11/25 | Loss: 0.00064529
Iteration 12/25 | Loss: 0.00064529
Iteration 13/25 | Loss: 0.00064529
Iteration 14/25 | Loss: 0.00064529
Iteration 15/25 | Loss: 0.00064529
Iteration 16/25 | Loss: 0.00064529
Iteration 17/25 | Loss: 0.00064529
Iteration 18/25 | Loss: 0.00064529
Iteration 19/25 | Loss: 0.00064529
Iteration 20/25 | Loss: 0.00064529
Iteration 21/25 | Loss: 0.00064529
Iteration 22/25 | Loss: 0.00064529
Iteration 23/25 | Loss: 0.00064529
Iteration 24/25 | Loss: 0.00064529
Iteration 25/25 | Loss: 0.00064529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86185718
Iteration 2/25 | Loss: 0.00074296
Iteration 3/25 | Loss: 0.00074296
Iteration 4/25 | Loss: 0.00074296
Iteration 5/25 | Loss: 0.00074296
Iteration 6/25 | Loss: 0.00074296
Iteration 7/25 | Loss: 0.00074296
Iteration 8/25 | Loss: 0.00074296
Iteration 9/25 | Loss: 0.00074296
Iteration 10/25 | Loss: 0.00074296
Iteration 11/25 | Loss: 0.00074296
Iteration 12/25 | Loss: 0.00074296
Iteration 13/25 | Loss: 0.00074296
Iteration 14/25 | Loss: 0.00074296
Iteration 15/25 | Loss: 0.00074296
Iteration 16/25 | Loss: 0.00074296
Iteration 17/25 | Loss: 0.00074296
Iteration 18/25 | Loss: 0.00074296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007429562392644584, 0.0007429562392644584, 0.0007429562392644584, 0.0007429562392644584, 0.0007429562392644584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007429562392644584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074296
Iteration 2/1000 | Loss: 0.00002545
Iteration 3/1000 | Loss: 0.00001881
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001578
Iteration 6/1000 | Loss: 0.00001512
Iteration 7/1000 | Loss: 0.00001468
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001429
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001420
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001419
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001415
Iteration 20/1000 | Loss: 0.00001414
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001413
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001395
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001392
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001391
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001383
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001383
Iteration 72/1000 | Loss: 0.00001382
Iteration 73/1000 | Loss: 0.00001382
Iteration 74/1000 | Loss: 0.00001382
Iteration 75/1000 | Loss: 0.00001382
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001381
Iteration 78/1000 | Loss: 0.00001381
Iteration 79/1000 | Loss: 0.00001381
Iteration 80/1000 | Loss: 0.00001381
Iteration 81/1000 | Loss: 0.00001381
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Iteration 152/1000 | Loss: 0.00001373
Iteration 153/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3725285498367157e-05, 1.3725285498367157e-05, 1.3725285498367157e-05, 1.3725285498367157e-05, 1.3725285498367157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3725285498367157e-05

Optimization complete. Final v2v error: 3.1583199501037598 mm

Highest mean error: 4.087259292602539 mm for frame 91

Lowest mean error: 2.6907970905303955 mm for frame 172

Saving results

Total time: 36.210389375686646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101296
Iteration 2/25 | Loss: 0.00254487
Iteration 3/25 | Loss: 0.00152948
Iteration 4/25 | Loss: 0.00141162
Iteration 5/25 | Loss: 0.00120714
Iteration 6/25 | Loss: 0.00114814
Iteration 7/25 | Loss: 0.00110325
Iteration 8/25 | Loss: 0.00108589
Iteration 9/25 | Loss: 0.00106841
Iteration 10/25 | Loss: 0.00107255
Iteration 11/25 | Loss: 0.00105195
Iteration 12/25 | Loss: 0.00104061
Iteration 13/25 | Loss: 0.00102334
Iteration 14/25 | Loss: 0.00101531
Iteration 15/25 | Loss: 0.00103309
Iteration 16/25 | Loss: 0.00101485
Iteration 17/25 | Loss: 0.00103690
Iteration 18/25 | Loss: 0.00101939
Iteration 19/25 | Loss: 0.00100380
Iteration 20/25 | Loss: 0.00100605
Iteration 21/25 | Loss: 0.00102048
Iteration 22/25 | Loss: 0.00101711
Iteration 23/25 | Loss: 0.00099373
Iteration 24/25 | Loss: 0.00097976
Iteration 25/25 | Loss: 0.00098007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37234664
Iteration 2/25 | Loss: 0.01133692
Iteration 3/25 | Loss: 0.00410424
Iteration 4/25 | Loss: 0.00410423
Iteration 5/25 | Loss: 0.00410423
Iteration 6/25 | Loss: 0.00410423
Iteration 7/25 | Loss: 0.00410423
Iteration 8/25 | Loss: 0.00410423
Iteration 9/25 | Loss: 0.00410423
Iteration 10/25 | Loss: 0.00410423
Iteration 11/25 | Loss: 0.00410423
Iteration 12/25 | Loss: 0.00410423
Iteration 13/25 | Loss: 0.00410423
Iteration 14/25 | Loss: 0.00410423
Iteration 15/25 | Loss: 0.00410423
Iteration 16/25 | Loss: 0.00410423
Iteration 17/25 | Loss: 0.00410423
Iteration 18/25 | Loss: 0.00410423
Iteration 19/25 | Loss: 0.00410423
Iteration 20/25 | Loss: 0.00410423
Iteration 21/25 | Loss: 0.00410423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004104230087250471, 0.004104230087250471, 0.004104230087250471, 0.004104230087250471, 0.004104230087250471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004104230087250471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00410423
Iteration 2/1000 | Loss: 0.00480849
Iteration 3/1000 | Loss: 0.00231735
Iteration 4/1000 | Loss: 0.00366139
Iteration 5/1000 | Loss: 0.00379681
Iteration 6/1000 | Loss: 0.00330691
Iteration 7/1000 | Loss: 0.00609622
Iteration 8/1000 | Loss: 0.00310509
Iteration 9/1000 | Loss: 0.00572839
Iteration 10/1000 | Loss: 0.00358363
Iteration 11/1000 | Loss: 0.00245150
Iteration 12/1000 | Loss: 0.00287393
Iteration 13/1000 | Loss: 0.00363057
Iteration 14/1000 | Loss: 0.00910277
Iteration 15/1000 | Loss: 0.00593217
Iteration 16/1000 | Loss: 0.00595756
Iteration 17/1000 | Loss: 0.00453636
Iteration 18/1000 | Loss: 0.00555249
Iteration 19/1000 | Loss: 0.00521735
Iteration 20/1000 | Loss: 0.00780889
Iteration 21/1000 | Loss: 0.00741514
Iteration 22/1000 | Loss: 0.00396192
Iteration 23/1000 | Loss: 0.00671037
Iteration 24/1000 | Loss: 0.00501484
Iteration 25/1000 | Loss: 0.00616391
Iteration 26/1000 | Loss: 0.00371248
Iteration 27/1000 | Loss: 0.00422313
Iteration 28/1000 | Loss: 0.00372531
Iteration 29/1000 | Loss: 0.00482551
Iteration 30/1000 | Loss: 0.00454784
Iteration 31/1000 | Loss: 0.00453005
Iteration 32/1000 | Loss: 0.00827659
Iteration 33/1000 | Loss: 0.00471745
Iteration 34/1000 | Loss: 0.01238089
Iteration 35/1000 | Loss: 0.00908976
Iteration 36/1000 | Loss: 0.00846008
Iteration 37/1000 | Loss: 0.00890894
Iteration 38/1000 | Loss: 0.00976982
Iteration 39/1000 | Loss: 0.00629341
Iteration 40/1000 | Loss: 0.00383305
Iteration 41/1000 | Loss: 0.00449318
Iteration 42/1000 | Loss: 0.00472349
Iteration 43/1000 | Loss: 0.00289137
Iteration 44/1000 | Loss: 0.00278375
Iteration 45/1000 | Loss: 0.00456713
Iteration 46/1000 | Loss: 0.00906534
Iteration 47/1000 | Loss: 0.00284047
Iteration 48/1000 | Loss: 0.00418215
Iteration 49/1000 | Loss: 0.00707705
Iteration 50/1000 | Loss: 0.00461607
Iteration 51/1000 | Loss: 0.00173257
Iteration 52/1000 | Loss: 0.00376190
Iteration 53/1000 | Loss: 0.00269016
Iteration 54/1000 | Loss: 0.00497172
Iteration 55/1000 | Loss: 0.00428053
Iteration 56/1000 | Loss: 0.00587301
Iteration 57/1000 | Loss: 0.00585744
Iteration 58/1000 | Loss: 0.00460624
Iteration 59/1000 | Loss: 0.00356475
Iteration 60/1000 | Loss: 0.00286870
Iteration 61/1000 | Loss: 0.00413472
Iteration 62/1000 | Loss: 0.00536730
Iteration 63/1000 | Loss: 0.00313485
Iteration 64/1000 | Loss: 0.00316020
Iteration 65/1000 | Loss: 0.00312184
Iteration 66/1000 | Loss: 0.00281218
Iteration 67/1000 | Loss: 0.00339442
Iteration 68/1000 | Loss: 0.00360824
Iteration 69/1000 | Loss: 0.00493080
Iteration 70/1000 | Loss: 0.00338079
Iteration 71/1000 | Loss: 0.00376046
Iteration 72/1000 | Loss: 0.00438360
Iteration 73/1000 | Loss: 0.00409471
Iteration 74/1000 | Loss: 0.00276936
Iteration 75/1000 | Loss: 0.00473968
Iteration 76/1000 | Loss: 0.00374994
Iteration 77/1000 | Loss: 0.00357900
Iteration 78/1000 | Loss: 0.00332869
Iteration 79/1000 | Loss: 0.00309347
Iteration 80/1000 | Loss: 0.00323915
Iteration 81/1000 | Loss: 0.00278188
Iteration 82/1000 | Loss: 0.00268528
Iteration 83/1000 | Loss: 0.00268635
Iteration 84/1000 | Loss: 0.00396181
Iteration 85/1000 | Loss: 0.00267160
Iteration 86/1000 | Loss: 0.00311783
Iteration 87/1000 | Loss: 0.00292933
Iteration 88/1000 | Loss: 0.00288368
Iteration 89/1000 | Loss: 0.00260548
Iteration 90/1000 | Loss: 0.00363128
Iteration 91/1000 | Loss: 0.00258252
Iteration 92/1000 | Loss: 0.00292631
Iteration 93/1000 | Loss: 0.00262830
Iteration 94/1000 | Loss: 0.00351532
Iteration 95/1000 | Loss: 0.00282455
Iteration 96/1000 | Loss: 0.00567568
Iteration 97/1000 | Loss: 0.00266866
Iteration 98/1000 | Loss: 0.00206591
Iteration 99/1000 | Loss: 0.00440497
Iteration 100/1000 | Loss: 0.00189907
Iteration 101/1000 | Loss: 0.00155569
Iteration 102/1000 | Loss: 0.00198008
Iteration 103/1000 | Loss: 0.00281484
Iteration 104/1000 | Loss: 0.00328848
Iteration 105/1000 | Loss: 0.00298185
Iteration 106/1000 | Loss: 0.00764865
Iteration 107/1000 | Loss: 0.00753482
Iteration 108/1000 | Loss: 0.00573524
Iteration 109/1000 | Loss: 0.00294150
Iteration 110/1000 | Loss: 0.00535725
Iteration 111/1000 | Loss: 0.00470479
Iteration 112/1000 | Loss: 0.00340194
Iteration 113/1000 | Loss: 0.00584139
Iteration 114/1000 | Loss: 0.00336136
Iteration 115/1000 | Loss: 0.00427277
Iteration 116/1000 | Loss: 0.00274268
Iteration 117/1000 | Loss: 0.00229947
Iteration 118/1000 | Loss: 0.00296386
Iteration 119/1000 | Loss: 0.00557545
Iteration 120/1000 | Loss: 0.00221068
Iteration 121/1000 | Loss: 0.00296992
Iteration 122/1000 | Loss: 0.00280186
Iteration 123/1000 | Loss: 0.00426570
Iteration 124/1000 | Loss: 0.00316692
Iteration 125/1000 | Loss: 0.00310019
Iteration 126/1000 | Loss: 0.00299755
Iteration 127/1000 | Loss: 0.00296330
Iteration 128/1000 | Loss: 0.00309227
Iteration 129/1000 | Loss: 0.00608404
Iteration 130/1000 | Loss: 0.00297739
Iteration 131/1000 | Loss: 0.00425771
Iteration 132/1000 | Loss: 0.00363133
Iteration 133/1000 | Loss: 0.00306177
Iteration 134/1000 | Loss: 0.00312264
Iteration 135/1000 | Loss: 0.00267712
Iteration 136/1000 | Loss: 0.00312227
Iteration 137/1000 | Loss: 0.00248345
Iteration 138/1000 | Loss: 0.00247716
Iteration 139/1000 | Loss: 0.00431270
Iteration 140/1000 | Loss: 0.00352855
Iteration 141/1000 | Loss: 0.00247663
Iteration 142/1000 | Loss: 0.00396023
Iteration 143/1000 | Loss: 0.00237605
Iteration 144/1000 | Loss: 0.00237244
Iteration 145/1000 | Loss: 0.00364701
Iteration 146/1000 | Loss: 0.00231010
Iteration 147/1000 | Loss: 0.00295075
Iteration 148/1000 | Loss: 0.00252367
Iteration 149/1000 | Loss: 0.00266561
Iteration 150/1000 | Loss: 0.00269610
Iteration 151/1000 | Loss: 0.00243879
Iteration 152/1000 | Loss: 0.00244521
Iteration 153/1000 | Loss: 0.00464486
Iteration 154/1000 | Loss: 0.00244004
Iteration 155/1000 | Loss: 0.00285531
Iteration 156/1000 | Loss: 0.00489965
Iteration 157/1000 | Loss: 0.00477730
Iteration 158/1000 | Loss: 0.00413274
Iteration 159/1000 | Loss: 0.00298643
Iteration 160/1000 | Loss: 0.00272009
Iteration 161/1000 | Loss: 0.00340869
Iteration 162/1000 | Loss: 0.00377725
Iteration 163/1000 | Loss: 0.00289531
Iteration 164/1000 | Loss: 0.00314212
Iteration 165/1000 | Loss: 0.00285508
Iteration 166/1000 | Loss: 0.00276783
Iteration 167/1000 | Loss: 0.00273705
Iteration 168/1000 | Loss: 0.00464328
Iteration 169/1000 | Loss: 0.00277611
Iteration 170/1000 | Loss: 0.00311381
Iteration 171/1000 | Loss: 0.00281382
Iteration 172/1000 | Loss: 0.00308228
Iteration 173/1000 | Loss: 0.00342841
Iteration 174/1000 | Loss: 0.00361865
Iteration 175/1000 | Loss: 0.00302635
Iteration 176/1000 | Loss: 0.00400247
Iteration 177/1000 | Loss: 0.00354059
Iteration 178/1000 | Loss: 0.00287777
Iteration 179/1000 | Loss: 0.00296773
Iteration 180/1000 | Loss: 0.00295878
Iteration 181/1000 | Loss: 0.00293031
Iteration 182/1000 | Loss: 0.00262913
Iteration 183/1000 | Loss: 0.00280687
Iteration 184/1000 | Loss: 0.00245166
Iteration 185/1000 | Loss: 0.00427829
Iteration 186/1000 | Loss: 0.00271651
Iteration 187/1000 | Loss: 0.00339433
Iteration 188/1000 | Loss: 0.00498148
Iteration 189/1000 | Loss: 0.00294373
Iteration 190/1000 | Loss: 0.00242236
Iteration 191/1000 | Loss: 0.00265129
Iteration 192/1000 | Loss: 0.00245782
Iteration 193/1000 | Loss: 0.00266575
Iteration 194/1000 | Loss: 0.00260933
Iteration 195/1000 | Loss: 0.00245217
Iteration 196/1000 | Loss: 0.00413788
Iteration 197/1000 | Loss: 0.00384540
Iteration 198/1000 | Loss: 0.00269312
Iteration 199/1000 | Loss: 0.00270962
Iteration 200/1000 | Loss: 0.00307960
Iteration 201/1000 | Loss: 0.00324847
Iteration 202/1000 | Loss: 0.00276334
Iteration 203/1000 | Loss: 0.00259165
Iteration 204/1000 | Loss: 0.00288025
Iteration 205/1000 | Loss: 0.00264851
Iteration 206/1000 | Loss: 0.00276286
Iteration 207/1000 | Loss: 0.00411348
Iteration 208/1000 | Loss: 0.00290658
Iteration 209/1000 | Loss: 0.00538036
Iteration 210/1000 | Loss: 0.00731318
Iteration 211/1000 | Loss: 0.00360354
Iteration 212/1000 | Loss: 0.00646712
Iteration 213/1000 | Loss: 0.00268432
Iteration 214/1000 | Loss: 0.00287221
Iteration 215/1000 | Loss: 0.00263189
Iteration 216/1000 | Loss: 0.00343185
Iteration 217/1000 | Loss: 0.00278007
Iteration 218/1000 | Loss: 0.00387085
Iteration 219/1000 | Loss: 0.00382048
Iteration 220/1000 | Loss: 0.00198436
Iteration 221/1000 | Loss: 0.00253099
Iteration 222/1000 | Loss: 0.00282840
Iteration 223/1000 | Loss: 0.00289898
Iteration 224/1000 | Loss: 0.00256744
Iteration 225/1000 | Loss: 0.00277302
Iteration 226/1000 | Loss: 0.00381964
Iteration 227/1000 | Loss: 0.00278173
Iteration 228/1000 | Loss: 0.00256652
Iteration 229/1000 | Loss: 0.00452981
Iteration 230/1000 | Loss: 0.00286431
Iteration 231/1000 | Loss: 0.00344830
Iteration 232/1000 | Loss: 0.00253218
Iteration 233/1000 | Loss: 0.00275523
Iteration 234/1000 | Loss: 0.00274451
Iteration 235/1000 | Loss: 0.00259337
Iteration 236/1000 | Loss: 0.00258917
Iteration 237/1000 | Loss: 0.00279679
Iteration 238/1000 | Loss: 0.00237313
Iteration 239/1000 | Loss: 0.00376179
Iteration 240/1000 | Loss: 0.00540657
Iteration 241/1000 | Loss: 0.00462250
Iteration 242/1000 | Loss: 0.00292345
Iteration 243/1000 | Loss: 0.00339872
Iteration 244/1000 | Loss: 0.00255269
Iteration 245/1000 | Loss: 0.00298239
Iteration 246/1000 | Loss: 0.00294596
Iteration 247/1000 | Loss: 0.00252026
Iteration 248/1000 | Loss: 0.00374260
Iteration 249/1000 | Loss: 0.00274731
Iteration 250/1000 | Loss: 0.00257943
Iteration 251/1000 | Loss: 0.00331745
Iteration 252/1000 | Loss: 0.00380252
Iteration 253/1000 | Loss: 0.00369650
Iteration 254/1000 | Loss: 0.00361975
Iteration 255/1000 | Loss: 0.00493991
Iteration 256/1000 | Loss: 0.00417361
Iteration 257/1000 | Loss: 0.00513356
Iteration 258/1000 | Loss: 0.00590837
Iteration 259/1000 | Loss: 0.00338123
Iteration 260/1000 | Loss: 0.00265867
Iteration 261/1000 | Loss: 0.00190328
Iteration 262/1000 | Loss: 0.00243462
Iteration 263/1000 | Loss: 0.00574209
Iteration 264/1000 | Loss: 0.00193521
Iteration 265/1000 | Loss: 0.00148483
Iteration 266/1000 | Loss: 0.00305085
Iteration 267/1000 | Loss: 0.00296479
Iteration 268/1000 | Loss: 0.00318696
Iteration 269/1000 | Loss: 0.00329285
Iteration 270/1000 | Loss: 0.00564059
Iteration 271/1000 | Loss: 0.00245094
Iteration 272/1000 | Loss: 0.00222591
Iteration 273/1000 | Loss: 0.00272073
Iteration 274/1000 | Loss: 0.00386566
Iteration 275/1000 | Loss: 0.00348232
Iteration 276/1000 | Loss: 0.00273079
Iteration 277/1000 | Loss: 0.00283779
Iteration 278/1000 | Loss: 0.00200413
Iteration 279/1000 | Loss: 0.00195488
Iteration 280/1000 | Loss: 0.00252769
Iteration 281/1000 | Loss: 0.00185787
Iteration 282/1000 | Loss: 0.00195887
Iteration 283/1000 | Loss: 0.00196705
Iteration 284/1000 | Loss: 0.00308279
Iteration 285/1000 | Loss: 0.00181033
Iteration 286/1000 | Loss: 0.00216829
Iteration 287/1000 | Loss: 0.00303909
Iteration 288/1000 | Loss: 0.00304522
Iteration 289/1000 | Loss: 0.00229042
Iteration 290/1000 | Loss: 0.00208509
Iteration 291/1000 | Loss: 0.00226402
Iteration 292/1000 | Loss: 0.00194801
Iteration 293/1000 | Loss: 0.00203090
Iteration 294/1000 | Loss: 0.00206217
Iteration 295/1000 | Loss: 0.00125315
Iteration 296/1000 | Loss: 0.00188242
Iteration 297/1000 | Loss: 0.00151994
Iteration 298/1000 | Loss: 0.00215911
Iteration 299/1000 | Loss: 0.00211725
Iteration 300/1000 | Loss: 0.00110221
Iteration 301/1000 | Loss: 0.00145728
Iteration 302/1000 | Loss: 0.00099361
Iteration 303/1000 | Loss: 0.00116992
Iteration 304/1000 | Loss: 0.00160771
Iteration 305/1000 | Loss: 0.00118143
Iteration 306/1000 | Loss: 0.00121878
Iteration 307/1000 | Loss: 0.00124537
Iteration 308/1000 | Loss: 0.00125560
Iteration 309/1000 | Loss: 0.00124733
Iteration 310/1000 | Loss: 0.00107953
Iteration 311/1000 | Loss: 0.00142610
Iteration 312/1000 | Loss: 0.00100401
Iteration 313/1000 | Loss: 0.00121075
Iteration 314/1000 | Loss: 0.00118011
Iteration 315/1000 | Loss: 0.00246480
Iteration 316/1000 | Loss: 0.00215353
Iteration 317/1000 | Loss: 0.00221225
Iteration 318/1000 | Loss: 0.00092216
Iteration 319/1000 | Loss: 0.00357286
Iteration 320/1000 | Loss: 0.00108104
Iteration 321/1000 | Loss: 0.00092359
Iteration 322/1000 | Loss: 0.00131774
Iteration 323/1000 | Loss: 0.00128757
Iteration 324/1000 | Loss: 0.00150202
Iteration 325/1000 | Loss: 0.00131989
Iteration 326/1000 | Loss: 0.00150530
Iteration 327/1000 | Loss: 0.00136184
Iteration 328/1000 | Loss: 0.00133575
Iteration 329/1000 | Loss: 0.00131894
Iteration 330/1000 | Loss: 0.00122911
Iteration 331/1000 | Loss: 0.00124426
Iteration 332/1000 | Loss: 0.00136347
Iteration 333/1000 | Loss: 0.00134630
Iteration 334/1000 | Loss: 0.00127845
Iteration 335/1000 | Loss: 0.00133958
Iteration 336/1000 | Loss: 0.00133544
Iteration 337/1000 | Loss: 0.00137147
Iteration 338/1000 | Loss: 0.00113498
Iteration 339/1000 | Loss: 0.00123191
Iteration 340/1000 | Loss: 0.00188467
Iteration 341/1000 | Loss: 0.00219254
Iteration 342/1000 | Loss: 0.00131629
Iteration 343/1000 | Loss: 0.00211102
Iteration 344/1000 | Loss: 0.00131455
Iteration 345/1000 | Loss: 0.00130129
Iteration 346/1000 | Loss: 0.00129907
Iteration 347/1000 | Loss: 0.00140870
Iteration 348/1000 | Loss: 0.00158776
Iteration 349/1000 | Loss: 0.00172168
Iteration 350/1000 | Loss: 0.00140269
Iteration 351/1000 | Loss: 0.00202027
Iteration 352/1000 | Loss: 0.00154828
Iteration 353/1000 | Loss: 0.00142299
Iteration 354/1000 | Loss: 0.00129507
Iteration 355/1000 | Loss: 0.00202160
Iteration 356/1000 | Loss: 0.00151358
Iteration 357/1000 | Loss: 0.00158787
Iteration 358/1000 | Loss: 0.00133015
Iteration 359/1000 | Loss: 0.00122710
Iteration 360/1000 | Loss: 0.00136228
Iteration 361/1000 | Loss: 0.00333190
Iteration 362/1000 | Loss: 0.00128144
Iteration 363/1000 | Loss: 0.00145069
Iteration 364/1000 | Loss: 0.00165003
Iteration 365/1000 | Loss: 0.00127376
Iteration 366/1000 | Loss: 0.00122166
Iteration 367/1000 | Loss: 0.00122286
Iteration 368/1000 | Loss: 0.00172481
Iteration 369/1000 | Loss: 0.00136941
Iteration 370/1000 | Loss: 0.00154891
Iteration 371/1000 | Loss: 0.00149289
Iteration 372/1000 | Loss: 0.00219524
Iteration 373/1000 | Loss: 0.00153121
Iteration 374/1000 | Loss: 0.00138462
Iteration 375/1000 | Loss: 0.00135246
Iteration 376/1000 | Loss: 0.00202874
Iteration 377/1000 | Loss: 0.00119504
Iteration 378/1000 | Loss: 0.00177145
Iteration 379/1000 | Loss: 0.00162105
Iteration 380/1000 | Loss: 0.00172227
Iteration 381/1000 | Loss: 0.00226857
Iteration 382/1000 | Loss: 0.00112323
Iteration 383/1000 | Loss: 0.00127690
Iteration 384/1000 | Loss: 0.00203759
Iteration 385/1000 | Loss: 0.00148423
Iteration 386/1000 | Loss: 0.00190296
Iteration 387/1000 | Loss: 0.00163489
Iteration 388/1000 | Loss: 0.00202906
Iteration 389/1000 | Loss: 0.00213729
Iteration 390/1000 | Loss: 0.00546399
Iteration 391/1000 | Loss: 0.00172088
Iteration 392/1000 | Loss: 0.00113200
Iteration 393/1000 | Loss: 0.00127384
Iteration 394/1000 | Loss: 0.00262099
Iteration 395/1000 | Loss: 0.00320665
Iteration 396/1000 | Loss: 0.00325498
Iteration 397/1000 | Loss: 0.00151200
Iteration 398/1000 | Loss: 0.00129837
Iteration 399/1000 | Loss: 0.00117691
Iteration 400/1000 | Loss: 0.00129314
Iteration 401/1000 | Loss: 0.00130032
Iteration 402/1000 | Loss: 0.00131396
Iteration 403/1000 | Loss: 0.00128189
Iteration 404/1000 | Loss: 0.00121116
Iteration 405/1000 | Loss: 0.00165278
Iteration 406/1000 | Loss: 0.00160864
Iteration 407/1000 | Loss: 0.00173769
Iteration 408/1000 | Loss: 0.00105727
Iteration 409/1000 | Loss: 0.00117855
Iteration 410/1000 | Loss: 0.00109060
Iteration 411/1000 | Loss: 0.00107478
Iteration 412/1000 | Loss: 0.00114330
Iteration 413/1000 | Loss: 0.00152776
Iteration 414/1000 | Loss: 0.00120504
Iteration 415/1000 | Loss: 0.00243588
Iteration 416/1000 | Loss: 0.00136200
Iteration 417/1000 | Loss: 0.00121266
Iteration 418/1000 | Loss: 0.00118268
Iteration 419/1000 | Loss: 0.00123904
Iteration 420/1000 | Loss: 0.00115716
Iteration 421/1000 | Loss: 0.00161146
Iteration 422/1000 | Loss: 0.00101028
Iteration 423/1000 | Loss: 0.00148021
Iteration 424/1000 | Loss: 0.00126136
Iteration 425/1000 | Loss: 0.00110354
Iteration 426/1000 | Loss: 0.00099246
Iteration 427/1000 | Loss: 0.00141107
Iteration 428/1000 | Loss: 0.00125631
Iteration 429/1000 | Loss: 0.00133067
Iteration 430/1000 | Loss: 0.00144099
Iteration 431/1000 | Loss: 0.00129765
Iteration 432/1000 | Loss: 0.00122513
Iteration 433/1000 | Loss: 0.00134603
Iteration 434/1000 | Loss: 0.00151250
Iteration 435/1000 | Loss: 0.00188931
Iteration 436/1000 | Loss: 0.00146596
Iteration 437/1000 | Loss: 0.00250664
Iteration 438/1000 | Loss: 0.00149060
Iteration 439/1000 | Loss: 0.00145453
Iteration 440/1000 | Loss: 0.00162244
Iteration 441/1000 | Loss: 0.00158040
Iteration 442/1000 | Loss: 0.00218026
Iteration 443/1000 | Loss: 0.00161072
Iteration 444/1000 | Loss: 0.00183336
Iteration 445/1000 | Loss: 0.00166262
Iteration 446/1000 | Loss: 0.00167698
Iteration 447/1000 | Loss: 0.00139179
Iteration 448/1000 | Loss: 0.00184054
Iteration 449/1000 | Loss: 0.00145813
Iteration 450/1000 | Loss: 0.00144133
Iteration 451/1000 | Loss: 0.00150401
Iteration 452/1000 | Loss: 0.00160921
Iteration 453/1000 | Loss: 0.00139808
Iteration 454/1000 | Loss: 0.00143024
Iteration 455/1000 | Loss: 0.00210091
Iteration 456/1000 | Loss: 0.00125164
Iteration 457/1000 | Loss: 0.00105534
Iteration 458/1000 | Loss: 0.00109349
Iteration 459/1000 | Loss: 0.00122749
Iteration 460/1000 | Loss: 0.00243313
Iteration 461/1000 | Loss: 0.00341028
Iteration 462/1000 | Loss: 0.00284447
Iteration 463/1000 | Loss: 0.00355223
Iteration 464/1000 | Loss: 0.00124423
Iteration 465/1000 | Loss: 0.00155037
Iteration 466/1000 | Loss: 0.00117735
Iteration 467/1000 | Loss: 0.00098497
Iteration 468/1000 | Loss: 0.00121494
Iteration 469/1000 | Loss: 0.00318846
Iteration 470/1000 | Loss: 0.00210996
Iteration 471/1000 | Loss: 0.00157524
Iteration 472/1000 | Loss: 0.00132626
Iteration 473/1000 | Loss: 0.00096422
Iteration 474/1000 | Loss: 0.00126332
Iteration 475/1000 | Loss: 0.00117268
Iteration 476/1000 | Loss: 0.00172606
Iteration 477/1000 | Loss: 0.00096120
Iteration 478/1000 | Loss: 0.00135396
Iteration 479/1000 | Loss: 0.00094709
Iteration 480/1000 | Loss: 0.00142758
Iteration 481/1000 | Loss: 0.00135271
Iteration 482/1000 | Loss: 0.00145564
Iteration 483/1000 | Loss: 0.00193179
Iteration 484/1000 | Loss: 0.00111149
Iteration 485/1000 | Loss: 0.00102302
Iteration 486/1000 | Loss: 0.00132995
Iteration 487/1000 | Loss: 0.00084131
Iteration 488/1000 | Loss: 0.00158843
Iteration 489/1000 | Loss: 0.00181163
Iteration 490/1000 | Loss: 0.00129074
Iteration 491/1000 | Loss: 0.00304606
Iteration 492/1000 | Loss: 0.00141416
Iteration 493/1000 | Loss: 0.00096574
Iteration 494/1000 | Loss: 0.00140161
Iteration 495/1000 | Loss: 0.00107711
Iteration 496/1000 | Loss: 0.00116304
Iteration 497/1000 | Loss: 0.00109970
Iteration 498/1000 | Loss: 0.00115142
Iteration 499/1000 | Loss: 0.00091755
Iteration 500/1000 | Loss: 0.00092954
Iteration 501/1000 | Loss: 0.00110176
Iteration 502/1000 | Loss: 0.00141346
Iteration 503/1000 | Loss: 0.00158706
Iteration 504/1000 | Loss: 0.00123048
Iteration 505/1000 | Loss: 0.00112418
Iteration 506/1000 | Loss: 0.00238895
Iteration 507/1000 | Loss: 0.00114571
Iteration 508/1000 | Loss: 0.00103912
Iteration 509/1000 | Loss: 0.00120780
Iteration 510/1000 | Loss: 0.00095232
Iteration 511/1000 | Loss: 0.00101552
Iteration 512/1000 | Loss: 0.00129940
Iteration 513/1000 | Loss: 0.00141212
Iteration 514/1000 | Loss: 0.00133642
Iteration 515/1000 | Loss: 0.00147303
Iteration 516/1000 | Loss: 0.00222160
Iteration 517/1000 | Loss: 0.00101781
Iteration 518/1000 | Loss: 0.00120422
Iteration 519/1000 | Loss: 0.00097573
Iteration 520/1000 | Loss: 0.00102059
Iteration 521/1000 | Loss: 0.00136450
Iteration 522/1000 | Loss: 0.00151962
Iteration 523/1000 | Loss: 0.00166079
Iteration 524/1000 | Loss: 0.00129580
Iteration 525/1000 | Loss: 0.00104352
Iteration 526/1000 | Loss: 0.00100269
Iteration 527/1000 | Loss: 0.00115836
Iteration 528/1000 | Loss: 0.00101566
Iteration 529/1000 | Loss: 0.00138032
Iteration 530/1000 | Loss: 0.00106198
Iteration 531/1000 | Loss: 0.00196047
Iteration 532/1000 | Loss: 0.00109832
Iteration 533/1000 | Loss: 0.00115925
Iteration 534/1000 | Loss: 0.00105173
Iteration 535/1000 | Loss: 0.00092867
Iteration 536/1000 | Loss: 0.00132113
Iteration 537/1000 | Loss: 0.00090908
Iteration 538/1000 | Loss: 0.00093397
Iteration 539/1000 | Loss: 0.00112403
Iteration 540/1000 | Loss: 0.00102172
Iteration 541/1000 | Loss: 0.00110388
Iteration 542/1000 | Loss: 0.00098880
Iteration 543/1000 | Loss: 0.00118265
Iteration 544/1000 | Loss: 0.00094347
Iteration 545/1000 | Loss: 0.00121265
Iteration 546/1000 | Loss: 0.00099005
Iteration 547/1000 | Loss: 0.00102923
Iteration 548/1000 | Loss: 0.00097075
Iteration 549/1000 | Loss: 0.00092154
Iteration 550/1000 | Loss: 0.00183392
Iteration 551/1000 | Loss: 0.00132048
Iteration 552/1000 | Loss: 0.00125850
Iteration 553/1000 | Loss: 0.00113711
Iteration 554/1000 | Loss: 0.00185021
Iteration 555/1000 | Loss: 0.00112691
Iteration 556/1000 | Loss: 0.00112001
Iteration 557/1000 | Loss: 0.00113456
Iteration 558/1000 | Loss: 0.00103327
Iteration 559/1000 | Loss: 0.00272221
Iteration 560/1000 | Loss: 0.00096497
Iteration 561/1000 | Loss: 0.00097632
Iteration 562/1000 | Loss: 0.00092757
Iteration 563/1000 | Loss: 0.00113011
Iteration 564/1000 | Loss: 0.00109394
Iteration 565/1000 | Loss: 0.00095686
Iteration 566/1000 | Loss: 0.00105288
Iteration 567/1000 | Loss: 0.00093540
Iteration 568/1000 | Loss: 0.00099337
Iteration 569/1000 | Loss: 0.00124777
Iteration 570/1000 | Loss: 0.00105918
Iteration 571/1000 | Loss: 0.00090926
Iteration 572/1000 | Loss: 0.00120345
Iteration 573/1000 | Loss: 0.00104662
Iteration 574/1000 | Loss: 0.00106545
Iteration 575/1000 | Loss: 0.00090258
Iteration 576/1000 | Loss: 0.00089585
Iteration 577/1000 | Loss: 0.00084070
Iteration 578/1000 | Loss: 0.00089717
Iteration 579/1000 | Loss: 0.00098885
Iteration 580/1000 | Loss: 0.00097656
Iteration 581/1000 | Loss: 0.00099015
Iteration 582/1000 | Loss: 0.00095119
Iteration 583/1000 | Loss: 0.00105389
Iteration 584/1000 | Loss: 0.00134759
Iteration 585/1000 | Loss: 0.00092289
Iteration 586/1000 | Loss: 0.00094739
Iteration 587/1000 | Loss: 0.00122130
Iteration 588/1000 | Loss: 0.00108484
Iteration 589/1000 | Loss: 0.00093183
Iteration 590/1000 | Loss: 0.00108187
Iteration 591/1000 | Loss: 0.00090808
Iteration 592/1000 | Loss: 0.00109397
Iteration 593/1000 | Loss: 0.00123898
Iteration 594/1000 | Loss: 0.00101766
Iteration 595/1000 | Loss: 0.00134375
Iteration 596/1000 | Loss: 0.00101811
Iteration 597/1000 | Loss: 0.00144470
Iteration 598/1000 | Loss: 0.00107880
Iteration 599/1000 | Loss: 0.00091020
Iteration 600/1000 | Loss: 0.00084371
Iteration 601/1000 | Loss: 0.00135391
Iteration 602/1000 | Loss: 0.00149288
Iteration 603/1000 | Loss: 0.00152836
Iteration 604/1000 | Loss: 0.00164813
Iteration 605/1000 | Loss: 0.00037999
Iteration 606/1000 | Loss: 0.00094708
Iteration 607/1000 | Loss: 0.00038511
Iteration 608/1000 | Loss: 0.00102819
Iteration 609/1000 | Loss: 0.00094665
Iteration 610/1000 | Loss: 0.00048144
Iteration 611/1000 | Loss: 0.00065200
Iteration 612/1000 | Loss: 0.00108201
Iteration 613/1000 | Loss: 0.00018521
Iteration 614/1000 | Loss: 0.00032201
Iteration 615/1000 | Loss: 0.00015194
Iteration 616/1000 | Loss: 0.00068701
Iteration 617/1000 | Loss: 0.00034974
Iteration 618/1000 | Loss: 0.00064635
Iteration 619/1000 | Loss: 0.00054667
Iteration 620/1000 | Loss: 0.00071875
Iteration 621/1000 | Loss: 0.00064936
Iteration 622/1000 | Loss: 0.00033766
Iteration 623/1000 | Loss: 0.00107066
Iteration 624/1000 | Loss: 0.00200877
Iteration 625/1000 | Loss: 0.00012512
Iteration 626/1000 | Loss: 0.00014224
Iteration 627/1000 | Loss: 0.00008458
Iteration 628/1000 | Loss: 0.00017572
Iteration 629/1000 | Loss: 0.00043900
Iteration 630/1000 | Loss: 0.00052235
Iteration 631/1000 | Loss: 0.00033137
Iteration 632/1000 | Loss: 0.00006565
Iteration 633/1000 | Loss: 0.00063496
Iteration 634/1000 | Loss: 0.00008475
Iteration 635/1000 | Loss: 0.00006715
Iteration 636/1000 | Loss: 0.00005608
Iteration 637/1000 | Loss: 0.00004458
Iteration 638/1000 | Loss: 0.00050512
Iteration 639/1000 | Loss: 0.00078826
Iteration 640/1000 | Loss: 0.00020697
Iteration 641/1000 | Loss: 0.00013141
Iteration 642/1000 | Loss: 0.00006197
Iteration 643/1000 | Loss: 0.00004256
Iteration 644/1000 | Loss: 0.00004265
Iteration 645/1000 | Loss: 0.00003753
Iteration 646/1000 | Loss: 0.00005314
Iteration 647/1000 | Loss: 0.00019452
Iteration 648/1000 | Loss: 0.00033696
Iteration 649/1000 | Loss: 0.00013688
Iteration 650/1000 | Loss: 0.00012061
Iteration 651/1000 | Loss: 0.00003620
Iteration 652/1000 | Loss: 0.00007027
Iteration 653/1000 | Loss: 0.00004681
Iteration 654/1000 | Loss: 0.00003538
Iteration 655/1000 | Loss: 0.00004506
Iteration 656/1000 | Loss: 0.00003503
Iteration 657/1000 | Loss: 0.00003468
Iteration 658/1000 | Loss: 0.00094635
Iteration 659/1000 | Loss: 0.00132560
Iteration 660/1000 | Loss: 0.00006535
Iteration 661/1000 | Loss: 0.00012890
Iteration 662/1000 | Loss: 0.00003211
Iteration 663/1000 | Loss: 0.00003097
Iteration 664/1000 | Loss: 0.00003037
Iteration 665/1000 | Loss: 0.00002987
Iteration 666/1000 | Loss: 0.00002980
Iteration 667/1000 | Loss: 0.00020619
Iteration 668/1000 | Loss: 0.00004092
Iteration 669/1000 | Loss: 0.00002958
Iteration 670/1000 | Loss: 0.00002954
Iteration 671/1000 | Loss: 0.00002949
Iteration 672/1000 | Loss: 0.00002947
Iteration 673/1000 | Loss: 0.00002947
Iteration 674/1000 | Loss: 0.00002946
Iteration 675/1000 | Loss: 0.00002933
Iteration 676/1000 | Loss: 0.00002943
Iteration 677/1000 | Loss: 0.00002936
Iteration 678/1000 | Loss: 0.00002928
Iteration 679/1000 | Loss: 0.00002928
Iteration 680/1000 | Loss: 0.00002911
Iteration 681/1000 | Loss: 0.00002910
Iteration 682/1000 | Loss: 0.00002908
Iteration 683/1000 | Loss: 0.00002906
Iteration 684/1000 | Loss: 0.00002905
Iteration 685/1000 | Loss: 0.00002904
Iteration 686/1000 | Loss: 0.00002904
Iteration 687/1000 | Loss: 0.00002904
Iteration 688/1000 | Loss: 0.00002904
Iteration 689/1000 | Loss: 0.00002904
Iteration 690/1000 | Loss: 0.00002903
Iteration 691/1000 | Loss: 0.00002903
Iteration 692/1000 | Loss: 0.00002903
Iteration 693/1000 | Loss: 0.00002903
Iteration 694/1000 | Loss: 0.00002903
Iteration 695/1000 | Loss: 0.00002903
Iteration 696/1000 | Loss: 0.00002903
Iteration 697/1000 | Loss: 0.00002903
Iteration 698/1000 | Loss: 0.00002903
Iteration 699/1000 | Loss: 0.00002903
Iteration 700/1000 | Loss: 0.00002903
Iteration 701/1000 | Loss: 0.00002903
Iteration 702/1000 | Loss: 0.00002902
Iteration 703/1000 | Loss: 0.00002902
Iteration 704/1000 | Loss: 0.00002902
Iteration 705/1000 | Loss: 0.00002902
Iteration 706/1000 | Loss: 0.00002902
Iteration 707/1000 | Loss: 0.00002902
Iteration 708/1000 | Loss: 0.00002902
Iteration 709/1000 | Loss: 0.00002902
Iteration 710/1000 | Loss: 0.00002902
Iteration 711/1000 | Loss: 0.00002902
Iteration 712/1000 | Loss: 0.00002902
Iteration 713/1000 | Loss: 0.00002902
Iteration 714/1000 | Loss: 0.00002901
Iteration 715/1000 | Loss: 0.00002901
Iteration 716/1000 | Loss: 0.00002901
Iteration 717/1000 | Loss: 0.00002900
Iteration 718/1000 | Loss: 0.00002900
Iteration 719/1000 | Loss: 0.00002900
Iteration 720/1000 | Loss: 0.00002900
Iteration 721/1000 | Loss: 0.00002900
Iteration 722/1000 | Loss: 0.00002900
Iteration 723/1000 | Loss: 0.00002900
Iteration 724/1000 | Loss: 0.00002900
Iteration 725/1000 | Loss: 0.00002900
Iteration 726/1000 | Loss: 0.00002900
Iteration 727/1000 | Loss: 0.00002900
Iteration 728/1000 | Loss: 0.00002899
Iteration 729/1000 | Loss: 0.00002899
Iteration 730/1000 | Loss: 0.00002899
Iteration 731/1000 | Loss: 0.00002899
Iteration 732/1000 | Loss: 0.00002899
Iteration 733/1000 | Loss: 0.00002899
Iteration 734/1000 | Loss: 0.00002899
Iteration 735/1000 | Loss: 0.00002899
Iteration 736/1000 | Loss: 0.00002899
Iteration 737/1000 | Loss: 0.00002899
Iteration 738/1000 | Loss: 0.00002898
Iteration 739/1000 | Loss: 0.00002898
Iteration 740/1000 | Loss: 0.00002898
Iteration 741/1000 | Loss: 0.00002898
Iteration 742/1000 | Loss: 0.00002898
Iteration 743/1000 | Loss: 0.00002897
Iteration 744/1000 | Loss: 0.00002911
Iteration 745/1000 | Loss: 0.00002911
Iteration 746/1000 | Loss: 0.00002900
Iteration 747/1000 | Loss: 0.00002900
Iteration 748/1000 | Loss: 0.00002903
Iteration 749/1000 | Loss: 0.00002902
Iteration 750/1000 | Loss: 0.00002901
Iteration 751/1000 | Loss: 0.00002892
Iteration 752/1000 | Loss: 0.00002891
Iteration 753/1000 | Loss: 0.00002891
Iteration 754/1000 | Loss: 0.00002890
Iteration 755/1000 | Loss: 0.00002887
Iteration 756/1000 | Loss: 0.00002887
Iteration 757/1000 | Loss: 0.00002887
Iteration 758/1000 | Loss: 0.00002887
Iteration 759/1000 | Loss: 0.00002887
Iteration 760/1000 | Loss: 0.00002887
Iteration 761/1000 | Loss: 0.00002887
Iteration 762/1000 | Loss: 0.00002887
Iteration 763/1000 | Loss: 0.00002887
Iteration 764/1000 | Loss: 0.00002886
Iteration 765/1000 | Loss: 0.00002886
Iteration 766/1000 | Loss: 0.00002886
Iteration 767/1000 | Loss: 0.00002886
Iteration 768/1000 | Loss: 0.00002886
Iteration 769/1000 | Loss: 0.00002886
Iteration 770/1000 | Loss: 0.00002886
Iteration 771/1000 | Loss: 0.00002885
Iteration 772/1000 | Loss: 0.00002885
Iteration 773/1000 | Loss: 0.00002885
Iteration 774/1000 | Loss: 0.00002885
Iteration 775/1000 | Loss: 0.00002885
Iteration 776/1000 | Loss: 0.00002885
Iteration 777/1000 | Loss: 0.00002884
Iteration 778/1000 | Loss: 0.00002884
Iteration 779/1000 | Loss: 0.00002884
Iteration 780/1000 | Loss: 0.00002884
Iteration 781/1000 | Loss: 0.00002883
Iteration 782/1000 | Loss: 0.00002883
Iteration 783/1000 | Loss: 0.00002883
Iteration 784/1000 | Loss: 0.00002883
Iteration 785/1000 | Loss: 0.00002883
Iteration 786/1000 | Loss: 0.00002883
Iteration 787/1000 | Loss: 0.00002883
Iteration 788/1000 | Loss: 0.00002883
Iteration 789/1000 | Loss: 0.00002883
Iteration 790/1000 | Loss: 0.00002883
Iteration 791/1000 | Loss: 0.00002882
Iteration 792/1000 | Loss: 0.00002882
Iteration 793/1000 | Loss: 0.00002882
Iteration 794/1000 | Loss: 0.00002882
Iteration 795/1000 | Loss: 0.00002881
Iteration 796/1000 | Loss: 0.00002881
Iteration 797/1000 | Loss: 0.00002881
Iteration 798/1000 | Loss: 0.00002909
Iteration 799/1000 | Loss: 0.00002909
Iteration 800/1000 | Loss: 0.00002909
Iteration 801/1000 | Loss: 0.00002909
Iteration 802/1000 | Loss: 0.00002909
Iteration 803/1000 | Loss: 0.00002909
Iteration 804/1000 | Loss: 0.00002909
Iteration 805/1000 | Loss: 0.00002909
Iteration 806/1000 | Loss: 0.00002909
Iteration 807/1000 | Loss: 0.00002909
Iteration 808/1000 | Loss: 0.00002909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 808. Stopping optimization.
Last 5 losses: [2.9085995265631936e-05, 2.9085995265631936e-05, 2.9085995265631936e-05, 2.9085995265631936e-05, 2.9085995265631936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9085995265631936e-05

Optimization complete. Final v2v error: 3.9618136882781982 mm

Highest mean error: 22.59090805053711 mm for frame 157

Lowest mean error: 2.9323809146881104 mm for frame 117

Saving results

Total time: 1129.4747200012207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715566
Iteration 2/25 | Loss: 0.00113020
Iteration 3/25 | Loss: 0.00081823
Iteration 4/25 | Loss: 0.00076632
Iteration 5/25 | Loss: 0.00075359
Iteration 6/25 | Loss: 0.00075051
Iteration 7/25 | Loss: 0.00073163
Iteration 8/25 | Loss: 0.00072776
Iteration 9/25 | Loss: 0.00072518
Iteration 10/25 | Loss: 0.00072481
Iteration 11/25 | Loss: 0.00072473
Iteration 12/25 | Loss: 0.00072473
Iteration 13/25 | Loss: 0.00072472
Iteration 14/25 | Loss: 0.00072472
Iteration 15/25 | Loss: 0.00072472
Iteration 16/25 | Loss: 0.00072472
Iteration 17/25 | Loss: 0.00072472
Iteration 18/25 | Loss: 0.00072472
Iteration 19/25 | Loss: 0.00072472
Iteration 20/25 | Loss: 0.00072472
Iteration 21/25 | Loss: 0.00072472
Iteration 22/25 | Loss: 0.00072472
Iteration 23/25 | Loss: 0.00072471
Iteration 24/25 | Loss: 0.00072471
Iteration 25/25 | Loss: 0.00072471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39426196
Iteration 2/25 | Loss: 0.00080676
Iteration 3/25 | Loss: 0.00080675
Iteration 4/25 | Loss: 0.00080675
Iteration 5/25 | Loss: 0.00080675
Iteration 6/25 | Loss: 0.00080675
Iteration 7/25 | Loss: 0.00080675
Iteration 8/25 | Loss: 0.00080675
Iteration 9/25 | Loss: 0.00080675
Iteration 10/25 | Loss: 0.00080675
Iteration 11/25 | Loss: 0.00080675
Iteration 12/25 | Loss: 0.00080675
Iteration 13/25 | Loss: 0.00080675
Iteration 14/25 | Loss: 0.00080675
Iteration 15/25 | Loss: 0.00080675
Iteration 16/25 | Loss: 0.00080675
Iteration 17/25 | Loss: 0.00080675
Iteration 18/25 | Loss: 0.00080675
Iteration 19/25 | Loss: 0.00080675
Iteration 20/25 | Loss: 0.00080675
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008067517774179578, 0.0008067517774179578, 0.0008067517774179578, 0.0008067517774179578, 0.0008067517774179578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008067517774179578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080675
Iteration 2/1000 | Loss: 0.00004246
Iteration 3/1000 | Loss: 0.00003078
Iteration 4/1000 | Loss: 0.00002497
Iteration 5/1000 | Loss: 0.00002307
Iteration 6/1000 | Loss: 0.00002220
Iteration 7/1000 | Loss: 0.00002174
Iteration 8/1000 | Loss: 0.00002120
Iteration 9/1000 | Loss: 0.00002077
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001997
Iteration 18/1000 | Loss: 0.00001997
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001991
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001988
Iteration 26/1000 | Loss: 0.00001987
Iteration 27/1000 | Loss: 0.00001987
Iteration 28/1000 | Loss: 0.00001986
Iteration 29/1000 | Loss: 0.00001986
Iteration 30/1000 | Loss: 0.00001985
Iteration 31/1000 | Loss: 0.00001985
Iteration 32/1000 | Loss: 0.00001984
Iteration 33/1000 | Loss: 0.00001984
Iteration 34/1000 | Loss: 0.00001982
Iteration 35/1000 | Loss: 0.00001982
Iteration 36/1000 | Loss: 0.00001981
Iteration 37/1000 | Loss: 0.00001981
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001978
Iteration 40/1000 | Loss: 0.00001977
Iteration 41/1000 | Loss: 0.00001976
Iteration 42/1000 | Loss: 0.00001976
Iteration 43/1000 | Loss: 0.00001975
Iteration 44/1000 | Loss: 0.00001975
Iteration 45/1000 | Loss: 0.00001975
Iteration 46/1000 | Loss: 0.00001974
Iteration 47/1000 | Loss: 0.00001974
Iteration 48/1000 | Loss: 0.00001974
Iteration 49/1000 | Loss: 0.00001974
Iteration 50/1000 | Loss: 0.00001973
Iteration 51/1000 | Loss: 0.00001973
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001971
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001971
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001970
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001969
Iteration 65/1000 | Loss: 0.00001968
Iteration 66/1000 | Loss: 0.00001968
Iteration 67/1000 | Loss: 0.00001968
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001968
Iteration 72/1000 | Loss: 0.00001968
Iteration 73/1000 | Loss: 0.00001967
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001965
Iteration 80/1000 | Loss: 0.00001965
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001959
Iteration 107/1000 | Loss: 0.00001958
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001957
Iteration 112/1000 | Loss: 0.00001957
Iteration 113/1000 | Loss: 0.00001957
Iteration 114/1000 | Loss: 0.00001957
Iteration 115/1000 | Loss: 0.00001956
Iteration 116/1000 | Loss: 0.00001956
Iteration 117/1000 | Loss: 0.00001956
Iteration 118/1000 | Loss: 0.00001956
Iteration 119/1000 | Loss: 0.00001956
Iteration 120/1000 | Loss: 0.00001956
Iteration 121/1000 | Loss: 0.00001956
Iteration 122/1000 | Loss: 0.00001956
Iteration 123/1000 | Loss: 0.00001956
Iteration 124/1000 | Loss: 0.00001956
Iteration 125/1000 | Loss: 0.00001955
Iteration 126/1000 | Loss: 0.00001955
Iteration 127/1000 | Loss: 0.00001955
Iteration 128/1000 | Loss: 0.00001955
Iteration 129/1000 | Loss: 0.00001955
Iteration 130/1000 | Loss: 0.00001955
Iteration 131/1000 | Loss: 0.00001955
Iteration 132/1000 | Loss: 0.00001955
Iteration 133/1000 | Loss: 0.00001955
Iteration 134/1000 | Loss: 0.00001955
Iteration 135/1000 | Loss: 0.00001955
Iteration 136/1000 | Loss: 0.00001955
Iteration 137/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.955372681550216e-05, 1.955372681550216e-05, 1.955372681550216e-05, 1.955372681550216e-05, 1.955372681550216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.955372681550216e-05

Optimization complete. Final v2v error: 3.7800424098968506 mm

Highest mean error: 4.449060440063477 mm for frame 57

Lowest mean error: 3.19720721244812 mm for frame 5

Saving results

Total time: 52.66843008995056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419406
Iteration 2/25 | Loss: 0.00089910
Iteration 3/25 | Loss: 0.00078314
Iteration 4/25 | Loss: 0.00075409
Iteration 5/25 | Loss: 0.00074734
Iteration 6/25 | Loss: 0.00074670
Iteration 7/25 | Loss: 0.00074670
Iteration 8/25 | Loss: 0.00074670
Iteration 9/25 | Loss: 0.00074670
Iteration 10/25 | Loss: 0.00074670
Iteration 11/25 | Loss: 0.00074670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007466990500688553, 0.0007466990500688553, 0.0007466990500688553, 0.0007466990500688553, 0.0007466990500688553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007466990500688553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25691783
Iteration 2/25 | Loss: 0.00112154
Iteration 3/25 | Loss: 0.00112154
Iteration 4/25 | Loss: 0.00112154
Iteration 5/25 | Loss: 0.00112154
Iteration 6/25 | Loss: 0.00112154
Iteration 7/25 | Loss: 0.00112154
Iteration 8/25 | Loss: 0.00112154
Iteration 9/25 | Loss: 0.00112154
Iteration 10/25 | Loss: 0.00112154
Iteration 11/25 | Loss: 0.00112154
Iteration 12/25 | Loss: 0.00112154
Iteration 13/25 | Loss: 0.00112154
Iteration 14/25 | Loss: 0.00112154
Iteration 15/25 | Loss: 0.00112154
Iteration 16/25 | Loss: 0.00112154
Iteration 17/25 | Loss: 0.00112154
Iteration 18/25 | Loss: 0.00112154
Iteration 19/25 | Loss: 0.00112154
Iteration 20/25 | Loss: 0.00112154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011215388076379895, 0.0011215388076379895, 0.0011215388076379895, 0.0011215388076379895, 0.0011215388076379895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011215388076379895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112154
Iteration 2/1000 | Loss: 0.00006095
Iteration 3/1000 | Loss: 0.00003815
Iteration 4/1000 | Loss: 0.00002810
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00002438
Iteration 7/1000 | Loss: 0.00002359
Iteration 8/1000 | Loss: 0.00002305
Iteration 9/1000 | Loss: 0.00002265
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002207
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002193
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002190
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002187
Iteration 23/1000 | Loss: 0.00002181
Iteration 24/1000 | Loss: 0.00002179
Iteration 25/1000 | Loss: 0.00002178
Iteration 26/1000 | Loss: 0.00002177
Iteration 27/1000 | Loss: 0.00002173
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002171
Iteration 30/1000 | Loss: 0.00002169
Iteration 31/1000 | Loss: 0.00002168
Iteration 32/1000 | Loss: 0.00002168
Iteration 33/1000 | Loss: 0.00002167
Iteration 34/1000 | Loss: 0.00002166
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002164
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002161
Iteration 44/1000 | Loss: 0.00002161
Iteration 45/1000 | Loss: 0.00002160
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002157
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002154
Iteration 60/1000 | Loss: 0.00002154
Iteration 61/1000 | Loss: 0.00002154
Iteration 62/1000 | Loss: 0.00002154
Iteration 63/1000 | Loss: 0.00002153
Iteration 64/1000 | Loss: 0.00002153
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002150
Iteration 69/1000 | Loss: 0.00002150
Iteration 70/1000 | Loss: 0.00002150
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002147
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002146
Iteration 80/1000 | Loss: 0.00002146
Iteration 81/1000 | Loss: 0.00002146
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002143
Iteration 90/1000 | Loss: 0.00002143
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00002142
Iteration 93/1000 | Loss: 0.00002142
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002142
Iteration 96/1000 | Loss: 0.00002141
Iteration 97/1000 | Loss: 0.00002141
Iteration 98/1000 | Loss: 0.00002141
Iteration 99/1000 | Loss: 0.00002140
Iteration 100/1000 | Loss: 0.00002140
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002140
Iteration 103/1000 | Loss: 0.00002140
Iteration 104/1000 | Loss: 0.00002140
Iteration 105/1000 | Loss: 0.00002140
Iteration 106/1000 | Loss: 0.00002140
Iteration 107/1000 | Loss: 0.00002140
Iteration 108/1000 | Loss: 0.00002140
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00002139
Iteration 118/1000 | Loss: 0.00002139
Iteration 119/1000 | Loss: 0.00002139
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002137
Iteration 128/1000 | Loss: 0.00002137
Iteration 129/1000 | Loss: 0.00002137
Iteration 130/1000 | Loss: 0.00002137
Iteration 131/1000 | Loss: 0.00002137
Iteration 132/1000 | Loss: 0.00002137
Iteration 133/1000 | Loss: 0.00002136
Iteration 134/1000 | Loss: 0.00002136
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002135
Iteration 138/1000 | Loss: 0.00002135
Iteration 139/1000 | Loss: 0.00002135
Iteration 140/1000 | Loss: 0.00002135
Iteration 141/1000 | Loss: 0.00002135
Iteration 142/1000 | Loss: 0.00002134
Iteration 143/1000 | Loss: 0.00002134
Iteration 144/1000 | Loss: 0.00002134
Iteration 145/1000 | Loss: 0.00002134
Iteration 146/1000 | Loss: 0.00002134
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002134
Iteration 149/1000 | Loss: 0.00002134
Iteration 150/1000 | Loss: 0.00002134
Iteration 151/1000 | Loss: 0.00002134
Iteration 152/1000 | Loss: 0.00002134
Iteration 153/1000 | Loss: 0.00002134
Iteration 154/1000 | Loss: 0.00002134
Iteration 155/1000 | Loss: 0.00002134
Iteration 156/1000 | Loss: 0.00002133
Iteration 157/1000 | Loss: 0.00002133
Iteration 158/1000 | Loss: 0.00002133
Iteration 159/1000 | Loss: 0.00002133
Iteration 160/1000 | Loss: 0.00002133
Iteration 161/1000 | Loss: 0.00002133
Iteration 162/1000 | Loss: 0.00002133
Iteration 163/1000 | Loss: 0.00002133
Iteration 164/1000 | Loss: 0.00002132
Iteration 165/1000 | Loss: 0.00002132
Iteration 166/1000 | Loss: 0.00002132
Iteration 167/1000 | Loss: 0.00002132
Iteration 168/1000 | Loss: 0.00002132
Iteration 169/1000 | Loss: 0.00002131
Iteration 170/1000 | Loss: 0.00002131
Iteration 171/1000 | Loss: 0.00002131
Iteration 172/1000 | Loss: 0.00002131
Iteration 173/1000 | Loss: 0.00002131
Iteration 174/1000 | Loss: 0.00002131
Iteration 175/1000 | Loss: 0.00002131
Iteration 176/1000 | Loss: 0.00002130
Iteration 177/1000 | Loss: 0.00002130
Iteration 178/1000 | Loss: 0.00002130
Iteration 179/1000 | Loss: 0.00002130
Iteration 180/1000 | Loss: 0.00002130
Iteration 181/1000 | Loss: 0.00002130
Iteration 182/1000 | Loss: 0.00002130
Iteration 183/1000 | Loss: 0.00002130
Iteration 184/1000 | Loss: 0.00002130
Iteration 185/1000 | Loss: 0.00002130
Iteration 186/1000 | Loss: 0.00002130
Iteration 187/1000 | Loss: 0.00002130
Iteration 188/1000 | Loss: 0.00002130
Iteration 189/1000 | Loss: 0.00002129
Iteration 190/1000 | Loss: 0.00002129
Iteration 191/1000 | Loss: 0.00002129
Iteration 192/1000 | Loss: 0.00002129
Iteration 193/1000 | Loss: 0.00002129
Iteration 194/1000 | Loss: 0.00002129
Iteration 195/1000 | Loss: 0.00002129
Iteration 196/1000 | Loss: 0.00002129
Iteration 197/1000 | Loss: 0.00002129
Iteration 198/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.129025597241707e-05, 2.129025597241707e-05, 2.129025597241707e-05, 2.129025597241707e-05, 2.129025597241707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.129025597241707e-05

Optimization complete. Final v2v error: 3.852055788040161 mm

Highest mean error: 4.736708164215088 mm for frame 181

Lowest mean error: 3.4659032821655273 mm for frame 130

Saving results

Total time: 47.59588074684143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919276
Iteration 2/25 | Loss: 0.00086994
Iteration 3/25 | Loss: 0.00074205
Iteration 4/25 | Loss: 0.00072136
Iteration 5/25 | Loss: 0.00071483
Iteration 6/25 | Loss: 0.00071294
Iteration 7/25 | Loss: 0.00071286
Iteration 8/25 | Loss: 0.00071286
Iteration 9/25 | Loss: 0.00071286
Iteration 10/25 | Loss: 0.00071286
Iteration 11/25 | Loss: 0.00071286
Iteration 12/25 | Loss: 0.00071286
Iteration 13/25 | Loss: 0.00071286
Iteration 14/25 | Loss: 0.00071286
Iteration 15/25 | Loss: 0.00071286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007128564757294953, 0.0007128564757294953, 0.0007128564757294953, 0.0007128564757294953, 0.0007128564757294953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007128564757294953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65063572
Iteration 2/25 | Loss: 0.00090531
Iteration 3/25 | Loss: 0.00090531
Iteration 4/25 | Loss: 0.00090531
Iteration 5/25 | Loss: 0.00090531
Iteration 6/25 | Loss: 0.00090531
Iteration 7/25 | Loss: 0.00090531
Iteration 8/25 | Loss: 0.00090531
Iteration 9/25 | Loss: 0.00090531
Iteration 10/25 | Loss: 0.00090531
Iteration 11/25 | Loss: 0.00090531
Iteration 12/25 | Loss: 0.00090531
Iteration 13/25 | Loss: 0.00090531
Iteration 14/25 | Loss: 0.00090531
Iteration 15/25 | Loss: 0.00090531
Iteration 16/25 | Loss: 0.00090531
Iteration 17/25 | Loss: 0.00090531
Iteration 18/25 | Loss: 0.00090531
Iteration 19/25 | Loss: 0.00090531
Iteration 20/25 | Loss: 0.00090531
Iteration 21/25 | Loss: 0.00090531
Iteration 22/25 | Loss: 0.00090531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009053096873685718, 0.0009053096873685718, 0.0009053096873685718, 0.0009053096873685718, 0.0009053096873685718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009053096873685718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090531
Iteration 2/1000 | Loss: 0.00004236
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002242
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001980
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001839
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001789
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001747
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001737
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001726
Iteration 27/1000 | Loss: 0.00001726
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001707
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001703
Iteration 82/1000 | Loss: 0.00001703
Iteration 83/1000 | Loss: 0.00001702
Iteration 84/1000 | Loss: 0.00001702
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001702
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001698
Iteration 118/1000 | Loss: 0.00001698
Iteration 119/1000 | Loss: 0.00001697
Iteration 120/1000 | Loss: 0.00001697
Iteration 121/1000 | Loss: 0.00001697
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001697
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001695
Iteration 133/1000 | Loss: 0.00001695
Iteration 134/1000 | Loss: 0.00001695
Iteration 135/1000 | Loss: 0.00001695
Iteration 136/1000 | Loss: 0.00001695
Iteration 137/1000 | Loss: 0.00001695
Iteration 138/1000 | Loss: 0.00001695
Iteration 139/1000 | Loss: 0.00001695
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001695
Iteration 143/1000 | Loss: 0.00001695
Iteration 144/1000 | Loss: 0.00001695
Iteration 145/1000 | Loss: 0.00001695
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001695
Iteration 149/1000 | Loss: 0.00001694
Iteration 150/1000 | Loss: 0.00001694
Iteration 151/1000 | Loss: 0.00001694
Iteration 152/1000 | Loss: 0.00001694
Iteration 153/1000 | Loss: 0.00001694
Iteration 154/1000 | Loss: 0.00001694
Iteration 155/1000 | Loss: 0.00001694
Iteration 156/1000 | Loss: 0.00001694
Iteration 157/1000 | Loss: 0.00001694
Iteration 158/1000 | Loss: 0.00001694
Iteration 159/1000 | Loss: 0.00001694
Iteration 160/1000 | Loss: 0.00001694
Iteration 161/1000 | Loss: 0.00001694
Iteration 162/1000 | Loss: 0.00001694
Iteration 163/1000 | Loss: 0.00001694
Iteration 164/1000 | Loss: 0.00001694
Iteration 165/1000 | Loss: 0.00001694
Iteration 166/1000 | Loss: 0.00001694
Iteration 167/1000 | Loss: 0.00001694
Iteration 168/1000 | Loss: 0.00001694
Iteration 169/1000 | Loss: 0.00001694
Iteration 170/1000 | Loss: 0.00001693
Iteration 171/1000 | Loss: 0.00001693
Iteration 172/1000 | Loss: 0.00001693
Iteration 173/1000 | Loss: 0.00001693
Iteration 174/1000 | Loss: 0.00001693
Iteration 175/1000 | Loss: 0.00001693
Iteration 176/1000 | Loss: 0.00001693
Iteration 177/1000 | Loss: 0.00001693
Iteration 178/1000 | Loss: 0.00001693
Iteration 179/1000 | Loss: 0.00001693
Iteration 180/1000 | Loss: 0.00001693
Iteration 181/1000 | Loss: 0.00001693
Iteration 182/1000 | Loss: 0.00001693
Iteration 183/1000 | Loss: 0.00001693
Iteration 184/1000 | Loss: 0.00001693
Iteration 185/1000 | Loss: 0.00001693
Iteration 186/1000 | Loss: 0.00001693
Iteration 187/1000 | Loss: 0.00001693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.6929681805777363e-05, 1.6929681805777363e-05, 1.6929681805777363e-05, 1.6929681805777363e-05, 1.6929681805777363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6929681805777363e-05

Optimization complete. Final v2v error: 3.5381534099578857 mm

Highest mean error: 3.8695175647735596 mm for frame 51

Lowest mean error: 3.0730793476104736 mm for frame 8

Saving results

Total time: 46.76327466964722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079696
Iteration 2/25 | Loss: 0.00236608
Iteration 3/25 | Loss: 0.00136549
Iteration 4/25 | Loss: 0.00102626
Iteration 5/25 | Loss: 0.00092425
Iteration 6/25 | Loss: 0.00088494
Iteration 7/25 | Loss: 0.00085566
Iteration 8/25 | Loss: 0.00085075
Iteration 9/25 | Loss: 0.00081451
Iteration 10/25 | Loss: 0.00079165
Iteration 11/25 | Loss: 0.00077713
Iteration 12/25 | Loss: 0.00076501
Iteration 13/25 | Loss: 0.00076392
Iteration 14/25 | Loss: 0.00076244
Iteration 15/25 | Loss: 0.00075786
Iteration 16/25 | Loss: 0.00075681
Iteration 17/25 | Loss: 0.00075577
Iteration 18/25 | Loss: 0.00075545
Iteration 19/25 | Loss: 0.00075524
Iteration 20/25 | Loss: 0.00075885
Iteration 21/25 | Loss: 0.00075884
Iteration 22/25 | Loss: 0.00075884
Iteration 23/25 | Loss: 0.00075884
Iteration 24/25 | Loss: 0.00075827
Iteration 25/25 | Loss: 0.00075982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29658151
Iteration 2/25 | Loss: 0.00156060
Iteration 3/25 | Loss: 0.00156059
Iteration 4/25 | Loss: 0.00128049
Iteration 5/25 | Loss: 0.00128044
Iteration 6/25 | Loss: 0.00128044
Iteration 7/25 | Loss: 0.00128044
Iteration 8/25 | Loss: 0.00128043
Iteration 9/25 | Loss: 0.00128043
Iteration 10/25 | Loss: 0.00128043
Iteration 11/25 | Loss: 0.00128043
Iteration 12/25 | Loss: 0.00128043
Iteration 13/25 | Loss: 0.00128043
Iteration 14/25 | Loss: 0.00128043
Iteration 15/25 | Loss: 0.00128043
Iteration 16/25 | Loss: 0.00128043
Iteration 17/25 | Loss: 0.00128043
Iteration 18/25 | Loss: 0.00128043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012804333819076419, 0.0012804333819076419, 0.0012804333819076419, 0.0012804333819076419, 0.0012804333819076419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012804333819076419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128043
Iteration 2/1000 | Loss: 0.00046775
Iteration 3/1000 | Loss: 0.00039548
Iteration 4/1000 | Loss: 0.00012175
Iteration 5/1000 | Loss: 0.00034764
Iteration 6/1000 | Loss: 0.00019697
Iteration 7/1000 | Loss: 0.00037935
Iteration 8/1000 | Loss: 0.00035251
Iteration 9/1000 | Loss: 0.00009248
Iteration 10/1000 | Loss: 0.00015442
Iteration 11/1000 | Loss: 0.00005756
Iteration 12/1000 | Loss: 0.00020501
Iteration 13/1000 | Loss: 0.00005260
Iteration 14/1000 | Loss: 0.00016320
Iteration 15/1000 | Loss: 0.00032965
Iteration 16/1000 | Loss: 0.00018111
Iteration 17/1000 | Loss: 0.00012803
Iteration 18/1000 | Loss: 0.00004774
Iteration 19/1000 | Loss: 0.00004694
Iteration 20/1000 | Loss: 0.00019633
Iteration 21/1000 | Loss: 0.00008068
Iteration 22/1000 | Loss: 0.00004567
Iteration 23/1000 | Loss: 0.00007795
Iteration 24/1000 | Loss: 0.00004511
Iteration 25/1000 | Loss: 0.00019224
Iteration 26/1000 | Loss: 0.00053899
Iteration 27/1000 | Loss: 0.00043796
Iteration 28/1000 | Loss: 0.00012936
Iteration 29/1000 | Loss: 0.00025492
Iteration 30/1000 | Loss: 0.00008369
Iteration 31/1000 | Loss: 0.00006118
Iteration 32/1000 | Loss: 0.00008783
Iteration 33/1000 | Loss: 0.00011031
Iteration 34/1000 | Loss: 0.00003529
Iteration 35/1000 | Loss: 0.00016754
Iteration 36/1000 | Loss: 0.00003708
Iteration 37/1000 | Loss: 0.00009630
Iteration 38/1000 | Loss: 0.00005679
Iteration 39/1000 | Loss: 0.00003627
Iteration 40/1000 | Loss: 0.00003164
Iteration 41/1000 | Loss: 0.00014745
Iteration 42/1000 | Loss: 0.00009609
Iteration 43/1000 | Loss: 0.00010747
Iteration 44/1000 | Loss: 0.00003191
Iteration 45/1000 | Loss: 0.00003121
Iteration 46/1000 | Loss: 0.00003087
Iteration 47/1000 | Loss: 0.00003047
Iteration 48/1000 | Loss: 0.00003030
Iteration 49/1000 | Loss: 0.00003020
Iteration 50/1000 | Loss: 0.00015338
Iteration 51/1000 | Loss: 0.00006353
Iteration 52/1000 | Loss: 0.00003268
Iteration 53/1000 | Loss: 0.00003070
Iteration 54/1000 | Loss: 0.00002974
Iteration 55/1000 | Loss: 0.00002917
Iteration 56/1000 | Loss: 0.00002877
Iteration 57/1000 | Loss: 0.00002857
Iteration 58/1000 | Loss: 0.00002853
Iteration 59/1000 | Loss: 0.00002848
Iteration 60/1000 | Loss: 0.00002847
Iteration 61/1000 | Loss: 0.00002847
Iteration 62/1000 | Loss: 0.00002846
Iteration 63/1000 | Loss: 0.00002845
Iteration 64/1000 | Loss: 0.00002845
Iteration 65/1000 | Loss: 0.00002844
Iteration 66/1000 | Loss: 0.00002844
Iteration 67/1000 | Loss: 0.00002843
Iteration 68/1000 | Loss: 0.00002842
Iteration 69/1000 | Loss: 0.00002842
Iteration 70/1000 | Loss: 0.00002842
Iteration 71/1000 | Loss: 0.00002842
Iteration 72/1000 | Loss: 0.00002842
Iteration 73/1000 | Loss: 0.00002842
Iteration 74/1000 | Loss: 0.00002842
Iteration 75/1000 | Loss: 0.00002842
Iteration 76/1000 | Loss: 0.00002842
Iteration 77/1000 | Loss: 0.00002841
Iteration 78/1000 | Loss: 0.00002841
Iteration 79/1000 | Loss: 0.00002841
Iteration 80/1000 | Loss: 0.00002841
Iteration 81/1000 | Loss: 0.00002841
Iteration 82/1000 | Loss: 0.00002841
Iteration 83/1000 | Loss: 0.00002840
Iteration 84/1000 | Loss: 0.00002839
Iteration 85/1000 | Loss: 0.00002839
Iteration 86/1000 | Loss: 0.00002839
Iteration 87/1000 | Loss: 0.00002838
Iteration 88/1000 | Loss: 0.00002838
Iteration 89/1000 | Loss: 0.00002837
Iteration 90/1000 | Loss: 0.00002837
Iteration 91/1000 | Loss: 0.00002837
Iteration 92/1000 | Loss: 0.00002837
Iteration 93/1000 | Loss: 0.00002837
Iteration 94/1000 | Loss: 0.00002837
Iteration 95/1000 | Loss: 0.00002837
Iteration 96/1000 | Loss: 0.00002836
Iteration 97/1000 | Loss: 0.00002836
Iteration 98/1000 | Loss: 0.00002836
Iteration 99/1000 | Loss: 0.00002835
Iteration 100/1000 | Loss: 0.00002835
Iteration 101/1000 | Loss: 0.00002834
Iteration 102/1000 | Loss: 0.00002834
Iteration 103/1000 | Loss: 0.00002834
Iteration 104/1000 | Loss: 0.00002834
Iteration 105/1000 | Loss: 0.00002833
Iteration 106/1000 | Loss: 0.00002833
Iteration 107/1000 | Loss: 0.00002833
Iteration 108/1000 | Loss: 0.00002832
Iteration 109/1000 | Loss: 0.00002832
Iteration 110/1000 | Loss: 0.00002832
Iteration 111/1000 | Loss: 0.00002831
Iteration 112/1000 | Loss: 0.00002831
Iteration 113/1000 | Loss: 0.00002831
Iteration 114/1000 | Loss: 0.00002831
Iteration 115/1000 | Loss: 0.00002831
Iteration 116/1000 | Loss: 0.00002831
Iteration 117/1000 | Loss: 0.00002831
Iteration 118/1000 | Loss: 0.00002831
Iteration 119/1000 | Loss: 0.00002830
Iteration 120/1000 | Loss: 0.00002830
Iteration 121/1000 | Loss: 0.00002830
Iteration 122/1000 | Loss: 0.00002830
Iteration 123/1000 | Loss: 0.00002830
Iteration 124/1000 | Loss: 0.00002830
Iteration 125/1000 | Loss: 0.00002830
Iteration 126/1000 | Loss: 0.00002830
Iteration 127/1000 | Loss: 0.00002830
Iteration 128/1000 | Loss: 0.00002830
Iteration 129/1000 | Loss: 0.00002830
Iteration 130/1000 | Loss: 0.00002830
Iteration 131/1000 | Loss: 0.00002830
Iteration 132/1000 | Loss: 0.00002830
Iteration 133/1000 | Loss: 0.00002830
Iteration 134/1000 | Loss: 0.00002830
Iteration 135/1000 | Loss: 0.00002830
Iteration 136/1000 | Loss: 0.00002830
Iteration 137/1000 | Loss: 0.00002830
Iteration 138/1000 | Loss: 0.00002830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.8299329642322846e-05, 2.8299329642322846e-05, 2.8299329642322846e-05, 2.8299329642322846e-05, 2.8299329642322846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8299329642322846e-05

Optimization complete. Final v2v error: 3.9672348499298096 mm

Highest mean error: 12.865107536315918 mm for frame 71

Lowest mean error: 3.441201686859131 mm for frame 45

Saving results

Total time: 143.71319460868835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379456
Iteration 2/25 | Loss: 0.00091477
Iteration 3/25 | Loss: 0.00078574
Iteration 4/25 | Loss: 0.00075667
Iteration 5/25 | Loss: 0.00074347
Iteration 6/25 | Loss: 0.00074070
Iteration 7/25 | Loss: 0.00073979
Iteration 8/25 | Loss: 0.00073979
Iteration 9/25 | Loss: 0.00073979
Iteration 10/25 | Loss: 0.00073979
Iteration 11/25 | Loss: 0.00073979
Iteration 12/25 | Loss: 0.00073979
Iteration 13/25 | Loss: 0.00073979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007397907320410013, 0.0007397907320410013, 0.0007397907320410013, 0.0007397907320410013, 0.0007397907320410013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007397907320410013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27669716
Iteration 2/25 | Loss: 0.00099928
Iteration 3/25 | Loss: 0.00099928
Iteration 4/25 | Loss: 0.00099928
Iteration 5/25 | Loss: 0.00099928
Iteration 6/25 | Loss: 0.00099928
Iteration 7/25 | Loss: 0.00099928
Iteration 8/25 | Loss: 0.00099928
Iteration 9/25 | Loss: 0.00099928
Iteration 10/25 | Loss: 0.00099928
Iteration 11/25 | Loss: 0.00099928
Iteration 12/25 | Loss: 0.00099928
Iteration 13/25 | Loss: 0.00099928
Iteration 14/25 | Loss: 0.00099928
Iteration 15/25 | Loss: 0.00099928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009992795530706644, 0.0009992795530706644, 0.0009992795530706644, 0.0009992795530706644, 0.0009992795530706644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009992795530706644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099928
Iteration 2/1000 | Loss: 0.00005115
Iteration 3/1000 | Loss: 0.00003471
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002486
Iteration 7/1000 | Loss: 0.00002396
Iteration 8/1000 | Loss: 0.00002317
Iteration 9/1000 | Loss: 0.00002271
Iteration 10/1000 | Loss: 0.00002234
Iteration 11/1000 | Loss: 0.00002211
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002190
Iteration 14/1000 | Loss: 0.00002184
Iteration 15/1000 | Loss: 0.00002178
Iteration 16/1000 | Loss: 0.00002167
Iteration 17/1000 | Loss: 0.00002166
Iteration 18/1000 | Loss: 0.00002164
Iteration 19/1000 | Loss: 0.00002161
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00002160
Iteration 22/1000 | Loss: 0.00002158
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002158
Iteration 26/1000 | Loss: 0.00002158
Iteration 27/1000 | Loss: 0.00002158
Iteration 28/1000 | Loss: 0.00002157
Iteration 29/1000 | Loss: 0.00002157
Iteration 30/1000 | Loss: 0.00002157
Iteration 31/1000 | Loss: 0.00002157
Iteration 32/1000 | Loss: 0.00002155
Iteration 33/1000 | Loss: 0.00002154
Iteration 34/1000 | Loss: 0.00002154
Iteration 35/1000 | Loss: 0.00002153
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002152
Iteration 38/1000 | Loss: 0.00002152
Iteration 39/1000 | Loss: 0.00002152
Iteration 40/1000 | Loss: 0.00002151
Iteration 41/1000 | Loss: 0.00002150
Iteration 42/1000 | Loss: 0.00002150
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002148
Iteration 46/1000 | Loss: 0.00002148
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002148
Iteration 49/1000 | Loss: 0.00002148
Iteration 50/1000 | Loss: 0.00002148
Iteration 51/1000 | Loss: 0.00002147
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002147
Iteration 54/1000 | Loss: 0.00002147
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00002147
Iteration 57/1000 | Loss: 0.00002147
Iteration 58/1000 | Loss: 0.00002147
Iteration 59/1000 | Loss: 0.00002147
Iteration 60/1000 | Loss: 0.00002147
Iteration 61/1000 | Loss: 0.00002146
Iteration 62/1000 | Loss: 0.00002146
Iteration 63/1000 | Loss: 0.00002146
Iteration 64/1000 | Loss: 0.00002146
Iteration 65/1000 | Loss: 0.00002146
Iteration 66/1000 | Loss: 0.00002145
Iteration 67/1000 | Loss: 0.00002145
Iteration 68/1000 | Loss: 0.00002145
Iteration 69/1000 | Loss: 0.00002145
Iteration 70/1000 | Loss: 0.00002145
Iteration 71/1000 | Loss: 0.00002144
Iteration 72/1000 | Loss: 0.00002144
Iteration 73/1000 | Loss: 0.00002144
Iteration 74/1000 | Loss: 0.00002144
Iteration 75/1000 | Loss: 0.00002144
Iteration 76/1000 | Loss: 0.00002143
Iteration 77/1000 | Loss: 0.00002143
Iteration 78/1000 | Loss: 0.00002143
Iteration 79/1000 | Loss: 0.00002143
Iteration 80/1000 | Loss: 0.00002143
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002143
Iteration 84/1000 | Loss: 0.00002143
Iteration 85/1000 | Loss: 0.00002143
Iteration 86/1000 | Loss: 0.00002143
Iteration 87/1000 | Loss: 0.00002142
Iteration 88/1000 | Loss: 0.00002142
Iteration 89/1000 | Loss: 0.00002142
Iteration 90/1000 | Loss: 0.00002142
Iteration 91/1000 | Loss: 0.00002142
Iteration 92/1000 | Loss: 0.00002142
Iteration 93/1000 | Loss: 0.00002142
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002141
Iteration 96/1000 | Loss: 0.00002141
Iteration 97/1000 | Loss: 0.00002141
Iteration 98/1000 | Loss: 0.00002140
Iteration 99/1000 | Loss: 0.00002140
Iteration 100/1000 | Loss: 0.00002140
Iteration 101/1000 | Loss: 0.00002139
Iteration 102/1000 | Loss: 0.00002139
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00002139
Iteration 105/1000 | Loss: 0.00002139
Iteration 106/1000 | Loss: 0.00002139
Iteration 107/1000 | Loss: 0.00002139
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002138
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002138
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002138
Iteration 126/1000 | Loss: 0.00002138
Iteration 127/1000 | Loss: 0.00002138
Iteration 128/1000 | Loss: 0.00002138
Iteration 129/1000 | Loss: 0.00002138
Iteration 130/1000 | Loss: 0.00002138
Iteration 131/1000 | Loss: 0.00002137
Iteration 132/1000 | Loss: 0.00002137
Iteration 133/1000 | Loss: 0.00002137
Iteration 134/1000 | Loss: 0.00002137
Iteration 135/1000 | Loss: 0.00002137
Iteration 136/1000 | Loss: 0.00002137
Iteration 137/1000 | Loss: 0.00002137
Iteration 138/1000 | Loss: 0.00002137
Iteration 139/1000 | Loss: 0.00002137
Iteration 140/1000 | Loss: 0.00002136
Iteration 141/1000 | Loss: 0.00002136
Iteration 142/1000 | Loss: 0.00002136
Iteration 143/1000 | Loss: 0.00002136
Iteration 144/1000 | Loss: 0.00002136
Iteration 145/1000 | Loss: 0.00002136
Iteration 146/1000 | Loss: 0.00002136
Iteration 147/1000 | Loss: 0.00002136
Iteration 148/1000 | Loss: 0.00002136
Iteration 149/1000 | Loss: 0.00002136
Iteration 150/1000 | Loss: 0.00002136
Iteration 151/1000 | Loss: 0.00002136
Iteration 152/1000 | Loss: 0.00002136
Iteration 153/1000 | Loss: 0.00002136
Iteration 154/1000 | Loss: 0.00002136
Iteration 155/1000 | Loss: 0.00002136
Iteration 156/1000 | Loss: 0.00002135
Iteration 157/1000 | Loss: 0.00002135
Iteration 158/1000 | Loss: 0.00002135
Iteration 159/1000 | Loss: 0.00002135
Iteration 160/1000 | Loss: 0.00002135
Iteration 161/1000 | Loss: 0.00002135
Iteration 162/1000 | Loss: 0.00002135
Iteration 163/1000 | Loss: 0.00002135
Iteration 164/1000 | Loss: 0.00002135
Iteration 165/1000 | Loss: 0.00002134
Iteration 166/1000 | Loss: 0.00002134
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002134
Iteration 169/1000 | Loss: 0.00002134
Iteration 170/1000 | Loss: 0.00002134
Iteration 171/1000 | Loss: 0.00002134
Iteration 172/1000 | Loss: 0.00002134
Iteration 173/1000 | Loss: 0.00002134
Iteration 174/1000 | Loss: 0.00002134
Iteration 175/1000 | Loss: 0.00002134
Iteration 176/1000 | Loss: 0.00002134
Iteration 177/1000 | Loss: 0.00002134
Iteration 178/1000 | Loss: 0.00002133
Iteration 179/1000 | Loss: 0.00002133
Iteration 180/1000 | Loss: 0.00002133
Iteration 181/1000 | Loss: 0.00002133
Iteration 182/1000 | Loss: 0.00002133
Iteration 183/1000 | Loss: 0.00002133
Iteration 184/1000 | Loss: 0.00002133
Iteration 185/1000 | Loss: 0.00002133
Iteration 186/1000 | Loss: 0.00002133
Iteration 187/1000 | Loss: 0.00002133
Iteration 188/1000 | Loss: 0.00002133
Iteration 189/1000 | Loss: 0.00002133
Iteration 190/1000 | Loss: 0.00002133
Iteration 191/1000 | Loss: 0.00002133
Iteration 192/1000 | Loss: 0.00002133
Iteration 193/1000 | Loss: 0.00002133
Iteration 194/1000 | Loss: 0.00002133
Iteration 195/1000 | Loss: 0.00002133
Iteration 196/1000 | Loss: 0.00002133
Iteration 197/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.1325404304661788e-05, 2.1325404304661788e-05, 2.1325404304661788e-05, 2.1325404304661788e-05, 2.1325404304661788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1325404304661788e-05

Optimization complete. Final v2v error: 3.9286563396453857 mm

Highest mean error: 4.486059188842773 mm for frame 208

Lowest mean error: 3.346076250076294 mm for frame 138

Saving results

Total time: 47.82297468185425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599344
Iteration 2/25 | Loss: 0.00124427
Iteration 3/25 | Loss: 0.00082804
Iteration 4/25 | Loss: 0.00076169
Iteration 5/25 | Loss: 0.00075379
Iteration 6/25 | Loss: 0.00075111
Iteration 7/25 | Loss: 0.00075068
Iteration 8/25 | Loss: 0.00075068
Iteration 9/25 | Loss: 0.00075068
Iteration 10/25 | Loss: 0.00075068
Iteration 11/25 | Loss: 0.00075068
Iteration 12/25 | Loss: 0.00075068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007506838301196694, 0.0007506838301196694, 0.0007506838301196694, 0.0007506838301196694, 0.0007506838301196694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007506838301196694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28192234
Iteration 2/25 | Loss: 0.00062246
Iteration 3/25 | Loss: 0.00062245
Iteration 4/25 | Loss: 0.00062245
Iteration 5/25 | Loss: 0.00062245
Iteration 6/25 | Loss: 0.00062245
Iteration 7/25 | Loss: 0.00062245
Iteration 8/25 | Loss: 0.00062245
Iteration 9/25 | Loss: 0.00062245
Iteration 10/25 | Loss: 0.00062245
Iteration 11/25 | Loss: 0.00062245
Iteration 12/25 | Loss: 0.00062245
Iteration 13/25 | Loss: 0.00062245
Iteration 14/25 | Loss: 0.00062245
Iteration 15/25 | Loss: 0.00062245
Iteration 16/25 | Loss: 0.00062245
Iteration 17/25 | Loss: 0.00062245
Iteration 18/25 | Loss: 0.00062245
Iteration 19/25 | Loss: 0.00062245
Iteration 20/25 | Loss: 0.00062245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006224473472684622, 0.0006224473472684622, 0.0006224473472684622, 0.0006224473472684622, 0.0006224473472684622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006224473472684622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062245
Iteration 2/1000 | Loss: 0.00003862
Iteration 3/1000 | Loss: 0.00002997
Iteration 4/1000 | Loss: 0.00002541
Iteration 5/1000 | Loss: 0.00002417
Iteration 6/1000 | Loss: 0.00002316
Iteration 7/1000 | Loss: 0.00002253
Iteration 8/1000 | Loss: 0.00002204
Iteration 9/1000 | Loss: 0.00002151
Iteration 10/1000 | Loss: 0.00002126
Iteration 11/1000 | Loss: 0.00002103
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002086
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002048
Iteration 16/1000 | Loss: 0.00002037
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002032
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002032
Iteration 22/1000 | Loss: 0.00002032
Iteration 23/1000 | Loss: 0.00002032
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002030
Iteration 26/1000 | Loss: 0.00002028
Iteration 27/1000 | Loss: 0.00002028
Iteration 28/1000 | Loss: 0.00002028
Iteration 29/1000 | Loss: 0.00002027
Iteration 30/1000 | Loss: 0.00002027
Iteration 31/1000 | Loss: 0.00002024
Iteration 32/1000 | Loss: 0.00002024
Iteration 33/1000 | Loss: 0.00002024
Iteration 34/1000 | Loss: 0.00002024
Iteration 35/1000 | Loss: 0.00002024
Iteration 36/1000 | Loss: 0.00002024
Iteration 37/1000 | Loss: 0.00002024
Iteration 38/1000 | Loss: 0.00002024
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002024
Iteration 43/1000 | Loss: 0.00002024
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002023
Iteration 47/1000 | Loss: 0.00002023
Iteration 48/1000 | Loss: 0.00002023
Iteration 49/1000 | Loss: 0.00002021
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002020
Iteration 66/1000 | Loss: 0.00002020
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002019
Iteration 72/1000 | Loss: 0.00002019
Iteration 73/1000 | Loss: 0.00002019
Iteration 74/1000 | Loss: 0.00002019
Iteration 75/1000 | Loss: 0.00002019
Iteration 76/1000 | Loss: 0.00002019
Iteration 77/1000 | Loss: 0.00002017
Iteration 78/1000 | Loss: 0.00002017
Iteration 79/1000 | Loss: 0.00002017
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002016
Iteration 82/1000 | Loss: 0.00002016
Iteration 83/1000 | Loss: 0.00002015
Iteration 84/1000 | Loss: 0.00002015
Iteration 85/1000 | Loss: 0.00002015
Iteration 86/1000 | Loss: 0.00002015
Iteration 87/1000 | Loss: 0.00002015
Iteration 88/1000 | Loss: 0.00002015
Iteration 89/1000 | Loss: 0.00002015
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002014
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002013
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002012
Iteration 100/1000 | Loss: 0.00002012
Iteration 101/1000 | Loss: 0.00002012
Iteration 102/1000 | Loss: 0.00002012
Iteration 103/1000 | Loss: 0.00002011
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002011
Iteration 106/1000 | Loss: 0.00002011
Iteration 107/1000 | Loss: 0.00002011
Iteration 108/1000 | Loss: 0.00002011
Iteration 109/1000 | Loss: 0.00002011
Iteration 110/1000 | Loss: 0.00002011
Iteration 111/1000 | Loss: 0.00002011
Iteration 112/1000 | Loss: 0.00002011
Iteration 113/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.0110543118789792e-05, 2.0110543118789792e-05, 2.0110543118789792e-05, 2.0110543118789792e-05, 2.0110543118789792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0110543118789792e-05

Optimization complete. Final v2v error: 3.932640314102173 mm

Highest mean error: 4.606422424316406 mm for frame 57

Lowest mean error: 3.525482654571533 mm for frame 89

Saving results

Total time: 36.95153617858887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022102
Iteration 2/25 | Loss: 0.00121441
Iteration 3/25 | Loss: 0.00082756
Iteration 4/25 | Loss: 0.00075964
Iteration 5/25 | Loss: 0.00072921
Iteration 6/25 | Loss: 0.00075886
Iteration 7/25 | Loss: 0.00070920
Iteration 8/25 | Loss: 0.00069201
Iteration 9/25 | Loss: 0.00068308
Iteration 10/25 | Loss: 0.00068535
Iteration 11/25 | Loss: 0.00068361
Iteration 12/25 | Loss: 0.00068147
Iteration 13/25 | Loss: 0.00068206
Iteration 14/25 | Loss: 0.00067935
Iteration 15/25 | Loss: 0.00067571
Iteration 16/25 | Loss: 0.00067451
Iteration 17/25 | Loss: 0.00067424
Iteration 18/25 | Loss: 0.00067416
Iteration 19/25 | Loss: 0.00067416
Iteration 20/25 | Loss: 0.00067416
Iteration 21/25 | Loss: 0.00067415
Iteration 22/25 | Loss: 0.00067415
Iteration 23/25 | Loss: 0.00067415
Iteration 24/25 | Loss: 0.00067415
Iteration 25/25 | Loss: 0.00067415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.29943085
Iteration 2/25 | Loss: 0.00123511
Iteration 3/25 | Loss: 0.00123503
Iteration 4/25 | Loss: 0.00123503
Iteration 5/25 | Loss: 0.00123503
Iteration 6/25 | Loss: 0.00123503
Iteration 7/25 | Loss: 0.00123503
Iteration 8/25 | Loss: 0.00123503
Iteration 9/25 | Loss: 0.00123503
Iteration 10/25 | Loss: 0.00123503
Iteration 11/25 | Loss: 0.00123503
Iteration 12/25 | Loss: 0.00123503
Iteration 13/25 | Loss: 0.00123503
Iteration 14/25 | Loss: 0.00123503
Iteration 15/25 | Loss: 0.00123503
Iteration 16/25 | Loss: 0.00123503
Iteration 17/25 | Loss: 0.00123503
Iteration 18/25 | Loss: 0.00123503
Iteration 19/25 | Loss: 0.00123503
Iteration 20/25 | Loss: 0.00123503
Iteration 21/25 | Loss: 0.00123503
Iteration 22/25 | Loss: 0.00123503
Iteration 23/25 | Loss: 0.00123503
Iteration 24/25 | Loss: 0.00123503
Iteration 25/25 | Loss: 0.00123503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123503
Iteration 2/1000 | Loss: 0.00022884
Iteration 3/1000 | Loss: 0.00007779
Iteration 4/1000 | Loss: 0.00014425
Iteration 5/1000 | Loss: 0.00017102
Iteration 6/1000 | Loss: 0.00015983
Iteration 7/1000 | Loss: 0.00011050
Iteration 8/1000 | Loss: 0.00016515
Iteration 9/1000 | Loss: 0.00018823
Iteration 10/1000 | Loss: 0.00009632
Iteration 11/1000 | Loss: 0.00008323
Iteration 12/1000 | Loss: 0.00011477
Iteration 13/1000 | Loss: 0.00007998
Iteration 14/1000 | Loss: 0.00003067
Iteration 15/1000 | Loss: 0.00004503
Iteration 16/1000 | Loss: 0.00027421
Iteration 17/1000 | Loss: 0.00007775
Iteration 18/1000 | Loss: 0.00006307
Iteration 19/1000 | Loss: 0.00004346
Iteration 20/1000 | Loss: 0.00008627
Iteration 21/1000 | Loss: 0.00005963
Iteration 22/1000 | Loss: 0.00004787
Iteration 23/1000 | Loss: 0.00006291
Iteration 24/1000 | Loss: 0.00003285
Iteration 25/1000 | Loss: 0.00006266
Iteration 26/1000 | Loss: 0.00004447
Iteration 27/1000 | Loss: 0.00006237
Iteration 28/1000 | Loss: 0.00005062
Iteration 29/1000 | Loss: 0.00006502
Iteration 30/1000 | Loss: 0.00006073
Iteration 31/1000 | Loss: 0.00007898
Iteration 32/1000 | Loss: 0.00010066
Iteration 33/1000 | Loss: 0.00006415
Iteration 34/1000 | Loss: 0.00008440
Iteration 35/1000 | Loss: 0.00004921
Iteration 36/1000 | Loss: 0.00004375
Iteration 37/1000 | Loss: 0.00007921
Iteration 38/1000 | Loss: 0.00008289
Iteration 39/1000 | Loss: 0.00005276
Iteration 40/1000 | Loss: 0.00006529
Iteration 41/1000 | Loss: 0.00009756
Iteration 42/1000 | Loss: 0.00006310
Iteration 43/1000 | Loss: 0.00008827
Iteration 44/1000 | Loss: 0.00008068
Iteration 45/1000 | Loss: 0.00009819
Iteration 46/1000 | Loss: 0.00008934
Iteration 47/1000 | Loss: 0.00010456
Iteration 48/1000 | Loss: 0.00008523
Iteration 49/1000 | Loss: 0.00009554
Iteration 50/1000 | Loss: 0.00008065
Iteration 51/1000 | Loss: 0.00006508
Iteration 52/1000 | Loss: 0.00007774
Iteration 53/1000 | Loss: 0.00007366
Iteration 54/1000 | Loss: 0.00008059
Iteration 55/1000 | Loss: 0.00005224
Iteration 56/1000 | Loss: 0.00002843
Iteration 57/1000 | Loss: 0.00005891
Iteration 58/1000 | Loss: 0.00005365
Iteration 59/1000 | Loss: 0.00004654
Iteration 60/1000 | Loss: 0.00006245
Iteration 61/1000 | Loss: 0.00004105
Iteration 62/1000 | Loss: 0.00008356
Iteration 63/1000 | Loss: 0.00007718
Iteration 64/1000 | Loss: 0.00006471
Iteration 65/1000 | Loss: 0.00005373
Iteration 66/1000 | Loss: 0.00007142
Iteration 67/1000 | Loss: 0.00007784
Iteration 68/1000 | Loss: 0.00005963
Iteration 69/1000 | Loss: 0.00008420
Iteration 70/1000 | Loss: 0.00007674
Iteration 71/1000 | Loss: 0.00006989
Iteration 72/1000 | Loss: 0.00007219
Iteration 73/1000 | Loss: 0.00007949
Iteration 74/1000 | Loss: 0.00004727
Iteration 75/1000 | Loss: 0.00005535
Iteration 76/1000 | Loss: 0.00004836
Iteration 77/1000 | Loss: 0.00003431
Iteration 78/1000 | Loss: 0.00004439
Iteration 79/1000 | Loss: 0.00007046
Iteration 80/1000 | Loss: 0.00004839
Iteration 81/1000 | Loss: 0.00003899
Iteration 82/1000 | Loss: 0.00005204
Iteration 83/1000 | Loss: 0.00004029
Iteration 84/1000 | Loss: 0.00004373
Iteration 85/1000 | Loss: 0.00002371
Iteration 86/1000 | Loss: 0.00002220
Iteration 87/1000 | Loss: 0.00004217
Iteration 88/1000 | Loss: 0.00003752
Iteration 89/1000 | Loss: 0.00003988
Iteration 90/1000 | Loss: 0.00002995
Iteration 91/1000 | Loss: 0.00003086
Iteration 92/1000 | Loss: 0.00004122
Iteration 93/1000 | Loss: 0.00003726
Iteration 94/1000 | Loss: 0.00002916
Iteration 95/1000 | Loss: 0.00004346
Iteration 96/1000 | Loss: 0.00003820
Iteration 97/1000 | Loss: 0.00004209
Iteration 98/1000 | Loss: 0.00003515
Iteration 99/1000 | Loss: 0.00004606
Iteration 100/1000 | Loss: 0.00003413
Iteration 101/1000 | Loss: 0.00004064
Iteration 102/1000 | Loss: 0.00003174
Iteration 103/1000 | Loss: 0.00004964
Iteration 104/1000 | Loss: 0.00004078
Iteration 105/1000 | Loss: 0.00004161
Iteration 106/1000 | Loss: 0.00003321
Iteration 107/1000 | Loss: 0.00003547
Iteration 108/1000 | Loss: 0.00004013
Iteration 109/1000 | Loss: 0.00005048
Iteration 110/1000 | Loss: 0.00004085
Iteration 111/1000 | Loss: 0.00004314
Iteration 112/1000 | Loss: 0.00004804
Iteration 113/1000 | Loss: 0.00002574
Iteration 114/1000 | Loss: 0.00002263
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001726
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001640
Iteration 122/1000 | Loss: 0.00001605
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001579
Iteration 126/1000 | Loss: 0.00001579
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001578
Iteration 131/1000 | Loss: 0.00001578
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001575
Iteration 137/1000 | Loss: 0.00001575
Iteration 138/1000 | Loss: 0.00001575
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001574
Iteration 141/1000 | Loss: 0.00001574
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Iteration 150/1000 | Loss: 0.00001574
Iteration 151/1000 | Loss: 0.00001573
Iteration 152/1000 | Loss: 0.00001573
Iteration 153/1000 | Loss: 0.00001573
Iteration 154/1000 | Loss: 0.00001573
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001572
Iteration 157/1000 | Loss: 0.00001572
Iteration 158/1000 | Loss: 0.00001572
Iteration 159/1000 | Loss: 0.00001572
Iteration 160/1000 | Loss: 0.00001572
Iteration 161/1000 | Loss: 0.00001572
Iteration 162/1000 | Loss: 0.00001571
Iteration 163/1000 | Loss: 0.00001571
Iteration 164/1000 | Loss: 0.00001571
Iteration 165/1000 | Loss: 0.00001571
Iteration 166/1000 | Loss: 0.00001570
Iteration 167/1000 | Loss: 0.00001570
Iteration 168/1000 | Loss: 0.00001570
Iteration 169/1000 | Loss: 0.00001570
Iteration 170/1000 | Loss: 0.00001570
Iteration 171/1000 | Loss: 0.00001570
Iteration 172/1000 | Loss: 0.00001570
Iteration 173/1000 | Loss: 0.00001569
Iteration 174/1000 | Loss: 0.00001569
Iteration 175/1000 | Loss: 0.00001569
Iteration 176/1000 | Loss: 0.00001569
Iteration 177/1000 | Loss: 0.00001568
Iteration 178/1000 | Loss: 0.00001568
Iteration 179/1000 | Loss: 0.00001568
Iteration 180/1000 | Loss: 0.00001568
Iteration 181/1000 | Loss: 0.00001568
Iteration 182/1000 | Loss: 0.00001568
Iteration 183/1000 | Loss: 0.00001568
Iteration 184/1000 | Loss: 0.00001568
Iteration 185/1000 | Loss: 0.00001568
Iteration 186/1000 | Loss: 0.00001568
Iteration 187/1000 | Loss: 0.00001567
Iteration 188/1000 | Loss: 0.00001567
Iteration 189/1000 | Loss: 0.00001567
Iteration 190/1000 | Loss: 0.00001567
Iteration 191/1000 | Loss: 0.00001567
Iteration 192/1000 | Loss: 0.00001567
Iteration 193/1000 | Loss: 0.00001567
Iteration 194/1000 | Loss: 0.00001566
Iteration 195/1000 | Loss: 0.00001566
Iteration 196/1000 | Loss: 0.00001566
Iteration 197/1000 | Loss: 0.00001566
Iteration 198/1000 | Loss: 0.00001565
Iteration 199/1000 | Loss: 0.00001565
Iteration 200/1000 | Loss: 0.00001565
Iteration 201/1000 | Loss: 0.00001565
Iteration 202/1000 | Loss: 0.00001565
Iteration 203/1000 | Loss: 0.00001565
Iteration 204/1000 | Loss: 0.00001565
Iteration 205/1000 | Loss: 0.00001565
Iteration 206/1000 | Loss: 0.00001564
Iteration 207/1000 | Loss: 0.00001564
Iteration 208/1000 | Loss: 0.00001564
Iteration 209/1000 | Loss: 0.00001564
Iteration 210/1000 | Loss: 0.00001564
Iteration 211/1000 | Loss: 0.00001564
Iteration 212/1000 | Loss: 0.00001564
Iteration 213/1000 | Loss: 0.00001564
Iteration 214/1000 | Loss: 0.00001563
Iteration 215/1000 | Loss: 0.00001563
Iteration 216/1000 | Loss: 0.00001563
Iteration 217/1000 | Loss: 0.00001563
Iteration 218/1000 | Loss: 0.00001562
Iteration 219/1000 | Loss: 0.00001562
Iteration 220/1000 | Loss: 0.00001562
Iteration 221/1000 | Loss: 0.00001562
Iteration 222/1000 | Loss: 0.00001562
Iteration 223/1000 | Loss: 0.00001562
Iteration 224/1000 | Loss: 0.00001562
Iteration 225/1000 | Loss: 0.00001561
Iteration 226/1000 | Loss: 0.00001561
Iteration 227/1000 | Loss: 0.00001561
Iteration 228/1000 | Loss: 0.00001561
Iteration 229/1000 | Loss: 0.00001561
Iteration 230/1000 | Loss: 0.00001561
Iteration 231/1000 | Loss: 0.00001561
Iteration 232/1000 | Loss: 0.00001561
Iteration 233/1000 | Loss: 0.00001561
Iteration 234/1000 | Loss: 0.00001560
Iteration 235/1000 | Loss: 0.00001560
Iteration 236/1000 | Loss: 0.00001559
Iteration 237/1000 | Loss: 0.00001559
Iteration 238/1000 | Loss: 0.00001559
Iteration 239/1000 | Loss: 0.00001558
Iteration 240/1000 | Loss: 0.00001558
Iteration 241/1000 | Loss: 0.00001558
Iteration 242/1000 | Loss: 0.00001557
Iteration 243/1000 | Loss: 0.00001557
Iteration 244/1000 | Loss: 0.00001557
Iteration 245/1000 | Loss: 0.00001557
Iteration 246/1000 | Loss: 0.00001557
Iteration 247/1000 | Loss: 0.00001557
Iteration 248/1000 | Loss: 0.00001557
Iteration 249/1000 | Loss: 0.00001557
Iteration 250/1000 | Loss: 0.00001557
Iteration 251/1000 | Loss: 0.00001556
Iteration 252/1000 | Loss: 0.00001556
Iteration 253/1000 | Loss: 0.00001556
Iteration 254/1000 | Loss: 0.00001556
Iteration 255/1000 | Loss: 0.00001556
Iteration 256/1000 | Loss: 0.00001556
Iteration 257/1000 | Loss: 0.00001556
Iteration 258/1000 | Loss: 0.00001556
Iteration 259/1000 | Loss: 0.00001556
Iteration 260/1000 | Loss: 0.00001556
Iteration 261/1000 | Loss: 0.00001556
Iteration 262/1000 | Loss: 0.00001556
Iteration 263/1000 | Loss: 0.00001556
Iteration 264/1000 | Loss: 0.00001556
Iteration 265/1000 | Loss: 0.00001556
Iteration 266/1000 | Loss: 0.00001556
Iteration 267/1000 | Loss: 0.00001556
Iteration 268/1000 | Loss: 0.00001556
Iteration 269/1000 | Loss: 0.00001556
Iteration 270/1000 | Loss: 0.00001556
Iteration 271/1000 | Loss: 0.00001556
Iteration 272/1000 | Loss: 0.00001556
Iteration 273/1000 | Loss: 0.00001556
Iteration 274/1000 | Loss: 0.00001556
Iteration 275/1000 | Loss: 0.00001556
Iteration 276/1000 | Loss: 0.00001556
Iteration 277/1000 | Loss: 0.00001556
Iteration 278/1000 | Loss: 0.00001556
Iteration 279/1000 | Loss: 0.00001556
Iteration 280/1000 | Loss: 0.00001556
Iteration 281/1000 | Loss: 0.00001556
Iteration 282/1000 | Loss: 0.00001556
Iteration 283/1000 | Loss: 0.00001556
Iteration 284/1000 | Loss: 0.00001556
Iteration 285/1000 | Loss: 0.00001556
Iteration 286/1000 | Loss: 0.00001556
Iteration 287/1000 | Loss: 0.00001556
Iteration 288/1000 | Loss: 0.00001556
Iteration 289/1000 | Loss: 0.00001556
Iteration 290/1000 | Loss: 0.00001556
Iteration 291/1000 | Loss: 0.00001556
Iteration 292/1000 | Loss: 0.00001556
Iteration 293/1000 | Loss: 0.00001556
Iteration 294/1000 | Loss: 0.00001556
Iteration 295/1000 | Loss: 0.00001556
Iteration 296/1000 | Loss: 0.00001556
Iteration 297/1000 | Loss: 0.00001556
Iteration 298/1000 | Loss: 0.00001556
Iteration 299/1000 | Loss: 0.00001556
Iteration 300/1000 | Loss: 0.00001556
Iteration 301/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.555894414195791e-05, 1.555894414195791e-05, 1.555894414195791e-05, 1.555894414195791e-05, 1.555894414195791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555894414195791e-05

Optimization complete. Final v2v error: 3.3685529232025146 mm

Highest mean error: 4.8443498611450195 mm for frame 29

Lowest mean error: 2.9510955810546875 mm for frame 159

Saving results

Total time: 217.18346548080444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806297
Iteration 2/25 | Loss: 0.00122396
Iteration 3/25 | Loss: 0.00106746
Iteration 4/25 | Loss: 0.00081100
Iteration 5/25 | Loss: 0.00077450
Iteration 6/25 | Loss: 0.00074847
Iteration 7/25 | Loss: 0.00074276
Iteration 8/25 | Loss: 0.00074126
Iteration 9/25 | Loss: 0.00074073
Iteration 10/25 | Loss: 0.00074040
Iteration 11/25 | Loss: 0.00074001
Iteration 12/25 | Loss: 0.00073972
Iteration 13/25 | Loss: 0.00073943
Iteration 14/25 | Loss: 0.00073933
Iteration 15/25 | Loss: 0.00073892
Iteration 16/25 | Loss: 0.00074030
Iteration 17/25 | Loss: 0.00073981
Iteration 18/25 | Loss: 0.00073861
Iteration 19/25 | Loss: 0.00073759
Iteration 20/25 | Loss: 0.00073728
Iteration 21/25 | Loss: 0.00073934
Iteration 22/25 | Loss: 0.00073883
Iteration 23/25 | Loss: 0.00073728
Iteration 24/25 | Loss: 0.00073668
Iteration 25/25 | Loss: 0.00073652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39210129
Iteration 2/25 | Loss: 0.00087770
Iteration 3/25 | Loss: 0.00087770
Iteration 4/25 | Loss: 0.00087770
Iteration 5/25 | Loss: 0.00087770
Iteration 6/25 | Loss: 0.00087770
Iteration 7/25 | Loss: 0.00087770
Iteration 8/25 | Loss: 0.00087770
Iteration 9/25 | Loss: 0.00087770
Iteration 10/25 | Loss: 0.00087770
Iteration 11/25 | Loss: 0.00087770
Iteration 12/25 | Loss: 0.00087770
Iteration 13/25 | Loss: 0.00087770
Iteration 14/25 | Loss: 0.00087770
Iteration 15/25 | Loss: 0.00087770
Iteration 16/25 | Loss: 0.00087770
Iteration 17/25 | Loss: 0.00087770
Iteration 18/25 | Loss: 0.00087770
Iteration 19/25 | Loss: 0.00087770
Iteration 20/25 | Loss: 0.00087770
Iteration 21/25 | Loss: 0.00087770
Iteration 22/25 | Loss: 0.00087770
Iteration 23/25 | Loss: 0.00087770
Iteration 24/25 | Loss: 0.00087770
Iteration 25/25 | Loss: 0.00087770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087770
Iteration 2/1000 | Loss: 0.00005117
Iteration 3/1000 | Loss: 0.00169764
Iteration 4/1000 | Loss: 0.00186513
Iteration 5/1000 | Loss: 0.00003050
Iteration 6/1000 | Loss: 0.00002693
Iteration 7/1000 | Loss: 0.00170506
Iteration 8/1000 | Loss: 0.00068998
Iteration 9/1000 | Loss: 0.00096541
Iteration 10/1000 | Loss: 0.00010595
Iteration 11/1000 | Loss: 0.00064071
Iteration 12/1000 | Loss: 0.00005716
Iteration 13/1000 | Loss: 0.00003731
Iteration 14/1000 | Loss: 0.00003216
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00004751
Iteration 17/1000 | Loss: 0.00029613
Iteration 18/1000 | Loss: 0.00020231
Iteration 19/1000 | Loss: 0.00031976
Iteration 20/1000 | Loss: 0.00020391
Iteration 21/1000 | Loss: 0.00002846
Iteration 22/1000 | Loss: 0.00024031
Iteration 23/1000 | Loss: 0.00024405
Iteration 24/1000 | Loss: 0.00022477
Iteration 25/1000 | Loss: 0.00025333
Iteration 26/1000 | Loss: 0.00030975
Iteration 27/1000 | Loss: 0.00003090
Iteration 28/1000 | Loss: 0.00002595
Iteration 29/1000 | Loss: 0.00002433
Iteration 30/1000 | Loss: 0.00002297
Iteration 31/1000 | Loss: 0.00002230
Iteration 32/1000 | Loss: 0.00002187
Iteration 33/1000 | Loss: 0.00002161
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002157
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002148
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002146
Iteration 40/1000 | Loss: 0.00002146
Iteration 41/1000 | Loss: 0.00002145
Iteration 42/1000 | Loss: 0.00002145
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002144
Iteration 45/1000 | Loss: 0.00002144
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002143
Iteration 48/1000 | Loss: 0.00002143
Iteration 49/1000 | Loss: 0.00002143
Iteration 50/1000 | Loss: 0.00002143
Iteration 51/1000 | Loss: 0.00002143
Iteration 52/1000 | Loss: 0.00002143
Iteration 53/1000 | Loss: 0.00002143
Iteration 54/1000 | Loss: 0.00002143
Iteration 55/1000 | Loss: 0.00002143
Iteration 56/1000 | Loss: 0.00002143
Iteration 57/1000 | Loss: 0.00002143
Iteration 58/1000 | Loss: 0.00002143
Iteration 59/1000 | Loss: 0.00002143
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002143
Iteration 63/1000 | Loss: 0.00002143
Iteration 64/1000 | Loss: 0.00002143
Iteration 65/1000 | Loss: 0.00002143
Iteration 66/1000 | Loss: 0.00002143
Iteration 67/1000 | Loss: 0.00002143
Iteration 68/1000 | Loss: 0.00002143
Iteration 69/1000 | Loss: 0.00002143
Iteration 70/1000 | Loss: 0.00002143
Iteration 71/1000 | Loss: 0.00002143
Iteration 72/1000 | Loss: 0.00002143
Iteration 73/1000 | Loss: 0.00002143
Iteration 74/1000 | Loss: 0.00002143
Iteration 75/1000 | Loss: 0.00002143
Iteration 76/1000 | Loss: 0.00002143
Iteration 77/1000 | Loss: 0.00002143
Iteration 78/1000 | Loss: 0.00002143
Iteration 79/1000 | Loss: 0.00002143
Iteration 80/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.14251358556794e-05, 2.14251358556794e-05, 2.14251358556794e-05, 2.14251358556794e-05, 2.14251358556794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.14251358556794e-05

Optimization complete. Final v2v error: 3.791599750518799 mm

Highest mean error: 14.280155181884766 mm for frame 2

Lowest mean error: 2.951143741607666 mm for frame 166

Saving results

Total time: 108.19394135475159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117723
Iteration 2/25 | Loss: 0.00628175
Iteration 3/25 | Loss: 0.00358459
Iteration 4/25 | Loss: 0.00304408
Iteration 5/25 | Loss: 0.00254487
Iteration 6/25 | Loss: 0.00208313
Iteration 7/25 | Loss: 0.00169188
Iteration 8/25 | Loss: 0.00141676
Iteration 9/25 | Loss: 0.00123857
Iteration 10/25 | Loss: 0.00115090
Iteration 11/25 | Loss: 0.00108622
Iteration 12/25 | Loss: 0.00106321
Iteration 13/25 | Loss: 0.00103357
Iteration 14/25 | Loss: 0.00100928
Iteration 15/25 | Loss: 0.00100111
Iteration 16/25 | Loss: 0.00099602
Iteration 17/25 | Loss: 0.00099479
Iteration 18/25 | Loss: 0.00099432
Iteration 19/25 | Loss: 0.00099418
Iteration 20/25 | Loss: 0.00099412
Iteration 21/25 | Loss: 0.00099411
Iteration 22/25 | Loss: 0.00099411
Iteration 23/25 | Loss: 0.00099411
Iteration 24/25 | Loss: 0.00099411
Iteration 25/25 | Loss: 0.00099411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50240856
Iteration 2/25 | Loss: 0.00109513
Iteration 3/25 | Loss: 0.00109513
Iteration 4/25 | Loss: 0.00109513
Iteration 5/25 | Loss: 0.00109513
Iteration 6/25 | Loss: 0.00109513
Iteration 7/25 | Loss: 0.00109513
Iteration 8/25 | Loss: 0.00109513
Iteration 9/25 | Loss: 0.00109513
Iteration 10/25 | Loss: 0.00109513
Iteration 11/25 | Loss: 0.00109513
Iteration 12/25 | Loss: 0.00109513
Iteration 13/25 | Loss: 0.00109513
Iteration 14/25 | Loss: 0.00109513
Iteration 15/25 | Loss: 0.00109513
Iteration 16/25 | Loss: 0.00109513
Iteration 17/25 | Loss: 0.00109513
Iteration 18/25 | Loss: 0.00109513
Iteration 19/25 | Loss: 0.00109513
Iteration 20/25 | Loss: 0.00109513
Iteration 21/25 | Loss: 0.00109513
Iteration 22/25 | Loss: 0.00109513
Iteration 23/25 | Loss: 0.00109513
Iteration 24/25 | Loss: 0.00109513
Iteration 25/25 | Loss: 0.00109513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109513
Iteration 2/1000 | Loss: 0.00018949
Iteration 3/1000 | Loss: 0.00013774
Iteration 4/1000 | Loss: 0.00011844
Iteration 5/1000 | Loss: 0.00011036
Iteration 6/1000 | Loss: 0.00010580
Iteration 7/1000 | Loss: 0.00010131
Iteration 8/1000 | Loss: 0.00009852
Iteration 9/1000 | Loss: 0.01478362
Iteration 10/1000 | Loss: 0.00013217
Iteration 11/1000 | Loss: 0.00008102
Iteration 12/1000 | Loss: 0.00009123
Iteration 13/1000 | Loss: 0.00008497
Iteration 14/1000 | Loss: 0.00007500
Iteration 15/1000 | Loss: 0.00016979
Iteration 16/1000 | Loss: 0.00006349
Iteration 17/1000 | Loss: 0.00008142
Iteration 18/1000 | Loss: 0.00008348
Iteration 19/1000 | Loss: 0.00008746
Iteration 20/1000 | Loss: 0.00004564
Iteration 21/1000 | Loss: 0.00003115
Iteration 22/1000 | Loss: 0.00002868
Iteration 23/1000 | Loss: 0.00002710
Iteration 24/1000 | Loss: 0.00002639
Iteration 25/1000 | Loss: 0.00002583
Iteration 26/1000 | Loss: 0.00002546
Iteration 27/1000 | Loss: 0.00002508
Iteration 28/1000 | Loss: 0.00002488
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002473
Iteration 32/1000 | Loss: 0.00002471
Iteration 33/1000 | Loss: 0.00002470
Iteration 34/1000 | Loss: 0.00002470
Iteration 35/1000 | Loss: 0.00002469
Iteration 36/1000 | Loss: 0.00002469
Iteration 37/1000 | Loss: 0.00002468
Iteration 38/1000 | Loss: 0.00002467
Iteration 39/1000 | Loss: 0.00002467
Iteration 40/1000 | Loss: 0.00002467
Iteration 41/1000 | Loss: 0.00002465
Iteration 42/1000 | Loss: 0.00002463
Iteration 43/1000 | Loss: 0.00002463
Iteration 44/1000 | Loss: 0.00002462
Iteration 45/1000 | Loss: 0.00002462
Iteration 46/1000 | Loss: 0.00002462
Iteration 47/1000 | Loss: 0.00002462
Iteration 48/1000 | Loss: 0.00002462
Iteration 49/1000 | Loss: 0.00002461
Iteration 50/1000 | Loss: 0.00002459
Iteration 51/1000 | Loss: 0.00002459
Iteration 52/1000 | Loss: 0.00002459
Iteration 53/1000 | Loss: 0.00002459
Iteration 54/1000 | Loss: 0.00002458
Iteration 55/1000 | Loss: 0.00002458
Iteration 56/1000 | Loss: 0.00002458
Iteration 57/1000 | Loss: 0.00002458
Iteration 58/1000 | Loss: 0.00002458
Iteration 59/1000 | Loss: 0.00002457
Iteration 60/1000 | Loss: 0.00002457
Iteration 61/1000 | Loss: 0.00002457
Iteration 62/1000 | Loss: 0.00002457
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00002456
Iteration 65/1000 | Loss: 0.00002456
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002456
Iteration 69/1000 | Loss: 0.00002455
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002454
Iteration 74/1000 | Loss: 0.00002454
Iteration 75/1000 | Loss: 0.00002453
Iteration 76/1000 | Loss: 0.00002453
Iteration 77/1000 | Loss: 0.00002453
Iteration 78/1000 | Loss: 0.00002452
Iteration 79/1000 | Loss: 0.00002452
Iteration 80/1000 | Loss: 0.00002452
Iteration 81/1000 | Loss: 0.00002452
Iteration 82/1000 | Loss: 0.00002452
Iteration 83/1000 | Loss: 0.00002452
Iteration 84/1000 | Loss: 0.00002452
Iteration 85/1000 | Loss: 0.00002451
Iteration 86/1000 | Loss: 0.00002451
Iteration 87/1000 | Loss: 0.00002451
Iteration 88/1000 | Loss: 0.00002451
Iteration 89/1000 | Loss: 0.00002451
Iteration 90/1000 | Loss: 0.00002451
Iteration 91/1000 | Loss: 0.00002451
Iteration 92/1000 | Loss: 0.00002451
Iteration 93/1000 | Loss: 0.00002450
Iteration 94/1000 | Loss: 0.00002450
Iteration 95/1000 | Loss: 0.00002450
Iteration 96/1000 | Loss: 0.00002450
Iteration 97/1000 | Loss: 0.00002450
Iteration 98/1000 | Loss: 0.00002450
Iteration 99/1000 | Loss: 0.00002450
Iteration 100/1000 | Loss: 0.00002450
Iteration 101/1000 | Loss: 0.00002450
Iteration 102/1000 | Loss: 0.00002450
Iteration 103/1000 | Loss: 0.00002450
Iteration 104/1000 | Loss: 0.00002450
Iteration 105/1000 | Loss: 0.00002450
Iteration 106/1000 | Loss: 0.00002449
Iteration 107/1000 | Loss: 0.00002449
Iteration 108/1000 | Loss: 0.00002449
Iteration 109/1000 | Loss: 0.00002449
Iteration 110/1000 | Loss: 0.00002449
Iteration 111/1000 | Loss: 0.00002449
Iteration 112/1000 | Loss: 0.00002449
Iteration 113/1000 | Loss: 0.00002449
Iteration 114/1000 | Loss: 0.00002449
Iteration 115/1000 | Loss: 0.00002449
Iteration 116/1000 | Loss: 0.00002449
Iteration 117/1000 | Loss: 0.00002449
Iteration 118/1000 | Loss: 0.00002449
Iteration 119/1000 | Loss: 0.00002449
Iteration 120/1000 | Loss: 0.00002449
Iteration 121/1000 | Loss: 0.00002449
Iteration 122/1000 | Loss: 0.00002449
Iteration 123/1000 | Loss: 0.00002449
Iteration 124/1000 | Loss: 0.00002449
Iteration 125/1000 | Loss: 0.00002449
Iteration 126/1000 | Loss: 0.00002449
Iteration 127/1000 | Loss: 0.00002449
Iteration 128/1000 | Loss: 0.00002449
Iteration 129/1000 | Loss: 0.00002449
Iteration 130/1000 | Loss: 0.00002449
Iteration 131/1000 | Loss: 0.00002449
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002449
Iteration 134/1000 | Loss: 0.00002449
Iteration 135/1000 | Loss: 0.00002449
Iteration 136/1000 | Loss: 0.00002449
Iteration 137/1000 | Loss: 0.00002449
Iteration 138/1000 | Loss: 0.00002449
Iteration 139/1000 | Loss: 0.00002449
Iteration 140/1000 | Loss: 0.00002449
Iteration 141/1000 | Loss: 0.00002449
Iteration 142/1000 | Loss: 0.00002449
Iteration 143/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.448582927172538e-05, 2.448582927172538e-05, 2.448582927172538e-05, 2.448582927172538e-05, 2.448582927172538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.448582927172538e-05

Optimization complete. Final v2v error: 4.2758893966674805 mm

Highest mean error: 8.50549030303955 mm for frame 29

Lowest mean error: 3.87213397026062 mm for frame 239

Saving results

Total time: 92.8837959766388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643281
Iteration 2/25 | Loss: 0.00126601
Iteration 3/25 | Loss: 0.00084401
Iteration 4/25 | Loss: 0.00080183
Iteration 5/25 | Loss: 0.00079275
Iteration 6/25 | Loss: 0.00078922
Iteration 7/25 | Loss: 0.00078873
Iteration 8/25 | Loss: 0.00078873
Iteration 9/25 | Loss: 0.00078873
Iteration 10/25 | Loss: 0.00078873
Iteration 11/25 | Loss: 0.00078873
Iteration 12/25 | Loss: 0.00078873
Iteration 13/25 | Loss: 0.00078873
Iteration 14/25 | Loss: 0.00078873
Iteration 15/25 | Loss: 0.00078873
Iteration 16/25 | Loss: 0.00078873
Iteration 17/25 | Loss: 0.00078873
Iteration 18/25 | Loss: 0.00078873
Iteration 19/25 | Loss: 0.00078873
Iteration 20/25 | Loss: 0.00078873
Iteration 21/25 | Loss: 0.00078873
Iteration 22/25 | Loss: 0.00078873
Iteration 23/25 | Loss: 0.00078873
Iteration 24/25 | Loss: 0.00078873
Iteration 25/25 | Loss: 0.00078873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27830398
Iteration 2/25 | Loss: 0.00059775
Iteration 3/25 | Loss: 0.00059774
Iteration 4/25 | Loss: 0.00059774
Iteration 5/25 | Loss: 0.00059774
Iteration 6/25 | Loss: 0.00059774
Iteration 7/25 | Loss: 0.00059774
Iteration 8/25 | Loss: 0.00059774
Iteration 9/25 | Loss: 0.00059774
Iteration 10/25 | Loss: 0.00059774
Iteration 11/25 | Loss: 0.00059774
Iteration 12/25 | Loss: 0.00059774
Iteration 13/25 | Loss: 0.00059774
Iteration 14/25 | Loss: 0.00059774
Iteration 15/25 | Loss: 0.00059774
Iteration 16/25 | Loss: 0.00059774
Iteration 17/25 | Loss: 0.00059774
Iteration 18/25 | Loss: 0.00059774
Iteration 19/25 | Loss: 0.00059774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005977426189929247, 0.0005977426189929247, 0.0005977426189929247, 0.0005977426189929247, 0.0005977426189929247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005977426189929247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059774
Iteration 2/1000 | Loss: 0.00003500
Iteration 3/1000 | Loss: 0.00002828
Iteration 4/1000 | Loss: 0.00002678
Iteration 5/1000 | Loss: 0.00002605
Iteration 6/1000 | Loss: 0.00002536
Iteration 7/1000 | Loss: 0.00002472
Iteration 8/1000 | Loss: 0.00002425
Iteration 9/1000 | Loss: 0.00002395
Iteration 10/1000 | Loss: 0.00002377
Iteration 11/1000 | Loss: 0.00002359
Iteration 12/1000 | Loss: 0.00002344
Iteration 13/1000 | Loss: 0.00002328
Iteration 14/1000 | Loss: 0.00002323
Iteration 15/1000 | Loss: 0.00002323
Iteration 16/1000 | Loss: 0.00002321
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002319
Iteration 20/1000 | Loss: 0.00002318
Iteration 21/1000 | Loss: 0.00002317
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002315
Iteration 25/1000 | Loss: 0.00002314
Iteration 26/1000 | Loss: 0.00002313
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002311
Iteration 29/1000 | Loss: 0.00002310
Iteration 30/1000 | Loss: 0.00002308
Iteration 31/1000 | Loss: 0.00002308
Iteration 32/1000 | Loss: 0.00002308
Iteration 33/1000 | Loss: 0.00002307
Iteration 34/1000 | Loss: 0.00002307
Iteration 35/1000 | Loss: 0.00002303
Iteration 36/1000 | Loss: 0.00002303
Iteration 37/1000 | Loss: 0.00002302
Iteration 38/1000 | Loss: 0.00002302
Iteration 39/1000 | Loss: 0.00002302
Iteration 40/1000 | Loss: 0.00002301
Iteration 41/1000 | Loss: 0.00002301
Iteration 42/1000 | Loss: 0.00002301
Iteration 43/1000 | Loss: 0.00002300
Iteration 44/1000 | Loss: 0.00002300
Iteration 45/1000 | Loss: 0.00002300
Iteration 46/1000 | Loss: 0.00002300
Iteration 47/1000 | Loss: 0.00002299
Iteration 48/1000 | Loss: 0.00002299
Iteration 49/1000 | Loss: 0.00002299
Iteration 50/1000 | Loss: 0.00002298
Iteration 51/1000 | Loss: 0.00002298
Iteration 52/1000 | Loss: 0.00002297
Iteration 53/1000 | Loss: 0.00002297
Iteration 54/1000 | Loss: 0.00002297
Iteration 55/1000 | Loss: 0.00002297
Iteration 56/1000 | Loss: 0.00002296
Iteration 57/1000 | Loss: 0.00002296
Iteration 58/1000 | Loss: 0.00002296
Iteration 59/1000 | Loss: 0.00002296
Iteration 60/1000 | Loss: 0.00002296
Iteration 61/1000 | Loss: 0.00002296
Iteration 62/1000 | Loss: 0.00002296
Iteration 63/1000 | Loss: 0.00002296
Iteration 64/1000 | Loss: 0.00002296
Iteration 65/1000 | Loss: 0.00002296
Iteration 66/1000 | Loss: 0.00002296
Iteration 67/1000 | Loss: 0.00002296
Iteration 68/1000 | Loss: 0.00002296
Iteration 69/1000 | Loss: 0.00002295
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002295
Iteration 72/1000 | Loss: 0.00002295
Iteration 73/1000 | Loss: 0.00002294
Iteration 74/1000 | Loss: 0.00002294
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002294
Iteration 78/1000 | Loss: 0.00002294
Iteration 79/1000 | Loss: 0.00002294
Iteration 80/1000 | Loss: 0.00002293
Iteration 81/1000 | Loss: 0.00002293
Iteration 82/1000 | Loss: 0.00002293
Iteration 83/1000 | Loss: 0.00002293
Iteration 84/1000 | Loss: 0.00002293
Iteration 85/1000 | Loss: 0.00002293
Iteration 86/1000 | Loss: 0.00002293
Iteration 87/1000 | Loss: 0.00002293
Iteration 88/1000 | Loss: 0.00002293
Iteration 89/1000 | Loss: 0.00002293
Iteration 90/1000 | Loss: 0.00002293
Iteration 91/1000 | Loss: 0.00002293
Iteration 92/1000 | Loss: 0.00002293
Iteration 93/1000 | Loss: 0.00002293
Iteration 94/1000 | Loss: 0.00002293
Iteration 95/1000 | Loss: 0.00002293
Iteration 96/1000 | Loss: 0.00002293
Iteration 97/1000 | Loss: 0.00002293
Iteration 98/1000 | Loss: 0.00002293
Iteration 99/1000 | Loss: 0.00002293
Iteration 100/1000 | Loss: 0.00002293
Iteration 101/1000 | Loss: 0.00002293
Iteration 102/1000 | Loss: 0.00002293
Iteration 103/1000 | Loss: 0.00002293
Iteration 104/1000 | Loss: 0.00002293
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002293
Iteration 109/1000 | Loss: 0.00002293
Iteration 110/1000 | Loss: 0.00002293
Iteration 111/1000 | Loss: 0.00002293
Iteration 112/1000 | Loss: 0.00002293
Iteration 113/1000 | Loss: 0.00002293
Iteration 114/1000 | Loss: 0.00002293
Iteration 115/1000 | Loss: 0.00002293
Iteration 116/1000 | Loss: 0.00002293
Iteration 117/1000 | Loss: 0.00002293
Iteration 118/1000 | Loss: 0.00002293
Iteration 119/1000 | Loss: 0.00002293
Iteration 120/1000 | Loss: 0.00002293
Iteration 121/1000 | Loss: 0.00002293
Iteration 122/1000 | Loss: 0.00002293
Iteration 123/1000 | Loss: 0.00002293
Iteration 124/1000 | Loss: 0.00002293
Iteration 125/1000 | Loss: 0.00002293
Iteration 126/1000 | Loss: 0.00002293
Iteration 127/1000 | Loss: 0.00002293
Iteration 128/1000 | Loss: 0.00002293
Iteration 129/1000 | Loss: 0.00002293
Iteration 130/1000 | Loss: 0.00002293
Iteration 131/1000 | Loss: 0.00002293
Iteration 132/1000 | Loss: 0.00002293
Iteration 133/1000 | Loss: 0.00002293
Iteration 134/1000 | Loss: 0.00002293
Iteration 135/1000 | Loss: 0.00002293
Iteration 136/1000 | Loss: 0.00002293
Iteration 137/1000 | Loss: 0.00002293
Iteration 138/1000 | Loss: 0.00002293
Iteration 139/1000 | Loss: 0.00002293
Iteration 140/1000 | Loss: 0.00002293
Iteration 141/1000 | Loss: 0.00002293
Iteration 142/1000 | Loss: 0.00002293
Iteration 143/1000 | Loss: 0.00002293
Iteration 144/1000 | Loss: 0.00002293
Iteration 145/1000 | Loss: 0.00002293
Iteration 146/1000 | Loss: 0.00002293
Iteration 147/1000 | Loss: 0.00002293
Iteration 148/1000 | Loss: 0.00002293
Iteration 149/1000 | Loss: 0.00002293
Iteration 150/1000 | Loss: 0.00002293
Iteration 151/1000 | Loss: 0.00002293
Iteration 152/1000 | Loss: 0.00002293
Iteration 153/1000 | Loss: 0.00002293
Iteration 154/1000 | Loss: 0.00002293
Iteration 155/1000 | Loss: 0.00002293
Iteration 156/1000 | Loss: 0.00002293
Iteration 157/1000 | Loss: 0.00002293
Iteration 158/1000 | Loss: 0.00002293
Iteration 159/1000 | Loss: 0.00002293
Iteration 160/1000 | Loss: 0.00002293
Iteration 161/1000 | Loss: 0.00002293
Iteration 162/1000 | Loss: 0.00002293
Iteration 163/1000 | Loss: 0.00002293
Iteration 164/1000 | Loss: 0.00002293
Iteration 165/1000 | Loss: 0.00002293
Iteration 166/1000 | Loss: 0.00002293
Iteration 167/1000 | Loss: 0.00002293
Iteration 168/1000 | Loss: 0.00002293
Iteration 169/1000 | Loss: 0.00002293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.2928390535525978e-05, 2.2928390535525978e-05, 2.2928390535525978e-05, 2.2928390535525978e-05, 2.2928390535525978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2928390535525978e-05

Optimization complete. Final v2v error: 4.010303974151611 mm

Highest mean error: 4.393850803375244 mm for frame 205

Lowest mean error: 3.4751780033111572 mm for frame 104

Saving results

Total time: 41.32579684257507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455079
Iteration 2/25 | Loss: 0.00090531
Iteration 3/25 | Loss: 0.00071304
Iteration 4/25 | Loss: 0.00069201
Iteration 5/25 | Loss: 0.00068635
Iteration 6/25 | Loss: 0.00068478
Iteration 7/25 | Loss: 0.00068453
Iteration 8/25 | Loss: 0.00068453
Iteration 9/25 | Loss: 0.00068453
Iteration 10/25 | Loss: 0.00068453
Iteration 11/25 | Loss: 0.00068453
Iteration 12/25 | Loss: 0.00068453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006845264579169452, 0.0006845264579169452, 0.0006845264579169452, 0.0006845264579169452, 0.0006845264579169452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006845264579169452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.00469065
Iteration 2/25 | Loss: 0.00069790
Iteration 3/25 | Loss: 0.00069789
Iteration 4/25 | Loss: 0.00069789
Iteration 5/25 | Loss: 0.00069789
Iteration 6/25 | Loss: 0.00069789
Iteration 7/25 | Loss: 0.00069789
Iteration 8/25 | Loss: 0.00069789
Iteration 9/25 | Loss: 0.00069789
Iteration 10/25 | Loss: 0.00069789
Iteration 11/25 | Loss: 0.00069789
Iteration 12/25 | Loss: 0.00069789
Iteration 13/25 | Loss: 0.00069789
Iteration 14/25 | Loss: 0.00069789
Iteration 15/25 | Loss: 0.00069789
Iteration 16/25 | Loss: 0.00069789
Iteration 17/25 | Loss: 0.00069789
Iteration 18/25 | Loss: 0.00069789
Iteration 19/25 | Loss: 0.00069789
Iteration 20/25 | Loss: 0.00069789
Iteration 21/25 | Loss: 0.00069789
Iteration 22/25 | Loss: 0.00069789
Iteration 23/25 | Loss: 0.00069789
Iteration 24/25 | Loss: 0.00069789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006978861638344824, 0.0006978861638344824, 0.0006978861638344824, 0.0006978861638344824, 0.0006978861638344824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006978861638344824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069789
Iteration 2/1000 | Loss: 0.00003515
Iteration 3/1000 | Loss: 0.00002732
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002117
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001817
Iteration 11/1000 | Loss: 0.00001801
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001781
Iteration 16/1000 | Loss: 0.00001777
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001768
Iteration 19/1000 | Loss: 0.00001767
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001755
Iteration 22/1000 | Loss: 0.00001755
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001745
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001744
Iteration 30/1000 | Loss: 0.00001744
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001742
Iteration 36/1000 | Loss: 0.00001741
Iteration 37/1000 | Loss: 0.00001741
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001739
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00001739
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001738
Iteration 48/1000 | Loss: 0.00001738
Iteration 49/1000 | Loss: 0.00001738
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001735
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001734
Iteration 78/1000 | Loss: 0.00001734
Iteration 79/1000 | Loss: 0.00001734
Iteration 80/1000 | Loss: 0.00001734
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001733
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001731
Iteration 94/1000 | Loss: 0.00001731
Iteration 95/1000 | Loss: 0.00001731
Iteration 96/1000 | Loss: 0.00001731
Iteration 97/1000 | Loss: 0.00001731
Iteration 98/1000 | Loss: 0.00001731
Iteration 99/1000 | Loss: 0.00001731
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001730
Iteration 105/1000 | Loss: 0.00001730
Iteration 106/1000 | Loss: 0.00001730
Iteration 107/1000 | Loss: 0.00001730
Iteration 108/1000 | Loss: 0.00001730
Iteration 109/1000 | Loss: 0.00001730
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001730
Iteration 113/1000 | Loss: 0.00001730
Iteration 114/1000 | Loss: 0.00001730
Iteration 115/1000 | Loss: 0.00001730
Iteration 116/1000 | Loss: 0.00001729
Iteration 117/1000 | Loss: 0.00001729
Iteration 118/1000 | Loss: 0.00001729
Iteration 119/1000 | Loss: 0.00001729
Iteration 120/1000 | Loss: 0.00001729
Iteration 121/1000 | Loss: 0.00001729
Iteration 122/1000 | Loss: 0.00001729
Iteration 123/1000 | Loss: 0.00001729
Iteration 124/1000 | Loss: 0.00001729
Iteration 125/1000 | Loss: 0.00001728
Iteration 126/1000 | Loss: 0.00001728
Iteration 127/1000 | Loss: 0.00001728
Iteration 128/1000 | Loss: 0.00001728
Iteration 129/1000 | Loss: 0.00001728
Iteration 130/1000 | Loss: 0.00001728
Iteration 131/1000 | Loss: 0.00001728
Iteration 132/1000 | Loss: 0.00001728
Iteration 133/1000 | Loss: 0.00001728
Iteration 134/1000 | Loss: 0.00001727
Iteration 135/1000 | Loss: 0.00001727
Iteration 136/1000 | Loss: 0.00001727
Iteration 137/1000 | Loss: 0.00001727
Iteration 138/1000 | Loss: 0.00001727
Iteration 139/1000 | Loss: 0.00001727
Iteration 140/1000 | Loss: 0.00001727
Iteration 141/1000 | Loss: 0.00001727
Iteration 142/1000 | Loss: 0.00001727
Iteration 143/1000 | Loss: 0.00001727
Iteration 144/1000 | Loss: 0.00001727
Iteration 145/1000 | Loss: 0.00001727
Iteration 146/1000 | Loss: 0.00001727
Iteration 147/1000 | Loss: 0.00001727
Iteration 148/1000 | Loss: 0.00001726
Iteration 149/1000 | Loss: 0.00001726
Iteration 150/1000 | Loss: 0.00001726
Iteration 151/1000 | Loss: 0.00001726
Iteration 152/1000 | Loss: 0.00001726
Iteration 153/1000 | Loss: 0.00001726
Iteration 154/1000 | Loss: 0.00001726
Iteration 155/1000 | Loss: 0.00001726
Iteration 156/1000 | Loss: 0.00001726
Iteration 157/1000 | Loss: 0.00001726
Iteration 158/1000 | Loss: 0.00001725
Iteration 159/1000 | Loss: 0.00001725
Iteration 160/1000 | Loss: 0.00001725
Iteration 161/1000 | Loss: 0.00001725
Iteration 162/1000 | Loss: 0.00001725
Iteration 163/1000 | Loss: 0.00001725
Iteration 164/1000 | Loss: 0.00001725
Iteration 165/1000 | Loss: 0.00001725
Iteration 166/1000 | Loss: 0.00001725
Iteration 167/1000 | Loss: 0.00001725
Iteration 168/1000 | Loss: 0.00001725
Iteration 169/1000 | Loss: 0.00001725
Iteration 170/1000 | Loss: 0.00001725
Iteration 171/1000 | Loss: 0.00001725
Iteration 172/1000 | Loss: 0.00001725
Iteration 173/1000 | Loss: 0.00001725
Iteration 174/1000 | Loss: 0.00001725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.725071888358798e-05, 1.725071888358798e-05, 1.725071888358798e-05, 1.725071888358798e-05, 1.725071888358798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.725071888358798e-05

Optimization complete. Final v2v error: 3.62282657623291 mm

Highest mean error: 4.1491289138793945 mm for frame 107

Lowest mean error: 2.9549736976623535 mm for frame 3

Saving results

Total time: 39.785513401031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525350
Iteration 2/25 | Loss: 0.00104353
Iteration 3/25 | Loss: 0.00090233
Iteration 4/25 | Loss: 0.00088479
Iteration 5/25 | Loss: 0.00087871
Iteration 6/25 | Loss: 0.00087752
Iteration 7/25 | Loss: 0.00087752
Iteration 8/25 | Loss: 0.00087752
Iteration 9/25 | Loss: 0.00087752
Iteration 10/25 | Loss: 0.00087752
Iteration 11/25 | Loss: 0.00087752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008775172755122185, 0.0008775172755122185, 0.0008775172755122185, 0.0008775172755122185, 0.0008775172755122185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008775172755122185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67909443
Iteration 2/25 | Loss: 0.00062817
Iteration 3/25 | Loss: 0.00062816
Iteration 4/25 | Loss: 0.00062816
Iteration 5/25 | Loss: 0.00062816
Iteration 6/25 | Loss: 0.00062816
Iteration 7/25 | Loss: 0.00062816
Iteration 8/25 | Loss: 0.00062816
Iteration 9/25 | Loss: 0.00062816
Iteration 10/25 | Loss: 0.00062816
Iteration 11/25 | Loss: 0.00062816
Iteration 12/25 | Loss: 0.00062816
Iteration 13/25 | Loss: 0.00062816
Iteration 14/25 | Loss: 0.00062816
Iteration 15/25 | Loss: 0.00062816
Iteration 16/25 | Loss: 0.00062816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006281607202254236, 0.0006281607202254236, 0.0006281607202254236, 0.0006281607202254236, 0.0006281607202254236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006281607202254236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062816
Iteration 2/1000 | Loss: 0.00004550
Iteration 3/1000 | Loss: 0.00003792
Iteration 4/1000 | Loss: 0.00003564
Iteration 5/1000 | Loss: 0.00003398
Iteration 6/1000 | Loss: 0.00003300
Iteration 7/1000 | Loss: 0.00003226
Iteration 8/1000 | Loss: 0.00003175
Iteration 9/1000 | Loss: 0.00003157
Iteration 10/1000 | Loss: 0.00003136
Iteration 11/1000 | Loss: 0.00003118
Iteration 12/1000 | Loss: 0.00003098
Iteration 13/1000 | Loss: 0.00003081
Iteration 14/1000 | Loss: 0.00003066
Iteration 15/1000 | Loss: 0.00003063
Iteration 16/1000 | Loss: 0.00003063
Iteration 17/1000 | Loss: 0.00003063
Iteration 18/1000 | Loss: 0.00003062
Iteration 19/1000 | Loss: 0.00003062
Iteration 20/1000 | Loss: 0.00003062
Iteration 21/1000 | Loss: 0.00003062
Iteration 22/1000 | Loss: 0.00003062
Iteration 23/1000 | Loss: 0.00003061
Iteration 24/1000 | Loss: 0.00003060
Iteration 25/1000 | Loss: 0.00003060
Iteration 26/1000 | Loss: 0.00003060
Iteration 27/1000 | Loss: 0.00003059
Iteration 28/1000 | Loss: 0.00003059
Iteration 29/1000 | Loss: 0.00003059
Iteration 30/1000 | Loss: 0.00003059
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003059
Iteration 33/1000 | Loss: 0.00003058
Iteration 34/1000 | Loss: 0.00003058
Iteration 35/1000 | Loss: 0.00003058
Iteration 36/1000 | Loss: 0.00003057
Iteration 37/1000 | Loss: 0.00003055
Iteration 38/1000 | Loss: 0.00003054
Iteration 39/1000 | Loss: 0.00003052
Iteration 40/1000 | Loss: 0.00003052
Iteration 41/1000 | Loss: 0.00003051
Iteration 42/1000 | Loss: 0.00003051
Iteration 43/1000 | Loss: 0.00003050
Iteration 44/1000 | Loss: 0.00003050
Iteration 45/1000 | Loss: 0.00003050
Iteration 46/1000 | Loss: 0.00003050
Iteration 47/1000 | Loss: 0.00003050
Iteration 48/1000 | Loss: 0.00003049
Iteration 49/1000 | Loss: 0.00003049
Iteration 50/1000 | Loss: 0.00003049
Iteration 51/1000 | Loss: 0.00003046
Iteration 52/1000 | Loss: 0.00003045
Iteration 53/1000 | Loss: 0.00003045
Iteration 54/1000 | Loss: 0.00003045
Iteration 55/1000 | Loss: 0.00003045
Iteration 56/1000 | Loss: 0.00003045
Iteration 57/1000 | Loss: 0.00003045
Iteration 58/1000 | Loss: 0.00003044
Iteration 59/1000 | Loss: 0.00003038
Iteration 60/1000 | Loss: 0.00003038
Iteration 61/1000 | Loss: 0.00003038
Iteration 62/1000 | Loss: 0.00003038
Iteration 63/1000 | Loss: 0.00003038
Iteration 64/1000 | Loss: 0.00003038
Iteration 65/1000 | Loss: 0.00003038
Iteration 66/1000 | Loss: 0.00003037
Iteration 67/1000 | Loss: 0.00003037
Iteration 68/1000 | Loss: 0.00003037
Iteration 69/1000 | Loss: 0.00003036
Iteration 70/1000 | Loss: 0.00003036
Iteration 71/1000 | Loss: 0.00003035
Iteration 72/1000 | Loss: 0.00003035
Iteration 73/1000 | Loss: 0.00003034
Iteration 74/1000 | Loss: 0.00003034
Iteration 75/1000 | Loss: 0.00003034
Iteration 76/1000 | Loss: 0.00003034
Iteration 77/1000 | Loss: 0.00003034
Iteration 78/1000 | Loss: 0.00003034
Iteration 79/1000 | Loss: 0.00003034
Iteration 80/1000 | Loss: 0.00003034
Iteration 81/1000 | Loss: 0.00003034
Iteration 82/1000 | Loss: 0.00003034
Iteration 83/1000 | Loss: 0.00003034
Iteration 84/1000 | Loss: 0.00003034
Iteration 85/1000 | Loss: 0.00003034
Iteration 86/1000 | Loss: 0.00003034
Iteration 87/1000 | Loss: 0.00003034
Iteration 88/1000 | Loss: 0.00003034
Iteration 89/1000 | Loss: 0.00003034
Iteration 90/1000 | Loss: 0.00003034
Iteration 91/1000 | Loss: 0.00003034
Iteration 92/1000 | Loss: 0.00003034
Iteration 93/1000 | Loss: 0.00003034
Iteration 94/1000 | Loss: 0.00003034
Iteration 95/1000 | Loss: 0.00003034
Iteration 96/1000 | Loss: 0.00003034
Iteration 97/1000 | Loss: 0.00003034
Iteration 98/1000 | Loss: 0.00003034
Iteration 99/1000 | Loss: 0.00003034
Iteration 100/1000 | Loss: 0.00003034
Iteration 101/1000 | Loss: 0.00003034
Iteration 102/1000 | Loss: 0.00003034
Iteration 103/1000 | Loss: 0.00003034
Iteration 104/1000 | Loss: 0.00003034
Iteration 105/1000 | Loss: 0.00003034
Iteration 106/1000 | Loss: 0.00003034
Iteration 107/1000 | Loss: 0.00003034
Iteration 108/1000 | Loss: 0.00003034
Iteration 109/1000 | Loss: 0.00003034
Iteration 110/1000 | Loss: 0.00003034
Iteration 111/1000 | Loss: 0.00003034
Iteration 112/1000 | Loss: 0.00003034
Iteration 113/1000 | Loss: 0.00003034
Iteration 114/1000 | Loss: 0.00003034
Iteration 115/1000 | Loss: 0.00003034
Iteration 116/1000 | Loss: 0.00003034
Iteration 117/1000 | Loss: 0.00003034
Iteration 118/1000 | Loss: 0.00003034
Iteration 119/1000 | Loss: 0.00003034
Iteration 120/1000 | Loss: 0.00003034
Iteration 121/1000 | Loss: 0.00003034
Iteration 122/1000 | Loss: 0.00003034
Iteration 123/1000 | Loss: 0.00003034
Iteration 124/1000 | Loss: 0.00003034
Iteration 125/1000 | Loss: 0.00003034
Iteration 126/1000 | Loss: 0.00003034
Iteration 127/1000 | Loss: 0.00003034
Iteration 128/1000 | Loss: 0.00003034
Iteration 129/1000 | Loss: 0.00003034
Iteration 130/1000 | Loss: 0.00003034
Iteration 131/1000 | Loss: 0.00003034
Iteration 132/1000 | Loss: 0.00003034
Iteration 133/1000 | Loss: 0.00003034
Iteration 134/1000 | Loss: 0.00003034
Iteration 135/1000 | Loss: 0.00003034
Iteration 136/1000 | Loss: 0.00003034
Iteration 137/1000 | Loss: 0.00003034
Iteration 138/1000 | Loss: 0.00003034
Iteration 139/1000 | Loss: 0.00003034
Iteration 140/1000 | Loss: 0.00003034
Iteration 141/1000 | Loss: 0.00003034
Iteration 142/1000 | Loss: 0.00003034
Iteration 143/1000 | Loss: 0.00003034
Iteration 144/1000 | Loss: 0.00003034
Iteration 145/1000 | Loss: 0.00003034
Iteration 146/1000 | Loss: 0.00003034
Iteration 147/1000 | Loss: 0.00003034
Iteration 148/1000 | Loss: 0.00003034
Iteration 149/1000 | Loss: 0.00003034
Iteration 150/1000 | Loss: 0.00003034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.0340450393850915e-05, 3.0340450393850915e-05, 3.0340450393850915e-05, 3.0340450393850915e-05, 3.0340450393850915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0340450393850915e-05

Optimization complete. Final v2v error: 4.5012311935424805 mm

Highest mean error: 4.660459041595459 mm for frame 145

Lowest mean error: 4.263839244842529 mm for frame 266

Saving results

Total time: 41.71174740791321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495957
Iteration 2/25 | Loss: 0.00108527
Iteration 3/25 | Loss: 0.00078546
Iteration 4/25 | Loss: 0.00074416
Iteration 5/25 | Loss: 0.00073700
Iteration 6/25 | Loss: 0.00073400
Iteration 7/25 | Loss: 0.00073304
Iteration 8/25 | Loss: 0.00073285
Iteration 9/25 | Loss: 0.00073285
Iteration 10/25 | Loss: 0.00073285
Iteration 11/25 | Loss: 0.00073285
Iteration 12/25 | Loss: 0.00073285
Iteration 13/25 | Loss: 0.00073285
Iteration 14/25 | Loss: 0.00073285
Iteration 15/25 | Loss: 0.00073285
Iteration 16/25 | Loss: 0.00073285
Iteration 17/25 | Loss: 0.00073285
Iteration 18/25 | Loss: 0.00073285
Iteration 19/25 | Loss: 0.00073285
Iteration 20/25 | Loss: 0.00073285
Iteration 21/25 | Loss: 0.00073285
Iteration 22/25 | Loss: 0.00073285
Iteration 23/25 | Loss: 0.00073285
Iteration 24/25 | Loss: 0.00073285
Iteration 25/25 | Loss: 0.00073285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28425121
Iteration 2/25 | Loss: 0.00085168
Iteration 3/25 | Loss: 0.00085168
Iteration 4/25 | Loss: 0.00085167
Iteration 5/25 | Loss: 0.00085167
Iteration 6/25 | Loss: 0.00085167
Iteration 7/25 | Loss: 0.00085167
Iteration 8/25 | Loss: 0.00085167
Iteration 9/25 | Loss: 0.00085167
Iteration 10/25 | Loss: 0.00085167
Iteration 11/25 | Loss: 0.00085167
Iteration 12/25 | Loss: 0.00085167
Iteration 13/25 | Loss: 0.00085167
Iteration 14/25 | Loss: 0.00085167
Iteration 15/25 | Loss: 0.00085167
Iteration 16/25 | Loss: 0.00085167
Iteration 17/25 | Loss: 0.00085167
Iteration 18/25 | Loss: 0.00085167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008516733651049435, 0.0008516733651049435, 0.0008516733651049435, 0.0008516733651049435, 0.0008516733651049435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008516733651049435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085167
Iteration 2/1000 | Loss: 0.00005098
Iteration 3/1000 | Loss: 0.00003345
Iteration 4/1000 | Loss: 0.00002901
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002579
Iteration 7/1000 | Loss: 0.00002507
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002369
Iteration 10/1000 | Loss: 0.00002338
Iteration 11/1000 | Loss: 0.00002315
Iteration 12/1000 | Loss: 0.00002295
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002270
Iteration 15/1000 | Loss: 0.00002247
Iteration 16/1000 | Loss: 0.00002233
Iteration 17/1000 | Loss: 0.00002233
Iteration 18/1000 | Loss: 0.00002232
Iteration 19/1000 | Loss: 0.00002226
Iteration 20/1000 | Loss: 0.00002219
Iteration 21/1000 | Loss: 0.00002216
Iteration 22/1000 | Loss: 0.00002214
Iteration 23/1000 | Loss: 0.00002214
Iteration 24/1000 | Loss: 0.00002213
Iteration 25/1000 | Loss: 0.00002213
Iteration 26/1000 | Loss: 0.00002213
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002205
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002205
Iteration 31/1000 | Loss: 0.00002204
Iteration 32/1000 | Loss: 0.00002204
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002203
Iteration 35/1000 | Loss: 0.00002202
Iteration 36/1000 | Loss: 0.00002202
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002201
Iteration 39/1000 | Loss: 0.00002201
Iteration 40/1000 | Loss: 0.00002201
Iteration 41/1000 | Loss: 0.00002201
Iteration 42/1000 | Loss: 0.00002200
Iteration 43/1000 | Loss: 0.00002200
Iteration 44/1000 | Loss: 0.00002199
Iteration 45/1000 | Loss: 0.00002198
Iteration 46/1000 | Loss: 0.00002198
Iteration 47/1000 | Loss: 0.00002198
Iteration 48/1000 | Loss: 0.00002197
Iteration 49/1000 | Loss: 0.00002197
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002195
Iteration 52/1000 | Loss: 0.00002194
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002193
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002193
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002192
Iteration 62/1000 | Loss: 0.00002191
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002190
Iteration 67/1000 | Loss: 0.00002190
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002190
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002189
Iteration 72/1000 | Loss: 0.00002189
Iteration 73/1000 | Loss: 0.00002189
Iteration 74/1000 | Loss: 0.00002188
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002187
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002187
Iteration 81/1000 | Loss: 0.00002187
Iteration 82/1000 | Loss: 0.00002187
Iteration 83/1000 | Loss: 0.00002187
Iteration 84/1000 | Loss: 0.00002187
Iteration 85/1000 | Loss: 0.00002187
Iteration 86/1000 | Loss: 0.00002187
Iteration 87/1000 | Loss: 0.00002187
Iteration 88/1000 | Loss: 0.00002186
Iteration 89/1000 | Loss: 0.00002186
Iteration 90/1000 | Loss: 0.00002186
Iteration 91/1000 | Loss: 0.00002186
Iteration 92/1000 | Loss: 0.00002186
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002185
Iteration 96/1000 | Loss: 0.00002185
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002184
Iteration 102/1000 | Loss: 0.00002184
Iteration 103/1000 | Loss: 0.00002184
Iteration 104/1000 | Loss: 0.00002184
Iteration 105/1000 | Loss: 0.00002184
Iteration 106/1000 | Loss: 0.00002184
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00002184
Iteration 111/1000 | Loss: 0.00002184
Iteration 112/1000 | Loss: 0.00002184
Iteration 113/1000 | Loss: 0.00002184
Iteration 114/1000 | Loss: 0.00002184
Iteration 115/1000 | Loss: 0.00002184
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002184
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002184
Iteration 121/1000 | Loss: 0.00002184
Iteration 122/1000 | Loss: 0.00002184
Iteration 123/1000 | Loss: 0.00002184
Iteration 124/1000 | Loss: 0.00002184
Iteration 125/1000 | Loss: 0.00002184
Iteration 126/1000 | Loss: 0.00002184
Iteration 127/1000 | Loss: 0.00002184
Iteration 128/1000 | Loss: 0.00002184
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.183835385949351e-05, 2.183835385949351e-05, 2.183835385949351e-05, 2.183835385949351e-05, 2.183835385949351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.183835385949351e-05

Optimization complete. Final v2v error: 3.853111505508423 mm

Highest mean error: 5.372940540313721 mm for frame 69

Lowest mean error: 3.275979518890381 mm for frame 19

Saving results

Total time: 42.3108696937561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033366
Iteration 2/25 | Loss: 0.00175954
Iteration 3/25 | Loss: 0.00104522
Iteration 4/25 | Loss: 0.00084687
Iteration 5/25 | Loss: 0.00080552
Iteration 6/25 | Loss: 0.00084130
Iteration 7/25 | Loss: 0.00082112
Iteration 8/25 | Loss: 0.00076379
Iteration 9/25 | Loss: 0.00078072
Iteration 10/25 | Loss: 0.00075813
Iteration 11/25 | Loss: 0.00073835
Iteration 12/25 | Loss: 0.00071760
Iteration 13/25 | Loss: 0.00071162
Iteration 14/25 | Loss: 0.00070845
Iteration 15/25 | Loss: 0.00070597
Iteration 16/25 | Loss: 0.00070532
Iteration 17/25 | Loss: 0.00070514
Iteration 18/25 | Loss: 0.00071266
Iteration 19/25 | Loss: 0.00071266
Iteration 20/25 | Loss: 0.00071266
Iteration 21/25 | Loss: 0.00071265
Iteration 22/25 | Loss: 0.00071265
Iteration 23/25 | Loss: 0.00071265
Iteration 24/25 | Loss: 0.00071265
Iteration 25/25 | Loss: 0.00071265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57329571
Iteration 2/25 | Loss: 0.00110569
Iteration 3/25 | Loss: 0.00097928
Iteration 4/25 | Loss: 0.00097928
Iteration 5/25 | Loss: 0.00097928
Iteration 6/25 | Loss: 0.00097928
Iteration 7/25 | Loss: 0.00097928
Iteration 8/25 | Loss: 0.00097928
Iteration 9/25 | Loss: 0.00097928
Iteration 10/25 | Loss: 0.00097928
Iteration 11/25 | Loss: 0.00097928
Iteration 12/25 | Loss: 0.00097928
Iteration 13/25 | Loss: 0.00097928
Iteration 14/25 | Loss: 0.00097928
Iteration 15/25 | Loss: 0.00097928
Iteration 16/25 | Loss: 0.00097928
Iteration 17/25 | Loss: 0.00097928
Iteration 18/25 | Loss: 0.00097928
Iteration 19/25 | Loss: 0.00097928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009792796336114407, 0.0009792796336114407, 0.0009792796336114407, 0.0009792796336114407, 0.0009792796336114407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009792796336114407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097928
Iteration 2/1000 | Loss: 0.00164041
Iteration 3/1000 | Loss: 0.00007278
Iteration 4/1000 | Loss: 0.00005166
Iteration 5/1000 | Loss: 0.00124353
Iteration 6/1000 | Loss: 0.00010913
Iteration 7/1000 | Loss: 0.00006636
Iteration 8/1000 | Loss: 0.00005478
Iteration 9/1000 | Loss: 0.00004569
Iteration 10/1000 | Loss: 0.00003984
Iteration 11/1000 | Loss: 0.00005310
Iteration 12/1000 | Loss: 0.00003502
Iteration 13/1000 | Loss: 0.00003051
Iteration 14/1000 | Loss: 0.00002934
Iteration 15/1000 | Loss: 0.00002822
Iteration 16/1000 | Loss: 0.00002652
Iteration 17/1000 | Loss: 0.00002497
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002214
Iteration 20/1000 | Loss: 0.00002116
Iteration 21/1000 | Loss: 0.00002044
Iteration 22/1000 | Loss: 0.00002003
Iteration 23/1000 | Loss: 0.00001972
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001931
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001920
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001908
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001902
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001899
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001897
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001893
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001892
Iteration 59/1000 | Loss: 0.00001892
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001891
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001889
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001886
Iteration 92/1000 | Loss: 0.00001886
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001884
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001884
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001882
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001882
Iteration 123/1000 | Loss: 0.00001882
Iteration 124/1000 | Loss: 0.00001882
Iteration 125/1000 | Loss: 0.00001882
Iteration 126/1000 | Loss: 0.00001882
Iteration 127/1000 | Loss: 0.00001882
Iteration 128/1000 | Loss: 0.00001882
Iteration 129/1000 | Loss: 0.00001882
Iteration 130/1000 | Loss: 0.00001882
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001882
Iteration 138/1000 | Loss: 0.00001882
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001882
Iteration 141/1000 | Loss: 0.00001882
Iteration 142/1000 | Loss: 0.00001882
Iteration 143/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8818676835508086e-05, 1.8818676835508086e-05, 1.8818676835508086e-05, 1.8818676835508086e-05, 1.8818676835508086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8818676835508086e-05

Optimization complete. Final v2v error: 3.4764487743377686 mm

Highest mean error: 5.068300724029541 mm for frame 87

Lowest mean error: 2.954519510269165 mm for frame 102

Saving results

Total time: 72.14135670661926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605121
Iteration 2/25 | Loss: 0.00087778
Iteration 3/25 | Loss: 0.00077104
Iteration 4/25 | Loss: 0.00074977
Iteration 5/25 | Loss: 0.00074347
Iteration 6/25 | Loss: 0.00074191
Iteration 7/25 | Loss: 0.00074181
Iteration 8/25 | Loss: 0.00074181
Iteration 9/25 | Loss: 0.00074181
Iteration 10/25 | Loss: 0.00074181
Iteration 11/25 | Loss: 0.00074181
Iteration 12/25 | Loss: 0.00074181
Iteration 13/25 | Loss: 0.00074181
Iteration 14/25 | Loss: 0.00074181
Iteration 15/25 | Loss: 0.00074181
Iteration 16/25 | Loss: 0.00074181
Iteration 17/25 | Loss: 0.00074181
Iteration 18/25 | Loss: 0.00074181
Iteration 19/25 | Loss: 0.00074181
Iteration 20/25 | Loss: 0.00074181
Iteration 21/25 | Loss: 0.00074181
Iteration 22/25 | Loss: 0.00074181
Iteration 23/25 | Loss: 0.00074181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007418128079734743, 0.0007418128079734743, 0.0007418128079734743, 0.0007418128079734743, 0.0007418128079734743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007418128079734743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.03005314
Iteration 2/25 | Loss: 0.00082054
Iteration 3/25 | Loss: 0.00082054
Iteration 4/25 | Loss: 0.00082054
Iteration 5/25 | Loss: 0.00082054
Iteration 6/25 | Loss: 0.00082054
Iteration 7/25 | Loss: 0.00082054
Iteration 8/25 | Loss: 0.00082054
Iteration 9/25 | Loss: 0.00082054
Iteration 10/25 | Loss: 0.00082054
Iteration 11/25 | Loss: 0.00082054
Iteration 12/25 | Loss: 0.00082054
Iteration 13/25 | Loss: 0.00082054
Iteration 14/25 | Loss: 0.00082054
Iteration 15/25 | Loss: 0.00082054
Iteration 16/25 | Loss: 0.00082054
Iteration 17/25 | Loss: 0.00082054
Iteration 18/25 | Loss: 0.00082054
Iteration 19/25 | Loss: 0.00082054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008205410558730364, 0.0008205410558730364, 0.0008205410558730364, 0.0008205410558730364, 0.0008205410558730364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008205410558730364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082054
Iteration 2/1000 | Loss: 0.00005075
Iteration 3/1000 | Loss: 0.00003816
Iteration 4/1000 | Loss: 0.00003340
Iteration 5/1000 | Loss: 0.00003056
Iteration 6/1000 | Loss: 0.00002884
Iteration 7/1000 | Loss: 0.00002731
Iteration 8/1000 | Loss: 0.00002647
Iteration 9/1000 | Loss: 0.00002615
Iteration 10/1000 | Loss: 0.00002592
Iteration 11/1000 | Loss: 0.00002565
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002538
Iteration 14/1000 | Loss: 0.00002528
Iteration 15/1000 | Loss: 0.00002527
Iteration 16/1000 | Loss: 0.00002519
Iteration 17/1000 | Loss: 0.00002515
Iteration 18/1000 | Loss: 0.00002512
Iteration 19/1000 | Loss: 0.00002512
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002511
Iteration 22/1000 | Loss: 0.00002506
Iteration 23/1000 | Loss: 0.00002506
Iteration 24/1000 | Loss: 0.00002506
Iteration 25/1000 | Loss: 0.00002506
Iteration 26/1000 | Loss: 0.00002505
Iteration 27/1000 | Loss: 0.00002501
Iteration 28/1000 | Loss: 0.00002496
Iteration 29/1000 | Loss: 0.00002495
Iteration 30/1000 | Loss: 0.00002495
Iteration 31/1000 | Loss: 0.00002494
Iteration 32/1000 | Loss: 0.00002494
Iteration 33/1000 | Loss: 0.00002493
Iteration 34/1000 | Loss: 0.00002493
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002492
Iteration 37/1000 | Loss: 0.00002492
Iteration 38/1000 | Loss: 0.00002492
Iteration 39/1000 | Loss: 0.00002492
Iteration 40/1000 | Loss: 0.00002492
Iteration 41/1000 | Loss: 0.00002492
Iteration 42/1000 | Loss: 0.00002492
Iteration 43/1000 | Loss: 0.00002491
Iteration 44/1000 | Loss: 0.00002491
Iteration 45/1000 | Loss: 0.00002491
Iteration 46/1000 | Loss: 0.00002491
Iteration 47/1000 | Loss: 0.00002491
Iteration 48/1000 | Loss: 0.00002491
Iteration 49/1000 | Loss: 0.00002490
Iteration 50/1000 | Loss: 0.00002490
Iteration 51/1000 | Loss: 0.00002490
Iteration 52/1000 | Loss: 0.00002490
Iteration 53/1000 | Loss: 0.00002490
Iteration 54/1000 | Loss: 0.00002490
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002489
Iteration 57/1000 | Loss: 0.00002489
Iteration 58/1000 | Loss: 0.00002489
Iteration 59/1000 | Loss: 0.00002489
Iteration 60/1000 | Loss: 0.00002489
Iteration 61/1000 | Loss: 0.00002489
Iteration 62/1000 | Loss: 0.00002489
Iteration 63/1000 | Loss: 0.00002489
Iteration 64/1000 | Loss: 0.00002488
Iteration 65/1000 | Loss: 0.00002488
Iteration 66/1000 | Loss: 0.00002487
Iteration 67/1000 | Loss: 0.00002487
Iteration 68/1000 | Loss: 0.00002487
Iteration 69/1000 | Loss: 0.00002487
Iteration 70/1000 | Loss: 0.00002487
Iteration 71/1000 | Loss: 0.00002487
Iteration 72/1000 | Loss: 0.00002487
Iteration 73/1000 | Loss: 0.00002487
Iteration 74/1000 | Loss: 0.00002486
Iteration 75/1000 | Loss: 0.00002486
Iteration 76/1000 | Loss: 0.00002486
Iteration 77/1000 | Loss: 0.00002486
Iteration 78/1000 | Loss: 0.00002486
Iteration 79/1000 | Loss: 0.00002486
Iteration 80/1000 | Loss: 0.00002485
Iteration 81/1000 | Loss: 0.00002485
Iteration 82/1000 | Loss: 0.00002485
Iteration 83/1000 | Loss: 0.00002485
Iteration 84/1000 | Loss: 0.00002485
Iteration 85/1000 | Loss: 0.00002484
Iteration 86/1000 | Loss: 0.00002484
Iteration 87/1000 | Loss: 0.00002484
Iteration 88/1000 | Loss: 0.00002484
Iteration 89/1000 | Loss: 0.00002484
Iteration 90/1000 | Loss: 0.00002484
Iteration 91/1000 | Loss: 0.00002484
Iteration 92/1000 | Loss: 0.00002484
Iteration 93/1000 | Loss: 0.00002483
Iteration 94/1000 | Loss: 0.00002483
Iteration 95/1000 | Loss: 0.00002483
Iteration 96/1000 | Loss: 0.00002483
Iteration 97/1000 | Loss: 0.00002483
Iteration 98/1000 | Loss: 0.00002483
Iteration 99/1000 | Loss: 0.00002483
Iteration 100/1000 | Loss: 0.00002483
Iteration 101/1000 | Loss: 0.00002483
Iteration 102/1000 | Loss: 0.00002483
Iteration 103/1000 | Loss: 0.00002483
Iteration 104/1000 | Loss: 0.00002483
Iteration 105/1000 | Loss: 0.00002482
Iteration 106/1000 | Loss: 0.00002482
Iteration 107/1000 | Loss: 0.00002482
Iteration 108/1000 | Loss: 0.00002482
Iteration 109/1000 | Loss: 0.00002482
Iteration 110/1000 | Loss: 0.00002482
Iteration 111/1000 | Loss: 0.00002482
Iteration 112/1000 | Loss: 0.00002482
Iteration 113/1000 | Loss: 0.00002482
Iteration 114/1000 | Loss: 0.00002481
Iteration 115/1000 | Loss: 0.00002481
Iteration 116/1000 | Loss: 0.00002481
Iteration 117/1000 | Loss: 0.00002481
Iteration 118/1000 | Loss: 0.00002481
Iteration 119/1000 | Loss: 0.00002481
Iteration 120/1000 | Loss: 0.00002481
Iteration 121/1000 | Loss: 0.00002481
Iteration 122/1000 | Loss: 0.00002481
Iteration 123/1000 | Loss: 0.00002481
Iteration 124/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.4812803530949168e-05, 2.4812803530949168e-05, 2.4812803530949168e-05, 2.4812803530949168e-05, 2.4812803530949168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4812803530949168e-05

Optimization complete. Final v2v error: 4.228124141693115 mm

Highest mean error: 4.506251335144043 mm for frame 157

Lowest mean error: 3.9055707454681396 mm for frame 51

Saving results

Total time: 39.48732280731201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721943
Iteration 2/25 | Loss: 0.00114686
Iteration 3/25 | Loss: 0.00082008
Iteration 4/25 | Loss: 0.00076328
Iteration 5/25 | Loss: 0.00075734
Iteration 6/25 | Loss: 0.00075515
Iteration 7/25 | Loss: 0.00075492
Iteration 8/25 | Loss: 0.00075492
Iteration 9/25 | Loss: 0.00075492
Iteration 10/25 | Loss: 0.00075492
Iteration 11/25 | Loss: 0.00075492
Iteration 12/25 | Loss: 0.00075492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007549209403805435, 0.0007549209403805435, 0.0007549209403805435, 0.0007549209403805435, 0.0007549209403805435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007549209403805435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23084080
Iteration 2/25 | Loss: 0.00077243
Iteration 3/25 | Loss: 0.00077243
Iteration 4/25 | Loss: 0.00077243
Iteration 5/25 | Loss: 0.00077243
Iteration 6/25 | Loss: 0.00077243
Iteration 7/25 | Loss: 0.00077243
Iteration 8/25 | Loss: 0.00077243
Iteration 9/25 | Loss: 0.00077243
Iteration 10/25 | Loss: 0.00077243
Iteration 11/25 | Loss: 0.00077243
Iteration 12/25 | Loss: 0.00077243
Iteration 13/25 | Loss: 0.00077243
Iteration 14/25 | Loss: 0.00077243
Iteration 15/25 | Loss: 0.00077243
Iteration 16/25 | Loss: 0.00077243
Iteration 17/25 | Loss: 0.00077243
Iteration 18/25 | Loss: 0.00077243
Iteration 19/25 | Loss: 0.00077243
Iteration 20/25 | Loss: 0.00077243
Iteration 21/25 | Loss: 0.00077243
Iteration 22/25 | Loss: 0.00077243
Iteration 23/25 | Loss: 0.00077243
Iteration 24/25 | Loss: 0.00077243
Iteration 25/25 | Loss: 0.00077243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077243
Iteration 2/1000 | Loss: 0.00003803
Iteration 3/1000 | Loss: 0.00002757
Iteration 4/1000 | Loss: 0.00002479
Iteration 5/1000 | Loss: 0.00002328
Iteration 6/1000 | Loss: 0.00002252
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002085
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002034
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001967
Iteration 15/1000 | Loss: 0.00001946
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001937
Iteration 19/1000 | Loss: 0.00001936
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001912
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001905
Iteration 26/1000 | Loss: 0.00001901
Iteration 27/1000 | Loss: 0.00001900
Iteration 28/1000 | Loss: 0.00001900
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001899
Iteration 36/1000 | Loss: 0.00001899
Iteration 37/1000 | Loss: 0.00001899
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001898
Iteration 40/1000 | Loss: 0.00001898
Iteration 41/1000 | Loss: 0.00001898
Iteration 42/1000 | Loss: 0.00001898
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001895
Iteration 56/1000 | Loss: 0.00001895
Iteration 57/1000 | Loss: 0.00001895
Iteration 58/1000 | Loss: 0.00001895
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001894
Iteration 61/1000 | Loss: 0.00001894
Iteration 62/1000 | Loss: 0.00001894
Iteration 63/1000 | Loss: 0.00001894
Iteration 64/1000 | Loss: 0.00001893
Iteration 65/1000 | Loss: 0.00001893
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001893
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001893
Iteration 71/1000 | Loss: 0.00001893
Iteration 72/1000 | Loss: 0.00001893
Iteration 73/1000 | Loss: 0.00001893
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001893
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001893
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.8916038243332878e-05, 1.8916038243332878e-05, 1.8916038243332878e-05, 1.8916038243332878e-05, 1.8916038243332878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8916038243332878e-05

Optimization complete. Final v2v error: 3.787051200866699 mm

Highest mean error: 4.357108116149902 mm for frame 205

Lowest mean error: 3.544203996658325 mm for frame 152

Saving results

Total time: 42.59426259994507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959399
Iteration 2/25 | Loss: 0.00112704
Iteration 3/25 | Loss: 0.00083363
Iteration 4/25 | Loss: 0.00080352
Iteration 5/25 | Loss: 0.00079140
Iteration 6/25 | Loss: 0.00078956
Iteration 7/25 | Loss: 0.00078943
Iteration 8/25 | Loss: 0.00078943
Iteration 9/25 | Loss: 0.00078943
Iteration 10/25 | Loss: 0.00078943
Iteration 11/25 | Loss: 0.00078943
Iteration 12/25 | Loss: 0.00078943
Iteration 13/25 | Loss: 0.00078943
Iteration 14/25 | Loss: 0.00078943
Iteration 15/25 | Loss: 0.00078943
Iteration 16/25 | Loss: 0.00078943
Iteration 17/25 | Loss: 0.00078943
Iteration 18/25 | Loss: 0.00078943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007894253358244896, 0.0007894253358244896, 0.0007894253358244896, 0.0007894253358244896, 0.0007894253358244896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007894253358244896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89523661
Iteration 2/25 | Loss: 0.00058615
Iteration 3/25 | Loss: 0.00058615
Iteration 4/25 | Loss: 0.00058615
Iteration 5/25 | Loss: 0.00058615
Iteration 6/25 | Loss: 0.00058615
Iteration 7/25 | Loss: 0.00058615
Iteration 8/25 | Loss: 0.00058615
Iteration 9/25 | Loss: 0.00058615
Iteration 10/25 | Loss: 0.00058615
Iteration 11/25 | Loss: 0.00058615
Iteration 12/25 | Loss: 0.00058615
Iteration 13/25 | Loss: 0.00058615
Iteration 14/25 | Loss: 0.00058615
Iteration 15/25 | Loss: 0.00058615
Iteration 16/25 | Loss: 0.00058615
Iteration 17/25 | Loss: 0.00058615
Iteration 18/25 | Loss: 0.00058615
Iteration 19/25 | Loss: 0.00058615
Iteration 20/25 | Loss: 0.00058615
Iteration 21/25 | Loss: 0.00058615
Iteration 22/25 | Loss: 0.00058615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005861451500095427, 0.0005861451500095427, 0.0005861451500095427, 0.0005861451500095427, 0.0005861451500095427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005861451500095427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058615
Iteration 2/1000 | Loss: 0.00004584
Iteration 3/1000 | Loss: 0.00003412
Iteration 4/1000 | Loss: 0.00003163
Iteration 5/1000 | Loss: 0.00003010
Iteration 6/1000 | Loss: 0.00002944
Iteration 7/1000 | Loss: 0.00002903
Iteration 8/1000 | Loss: 0.00002853
Iteration 9/1000 | Loss: 0.00002821
Iteration 10/1000 | Loss: 0.00002800
Iteration 11/1000 | Loss: 0.00002786
Iteration 12/1000 | Loss: 0.00002775
Iteration 13/1000 | Loss: 0.00002768
Iteration 14/1000 | Loss: 0.00002760
Iteration 15/1000 | Loss: 0.00002753
Iteration 16/1000 | Loss: 0.00002750
Iteration 17/1000 | Loss: 0.00002750
Iteration 18/1000 | Loss: 0.00002749
Iteration 19/1000 | Loss: 0.00002749
Iteration 20/1000 | Loss: 0.00002748
Iteration 21/1000 | Loss: 0.00002748
Iteration 22/1000 | Loss: 0.00002745
Iteration 23/1000 | Loss: 0.00002745
Iteration 24/1000 | Loss: 0.00002745
Iteration 25/1000 | Loss: 0.00002744
Iteration 26/1000 | Loss: 0.00002742
Iteration 27/1000 | Loss: 0.00002742
Iteration 28/1000 | Loss: 0.00002739
Iteration 29/1000 | Loss: 0.00002739
Iteration 30/1000 | Loss: 0.00002738
Iteration 31/1000 | Loss: 0.00002737
Iteration 32/1000 | Loss: 0.00002736
Iteration 33/1000 | Loss: 0.00002736
Iteration 34/1000 | Loss: 0.00002736
Iteration 35/1000 | Loss: 0.00002735
Iteration 36/1000 | Loss: 0.00002735
Iteration 37/1000 | Loss: 0.00002735
Iteration 38/1000 | Loss: 0.00002734
Iteration 39/1000 | Loss: 0.00002734
Iteration 40/1000 | Loss: 0.00002733
Iteration 41/1000 | Loss: 0.00002733
Iteration 42/1000 | Loss: 0.00002732
Iteration 43/1000 | Loss: 0.00002732
Iteration 44/1000 | Loss: 0.00002731
Iteration 45/1000 | Loss: 0.00002731
Iteration 46/1000 | Loss: 0.00002730
Iteration 47/1000 | Loss: 0.00002729
Iteration 48/1000 | Loss: 0.00002728
Iteration 49/1000 | Loss: 0.00002728
Iteration 50/1000 | Loss: 0.00002726
Iteration 51/1000 | Loss: 0.00002725
Iteration 52/1000 | Loss: 0.00002725
Iteration 53/1000 | Loss: 0.00002724
Iteration 54/1000 | Loss: 0.00002723
Iteration 55/1000 | Loss: 0.00002723
Iteration 56/1000 | Loss: 0.00002723
Iteration 57/1000 | Loss: 0.00002723
Iteration 58/1000 | Loss: 0.00002722
Iteration 59/1000 | Loss: 0.00002722
Iteration 60/1000 | Loss: 0.00002722
Iteration 61/1000 | Loss: 0.00002722
Iteration 62/1000 | Loss: 0.00002722
Iteration 63/1000 | Loss: 0.00002722
Iteration 64/1000 | Loss: 0.00002721
Iteration 65/1000 | Loss: 0.00002721
Iteration 66/1000 | Loss: 0.00002720
Iteration 67/1000 | Loss: 0.00002720
Iteration 68/1000 | Loss: 0.00002719
Iteration 69/1000 | Loss: 0.00002719
Iteration 70/1000 | Loss: 0.00002719
Iteration 71/1000 | Loss: 0.00002719
Iteration 72/1000 | Loss: 0.00002718
Iteration 73/1000 | Loss: 0.00002718
Iteration 74/1000 | Loss: 0.00002718
Iteration 75/1000 | Loss: 0.00002717
Iteration 76/1000 | Loss: 0.00002717
Iteration 77/1000 | Loss: 0.00002717
Iteration 78/1000 | Loss: 0.00002717
Iteration 79/1000 | Loss: 0.00002717
Iteration 80/1000 | Loss: 0.00002717
Iteration 81/1000 | Loss: 0.00002717
Iteration 82/1000 | Loss: 0.00002717
Iteration 83/1000 | Loss: 0.00002717
Iteration 84/1000 | Loss: 0.00002717
Iteration 85/1000 | Loss: 0.00002717
Iteration 86/1000 | Loss: 0.00002717
Iteration 87/1000 | Loss: 0.00002717
Iteration 88/1000 | Loss: 0.00002717
Iteration 89/1000 | Loss: 0.00002717
Iteration 90/1000 | Loss: 0.00002716
Iteration 91/1000 | Loss: 0.00002716
Iteration 92/1000 | Loss: 0.00002716
Iteration 93/1000 | Loss: 0.00002716
Iteration 94/1000 | Loss: 0.00002716
Iteration 95/1000 | Loss: 0.00002716
Iteration 96/1000 | Loss: 0.00002716
Iteration 97/1000 | Loss: 0.00002716
Iteration 98/1000 | Loss: 0.00002716
Iteration 99/1000 | Loss: 0.00002716
Iteration 100/1000 | Loss: 0.00002716
Iteration 101/1000 | Loss: 0.00002715
Iteration 102/1000 | Loss: 0.00002715
Iteration 103/1000 | Loss: 0.00002715
Iteration 104/1000 | Loss: 0.00002715
Iteration 105/1000 | Loss: 0.00002715
Iteration 106/1000 | Loss: 0.00002715
Iteration 107/1000 | Loss: 0.00002714
Iteration 108/1000 | Loss: 0.00002714
Iteration 109/1000 | Loss: 0.00002714
Iteration 110/1000 | Loss: 0.00002714
Iteration 111/1000 | Loss: 0.00002714
Iteration 112/1000 | Loss: 0.00002714
Iteration 113/1000 | Loss: 0.00002714
Iteration 114/1000 | Loss: 0.00002714
Iteration 115/1000 | Loss: 0.00002713
Iteration 116/1000 | Loss: 0.00002713
Iteration 117/1000 | Loss: 0.00002713
Iteration 118/1000 | Loss: 0.00002713
Iteration 119/1000 | Loss: 0.00002713
Iteration 120/1000 | Loss: 0.00002712
Iteration 121/1000 | Loss: 0.00002712
Iteration 122/1000 | Loss: 0.00002712
Iteration 123/1000 | Loss: 0.00002712
Iteration 124/1000 | Loss: 0.00002712
Iteration 125/1000 | Loss: 0.00002712
Iteration 126/1000 | Loss: 0.00002712
Iteration 127/1000 | Loss: 0.00002711
Iteration 128/1000 | Loss: 0.00002711
Iteration 129/1000 | Loss: 0.00002711
Iteration 130/1000 | Loss: 0.00002711
Iteration 131/1000 | Loss: 0.00002711
Iteration 132/1000 | Loss: 0.00002711
Iteration 133/1000 | Loss: 0.00002711
Iteration 134/1000 | Loss: 0.00002711
Iteration 135/1000 | Loss: 0.00002711
Iteration 136/1000 | Loss: 0.00002711
Iteration 137/1000 | Loss: 0.00002711
Iteration 138/1000 | Loss: 0.00002711
Iteration 139/1000 | Loss: 0.00002711
Iteration 140/1000 | Loss: 0.00002711
Iteration 141/1000 | Loss: 0.00002710
Iteration 142/1000 | Loss: 0.00002710
Iteration 143/1000 | Loss: 0.00002710
Iteration 144/1000 | Loss: 0.00002710
Iteration 145/1000 | Loss: 0.00002710
Iteration 146/1000 | Loss: 0.00002710
Iteration 147/1000 | Loss: 0.00002710
Iteration 148/1000 | Loss: 0.00002710
Iteration 149/1000 | Loss: 0.00002710
Iteration 150/1000 | Loss: 0.00002710
Iteration 151/1000 | Loss: 0.00002710
Iteration 152/1000 | Loss: 0.00002710
Iteration 153/1000 | Loss: 0.00002710
Iteration 154/1000 | Loss: 0.00002710
Iteration 155/1000 | Loss: 0.00002709
Iteration 156/1000 | Loss: 0.00002709
Iteration 157/1000 | Loss: 0.00002709
Iteration 158/1000 | Loss: 0.00002709
Iteration 159/1000 | Loss: 0.00002709
Iteration 160/1000 | Loss: 0.00002709
Iteration 161/1000 | Loss: 0.00002709
Iteration 162/1000 | Loss: 0.00002708
Iteration 163/1000 | Loss: 0.00002708
Iteration 164/1000 | Loss: 0.00002708
Iteration 165/1000 | Loss: 0.00002708
Iteration 166/1000 | Loss: 0.00002708
Iteration 167/1000 | Loss: 0.00002708
Iteration 168/1000 | Loss: 0.00002708
Iteration 169/1000 | Loss: 0.00002708
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002708
Iteration 172/1000 | Loss: 0.00002708
Iteration 173/1000 | Loss: 0.00002707
Iteration 174/1000 | Loss: 0.00002707
Iteration 175/1000 | Loss: 0.00002707
Iteration 176/1000 | Loss: 0.00002707
Iteration 177/1000 | Loss: 0.00002707
Iteration 178/1000 | Loss: 0.00002707
Iteration 179/1000 | Loss: 0.00002707
Iteration 180/1000 | Loss: 0.00002707
Iteration 181/1000 | Loss: 0.00002707
Iteration 182/1000 | Loss: 0.00002707
Iteration 183/1000 | Loss: 0.00002707
Iteration 184/1000 | Loss: 0.00002707
Iteration 185/1000 | Loss: 0.00002707
Iteration 186/1000 | Loss: 0.00002706
Iteration 187/1000 | Loss: 0.00002706
Iteration 188/1000 | Loss: 0.00002706
Iteration 189/1000 | Loss: 0.00002706
Iteration 190/1000 | Loss: 0.00002706
Iteration 191/1000 | Loss: 0.00002706
Iteration 192/1000 | Loss: 0.00002706
Iteration 193/1000 | Loss: 0.00002706
Iteration 194/1000 | Loss: 0.00002705
Iteration 195/1000 | Loss: 0.00002705
Iteration 196/1000 | Loss: 0.00002705
Iteration 197/1000 | Loss: 0.00002705
Iteration 198/1000 | Loss: 0.00002705
Iteration 199/1000 | Loss: 0.00002705
Iteration 200/1000 | Loss: 0.00002705
Iteration 201/1000 | Loss: 0.00002705
Iteration 202/1000 | Loss: 0.00002705
Iteration 203/1000 | Loss: 0.00002705
Iteration 204/1000 | Loss: 0.00002705
Iteration 205/1000 | Loss: 0.00002705
Iteration 206/1000 | Loss: 0.00002705
Iteration 207/1000 | Loss: 0.00002705
Iteration 208/1000 | Loss: 0.00002705
Iteration 209/1000 | Loss: 0.00002705
Iteration 210/1000 | Loss: 0.00002705
Iteration 211/1000 | Loss: 0.00002705
Iteration 212/1000 | Loss: 0.00002705
Iteration 213/1000 | Loss: 0.00002705
Iteration 214/1000 | Loss: 0.00002705
Iteration 215/1000 | Loss: 0.00002705
Iteration 216/1000 | Loss: 0.00002705
Iteration 217/1000 | Loss: 0.00002705
Iteration 218/1000 | Loss: 0.00002705
Iteration 219/1000 | Loss: 0.00002705
Iteration 220/1000 | Loss: 0.00002705
Iteration 221/1000 | Loss: 0.00002705
Iteration 222/1000 | Loss: 0.00002705
Iteration 223/1000 | Loss: 0.00002705
Iteration 224/1000 | Loss: 0.00002705
Iteration 225/1000 | Loss: 0.00002705
Iteration 226/1000 | Loss: 0.00002705
Iteration 227/1000 | Loss: 0.00002705
Iteration 228/1000 | Loss: 0.00002705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.7049381969845854e-05, 2.7049381969845854e-05, 2.7049381969845854e-05, 2.7049381969845854e-05, 2.7049381969845854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7049381969845854e-05

Optimization complete. Final v2v error: 4.436670303344727 mm

Highest mean error: 5.064419269561768 mm for frame 8

Lowest mean error: 3.908730983734131 mm for frame 142

Saving results

Total time: 43.52185845375061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951112
Iteration 2/25 | Loss: 0.00100681
Iteration 3/25 | Loss: 0.00082469
Iteration 4/25 | Loss: 0.00078729
Iteration 5/25 | Loss: 0.00076946
Iteration 6/25 | Loss: 0.00076428
Iteration 7/25 | Loss: 0.00076324
Iteration 8/25 | Loss: 0.00076314
Iteration 9/25 | Loss: 0.00076314
Iteration 10/25 | Loss: 0.00076314
Iteration 11/25 | Loss: 0.00076314
Iteration 12/25 | Loss: 0.00076314
Iteration 13/25 | Loss: 0.00076314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007631366024725139, 0.0007631366024725139, 0.0007631366024725139, 0.0007631366024725139, 0.0007631366024725139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007631366024725139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28326619
Iteration 2/25 | Loss: 0.00080239
Iteration 3/25 | Loss: 0.00080238
Iteration 4/25 | Loss: 0.00080238
Iteration 5/25 | Loss: 0.00080238
Iteration 6/25 | Loss: 0.00080238
Iteration 7/25 | Loss: 0.00080238
Iteration 8/25 | Loss: 0.00080238
Iteration 9/25 | Loss: 0.00080238
Iteration 10/25 | Loss: 0.00080238
Iteration 11/25 | Loss: 0.00080238
Iteration 12/25 | Loss: 0.00080238
Iteration 13/25 | Loss: 0.00080238
Iteration 14/25 | Loss: 0.00080238
Iteration 15/25 | Loss: 0.00080238
Iteration 16/25 | Loss: 0.00080238
Iteration 17/25 | Loss: 0.00080238
Iteration 18/25 | Loss: 0.00080238
Iteration 19/25 | Loss: 0.00080238
Iteration 20/25 | Loss: 0.00080238
Iteration 21/25 | Loss: 0.00080238
Iteration 22/25 | Loss: 0.00080238
Iteration 23/25 | Loss: 0.00080238
Iteration 24/25 | Loss: 0.00080238
Iteration 25/25 | Loss: 0.00080238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080238
Iteration 2/1000 | Loss: 0.00006837
Iteration 3/1000 | Loss: 0.00004522
Iteration 4/1000 | Loss: 0.00003711
Iteration 5/1000 | Loss: 0.00003480
Iteration 6/1000 | Loss: 0.00003297
Iteration 7/1000 | Loss: 0.00003184
Iteration 8/1000 | Loss: 0.00003078
Iteration 9/1000 | Loss: 0.00002998
Iteration 10/1000 | Loss: 0.00002938
Iteration 11/1000 | Loss: 0.00002910
Iteration 12/1000 | Loss: 0.00002909
Iteration 13/1000 | Loss: 0.00002891
Iteration 14/1000 | Loss: 0.00002870
Iteration 15/1000 | Loss: 0.00002865
Iteration 16/1000 | Loss: 0.00002865
Iteration 17/1000 | Loss: 0.00002862
Iteration 18/1000 | Loss: 0.00002861
Iteration 19/1000 | Loss: 0.00002861
Iteration 20/1000 | Loss: 0.00002861
Iteration 21/1000 | Loss: 0.00002860
Iteration 22/1000 | Loss: 0.00002860
Iteration 23/1000 | Loss: 0.00002859
Iteration 24/1000 | Loss: 0.00002859
Iteration 25/1000 | Loss: 0.00002859
Iteration 26/1000 | Loss: 0.00002858
Iteration 27/1000 | Loss: 0.00002858
Iteration 28/1000 | Loss: 0.00002857
Iteration 29/1000 | Loss: 0.00002852
Iteration 30/1000 | Loss: 0.00002852
Iteration 31/1000 | Loss: 0.00002851
Iteration 32/1000 | Loss: 0.00002850
Iteration 33/1000 | Loss: 0.00002850
Iteration 34/1000 | Loss: 0.00002849
Iteration 35/1000 | Loss: 0.00002849
Iteration 36/1000 | Loss: 0.00002849
Iteration 37/1000 | Loss: 0.00002849
Iteration 38/1000 | Loss: 0.00002849
Iteration 39/1000 | Loss: 0.00002849
Iteration 40/1000 | Loss: 0.00002849
Iteration 41/1000 | Loss: 0.00002849
Iteration 42/1000 | Loss: 0.00002849
Iteration 43/1000 | Loss: 0.00002849
Iteration 44/1000 | Loss: 0.00002849
Iteration 45/1000 | Loss: 0.00002848
Iteration 46/1000 | Loss: 0.00002848
Iteration 47/1000 | Loss: 0.00002848
Iteration 48/1000 | Loss: 0.00002848
Iteration 49/1000 | Loss: 0.00002848
Iteration 50/1000 | Loss: 0.00002846
Iteration 51/1000 | Loss: 0.00002846
Iteration 52/1000 | Loss: 0.00002846
Iteration 53/1000 | Loss: 0.00002846
Iteration 54/1000 | Loss: 0.00002845
Iteration 55/1000 | Loss: 0.00002845
Iteration 56/1000 | Loss: 0.00002845
Iteration 57/1000 | Loss: 0.00002845
Iteration 58/1000 | Loss: 0.00002844
Iteration 59/1000 | Loss: 0.00002844
Iteration 60/1000 | Loss: 0.00002843
Iteration 61/1000 | Loss: 0.00002843
Iteration 62/1000 | Loss: 0.00002843
Iteration 63/1000 | Loss: 0.00002842
Iteration 64/1000 | Loss: 0.00002842
Iteration 65/1000 | Loss: 0.00002842
Iteration 66/1000 | Loss: 0.00002841
Iteration 67/1000 | Loss: 0.00002841
Iteration 68/1000 | Loss: 0.00002840
Iteration 69/1000 | Loss: 0.00002840
Iteration 70/1000 | Loss: 0.00002840
Iteration 71/1000 | Loss: 0.00002840
Iteration 72/1000 | Loss: 0.00002840
Iteration 73/1000 | Loss: 0.00002840
Iteration 74/1000 | Loss: 0.00002840
Iteration 75/1000 | Loss: 0.00002840
Iteration 76/1000 | Loss: 0.00002840
Iteration 77/1000 | Loss: 0.00002839
Iteration 78/1000 | Loss: 0.00002838
Iteration 79/1000 | Loss: 0.00002838
Iteration 80/1000 | Loss: 0.00002838
Iteration 81/1000 | Loss: 0.00002838
Iteration 82/1000 | Loss: 0.00002838
Iteration 83/1000 | Loss: 0.00002838
Iteration 84/1000 | Loss: 0.00002837
Iteration 85/1000 | Loss: 0.00002837
Iteration 86/1000 | Loss: 0.00002837
Iteration 87/1000 | Loss: 0.00002836
Iteration 88/1000 | Loss: 0.00002836
Iteration 89/1000 | Loss: 0.00002836
Iteration 90/1000 | Loss: 0.00002835
Iteration 91/1000 | Loss: 0.00002835
Iteration 92/1000 | Loss: 0.00002835
Iteration 93/1000 | Loss: 0.00002835
Iteration 94/1000 | Loss: 0.00002835
Iteration 95/1000 | Loss: 0.00002835
Iteration 96/1000 | Loss: 0.00002835
Iteration 97/1000 | Loss: 0.00002835
Iteration 98/1000 | Loss: 0.00002835
Iteration 99/1000 | Loss: 0.00002835
Iteration 100/1000 | Loss: 0.00002835
Iteration 101/1000 | Loss: 0.00002834
Iteration 102/1000 | Loss: 0.00002834
Iteration 103/1000 | Loss: 0.00002833
Iteration 104/1000 | Loss: 0.00002833
Iteration 105/1000 | Loss: 0.00002833
Iteration 106/1000 | Loss: 0.00002833
Iteration 107/1000 | Loss: 0.00002833
Iteration 108/1000 | Loss: 0.00002833
Iteration 109/1000 | Loss: 0.00002833
Iteration 110/1000 | Loss: 0.00002832
Iteration 111/1000 | Loss: 0.00002832
Iteration 112/1000 | Loss: 0.00002832
Iteration 113/1000 | Loss: 0.00002832
Iteration 114/1000 | Loss: 0.00002832
Iteration 115/1000 | Loss: 0.00002832
Iteration 116/1000 | Loss: 0.00002832
Iteration 117/1000 | Loss: 0.00002832
Iteration 118/1000 | Loss: 0.00002831
Iteration 119/1000 | Loss: 0.00002831
Iteration 120/1000 | Loss: 0.00002831
Iteration 121/1000 | Loss: 0.00002831
Iteration 122/1000 | Loss: 0.00002830
Iteration 123/1000 | Loss: 0.00002830
Iteration 124/1000 | Loss: 0.00002830
Iteration 125/1000 | Loss: 0.00002830
Iteration 126/1000 | Loss: 0.00002830
Iteration 127/1000 | Loss: 0.00002830
Iteration 128/1000 | Loss: 0.00002829
Iteration 129/1000 | Loss: 0.00002829
Iteration 130/1000 | Loss: 0.00002829
Iteration 131/1000 | Loss: 0.00002829
Iteration 132/1000 | Loss: 0.00002829
Iteration 133/1000 | Loss: 0.00002829
Iteration 134/1000 | Loss: 0.00002829
Iteration 135/1000 | Loss: 0.00002829
Iteration 136/1000 | Loss: 0.00002829
Iteration 137/1000 | Loss: 0.00002829
Iteration 138/1000 | Loss: 0.00002829
Iteration 139/1000 | Loss: 0.00002829
Iteration 140/1000 | Loss: 0.00002829
Iteration 141/1000 | Loss: 0.00002829
Iteration 142/1000 | Loss: 0.00002828
Iteration 143/1000 | Loss: 0.00002828
Iteration 144/1000 | Loss: 0.00002828
Iteration 145/1000 | Loss: 0.00002828
Iteration 146/1000 | Loss: 0.00002828
Iteration 147/1000 | Loss: 0.00002828
Iteration 148/1000 | Loss: 0.00002828
Iteration 149/1000 | Loss: 0.00002828
Iteration 150/1000 | Loss: 0.00002828
Iteration 151/1000 | Loss: 0.00002828
Iteration 152/1000 | Loss: 0.00002828
Iteration 153/1000 | Loss: 0.00002828
Iteration 154/1000 | Loss: 0.00002827
Iteration 155/1000 | Loss: 0.00002827
Iteration 156/1000 | Loss: 0.00002827
Iteration 157/1000 | Loss: 0.00002827
Iteration 158/1000 | Loss: 0.00002827
Iteration 159/1000 | Loss: 0.00002827
Iteration 160/1000 | Loss: 0.00002827
Iteration 161/1000 | Loss: 0.00002827
Iteration 162/1000 | Loss: 0.00002827
Iteration 163/1000 | Loss: 0.00002827
Iteration 164/1000 | Loss: 0.00002827
Iteration 165/1000 | Loss: 0.00002827
Iteration 166/1000 | Loss: 0.00002827
Iteration 167/1000 | Loss: 0.00002826
Iteration 168/1000 | Loss: 0.00002826
Iteration 169/1000 | Loss: 0.00002826
Iteration 170/1000 | Loss: 0.00002826
Iteration 171/1000 | Loss: 0.00002826
Iteration 172/1000 | Loss: 0.00002826
Iteration 173/1000 | Loss: 0.00002826
Iteration 174/1000 | Loss: 0.00002826
Iteration 175/1000 | Loss: 0.00002826
Iteration 176/1000 | Loss: 0.00002826
Iteration 177/1000 | Loss: 0.00002826
Iteration 178/1000 | Loss: 0.00002826
Iteration 179/1000 | Loss: 0.00002826
Iteration 180/1000 | Loss: 0.00002826
Iteration 181/1000 | Loss: 0.00002826
Iteration 182/1000 | Loss: 0.00002826
Iteration 183/1000 | Loss: 0.00002826
Iteration 184/1000 | Loss: 0.00002825
Iteration 185/1000 | Loss: 0.00002825
Iteration 186/1000 | Loss: 0.00002825
Iteration 187/1000 | Loss: 0.00002825
Iteration 188/1000 | Loss: 0.00002825
Iteration 189/1000 | Loss: 0.00002825
Iteration 190/1000 | Loss: 0.00002825
Iteration 191/1000 | Loss: 0.00002825
Iteration 192/1000 | Loss: 0.00002825
Iteration 193/1000 | Loss: 0.00002825
Iteration 194/1000 | Loss: 0.00002825
Iteration 195/1000 | Loss: 0.00002825
Iteration 196/1000 | Loss: 0.00002825
Iteration 197/1000 | Loss: 0.00002825
Iteration 198/1000 | Loss: 0.00002825
Iteration 199/1000 | Loss: 0.00002825
Iteration 200/1000 | Loss: 0.00002825
Iteration 201/1000 | Loss: 0.00002825
Iteration 202/1000 | Loss: 0.00002825
Iteration 203/1000 | Loss: 0.00002825
Iteration 204/1000 | Loss: 0.00002825
Iteration 205/1000 | Loss: 0.00002825
Iteration 206/1000 | Loss: 0.00002825
Iteration 207/1000 | Loss: 0.00002825
Iteration 208/1000 | Loss: 0.00002825
Iteration 209/1000 | Loss: 0.00002825
Iteration 210/1000 | Loss: 0.00002825
Iteration 211/1000 | Loss: 0.00002825
Iteration 212/1000 | Loss: 0.00002825
Iteration 213/1000 | Loss: 0.00002825
Iteration 214/1000 | Loss: 0.00002825
Iteration 215/1000 | Loss: 0.00002825
Iteration 216/1000 | Loss: 0.00002825
Iteration 217/1000 | Loss: 0.00002825
Iteration 218/1000 | Loss: 0.00002825
Iteration 219/1000 | Loss: 0.00002825
Iteration 220/1000 | Loss: 0.00002825
Iteration 221/1000 | Loss: 0.00002825
Iteration 222/1000 | Loss: 0.00002825
Iteration 223/1000 | Loss: 0.00002825
Iteration 224/1000 | Loss: 0.00002825
Iteration 225/1000 | Loss: 0.00002825
Iteration 226/1000 | Loss: 0.00002825
Iteration 227/1000 | Loss: 0.00002825
Iteration 228/1000 | Loss: 0.00002825
Iteration 229/1000 | Loss: 0.00002825
Iteration 230/1000 | Loss: 0.00002825
Iteration 231/1000 | Loss: 0.00002825
Iteration 232/1000 | Loss: 0.00002825
Iteration 233/1000 | Loss: 0.00002825
Iteration 234/1000 | Loss: 0.00002825
Iteration 235/1000 | Loss: 0.00002825
Iteration 236/1000 | Loss: 0.00002825
Iteration 237/1000 | Loss: 0.00002825
Iteration 238/1000 | Loss: 0.00002825
Iteration 239/1000 | Loss: 0.00002825
Iteration 240/1000 | Loss: 0.00002825
Iteration 241/1000 | Loss: 0.00002825
Iteration 242/1000 | Loss: 0.00002825
Iteration 243/1000 | Loss: 0.00002825
Iteration 244/1000 | Loss: 0.00002825
Iteration 245/1000 | Loss: 0.00002825
Iteration 246/1000 | Loss: 0.00002825
Iteration 247/1000 | Loss: 0.00002825
Iteration 248/1000 | Loss: 0.00002825
Iteration 249/1000 | Loss: 0.00002825
Iteration 250/1000 | Loss: 0.00002825
Iteration 251/1000 | Loss: 0.00002825
Iteration 252/1000 | Loss: 0.00002825
Iteration 253/1000 | Loss: 0.00002825
Iteration 254/1000 | Loss: 0.00002825
Iteration 255/1000 | Loss: 0.00002825
Iteration 256/1000 | Loss: 0.00002825
Iteration 257/1000 | Loss: 0.00002825
Iteration 258/1000 | Loss: 0.00002825
Iteration 259/1000 | Loss: 0.00002825
Iteration 260/1000 | Loss: 0.00002825
Iteration 261/1000 | Loss: 0.00002825
Iteration 262/1000 | Loss: 0.00002825
Iteration 263/1000 | Loss: 0.00002825
Iteration 264/1000 | Loss: 0.00002825
Iteration 265/1000 | Loss: 0.00002825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.8246824513189495e-05, 2.8246824513189495e-05, 2.8246824513189495e-05, 2.8246824513189495e-05, 2.8246824513189495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8246824513189495e-05

Optimization complete. Final v2v error: 4.339452266693115 mm

Highest mean error: 6.554192066192627 mm for frame 70

Lowest mean error: 3.6800811290740967 mm for frame 0

Saving results

Total time: 43.2567024230957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810040
Iteration 2/25 | Loss: 0.00090388
Iteration 3/25 | Loss: 0.00070843
Iteration 4/25 | Loss: 0.00067922
Iteration 5/25 | Loss: 0.00067375
Iteration 6/25 | Loss: 0.00067158
Iteration 7/25 | Loss: 0.00067123
Iteration 8/25 | Loss: 0.00067123
Iteration 9/25 | Loss: 0.00067123
Iteration 10/25 | Loss: 0.00067123
Iteration 11/25 | Loss: 0.00067123
Iteration 12/25 | Loss: 0.00067123
Iteration 13/25 | Loss: 0.00067123
Iteration 14/25 | Loss: 0.00067123
Iteration 15/25 | Loss: 0.00067123
Iteration 16/25 | Loss: 0.00067123
Iteration 17/25 | Loss: 0.00067123
Iteration 18/25 | Loss: 0.00067123
Iteration 19/25 | Loss: 0.00067123
Iteration 20/25 | Loss: 0.00067123
Iteration 21/25 | Loss: 0.00067123
Iteration 22/25 | Loss: 0.00067123
Iteration 23/25 | Loss: 0.00067123
Iteration 24/25 | Loss: 0.00067123
Iteration 25/25 | Loss: 0.00067123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26873815
Iteration 2/25 | Loss: 0.00083711
Iteration 3/25 | Loss: 0.00083711
Iteration 4/25 | Loss: 0.00083711
Iteration 5/25 | Loss: 0.00083711
Iteration 6/25 | Loss: 0.00083711
Iteration 7/25 | Loss: 0.00083711
Iteration 8/25 | Loss: 0.00083711
Iteration 9/25 | Loss: 0.00083711
Iteration 10/25 | Loss: 0.00083711
Iteration 11/25 | Loss: 0.00083711
Iteration 12/25 | Loss: 0.00083711
Iteration 13/25 | Loss: 0.00083711
Iteration 14/25 | Loss: 0.00083711
Iteration 15/25 | Loss: 0.00083711
Iteration 16/25 | Loss: 0.00083711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000837108411360532, 0.000837108411360532, 0.000837108411360532, 0.000837108411360532, 0.000837108411360532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000837108411360532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083711
Iteration 2/1000 | Loss: 0.00003528
Iteration 3/1000 | Loss: 0.00002451
Iteration 4/1000 | Loss: 0.00002014
Iteration 5/1000 | Loss: 0.00001805
Iteration 6/1000 | Loss: 0.00001723
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001585
Iteration 10/1000 | Loss: 0.00001558
Iteration 11/1000 | Loss: 0.00001542
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001533
Iteration 16/1000 | Loss: 0.00001531
Iteration 17/1000 | Loss: 0.00001529
Iteration 18/1000 | Loss: 0.00001524
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001480
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001479
Iteration 32/1000 | Loss: 0.00001479
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001477
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001472
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001467
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001467
Iteration 69/1000 | Loss: 0.00001467
Iteration 70/1000 | Loss: 0.00001467
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001465
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001465
Iteration 90/1000 | Loss: 0.00001465
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001465
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001465
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001465
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.4652965546702035e-05, 1.4652965546702035e-05, 1.4652965546702035e-05, 1.4652965546702035e-05, 1.4652965546702035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4652965546702035e-05

Optimization complete. Final v2v error: 3.3224103450775146 mm

Highest mean error: 3.691417694091797 mm for frame 129

Lowest mean error: 2.9956719875335693 mm for frame 14

Saving results

Total time: 41.08137392997742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024692
Iteration 2/25 | Loss: 0.00270616
Iteration 3/25 | Loss: 0.00178943
Iteration 4/25 | Loss: 0.00157854
Iteration 5/25 | Loss: 0.00146989
Iteration 6/25 | Loss: 0.00131890
Iteration 7/25 | Loss: 0.00126713
Iteration 8/25 | Loss: 0.00129286
Iteration 9/25 | Loss: 0.00117606
Iteration 10/25 | Loss: 0.00114646
Iteration 11/25 | Loss: 0.00113998
Iteration 12/25 | Loss: 0.00109557
Iteration 13/25 | Loss: 0.00104759
Iteration 14/25 | Loss: 0.00103340
Iteration 15/25 | Loss: 0.00101734
Iteration 16/25 | Loss: 0.00100969
Iteration 17/25 | Loss: 0.00100851
Iteration 18/25 | Loss: 0.00100159
Iteration 19/25 | Loss: 0.00100066
Iteration 20/25 | Loss: 0.00099338
Iteration 21/25 | Loss: 0.00099907
Iteration 22/25 | Loss: 0.00099063
Iteration 23/25 | Loss: 0.00099287
Iteration 24/25 | Loss: 0.00099723
Iteration 25/25 | Loss: 0.00099595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26603210
Iteration 2/25 | Loss: 0.00222791
Iteration 3/25 | Loss: 0.00221806
Iteration 4/25 | Loss: 0.00221806
Iteration 5/25 | Loss: 0.00221806
Iteration 6/25 | Loss: 0.00221806
Iteration 7/25 | Loss: 0.00221806
Iteration 8/25 | Loss: 0.00221806
Iteration 9/25 | Loss: 0.00221806
Iteration 10/25 | Loss: 0.00221806
Iteration 11/25 | Loss: 0.00221806
Iteration 12/25 | Loss: 0.00221806
Iteration 13/25 | Loss: 0.00221806
Iteration 14/25 | Loss: 0.00221806
Iteration 15/25 | Loss: 0.00221806
Iteration 16/25 | Loss: 0.00221806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022180599626153708, 0.0022180599626153708, 0.0022180599626153708, 0.0022180599626153708, 0.0022180599626153708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022180599626153708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221806
Iteration 2/1000 | Loss: 0.00060816
Iteration 3/1000 | Loss: 0.00061263
Iteration 4/1000 | Loss: 0.00065218
Iteration 5/1000 | Loss: 0.00062389
Iteration 6/1000 | Loss: 0.00054171
Iteration 7/1000 | Loss: 0.00028005
Iteration 8/1000 | Loss: 0.00086499
Iteration 9/1000 | Loss: 0.00031272
Iteration 10/1000 | Loss: 0.00027392
Iteration 11/1000 | Loss: 0.00037335
Iteration 12/1000 | Loss: 0.00044841
Iteration 13/1000 | Loss: 0.00049964
Iteration 14/1000 | Loss: 0.00035703
Iteration 15/1000 | Loss: 0.00014908
Iteration 16/1000 | Loss: 0.00155013
Iteration 17/1000 | Loss: 0.00015937
Iteration 18/1000 | Loss: 0.00011799
Iteration 19/1000 | Loss: 0.00024009
Iteration 20/1000 | Loss: 0.00022317
Iteration 21/1000 | Loss: 0.00029574
Iteration 22/1000 | Loss: 0.00020473
Iteration 23/1000 | Loss: 0.00019738
Iteration 24/1000 | Loss: 0.00020305
Iteration 25/1000 | Loss: 0.00019646
Iteration 26/1000 | Loss: 0.00019402
Iteration 27/1000 | Loss: 0.00018167
Iteration 28/1000 | Loss: 0.00018345
Iteration 29/1000 | Loss: 0.00010603
Iteration 30/1000 | Loss: 0.00013118
Iteration 31/1000 | Loss: 0.00019850
Iteration 32/1000 | Loss: 0.00023711
Iteration 33/1000 | Loss: 0.00013759
Iteration 34/1000 | Loss: 0.00015383
Iteration 35/1000 | Loss: 0.00011147
Iteration 36/1000 | Loss: 0.00024907
Iteration 37/1000 | Loss: 0.00022008
Iteration 38/1000 | Loss: 0.00023624
Iteration 39/1000 | Loss: 0.00023314
Iteration 40/1000 | Loss: 0.00017782
Iteration 41/1000 | Loss: 0.00030977
Iteration 42/1000 | Loss: 0.00010434
Iteration 43/1000 | Loss: 0.00010072
Iteration 44/1000 | Loss: 0.00009375
Iteration 45/1000 | Loss: 0.00009238
Iteration 46/1000 | Loss: 0.00009212
Iteration 47/1000 | Loss: 0.00025883
Iteration 48/1000 | Loss: 0.00019725
Iteration 49/1000 | Loss: 0.00008925
Iteration 50/1000 | Loss: 0.00008838
Iteration 51/1000 | Loss: 0.00009820
Iteration 52/1000 | Loss: 0.00029022
Iteration 53/1000 | Loss: 0.00016429
Iteration 54/1000 | Loss: 0.00026595
Iteration 55/1000 | Loss: 0.00018205
Iteration 56/1000 | Loss: 0.00035079
Iteration 57/1000 | Loss: 0.00017713
Iteration 58/1000 | Loss: 0.00025671
Iteration 59/1000 | Loss: 0.00023623
Iteration 60/1000 | Loss: 0.00031656
Iteration 61/1000 | Loss: 0.00019927
Iteration 62/1000 | Loss: 0.00012228
Iteration 63/1000 | Loss: 0.00086761
Iteration 64/1000 | Loss: 0.00069314
Iteration 65/1000 | Loss: 0.00022238
Iteration 66/1000 | Loss: 0.00014738
Iteration 67/1000 | Loss: 0.00019830
Iteration 68/1000 | Loss: 0.00022261
Iteration 69/1000 | Loss: 0.00009711
Iteration 70/1000 | Loss: 0.00021551
Iteration 71/1000 | Loss: 0.00023683
Iteration 72/1000 | Loss: 0.00017951
Iteration 73/1000 | Loss: 0.00021098
Iteration 74/1000 | Loss: 0.00024010
Iteration 75/1000 | Loss: 0.00019755
Iteration 76/1000 | Loss: 0.00081121
Iteration 77/1000 | Loss: 0.00044407
Iteration 78/1000 | Loss: 0.00016750
Iteration 79/1000 | Loss: 0.00021973
Iteration 80/1000 | Loss: 0.00020740
Iteration 81/1000 | Loss: 0.00017100
Iteration 82/1000 | Loss: 0.00017001
Iteration 83/1000 | Loss: 0.00021286
Iteration 84/1000 | Loss: 0.00018153
Iteration 85/1000 | Loss: 0.00018480
Iteration 86/1000 | Loss: 0.00017543
Iteration 87/1000 | Loss: 0.00010018
Iteration 88/1000 | Loss: 0.00033987
Iteration 89/1000 | Loss: 0.00011580
Iteration 90/1000 | Loss: 0.00009769
Iteration 91/1000 | Loss: 0.00009341
Iteration 92/1000 | Loss: 0.00009247
Iteration 93/1000 | Loss: 0.00009297
Iteration 94/1000 | Loss: 0.00068320
Iteration 95/1000 | Loss: 0.00061624
Iteration 96/1000 | Loss: 0.00026234
Iteration 97/1000 | Loss: 0.00030587
Iteration 98/1000 | Loss: 0.00026033
Iteration 99/1000 | Loss: 0.00009602
Iteration 100/1000 | Loss: 0.00060218
Iteration 101/1000 | Loss: 0.00011265
Iteration 102/1000 | Loss: 0.00009356
Iteration 103/1000 | Loss: 0.00008869
Iteration 104/1000 | Loss: 0.00008526
Iteration 105/1000 | Loss: 0.00009162
Iteration 106/1000 | Loss: 0.00010743
Iteration 107/1000 | Loss: 0.00010370
Iteration 108/1000 | Loss: 0.00008341
Iteration 109/1000 | Loss: 0.00009982
Iteration 110/1000 | Loss: 0.00009013
Iteration 111/1000 | Loss: 0.00008277
Iteration 112/1000 | Loss: 0.00009019
Iteration 113/1000 | Loss: 0.00008265
Iteration 114/1000 | Loss: 0.00008093
Iteration 115/1000 | Loss: 0.00008060
Iteration 116/1000 | Loss: 0.00008032
Iteration 117/1000 | Loss: 0.00009393
Iteration 118/1000 | Loss: 0.00008001
Iteration 119/1000 | Loss: 0.00007986
Iteration 120/1000 | Loss: 0.00008524
Iteration 121/1000 | Loss: 0.00007975
Iteration 122/1000 | Loss: 0.00007975
Iteration 123/1000 | Loss: 0.00007975
Iteration 124/1000 | Loss: 0.00007975
Iteration 125/1000 | Loss: 0.00007975
Iteration 126/1000 | Loss: 0.00007975
Iteration 127/1000 | Loss: 0.00007975
Iteration 128/1000 | Loss: 0.00007975
Iteration 129/1000 | Loss: 0.00007975
Iteration 130/1000 | Loss: 0.00007975
Iteration 131/1000 | Loss: 0.00007974
Iteration 132/1000 | Loss: 0.00008388
Iteration 133/1000 | Loss: 0.00007967
Iteration 134/1000 | Loss: 0.00007965
Iteration 135/1000 | Loss: 0.00007965
Iteration 136/1000 | Loss: 0.00007965
Iteration 137/1000 | Loss: 0.00007965
Iteration 138/1000 | Loss: 0.00007965
Iteration 139/1000 | Loss: 0.00007965
Iteration 140/1000 | Loss: 0.00007965
Iteration 141/1000 | Loss: 0.00007964
Iteration 142/1000 | Loss: 0.00007964
Iteration 143/1000 | Loss: 0.00007964
Iteration 144/1000 | Loss: 0.00007964
Iteration 145/1000 | Loss: 0.00007964
Iteration 146/1000 | Loss: 0.00007964
Iteration 147/1000 | Loss: 0.00007964
Iteration 148/1000 | Loss: 0.00007961
Iteration 149/1000 | Loss: 0.00007961
Iteration 150/1000 | Loss: 0.00007960
Iteration 151/1000 | Loss: 0.00007960
Iteration 152/1000 | Loss: 0.00007960
Iteration 153/1000 | Loss: 0.00007960
Iteration 154/1000 | Loss: 0.00007960
Iteration 155/1000 | Loss: 0.00007960
Iteration 156/1000 | Loss: 0.00007960
Iteration 157/1000 | Loss: 0.00007960
Iteration 158/1000 | Loss: 0.00007959
Iteration 159/1000 | Loss: 0.00007959
Iteration 160/1000 | Loss: 0.00007959
Iteration 161/1000 | Loss: 0.00007958
Iteration 162/1000 | Loss: 0.00007958
Iteration 163/1000 | Loss: 0.00007958
Iteration 164/1000 | Loss: 0.00007958
Iteration 165/1000 | Loss: 0.00007958
Iteration 166/1000 | Loss: 0.00007958
Iteration 167/1000 | Loss: 0.00007957
Iteration 168/1000 | Loss: 0.00007957
Iteration 169/1000 | Loss: 0.00007957
Iteration 170/1000 | Loss: 0.00007956
Iteration 171/1000 | Loss: 0.00007956
Iteration 172/1000 | Loss: 0.00007956
Iteration 173/1000 | Loss: 0.00007956
Iteration 174/1000 | Loss: 0.00007956
Iteration 175/1000 | Loss: 0.00007956
Iteration 176/1000 | Loss: 0.00007956
Iteration 177/1000 | Loss: 0.00007956
Iteration 178/1000 | Loss: 0.00007956
Iteration 179/1000 | Loss: 0.00007956
Iteration 180/1000 | Loss: 0.00007955
Iteration 181/1000 | Loss: 0.00007955
Iteration 182/1000 | Loss: 0.00007955
Iteration 183/1000 | Loss: 0.00007955
Iteration 184/1000 | Loss: 0.00007954
Iteration 185/1000 | Loss: 0.00007953
Iteration 186/1000 | Loss: 0.00007953
Iteration 187/1000 | Loss: 0.00007953
Iteration 188/1000 | Loss: 0.00007953
Iteration 189/1000 | Loss: 0.00007953
Iteration 190/1000 | Loss: 0.00008757
Iteration 191/1000 | Loss: 0.00007984
Iteration 192/1000 | Loss: 0.00007947
Iteration 193/1000 | Loss: 0.00007946
Iteration 194/1000 | Loss: 0.00007945
Iteration 195/1000 | Loss: 0.00007945
Iteration 196/1000 | Loss: 0.00007945
Iteration 197/1000 | Loss: 0.00007945
Iteration 198/1000 | Loss: 0.00007945
Iteration 199/1000 | Loss: 0.00007945
Iteration 200/1000 | Loss: 0.00007945
Iteration 201/1000 | Loss: 0.00007944
Iteration 202/1000 | Loss: 0.00007944
Iteration 203/1000 | Loss: 0.00007944
Iteration 204/1000 | Loss: 0.00007944
Iteration 205/1000 | Loss: 0.00007944
Iteration 206/1000 | Loss: 0.00007944
Iteration 207/1000 | Loss: 0.00007944
Iteration 208/1000 | Loss: 0.00007944
Iteration 209/1000 | Loss: 0.00007943
Iteration 210/1000 | Loss: 0.00007943
Iteration 211/1000 | Loss: 0.00007943
Iteration 212/1000 | Loss: 0.00007943
Iteration 213/1000 | Loss: 0.00007943
Iteration 214/1000 | Loss: 0.00007943
Iteration 215/1000 | Loss: 0.00007943
Iteration 216/1000 | Loss: 0.00007943
Iteration 217/1000 | Loss: 0.00007943
Iteration 218/1000 | Loss: 0.00007943
Iteration 219/1000 | Loss: 0.00007943
Iteration 220/1000 | Loss: 0.00007943
Iteration 221/1000 | Loss: 0.00007943
Iteration 222/1000 | Loss: 0.00007943
Iteration 223/1000 | Loss: 0.00007943
Iteration 224/1000 | Loss: 0.00007943
Iteration 225/1000 | Loss: 0.00007943
Iteration 226/1000 | Loss: 0.00007943
Iteration 227/1000 | Loss: 0.00007943
Iteration 228/1000 | Loss: 0.00007943
Iteration 229/1000 | Loss: 0.00007943
Iteration 230/1000 | Loss: 0.00007943
Iteration 231/1000 | Loss: 0.00007943
Iteration 232/1000 | Loss: 0.00007943
Iteration 233/1000 | Loss: 0.00007943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [7.943006494315341e-05, 7.943006494315341e-05, 7.943006494315341e-05, 7.943006494315341e-05, 7.943006494315341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.943006494315341e-05

Optimization complete. Final v2v error: 5.73076057434082 mm

Highest mean error: 12.26986026763916 mm for frame 8

Lowest mean error: 3.8605446815490723 mm for frame 45

Saving results

Total time: 218.4202744960785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0539/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0539/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805982
Iteration 2/25 | Loss: 0.00116581
Iteration 3/25 | Loss: 0.00080462
Iteration 4/25 | Loss: 0.00075183
Iteration 5/25 | Loss: 0.00073569
Iteration 6/25 | Loss: 0.00074049
Iteration 7/25 | Loss: 0.00073136
Iteration 8/25 | Loss: 0.00072260
Iteration 9/25 | Loss: 0.00071806
Iteration 10/25 | Loss: 0.00071702
Iteration 11/25 | Loss: 0.00071524
Iteration 12/25 | Loss: 0.00071438
Iteration 13/25 | Loss: 0.00071310
Iteration 14/25 | Loss: 0.00071359
Iteration 15/25 | Loss: 0.00071213
Iteration 16/25 | Loss: 0.00070881
Iteration 17/25 | Loss: 0.00071081
Iteration 18/25 | Loss: 0.00071081
Iteration 19/25 | Loss: 0.00070996
Iteration 20/25 | Loss: 0.00071019
Iteration 21/25 | Loss: 0.00071075
Iteration 22/25 | Loss: 0.00071045
Iteration 23/25 | Loss: 0.00071033
Iteration 24/25 | Loss: 0.00071033
Iteration 25/25 | Loss: 0.00071110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79435611
Iteration 2/25 | Loss: 0.00083928
Iteration 3/25 | Loss: 0.00083928
Iteration 4/25 | Loss: 0.00083928
Iteration 5/25 | Loss: 0.00083928
Iteration 6/25 | Loss: 0.00083928
Iteration 7/25 | Loss: 0.00083928
Iteration 8/25 | Loss: 0.00083928
Iteration 9/25 | Loss: 0.00083928
Iteration 10/25 | Loss: 0.00083928
Iteration 11/25 | Loss: 0.00083928
Iteration 12/25 | Loss: 0.00083928
Iteration 13/25 | Loss: 0.00083928
Iteration 14/25 | Loss: 0.00083928
Iteration 15/25 | Loss: 0.00083928
Iteration 16/25 | Loss: 0.00083928
Iteration 17/25 | Loss: 0.00083928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000839277112390846, 0.000839277112390846, 0.000839277112390846, 0.000839277112390846, 0.000839277112390846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000839277112390846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083928
Iteration 2/1000 | Loss: 0.00010307
Iteration 3/1000 | Loss: 0.00004588
Iteration 4/1000 | Loss: 0.00011899
Iteration 5/1000 | Loss: 0.00010251
Iteration 6/1000 | Loss: 0.00014177
Iteration 7/1000 | Loss: 0.00010368
Iteration 8/1000 | Loss: 0.00011261
Iteration 9/1000 | Loss: 0.00003979
Iteration 10/1000 | Loss: 0.00005969
Iteration 11/1000 | Loss: 0.00010638
Iteration 12/1000 | Loss: 0.00006976
Iteration 13/1000 | Loss: 0.00007926
Iteration 14/1000 | Loss: 0.00015844
Iteration 15/1000 | Loss: 0.00011369
Iteration 16/1000 | Loss: 0.00002340
Iteration 17/1000 | Loss: 0.00010491
Iteration 18/1000 | Loss: 0.00010186
Iteration 19/1000 | Loss: 0.00002858
Iteration 20/1000 | Loss: 0.00002465
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00017463
Iteration 23/1000 | Loss: 0.00013172
Iteration 24/1000 | Loss: 0.00015082
Iteration 25/1000 | Loss: 0.00005682
Iteration 26/1000 | Loss: 0.00009791
Iteration 27/1000 | Loss: 0.00004551
Iteration 28/1000 | Loss: 0.00011381
Iteration 29/1000 | Loss: 0.00004878
Iteration 30/1000 | Loss: 0.00008837
Iteration 31/1000 | Loss: 0.00010284
Iteration 32/1000 | Loss: 0.00011841
Iteration 33/1000 | Loss: 0.00019017
Iteration 34/1000 | Loss: 0.00008166
Iteration 35/1000 | Loss: 0.00009538
Iteration 36/1000 | Loss: 0.00008629
Iteration 37/1000 | Loss: 0.00006552
Iteration 38/1000 | Loss: 0.00002838
Iteration 39/1000 | Loss: 0.00002360
Iteration 40/1000 | Loss: 0.00003508
Iteration 41/1000 | Loss: 0.00002282
Iteration 42/1000 | Loss: 0.00002101
Iteration 43/1000 | Loss: 0.00002745
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001984
Iteration 48/1000 | Loss: 0.00001979
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001934
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001931
Iteration 55/1000 | Loss: 0.00001930
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001929
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001928
Iteration 63/1000 | Loss: 0.00001928
Iteration 64/1000 | Loss: 0.00001928
Iteration 65/1000 | Loss: 0.00001928
Iteration 66/1000 | Loss: 0.00001927
Iteration 67/1000 | Loss: 0.00001927
Iteration 68/1000 | Loss: 0.00001927
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001926
Iteration 73/1000 | Loss: 0.00001926
Iteration 74/1000 | Loss: 0.00001925
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001925
Iteration 78/1000 | Loss: 0.00001925
Iteration 79/1000 | Loss: 0.00001925
Iteration 80/1000 | Loss: 0.00001925
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001923
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001919
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001918
Iteration 104/1000 | Loss: 0.00001918
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001918
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001917
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001916
Iteration 119/1000 | Loss: 0.00001916
Iteration 120/1000 | Loss: 0.00001916
Iteration 121/1000 | Loss: 0.00001916
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001916
Iteration 124/1000 | Loss: 0.00001916
Iteration 125/1000 | Loss: 0.00001916
Iteration 126/1000 | Loss: 0.00001916
Iteration 127/1000 | Loss: 0.00001915
Iteration 128/1000 | Loss: 0.00001915
Iteration 129/1000 | Loss: 0.00001915
Iteration 130/1000 | Loss: 0.00001915
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001915
Iteration 134/1000 | Loss: 0.00001915
Iteration 135/1000 | Loss: 0.00001915
Iteration 136/1000 | Loss: 0.00001915
Iteration 137/1000 | Loss: 0.00001915
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001914
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001914
Iteration 143/1000 | Loss: 0.00001914
Iteration 144/1000 | Loss: 0.00001914
Iteration 145/1000 | Loss: 0.00001914
Iteration 146/1000 | Loss: 0.00001914
Iteration 147/1000 | Loss: 0.00001914
Iteration 148/1000 | Loss: 0.00001914
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001913
Iteration 152/1000 | Loss: 0.00001913
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001913
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001913
Iteration 159/1000 | Loss: 0.00001912
Iteration 160/1000 | Loss: 0.00001912
Iteration 161/1000 | Loss: 0.00001912
Iteration 162/1000 | Loss: 0.00001912
Iteration 163/1000 | Loss: 0.00001912
Iteration 164/1000 | Loss: 0.00001912
Iteration 165/1000 | Loss: 0.00001911
Iteration 166/1000 | Loss: 0.00001911
Iteration 167/1000 | Loss: 0.00001911
Iteration 168/1000 | Loss: 0.00001911
Iteration 169/1000 | Loss: 0.00001911
Iteration 170/1000 | Loss: 0.00001911
Iteration 171/1000 | Loss: 0.00001911
Iteration 172/1000 | Loss: 0.00001911
Iteration 173/1000 | Loss: 0.00001911
Iteration 174/1000 | Loss: 0.00001911
Iteration 175/1000 | Loss: 0.00001911
Iteration 176/1000 | Loss: 0.00001911
Iteration 177/1000 | Loss: 0.00001911
Iteration 178/1000 | Loss: 0.00001911
Iteration 179/1000 | Loss: 0.00001911
Iteration 180/1000 | Loss: 0.00001911
Iteration 181/1000 | Loss: 0.00001911
Iteration 182/1000 | Loss: 0.00001911
Iteration 183/1000 | Loss: 0.00001911
Iteration 184/1000 | Loss: 0.00001911
Iteration 185/1000 | Loss: 0.00001911
Iteration 186/1000 | Loss: 0.00001911
Iteration 187/1000 | Loss: 0.00001911
Iteration 188/1000 | Loss: 0.00001911
Iteration 189/1000 | Loss: 0.00001911
Iteration 190/1000 | Loss: 0.00001911
Iteration 191/1000 | Loss: 0.00001911
Iteration 192/1000 | Loss: 0.00001911
Iteration 193/1000 | Loss: 0.00001911
Iteration 194/1000 | Loss: 0.00001911
Iteration 195/1000 | Loss: 0.00001911
Iteration 196/1000 | Loss: 0.00001911
Iteration 197/1000 | Loss: 0.00001911
Iteration 198/1000 | Loss: 0.00001911
Iteration 199/1000 | Loss: 0.00001911
Iteration 200/1000 | Loss: 0.00001911
Iteration 201/1000 | Loss: 0.00001911
Iteration 202/1000 | Loss: 0.00001911
Iteration 203/1000 | Loss: 0.00001911
Iteration 204/1000 | Loss: 0.00001911
Iteration 205/1000 | Loss: 0.00001911
Iteration 206/1000 | Loss: 0.00001911
Iteration 207/1000 | Loss: 0.00001911
Iteration 208/1000 | Loss: 0.00001911
Iteration 209/1000 | Loss: 0.00001911
Iteration 210/1000 | Loss: 0.00001911
Iteration 211/1000 | Loss: 0.00001911
Iteration 212/1000 | Loss: 0.00001911
Iteration 213/1000 | Loss: 0.00001911
Iteration 214/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.9106690160697326e-05, 1.9106690160697326e-05, 1.9106690160697326e-05, 1.9106690160697326e-05, 1.9106690160697326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9106690160697326e-05

Optimization complete. Final v2v error: 3.768670082092285 mm

Highest mean error: 4.211246967315674 mm for frame 39

Lowest mean error: 3.1967875957489014 mm for frame 234

Saving results

Total time: 134.50120663642883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449714
Iteration 2/25 | Loss: 0.00131392
Iteration 3/25 | Loss: 0.00115009
Iteration 4/25 | Loss: 0.00112604
Iteration 5/25 | Loss: 0.00111983
Iteration 6/25 | Loss: 0.00111834
Iteration 7/25 | Loss: 0.00111834
Iteration 8/25 | Loss: 0.00111834
Iteration 9/25 | Loss: 0.00111834
Iteration 10/25 | Loss: 0.00111833
Iteration 11/25 | Loss: 0.00111834
Iteration 12/25 | Loss: 0.00111834
Iteration 13/25 | Loss: 0.00111834
Iteration 14/25 | Loss: 0.00111834
Iteration 15/25 | Loss: 0.00111834
Iteration 16/25 | Loss: 0.00111834
Iteration 17/25 | Loss: 0.00111834
Iteration 18/25 | Loss: 0.00111834
Iteration 19/25 | Loss: 0.00111834
Iteration 20/25 | Loss: 0.00111833
Iteration 21/25 | Loss: 0.00111833
Iteration 22/25 | Loss: 0.00111833
Iteration 23/25 | Loss: 0.00111833
Iteration 24/25 | Loss: 0.00111833
Iteration 25/25 | Loss: 0.00111834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25930822
Iteration 2/25 | Loss: 0.00115721
Iteration 3/25 | Loss: 0.00115721
Iteration 4/25 | Loss: 0.00115721
Iteration 5/25 | Loss: 0.00115721
Iteration 6/25 | Loss: 0.00115721
Iteration 7/25 | Loss: 0.00115721
Iteration 8/25 | Loss: 0.00115720
Iteration 9/25 | Loss: 0.00115720
Iteration 10/25 | Loss: 0.00115720
Iteration 11/25 | Loss: 0.00115720
Iteration 12/25 | Loss: 0.00115720
Iteration 13/25 | Loss: 0.00115720
Iteration 14/25 | Loss: 0.00115720
Iteration 15/25 | Loss: 0.00115720
Iteration 16/25 | Loss: 0.00115720
Iteration 17/25 | Loss: 0.00115720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011572042712941766, 0.0011572042712941766, 0.0011572042712941766, 0.0011572042712941766, 0.0011572042712941766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011572042712941766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115720
Iteration 2/1000 | Loss: 0.00004642
Iteration 3/1000 | Loss: 0.00002516
Iteration 4/1000 | Loss: 0.00002082
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001719
Iteration 8/1000 | Loss: 0.00001672
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001584
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001491
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001490
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001473
Iteration 81/1000 | Loss: 0.00001473
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001466
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001464
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001463
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001461
Iteration 157/1000 | Loss: 0.00001461
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001459
Iteration 169/1000 | Loss: 0.00001459
Iteration 170/1000 | Loss: 0.00001459
Iteration 171/1000 | Loss: 0.00001459
Iteration 172/1000 | Loss: 0.00001459
Iteration 173/1000 | Loss: 0.00001459
Iteration 174/1000 | Loss: 0.00001458
Iteration 175/1000 | Loss: 0.00001458
Iteration 176/1000 | Loss: 0.00001458
Iteration 177/1000 | Loss: 0.00001457
Iteration 178/1000 | Loss: 0.00001457
Iteration 179/1000 | Loss: 0.00001457
Iteration 180/1000 | Loss: 0.00001457
Iteration 181/1000 | Loss: 0.00001457
Iteration 182/1000 | Loss: 0.00001457
Iteration 183/1000 | Loss: 0.00001456
Iteration 184/1000 | Loss: 0.00001456
Iteration 185/1000 | Loss: 0.00001456
Iteration 186/1000 | Loss: 0.00001456
Iteration 187/1000 | Loss: 0.00001455
Iteration 188/1000 | Loss: 0.00001455
Iteration 189/1000 | Loss: 0.00001455
Iteration 190/1000 | Loss: 0.00001455
Iteration 191/1000 | Loss: 0.00001455
Iteration 192/1000 | Loss: 0.00001455
Iteration 193/1000 | Loss: 0.00001455
Iteration 194/1000 | Loss: 0.00001455
Iteration 195/1000 | Loss: 0.00001455
Iteration 196/1000 | Loss: 0.00001455
Iteration 197/1000 | Loss: 0.00001455
Iteration 198/1000 | Loss: 0.00001455
Iteration 199/1000 | Loss: 0.00001455
Iteration 200/1000 | Loss: 0.00001455
Iteration 201/1000 | Loss: 0.00001455
Iteration 202/1000 | Loss: 0.00001455
Iteration 203/1000 | Loss: 0.00001455
Iteration 204/1000 | Loss: 0.00001455
Iteration 205/1000 | Loss: 0.00001455
Iteration 206/1000 | Loss: 0.00001455
Iteration 207/1000 | Loss: 0.00001455
Iteration 208/1000 | Loss: 0.00001455
Iteration 209/1000 | Loss: 0.00001455
Iteration 210/1000 | Loss: 0.00001455
Iteration 211/1000 | Loss: 0.00001455
Iteration 212/1000 | Loss: 0.00001455
Iteration 213/1000 | Loss: 0.00001455
Iteration 214/1000 | Loss: 0.00001455
Iteration 215/1000 | Loss: 0.00001455
Iteration 216/1000 | Loss: 0.00001455
Iteration 217/1000 | Loss: 0.00001455
Iteration 218/1000 | Loss: 0.00001455
Iteration 219/1000 | Loss: 0.00001455
Iteration 220/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.4551357708114665e-05, 1.4551357708114665e-05, 1.4551357708114665e-05, 1.4551357708114665e-05, 1.4551357708114665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4551357708114665e-05

Optimization complete. Final v2v error: 3.2135684490203857 mm

Highest mean error: 3.8537192344665527 mm for frame 247

Lowest mean error: 2.814687490463257 mm for frame 21

Saving results

Total time: 49.89724898338318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512728
Iteration 2/25 | Loss: 0.00159277
Iteration 3/25 | Loss: 0.00131450
Iteration 4/25 | Loss: 0.00127394
Iteration 5/25 | Loss: 0.00126081
Iteration 6/25 | Loss: 0.00126561
Iteration 7/25 | Loss: 0.00125771
Iteration 8/25 | Loss: 0.00124745
Iteration 9/25 | Loss: 0.00124508
Iteration 10/25 | Loss: 0.00124459
Iteration 11/25 | Loss: 0.00124444
Iteration 12/25 | Loss: 0.00124547
Iteration 13/25 | Loss: 0.00124462
Iteration 14/25 | Loss: 0.00124403
Iteration 15/25 | Loss: 0.00124339
Iteration 16/25 | Loss: 0.00124303
Iteration 17/25 | Loss: 0.00124285
Iteration 18/25 | Loss: 0.00124280
Iteration 19/25 | Loss: 0.00124280
Iteration 20/25 | Loss: 0.00124280
Iteration 21/25 | Loss: 0.00124280
Iteration 22/25 | Loss: 0.00124280
Iteration 23/25 | Loss: 0.00124279
Iteration 24/25 | Loss: 0.00124279
Iteration 25/25 | Loss: 0.00124279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33328128
Iteration 2/25 | Loss: 0.00088101
Iteration 3/25 | Loss: 0.00088099
Iteration 4/25 | Loss: 0.00088099
Iteration 5/25 | Loss: 0.00088098
Iteration 6/25 | Loss: 0.00088098
Iteration 7/25 | Loss: 0.00088098
Iteration 8/25 | Loss: 0.00088098
Iteration 9/25 | Loss: 0.00088098
Iteration 10/25 | Loss: 0.00088098
Iteration 11/25 | Loss: 0.00088098
Iteration 12/25 | Loss: 0.00088098
Iteration 13/25 | Loss: 0.00088098
Iteration 14/25 | Loss: 0.00088098
Iteration 15/25 | Loss: 0.00088098
Iteration 16/25 | Loss: 0.00088098
Iteration 17/25 | Loss: 0.00088098
Iteration 18/25 | Loss: 0.00088098
Iteration 19/25 | Loss: 0.00088098
Iteration 20/25 | Loss: 0.00088098
Iteration 21/25 | Loss: 0.00088098
Iteration 22/25 | Loss: 0.00088098
Iteration 23/25 | Loss: 0.00088098
Iteration 24/25 | Loss: 0.00088098
Iteration 25/25 | Loss: 0.00088098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088098
Iteration 2/1000 | Loss: 0.00041101
Iteration 3/1000 | Loss: 0.00058990
Iteration 4/1000 | Loss: 0.00017696
Iteration 5/1000 | Loss: 0.00014120
Iteration 6/1000 | Loss: 0.00022129
Iteration 7/1000 | Loss: 0.00017855
Iteration 8/1000 | Loss: 0.00023677
Iteration 9/1000 | Loss: 0.00028945
Iteration 10/1000 | Loss: 0.00019672
Iteration 11/1000 | Loss: 0.00014659
Iteration 12/1000 | Loss: 0.00013776
Iteration 13/1000 | Loss: 0.00014316
Iteration 14/1000 | Loss: 0.00011330
Iteration 15/1000 | Loss: 0.00015717
Iteration 16/1000 | Loss: 0.00011356
Iteration 17/1000 | Loss: 0.00014761
Iteration 18/1000 | Loss: 0.00011463
Iteration 19/1000 | Loss: 0.00022858
Iteration 20/1000 | Loss: 0.00015414
Iteration 21/1000 | Loss: 0.00004830
Iteration 22/1000 | Loss: 0.00004327
Iteration 23/1000 | Loss: 0.00004137
Iteration 24/1000 | Loss: 0.00004019
Iteration 25/1000 | Loss: 0.00006290
Iteration 26/1000 | Loss: 0.00004369
Iteration 27/1000 | Loss: 0.00008689
Iteration 28/1000 | Loss: 0.00008452
Iteration 29/1000 | Loss: 0.00006306
Iteration 30/1000 | Loss: 0.00037073
Iteration 31/1000 | Loss: 0.00011975
Iteration 32/1000 | Loss: 0.00004125
Iteration 33/1000 | Loss: 0.00003906
Iteration 34/1000 | Loss: 0.00003823
Iteration 35/1000 | Loss: 0.00003789
Iteration 36/1000 | Loss: 0.00003741
Iteration 37/1000 | Loss: 0.00022727
Iteration 38/1000 | Loss: 0.00010291
Iteration 39/1000 | Loss: 0.00003758
Iteration 40/1000 | Loss: 0.00003711
Iteration 41/1000 | Loss: 0.00008565
Iteration 42/1000 | Loss: 0.00005651
Iteration 43/1000 | Loss: 0.00027366
Iteration 44/1000 | Loss: 0.00012054
Iteration 45/1000 | Loss: 0.00003749
Iteration 46/1000 | Loss: 0.00003700
Iteration 47/1000 | Loss: 0.00021131
Iteration 48/1000 | Loss: 0.00008827
Iteration 49/1000 | Loss: 0.00004583
Iteration 50/1000 | Loss: 0.00004144
Iteration 51/1000 | Loss: 0.00003777
Iteration 52/1000 | Loss: 0.00003695
Iteration 53/1000 | Loss: 0.00020290
Iteration 54/1000 | Loss: 0.00018868
Iteration 55/1000 | Loss: 0.00009328
Iteration 56/1000 | Loss: 0.00003931
Iteration 57/1000 | Loss: 0.00003762
Iteration 58/1000 | Loss: 0.00003682
Iteration 59/1000 | Loss: 0.00003656
Iteration 60/1000 | Loss: 0.00003640
Iteration 61/1000 | Loss: 0.00003630
Iteration 62/1000 | Loss: 0.00003627
Iteration 63/1000 | Loss: 0.00003627
Iteration 64/1000 | Loss: 0.00003627
Iteration 65/1000 | Loss: 0.00003627
Iteration 66/1000 | Loss: 0.00003627
Iteration 67/1000 | Loss: 0.00003627
Iteration 68/1000 | Loss: 0.00003627
Iteration 69/1000 | Loss: 0.00003627
Iteration 70/1000 | Loss: 0.00003627
Iteration 71/1000 | Loss: 0.00003627
Iteration 72/1000 | Loss: 0.00003627
Iteration 73/1000 | Loss: 0.00003626
Iteration 74/1000 | Loss: 0.00003624
Iteration 75/1000 | Loss: 0.00003623
Iteration 76/1000 | Loss: 0.00003623
Iteration 77/1000 | Loss: 0.00003622
Iteration 78/1000 | Loss: 0.00003620
Iteration 79/1000 | Loss: 0.00003620
Iteration 80/1000 | Loss: 0.00003619
Iteration 81/1000 | Loss: 0.00003619
Iteration 82/1000 | Loss: 0.00003619
Iteration 83/1000 | Loss: 0.00003618
Iteration 84/1000 | Loss: 0.00003618
Iteration 85/1000 | Loss: 0.00003617
Iteration 86/1000 | Loss: 0.00003617
Iteration 87/1000 | Loss: 0.00003617
Iteration 88/1000 | Loss: 0.00003617
Iteration 89/1000 | Loss: 0.00003617
Iteration 90/1000 | Loss: 0.00003617
Iteration 91/1000 | Loss: 0.00003617
Iteration 92/1000 | Loss: 0.00003617
Iteration 93/1000 | Loss: 0.00003616
Iteration 94/1000 | Loss: 0.00003616
Iteration 95/1000 | Loss: 0.00003616
Iteration 96/1000 | Loss: 0.00003616
Iteration 97/1000 | Loss: 0.00003616
Iteration 98/1000 | Loss: 0.00003616
Iteration 99/1000 | Loss: 0.00003616
Iteration 100/1000 | Loss: 0.00003616
Iteration 101/1000 | Loss: 0.00003615
Iteration 102/1000 | Loss: 0.00003615
Iteration 103/1000 | Loss: 0.00003615
Iteration 104/1000 | Loss: 0.00003615
Iteration 105/1000 | Loss: 0.00003615
Iteration 106/1000 | Loss: 0.00003615
Iteration 107/1000 | Loss: 0.00003615
Iteration 108/1000 | Loss: 0.00003614
Iteration 109/1000 | Loss: 0.00003614
Iteration 110/1000 | Loss: 0.00003614
Iteration 111/1000 | Loss: 0.00003614
Iteration 112/1000 | Loss: 0.00003614
Iteration 113/1000 | Loss: 0.00003614
Iteration 114/1000 | Loss: 0.00003613
Iteration 115/1000 | Loss: 0.00003613
Iteration 116/1000 | Loss: 0.00003613
Iteration 117/1000 | Loss: 0.00003613
Iteration 118/1000 | Loss: 0.00003613
Iteration 119/1000 | Loss: 0.00003613
Iteration 120/1000 | Loss: 0.00003612
Iteration 121/1000 | Loss: 0.00003612
Iteration 122/1000 | Loss: 0.00003612
Iteration 123/1000 | Loss: 0.00003612
Iteration 124/1000 | Loss: 0.00003612
Iteration 125/1000 | Loss: 0.00003612
Iteration 126/1000 | Loss: 0.00003612
Iteration 127/1000 | Loss: 0.00003612
Iteration 128/1000 | Loss: 0.00003612
Iteration 129/1000 | Loss: 0.00003612
Iteration 130/1000 | Loss: 0.00003612
Iteration 131/1000 | Loss: 0.00003612
Iteration 132/1000 | Loss: 0.00003612
Iteration 133/1000 | Loss: 0.00003612
Iteration 134/1000 | Loss: 0.00003611
Iteration 135/1000 | Loss: 0.00003611
Iteration 136/1000 | Loss: 0.00003611
Iteration 137/1000 | Loss: 0.00003611
Iteration 138/1000 | Loss: 0.00003611
Iteration 139/1000 | Loss: 0.00003611
Iteration 140/1000 | Loss: 0.00003611
Iteration 141/1000 | Loss: 0.00003611
Iteration 142/1000 | Loss: 0.00003611
Iteration 143/1000 | Loss: 0.00003611
Iteration 144/1000 | Loss: 0.00003611
Iteration 145/1000 | Loss: 0.00003611
Iteration 146/1000 | Loss: 0.00003611
Iteration 147/1000 | Loss: 0.00003611
Iteration 148/1000 | Loss: 0.00003611
Iteration 149/1000 | Loss: 0.00003611
Iteration 150/1000 | Loss: 0.00003611
Iteration 151/1000 | Loss: 0.00003611
Iteration 152/1000 | Loss: 0.00003611
Iteration 153/1000 | Loss: 0.00003610
Iteration 154/1000 | Loss: 0.00003610
Iteration 155/1000 | Loss: 0.00003610
Iteration 156/1000 | Loss: 0.00003610
Iteration 157/1000 | Loss: 0.00003610
Iteration 158/1000 | Loss: 0.00003610
Iteration 159/1000 | Loss: 0.00003610
Iteration 160/1000 | Loss: 0.00003610
Iteration 161/1000 | Loss: 0.00003610
Iteration 162/1000 | Loss: 0.00003609
Iteration 163/1000 | Loss: 0.00003609
Iteration 164/1000 | Loss: 0.00003609
Iteration 165/1000 | Loss: 0.00003609
Iteration 166/1000 | Loss: 0.00003609
Iteration 167/1000 | Loss: 0.00003609
Iteration 168/1000 | Loss: 0.00003609
Iteration 169/1000 | Loss: 0.00003609
Iteration 170/1000 | Loss: 0.00007624
Iteration 171/1000 | Loss: 0.00005904
Iteration 172/1000 | Loss: 0.00008152
Iteration 173/1000 | Loss: 0.00005968
Iteration 174/1000 | Loss: 0.00008634
Iteration 175/1000 | Loss: 0.00006785
Iteration 176/1000 | Loss: 0.00003629
Iteration 177/1000 | Loss: 0.00003608
Iteration 178/1000 | Loss: 0.00003608
Iteration 179/1000 | Loss: 0.00003608
Iteration 180/1000 | Loss: 0.00003607
Iteration 181/1000 | Loss: 0.00003607
Iteration 182/1000 | Loss: 0.00003607
Iteration 183/1000 | Loss: 0.00003607
Iteration 184/1000 | Loss: 0.00003607
Iteration 185/1000 | Loss: 0.00003607
Iteration 186/1000 | Loss: 0.00003607
Iteration 187/1000 | Loss: 0.00003607
Iteration 188/1000 | Loss: 0.00003607
Iteration 189/1000 | Loss: 0.00003606
Iteration 190/1000 | Loss: 0.00003606
Iteration 191/1000 | Loss: 0.00003606
Iteration 192/1000 | Loss: 0.00003606
Iteration 193/1000 | Loss: 0.00003606
Iteration 194/1000 | Loss: 0.00003606
Iteration 195/1000 | Loss: 0.00003605
Iteration 196/1000 | Loss: 0.00003605
Iteration 197/1000 | Loss: 0.00003605
Iteration 198/1000 | Loss: 0.00003605
Iteration 199/1000 | Loss: 0.00003605
Iteration 200/1000 | Loss: 0.00003605
Iteration 201/1000 | Loss: 0.00003605
Iteration 202/1000 | Loss: 0.00003605
Iteration 203/1000 | Loss: 0.00003605
Iteration 204/1000 | Loss: 0.00003605
Iteration 205/1000 | Loss: 0.00003605
Iteration 206/1000 | Loss: 0.00003605
Iteration 207/1000 | Loss: 0.00003605
Iteration 208/1000 | Loss: 0.00003605
Iteration 209/1000 | Loss: 0.00003604
Iteration 210/1000 | Loss: 0.00003604
Iteration 211/1000 | Loss: 0.00003604
Iteration 212/1000 | Loss: 0.00003604
Iteration 213/1000 | Loss: 0.00003604
Iteration 214/1000 | Loss: 0.00003604
Iteration 215/1000 | Loss: 0.00003604
Iteration 216/1000 | Loss: 0.00003604
Iteration 217/1000 | Loss: 0.00003604
Iteration 218/1000 | Loss: 0.00003604
Iteration 219/1000 | Loss: 0.00003604
Iteration 220/1000 | Loss: 0.00003604
Iteration 221/1000 | Loss: 0.00003604
Iteration 222/1000 | Loss: 0.00003604
Iteration 223/1000 | Loss: 0.00003604
Iteration 224/1000 | Loss: 0.00003604
Iteration 225/1000 | Loss: 0.00003603
Iteration 226/1000 | Loss: 0.00003603
Iteration 227/1000 | Loss: 0.00003603
Iteration 228/1000 | Loss: 0.00003603
Iteration 229/1000 | Loss: 0.00003603
Iteration 230/1000 | Loss: 0.00003603
Iteration 231/1000 | Loss: 0.00003603
Iteration 232/1000 | Loss: 0.00003603
Iteration 233/1000 | Loss: 0.00003603
Iteration 234/1000 | Loss: 0.00003603
Iteration 235/1000 | Loss: 0.00003602
Iteration 236/1000 | Loss: 0.00003602
Iteration 237/1000 | Loss: 0.00003602
Iteration 238/1000 | Loss: 0.00003602
Iteration 239/1000 | Loss: 0.00003602
Iteration 240/1000 | Loss: 0.00003602
Iteration 241/1000 | Loss: 0.00003602
Iteration 242/1000 | Loss: 0.00003601
Iteration 243/1000 | Loss: 0.00003601
Iteration 244/1000 | Loss: 0.00003601
Iteration 245/1000 | Loss: 0.00003601
Iteration 246/1000 | Loss: 0.00003601
Iteration 247/1000 | Loss: 0.00003601
Iteration 248/1000 | Loss: 0.00003601
Iteration 249/1000 | Loss: 0.00003600
Iteration 250/1000 | Loss: 0.00003600
Iteration 251/1000 | Loss: 0.00003600
Iteration 252/1000 | Loss: 0.00003599
Iteration 253/1000 | Loss: 0.00003599
Iteration 254/1000 | Loss: 0.00003599
Iteration 255/1000 | Loss: 0.00019173
Iteration 256/1000 | Loss: 0.00014060
Iteration 257/1000 | Loss: 0.00015264
Iteration 258/1000 | Loss: 0.00021159
Iteration 259/1000 | Loss: 0.00007355
Iteration 260/1000 | Loss: 0.00012822
Iteration 261/1000 | Loss: 0.00007622
Iteration 262/1000 | Loss: 0.00014893
Iteration 263/1000 | Loss: 0.00008859
Iteration 264/1000 | Loss: 0.00012613
Iteration 265/1000 | Loss: 0.00011814
Iteration 266/1000 | Loss: 0.00012791
Iteration 267/1000 | Loss: 0.00013895
Iteration 268/1000 | Loss: 0.00020338
Iteration 269/1000 | Loss: 0.00012705
Iteration 270/1000 | Loss: 0.00014687
Iteration 271/1000 | Loss: 0.00017544
Iteration 272/1000 | Loss: 0.00006112
Iteration 273/1000 | Loss: 0.00013191
Iteration 274/1000 | Loss: 0.00010729
Iteration 275/1000 | Loss: 0.00015712
Iteration 276/1000 | Loss: 0.00010177
Iteration 277/1000 | Loss: 0.00004626
Iteration 278/1000 | Loss: 0.00004266
Iteration 279/1000 | Loss: 0.00004155
Iteration 280/1000 | Loss: 0.00003899
Iteration 281/1000 | Loss: 0.00003759
Iteration 282/1000 | Loss: 0.00003701
Iteration 283/1000 | Loss: 0.00010098
Iteration 284/1000 | Loss: 0.00007771
Iteration 285/1000 | Loss: 0.00009404
Iteration 286/1000 | Loss: 0.00004213
Iteration 287/1000 | Loss: 0.00004093
Iteration 288/1000 | Loss: 0.00003994
Iteration 289/1000 | Loss: 0.00003902
Iteration 290/1000 | Loss: 0.00003818
Iteration 291/1000 | Loss: 0.00003773
Iteration 292/1000 | Loss: 0.00003749
Iteration 293/1000 | Loss: 0.00003749
Iteration 294/1000 | Loss: 0.00003748
Iteration 295/1000 | Loss: 0.00003744
Iteration 296/1000 | Loss: 0.00003737
Iteration 297/1000 | Loss: 0.00003735
Iteration 298/1000 | Loss: 0.00003734
Iteration 299/1000 | Loss: 0.00003733
Iteration 300/1000 | Loss: 0.00003733
Iteration 301/1000 | Loss: 0.00003732
Iteration 302/1000 | Loss: 0.00003727
Iteration 303/1000 | Loss: 0.00003716
Iteration 304/1000 | Loss: 0.00003714
Iteration 305/1000 | Loss: 0.00003713
Iteration 306/1000 | Loss: 0.00003712
Iteration 307/1000 | Loss: 0.00007117
Iteration 308/1000 | Loss: 0.00003756
Iteration 309/1000 | Loss: 0.00003632
Iteration 310/1000 | Loss: 0.00012656
Iteration 311/1000 | Loss: 0.00008487
Iteration 312/1000 | Loss: 0.00012450
Iteration 313/1000 | Loss: 0.00010198
Iteration 314/1000 | Loss: 0.00015158
Iteration 315/1000 | Loss: 0.00009551
Iteration 316/1000 | Loss: 0.00015382
Iteration 317/1000 | Loss: 0.00009375
Iteration 318/1000 | Loss: 0.00013951
Iteration 319/1000 | Loss: 0.00008975
Iteration 320/1000 | Loss: 0.00003567
Iteration 321/1000 | Loss: 0.00003542
Iteration 322/1000 | Loss: 0.00003541
Iteration 323/1000 | Loss: 0.00003540
Iteration 324/1000 | Loss: 0.00003540
Iteration 325/1000 | Loss: 0.00003540
Iteration 326/1000 | Loss: 0.00003540
Iteration 327/1000 | Loss: 0.00003539
Iteration 328/1000 | Loss: 0.00003539
Iteration 329/1000 | Loss: 0.00003539
Iteration 330/1000 | Loss: 0.00003539
Iteration 331/1000 | Loss: 0.00003538
Iteration 332/1000 | Loss: 0.00003538
Iteration 333/1000 | Loss: 0.00003534
Iteration 334/1000 | Loss: 0.00003534
Iteration 335/1000 | Loss: 0.00003533
Iteration 336/1000 | Loss: 0.00003529
Iteration 337/1000 | Loss: 0.00003528
Iteration 338/1000 | Loss: 0.00003527
Iteration 339/1000 | Loss: 0.00003523
Iteration 340/1000 | Loss: 0.00003522
Iteration 341/1000 | Loss: 0.00003521
Iteration 342/1000 | Loss: 0.00003519
Iteration 343/1000 | Loss: 0.00003519
Iteration 344/1000 | Loss: 0.00003519
Iteration 345/1000 | Loss: 0.00003519
Iteration 346/1000 | Loss: 0.00003518
Iteration 347/1000 | Loss: 0.00003518
Iteration 348/1000 | Loss: 0.00003518
Iteration 349/1000 | Loss: 0.00003518
Iteration 350/1000 | Loss: 0.00003517
Iteration 351/1000 | Loss: 0.00003517
Iteration 352/1000 | Loss: 0.00003517
Iteration 353/1000 | Loss: 0.00003517
Iteration 354/1000 | Loss: 0.00003517
Iteration 355/1000 | Loss: 0.00003517
Iteration 356/1000 | Loss: 0.00003517
Iteration 357/1000 | Loss: 0.00003517
Iteration 358/1000 | Loss: 0.00003517
Iteration 359/1000 | Loss: 0.00003517
Iteration 360/1000 | Loss: 0.00003517
Iteration 361/1000 | Loss: 0.00003516
Iteration 362/1000 | Loss: 0.00003516
Iteration 363/1000 | Loss: 0.00003516
Iteration 364/1000 | Loss: 0.00003516
Iteration 365/1000 | Loss: 0.00003516
Iteration 366/1000 | Loss: 0.00003516
Iteration 367/1000 | Loss: 0.00003516
Iteration 368/1000 | Loss: 0.00003516
Iteration 369/1000 | Loss: 0.00003516
Iteration 370/1000 | Loss: 0.00003516
Iteration 371/1000 | Loss: 0.00003516
Iteration 372/1000 | Loss: 0.00003516
Iteration 373/1000 | Loss: 0.00003516
Iteration 374/1000 | Loss: 0.00003516
Iteration 375/1000 | Loss: 0.00003516
Iteration 376/1000 | Loss: 0.00003516
Iteration 377/1000 | Loss: 0.00003516
Iteration 378/1000 | Loss: 0.00003516
Iteration 379/1000 | Loss: 0.00003516
Iteration 380/1000 | Loss: 0.00003516
Iteration 381/1000 | Loss: 0.00003516
Iteration 382/1000 | Loss: 0.00003516
Iteration 383/1000 | Loss: 0.00003516
Iteration 384/1000 | Loss: 0.00003516
Iteration 385/1000 | Loss: 0.00003516
Iteration 386/1000 | Loss: 0.00003516
Iteration 387/1000 | Loss: 0.00003516
Iteration 388/1000 | Loss: 0.00003516
Iteration 389/1000 | Loss: 0.00003516
Iteration 390/1000 | Loss: 0.00003516
Iteration 391/1000 | Loss: 0.00003516
Iteration 392/1000 | Loss: 0.00003516
Iteration 393/1000 | Loss: 0.00003516
Iteration 394/1000 | Loss: 0.00003516
Iteration 395/1000 | Loss: 0.00003516
Iteration 396/1000 | Loss: 0.00003516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [3.51589587808121e-05, 3.51589587808121e-05, 3.51589587808121e-05, 3.51589587808121e-05, 3.51589587808121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.51589587808121e-05

Optimization complete. Final v2v error: 4.51731014251709 mm

Highest mean error: 5.96051025390625 mm for frame 16

Lowest mean error: 4.099931240081787 mm for frame 194

Saving results

Total time: 243.2403175830841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741220
Iteration 2/25 | Loss: 0.00164422
Iteration 3/25 | Loss: 0.00125176
Iteration 4/25 | Loss: 0.00118973
Iteration 5/25 | Loss: 0.00117805
Iteration 6/25 | Loss: 0.00116308
Iteration 7/25 | Loss: 0.00115158
Iteration 8/25 | Loss: 0.00114802
Iteration 9/25 | Loss: 0.00114444
Iteration 10/25 | Loss: 0.00114334
Iteration 11/25 | Loss: 0.00114310
Iteration 12/25 | Loss: 0.00114467
Iteration 13/25 | Loss: 0.00114681
Iteration 14/25 | Loss: 0.00114349
Iteration 15/25 | Loss: 0.00114202
Iteration 16/25 | Loss: 0.00113963
Iteration 17/25 | Loss: 0.00113889
Iteration 18/25 | Loss: 0.00113858
Iteration 19/25 | Loss: 0.00113829
Iteration 20/25 | Loss: 0.00113808
Iteration 21/25 | Loss: 0.00113765
Iteration 22/25 | Loss: 0.00113732
Iteration 23/25 | Loss: 0.00113953
Iteration 24/25 | Loss: 0.00113721
Iteration 25/25 | Loss: 0.00113945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.81208420
Iteration 2/25 | Loss: 0.00088980
Iteration 3/25 | Loss: 0.00088967
Iteration 4/25 | Loss: 0.00088967
Iteration 5/25 | Loss: 0.00088967
Iteration 6/25 | Loss: 0.00088967
Iteration 7/25 | Loss: 0.00088967
Iteration 8/25 | Loss: 0.00088967
Iteration 9/25 | Loss: 0.00088967
Iteration 10/25 | Loss: 0.00088967
Iteration 11/25 | Loss: 0.00088967
Iteration 12/25 | Loss: 0.00088967
Iteration 13/25 | Loss: 0.00088967
Iteration 14/25 | Loss: 0.00088967
Iteration 15/25 | Loss: 0.00088967
Iteration 16/25 | Loss: 0.00088967
Iteration 17/25 | Loss: 0.00088967
Iteration 18/25 | Loss: 0.00088967
Iteration 19/25 | Loss: 0.00088967
Iteration 20/25 | Loss: 0.00088967
Iteration 21/25 | Loss: 0.00088967
Iteration 22/25 | Loss: 0.00088967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008896702202036977, 0.0008896702202036977, 0.0008896702202036977, 0.0008896702202036977, 0.0008896702202036977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008896702202036977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088967
Iteration 2/1000 | Loss: 0.00008785
Iteration 3/1000 | Loss: 0.00006722
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002443
Iteration 6/1000 | Loss: 0.00002256
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00008827
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001933
Iteration 11/1000 | Loss: 0.00001896
Iteration 12/1000 | Loss: 0.00002762
Iteration 13/1000 | Loss: 0.00002760
Iteration 14/1000 | Loss: 0.00001948
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001769
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001753
Iteration 22/1000 | Loss: 0.00001752
Iteration 23/1000 | Loss: 0.00001752
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001742
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001735
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001722
Iteration 44/1000 | Loss: 0.00002767
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001714
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.7137475879280828e-05, 1.7137475879280828e-05, 1.7137475879280828e-05, 1.7137475879280828e-05, 1.7137475879280828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7137475879280828e-05

Optimization complete. Final v2v error: 3.41062593460083 mm

Highest mean error: 5.359890460968018 mm for frame 173

Lowest mean error: 2.6079041957855225 mm for frame 118

Saving results

Total time: 84.93325066566467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045622
Iteration 2/25 | Loss: 0.01045622
Iteration 3/25 | Loss: 0.01045622
Iteration 4/25 | Loss: 0.01045622
Iteration 5/25 | Loss: 0.01045621
Iteration 6/25 | Loss: 0.01045621
Iteration 7/25 | Loss: 0.01045621
Iteration 8/25 | Loss: 0.01045621
Iteration 9/25 | Loss: 0.01045621
Iteration 10/25 | Loss: 0.01045621
Iteration 11/25 | Loss: 0.01045621
Iteration 12/25 | Loss: 0.01045621
Iteration 13/25 | Loss: 0.01045621
Iteration 14/25 | Loss: 0.01045621
Iteration 15/25 | Loss: 0.01045621
Iteration 16/25 | Loss: 0.01045620
Iteration 17/25 | Loss: 0.01045620
Iteration 18/25 | Loss: 0.01045620
Iteration 19/25 | Loss: 0.01045620
Iteration 20/25 | Loss: 0.01045620
Iteration 21/25 | Loss: 0.01045620
Iteration 22/25 | Loss: 0.01045620
Iteration 23/25 | Loss: 0.01045620
Iteration 24/25 | Loss: 0.01045620
Iteration 25/25 | Loss: 0.01045620

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69650006
Iteration 2/25 | Loss: 0.09563239
Iteration 3/25 | Loss: 0.09256068
Iteration 4/25 | Loss: 0.09052900
Iteration 5/25 | Loss: 0.09036574
Iteration 6/25 | Loss: 0.09036571
Iteration 7/25 | Loss: 0.09036570
Iteration 8/25 | Loss: 0.09036570
Iteration 9/25 | Loss: 0.09036570
Iteration 10/25 | Loss: 0.09036570
Iteration 11/25 | Loss: 0.09036570
Iteration 12/25 | Loss: 0.09036570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0903657004237175, 0.0903657004237175, 0.0903657004237175, 0.0903657004237175, 0.0903657004237175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0903657004237175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09036570
Iteration 2/1000 | Loss: 0.00431786
Iteration 3/1000 | Loss: 0.00202351
Iteration 4/1000 | Loss: 0.00050361
Iteration 5/1000 | Loss: 0.00026286
Iteration 6/1000 | Loss: 0.00024108
Iteration 7/1000 | Loss: 0.00012161
Iteration 8/1000 | Loss: 0.00011224
Iteration 9/1000 | Loss: 0.00004812
Iteration 10/1000 | Loss: 0.00009033
Iteration 11/1000 | Loss: 0.00007212
Iteration 12/1000 | Loss: 0.00003268
Iteration 13/1000 | Loss: 0.00026971
Iteration 14/1000 | Loss: 0.00002811
Iteration 15/1000 | Loss: 0.00007944
Iteration 16/1000 | Loss: 0.00025668
Iteration 17/1000 | Loss: 0.00071175
Iteration 18/1000 | Loss: 0.00030649
Iteration 19/1000 | Loss: 0.00013102
Iteration 20/1000 | Loss: 0.00009872
Iteration 21/1000 | Loss: 0.00002690
Iteration 22/1000 | Loss: 0.00006897
Iteration 23/1000 | Loss: 0.00007171
Iteration 24/1000 | Loss: 0.00005609
Iteration 25/1000 | Loss: 0.00002390
Iteration 26/1000 | Loss: 0.00012530
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00005652
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00032444
Iteration 31/1000 | Loss: 0.00004697
Iteration 32/1000 | Loss: 0.00008707
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00005040
Iteration 35/1000 | Loss: 0.00006548
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00002272
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00012064
Iteration 40/1000 | Loss: 0.00013715
Iteration 41/1000 | Loss: 0.00014764
Iteration 42/1000 | Loss: 0.00005211
Iteration 43/1000 | Loss: 0.00092761
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00002596
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00002537
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00005112
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00012681
Iteration 54/1000 | Loss: 0.00014978
Iteration 55/1000 | Loss: 0.00004179
Iteration 56/1000 | Loss: 0.00002659
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00002487
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00003201
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00005763
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00005398
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.1703759810188785e-05, 1.1703759810188785e-05, 1.1703759810188785e-05, 1.1703759810188785e-05, 1.1703759810188785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1703759810188785e-05

Optimization complete. Final v2v error: 2.914623737335205 mm

Highest mean error: 3.4687259197235107 mm for frame 234

Lowest mean error: 2.410630702972412 mm for frame 151

Saving results

Total time: 114.11224555969238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415612
Iteration 2/25 | Loss: 0.00117823
Iteration 3/25 | Loss: 0.00109085
Iteration 4/25 | Loss: 0.00107575
Iteration 5/25 | Loss: 0.00107040
Iteration 6/25 | Loss: 0.00106908
Iteration 7/25 | Loss: 0.00106896
Iteration 8/25 | Loss: 0.00106896
Iteration 9/25 | Loss: 0.00106896
Iteration 10/25 | Loss: 0.00106896
Iteration 11/25 | Loss: 0.00106896
Iteration 12/25 | Loss: 0.00106896
Iteration 13/25 | Loss: 0.00106896
Iteration 14/25 | Loss: 0.00106896
Iteration 15/25 | Loss: 0.00106896
Iteration 16/25 | Loss: 0.00106896
Iteration 17/25 | Loss: 0.00106896
Iteration 18/25 | Loss: 0.00106896
Iteration 19/25 | Loss: 0.00106896
Iteration 20/25 | Loss: 0.00106896
Iteration 21/25 | Loss: 0.00106895
Iteration 22/25 | Loss: 0.00106896
Iteration 23/25 | Loss: 0.00106896
Iteration 24/25 | Loss: 0.00106896
Iteration 25/25 | Loss: 0.00106895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69994366
Iteration 2/25 | Loss: 0.00085657
Iteration 3/25 | Loss: 0.00085657
Iteration 4/25 | Loss: 0.00085657
Iteration 5/25 | Loss: 0.00085657
Iteration 6/25 | Loss: 0.00085657
Iteration 7/25 | Loss: 0.00085657
Iteration 8/25 | Loss: 0.00085657
Iteration 9/25 | Loss: 0.00085657
Iteration 10/25 | Loss: 0.00085657
Iteration 11/25 | Loss: 0.00085657
Iteration 12/25 | Loss: 0.00085657
Iteration 13/25 | Loss: 0.00085657
Iteration 14/25 | Loss: 0.00085657
Iteration 15/25 | Loss: 0.00085657
Iteration 16/25 | Loss: 0.00085657
Iteration 17/25 | Loss: 0.00085657
Iteration 18/25 | Loss: 0.00085657
Iteration 19/25 | Loss: 0.00085657
Iteration 20/25 | Loss: 0.00085657
Iteration 21/25 | Loss: 0.00085657
Iteration 22/25 | Loss: 0.00085657
Iteration 23/25 | Loss: 0.00085657
Iteration 24/25 | Loss: 0.00085657
Iteration 25/25 | Loss: 0.00085657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085657
Iteration 2/1000 | Loss: 0.00002870
Iteration 3/1000 | Loss: 0.00001878
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001190
Iteration 8/1000 | Loss: 0.00001161
Iteration 9/1000 | Loss: 0.00001130
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001107
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001094
Iteration 15/1000 | Loss: 0.00001093
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001072
Iteration 19/1000 | Loss: 0.00001071
Iteration 20/1000 | Loss: 0.00001067
Iteration 21/1000 | Loss: 0.00001067
Iteration 22/1000 | Loss: 0.00001061
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001054
Iteration 29/1000 | Loss: 0.00001054
Iteration 30/1000 | Loss: 0.00001053
Iteration 31/1000 | Loss: 0.00001053
Iteration 32/1000 | Loss: 0.00001052
Iteration 33/1000 | Loss: 0.00001052
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001051
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001050
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001049
Iteration 40/1000 | Loss: 0.00001049
Iteration 41/1000 | Loss: 0.00001049
Iteration 42/1000 | Loss: 0.00001048
Iteration 43/1000 | Loss: 0.00001048
Iteration 44/1000 | Loss: 0.00001047
Iteration 45/1000 | Loss: 0.00001046
Iteration 46/1000 | Loss: 0.00001046
Iteration 47/1000 | Loss: 0.00001046
Iteration 48/1000 | Loss: 0.00001045
Iteration 49/1000 | Loss: 0.00001045
Iteration 50/1000 | Loss: 0.00001045
Iteration 51/1000 | Loss: 0.00001044
Iteration 52/1000 | Loss: 0.00001044
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001043
Iteration 56/1000 | Loss: 0.00001043
Iteration 57/1000 | Loss: 0.00001043
Iteration 58/1000 | Loss: 0.00001042
Iteration 59/1000 | Loss: 0.00001042
Iteration 60/1000 | Loss: 0.00001042
Iteration 61/1000 | Loss: 0.00001042
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001042
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001039
Iteration 70/1000 | Loss: 0.00001039
Iteration 71/1000 | Loss: 0.00001039
Iteration 72/1000 | Loss: 0.00001039
Iteration 73/1000 | Loss: 0.00001039
Iteration 74/1000 | Loss: 0.00001039
Iteration 75/1000 | Loss: 0.00001039
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00001038
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001037
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001036
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001036
Iteration 86/1000 | Loss: 0.00001036
Iteration 87/1000 | Loss: 0.00001036
Iteration 88/1000 | Loss: 0.00001036
Iteration 89/1000 | Loss: 0.00001036
Iteration 90/1000 | Loss: 0.00001036
Iteration 91/1000 | Loss: 0.00001036
Iteration 92/1000 | Loss: 0.00001036
Iteration 93/1000 | Loss: 0.00001036
Iteration 94/1000 | Loss: 0.00001036
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001036
Iteration 98/1000 | Loss: 0.00001036
Iteration 99/1000 | Loss: 0.00001036
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Iteration 124/1000 | Loss: 0.00001036
Iteration 125/1000 | Loss: 0.00001036
Iteration 126/1000 | Loss: 0.00001036
Iteration 127/1000 | Loss: 0.00001036
Iteration 128/1000 | Loss: 0.00001036
Iteration 129/1000 | Loss: 0.00001036
Iteration 130/1000 | Loss: 0.00001036
Iteration 131/1000 | Loss: 0.00001036
Iteration 132/1000 | Loss: 0.00001036
Iteration 133/1000 | Loss: 0.00001036
Iteration 134/1000 | Loss: 0.00001036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.0356054190197028e-05, 1.0356054190197028e-05, 1.0356054190197028e-05, 1.0356054190197028e-05, 1.0356054190197028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0356054190197028e-05

Optimization complete. Final v2v error: 2.756268262863159 mm

Highest mean error: 3.7161216735839844 mm for frame 74

Lowest mean error: 2.4387481212615967 mm for frame 96

Saving results

Total time: 34.97483706474304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005542
Iteration 2/25 | Loss: 0.00239906
Iteration 3/25 | Loss: 0.00183265
Iteration 4/25 | Loss: 0.00153487
Iteration 5/25 | Loss: 0.00148996
Iteration 6/25 | Loss: 0.00151910
Iteration 7/25 | Loss: 0.00140845
Iteration 8/25 | Loss: 0.00133990
Iteration 9/25 | Loss: 0.00127052
Iteration 10/25 | Loss: 0.00124291
Iteration 11/25 | Loss: 0.00123086
Iteration 12/25 | Loss: 0.00122352
Iteration 13/25 | Loss: 0.00121593
Iteration 14/25 | Loss: 0.00120902
Iteration 15/25 | Loss: 0.00120275
Iteration 16/25 | Loss: 0.00120655
Iteration 17/25 | Loss: 0.00119744
Iteration 18/25 | Loss: 0.00118976
Iteration 19/25 | Loss: 0.00117949
Iteration 20/25 | Loss: 0.00118467
Iteration 21/25 | Loss: 0.00118521
Iteration 22/25 | Loss: 0.00117882
Iteration 23/25 | Loss: 0.00118124
Iteration 24/25 | Loss: 0.00118461
Iteration 25/25 | Loss: 0.00118604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44515848
Iteration 2/25 | Loss: 0.00199279
Iteration 3/25 | Loss: 0.00199278
Iteration 4/25 | Loss: 0.00199278
Iteration 5/25 | Loss: 0.00199278
Iteration 6/25 | Loss: 0.00199278
Iteration 7/25 | Loss: 0.00199278
Iteration 8/25 | Loss: 0.00199278
Iteration 9/25 | Loss: 0.00199278
Iteration 10/25 | Loss: 0.00199278
Iteration 11/25 | Loss: 0.00199278
Iteration 12/25 | Loss: 0.00199278
Iteration 13/25 | Loss: 0.00199278
Iteration 14/25 | Loss: 0.00199278
Iteration 15/25 | Loss: 0.00199278
Iteration 16/25 | Loss: 0.00199278
Iteration 17/25 | Loss: 0.00199278
Iteration 18/25 | Loss: 0.00199278
Iteration 19/25 | Loss: 0.00199278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001992778852581978, 0.001992778852581978, 0.001992778852581978, 0.001992778852581978, 0.001992778852581978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001992778852581978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199278
Iteration 2/1000 | Loss: 0.00024633
Iteration 3/1000 | Loss: 0.00050873
Iteration 4/1000 | Loss: 0.00067872
Iteration 5/1000 | Loss: 0.00050919
Iteration 6/1000 | Loss: 0.00342895
Iteration 7/1000 | Loss: 0.00042392
Iteration 8/1000 | Loss: 0.00041476
Iteration 9/1000 | Loss: 0.00035526
Iteration 10/1000 | Loss: 0.00031946
Iteration 11/1000 | Loss: 0.00098507
Iteration 12/1000 | Loss: 0.00062444
Iteration 13/1000 | Loss: 0.00068012
Iteration 14/1000 | Loss: 0.00042860
Iteration 15/1000 | Loss: 0.00047358
Iteration 16/1000 | Loss: 0.00040467
Iteration 17/1000 | Loss: 0.00056236
Iteration 18/1000 | Loss: 0.00019417
Iteration 19/1000 | Loss: 0.00034482
Iteration 20/1000 | Loss: 0.00049109
Iteration 21/1000 | Loss: 0.00033835
Iteration 22/1000 | Loss: 0.00021535
Iteration 23/1000 | Loss: 0.00013713
Iteration 24/1000 | Loss: 0.00362772
Iteration 25/1000 | Loss: 0.00132055
Iteration 26/1000 | Loss: 0.00091010
Iteration 27/1000 | Loss: 0.00099097
Iteration 28/1000 | Loss: 0.00024689
Iteration 29/1000 | Loss: 0.00020194
Iteration 30/1000 | Loss: 0.00079948
Iteration 31/1000 | Loss: 0.00077976
Iteration 32/1000 | Loss: 0.00030033
Iteration 33/1000 | Loss: 0.00038889
Iteration 34/1000 | Loss: 0.00043225
Iteration 35/1000 | Loss: 0.00048887
Iteration 36/1000 | Loss: 0.00027586
Iteration 37/1000 | Loss: 0.00015982
Iteration 38/1000 | Loss: 0.00105868
Iteration 39/1000 | Loss: 0.00062118
Iteration 40/1000 | Loss: 0.00052807
Iteration 41/1000 | Loss: 0.00059727
Iteration 42/1000 | Loss: 0.00119920
Iteration 43/1000 | Loss: 0.00045533
Iteration 44/1000 | Loss: 0.00019846
Iteration 45/1000 | Loss: 0.00023581
Iteration 46/1000 | Loss: 0.00061968
Iteration 47/1000 | Loss: 0.00050149
Iteration 48/1000 | Loss: 0.00029457
Iteration 49/1000 | Loss: 0.00016763
Iteration 50/1000 | Loss: 0.00033711
Iteration 51/1000 | Loss: 0.00035935
Iteration 52/1000 | Loss: 0.00022162
Iteration 53/1000 | Loss: 0.00028581
Iteration 54/1000 | Loss: 0.00043132
Iteration 55/1000 | Loss: 0.00058319
Iteration 56/1000 | Loss: 0.00051679
Iteration 57/1000 | Loss: 0.00035230
Iteration 58/1000 | Loss: 0.00032624
Iteration 59/1000 | Loss: 0.00024143
Iteration 60/1000 | Loss: 0.00016218
Iteration 61/1000 | Loss: 0.00027885
Iteration 62/1000 | Loss: 0.00027964
Iteration 63/1000 | Loss: 0.00026975
Iteration 64/1000 | Loss: 0.00051020
Iteration 65/1000 | Loss: 0.00043965
Iteration 66/1000 | Loss: 0.00034755
Iteration 67/1000 | Loss: 0.00010890
Iteration 68/1000 | Loss: 0.00016690
Iteration 69/1000 | Loss: 0.00016903
Iteration 70/1000 | Loss: 0.00005398
Iteration 71/1000 | Loss: 0.00023954
Iteration 72/1000 | Loss: 0.00018055
Iteration 73/1000 | Loss: 0.00026633
Iteration 74/1000 | Loss: 0.00009215
Iteration 75/1000 | Loss: 0.00017747
Iteration 76/1000 | Loss: 0.00021156
Iteration 77/1000 | Loss: 0.00027120
Iteration 78/1000 | Loss: 0.00031010
Iteration 79/1000 | Loss: 0.00007403
Iteration 80/1000 | Loss: 0.00004861
Iteration 81/1000 | Loss: 0.00016133
Iteration 82/1000 | Loss: 0.00003234
Iteration 83/1000 | Loss: 0.00002919
Iteration 84/1000 | Loss: 0.00002769
Iteration 85/1000 | Loss: 0.00013719
Iteration 86/1000 | Loss: 0.00010706
Iteration 87/1000 | Loss: 0.00002851
Iteration 88/1000 | Loss: 0.00002011
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00005430
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00005021
Iteration 93/1000 | Loss: 0.00001788
Iteration 94/1000 | Loss: 0.00037848
Iteration 95/1000 | Loss: 0.00002612
Iteration 96/1000 | Loss: 0.00008617
Iteration 97/1000 | Loss: 0.00001836
Iteration 98/1000 | Loss: 0.00004390
Iteration 99/1000 | Loss: 0.00015570
Iteration 100/1000 | Loss: 0.00001780
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00004084
Iteration 104/1000 | Loss: 0.00003203
Iteration 105/1000 | Loss: 0.00003064
Iteration 106/1000 | Loss: 0.00002315
Iteration 107/1000 | Loss: 0.00001623
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001469
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001466
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001462
Iteration 122/1000 | Loss: 0.00001462
Iteration 123/1000 | Loss: 0.00001462
Iteration 124/1000 | Loss: 0.00001461
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001453
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001452
Iteration 134/1000 | Loss: 0.00001452
Iteration 135/1000 | Loss: 0.00001452
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001450
Iteration 144/1000 | Loss: 0.00001450
Iteration 145/1000 | Loss: 0.00001450
Iteration 146/1000 | Loss: 0.00001450
Iteration 147/1000 | Loss: 0.00001450
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001449
Iteration 150/1000 | Loss: 0.00001449
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001448
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001446
Iteration 166/1000 | Loss: 0.00001446
Iteration 167/1000 | Loss: 0.00001446
Iteration 168/1000 | Loss: 0.00001446
Iteration 169/1000 | Loss: 0.00001446
Iteration 170/1000 | Loss: 0.00001446
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001444
Iteration 175/1000 | Loss: 0.00001444
Iteration 176/1000 | Loss: 0.00001444
Iteration 177/1000 | Loss: 0.00001444
Iteration 178/1000 | Loss: 0.00001444
Iteration 179/1000 | Loss: 0.00001443
Iteration 180/1000 | Loss: 0.00001443
Iteration 181/1000 | Loss: 0.00001443
Iteration 182/1000 | Loss: 0.00001442
Iteration 183/1000 | Loss: 0.00001442
Iteration 184/1000 | Loss: 0.00001442
Iteration 185/1000 | Loss: 0.00001441
Iteration 186/1000 | Loss: 0.00001441
Iteration 187/1000 | Loss: 0.00001441
Iteration 188/1000 | Loss: 0.00001441
Iteration 189/1000 | Loss: 0.00001441
Iteration 190/1000 | Loss: 0.00001441
Iteration 191/1000 | Loss: 0.00001441
Iteration 192/1000 | Loss: 0.00001441
Iteration 193/1000 | Loss: 0.00001441
Iteration 194/1000 | Loss: 0.00001440
Iteration 195/1000 | Loss: 0.00001440
Iteration 196/1000 | Loss: 0.00001440
Iteration 197/1000 | Loss: 0.00001439
Iteration 198/1000 | Loss: 0.00001439
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001437
Iteration 203/1000 | Loss: 0.00001437
Iteration 204/1000 | Loss: 0.00001437
Iteration 205/1000 | Loss: 0.00001437
Iteration 206/1000 | Loss: 0.00001436
Iteration 207/1000 | Loss: 0.00001436
Iteration 208/1000 | Loss: 0.00001436
Iteration 209/1000 | Loss: 0.00001436
Iteration 210/1000 | Loss: 0.00001435
Iteration 211/1000 | Loss: 0.00001435
Iteration 212/1000 | Loss: 0.00001435
Iteration 213/1000 | Loss: 0.00001434
Iteration 214/1000 | Loss: 0.00004511
Iteration 215/1000 | Loss: 0.00001441
Iteration 216/1000 | Loss: 0.00001434
Iteration 217/1000 | Loss: 0.00001433
Iteration 218/1000 | Loss: 0.00001432
Iteration 219/1000 | Loss: 0.00001432
Iteration 220/1000 | Loss: 0.00001432
Iteration 221/1000 | Loss: 0.00001432
Iteration 222/1000 | Loss: 0.00001432
Iteration 223/1000 | Loss: 0.00001432
Iteration 224/1000 | Loss: 0.00001432
Iteration 225/1000 | Loss: 0.00001432
Iteration 226/1000 | Loss: 0.00001431
Iteration 227/1000 | Loss: 0.00001431
Iteration 228/1000 | Loss: 0.00001431
Iteration 229/1000 | Loss: 0.00001431
Iteration 230/1000 | Loss: 0.00001431
Iteration 231/1000 | Loss: 0.00001430
Iteration 232/1000 | Loss: 0.00001430
Iteration 233/1000 | Loss: 0.00001430
Iteration 234/1000 | Loss: 0.00001430
Iteration 235/1000 | Loss: 0.00001430
Iteration 236/1000 | Loss: 0.00001430
Iteration 237/1000 | Loss: 0.00001430
Iteration 238/1000 | Loss: 0.00001430
Iteration 239/1000 | Loss: 0.00001430
Iteration 240/1000 | Loss: 0.00001430
Iteration 241/1000 | Loss: 0.00001430
Iteration 242/1000 | Loss: 0.00001430
Iteration 243/1000 | Loss: 0.00001430
Iteration 244/1000 | Loss: 0.00001430
Iteration 245/1000 | Loss: 0.00001430
Iteration 246/1000 | Loss: 0.00001430
Iteration 247/1000 | Loss: 0.00001430
Iteration 248/1000 | Loss: 0.00001430
Iteration 249/1000 | Loss: 0.00001430
Iteration 250/1000 | Loss: 0.00001430
Iteration 251/1000 | Loss: 0.00001430
Iteration 252/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.4295846995082684e-05, 1.4295846995082684e-05, 1.4295846995082684e-05, 1.4295846995082684e-05, 1.4295846995082684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4295846995082684e-05

Optimization complete. Final v2v error: 3.1434383392333984 mm

Highest mean error: 4.064363956451416 mm for frame 84

Lowest mean error: 2.62650203704834 mm for frame 31

Saving results

Total time: 203.8411500453949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030765
Iteration 2/25 | Loss: 0.00164910
Iteration 3/25 | Loss: 0.00143123
Iteration 4/25 | Loss: 0.00119585
Iteration 5/25 | Loss: 0.00122300
Iteration 6/25 | Loss: 0.00120538
Iteration 7/25 | Loss: 0.00118842
Iteration 8/25 | Loss: 0.00115670
Iteration 9/25 | Loss: 0.00113640
Iteration 10/25 | Loss: 0.00114311
Iteration 11/25 | Loss: 0.00113223
Iteration 12/25 | Loss: 0.00112342
Iteration 13/25 | Loss: 0.00112320
Iteration 14/25 | Loss: 0.00112468
Iteration 15/25 | Loss: 0.00112330
Iteration 16/25 | Loss: 0.00112290
Iteration 17/25 | Loss: 0.00112197
Iteration 18/25 | Loss: 0.00112367
Iteration 19/25 | Loss: 0.00112267
Iteration 20/25 | Loss: 0.00112300
Iteration 21/25 | Loss: 0.00112326
Iteration 22/25 | Loss: 0.00112167
Iteration 23/25 | Loss: 0.00112443
Iteration 24/25 | Loss: 0.00112318
Iteration 25/25 | Loss: 0.00112260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38286734
Iteration 2/25 | Loss: 0.00180721
Iteration 3/25 | Loss: 0.00128709
Iteration 4/25 | Loss: 0.00128709
Iteration 5/25 | Loss: 0.00128709
Iteration 6/25 | Loss: 0.00128709
Iteration 7/25 | Loss: 0.00128708
Iteration 8/25 | Loss: 0.00128708
Iteration 9/25 | Loss: 0.00128708
Iteration 10/25 | Loss: 0.00128708
Iteration 11/25 | Loss: 0.00128708
Iteration 12/25 | Loss: 0.00128708
Iteration 13/25 | Loss: 0.00128708
Iteration 14/25 | Loss: 0.00128708
Iteration 15/25 | Loss: 0.00128708
Iteration 16/25 | Loss: 0.00128708
Iteration 17/25 | Loss: 0.00128708
Iteration 18/25 | Loss: 0.00128708
Iteration 19/25 | Loss: 0.00128708
Iteration 20/25 | Loss: 0.00128708
Iteration 21/25 | Loss: 0.00128708
Iteration 22/25 | Loss: 0.00128708
Iteration 23/25 | Loss: 0.00128708
Iteration 24/25 | Loss: 0.00128708
Iteration 25/25 | Loss: 0.00128708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128708
Iteration 2/1000 | Loss: 0.00034814
Iteration 3/1000 | Loss: 0.00040274
Iteration 4/1000 | Loss: 0.00130417
Iteration 5/1000 | Loss: 0.00006946
Iteration 6/1000 | Loss: 0.00006380
Iteration 7/1000 | Loss: 0.00004024
Iteration 8/1000 | Loss: 0.00027974
Iteration 9/1000 | Loss: 0.00230631
Iteration 10/1000 | Loss: 0.00032091
Iteration 11/1000 | Loss: 0.00009333
Iteration 12/1000 | Loss: 0.00053508
Iteration 13/1000 | Loss: 0.00021484
Iteration 14/1000 | Loss: 0.00006615
Iteration 15/1000 | Loss: 0.00007202
Iteration 16/1000 | Loss: 0.00005296
Iteration 17/1000 | Loss: 0.00003826
Iteration 18/1000 | Loss: 0.00005108
Iteration 19/1000 | Loss: 0.00003814
Iteration 20/1000 | Loss: 0.00003561
Iteration 21/1000 | Loss: 0.00003228
Iteration 22/1000 | Loss: 0.00003679
Iteration 23/1000 | Loss: 0.00004562
Iteration 24/1000 | Loss: 0.00004995
Iteration 25/1000 | Loss: 0.00004791
Iteration 26/1000 | Loss: 0.00004368
Iteration 27/1000 | Loss: 0.00004585
Iteration 28/1000 | Loss: 0.00004762
Iteration 29/1000 | Loss: 0.00026405
Iteration 30/1000 | Loss: 0.00025416
Iteration 31/1000 | Loss: 0.00006618
Iteration 32/1000 | Loss: 0.00005412
Iteration 33/1000 | Loss: 0.00005600
Iteration 34/1000 | Loss: 0.00005143
Iteration 35/1000 | Loss: 0.00004819
Iteration 36/1000 | Loss: 0.00003935
Iteration 37/1000 | Loss: 0.00004248
Iteration 38/1000 | Loss: 0.00003358
Iteration 39/1000 | Loss: 0.00003173
Iteration 40/1000 | Loss: 0.00004443
Iteration 41/1000 | Loss: 0.00059078
Iteration 42/1000 | Loss: 0.00104493
Iteration 43/1000 | Loss: 0.00008452
Iteration 44/1000 | Loss: 0.00005803
Iteration 45/1000 | Loss: 0.00006101
Iteration 46/1000 | Loss: 0.00003339
Iteration 47/1000 | Loss: 0.00005931
Iteration 48/1000 | Loss: 0.00002149
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001135
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000978
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000967
Iteration 64/1000 | Loss: 0.00000967
Iteration 65/1000 | Loss: 0.00000962
Iteration 66/1000 | Loss: 0.00000961
Iteration 67/1000 | Loss: 0.00000961
Iteration 68/1000 | Loss: 0.00000961
Iteration 69/1000 | Loss: 0.00000961
Iteration 70/1000 | Loss: 0.00000960
Iteration 71/1000 | Loss: 0.00000960
Iteration 72/1000 | Loss: 0.00000959
Iteration 73/1000 | Loss: 0.00000958
Iteration 74/1000 | Loss: 0.00000958
Iteration 75/1000 | Loss: 0.00000958
Iteration 76/1000 | Loss: 0.00000958
Iteration 77/1000 | Loss: 0.00000958
Iteration 78/1000 | Loss: 0.00000958
Iteration 79/1000 | Loss: 0.00000958
Iteration 80/1000 | Loss: 0.00000958
Iteration 81/1000 | Loss: 0.00000958
Iteration 82/1000 | Loss: 0.00000958
Iteration 83/1000 | Loss: 0.00000958
Iteration 84/1000 | Loss: 0.00000958
Iteration 85/1000 | Loss: 0.00000958
Iteration 86/1000 | Loss: 0.00000957
Iteration 87/1000 | Loss: 0.00000957
Iteration 88/1000 | Loss: 0.00000957
Iteration 89/1000 | Loss: 0.00000957
Iteration 90/1000 | Loss: 0.00000957
Iteration 91/1000 | Loss: 0.00000957
Iteration 92/1000 | Loss: 0.00000957
Iteration 93/1000 | Loss: 0.00000956
Iteration 94/1000 | Loss: 0.00000956
Iteration 95/1000 | Loss: 0.00000956
Iteration 96/1000 | Loss: 0.00000955
Iteration 97/1000 | Loss: 0.00000955
Iteration 98/1000 | Loss: 0.00000955
Iteration 99/1000 | Loss: 0.00000955
Iteration 100/1000 | Loss: 0.00000954
Iteration 101/1000 | Loss: 0.00000954
Iteration 102/1000 | Loss: 0.00000954
Iteration 103/1000 | Loss: 0.00000954
Iteration 104/1000 | Loss: 0.00000954
Iteration 105/1000 | Loss: 0.00000954
Iteration 106/1000 | Loss: 0.00000953
Iteration 107/1000 | Loss: 0.00000953
Iteration 108/1000 | Loss: 0.00000953
Iteration 109/1000 | Loss: 0.00000953
Iteration 110/1000 | Loss: 0.00000953
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000951
Iteration 113/1000 | Loss: 0.00000951
Iteration 114/1000 | Loss: 0.00000951
Iteration 115/1000 | Loss: 0.00000950
Iteration 116/1000 | Loss: 0.00000950
Iteration 117/1000 | Loss: 0.00000950
Iteration 118/1000 | Loss: 0.00000950
Iteration 119/1000 | Loss: 0.00000949
Iteration 120/1000 | Loss: 0.00000949
Iteration 121/1000 | Loss: 0.00000949
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000947
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000947
Iteration 131/1000 | Loss: 0.00000947
Iteration 132/1000 | Loss: 0.00000947
Iteration 133/1000 | Loss: 0.00000947
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000947
Iteration 136/1000 | Loss: 0.00000947
Iteration 137/1000 | Loss: 0.00000947
Iteration 138/1000 | Loss: 0.00000946
Iteration 139/1000 | Loss: 0.00000946
Iteration 140/1000 | Loss: 0.00000946
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000946
Iteration 146/1000 | Loss: 0.00000945
Iteration 147/1000 | Loss: 0.00000945
Iteration 148/1000 | Loss: 0.00000945
Iteration 149/1000 | Loss: 0.00000945
Iteration 150/1000 | Loss: 0.00000945
Iteration 151/1000 | Loss: 0.00000945
Iteration 152/1000 | Loss: 0.00000945
Iteration 153/1000 | Loss: 0.00000945
Iteration 154/1000 | Loss: 0.00000945
Iteration 155/1000 | Loss: 0.00000945
Iteration 156/1000 | Loss: 0.00000945
Iteration 157/1000 | Loss: 0.00000945
Iteration 158/1000 | Loss: 0.00000944
Iteration 159/1000 | Loss: 0.00000944
Iteration 160/1000 | Loss: 0.00000944
Iteration 161/1000 | Loss: 0.00000944
Iteration 162/1000 | Loss: 0.00000944
Iteration 163/1000 | Loss: 0.00000944
Iteration 164/1000 | Loss: 0.00000944
Iteration 165/1000 | Loss: 0.00000944
Iteration 166/1000 | Loss: 0.00000944
Iteration 167/1000 | Loss: 0.00000944
Iteration 168/1000 | Loss: 0.00000944
Iteration 169/1000 | Loss: 0.00000944
Iteration 170/1000 | Loss: 0.00000944
Iteration 171/1000 | Loss: 0.00000944
Iteration 172/1000 | Loss: 0.00000944
Iteration 173/1000 | Loss: 0.00000944
Iteration 174/1000 | Loss: 0.00000944
Iteration 175/1000 | Loss: 0.00000944
Iteration 176/1000 | Loss: 0.00000944
Iteration 177/1000 | Loss: 0.00000944
Iteration 178/1000 | Loss: 0.00000943
Iteration 179/1000 | Loss: 0.00000943
Iteration 180/1000 | Loss: 0.00000943
Iteration 181/1000 | Loss: 0.00000943
Iteration 182/1000 | Loss: 0.00000943
Iteration 183/1000 | Loss: 0.00000943
Iteration 184/1000 | Loss: 0.00000943
Iteration 185/1000 | Loss: 0.00000943
Iteration 186/1000 | Loss: 0.00000943
Iteration 187/1000 | Loss: 0.00000943
Iteration 188/1000 | Loss: 0.00000943
Iteration 189/1000 | Loss: 0.00000943
Iteration 190/1000 | Loss: 0.00000943
Iteration 191/1000 | Loss: 0.00000943
Iteration 192/1000 | Loss: 0.00000943
Iteration 193/1000 | Loss: 0.00000943
Iteration 194/1000 | Loss: 0.00000943
Iteration 195/1000 | Loss: 0.00000943
Iteration 196/1000 | Loss: 0.00000943
Iteration 197/1000 | Loss: 0.00000943
Iteration 198/1000 | Loss: 0.00000943
Iteration 199/1000 | Loss: 0.00000943
Iteration 200/1000 | Loss: 0.00000942
Iteration 201/1000 | Loss: 0.00000942
Iteration 202/1000 | Loss: 0.00000942
Iteration 203/1000 | Loss: 0.00000942
Iteration 204/1000 | Loss: 0.00000942
Iteration 205/1000 | Loss: 0.00000942
Iteration 206/1000 | Loss: 0.00000942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [9.424637028132565e-06, 9.424637028132565e-06, 9.424637028132565e-06, 9.424637028132565e-06, 9.424637028132565e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.424637028132565e-06

Optimization complete. Final v2v error: 2.609902858734131 mm

Highest mean error: 3.189927816390991 mm for frame 95

Lowest mean error: 2.367910861968994 mm for frame 62

Saving results

Total time: 137.82556319236755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033472
Iteration 2/25 | Loss: 0.00182311
Iteration 3/25 | Loss: 0.00128804
Iteration 4/25 | Loss: 0.00122953
Iteration 5/25 | Loss: 0.00121852
Iteration 6/25 | Loss: 0.00121517
Iteration 7/25 | Loss: 0.00121496
Iteration 8/25 | Loss: 0.00121496
Iteration 9/25 | Loss: 0.00121490
Iteration 10/25 | Loss: 0.00121490
Iteration 11/25 | Loss: 0.00121490
Iteration 12/25 | Loss: 0.00121490
Iteration 13/25 | Loss: 0.00121490
Iteration 14/25 | Loss: 0.00121490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012148955138400197, 0.0012148955138400197, 0.0012148955138400197, 0.0012148955138400197, 0.0012148955138400197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012148955138400197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89750278
Iteration 2/25 | Loss: 0.00074982
Iteration 3/25 | Loss: 0.00074982
Iteration 4/25 | Loss: 0.00074982
Iteration 5/25 | Loss: 0.00074982
Iteration 6/25 | Loss: 0.00074982
Iteration 7/25 | Loss: 0.00074982
Iteration 8/25 | Loss: 0.00074982
Iteration 9/25 | Loss: 0.00074982
Iteration 10/25 | Loss: 0.00074982
Iteration 11/25 | Loss: 0.00074982
Iteration 12/25 | Loss: 0.00074982
Iteration 13/25 | Loss: 0.00074982
Iteration 14/25 | Loss: 0.00074982
Iteration 15/25 | Loss: 0.00074982
Iteration 16/25 | Loss: 0.00074982
Iteration 17/25 | Loss: 0.00074982
Iteration 18/25 | Loss: 0.00074982
Iteration 19/25 | Loss: 0.00074982
Iteration 20/25 | Loss: 0.00074982
Iteration 21/25 | Loss: 0.00074982
Iteration 22/25 | Loss: 0.00074982
Iteration 23/25 | Loss: 0.00074982
Iteration 24/25 | Loss: 0.00074982
Iteration 25/25 | Loss: 0.00074982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074982
Iteration 2/1000 | Loss: 0.00006862
Iteration 3/1000 | Loss: 0.00004179
Iteration 4/1000 | Loss: 0.00003274
Iteration 5/1000 | Loss: 0.00002995
Iteration 6/1000 | Loss: 0.00002871
Iteration 7/1000 | Loss: 0.00002775
Iteration 8/1000 | Loss: 0.00002705
Iteration 9/1000 | Loss: 0.00002655
Iteration 10/1000 | Loss: 0.00002619
Iteration 11/1000 | Loss: 0.00002584
Iteration 12/1000 | Loss: 0.00002553
Iteration 13/1000 | Loss: 0.00002529
Iteration 14/1000 | Loss: 0.00002513
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002495
Iteration 17/1000 | Loss: 0.00002494
Iteration 18/1000 | Loss: 0.00002479
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002469
Iteration 21/1000 | Loss: 0.00002469
Iteration 22/1000 | Loss: 0.00002463
Iteration 23/1000 | Loss: 0.00002462
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002457
Iteration 26/1000 | Loss: 0.00002457
Iteration 27/1000 | Loss: 0.00002456
Iteration 28/1000 | Loss: 0.00002454
Iteration 29/1000 | Loss: 0.00002453
Iteration 30/1000 | Loss: 0.00002453
Iteration 31/1000 | Loss: 0.00002453
Iteration 32/1000 | Loss: 0.00002453
Iteration 33/1000 | Loss: 0.00002453
Iteration 34/1000 | Loss: 0.00002452
Iteration 35/1000 | Loss: 0.00002452
Iteration 36/1000 | Loss: 0.00002451
Iteration 37/1000 | Loss: 0.00002451
Iteration 38/1000 | Loss: 0.00002451
Iteration 39/1000 | Loss: 0.00002449
Iteration 40/1000 | Loss: 0.00002449
Iteration 41/1000 | Loss: 0.00002448
Iteration 42/1000 | Loss: 0.00002448
Iteration 43/1000 | Loss: 0.00002448
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002447
Iteration 46/1000 | Loss: 0.00002447
Iteration 47/1000 | Loss: 0.00002446
Iteration 48/1000 | Loss: 0.00002446
Iteration 49/1000 | Loss: 0.00002445
Iteration 50/1000 | Loss: 0.00002445
Iteration 51/1000 | Loss: 0.00002444
Iteration 52/1000 | Loss: 0.00002444
Iteration 53/1000 | Loss: 0.00002443
Iteration 54/1000 | Loss: 0.00002443
Iteration 55/1000 | Loss: 0.00002442
Iteration 56/1000 | Loss: 0.00002441
Iteration 57/1000 | Loss: 0.00002441
Iteration 58/1000 | Loss: 0.00002440
Iteration 59/1000 | Loss: 0.00002440
Iteration 60/1000 | Loss: 0.00002440
Iteration 61/1000 | Loss: 0.00002439
Iteration 62/1000 | Loss: 0.00002439
Iteration 63/1000 | Loss: 0.00002439
Iteration 64/1000 | Loss: 0.00002439
Iteration 65/1000 | Loss: 0.00002438
Iteration 66/1000 | Loss: 0.00002438
Iteration 67/1000 | Loss: 0.00002437
Iteration 68/1000 | Loss: 0.00002437
Iteration 69/1000 | Loss: 0.00002436
Iteration 70/1000 | Loss: 0.00002436
Iteration 71/1000 | Loss: 0.00002435
Iteration 72/1000 | Loss: 0.00002435
Iteration 73/1000 | Loss: 0.00002435
Iteration 74/1000 | Loss: 0.00002434
Iteration 75/1000 | Loss: 0.00002434
Iteration 76/1000 | Loss: 0.00002433
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002433
Iteration 79/1000 | Loss: 0.00002433
Iteration 80/1000 | Loss: 0.00002433
Iteration 81/1000 | Loss: 0.00002433
Iteration 82/1000 | Loss: 0.00002433
Iteration 83/1000 | Loss: 0.00002432
Iteration 84/1000 | Loss: 0.00002432
Iteration 85/1000 | Loss: 0.00002432
Iteration 86/1000 | Loss: 0.00002431
Iteration 87/1000 | Loss: 0.00002431
Iteration 88/1000 | Loss: 0.00002431
Iteration 89/1000 | Loss: 0.00002431
Iteration 90/1000 | Loss: 0.00002431
Iteration 91/1000 | Loss: 0.00002431
Iteration 92/1000 | Loss: 0.00002430
Iteration 93/1000 | Loss: 0.00002430
Iteration 94/1000 | Loss: 0.00002430
Iteration 95/1000 | Loss: 0.00002430
Iteration 96/1000 | Loss: 0.00002430
Iteration 97/1000 | Loss: 0.00002430
Iteration 98/1000 | Loss: 0.00002430
Iteration 99/1000 | Loss: 0.00002429
Iteration 100/1000 | Loss: 0.00002429
Iteration 101/1000 | Loss: 0.00002429
Iteration 102/1000 | Loss: 0.00002429
Iteration 103/1000 | Loss: 0.00002429
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00002429
Iteration 106/1000 | Loss: 0.00002429
Iteration 107/1000 | Loss: 0.00002429
Iteration 108/1000 | Loss: 0.00002429
Iteration 109/1000 | Loss: 0.00002429
Iteration 110/1000 | Loss: 0.00002429
Iteration 111/1000 | Loss: 0.00002429
Iteration 112/1000 | Loss: 0.00002429
Iteration 113/1000 | Loss: 0.00002429
Iteration 114/1000 | Loss: 0.00002429
Iteration 115/1000 | Loss: 0.00002429
Iteration 116/1000 | Loss: 0.00002429
Iteration 117/1000 | Loss: 0.00002429
Iteration 118/1000 | Loss: 0.00002429
Iteration 119/1000 | Loss: 0.00002429
Iteration 120/1000 | Loss: 0.00002429
Iteration 121/1000 | Loss: 0.00002429
Iteration 122/1000 | Loss: 0.00002429
Iteration 123/1000 | Loss: 0.00002429
Iteration 124/1000 | Loss: 0.00002429
Iteration 125/1000 | Loss: 0.00002429
Iteration 126/1000 | Loss: 0.00002429
Iteration 127/1000 | Loss: 0.00002429
Iteration 128/1000 | Loss: 0.00002429
Iteration 129/1000 | Loss: 0.00002429
Iteration 130/1000 | Loss: 0.00002429
Iteration 131/1000 | Loss: 0.00002429
Iteration 132/1000 | Loss: 0.00002429
Iteration 133/1000 | Loss: 0.00002429
Iteration 134/1000 | Loss: 0.00002429
Iteration 135/1000 | Loss: 0.00002429
Iteration 136/1000 | Loss: 0.00002429
Iteration 137/1000 | Loss: 0.00002429
Iteration 138/1000 | Loss: 0.00002429
Iteration 139/1000 | Loss: 0.00002429
Iteration 140/1000 | Loss: 0.00002429
Iteration 141/1000 | Loss: 0.00002429
Iteration 142/1000 | Loss: 0.00002429
Iteration 143/1000 | Loss: 0.00002429
Iteration 144/1000 | Loss: 0.00002429
Iteration 145/1000 | Loss: 0.00002429
Iteration 146/1000 | Loss: 0.00002429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.4287324777105823e-05, 2.4287324777105823e-05, 2.4287324777105823e-05, 2.4287324777105823e-05, 2.4287324777105823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4287324777105823e-05

Optimization complete. Final v2v error: 4.030419826507568 mm

Highest mean error: 4.809149265289307 mm for frame 79

Lowest mean error: 3.580881118774414 mm for frame 136

Saving results

Total time: 43.82657814025879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455404
Iteration 2/25 | Loss: 0.00124463
Iteration 3/25 | Loss: 0.00116918
Iteration 4/25 | Loss: 0.00116031
Iteration 5/25 | Loss: 0.00115759
Iteration 6/25 | Loss: 0.00115750
Iteration 7/25 | Loss: 0.00115750
Iteration 8/25 | Loss: 0.00115750
Iteration 9/25 | Loss: 0.00115750
Iteration 10/25 | Loss: 0.00115750
Iteration 11/25 | Loss: 0.00115750
Iteration 12/25 | Loss: 0.00115750
Iteration 13/25 | Loss: 0.00115750
Iteration 14/25 | Loss: 0.00115750
Iteration 15/25 | Loss: 0.00115750
Iteration 16/25 | Loss: 0.00115750
Iteration 17/25 | Loss: 0.00115750
Iteration 18/25 | Loss: 0.00115750
Iteration 19/25 | Loss: 0.00115750
Iteration 20/25 | Loss: 0.00115750
Iteration 21/25 | Loss: 0.00115750
Iteration 22/25 | Loss: 0.00115750
Iteration 23/25 | Loss: 0.00115750
Iteration 24/25 | Loss: 0.00115750
Iteration 25/25 | Loss: 0.00115750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37176609
Iteration 2/25 | Loss: 0.00104804
Iteration 3/25 | Loss: 0.00104803
Iteration 4/25 | Loss: 0.00104803
Iteration 5/25 | Loss: 0.00104803
Iteration 6/25 | Loss: 0.00104803
Iteration 7/25 | Loss: 0.00104803
Iteration 8/25 | Loss: 0.00104803
Iteration 9/25 | Loss: 0.00104803
Iteration 10/25 | Loss: 0.00104803
Iteration 11/25 | Loss: 0.00104803
Iteration 12/25 | Loss: 0.00104803
Iteration 13/25 | Loss: 0.00104803
Iteration 14/25 | Loss: 0.00104803
Iteration 15/25 | Loss: 0.00104803
Iteration 16/25 | Loss: 0.00104803
Iteration 17/25 | Loss: 0.00104803
Iteration 18/25 | Loss: 0.00104803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010480311466380954, 0.0010480311466380954, 0.0010480311466380954, 0.0010480311466380954, 0.0010480311466380954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010480311466380954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104803
Iteration 2/1000 | Loss: 0.00004200
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00002015
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00001752
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001590
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001571
Iteration 12/1000 | Loss: 0.00001562
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001554
Iteration 15/1000 | Loss: 0.00001554
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001543
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001524
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001523
Iteration 38/1000 | Loss: 0.00001523
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001523
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001522
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001519
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001517
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001509
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001509
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001508
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001495
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001495
Iteration 91/1000 | Loss: 0.00001495
Iteration 92/1000 | Loss: 0.00001495
Iteration 93/1000 | Loss: 0.00001495
Iteration 94/1000 | Loss: 0.00001495
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001495
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001493
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001491
Iteration 108/1000 | Loss: 0.00001491
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001489
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001488
Iteration 114/1000 | Loss: 0.00001488
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001486
Iteration 119/1000 | Loss: 0.00001486
Iteration 120/1000 | Loss: 0.00001486
Iteration 121/1000 | Loss: 0.00001486
Iteration 122/1000 | Loss: 0.00001486
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001483
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001483
Iteration 133/1000 | Loss: 0.00001482
Iteration 134/1000 | Loss: 0.00001482
Iteration 135/1000 | Loss: 0.00001482
Iteration 136/1000 | Loss: 0.00001482
Iteration 137/1000 | Loss: 0.00001482
Iteration 138/1000 | Loss: 0.00001482
Iteration 139/1000 | Loss: 0.00001482
Iteration 140/1000 | Loss: 0.00001482
Iteration 141/1000 | Loss: 0.00001481
Iteration 142/1000 | Loss: 0.00001481
Iteration 143/1000 | Loss: 0.00001481
Iteration 144/1000 | Loss: 0.00001481
Iteration 145/1000 | Loss: 0.00001481
Iteration 146/1000 | Loss: 0.00001480
Iteration 147/1000 | Loss: 0.00001480
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Iteration 150/1000 | Loss: 0.00001479
Iteration 151/1000 | Loss: 0.00001479
Iteration 152/1000 | Loss: 0.00001479
Iteration 153/1000 | Loss: 0.00001479
Iteration 154/1000 | Loss: 0.00001479
Iteration 155/1000 | Loss: 0.00001479
Iteration 156/1000 | Loss: 0.00001479
Iteration 157/1000 | Loss: 0.00001479
Iteration 158/1000 | Loss: 0.00001479
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001478
Iteration 161/1000 | Loss: 0.00001478
Iteration 162/1000 | Loss: 0.00001478
Iteration 163/1000 | Loss: 0.00001478
Iteration 164/1000 | Loss: 0.00001478
Iteration 165/1000 | Loss: 0.00001478
Iteration 166/1000 | Loss: 0.00001478
Iteration 167/1000 | Loss: 0.00001478
Iteration 168/1000 | Loss: 0.00001478
Iteration 169/1000 | Loss: 0.00001477
Iteration 170/1000 | Loss: 0.00001477
Iteration 171/1000 | Loss: 0.00001477
Iteration 172/1000 | Loss: 0.00001476
Iteration 173/1000 | Loss: 0.00001476
Iteration 174/1000 | Loss: 0.00001476
Iteration 175/1000 | Loss: 0.00001476
Iteration 176/1000 | Loss: 0.00001476
Iteration 177/1000 | Loss: 0.00001475
Iteration 178/1000 | Loss: 0.00001475
Iteration 179/1000 | Loss: 0.00001475
Iteration 180/1000 | Loss: 0.00001475
Iteration 181/1000 | Loss: 0.00001475
Iteration 182/1000 | Loss: 0.00001475
Iteration 183/1000 | Loss: 0.00001475
Iteration 184/1000 | Loss: 0.00001474
Iteration 185/1000 | Loss: 0.00001474
Iteration 186/1000 | Loss: 0.00001474
Iteration 187/1000 | Loss: 0.00001473
Iteration 188/1000 | Loss: 0.00001473
Iteration 189/1000 | Loss: 0.00001473
Iteration 190/1000 | Loss: 0.00001472
Iteration 191/1000 | Loss: 0.00001472
Iteration 192/1000 | Loss: 0.00001472
Iteration 193/1000 | Loss: 0.00001472
Iteration 194/1000 | Loss: 0.00001472
Iteration 195/1000 | Loss: 0.00001471
Iteration 196/1000 | Loss: 0.00001471
Iteration 197/1000 | Loss: 0.00001471
Iteration 198/1000 | Loss: 0.00001471
Iteration 199/1000 | Loss: 0.00001471
Iteration 200/1000 | Loss: 0.00001471
Iteration 201/1000 | Loss: 0.00001471
Iteration 202/1000 | Loss: 0.00001471
Iteration 203/1000 | Loss: 0.00001471
Iteration 204/1000 | Loss: 0.00001470
Iteration 205/1000 | Loss: 0.00001470
Iteration 206/1000 | Loss: 0.00001470
Iteration 207/1000 | Loss: 0.00001470
Iteration 208/1000 | Loss: 0.00001470
Iteration 209/1000 | Loss: 0.00001470
Iteration 210/1000 | Loss: 0.00001470
Iteration 211/1000 | Loss: 0.00001470
Iteration 212/1000 | Loss: 0.00001470
Iteration 213/1000 | Loss: 0.00001470
Iteration 214/1000 | Loss: 0.00001470
Iteration 215/1000 | Loss: 0.00001470
Iteration 216/1000 | Loss: 0.00001470
Iteration 217/1000 | Loss: 0.00001469
Iteration 218/1000 | Loss: 0.00001469
Iteration 219/1000 | Loss: 0.00001469
Iteration 220/1000 | Loss: 0.00001468
Iteration 221/1000 | Loss: 0.00001468
Iteration 222/1000 | Loss: 0.00001468
Iteration 223/1000 | Loss: 0.00001468
Iteration 224/1000 | Loss: 0.00001468
Iteration 225/1000 | Loss: 0.00001468
Iteration 226/1000 | Loss: 0.00001468
Iteration 227/1000 | Loss: 0.00001468
Iteration 228/1000 | Loss: 0.00001468
Iteration 229/1000 | Loss: 0.00001467
Iteration 230/1000 | Loss: 0.00001467
Iteration 231/1000 | Loss: 0.00001467
Iteration 232/1000 | Loss: 0.00001467
Iteration 233/1000 | Loss: 0.00001467
Iteration 234/1000 | Loss: 0.00001467
Iteration 235/1000 | Loss: 0.00001467
Iteration 236/1000 | Loss: 0.00001467
Iteration 237/1000 | Loss: 0.00001467
Iteration 238/1000 | Loss: 0.00001467
Iteration 239/1000 | Loss: 0.00001466
Iteration 240/1000 | Loss: 0.00001466
Iteration 241/1000 | Loss: 0.00001466
Iteration 242/1000 | Loss: 0.00001466
Iteration 243/1000 | Loss: 0.00001466
Iteration 244/1000 | Loss: 0.00001466
Iteration 245/1000 | Loss: 0.00001466
Iteration 246/1000 | Loss: 0.00001466
Iteration 247/1000 | Loss: 0.00001466
Iteration 248/1000 | Loss: 0.00001466
Iteration 249/1000 | Loss: 0.00001466
Iteration 250/1000 | Loss: 0.00001466
Iteration 251/1000 | Loss: 0.00001466
Iteration 252/1000 | Loss: 0.00001466
Iteration 253/1000 | Loss: 0.00001466
Iteration 254/1000 | Loss: 0.00001466
Iteration 255/1000 | Loss: 0.00001465
Iteration 256/1000 | Loss: 0.00001465
Iteration 257/1000 | Loss: 0.00001465
Iteration 258/1000 | Loss: 0.00001465
Iteration 259/1000 | Loss: 0.00001465
Iteration 260/1000 | Loss: 0.00001465
Iteration 261/1000 | Loss: 0.00001465
Iteration 262/1000 | Loss: 0.00001465
Iteration 263/1000 | Loss: 0.00001464
Iteration 264/1000 | Loss: 0.00001464
Iteration 265/1000 | Loss: 0.00001464
Iteration 266/1000 | Loss: 0.00001464
Iteration 267/1000 | Loss: 0.00001464
Iteration 268/1000 | Loss: 0.00001464
Iteration 269/1000 | Loss: 0.00001464
Iteration 270/1000 | Loss: 0.00001464
Iteration 271/1000 | Loss: 0.00001464
Iteration 272/1000 | Loss: 0.00001464
Iteration 273/1000 | Loss: 0.00001464
Iteration 274/1000 | Loss: 0.00001464
Iteration 275/1000 | Loss: 0.00001464
Iteration 276/1000 | Loss: 0.00001464
Iteration 277/1000 | Loss: 0.00001464
Iteration 278/1000 | Loss: 0.00001464
Iteration 279/1000 | Loss: 0.00001464
Iteration 280/1000 | Loss: 0.00001464
Iteration 281/1000 | Loss: 0.00001464
Iteration 282/1000 | Loss: 0.00001464
Iteration 283/1000 | Loss: 0.00001464
Iteration 284/1000 | Loss: 0.00001464
Iteration 285/1000 | Loss: 0.00001464
Iteration 286/1000 | Loss: 0.00001464
Iteration 287/1000 | Loss: 0.00001464
Iteration 288/1000 | Loss: 0.00001464
Iteration 289/1000 | Loss: 0.00001464
Iteration 290/1000 | Loss: 0.00001464
Iteration 291/1000 | Loss: 0.00001464
Iteration 292/1000 | Loss: 0.00001464
Iteration 293/1000 | Loss: 0.00001464
Iteration 294/1000 | Loss: 0.00001464
Iteration 295/1000 | Loss: 0.00001464
Iteration 296/1000 | Loss: 0.00001464
Iteration 297/1000 | Loss: 0.00001464
Iteration 298/1000 | Loss: 0.00001464
Iteration 299/1000 | Loss: 0.00001464
Iteration 300/1000 | Loss: 0.00001464
Iteration 301/1000 | Loss: 0.00001464
Iteration 302/1000 | Loss: 0.00001464
Iteration 303/1000 | Loss: 0.00001464
Iteration 304/1000 | Loss: 0.00001464
Iteration 305/1000 | Loss: 0.00001464
Iteration 306/1000 | Loss: 0.00001464
Iteration 307/1000 | Loss: 0.00001464
Iteration 308/1000 | Loss: 0.00001464
Iteration 309/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.4641193047282286e-05, 1.4641193047282286e-05, 1.4641193047282286e-05, 1.4641193047282286e-05, 1.4641193047282286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4641193047282286e-05

Optimization complete. Final v2v error: 3.1794774532318115 mm

Highest mean error: 3.5175745487213135 mm for frame 9

Lowest mean error: 2.5965845584869385 mm for frame 147

Saving results

Total time: 45.754600048065186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041801
Iteration 2/25 | Loss: 0.01041800
Iteration 3/25 | Loss: 0.01041800
Iteration 4/25 | Loss: 0.01041800
Iteration 5/25 | Loss: 0.01041800
Iteration 6/25 | Loss: 0.01041800
Iteration 7/25 | Loss: 0.01041800
Iteration 8/25 | Loss: 0.01041800
Iteration 9/25 | Loss: 0.01041800
Iteration 10/25 | Loss: 0.01041800
Iteration 11/25 | Loss: 0.01041799
Iteration 12/25 | Loss: 0.01041799
Iteration 13/25 | Loss: 0.01041799
Iteration 14/25 | Loss: 0.01041799
Iteration 15/25 | Loss: 0.01041799
Iteration 16/25 | Loss: 0.01041799
Iteration 17/25 | Loss: 0.01041798
Iteration 18/25 | Loss: 0.01041798
Iteration 19/25 | Loss: 0.01041798
Iteration 20/25 | Loss: 0.01041798
Iteration 21/25 | Loss: 0.01041798
Iteration 22/25 | Loss: 0.01041798
Iteration 23/25 | Loss: 0.01041798
Iteration 24/25 | Loss: 0.01041797
Iteration 25/25 | Loss: 0.01041797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30606687
Iteration 2/25 | Loss: 0.18661186
Iteration 3/25 | Loss: 0.18233411
Iteration 4/25 | Loss: 0.17751625
Iteration 5/25 | Loss: 0.17740895
Iteration 6/25 | Loss: 0.17740895
Iteration 7/25 | Loss: 0.17740893
Iteration 8/25 | Loss: 0.17740890
Iteration 9/25 | Loss: 0.17740890
Iteration 10/25 | Loss: 0.17740893
Iteration 11/25 | Loss: 0.17740893
Iteration 12/25 | Loss: 0.17740890
Iteration 13/25 | Loss: 0.17740890
Iteration 14/25 | Loss: 0.17740890
Iteration 15/25 | Loss: 0.17740890
Iteration 16/25 | Loss: 0.17740890
Iteration 17/25 | Loss: 0.17740890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.17740890383720398, 0.17740890383720398, 0.17740890383720398, 0.17740890383720398, 0.17740890383720398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17740890383720398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17740890
Iteration 2/1000 | Loss: 0.00279062
Iteration 3/1000 | Loss: 0.00099058
Iteration 4/1000 | Loss: 0.00084105
Iteration 5/1000 | Loss: 0.00013858
Iteration 6/1000 | Loss: 0.00012071
Iteration 7/1000 | Loss: 0.00003810
Iteration 8/1000 | Loss: 0.00003036
Iteration 9/1000 | Loss: 0.00012419
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002315
Iteration 12/1000 | Loss: 0.00002173
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00028207
Iteration 15/1000 | Loss: 0.00051847
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00001925
Iteration 18/1000 | Loss: 0.00050872
Iteration 19/1000 | Loss: 0.00001968
Iteration 20/1000 | Loss: 0.00050833
Iteration 21/1000 | Loss: 0.00122922
Iteration 22/1000 | Loss: 0.00007980
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00038440
Iteration 26/1000 | Loss: 0.00150835
Iteration 27/1000 | Loss: 0.00002806
Iteration 28/1000 | Loss: 0.00010315
Iteration 29/1000 | Loss: 0.00001687
Iteration 30/1000 | Loss: 0.00001623
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001547
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001454
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001446
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001442
Iteration 46/1000 | Loss: 0.00001441
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.422889454261167e-05, 1.422889454261167e-05, 1.422889454261167e-05, 1.422889454261167e-05, 1.422889454261167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.422889454261167e-05

Optimization complete. Final v2v error: 3.2456881999969482 mm

Highest mean error: 3.396278142929077 mm for frame 50

Lowest mean error: 3.0233867168426514 mm for frame 220

Saving results

Total time: 74.35680198669434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043067
Iteration 2/25 | Loss: 0.01043067
Iteration 3/25 | Loss: 0.01043067
Iteration 4/25 | Loss: 0.00287936
Iteration 5/25 | Loss: 0.00179340
Iteration 6/25 | Loss: 0.00184448
Iteration 7/25 | Loss: 0.00162235
Iteration 8/25 | Loss: 0.00157318
Iteration 9/25 | Loss: 0.00148425
Iteration 10/25 | Loss: 0.00145189
Iteration 11/25 | Loss: 0.00144340
Iteration 12/25 | Loss: 0.00141954
Iteration 13/25 | Loss: 0.00140884
Iteration 14/25 | Loss: 0.00140062
Iteration 15/25 | Loss: 0.00138510
Iteration 16/25 | Loss: 0.00137680
Iteration 17/25 | Loss: 0.00137099
Iteration 18/25 | Loss: 0.00137392
Iteration 19/25 | Loss: 0.00137287
Iteration 20/25 | Loss: 0.00136776
Iteration 21/25 | Loss: 0.00135860
Iteration 22/25 | Loss: 0.00135335
Iteration 23/25 | Loss: 0.00135192
Iteration 24/25 | Loss: 0.00135030
Iteration 25/25 | Loss: 0.00134990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31405139
Iteration 2/25 | Loss: 0.00286711
Iteration 3/25 | Loss: 0.00278992
Iteration 4/25 | Loss: 0.00278992
Iteration 5/25 | Loss: 0.00278992
Iteration 6/25 | Loss: 0.00278992
Iteration 7/25 | Loss: 0.00278992
Iteration 8/25 | Loss: 0.00278992
Iteration 9/25 | Loss: 0.00278992
Iteration 10/25 | Loss: 0.00278991
Iteration 11/25 | Loss: 0.00278991
Iteration 12/25 | Loss: 0.00278991
Iteration 13/25 | Loss: 0.00278991
Iteration 14/25 | Loss: 0.00278991
Iteration 15/25 | Loss: 0.00278991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027899148408323526, 0.0027899148408323526, 0.0027899148408323526, 0.0027899148408323526, 0.0027899148408323526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027899148408323526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278991
Iteration 2/1000 | Loss: 0.00370458
Iteration 3/1000 | Loss: 0.00293671
Iteration 4/1000 | Loss: 0.01135400
Iteration 5/1000 | Loss: 0.00186236
Iteration 6/1000 | Loss: 0.00323715
Iteration 7/1000 | Loss: 0.00228727
Iteration 8/1000 | Loss: 0.00250323
Iteration 9/1000 | Loss: 0.00178074
Iteration 10/1000 | Loss: 0.00242643
Iteration 11/1000 | Loss: 0.00213828
Iteration 12/1000 | Loss: 0.00092184
Iteration 13/1000 | Loss: 0.00021002
Iteration 14/1000 | Loss: 0.00224613
Iteration 15/1000 | Loss: 0.00080835
Iteration 16/1000 | Loss: 0.00036369
Iteration 17/1000 | Loss: 0.00071032
Iteration 18/1000 | Loss: 0.00028689
Iteration 19/1000 | Loss: 0.00060877
Iteration 20/1000 | Loss: 0.00023456
Iteration 21/1000 | Loss: 0.00057666
Iteration 22/1000 | Loss: 0.00019629
Iteration 23/1000 | Loss: 0.00010797
Iteration 24/1000 | Loss: 0.00045118
Iteration 25/1000 | Loss: 0.00017769
Iteration 26/1000 | Loss: 0.00086057
Iteration 27/1000 | Loss: 0.00041466
Iteration 28/1000 | Loss: 0.00664160
Iteration 29/1000 | Loss: 0.00224826
Iteration 30/1000 | Loss: 0.00025085
Iteration 31/1000 | Loss: 0.00170102
Iteration 32/1000 | Loss: 0.00085808
Iteration 33/1000 | Loss: 0.00367005
Iteration 34/1000 | Loss: 0.00251613
Iteration 35/1000 | Loss: 0.00106931
Iteration 36/1000 | Loss: 0.00220658
Iteration 37/1000 | Loss: 0.00184300
Iteration 38/1000 | Loss: 0.00301855
Iteration 39/1000 | Loss: 0.00180877
Iteration 40/1000 | Loss: 0.00440447
Iteration 41/1000 | Loss: 0.00210700
Iteration 42/1000 | Loss: 0.00229092
Iteration 43/1000 | Loss: 0.00254309
Iteration 44/1000 | Loss: 0.00280582
Iteration 45/1000 | Loss: 0.00199753
Iteration 46/1000 | Loss: 0.00227675
Iteration 47/1000 | Loss: 0.00131585
Iteration 48/1000 | Loss: 0.00082745
Iteration 49/1000 | Loss: 0.00076856
Iteration 50/1000 | Loss: 0.00162007
Iteration 51/1000 | Loss: 0.00055634
Iteration 52/1000 | Loss: 0.00009661
Iteration 53/1000 | Loss: 0.00112830
Iteration 54/1000 | Loss: 0.00087135
Iteration 55/1000 | Loss: 0.00068847
Iteration 56/1000 | Loss: 0.00181674
Iteration 57/1000 | Loss: 0.00061733
Iteration 58/1000 | Loss: 0.00069567
Iteration 59/1000 | Loss: 0.00013769
Iteration 60/1000 | Loss: 0.00073333
Iteration 61/1000 | Loss: 0.00032683
Iteration 62/1000 | Loss: 0.00098058
Iteration 63/1000 | Loss: 0.00086419
Iteration 64/1000 | Loss: 0.00103848
Iteration 65/1000 | Loss: 0.00163290
Iteration 66/1000 | Loss: 0.00146607
Iteration 67/1000 | Loss: 0.00070479
Iteration 68/1000 | Loss: 0.00081005
Iteration 69/1000 | Loss: 0.00102689
Iteration 70/1000 | Loss: 0.00051865
Iteration 71/1000 | Loss: 0.00056905
Iteration 72/1000 | Loss: 0.00085421
Iteration 73/1000 | Loss: 0.00046453
Iteration 74/1000 | Loss: 0.00049029
Iteration 75/1000 | Loss: 0.00031622
Iteration 76/1000 | Loss: 0.00037452
Iteration 77/1000 | Loss: 0.00021442
Iteration 78/1000 | Loss: 0.00033467
Iteration 79/1000 | Loss: 0.00022775
Iteration 80/1000 | Loss: 0.00005873
Iteration 81/1000 | Loss: 0.00010178
Iteration 82/1000 | Loss: 0.00028162
Iteration 83/1000 | Loss: 0.00040288
Iteration 84/1000 | Loss: 0.00069936
Iteration 85/1000 | Loss: 0.00047706
Iteration 86/1000 | Loss: 0.00022601
Iteration 87/1000 | Loss: 0.00038843
Iteration 88/1000 | Loss: 0.00033149
Iteration 89/1000 | Loss: 0.00055819
Iteration 90/1000 | Loss: 0.00034209
Iteration 91/1000 | Loss: 0.00070749
Iteration 92/1000 | Loss: 0.00024561
Iteration 93/1000 | Loss: 0.00055731
Iteration 94/1000 | Loss: 0.00042743
Iteration 95/1000 | Loss: 0.00039968
Iteration 96/1000 | Loss: 0.00039445
Iteration 97/1000 | Loss: 0.00035381
Iteration 98/1000 | Loss: 0.00050138
Iteration 99/1000 | Loss: 0.00070767
Iteration 100/1000 | Loss: 0.00007278
Iteration 101/1000 | Loss: 0.00004204
Iteration 102/1000 | Loss: 0.00022140
Iteration 103/1000 | Loss: 0.00038931
Iteration 104/1000 | Loss: 0.00013655
Iteration 105/1000 | Loss: 0.00004298
Iteration 106/1000 | Loss: 0.00009346
Iteration 107/1000 | Loss: 0.00011421
Iteration 108/1000 | Loss: 0.00003266
Iteration 109/1000 | Loss: 0.00004250
Iteration 110/1000 | Loss: 0.00003488
Iteration 111/1000 | Loss: 0.00006131
Iteration 112/1000 | Loss: 0.00005682
Iteration 113/1000 | Loss: 0.00003674
Iteration 114/1000 | Loss: 0.00055032
Iteration 115/1000 | Loss: 0.00070497
Iteration 116/1000 | Loss: 0.00059287
Iteration 117/1000 | Loss: 0.00048039
Iteration 118/1000 | Loss: 0.00052012
Iteration 119/1000 | Loss: 0.00006556
Iteration 120/1000 | Loss: 0.00005457
Iteration 121/1000 | Loss: 0.00018509
Iteration 122/1000 | Loss: 0.00004617
Iteration 123/1000 | Loss: 0.00003345
Iteration 124/1000 | Loss: 0.00006535
Iteration 125/1000 | Loss: 0.00005823
Iteration 126/1000 | Loss: 0.00002074
Iteration 127/1000 | Loss: 0.00029705
Iteration 128/1000 | Loss: 0.00005360
Iteration 129/1000 | Loss: 0.00004370
Iteration 130/1000 | Loss: 0.00008270
Iteration 131/1000 | Loss: 0.00003183
Iteration 132/1000 | Loss: 0.00002926
Iteration 133/1000 | Loss: 0.00002888
Iteration 134/1000 | Loss: 0.00033085
Iteration 135/1000 | Loss: 0.00004013
Iteration 136/1000 | Loss: 0.00004302
Iteration 137/1000 | Loss: 0.00004124
Iteration 138/1000 | Loss: 0.00002984
Iteration 139/1000 | Loss: 0.00002782
Iteration 140/1000 | Loss: 0.00003075
Iteration 141/1000 | Loss: 0.00002867
Iteration 142/1000 | Loss: 0.00006335
Iteration 143/1000 | Loss: 0.00031492
Iteration 144/1000 | Loss: 0.00004109
Iteration 145/1000 | Loss: 0.00015076
Iteration 146/1000 | Loss: 0.00002994
Iteration 147/1000 | Loss: 0.00007253
Iteration 148/1000 | Loss: 0.00016797
Iteration 149/1000 | Loss: 0.00004903
Iteration 150/1000 | Loss: 0.00002840
Iteration 151/1000 | Loss: 0.00007309
Iteration 152/1000 | Loss: 0.00002981
Iteration 153/1000 | Loss: 0.00004421
Iteration 154/1000 | Loss: 0.00018343
Iteration 155/1000 | Loss: 0.00005442
Iteration 156/1000 | Loss: 0.00002919
Iteration 157/1000 | Loss: 0.00002922
Iteration 158/1000 | Loss: 0.00002949
Iteration 159/1000 | Loss: 0.00002786
Iteration 160/1000 | Loss: 0.00003120
Iteration 161/1000 | Loss: 0.00002495
Iteration 162/1000 | Loss: 0.00002567
Iteration 163/1000 | Loss: 0.00002547
Iteration 164/1000 | Loss: 0.00005385
Iteration 165/1000 | Loss: 0.00002751
Iteration 166/1000 | Loss: 0.00008823
Iteration 167/1000 | Loss: 0.00002538
Iteration 168/1000 | Loss: 0.00003061
Iteration 169/1000 | Loss: 0.00004677
Iteration 170/1000 | Loss: 0.00002271
Iteration 171/1000 | Loss: 0.00003237
Iteration 172/1000 | Loss: 0.00002545
Iteration 173/1000 | Loss: 0.00002536
Iteration 174/1000 | Loss: 0.00002708
Iteration 175/1000 | Loss: 0.00002789
Iteration 176/1000 | Loss: 0.00003345
Iteration 177/1000 | Loss: 0.00002957
Iteration 178/1000 | Loss: 0.00002925
Iteration 179/1000 | Loss: 0.00002756
Iteration 180/1000 | Loss: 0.00003131
Iteration 181/1000 | Loss: 0.00003055
Iteration 182/1000 | Loss: 0.00002924
Iteration 183/1000 | Loss: 0.00002663
Iteration 184/1000 | Loss: 0.00005930
Iteration 185/1000 | Loss: 0.00003361
Iteration 186/1000 | Loss: 0.00002553
Iteration 187/1000 | Loss: 0.00002635
Iteration 188/1000 | Loss: 0.00003684
Iteration 189/1000 | Loss: 0.00003679
Iteration 190/1000 | Loss: 0.00002583
Iteration 191/1000 | Loss: 0.00002556
Iteration 192/1000 | Loss: 0.00006322
Iteration 193/1000 | Loss: 0.00003126
Iteration 194/1000 | Loss: 0.00002579
Iteration 195/1000 | Loss: 0.00002624
Iteration 196/1000 | Loss: 0.00003905
Iteration 197/1000 | Loss: 0.00061311
Iteration 198/1000 | Loss: 0.00006547
Iteration 199/1000 | Loss: 0.00002737
Iteration 200/1000 | Loss: 0.00002654
Iteration 201/1000 | Loss: 0.00002710
Iteration 202/1000 | Loss: 0.00002596
Iteration 203/1000 | Loss: 0.00002632
Iteration 204/1000 | Loss: 0.00002522
Iteration 205/1000 | Loss: 0.00002770
Iteration 206/1000 | Loss: 0.00002608
Iteration 207/1000 | Loss: 0.00002821
Iteration 208/1000 | Loss: 0.00002813
Iteration 209/1000 | Loss: 0.00002505
Iteration 210/1000 | Loss: 0.00003186
Iteration 211/1000 | Loss: 0.00002762
Iteration 212/1000 | Loss: 0.00002483
Iteration 213/1000 | Loss: 0.00002736
Iteration 214/1000 | Loss: 0.00030465
Iteration 215/1000 | Loss: 0.00002505
Iteration 216/1000 | Loss: 0.00001785
Iteration 217/1000 | Loss: 0.00001785
Iteration 218/1000 | Loss: 0.00001596
Iteration 219/1000 | Loss: 0.00001932
Iteration 220/1000 | Loss: 0.00001542
Iteration 221/1000 | Loss: 0.00030387
Iteration 222/1000 | Loss: 0.00010549
Iteration 223/1000 | Loss: 0.00032576
Iteration 224/1000 | Loss: 0.00007980
Iteration 225/1000 | Loss: 0.00005031
Iteration 226/1000 | Loss: 0.00001735
Iteration 227/1000 | Loss: 0.00001599
Iteration 228/1000 | Loss: 0.00001459
Iteration 229/1000 | Loss: 0.00001703
Iteration 230/1000 | Loss: 0.00001455
Iteration 231/1000 | Loss: 0.00001415
Iteration 232/1000 | Loss: 0.00001413
Iteration 233/1000 | Loss: 0.00001528
Iteration 234/1000 | Loss: 0.00001411
Iteration 235/1000 | Loss: 0.00001411
Iteration 236/1000 | Loss: 0.00001410
Iteration 237/1000 | Loss: 0.00001410
Iteration 238/1000 | Loss: 0.00001410
Iteration 239/1000 | Loss: 0.00001409
Iteration 240/1000 | Loss: 0.00001407
Iteration 241/1000 | Loss: 0.00001407
Iteration 242/1000 | Loss: 0.00001407
Iteration 243/1000 | Loss: 0.00001513
Iteration 244/1000 | Loss: 0.00001458
Iteration 245/1000 | Loss: 0.00002249
Iteration 246/1000 | Loss: 0.00001807
Iteration 247/1000 | Loss: 0.00001458
Iteration 248/1000 | Loss: 0.00001397
Iteration 249/1000 | Loss: 0.00001397
Iteration 250/1000 | Loss: 0.00001397
Iteration 251/1000 | Loss: 0.00001397
Iteration 252/1000 | Loss: 0.00001397
Iteration 253/1000 | Loss: 0.00001400
Iteration 254/1000 | Loss: 0.00001397
Iteration 255/1000 | Loss: 0.00001397
Iteration 256/1000 | Loss: 0.00001397
Iteration 257/1000 | Loss: 0.00001397
Iteration 258/1000 | Loss: 0.00001397
Iteration 259/1000 | Loss: 0.00001396
Iteration 260/1000 | Loss: 0.00001396
Iteration 261/1000 | Loss: 0.00001396
Iteration 262/1000 | Loss: 0.00001396
Iteration 263/1000 | Loss: 0.00001398
Iteration 264/1000 | Loss: 0.00001396
Iteration 265/1000 | Loss: 0.00001396
Iteration 266/1000 | Loss: 0.00001395
Iteration 267/1000 | Loss: 0.00001489
Iteration 268/1000 | Loss: 0.00001423
Iteration 269/1000 | Loss: 0.00001422
Iteration 270/1000 | Loss: 0.00001422
Iteration 271/1000 | Loss: 0.00001422
Iteration 272/1000 | Loss: 0.00001422
Iteration 273/1000 | Loss: 0.00001422
Iteration 274/1000 | Loss: 0.00001421
Iteration 275/1000 | Loss: 0.00001420
Iteration 276/1000 | Loss: 0.00001419
Iteration 277/1000 | Loss: 0.00001419
Iteration 278/1000 | Loss: 0.00001419
Iteration 279/1000 | Loss: 0.00001434
Iteration 280/1000 | Loss: 0.00001434
Iteration 281/1000 | Loss: 0.00002130
Iteration 282/1000 | Loss: 0.00001413
Iteration 283/1000 | Loss: 0.00001391
Iteration 284/1000 | Loss: 0.00001390
Iteration 285/1000 | Loss: 0.00001390
Iteration 286/1000 | Loss: 0.00001390
Iteration 287/1000 | Loss: 0.00001390
Iteration 288/1000 | Loss: 0.00001390
Iteration 289/1000 | Loss: 0.00001390
Iteration 290/1000 | Loss: 0.00001390
Iteration 291/1000 | Loss: 0.00001390
Iteration 292/1000 | Loss: 0.00001390
Iteration 293/1000 | Loss: 0.00001390
Iteration 294/1000 | Loss: 0.00001389
Iteration 295/1000 | Loss: 0.00001389
Iteration 296/1000 | Loss: 0.00001389
Iteration 297/1000 | Loss: 0.00001389
Iteration 298/1000 | Loss: 0.00001389
Iteration 299/1000 | Loss: 0.00001389
Iteration 300/1000 | Loss: 0.00001389
Iteration 301/1000 | Loss: 0.00001389
Iteration 302/1000 | Loss: 0.00001388
Iteration 303/1000 | Loss: 0.00001388
Iteration 304/1000 | Loss: 0.00001388
Iteration 305/1000 | Loss: 0.00001388
Iteration 306/1000 | Loss: 0.00001388
Iteration 307/1000 | Loss: 0.00001388
Iteration 308/1000 | Loss: 0.00001388
Iteration 309/1000 | Loss: 0.00001388
Iteration 310/1000 | Loss: 0.00001388
Iteration 311/1000 | Loss: 0.00001388
Iteration 312/1000 | Loss: 0.00001387
Iteration 313/1000 | Loss: 0.00001387
Iteration 314/1000 | Loss: 0.00001387
Iteration 315/1000 | Loss: 0.00001387
Iteration 316/1000 | Loss: 0.00001387
Iteration 317/1000 | Loss: 0.00001387
Iteration 318/1000 | Loss: 0.00001387
Iteration 319/1000 | Loss: 0.00001387
Iteration 320/1000 | Loss: 0.00001387
Iteration 321/1000 | Loss: 0.00001387
Iteration 322/1000 | Loss: 0.00001387
Iteration 323/1000 | Loss: 0.00001387
Iteration 324/1000 | Loss: 0.00001387
Iteration 325/1000 | Loss: 0.00001387
Iteration 326/1000 | Loss: 0.00001520
Iteration 327/1000 | Loss: 0.00001393
Iteration 328/1000 | Loss: 0.00001388
Iteration 329/1000 | Loss: 0.00001388
Iteration 330/1000 | Loss: 0.00001388
Iteration 331/1000 | Loss: 0.00001388
Iteration 332/1000 | Loss: 0.00001388
Iteration 333/1000 | Loss: 0.00001388
Iteration 334/1000 | Loss: 0.00001388
Iteration 335/1000 | Loss: 0.00001387
Iteration 336/1000 | Loss: 0.00001387
Iteration 337/1000 | Loss: 0.00001387
Iteration 338/1000 | Loss: 0.00001386
Iteration 339/1000 | Loss: 0.00001386
Iteration 340/1000 | Loss: 0.00001386
Iteration 341/1000 | Loss: 0.00001386
Iteration 342/1000 | Loss: 0.00001386
Iteration 343/1000 | Loss: 0.00001385
Iteration 344/1000 | Loss: 0.00001385
Iteration 345/1000 | Loss: 0.00001385
Iteration 346/1000 | Loss: 0.00001385
Iteration 347/1000 | Loss: 0.00001385
Iteration 348/1000 | Loss: 0.00001385
Iteration 349/1000 | Loss: 0.00001385
Iteration 350/1000 | Loss: 0.00001384
Iteration 351/1000 | Loss: 0.00001384
Iteration 352/1000 | Loss: 0.00001384
Iteration 353/1000 | Loss: 0.00001384
Iteration 354/1000 | Loss: 0.00001384
Iteration 355/1000 | Loss: 0.00001384
Iteration 356/1000 | Loss: 0.00001384
Iteration 357/1000 | Loss: 0.00001384
Iteration 358/1000 | Loss: 0.00001384
Iteration 359/1000 | Loss: 0.00001384
Iteration 360/1000 | Loss: 0.00001384
Iteration 361/1000 | Loss: 0.00001384
Iteration 362/1000 | Loss: 0.00001384
Iteration 363/1000 | Loss: 0.00001384
Iteration 364/1000 | Loss: 0.00001384
Iteration 365/1000 | Loss: 0.00001383
Iteration 366/1000 | Loss: 0.00001383
Iteration 367/1000 | Loss: 0.00001383
Iteration 368/1000 | Loss: 0.00001383
Iteration 369/1000 | Loss: 0.00001383
Iteration 370/1000 | Loss: 0.00001383
Iteration 371/1000 | Loss: 0.00001383
Iteration 372/1000 | Loss: 0.00001383
Iteration 373/1000 | Loss: 0.00001383
Iteration 374/1000 | Loss: 0.00001383
Iteration 375/1000 | Loss: 0.00001383
Iteration 376/1000 | Loss: 0.00001383
Iteration 377/1000 | Loss: 0.00001383
Iteration 378/1000 | Loss: 0.00001383
Iteration 379/1000 | Loss: 0.00001383
Iteration 380/1000 | Loss: 0.00001519
Iteration 381/1000 | Loss: 0.00001383
Iteration 382/1000 | Loss: 0.00001383
Iteration 383/1000 | Loss: 0.00001383
Iteration 384/1000 | Loss: 0.00001382
Iteration 385/1000 | Loss: 0.00001382
Iteration 386/1000 | Loss: 0.00001382
Iteration 387/1000 | Loss: 0.00001382
Iteration 388/1000 | Loss: 0.00001382
Iteration 389/1000 | Loss: 0.00001382
Iteration 390/1000 | Loss: 0.00001382
Iteration 391/1000 | Loss: 0.00001382
Iteration 392/1000 | Loss: 0.00001382
Iteration 393/1000 | Loss: 0.00001382
Iteration 394/1000 | Loss: 0.00001382
Iteration 395/1000 | Loss: 0.00001382
Iteration 396/1000 | Loss: 0.00001382
Iteration 397/1000 | Loss: 0.00001382
Iteration 398/1000 | Loss: 0.00001382
Iteration 399/1000 | Loss: 0.00001382
Iteration 400/1000 | Loss: 0.00001382
Iteration 401/1000 | Loss: 0.00001382
Iteration 402/1000 | Loss: 0.00001382
Iteration 403/1000 | Loss: 0.00001381
Iteration 404/1000 | Loss: 0.00001381
Iteration 405/1000 | Loss: 0.00001381
Iteration 406/1000 | Loss: 0.00001381
Iteration 407/1000 | Loss: 0.00001381
Iteration 408/1000 | Loss: 0.00001381
Iteration 409/1000 | Loss: 0.00001381
Iteration 410/1000 | Loss: 0.00001381
Iteration 411/1000 | Loss: 0.00001381
Iteration 412/1000 | Loss: 0.00001381
Iteration 413/1000 | Loss: 0.00001381
Iteration 414/1000 | Loss: 0.00001381
Iteration 415/1000 | Loss: 0.00001381
Iteration 416/1000 | Loss: 0.00001381
Iteration 417/1000 | Loss: 0.00001381
Iteration 418/1000 | Loss: 0.00001381
Iteration 419/1000 | Loss: 0.00001381
Iteration 420/1000 | Loss: 0.00001381
Iteration 421/1000 | Loss: 0.00001381
Iteration 422/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 422. Stopping optimization.
Last 5 losses: [1.3811856661050115e-05, 1.3811856661050115e-05, 1.3811856661050115e-05, 1.3811856661050115e-05, 1.3811856661050115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3811856661050115e-05

Optimization complete. Final v2v error: 3.0509181022644043 mm

Highest mean error: 5.853501796722412 mm for frame 28

Lowest mean error: 2.5382745265960693 mm for frame 173

Saving results

Total time: 435.9330596923828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969214
Iteration 2/25 | Loss: 0.00161270
Iteration 3/25 | Loss: 0.00136690
Iteration 4/25 | Loss: 0.00131481
Iteration 5/25 | Loss: 0.00129394
Iteration 6/25 | Loss: 0.00128828
Iteration 7/25 | Loss: 0.00128650
Iteration 8/25 | Loss: 0.00128607
Iteration 9/25 | Loss: 0.00128607
Iteration 10/25 | Loss: 0.00128607
Iteration 11/25 | Loss: 0.00128607
Iteration 12/25 | Loss: 0.00128607
Iteration 13/25 | Loss: 0.00128607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012860656715929508, 0.0012860656715929508, 0.0012860656715929508, 0.0012860656715929508, 0.0012860656715929508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012860656715929508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26927638
Iteration 2/25 | Loss: 0.00185681
Iteration 3/25 | Loss: 0.00185649
Iteration 4/25 | Loss: 0.00185649
Iteration 5/25 | Loss: 0.00185649
Iteration 6/25 | Loss: 0.00185649
Iteration 7/25 | Loss: 0.00185649
Iteration 8/25 | Loss: 0.00185649
Iteration 9/25 | Loss: 0.00185649
Iteration 10/25 | Loss: 0.00185649
Iteration 11/25 | Loss: 0.00185649
Iteration 12/25 | Loss: 0.00185649
Iteration 13/25 | Loss: 0.00185649
Iteration 14/25 | Loss: 0.00185649
Iteration 15/25 | Loss: 0.00185649
Iteration 16/25 | Loss: 0.00185649
Iteration 17/25 | Loss: 0.00185649
Iteration 18/25 | Loss: 0.00185649
Iteration 19/25 | Loss: 0.00185649
Iteration 20/25 | Loss: 0.00185649
Iteration 21/25 | Loss: 0.00185649
Iteration 22/25 | Loss: 0.00185649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018564907368272543, 0.0018564907368272543, 0.0018564907368272543, 0.0018564907368272543, 0.0018564907368272543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018564907368272543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185649
Iteration 2/1000 | Loss: 0.00018389
Iteration 3/1000 | Loss: 0.00013484
Iteration 4/1000 | Loss: 0.00011024
Iteration 5/1000 | Loss: 0.00010036
Iteration 6/1000 | Loss: 0.00009540
Iteration 7/1000 | Loss: 0.00009134
Iteration 8/1000 | Loss: 0.00008802
Iteration 9/1000 | Loss: 0.00008587
Iteration 10/1000 | Loss: 0.00104095
Iteration 11/1000 | Loss: 0.00039902
Iteration 12/1000 | Loss: 0.00150671
Iteration 13/1000 | Loss: 0.00182926
Iteration 14/1000 | Loss: 0.00174974
Iteration 15/1000 | Loss: 0.00197416
Iteration 16/1000 | Loss: 0.00108406
Iteration 17/1000 | Loss: 0.00018771
Iteration 18/1000 | Loss: 0.00041368
Iteration 19/1000 | Loss: 0.00071718
Iteration 20/1000 | Loss: 0.00096553
Iteration 21/1000 | Loss: 0.00059189
Iteration 22/1000 | Loss: 0.00097449
Iteration 23/1000 | Loss: 0.00025457
Iteration 24/1000 | Loss: 0.00085940
Iteration 25/1000 | Loss: 0.00075320
Iteration 26/1000 | Loss: 0.00128517
Iteration 27/1000 | Loss: 0.00031459
Iteration 28/1000 | Loss: 0.00057847
Iteration 29/1000 | Loss: 0.00028485
Iteration 30/1000 | Loss: 0.00039745
Iteration 31/1000 | Loss: 0.00120041
Iteration 32/1000 | Loss: 0.00034046
Iteration 33/1000 | Loss: 0.00077748
Iteration 34/1000 | Loss: 0.00033800
Iteration 35/1000 | Loss: 0.00117910
Iteration 36/1000 | Loss: 0.00034940
Iteration 37/1000 | Loss: 0.00124086
Iteration 38/1000 | Loss: 0.00011416
Iteration 39/1000 | Loss: 0.00018922
Iteration 40/1000 | Loss: 0.00007396
Iteration 41/1000 | Loss: 0.00006980
Iteration 42/1000 | Loss: 0.00006721
Iteration 43/1000 | Loss: 0.00006531
Iteration 44/1000 | Loss: 0.00006377
Iteration 45/1000 | Loss: 0.00006293
Iteration 46/1000 | Loss: 0.00032825
Iteration 47/1000 | Loss: 0.00006211
Iteration 48/1000 | Loss: 0.00056135
Iteration 49/1000 | Loss: 0.00155986
Iteration 50/1000 | Loss: 0.00087565
Iteration 51/1000 | Loss: 0.00005621
Iteration 52/1000 | Loss: 0.00004855
Iteration 53/1000 | Loss: 0.00004548
Iteration 54/1000 | Loss: 0.00004271
Iteration 55/1000 | Loss: 0.00004079
Iteration 56/1000 | Loss: 0.00003945
Iteration 57/1000 | Loss: 0.00003856
Iteration 58/1000 | Loss: 0.00003795
Iteration 59/1000 | Loss: 0.00003754
Iteration 60/1000 | Loss: 0.00003718
Iteration 61/1000 | Loss: 0.00003678
Iteration 62/1000 | Loss: 0.00003647
Iteration 63/1000 | Loss: 0.00003630
Iteration 64/1000 | Loss: 0.00003629
Iteration 65/1000 | Loss: 0.00003625
Iteration 66/1000 | Loss: 0.00003621
Iteration 67/1000 | Loss: 0.00003617
Iteration 68/1000 | Loss: 0.00003615
Iteration 69/1000 | Loss: 0.00003613
Iteration 70/1000 | Loss: 0.00003596
Iteration 71/1000 | Loss: 0.00003594
Iteration 72/1000 | Loss: 0.00003588
Iteration 73/1000 | Loss: 0.00003587
Iteration 74/1000 | Loss: 0.00003587
Iteration 75/1000 | Loss: 0.00003586
Iteration 76/1000 | Loss: 0.00003581
Iteration 77/1000 | Loss: 0.00003579
Iteration 78/1000 | Loss: 0.00003579
Iteration 79/1000 | Loss: 0.00003578
Iteration 80/1000 | Loss: 0.00003578
Iteration 81/1000 | Loss: 0.00003577
Iteration 82/1000 | Loss: 0.00003577
Iteration 83/1000 | Loss: 0.00003576
Iteration 84/1000 | Loss: 0.00003576
Iteration 85/1000 | Loss: 0.00003576
Iteration 86/1000 | Loss: 0.00003575
Iteration 87/1000 | Loss: 0.00003575
Iteration 88/1000 | Loss: 0.00003575
Iteration 89/1000 | Loss: 0.00003573
Iteration 90/1000 | Loss: 0.00003573
Iteration 91/1000 | Loss: 0.00003572
Iteration 92/1000 | Loss: 0.00003571
Iteration 93/1000 | Loss: 0.00003571
Iteration 94/1000 | Loss: 0.00003570
Iteration 95/1000 | Loss: 0.00003570
Iteration 96/1000 | Loss: 0.00003569
Iteration 97/1000 | Loss: 0.00003569
Iteration 98/1000 | Loss: 0.00003569
Iteration 99/1000 | Loss: 0.00003569
Iteration 100/1000 | Loss: 0.00003569
Iteration 101/1000 | Loss: 0.00003569
Iteration 102/1000 | Loss: 0.00003569
Iteration 103/1000 | Loss: 0.00003569
Iteration 104/1000 | Loss: 0.00003569
Iteration 105/1000 | Loss: 0.00003569
Iteration 106/1000 | Loss: 0.00003569
Iteration 107/1000 | Loss: 0.00003568
Iteration 108/1000 | Loss: 0.00003568
Iteration 109/1000 | Loss: 0.00003568
Iteration 110/1000 | Loss: 0.00003568
Iteration 111/1000 | Loss: 0.00003568
Iteration 112/1000 | Loss: 0.00003568
Iteration 113/1000 | Loss: 0.00003568
Iteration 114/1000 | Loss: 0.00003568
Iteration 115/1000 | Loss: 0.00003568
Iteration 116/1000 | Loss: 0.00003568
Iteration 117/1000 | Loss: 0.00003568
Iteration 118/1000 | Loss: 0.00003567
Iteration 119/1000 | Loss: 0.00003567
Iteration 120/1000 | Loss: 0.00003567
Iteration 121/1000 | Loss: 0.00003566
Iteration 122/1000 | Loss: 0.00003566
Iteration 123/1000 | Loss: 0.00003565
Iteration 124/1000 | Loss: 0.00003565
Iteration 125/1000 | Loss: 0.00003564
Iteration 126/1000 | Loss: 0.00003564
Iteration 127/1000 | Loss: 0.00003564
Iteration 128/1000 | Loss: 0.00003564
Iteration 129/1000 | Loss: 0.00003563
Iteration 130/1000 | Loss: 0.00003562
Iteration 131/1000 | Loss: 0.00003561
Iteration 132/1000 | Loss: 0.00003561
Iteration 133/1000 | Loss: 0.00003561
Iteration 134/1000 | Loss: 0.00003559
Iteration 135/1000 | Loss: 0.00003559
Iteration 136/1000 | Loss: 0.00003559
Iteration 137/1000 | Loss: 0.00003558
Iteration 138/1000 | Loss: 0.00003558
Iteration 139/1000 | Loss: 0.00003558
Iteration 140/1000 | Loss: 0.00003558
Iteration 141/1000 | Loss: 0.00003557
Iteration 142/1000 | Loss: 0.00003557
Iteration 143/1000 | Loss: 0.00003557
Iteration 144/1000 | Loss: 0.00003557
Iteration 145/1000 | Loss: 0.00003556
Iteration 146/1000 | Loss: 0.00003556
Iteration 147/1000 | Loss: 0.00003556
Iteration 148/1000 | Loss: 0.00003555
Iteration 149/1000 | Loss: 0.00003555
Iteration 150/1000 | Loss: 0.00003555
Iteration 151/1000 | Loss: 0.00003554
Iteration 152/1000 | Loss: 0.00003554
Iteration 153/1000 | Loss: 0.00003554
Iteration 154/1000 | Loss: 0.00003554
Iteration 155/1000 | Loss: 0.00003554
Iteration 156/1000 | Loss: 0.00003554
Iteration 157/1000 | Loss: 0.00003553
Iteration 158/1000 | Loss: 0.00003553
Iteration 159/1000 | Loss: 0.00003553
Iteration 160/1000 | Loss: 0.00003553
Iteration 161/1000 | Loss: 0.00003553
Iteration 162/1000 | Loss: 0.00003553
Iteration 163/1000 | Loss: 0.00003552
Iteration 164/1000 | Loss: 0.00003552
Iteration 165/1000 | Loss: 0.00003552
Iteration 166/1000 | Loss: 0.00003552
Iteration 167/1000 | Loss: 0.00003551
Iteration 168/1000 | Loss: 0.00003551
Iteration 169/1000 | Loss: 0.00003551
Iteration 170/1000 | Loss: 0.00003551
Iteration 171/1000 | Loss: 0.00003550
Iteration 172/1000 | Loss: 0.00003550
Iteration 173/1000 | Loss: 0.00003550
Iteration 174/1000 | Loss: 0.00003549
Iteration 175/1000 | Loss: 0.00003549
Iteration 176/1000 | Loss: 0.00003549
Iteration 177/1000 | Loss: 0.00003548
Iteration 178/1000 | Loss: 0.00003548
Iteration 179/1000 | Loss: 0.00003548
Iteration 180/1000 | Loss: 0.00003548
Iteration 181/1000 | Loss: 0.00003548
Iteration 182/1000 | Loss: 0.00003547
Iteration 183/1000 | Loss: 0.00003547
Iteration 184/1000 | Loss: 0.00003547
Iteration 185/1000 | Loss: 0.00003547
Iteration 186/1000 | Loss: 0.00003546
Iteration 187/1000 | Loss: 0.00003546
Iteration 188/1000 | Loss: 0.00003546
Iteration 189/1000 | Loss: 0.00003546
Iteration 190/1000 | Loss: 0.00003546
Iteration 191/1000 | Loss: 0.00003545
Iteration 192/1000 | Loss: 0.00003545
Iteration 193/1000 | Loss: 0.00003544
Iteration 194/1000 | Loss: 0.00003544
Iteration 195/1000 | Loss: 0.00003544
Iteration 196/1000 | Loss: 0.00003544
Iteration 197/1000 | Loss: 0.00003543
Iteration 198/1000 | Loss: 0.00003543
Iteration 199/1000 | Loss: 0.00003543
Iteration 200/1000 | Loss: 0.00003542
Iteration 201/1000 | Loss: 0.00003542
Iteration 202/1000 | Loss: 0.00003542
Iteration 203/1000 | Loss: 0.00003541
Iteration 204/1000 | Loss: 0.00003541
Iteration 205/1000 | Loss: 0.00003541
Iteration 206/1000 | Loss: 0.00003541
Iteration 207/1000 | Loss: 0.00003541
Iteration 208/1000 | Loss: 0.00003540
Iteration 209/1000 | Loss: 0.00003540
Iteration 210/1000 | Loss: 0.00003540
Iteration 211/1000 | Loss: 0.00003540
Iteration 212/1000 | Loss: 0.00003540
Iteration 213/1000 | Loss: 0.00003539
Iteration 214/1000 | Loss: 0.00003539
Iteration 215/1000 | Loss: 0.00003539
Iteration 216/1000 | Loss: 0.00003538
Iteration 217/1000 | Loss: 0.00003538
Iteration 218/1000 | Loss: 0.00003538
Iteration 219/1000 | Loss: 0.00003538
Iteration 220/1000 | Loss: 0.00003537
Iteration 221/1000 | Loss: 0.00003537
Iteration 222/1000 | Loss: 0.00003537
Iteration 223/1000 | Loss: 0.00003536
Iteration 224/1000 | Loss: 0.00003536
Iteration 225/1000 | Loss: 0.00003536
Iteration 226/1000 | Loss: 0.00003536
Iteration 227/1000 | Loss: 0.00003536
Iteration 228/1000 | Loss: 0.00003536
Iteration 229/1000 | Loss: 0.00003535
Iteration 230/1000 | Loss: 0.00003535
Iteration 231/1000 | Loss: 0.00003535
Iteration 232/1000 | Loss: 0.00003534
Iteration 233/1000 | Loss: 0.00003534
Iteration 234/1000 | Loss: 0.00003534
Iteration 235/1000 | Loss: 0.00003534
Iteration 236/1000 | Loss: 0.00003534
Iteration 237/1000 | Loss: 0.00003534
Iteration 238/1000 | Loss: 0.00003534
Iteration 239/1000 | Loss: 0.00003534
Iteration 240/1000 | Loss: 0.00003533
Iteration 241/1000 | Loss: 0.00003533
Iteration 242/1000 | Loss: 0.00003533
Iteration 243/1000 | Loss: 0.00003533
Iteration 244/1000 | Loss: 0.00003533
Iteration 245/1000 | Loss: 0.00003533
Iteration 246/1000 | Loss: 0.00003532
Iteration 247/1000 | Loss: 0.00003532
Iteration 248/1000 | Loss: 0.00003532
Iteration 249/1000 | Loss: 0.00003532
Iteration 250/1000 | Loss: 0.00003532
Iteration 251/1000 | Loss: 0.00003532
Iteration 252/1000 | Loss: 0.00003532
Iteration 253/1000 | Loss: 0.00003532
Iteration 254/1000 | Loss: 0.00003532
Iteration 255/1000 | Loss: 0.00003532
Iteration 256/1000 | Loss: 0.00003532
Iteration 257/1000 | Loss: 0.00003532
Iteration 258/1000 | Loss: 0.00003532
Iteration 259/1000 | Loss: 0.00003532
Iteration 260/1000 | Loss: 0.00003531
Iteration 261/1000 | Loss: 0.00003531
Iteration 262/1000 | Loss: 0.00003531
Iteration 263/1000 | Loss: 0.00003531
Iteration 264/1000 | Loss: 0.00003531
Iteration 265/1000 | Loss: 0.00003531
Iteration 266/1000 | Loss: 0.00003531
Iteration 267/1000 | Loss: 0.00003531
Iteration 268/1000 | Loss: 0.00003531
Iteration 269/1000 | Loss: 0.00003531
Iteration 270/1000 | Loss: 0.00003531
Iteration 271/1000 | Loss: 0.00003531
Iteration 272/1000 | Loss: 0.00003531
Iteration 273/1000 | Loss: 0.00003531
Iteration 274/1000 | Loss: 0.00003531
Iteration 275/1000 | Loss: 0.00003531
Iteration 276/1000 | Loss: 0.00003531
Iteration 277/1000 | Loss: 0.00003530
Iteration 278/1000 | Loss: 0.00003530
Iteration 279/1000 | Loss: 0.00003530
Iteration 280/1000 | Loss: 0.00003530
Iteration 281/1000 | Loss: 0.00003530
Iteration 282/1000 | Loss: 0.00003530
Iteration 283/1000 | Loss: 0.00003530
Iteration 284/1000 | Loss: 0.00003530
Iteration 285/1000 | Loss: 0.00003530
Iteration 286/1000 | Loss: 0.00003530
Iteration 287/1000 | Loss: 0.00003530
Iteration 288/1000 | Loss: 0.00003530
Iteration 289/1000 | Loss: 0.00003530
Iteration 290/1000 | Loss: 0.00003530
Iteration 291/1000 | Loss: 0.00003530
Iteration 292/1000 | Loss: 0.00003530
Iteration 293/1000 | Loss: 0.00003530
Iteration 294/1000 | Loss: 0.00003529
Iteration 295/1000 | Loss: 0.00003529
Iteration 296/1000 | Loss: 0.00003529
Iteration 297/1000 | Loss: 0.00003529
Iteration 298/1000 | Loss: 0.00003529
Iteration 299/1000 | Loss: 0.00003529
Iteration 300/1000 | Loss: 0.00003529
Iteration 301/1000 | Loss: 0.00003529
Iteration 302/1000 | Loss: 0.00003528
Iteration 303/1000 | Loss: 0.00003528
Iteration 304/1000 | Loss: 0.00003528
Iteration 305/1000 | Loss: 0.00003528
Iteration 306/1000 | Loss: 0.00003528
Iteration 307/1000 | Loss: 0.00003528
Iteration 308/1000 | Loss: 0.00003528
Iteration 309/1000 | Loss: 0.00003528
Iteration 310/1000 | Loss: 0.00003528
Iteration 311/1000 | Loss: 0.00003528
Iteration 312/1000 | Loss: 0.00003528
Iteration 313/1000 | Loss: 0.00003528
Iteration 314/1000 | Loss: 0.00003528
Iteration 315/1000 | Loss: 0.00003528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [3.528211163938977e-05, 3.528211163938977e-05, 3.528211163938977e-05, 3.528211163938977e-05, 3.528211163938977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.528211163938977e-05

Optimization complete. Final v2v error: 4.723001480102539 mm

Highest mean error: 6.379568099975586 mm for frame 138

Lowest mean error: 2.8803658485412598 mm for frame 38

Saving results

Total time: 137.9046347141266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778660
Iteration 2/25 | Loss: 0.00145022
Iteration 3/25 | Loss: 0.00117458
Iteration 4/25 | Loss: 0.00115200
Iteration 5/25 | Loss: 0.00114774
Iteration 6/25 | Loss: 0.00114774
Iteration 7/25 | Loss: 0.00114774
Iteration 8/25 | Loss: 0.00114774
Iteration 9/25 | Loss: 0.00114774
Iteration 10/25 | Loss: 0.00114774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001147736911661923, 0.001147736911661923, 0.001147736911661923, 0.001147736911661923, 0.001147736911661923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001147736911661923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22894585
Iteration 2/25 | Loss: 0.00065778
Iteration 3/25 | Loss: 0.00065776
Iteration 4/25 | Loss: 0.00065776
Iteration 5/25 | Loss: 0.00065775
Iteration 6/25 | Loss: 0.00065775
Iteration 7/25 | Loss: 0.00065775
Iteration 8/25 | Loss: 0.00065775
Iteration 9/25 | Loss: 0.00065775
Iteration 10/25 | Loss: 0.00065775
Iteration 11/25 | Loss: 0.00065775
Iteration 12/25 | Loss: 0.00065775
Iteration 13/25 | Loss: 0.00065775
Iteration 14/25 | Loss: 0.00065775
Iteration 15/25 | Loss: 0.00065775
Iteration 16/25 | Loss: 0.00065775
Iteration 17/25 | Loss: 0.00065775
Iteration 18/25 | Loss: 0.00065775
Iteration 19/25 | Loss: 0.00065775
Iteration 20/25 | Loss: 0.00065775
Iteration 21/25 | Loss: 0.00065775
Iteration 22/25 | Loss: 0.00065775
Iteration 23/25 | Loss: 0.00065775
Iteration 24/25 | Loss: 0.00065775
Iteration 25/25 | Loss: 0.00065775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065775
Iteration 2/1000 | Loss: 0.00004194
Iteration 3/1000 | Loss: 0.00002947
Iteration 4/1000 | Loss: 0.00002530
Iteration 5/1000 | Loss: 0.00002419
Iteration 6/1000 | Loss: 0.00002306
Iteration 7/1000 | Loss: 0.00002238
Iteration 8/1000 | Loss: 0.00002199
Iteration 9/1000 | Loss: 0.00002163
Iteration 10/1000 | Loss: 0.00002129
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002087
Iteration 13/1000 | Loss: 0.00002069
Iteration 14/1000 | Loss: 0.00002062
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002047
Iteration 17/1000 | Loss: 0.00002032
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002030
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00002013
Iteration 24/1000 | Loss: 0.00002013
Iteration 25/1000 | Loss: 0.00002012
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002011
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002009
Iteration 34/1000 | Loss: 0.00002008
Iteration 35/1000 | Loss: 0.00002008
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002005
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00002002
Iteration 47/1000 | Loss: 0.00002002
Iteration 48/1000 | Loss: 0.00002002
Iteration 49/1000 | Loss: 0.00002001
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002001
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00002000
Iteration 55/1000 | Loss: 0.00002000
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00001999
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001997
Iteration 70/1000 | Loss: 0.00001997
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001996
Iteration 74/1000 | Loss: 0.00001996
Iteration 75/1000 | Loss: 0.00001995
Iteration 76/1000 | Loss: 0.00001995
Iteration 77/1000 | Loss: 0.00001995
Iteration 78/1000 | Loss: 0.00001995
Iteration 79/1000 | Loss: 0.00001994
Iteration 80/1000 | Loss: 0.00001994
Iteration 81/1000 | Loss: 0.00001994
Iteration 82/1000 | Loss: 0.00001994
Iteration 83/1000 | Loss: 0.00001994
Iteration 84/1000 | Loss: 0.00001994
Iteration 85/1000 | Loss: 0.00001993
Iteration 86/1000 | Loss: 0.00001993
Iteration 87/1000 | Loss: 0.00001993
Iteration 88/1000 | Loss: 0.00001993
Iteration 89/1000 | Loss: 0.00001993
Iteration 90/1000 | Loss: 0.00001993
Iteration 91/1000 | Loss: 0.00001993
Iteration 92/1000 | Loss: 0.00001993
Iteration 93/1000 | Loss: 0.00001992
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00001992
Iteration 96/1000 | Loss: 0.00001992
Iteration 97/1000 | Loss: 0.00001992
Iteration 98/1000 | Loss: 0.00001992
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001991
Iteration 101/1000 | Loss: 0.00001991
Iteration 102/1000 | Loss: 0.00001991
Iteration 103/1000 | Loss: 0.00001990
Iteration 104/1000 | Loss: 0.00001990
Iteration 105/1000 | Loss: 0.00001990
Iteration 106/1000 | Loss: 0.00001990
Iteration 107/1000 | Loss: 0.00001990
Iteration 108/1000 | Loss: 0.00001990
Iteration 109/1000 | Loss: 0.00001990
Iteration 110/1000 | Loss: 0.00001989
Iteration 111/1000 | Loss: 0.00001989
Iteration 112/1000 | Loss: 0.00001989
Iteration 113/1000 | Loss: 0.00001989
Iteration 114/1000 | Loss: 0.00001989
Iteration 115/1000 | Loss: 0.00001989
Iteration 116/1000 | Loss: 0.00001988
Iteration 117/1000 | Loss: 0.00001988
Iteration 118/1000 | Loss: 0.00001988
Iteration 119/1000 | Loss: 0.00001988
Iteration 120/1000 | Loss: 0.00001988
Iteration 121/1000 | Loss: 0.00001988
Iteration 122/1000 | Loss: 0.00001988
Iteration 123/1000 | Loss: 0.00001988
Iteration 124/1000 | Loss: 0.00001988
Iteration 125/1000 | Loss: 0.00001988
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001987
Iteration 131/1000 | Loss: 0.00001987
Iteration 132/1000 | Loss: 0.00001987
Iteration 133/1000 | Loss: 0.00001987
Iteration 134/1000 | Loss: 0.00001987
Iteration 135/1000 | Loss: 0.00001987
Iteration 136/1000 | Loss: 0.00001987
Iteration 137/1000 | Loss: 0.00001987
Iteration 138/1000 | Loss: 0.00001987
Iteration 139/1000 | Loss: 0.00001987
Iteration 140/1000 | Loss: 0.00001987
Iteration 141/1000 | Loss: 0.00001987
Iteration 142/1000 | Loss: 0.00001987
Iteration 143/1000 | Loss: 0.00001987
Iteration 144/1000 | Loss: 0.00001986
Iteration 145/1000 | Loss: 0.00001986
Iteration 146/1000 | Loss: 0.00001986
Iteration 147/1000 | Loss: 0.00001986
Iteration 148/1000 | Loss: 0.00001986
Iteration 149/1000 | Loss: 0.00001986
Iteration 150/1000 | Loss: 0.00001986
Iteration 151/1000 | Loss: 0.00001985
Iteration 152/1000 | Loss: 0.00001985
Iteration 153/1000 | Loss: 0.00001985
Iteration 154/1000 | Loss: 0.00001985
Iteration 155/1000 | Loss: 0.00001985
Iteration 156/1000 | Loss: 0.00001985
Iteration 157/1000 | Loss: 0.00001985
Iteration 158/1000 | Loss: 0.00001985
Iteration 159/1000 | Loss: 0.00001985
Iteration 160/1000 | Loss: 0.00001984
Iteration 161/1000 | Loss: 0.00001984
Iteration 162/1000 | Loss: 0.00001984
Iteration 163/1000 | Loss: 0.00001984
Iteration 164/1000 | Loss: 0.00001984
Iteration 165/1000 | Loss: 0.00001984
Iteration 166/1000 | Loss: 0.00001984
Iteration 167/1000 | Loss: 0.00001983
Iteration 168/1000 | Loss: 0.00001983
Iteration 169/1000 | Loss: 0.00001983
Iteration 170/1000 | Loss: 0.00001983
Iteration 171/1000 | Loss: 0.00001983
Iteration 172/1000 | Loss: 0.00001983
Iteration 173/1000 | Loss: 0.00001983
Iteration 174/1000 | Loss: 0.00001983
Iteration 175/1000 | Loss: 0.00001983
Iteration 176/1000 | Loss: 0.00001983
Iteration 177/1000 | Loss: 0.00001983
Iteration 178/1000 | Loss: 0.00001983
Iteration 179/1000 | Loss: 0.00001983
Iteration 180/1000 | Loss: 0.00001983
Iteration 181/1000 | Loss: 0.00001983
Iteration 182/1000 | Loss: 0.00001983
Iteration 183/1000 | Loss: 0.00001983
Iteration 184/1000 | Loss: 0.00001983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.9830842575174756e-05, 1.9830842575174756e-05, 1.9830842575174756e-05, 1.9830842575174756e-05, 1.9830842575174756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9830842575174756e-05

Optimization complete. Final v2v error: 3.6879279613494873 mm

Highest mean error: 4.698774814605713 mm for frame 137

Lowest mean error: 3.1264538764953613 mm for frame 58

Saving results

Total time: 47.934032917022705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022798
Iteration 2/25 | Loss: 0.00268026
Iteration 3/25 | Loss: 0.00171843
Iteration 4/25 | Loss: 0.00155817
Iteration 5/25 | Loss: 0.00143273
Iteration 6/25 | Loss: 0.00134927
Iteration 7/25 | Loss: 0.00128315
Iteration 8/25 | Loss: 0.00127732
Iteration 9/25 | Loss: 0.00124840
Iteration 10/25 | Loss: 0.00124241
Iteration 11/25 | Loss: 0.00121586
Iteration 12/25 | Loss: 0.00119438
Iteration 13/25 | Loss: 0.00118415
Iteration 14/25 | Loss: 0.00119327
Iteration 15/25 | Loss: 0.00123846
Iteration 16/25 | Loss: 0.00123767
Iteration 17/25 | Loss: 0.00122274
Iteration 18/25 | Loss: 0.00118378
Iteration 19/25 | Loss: 0.00116791
Iteration 20/25 | Loss: 0.00113938
Iteration 21/25 | Loss: 0.00112656
Iteration 22/25 | Loss: 0.00112139
Iteration 23/25 | Loss: 0.00111480
Iteration 24/25 | Loss: 0.00111407
Iteration 25/25 | Loss: 0.00111191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40205097
Iteration 2/25 | Loss: 0.00179180
Iteration 3/25 | Loss: 0.00170325
Iteration 4/25 | Loss: 0.00170325
Iteration 5/25 | Loss: 0.00169276
Iteration 6/25 | Loss: 0.00169276
Iteration 7/25 | Loss: 0.00169276
Iteration 8/25 | Loss: 0.00169276
Iteration 9/25 | Loss: 0.00169276
Iteration 10/25 | Loss: 0.00169276
Iteration 11/25 | Loss: 0.00169276
Iteration 12/25 | Loss: 0.00169276
Iteration 13/25 | Loss: 0.00169276
Iteration 14/25 | Loss: 0.00169276
Iteration 15/25 | Loss: 0.00169276
Iteration 16/25 | Loss: 0.00169276
Iteration 17/25 | Loss: 0.00169276
Iteration 18/25 | Loss: 0.00169276
Iteration 19/25 | Loss: 0.00169276
Iteration 20/25 | Loss: 0.00169276
Iteration 21/25 | Loss: 0.00169276
Iteration 22/25 | Loss: 0.00169276
Iteration 23/25 | Loss: 0.00169276
Iteration 24/25 | Loss: 0.00169276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016927573597058654, 0.0016927573597058654, 0.0016927573597058654, 0.0016927573597058654, 0.0016927573597058654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016927573597058654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169276
Iteration 2/1000 | Loss: 0.00038931
Iteration 3/1000 | Loss: 0.00034032
Iteration 4/1000 | Loss: 0.00039210
Iteration 5/1000 | Loss: 0.00036180
Iteration 6/1000 | Loss: 0.00047744
Iteration 7/1000 | Loss: 0.00036555
Iteration 8/1000 | Loss: 0.00023823
Iteration 9/1000 | Loss: 0.00018256
Iteration 10/1000 | Loss: 0.00023584
Iteration 11/1000 | Loss: 0.00031914
Iteration 12/1000 | Loss: 0.00060259
Iteration 13/1000 | Loss: 0.00141273
Iteration 14/1000 | Loss: 0.00039540
Iteration 15/1000 | Loss: 0.00086190
Iteration 16/1000 | Loss: 0.00019032
Iteration 17/1000 | Loss: 0.00028629
Iteration 18/1000 | Loss: 0.00093763
Iteration 19/1000 | Loss: 0.00022519
Iteration 20/1000 | Loss: 0.00024143
Iteration 21/1000 | Loss: 0.00012699
Iteration 22/1000 | Loss: 0.00016369
Iteration 23/1000 | Loss: 0.00017284
Iteration 24/1000 | Loss: 0.00020615
Iteration 25/1000 | Loss: 0.00034084
Iteration 26/1000 | Loss: 0.00029698
Iteration 27/1000 | Loss: 0.00022969
Iteration 28/1000 | Loss: 0.00007895
Iteration 29/1000 | Loss: 0.00028712
Iteration 30/1000 | Loss: 0.00023495
Iteration 31/1000 | Loss: 0.00023909
Iteration 32/1000 | Loss: 0.00025516
Iteration 33/1000 | Loss: 0.00025429
Iteration 34/1000 | Loss: 0.00026843
Iteration 35/1000 | Loss: 0.00034280
Iteration 36/1000 | Loss: 0.00028835
Iteration 37/1000 | Loss: 0.00024204
Iteration 38/1000 | Loss: 0.00016984
Iteration 39/1000 | Loss: 0.00012621
Iteration 40/1000 | Loss: 0.00023629
Iteration 41/1000 | Loss: 0.00024586
Iteration 42/1000 | Loss: 0.00014627
Iteration 43/1000 | Loss: 0.00014076
Iteration 44/1000 | Loss: 0.00014979
Iteration 45/1000 | Loss: 0.00011218
Iteration 46/1000 | Loss: 0.00015268
Iteration 47/1000 | Loss: 0.00019769
Iteration 48/1000 | Loss: 0.00024906
Iteration 49/1000 | Loss: 0.00027037
Iteration 50/1000 | Loss: 0.00027640
Iteration 51/1000 | Loss: 0.00016086
Iteration 52/1000 | Loss: 0.00013992
Iteration 53/1000 | Loss: 0.00016198
Iteration 54/1000 | Loss: 0.00011230
Iteration 55/1000 | Loss: 0.00012648
Iteration 56/1000 | Loss: 0.00016696
Iteration 57/1000 | Loss: 0.00023601
Iteration 58/1000 | Loss: 0.00017707
Iteration 59/1000 | Loss: 0.00022950
Iteration 60/1000 | Loss: 0.00017457
Iteration 61/1000 | Loss: 0.00025660
Iteration 62/1000 | Loss: 0.00017454
Iteration 63/1000 | Loss: 0.00032555
Iteration 64/1000 | Loss: 0.00026032
Iteration 65/1000 | Loss: 0.00028664
Iteration 66/1000 | Loss: 0.00022968
Iteration 67/1000 | Loss: 0.00027068
Iteration 68/1000 | Loss: 0.00023907
Iteration 69/1000 | Loss: 0.00019165
Iteration 70/1000 | Loss: 0.00022110
Iteration 71/1000 | Loss: 0.00010817
Iteration 72/1000 | Loss: 0.00004157
Iteration 73/1000 | Loss: 0.00007271
Iteration 74/1000 | Loss: 0.00006416
Iteration 75/1000 | Loss: 0.00005449
Iteration 76/1000 | Loss: 0.00008193
Iteration 77/1000 | Loss: 0.00012544
Iteration 78/1000 | Loss: 0.00009763
Iteration 79/1000 | Loss: 0.00012544
Iteration 80/1000 | Loss: 0.00009763
Iteration 81/1000 | Loss: 0.00004861
Iteration 82/1000 | Loss: 0.00010642
Iteration 83/1000 | Loss: 0.00004043
Iteration 84/1000 | Loss: 0.00004491
Iteration 85/1000 | Loss: 0.00002827
Iteration 86/1000 | Loss: 0.00011828
Iteration 87/1000 | Loss: 0.00010031
Iteration 88/1000 | Loss: 0.00010618
Iteration 89/1000 | Loss: 0.00010985
Iteration 90/1000 | Loss: 0.00010476
Iteration 91/1000 | Loss: 0.00028641
Iteration 92/1000 | Loss: 0.00013546
Iteration 93/1000 | Loss: 0.00009747
Iteration 94/1000 | Loss: 0.00010918
Iteration 95/1000 | Loss: 0.00011424
Iteration 96/1000 | Loss: 0.00010233
Iteration 97/1000 | Loss: 0.00011069
Iteration 98/1000 | Loss: 0.00010273
Iteration 99/1000 | Loss: 0.00013306
Iteration 100/1000 | Loss: 0.00020625
Iteration 101/1000 | Loss: 0.00010008
Iteration 102/1000 | Loss: 0.00012755
Iteration 103/1000 | Loss: 0.00008019
Iteration 104/1000 | Loss: 0.00023204
Iteration 105/1000 | Loss: 0.00013910
Iteration 106/1000 | Loss: 0.00015296
Iteration 107/1000 | Loss: 0.00014440
Iteration 108/1000 | Loss: 0.00019269
Iteration 109/1000 | Loss: 0.00022045
Iteration 110/1000 | Loss: 0.00021744
Iteration 111/1000 | Loss: 0.00013377
Iteration 112/1000 | Loss: 0.00009021
Iteration 113/1000 | Loss: 0.00006578
Iteration 114/1000 | Loss: 0.00022879
Iteration 115/1000 | Loss: 0.00037764
Iteration 116/1000 | Loss: 0.00022700
Iteration 117/1000 | Loss: 0.00044495
Iteration 118/1000 | Loss: 0.00047309
Iteration 119/1000 | Loss: 0.00016220
Iteration 120/1000 | Loss: 0.00018590
Iteration 121/1000 | Loss: 0.00011479
Iteration 122/1000 | Loss: 0.00004166
Iteration 123/1000 | Loss: 0.00033037
Iteration 124/1000 | Loss: 0.00056742
Iteration 125/1000 | Loss: 0.00021596
Iteration 126/1000 | Loss: 0.00128378
Iteration 127/1000 | Loss: 0.00063798
Iteration 128/1000 | Loss: 0.00102807
Iteration 129/1000 | Loss: 0.00015433
Iteration 130/1000 | Loss: 0.00017918
Iteration 131/1000 | Loss: 0.00014261
Iteration 132/1000 | Loss: 0.00005277
Iteration 133/1000 | Loss: 0.00006192
Iteration 134/1000 | Loss: 0.00031590
Iteration 135/1000 | Loss: 0.00019071
Iteration 136/1000 | Loss: 0.00012719
Iteration 137/1000 | Loss: 0.00022933
Iteration 138/1000 | Loss: 0.00008295
Iteration 139/1000 | Loss: 0.00021152
Iteration 140/1000 | Loss: 0.00023338
Iteration 141/1000 | Loss: 0.00024810
Iteration 142/1000 | Loss: 0.00013646
Iteration 143/1000 | Loss: 0.00015092
Iteration 144/1000 | Loss: 0.00013379
Iteration 145/1000 | Loss: 0.00008667
Iteration 146/1000 | Loss: 0.00009892
Iteration 147/1000 | Loss: 0.00008991
Iteration 148/1000 | Loss: 0.00006198
Iteration 149/1000 | Loss: 0.00009523
Iteration 150/1000 | Loss: 0.00019236
Iteration 151/1000 | Loss: 0.00028460
Iteration 152/1000 | Loss: 0.00018646
Iteration 153/1000 | Loss: 0.00009130
Iteration 154/1000 | Loss: 0.00007402
Iteration 155/1000 | Loss: 0.00014009
Iteration 156/1000 | Loss: 0.00021031
Iteration 157/1000 | Loss: 0.00048932
Iteration 158/1000 | Loss: 0.00016783
Iteration 159/1000 | Loss: 0.00002105
Iteration 160/1000 | Loss: 0.00014458
Iteration 161/1000 | Loss: 0.00015750
Iteration 162/1000 | Loss: 0.00005724
Iteration 163/1000 | Loss: 0.00019718
Iteration 164/1000 | Loss: 0.00003722
Iteration 165/1000 | Loss: 0.00003888
Iteration 166/1000 | Loss: 0.00001802
Iteration 167/1000 | Loss: 0.00004086
Iteration 168/1000 | Loss: 0.00003036
Iteration 169/1000 | Loss: 0.00002805
Iteration 170/1000 | Loss: 0.00006720
Iteration 171/1000 | Loss: 0.00002695
Iteration 172/1000 | Loss: 0.00003225
Iteration 173/1000 | Loss: 0.00003882
Iteration 174/1000 | Loss: 0.00002879
Iteration 175/1000 | Loss: 0.00002629
Iteration 176/1000 | Loss: 0.00015872
Iteration 177/1000 | Loss: 0.00014363
Iteration 178/1000 | Loss: 0.00003791
Iteration 179/1000 | Loss: 0.00003753
Iteration 180/1000 | Loss: 0.00016584
Iteration 181/1000 | Loss: 0.00013627
Iteration 182/1000 | Loss: 0.00005585
Iteration 183/1000 | Loss: 0.00017091
Iteration 184/1000 | Loss: 0.00031452
Iteration 185/1000 | Loss: 0.00011334
Iteration 186/1000 | Loss: 0.00008798
Iteration 187/1000 | Loss: 0.00011331
Iteration 188/1000 | Loss: 0.00020076
Iteration 189/1000 | Loss: 0.00019469
Iteration 190/1000 | Loss: 0.00010432
Iteration 191/1000 | Loss: 0.00019165
Iteration 192/1000 | Loss: 0.00019539
Iteration 193/1000 | Loss: 0.00007089
Iteration 194/1000 | Loss: 0.00014231
Iteration 195/1000 | Loss: 0.00010358
Iteration 196/1000 | Loss: 0.00002499
Iteration 197/1000 | Loss: 0.00004297
Iteration 198/1000 | Loss: 0.00002815
Iteration 199/1000 | Loss: 0.00002984
Iteration 200/1000 | Loss: 0.00011456
Iteration 201/1000 | Loss: 0.00039604
Iteration 202/1000 | Loss: 0.00002967
Iteration 203/1000 | Loss: 0.00011890
Iteration 204/1000 | Loss: 0.00037173
Iteration 205/1000 | Loss: 0.00020568
Iteration 206/1000 | Loss: 0.00060408
Iteration 207/1000 | Loss: 0.00022619
Iteration 208/1000 | Loss: 0.00026385
Iteration 209/1000 | Loss: 0.00015319
Iteration 210/1000 | Loss: 0.00015554
Iteration 211/1000 | Loss: 0.00017351
Iteration 212/1000 | Loss: 0.00069307
Iteration 213/1000 | Loss: 0.00016718
Iteration 214/1000 | Loss: 0.00018165
Iteration 215/1000 | Loss: 0.00014709
Iteration 216/1000 | Loss: 0.00013186
Iteration 217/1000 | Loss: 0.00016262
Iteration 218/1000 | Loss: 0.00020804
Iteration 219/1000 | Loss: 0.00020553
Iteration 220/1000 | Loss: 0.00020427
Iteration 221/1000 | Loss: 0.00020423
Iteration 222/1000 | Loss: 0.00022600
Iteration 223/1000 | Loss: 0.00020954
Iteration 224/1000 | Loss: 0.00019882
Iteration 225/1000 | Loss: 0.00017532
Iteration 226/1000 | Loss: 0.00003212
Iteration 227/1000 | Loss: 0.00010800
Iteration 228/1000 | Loss: 0.00007266
Iteration 229/1000 | Loss: 0.00003180
Iteration 230/1000 | Loss: 0.00003213
Iteration 231/1000 | Loss: 0.00003184
Iteration 232/1000 | Loss: 0.00003177
Iteration 233/1000 | Loss: 0.00003294
Iteration 234/1000 | Loss: 0.00002989
Iteration 235/1000 | Loss: 0.00002915
Iteration 236/1000 | Loss: 0.00003716
Iteration 237/1000 | Loss: 0.00025558
Iteration 238/1000 | Loss: 0.00021475
Iteration 239/1000 | Loss: 0.00010215
Iteration 240/1000 | Loss: 0.00004161
Iteration 241/1000 | Loss: 0.00003238
Iteration 242/1000 | Loss: 0.00003530
Iteration 243/1000 | Loss: 0.00003293
Iteration 244/1000 | Loss: 0.00002808
Iteration 245/1000 | Loss: 0.00005647
Iteration 246/1000 | Loss: 0.00004837
Iteration 247/1000 | Loss: 0.00010895
Iteration 248/1000 | Loss: 0.00002140
Iteration 249/1000 | Loss: 0.00001994
Iteration 250/1000 | Loss: 0.00001594
Iteration 251/1000 | Loss: 0.00001501
Iteration 252/1000 | Loss: 0.00001464
Iteration 253/1000 | Loss: 0.00002147
Iteration 254/1000 | Loss: 0.00006684
Iteration 255/1000 | Loss: 0.00001914
Iteration 256/1000 | Loss: 0.00001426
Iteration 257/1000 | Loss: 0.00004548
Iteration 258/1000 | Loss: 0.00004314
Iteration 259/1000 | Loss: 0.00001848
Iteration 260/1000 | Loss: 0.00001382
Iteration 261/1000 | Loss: 0.00001381
Iteration 262/1000 | Loss: 0.00001381
Iteration 263/1000 | Loss: 0.00001381
Iteration 264/1000 | Loss: 0.00001377
Iteration 265/1000 | Loss: 0.00001377
Iteration 266/1000 | Loss: 0.00001376
Iteration 267/1000 | Loss: 0.00001376
Iteration 268/1000 | Loss: 0.00008923
Iteration 269/1000 | Loss: 0.00001359
Iteration 270/1000 | Loss: 0.00001350
Iteration 271/1000 | Loss: 0.00003711
Iteration 272/1000 | Loss: 0.00001655
Iteration 273/1000 | Loss: 0.00001341
Iteration 274/1000 | Loss: 0.00001337
Iteration 275/1000 | Loss: 0.00003565
Iteration 276/1000 | Loss: 0.00001331
Iteration 277/1000 | Loss: 0.00001324
Iteration 278/1000 | Loss: 0.00001324
Iteration 279/1000 | Loss: 0.00001322
Iteration 280/1000 | Loss: 0.00001322
Iteration 281/1000 | Loss: 0.00001322
Iteration 282/1000 | Loss: 0.00001321
Iteration 283/1000 | Loss: 0.00001321
Iteration 284/1000 | Loss: 0.00001321
Iteration 285/1000 | Loss: 0.00001320
Iteration 286/1000 | Loss: 0.00001319
Iteration 287/1000 | Loss: 0.00001318
Iteration 288/1000 | Loss: 0.00001318
Iteration 289/1000 | Loss: 0.00001317
Iteration 290/1000 | Loss: 0.00001317
Iteration 291/1000 | Loss: 0.00001316
Iteration 292/1000 | Loss: 0.00001316
Iteration 293/1000 | Loss: 0.00001316
Iteration 294/1000 | Loss: 0.00001316
Iteration 295/1000 | Loss: 0.00001315
Iteration 296/1000 | Loss: 0.00001315
Iteration 297/1000 | Loss: 0.00001315
Iteration 298/1000 | Loss: 0.00001315
Iteration 299/1000 | Loss: 0.00001315
Iteration 300/1000 | Loss: 0.00003214
Iteration 301/1000 | Loss: 0.00001366
Iteration 302/1000 | Loss: 0.00001366
Iteration 303/1000 | Loss: 0.00001311
Iteration 304/1000 | Loss: 0.00001311
Iteration 305/1000 | Loss: 0.00001311
Iteration 306/1000 | Loss: 0.00001311
Iteration 307/1000 | Loss: 0.00001310
Iteration 308/1000 | Loss: 0.00001310
Iteration 309/1000 | Loss: 0.00001310
Iteration 310/1000 | Loss: 0.00001310
Iteration 311/1000 | Loss: 0.00001310
Iteration 312/1000 | Loss: 0.00001309
Iteration 313/1000 | Loss: 0.00001309
Iteration 314/1000 | Loss: 0.00001309
Iteration 315/1000 | Loss: 0.00001309
Iteration 316/1000 | Loss: 0.00001309
Iteration 317/1000 | Loss: 0.00001969
Iteration 318/1000 | Loss: 0.00001349
Iteration 319/1000 | Loss: 0.00001308
Iteration 320/1000 | Loss: 0.00001308
Iteration 321/1000 | Loss: 0.00001308
Iteration 322/1000 | Loss: 0.00001308
Iteration 323/1000 | Loss: 0.00001307
Iteration 324/1000 | Loss: 0.00001307
Iteration 325/1000 | Loss: 0.00001307
Iteration 326/1000 | Loss: 0.00001307
Iteration 327/1000 | Loss: 0.00001307
Iteration 328/1000 | Loss: 0.00001307
Iteration 329/1000 | Loss: 0.00001307
Iteration 330/1000 | Loss: 0.00001307
Iteration 331/1000 | Loss: 0.00001307
Iteration 332/1000 | Loss: 0.00001307
Iteration 333/1000 | Loss: 0.00001307
Iteration 334/1000 | Loss: 0.00001307
Iteration 335/1000 | Loss: 0.00001307
Iteration 336/1000 | Loss: 0.00001307
Iteration 337/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [1.3073972695565317e-05, 1.3073972695565317e-05, 1.3073972695565317e-05, 1.3073972695565317e-05, 1.3073972695565317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3073972695565317e-05

Optimization complete. Final v2v error: 3.018791675567627 mm

Highest mean error: 4.790414810180664 mm for frame 67

Lowest mean error: 2.320920467376709 mm for frame 164

Saving results

Total time: 481.14193201065063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919308
Iteration 2/25 | Loss: 0.00137691
Iteration 3/25 | Loss: 0.00118124
Iteration 4/25 | Loss: 0.00115023
Iteration 5/25 | Loss: 0.00114136
Iteration 6/25 | Loss: 0.00113939
Iteration 7/25 | Loss: 0.00113935
Iteration 8/25 | Loss: 0.00113935
Iteration 9/25 | Loss: 0.00113935
Iteration 10/25 | Loss: 0.00113935
Iteration 11/25 | Loss: 0.00113935
Iteration 12/25 | Loss: 0.00113935
Iteration 13/25 | Loss: 0.00113935
Iteration 14/25 | Loss: 0.00113935
Iteration 15/25 | Loss: 0.00113935
Iteration 16/25 | Loss: 0.00113935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011393465101718903, 0.0011393465101718903, 0.0011393465101718903, 0.0011393465101718903, 0.0011393465101718903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011393465101718903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46879530
Iteration 2/25 | Loss: 0.00102351
Iteration 3/25 | Loss: 0.00102351
Iteration 4/25 | Loss: 0.00102351
Iteration 5/25 | Loss: 0.00102351
Iteration 6/25 | Loss: 0.00102351
Iteration 7/25 | Loss: 0.00102351
Iteration 8/25 | Loss: 0.00102351
Iteration 9/25 | Loss: 0.00102351
Iteration 10/25 | Loss: 0.00102351
Iteration 11/25 | Loss: 0.00102351
Iteration 12/25 | Loss: 0.00102351
Iteration 13/25 | Loss: 0.00102351
Iteration 14/25 | Loss: 0.00102351
Iteration 15/25 | Loss: 0.00102351
Iteration 16/25 | Loss: 0.00102351
Iteration 17/25 | Loss: 0.00102351
Iteration 18/25 | Loss: 0.00102351
Iteration 19/25 | Loss: 0.00102351
Iteration 20/25 | Loss: 0.00102351
Iteration 21/25 | Loss: 0.00102351
Iteration 22/25 | Loss: 0.00102351
Iteration 23/25 | Loss: 0.00102351
Iteration 24/25 | Loss: 0.00102351
Iteration 25/25 | Loss: 0.00102351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102351
Iteration 2/1000 | Loss: 0.00004674
Iteration 3/1000 | Loss: 0.00002955
Iteration 4/1000 | Loss: 0.00002280
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001867
Iteration 8/1000 | Loss: 0.00001826
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001743
Iteration 16/1000 | Loss: 0.00001739
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001737
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001721
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001715
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001698
Iteration 54/1000 | Loss: 0.00001698
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001687
Iteration 69/1000 | Loss: 0.00001687
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001687
Iteration 72/1000 | Loss: 0.00001687
Iteration 73/1000 | Loss: 0.00001687
Iteration 74/1000 | Loss: 0.00001687
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001686
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001676
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001675
Iteration 108/1000 | Loss: 0.00001674
Iteration 109/1000 | Loss: 0.00001674
Iteration 110/1000 | Loss: 0.00001674
Iteration 111/1000 | Loss: 0.00001673
Iteration 112/1000 | Loss: 0.00001673
Iteration 113/1000 | Loss: 0.00001673
Iteration 114/1000 | Loss: 0.00001673
Iteration 115/1000 | Loss: 0.00001673
Iteration 116/1000 | Loss: 0.00001673
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001673
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001671
Iteration 126/1000 | Loss: 0.00001671
Iteration 127/1000 | Loss: 0.00001671
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001669
Iteration 136/1000 | Loss: 0.00001669
Iteration 137/1000 | Loss: 0.00001669
Iteration 138/1000 | Loss: 0.00001669
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001669
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001669
Iteration 144/1000 | Loss: 0.00001669
Iteration 145/1000 | Loss: 0.00001669
Iteration 146/1000 | Loss: 0.00001669
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001668
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001667
Iteration 154/1000 | Loss: 0.00001667
Iteration 155/1000 | Loss: 0.00001667
Iteration 156/1000 | Loss: 0.00001667
Iteration 157/1000 | Loss: 0.00001666
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001666
Iteration 161/1000 | Loss: 0.00001666
Iteration 162/1000 | Loss: 0.00001666
Iteration 163/1000 | Loss: 0.00001666
Iteration 164/1000 | Loss: 0.00001666
Iteration 165/1000 | Loss: 0.00001666
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001666
Iteration 168/1000 | Loss: 0.00001666
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001665
Iteration 171/1000 | Loss: 0.00001665
Iteration 172/1000 | Loss: 0.00001665
Iteration 173/1000 | Loss: 0.00001665
Iteration 174/1000 | Loss: 0.00001665
Iteration 175/1000 | Loss: 0.00001665
Iteration 176/1000 | Loss: 0.00001664
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001663
Iteration 180/1000 | Loss: 0.00001663
Iteration 181/1000 | Loss: 0.00001663
Iteration 182/1000 | Loss: 0.00001663
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001662
Iteration 187/1000 | Loss: 0.00001662
Iteration 188/1000 | Loss: 0.00001662
Iteration 189/1000 | Loss: 0.00001662
Iteration 190/1000 | Loss: 0.00001662
Iteration 191/1000 | Loss: 0.00001662
Iteration 192/1000 | Loss: 0.00001662
Iteration 193/1000 | Loss: 0.00001662
Iteration 194/1000 | Loss: 0.00001662
Iteration 195/1000 | Loss: 0.00001662
Iteration 196/1000 | Loss: 0.00001662
Iteration 197/1000 | Loss: 0.00001662
Iteration 198/1000 | Loss: 0.00001662
Iteration 199/1000 | Loss: 0.00001661
Iteration 200/1000 | Loss: 0.00001661
Iteration 201/1000 | Loss: 0.00001661
Iteration 202/1000 | Loss: 0.00001661
Iteration 203/1000 | Loss: 0.00001660
Iteration 204/1000 | Loss: 0.00001660
Iteration 205/1000 | Loss: 0.00001660
Iteration 206/1000 | Loss: 0.00001660
Iteration 207/1000 | Loss: 0.00001660
Iteration 208/1000 | Loss: 0.00001660
Iteration 209/1000 | Loss: 0.00001660
Iteration 210/1000 | Loss: 0.00001660
Iteration 211/1000 | Loss: 0.00001660
Iteration 212/1000 | Loss: 0.00001660
Iteration 213/1000 | Loss: 0.00001660
Iteration 214/1000 | Loss: 0.00001659
Iteration 215/1000 | Loss: 0.00001659
Iteration 216/1000 | Loss: 0.00001659
Iteration 217/1000 | Loss: 0.00001659
Iteration 218/1000 | Loss: 0.00001659
Iteration 219/1000 | Loss: 0.00001659
Iteration 220/1000 | Loss: 0.00001659
Iteration 221/1000 | Loss: 0.00001659
Iteration 222/1000 | Loss: 0.00001659
Iteration 223/1000 | Loss: 0.00001659
Iteration 224/1000 | Loss: 0.00001659
Iteration 225/1000 | Loss: 0.00001659
Iteration 226/1000 | Loss: 0.00001658
Iteration 227/1000 | Loss: 0.00001658
Iteration 228/1000 | Loss: 0.00001657
Iteration 229/1000 | Loss: 0.00001657
Iteration 230/1000 | Loss: 0.00001656
Iteration 231/1000 | Loss: 0.00001656
Iteration 232/1000 | Loss: 0.00001656
Iteration 233/1000 | Loss: 0.00001656
Iteration 234/1000 | Loss: 0.00001656
Iteration 235/1000 | Loss: 0.00001656
Iteration 236/1000 | Loss: 0.00001656
Iteration 237/1000 | Loss: 0.00001656
Iteration 238/1000 | Loss: 0.00001656
Iteration 239/1000 | Loss: 0.00001656
Iteration 240/1000 | Loss: 0.00001655
Iteration 241/1000 | Loss: 0.00001655
Iteration 242/1000 | Loss: 0.00001655
Iteration 243/1000 | Loss: 0.00001655
Iteration 244/1000 | Loss: 0.00001654
Iteration 245/1000 | Loss: 0.00001654
Iteration 246/1000 | Loss: 0.00001654
Iteration 247/1000 | Loss: 0.00001654
Iteration 248/1000 | Loss: 0.00001654
Iteration 249/1000 | Loss: 0.00001654
Iteration 250/1000 | Loss: 0.00001654
Iteration 251/1000 | Loss: 0.00001654
Iteration 252/1000 | Loss: 0.00001654
Iteration 253/1000 | Loss: 0.00001653
Iteration 254/1000 | Loss: 0.00001653
Iteration 255/1000 | Loss: 0.00001653
Iteration 256/1000 | Loss: 0.00001653
Iteration 257/1000 | Loss: 0.00001653
Iteration 258/1000 | Loss: 0.00001653
Iteration 259/1000 | Loss: 0.00001653
Iteration 260/1000 | Loss: 0.00001652
Iteration 261/1000 | Loss: 0.00001652
Iteration 262/1000 | Loss: 0.00001652
Iteration 263/1000 | Loss: 0.00001652
Iteration 264/1000 | Loss: 0.00001652
Iteration 265/1000 | Loss: 0.00001652
Iteration 266/1000 | Loss: 0.00001652
Iteration 267/1000 | Loss: 0.00001652
Iteration 268/1000 | Loss: 0.00001652
Iteration 269/1000 | Loss: 0.00001652
Iteration 270/1000 | Loss: 0.00001652
Iteration 271/1000 | Loss: 0.00001652
Iteration 272/1000 | Loss: 0.00001652
Iteration 273/1000 | Loss: 0.00001651
Iteration 274/1000 | Loss: 0.00001651
Iteration 275/1000 | Loss: 0.00001651
Iteration 276/1000 | Loss: 0.00001651
Iteration 277/1000 | Loss: 0.00001651
Iteration 278/1000 | Loss: 0.00001651
Iteration 279/1000 | Loss: 0.00001651
Iteration 280/1000 | Loss: 0.00001651
Iteration 281/1000 | Loss: 0.00001651
Iteration 282/1000 | Loss: 0.00001650
Iteration 283/1000 | Loss: 0.00001650
Iteration 284/1000 | Loss: 0.00001650
Iteration 285/1000 | Loss: 0.00001650
Iteration 286/1000 | Loss: 0.00001650
Iteration 287/1000 | Loss: 0.00001650
Iteration 288/1000 | Loss: 0.00001650
Iteration 289/1000 | Loss: 0.00001650
Iteration 290/1000 | Loss: 0.00001650
Iteration 291/1000 | Loss: 0.00001650
Iteration 292/1000 | Loss: 0.00001650
Iteration 293/1000 | Loss: 0.00001650
Iteration 294/1000 | Loss: 0.00001650
Iteration 295/1000 | Loss: 0.00001650
Iteration 296/1000 | Loss: 0.00001650
Iteration 297/1000 | Loss: 0.00001650
Iteration 298/1000 | Loss: 0.00001650
Iteration 299/1000 | Loss: 0.00001650
Iteration 300/1000 | Loss: 0.00001650
Iteration 301/1000 | Loss: 0.00001649
Iteration 302/1000 | Loss: 0.00001649
Iteration 303/1000 | Loss: 0.00001649
Iteration 304/1000 | Loss: 0.00001649
Iteration 305/1000 | Loss: 0.00001649
Iteration 306/1000 | Loss: 0.00001649
Iteration 307/1000 | Loss: 0.00001648
Iteration 308/1000 | Loss: 0.00001648
Iteration 309/1000 | Loss: 0.00001648
Iteration 310/1000 | Loss: 0.00001648
Iteration 311/1000 | Loss: 0.00001648
Iteration 312/1000 | Loss: 0.00001648
Iteration 313/1000 | Loss: 0.00001648
Iteration 314/1000 | Loss: 0.00001648
Iteration 315/1000 | Loss: 0.00001648
Iteration 316/1000 | Loss: 0.00001648
Iteration 317/1000 | Loss: 0.00001648
Iteration 318/1000 | Loss: 0.00001648
Iteration 319/1000 | Loss: 0.00001648
Iteration 320/1000 | Loss: 0.00001648
Iteration 321/1000 | Loss: 0.00001648
Iteration 322/1000 | Loss: 0.00001648
Iteration 323/1000 | Loss: 0.00001648
Iteration 324/1000 | Loss: 0.00001648
Iteration 325/1000 | Loss: 0.00001648
Iteration 326/1000 | Loss: 0.00001648
Iteration 327/1000 | Loss: 0.00001648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [1.6480160411447287e-05, 1.6480160411447287e-05, 1.6480160411447287e-05, 1.6480160411447287e-05, 1.6480160411447287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6480160411447287e-05

Optimization complete. Final v2v error: 3.4092774391174316 mm

Highest mean error: 3.6899309158325195 mm for frame 153

Lowest mean error: 3.068880796432495 mm for frame 1

Saving results

Total time: 52.15922570228577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00352487
Iteration 2/25 | Loss: 0.00125494
Iteration 3/25 | Loss: 0.00111424
Iteration 4/25 | Loss: 0.00109447
Iteration 5/25 | Loss: 0.00109019
Iteration 6/25 | Loss: 0.00108910
Iteration 7/25 | Loss: 0.00108910
Iteration 8/25 | Loss: 0.00108910
Iteration 9/25 | Loss: 0.00108910
Iteration 10/25 | Loss: 0.00108910
Iteration 11/25 | Loss: 0.00108910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010891023557633162, 0.0010891023557633162, 0.0010891023557633162, 0.0010891023557633162, 0.0010891023557633162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010891023557633162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37883818
Iteration 2/25 | Loss: 0.00095128
Iteration 3/25 | Loss: 0.00095128
Iteration 4/25 | Loss: 0.00095127
Iteration 5/25 | Loss: 0.00095127
Iteration 6/25 | Loss: 0.00095127
Iteration 7/25 | Loss: 0.00095127
Iteration 8/25 | Loss: 0.00095127
Iteration 9/25 | Loss: 0.00095127
Iteration 10/25 | Loss: 0.00095127
Iteration 11/25 | Loss: 0.00095127
Iteration 12/25 | Loss: 0.00095127
Iteration 13/25 | Loss: 0.00095127
Iteration 14/25 | Loss: 0.00095127
Iteration 15/25 | Loss: 0.00095127
Iteration 16/25 | Loss: 0.00095127
Iteration 17/25 | Loss: 0.00095127
Iteration 18/25 | Loss: 0.00095127
Iteration 19/25 | Loss: 0.00095127
Iteration 20/25 | Loss: 0.00095127
Iteration 21/25 | Loss: 0.00095127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009512719698250294, 0.0009512719698250294, 0.0009512719698250294, 0.0009512719698250294, 0.0009512719698250294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009512719698250294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095127
Iteration 2/1000 | Loss: 0.00003277
Iteration 3/1000 | Loss: 0.00001691
Iteration 4/1000 | Loss: 0.00001420
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001232
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001184
Iteration 10/1000 | Loss: 0.00001160
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001132
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001114
Iteration 53/1000 | Loss: 0.00001114
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001113
Iteration 56/1000 | Loss: 0.00001113
Iteration 57/1000 | Loss: 0.00001113
Iteration 58/1000 | Loss: 0.00001113
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001111
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001111
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001111
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001106
Iteration 117/1000 | Loss: 0.00001106
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001104
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001104
Iteration 137/1000 | Loss: 0.00001104
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001103
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001103
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001100
Iteration 164/1000 | Loss: 0.00001100
Iteration 165/1000 | Loss: 0.00001100
Iteration 166/1000 | Loss: 0.00001100
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001100
Iteration 190/1000 | Loss: 0.00001100
Iteration 191/1000 | Loss: 0.00001100
Iteration 192/1000 | Loss: 0.00001100
Iteration 193/1000 | Loss: 0.00001100
Iteration 194/1000 | Loss: 0.00001100
Iteration 195/1000 | Loss: 0.00001100
Iteration 196/1000 | Loss: 0.00001100
Iteration 197/1000 | Loss: 0.00001100
Iteration 198/1000 | Loss: 0.00001100
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.1001300663338043e-05, 1.1001300663338043e-05, 1.1001300663338043e-05, 1.1001300663338043e-05, 1.1001300663338043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1001300663338043e-05

Optimization complete. Final v2v error: 2.831024408340454 mm

Highest mean error: 3.163597345352173 mm for frame 14

Lowest mean error: 2.640413999557495 mm for frame 114

Saving results

Total time: 37.939189434051514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755526
Iteration 2/25 | Loss: 0.00155685
Iteration 3/25 | Loss: 0.00132313
Iteration 4/25 | Loss: 0.00127686
Iteration 5/25 | Loss: 0.00126949
Iteration 6/25 | Loss: 0.00126721
Iteration 7/25 | Loss: 0.00126721
Iteration 8/25 | Loss: 0.00126721
Iteration 9/25 | Loss: 0.00126721
Iteration 10/25 | Loss: 0.00126721
Iteration 11/25 | Loss: 0.00126721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012672129087150097, 0.0012672129087150097, 0.0012672129087150097, 0.0012672129087150097, 0.0012672129087150097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012672129087150097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18932927
Iteration 2/25 | Loss: 0.00126696
Iteration 3/25 | Loss: 0.00126694
Iteration 4/25 | Loss: 0.00126694
Iteration 5/25 | Loss: 0.00126694
Iteration 6/25 | Loss: 0.00126694
Iteration 7/25 | Loss: 0.00126694
Iteration 8/25 | Loss: 0.00126694
Iteration 9/25 | Loss: 0.00126694
Iteration 10/25 | Loss: 0.00126694
Iteration 11/25 | Loss: 0.00126694
Iteration 12/25 | Loss: 0.00126694
Iteration 13/25 | Loss: 0.00126694
Iteration 14/25 | Loss: 0.00126694
Iteration 15/25 | Loss: 0.00126694
Iteration 16/25 | Loss: 0.00126694
Iteration 17/25 | Loss: 0.00126694
Iteration 18/25 | Loss: 0.00126694
Iteration 19/25 | Loss: 0.00126694
Iteration 20/25 | Loss: 0.00126694
Iteration 21/25 | Loss: 0.00126694
Iteration 22/25 | Loss: 0.00126694
Iteration 23/25 | Loss: 0.00126694
Iteration 24/25 | Loss: 0.00126694
Iteration 25/25 | Loss: 0.00126694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126694
Iteration 2/1000 | Loss: 0.00007878
Iteration 3/1000 | Loss: 0.00005010
Iteration 4/1000 | Loss: 0.00004174
Iteration 5/1000 | Loss: 0.00003838
Iteration 6/1000 | Loss: 0.00003635
Iteration 7/1000 | Loss: 0.00003513
Iteration 8/1000 | Loss: 0.00003425
Iteration 9/1000 | Loss: 0.00003365
Iteration 10/1000 | Loss: 0.00003323
Iteration 11/1000 | Loss: 0.00003279
Iteration 12/1000 | Loss: 0.00003243
Iteration 13/1000 | Loss: 0.00003216
Iteration 14/1000 | Loss: 0.00003191
Iteration 15/1000 | Loss: 0.00003181
Iteration 16/1000 | Loss: 0.00003164
Iteration 17/1000 | Loss: 0.00003149
Iteration 18/1000 | Loss: 0.00003144
Iteration 19/1000 | Loss: 0.00003140
Iteration 20/1000 | Loss: 0.00003138
Iteration 21/1000 | Loss: 0.00003129
Iteration 22/1000 | Loss: 0.00003127
Iteration 23/1000 | Loss: 0.00003126
Iteration 24/1000 | Loss: 0.00003126
Iteration 25/1000 | Loss: 0.00003125
Iteration 26/1000 | Loss: 0.00003125
Iteration 27/1000 | Loss: 0.00003124
Iteration 28/1000 | Loss: 0.00003120
Iteration 29/1000 | Loss: 0.00003119
Iteration 30/1000 | Loss: 0.00003119
Iteration 31/1000 | Loss: 0.00003119
Iteration 32/1000 | Loss: 0.00003116
Iteration 33/1000 | Loss: 0.00003115
Iteration 34/1000 | Loss: 0.00003114
Iteration 35/1000 | Loss: 0.00003114
Iteration 36/1000 | Loss: 0.00003112
Iteration 37/1000 | Loss: 0.00003112
Iteration 38/1000 | Loss: 0.00003112
Iteration 39/1000 | Loss: 0.00003112
Iteration 40/1000 | Loss: 0.00003109
Iteration 41/1000 | Loss: 0.00003108
Iteration 42/1000 | Loss: 0.00003108
Iteration 43/1000 | Loss: 0.00003107
Iteration 44/1000 | Loss: 0.00003107
Iteration 45/1000 | Loss: 0.00003107
Iteration 46/1000 | Loss: 0.00003106
Iteration 47/1000 | Loss: 0.00003106
Iteration 48/1000 | Loss: 0.00003104
Iteration 49/1000 | Loss: 0.00003104
Iteration 50/1000 | Loss: 0.00003104
Iteration 51/1000 | Loss: 0.00003103
Iteration 52/1000 | Loss: 0.00003103
Iteration 53/1000 | Loss: 0.00003103
Iteration 54/1000 | Loss: 0.00003102
Iteration 55/1000 | Loss: 0.00003101
Iteration 56/1000 | Loss: 0.00003101
Iteration 57/1000 | Loss: 0.00003101
Iteration 58/1000 | Loss: 0.00003101
Iteration 59/1000 | Loss: 0.00003100
Iteration 60/1000 | Loss: 0.00003100
Iteration 61/1000 | Loss: 0.00003100
Iteration 62/1000 | Loss: 0.00003099
Iteration 63/1000 | Loss: 0.00003099
Iteration 64/1000 | Loss: 0.00003099
Iteration 65/1000 | Loss: 0.00003098
Iteration 66/1000 | Loss: 0.00003098
Iteration 67/1000 | Loss: 0.00003098
Iteration 68/1000 | Loss: 0.00003097
Iteration 69/1000 | Loss: 0.00003097
Iteration 70/1000 | Loss: 0.00003097
Iteration 71/1000 | Loss: 0.00003097
Iteration 72/1000 | Loss: 0.00003097
Iteration 73/1000 | Loss: 0.00003097
Iteration 74/1000 | Loss: 0.00003096
Iteration 75/1000 | Loss: 0.00003096
Iteration 76/1000 | Loss: 0.00003096
Iteration 77/1000 | Loss: 0.00003096
Iteration 78/1000 | Loss: 0.00003096
Iteration 79/1000 | Loss: 0.00003096
Iteration 80/1000 | Loss: 0.00003096
Iteration 81/1000 | Loss: 0.00003096
Iteration 82/1000 | Loss: 0.00003096
Iteration 83/1000 | Loss: 0.00003096
Iteration 84/1000 | Loss: 0.00003095
Iteration 85/1000 | Loss: 0.00003095
Iteration 86/1000 | Loss: 0.00003095
Iteration 87/1000 | Loss: 0.00003095
Iteration 88/1000 | Loss: 0.00003095
Iteration 89/1000 | Loss: 0.00003095
Iteration 90/1000 | Loss: 0.00003094
Iteration 91/1000 | Loss: 0.00003094
Iteration 92/1000 | Loss: 0.00003093
Iteration 93/1000 | Loss: 0.00003093
Iteration 94/1000 | Loss: 0.00003093
Iteration 95/1000 | Loss: 0.00003092
Iteration 96/1000 | Loss: 0.00003092
Iteration 97/1000 | Loss: 0.00003092
Iteration 98/1000 | Loss: 0.00003092
Iteration 99/1000 | Loss: 0.00003092
Iteration 100/1000 | Loss: 0.00003092
Iteration 101/1000 | Loss: 0.00003091
Iteration 102/1000 | Loss: 0.00003091
Iteration 103/1000 | Loss: 0.00003090
Iteration 104/1000 | Loss: 0.00003090
Iteration 105/1000 | Loss: 0.00003090
Iteration 106/1000 | Loss: 0.00003090
Iteration 107/1000 | Loss: 0.00003090
Iteration 108/1000 | Loss: 0.00003090
Iteration 109/1000 | Loss: 0.00003090
Iteration 110/1000 | Loss: 0.00003090
Iteration 111/1000 | Loss: 0.00003090
Iteration 112/1000 | Loss: 0.00003090
Iteration 113/1000 | Loss: 0.00003089
Iteration 114/1000 | Loss: 0.00003089
Iteration 115/1000 | Loss: 0.00003089
Iteration 116/1000 | Loss: 0.00003089
Iteration 117/1000 | Loss: 0.00003089
Iteration 118/1000 | Loss: 0.00003088
Iteration 119/1000 | Loss: 0.00003088
Iteration 120/1000 | Loss: 0.00003088
Iteration 121/1000 | Loss: 0.00003088
Iteration 122/1000 | Loss: 0.00003088
Iteration 123/1000 | Loss: 0.00003088
Iteration 124/1000 | Loss: 0.00003088
Iteration 125/1000 | Loss: 0.00003088
Iteration 126/1000 | Loss: 0.00003088
Iteration 127/1000 | Loss: 0.00003088
Iteration 128/1000 | Loss: 0.00003088
Iteration 129/1000 | Loss: 0.00003087
Iteration 130/1000 | Loss: 0.00003087
Iteration 131/1000 | Loss: 0.00003087
Iteration 132/1000 | Loss: 0.00003087
Iteration 133/1000 | Loss: 0.00003087
Iteration 134/1000 | Loss: 0.00003086
Iteration 135/1000 | Loss: 0.00003086
Iteration 136/1000 | Loss: 0.00003086
Iteration 137/1000 | Loss: 0.00003086
Iteration 138/1000 | Loss: 0.00003086
Iteration 139/1000 | Loss: 0.00003086
Iteration 140/1000 | Loss: 0.00003086
Iteration 141/1000 | Loss: 0.00003086
Iteration 142/1000 | Loss: 0.00003085
Iteration 143/1000 | Loss: 0.00003085
Iteration 144/1000 | Loss: 0.00003085
Iteration 145/1000 | Loss: 0.00003085
Iteration 146/1000 | Loss: 0.00003085
Iteration 147/1000 | Loss: 0.00003085
Iteration 148/1000 | Loss: 0.00003085
Iteration 149/1000 | Loss: 0.00003085
Iteration 150/1000 | Loss: 0.00003085
Iteration 151/1000 | Loss: 0.00003085
Iteration 152/1000 | Loss: 0.00003085
Iteration 153/1000 | Loss: 0.00003085
Iteration 154/1000 | Loss: 0.00003085
Iteration 155/1000 | Loss: 0.00003085
Iteration 156/1000 | Loss: 0.00003085
Iteration 157/1000 | Loss: 0.00003085
Iteration 158/1000 | Loss: 0.00003085
Iteration 159/1000 | Loss: 0.00003085
Iteration 160/1000 | Loss: 0.00003085
Iteration 161/1000 | Loss: 0.00003085
Iteration 162/1000 | Loss: 0.00003085
Iteration 163/1000 | Loss: 0.00003085
Iteration 164/1000 | Loss: 0.00003085
Iteration 165/1000 | Loss: 0.00003085
Iteration 166/1000 | Loss: 0.00003085
Iteration 167/1000 | Loss: 0.00003085
Iteration 168/1000 | Loss: 0.00003085
Iteration 169/1000 | Loss: 0.00003085
Iteration 170/1000 | Loss: 0.00003085
Iteration 171/1000 | Loss: 0.00003085
Iteration 172/1000 | Loss: 0.00003085
Iteration 173/1000 | Loss: 0.00003085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.0852952477289364e-05, 3.0852952477289364e-05, 3.0852952477289364e-05, 3.0852952477289364e-05, 3.0852952477289364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0852952477289364e-05

Optimization complete. Final v2v error: 4.537558555603027 mm

Highest mean error: 6.0279541015625 mm for frame 55

Lowest mean error: 3.435114622116089 mm for frame 190

Saving results

Total time: 53.09002900123596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483614
Iteration 2/25 | Loss: 0.00117219
Iteration 3/25 | Loss: 0.00109071
Iteration 4/25 | Loss: 0.00107581
Iteration 5/25 | Loss: 0.00107031
Iteration 6/25 | Loss: 0.00106968
Iteration 7/25 | Loss: 0.00106968
Iteration 8/25 | Loss: 0.00106968
Iteration 9/25 | Loss: 0.00106968
Iteration 10/25 | Loss: 0.00106968
Iteration 11/25 | Loss: 0.00106968
Iteration 12/25 | Loss: 0.00106968
Iteration 13/25 | Loss: 0.00106968
Iteration 14/25 | Loss: 0.00106968
Iteration 15/25 | Loss: 0.00106968
Iteration 16/25 | Loss: 0.00106968
Iteration 17/25 | Loss: 0.00106968
Iteration 18/25 | Loss: 0.00106968
Iteration 19/25 | Loss: 0.00106968
Iteration 20/25 | Loss: 0.00106968
Iteration 21/25 | Loss: 0.00106968
Iteration 22/25 | Loss: 0.00106968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010696777608245611, 0.0010696777608245611, 0.0010696777608245611, 0.0010696777608245611, 0.0010696777608245611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010696777608245611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33359909
Iteration 2/25 | Loss: 0.00079463
Iteration 3/25 | Loss: 0.00079463
Iteration 4/25 | Loss: 0.00079462
Iteration 5/25 | Loss: 0.00079462
Iteration 6/25 | Loss: 0.00079462
Iteration 7/25 | Loss: 0.00079462
Iteration 8/25 | Loss: 0.00079462
Iteration 9/25 | Loss: 0.00079462
Iteration 10/25 | Loss: 0.00079462
Iteration 11/25 | Loss: 0.00079462
Iteration 12/25 | Loss: 0.00079462
Iteration 13/25 | Loss: 0.00079462
Iteration 14/25 | Loss: 0.00079462
Iteration 15/25 | Loss: 0.00079462
Iteration 16/25 | Loss: 0.00079462
Iteration 17/25 | Loss: 0.00079462
Iteration 18/25 | Loss: 0.00079462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007946217665448785, 0.0007946217665448785, 0.0007946217665448785, 0.0007946217665448785, 0.0007946217665448785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007946217665448785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079462
Iteration 2/1000 | Loss: 0.00001815
Iteration 3/1000 | Loss: 0.00001431
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001246
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001144
Iteration 9/1000 | Loss: 0.00001114
Iteration 10/1000 | Loss: 0.00001093
Iteration 11/1000 | Loss: 0.00001088
Iteration 12/1000 | Loss: 0.00001087
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001085
Iteration 17/1000 | Loss: 0.00001084
Iteration 18/1000 | Loss: 0.00001082
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001074
Iteration 21/1000 | Loss: 0.00001070
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001070
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001059
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001056
Iteration 28/1000 | Loss: 0.00001055
Iteration 29/1000 | Loss: 0.00001054
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001044
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001038
Iteration 42/1000 | Loss: 0.00001038
Iteration 43/1000 | Loss: 0.00001038
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001035
Iteration 56/1000 | Loss: 0.00001035
Iteration 57/1000 | Loss: 0.00001035
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001034
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001033
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001031
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001028
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001027
Iteration 75/1000 | Loss: 0.00001027
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001027
Iteration 78/1000 | Loss: 0.00001027
Iteration 79/1000 | Loss: 0.00001027
Iteration 80/1000 | Loss: 0.00001027
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001026
Iteration 84/1000 | Loss: 0.00001026
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001024
Iteration 87/1000 | Loss: 0.00001024
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001022
Iteration 96/1000 | Loss: 0.00001022
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001022
Iteration 99/1000 | Loss: 0.00001022
Iteration 100/1000 | Loss: 0.00001022
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001021
Iteration 103/1000 | Loss: 0.00001021
Iteration 104/1000 | Loss: 0.00001021
Iteration 105/1000 | Loss: 0.00001021
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001020
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001019
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001018
Iteration 130/1000 | Loss: 0.00001018
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001016
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001015
Iteration 144/1000 | Loss: 0.00001015
Iteration 145/1000 | Loss: 0.00001015
Iteration 146/1000 | Loss: 0.00001015
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001014
Iteration 154/1000 | Loss: 0.00001014
Iteration 155/1000 | Loss: 0.00001014
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001013
Iteration 160/1000 | Loss: 0.00001013
Iteration 161/1000 | Loss: 0.00001013
Iteration 162/1000 | Loss: 0.00001013
Iteration 163/1000 | Loss: 0.00001013
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Iteration 166/1000 | Loss: 0.00001012
Iteration 167/1000 | Loss: 0.00001012
Iteration 168/1000 | Loss: 0.00001012
Iteration 169/1000 | Loss: 0.00001012
Iteration 170/1000 | Loss: 0.00001012
Iteration 171/1000 | Loss: 0.00001012
Iteration 172/1000 | Loss: 0.00001012
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001012
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001012
Iteration 179/1000 | Loss: 0.00001012
Iteration 180/1000 | Loss: 0.00001012
Iteration 181/1000 | Loss: 0.00001012
Iteration 182/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.0121603736479301e-05, 1.0121603736479301e-05, 1.0121603736479301e-05, 1.0121603736479301e-05, 1.0121603736479301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0121603736479301e-05

Optimization complete. Final v2v error: 2.7665603160858154 mm

Highest mean error: 3.01035737991333 mm for frame 169

Lowest mean error: 2.636697292327881 mm for frame 37

Saving results

Total time: 44.18309807777405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380188
Iteration 2/25 | Loss: 0.00117924
Iteration 3/25 | Loss: 0.00109355
Iteration 4/25 | Loss: 0.00108562
Iteration 5/25 | Loss: 0.00108350
Iteration 6/25 | Loss: 0.00108324
Iteration 7/25 | Loss: 0.00108324
Iteration 8/25 | Loss: 0.00108324
Iteration 9/25 | Loss: 0.00108324
Iteration 10/25 | Loss: 0.00108324
Iteration 11/25 | Loss: 0.00108324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010832395637407899, 0.0010832395637407899, 0.0010832395637407899, 0.0010832395637407899, 0.0010832395637407899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010832395637407899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38081169
Iteration 2/25 | Loss: 0.00081671
Iteration 3/25 | Loss: 0.00081671
Iteration 4/25 | Loss: 0.00081670
Iteration 5/25 | Loss: 0.00081670
Iteration 6/25 | Loss: 0.00081670
Iteration 7/25 | Loss: 0.00081670
Iteration 8/25 | Loss: 0.00081670
Iteration 9/25 | Loss: 0.00081670
Iteration 10/25 | Loss: 0.00081670
Iteration 11/25 | Loss: 0.00081670
Iteration 12/25 | Loss: 0.00081670
Iteration 13/25 | Loss: 0.00081670
Iteration 14/25 | Loss: 0.00081670
Iteration 15/25 | Loss: 0.00081670
Iteration 16/25 | Loss: 0.00081670
Iteration 17/25 | Loss: 0.00081670
Iteration 18/25 | Loss: 0.00081670
Iteration 19/25 | Loss: 0.00081670
Iteration 20/25 | Loss: 0.00081670
Iteration 21/25 | Loss: 0.00081670
Iteration 22/25 | Loss: 0.00081670
Iteration 23/25 | Loss: 0.00081670
Iteration 24/25 | Loss: 0.00081670
Iteration 25/25 | Loss: 0.00081670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081670
Iteration 2/1000 | Loss: 0.00001994
Iteration 3/1000 | Loss: 0.00001347
Iteration 4/1000 | Loss: 0.00001229
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001106
Iteration 8/1000 | Loss: 0.00001102
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001081
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001064
Iteration 15/1000 | Loss: 0.00001061
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001048
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001047
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001046
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001045
Iteration 38/1000 | Loss: 0.00001045
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001043
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001033
Iteration 55/1000 | Loss: 0.00001033
Iteration 56/1000 | Loss: 0.00001032
Iteration 57/1000 | Loss: 0.00001032
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001031
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001029
Iteration 71/1000 | Loss: 0.00001029
Iteration 72/1000 | Loss: 0.00001029
Iteration 73/1000 | Loss: 0.00001028
Iteration 74/1000 | Loss: 0.00001028
Iteration 75/1000 | Loss: 0.00001028
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001027
Iteration 78/1000 | Loss: 0.00001027
Iteration 79/1000 | Loss: 0.00001026
Iteration 80/1000 | Loss: 0.00001026
Iteration 81/1000 | Loss: 0.00001026
Iteration 82/1000 | Loss: 0.00001026
Iteration 83/1000 | Loss: 0.00001026
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001025
Iteration 90/1000 | Loss: 0.00001025
Iteration 91/1000 | Loss: 0.00001025
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001024
Iteration 94/1000 | Loss: 0.00001024
Iteration 95/1000 | Loss: 0.00001024
Iteration 96/1000 | Loss: 0.00001024
Iteration 97/1000 | Loss: 0.00001024
Iteration 98/1000 | Loss: 0.00001024
Iteration 99/1000 | Loss: 0.00001024
Iteration 100/1000 | Loss: 0.00001024
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001023
Iteration 106/1000 | Loss: 0.00001023
Iteration 107/1000 | Loss: 0.00001023
Iteration 108/1000 | Loss: 0.00001023
Iteration 109/1000 | Loss: 0.00001023
Iteration 110/1000 | Loss: 0.00001023
Iteration 111/1000 | Loss: 0.00001023
Iteration 112/1000 | Loss: 0.00001023
Iteration 113/1000 | Loss: 0.00001023
Iteration 114/1000 | Loss: 0.00001022
Iteration 115/1000 | Loss: 0.00001022
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001021
Iteration 119/1000 | Loss: 0.00001021
Iteration 120/1000 | Loss: 0.00001021
Iteration 121/1000 | Loss: 0.00001021
Iteration 122/1000 | Loss: 0.00001021
Iteration 123/1000 | Loss: 0.00001021
Iteration 124/1000 | Loss: 0.00001021
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001019
Iteration 129/1000 | Loss: 0.00001019
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001018
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001017
Iteration 144/1000 | Loss: 0.00001017
Iteration 145/1000 | Loss: 0.00001017
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001015
Iteration 150/1000 | Loss: 0.00001015
Iteration 151/1000 | Loss: 0.00001015
Iteration 152/1000 | Loss: 0.00001015
Iteration 153/1000 | Loss: 0.00001015
Iteration 154/1000 | Loss: 0.00001015
Iteration 155/1000 | Loss: 0.00001015
Iteration 156/1000 | Loss: 0.00001015
Iteration 157/1000 | Loss: 0.00001015
Iteration 158/1000 | Loss: 0.00001014
Iteration 159/1000 | Loss: 0.00001014
Iteration 160/1000 | Loss: 0.00001014
Iteration 161/1000 | Loss: 0.00001014
Iteration 162/1000 | Loss: 0.00001014
Iteration 163/1000 | Loss: 0.00001014
Iteration 164/1000 | Loss: 0.00001014
Iteration 165/1000 | Loss: 0.00001014
Iteration 166/1000 | Loss: 0.00001014
Iteration 167/1000 | Loss: 0.00001014
Iteration 168/1000 | Loss: 0.00001014
Iteration 169/1000 | Loss: 0.00001014
Iteration 170/1000 | Loss: 0.00001014
Iteration 171/1000 | Loss: 0.00001014
Iteration 172/1000 | Loss: 0.00001014
Iteration 173/1000 | Loss: 0.00001014
Iteration 174/1000 | Loss: 0.00001014
Iteration 175/1000 | Loss: 0.00001014
Iteration 176/1000 | Loss: 0.00001014
Iteration 177/1000 | Loss: 0.00001013
Iteration 178/1000 | Loss: 0.00001013
Iteration 179/1000 | Loss: 0.00001013
Iteration 180/1000 | Loss: 0.00001013
Iteration 181/1000 | Loss: 0.00001013
Iteration 182/1000 | Loss: 0.00001013
Iteration 183/1000 | Loss: 0.00001013
Iteration 184/1000 | Loss: 0.00001013
Iteration 185/1000 | Loss: 0.00001013
Iteration 186/1000 | Loss: 0.00001013
Iteration 187/1000 | Loss: 0.00001013
Iteration 188/1000 | Loss: 0.00001013
Iteration 189/1000 | Loss: 0.00001013
Iteration 190/1000 | Loss: 0.00001012
Iteration 191/1000 | Loss: 0.00001012
Iteration 192/1000 | Loss: 0.00001012
Iteration 193/1000 | Loss: 0.00001012
Iteration 194/1000 | Loss: 0.00001012
Iteration 195/1000 | Loss: 0.00001012
Iteration 196/1000 | Loss: 0.00001012
Iteration 197/1000 | Loss: 0.00001012
Iteration 198/1000 | Loss: 0.00001012
Iteration 199/1000 | Loss: 0.00001011
Iteration 200/1000 | Loss: 0.00001011
Iteration 201/1000 | Loss: 0.00001011
Iteration 202/1000 | Loss: 0.00001011
Iteration 203/1000 | Loss: 0.00001011
Iteration 204/1000 | Loss: 0.00001010
Iteration 205/1000 | Loss: 0.00001010
Iteration 206/1000 | Loss: 0.00001010
Iteration 207/1000 | Loss: 0.00001010
Iteration 208/1000 | Loss: 0.00001010
Iteration 209/1000 | Loss: 0.00001010
Iteration 210/1000 | Loss: 0.00001010
Iteration 211/1000 | Loss: 0.00001010
Iteration 212/1000 | Loss: 0.00001010
Iteration 213/1000 | Loss: 0.00001010
Iteration 214/1000 | Loss: 0.00001010
Iteration 215/1000 | Loss: 0.00001010
Iteration 216/1000 | Loss: 0.00001010
Iteration 217/1000 | Loss: 0.00001010
Iteration 218/1000 | Loss: 0.00001010
Iteration 219/1000 | Loss: 0.00001010
Iteration 220/1000 | Loss: 0.00001010
Iteration 221/1000 | Loss: 0.00001010
Iteration 222/1000 | Loss: 0.00001010
Iteration 223/1000 | Loss: 0.00001010
Iteration 224/1000 | Loss: 0.00001010
Iteration 225/1000 | Loss: 0.00001010
Iteration 226/1000 | Loss: 0.00001010
Iteration 227/1000 | Loss: 0.00001010
Iteration 228/1000 | Loss: 0.00001010
Iteration 229/1000 | Loss: 0.00001010
Iteration 230/1000 | Loss: 0.00001010
Iteration 231/1000 | Loss: 0.00001010
Iteration 232/1000 | Loss: 0.00001010
Iteration 233/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.00968254628242e-05, 1.00968254628242e-05, 1.00968254628242e-05, 1.00968254628242e-05, 1.00968254628242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.00968254628242e-05

Optimization complete. Final v2v error: 2.7013936042785645 mm

Highest mean error: 2.901637077331543 mm for frame 87

Lowest mean error: 2.504507541656494 mm for frame 68

Saving results

Total time: 36.46268057823181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938546
Iteration 2/25 | Loss: 0.00165562
Iteration 3/25 | Loss: 0.00130493
Iteration 4/25 | Loss: 0.00128308
Iteration 5/25 | Loss: 0.00127599
Iteration 6/25 | Loss: 0.00127466
Iteration 7/25 | Loss: 0.00127466
Iteration 8/25 | Loss: 0.00127466
Iteration 9/25 | Loss: 0.00127466
Iteration 10/25 | Loss: 0.00127466
Iteration 11/25 | Loss: 0.00127466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001274655805900693, 0.001274655805900693, 0.001274655805900693, 0.001274655805900693, 0.001274655805900693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274655805900693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86541200
Iteration 2/25 | Loss: 0.00106722
Iteration 3/25 | Loss: 0.00106722
Iteration 4/25 | Loss: 0.00106722
Iteration 5/25 | Loss: 0.00106722
Iteration 6/25 | Loss: 0.00106722
Iteration 7/25 | Loss: 0.00106722
Iteration 8/25 | Loss: 0.00106722
Iteration 9/25 | Loss: 0.00106722
Iteration 10/25 | Loss: 0.00106722
Iteration 11/25 | Loss: 0.00106722
Iteration 12/25 | Loss: 0.00106722
Iteration 13/25 | Loss: 0.00106722
Iteration 14/25 | Loss: 0.00106722
Iteration 15/25 | Loss: 0.00106722
Iteration 16/25 | Loss: 0.00106722
Iteration 17/25 | Loss: 0.00106722
Iteration 18/25 | Loss: 0.00106722
Iteration 19/25 | Loss: 0.00106722
Iteration 20/25 | Loss: 0.00106722
Iteration 21/25 | Loss: 0.00106722
Iteration 22/25 | Loss: 0.00106722
Iteration 23/25 | Loss: 0.00106722
Iteration 24/25 | Loss: 0.00106722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010672190692275763, 0.0010672190692275763, 0.0010672190692275763, 0.0010672190692275763, 0.0010672190692275763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010672190692275763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106722
Iteration 2/1000 | Loss: 0.00006133
Iteration 3/1000 | Loss: 0.00004236
Iteration 4/1000 | Loss: 0.00003486
Iteration 5/1000 | Loss: 0.00003295
Iteration 6/1000 | Loss: 0.00003189
Iteration 7/1000 | Loss: 0.00003107
Iteration 8/1000 | Loss: 0.00003061
Iteration 9/1000 | Loss: 0.00003008
Iteration 10/1000 | Loss: 0.00002978
Iteration 11/1000 | Loss: 0.00002948
Iteration 12/1000 | Loss: 0.00002918
Iteration 13/1000 | Loss: 0.00002889
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00002845
Iteration 16/1000 | Loss: 0.00002822
Iteration 17/1000 | Loss: 0.00002809
Iteration 18/1000 | Loss: 0.00002807
Iteration 19/1000 | Loss: 0.00002798
Iteration 20/1000 | Loss: 0.00002789
Iteration 21/1000 | Loss: 0.00002784
Iteration 22/1000 | Loss: 0.00002781
Iteration 23/1000 | Loss: 0.00002774
Iteration 24/1000 | Loss: 0.00002774
Iteration 25/1000 | Loss: 0.00002772
Iteration 26/1000 | Loss: 0.00002772
Iteration 27/1000 | Loss: 0.00002771
Iteration 28/1000 | Loss: 0.00002771
Iteration 29/1000 | Loss: 0.00002770
Iteration 30/1000 | Loss: 0.00002770
Iteration 31/1000 | Loss: 0.00002769
Iteration 32/1000 | Loss: 0.00002768
Iteration 33/1000 | Loss: 0.00002768
Iteration 34/1000 | Loss: 0.00002767
Iteration 35/1000 | Loss: 0.00002766
Iteration 36/1000 | Loss: 0.00002766
Iteration 37/1000 | Loss: 0.00002766
Iteration 38/1000 | Loss: 0.00002766
Iteration 39/1000 | Loss: 0.00002764
Iteration 40/1000 | Loss: 0.00002764
Iteration 41/1000 | Loss: 0.00002764
Iteration 42/1000 | Loss: 0.00002763
Iteration 43/1000 | Loss: 0.00002763
Iteration 44/1000 | Loss: 0.00002763
Iteration 45/1000 | Loss: 0.00002763
Iteration 46/1000 | Loss: 0.00002762
Iteration 47/1000 | Loss: 0.00002762
Iteration 48/1000 | Loss: 0.00002762
Iteration 49/1000 | Loss: 0.00002762
Iteration 50/1000 | Loss: 0.00002762
Iteration 51/1000 | Loss: 0.00002762
Iteration 52/1000 | Loss: 0.00002761
Iteration 53/1000 | Loss: 0.00002761
Iteration 54/1000 | Loss: 0.00002761
Iteration 55/1000 | Loss: 0.00002761
Iteration 56/1000 | Loss: 0.00002761
Iteration 57/1000 | Loss: 0.00002761
Iteration 58/1000 | Loss: 0.00002760
Iteration 59/1000 | Loss: 0.00002760
Iteration 60/1000 | Loss: 0.00002760
Iteration 61/1000 | Loss: 0.00002759
Iteration 62/1000 | Loss: 0.00002759
Iteration 63/1000 | Loss: 0.00002759
Iteration 64/1000 | Loss: 0.00002759
Iteration 65/1000 | Loss: 0.00002758
Iteration 66/1000 | Loss: 0.00002758
Iteration 67/1000 | Loss: 0.00002758
Iteration 68/1000 | Loss: 0.00002758
Iteration 69/1000 | Loss: 0.00002758
Iteration 70/1000 | Loss: 0.00002758
Iteration 71/1000 | Loss: 0.00002758
Iteration 72/1000 | Loss: 0.00002758
Iteration 73/1000 | Loss: 0.00002758
Iteration 74/1000 | Loss: 0.00002758
Iteration 75/1000 | Loss: 0.00002758
Iteration 76/1000 | Loss: 0.00002757
Iteration 77/1000 | Loss: 0.00002757
Iteration 78/1000 | Loss: 0.00002757
Iteration 79/1000 | Loss: 0.00002757
Iteration 80/1000 | Loss: 0.00002757
Iteration 81/1000 | Loss: 0.00002757
Iteration 82/1000 | Loss: 0.00002757
Iteration 83/1000 | Loss: 0.00002756
Iteration 84/1000 | Loss: 0.00002756
Iteration 85/1000 | Loss: 0.00002756
Iteration 86/1000 | Loss: 0.00002756
Iteration 87/1000 | Loss: 0.00002756
Iteration 88/1000 | Loss: 0.00002756
Iteration 89/1000 | Loss: 0.00002756
Iteration 90/1000 | Loss: 0.00002756
Iteration 91/1000 | Loss: 0.00002756
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00002755
Iteration 94/1000 | Loss: 0.00002755
Iteration 95/1000 | Loss: 0.00002755
Iteration 96/1000 | Loss: 0.00002755
Iteration 97/1000 | Loss: 0.00002755
Iteration 98/1000 | Loss: 0.00002755
Iteration 99/1000 | Loss: 0.00002755
Iteration 100/1000 | Loss: 0.00002755
Iteration 101/1000 | Loss: 0.00002754
Iteration 102/1000 | Loss: 0.00002754
Iteration 103/1000 | Loss: 0.00002754
Iteration 104/1000 | Loss: 0.00002754
Iteration 105/1000 | Loss: 0.00002754
Iteration 106/1000 | Loss: 0.00002754
Iteration 107/1000 | Loss: 0.00002754
Iteration 108/1000 | Loss: 0.00002754
Iteration 109/1000 | Loss: 0.00002754
Iteration 110/1000 | Loss: 0.00002753
Iteration 111/1000 | Loss: 0.00002753
Iteration 112/1000 | Loss: 0.00002753
Iteration 113/1000 | Loss: 0.00002753
Iteration 114/1000 | Loss: 0.00002753
Iteration 115/1000 | Loss: 0.00002752
Iteration 116/1000 | Loss: 0.00002752
Iteration 117/1000 | Loss: 0.00002752
Iteration 118/1000 | Loss: 0.00002752
Iteration 119/1000 | Loss: 0.00002752
Iteration 120/1000 | Loss: 0.00002752
Iteration 121/1000 | Loss: 0.00002752
Iteration 122/1000 | Loss: 0.00002752
Iteration 123/1000 | Loss: 0.00002752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.751964348135516e-05, 2.751964348135516e-05, 2.751964348135516e-05, 2.751964348135516e-05, 2.751964348135516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.751964348135516e-05

Optimization complete. Final v2v error: 4.318820476531982 mm

Highest mean error: 5.561734676361084 mm for frame 120

Lowest mean error: 3.375190258026123 mm for frame 0

Saving results

Total time: 49.46740365028381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038234
Iteration 2/25 | Loss: 0.00240275
Iteration 3/25 | Loss: 0.00241021
Iteration 4/25 | Loss: 0.00207968
Iteration 5/25 | Loss: 0.00207682
Iteration 6/25 | Loss: 0.00176890
Iteration 7/25 | Loss: 0.00150405
Iteration 8/25 | Loss: 0.00136652
Iteration 9/25 | Loss: 0.00136186
Iteration 10/25 | Loss: 0.00130494
Iteration 11/25 | Loss: 0.00128625
Iteration 12/25 | Loss: 0.00128930
Iteration 13/25 | Loss: 0.00126670
Iteration 14/25 | Loss: 0.00127279
Iteration 15/25 | Loss: 0.00126220
Iteration 16/25 | Loss: 0.00124892
Iteration 17/25 | Loss: 0.00125243
Iteration 18/25 | Loss: 0.00124113
Iteration 19/25 | Loss: 0.00123754
Iteration 20/25 | Loss: 0.00124227
Iteration 21/25 | Loss: 0.00125074
Iteration 22/25 | Loss: 0.00123236
Iteration 23/25 | Loss: 0.00122968
Iteration 24/25 | Loss: 0.00123074
Iteration 25/25 | Loss: 0.00123304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30062377
Iteration 2/25 | Loss: 0.00156858
Iteration 3/25 | Loss: 0.00098325
Iteration 4/25 | Loss: 0.00097546
Iteration 5/25 | Loss: 0.00097546
Iteration 6/25 | Loss: 0.00097545
Iteration 7/25 | Loss: 0.00097545
Iteration 8/25 | Loss: 0.00097545
Iteration 9/25 | Loss: 0.00097545
Iteration 10/25 | Loss: 0.00097545
Iteration 11/25 | Loss: 0.00097545
Iteration 12/25 | Loss: 0.00097545
Iteration 13/25 | Loss: 0.00097545
Iteration 14/25 | Loss: 0.00097545
Iteration 15/25 | Loss: 0.00097545
Iteration 16/25 | Loss: 0.00097545
Iteration 17/25 | Loss: 0.00097545
Iteration 18/25 | Loss: 0.00097545
Iteration 19/25 | Loss: 0.00097545
Iteration 20/25 | Loss: 0.00097545
Iteration 21/25 | Loss: 0.00097545
Iteration 22/25 | Loss: 0.00097545
Iteration 23/25 | Loss: 0.00097545
Iteration 24/25 | Loss: 0.00097545
Iteration 25/25 | Loss: 0.00097545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097545
Iteration 2/1000 | Loss: 0.00043192
Iteration 3/1000 | Loss: 0.00051896
Iteration 4/1000 | Loss: 0.00011420
Iteration 5/1000 | Loss: 0.00010160
Iteration 6/1000 | Loss: 0.00012152
Iteration 7/1000 | Loss: 0.00102819
Iteration 8/1000 | Loss: 0.00011384
Iteration 9/1000 | Loss: 0.00028711
Iteration 10/1000 | Loss: 0.00005292
Iteration 11/1000 | Loss: 0.00027829
Iteration 12/1000 | Loss: 0.00008974
Iteration 13/1000 | Loss: 0.00035494
Iteration 14/1000 | Loss: 0.00010691
Iteration 15/1000 | Loss: 0.00032228
Iteration 16/1000 | Loss: 0.00003252
Iteration 17/1000 | Loss: 0.00004913
Iteration 18/1000 | Loss: 0.00041982
Iteration 19/1000 | Loss: 0.00009230
Iteration 20/1000 | Loss: 0.00008659
Iteration 21/1000 | Loss: 0.00002751
Iteration 22/1000 | Loss: 0.00004494
Iteration 23/1000 | Loss: 0.00005354
Iteration 24/1000 | Loss: 0.00045849
Iteration 25/1000 | Loss: 0.00044632
Iteration 26/1000 | Loss: 0.00104496
Iteration 27/1000 | Loss: 0.00025659
Iteration 28/1000 | Loss: 0.00003425
Iteration 29/1000 | Loss: 0.00035565
Iteration 30/1000 | Loss: 0.00061300
Iteration 31/1000 | Loss: 0.00002724
Iteration 32/1000 | Loss: 0.00030903
Iteration 33/1000 | Loss: 0.00002330
Iteration 34/1000 | Loss: 0.00002265
Iteration 35/1000 | Loss: 0.00049040
Iteration 36/1000 | Loss: 0.00044125
Iteration 37/1000 | Loss: 0.00019240
Iteration 38/1000 | Loss: 0.00006640
Iteration 39/1000 | Loss: 0.00009060
Iteration 40/1000 | Loss: 0.00051571
Iteration 41/1000 | Loss: 0.00242647
Iteration 42/1000 | Loss: 0.00015949
Iteration 43/1000 | Loss: 0.00009474
Iteration 44/1000 | Loss: 0.00022963
Iteration 45/1000 | Loss: 0.00003489
Iteration 46/1000 | Loss: 0.00011729
Iteration 47/1000 | Loss: 0.00002509
Iteration 48/1000 | Loss: 0.00002270
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00018845
Iteration 51/1000 | Loss: 0.00008812
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00003409
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00005591
Iteration 56/1000 | Loss: 0.00001884
Iteration 57/1000 | Loss: 0.00004735
Iteration 58/1000 | Loss: 0.00015587
Iteration 59/1000 | Loss: 0.00148503
Iteration 60/1000 | Loss: 0.00141349
Iteration 61/1000 | Loss: 0.00044807
Iteration 62/1000 | Loss: 0.00041843
Iteration 63/1000 | Loss: 0.00002599
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00011673
Iteration 67/1000 | Loss: 0.00004352
Iteration 68/1000 | Loss: 0.00008240
Iteration 69/1000 | Loss: 0.00003010
Iteration 70/1000 | Loss: 0.00002306
Iteration 71/1000 | Loss: 0.00003942
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00003494
Iteration 75/1000 | Loss: 0.00033209
Iteration 76/1000 | Loss: 0.00019946
Iteration 77/1000 | Loss: 0.00016593
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00004419
Iteration 80/1000 | Loss: 0.00003523
Iteration 81/1000 | Loss: 0.00022317
Iteration 82/1000 | Loss: 0.00069581
Iteration 83/1000 | Loss: 0.00008000
Iteration 84/1000 | Loss: 0.00005013
Iteration 85/1000 | Loss: 0.00004600
Iteration 86/1000 | Loss: 0.00002302
Iteration 87/1000 | Loss: 0.00005067
Iteration 88/1000 | Loss: 0.00002339
Iteration 89/1000 | Loss: 0.00002607
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00004825
Iteration 93/1000 | Loss: 0.00004670
Iteration 94/1000 | Loss: 0.00149577
Iteration 95/1000 | Loss: 0.00004803
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001863
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001851
Iteration 102/1000 | Loss: 0.00001851
Iteration 103/1000 | Loss: 0.00001851
Iteration 104/1000 | Loss: 0.00001851
Iteration 105/1000 | Loss: 0.00001851
Iteration 106/1000 | Loss: 0.00001851
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001850
Iteration 109/1000 | Loss: 0.00001850
Iteration 110/1000 | Loss: 0.00001850
Iteration 111/1000 | Loss: 0.00001850
Iteration 112/1000 | Loss: 0.00001850
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001849
Iteration 118/1000 | Loss: 0.00001849
Iteration 119/1000 | Loss: 0.00001849
Iteration 120/1000 | Loss: 0.00001849
Iteration 121/1000 | Loss: 0.00001849
Iteration 122/1000 | Loss: 0.00001849
Iteration 123/1000 | Loss: 0.00001849
Iteration 124/1000 | Loss: 0.00001849
Iteration 125/1000 | Loss: 0.00001848
Iteration 126/1000 | Loss: 0.00001848
Iteration 127/1000 | Loss: 0.00001848
Iteration 128/1000 | Loss: 0.00001848
Iteration 129/1000 | Loss: 0.00001848
Iteration 130/1000 | Loss: 0.00001848
Iteration 131/1000 | Loss: 0.00001847
Iteration 132/1000 | Loss: 0.00001847
Iteration 133/1000 | Loss: 0.00001847
Iteration 134/1000 | Loss: 0.00001847
Iteration 135/1000 | Loss: 0.00001847
Iteration 136/1000 | Loss: 0.00001847
Iteration 137/1000 | Loss: 0.00001847
Iteration 138/1000 | Loss: 0.00001847
Iteration 139/1000 | Loss: 0.00001847
Iteration 140/1000 | Loss: 0.00001847
Iteration 141/1000 | Loss: 0.00001847
Iteration 142/1000 | Loss: 0.00001847
Iteration 143/1000 | Loss: 0.00001847
Iteration 144/1000 | Loss: 0.00001847
Iteration 145/1000 | Loss: 0.00001847
Iteration 146/1000 | Loss: 0.00001847
Iteration 147/1000 | Loss: 0.00001847
Iteration 148/1000 | Loss: 0.00001847
Iteration 149/1000 | Loss: 0.00001847
Iteration 150/1000 | Loss: 0.00001847
Iteration 151/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.8467884729034267e-05, 1.8467884729034267e-05, 1.8467884729034267e-05, 1.8467884729034267e-05, 1.8467884729034267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8467884729034267e-05

Optimization complete. Final v2v error: 3.5370829105377197 mm

Highest mean error: 4.055720329284668 mm for frame 19

Lowest mean error: 3.2837131023406982 mm for frame 6

Saving results

Total time: 181.795095205307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460959
Iteration 2/25 | Loss: 0.00143148
Iteration 3/25 | Loss: 0.00113679
Iteration 4/25 | Loss: 0.00111926
Iteration 5/25 | Loss: 0.00111438
Iteration 6/25 | Loss: 0.00111277
Iteration 7/25 | Loss: 0.00111270
Iteration 8/25 | Loss: 0.00111270
Iteration 9/25 | Loss: 0.00111270
Iteration 10/25 | Loss: 0.00111270
Iteration 11/25 | Loss: 0.00111270
Iteration 12/25 | Loss: 0.00111270
Iteration 13/25 | Loss: 0.00111270
Iteration 14/25 | Loss: 0.00111270
Iteration 15/25 | Loss: 0.00111270
Iteration 16/25 | Loss: 0.00111270
Iteration 17/25 | Loss: 0.00111270
Iteration 18/25 | Loss: 0.00111270
Iteration 19/25 | Loss: 0.00111270
Iteration 20/25 | Loss: 0.00111270
Iteration 21/25 | Loss: 0.00111270
Iteration 22/25 | Loss: 0.00111270
Iteration 23/25 | Loss: 0.00111270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011127014877274632, 0.0011127014877274632, 0.0011127014877274632, 0.0011127014877274632, 0.0011127014877274632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011127014877274632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45508778
Iteration 2/25 | Loss: 0.00095452
Iteration 3/25 | Loss: 0.00095452
Iteration 4/25 | Loss: 0.00095452
Iteration 5/25 | Loss: 0.00095452
Iteration 6/25 | Loss: 0.00095452
Iteration 7/25 | Loss: 0.00095452
Iteration 8/25 | Loss: 0.00095452
Iteration 9/25 | Loss: 0.00095452
Iteration 10/25 | Loss: 0.00095451
Iteration 11/25 | Loss: 0.00095451
Iteration 12/25 | Loss: 0.00095451
Iteration 13/25 | Loss: 0.00095451
Iteration 14/25 | Loss: 0.00095451
Iteration 15/25 | Loss: 0.00095451
Iteration 16/25 | Loss: 0.00095451
Iteration 17/25 | Loss: 0.00095451
Iteration 18/25 | Loss: 0.00095451
Iteration 19/25 | Loss: 0.00095451
Iteration 20/25 | Loss: 0.00095451
Iteration 21/25 | Loss: 0.00095451
Iteration 22/25 | Loss: 0.00095451
Iteration 23/25 | Loss: 0.00095451
Iteration 24/25 | Loss: 0.00095451
Iteration 25/25 | Loss: 0.00095451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095451
Iteration 2/1000 | Loss: 0.00003190
Iteration 3/1000 | Loss: 0.00002126
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001417
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001350
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001335
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001333
Iteration 22/1000 | Loss: 0.00001331
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001330
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001322
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001319
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001307
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001299
Iteration 79/1000 | Loss: 0.00001299
Iteration 80/1000 | Loss: 0.00001299
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001294
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001293
Iteration 94/1000 | Loss: 0.00001293
Iteration 95/1000 | Loss: 0.00001293
Iteration 96/1000 | Loss: 0.00001293
Iteration 97/1000 | Loss: 0.00001293
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001292
Iteration 101/1000 | Loss: 0.00001292
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Iteration 114/1000 | Loss: 0.00001290
Iteration 115/1000 | Loss: 0.00001290
Iteration 116/1000 | Loss: 0.00001290
Iteration 117/1000 | Loss: 0.00001290
Iteration 118/1000 | Loss: 0.00001289
Iteration 119/1000 | Loss: 0.00001289
Iteration 120/1000 | Loss: 0.00001289
Iteration 121/1000 | Loss: 0.00001289
Iteration 122/1000 | Loss: 0.00001289
Iteration 123/1000 | Loss: 0.00001289
Iteration 124/1000 | Loss: 0.00001289
Iteration 125/1000 | Loss: 0.00001289
Iteration 126/1000 | Loss: 0.00001289
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001288
Iteration 130/1000 | Loss: 0.00001288
Iteration 131/1000 | Loss: 0.00001288
Iteration 132/1000 | Loss: 0.00001288
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001286
Iteration 139/1000 | Loss: 0.00001286
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001286
Iteration 142/1000 | Loss: 0.00001286
Iteration 143/1000 | Loss: 0.00001286
Iteration 144/1000 | Loss: 0.00001286
Iteration 145/1000 | Loss: 0.00001286
Iteration 146/1000 | Loss: 0.00001286
Iteration 147/1000 | Loss: 0.00001286
Iteration 148/1000 | Loss: 0.00001286
Iteration 149/1000 | Loss: 0.00001286
Iteration 150/1000 | Loss: 0.00001286
Iteration 151/1000 | Loss: 0.00001286
Iteration 152/1000 | Loss: 0.00001285
Iteration 153/1000 | Loss: 0.00001285
Iteration 154/1000 | Loss: 0.00001285
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Iteration 157/1000 | Loss: 0.00001285
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001285
Iteration 160/1000 | Loss: 0.00001284
Iteration 161/1000 | Loss: 0.00001284
Iteration 162/1000 | Loss: 0.00001284
Iteration 163/1000 | Loss: 0.00001284
Iteration 164/1000 | Loss: 0.00001284
Iteration 165/1000 | Loss: 0.00001284
Iteration 166/1000 | Loss: 0.00001284
Iteration 167/1000 | Loss: 0.00001284
Iteration 168/1000 | Loss: 0.00001284
Iteration 169/1000 | Loss: 0.00001284
Iteration 170/1000 | Loss: 0.00001284
Iteration 171/1000 | Loss: 0.00001284
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001283
Iteration 175/1000 | Loss: 0.00001283
Iteration 176/1000 | Loss: 0.00001283
Iteration 177/1000 | Loss: 0.00001283
Iteration 178/1000 | Loss: 0.00001283
Iteration 179/1000 | Loss: 0.00001283
Iteration 180/1000 | Loss: 0.00001283
Iteration 181/1000 | Loss: 0.00001283
Iteration 182/1000 | Loss: 0.00001282
Iteration 183/1000 | Loss: 0.00001282
Iteration 184/1000 | Loss: 0.00001282
Iteration 185/1000 | Loss: 0.00001282
Iteration 186/1000 | Loss: 0.00001282
Iteration 187/1000 | Loss: 0.00001282
Iteration 188/1000 | Loss: 0.00001282
Iteration 189/1000 | Loss: 0.00001282
Iteration 190/1000 | Loss: 0.00001282
Iteration 191/1000 | Loss: 0.00001282
Iteration 192/1000 | Loss: 0.00001282
Iteration 193/1000 | Loss: 0.00001282
Iteration 194/1000 | Loss: 0.00001281
Iteration 195/1000 | Loss: 0.00001281
Iteration 196/1000 | Loss: 0.00001281
Iteration 197/1000 | Loss: 0.00001281
Iteration 198/1000 | Loss: 0.00001281
Iteration 199/1000 | Loss: 0.00001281
Iteration 200/1000 | Loss: 0.00001281
Iteration 201/1000 | Loss: 0.00001281
Iteration 202/1000 | Loss: 0.00001281
Iteration 203/1000 | Loss: 0.00001281
Iteration 204/1000 | Loss: 0.00001281
Iteration 205/1000 | Loss: 0.00001281
Iteration 206/1000 | Loss: 0.00001280
Iteration 207/1000 | Loss: 0.00001280
Iteration 208/1000 | Loss: 0.00001280
Iteration 209/1000 | Loss: 0.00001280
Iteration 210/1000 | Loss: 0.00001280
Iteration 211/1000 | Loss: 0.00001280
Iteration 212/1000 | Loss: 0.00001280
Iteration 213/1000 | Loss: 0.00001280
Iteration 214/1000 | Loss: 0.00001280
Iteration 215/1000 | Loss: 0.00001280
Iteration 216/1000 | Loss: 0.00001280
Iteration 217/1000 | Loss: 0.00001280
Iteration 218/1000 | Loss: 0.00001280
Iteration 219/1000 | Loss: 0.00001280
Iteration 220/1000 | Loss: 0.00001280
Iteration 221/1000 | Loss: 0.00001280
Iteration 222/1000 | Loss: 0.00001280
Iteration 223/1000 | Loss: 0.00001280
Iteration 224/1000 | Loss: 0.00001280
Iteration 225/1000 | Loss: 0.00001280
Iteration 226/1000 | Loss: 0.00001279
Iteration 227/1000 | Loss: 0.00001279
Iteration 228/1000 | Loss: 0.00001279
Iteration 229/1000 | Loss: 0.00001279
Iteration 230/1000 | Loss: 0.00001279
Iteration 231/1000 | Loss: 0.00001279
Iteration 232/1000 | Loss: 0.00001279
Iteration 233/1000 | Loss: 0.00001279
Iteration 234/1000 | Loss: 0.00001279
Iteration 235/1000 | Loss: 0.00001279
Iteration 236/1000 | Loss: 0.00001279
Iteration 237/1000 | Loss: 0.00001279
Iteration 238/1000 | Loss: 0.00001279
Iteration 239/1000 | Loss: 0.00001279
Iteration 240/1000 | Loss: 0.00001279
Iteration 241/1000 | Loss: 0.00001279
Iteration 242/1000 | Loss: 0.00001279
Iteration 243/1000 | Loss: 0.00001279
Iteration 244/1000 | Loss: 0.00001279
Iteration 245/1000 | Loss: 0.00001279
Iteration 246/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.2790427717845887e-05, 1.2790427717845887e-05, 1.2790427717845887e-05, 1.2790427717845887e-05, 1.2790427717845887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2790427717845887e-05

Optimization complete. Final v2v error: 2.9313669204711914 mm

Highest mean error: 3.485334634780884 mm for frame 15

Lowest mean error: 2.301912784576416 mm for frame 108

Saving results

Total time: 45.56994342803955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844485
Iteration 2/25 | Loss: 0.00129182
Iteration 3/25 | Loss: 0.00117008
Iteration 4/25 | Loss: 0.00109106
Iteration 5/25 | Loss: 0.00108844
Iteration 6/25 | Loss: 0.00108783
Iteration 7/25 | Loss: 0.00108771
Iteration 8/25 | Loss: 0.00108771
Iteration 9/25 | Loss: 0.00108771
Iteration 10/25 | Loss: 0.00108771
Iteration 11/25 | Loss: 0.00108771
Iteration 12/25 | Loss: 0.00108771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001087706652469933, 0.001087706652469933, 0.001087706652469933, 0.001087706652469933, 0.001087706652469933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001087706652469933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.46994448
Iteration 2/25 | Loss: 0.00087768
Iteration 3/25 | Loss: 0.00087767
Iteration 4/25 | Loss: 0.00087766
Iteration 5/25 | Loss: 0.00087766
Iteration 6/25 | Loss: 0.00087766
Iteration 7/25 | Loss: 0.00087766
Iteration 8/25 | Loss: 0.00087766
Iteration 9/25 | Loss: 0.00087766
Iteration 10/25 | Loss: 0.00087766
Iteration 11/25 | Loss: 0.00087766
Iteration 12/25 | Loss: 0.00087766
Iteration 13/25 | Loss: 0.00087766
Iteration 14/25 | Loss: 0.00087766
Iteration 15/25 | Loss: 0.00087766
Iteration 16/25 | Loss: 0.00087766
Iteration 17/25 | Loss: 0.00087766
Iteration 18/25 | Loss: 0.00087766
Iteration 19/25 | Loss: 0.00087766
Iteration 20/25 | Loss: 0.00087766
Iteration 21/25 | Loss: 0.00087766
Iteration 22/25 | Loss: 0.00087766
Iteration 23/25 | Loss: 0.00087766
Iteration 24/25 | Loss: 0.00087766
Iteration 25/25 | Loss: 0.00087766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087766
Iteration 2/1000 | Loss: 0.00004257
Iteration 3/1000 | Loss: 0.00001328
Iteration 4/1000 | Loss: 0.00001147
Iteration 5/1000 | Loss: 0.00005246
Iteration 6/1000 | Loss: 0.00001054
Iteration 7/1000 | Loss: 0.00001047
Iteration 8/1000 | Loss: 0.00004137
Iteration 9/1000 | Loss: 0.00001045
Iteration 10/1000 | Loss: 0.00001019
Iteration 11/1000 | Loss: 0.00001000
Iteration 12/1000 | Loss: 0.00000980
Iteration 13/1000 | Loss: 0.00000979
Iteration 14/1000 | Loss: 0.00000973
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000968
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000959
Iteration 19/1000 | Loss: 0.00000959
Iteration 20/1000 | Loss: 0.00000959
Iteration 21/1000 | Loss: 0.00000958
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000952
Iteration 24/1000 | Loss: 0.00000952
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000951
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000951
Iteration 29/1000 | Loss: 0.00000951
Iteration 30/1000 | Loss: 0.00000951
Iteration 31/1000 | Loss: 0.00000951
Iteration 32/1000 | Loss: 0.00000951
Iteration 33/1000 | Loss: 0.00000951
Iteration 34/1000 | Loss: 0.00000951
Iteration 35/1000 | Loss: 0.00000951
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000951
Iteration 38/1000 | Loss: 0.00000951
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000951
Iteration 45/1000 | Loss: 0.00000951
Iteration 46/1000 | Loss: 0.00000951
Iteration 47/1000 | Loss: 0.00000951
Iteration 48/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [9.509827577858232e-06, 9.509827577858232e-06, 9.509827577858232e-06, 9.509827577858232e-06, 9.509827577858232e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.509827577858232e-06

Optimization complete. Final v2v error: 2.6453311443328857 mm

Highest mean error: 3.249908685684204 mm for frame 92

Lowest mean error: 2.3957386016845703 mm for frame 6

Saving results

Total time: 33.669235706329346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880959
Iteration 2/25 | Loss: 0.00125104
Iteration 3/25 | Loss: 0.00118848
Iteration 4/25 | Loss: 0.00118083
Iteration 5/25 | Loss: 0.00117866
Iteration 6/25 | Loss: 0.00117866
Iteration 7/25 | Loss: 0.00117866
Iteration 8/25 | Loss: 0.00117866
Iteration 9/25 | Loss: 0.00117866
Iteration 10/25 | Loss: 0.00117866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011786557734012604, 0.0011786557734012604, 0.0011786557734012604, 0.0011786557734012604, 0.0011786557734012604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011786557734012604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30339015
Iteration 2/25 | Loss: 0.00092947
Iteration 3/25 | Loss: 0.00092947
Iteration 4/25 | Loss: 0.00092947
Iteration 5/25 | Loss: 0.00092947
Iteration 6/25 | Loss: 0.00092947
Iteration 7/25 | Loss: 0.00092947
Iteration 8/25 | Loss: 0.00092947
Iteration 9/25 | Loss: 0.00092947
Iteration 10/25 | Loss: 0.00092947
Iteration 11/25 | Loss: 0.00092947
Iteration 12/25 | Loss: 0.00092947
Iteration 13/25 | Loss: 0.00092947
Iteration 14/25 | Loss: 0.00092947
Iteration 15/25 | Loss: 0.00092947
Iteration 16/25 | Loss: 0.00092947
Iteration 17/25 | Loss: 0.00092947
Iteration 18/25 | Loss: 0.00092947
Iteration 19/25 | Loss: 0.00092947
Iteration 20/25 | Loss: 0.00092947
Iteration 21/25 | Loss: 0.00092947
Iteration 22/25 | Loss: 0.00092947
Iteration 23/25 | Loss: 0.00092947
Iteration 24/25 | Loss: 0.00092947
Iteration 25/25 | Loss: 0.00092947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092947
Iteration 2/1000 | Loss: 0.00003118
Iteration 3/1000 | Loss: 0.00001802
Iteration 4/1000 | Loss: 0.00001628
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001537
Iteration 7/1000 | Loss: 0.00001513
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001469
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001449
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001442
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001439
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001432
Iteration 27/1000 | Loss: 0.00001431
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001429
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001422
Iteration 34/1000 | Loss: 0.00001418
Iteration 35/1000 | Loss: 0.00001418
Iteration 36/1000 | Loss: 0.00001416
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001409
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001409
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001408
Iteration 67/1000 | Loss: 0.00001408
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001405
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001405
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001402
Iteration 109/1000 | Loss: 0.00001402
Iteration 110/1000 | Loss: 0.00001402
Iteration 111/1000 | Loss: 0.00001402
Iteration 112/1000 | Loss: 0.00001402
Iteration 113/1000 | Loss: 0.00001402
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001401
Iteration 119/1000 | Loss: 0.00001401
Iteration 120/1000 | Loss: 0.00001401
Iteration 121/1000 | Loss: 0.00001401
Iteration 122/1000 | Loss: 0.00001401
Iteration 123/1000 | Loss: 0.00001401
Iteration 124/1000 | Loss: 0.00001401
Iteration 125/1000 | Loss: 0.00001401
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001400
Iteration 129/1000 | Loss: 0.00001400
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001400
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001399
Iteration 136/1000 | Loss: 0.00001399
Iteration 137/1000 | Loss: 0.00001399
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001399
Iteration 147/1000 | Loss: 0.00001399
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001398
Iteration 159/1000 | Loss: 0.00001398
Iteration 160/1000 | Loss: 0.00001398
Iteration 161/1000 | Loss: 0.00001398
Iteration 162/1000 | Loss: 0.00001398
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001397
Iteration 169/1000 | Loss: 0.00001397
Iteration 170/1000 | Loss: 0.00001397
Iteration 171/1000 | Loss: 0.00001397
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001397
Iteration 175/1000 | Loss: 0.00001397
Iteration 176/1000 | Loss: 0.00001397
Iteration 177/1000 | Loss: 0.00001397
Iteration 178/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3971746739116497e-05, 1.3971746739116497e-05, 1.3971746739116497e-05, 1.3971746739116497e-05, 1.3971746739116497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3971746739116497e-05

Optimization complete. Final v2v error: 3.1408560276031494 mm

Highest mean error: 3.311065673828125 mm for frame 44

Lowest mean error: 2.995673418045044 mm for frame 196

Saving results

Total time: 35.404544830322266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750195
Iteration 2/25 | Loss: 0.00166181
Iteration 3/25 | Loss: 0.00129437
Iteration 4/25 | Loss: 0.00116788
Iteration 5/25 | Loss: 0.00116303
Iteration 6/25 | Loss: 0.00113831
Iteration 7/25 | Loss: 0.00111678
Iteration 8/25 | Loss: 0.00110790
Iteration 9/25 | Loss: 0.00110403
Iteration 10/25 | Loss: 0.00110257
Iteration 11/25 | Loss: 0.00110184
Iteration 12/25 | Loss: 0.00110126
Iteration 13/25 | Loss: 0.00110100
Iteration 14/25 | Loss: 0.00110090
Iteration 15/25 | Loss: 0.00110089
Iteration 16/25 | Loss: 0.00110089
Iteration 17/25 | Loss: 0.00110089
Iteration 18/25 | Loss: 0.00110088
Iteration 19/25 | Loss: 0.00110088
Iteration 20/25 | Loss: 0.00110088
Iteration 21/25 | Loss: 0.00110088
Iteration 22/25 | Loss: 0.00110088
Iteration 23/25 | Loss: 0.00110088
Iteration 24/25 | Loss: 0.00110088
Iteration 25/25 | Loss: 0.00110088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.68740654
Iteration 2/25 | Loss: 0.00088209
Iteration 3/25 | Loss: 0.00088207
Iteration 4/25 | Loss: 0.00088207
Iteration 5/25 | Loss: 0.00088207
Iteration 6/25 | Loss: 0.00088207
Iteration 7/25 | Loss: 0.00088207
Iteration 8/25 | Loss: 0.00088207
Iteration 9/25 | Loss: 0.00088207
Iteration 10/25 | Loss: 0.00088207
Iteration 11/25 | Loss: 0.00088207
Iteration 12/25 | Loss: 0.00088207
Iteration 13/25 | Loss: 0.00088207
Iteration 14/25 | Loss: 0.00088207
Iteration 15/25 | Loss: 0.00088207
Iteration 16/25 | Loss: 0.00088207
Iteration 17/25 | Loss: 0.00088207
Iteration 18/25 | Loss: 0.00088207
Iteration 19/25 | Loss: 0.00088207
Iteration 20/25 | Loss: 0.00088207
Iteration 21/25 | Loss: 0.00088207
Iteration 22/25 | Loss: 0.00088207
Iteration 23/25 | Loss: 0.00088207
Iteration 24/25 | Loss: 0.00088207
Iteration 25/25 | Loss: 0.00088207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088207
Iteration 2/1000 | Loss: 0.00002026
Iteration 3/1000 | Loss: 0.00001617
Iteration 4/1000 | Loss: 0.00005015
Iteration 5/1000 | Loss: 0.00002097
Iteration 6/1000 | Loss: 0.00001565
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001369
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00003054
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001229
Iteration 22/1000 | Loss: 0.00001224
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00005063
Iteration 30/1000 | Loss: 0.00009577
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00004348
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001339
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001188
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001184
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001168
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001168
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001165
Iteration 112/1000 | Loss: 0.00001165
Iteration 113/1000 | Loss: 0.00001165
Iteration 114/1000 | Loss: 0.00001165
Iteration 115/1000 | Loss: 0.00001165
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001164
Iteration 121/1000 | Loss: 0.00001164
Iteration 122/1000 | Loss: 0.00001164
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001160
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001155
Iteration 174/1000 | Loss: 0.00001155
Iteration 175/1000 | Loss: 0.00001155
Iteration 176/1000 | Loss: 0.00001155
Iteration 177/1000 | Loss: 0.00001155
Iteration 178/1000 | Loss: 0.00001155
Iteration 179/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1554843695193995e-05, 1.1554843695193995e-05, 1.1554843695193995e-05, 1.1554843695193995e-05, 1.1554843695193995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1554843695193995e-05

Optimization complete. Final v2v error: 2.8977854251861572 mm

Highest mean error: 3.453977346420288 mm for frame 211

Lowest mean error: 2.477703332901001 mm for frame 194

Saving results

Total time: 77.06946086883545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715446
Iteration 2/25 | Loss: 0.00165797
Iteration 3/25 | Loss: 0.00133140
Iteration 4/25 | Loss: 0.00126354
Iteration 5/25 | Loss: 0.00124044
Iteration 6/25 | Loss: 0.00123255
Iteration 7/25 | Loss: 0.00125051
Iteration 8/25 | Loss: 0.00125205
Iteration 9/25 | Loss: 0.00123580
Iteration 10/25 | Loss: 0.00123783
Iteration 11/25 | Loss: 0.00121693
Iteration 12/25 | Loss: 0.00122400
Iteration 13/25 | Loss: 0.00121558
Iteration 14/25 | Loss: 0.00121937
Iteration 15/25 | Loss: 0.00121540
Iteration 16/25 | Loss: 0.00121086
Iteration 17/25 | Loss: 0.00120767
Iteration 18/25 | Loss: 0.00120383
Iteration 19/25 | Loss: 0.00119667
Iteration 20/25 | Loss: 0.00119489
Iteration 21/25 | Loss: 0.00120045
Iteration 22/25 | Loss: 0.00119685
Iteration 23/25 | Loss: 0.00119867
Iteration 24/25 | Loss: 0.00119565
Iteration 25/25 | Loss: 0.00119401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.08661461
Iteration 2/25 | Loss: 0.00194795
Iteration 3/25 | Loss: 0.00194774
Iteration 4/25 | Loss: 0.00194774
Iteration 5/25 | Loss: 0.00194774
Iteration 6/25 | Loss: 0.00194774
Iteration 7/25 | Loss: 0.00194774
Iteration 8/25 | Loss: 0.00194774
Iteration 9/25 | Loss: 0.00194774
Iteration 10/25 | Loss: 0.00194774
Iteration 11/25 | Loss: 0.00194774
Iteration 12/25 | Loss: 0.00194774
Iteration 13/25 | Loss: 0.00194774
Iteration 14/25 | Loss: 0.00194774
Iteration 15/25 | Loss: 0.00194774
Iteration 16/25 | Loss: 0.00194774
Iteration 17/25 | Loss: 0.00194774
Iteration 18/25 | Loss: 0.00194774
Iteration 19/25 | Loss: 0.00194774
Iteration 20/25 | Loss: 0.00194774
Iteration 21/25 | Loss: 0.00194774
Iteration 22/25 | Loss: 0.00194774
Iteration 23/25 | Loss: 0.00194774
Iteration 24/25 | Loss: 0.00194774
Iteration 25/25 | Loss: 0.00194774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194774
Iteration 2/1000 | Loss: 0.00037484
Iteration 3/1000 | Loss: 0.00067930
Iteration 4/1000 | Loss: 0.00056148
Iteration 5/1000 | Loss: 0.00051529
Iteration 6/1000 | Loss: 0.00074194
Iteration 7/1000 | Loss: 0.00088698
Iteration 8/1000 | Loss: 0.00528596
Iteration 9/1000 | Loss: 0.00979335
Iteration 10/1000 | Loss: 0.00450645
Iteration 11/1000 | Loss: 0.00089287
Iteration 12/1000 | Loss: 0.00063382
Iteration 13/1000 | Loss: 0.00074238
Iteration 14/1000 | Loss: 0.00165331
Iteration 15/1000 | Loss: 0.00042504
Iteration 16/1000 | Loss: 0.00135493
Iteration 17/1000 | Loss: 0.00025497
Iteration 18/1000 | Loss: 0.00025169
Iteration 19/1000 | Loss: 0.00027784
Iteration 20/1000 | Loss: 0.00028536
Iteration 21/1000 | Loss: 0.00030427
Iteration 22/1000 | Loss: 0.00029371
Iteration 23/1000 | Loss: 0.00017347
Iteration 24/1000 | Loss: 0.00010416
Iteration 25/1000 | Loss: 0.00028355
Iteration 26/1000 | Loss: 0.00018894
Iteration 27/1000 | Loss: 0.00022386
Iteration 28/1000 | Loss: 0.00017562
Iteration 29/1000 | Loss: 0.00011308
Iteration 30/1000 | Loss: 0.00014821
Iteration 31/1000 | Loss: 0.00021201
Iteration 32/1000 | Loss: 0.00018489
Iteration 33/1000 | Loss: 0.00020785
Iteration 34/1000 | Loss: 0.00017177
Iteration 35/1000 | Loss: 0.00016218
Iteration 36/1000 | Loss: 0.00012964
Iteration 37/1000 | Loss: 0.00019057
Iteration 38/1000 | Loss: 0.00010108
Iteration 39/1000 | Loss: 0.00026365
Iteration 40/1000 | Loss: 0.00028393
Iteration 41/1000 | Loss: 0.00022288
Iteration 42/1000 | Loss: 0.00021818
Iteration 43/1000 | Loss: 0.00023402
Iteration 44/1000 | Loss: 0.00018356
Iteration 45/1000 | Loss: 0.00029307
Iteration 46/1000 | Loss: 0.00013638
Iteration 47/1000 | Loss: 0.00006049
Iteration 48/1000 | Loss: 0.00058554
Iteration 49/1000 | Loss: 0.00025318
Iteration 50/1000 | Loss: 0.00053548
Iteration 51/1000 | Loss: 0.00004340
Iteration 52/1000 | Loss: 0.00003897
Iteration 53/1000 | Loss: 0.00017125
Iteration 54/1000 | Loss: 0.00017111
Iteration 55/1000 | Loss: 0.00015604
Iteration 56/1000 | Loss: 0.00048233
Iteration 57/1000 | Loss: 0.00015207
Iteration 58/1000 | Loss: 0.00003903
Iteration 59/1000 | Loss: 0.00019359
Iteration 60/1000 | Loss: 0.00015059
Iteration 61/1000 | Loss: 0.00003333
Iteration 62/1000 | Loss: 0.00003629
Iteration 63/1000 | Loss: 0.00003016
Iteration 64/1000 | Loss: 0.00002844
Iteration 65/1000 | Loss: 0.00002695
Iteration 66/1000 | Loss: 0.00014251
Iteration 67/1000 | Loss: 0.00002810
Iteration 68/1000 | Loss: 0.00002515
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002274
Iteration 71/1000 | Loss: 0.00002223
Iteration 72/1000 | Loss: 0.00002186
Iteration 73/1000 | Loss: 0.00002166
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002143
Iteration 76/1000 | Loss: 0.00002127
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002104
Iteration 80/1000 | Loss: 0.00002097
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002095
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002091
Iteration 86/1000 | Loss: 0.00002090
Iteration 87/1000 | Loss: 0.00002089
Iteration 88/1000 | Loss: 0.00002088
Iteration 89/1000 | Loss: 0.00002087
Iteration 90/1000 | Loss: 0.00002085
Iteration 91/1000 | Loss: 0.00002085
Iteration 92/1000 | Loss: 0.00002085
Iteration 93/1000 | Loss: 0.00002085
Iteration 94/1000 | Loss: 0.00002085
Iteration 95/1000 | Loss: 0.00002085
Iteration 96/1000 | Loss: 0.00002085
Iteration 97/1000 | Loss: 0.00002085
Iteration 98/1000 | Loss: 0.00002084
Iteration 99/1000 | Loss: 0.00002084
Iteration 100/1000 | Loss: 0.00002082
Iteration 101/1000 | Loss: 0.00002082
Iteration 102/1000 | Loss: 0.00002082
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002075
Iteration 106/1000 | Loss: 0.00002075
Iteration 107/1000 | Loss: 0.00002075
Iteration 108/1000 | Loss: 0.00002074
Iteration 109/1000 | Loss: 0.00002074
Iteration 110/1000 | Loss: 0.00002073
Iteration 111/1000 | Loss: 0.00002073
Iteration 112/1000 | Loss: 0.00002072
Iteration 113/1000 | Loss: 0.00002072
Iteration 114/1000 | Loss: 0.00002071
Iteration 115/1000 | Loss: 0.00002071
Iteration 116/1000 | Loss: 0.00002071
Iteration 117/1000 | Loss: 0.00002070
Iteration 118/1000 | Loss: 0.00002069
Iteration 119/1000 | Loss: 0.00002069
Iteration 120/1000 | Loss: 0.00002069
Iteration 121/1000 | Loss: 0.00002068
Iteration 122/1000 | Loss: 0.00002068
Iteration 123/1000 | Loss: 0.00002068
Iteration 124/1000 | Loss: 0.00002067
Iteration 125/1000 | Loss: 0.00002067
Iteration 126/1000 | Loss: 0.00002067
Iteration 127/1000 | Loss: 0.00002067
Iteration 128/1000 | Loss: 0.00002066
Iteration 129/1000 | Loss: 0.00002066
Iteration 130/1000 | Loss: 0.00002066
Iteration 131/1000 | Loss: 0.00002065
Iteration 132/1000 | Loss: 0.00002065
Iteration 133/1000 | Loss: 0.00002065
Iteration 134/1000 | Loss: 0.00002064
Iteration 135/1000 | Loss: 0.00002064
Iteration 136/1000 | Loss: 0.00002064
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002064
Iteration 139/1000 | Loss: 0.00002063
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00002063
Iteration 142/1000 | Loss: 0.00002063
Iteration 143/1000 | Loss: 0.00002063
Iteration 144/1000 | Loss: 0.00002063
Iteration 145/1000 | Loss: 0.00002063
Iteration 146/1000 | Loss: 0.00002063
Iteration 147/1000 | Loss: 0.00002063
Iteration 148/1000 | Loss: 0.00002063
Iteration 149/1000 | Loss: 0.00002062
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002062
Iteration 152/1000 | Loss: 0.00002062
Iteration 153/1000 | Loss: 0.00002062
Iteration 154/1000 | Loss: 0.00002062
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002062
Iteration 157/1000 | Loss: 0.00002061
Iteration 158/1000 | Loss: 0.00002061
Iteration 159/1000 | Loss: 0.00002061
Iteration 160/1000 | Loss: 0.00002061
Iteration 161/1000 | Loss: 0.00002061
Iteration 162/1000 | Loss: 0.00002060
Iteration 163/1000 | Loss: 0.00002060
Iteration 164/1000 | Loss: 0.00002060
Iteration 165/1000 | Loss: 0.00002060
Iteration 166/1000 | Loss: 0.00002060
Iteration 167/1000 | Loss: 0.00002060
Iteration 168/1000 | Loss: 0.00002060
Iteration 169/1000 | Loss: 0.00002060
Iteration 170/1000 | Loss: 0.00002059
Iteration 171/1000 | Loss: 0.00002059
Iteration 172/1000 | Loss: 0.00002059
Iteration 173/1000 | Loss: 0.00002059
Iteration 174/1000 | Loss: 0.00002059
Iteration 175/1000 | Loss: 0.00002059
Iteration 176/1000 | Loss: 0.00002059
Iteration 177/1000 | Loss: 0.00002058
Iteration 178/1000 | Loss: 0.00002058
Iteration 179/1000 | Loss: 0.00002058
Iteration 180/1000 | Loss: 0.00002058
Iteration 181/1000 | Loss: 0.00002058
Iteration 182/1000 | Loss: 0.00002058
Iteration 183/1000 | Loss: 0.00002058
Iteration 184/1000 | Loss: 0.00002057
Iteration 185/1000 | Loss: 0.00002057
Iteration 186/1000 | Loss: 0.00002057
Iteration 187/1000 | Loss: 0.00002057
Iteration 188/1000 | Loss: 0.00002057
Iteration 189/1000 | Loss: 0.00002057
Iteration 190/1000 | Loss: 0.00002056
Iteration 191/1000 | Loss: 0.00002056
Iteration 192/1000 | Loss: 0.00002056
Iteration 193/1000 | Loss: 0.00002056
Iteration 194/1000 | Loss: 0.00002056
Iteration 195/1000 | Loss: 0.00002056
Iteration 196/1000 | Loss: 0.00002056
Iteration 197/1000 | Loss: 0.00002056
Iteration 198/1000 | Loss: 0.00002056
Iteration 199/1000 | Loss: 0.00002056
Iteration 200/1000 | Loss: 0.00002055
Iteration 201/1000 | Loss: 0.00002055
Iteration 202/1000 | Loss: 0.00002055
Iteration 203/1000 | Loss: 0.00002055
Iteration 204/1000 | Loss: 0.00002055
Iteration 205/1000 | Loss: 0.00002055
Iteration 206/1000 | Loss: 0.00002055
Iteration 207/1000 | Loss: 0.00002055
Iteration 208/1000 | Loss: 0.00002055
Iteration 209/1000 | Loss: 0.00002055
Iteration 210/1000 | Loss: 0.00002055
Iteration 211/1000 | Loss: 0.00002055
Iteration 212/1000 | Loss: 0.00002055
Iteration 213/1000 | Loss: 0.00002054
Iteration 214/1000 | Loss: 0.00002054
Iteration 215/1000 | Loss: 0.00002054
Iteration 216/1000 | Loss: 0.00002054
Iteration 217/1000 | Loss: 0.00002054
Iteration 218/1000 | Loss: 0.00002054
Iteration 219/1000 | Loss: 0.00002054
Iteration 220/1000 | Loss: 0.00002054
Iteration 221/1000 | Loss: 0.00002054
Iteration 222/1000 | Loss: 0.00002054
Iteration 223/1000 | Loss: 0.00002054
Iteration 224/1000 | Loss: 0.00002054
Iteration 225/1000 | Loss: 0.00002054
Iteration 226/1000 | Loss: 0.00002054
Iteration 227/1000 | Loss: 0.00002054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.054290052910801e-05, 2.054290052910801e-05, 2.054290052910801e-05, 2.054290052910801e-05, 2.054290052910801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.054290052910801e-05

Optimization complete. Final v2v error: 3.629711389541626 mm

Highest mean error: 5.833021640777588 mm for frame 75

Lowest mean error: 2.626039981842041 mm for frame 15

Saving results

Total time: 164.14066791534424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00543936
Iteration 2/25 | Loss: 0.00138951
Iteration 3/25 | Loss: 0.00117516
Iteration 4/25 | Loss: 0.00115731
Iteration 5/25 | Loss: 0.00115431
Iteration 6/25 | Loss: 0.00115421
Iteration 7/25 | Loss: 0.00115421
Iteration 8/25 | Loss: 0.00115421
Iteration 9/25 | Loss: 0.00115421
Iteration 10/25 | Loss: 0.00115421
Iteration 11/25 | Loss: 0.00115421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011542149586603045, 0.0011542149586603045, 0.0011542149586603045, 0.0011542149586603045, 0.0011542149586603045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011542149586603045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11705196
Iteration 2/25 | Loss: 0.00057740
Iteration 3/25 | Loss: 0.00057738
Iteration 4/25 | Loss: 0.00057738
Iteration 5/25 | Loss: 0.00057738
Iteration 6/25 | Loss: 0.00057738
Iteration 7/25 | Loss: 0.00057738
Iteration 8/25 | Loss: 0.00057738
Iteration 9/25 | Loss: 0.00057738
Iteration 10/25 | Loss: 0.00057738
Iteration 11/25 | Loss: 0.00057738
Iteration 12/25 | Loss: 0.00057738
Iteration 13/25 | Loss: 0.00057738
Iteration 14/25 | Loss: 0.00057738
Iteration 15/25 | Loss: 0.00057738
Iteration 16/25 | Loss: 0.00057738
Iteration 17/25 | Loss: 0.00057738
Iteration 18/25 | Loss: 0.00057738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005773750599473715, 0.0005773750599473715, 0.0005773750599473715, 0.0005773750599473715, 0.0005773750599473715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005773750599473715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057738
Iteration 2/1000 | Loss: 0.00002976
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001824
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001695
Iteration 10/1000 | Loss: 0.00001661
Iteration 11/1000 | Loss: 0.00001640
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001608
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001604
Iteration 20/1000 | Loss: 0.00001604
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001595
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001594
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001592
Iteration 32/1000 | Loss: 0.00001591
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001590
Iteration 36/1000 | Loss: 0.00001590
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001587
Iteration 43/1000 | Loss: 0.00001587
Iteration 44/1000 | Loss: 0.00001587
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001586
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001584
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001584
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001584
Iteration 68/1000 | Loss: 0.00001584
Iteration 69/1000 | Loss: 0.00001584
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001580
Iteration 82/1000 | Loss: 0.00001579
Iteration 83/1000 | Loss: 0.00001579
Iteration 84/1000 | Loss: 0.00001579
Iteration 85/1000 | Loss: 0.00001579
Iteration 86/1000 | Loss: 0.00001578
Iteration 87/1000 | Loss: 0.00001578
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001577
Iteration 91/1000 | Loss: 0.00001577
Iteration 92/1000 | Loss: 0.00001577
Iteration 93/1000 | Loss: 0.00001577
Iteration 94/1000 | Loss: 0.00001577
Iteration 95/1000 | Loss: 0.00001577
Iteration 96/1000 | Loss: 0.00001577
Iteration 97/1000 | Loss: 0.00001577
Iteration 98/1000 | Loss: 0.00001576
Iteration 99/1000 | Loss: 0.00001576
Iteration 100/1000 | Loss: 0.00001576
Iteration 101/1000 | Loss: 0.00001576
Iteration 102/1000 | Loss: 0.00001576
Iteration 103/1000 | Loss: 0.00001576
Iteration 104/1000 | Loss: 0.00001576
Iteration 105/1000 | Loss: 0.00001576
Iteration 106/1000 | Loss: 0.00001576
Iteration 107/1000 | Loss: 0.00001576
Iteration 108/1000 | Loss: 0.00001575
Iteration 109/1000 | Loss: 0.00001575
Iteration 110/1000 | Loss: 0.00001575
Iteration 111/1000 | Loss: 0.00001575
Iteration 112/1000 | Loss: 0.00001574
Iteration 113/1000 | Loss: 0.00001574
Iteration 114/1000 | Loss: 0.00001574
Iteration 115/1000 | Loss: 0.00001574
Iteration 116/1000 | Loss: 0.00001574
Iteration 117/1000 | Loss: 0.00001574
Iteration 118/1000 | Loss: 0.00001574
Iteration 119/1000 | Loss: 0.00001574
Iteration 120/1000 | Loss: 0.00001573
Iteration 121/1000 | Loss: 0.00001573
Iteration 122/1000 | Loss: 0.00001573
Iteration 123/1000 | Loss: 0.00001573
Iteration 124/1000 | Loss: 0.00001572
Iteration 125/1000 | Loss: 0.00001572
Iteration 126/1000 | Loss: 0.00001572
Iteration 127/1000 | Loss: 0.00001572
Iteration 128/1000 | Loss: 0.00001572
Iteration 129/1000 | Loss: 0.00001571
Iteration 130/1000 | Loss: 0.00001571
Iteration 131/1000 | Loss: 0.00001571
Iteration 132/1000 | Loss: 0.00001571
Iteration 133/1000 | Loss: 0.00001571
Iteration 134/1000 | Loss: 0.00001570
Iteration 135/1000 | Loss: 0.00001570
Iteration 136/1000 | Loss: 0.00001569
Iteration 137/1000 | Loss: 0.00001569
Iteration 138/1000 | Loss: 0.00001568
Iteration 139/1000 | Loss: 0.00001568
Iteration 140/1000 | Loss: 0.00001568
Iteration 141/1000 | Loss: 0.00001568
Iteration 142/1000 | Loss: 0.00001568
Iteration 143/1000 | Loss: 0.00001568
Iteration 144/1000 | Loss: 0.00001568
Iteration 145/1000 | Loss: 0.00001568
Iteration 146/1000 | Loss: 0.00001568
Iteration 147/1000 | Loss: 0.00001568
Iteration 148/1000 | Loss: 0.00001567
Iteration 149/1000 | Loss: 0.00001567
Iteration 150/1000 | Loss: 0.00001567
Iteration 151/1000 | Loss: 0.00001567
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001566
Iteration 157/1000 | Loss: 0.00001566
Iteration 158/1000 | Loss: 0.00001565
Iteration 159/1000 | Loss: 0.00001565
Iteration 160/1000 | Loss: 0.00001564
Iteration 161/1000 | Loss: 0.00001564
Iteration 162/1000 | Loss: 0.00001564
Iteration 163/1000 | Loss: 0.00001564
Iteration 164/1000 | Loss: 0.00001564
Iteration 165/1000 | Loss: 0.00001564
Iteration 166/1000 | Loss: 0.00001564
Iteration 167/1000 | Loss: 0.00001564
Iteration 168/1000 | Loss: 0.00001564
Iteration 169/1000 | Loss: 0.00001564
Iteration 170/1000 | Loss: 0.00001564
Iteration 171/1000 | Loss: 0.00001564
Iteration 172/1000 | Loss: 0.00001564
Iteration 173/1000 | Loss: 0.00001564
Iteration 174/1000 | Loss: 0.00001563
Iteration 175/1000 | Loss: 0.00001563
Iteration 176/1000 | Loss: 0.00001563
Iteration 177/1000 | Loss: 0.00001562
Iteration 178/1000 | Loss: 0.00001562
Iteration 179/1000 | Loss: 0.00001562
Iteration 180/1000 | Loss: 0.00001562
Iteration 181/1000 | Loss: 0.00001562
Iteration 182/1000 | Loss: 0.00001562
Iteration 183/1000 | Loss: 0.00001562
Iteration 184/1000 | Loss: 0.00001562
Iteration 185/1000 | Loss: 0.00001562
Iteration 186/1000 | Loss: 0.00001562
Iteration 187/1000 | Loss: 0.00001562
Iteration 188/1000 | Loss: 0.00001562
Iteration 189/1000 | Loss: 0.00001562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.5617039025528356e-05, 1.5617039025528356e-05, 1.5617039025528356e-05, 1.5617039025528356e-05, 1.5617039025528356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5617039025528356e-05

Optimization complete. Final v2v error: 3.350537061691284 mm

Highest mean error: 3.9491312503814697 mm for frame 50

Lowest mean error: 2.932187557220459 mm for frame 74

Saving results

Total time: 45.02387070655823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808367
Iteration 2/25 | Loss: 0.00156186
Iteration 3/25 | Loss: 0.00123508
Iteration 4/25 | Loss: 0.00120542
Iteration 5/25 | Loss: 0.00120063
Iteration 6/25 | Loss: 0.00119965
Iteration 7/25 | Loss: 0.00119965
Iteration 8/25 | Loss: 0.00119965
Iteration 9/25 | Loss: 0.00119965
Iteration 10/25 | Loss: 0.00119965
Iteration 11/25 | Loss: 0.00119965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011996501125395298, 0.0011996501125395298, 0.0011996501125395298, 0.0011996501125395298, 0.0011996501125395298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011996501125395298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35405469
Iteration 2/25 | Loss: 0.00037351
Iteration 3/25 | Loss: 0.00037351
Iteration 4/25 | Loss: 0.00037351
Iteration 5/25 | Loss: 0.00037351
Iteration 6/25 | Loss: 0.00037350
Iteration 7/25 | Loss: 0.00037350
Iteration 8/25 | Loss: 0.00037350
Iteration 9/25 | Loss: 0.00037350
Iteration 10/25 | Loss: 0.00037350
Iteration 11/25 | Loss: 0.00037350
Iteration 12/25 | Loss: 0.00037350
Iteration 13/25 | Loss: 0.00037350
Iteration 14/25 | Loss: 0.00037350
Iteration 15/25 | Loss: 0.00037350
Iteration 16/25 | Loss: 0.00037350
Iteration 17/25 | Loss: 0.00037350
Iteration 18/25 | Loss: 0.00037350
Iteration 19/25 | Loss: 0.00037350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003735036007128656, 0.0003735036007128656, 0.0003735036007128656, 0.0003735036007128656, 0.0003735036007128656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003735036007128656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037350
Iteration 2/1000 | Loss: 0.00004690
Iteration 3/1000 | Loss: 0.00004045
Iteration 4/1000 | Loss: 0.00003860
Iteration 5/1000 | Loss: 0.00003757
Iteration 6/1000 | Loss: 0.00003659
Iteration 7/1000 | Loss: 0.00003577
Iteration 8/1000 | Loss: 0.00003525
Iteration 9/1000 | Loss: 0.00003488
Iteration 10/1000 | Loss: 0.00003468
Iteration 11/1000 | Loss: 0.00003448
Iteration 12/1000 | Loss: 0.00003438
Iteration 13/1000 | Loss: 0.00003428
Iteration 14/1000 | Loss: 0.00003425
Iteration 15/1000 | Loss: 0.00003419
Iteration 16/1000 | Loss: 0.00003418
Iteration 17/1000 | Loss: 0.00003416
Iteration 18/1000 | Loss: 0.00003416
Iteration 19/1000 | Loss: 0.00003416
Iteration 20/1000 | Loss: 0.00003416
Iteration 21/1000 | Loss: 0.00003415
Iteration 22/1000 | Loss: 0.00003415
Iteration 23/1000 | Loss: 0.00003415
Iteration 24/1000 | Loss: 0.00003415
Iteration 25/1000 | Loss: 0.00003415
Iteration 26/1000 | Loss: 0.00003415
Iteration 27/1000 | Loss: 0.00003415
Iteration 28/1000 | Loss: 0.00003415
Iteration 29/1000 | Loss: 0.00003415
Iteration 30/1000 | Loss: 0.00003414
Iteration 31/1000 | Loss: 0.00003413
Iteration 32/1000 | Loss: 0.00003413
Iteration 33/1000 | Loss: 0.00003413
Iteration 34/1000 | Loss: 0.00003413
Iteration 35/1000 | Loss: 0.00003413
Iteration 36/1000 | Loss: 0.00003413
Iteration 37/1000 | Loss: 0.00003413
Iteration 38/1000 | Loss: 0.00003412
Iteration 39/1000 | Loss: 0.00003411
Iteration 40/1000 | Loss: 0.00003411
Iteration 41/1000 | Loss: 0.00003411
Iteration 42/1000 | Loss: 0.00003411
Iteration 43/1000 | Loss: 0.00003411
Iteration 44/1000 | Loss: 0.00003411
Iteration 45/1000 | Loss: 0.00003411
Iteration 46/1000 | Loss: 0.00003410
Iteration 47/1000 | Loss: 0.00003410
Iteration 48/1000 | Loss: 0.00003410
Iteration 49/1000 | Loss: 0.00003410
Iteration 50/1000 | Loss: 0.00003410
Iteration 51/1000 | Loss: 0.00003409
Iteration 52/1000 | Loss: 0.00003409
Iteration 53/1000 | Loss: 0.00003409
Iteration 54/1000 | Loss: 0.00003409
Iteration 55/1000 | Loss: 0.00003409
Iteration 56/1000 | Loss: 0.00003409
Iteration 57/1000 | Loss: 0.00003409
Iteration 58/1000 | Loss: 0.00003409
Iteration 59/1000 | Loss: 0.00003408
Iteration 60/1000 | Loss: 0.00003408
Iteration 61/1000 | Loss: 0.00003408
Iteration 62/1000 | Loss: 0.00003408
Iteration 63/1000 | Loss: 0.00003408
Iteration 64/1000 | Loss: 0.00003408
Iteration 65/1000 | Loss: 0.00003408
Iteration 66/1000 | Loss: 0.00003408
Iteration 67/1000 | Loss: 0.00003408
Iteration 68/1000 | Loss: 0.00003408
Iteration 69/1000 | Loss: 0.00003408
Iteration 70/1000 | Loss: 0.00003408
Iteration 71/1000 | Loss: 0.00003408
Iteration 72/1000 | Loss: 0.00003408
Iteration 73/1000 | Loss: 0.00003408
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00003408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [3.4075095754815266e-05, 3.4075095754815266e-05, 3.4075095754815266e-05, 3.4075095754815266e-05, 3.4075095754815266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4075095754815266e-05

Optimization complete. Final v2v error: 4.7630414962768555 mm

Highest mean error: 5.090824127197266 mm for frame 42

Lowest mean error: 4.214343547821045 mm for frame 120

Saving results

Total time: 30.235261917114258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676518
Iteration 2/25 | Loss: 0.00120089
Iteration 3/25 | Loss: 0.00108774
Iteration 4/25 | Loss: 0.00107386
Iteration 5/25 | Loss: 0.00106946
Iteration 6/25 | Loss: 0.00106840
Iteration 7/25 | Loss: 0.00106830
Iteration 8/25 | Loss: 0.00106830
Iteration 9/25 | Loss: 0.00106830
Iteration 10/25 | Loss: 0.00106830
Iteration 11/25 | Loss: 0.00106830
Iteration 12/25 | Loss: 0.00106830
Iteration 13/25 | Loss: 0.00106830
Iteration 14/25 | Loss: 0.00106830
Iteration 15/25 | Loss: 0.00106830
Iteration 16/25 | Loss: 0.00106830
Iteration 17/25 | Loss: 0.00106830
Iteration 18/25 | Loss: 0.00106830
Iteration 19/25 | Loss: 0.00106830
Iteration 20/25 | Loss: 0.00106830
Iteration 21/25 | Loss: 0.00106830
Iteration 22/25 | Loss: 0.00106830
Iteration 23/25 | Loss: 0.00106830
Iteration 24/25 | Loss: 0.00106830
Iteration 25/25 | Loss: 0.00106830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43348670
Iteration 2/25 | Loss: 0.00082655
Iteration 3/25 | Loss: 0.00082655
Iteration 4/25 | Loss: 0.00082655
Iteration 5/25 | Loss: 0.00082655
Iteration 6/25 | Loss: 0.00082655
Iteration 7/25 | Loss: 0.00082655
Iteration 8/25 | Loss: 0.00082655
Iteration 9/25 | Loss: 0.00082655
Iteration 10/25 | Loss: 0.00082655
Iteration 11/25 | Loss: 0.00082655
Iteration 12/25 | Loss: 0.00082655
Iteration 13/25 | Loss: 0.00082655
Iteration 14/25 | Loss: 0.00082655
Iteration 15/25 | Loss: 0.00082655
Iteration 16/25 | Loss: 0.00082655
Iteration 17/25 | Loss: 0.00082655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008265489595942199, 0.0008265489595942199, 0.0008265489595942199, 0.0008265489595942199, 0.0008265489595942199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008265489595942199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082655
Iteration 2/1000 | Loss: 0.00002369
Iteration 3/1000 | Loss: 0.00001450
Iteration 4/1000 | Loss: 0.00001166
Iteration 5/1000 | Loss: 0.00001105
Iteration 6/1000 | Loss: 0.00001053
Iteration 7/1000 | Loss: 0.00001052
Iteration 8/1000 | Loss: 0.00001032
Iteration 9/1000 | Loss: 0.00001006
Iteration 10/1000 | Loss: 0.00000997
Iteration 11/1000 | Loss: 0.00000989
Iteration 12/1000 | Loss: 0.00000985
Iteration 13/1000 | Loss: 0.00000984
Iteration 14/1000 | Loss: 0.00000983
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000967
Iteration 17/1000 | Loss: 0.00000953
Iteration 18/1000 | Loss: 0.00000947
Iteration 19/1000 | Loss: 0.00000938
Iteration 20/1000 | Loss: 0.00000937
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000929
Iteration 25/1000 | Loss: 0.00000929
Iteration 26/1000 | Loss: 0.00000929
Iteration 27/1000 | Loss: 0.00000928
Iteration 28/1000 | Loss: 0.00000928
Iteration 29/1000 | Loss: 0.00000927
Iteration 30/1000 | Loss: 0.00000927
Iteration 31/1000 | Loss: 0.00000927
Iteration 32/1000 | Loss: 0.00000927
Iteration 33/1000 | Loss: 0.00000925
Iteration 34/1000 | Loss: 0.00000924
Iteration 35/1000 | Loss: 0.00000924
Iteration 36/1000 | Loss: 0.00000924
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000924
Iteration 41/1000 | Loss: 0.00000924
Iteration 42/1000 | Loss: 0.00000923
Iteration 43/1000 | Loss: 0.00000921
Iteration 44/1000 | Loss: 0.00000921
Iteration 45/1000 | Loss: 0.00000921
Iteration 46/1000 | Loss: 0.00000920
Iteration 47/1000 | Loss: 0.00000920
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000919
Iteration 50/1000 | Loss: 0.00000918
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000918
Iteration 53/1000 | Loss: 0.00000917
Iteration 54/1000 | Loss: 0.00000917
Iteration 55/1000 | Loss: 0.00000917
Iteration 56/1000 | Loss: 0.00000916
Iteration 57/1000 | Loss: 0.00000916
Iteration 58/1000 | Loss: 0.00000915
Iteration 59/1000 | Loss: 0.00000915
Iteration 60/1000 | Loss: 0.00000915
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000915
Iteration 63/1000 | Loss: 0.00000915
Iteration 64/1000 | Loss: 0.00000915
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000915
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000914
Iteration 71/1000 | Loss: 0.00000914
Iteration 72/1000 | Loss: 0.00000913
Iteration 73/1000 | Loss: 0.00000913
Iteration 74/1000 | Loss: 0.00000913
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000913
Iteration 79/1000 | Loss: 0.00000913
Iteration 80/1000 | Loss: 0.00000912
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000912
Iteration 83/1000 | Loss: 0.00000912
Iteration 84/1000 | Loss: 0.00000912
Iteration 85/1000 | Loss: 0.00000912
Iteration 86/1000 | Loss: 0.00000912
Iteration 87/1000 | Loss: 0.00000911
Iteration 88/1000 | Loss: 0.00000911
Iteration 89/1000 | Loss: 0.00000911
Iteration 90/1000 | Loss: 0.00000911
Iteration 91/1000 | Loss: 0.00000911
Iteration 92/1000 | Loss: 0.00000910
Iteration 93/1000 | Loss: 0.00000910
Iteration 94/1000 | Loss: 0.00000910
Iteration 95/1000 | Loss: 0.00000910
Iteration 96/1000 | Loss: 0.00000910
Iteration 97/1000 | Loss: 0.00000910
Iteration 98/1000 | Loss: 0.00000910
Iteration 99/1000 | Loss: 0.00000910
Iteration 100/1000 | Loss: 0.00000910
Iteration 101/1000 | Loss: 0.00000910
Iteration 102/1000 | Loss: 0.00000910
Iteration 103/1000 | Loss: 0.00000910
Iteration 104/1000 | Loss: 0.00000909
Iteration 105/1000 | Loss: 0.00000909
Iteration 106/1000 | Loss: 0.00000909
Iteration 107/1000 | Loss: 0.00000909
Iteration 108/1000 | Loss: 0.00000909
Iteration 109/1000 | Loss: 0.00000909
Iteration 110/1000 | Loss: 0.00000909
Iteration 111/1000 | Loss: 0.00000909
Iteration 112/1000 | Loss: 0.00000908
Iteration 113/1000 | Loss: 0.00000908
Iteration 114/1000 | Loss: 0.00000908
Iteration 115/1000 | Loss: 0.00000907
Iteration 116/1000 | Loss: 0.00000907
Iteration 117/1000 | Loss: 0.00000907
Iteration 118/1000 | Loss: 0.00000907
Iteration 119/1000 | Loss: 0.00000907
Iteration 120/1000 | Loss: 0.00000907
Iteration 121/1000 | Loss: 0.00000907
Iteration 122/1000 | Loss: 0.00000907
Iteration 123/1000 | Loss: 0.00000906
Iteration 124/1000 | Loss: 0.00000906
Iteration 125/1000 | Loss: 0.00000906
Iteration 126/1000 | Loss: 0.00000906
Iteration 127/1000 | Loss: 0.00000906
Iteration 128/1000 | Loss: 0.00000906
Iteration 129/1000 | Loss: 0.00000905
Iteration 130/1000 | Loss: 0.00000905
Iteration 131/1000 | Loss: 0.00000905
Iteration 132/1000 | Loss: 0.00000905
Iteration 133/1000 | Loss: 0.00000905
Iteration 134/1000 | Loss: 0.00000905
Iteration 135/1000 | Loss: 0.00000905
Iteration 136/1000 | Loss: 0.00000905
Iteration 137/1000 | Loss: 0.00000905
Iteration 138/1000 | Loss: 0.00000905
Iteration 139/1000 | Loss: 0.00000904
Iteration 140/1000 | Loss: 0.00000904
Iteration 141/1000 | Loss: 0.00000904
Iteration 142/1000 | Loss: 0.00000904
Iteration 143/1000 | Loss: 0.00000904
Iteration 144/1000 | Loss: 0.00000904
Iteration 145/1000 | Loss: 0.00000904
Iteration 146/1000 | Loss: 0.00000904
Iteration 147/1000 | Loss: 0.00000904
Iteration 148/1000 | Loss: 0.00000904
Iteration 149/1000 | Loss: 0.00000903
Iteration 150/1000 | Loss: 0.00000903
Iteration 151/1000 | Loss: 0.00000903
Iteration 152/1000 | Loss: 0.00000903
Iteration 153/1000 | Loss: 0.00000902
Iteration 154/1000 | Loss: 0.00000902
Iteration 155/1000 | Loss: 0.00000902
Iteration 156/1000 | Loss: 0.00000902
Iteration 157/1000 | Loss: 0.00000902
Iteration 158/1000 | Loss: 0.00000902
Iteration 159/1000 | Loss: 0.00000902
Iteration 160/1000 | Loss: 0.00000902
Iteration 161/1000 | Loss: 0.00000902
Iteration 162/1000 | Loss: 0.00000902
Iteration 163/1000 | Loss: 0.00000902
Iteration 164/1000 | Loss: 0.00000901
Iteration 165/1000 | Loss: 0.00000901
Iteration 166/1000 | Loss: 0.00000901
Iteration 167/1000 | Loss: 0.00000901
Iteration 168/1000 | Loss: 0.00000901
Iteration 169/1000 | Loss: 0.00000901
Iteration 170/1000 | Loss: 0.00000901
Iteration 171/1000 | Loss: 0.00000901
Iteration 172/1000 | Loss: 0.00000901
Iteration 173/1000 | Loss: 0.00000900
Iteration 174/1000 | Loss: 0.00000900
Iteration 175/1000 | Loss: 0.00000900
Iteration 176/1000 | Loss: 0.00000900
Iteration 177/1000 | Loss: 0.00000900
Iteration 178/1000 | Loss: 0.00000900
Iteration 179/1000 | Loss: 0.00000900
Iteration 180/1000 | Loss: 0.00000899
Iteration 181/1000 | Loss: 0.00000899
Iteration 182/1000 | Loss: 0.00000899
Iteration 183/1000 | Loss: 0.00000899
Iteration 184/1000 | Loss: 0.00000899
Iteration 185/1000 | Loss: 0.00000899
Iteration 186/1000 | Loss: 0.00000899
Iteration 187/1000 | Loss: 0.00000899
Iteration 188/1000 | Loss: 0.00000899
Iteration 189/1000 | Loss: 0.00000899
Iteration 190/1000 | Loss: 0.00000899
Iteration 191/1000 | Loss: 0.00000899
Iteration 192/1000 | Loss: 0.00000899
Iteration 193/1000 | Loss: 0.00000899
Iteration 194/1000 | Loss: 0.00000899
Iteration 195/1000 | Loss: 0.00000899
Iteration 196/1000 | Loss: 0.00000899
Iteration 197/1000 | Loss: 0.00000899
Iteration 198/1000 | Loss: 0.00000899
Iteration 199/1000 | Loss: 0.00000899
Iteration 200/1000 | Loss: 0.00000899
Iteration 201/1000 | Loss: 0.00000899
Iteration 202/1000 | Loss: 0.00000899
Iteration 203/1000 | Loss: 0.00000899
Iteration 204/1000 | Loss: 0.00000899
Iteration 205/1000 | Loss: 0.00000899
Iteration 206/1000 | Loss: 0.00000899
Iteration 207/1000 | Loss: 0.00000899
Iteration 208/1000 | Loss: 0.00000899
Iteration 209/1000 | Loss: 0.00000899
Iteration 210/1000 | Loss: 0.00000899
Iteration 211/1000 | Loss: 0.00000899
Iteration 212/1000 | Loss: 0.00000899
Iteration 213/1000 | Loss: 0.00000899
Iteration 214/1000 | Loss: 0.00000899
Iteration 215/1000 | Loss: 0.00000899
Iteration 216/1000 | Loss: 0.00000899
Iteration 217/1000 | Loss: 0.00000899
Iteration 218/1000 | Loss: 0.00000899
Iteration 219/1000 | Loss: 0.00000899
Iteration 220/1000 | Loss: 0.00000899
Iteration 221/1000 | Loss: 0.00000899
Iteration 222/1000 | Loss: 0.00000899
Iteration 223/1000 | Loss: 0.00000899
Iteration 224/1000 | Loss: 0.00000899
Iteration 225/1000 | Loss: 0.00000899
Iteration 226/1000 | Loss: 0.00000899
Iteration 227/1000 | Loss: 0.00000899
Iteration 228/1000 | Loss: 0.00000899
Iteration 229/1000 | Loss: 0.00000899
Iteration 230/1000 | Loss: 0.00000899
Iteration 231/1000 | Loss: 0.00000899
Iteration 232/1000 | Loss: 0.00000899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [8.987496585177723e-06, 8.987496585177723e-06, 8.987496585177723e-06, 8.987496585177723e-06, 8.987496585177723e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.987496585177723e-06

Optimization complete. Final v2v error: 2.54338002204895 mm

Highest mean error: 3.3764891624450684 mm for frame 73

Lowest mean error: 2.299872875213623 mm for frame 0

Saving results

Total time: 40.83738851547241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028109
Iteration 2/25 | Loss: 0.00202394
Iteration 3/25 | Loss: 0.00143282
Iteration 4/25 | Loss: 0.00140628
Iteration 5/25 | Loss: 0.00140234
Iteration 6/25 | Loss: 0.00140112
Iteration 7/25 | Loss: 0.00140112
Iteration 8/25 | Loss: 0.00140112
Iteration 9/25 | Loss: 0.00140112
Iteration 10/25 | Loss: 0.00140112
Iteration 11/25 | Loss: 0.00140112
Iteration 12/25 | Loss: 0.00140112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014011164894327521, 0.0014011164894327521, 0.0014011164894327521, 0.0014011164894327521, 0.0014011164894327521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014011164894327521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66149914
Iteration 2/25 | Loss: 0.00086978
Iteration 3/25 | Loss: 0.00086978
Iteration 4/25 | Loss: 0.00086978
Iteration 5/25 | Loss: 0.00086978
Iteration 6/25 | Loss: 0.00086978
Iteration 7/25 | Loss: 0.00086978
Iteration 8/25 | Loss: 0.00086978
Iteration 9/25 | Loss: 0.00086978
Iteration 10/25 | Loss: 0.00086978
Iteration 11/25 | Loss: 0.00086978
Iteration 12/25 | Loss: 0.00086978
Iteration 13/25 | Loss: 0.00086978
Iteration 14/25 | Loss: 0.00086978
Iteration 15/25 | Loss: 0.00086978
Iteration 16/25 | Loss: 0.00086978
Iteration 17/25 | Loss: 0.00086978
Iteration 18/25 | Loss: 0.00086978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008697777520865202, 0.0008697777520865202, 0.0008697777520865202, 0.0008697777520865202, 0.0008697777520865202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008697777520865202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086978
Iteration 2/1000 | Loss: 0.00012460
Iteration 3/1000 | Loss: 0.00007070
Iteration 4/1000 | Loss: 0.00005494
Iteration 5/1000 | Loss: 0.00005159
Iteration 6/1000 | Loss: 0.00004956
Iteration 7/1000 | Loss: 0.00004884
Iteration 8/1000 | Loss: 0.00004818
Iteration 9/1000 | Loss: 0.00004738
Iteration 10/1000 | Loss: 0.00004646
Iteration 11/1000 | Loss: 0.00004605
Iteration 12/1000 | Loss: 0.00004568
Iteration 13/1000 | Loss: 0.00004525
Iteration 14/1000 | Loss: 0.00004488
Iteration 15/1000 | Loss: 0.00004457
Iteration 16/1000 | Loss: 0.00004432
Iteration 17/1000 | Loss: 0.00004414
Iteration 18/1000 | Loss: 0.00004400
Iteration 19/1000 | Loss: 0.00004388
Iteration 20/1000 | Loss: 0.00004383
Iteration 21/1000 | Loss: 0.00004380
Iteration 22/1000 | Loss: 0.00004380
Iteration 23/1000 | Loss: 0.00004379
Iteration 24/1000 | Loss: 0.00004377
Iteration 25/1000 | Loss: 0.00004376
Iteration 26/1000 | Loss: 0.00004376
Iteration 27/1000 | Loss: 0.00004375
Iteration 28/1000 | Loss: 0.00004373
Iteration 29/1000 | Loss: 0.00004373
Iteration 30/1000 | Loss: 0.00004373
Iteration 31/1000 | Loss: 0.00004373
Iteration 32/1000 | Loss: 0.00004372
Iteration 33/1000 | Loss: 0.00004372
Iteration 34/1000 | Loss: 0.00004372
Iteration 35/1000 | Loss: 0.00004372
Iteration 36/1000 | Loss: 0.00004372
Iteration 37/1000 | Loss: 0.00004372
Iteration 38/1000 | Loss: 0.00004371
Iteration 39/1000 | Loss: 0.00004370
Iteration 40/1000 | Loss: 0.00004369
Iteration 41/1000 | Loss: 0.00004369
Iteration 42/1000 | Loss: 0.00004369
Iteration 43/1000 | Loss: 0.00004368
Iteration 44/1000 | Loss: 0.00004368
Iteration 45/1000 | Loss: 0.00004368
Iteration 46/1000 | Loss: 0.00004366
Iteration 47/1000 | Loss: 0.00004366
Iteration 48/1000 | Loss: 0.00004365
Iteration 49/1000 | Loss: 0.00004365
Iteration 50/1000 | Loss: 0.00004364
Iteration 51/1000 | Loss: 0.00004364
Iteration 52/1000 | Loss: 0.00004364
Iteration 53/1000 | Loss: 0.00004364
Iteration 54/1000 | Loss: 0.00004364
Iteration 55/1000 | Loss: 0.00004364
Iteration 56/1000 | Loss: 0.00004363
Iteration 57/1000 | Loss: 0.00004362
Iteration 58/1000 | Loss: 0.00004362
Iteration 59/1000 | Loss: 0.00004361
Iteration 60/1000 | Loss: 0.00004361
Iteration 61/1000 | Loss: 0.00004361
Iteration 62/1000 | Loss: 0.00004361
Iteration 63/1000 | Loss: 0.00004361
Iteration 64/1000 | Loss: 0.00004361
Iteration 65/1000 | Loss: 0.00004361
Iteration 66/1000 | Loss: 0.00004360
Iteration 67/1000 | Loss: 0.00004360
Iteration 68/1000 | Loss: 0.00004360
Iteration 69/1000 | Loss: 0.00004360
Iteration 70/1000 | Loss: 0.00004360
Iteration 71/1000 | Loss: 0.00004360
Iteration 72/1000 | Loss: 0.00004360
Iteration 73/1000 | Loss: 0.00004360
Iteration 74/1000 | Loss: 0.00004360
Iteration 75/1000 | Loss: 0.00004360
Iteration 76/1000 | Loss: 0.00004360
Iteration 77/1000 | Loss: 0.00004360
Iteration 78/1000 | Loss: 0.00004359
Iteration 79/1000 | Loss: 0.00004359
Iteration 80/1000 | Loss: 0.00004359
Iteration 81/1000 | Loss: 0.00004359
Iteration 82/1000 | Loss: 0.00004358
Iteration 83/1000 | Loss: 0.00004357
Iteration 84/1000 | Loss: 0.00004357
Iteration 85/1000 | Loss: 0.00004357
Iteration 86/1000 | Loss: 0.00004357
Iteration 87/1000 | Loss: 0.00004357
Iteration 88/1000 | Loss: 0.00004357
Iteration 89/1000 | Loss: 0.00004357
Iteration 90/1000 | Loss: 0.00004357
Iteration 91/1000 | Loss: 0.00004357
Iteration 92/1000 | Loss: 0.00004357
Iteration 93/1000 | Loss: 0.00004357
Iteration 94/1000 | Loss: 0.00004356
Iteration 95/1000 | Loss: 0.00004356
Iteration 96/1000 | Loss: 0.00004356
Iteration 97/1000 | Loss: 0.00004356
Iteration 98/1000 | Loss: 0.00004355
Iteration 99/1000 | Loss: 0.00004355
Iteration 100/1000 | Loss: 0.00004355
Iteration 101/1000 | Loss: 0.00004355
Iteration 102/1000 | Loss: 0.00004354
Iteration 103/1000 | Loss: 0.00004354
Iteration 104/1000 | Loss: 0.00004354
Iteration 105/1000 | Loss: 0.00004354
Iteration 106/1000 | Loss: 0.00004354
Iteration 107/1000 | Loss: 0.00004353
Iteration 108/1000 | Loss: 0.00004353
Iteration 109/1000 | Loss: 0.00004353
Iteration 110/1000 | Loss: 0.00004353
Iteration 111/1000 | Loss: 0.00004353
Iteration 112/1000 | Loss: 0.00004352
Iteration 113/1000 | Loss: 0.00004352
Iteration 114/1000 | Loss: 0.00004352
Iteration 115/1000 | Loss: 0.00004352
Iteration 116/1000 | Loss: 0.00004352
Iteration 117/1000 | Loss: 0.00004352
Iteration 118/1000 | Loss: 0.00004352
Iteration 119/1000 | Loss: 0.00004352
Iteration 120/1000 | Loss: 0.00004352
Iteration 121/1000 | Loss: 0.00004352
Iteration 122/1000 | Loss: 0.00004352
Iteration 123/1000 | Loss: 0.00004352
Iteration 124/1000 | Loss: 0.00004351
Iteration 125/1000 | Loss: 0.00004351
Iteration 126/1000 | Loss: 0.00004351
Iteration 127/1000 | Loss: 0.00004351
Iteration 128/1000 | Loss: 0.00004351
Iteration 129/1000 | Loss: 0.00004351
Iteration 130/1000 | Loss: 0.00004351
Iteration 131/1000 | Loss: 0.00004351
Iteration 132/1000 | Loss: 0.00004351
Iteration 133/1000 | Loss: 0.00004351
Iteration 134/1000 | Loss: 0.00004351
Iteration 135/1000 | Loss: 0.00004350
Iteration 136/1000 | Loss: 0.00004350
Iteration 137/1000 | Loss: 0.00004350
Iteration 138/1000 | Loss: 0.00004350
Iteration 139/1000 | Loss: 0.00004350
Iteration 140/1000 | Loss: 0.00004349
Iteration 141/1000 | Loss: 0.00004349
Iteration 142/1000 | Loss: 0.00004349
Iteration 143/1000 | Loss: 0.00004349
Iteration 144/1000 | Loss: 0.00004349
Iteration 145/1000 | Loss: 0.00004349
Iteration 146/1000 | Loss: 0.00004349
Iteration 147/1000 | Loss: 0.00004349
Iteration 148/1000 | Loss: 0.00004349
Iteration 149/1000 | Loss: 0.00004349
Iteration 150/1000 | Loss: 0.00004349
Iteration 151/1000 | Loss: 0.00004349
Iteration 152/1000 | Loss: 0.00004349
Iteration 153/1000 | Loss: 0.00004349
Iteration 154/1000 | Loss: 0.00004348
Iteration 155/1000 | Loss: 0.00004348
Iteration 156/1000 | Loss: 0.00004348
Iteration 157/1000 | Loss: 0.00004348
Iteration 158/1000 | Loss: 0.00004348
Iteration 159/1000 | Loss: 0.00004348
Iteration 160/1000 | Loss: 0.00004348
Iteration 161/1000 | Loss: 0.00004348
Iteration 162/1000 | Loss: 0.00004348
Iteration 163/1000 | Loss: 0.00004348
Iteration 164/1000 | Loss: 0.00004347
Iteration 165/1000 | Loss: 0.00004347
Iteration 166/1000 | Loss: 0.00004347
Iteration 167/1000 | Loss: 0.00004347
Iteration 168/1000 | Loss: 0.00004347
Iteration 169/1000 | Loss: 0.00004347
Iteration 170/1000 | Loss: 0.00004347
Iteration 171/1000 | Loss: 0.00004347
Iteration 172/1000 | Loss: 0.00004347
Iteration 173/1000 | Loss: 0.00004347
Iteration 174/1000 | Loss: 0.00004347
Iteration 175/1000 | Loss: 0.00004347
Iteration 176/1000 | Loss: 0.00004347
Iteration 177/1000 | Loss: 0.00004347
Iteration 178/1000 | Loss: 0.00004347
Iteration 179/1000 | Loss: 0.00004347
Iteration 180/1000 | Loss: 0.00004347
Iteration 181/1000 | Loss: 0.00004347
Iteration 182/1000 | Loss: 0.00004347
Iteration 183/1000 | Loss: 0.00004347
Iteration 184/1000 | Loss: 0.00004347
Iteration 185/1000 | Loss: 0.00004347
Iteration 186/1000 | Loss: 0.00004347
Iteration 187/1000 | Loss: 0.00004347
Iteration 188/1000 | Loss: 0.00004347
Iteration 189/1000 | Loss: 0.00004347
Iteration 190/1000 | Loss: 0.00004347
Iteration 191/1000 | Loss: 0.00004347
Iteration 192/1000 | Loss: 0.00004347
Iteration 193/1000 | Loss: 0.00004347
Iteration 194/1000 | Loss: 0.00004347
Iteration 195/1000 | Loss: 0.00004347
Iteration 196/1000 | Loss: 0.00004347
Iteration 197/1000 | Loss: 0.00004347
Iteration 198/1000 | Loss: 0.00004347
Iteration 199/1000 | Loss: 0.00004347
Iteration 200/1000 | Loss: 0.00004347
Iteration 201/1000 | Loss: 0.00004347
Iteration 202/1000 | Loss: 0.00004347
Iteration 203/1000 | Loss: 0.00004347
Iteration 204/1000 | Loss: 0.00004347
Iteration 205/1000 | Loss: 0.00004347
Iteration 206/1000 | Loss: 0.00004347
Iteration 207/1000 | Loss: 0.00004347
Iteration 208/1000 | Loss: 0.00004347
Iteration 209/1000 | Loss: 0.00004347
Iteration 210/1000 | Loss: 0.00004347
Iteration 211/1000 | Loss: 0.00004347
Iteration 212/1000 | Loss: 0.00004347
Iteration 213/1000 | Loss: 0.00004347
Iteration 214/1000 | Loss: 0.00004347
Iteration 215/1000 | Loss: 0.00004347
Iteration 216/1000 | Loss: 0.00004347
Iteration 217/1000 | Loss: 0.00004347
Iteration 218/1000 | Loss: 0.00004347
Iteration 219/1000 | Loss: 0.00004347
Iteration 220/1000 | Loss: 0.00004347
Iteration 221/1000 | Loss: 0.00004347
Iteration 222/1000 | Loss: 0.00004347
Iteration 223/1000 | Loss: 0.00004347
Iteration 224/1000 | Loss: 0.00004347
Iteration 225/1000 | Loss: 0.00004347
Iteration 226/1000 | Loss: 0.00004347
Iteration 227/1000 | Loss: 0.00004347
Iteration 228/1000 | Loss: 0.00004347
Iteration 229/1000 | Loss: 0.00004347
Iteration 230/1000 | Loss: 0.00004347
Iteration 231/1000 | Loss: 0.00004347
Iteration 232/1000 | Loss: 0.00004347
Iteration 233/1000 | Loss: 0.00004347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [4.346542846178636e-05, 4.346542846178636e-05, 4.346542846178636e-05, 4.346542846178636e-05, 4.346542846178636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.346542846178636e-05

Optimization complete. Final v2v error: 4.956484794616699 mm

Highest mean error: 5.5256571769714355 mm for frame 64

Lowest mean error: 3.6673502922058105 mm for frame 20

Saving results

Total time: 48.4787380695343
