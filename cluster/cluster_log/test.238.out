Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=238, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13328-13383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053149
Iteration 2/25 | Loss: 0.01053149
Iteration 3/25 | Loss: 0.00367818
Iteration 4/25 | Loss: 0.00220288
Iteration 5/25 | Loss: 0.00190573
Iteration 6/25 | Loss: 0.00175213
Iteration 7/25 | Loss: 0.00169322
Iteration 8/25 | Loss: 0.00158992
Iteration 9/25 | Loss: 0.00146371
Iteration 10/25 | Loss: 0.00141385
Iteration 11/25 | Loss: 0.00139561
Iteration 12/25 | Loss: 0.00134084
Iteration 13/25 | Loss: 0.00133395
Iteration 14/25 | Loss: 0.00132963
Iteration 15/25 | Loss: 0.00132309
Iteration 16/25 | Loss: 0.00131866
Iteration 17/25 | Loss: 0.00131657
Iteration 18/25 | Loss: 0.00131720
Iteration 19/25 | Loss: 0.00131749
Iteration 20/25 | Loss: 0.00131998
Iteration 21/25 | Loss: 0.00132151
Iteration 22/25 | Loss: 0.00131936
Iteration 23/25 | Loss: 0.00131680
Iteration 24/25 | Loss: 0.00131474
Iteration 25/25 | Loss: 0.00131458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38494694
Iteration 2/25 | Loss: 0.00337419
Iteration 3/25 | Loss: 0.00312147
Iteration 4/25 | Loss: 0.00312147
Iteration 5/25 | Loss: 0.00312147
Iteration 6/25 | Loss: 0.00312147
Iteration 7/25 | Loss: 0.00312147
Iteration 8/25 | Loss: 0.00312147
Iteration 9/25 | Loss: 0.00312147
Iteration 10/25 | Loss: 0.00312147
Iteration 11/25 | Loss: 0.00312147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.003121465677395463, 0.003121465677395463, 0.003121465677395463, 0.003121465677395463, 0.003121465677395463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003121465677395463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312147
Iteration 2/1000 | Loss: 0.00054689
Iteration 3/1000 | Loss: 0.00059805
Iteration 4/1000 | Loss: 0.00060154
Iteration 5/1000 | Loss: 0.00038462
Iteration 6/1000 | Loss: 0.00031362
Iteration 7/1000 | Loss: 0.00104631
Iteration 8/1000 | Loss: 0.00317994
Iteration 9/1000 | Loss: 0.00120895
Iteration 10/1000 | Loss: 0.00288072
Iteration 11/1000 | Loss: 0.00378459
Iteration 12/1000 | Loss: 0.00175215
Iteration 13/1000 | Loss: 0.00042746
Iteration 14/1000 | Loss: 0.00027931
Iteration 15/1000 | Loss: 0.00023243
Iteration 16/1000 | Loss: 0.00027064
Iteration 17/1000 | Loss: 0.00026973
Iteration 18/1000 | Loss: 0.00042596
Iteration 19/1000 | Loss: 0.00027175
Iteration 20/1000 | Loss: 0.00019344
Iteration 21/1000 | Loss: 0.00091720
Iteration 22/1000 | Loss: 0.00025462
Iteration 23/1000 | Loss: 0.00061414
Iteration 24/1000 | Loss: 0.00058983
Iteration 25/1000 | Loss: 0.00055383
Iteration 26/1000 | Loss: 0.00031034
Iteration 27/1000 | Loss: 0.00017346
Iteration 28/1000 | Loss: 0.00016943
Iteration 29/1000 | Loss: 0.00028633
Iteration 30/1000 | Loss: 0.00021501
Iteration 31/1000 | Loss: 0.00022899
Iteration 32/1000 | Loss: 0.00017202
Iteration 33/1000 | Loss: 0.00021549
Iteration 34/1000 | Loss: 0.00016610
Iteration 35/1000 | Loss: 0.00015889
Iteration 36/1000 | Loss: 0.00026236
Iteration 37/1000 | Loss: 0.00361296
Iteration 38/1000 | Loss: 0.00697599
Iteration 39/1000 | Loss: 0.00472085
Iteration 40/1000 | Loss: 0.00722161
Iteration 41/1000 | Loss: 0.00561320
Iteration 42/1000 | Loss: 0.00764421
Iteration 43/1000 | Loss: 0.00902297
Iteration 44/1000 | Loss: 0.01172922
Iteration 45/1000 | Loss: 0.00553916
Iteration 46/1000 | Loss: 0.00582300
Iteration 47/1000 | Loss: 0.00106263
Iteration 48/1000 | Loss: 0.00061486
Iteration 49/1000 | Loss: 0.00136806
Iteration 50/1000 | Loss: 0.00158464
Iteration 51/1000 | Loss: 0.00037601
Iteration 52/1000 | Loss: 0.00174750
Iteration 53/1000 | Loss: 0.00195657
Iteration 54/1000 | Loss: 0.00327041
Iteration 55/1000 | Loss: 0.00155162
Iteration 56/1000 | Loss: 0.00133730
Iteration 57/1000 | Loss: 0.00077509
Iteration 58/1000 | Loss: 0.00041190
Iteration 59/1000 | Loss: 0.00047397
Iteration 60/1000 | Loss: 0.00104029
Iteration 61/1000 | Loss: 0.00098412
Iteration 62/1000 | Loss: 0.00045155
Iteration 63/1000 | Loss: 0.00137472
Iteration 64/1000 | Loss: 0.00067952
Iteration 65/1000 | Loss: 0.00143502
Iteration 66/1000 | Loss: 0.00057274
Iteration 67/1000 | Loss: 0.00015482
Iteration 68/1000 | Loss: 0.00067809
Iteration 69/1000 | Loss: 0.00006037
Iteration 70/1000 | Loss: 0.00061340
Iteration 71/1000 | Loss: 0.00023564
Iteration 72/1000 | Loss: 0.00022836
Iteration 73/1000 | Loss: 0.00004763
Iteration 74/1000 | Loss: 0.00004291
Iteration 75/1000 | Loss: 0.00050574
Iteration 76/1000 | Loss: 0.00015066
Iteration 77/1000 | Loss: 0.00003668
Iteration 78/1000 | Loss: 0.00003496
Iteration 79/1000 | Loss: 0.00003360
Iteration 80/1000 | Loss: 0.00003276
Iteration 81/1000 | Loss: 0.00003211
Iteration 82/1000 | Loss: 0.00003167
Iteration 83/1000 | Loss: 0.00003136
Iteration 84/1000 | Loss: 0.00003150
Iteration 85/1000 | Loss: 0.00003135
Iteration 86/1000 | Loss: 0.00003141
Iteration 87/1000 | Loss: 0.00003119
Iteration 88/1000 | Loss: 0.00003118
Iteration 89/1000 | Loss: 0.00003116
Iteration 90/1000 | Loss: 0.00003110
Iteration 91/1000 | Loss: 0.00003110
Iteration 92/1000 | Loss: 0.00003110
Iteration 93/1000 | Loss: 0.00003110
Iteration 94/1000 | Loss: 0.00003109
Iteration 95/1000 | Loss: 0.00003103
Iteration 96/1000 | Loss: 0.00003102
Iteration 97/1000 | Loss: 0.00003101
Iteration 98/1000 | Loss: 0.00003100
Iteration 99/1000 | Loss: 0.00003100
Iteration 100/1000 | Loss: 0.00003100
Iteration 101/1000 | Loss: 0.00003100
Iteration 102/1000 | Loss: 0.00003100
Iteration 103/1000 | Loss: 0.00003100
Iteration 104/1000 | Loss: 0.00003100
Iteration 105/1000 | Loss: 0.00003100
Iteration 106/1000 | Loss: 0.00003100
Iteration 107/1000 | Loss: 0.00003100
Iteration 108/1000 | Loss: 0.00003099
Iteration 109/1000 | Loss: 0.00003099
Iteration 110/1000 | Loss: 0.00003099
Iteration 111/1000 | Loss: 0.00003098
Iteration 112/1000 | Loss: 0.00003098
Iteration 113/1000 | Loss: 0.00003097
Iteration 114/1000 | Loss: 0.00003097
Iteration 115/1000 | Loss: 0.00003097
Iteration 116/1000 | Loss: 0.00003097
Iteration 117/1000 | Loss: 0.00003097
Iteration 118/1000 | Loss: 0.00003096
Iteration 119/1000 | Loss: 0.00003096
Iteration 120/1000 | Loss: 0.00003096
Iteration 121/1000 | Loss: 0.00003096
Iteration 122/1000 | Loss: 0.00003096
Iteration 123/1000 | Loss: 0.00003096
Iteration 124/1000 | Loss: 0.00003096
Iteration 125/1000 | Loss: 0.00003096
Iteration 126/1000 | Loss: 0.00003096
Iteration 127/1000 | Loss: 0.00003096
Iteration 128/1000 | Loss: 0.00003096
Iteration 129/1000 | Loss: 0.00003096
Iteration 130/1000 | Loss: 0.00003095
Iteration 131/1000 | Loss: 0.00003095
Iteration 132/1000 | Loss: 0.00003107
Iteration 133/1000 | Loss: 0.00003097
Iteration 134/1000 | Loss: 0.00003097
Iteration 135/1000 | Loss: 0.00003096
Iteration 136/1000 | Loss: 0.00003094
Iteration 137/1000 | Loss: 0.00003094
Iteration 138/1000 | Loss: 0.00003094
Iteration 139/1000 | Loss: 0.00003094
Iteration 140/1000 | Loss: 0.00003105
Iteration 141/1000 | Loss: 0.00003105
Iteration 142/1000 | Loss: 0.00003105
Iteration 143/1000 | Loss: 0.00003104
Iteration 144/1000 | Loss: 0.00003104
Iteration 145/1000 | Loss: 0.00003104
Iteration 146/1000 | Loss: 0.00003094
Iteration 147/1000 | Loss: 0.00003094
Iteration 148/1000 | Loss: 0.00003094
Iteration 149/1000 | Loss: 0.00003094
Iteration 150/1000 | Loss: 0.00003094
Iteration 151/1000 | Loss: 0.00003093
Iteration 152/1000 | Loss: 0.00003093
Iteration 153/1000 | Loss: 0.00003093
Iteration 154/1000 | Loss: 0.00003093
Iteration 155/1000 | Loss: 0.00003093
Iteration 156/1000 | Loss: 0.00003093
Iteration 157/1000 | Loss: 0.00003093
Iteration 158/1000 | Loss: 0.00003093
Iteration 159/1000 | Loss: 0.00003093
Iteration 160/1000 | Loss: 0.00003093
Iteration 161/1000 | Loss: 0.00003093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.0931671062717214e-05, 3.0931671062717214e-05, 3.0931671062717214e-05, 3.0931671062717214e-05, 3.0931671062717214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0931671062717214e-05

Optimization complete. Final v2v error: 4.797762393951416 mm

Highest mean error: 10.17746639251709 mm for frame 68

Lowest mean error: 4.583388805389404 mm for frame 88

Saving results

Total time: 200.20519089698792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00717351
Iteration 2/25 | Loss: 0.00147707
Iteration 3/25 | Loss: 0.00119176
Iteration 4/25 | Loss: 0.00117413
Iteration 5/25 | Loss: 0.00115480
Iteration 6/25 | Loss: 0.00113955
Iteration 7/25 | Loss: 0.00113299
Iteration 8/25 | Loss: 0.00112831
Iteration 9/25 | Loss: 0.00112295
Iteration 10/25 | Loss: 0.00112137
Iteration 11/25 | Loss: 0.00112124
Iteration 12/25 | Loss: 0.00112124
Iteration 13/25 | Loss: 0.00112124
Iteration 14/25 | Loss: 0.00112124
Iteration 15/25 | Loss: 0.00112124
Iteration 16/25 | Loss: 0.00112124
Iteration 17/25 | Loss: 0.00112124
Iteration 18/25 | Loss: 0.00112124
Iteration 19/25 | Loss: 0.00112124
Iteration 20/25 | Loss: 0.00112124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011212360113859177, 0.0011212360113859177, 0.0011212360113859177, 0.0011212360113859177, 0.0011212360113859177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011212360113859177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15623760
Iteration 2/25 | Loss: 0.00124663
Iteration 3/25 | Loss: 0.00124636
Iteration 4/25 | Loss: 0.00124636
Iteration 5/25 | Loss: 0.00124636
Iteration 6/25 | Loss: 0.00124636
Iteration 7/25 | Loss: 0.00124636
Iteration 8/25 | Loss: 0.00124636
Iteration 9/25 | Loss: 0.00124636
Iteration 10/25 | Loss: 0.00124636
Iteration 11/25 | Loss: 0.00124636
Iteration 12/25 | Loss: 0.00124636
Iteration 13/25 | Loss: 0.00124636
Iteration 14/25 | Loss: 0.00124636
Iteration 15/25 | Loss: 0.00124636
Iteration 16/25 | Loss: 0.00124636
Iteration 17/25 | Loss: 0.00124636
Iteration 18/25 | Loss: 0.00124636
Iteration 19/25 | Loss: 0.00124636
Iteration 20/25 | Loss: 0.00124636
Iteration 21/25 | Loss: 0.00124636
Iteration 22/25 | Loss: 0.00124636
Iteration 23/25 | Loss: 0.00124636
Iteration 24/25 | Loss: 0.00124636
Iteration 25/25 | Loss: 0.00124636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012463623424991965, 0.0012463623424991965, 0.0012463623424991965, 0.0012463623424991965, 0.0012463623424991965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012463623424991965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124636
Iteration 2/1000 | Loss: 0.00009354
Iteration 3/1000 | Loss: 0.00005442
Iteration 4/1000 | Loss: 0.00004457
Iteration 5/1000 | Loss: 0.00004100
Iteration 6/1000 | Loss: 0.00003825
Iteration 7/1000 | Loss: 0.00003671
Iteration 8/1000 | Loss: 0.00003605
Iteration 9/1000 | Loss: 0.00003535
Iteration 10/1000 | Loss: 0.00003474
Iteration 11/1000 | Loss: 0.00003429
Iteration 12/1000 | Loss: 0.00003389
Iteration 13/1000 | Loss: 0.00003356
Iteration 14/1000 | Loss: 0.00003323
Iteration 15/1000 | Loss: 0.00003301
Iteration 16/1000 | Loss: 0.00003280
Iteration 17/1000 | Loss: 0.00003273
Iteration 18/1000 | Loss: 0.00003260
Iteration 19/1000 | Loss: 0.00003258
Iteration 20/1000 | Loss: 0.00003255
Iteration 21/1000 | Loss: 0.00003254
Iteration 22/1000 | Loss: 0.00003249
Iteration 23/1000 | Loss: 0.00003249
Iteration 24/1000 | Loss: 0.00003248
Iteration 25/1000 | Loss: 0.00003244
Iteration 26/1000 | Loss: 0.00003243
Iteration 27/1000 | Loss: 0.00003243
Iteration 28/1000 | Loss: 0.00003243
Iteration 29/1000 | Loss: 0.00003242
Iteration 30/1000 | Loss: 0.00003242
Iteration 31/1000 | Loss: 0.00003242
Iteration 32/1000 | Loss: 0.00003241
Iteration 33/1000 | Loss: 0.00003241
Iteration 34/1000 | Loss: 0.00003240
Iteration 35/1000 | Loss: 0.00003239
Iteration 36/1000 | Loss: 0.00003239
Iteration 37/1000 | Loss: 0.00003239
Iteration 38/1000 | Loss: 0.00003238
Iteration 39/1000 | Loss: 0.00003238
Iteration 40/1000 | Loss: 0.00003238
Iteration 41/1000 | Loss: 0.00003237
Iteration 42/1000 | Loss: 0.00003237
Iteration 43/1000 | Loss: 0.00003237
Iteration 44/1000 | Loss: 0.00003236
Iteration 45/1000 | Loss: 0.00003236
Iteration 46/1000 | Loss: 0.00003235
Iteration 47/1000 | Loss: 0.00003235
Iteration 48/1000 | Loss: 0.00003234
Iteration 49/1000 | Loss: 0.00003234
Iteration 50/1000 | Loss: 0.00003234
Iteration 51/1000 | Loss: 0.00003233
Iteration 52/1000 | Loss: 0.00003233
Iteration 53/1000 | Loss: 0.00003233
Iteration 54/1000 | Loss: 0.00003233
Iteration 55/1000 | Loss: 0.00003232
Iteration 56/1000 | Loss: 0.00003232
Iteration 57/1000 | Loss: 0.00003232
Iteration 58/1000 | Loss: 0.00003231
Iteration 59/1000 | Loss: 0.00003231
Iteration 60/1000 | Loss: 0.00003230
Iteration 61/1000 | Loss: 0.00003230
Iteration 62/1000 | Loss: 0.00003230
Iteration 63/1000 | Loss: 0.00003230
Iteration 64/1000 | Loss: 0.00003229
Iteration 65/1000 | Loss: 0.00003229
Iteration 66/1000 | Loss: 0.00003229
Iteration 67/1000 | Loss: 0.00003229
Iteration 68/1000 | Loss: 0.00003228
Iteration 69/1000 | Loss: 0.00003228
Iteration 70/1000 | Loss: 0.00003228
Iteration 71/1000 | Loss: 0.00003228
Iteration 72/1000 | Loss: 0.00003227
Iteration 73/1000 | Loss: 0.00003227
Iteration 74/1000 | Loss: 0.00003226
Iteration 75/1000 | Loss: 0.00003226
Iteration 76/1000 | Loss: 0.00003226
Iteration 77/1000 | Loss: 0.00003226
Iteration 78/1000 | Loss: 0.00003225
Iteration 79/1000 | Loss: 0.00003225
Iteration 80/1000 | Loss: 0.00003225
Iteration 81/1000 | Loss: 0.00003225
Iteration 82/1000 | Loss: 0.00003224
Iteration 83/1000 | Loss: 0.00003224
Iteration 84/1000 | Loss: 0.00003224
Iteration 85/1000 | Loss: 0.00003224
Iteration 86/1000 | Loss: 0.00003223
Iteration 87/1000 | Loss: 0.00003223
Iteration 88/1000 | Loss: 0.00003223
Iteration 89/1000 | Loss: 0.00003223
Iteration 90/1000 | Loss: 0.00003222
Iteration 91/1000 | Loss: 0.00003222
Iteration 92/1000 | Loss: 0.00003222
Iteration 93/1000 | Loss: 0.00003222
Iteration 94/1000 | Loss: 0.00003221
Iteration 95/1000 | Loss: 0.00003221
Iteration 96/1000 | Loss: 0.00003221
Iteration 97/1000 | Loss: 0.00003221
Iteration 98/1000 | Loss: 0.00003220
Iteration 99/1000 | Loss: 0.00003220
Iteration 100/1000 | Loss: 0.00003220
Iteration 101/1000 | Loss: 0.00003220
Iteration 102/1000 | Loss: 0.00003220
Iteration 103/1000 | Loss: 0.00003220
Iteration 104/1000 | Loss: 0.00003220
Iteration 105/1000 | Loss: 0.00003220
Iteration 106/1000 | Loss: 0.00003219
Iteration 107/1000 | Loss: 0.00003219
Iteration 108/1000 | Loss: 0.00003219
Iteration 109/1000 | Loss: 0.00003219
Iteration 110/1000 | Loss: 0.00003219
Iteration 111/1000 | Loss: 0.00003219
Iteration 112/1000 | Loss: 0.00003219
Iteration 113/1000 | Loss: 0.00003219
Iteration 114/1000 | Loss: 0.00003219
Iteration 115/1000 | Loss: 0.00003219
Iteration 116/1000 | Loss: 0.00003219
Iteration 117/1000 | Loss: 0.00003219
Iteration 118/1000 | Loss: 0.00003219
Iteration 119/1000 | Loss: 0.00003219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [3.2193529477808625e-05, 3.2193529477808625e-05, 3.2193529477808625e-05, 3.2193529477808625e-05, 3.2193529477808625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2193529477808625e-05

Optimization complete. Final v2v error: 4.7198710441589355 mm

Highest mean error: 6.698284149169922 mm for frame 132

Lowest mean error: 3.835883140563965 mm for frame 173

Saving results

Total time: 61.25877118110657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503842
Iteration 2/25 | Loss: 0.00129257
Iteration 3/25 | Loss: 0.00117144
Iteration 4/25 | Loss: 0.00113426
Iteration 5/25 | Loss: 0.00112118
Iteration 6/25 | Loss: 0.00111859
Iteration 7/25 | Loss: 0.00111859
Iteration 8/25 | Loss: 0.00111859
Iteration 9/25 | Loss: 0.00111859
Iteration 10/25 | Loss: 0.00111859
Iteration 11/25 | Loss: 0.00111859
Iteration 12/25 | Loss: 0.00111859
Iteration 13/25 | Loss: 0.00111859
Iteration 14/25 | Loss: 0.00111859
Iteration 15/25 | Loss: 0.00111859
Iteration 16/25 | Loss: 0.00111859
Iteration 17/25 | Loss: 0.00111859
Iteration 18/25 | Loss: 0.00111859
Iteration 19/25 | Loss: 0.00111859
Iteration 20/25 | Loss: 0.00111859
Iteration 21/25 | Loss: 0.00111859
Iteration 22/25 | Loss: 0.00111859
Iteration 23/25 | Loss: 0.00111859
Iteration 24/25 | Loss: 0.00111859
Iteration 25/25 | Loss: 0.00111859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59768021
Iteration 2/25 | Loss: 0.00133956
Iteration 3/25 | Loss: 0.00133956
Iteration 4/25 | Loss: 0.00133956
Iteration 5/25 | Loss: 0.00133956
Iteration 6/25 | Loss: 0.00133956
Iteration 7/25 | Loss: 0.00133956
Iteration 8/25 | Loss: 0.00133956
Iteration 9/25 | Loss: 0.00133956
Iteration 10/25 | Loss: 0.00133956
Iteration 11/25 | Loss: 0.00133956
Iteration 12/25 | Loss: 0.00133956
Iteration 13/25 | Loss: 0.00133956
Iteration 14/25 | Loss: 0.00133956
Iteration 15/25 | Loss: 0.00133956
Iteration 16/25 | Loss: 0.00133956
Iteration 17/25 | Loss: 0.00133956
Iteration 18/25 | Loss: 0.00133956
Iteration 19/25 | Loss: 0.00133956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013395622372627258, 0.0013395622372627258, 0.0013395622372627258, 0.0013395622372627258, 0.0013395622372627258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013395622372627258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133956
Iteration 2/1000 | Loss: 0.00008166
Iteration 3/1000 | Loss: 0.00004334
Iteration 4/1000 | Loss: 0.00003508
Iteration 5/1000 | Loss: 0.00003181
Iteration 6/1000 | Loss: 0.00002983
Iteration 7/1000 | Loss: 0.00002874
Iteration 8/1000 | Loss: 0.00002790
Iteration 9/1000 | Loss: 0.00002720
Iteration 10/1000 | Loss: 0.00002667
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002579
Iteration 13/1000 | Loss: 0.00002555
Iteration 14/1000 | Loss: 0.00002539
Iteration 15/1000 | Loss: 0.00002538
Iteration 16/1000 | Loss: 0.00002528
Iteration 17/1000 | Loss: 0.00002525
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002521
Iteration 20/1000 | Loss: 0.00002520
Iteration 21/1000 | Loss: 0.00002520
Iteration 22/1000 | Loss: 0.00002518
Iteration 23/1000 | Loss: 0.00002517
Iteration 24/1000 | Loss: 0.00002513
Iteration 25/1000 | Loss: 0.00002508
Iteration 26/1000 | Loss: 0.00002506
Iteration 27/1000 | Loss: 0.00002505
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002502
Iteration 30/1000 | Loss: 0.00002496
Iteration 31/1000 | Loss: 0.00002495
Iteration 32/1000 | Loss: 0.00002494
Iteration 33/1000 | Loss: 0.00002494
Iteration 34/1000 | Loss: 0.00002493
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002492
Iteration 37/1000 | Loss: 0.00002492
Iteration 38/1000 | Loss: 0.00002491
Iteration 39/1000 | Loss: 0.00002491
Iteration 40/1000 | Loss: 0.00002490
Iteration 41/1000 | Loss: 0.00002489
Iteration 42/1000 | Loss: 0.00002486
Iteration 43/1000 | Loss: 0.00002486
Iteration 44/1000 | Loss: 0.00002485
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002483
Iteration 48/1000 | Loss: 0.00002483
Iteration 49/1000 | Loss: 0.00002483
Iteration 50/1000 | Loss: 0.00002483
Iteration 51/1000 | Loss: 0.00002482
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002482
Iteration 54/1000 | Loss: 0.00002482
Iteration 55/1000 | Loss: 0.00002482
Iteration 56/1000 | Loss: 0.00002482
Iteration 57/1000 | Loss: 0.00002481
Iteration 58/1000 | Loss: 0.00002481
Iteration 59/1000 | Loss: 0.00002481
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002481
Iteration 62/1000 | Loss: 0.00002481
Iteration 63/1000 | Loss: 0.00002480
Iteration 64/1000 | Loss: 0.00002480
Iteration 65/1000 | Loss: 0.00002480
Iteration 66/1000 | Loss: 0.00002479
Iteration 67/1000 | Loss: 0.00002479
Iteration 68/1000 | Loss: 0.00002479
Iteration 69/1000 | Loss: 0.00002478
Iteration 70/1000 | Loss: 0.00002478
Iteration 71/1000 | Loss: 0.00002478
Iteration 72/1000 | Loss: 0.00002477
Iteration 73/1000 | Loss: 0.00002477
Iteration 74/1000 | Loss: 0.00002477
Iteration 75/1000 | Loss: 0.00002477
Iteration 76/1000 | Loss: 0.00002476
Iteration 77/1000 | Loss: 0.00002476
Iteration 78/1000 | Loss: 0.00002476
Iteration 79/1000 | Loss: 0.00002476
Iteration 80/1000 | Loss: 0.00002476
Iteration 81/1000 | Loss: 0.00002476
Iteration 82/1000 | Loss: 0.00002476
Iteration 83/1000 | Loss: 0.00002476
Iteration 84/1000 | Loss: 0.00002476
Iteration 85/1000 | Loss: 0.00002476
Iteration 86/1000 | Loss: 0.00002476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.475744258845225e-05, 2.475744258845225e-05, 2.475744258845225e-05, 2.475744258845225e-05, 2.475744258845225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.475744258845225e-05

Optimization complete. Final v2v error: 4.2646164894104 mm

Highest mean error: 5.373772144317627 mm for frame 39

Lowest mean error: 3.6692707538604736 mm for frame 236

Saving results

Total time: 44.74900841712952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821270
Iteration 2/25 | Loss: 0.00157035
Iteration 3/25 | Loss: 0.00125212
Iteration 4/25 | Loss: 0.00120712
Iteration 5/25 | Loss: 0.00118784
Iteration 6/25 | Loss: 0.00118423
Iteration 7/25 | Loss: 0.00118381
Iteration 8/25 | Loss: 0.00118381
Iteration 9/25 | Loss: 0.00118381
Iteration 10/25 | Loss: 0.00118381
Iteration 11/25 | Loss: 0.00118381
Iteration 12/25 | Loss: 0.00118381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001183808664791286, 0.001183808664791286, 0.001183808664791286, 0.001183808664791286, 0.001183808664791286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001183808664791286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.86625385
Iteration 2/25 | Loss: 0.00099983
Iteration 3/25 | Loss: 0.00099983
Iteration 4/25 | Loss: 0.00099983
Iteration 5/25 | Loss: 0.00099983
Iteration 6/25 | Loss: 0.00099983
Iteration 7/25 | Loss: 0.00099983
Iteration 8/25 | Loss: 0.00099983
Iteration 9/25 | Loss: 0.00099983
Iteration 10/25 | Loss: 0.00099983
Iteration 11/25 | Loss: 0.00099983
Iteration 12/25 | Loss: 0.00099983
Iteration 13/25 | Loss: 0.00099983
Iteration 14/25 | Loss: 0.00099983
Iteration 15/25 | Loss: 0.00099983
Iteration 16/25 | Loss: 0.00099983
Iteration 17/25 | Loss: 0.00099983
Iteration 18/25 | Loss: 0.00099983
Iteration 19/25 | Loss: 0.00099983
Iteration 20/25 | Loss: 0.00099983
Iteration 21/25 | Loss: 0.00099983
Iteration 22/25 | Loss: 0.00099983
Iteration 23/25 | Loss: 0.00099983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009998294990509748, 0.0009998294990509748, 0.0009998294990509748, 0.0009998294990509748, 0.0009998294990509748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998294990509748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099983
Iteration 2/1000 | Loss: 0.00011700
Iteration 3/1000 | Loss: 0.00007194
Iteration 4/1000 | Loss: 0.00006046
Iteration 5/1000 | Loss: 0.00005647
Iteration 6/1000 | Loss: 0.00005353
Iteration 7/1000 | Loss: 0.00005231
Iteration 8/1000 | Loss: 0.00005147
Iteration 9/1000 | Loss: 0.00005046
Iteration 10/1000 | Loss: 0.00004963
Iteration 11/1000 | Loss: 0.00004904
Iteration 12/1000 | Loss: 0.00004863
Iteration 13/1000 | Loss: 0.00004831
Iteration 14/1000 | Loss: 0.00004806
Iteration 15/1000 | Loss: 0.00004786
Iteration 16/1000 | Loss: 0.00004777
Iteration 17/1000 | Loss: 0.00004775
Iteration 18/1000 | Loss: 0.00004766
Iteration 19/1000 | Loss: 0.00004765
Iteration 20/1000 | Loss: 0.00004763
Iteration 21/1000 | Loss: 0.00004749
Iteration 22/1000 | Loss: 0.00004745
Iteration 23/1000 | Loss: 0.00004743
Iteration 24/1000 | Loss: 0.00004742
Iteration 25/1000 | Loss: 0.00004740
Iteration 26/1000 | Loss: 0.00004733
Iteration 27/1000 | Loss: 0.00004729
Iteration 28/1000 | Loss: 0.00004729
Iteration 29/1000 | Loss: 0.00004726
Iteration 30/1000 | Loss: 0.00004723
Iteration 31/1000 | Loss: 0.00004722
Iteration 32/1000 | Loss: 0.00004722
Iteration 33/1000 | Loss: 0.00004721
Iteration 34/1000 | Loss: 0.00004720
Iteration 35/1000 | Loss: 0.00004720
Iteration 36/1000 | Loss: 0.00004720
Iteration 37/1000 | Loss: 0.00004719
Iteration 38/1000 | Loss: 0.00004716
Iteration 39/1000 | Loss: 0.00004716
Iteration 40/1000 | Loss: 0.00004711
Iteration 41/1000 | Loss: 0.00004707
Iteration 42/1000 | Loss: 0.00004707
Iteration 43/1000 | Loss: 0.00004707
Iteration 44/1000 | Loss: 0.00004707
Iteration 45/1000 | Loss: 0.00004707
Iteration 46/1000 | Loss: 0.00004706
Iteration 47/1000 | Loss: 0.00004706
Iteration 48/1000 | Loss: 0.00004706
Iteration 49/1000 | Loss: 0.00004704
Iteration 50/1000 | Loss: 0.00004703
Iteration 51/1000 | Loss: 0.00004703
Iteration 52/1000 | Loss: 0.00004703
Iteration 53/1000 | Loss: 0.00004702
Iteration 54/1000 | Loss: 0.00004701
Iteration 55/1000 | Loss: 0.00004701
Iteration 56/1000 | Loss: 0.00004701
Iteration 57/1000 | Loss: 0.00004700
Iteration 58/1000 | Loss: 0.00004700
Iteration 59/1000 | Loss: 0.00004700
Iteration 60/1000 | Loss: 0.00004700
Iteration 61/1000 | Loss: 0.00004700
Iteration 62/1000 | Loss: 0.00004699
Iteration 63/1000 | Loss: 0.00004699
Iteration 64/1000 | Loss: 0.00004699
Iteration 65/1000 | Loss: 0.00004699
Iteration 66/1000 | Loss: 0.00004699
Iteration 67/1000 | Loss: 0.00004699
Iteration 68/1000 | Loss: 0.00004698
Iteration 69/1000 | Loss: 0.00004698
Iteration 70/1000 | Loss: 0.00004698
Iteration 71/1000 | Loss: 0.00004697
Iteration 72/1000 | Loss: 0.00004697
Iteration 73/1000 | Loss: 0.00004697
Iteration 74/1000 | Loss: 0.00004696
Iteration 75/1000 | Loss: 0.00004696
Iteration 76/1000 | Loss: 0.00004696
Iteration 77/1000 | Loss: 0.00004696
Iteration 78/1000 | Loss: 0.00004695
Iteration 79/1000 | Loss: 0.00004695
Iteration 80/1000 | Loss: 0.00004694
Iteration 81/1000 | Loss: 0.00004694
Iteration 82/1000 | Loss: 0.00004694
Iteration 83/1000 | Loss: 0.00004693
Iteration 84/1000 | Loss: 0.00004693
Iteration 85/1000 | Loss: 0.00004693
Iteration 86/1000 | Loss: 0.00004693
Iteration 87/1000 | Loss: 0.00004693
Iteration 88/1000 | Loss: 0.00004692
Iteration 89/1000 | Loss: 0.00004692
Iteration 90/1000 | Loss: 0.00004692
Iteration 91/1000 | Loss: 0.00004692
Iteration 92/1000 | Loss: 0.00004692
Iteration 93/1000 | Loss: 0.00004692
Iteration 94/1000 | Loss: 0.00004692
Iteration 95/1000 | Loss: 0.00004691
Iteration 96/1000 | Loss: 0.00004691
Iteration 97/1000 | Loss: 0.00004691
Iteration 98/1000 | Loss: 0.00004691
Iteration 99/1000 | Loss: 0.00004691
Iteration 100/1000 | Loss: 0.00004690
Iteration 101/1000 | Loss: 0.00004690
Iteration 102/1000 | Loss: 0.00004690
Iteration 103/1000 | Loss: 0.00004690
Iteration 104/1000 | Loss: 0.00004690
Iteration 105/1000 | Loss: 0.00004690
Iteration 106/1000 | Loss: 0.00004690
Iteration 107/1000 | Loss: 0.00004690
Iteration 108/1000 | Loss: 0.00004689
Iteration 109/1000 | Loss: 0.00004689
Iteration 110/1000 | Loss: 0.00004689
Iteration 111/1000 | Loss: 0.00004689
Iteration 112/1000 | Loss: 0.00004688
Iteration 113/1000 | Loss: 0.00004688
Iteration 114/1000 | Loss: 0.00004688
Iteration 115/1000 | Loss: 0.00004688
Iteration 116/1000 | Loss: 0.00004687
Iteration 117/1000 | Loss: 0.00004687
Iteration 118/1000 | Loss: 0.00004687
Iteration 119/1000 | Loss: 0.00004687
Iteration 120/1000 | Loss: 0.00004686
Iteration 121/1000 | Loss: 0.00004686
Iteration 122/1000 | Loss: 0.00004686
Iteration 123/1000 | Loss: 0.00004686
Iteration 124/1000 | Loss: 0.00004686
Iteration 125/1000 | Loss: 0.00004686
Iteration 126/1000 | Loss: 0.00004686
Iteration 127/1000 | Loss: 0.00004686
Iteration 128/1000 | Loss: 0.00004685
Iteration 129/1000 | Loss: 0.00004685
Iteration 130/1000 | Loss: 0.00004685
Iteration 131/1000 | Loss: 0.00004684
Iteration 132/1000 | Loss: 0.00004684
Iteration 133/1000 | Loss: 0.00004684
Iteration 134/1000 | Loss: 0.00004683
Iteration 135/1000 | Loss: 0.00004683
Iteration 136/1000 | Loss: 0.00004683
Iteration 137/1000 | Loss: 0.00004683
Iteration 138/1000 | Loss: 0.00004683
Iteration 139/1000 | Loss: 0.00004682
Iteration 140/1000 | Loss: 0.00004682
Iteration 141/1000 | Loss: 0.00004682
Iteration 142/1000 | Loss: 0.00004682
Iteration 143/1000 | Loss: 0.00004682
Iteration 144/1000 | Loss: 0.00004682
Iteration 145/1000 | Loss: 0.00004681
Iteration 146/1000 | Loss: 0.00004681
Iteration 147/1000 | Loss: 0.00004681
Iteration 148/1000 | Loss: 0.00004681
Iteration 149/1000 | Loss: 0.00004680
Iteration 150/1000 | Loss: 0.00004680
Iteration 151/1000 | Loss: 0.00004679
Iteration 152/1000 | Loss: 0.00004679
Iteration 153/1000 | Loss: 0.00004679
Iteration 154/1000 | Loss: 0.00004679
Iteration 155/1000 | Loss: 0.00004678
Iteration 156/1000 | Loss: 0.00004678
Iteration 157/1000 | Loss: 0.00004678
Iteration 158/1000 | Loss: 0.00004677
Iteration 159/1000 | Loss: 0.00004677
Iteration 160/1000 | Loss: 0.00004677
Iteration 161/1000 | Loss: 0.00004677
Iteration 162/1000 | Loss: 0.00004676
Iteration 163/1000 | Loss: 0.00004676
Iteration 164/1000 | Loss: 0.00004675
Iteration 165/1000 | Loss: 0.00004675
Iteration 166/1000 | Loss: 0.00004675
Iteration 167/1000 | Loss: 0.00004675
Iteration 168/1000 | Loss: 0.00004675
Iteration 169/1000 | Loss: 0.00004675
Iteration 170/1000 | Loss: 0.00004675
Iteration 171/1000 | Loss: 0.00004675
Iteration 172/1000 | Loss: 0.00004675
Iteration 173/1000 | Loss: 0.00004675
Iteration 174/1000 | Loss: 0.00004675
Iteration 175/1000 | Loss: 0.00004674
Iteration 176/1000 | Loss: 0.00004674
Iteration 177/1000 | Loss: 0.00004673
Iteration 178/1000 | Loss: 0.00004673
Iteration 179/1000 | Loss: 0.00004673
Iteration 180/1000 | Loss: 0.00004673
Iteration 181/1000 | Loss: 0.00004672
Iteration 182/1000 | Loss: 0.00004672
Iteration 183/1000 | Loss: 0.00004672
Iteration 184/1000 | Loss: 0.00004672
Iteration 185/1000 | Loss: 0.00004672
Iteration 186/1000 | Loss: 0.00004672
Iteration 187/1000 | Loss: 0.00004672
Iteration 188/1000 | Loss: 0.00004671
Iteration 189/1000 | Loss: 0.00004671
Iteration 190/1000 | Loss: 0.00004671
Iteration 191/1000 | Loss: 0.00004671
Iteration 192/1000 | Loss: 0.00004671
Iteration 193/1000 | Loss: 0.00004671
Iteration 194/1000 | Loss: 0.00004670
Iteration 195/1000 | Loss: 0.00004670
Iteration 196/1000 | Loss: 0.00004670
Iteration 197/1000 | Loss: 0.00004669
Iteration 198/1000 | Loss: 0.00004669
Iteration 199/1000 | Loss: 0.00004669
Iteration 200/1000 | Loss: 0.00004669
Iteration 201/1000 | Loss: 0.00004668
Iteration 202/1000 | Loss: 0.00004668
Iteration 203/1000 | Loss: 0.00004668
Iteration 204/1000 | Loss: 0.00004668
Iteration 205/1000 | Loss: 0.00004668
Iteration 206/1000 | Loss: 0.00004668
Iteration 207/1000 | Loss: 0.00004667
Iteration 208/1000 | Loss: 0.00004667
Iteration 209/1000 | Loss: 0.00004667
Iteration 210/1000 | Loss: 0.00004667
Iteration 211/1000 | Loss: 0.00004666
Iteration 212/1000 | Loss: 0.00004666
Iteration 213/1000 | Loss: 0.00004666
Iteration 214/1000 | Loss: 0.00004666
Iteration 215/1000 | Loss: 0.00004665
Iteration 216/1000 | Loss: 0.00004665
Iteration 217/1000 | Loss: 0.00004665
Iteration 218/1000 | Loss: 0.00004665
Iteration 219/1000 | Loss: 0.00004665
Iteration 220/1000 | Loss: 0.00004665
Iteration 221/1000 | Loss: 0.00004665
Iteration 222/1000 | Loss: 0.00004665
Iteration 223/1000 | Loss: 0.00004664
Iteration 224/1000 | Loss: 0.00004664
Iteration 225/1000 | Loss: 0.00004664
Iteration 226/1000 | Loss: 0.00004664
Iteration 227/1000 | Loss: 0.00004664
Iteration 228/1000 | Loss: 0.00004663
Iteration 229/1000 | Loss: 0.00004663
Iteration 230/1000 | Loss: 0.00004663
Iteration 231/1000 | Loss: 0.00004663
Iteration 232/1000 | Loss: 0.00004663
Iteration 233/1000 | Loss: 0.00004662
Iteration 234/1000 | Loss: 0.00004662
Iteration 235/1000 | Loss: 0.00004662
Iteration 236/1000 | Loss: 0.00004662
Iteration 237/1000 | Loss: 0.00004662
Iteration 238/1000 | Loss: 0.00004662
Iteration 239/1000 | Loss: 0.00004662
Iteration 240/1000 | Loss: 0.00004662
Iteration 241/1000 | Loss: 0.00004662
Iteration 242/1000 | Loss: 0.00004661
Iteration 243/1000 | Loss: 0.00004661
Iteration 244/1000 | Loss: 0.00004661
Iteration 245/1000 | Loss: 0.00004661
Iteration 246/1000 | Loss: 0.00004661
Iteration 247/1000 | Loss: 0.00004661
Iteration 248/1000 | Loss: 0.00004661
Iteration 249/1000 | Loss: 0.00004661
Iteration 250/1000 | Loss: 0.00004661
Iteration 251/1000 | Loss: 0.00004661
Iteration 252/1000 | Loss: 0.00004660
Iteration 253/1000 | Loss: 0.00004660
Iteration 254/1000 | Loss: 0.00004660
Iteration 255/1000 | Loss: 0.00004660
Iteration 256/1000 | Loss: 0.00004660
Iteration 257/1000 | Loss: 0.00004660
Iteration 258/1000 | Loss: 0.00004660
Iteration 259/1000 | Loss: 0.00004660
Iteration 260/1000 | Loss: 0.00004660
Iteration 261/1000 | Loss: 0.00004660
Iteration 262/1000 | Loss: 0.00004660
Iteration 263/1000 | Loss: 0.00004660
Iteration 264/1000 | Loss: 0.00004660
Iteration 265/1000 | Loss: 0.00004660
Iteration 266/1000 | Loss: 0.00004660
Iteration 267/1000 | Loss: 0.00004660
Iteration 268/1000 | Loss: 0.00004660
Iteration 269/1000 | Loss: 0.00004659
Iteration 270/1000 | Loss: 0.00004659
Iteration 271/1000 | Loss: 0.00004659
Iteration 272/1000 | Loss: 0.00004659
Iteration 273/1000 | Loss: 0.00004659
Iteration 274/1000 | Loss: 0.00004659
Iteration 275/1000 | Loss: 0.00004659
Iteration 276/1000 | Loss: 0.00004659
Iteration 277/1000 | Loss: 0.00004659
Iteration 278/1000 | Loss: 0.00004658
Iteration 279/1000 | Loss: 0.00004658
Iteration 280/1000 | Loss: 0.00004658
Iteration 281/1000 | Loss: 0.00004658
Iteration 282/1000 | Loss: 0.00004658
Iteration 283/1000 | Loss: 0.00004658
Iteration 284/1000 | Loss: 0.00004658
Iteration 285/1000 | Loss: 0.00004658
Iteration 286/1000 | Loss: 0.00004658
Iteration 287/1000 | Loss: 0.00004658
Iteration 288/1000 | Loss: 0.00004658
Iteration 289/1000 | Loss: 0.00004658
Iteration 290/1000 | Loss: 0.00004658
Iteration 291/1000 | Loss: 0.00004658
Iteration 292/1000 | Loss: 0.00004658
Iteration 293/1000 | Loss: 0.00004658
Iteration 294/1000 | Loss: 0.00004658
Iteration 295/1000 | Loss: 0.00004658
Iteration 296/1000 | Loss: 0.00004658
Iteration 297/1000 | Loss: 0.00004658
Iteration 298/1000 | Loss: 0.00004658
Iteration 299/1000 | Loss: 0.00004658
Iteration 300/1000 | Loss: 0.00004658
Iteration 301/1000 | Loss: 0.00004658
Iteration 302/1000 | Loss: 0.00004658
Iteration 303/1000 | Loss: 0.00004658
Iteration 304/1000 | Loss: 0.00004658
Iteration 305/1000 | Loss: 0.00004658
Iteration 306/1000 | Loss: 0.00004658
Iteration 307/1000 | Loss: 0.00004658
Iteration 308/1000 | Loss: 0.00004658
Iteration 309/1000 | Loss: 0.00004658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [4.6583052608184516e-05, 4.6583052608184516e-05, 4.6583052608184516e-05, 4.6583052608184516e-05, 4.6583052608184516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6583052608184516e-05

Optimization complete. Final v2v error: 5.628745079040527 mm

Highest mean error: 6.878037452697754 mm for frame 42

Lowest mean error: 4.381592273712158 mm for frame 19

Saving results

Total time: 69.41206526756287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413102
Iteration 2/25 | Loss: 0.00120093
Iteration 3/25 | Loss: 0.00103394
Iteration 4/25 | Loss: 0.00100917
Iteration 5/25 | Loss: 0.00100471
Iteration 6/25 | Loss: 0.00100408
Iteration 7/25 | Loss: 0.00100408
Iteration 8/25 | Loss: 0.00100408
Iteration 9/25 | Loss: 0.00100408
Iteration 10/25 | Loss: 0.00100408
Iteration 11/25 | Loss: 0.00100408
Iteration 12/25 | Loss: 0.00100408
Iteration 13/25 | Loss: 0.00100408
Iteration 14/25 | Loss: 0.00100408
Iteration 15/25 | Loss: 0.00100408
Iteration 16/25 | Loss: 0.00100408
Iteration 17/25 | Loss: 0.00100408
Iteration 18/25 | Loss: 0.00100408
Iteration 19/25 | Loss: 0.00100408
Iteration 20/25 | Loss: 0.00100408
Iteration 21/25 | Loss: 0.00100408
Iteration 22/25 | Loss: 0.00100408
Iteration 23/25 | Loss: 0.00100408
Iteration 24/25 | Loss: 0.00100408
Iteration 25/25 | Loss: 0.00100408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40925539
Iteration 2/25 | Loss: 0.00108896
Iteration 3/25 | Loss: 0.00108896
Iteration 4/25 | Loss: 0.00108896
Iteration 5/25 | Loss: 0.00108896
Iteration 6/25 | Loss: 0.00108896
Iteration 7/25 | Loss: 0.00108896
Iteration 8/25 | Loss: 0.00108896
Iteration 9/25 | Loss: 0.00108896
Iteration 10/25 | Loss: 0.00108896
Iteration 11/25 | Loss: 0.00108896
Iteration 12/25 | Loss: 0.00108896
Iteration 13/25 | Loss: 0.00108896
Iteration 14/25 | Loss: 0.00108896
Iteration 15/25 | Loss: 0.00108896
Iteration 16/25 | Loss: 0.00108896
Iteration 17/25 | Loss: 0.00108896
Iteration 18/25 | Loss: 0.00108896
Iteration 19/25 | Loss: 0.00108896
Iteration 20/25 | Loss: 0.00108896
Iteration 21/25 | Loss: 0.00108896
Iteration 22/25 | Loss: 0.00108896
Iteration 23/25 | Loss: 0.00108896
Iteration 24/25 | Loss: 0.00108896
Iteration 25/25 | Loss: 0.00108896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108896
Iteration 2/1000 | Loss: 0.00004396
Iteration 3/1000 | Loss: 0.00002934
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001889
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001795
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001737
Iteration 11/1000 | Loss: 0.00001730
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001702
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001698
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001696
Iteration 23/1000 | Loss: 0.00001696
Iteration 24/1000 | Loss: 0.00001695
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001695
Iteration 27/1000 | Loss: 0.00001695
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001695
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001694
Iteration 33/1000 | Loss: 0.00001694
Iteration 34/1000 | Loss: 0.00001694
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001690
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001689
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001687
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001682
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001680
Iteration 104/1000 | Loss: 0.00001679
Iteration 105/1000 | Loss: 0.00001679
Iteration 106/1000 | Loss: 0.00001679
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00001679
Iteration 109/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.6794965631561354e-05, 1.6794965631561354e-05, 1.6794965631561354e-05, 1.6794965631561354e-05, 1.6794965631561354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6794965631561354e-05

Optimization complete. Final v2v error: 3.613931179046631 mm

Highest mean error: 3.838728666305542 mm for frame 114

Lowest mean error: 3.394329071044922 mm for frame 61

Saving results

Total time: 34.23596167564392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902740
Iteration 2/25 | Loss: 0.00128461
Iteration 3/25 | Loss: 0.00111218
Iteration 4/25 | Loss: 0.00109381
Iteration 5/25 | Loss: 0.00108789
Iteration 6/25 | Loss: 0.00108698
Iteration 7/25 | Loss: 0.00108698
Iteration 8/25 | Loss: 0.00108698
Iteration 9/25 | Loss: 0.00108698
Iteration 10/25 | Loss: 0.00108698
Iteration 11/25 | Loss: 0.00108698
Iteration 12/25 | Loss: 0.00108698
Iteration 13/25 | Loss: 0.00108698
Iteration 14/25 | Loss: 0.00108698
Iteration 15/25 | Loss: 0.00108698
Iteration 16/25 | Loss: 0.00108698
Iteration 17/25 | Loss: 0.00108698
Iteration 18/25 | Loss: 0.00108698
Iteration 19/25 | Loss: 0.00108698
Iteration 20/25 | Loss: 0.00108698
Iteration 21/25 | Loss: 0.00108698
Iteration 22/25 | Loss: 0.00108698
Iteration 23/25 | Loss: 0.00108698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010869826655834913, 0.0010869826655834913, 0.0010869826655834913, 0.0010869826655834913, 0.0010869826655834913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010869826655834913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38628328
Iteration 2/25 | Loss: 0.00120445
Iteration 3/25 | Loss: 0.00120439
Iteration 4/25 | Loss: 0.00120439
Iteration 5/25 | Loss: 0.00120439
Iteration 6/25 | Loss: 0.00120439
Iteration 7/25 | Loss: 0.00120438
Iteration 8/25 | Loss: 0.00120438
Iteration 9/25 | Loss: 0.00120438
Iteration 10/25 | Loss: 0.00120438
Iteration 11/25 | Loss: 0.00120438
Iteration 12/25 | Loss: 0.00120438
Iteration 13/25 | Loss: 0.00120438
Iteration 14/25 | Loss: 0.00120438
Iteration 15/25 | Loss: 0.00120438
Iteration 16/25 | Loss: 0.00120438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001204384258016944, 0.001204384258016944, 0.001204384258016944, 0.001204384258016944, 0.001204384258016944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001204384258016944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120438
Iteration 2/1000 | Loss: 0.00007274
Iteration 3/1000 | Loss: 0.00003792
Iteration 4/1000 | Loss: 0.00002957
Iteration 5/1000 | Loss: 0.00002667
Iteration 6/1000 | Loss: 0.00002478
Iteration 7/1000 | Loss: 0.00002402
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002290
Iteration 10/1000 | Loss: 0.00002252
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002213
Iteration 13/1000 | Loss: 0.00002204
Iteration 14/1000 | Loss: 0.00002201
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002188
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002174
Iteration 22/1000 | Loss: 0.00002173
Iteration 23/1000 | Loss: 0.00002173
Iteration 24/1000 | Loss: 0.00002173
Iteration 25/1000 | Loss: 0.00002172
Iteration 26/1000 | Loss: 0.00002169
Iteration 27/1000 | Loss: 0.00002167
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002166
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002162
Iteration 35/1000 | Loss: 0.00002162
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002161
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002160
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00002159
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002158
Iteration 45/1000 | Loss: 0.00002158
Iteration 46/1000 | Loss: 0.00002158
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002157
Iteration 51/1000 | Loss: 0.00002157
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002155
Iteration 59/1000 | Loss: 0.00002155
Iteration 60/1000 | Loss: 0.00002155
Iteration 61/1000 | Loss: 0.00002154
Iteration 62/1000 | Loss: 0.00002154
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002154
Iteration 65/1000 | Loss: 0.00002154
Iteration 66/1000 | Loss: 0.00002153
Iteration 67/1000 | Loss: 0.00002153
Iteration 68/1000 | Loss: 0.00002153
Iteration 69/1000 | Loss: 0.00002153
Iteration 70/1000 | Loss: 0.00002153
Iteration 71/1000 | Loss: 0.00002153
Iteration 72/1000 | Loss: 0.00002153
Iteration 73/1000 | Loss: 0.00002153
Iteration 74/1000 | Loss: 0.00002153
Iteration 75/1000 | Loss: 0.00002153
Iteration 76/1000 | Loss: 0.00002152
Iteration 77/1000 | Loss: 0.00002152
Iteration 78/1000 | Loss: 0.00002152
Iteration 79/1000 | Loss: 0.00002152
Iteration 80/1000 | Loss: 0.00002152
Iteration 81/1000 | Loss: 0.00002152
Iteration 82/1000 | Loss: 0.00002152
Iteration 83/1000 | Loss: 0.00002152
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002152
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002151
Iteration 93/1000 | Loss: 0.00002151
Iteration 94/1000 | Loss: 0.00002151
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002151
Iteration 103/1000 | Loss: 0.00002151
Iteration 104/1000 | Loss: 0.00002151
Iteration 105/1000 | Loss: 0.00002151
Iteration 106/1000 | Loss: 0.00002151
Iteration 107/1000 | Loss: 0.00002151
Iteration 108/1000 | Loss: 0.00002151
Iteration 109/1000 | Loss: 0.00002151
Iteration 110/1000 | Loss: 0.00002151
Iteration 111/1000 | Loss: 0.00002151
Iteration 112/1000 | Loss: 0.00002151
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.150853833882138e-05, 2.150853833882138e-05, 2.150853833882138e-05, 2.150853833882138e-05, 2.150853833882138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.150853833882138e-05

Optimization complete. Final v2v error: 4.052574634552002 mm

Highest mean error: 4.621237754821777 mm for frame 48

Lowest mean error: 3.52386212348938 mm for frame 136

Saving results

Total time: 42.729750871658325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037919
Iteration 2/25 | Loss: 0.00184672
Iteration 3/25 | Loss: 0.00126548
Iteration 4/25 | Loss: 0.00122872
Iteration 5/25 | Loss: 0.00121458
Iteration 6/25 | Loss: 0.00121154
Iteration 7/25 | Loss: 0.00121128
Iteration 8/25 | Loss: 0.00121128
Iteration 9/25 | Loss: 0.00121128
Iteration 10/25 | Loss: 0.00121128
Iteration 11/25 | Loss: 0.00121128
Iteration 12/25 | Loss: 0.00121128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012112807016819715, 0.0012112807016819715, 0.0012112807016819715, 0.0012112807016819715, 0.0012112807016819715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012112807016819715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79629052
Iteration 2/25 | Loss: 0.00075738
Iteration 3/25 | Loss: 0.00075738
Iteration 4/25 | Loss: 0.00075737
Iteration 5/25 | Loss: 0.00075737
Iteration 6/25 | Loss: 0.00075737
Iteration 7/25 | Loss: 0.00075737
Iteration 8/25 | Loss: 0.00075737
Iteration 9/25 | Loss: 0.00075737
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0007573748589493334, 0.0007573748589493334, 0.0007573748589493334, 0.0007573748589493334, 0.0007573748589493334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007573748589493334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075737
Iteration 2/1000 | Loss: 0.00010302
Iteration 3/1000 | Loss: 0.00006441
Iteration 4/1000 | Loss: 0.00005627
Iteration 5/1000 | Loss: 0.00005214
Iteration 6/1000 | Loss: 0.00005017
Iteration 7/1000 | Loss: 0.00004897
Iteration 8/1000 | Loss: 0.00004795
Iteration 9/1000 | Loss: 0.00004736
Iteration 10/1000 | Loss: 0.00004698
Iteration 11/1000 | Loss: 0.00004665
Iteration 12/1000 | Loss: 0.00004636
Iteration 13/1000 | Loss: 0.00004618
Iteration 14/1000 | Loss: 0.00004608
Iteration 15/1000 | Loss: 0.00004606
Iteration 16/1000 | Loss: 0.00004606
Iteration 17/1000 | Loss: 0.00004598
Iteration 18/1000 | Loss: 0.00004585
Iteration 19/1000 | Loss: 0.00004569
Iteration 20/1000 | Loss: 0.00004564
Iteration 21/1000 | Loss: 0.00004561
Iteration 22/1000 | Loss: 0.00004554
Iteration 23/1000 | Loss: 0.00004552
Iteration 24/1000 | Loss: 0.00004551
Iteration 25/1000 | Loss: 0.00004546
Iteration 26/1000 | Loss: 0.00004539
Iteration 27/1000 | Loss: 0.00004535
Iteration 28/1000 | Loss: 0.00004534
Iteration 29/1000 | Loss: 0.00004534
Iteration 30/1000 | Loss: 0.00004534
Iteration 31/1000 | Loss: 0.00004534
Iteration 32/1000 | Loss: 0.00004533
Iteration 33/1000 | Loss: 0.00004533
Iteration 34/1000 | Loss: 0.00004533
Iteration 35/1000 | Loss: 0.00004533
Iteration 36/1000 | Loss: 0.00004532
Iteration 37/1000 | Loss: 0.00004532
Iteration 38/1000 | Loss: 0.00004532
Iteration 39/1000 | Loss: 0.00004532
Iteration 40/1000 | Loss: 0.00004532
Iteration 41/1000 | Loss: 0.00004532
Iteration 42/1000 | Loss: 0.00004532
Iteration 43/1000 | Loss: 0.00004532
Iteration 44/1000 | Loss: 0.00004532
Iteration 45/1000 | Loss: 0.00004532
Iteration 46/1000 | Loss: 0.00004531
Iteration 47/1000 | Loss: 0.00004531
Iteration 48/1000 | Loss: 0.00004531
Iteration 49/1000 | Loss: 0.00004531
Iteration 50/1000 | Loss: 0.00004529
Iteration 51/1000 | Loss: 0.00004529
Iteration 52/1000 | Loss: 0.00004529
Iteration 53/1000 | Loss: 0.00004529
Iteration 54/1000 | Loss: 0.00004529
Iteration 55/1000 | Loss: 0.00004529
Iteration 56/1000 | Loss: 0.00004529
Iteration 57/1000 | Loss: 0.00004528
Iteration 58/1000 | Loss: 0.00004528
Iteration 59/1000 | Loss: 0.00004528
Iteration 60/1000 | Loss: 0.00004528
Iteration 61/1000 | Loss: 0.00004528
Iteration 62/1000 | Loss: 0.00004528
Iteration 63/1000 | Loss: 0.00004528
Iteration 64/1000 | Loss: 0.00004528
Iteration 65/1000 | Loss: 0.00004527
Iteration 66/1000 | Loss: 0.00004527
Iteration 67/1000 | Loss: 0.00004526
Iteration 68/1000 | Loss: 0.00004524
Iteration 69/1000 | Loss: 0.00004522
Iteration 70/1000 | Loss: 0.00004521
Iteration 71/1000 | Loss: 0.00004521
Iteration 72/1000 | Loss: 0.00004521
Iteration 73/1000 | Loss: 0.00004519
Iteration 74/1000 | Loss: 0.00004519
Iteration 75/1000 | Loss: 0.00004519
Iteration 76/1000 | Loss: 0.00004519
Iteration 77/1000 | Loss: 0.00004518
Iteration 78/1000 | Loss: 0.00004518
Iteration 79/1000 | Loss: 0.00004518
Iteration 80/1000 | Loss: 0.00004518
Iteration 81/1000 | Loss: 0.00004517
Iteration 82/1000 | Loss: 0.00004516
Iteration 83/1000 | Loss: 0.00004516
Iteration 84/1000 | Loss: 0.00004516
Iteration 85/1000 | Loss: 0.00004516
Iteration 86/1000 | Loss: 0.00004516
Iteration 87/1000 | Loss: 0.00004515
Iteration 88/1000 | Loss: 0.00004515
Iteration 89/1000 | Loss: 0.00004514
Iteration 90/1000 | Loss: 0.00004514
Iteration 91/1000 | Loss: 0.00004514
Iteration 92/1000 | Loss: 0.00004513
Iteration 93/1000 | Loss: 0.00004513
Iteration 94/1000 | Loss: 0.00004513
Iteration 95/1000 | Loss: 0.00004513
Iteration 96/1000 | Loss: 0.00004513
Iteration 97/1000 | Loss: 0.00004513
Iteration 98/1000 | Loss: 0.00004513
Iteration 99/1000 | Loss: 0.00004513
Iteration 100/1000 | Loss: 0.00004513
Iteration 101/1000 | Loss: 0.00004513
Iteration 102/1000 | Loss: 0.00004513
Iteration 103/1000 | Loss: 0.00004513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [4.513064050115645e-05, 4.513064050115645e-05, 4.513064050115645e-05, 4.513064050115645e-05, 4.513064050115645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.513064050115645e-05

Optimization complete. Final v2v error: 5.44836950302124 mm

Highest mean error: 6.533957004547119 mm for frame 104

Lowest mean error: 4.534524917602539 mm for frame 31

Saving results

Total time: 44.99609065055847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00349459
Iteration 2/25 | Loss: 0.00116568
Iteration 3/25 | Loss: 0.00106886
Iteration 4/25 | Loss: 0.00105253
Iteration 5/25 | Loss: 0.00104712
Iteration 6/25 | Loss: 0.00104600
Iteration 7/25 | Loss: 0.00104600
Iteration 8/25 | Loss: 0.00104600
Iteration 9/25 | Loss: 0.00104600
Iteration 10/25 | Loss: 0.00104600
Iteration 11/25 | Loss: 0.00104600
Iteration 12/25 | Loss: 0.00104600
Iteration 13/25 | Loss: 0.00104600
Iteration 14/25 | Loss: 0.00104600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010459971381351352, 0.0010459971381351352, 0.0010459971381351352, 0.0010459971381351352, 0.0010459971381351352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010459971381351352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40141928
Iteration 2/25 | Loss: 0.00132348
Iteration 3/25 | Loss: 0.00132348
Iteration 4/25 | Loss: 0.00132347
Iteration 5/25 | Loss: 0.00132347
Iteration 6/25 | Loss: 0.00132347
Iteration 7/25 | Loss: 0.00132347
Iteration 8/25 | Loss: 0.00132347
Iteration 9/25 | Loss: 0.00132347
Iteration 10/25 | Loss: 0.00132347
Iteration 11/25 | Loss: 0.00132347
Iteration 12/25 | Loss: 0.00132347
Iteration 13/25 | Loss: 0.00132347
Iteration 14/25 | Loss: 0.00132347
Iteration 15/25 | Loss: 0.00132347
Iteration 16/25 | Loss: 0.00132347
Iteration 17/25 | Loss: 0.00132347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013234727084636688, 0.0013234727084636688, 0.0013234727084636688, 0.0013234727084636688, 0.0013234727084636688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013234727084636688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132347
Iteration 2/1000 | Loss: 0.00007113
Iteration 3/1000 | Loss: 0.00003167
Iteration 4/1000 | Loss: 0.00002511
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002097
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00001985
Iteration 9/1000 | Loss: 0.00001952
Iteration 10/1000 | Loss: 0.00001927
Iteration 11/1000 | Loss: 0.00001913
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001908
Iteration 14/1000 | Loss: 0.00001907
Iteration 15/1000 | Loss: 0.00001905
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001887
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001886
Iteration 28/1000 | Loss: 0.00001886
Iteration 29/1000 | Loss: 0.00001885
Iteration 30/1000 | Loss: 0.00001885
Iteration 31/1000 | Loss: 0.00001885
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001884
Iteration 34/1000 | Loss: 0.00001884
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001883
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001883
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001882
Iteration 41/1000 | Loss: 0.00001882
Iteration 42/1000 | Loss: 0.00001881
Iteration 43/1000 | Loss: 0.00001881
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001879
Iteration 46/1000 | Loss: 0.00001879
Iteration 47/1000 | Loss: 0.00001879
Iteration 48/1000 | Loss: 0.00001879
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001879
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001878
Iteration 53/1000 | Loss: 0.00001878
Iteration 54/1000 | Loss: 0.00001878
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001878
Iteration 58/1000 | Loss: 0.00001877
Iteration 59/1000 | Loss: 0.00001877
Iteration 60/1000 | Loss: 0.00001877
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00001876
Iteration 65/1000 | Loss: 0.00001876
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001876
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001875
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001875
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001874
Iteration 84/1000 | Loss: 0.00001874
Iteration 85/1000 | Loss: 0.00001874
Iteration 86/1000 | Loss: 0.00001874
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001873
Iteration 90/1000 | Loss: 0.00001873
Iteration 91/1000 | Loss: 0.00001873
Iteration 92/1000 | Loss: 0.00001873
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001873
Iteration 99/1000 | Loss: 0.00001873
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001870
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001870
Iteration 140/1000 | Loss: 0.00001870
Iteration 141/1000 | Loss: 0.00001870
Iteration 142/1000 | Loss: 0.00001870
Iteration 143/1000 | Loss: 0.00001870
Iteration 144/1000 | Loss: 0.00001870
Iteration 145/1000 | Loss: 0.00001870
Iteration 146/1000 | Loss: 0.00001870
Iteration 147/1000 | Loss: 0.00001870
Iteration 148/1000 | Loss: 0.00001870
Iteration 149/1000 | Loss: 0.00001870
Iteration 150/1000 | Loss: 0.00001870
Iteration 151/1000 | Loss: 0.00001870
Iteration 152/1000 | Loss: 0.00001870
Iteration 153/1000 | Loss: 0.00001870
Iteration 154/1000 | Loss: 0.00001870
Iteration 155/1000 | Loss: 0.00001870
Iteration 156/1000 | Loss: 0.00001870
Iteration 157/1000 | Loss: 0.00001870
Iteration 158/1000 | Loss: 0.00001870
Iteration 159/1000 | Loss: 0.00001870
Iteration 160/1000 | Loss: 0.00001870
Iteration 161/1000 | Loss: 0.00001870
Iteration 162/1000 | Loss: 0.00001870
Iteration 163/1000 | Loss: 0.00001870
Iteration 164/1000 | Loss: 0.00001870
Iteration 165/1000 | Loss: 0.00001870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.8700098735280335e-05, 1.8700098735280335e-05, 1.8700098735280335e-05, 1.8700098735280335e-05, 1.8700098735280335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8700098735280335e-05

Optimization complete. Final v2v error: 3.7551074028015137 mm

Highest mean error: 3.8996787071228027 mm for frame 77

Lowest mean error: 3.5379831790924072 mm for frame 0

Saving results

Total time: 41.242738008499146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897169
Iteration 2/25 | Loss: 0.00147502
Iteration 3/25 | Loss: 0.00122196
Iteration 4/25 | Loss: 0.00116899
Iteration 5/25 | Loss: 0.00116113
Iteration 6/25 | Loss: 0.00115975
Iteration 7/25 | Loss: 0.00115968
Iteration 8/25 | Loss: 0.00115968
Iteration 9/25 | Loss: 0.00115968
Iteration 10/25 | Loss: 0.00115968
Iteration 11/25 | Loss: 0.00115968
Iteration 12/25 | Loss: 0.00115968
Iteration 13/25 | Loss: 0.00115968
Iteration 14/25 | Loss: 0.00115968
Iteration 15/25 | Loss: 0.00115968
Iteration 16/25 | Loss: 0.00115968
Iteration 17/25 | Loss: 0.00115968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011596772819757462, 0.0011596772819757462, 0.0011596772819757462, 0.0011596772819757462, 0.0011596772819757462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011596772819757462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98132700
Iteration 2/25 | Loss: 0.00073386
Iteration 3/25 | Loss: 0.00073385
Iteration 4/25 | Loss: 0.00073385
Iteration 5/25 | Loss: 0.00073385
Iteration 6/25 | Loss: 0.00073385
Iteration 7/25 | Loss: 0.00073385
Iteration 8/25 | Loss: 0.00073385
Iteration 9/25 | Loss: 0.00073385
Iteration 10/25 | Loss: 0.00073385
Iteration 11/25 | Loss: 0.00073385
Iteration 12/25 | Loss: 0.00073385
Iteration 13/25 | Loss: 0.00073385
Iteration 14/25 | Loss: 0.00073385
Iteration 15/25 | Loss: 0.00073385
Iteration 16/25 | Loss: 0.00073385
Iteration 17/25 | Loss: 0.00073385
Iteration 18/25 | Loss: 0.00073385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007338471477851272, 0.0007338471477851272, 0.0007338471477851272, 0.0007338471477851272, 0.0007338471477851272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007338471477851272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073385
Iteration 2/1000 | Loss: 0.00009580
Iteration 3/1000 | Loss: 0.00005976
Iteration 4/1000 | Loss: 0.00005049
Iteration 5/1000 | Loss: 0.00004338
Iteration 6/1000 | Loss: 0.00004119
Iteration 7/1000 | Loss: 0.00003945
Iteration 8/1000 | Loss: 0.00003832
Iteration 9/1000 | Loss: 0.00003716
Iteration 10/1000 | Loss: 0.00003667
Iteration 11/1000 | Loss: 0.00003634
Iteration 12/1000 | Loss: 0.00003597
Iteration 13/1000 | Loss: 0.00003582
Iteration 14/1000 | Loss: 0.00003578
Iteration 15/1000 | Loss: 0.00003568
Iteration 16/1000 | Loss: 0.00003567
Iteration 17/1000 | Loss: 0.00003567
Iteration 18/1000 | Loss: 0.00003566
Iteration 19/1000 | Loss: 0.00003565
Iteration 20/1000 | Loss: 0.00003565
Iteration 21/1000 | Loss: 0.00003565
Iteration 22/1000 | Loss: 0.00003564
Iteration 23/1000 | Loss: 0.00003564
Iteration 24/1000 | Loss: 0.00003564
Iteration 25/1000 | Loss: 0.00003564
Iteration 26/1000 | Loss: 0.00003564
Iteration 27/1000 | Loss: 0.00003564
Iteration 28/1000 | Loss: 0.00003563
Iteration 29/1000 | Loss: 0.00003563
Iteration 30/1000 | Loss: 0.00003563
Iteration 31/1000 | Loss: 0.00003563
Iteration 32/1000 | Loss: 0.00003563
Iteration 33/1000 | Loss: 0.00003562
Iteration 34/1000 | Loss: 0.00003562
Iteration 35/1000 | Loss: 0.00003562
Iteration 36/1000 | Loss: 0.00003562
Iteration 37/1000 | Loss: 0.00003562
Iteration 38/1000 | Loss: 0.00003562
Iteration 39/1000 | Loss: 0.00003562
Iteration 40/1000 | Loss: 0.00003562
Iteration 41/1000 | Loss: 0.00003562
Iteration 42/1000 | Loss: 0.00003562
Iteration 43/1000 | Loss: 0.00003562
Iteration 44/1000 | Loss: 0.00003561
Iteration 45/1000 | Loss: 0.00003561
Iteration 46/1000 | Loss: 0.00003561
Iteration 47/1000 | Loss: 0.00003561
Iteration 48/1000 | Loss: 0.00003561
Iteration 49/1000 | Loss: 0.00003561
Iteration 50/1000 | Loss: 0.00003561
Iteration 51/1000 | Loss: 0.00003561
Iteration 52/1000 | Loss: 0.00003561
Iteration 53/1000 | Loss: 0.00003561
Iteration 54/1000 | Loss: 0.00003561
Iteration 55/1000 | Loss: 0.00003561
Iteration 56/1000 | Loss: 0.00003561
Iteration 57/1000 | Loss: 0.00003561
Iteration 58/1000 | Loss: 0.00003561
Iteration 59/1000 | Loss: 0.00003561
Iteration 60/1000 | Loss: 0.00003561
Iteration 61/1000 | Loss: 0.00003561
Iteration 62/1000 | Loss: 0.00003561
Iteration 63/1000 | Loss: 0.00003561
Iteration 64/1000 | Loss: 0.00003561
Iteration 65/1000 | Loss: 0.00003561
Iteration 66/1000 | Loss: 0.00003561
Iteration 67/1000 | Loss: 0.00003561
Iteration 68/1000 | Loss: 0.00003561
Iteration 69/1000 | Loss: 0.00003561
Iteration 70/1000 | Loss: 0.00003561
Iteration 71/1000 | Loss: 0.00003561
Iteration 72/1000 | Loss: 0.00003561
Iteration 73/1000 | Loss: 0.00003561
Iteration 74/1000 | Loss: 0.00003561
Iteration 75/1000 | Loss: 0.00003561
Iteration 76/1000 | Loss: 0.00003561
Iteration 77/1000 | Loss: 0.00003561
Iteration 78/1000 | Loss: 0.00003561
Iteration 79/1000 | Loss: 0.00003561
Iteration 80/1000 | Loss: 0.00003561
Iteration 81/1000 | Loss: 0.00003561
Iteration 82/1000 | Loss: 0.00003561
Iteration 83/1000 | Loss: 0.00003561
Iteration 84/1000 | Loss: 0.00003561
Iteration 85/1000 | Loss: 0.00003561
Iteration 86/1000 | Loss: 0.00003561
Iteration 87/1000 | Loss: 0.00003561
Iteration 88/1000 | Loss: 0.00003561
Iteration 89/1000 | Loss: 0.00003561
Iteration 90/1000 | Loss: 0.00003561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.560558252502233e-05, 3.560558252502233e-05, 3.560558252502233e-05, 3.560558252502233e-05, 3.560558252502233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.560558252502233e-05

Optimization complete. Final v2v error: 4.963925361633301 mm

Highest mean error: 5.421570301055908 mm for frame 126

Lowest mean error: 4.649208068847656 mm for frame 26

Saving results

Total time: 32.877461671829224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888681
Iteration 2/25 | Loss: 0.00140645
Iteration 3/25 | Loss: 0.00117794
Iteration 4/25 | Loss: 0.00115846
Iteration 5/25 | Loss: 0.00115493
Iteration 6/25 | Loss: 0.00115464
Iteration 7/25 | Loss: 0.00115464
Iteration 8/25 | Loss: 0.00115464
Iteration 9/25 | Loss: 0.00115464
Iteration 10/25 | Loss: 0.00115464
Iteration 11/25 | Loss: 0.00115464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011546352179720998, 0.0011546352179720998, 0.0011546352179720998, 0.0011546352179720998, 0.0011546352179720998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011546352179720998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97643876
Iteration 2/25 | Loss: 0.00069337
Iteration 3/25 | Loss: 0.00069337
Iteration 4/25 | Loss: 0.00069337
Iteration 5/25 | Loss: 0.00069337
Iteration 6/25 | Loss: 0.00069337
Iteration 7/25 | Loss: 0.00069337
Iteration 8/25 | Loss: 0.00069337
Iteration 9/25 | Loss: 0.00069337
Iteration 10/25 | Loss: 0.00069337
Iteration 11/25 | Loss: 0.00069337
Iteration 12/25 | Loss: 0.00069337
Iteration 13/25 | Loss: 0.00069337
Iteration 14/25 | Loss: 0.00069337
Iteration 15/25 | Loss: 0.00069337
Iteration 16/25 | Loss: 0.00069337
Iteration 17/25 | Loss: 0.00069337
Iteration 18/25 | Loss: 0.00069337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006933659315109253, 0.0006933659315109253, 0.0006933659315109253, 0.0006933659315109253, 0.0006933659315109253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006933659315109253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069337
Iteration 2/1000 | Loss: 0.00007744
Iteration 3/1000 | Loss: 0.00005494
Iteration 4/1000 | Loss: 0.00004369
Iteration 5/1000 | Loss: 0.00003869
Iteration 6/1000 | Loss: 0.00003720
Iteration 7/1000 | Loss: 0.00003645
Iteration 8/1000 | Loss: 0.00003557
Iteration 9/1000 | Loss: 0.00003504
Iteration 10/1000 | Loss: 0.00003475
Iteration 11/1000 | Loss: 0.00003455
Iteration 12/1000 | Loss: 0.00003443
Iteration 13/1000 | Loss: 0.00003432
Iteration 14/1000 | Loss: 0.00003432
Iteration 15/1000 | Loss: 0.00003424
Iteration 16/1000 | Loss: 0.00003421
Iteration 17/1000 | Loss: 0.00003420
Iteration 18/1000 | Loss: 0.00003415
Iteration 19/1000 | Loss: 0.00003415
Iteration 20/1000 | Loss: 0.00003411
Iteration 21/1000 | Loss: 0.00003410
Iteration 22/1000 | Loss: 0.00003409
Iteration 23/1000 | Loss: 0.00003409
Iteration 24/1000 | Loss: 0.00003409
Iteration 25/1000 | Loss: 0.00003408
Iteration 26/1000 | Loss: 0.00003408
Iteration 27/1000 | Loss: 0.00003404
Iteration 28/1000 | Loss: 0.00003399
Iteration 29/1000 | Loss: 0.00003399
Iteration 30/1000 | Loss: 0.00003399
Iteration 31/1000 | Loss: 0.00003399
Iteration 32/1000 | Loss: 0.00003399
Iteration 33/1000 | Loss: 0.00003399
Iteration 34/1000 | Loss: 0.00003398
Iteration 35/1000 | Loss: 0.00003398
Iteration 36/1000 | Loss: 0.00003398
Iteration 37/1000 | Loss: 0.00003398
Iteration 38/1000 | Loss: 0.00003398
Iteration 39/1000 | Loss: 0.00003398
Iteration 40/1000 | Loss: 0.00003397
Iteration 41/1000 | Loss: 0.00003396
Iteration 42/1000 | Loss: 0.00003396
Iteration 43/1000 | Loss: 0.00003396
Iteration 44/1000 | Loss: 0.00003396
Iteration 45/1000 | Loss: 0.00003396
Iteration 46/1000 | Loss: 0.00003396
Iteration 47/1000 | Loss: 0.00003395
Iteration 48/1000 | Loss: 0.00003395
Iteration 49/1000 | Loss: 0.00003392
Iteration 50/1000 | Loss: 0.00003389
Iteration 51/1000 | Loss: 0.00003389
Iteration 52/1000 | Loss: 0.00003389
Iteration 53/1000 | Loss: 0.00003389
Iteration 54/1000 | Loss: 0.00003388
Iteration 55/1000 | Loss: 0.00003388
Iteration 56/1000 | Loss: 0.00003388
Iteration 57/1000 | Loss: 0.00003388
Iteration 58/1000 | Loss: 0.00003388
Iteration 59/1000 | Loss: 0.00003388
Iteration 60/1000 | Loss: 0.00003388
Iteration 61/1000 | Loss: 0.00003384
Iteration 62/1000 | Loss: 0.00003384
Iteration 63/1000 | Loss: 0.00003383
Iteration 64/1000 | Loss: 0.00003383
Iteration 65/1000 | Loss: 0.00003383
Iteration 66/1000 | Loss: 0.00003382
Iteration 67/1000 | Loss: 0.00003382
Iteration 68/1000 | Loss: 0.00003381
Iteration 69/1000 | Loss: 0.00003381
Iteration 70/1000 | Loss: 0.00003381
Iteration 71/1000 | Loss: 0.00003381
Iteration 72/1000 | Loss: 0.00003380
Iteration 73/1000 | Loss: 0.00003380
Iteration 74/1000 | Loss: 0.00003380
Iteration 75/1000 | Loss: 0.00003380
Iteration 76/1000 | Loss: 0.00003380
Iteration 77/1000 | Loss: 0.00003380
Iteration 78/1000 | Loss: 0.00003379
Iteration 79/1000 | Loss: 0.00003379
Iteration 80/1000 | Loss: 0.00003379
Iteration 81/1000 | Loss: 0.00003379
Iteration 82/1000 | Loss: 0.00003379
Iteration 83/1000 | Loss: 0.00003379
Iteration 84/1000 | Loss: 0.00003379
Iteration 85/1000 | Loss: 0.00003379
Iteration 86/1000 | Loss: 0.00003378
Iteration 87/1000 | Loss: 0.00003378
Iteration 88/1000 | Loss: 0.00003378
Iteration 89/1000 | Loss: 0.00003378
Iteration 90/1000 | Loss: 0.00003378
Iteration 91/1000 | Loss: 0.00003378
Iteration 92/1000 | Loss: 0.00003378
Iteration 93/1000 | Loss: 0.00003378
Iteration 94/1000 | Loss: 0.00003378
Iteration 95/1000 | Loss: 0.00003378
Iteration 96/1000 | Loss: 0.00003378
Iteration 97/1000 | Loss: 0.00003378
Iteration 98/1000 | Loss: 0.00003378
Iteration 99/1000 | Loss: 0.00003378
Iteration 100/1000 | Loss: 0.00003378
Iteration 101/1000 | Loss: 0.00003378
Iteration 102/1000 | Loss: 0.00003378
Iteration 103/1000 | Loss: 0.00003378
Iteration 104/1000 | Loss: 0.00003378
Iteration 105/1000 | Loss: 0.00003378
Iteration 106/1000 | Loss: 0.00003378
Iteration 107/1000 | Loss: 0.00003378
Iteration 108/1000 | Loss: 0.00003378
Iteration 109/1000 | Loss: 0.00003378
Iteration 110/1000 | Loss: 0.00003378
Iteration 111/1000 | Loss: 0.00003378
Iteration 112/1000 | Loss: 0.00003378
Iteration 113/1000 | Loss: 0.00003378
Iteration 114/1000 | Loss: 0.00003378
Iteration 115/1000 | Loss: 0.00003378
Iteration 116/1000 | Loss: 0.00003378
Iteration 117/1000 | Loss: 0.00003378
Iteration 118/1000 | Loss: 0.00003378
Iteration 119/1000 | Loss: 0.00003378
Iteration 120/1000 | Loss: 0.00003378
Iteration 121/1000 | Loss: 0.00003378
Iteration 122/1000 | Loss: 0.00003378
Iteration 123/1000 | Loss: 0.00003378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.377833854756318e-05, 3.377833854756318e-05, 3.377833854756318e-05, 3.377833854756318e-05, 3.377833854756318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.377833854756318e-05

Optimization complete. Final v2v error: 4.9398512840271 mm

Highest mean error: 5.007256031036377 mm for frame 1

Lowest mean error: 4.880449295043945 mm for frame 32

Saving results

Total time: 34.564855098724365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912933
Iteration 2/25 | Loss: 0.00136963
Iteration 3/25 | Loss: 0.00112285
Iteration 4/25 | Loss: 0.00109415
Iteration 5/25 | Loss: 0.00108666
Iteration 6/25 | Loss: 0.00108568
Iteration 7/25 | Loss: 0.00108568
Iteration 8/25 | Loss: 0.00108568
Iteration 9/25 | Loss: 0.00108568
Iteration 10/25 | Loss: 0.00108568
Iteration 11/25 | Loss: 0.00108568
Iteration 12/25 | Loss: 0.00108568
Iteration 13/25 | Loss: 0.00108568
Iteration 14/25 | Loss: 0.00108568
Iteration 15/25 | Loss: 0.00108568
Iteration 16/25 | Loss: 0.00108568
Iteration 17/25 | Loss: 0.00108568
Iteration 18/25 | Loss: 0.00108568
Iteration 19/25 | Loss: 0.00108568
Iteration 20/25 | Loss: 0.00108568
Iteration 21/25 | Loss: 0.00108568
Iteration 22/25 | Loss: 0.00108568
Iteration 23/25 | Loss: 0.00108568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010856779990717769, 0.0010856779990717769, 0.0010856779990717769, 0.0010856779990717769, 0.0010856779990717769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010856779990717769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92892021
Iteration 2/25 | Loss: 0.00056708
Iteration 3/25 | Loss: 0.00056707
Iteration 4/25 | Loss: 0.00056707
Iteration 5/25 | Loss: 0.00056707
Iteration 6/25 | Loss: 0.00056707
Iteration 7/25 | Loss: 0.00056707
Iteration 8/25 | Loss: 0.00056707
Iteration 9/25 | Loss: 0.00056707
Iteration 10/25 | Loss: 0.00056707
Iteration 11/25 | Loss: 0.00056707
Iteration 12/25 | Loss: 0.00056707
Iteration 13/25 | Loss: 0.00056707
Iteration 14/25 | Loss: 0.00056707
Iteration 15/25 | Loss: 0.00056707
Iteration 16/25 | Loss: 0.00056707
Iteration 17/25 | Loss: 0.00056707
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005670698010362685, 0.0005670698010362685, 0.0005670698010362685, 0.0005670698010362685, 0.0005670698010362685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005670698010362685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056707
Iteration 2/1000 | Loss: 0.00006543
Iteration 3/1000 | Loss: 0.00004650
Iteration 4/1000 | Loss: 0.00004068
Iteration 5/1000 | Loss: 0.00003821
Iteration 6/1000 | Loss: 0.00003657
Iteration 7/1000 | Loss: 0.00003547
Iteration 8/1000 | Loss: 0.00003468
Iteration 9/1000 | Loss: 0.00003420
Iteration 10/1000 | Loss: 0.00003395
Iteration 11/1000 | Loss: 0.00003382
Iteration 12/1000 | Loss: 0.00003382
Iteration 13/1000 | Loss: 0.00003382
Iteration 14/1000 | Loss: 0.00003379
Iteration 15/1000 | Loss: 0.00003376
Iteration 16/1000 | Loss: 0.00003376
Iteration 17/1000 | Loss: 0.00003376
Iteration 18/1000 | Loss: 0.00003375
Iteration 19/1000 | Loss: 0.00003375
Iteration 20/1000 | Loss: 0.00003374
Iteration 21/1000 | Loss: 0.00003373
Iteration 22/1000 | Loss: 0.00003369
Iteration 23/1000 | Loss: 0.00003368
Iteration 24/1000 | Loss: 0.00003368
Iteration 25/1000 | Loss: 0.00003368
Iteration 26/1000 | Loss: 0.00003368
Iteration 27/1000 | Loss: 0.00003368
Iteration 28/1000 | Loss: 0.00003368
Iteration 29/1000 | Loss: 0.00003368
Iteration 30/1000 | Loss: 0.00003368
Iteration 31/1000 | Loss: 0.00003368
Iteration 32/1000 | Loss: 0.00003368
Iteration 33/1000 | Loss: 0.00003368
Iteration 34/1000 | Loss: 0.00003368
Iteration 35/1000 | Loss: 0.00003368
Iteration 36/1000 | Loss: 0.00003368
Iteration 37/1000 | Loss: 0.00003368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [3.367655517649837e-05, 3.367655517649837e-05, 3.367655517649837e-05, 3.367655517649837e-05, 3.367655517649837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.367655517649837e-05

Optimization complete. Final v2v error: 4.895957946777344 mm

Highest mean error: 5.028801918029785 mm for frame 3

Lowest mean error: 4.790266990661621 mm for frame 95

Saving results

Total time: 27.586936712265015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060081
Iteration 2/25 | Loss: 0.00183049
Iteration 3/25 | Loss: 0.00131013
Iteration 4/25 | Loss: 0.00115944
Iteration 5/25 | Loss: 0.00111278
Iteration 6/25 | Loss: 0.00107438
Iteration 7/25 | Loss: 0.00106783
Iteration 8/25 | Loss: 0.00105607
Iteration 9/25 | Loss: 0.00100126
Iteration 10/25 | Loss: 0.00099445
Iteration 11/25 | Loss: 0.00099354
Iteration 12/25 | Loss: 0.00099329
Iteration 13/25 | Loss: 0.00099303
Iteration 14/25 | Loss: 0.00099292
Iteration 15/25 | Loss: 0.00099292
Iteration 16/25 | Loss: 0.00099291
Iteration 17/25 | Loss: 0.00099291
Iteration 18/25 | Loss: 0.00099291
Iteration 19/25 | Loss: 0.00099291
Iteration 20/25 | Loss: 0.00099291
Iteration 21/25 | Loss: 0.00099291
Iteration 22/25 | Loss: 0.00099291
Iteration 23/25 | Loss: 0.00099291
Iteration 24/25 | Loss: 0.00099291
Iteration 25/25 | Loss: 0.00099291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95394897
Iteration 2/25 | Loss: 0.00127090
Iteration 3/25 | Loss: 0.00127090
Iteration 4/25 | Loss: 0.00127090
Iteration 5/25 | Loss: 0.00127090
Iteration 6/25 | Loss: 0.00127090
Iteration 7/25 | Loss: 0.00127090
Iteration 8/25 | Loss: 0.00127090
Iteration 9/25 | Loss: 0.00127090
Iteration 10/25 | Loss: 0.00127090
Iteration 11/25 | Loss: 0.00127090
Iteration 12/25 | Loss: 0.00127090
Iteration 13/25 | Loss: 0.00127090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012708994327113032, 0.0012708994327113032, 0.0012708994327113032, 0.0012708994327113032, 0.0012708994327113032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012708994327113032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127090
Iteration 2/1000 | Loss: 0.00010368
Iteration 3/1000 | Loss: 0.00005735
Iteration 4/1000 | Loss: 0.00003605
Iteration 5/1000 | Loss: 0.00002988
Iteration 6/1000 | Loss: 0.00002643
Iteration 7/1000 | Loss: 0.00038487
Iteration 8/1000 | Loss: 0.00003933
Iteration 9/1000 | Loss: 0.00003236
Iteration 10/1000 | Loss: 0.00002973
Iteration 11/1000 | Loss: 0.00002833
Iteration 12/1000 | Loss: 0.00002713
Iteration 13/1000 | Loss: 0.00002552
Iteration 14/1000 | Loss: 0.00002573
Iteration 15/1000 | Loss: 0.00002351
Iteration 16/1000 | Loss: 0.00002299
Iteration 17/1000 | Loss: 0.00002270
Iteration 18/1000 | Loss: 0.00002238
Iteration 19/1000 | Loss: 0.00002208
Iteration 20/1000 | Loss: 0.00002172
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002139
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002135
Iteration 25/1000 | Loss: 0.00002134
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002132
Iteration 30/1000 | Loss: 0.00002131
Iteration 31/1000 | Loss: 0.00002131
Iteration 32/1000 | Loss: 0.00002131
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002131
Iteration 35/1000 | Loss: 0.00002131
Iteration 36/1000 | Loss: 0.00002131
Iteration 37/1000 | Loss: 0.00002131
Iteration 38/1000 | Loss: 0.00002131
Iteration 39/1000 | Loss: 0.00002131
Iteration 40/1000 | Loss: 0.00002131
Iteration 41/1000 | Loss: 0.00002131
Iteration 42/1000 | Loss: 0.00002131
Iteration 43/1000 | Loss: 0.00002131
Iteration 44/1000 | Loss: 0.00002130
Iteration 45/1000 | Loss: 0.00002130
Iteration 46/1000 | Loss: 0.00002129
Iteration 47/1000 | Loss: 0.00002129
Iteration 48/1000 | Loss: 0.00002129
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002127
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002127
Iteration 56/1000 | Loss: 0.00002127
Iteration 57/1000 | Loss: 0.00002127
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002126
Iteration 60/1000 | Loss: 0.00002126
Iteration 61/1000 | Loss: 0.00002126
Iteration 62/1000 | Loss: 0.00002126
Iteration 63/1000 | Loss: 0.00002126
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002125
Iteration 68/1000 | Loss: 0.00002125
Iteration 69/1000 | Loss: 0.00002125
Iteration 70/1000 | Loss: 0.00002125
Iteration 71/1000 | Loss: 0.00002125
Iteration 72/1000 | Loss: 0.00002125
Iteration 73/1000 | Loss: 0.00002125
Iteration 74/1000 | Loss: 0.00002125
Iteration 75/1000 | Loss: 0.00002125
Iteration 76/1000 | Loss: 0.00002125
Iteration 77/1000 | Loss: 0.00002125
Iteration 78/1000 | Loss: 0.00002125
Iteration 79/1000 | Loss: 0.00002125
Iteration 80/1000 | Loss: 0.00002125
Iteration 81/1000 | Loss: 0.00002125
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.1249450583127327e-05, 2.1249450583127327e-05, 2.1249450583127327e-05, 2.1249450583127327e-05, 2.1249450583127327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1249450583127327e-05

Optimization complete. Final v2v error: 4.045814514160156 mm

Highest mean error: 8.797770500183105 mm for frame 113

Lowest mean error: 3.637320041656494 mm for frame 61

Saving results

Total time: 57.26027250289917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402516
Iteration 2/25 | Loss: 0.00110369
Iteration 3/25 | Loss: 0.00102136
Iteration 4/25 | Loss: 0.00101199
Iteration 5/25 | Loss: 0.00100898
Iteration 6/25 | Loss: 0.00100790
Iteration 7/25 | Loss: 0.00100790
Iteration 8/25 | Loss: 0.00100790
Iteration 9/25 | Loss: 0.00100790
Iteration 10/25 | Loss: 0.00100790
Iteration 11/25 | Loss: 0.00100790
Iteration 12/25 | Loss: 0.00100790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010078988270834088, 0.0010078988270834088, 0.0010078988270834088, 0.0010078988270834088, 0.0010078988270834088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010078988270834088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43114531
Iteration 2/25 | Loss: 0.00135011
Iteration 3/25 | Loss: 0.00135010
Iteration 4/25 | Loss: 0.00135010
Iteration 5/25 | Loss: 0.00135010
Iteration 6/25 | Loss: 0.00135010
Iteration 7/25 | Loss: 0.00135010
Iteration 8/25 | Loss: 0.00135010
Iteration 9/25 | Loss: 0.00135010
Iteration 10/25 | Loss: 0.00135010
Iteration 11/25 | Loss: 0.00135010
Iteration 12/25 | Loss: 0.00135010
Iteration 13/25 | Loss: 0.00135010
Iteration 14/25 | Loss: 0.00135010
Iteration 15/25 | Loss: 0.00135010
Iteration 16/25 | Loss: 0.00135010
Iteration 17/25 | Loss: 0.00135010
Iteration 18/25 | Loss: 0.00135010
Iteration 19/25 | Loss: 0.00135010
Iteration 20/25 | Loss: 0.00135010
Iteration 21/25 | Loss: 0.00135010
Iteration 22/25 | Loss: 0.00135010
Iteration 23/25 | Loss: 0.00135010
Iteration 24/25 | Loss: 0.00135010
Iteration 25/25 | Loss: 0.00135010

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135010
Iteration 2/1000 | Loss: 0.00004938
Iteration 3/1000 | Loss: 0.00002548
Iteration 4/1000 | Loss: 0.00002105
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001786
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001751
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001750
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001735
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001728
Iteration 41/1000 | Loss: 0.00001728
Iteration 42/1000 | Loss: 0.00001728
Iteration 43/1000 | Loss: 0.00001728
Iteration 44/1000 | Loss: 0.00001728
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001725
Iteration 54/1000 | Loss: 0.00001725
Iteration 55/1000 | Loss: 0.00001725
Iteration 56/1000 | Loss: 0.00001724
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001722
Iteration 71/1000 | Loss: 0.00001722
Iteration 72/1000 | Loss: 0.00001722
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001720
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001720
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001719
Iteration 121/1000 | Loss: 0.00001719
Iteration 122/1000 | Loss: 0.00001719
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00001719
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.7186383047373965e-05, 1.7186383047373965e-05, 1.7186383047373965e-05, 1.7186383047373965e-05, 1.7186383047373965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7186383047373965e-05

Optimization complete. Final v2v error: 3.6632585525512695 mm

Highest mean error: 3.9240756034851074 mm for frame 85

Lowest mean error: 3.3830811977386475 mm for frame 0

Saving results

Total time: 32.2299427986145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594856
Iteration 2/25 | Loss: 0.00132077
Iteration 3/25 | Loss: 0.00111162
Iteration 4/25 | Loss: 0.00102186
Iteration 5/25 | Loss: 0.00100539
Iteration 6/25 | Loss: 0.00099873
Iteration 7/25 | Loss: 0.00100061
Iteration 8/25 | Loss: 0.00099651
Iteration 9/25 | Loss: 0.00100012
Iteration 10/25 | Loss: 0.00099743
Iteration 11/25 | Loss: 0.00099510
Iteration 12/25 | Loss: 0.00099517
Iteration 13/25 | Loss: 0.00099461
Iteration 14/25 | Loss: 0.00099398
Iteration 15/25 | Loss: 0.00099359
Iteration 16/25 | Loss: 0.00099335
Iteration 17/25 | Loss: 0.00099315
Iteration 18/25 | Loss: 0.00099308
Iteration 19/25 | Loss: 0.00099308
Iteration 20/25 | Loss: 0.00099308
Iteration 21/25 | Loss: 0.00099308
Iteration 22/25 | Loss: 0.00099308
Iteration 23/25 | Loss: 0.00099308
Iteration 24/25 | Loss: 0.00099308
Iteration 25/25 | Loss: 0.00099307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.22966146
Iteration 2/25 | Loss: 0.00123133
Iteration 3/25 | Loss: 0.00117844
Iteration 4/25 | Loss: 0.00117844
Iteration 5/25 | Loss: 0.00117844
Iteration 6/25 | Loss: 0.00117844
Iteration 7/25 | Loss: 0.00117844
Iteration 8/25 | Loss: 0.00117844
Iteration 9/25 | Loss: 0.00117844
Iteration 10/25 | Loss: 0.00117844
Iteration 11/25 | Loss: 0.00117844
Iteration 12/25 | Loss: 0.00117844
Iteration 13/25 | Loss: 0.00117843
Iteration 14/25 | Loss: 0.00117843
Iteration 15/25 | Loss: 0.00117843
Iteration 16/25 | Loss: 0.00117843
Iteration 17/25 | Loss: 0.00117843
Iteration 18/25 | Loss: 0.00117843
Iteration 19/25 | Loss: 0.00117843
Iteration 20/25 | Loss: 0.00117843
Iteration 21/25 | Loss: 0.00117844
Iteration 22/25 | Loss: 0.00117843
Iteration 23/25 | Loss: 0.00117843
Iteration 24/25 | Loss: 0.00117843
Iteration 25/25 | Loss: 0.00117843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117844
Iteration 2/1000 | Loss: 0.00004493
Iteration 3/1000 | Loss: 0.00002813
Iteration 4/1000 | Loss: 0.00002404
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002135
Iteration 7/1000 | Loss: 0.00002065
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001977
Iteration 10/1000 | Loss: 0.00001957
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001943
Iteration 21/1000 | Loss: 0.00001943
Iteration 22/1000 | Loss: 0.00001942
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001942
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001939
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001938
Iteration 41/1000 | Loss: 0.00001938
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001933
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001931
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001931
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001931
Iteration 81/1000 | Loss: 0.00001931
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001930
Iteration 85/1000 | Loss: 0.00001930
Iteration 86/1000 | Loss: 0.00001930
Iteration 87/1000 | Loss: 0.00001930
Iteration 88/1000 | Loss: 0.00001930
Iteration 89/1000 | Loss: 0.00001930
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001929
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001929
Iteration 102/1000 | Loss: 0.00001929
Iteration 103/1000 | Loss: 0.00001929
Iteration 104/1000 | Loss: 0.00001929
Iteration 105/1000 | Loss: 0.00001929
Iteration 106/1000 | Loss: 0.00001929
Iteration 107/1000 | Loss: 0.00001929
Iteration 108/1000 | Loss: 0.00001929
Iteration 109/1000 | Loss: 0.00001929
Iteration 110/1000 | Loss: 0.00001929
Iteration 111/1000 | Loss: 0.00001928
Iteration 112/1000 | Loss: 0.00001928
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001928
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001928
Iteration 120/1000 | Loss: 0.00001928
Iteration 121/1000 | Loss: 0.00001928
Iteration 122/1000 | Loss: 0.00001928
Iteration 123/1000 | Loss: 0.00001928
Iteration 124/1000 | Loss: 0.00001928
Iteration 125/1000 | Loss: 0.00001928
Iteration 126/1000 | Loss: 0.00001928
Iteration 127/1000 | Loss: 0.00001928
Iteration 128/1000 | Loss: 0.00001928
Iteration 129/1000 | Loss: 0.00001928
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.9283415895188227e-05, 1.9283415895188227e-05, 1.9283415895188227e-05, 1.9283415895188227e-05, 1.9283415895188227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9283415895188227e-05

Optimization complete. Final v2v error: 3.760998487472534 mm

Highest mean error: 8.891539573669434 mm for frame 214

Lowest mean error: 3.4160592555999756 mm for frame 104

Saving results

Total time: 62.11405301094055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468442
Iteration 2/25 | Loss: 0.00123562
Iteration 3/25 | Loss: 0.00114047
Iteration 4/25 | Loss: 0.00110629
Iteration 5/25 | Loss: 0.00109521
Iteration 6/25 | Loss: 0.00109348
Iteration 7/25 | Loss: 0.00109314
Iteration 8/25 | Loss: 0.00109314
Iteration 9/25 | Loss: 0.00109314
Iteration 10/25 | Loss: 0.00109314
Iteration 11/25 | Loss: 0.00109314
Iteration 12/25 | Loss: 0.00109314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001093140454031527, 0.001093140454031527, 0.001093140454031527, 0.001093140454031527, 0.001093140454031527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001093140454031527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27604759
Iteration 2/25 | Loss: 0.00104702
Iteration 3/25 | Loss: 0.00104702
Iteration 4/25 | Loss: 0.00104702
Iteration 5/25 | Loss: 0.00104701
Iteration 6/25 | Loss: 0.00104701
Iteration 7/25 | Loss: 0.00104701
Iteration 8/25 | Loss: 0.00104701
Iteration 9/25 | Loss: 0.00104701
Iteration 10/25 | Loss: 0.00104701
Iteration 11/25 | Loss: 0.00104701
Iteration 12/25 | Loss: 0.00104701
Iteration 13/25 | Loss: 0.00104701
Iteration 14/25 | Loss: 0.00104701
Iteration 15/25 | Loss: 0.00104701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010470139095559716, 0.0010470139095559716, 0.0010470139095559716, 0.0010470139095559716, 0.0010470139095559716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010470139095559716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104701
Iteration 2/1000 | Loss: 0.00007279
Iteration 3/1000 | Loss: 0.00005147
Iteration 4/1000 | Loss: 0.00003167
Iteration 5/1000 | Loss: 0.00002854
Iteration 6/1000 | Loss: 0.00002665
Iteration 7/1000 | Loss: 0.00002557
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002434
Iteration 10/1000 | Loss: 0.00002383
Iteration 11/1000 | Loss: 0.00002361
Iteration 12/1000 | Loss: 0.00002339
Iteration 13/1000 | Loss: 0.00002339
Iteration 14/1000 | Loss: 0.00002327
Iteration 15/1000 | Loss: 0.00002321
Iteration 16/1000 | Loss: 0.00002314
Iteration 17/1000 | Loss: 0.00002314
Iteration 18/1000 | Loss: 0.00002314
Iteration 19/1000 | Loss: 0.00002307
Iteration 20/1000 | Loss: 0.00002306
Iteration 21/1000 | Loss: 0.00002305
Iteration 22/1000 | Loss: 0.00002301
Iteration 23/1000 | Loss: 0.00002301
Iteration 24/1000 | Loss: 0.00002300
Iteration 25/1000 | Loss: 0.00002300
Iteration 26/1000 | Loss: 0.00002300
Iteration 27/1000 | Loss: 0.00002299
Iteration 28/1000 | Loss: 0.00002299
Iteration 29/1000 | Loss: 0.00002299
Iteration 30/1000 | Loss: 0.00002298
Iteration 31/1000 | Loss: 0.00002298
Iteration 32/1000 | Loss: 0.00002297
Iteration 33/1000 | Loss: 0.00002296
Iteration 34/1000 | Loss: 0.00002296
Iteration 35/1000 | Loss: 0.00002295
Iteration 36/1000 | Loss: 0.00002295
Iteration 37/1000 | Loss: 0.00002294
Iteration 38/1000 | Loss: 0.00002294
Iteration 39/1000 | Loss: 0.00002293
Iteration 40/1000 | Loss: 0.00002293
Iteration 41/1000 | Loss: 0.00002292
Iteration 42/1000 | Loss: 0.00002292
Iteration 43/1000 | Loss: 0.00002291
Iteration 44/1000 | Loss: 0.00002291
Iteration 45/1000 | Loss: 0.00002291
Iteration 46/1000 | Loss: 0.00002290
Iteration 47/1000 | Loss: 0.00002290
Iteration 48/1000 | Loss: 0.00002290
Iteration 49/1000 | Loss: 0.00002289
Iteration 50/1000 | Loss: 0.00002289
Iteration 51/1000 | Loss: 0.00002289
Iteration 52/1000 | Loss: 0.00002289
Iteration 53/1000 | Loss: 0.00002289
Iteration 54/1000 | Loss: 0.00002288
Iteration 55/1000 | Loss: 0.00002288
Iteration 56/1000 | Loss: 0.00002288
Iteration 57/1000 | Loss: 0.00002288
Iteration 58/1000 | Loss: 0.00002288
Iteration 59/1000 | Loss: 0.00002288
Iteration 60/1000 | Loss: 0.00002288
Iteration 61/1000 | Loss: 0.00002288
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002288
Iteration 66/1000 | Loss: 0.00002288
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002287
Iteration 71/1000 | Loss: 0.00002287
Iteration 72/1000 | Loss: 0.00002287
Iteration 73/1000 | Loss: 0.00002287
Iteration 74/1000 | Loss: 0.00002286
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002285
Iteration 78/1000 | Loss: 0.00002285
Iteration 79/1000 | Loss: 0.00002285
Iteration 80/1000 | Loss: 0.00002285
Iteration 81/1000 | Loss: 0.00002285
Iteration 82/1000 | Loss: 0.00002284
Iteration 83/1000 | Loss: 0.00002284
Iteration 84/1000 | Loss: 0.00002284
Iteration 85/1000 | Loss: 0.00002284
Iteration 86/1000 | Loss: 0.00002284
Iteration 87/1000 | Loss: 0.00002284
Iteration 88/1000 | Loss: 0.00002284
Iteration 89/1000 | Loss: 0.00002284
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002283
Iteration 104/1000 | Loss: 0.00002283
Iteration 105/1000 | Loss: 0.00002283
Iteration 106/1000 | Loss: 0.00002283
Iteration 107/1000 | Loss: 0.00002283
Iteration 108/1000 | Loss: 0.00002283
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002282
Iteration 112/1000 | Loss: 0.00002282
Iteration 113/1000 | Loss: 0.00002282
Iteration 114/1000 | Loss: 0.00002282
Iteration 115/1000 | Loss: 0.00002282
Iteration 116/1000 | Loss: 0.00002282
Iteration 117/1000 | Loss: 0.00002282
Iteration 118/1000 | Loss: 0.00002281
Iteration 119/1000 | Loss: 0.00002281
Iteration 120/1000 | Loss: 0.00002281
Iteration 121/1000 | Loss: 0.00002281
Iteration 122/1000 | Loss: 0.00002281
Iteration 123/1000 | Loss: 0.00002281
Iteration 124/1000 | Loss: 0.00002281
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002280
Iteration 127/1000 | Loss: 0.00002280
Iteration 128/1000 | Loss: 0.00002280
Iteration 129/1000 | Loss: 0.00002279
Iteration 130/1000 | Loss: 0.00002279
Iteration 131/1000 | Loss: 0.00002279
Iteration 132/1000 | Loss: 0.00002279
Iteration 133/1000 | Loss: 0.00002279
Iteration 134/1000 | Loss: 0.00002279
Iteration 135/1000 | Loss: 0.00002278
Iteration 136/1000 | Loss: 0.00002278
Iteration 137/1000 | Loss: 0.00002278
Iteration 138/1000 | Loss: 0.00002278
Iteration 139/1000 | Loss: 0.00002277
Iteration 140/1000 | Loss: 0.00002277
Iteration 141/1000 | Loss: 0.00002277
Iteration 142/1000 | Loss: 0.00002277
Iteration 143/1000 | Loss: 0.00002277
Iteration 144/1000 | Loss: 0.00002276
Iteration 145/1000 | Loss: 0.00002276
Iteration 146/1000 | Loss: 0.00002276
Iteration 147/1000 | Loss: 0.00002276
Iteration 148/1000 | Loss: 0.00002276
Iteration 149/1000 | Loss: 0.00002276
Iteration 150/1000 | Loss: 0.00002276
Iteration 151/1000 | Loss: 0.00002276
Iteration 152/1000 | Loss: 0.00002276
Iteration 153/1000 | Loss: 0.00002276
Iteration 154/1000 | Loss: 0.00002276
Iteration 155/1000 | Loss: 0.00002276
Iteration 156/1000 | Loss: 0.00002276
Iteration 157/1000 | Loss: 0.00002275
Iteration 158/1000 | Loss: 0.00002275
Iteration 159/1000 | Loss: 0.00002275
Iteration 160/1000 | Loss: 0.00002275
Iteration 161/1000 | Loss: 0.00002275
Iteration 162/1000 | Loss: 0.00002275
Iteration 163/1000 | Loss: 0.00002275
Iteration 164/1000 | Loss: 0.00002275
Iteration 165/1000 | Loss: 0.00002275
Iteration 166/1000 | Loss: 0.00002275
Iteration 167/1000 | Loss: 0.00002275
Iteration 168/1000 | Loss: 0.00002275
Iteration 169/1000 | Loss: 0.00002275
Iteration 170/1000 | Loss: 0.00002275
Iteration 171/1000 | Loss: 0.00002275
Iteration 172/1000 | Loss: 0.00002275
Iteration 173/1000 | Loss: 0.00002275
Iteration 174/1000 | Loss: 0.00002275
Iteration 175/1000 | Loss: 0.00002275
Iteration 176/1000 | Loss: 0.00002275
Iteration 177/1000 | Loss: 0.00002275
Iteration 178/1000 | Loss: 0.00002275
Iteration 179/1000 | Loss: 0.00002275
Iteration 180/1000 | Loss: 0.00002275
Iteration 181/1000 | Loss: 0.00002275
Iteration 182/1000 | Loss: 0.00002275
Iteration 183/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.2747874027118087e-05, 2.2747874027118087e-05, 2.2747874027118087e-05, 2.2747874027118087e-05, 2.2747874027118087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2747874027118087e-05

Optimization complete. Final v2v error: 4.047045707702637 mm

Highest mean error: 4.26690149307251 mm for frame 34

Lowest mean error: 3.6682791709899902 mm for frame 13

Saving results

Total time: 39.844993114471436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942433
Iteration 2/25 | Loss: 0.00226431
Iteration 3/25 | Loss: 0.00165637
Iteration 4/25 | Loss: 0.00144511
Iteration 5/25 | Loss: 0.00134750
Iteration 6/25 | Loss: 0.00133701
Iteration 7/25 | Loss: 0.00135799
Iteration 8/25 | Loss: 0.00132420
Iteration 9/25 | Loss: 0.00127300
Iteration 10/25 | Loss: 0.00124446
Iteration 11/25 | Loss: 0.00122291
Iteration 12/25 | Loss: 0.00121718
Iteration 13/25 | Loss: 0.00121152
Iteration 14/25 | Loss: 0.00120662
Iteration 15/25 | Loss: 0.00120796
Iteration 16/25 | Loss: 0.00120537
Iteration 17/25 | Loss: 0.00120068
Iteration 18/25 | Loss: 0.00119803
Iteration 19/25 | Loss: 0.00119715
Iteration 20/25 | Loss: 0.00118569
Iteration 21/25 | Loss: 0.00117990
Iteration 22/25 | Loss: 0.00117461
Iteration 23/25 | Loss: 0.00117415
Iteration 24/25 | Loss: 0.00116410
Iteration 25/25 | Loss: 0.00116157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14279842
Iteration 2/25 | Loss: 0.00117938
Iteration 3/25 | Loss: 0.00117937
Iteration 4/25 | Loss: 0.00117937
Iteration 5/25 | Loss: 0.00117937
Iteration 6/25 | Loss: 0.00117937
Iteration 7/25 | Loss: 0.00117937
Iteration 8/25 | Loss: 0.00117937
Iteration 9/25 | Loss: 0.00117937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0011793716112151742, 0.0011793716112151742, 0.0011793716112151742, 0.0011793716112151742, 0.0011793716112151742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011793716112151742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117937
Iteration 2/1000 | Loss: 0.00022330
Iteration 3/1000 | Loss: 0.00032956
Iteration 4/1000 | Loss: 0.00027940
Iteration 5/1000 | Loss: 0.00023066
Iteration 6/1000 | Loss: 0.00023566
Iteration 7/1000 | Loss: 0.00029964
Iteration 8/1000 | Loss: 0.00023386
Iteration 9/1000 | Loss: 0.00029747
Iteration 10/1000 | Loss: 0.00028186
Iteration 11/1000 | Loss: 0.00026097
Iteration 12/1000 | Loss: 0.00032353
Iteration 13/1000 | Loss: 0.00027814
Iteration 14/1000 | Loss: 0.00034390
Iteration 15/1000 | Loss: 0.00029564
Iteration 16/1000 | Loss: 0.00033836
Iteration 17/1000 | Loss: 0.00023837
Iteration 18/1000 | Loss: 0.00026596
Iteration 19/1000 | Loss: 0.00036513
Iteration 20/1000 | Loss: 0.00029460
Iteration 21/1000 | Loss: 0.00024634
Iteration 22/1000 | Loss: 0.00029436
Iteration 23/1000 | Loss: 0.00033263
Iteration 24/1000 | Loss: 0.00041102
Iteration 25/1000 | Loss: 0.00038222
Iteration 26/1000 | Loss: 0.00024712
Iteration 27/1000 | Loss: 0.00026640
Iteration 28/1000 | Loss: 0.00019623
Iteration 29/1000 | Loss: 0.00029220
Iteration 30/1000 | Loss: 0.00026967
Iteration 31/1000 | Loss: 0.00027212
Iteration 32/1000 | Loss: 0.00023809
Iteration 33/1000 | Loss: 0.00026751
Iteration 34/1000 | Loss: 0.00026722
Iteration 35/1000 | Loss: 0.00022998
Iteration 36/1000 | Loss: 0.00019119
Iteration 37/1000 | Loss: 0.00028428
Iteration 38/1000 | Loss: 0.00026203
Iteration 39/1000 | Loss: 0.00028035
Iteration 40/1000 | Loss: 0.00023768
Iteration 41/1000 | Loss: 0.00021776
Iteration 42/1000 | Loss: 0.00023857
Iteration 43/1000 | Loss: 0.00028522
Iteration 44/1000 | Loss: 0.00032319
Iteration 45/1000 | Loss: 0.00022502
Iteration 46/1000 | Loss: 0.00028781
Iteration 47/1000 | Loss: 0.00024713
Iteration 48/1000 | Loss: 0.00023074
Iteration 49/1000 | Loss: 0.00024742
Iteration 50/1000 | Loss: 0.00023496
Iteration 51/1000 | Loss: 0.00030040
Iteration 52/1000 | Loss: 0.00027426
Iteration 53/1000 | Loss: 0.00026709
Iteration 54/1000 | Loss: 0.00035544
Iteration 55/1000 | Loss: 0.00026297
Iteration 56/1000 | Loss: 0.00026156
Iteration 57/1000 | Loss: 0.00028454
Iteration 58/1000 | Loss: 0.00028548
Iteration 59/1000 | Loss: 0.00022905
Iteration 60/1000 | Loss: 0.00023063
Iteration 61/1000 | Loss: 0.00033811
Iteration 62/1000 | Loss: 0.00026055
Iteration 63/1000 | Loss: 0.00016537
Iteration 64/1000 | Loss: 0.00019478
Iteration 65/1000 | Loss: 0.00020634
Iteration 66/1000 | Loss: 0.00029116
Iteration 67/1000 | Loss: 0.00029659
Iteration 68/1000 | Loss: 0.00027763
Iteration 69/1000 | Loss: 0.00027690
Iteration 70/1000 | Loss: 0.00026624
Iteration 71/1000 | Loss: 0.00022190
Iteration 72/1000 | Loss: 0.00023094
Iteration 73/1000 | Loss: 0.00016181
Iteration 74/1000 | Loss: 0.00017152
Iteration 75/1000 | Loss: 0.00019797
Iteration 76/1000 | Loss: 0.00018044
Iteration 77/1000 | Loss: 0.00023954
Iteration 78/1000 | Loss: 0.00021662
Iteration 79/1000 | Loss: 0.00023586
Iteration 80/1000 | Loss: 0.00022165
Iteration 81/1000 | Loss: 0.00023485
Iteration 82/1000 | Loss: 0.00022416
Iteration 83/1000 | Loss: 0.00020148
Iteration 84/1000 | Loss: 0.00020471
Iteration 85/1000 | Loss: 0.00023255
Iteration 86/1000 | Loss: 0.00022127
Iteration 87/1000 | Loss: 0.00029755
Iteration 88/1000 | Loss: 0.00023602
Iteration 89/1000 | Loss: 0.00038705
Iteration 90/1000 | Loss: 0.00018855
Iteration 91/1000 | Loss: 0.00018370
Iteration 92/1000 | Loss: 0.00016546
Iteration 93/1000 | Loss: 0.00019222
Iteration 94/1000 | Loss: 0.00023484
Iteration 95/1000 | Loss: 0.00022166
Iteration 96/1000 | Loss: 0.00013647
Iteration 97/1000 | Loss: 0.00015532
Iteration 98/1000 | Loss: 0.00017388
Iteration 99/1000 | Loss: 0.00020803
Iteration 100/1000 | Loss: 0.00020238
Iteration 101/1000 | Loss: 0.00021194
Iteration 102/1000 | Loss: 0.00020878
Iteration 103/1000 | Loss: 0.00035267
Iteration 104/1000 | Loss: 0.00023984
Iteration 105/1000 | Loss: 0.00029332
Iteration 106/1000 | Loss: 0.00022630
Iteration 107/1000 | Loss: 0.00025170
Iteration 108/1000 | Loss: 0.00026203
Iteration 109/1000 | Loss: 0.00024221
Iteration 110/1000 | Loss: 0.00024646
Iteration 111/1000 | Loss: 0.00027545
Iteration 112/1000 | Loss: 0.00023876
Iteration 113/1000 | Loss: 0.00016522
Iteration 114/1000 | Loss: 0.00018101
Iteration 115/1000 | Loss: 0.00021043
Iteration 116/1000 | Loss: 0.00019146
Iteration 117/1000 | Loss: 0.00016544
Iteration 118/1000 | Loss: 0.00017140
Iteration 119/1000 | Loss: 0.00017734
Iteration 120/1000 | Loss: 0.00019179
Iteration 121/1000 | Loss: 0.00019789
Iteration 122/1000 | Loss: 0.00017307
Iteration 123/1000 | Loss: 0.00020073
Iteration 124/1000 | Loss: 0.00019656
Iteration 125/1000 | Loss: 0.00019665
Iteration 126/1000 | Loss: 0.00013522
Iteration 127/1000 | Loss: 0.00020508
Iteration 128/1000 | Loss: 0.00019198
Iteration 129/1000 | Loss: 0.00019746
Iteration 130/1000 | Loss: 0.00018973
Iteration 131/1000 | Loss: 0.00020514
Iteration 132/1000 | Loss: 0.00020565
Iteration 133/1000 | Loss: 0.00019494
Iteration 134/1000 | Loss: 0.00018510
Iteration 135/1000 | Loss: 0.00024254
Iteration 136/1000 | Loss: 0.00011726
Iteration 137/1000 | Loss: 0.00011061
Iteration 138/1000 | Loss: 0.00013775
Iteration 139/1000 | Loss: 0.00019815
Iteration 140/1000 | Loss: 0.00013174
Iteration 141/1000 | Loss: 0.00010844
Iteration 142/1000 | Loss: 0.00012591
Iteration 143/1000 | Loss: 0.00019502
Iteration 144/1000 | Loss: 0.00014508
Iteration 145/1000 | Loss: 0.00012683
Iteration 146/1000 | Loss: 0.00012954
Iteration 147/1000 | Loss: 0.00015276
Iteration 148/1000 | Loss: 0.00014083
Iteration 149/1000 | Loss: 0.00014096
Iteration 150/1000 | Loss: 0.00014963
Iteration 151/1000 | Loss: 0.00012643
Iteration 152/1000 | Loss: 0.00014747
Iteration 153/1000 | Loss: 0.00012286
Iteration 154/1000 | Loss: 0.00011279
Iteration 155/1000 | Loss: 0.00012040
Iteration 156/1000 | Loss: 0.00010760
Iteration 157/1000 | Loss: 0.00011252
Iteration 158/1000 | Loss: 0.00016759
Iteration 159/1000 | Loss: 0.00016902
Iteration 160/1000 | Loss: 0.00018993
Iteration 161/1000 | Loss: 0.00024068
Iteration 162/1000 | Loss: 0.00018530
Iteration 163/1000 | Loss: 0.00019176
Iteration 164/1000 | Loss: 0.00018442
Iteration 165/1000 | Loss: 0.00017604
Iteration 166/1000 | Loss: 0.00017093
Iteration 167/1000 | Loss: 0.00016522
Iteration 168/1000 | Loss: 0.00010299
Iteration 169/1000 | Loss: 0.00013437
Iteration 170/1000 | Loss: 0.00011481
Iteration 171/1000 | Loss: 0.00014974
Iteration 172/1000 | Loss: 0.00017225
Iteration 173/1000 | Loss: 0.00009660
Iteration 174/1000 | Loss: 0.00011740
Iteration 175/1000 | Loss: 0.00011351
Iteration 176/1000 | Loss: 0.00012218
Iteration 177/1000 | Loss: 0.00013703
Iteration 178/1000 | Loss: 0.00013999
Iteration 179/1000 | Loss: 0.00012707
Iteration 180/1000 | Loss: 0.00008611
Iteration 181/1000 | Loss: 0.00010437
Iteration 182/1000 | Loss: 0.00010546
Iteration 183/1000 | Loss: 0.00011479
Iteration 184/1000 | Loss: 0.00012152
Iteration 185/1000 | Loss: 0.00015099
Iteration 186/1000 | Loss: 0.00014489
Iteration 187/1000 | Loss: 0.00013462
Iteration 188/1000 | Loss: 0.00014589
Iteration 189/1000 | Loss: 0.00014844
Iteration 190/1000 | Loss: 0.00019024
Iteration 191/1000 | Loss: 0.00011199
Iteration 192/1000 | Loss: 0.00007955
Iteration 193/1000 | Loss: 0.00013330
Iteration 194/1000 | Loss: 0.00013288
Iteration 195/1000 | Loss: 0.00014936
Iteration 196/1000 | Loss: 0.00013258
Iteration 197/1000 | Loss: 0.00013563
Iteration 198/1000 | Loss: 0.00015492
Iteration 199/1000 | Loss: 0.00014485
Iteration 200/1000 | Loss: 0.00015638
Iteration 201/1000 | Loss: 0.00011359
Iteration 202/1000 | Loss: 0.00013523
Iteration 203/1000 | Loss: 0.00008899
Iteration 204/1000 | Loss: 0.00011243
Iteration 205/1000 | Loss: 0.00009453
Iteration 206/1000 | Loss: 0.00008665
Iteration 207/1000 | Loss: 0.00008553
Iteration 208/1000 | Loss: 0.00010408
Iteration 209/1000 | Loss: 0.00010790
Iteration 210/1000 | Loss: 0.00010626
Iteration 211/1000 | Loss: 0.00011800
Iteration 212/1000 | Loss: 0.00010222
Iteration 213/1000 | Loss: 0.00010626
Iteration 214/1000 | Loss: 0.00009693
Iteration 215/1000 | Loss: 0.00010200
Iteration 216/1000 | Loss: 0.00010736
Iteration 217/1000 | Loss: 0.00010972
Iteration 218/1000 | Loss: 0.00010930
Iteration 219/1000 | Loss: 0.00011098
Iteration 220/1000 | Loss: 0.00010645
Iteration 221/1000 | Loss: 0.00011661
Iteration 222/1000 | Loss: 0.00011230
Iteration 223/1000 | Loss: 0.00011518
Iteration 224/1000 | Loss: 0.00011233
Iteration 225/1000 | Loss: 0.00009560
Iteration 226/1000 | Loss: 0.00008394
Iteration 227/1000 | Loss: 0.00013789
Iteration 228/1000 | Loss: 0.00016782
Iteration 229/1000 | Loss: 0.00006696
Iteration 230/1000 | Loss: 0.00010612
Iteration 231/1000 | Loss: 0.00009465
Iteration 232/1000 | Loss: 0.00008354
Iteration 233/1000 | Loss: 0.00011684
Iteration 234/1000 | Loss: 0.00013099
Iteration 235/1000 | Loss: 0.00011978
Iteration 236/1000 | Loss: 0.00011169
Iteration 237/1000 | Loss: 0.00010728
Iteration 238/1000 | Loss: 0.00009876
Iteration 239/1000 | Loss: 0.00007646
Iteration 240/1000 | Loss: 0.00007739
Iteration 241/1000 | Loss: 0.00006312
Iteration 242/1000 | Loss: 0.00018490
Iteration 243/1000 | Loss: 0.00015980
Iteration 244/1000 | Loss: 0.00009592
Iteration 245/1000 | Loss: 0.00007622
Iteration 246/1000 | Loss: 0.00009077
Iteration 247/1000 | Loss: 0.00008905
Iteration 248/1000 | Loss: 0.00009898
Iteration 249/1000 | Loss: 0.00019937
Iteration 250/1000 | Loss: 0.00011620
Iteration 251/1000 | Loss: 0.00009712
Iteration 252/1000 | Loss: 0.00010024
Iteration 253/1000 | Loss: 0.00009372
Iteration 254/1000 | Loss: 0.00004986
Iteration 255/1000 | Loss: 0.00007377
Iteration 256/1000 | Loss: 0.00006700
Iteration 257/1000 | Loss: 0.00005542
Iteration 258/1000 | Loss: 0.00007455
Iteration 259/1000 | Loss: 0.00007333
Iteration 260/1000 | Loss: 0.00008014
Iteration 261/1000 | Loss: 0.00008553
Iteration 262/1000 | Loss: 0.00007091
Iteration 263/1000 | Loss: 0.00006628
Iteration 264/1000 | Loss: 0.00009340
Iteration 265/1000 | Loss: 0.00008555
Iteration 266/1000 | Loss: 0.00005628
Iteration 267/1000 | Loss: 0.00006960
Iteration 268/1000 | Loss: 0.00007694
Iteration 269/1000 | Loss: 0.00007844
Iteration 270/1000 | Loss: 0.00007412
Iteration 271/1000 | Loss: 0.00007763
Iteration 272/1000 | Loss: 0.00007814
Iteration 273/1000 | Loss: 0.00007399
Iteration 274/1000 | Loss: 0.00007666
Iteration 275/1000 | Loss: 0.00007763
Iteration 276/1000 | Loss: 0.00005710
Iteration 277/1000 | Loss: 0.00007774
Iteration 278/1000 | Loss: 0.00009022
Iteration 279/1000 | Loss: 0.00007726
Iteration 280/1000 | Loss: 0.00007650
Iteration 281/1000 | Loss: 0.00008723
Iteration 282/1000 | Loss: 0.00008229
Iteration 283/1000 | Loss: 0.00010390
Iteration 284/1000 | Loss: 0.00008091
Iteration 285/1000 | Loss: 0.00009411
Iteration 286/1000 | Loss: 0.00009593
Iteration 287/1000 | Loss: 0.00010066
Iteration 288/1000 | Loss: 0.00011882
Iteration 289/1000 | Loss: 0.00008626
Iteration 290/1000 | Loss: 0.00009261
Iteration 291/1000 | Loss: 0.00011826
Iteration 292/1000 | Loss: 0.00007248
Iteration 293/1000 | Loss: 0.00006027
Iteration 294/1000 | Loss: 0.00008149
Iteration 295/1000 | Loss: 0.00007524
Iteration 296/1000 | Loss: 0.00008153
Iteration 297/1000 | Loss: 0.00007849
Iteration 298/1000 | Loss: 0.00007259
Iteration 299/1000 | Loss: 0.00007289
Iteration 300/1000 | Loss: 0.00005818
Iteration 301/1000 | Loss: 0.00004401
Iteration 302/1000 | Loss: 0.00004106
Iteration 303/1000 | Loss: 0.00006462
Iteration 304/1000 | Loss: 0.00006846
Iteration 305/1000 | Loss: 0.00006545
Iteration 306/1000 | Loss: 0.00005480
Iteration 307/1000 | Loss: 0.00006308
Iteration 308/1000 | Loss: 0.00006656
Iteration 309/1000 | Loss: 0.00006816
Iteration 310/1000 | Loss: 0.00006115
Iteration 311/1000 | Loss: 0.00007923
Iteration 312/1000 | Loss: 0.00008662
Iteration 313/1000 | Loss: 0.00009172
Iteration 314/1000 | Loss: 0.00011169
Iteration 315/1000 | Loss: 0.00007464
Iteration 316/1000 | Loss: 0.00005614
Iteration 317/1000 | Loss: 0.00004446
Iteration 318/1000 | Loss: 0.00004224
Iteration 319/1000 | Loss: 0.00004864
Iteration 320/1000 | Loss: 0.00005173
Iteration 321/1000 | Loss: 0.00005101
Iteration 322/1000 | Loss: 0.00006387
Iteration 323/1000 | Loss: 0.00005702
Iteration 324/1000 | Loss: 0.00004834
Iteration 325/1000 | Loss: 0.00004112
Iteration 326/1000 | Loss: 0.00005692
Iteration 327/1000 | Loss: 0.00005875
Iteration 328/1000 | Loss: 0.00008686
Iteration 329/1000 | Loss: 0.00006599
Iteration 330/1000 | Loss: 0.00005123
Iteration 331/1000 | Loss: 0.00007432
Iteration 332/1000 | Loss: 0.00006068
Iteration 333/1000 | Loss: 0.00006637
Iteration 334/1000 | Loss: 0.00006122
Iteration 335/1000 | Loss: 0.00004033
Iteration 336/1000 | Loss: 0.00005210
Iteration 337/1000 | Loss: 0.00004932
Iteration 338/1000 | Loss: 0.00004702
Iteration 339/1000 | Loss: 0.00005321
Iteration 340/1000 | Loss: 0.00004834
Iteration 341/1000 | Loss: 0.00003291
Iteration 342/1000 | Loss: 0.00005506
Iteration 343/1000 | Loss: 0.00004881
Iteration 344/1000 | Loss: 0.00004010
Iteration 345/1000 | Loss: 0.00004255
Iteration 346/1000 | Loss: 0.00003923
Iteration 347/1000 | Loss: 0.00003798
Iteration 348/1000 | Loss: 0.00003899
Iteration 349/1000 | Loss: 0.00003911
Iteration 350/1000 | Loss: 0.00003942
Iteration 351/1000 | Loss: 0.00003875
Iteration 352/1000 | Loss: 0.00003953
Iteration 353/1000 | Loss: 0.00003903
Iteration 354/1000 | Loss: 0.00005842
Iteration 355/1000 | Loss: 0.00005646
Iteration 356/1000 | Loss: 0.00004242
Iteration 357/1000 | Loss: 0.00004302
Iteration 358/1000 | Loss: 0.00006271
Iteration 359/1000 | Loss: 0.00008128
Iteration 360/1000 | Loss: 0.00006388
Iteration 361/1000 | Loss: 0.00007454
Iteration 362/1000 | Loss: 0.00007121
Iteration 363/1000 | Loss: 0.00007267
Iteration 364/1000 | Loss: 0.00007065
Iteration 365/1000 | Loss: 0.00007072
Iteration 366/1000 | Loss: 0.00006792
Iteration 367/1000 | Loss: 0.00003755
Iteration 368/1000 | Loss: 0.00003554
Iteration 369/1000 | Loss: 0.00002941
Iteration 370/1000 | Loss: 0.00002733
Iteration 371/1000 | Loss: 0.00002613
Iteration 372/1000 | Loss: 0.00002474
Iteration 373/1000 | Loss: 0.00002386
Iteration 374/1000 | Loss: 0.00002322
Iteration 375/1000 | Loss: 0.00002263
Iteration 376/1000 | Loss: 0.00002223
Iteration 377/1000 | Loss: 0.00002196
Iteration 378/1000 | Loss: 0.00002187
Iteration 379/1000 | Loss: 0.00002185
Iteration 380/1000 | Loss: 0.00002184
Iteration 381/1000 | Loss: 0.00002183
Iteration 382/1000 | Loss: 0.00002183
Iteration 383/1000 | Loss: 0.00002182
Iteration 384/1000 | Loss: 0.00002180
Iteration 385/1000 | Loss: 0.00002179
Iteration 386/1000 | Loss: 0.00002179
Iteration 387/1000 | Loss: 0.00002179
Iteration 388/1000 | Loss: 0.00002179
Iteration 389/1000 | Loss: 0.00002178
Iteration 390/1000 | Loss: 0.00002178
Iteration 391/1000 | Loss: 0.00002178
Iteration 392/1000 | Loss: 0.00002178
Iteration 393/1000 | Loss: 0.00002178
Iteration 394/1000 | Loss: 0.00002178
Iteration 395/1000 | Loss: 0.00002178
Iteration 396/1000 | Loss: 0.00002178
Iteration 397/1000 | Loss: 0.00002178
Iteration 398/1000 | Loss: 0.00002178
Iteration 399/1000 | Loss: 0.00002178
Iteration 400/1000 | Loss: 0.00002178
Iteration 401/1000 | Loss: 0.00002178
Iteration 402/1000 | Loss: 0.00002178
Iteration 403/1000 | Loss: 0.00002178
Iteration 404/1000 | Loss: 0.00002178
Iteration 405/1000 | Loss: 0.00002178
Iteration 406/1000 | Loss: 0.00002178
Iteration 407/1000 | Loss: 0.00002178
Iteration 408/1000 | Loss: 0.00002178
Iteration 409/1000 | Loss: 0.00002178
Iteration 410/1000 | Loss: 0.00002178
Iteration 411/1000 | Loss: 0.00002178
Iteration 412/1000 | Loss: 0.00002178
Iteration 413/1000 | Loss: 0.00002178
Iteration 414/1000 | Loss: 0.00002178
Iteration 415/1000 | Loss: 0.00002178
Iteration 416/1000 | Loss: 0.00002178
Iteration 417/1000 | Loss: 0.00002178
Iteration 418/1000 | Loss: 0.00002178
Iteration 419/1000 | Loss: 0.00002178
Iteration 420/1000 | Loss: 0.00002178
Iteration 421/1000 | Loss: 0.00002178
Iteration 422/1000 | Loss: 0.00002178
Iteration 423/1000 | Loss: 0.00002178
Iteration 424/1000 | Loss: 0.00002178
Iteration 425/1000 | Loss: 0.00002178
Iteration 426/1000 | Loss: 0.00002178
Iteration 427/1000 | Loss: 0.00002178
Iteration 428/1000 | Loss: 0.00002178
Iteration 429/1000 | Loss: 0.00002178
Iteration 430/1000 | Loss: 0.00002178
Iteration 431/1000 | Loss: 0.00002178
Iteration 432/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 432. Stopping optimization.
Last 5 losses: [2.177995520469267e-05, 2.177995520469267e-05, 2.177995520469267e-05, 2.177995520469267e-05, 2.177995520469267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.177995520469267e-05

Optimization complete. Final v2v error: 3.9498350620269775 mm

Highest mean error: 12.078752517700195 mm for frame 25

Lowest mean error: 3.7102036476135254 mm for frame 69

Saving results

Total time: 664.602392911911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401843
Iteration 2/25 | Loss: 0.00128020
Iteration 3/25 | Loss: 0.00114252
Iteration 4/25 | Loss: 0.00111394
Iteration 5/25 | Loss: 0.00110305
Iteration 6/25 | Loss: 0.00110046
Iteration 7/25 | Loss: 0.00110005
Iteration 8/25 | Loss: 0.00110005
Iteration 9/25 | Loss: 0.00110005
Iteration 10/25 | Loss: 0.00110005
Iteration 11/25 | Loss: 0.00110005
Iteration 12/25 | Loss: 0.00110005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001100045396015048, 0.001100045396015048, 0.001100045396015048, 0.001100045396015048, 0.001100045396015048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001100045396015048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36580563
Iteration 2/25 | Loss: 0.00102843
Iteration 3/25 | Loss: 0.00102843
Iteration 4/25 | Loss: 0.00102843
Iteration 5/25 | Loss: 0.00102843
Iteration 6/25 | Loss: 0.00102843
Iteration 7/25 | Loss: 0.00102843
Iteration 8/25 | Loss: 0.00102843
Iteration 9/25 | Loss: 0.00102843
Iteration 10/25 | Loss: 0.00102843
Iteration 11/25 | Loss: 0.00102843
Iteration 12/25 | Loss: 0.00102843
Iteration 13/25 | Loss: 0.00102843
Iteration 14/25 | Loss: 0.00102843
Iteration 15/25 | Loss: 0.00102843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010284307645633817, 0.0010284307645633817, 0.0010284307645633817, 0.0010284307645633817, 0.0010284307645633817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010284307645633817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102843
Iteration 2/1000 | Loss: 0.00009930
Iteration 3/1000 | Loss: 0.00006635
Iteration 4/1000 | Loss: 0.00004641
Iteration 5/1000 | Loss: 0.00004112
Iteration 6/1000 | Loss: 0.00003753
Iteration 7/1000 | Loss: 0.00003578
Iteration 8/1000 | Loss: 0.00003487
Iteration 9/1000 | Loss: 0.00003411
Iteration 10/1000 | Loss: 0.00003343
Iteration 11/1000 | Loss: 0.00003288
Iteration 12/1000 | Loss: 0.00003261
Iteration 13/1000 | Loss: 0.00003254
Iteration 14/1000 | Loss: 0.00003250
Iteration 15/1000 | Loss: 0.00003248
Iteration 16/1000 | Loss: 0.00003229
Iteration 17/1000 | Loss: 0.00003212
Iteration 18/1000 | Loss: 0.00003209
Iteration 19/1000 | Loss: 0.00003209
Iteration 20/1000 | Loss: 0.00003202
Iteration 21/1000 | Loss: 0.00003201
Iteration 22/1000 | Loss: 0.00003200
Iteration 23/1000 | Loss: 0.00003200
Iteration 24/1000 | Loss: 0.00003199
Iteration 25/1000 | Loss: 0.00003199
Iteration 26/1000 | Loss: 0.00003199
Iteration 27/1000 | Loss: 0.00003199
Iteration 28/1000 | Loss: 0.00003198
Iteration 29/1000 | Loss: 0.00003198
Iteration 30/1000 | Loss: 0.00003198
Iteration 31/1000 | Loss: 0.00003198
Iteration 32/1000 | Loss: 0.00003197
Iteration 33/1000 | Loss: 0.00003197
Iteration 34/1000 | Loss: 0.00003197
Iteration 35/1000 | Loss: 0.00003196
Iteration 36/1000 | Loss: 0.00003196
Iteration 37/1000 | Loss: 0.00003196
Iteration 38/1000 | Loss: 0.00003195
Iteration 39/1000 | Loss: 0.00003195
Iteration 40/1000 | Loss: 0.00003195
Iteration 41/1000 | Loss: 0.00003195
Iteration 42/1000 | Loss: 0.00003194
Iteration 43/1000 | Loss: 0.00003194
Iteration 44/1000 | Loss: 0.00003194
Iteration 45/1000 | Loss: 0.00003193
Iteration 46/1000 | Loss: 0.00003193
Iteration 47/1000 | Loss: 0.00003193
Iteration 48/1000 | Loss: 0.00003193
Iteration 49/1000 | Loss: 0.00003193
Iteration 50/1000 | Loss: 0.00003193
Iteration 51/1000 | Loss: 0.00003192
Iteration 52/1000 | Loss: 0.00003192
Iteration 53/1000 | Loss: 0.00003192
Iteration 54/1000 | Loss: 0.00003192
Iteration 55/1000 | Loss: 0.00003191
Iteration 56/1000 | Loss: 0.00003191
Iteration 57/1000 | Loss: 0.00003191
Iteration 58/1000 | Loss: 0.00003190
Iteration 59/1000 | Loss: 0.00003190
Iteration 60/1000 | Loss: 0.00003190
Iteration 61/1000 | Loss: 0.00003189
Iteration 62/1000 | Loss: 0.00003189
Iteration 63/1000 | Loss: 0.00003189
Iteration 64/1000 | Loss: 0.00003188
Iteration 65/1000 | Loss: 0.00003188
Iteration 66/1000 | Loss: 0.00003188
Iteration 67/1000 | Loss: 0.00003188
Iteration 68/1000 | Loss: 0.00003187
Iteration 69/1000 | Loss: 0.00003187
Iteration 70/1000 | Loss: 0.00003186
Iteration 71/1000 | Loss: 0.00003186
Iteration 72/1000 | Loss: 0.00003186
Iteration 73/1000 | Loss: 0.00003186
Iteration 74/1000 | Loss: 0.00003186
Iteration 75/1000 | Loss: 0.00003185
Iteration 76/1000 | Loss: 0.00003185
Iteration 77/1000 | Loss: 0.00003185
Iteration 78/1000 | Loss: 0.00003185
Iteration 79/1000 | Loss: 0.00003185
Iteration 80/1000 | Loss: 0.00003185
Iteration 81/1000 | Loss: 0.00003185
Iteration 82/1000 | Loss: 0.00003184
Iteration 83/1000 | Loss: 0.00003184
Iteration 84/1000 | Loss: 0.00003184
Iteration 85/1000 | Loss: 0.00003184
Iteration 86/1000 | Loss: 0.00003184
Iteration 87/1000 | Loss: 0.00003184
Iteration 88/1000 | Loss: 0.00003184
Iteration 89/1000 | Loss: 0.00003184
Iteration 90/1000 | Loss: 0.00003184
Iteration 91/1000 | Loss: 0.00003184
Iteration 92/1000 | Loss: 0.00003184
Iteration 93/1000 | Loss: 0.00003184
Iteration 94/1000 | Loss: 0.00003184
Iteration 95/1000 | Loss: 0.00003184
Iteration 96/1000 | Loss: 0.00003183
Iteration 97/1000 | Loss: 0.00003183
Iteration 98/1000 | Loss: 0.00003183
Iteration 99/1000 | Loss: 0.00003183
Iteration 100/1000 | Loss: 0.00003183
Iteration 101/1000 | Loss: 0.00003183
Iteration 102/1000 | Loss: 0.00003183
Iteration 103/1000 | Loss: 0.00003183
Iteration 104/1000 | Loss: 0.00003183
Iteration 105/1000 | Loss: 0.00003183
Iteration 106/1000 | Loss: 0.00003183
Iteration 107/1000 | Loss: 0.00003183
Iteration 108/1000 | Loss: 0.00003183
Iteration 109/1000 | Loss: 0.00003183
Iteration 110/1000 | Loss: 0.00003183
Iteration 111/1000 | Loss: 0.00003182
Iteration 112/1000 | Loss: 0.00003182
Iteration 113/1000 | Loss: 0.00003182
Iteration 114/1000 | Loss: 0.00003182
Iteration 115/1000 | Loss: 0.00003182
Iteration 116/1000 | Loss: 0.00003182
Iteration 117/1000 | Loss: 0.00003182
Iteration 118/1000 | Loss: 0.00003182
Iteration 119/1000 | Loss: 0.00003182
Iteration 120/1000 | Loss: 0.00003181
Iteration 121/1000 | Loss: 0.00003181
Iteration 122/1000 | Loss: 0.00003181
Iteration 123/1000 | Loss: 0.00003181
Iteration 124/1000 | Loss: 0.00003181
Iteration 125/1000 | Loss: 0.00003181
Iteration 126/1000 | Loss: 0.00003181
Iteration 127/1000 | Loss: 0.00003181
Iteration 128/1000 | Loss: 0.00003181
Iteration 129/1000 | Loss: 0.00003181
Iteration 130/1000 | Loss: 0.00003180
Iteration 131/1000 | Loss: 0.00003180
Iteration 132/1000 | Loss: 0.00003180
Iteration 133/1000 | Loss: 0.00003180
Iteration 134/1000 | Loss: 0.00003180
Iteration 135/1000 | Loss: 0.00003180
Iteration 136/1000 | Loss: 0.00003180
Iteration 137/1000 | Loss: 0.00003180
Iteration 138/1000 | Loss: 0.00003180
Iteration 139/1000 | Loss: 0.00003180
Iteration 140/1000 | Loss: 0.00003179
Iteration 141/1000 | Loss: 0.00003179
Iteration 142/1000 | Loss: 0.00003179
Iteration 143/1000 | Loss: 0.00003179
Iteration 144/1000 | Loss: 0.00003179
Iteration 145/1000 | Loss: 0.00003179
Iteration 146/1000 | Loss: 0.00003179
Iteration 147/1000 | Loss: 0.00003179
Iteration 148/1000 | Loss: 0.00003179
Iteration 149/1000 | Loss: 0.00003179
Iteration 150/1000 | Loss: 0.00003179
Iteration 151/1000 | Loss: 0.00003179
Iteration 152/1000 | Loss: 0.00003178
Iteration 153/1000 | Loss: 0.00003178
Iteration 154/1000 | Loss: 0.00003178
Iteration 155/1000 | Loss: 0.00003178
Iteration 156/1000 | Loss: 0.00003178
Iteration 157/1000 | Loss: 0.00003178
Iteration 158/1000 | Loss: 0.00003178
Iteration 159/1000 | Loss: 0.00003178
Iteration 160/1000 | Loss: 0.00003178
Iteration 161/1000 | Loss: 0.00003178
Iteration 162/1000 | Loss: 0.00003178
Iteration 163/1000 | Loss: 0.00003178
Iteration 164/1000 | Loss: 0.00003178
Iteration 165/1000 | Loss: 0.00003178
Iteration 166/1000 | Loss: 0.00003178
Iteration 167/1000 | Loss: 0.00003178
Iteration 168/1000 | Loss: 0.00003178
Iteration 169/1000 | Loss: 0.00003178
Iteration 170/1000 | Loss: 0.00003178
Iteration 171/1000 | Loss: 0.00003178
Iteration 172/1000 | Loss: 0.00003178
Iteration 173/1000 | Loss: 0.00003178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.1777381082065403e-05, 3.1777381082065403e-05, 3.1777381082065403e-05, 3.1777381082065403e-05, 3.1777381082065403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1777381082065403e-05

Optimization complete. Final v2v error: 4.748953342437744 mm

Highest mean error: 5.248836994171143 mm for frame 92

Lowest mean error: 4.197122097015381 mm for frame 36

Saving results

Total time: 41.678996562957764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444338
Iteration 2/25 | Loss: 0.00121139
Iteration 3/25 | Loss: 0.00112571
Iteration 4/25 | Loss: 0.00110304
Iteration 5/25 | Loss: 0.00109645
Iteration 6/25 | Loss: 0.00109513
Iteration 7/25 | Loss: 0.00109513
Iteration 8/25 | Loss: 0.00109513
Iteration 9/25 | Loss: 0.00109513
Iteration 10/25 | Loss: 0.00109513
Iteration 11/25 | Loss: 0.00109513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010951340664178133, 0.0010951340664178133, 0.0010951340664178133, 0.0010951340664178133, 0.0010951340664178133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010951340664178133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44148970
Iteration 2/25 | Loss: 0.00105606
Iteration 3/25 | Loss: 0.00105605
Iteration 4/25 | Loss: 0.00105605
Iteration 5/25 | Loss: 0.00105605
Iteration 6/25 | Loss: 0.00105605
Iteration 7/25 | Loss: 0.00105605
Iteration 8/25 | Loss: 0.00105605
Iteration 9/25 | Loss: 0.00105605
Iteration 10/25 | Loss: 0.00105605
Iteration 11/25 | Loss: 0.00105605
Iteration 12/25 | Loss: 0.00105605
Iteration 13/25 | Loss: 0.00105605
Iteration 14/25 | Loss: 0.00105605
Iteration 15/25 | Loss: 0.00105605
Iteration 16/25 | Loss: 0.00105605
Iteration 17/25 | Loss: 0.00105605
Iteration 18/25 | Loss: 0.00105605
Iteration 19/25 | Loss: 0.00105605
Iteration 20/25 | Loss: 0.00105605
Iteration 21/25 | Loss: 0.00105605
Iteration 22/25 | Loss: 0.00105605
Iteration 23/25 | Loss: 0.00105605
Iteration 24/25 | Loss: 0.00105605
Iteration 25/25 | Loss: 0.00105605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105605
Iteration 2/1000 | Loss: 0.00007371
Iteration 3/1000 | Loss: 0.00004270
Iteration 4/1000 | Loss: 0.00003790
Iteration 5/1000 | Loss: 0.00003406
Iteration 6/1000 | Loss: 0.00003214
Iteration 7/1000 | Loss: 0.00003094
Iteration 8/1000 | Loss: 0.00003011
Iteration 9/1000 | Loss: 0.00002937
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002877
Iteration 12/1000 | Loss: 0.00002874
Iteration 13/1000 | Loss: 0.00002866
Iteration 14/1000 | Loss: 0.00002864
Iteration 15/1000 | Loss: 0.00002864
Iteration 16/1000 | Loss: 0.00002862
Iteration 17/1000 | Loss: 0.00002862
Iteration 18/1000 | Loss: 0.00002859
Iteration 19/1000 | Loss: 0.00002858
Iteration 20/1000 | Loss: 0.00002857
Iteration 21/1000 | Loss: 0.00002857
Iteration 22/1000 | Loss: 0.00002856
Iteration 23/1000 | Loss: 0.00002856
Iteration 24/1000 | Loss: 0.00002856
Iteration 25/1000 | Loss: 0.00002856
Iteration 26/1000 | Loss: 0.00002855
Iteration 27/1000 | Loss: 0.00002854
Iteration 28/1000 | Loss: 0.00002854
Iteration 29/1000 | Loss: 0.00002853
Iteration 30/1000 | Loss: 0.00002853
Iteration 31/1000 | Loss: 0.00002853
Iteration 32/1000 | Loss: 0.00002853
Iteration 33/1000 | Loss: 0.00002853
Iteration 34/1000 | Loss: 0.00002852
Iteration 35/1000 | Loss: 0.00002852
Iteration 36/1000 | Loss: 0.00002852
Iteration 37/1000 | Loss: 0.00002852
Iteration 38/1000 | Loss: 0.00002852
Iteration 39/1000 | Loss: 0.00002851
Iteration 40/1000 | Loss: 0.00002851
Iteration 41/1000 | Loss: 0.00002851
Iteration 42/1000 | Loss: 0.00002851
Iteration 43/1000 | Loss: 0.00002851
Iteration 44/1000 | Loss: 0.00002851
Iteration 45/1000 | Loss: 0.00002850
Iteration 46/1000 | Loss: 0.00002850
Iteration 47/1000 | Loss: 0.00002849
Iteration 48/1000 | Loss: 0.00002849
Iteration 49/1000 | Loss: 0.00002848
Iteration 50/1000 | Loss: 0.00002848
Iteration 51/1000 | Loss: 0.00002848
Iteration 52/1000 | Loss: 0.00002847
Iteration 53/1000 | Loss: 0.00002847
Iteration 54/1000 | Loss: 0.00002847
Iteration 55/1000 | Loss: 0.00002846
Iteration 56/1000 | Loss: 0.00002846
Iteration 57/1000 | Loss: 0.00002846
Iteration 58/1000 | Loss: 0.00002846
Iteration 59/1000 | Loss: 0.00002845
Iteration 60/1000 | Loss: 0.00002845
Iteration 61/1000 | Loss: 0.00002845
Iteration 62/1000 | Loss: 0.00002845
Iteration 63/1000 | Loss: 0.00002845
Iteration 64/1000 | Loss: 0.00002845
Iteration 65/1000 | Loss: 0.00002844
Iteration 66/1000 | Loss: 0.00002844
Iteration 67/1000 | Loss: 0.00002844
Iteration 68/1000 | Loss: 0.00002844
Iteration 69/1000 | Loss: 0.00002843
Iteration 70/1000 | Loss: 0.00002843
Iteration 71/1000 | Loss: 0.00002843
Iteration 72/1000 | Loss: 0.00002843
Iteration 73/1000 | Loss: 0.00002842
Iteration 74/1000 | Loss: 0.00002842
Iteration 75/1000 | Loss: 0.00002842
Iteration 76/1000 | Loss: 0.00002842
Iteration 77/1000 | Loss: 0.00002842
Iteration 78/1000 | Loss: 0.00002842
Iteration 79/1000 | Loss: 0.00002841
Iteration 80/1000 | Loss: 0.00002841
Iteration 81/1000 | Loss: 0.00002841
Iteration 82/1000 | Loss: 0.00002841
Iteration 83/1000 | Loss: 0.00002840
Iteration 84/1000 | Loss: 0.00002840
Iteration 85/1000 | Loss: 0.00002840
Iteration 86/1000 | Loss: 0.00002840
Iteration 87/1000 | Loss: 0.00002840
Iteration 88/1000 | Loss: 0.00002840
Iteration 89/1000 | Loss: 0.00002840
Iteration 90/1000 | Loss: 0.00002840
Iteration 91/1000 | Loss: 0.00002840
Iteration 92/1000 | Loss: 0.00002840
Iteration 93/1000 | Loss: 0.00002840
Iteration 94/1000 | Loss: 0.00002839
Iteration 95/1000 | Loss: 0.00002839
Iteration 96/1000 | Loss: 0.00002839
Iteration 97/1000 | Loss: 0.00002838
Iteration 98/1000 | Loss: 0.00002838
Iteration 99/1000 | Loss: 0.00002838
Iteration 100/1000 | Loss: 0.00002838
Iteration 101/1000 | Loss: 0.00002838
Iteration 102/1000 | Loss: 0.00002838
Iteration 103/1000 | Loss: 0.00002838
Iteration 104/1000 | Loss: 0.00002838
Iteration 105/1000 | Loss: 0.00002838
Iteration 106/1000 | Loss: 0.00002838
Iteration 107/1000 | Loss: 0.00002838
Iteration 108/1000 | Loss: 0.00002838
Iteration 109/1000 | Loss: 0.00002837
Iteration 110/1000 | Loss: 0.00002837
Iteration 111/1000 | Loss: 0.00002837
Iteration 112/1000 | Loss: 0.00002837
Iteration 113/1000 | Loss: 0.00002837
Iteration 114/1000 | Loss: 0.00002837
Iteration 115/1000 | Loss: 0.00002837
Iteration 116/1000 | Loss: 0.00002837
Iteration 117/1000 | Loss: 0.00002837
Iteration 118/1000 | Loss: 0.00002837
Iteration 119/1000 | Loss: 0.00002837
Iteration 120/1000 | Loss: 0.00002837
Iteration 121/1000 | Loss: 0.00002837
Iteration 122/1000 | Loss: 0.00002836
Iteration 123/1000 | Loss: 0.00002836
Iteration 124/1000 | Loss: 0.00002836
Iteration 125/1000 | Loss: 0.00002836
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002836
Iteration 128/1000 | Loss: 0.00002836
Iteration 129/1000 | Loss: 0.00002836
Iteration 130/1000 | Loss: 0.00002836
Iteration 131/1000 | Loss: 0.00002836
Iteration 132/1000 | Loss: 0.00002836
Iteration 133/1000 | Loss: 0.00002836
Iteration 134/1000 | Loss: 0.00002836
Iteration 135/1000 | Loss: 0.00002836
Iteration 136/1000 | Loss: 0.00002835
Iteration 137/1000 | Loss: 0.00002835
Iteration 138/1000 | Loss: 0.00002835
Iteration 139/1000 | Loss: 0.00002835
Iteration 140/1000 | Loss: 0.00002835
Iteration 141/1000 | Loss: 0.00002835
Iteration 142/1000 | Loss: 0.00002835
Iteration 143/1000 | Loss: 0.00002835
Iteration 144/1000 | Loss: 0.00002835
Iteration 145/1000 | Loss: 0.00002834
Iteration 146/1000 | Loss: 0.00002834
Iteration 147/1000 | Loss: 0.00002834
Iteration 148/1000 | Loss: 0.00002834
Iteration 149/1000 | Loss: 0.00002834
Iteration 150/1000 | Loss: 0.00002834
Iteration 151/1000 | Loss: 0.00002834
Iteration 152/1000 | Loss: 0.00002834
Iteration 153/1000 | Loss: 0.00002833
Iteration 154/1000 | Loss: 0.00002833
Iteration 155/1000 | Loss: 0.00002833
Iteration 156/1000 | Loss: 0.00002833
Iteration 157/1000 | Loss: 0.00002833
Iteration 158/1000 | Loss: 0.00002833
Iteration 159/1000 | Loss: 0.00002832
Iteration 160/1000 | Loss: 0.00002832
Iteration 161/1000 | Loss: 0.00002832
Iteration 162/1000 | Loss: 0.00002832
Iteration 163/1000 | Loss: 0.00002832
Iteration 164/1000 | Loss: 0.00002832
Iteration 165/1000 | Loss: 0.00002832
Iteration 166/1000 | Loss: 0.00002832
Iteration 167/1000 | Loss: 0.00002832
Iteration 168/1000 | Loss: 0.00002832
Iteration 169/1000 | Loss: 0.00002832
Iteration 170/1000 | Loss: 0.00002832
Iteration 171/1000 | Loss: 0.00002832
Iteration 172/1000 | Loss: 0.00002832
Iteration 173/1000 | Loss: 0.00002832
Iteration 174/1000 | Loss: 0.00002832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.8319278499111533e-05, 2.8319278499111533e-05, 2.8319278499111533e-05, 2.8319278499111533e-05, 2.8319278499111533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8319278499111533e-05

Optimization complete. Final v2v error: 4.556860446929932 mm

Highest mean error: 5.061516284942627 mm for frame 13

Lowest mean error: 4.105557918548584 mm for frame 29

Saving results

Total time: 42.49573278427124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00665598
Iteration 2/25 | Loss: 0.00132153
Iteration 3/25 | Loss: 0.00109154
Iteration 4/25 | Loss: 0.00105228
Iteration 5/25 | Loss: 0.00104686
Iteration 6/25 | Loss: 0.00104602
Iteration 7/25 | Loss: 0.00104602
Iteration 8/25 | Loss: 0.00104602
Iteration 9/25 | Loss: 0.00104602
Iteration 10/25 | Loss: 0.00104602
Iteration 11/25 | Loss: 0.00104602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010460175108164549, 0.0010460175108164549, 0.0010460175108164549, 0.0010460175108164549, 0.0010460175108164549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010460175108164549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41679919
Iteration 2/25 | Loss: 0.00091578
Iteration 3/25 | Loss: 0.00091574
Iteration 4/25 | Loss: 0.00091574
Iteration 5/25 | Loss: 0.00091574
Iteration 6/25 | Loss: 0.00091574
Iteration 7/25 | Loss: 0.00091574
Iteration 8/25 | Loss: 0.00091574
Iteration 9/25 | Loss: 0.00091574
Iteration 10/25 | Loss: 0.00091574
Iteration 11/25 | Loss: 0.00091574
Iteration 12/25 | Loss: 0.00091574
Iteration 13/25 | Loss: 0.00091574
Iteration 14/25 | Loss: 0.00091574
Iteration 15/25 | Loss: 0.00091574
Iteration 16/25 | Loss: 0.00091574
Iteration 17/25 | Loss: 0.00091574
Iteration 18/25 | Loss: 0.00091574
Iteration 19/25 | Loss: 0.00091574
Iteration 20/25 | Loss: 0.00091574
Iteration 21/25 | Loss: 0.00091574
Iteration 22/25 | Loss: 0.00091574
Iteration 23/25 | Loss: 0.00091574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009157397435046732, 0.0009157397435046732, 0.0009157397435046732, 0.0009157397435046732, 0.0009157397435046732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009157397435046732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091574
Iteration 2/1000 | Loss: 0.00004550
Iteration 3/1000 | Loss: 0.00002704
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001814
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001739
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001699
Iteration 12/1000 | Loss: 0.00001683
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001672
Iteration 22/1000 | Loss: 0.00001671
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001671
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001671
Iteration 32/1000 | Loss: 0.00001671
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001670
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001668
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001666
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001664
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001661
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001660
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001660
Iteration 61/1000 | Loss: 0.00001660
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001659
Iteration 65/1000 | Loss: 0.00001659
Iteration 66/1000 | Loss: 0.00001659
Iteration 67/1000 | Loss: 0.00001659
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001657
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Iteration 103/1000 | Loss: 0.00001656
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001655
Iteration 111/1000 | Loss: 0.00001655
Iteration 112/1000 | Loss: 0.00001655
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001655
Iteration 115/1000 | Loss: 0.00001655
Iteration 116/1000 | Loss: 0.00001655
Iteration 117/1000 | Loss: 0.00001655
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001655
Iteration 122/1000 | Loss: 0.00001655
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001655
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001655
Iteration 130/1000 | Loss: 0.00001655
Iteration 131/1000 | Loss: 0.00001655
Iteration 132/1000 | Loss: 0.00001655
Iteration 133/1000 | Loss: 0.00001655
Iteration 134/1000 | Loss: 0.00001655
Iteration 135/1000 | Loss: 0.00001655
Iteration 136/1000 | Loss: 0.00001655
Iteration 137/1000 | Loss: 0.00001655
Iteration 138/1000 | Loss: 0.00001655
Iteration 139/1000 | Loss: 0.00001655
Iteration 140/1000 | Loss: 0.00001655
Iteration 141/1000 | Loss: 0.00001655
Iteration 142/1000 | Loss: 0.00001655
Iteration 143/1000 | Loss: 0.00001655
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.6553405657759868e-05, 1.6553405657759868e-05, 1.6553405657759868e-05, 1.6553405657759868e-05, 1.6553405657759868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6553405657759868e-05

Optimization complete. Final v2v error: 3.560185432434082 mm

Highest mean error: 3.839289903640747 mm for frame 137

Lowest mean error: 3.3290932178497314 mm for frame 173

Saving results

Total time: 34.9686017036438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096033
Iteration 2/25 | Loss: 0.00332986
Iteration 3/25 | Loss: 0.00237047
Iteration 4/25 | Loss: 0.00228302
Iteration 5/25 | Loss: 0.00214280
Iteration 6/25 | Loss: 0.00208736
Iteration 7/25 | Loss: 0.00206975
Iteration 8/25 | Loss: 0.00205741
Iteration 9/25 | Loss: 0.00200388
Iteration 10/25 | Loss: 0.00198780
Iteration 11/25 | Loss: 0.00195694
Iteration 12/25 | Loss: 0.00194675
Iteration 13/25 | Loss: 0.00193549
Iteration 14/25 | Loss: 0.00193486
Iteration 15/25 | Loss: 0.00191786
Iteration 16/25 | Loss: 0.00191879
Iteration 17/25 | Loss: 0.00191062
Iteration 18/25 | Loss: 0.00190927
Iteration 19/25 | Loss: 0.00190857
Iteration 20/25 | Loss: 0.00190801
Iteration 21/25 | Loss: 0.00190758
Iteration 22/25 | Loss: 0.00191074
Iteration 23/25 | Loss: 0.00190760
Iteration 24/25 | Loss: 0.00190673
Iteration 25/25 | Loss: 0.00190626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39603698
Iteration 2/25 | Loss: 0.01769167
Iteration 3/25 | Loss: 0.01217430
Iteration 4/25 | Loss: 0.01217412
Iteration 5/25 | Loss: 0.01217412
Iteration 6/25 | Loss: 0.01217412
Iteration 7/25 | Loss: 0.01217412
Iteration 8/25 | Loss: 0.01217412
Iteration 9/25 | Loss: 0.01217412
Iteration 10/25 | Loss: 0.01217412
Iteration 11/25 | Loss: 0.01217412
Iteration 12/25 | Loss: 0.01217412
Iteration 13/25 | Loss: 0.01217412
Iteration 14/25 | Loss: 0.01217412
Iteration 15/25 | Loss: 0.01217412
Iteration 16/25 | Loss: 0.01217412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.012174118310213089, 0.012174118310213089, 0.012174118310213089, 0.012174118310213089, 0.012174118310213089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.012174118310213089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01217412
Iteration 2/1000 | Loss: 0.00814436
Iteration 3/1000 | Loss: 0.01891853
Iteration 4/1000 | Loss: 0.02463504
Iteration 5/1000 | Loss: 0.01607324
Iteration 6/1000 | Loss: 0.01094843
Iteration 7/1000 | Loss: 0.00633966
Iteration 8/1000 | Loss: 0.00641026
Iteration 9/1000 | Loss: 0.00320311
Iteration 10/1000 | Loss: 0.00215175
Iteration 11/1000 | Loss: 0.00522430
Iteration 12/1000 | Loss: 0.00796847
Iteration 13/1000 | Loss: 0.00735358
Iteration 14/1000 | Loss: 0.00292396
Iteration 15/1000 | Loss: 0.00188414
Iteration 16/1000 | Loss: 0.00523962
Iteration 17/1000 | Loss: 0.00270226
Iteration 18/1000 | Loss: 0.00280688
Iteration 19/1000 | Loss: 0.00604879
Iteration 20/1000 | Loss: 0.00269382
Iteration 21/1000 | Loss: 0.00234881
Iteration 22/1000 | Loss: 0.00138855
Iteration 23/1000 | Loss: 0.00197260
Iteration 24/1000 | Loss: 0.00309136
Iteration 25/1000 | Loss: 0.00356284
Iteration 26/1000 | Loss: 0.00308289
Iteration 27/1000 | Loss: 0.00244266
Iteration 28/1000 | Loss: 0.00268721
Iteration 29/1000 | Loss: 0.00179555
Iteration 30/1000 | Loss: 0.00287533
Iteration 31/1000 | Loss: 0.00218630
Iteration 32/1000 | Loss: 0.00262172
Iteration 33/1000 | Loss: 0.00237946
Iteration 34/1000 | Loss: 0.00152288
Iteration 35/1000 | Loss: 0.00327069
Iteration 36/1000 | Loss: 0.00199418
Iteration 37/1000 | Loss: 0.00256110
Iteration 38/1000 | Loss: 0.00168253
Iteration 39/1000 | Loss: 0.00336268
Iteration 40/1000 | Loss: 0.00179535
Iteration 41/1000 | Loss: 0.00218940
Iteration 42/1000 | Loss: 0.00350440
Iteration 43/1000 | Loss: 0.00331669
Iteration 44/1000 | Loss: 0.00341312
Iteration 45/1000 | Loss: 0.00340892
Iteration 46/1000 | Loss: 0.00315775
Iteration 47/1000 | Loss: 0.00282636
Iteration 48/1000 | Loss: 0.00218095
Iteration 49/1000 | Loss: 0.00268299
Iteration 50/1000 | Loss: 0.00339778
Iteration 51/1000 | Loss: 0.00150714
Iteration 52/1000 | Loss: 0.00235558
Iteration 53/1000 | Loss: 0.00225240
Iteration 54/1000 | Loss: 0.00244250
Iteration 55/1000 | Loss: 0.00187921
Iteration 56/1000 | Loss: 0.00131789
Iteration 57/1000 | Loss: 0.00252211
Iteration 58/1000 | Loss: 0.00233554
Iteration 59/1000 | Loss: 0.00160720
Iteration 60/1000 | Loss: 0.00145597
Iteration 61/1000 | Loss: 0.00217692
Iteration 62/1000 | Loss: 0.00161682
Iteration 63/1000 | Loss: 0.00243465
Iteration 64/1000 | Loss: 0.00232100
Iteration 65/1000 | Loss: 0.00329947
Iteration 66/1000 | Loss: 0.00181250
Iteration 67/1000 | Loss: 0.00257756
Iteration 68/1000 | Loss: 0.00342788
Iteration 69/1000 | Loss: 0.00266840
Iteration 70/1000 | Loss: 0.00318225
Iteration 71/1000 | Loss: 0.00268443
Iteration 72/1000 | Loss: 0.00555164
Iteration 73/1000 | Loss: 0.00299460
Iteration 74/1000 | Loss: 0.00391484
Iteration 75/1000 | Loss: 0.00274367
Iteration 76/1000 | Loss: 0.00395145
Iteration 77/1000 | Loss: 0.00148856
Iteration 78/1000 | Loss: 0.00270329
Iteration 79/1000 | Loss: 0.00466341
Iteration 80/1000 | Loss: 0.00296175
Iteration 81/1000 | Loss: 0.00226392
Iteration 82/1000 | Loss: 0.00391188
Iteration 83/1000 | Loss: 0.00588963
Iteration 84/1000 | Loss: 0.00246282
Iteration 85/1000 | Loss: 0.00261198
Iteration 86/1000 | Loss: 0.00256350
Iteration 87/1000 | Loss: 0.00133917
Iteration 88/1000 | Loss: 0.00135491
Iteration 89/1000 | Loss: 0.00222230
Iteration 90/1000 | Loss: 0.00264708
Iteration 91/1000 | Loss: 0.00279689
Iteration 92/1000 | Loss: 0.00199897
Iteration 93/1000 | Loss: 0.00142441
Iteration 94/1000 | Loss: 0.00156570
Iteration 95/1000 | Loss: 0.00174650
Iteration 96/1000 | Loss: 0.00247179
Iteration 97/1000 | Loss: 0.00253630
Iteration 98/1000 | Loss: 0.00206113
Iteration 99/1000 | Loss: 0.00193227
Iteration 100/1000 | Loss: 0.00191427
Iteration 101/1000 | Loss: 0.00183265
Iteration 102/1000 | Loss: 0.00168222
Iteration 103/1000 | Loss: 0.00206746
Iteration 104/1000 | Loss: 0.00253113
Iteration 105/1000 | Loss: 0.00163729
Iteration 106/1000 | Loss: 0.00170597
Iteration 107/1000 | Loss: 0.00311507
Iteration 108/1000 | Loss: 0.00144873
Iteration 109/1000 | Loss: 0.00193950
Iteration 110/1000 | Loss: 0.00191308
Iteration 111/1000 | Loss: 0.00182711
Iteration 112/1000 | Loss: 0.00260625
Iteration 113/1000 | Loss: 0.00212005
Iteration 114/1000 | Loss: 0.00189659
Iteration 115/1000 | Loss: 0.00159069
Iteration 116/1000 | Loss: 0.00164725
Iteration 117/1000 | Loss: 0.00166708
Iteration 118/1000 | Loss: 0.00145514
Iteration 119/1000 | Loss: 0.00126189
Iteration 120/1000 | Loss: 0.00168814
Iteration 121/1000 | Loss: 0.00091906
Iteration 122/1000 | Loss: 0.00111558
Iteration 123/1000 | Loss: 0.00087057
Iteration 124/1000 | Loss: 0.00198060
Iteration 125/1000 | Loss: 0.00115781
Iteration 126/1000 | Loss: 0.00142978
Iteration 127/1000 | Loss: 0.00151412
Iteration 128/1000 | Loss: 0.00125651
Iteration 129/1000 | Loss: 0.00132235
Iteration 130/1000 | Loss: 0.00264119
Iteration 131/1000 | Loss: 0.00211652
Iteration 132/1000 | Loss: 0.00191594
Iteration 133/1000 | Loss: 0.00324269
Iteration 134/1000 | Loss: 0.00096655
Iteration 135/1000 | Loss: 0.00168733
Iteration 136/1000 | Loss: 0.00119305
Iteration 137/1000 | Loss: 0.00211186
Iteration 138/1000 | Loss: 0.00086013
Iteration 139/1000 | Loss: 0.00081127
Iteration 140/1000 | Loss: 0.00047667
Iteration 141/1000 | Loss: 0.00086817
Iteration 142/1000 | Loss: 0.00073180
Iteration 143/1000 | Loss: 0.00093481
Iteration 144/1000 | Loss: 0.00131361
Iteration 145/1000 | Loss: 0.00075644
Iteration 146/1000 | Loss: 0.00053061
Iteration 147/1000 | Loss: 0.00084049
Iteration 148/1000 | Loss: 0.00154063
Iteration 149/1000 | Loss: 0.00046580
Iteration 150/1000 | Loss: 0.00064535
Iteration 151/1000 | Loss: 0.00063370
Iteration 152/1000 | Loss: 0.00060630
Iteration 153/1000 | Loss: 0.00051735
Iteration 154/1000 | Loss: 0.00185766
Iteration 155/1000 | Loss: 0.00122318
Iteration 156/1000 | Loss: 0.00162715
Iteration 157/1000 | Loss: 0.00089592
Iteration 158/1000 | Loss: 0.00069110
Iteration 159/1000 | Loss: 0.00089021
Iteration 160/1000 | Loss: 0.00072728
Iteration 161/1000 | Loss: 0.00045596
Iteration 162/1000 | Loss: 0.00033258
Iteration 163/1000 | Loss: 0.00038362
Iteration 164/1000 | Loss: 0.00057157
Iteration 165/1000 | Loss: 0.00057224
Iteration 166/1000 | Loss: 0.00044860
Iteration 167/1000 | Loss: 0.00031947
Iteration 168/1000 | Loss: 0.00032786
Iteration 169/1000 | Loss: 0.00033569
Iteration 170/1000 | Loss: 0.00027963
Iteration 171/1000 | Loss: 0.00019987
Iteration 172/1000 | Loss: 0.00020943
Iteration 173/1000 | Loss: 0.00026806
Iteration 174/1000 | Loss: 0.00016004
Iteration 175/1000 | Loss: 0.00025033
Iteration 176/1000 | Loss: 0.00132954
Iteration 177/1000 | Loss: 0.00103916
Iteration 178/1000 | Loss: 0.00125613
Iteration 179/1000 | Loss: 0.00079726
Iteration 180/1000 | Loss: 0.00126251
Iteration 181/1000 | Loss: 0.00071575
Iteration 182/1000 | Loss: 0.00083118
Iteration 183/1000 | Loss: 0.00150837
Iteration 184/1000 | Loss: 0.00022609
Iteration 185/1000 | Loss: 0.00037319
Iteration 186/1000 | Loss: 0.00038725
Iteration 187/1000 | Loss: 0.00040088
Iteration 188/1000 | Loss: 0.00027196
Iteration 189/1000 | Loss: 0.00023357
Iteration 190/1000 | Loss: 0.00123320
Iteration 191/1000 | Loss: 0.00085764
Iteration 192/1000 | Loss: 0.00020960
Iteration 193/1000 | Loss: 0.00036986
Iteration 194/1000 | Loss: 0.00167232
Iteration 195/1000 | Loss: 0.00026025
Iteration 196/1000 | Loss: 0.00096050
Iteration 197/1000 | Loss: 0.00067991
Iteration 198/1000 | Loss: 0.00011556
Iteration 199/1000 | Loss: 0.00030640
Iteration 200/1000 | Loss: 0.00012509
Iteration 201/1000 | Loss: 0.00061274
Iteration 202/1000 | Loss: 0.00021133
Iteration 203/1000 | Loss: 0.00040694
Iteration 204/1000 | Loss: 0.00045619
Iteration 205/1000 | Loss: 0.00061554
Iteration 206/1000 | Loss: 0.00016631
Iteration 207/1000 | Loss: 0.00031816
Iteration 208/1000 | Loss: 0.00053199
Iteration 209/1000 | Loss: 0.00010964
Iteration 210/1000 | Loss: 0.00046558
Iteration 211/1000 | Loss: 0.00036219
Iteration 212/1000 | Loss: 0.00027553
Iteration 213/1000 | Loss: 0.00048519
Iteration 214/1000 | Loss: 0.00030332
Iteration 215/1000 | Loss: 0.00026952
Iteration 216/1000 | Loss: 0.00059263
Iteration 217/1000 | Loss: 0.00055354
Iteration 218/1000 | Loss: 0.00023268
Iteration 219/1000 | Loss: 0.00021522
Iteration 220/1000 | Loss: 0.00020490
Iteration 221/1000 | Loss: 0.00079343
Iteration 222/1000 | Loss: 0.00033952
Iteration 223/1000 | Loss: 0.00104417
Iteration 224/1000 | Loss: 0.00026245
Iteration 225/1000 | Loss: 0.00024949
Iteration 226/1000 | Loss: 0.00017273
Iteration 227/1000 | Loss: 0.00007244
Iteration 228/1000 | Loss: 0.00025984
Iteration 229/1000 | Loss: 0.00009760
Iteration 230/1000 | Loss: 0.00007201
Iteration 231/1000 | Loss: 0.00004847
Iteration 232/1000 | Loss: 0.00032600
Iteration 233/1000 | Loss: 0.00011419
Iteration 234/1000 | Loss: 0.00006785
Iteration 235/1000 | Loss: 0.00010947
Iteration 236/1000 | Loss: 0.00054655
Iteration 237/1000 | Loss: 0.00041116
Iteration 238/1000 | Loss: 0.00044750
Iteration 239/1000 | Loss: 0.00017426
Iteration 240/1000 | Loss: 0.00018104
Iteration 241/1000 | Loss: 0.00005217
Iteration 242/1000 | Loss: 0.00004732
Iteration 243/1000 | Loss: 0.00004483
Iteration 244/1000 | Loss: 0.00004337
Iteration 245/1000 | Loss: 0.00010842
Iteration 246/1000 | Loss: 0.00004085
Iteration 247/1000 | Loss: 0.00003966
Iteration 248/1000 | Loss: 0.00003889
Iteration 249/1000 | Loss: 0.00015353
Iteration 250/1000 | Loss: 0.00074957
Iteration 251/1000 | Loss: 0.00011612
Iteration 252/1000 | Loss: 0.00017250
Iteration 253/1000 | Loss: 0.00028433
Iteration 254/1000 | Loss: 0.00014274
Iteration 255/1000 | Loss: 0.00049216
Iteration 256/1000 | Loss: 0.00059645
Iteration 257/1000 | Loss: 0.00047832
Iteration 258/1000 | Loss: 0.00004625
Iteration 259/1000 | Loss: 0.00009766
Iteration 260/1000 | Loss: 0.00007907
Iteration 261/1000 | Loss: 0.00003989
Iteration 262/1000 | Loss: 0.00003669
Iteration 263/1000 | Loss: 0.00003587
Iteration 264/1000 | Loss: 0.00003529
Iteration 265/1000 | Loss: 0.00003493
Iteration 266/1000 | Loss: 0.00003481
Iteration 267/1000 | Loss: 0.00003456
Iteration 268/1000 | Loss: 0.00007763
Iteration 269/1000 | Loss: 0.00011510
Iteration 270/1000 | Loss: 0.00019251
Iteration 271/1000 | Loss: 0.00095089
Iteration 272/1000 | Loss: 0.00015762
Iteration 273/1000 | Loss: 0.00049570
Iteration 274/1000 | Loss: 0.00017329
Iteration 275/1000 | Loss: 0.00037614
Iteration 276/1000 | Loss: 0.00016347
Iteration 277/1000 | Loss: 0.00024816
Iteration 278/1000 | Loss: 0.00035123
Iteration 279/1000 | Loss: 0.00016179
Iteration 280/1000 | Loss: 0.00019620
Iteration 281/1000 | Loss: 0.00004248
Iteration 282/1000 | Loss: 0.00041616
Iteration 283/1000 | Loss: 0.00003740
Iteration 284/1000 | Loss: 0.00003578
Iteration 285/1000 | Loss: 0.00003492
Iteration 286/1000 | Loss: 0.00003429
Iteration 287/1000 | Loss: 0.00003390
Iteration 288/1000 | Loss: 0.00015026
Iteration 289/1000 | Loss: 0.00004660
Iteration 290/1000 | Loss: 0.00015661
Iteration 291/1000 | Loss: 0.00010255
Iteration 292/1000 | Loss: 0.00014994
Iteration 293/1000 | Loss: 0.00012710
Iteration 294/1000 | Loss: 0.00011803
Iteration 295/1000 | Loss: 0.00005798
Iteration 296/1000 | Loss: 0.00003613
Iteration 297/1000 | Loss: 0.00034688
Iteration 298/1000 | Loss: 0.00040329
Iteration 299/1000 | Loss: 0.00004063
Iteration 300/1000 | Loss: 0.00035363
Iteration 301/1000 | Loss: 0.00018628
Iteration 302/1000 | Loss: 0.00047729
Iteration 303/1000 | Loss: 0.00015338
Iteration 304/1000 | Loss: 0.00025663
Iteration 305/1000 | Loss: 0.00022600
Iteration 306/1000 | Loss: 0.00016057
Iteration 307/1000 | Loss: 0.00004317
Iteration 308/1000 | Loss: 0.00013433
Iteration 309/1000 | Loss: 0.00010994
Iteration 310/1000 | Loss: 0.00003783
Iteration 311/1000 | Loss: 0.00027864
Iteration 312/1000 | Loss: 0.00004296
Iteration 313/1000 | Loss: 0.00003948
Iteration 314/1000 | Loss: 0.00006911
Iteration 315/1000 | Loss: 0.00003948
Iteration 316/1000 | Loss: 0.00010340
Iteration 317/1000 | Loss: 0.00033841
Iteration 318/1000 | Loss: 0.00015390
Iteration 319/1000 | Loss: 0.00009776
Iteration 320/1000 | Loss: 0.00022261
Iteration 321/1000 | Loss: 0.00005246
Iteration 322/1000 | Loss: 0.00006100
Iteration 323/1000 | Loss: 0.00011386
Iteration 324/1000 | Loss: 0.00005676
Iteration 325/1000 | Loss: 0.00011854
Iteration 326/1000 | Loss: 0.00005386
Iteration 327/1000 | Loss: 0.00011931
Iteration 328/1000 | Loss: 0.00013698
Iteration 329/1000 | Loss: 0.00011398
Iteration 330/1000 | Loss: 0.00006505
Iteration 331/1000 | Loss: 0.00013171
Iteration 332/1000 | Loss: 0.00008550
Iteration 333/1000 | Loss: 0.00005390
Iteration 334/1000 | Loss: 0.00006529
Iteration 335/1000 | Loss: 0.00008360
Iteration 336/1000 | Loss: 0.00010806
Iteration 337/1000 | Loss: 0.00008610
Iteration 338/1000 | Loss: 0.00016883
Iteration 339/1000 | Loss: 0.00010543
Iteration 340/1000 | Loss: 0.00036220
Iteration 341/1000 | Loss: 0.00008843
Iteration 342/1000 | Loss: 0.00009999
Iteration 343/1000 | Loss: 0.00008866
Iteration 344/1000 | Loss: 0.00006199
Iteration 345/1000 | Loss: 0.00003851
Iteration 346/1000 | Loss: 0.00019580
Iteration 347/1000 | Loss: 0.00003550
Iteration 348/1000 | Loss: 0.00003475
Iteration 349/1000 | Loss: 0.00003446
Iteration 350/1000 | Loss: 0.00003408
Iteration 351/1000 | Loss: 0.00003372
Iteration 352/1000 | Loss: 0.00003349
Iteration 353/1000 | Loss: 0.00003344
Iteration 354/1000 | Loss: 0.00003335
Iteration 355/1000 | Loss: 0.00003307
Iteration 356/1000 | Loss: 0.00022607
Iteration 357/1000 | Loss: 0.00066660
Iteration 358/1000 | Loss: 0.00003267
Iteration 359/1000 | Loss: 0.00003206
Iteration 360/1000 | Loss: 0.00014236
Iteration 361/1000 | Loss: 0.00003182
Iteration 362/1000 | Loss: 0.00003163
Iteration 363/1000 | Loss: 0.00003162
Iteration 364/1000 | Loss: 0.00003161
Iteration 365/1000 | Loss: 0.00003161
Iteration 366/1000 | Loss: 0.00003160
Iteration 367/1000 | Loss: 0.00003160
Iteration 368/1000 | Loss: 0.00003159
Iteration 369/1000 | Loss: 0.00003159
Iteration 370/1000 | Loss: 0.00003159
Iteration 371/1000 | Loss: 0.00003158
Iteration 372/1000 | Loss: 0.00003155
Iteration 373/1000 | Loss: 0.00003154
Iteration 374/1000 | Loss: 0.00003154
Iteration 375/1000 | Loss: 0.00003153
Iteration 376/1000 | Loss: 0.00003153
Iteration 377/1000 | Loss: 0.00003151
Iteration 378/1000 | Loss: 0.00003151
Iteration 379/1000 | Loss: 0.00003150
Iteration 380/1000 | Loss: 0.00003150
Iteration 381/1000 | Loss: 0.00003150
Iteration 382/1000 | Loss: 0.00003150
Iteration 383/1000 | Loss: 0.00003150
Iteration 384/1000 | Loss: 0.00003150
Iteration 385/1000 | Loss: 0.00003150
Iteration 386/1000 | Loss: 0.00003150
Iteration 387/1000 | Loss: 0.00003150
Iteration 388/1000 | Loss: 0.00003150
Iteration 389/1000 | Loss: 0.00003149
Iteration 390/1000 | Loss: 0.00003149
Iteration 391/1000 | Loss: 0.00003149
Iteration 392/1000 | Loss: 0.00003149
Iteration 393/1000 | Loss: 0.00003149
Iteration 394/1000 | Loss: 0.00003149
Iteration 395/1000 | Loss: 0.00003149
Iteration 396/1000 | Loss: 0.00003149
Iteration 397/1000 | Loss: 0.00003149
Iteration 398/1000 | Loss: 0.00003149
Iteration 399/1000 | Loss: 0.00003149
Iteration 400/1000 | Loss: 0.00003149
Iteration 401/1000 | Loss: 0.00003149
Iteration 402/1000 | Loss: 0.00003149
Iteration 403/1000 | Loss: 0.00003149
Iteration 404/1000 | Loss: 0.00003149
Iteration 405/1000 | Loss: 0.00003148
Iteration 406/1000 | Loss: 0.00003148
Iteration 407/1000 | Loss: 0.00003148
Iteration 408/1000 | Loss: 0.00003148
Iteration 409/1000 | Loss: 0.00003148
Iteration 410/1000 | Loss: 0.00003148
Iteration 411/1000 | Loss: 0.00003148
Iteration 412/1000 | Loss: 0.00003148
Iteration 413/1000 | Loss: 0.00003148
Iteration 414/1000 | Loss: 0.00003148
Iteration 415/1000 | Loss: 0.00003148
Iteration 416/1000 | Loss: 0.00003148
Iteration 417/1000 | Loss: 0.00003148
Iteration 418/1000 | Loss: 0.00003148
Iteration 419/1000 | Loss: 0.00003148
Iteration 420/1000 | Loss: 0.00003148
Iteration 421/1000 | Loss: 0.00003148
Iteration 422/1000 | Loss: 0.00003147
Iteration 423/1000 | Loss: 0.00003147
Iteration 424/1000 | Loss: 0.00003147
Iteration 425/1000 | Loss: 0.00003146
Iteration 426/1000 | Loss: 0.00003146
Iteration 427/1000 | Loss: 0.00003146
Iteration 428/1000 | Loss: 0.00003146
Iteration 429/1000 | Loss: 0.00003146
Iteration 430/1000 | Loss: 0.00003145
Iteration 431/1000 | Loss: 0.00003145
Iteration 432/1000 | Loss: 0.00003145
Iteration 433/1000 | Loss: 0.00003145
Iteration 434/1000 | Loss: 0.00003145
Iteration 435/1000 | Loss: 0.00003145
Iteration 436/1000 | Loss: 0.00003145
Iteration 437/1000 | Loss: 0.00003145
Iteration 438/1000 | Loss: 0.00003145
Iteration 439/1000 | Loss: 0.00003144
Iteration 440/1000 | Loss: 0.00003144
Iteration 441/1000 | Loss: 0.00003144
Iteration 442/1000 | Loss: 0.00003144
Iteration 443/1000 | Loss: 0.00003144
Iteration 444/1000 | Loss: 0.00003144
Iteration 445/1000 | Loss: 0.00003144
Iteration 446/1000 | Loss: 0.00003144
Iteration 447/1000 | Loss: 0.00003144
Iteration 448/1000 | Loss: 0.00003144
Iteration 449/1000 | Loss: 0.00003144
Iteration 450/1000 | Loss: 0.00003144
Iteration 451/1000 | Loss: 0.00003144
Iteration 452/1000 | Loss: 0.00003144
Iteration 453/1000 | Loss: 0.00003144
Iteration 454/1000 | Loss: 0.00003144
Iteration 455/1000 | Loss: 0.00003144
Iteration 456/1000 | Loss: 0.00003144
Iteration 457/1000 | Loss: 0.00003144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 457. Stopping optimization.
Last 5 losses: [3.143721187370829e-05, 3.143721187370829e-05, 3.143721187370829e-05, 3.143721187370829e-05, 3.143721187370829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.143721187370829e-05

Optimization complete. Final v2v error: 4.693826198577881 mm

Highest mean error: 8.089441299438477 mm for frame 122

Lowest mean error: 4.311624050140381 mm for frame 11

Saving results

Total time: 644.7126443386078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395748
Iteration 2/25 | Loss: 0.00115486
Iteration 3/25 | Loss: 0.00100790
Iteration 4/25 | Loss: 0.00099081
Iteration 5/25 | Loss: 0.00098763
Iteration 6/25 | Loss: 0.00098696
Iteration 7/25 | Loss: 0.00098696
Iteration 8/25 | Loss: 0.00098696
Iteration 9/25 | Loss: 0.00098696
Iteration 10/25 | Loss: 0.00098696
Iteration 11/25 | Loss: 0.00098696
Iteration 12/25 | Loss: 0.00098696
Iteration 13/25 | Loss: 0.00098696
Iteration 14/25 | Loss: 0.00098696
Iteration 15/25 | Loss: 0.00098696
Iteration 16/25 | Loss: 0.00098696
Iteration 17/25 | Loss: 0.00098696
Iteration 18/25 | Loss: 0.00098696
Iteration 19/25 | Loss: 0.00098696
Iteration 20/25 | Loss: 0.00098696
Iteration 21/25 | Loss: 0.00098696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00098695931956172, 0.00098695931956172, 0.00098695931956172, 0.00098695931956172, 0.00098695931956172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00098695931956172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43336511
Iteration 2/25 | Loss: 0.00116782
Iteration 3/25 | Loss: 0.00116782
Iteration 4/25 | Loss: 0.00116782
Iteration 5/25 | Loss: 0.00116782
Iteration 6/25 | Loss: 0.00116782
Iteration 7/25 | Loss: 0.00116782
Iteration 8/25 | Loss: 0.00116782
Iteration 9/25 | Loss: 0.00116782
Iteration 10/25 | Loss: 0.00116782
Iteration 11/25 | Loss: 0.00116782
Iteration 12/25 | Loss: 0.00116782
Iteration 13/25 | Loss: 0.00116782
Iteration 14/25 | Loss: 0.00116782
Iteration 15/25 | Loss: 0.00116782
Iteration 16/25 | Loss: 0.00116782
Iteration 17/25 | Loss: 0.00116782
Iteration 18/25 | Loss: 0.00116782
Iteration 19/25 | Loss: 0.00116782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011678161099553108, 0.0011678161099553108, 0.0011678161099553108, 0.0011678161099553108, 0.0011678161099553108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011678161099553108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116782
Iteration 2/1000 | Loss: 0.00005110
Iteration 3/1000 | Loss: 0.00002845
Iteration 4/1000 | Loss: 0.00002217
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001814
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001784
Iteration 11/1000 | Loss: 0.00001783
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001753
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001720
Iteration 79/1000 | Loss: 0.00001720
Iteration 80/1000 | Loss: 0.00001720
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001719
Iteration 98/1000 | Loss: 0.00001719
Iteration 99/1000 | Loss: 0.00001719
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001718
Iteration 122/1000 | Loss: 0.00001718
Iteration 123/1000 | Loss: 0.00001718
Iteration 124/1000 | Loss: 0.00001718
Iteration 125/1000 | Loss: 0.00001717
Iteration 126/1000 | Loss: 0.00001717
Iteration 127/1000 | Loss: 0.00001717
Iteration 128/1000 | Loss: 0.00001717
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00001717
Iteration 131/1000 | Loss: 0.00001717
Iteration 132/1000 | Loss: 0.00001717
Iteration 133/1000 | Loss: 0.00001717
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001717
Iteration 137/1000 | Loss: 0.00001717
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.7161060895887204e-05, 1.7161060895887204e-05, 1.7161060895887204e-05, 1.7161060895887204e-05, 1.7161060895887204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7161060895887204e-05

Optimization complete. Final v2v error: 3.5570971965789795 mm

Highest mean error: 4.291275978088379 mm for frame 73

Lowest mean error: 3.176666021347046 mm for frame 166

Saving results

Total time: 34.35480499267578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446123
Iteration 2/25 | Loss: 0.00122953
Iteration 3/25 | Loss: 0.00113983
Iteration 4/25 | Loss: 0.00111334
Iteration 5/25 | Loss: 0.00110535
Iteration 6/25 | Loss: 0.00110422
Iteration 7/25 | Loss: 0.00110422
Iteration 8/25 | Loss: 0.00110422
Iteration 9/25 | Loss: 0.00110422
Iteration 10/25 | Loss: 0.00110422
Iteration 11/25 | Loss: 0.00110422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011042164405807853, 0.0011042164405807853, 0.0011042164405807853, 0.0011042164405807853, 0.0011042164405807853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011042164405807853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44127369
Iteration 2/25 | Loss: 0.00107658
Iteration 3/25 | Loss: 0.00107658
Iteration 4/25 | Loss: 0.00107658
Iteration 5/25 | Loss: 0.00107658
Iteration 6/25 | Loss: 0.00107658
Iteration 7/25 | Loss: 0.00107658
Iteration 8/25 | Loss: 0.00107657
Iteration 9/25 | Loss: 0.00107657
Iteration 10/25 | Loss: 0.00107657
Iteration 11/25 | Loss: 0.00107657
Iteration 12/25 | Loss: 0.00107657
Iteration 13/25 | Loss: 0.00107657
Iteration 14/25 | Loss: 0.00107657
Iteration 15/25 | Loss: 0.00107657
Iteration 16/25 | Loss: 0.00107657
Iteration 17/25 | Loss: 0.00107657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010765743209049106, 0.0010765743209049106, 0.0010765743209049106, 0.0010765743209049106, 0.0010765743209049106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010765743209049106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107657
Iteration 2/1000 | Loss: 0.00007709
Iteration 3/1000 | Loss: 0.00004366
Iteration 4/1000 | Loss: 0.00003838
Iteration 5/1000 | Loss: 0.00003428
Iteration 6/1000 | Loss: 0.00003281
Iteration 7/1000 | Loss: 0.00003190
Iteration 8/1000 | Loss: 0.00003114
Iteration 9/1000 | Loss: 0.00003053
Iteration 10/1000 | Loss: 0.00003021
Iteration 11/1000 | Loss: 0.00002997
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00002989
Iteration 14/1000 | Loss: 0.00002988
Iteration 15/1000 | Loss: 0.00002987
Iteration 16/1000 | Loss: 0.00002986
Iteration 17/1000 | Loss: 0.00002978
Iteration 18/1000 | Loss: 0.00002976
Iteration 19/1000 | Loss: 0.00002976
Iteration 20/1000 | Loss: 0.00002976
Iteration 21/1000 | Loss: 0.00002975
Iteration 22/1000 | Loss: 0.00002973
Iteration 23/1000 | Loss: 0.00002973
Iteration 24/1000 | Loss: 0.00002972
Iteration 25/1000 | Loss: 0.00002972
Iteration 26/1000 | Loss: 0.00002971
Iteration 27/1000 | Loss: 0.00002971
Iteration 28/1000 | Loss: 0.00002970
Iteration 29/1000 | Loss: 0.00002970
Iteration 30/1000 | Loss: 0.00002970
Iteration 31/1000 | Loss: 0.00002970
Iteration 32/1000 | Loss: 0.00002969
Iteration 33/1000 | Loss: 0.00002969
Iteration 34/1000 | Loss: 0.00002968
Iteration 35/1000 | Loss: 0.00002968
Iteration 36/1000 | Loss: 0.00002968
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00002966
Iteration 40/1000 | Loss: 0.00002966
Iteration 41/1000 | Loss: 0.00002966
Iteration 42/1000 | Loss: 0.00002966
Iteration 43/1000 | Loss: 0.00002965
Iteration 44/1000 | Loss: 0.00002965
Iteration 45/1000 | Loss: 0.00002965
Iteration 46/1000 | Loss: 0.00002965
Iteration 47/1000 | Loss: 0.00002965
Iteration 48/1000 | Loss: 0.00002965
Iteration 49/1000 | Loss: 0.00002965
Iteration 50/1000 | Loss: 0.00002965
Iteration 51/1000 | Loss: 0.00002964
Iteration 52/1000 | Loss: 0.00002964
Iteration 53/1000 | Loss: 0.00002964
Iteration 54/1000 | Loss: 0.00002963
Iteration 55/1000 | Loss: 0.00002963
Iteration 56/1000 | Loss: 0.00002963
Iteration 57/1000 | Loss: 0.00002963
Iteration 58/1000 | Loss: 0.00002963
Iteration 59/1000 | Loss: 0.00002963
Iteration 60/1000 | Loss: 0.00002963
Iteration 61/1000 | Loss: 0.00002963
Iteration 62/1000 | Loss: 0.00002962
Iteration 63/1000 | Loss: 0.00002962
Iteration 64/1000 | Loss: 0.00002962
Iteration 65/1000 | Loss: 0.00002961
Iteration 66/1000 | Loss: 0.00002961
Iteration 67/1000 | Loss: 0.00002961
Iteration 68/1000 | Loss: 0.00002960
Iteration 69/1000 | Loss: 0.00002960
Iteration 70/1000 | Loss: 0.00002960
Iteration 71/1000 | Loss: 0.00002959
Iteration 72/1000 | Loss: 0.00002959
Iteration 73/1000 | Loss: 0.00002959
Iteration 74/1000 | Loss: 0.00002959
Iteration 75/1000 | Loss: 0.00002959
Iteration 76/1000 | Loss: 0.00002959
Iteration 77/1000 | Loss: 0.00002959
Iteration 78/1000 | Loss: 0.00002959
Iteration 79/1000 | Loss: 0.00002959
Iteration 80/1000 | Loss: 0.00002959
Iteration 81/1000 | Loss: 0.00002958
Iteration 82/1000 | Loss: 0.00002958
Iteration 83/1000 | Loss: 0.00002958
Iteration 84/1000 | Loss: 0.00002958
Iteration 85/1000 | Loss: 0.00002958
Iteration 86/1000 | Loss: 0.00002958
Iteration 87/1000 | Loss: 0.00002958
Iteration 88/1000 | Loss: 0.00002957
Iteration 89/1000 | Loss: 0.00002957
Iteration 90/1000 | Loss: 0.00002957
Iteration 91/1000 | Loss: 0.00002956
Iteration 92/1000 | Loss: 0.00002956
Iteration 93/1000 | Loss: 0.00002956
Iteration 94/1000 | Loss: 0.00002956
Iteration 95/1000 | Loss: 0.00002956
Iteration 96/1000 | Loss: 0.00002956
Iteration 97/1000 | Loss: 0.00002956
Iteration 98/1000 | Loss: 0.00002956
Iteration 99/1000 | Loss: 0.00002956
Iteration 100/1000 | Loss: 0.00002956
Iteration 101/1000 | Loss: 0.00002956
Iteration 102/1000 | Loss: 0.00002956
Iteration 103/1000 | Loss: 0.00002956
Iteration 104/1000 | Loss: 0.00002956
Iteration 105/1000 | Loss: 0.00002955
Iteration 106/1000 | Loss: 0.00002955
Iteration 107/1000 | Loss: 0.00002955
Iteration 108/1000 | Loss: 0.00002955
Iteration 109/1000 | Loss: 0.00002955
Iteration 110/1000 | Loss: 0.00002955
Iteration 111/1000 | Loss: 0.00002955
Iteration 112/1000 | Loss: 0.00002955
Iteration 113/1000 | Loss: 0.00002955
Iteration 114/1000 | Loss: 0.00002955
Iteration 115/1000 | Loss: 0.00002955
Iteration 116/1000 | Loss: 0.00002955
Iteration 117/1000 | Loss: 0.00002955
Iteration 118/1000 | Loss: 0.00002954
Iteration 119/1000 | Loss: 0.00002954
Iteration 120/1000 | Loss: 0.00002954
Iteration 121/1000 | Loss: 0.00002954
Iteration 122/1000 | Loss: 0.00002954
Iteration 123/1000 | Loss: 0.00002954
Iteration 124/1000 | Loss: 0.00002954
Iteration 125/1000 | Loss: 0.00002954
Iteration 126/1000 | Loss: 0.00002953
Iteration 127/1000 | Loss: 0.00002953
Iteration 128/1000 | Loss: 0.00002953
Iteration 129/1000 | Loss: 0.00002953
Iteration 130/1000 | Loss: 0.00002953
Iteration 131/1000 | Loss: 0.00002953
Iteration 132/1000 | Loss: 0.00002953
Iteration 133/1000 | Loss: 0.00002953
Iteration 134/1000 | Loss: 0.00002953
Iteration 135/1000 | Loss: 0.00002953
Iteration 136/1000 | Loss: 0.00002953
Iteration 137/1000 | Loss: 0.00002953
Iteration 138/1000 | Loss: 0.00002953
Iteration 139/1000 | Loss: 0.00002953
Iteration 140/1000 | Loss: 0.00002953
Iteration 141/1000 | Loss: 0.00002953
Iteration 142/1000 | Loss: 0.00002952
Iteration 143/1000 | Loss: 0.00002952
Iteration 144/1000 | Loss: 0.00002952
Iteration 145/1000 | Loss: 0.00002952
Iteration 146/1000 | Loss: 0.00002952
Iteration 147/1000 | Loss: 0.00002952
Iteration 148/1000 | Loss: 0.00002952
Iteration 149/1000 | Loss: 0.00002952
Iteration 150/1000 | Loss: 0.00002952
Iteration 151/1000 | Loss: 0.00002952
Iteration 152/1000 | Loss: 0.00002952
Iteration 153/1000 | Loss: 0.00002952
Iteration 154/1000 | Loss: 0.00002952
Iteration 155/1000 | Loss: 0.00002952
Iteration 156/1000 | Loss: 0.00002952
Iteration 157/1000 | Loss: 0.00002951
Iteration 158/1000 | Loss: 0.00002951
Iteration 159/1000 | Loss: 0.00002951
Iteration 160/1000 | Loss: 0.00002951
Iteration 161/1000 | Loss: 0.00002951
Iteration 162/1000 | Loss: 0.00002951
Iteration 163/1000 | Loss: 0.00002951
Iteration 164/1000 | Loss: 0.00002951
Iteration 165/1000 | Loss: 0.00002951
Iteration 166/1000 | Loss: 0.00002951
Iteration 167/1000 | Loss: 0.00002951
Iteration 168/1000 | Loss: 0.00002951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.951331771328114e-05, 2.951331771328114e-05, 2.951331771328114e-05, 2.951331771328114e-05, 2.951331771328114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.951331771328114e-05

Optimization complete. Final v2v error: 4.670107841491699 mm

Highest mean error: 5.075079917907715 mm for frame 158

Lowest mean error: 4.278470039367676 mm for frame 89

Saving results

Total time: 41.76274752616882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145166
Iteration 2/25 | Loss: 0.00213453
Iteration 3/25 | Loss: 0.00160795
Iteration 4/25 | Loss: 0.00156499
Iteration 5/25 | Loss: 0.00150219
Iteration 6/25 | Loss: 0.00148416
Iteration 7/25 | Loss: 0.00148553
Iteration 8/25 | Loss: 0.00149091
Iteration 9/25 | Loss: 0.00144692
Iteration 10/25 | Loss: 0.00143676
Iteration 11/25 | Loss: 0.00144572
Iteration 12/25 | Loss: 0.00143930
Iteration 13/25 | Loss: 0.00144353
Iteration 14/25 | Loss: 0.00144040
Iteration 15/25 | Loss: 0.00143672
Iteration 16/25 | Loss: 0.00143684
Iteration 17/25 | Loss: 0.00142793
Iteration 18/25 | Loss: 0.00143746
Iteration 19/25 | Loss: 0.00142567
Iteration 20/25 | Loss: 0.00142476
Iteration 21/25 | Loss: 0.00142805
Iteration 22/25 | Loss: 0.00142854
Iteration 23/25 | Loss: 0.00142616
Iteration 24/25 | Loss: 0.00142964
Iteration 25/25 | Loss: 0.00142842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.05872297
Iteration 2/25 | Loss: 0.00123963
Iteration 3/25 | Loss: 0.00123939
Iteration 4/25 | Loss: 0.00123939
Iteration 5/25 | Loss: 0.00123939
Iteration 6/25 | Loss: 0.00123938
Iteration 7/25 | Loss: 0.00123938
Iteration 8/25 | Loss: 0.00123938
Iteration 9/25 | Loss: 0.00123938
Iteration 10/25 | Loss: 0.00123938
Iteration 11/25 | Loss: 0.00123938
Iteration 12/25 | Loss: 0.00123938
Iteration 13/25 | Loss: 0.00123938
Iteration 14/25 | Loss: 0.00123938
Iteration 15/25 | Loss: 0.00123938
Iteration 16/25 | Loss: 0.00123938
Iteration 17/25 | Loss: 0.00123938
Iteration 18/25 | Loss: 0.00123938
Iteration 19/25 | Loss: 0.00123938
Iteration 20/25 | Loss: 0.00123938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001239383709616959, 0.001239383709616959, 0.001239383709616959, 0.001239383709616959, 0.001239383709616959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001239383709616959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123938
Iteration 2/1000 | Loss: 0.00008613
Iteration 3/1000 | Loss: 0.00021454
Iteration 4/1000 | Loss: 0.00018455
Iteration 5/1000 | Loss: 0.00005967
Iteration 6/1000 | Loss: 0.00019308
Iteration 7/1000 | Loss: 0.00017904
Iteration 8/1000 | Loss: 0.00020207
Iteration 9/1000 | Loss: 0.00021418
Iteration 10/1000 | Loss: 0.00005622
Iteration 11/1000 | Loss: 0.00004693
Iteration 12/1000 | Loss: 0.00004414
Iteration 13/1000 | Loss: 0.00004218
Iteration 14/1000 | Loss: 0.00004041
Iteration 15/1000 | Loss: 0.00003776
Iteration 16/1000 | Loss: 0.00003659
Iteration 17/1000 | Loss: 0.00003585
Iteration 18/1000 | Loss: 0.00003535
Iteration 19/1000 | Loss: 0.00003500
Iteration 20/1000 | Loss: 0.00003471
Iteration 21/1000 | Loss: 0.00003442
Iteration 22/1000 | Loss: 0.00003408
Iteration 23/1000 | Loss: 0.00003385
Iteration 24/1000 | Loss: 0.00003368
Iteration 25/1000 | Loss: 0.00003353
Iteration 26/1000 | Loss: 0.00003353
Iteration 27/1000 | Loss: 0.00003353
Iteration 28/1000 | Loss: 0.00003352
Iteration 29/1000 | Loss: 0.00003350
Iteration 30/1000 | Loss: 0.00003350
Iteration 31/1000 | Loss: 0.00003350
Iteration 32/1000 | Loss: 0.00003344
Iteration 33/1000 | Loss: 0.00003344
Iteration 34/1000 | Loss: 0.00003342
Iteration 35/1000 | Loss: 0.00003338
Iteration 36/1000 | Loss: 0.00003335
Iteration 37/1000 | Loss: 0.00003335
Iteration 38/1000 | Loss: 0.00003334
Iteration 39/1000 | Loss: 0.00003331
Iteration 40/1000 | Loss: 0.00003331
Iteration 41/1000 | Loss: 0.00003326
Iteration 42/1000 | Loss: 0.00003326
Iteration 43/1000 | Loss: 0.00003325
Iteration 44/1000 | Loss: 0.00003324
Iteration 45/1000 | Loss: 0.00003323
Iteration 46/1000 | Loss: 0.00003323
Iteration 47/1000 | Loss: 0.00003322
Iteration 48/1000 | Loss: 0.00003318
Iteration 49/1000 | Loss: 0.00003318
Iteration 50/1000 | Loss: 0.00003317
Iteration 51/1000 | Loss: 0.00003316
Iteration 52/1000 | Loss: 0.00003316
Iteration 53/1000 | Loss: 0.00003316
Iteration 54/1000 | Loss: 0.00003316
Iteration 55/1000 | Loss: 0.00003315
Iteration 56/1000 | Loss: 0.00003315
Iteration 57/1000 | Loss: 0.00003315
Iteration 58/1000 | Loss: 0.00003315
Iteration 59/1000 | Loss: 0.00003315
Iteration 60/1000 | Loss: 0.00003315
Iteration 61/1000 | Loss: 0.00003315
Iteration 62/1000 | Loss: 0.00003314
Iteration 63/1000 | Loss: 0.00003314
Iteration 64/1000 | Loss: 0.00003314
Iteration 65/1000 | Loss: 0.00003313
Iteration 66/1000 | Loss: 0.00003313
Iteration 67/1000 | Loss: 0.00003313
Iteration 68/1000 | Loss: 0.00003313
Iteration 69/1000 | Loss: 0.00003313
Iteration 70/1000 | Loss: 0.00003312
Iteration 71/1000 | Loss: 0.00003312
Iteration 72/1000 | Loss: 0.00003312
Iteration 73/1000 | Loss: 0.00003311
Iteration 74/1000 | Loss: 0.00003311
Iteration 75/1000 | Loss: 0.00003311
Iteration 76/1000 | Loss: 0.00003310
Iteration 77/1000 | Loss: 0.00003310
Iteration 78/1000 | Loss: 0.00003310
Iteration 79/1000 | Loss: 0.00003310
Iteration 80/1000 | Loss: 0.00003310
Iteration 81/1000 | Loss: 0.00003310
Iteration 82/1000 | Loss: 0.00003309
Iteration 83/1000 | Loss: 0.00003309
Iteration 84/1000 | Loss: 0.00003309
Iteration 85/1000 | Loss: 0.00003309
Iteration 86/1000 | Loss: 0.00003309
Iteration 87/1000 | Loss: 0.00003309
Iteration 88/1000 | Loss: 0.00003308
Iteration 89/1000 | Loss: 0.00003308
Iteration 90/1000 | Loss: 0.00003308
Iteration 91/1000 | Loss: 0.00003307
Iteration 92/1000 | Loss: 0.00003307
Iteration 93/1000 | Loss: 0.00003307
Iteration 94/1000 | Loss: 0.00003306
Iteration 95/1000 | Loss: 0.00003306
Iteration 96/1000 | Loss: 0.00003306
Iteration 97/1000 | Loss: 0.00003306
Iteration 98/1000 | Loss: 0.00003306
Iteration 99/1000 | Loss: 0.00003306
Iteration 100/1000 | Loss: 0.00003306
Iteration 101/1000 | Loss: 0.00003306
Iteration 102/1000 | Loss: 0.00003306
Iteration 103/1000 | Loss: 0.00003306
Iteration 104/1000 | Loss: 0.00003305
Iteration 105/1000 | Loss: 0.00003305
Iteration 106/1000 | Loss: 0.00003305
Iteration 107/1000 | Loss: 0.00003304
Iteration 108/1000 | Loss: 0.00003304
Iteration 109/1000 | Loss: 0.00003304
Iteration 110/1000 | Loss: 0.00003304
Iteration 111/1000 | Loss: 0.00003303
Iteration 112/1000 | Loss: 0.00003303
Iteration 113/1000 | Loss: 0.00003303
Iteration 114/1000 | Loss: 0.00003303
Iteration 115/1000 | Loss: 0.00003303
Iteration 116/1000 | Loss: 0.00003303
Iteration 117/1000 | Loss: 0.00003303
Iteration 118/1000 | Loss: 0.00003303
Iteration 119/1000 | Loss: 0.00003303
Iteration 120/1000 | Loss: 0.00003303
Iteration 121/1000 | Loss: 0.00003302
Iteration 122/1000 | Loss: 0.00003302
Iteration 123/1000 | Loss: 0.00003302
Iteration 124/1000 | Loss: 0.00003302
Iteration 125/1000 | Loss: 0.00003302
Iteration 126/1000 | Loss: 0.00003302
Iteration 127/1000 | Loss: 0.00003302
Iteration 128/1000 | Loss: 0.00003301
Iteration 129/1000 | Loss: 0.00003301
Iteration 130/1000 | Loss: 0.00003301
Iteration 131/1000 | Loss: 0.00003301
Iteration 132/1000 | Loss: 0.00003301
Iteration 133/1000 | Loss: 0.00003301
Iteration 134/1000 | Loss: 0.00003301
Iteration 135/1000 | Loss: 0.00003301
Iteration 136/1000 | Loss: 0.00003301
Iteration 137/1000 | Loss: 0.00003301
Iteration 138/1000 | Loss: 0.00003301
Iteration 139/1000 | Loss: 0.00003301
Iteration 140/1000 | Loss: 0.00003301
Iteration 141/1000 | Loss: 0.00003301
Iteration 142/1000 | Loss: 0.00003301
Iteration 143/1000 | Loss: 0.00003301
Iteration 144/1000 | Loss: 0.00003301
Iteration 145/1000 | Loss: 0.00003300
Iteration 146/1000 | Loss: 0.00003300
Iteration 147/1000 | Loss: 0.00003300
Iteration 148/1000 | Loss: 0.00003300
Iteration 149/1000 | Loss: 0.00003300
Iteration 150/1000 | Loss: 0.00003300
Iteration 151/1000 | Loss: 0.00003300
Iteration 152/1000 | Loss: 0.00003300
Iteration 153/1000 | Loss: 0.00003300
Iteration 154/1000 | Loss: 0.00003300
Iteration 155/1000 | Loss: 0.00003300
Iteration 156/1000 | Loss: 0.00003300
Iteration 157/1000 | Loss: 0.00003300
Iteration 158/1000 | Loss: 0.00003300
Iteration 159/1000 | Loss: 0.00003300
Iteration 160/1000 | Loss: 0.00003300
Iteration 161/1000 | Loss: 0.00003300
Iteration 162/1000 | Loss: 0.00003300
Iteration 163/1000 | Loss: 0.00003299
Iteration 164/1000 | Loss: 0.00003299
Iteration 165/1000 | Loss: 0.00003299
Iteration 166/1000 | Loss: 0.00003299
Iteration 167/1000 | Loss: 0.00003299
Iteration 168/1000 | Loss: 0.00003299
Iteration 169/1000 | Loss: 0.00003299
Iteration 170/1000 | Loss: 0.00003299
Iteration 171/1000 | Loss: 0.00003299
Iteration 172/1000 | Loss: 0.00003299
Iteration 173/1000 | Loss: 0.00003299
Iteration 174/1000 | Loss: 0.00003298
Iteration 175/1000 | Loss: 0.00003298
Iteration 176/1000 | Loss: 0.00003298
Iteration 177/1000 | Loss: 0.00003298
Iteration 178/1000 | Loss: 0.00003298
Iteration 179/1000 | Loss: 0.00003298
Iteration 180/1000 | Loss: 0.00003298
Iteration 181/1000 | Loss: 0.00003298
Iteration 182/1000 | Loss: 0.00003298
Iteration 183/1000 | Loss: 0.00003298
Iteration 184/1000 | Loss: 0.00003298
Iteration 185/1000 | Loss: 0.00003297
Iteration 186/1000 | Loss: 0.00003297
Iteration 187/1000 | Loss: 0.00003297
Iteration 188/1000 | Loss: 0.00003297
Iteration 189/1000 | Loss: 0.00003297
Iteration 190/1000 | Loss: 0.00003297
Iteration 191/1000 | Loss: 0.00003296
Iteration 192/1000 | Loss: 0.00003296
Iteration 193/1000 | Loss: 0.00003296
Iteration 194/1000 | Loss: 0.00003296
Iteration 195/1000 | Loss: 0.00003295
Iteration 196/1000 | Loss: 0.00003295
Iteration 197/1000 | Loss: 0.00003295
Iteration 198/1000 | Loss: 0.00003295
Iteration 199/1000 | Loss: 0.00003295
Iteration 200/1000 | Loss: 0.00003294
Iteration 201/1000 | Loss: 0.00003294
Iteration 202/1000 | Loss: 0.00003294
Iteration 203/1000 | Loss: 0.00003294
Iteration 204/1000 | Loss: 0.00003293
Iteration 205/1000 | Loss: 0.00003293
Iteration 206/1000 | Loss: 0.00003293
Iteration 207/1000 | Loss: 0.00003293
Iteration 208/1000 | Loss: 0.00003293
Iteration 209/1000 | Loss: 0.00003292
Iteration 210/1000 | Loss: 0.00003292
Iteration 211/1000 | Loss: 0.00003292
Iteration 212/1000 | Loss: 0.00003292
Iteration 213/1000 | Loss: 0.00003292
Iteration 214/1000 | Loss: 0.00003292
Iteration 215/1000 | Loss: 0.00003292
Iteration 216/1000 | Loss: 0.00003292
Iteration 217/1000 | Loss: 0.00003292
Iteration 218/1000 | Loss: 0.00003292
Iteration 219/1000 | Loss: 0.00003291
Iteration 220/1000 | Loss: 0.00003291
Iteration 221/1000 | Loss: 0.00003291
Iteration 222/1000 | Loss: 0.00003291
Iteration 223/1000 | Loss: 0.00003291
Iteration 224/1000 | Loss: 0.00003291
Iteration 225/1000 | Loss: 0.00003291
Iteration 226/1000 | Loss: 0.00003291
Iteration 227/1000 | Loss: 0.00003290
Iteration 228/1000 | Loss: 0.00003290
Iteration 229/1000 | Loss: 0.00003290
Iteration 230/1000 | Loss: 0.00003290
Iteration 231/1000 | Loss: 0.00003290
Iteration 232/1000 | Loss: 0.00003290
Iteration 233/1000 | Loss: 0.00003290
Iteration 234/1000 | Loss: 0.00003290
Iteration 235/1000 | Loss: 0.00003290
Iteration 236/1000 | Loss: 0.00003290
Iteration 237/1000 | Loss: 0.00003290
Iteration 238/1000 | Loss: 0.00003290
Iteration 239/1000 | Loss: 0.00003290
Iteration 240/1000 | Loss: 0.00003290
Iteration 241/1000 | Loss: 0.00003290
Iteration 242/1000 | Loss: 0.00003290
Iteration 243/1000 | Loss: 0.00003289
Iteration 244/1000 | Loss: 0.00003289
Iteration 245/1000 | Loss: 0.00003289
Iteration 246/1000 | Loss: 0.00003289
Iteration 247/1000 | Loss: 0.00003289
Iteration 248/1000 | Loss: 0.00003289
Iteration 249/1000 | Loss: 0.00003289
Iteration 250/1000 | Loss: 0.00003289
Iteration 251/1000 | Loss: 0.00003289
Iteration 252/1000 | Loss: 0.00003289
Iteration 253/1000 | Loss: 0.00003289
Iteration 254/1000 | Loss: 0.00003289
Iteration 255/1000 | Loss: 0.00003289
Iteration 256/1000 | Loss: 0.00003289
Iteration 257/1000 | Loss: 0.00003289
Iteration 258/1000 | Loss: 0.00003288
Iteration 259/1000 | Loss: 0.00003288
Iteration 260/1000 | Loss: 0.00003288
Iteration 261/1000 | Loss: 0.00003288
Iteration 262/1000 | Loss: 0.00003288
Iteration 263/1000 | Loss: 0.00003288
Iteration 264/1000 | Loss: 0.00003288
Iteration 265/1000 | Loss: 0.00003288
Iteration 266/1000 | Loss: 0.00003288
Iteration 267/1000 | Loss: 0.00003288
Iteration 268/1000 | Loss: 0.00003288
Iteration 269/1000 | Loss: 0.00003287
Iteration 270/1000 | Loss: 0.00003287
Iteration 271/1000 | Loss: 0.00003287
Iteration 272/1000 | Loss: 0.00003287
Iteration 273/1000 | Loss: 0.00003287
Iteration 274/1000 | Loss: 0.00003287
Iteration 275/1000 | Loss: 0.00003287
Iteration 276/1000 | Loss: 0.00003287
Iteration 277/1000 | Loss: 0.00003287
Iteration 278/1000 | Loss: 0.00003287
Iteration 279/1000 | Loss: 0.00003287
Iteration 280/1000 | Loss: 0.00003287
Iteration 281/1000 | Loss: 0.00003287
Iteration 282/1000 | Loss: 0.00003287
Iteration 283/1000 | Loss: 0.00003287
Iteration 284/1000 | Loss: 0.00003287
Iteration 285/1000 | Loss: 0.00003287
Iteration 286/1000 | Loss: 0.00003287
Iteration 287/1000 | Loss: 0.00003287
Iteration 288/1000 | Loss: 0.00003287
Iteration 289/1000 | Loss: 0.00003286
Iteration 290/1000 | Loss: 0.00003286
Iteration 291/1000 | Loss: 0.00003286
Iteration 292/1000 | Loss: 0.00003286
Iteration 293/1000 | Loss: 0.00003286
Iteration 294/1000 | Loss: 0.00003286
Iteration 295/1000 | Loss: 0.00003286
Iteration 296/1000 | Loss: 0.00003286
Iteration 297/1000 | Loss: 0.00003286
Iteration 298/1000 | Loss: 0.00003286
Iteration 299/1000 | Loss: 0.00003286
Iteration 300/1000 | Loss: 0.00003286
Iteration 301/1000 | Loss: 0.00003286
Iteration 302/1000 | Loss: 0.00003286
Iteration 303/1000 | Loss: 0.00003286
Iteration 304/1000 | Loss: 0.00003286
Iteration 305/1000 | Loss: 0.00003286
Iteration 306/1000 | Loss: 0.00003286
Iteration 307/1000 | Loss: 0.00003286
Iteration 308/1000 | Loss: 0.00003286
Iteration 309/1000 | Loss: 0.00003286
Iteration 310/1000 | Loss: 0.00003286
Iteration 311/1000 | Loss: 0.00003286
Iteration 312/1000 | Loss: 0.00003286
Iteration 313/1000 | Loss: 0.00003286
Iteration 314/1000 | Loss: 0.00003286
Iteration 315/1000 | Loss: 0.00003286
Iteration 316/1000 | Loss: 0.00003286
Iteration 317/1000 | Loss: 0.00003286
Iteration 318/1000 | Loss: 0.00003286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [3.286465289420448e-05, 3.286465289420448e-05, 3.286465289420448e-05, 3.286465289420448e-05, 3.286465289420448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.286465289420448e-05

Optimization complete. Final v2v error: 4.695446491241455 mm

Highest mean error: 10.984371185302734 mm for frame 13

Lowest mean error: 3.60198712348938 mm for frame 80

Saving results

Total time: 105.75857925415039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01147968
Iteration 2/25 | Loss: 0.00191422
Iteration 3/25 | Loss: 0.00157240
Iteration 4/25 | Loss: 0.00148805
Iteration 5/25 | Loss: 0.00147142
Iteration 6/25 | Loss: 0.00146660
Iteration 7/25 | Loss: 0.00146749
Iteration 8/25 | Loss: 0.00146132
Iteration 9/25 | Loss: 0.00145825
Iteration 10/25 | Loss: 0.00146369
Iteration 11/25 | Loss: 0.00145872
Iteration 12/25 | Loss: 0.00145794
Iteration 13/25 | Loss: 0.00146135
Iteration 14/25 | Loss: 0.00145912
Iteration 15/25 | Loss: 0.00145349
Iteration 16/25 | Loss: 0.00145098
Iteration 17/25 | Loss: 0.00145259
Iteration 18/25 | Loss: 0.00145029
Iteration 19/25 | Loss: 0.00144936
Iteration 20/25 | Loss: 0.00145264
Iteration 21/25 | Loss: 0.00144860
Iteration 22/25 | Loss: 0.00144830
Iteration 23/25 | Loss: 0.00144828
Iteration 24/25 | Loss: 0.00144828
Iteration 25/25 | Loss: 0.00144827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.06246853
Iteration 2/25 | Loss: 0.00109651
Iteration 3/25 | Loss: 0.00106822
Iteration 4/25 | Loss: 0.00106822
Iteration 5/25 | Loss: 0.00106822
Iteration 6/25 | Loss: 0.00106821
Iteration 7/25 | Loss: 0.00106821
Iteration 8/25 | Loss: 0.00106821
Iteration 9/25 | Loss: 0.00106821
Iteration 10/25 | Loss: 0.00106821
Iteration 11/25 | Loss: 0.00106821
Iteration 12/25 | Loss: 0.00106821
Iteration 13/25 | Loss: 0.00106821
Iteration 14/25 | Loss: 0.00106821
Iteration 15/25 | Loss: 0.00106821
Iteration 16/25 | Loss: 0.00106821
Iteration 17/25 | Loss: 0.00106821
Iteration 18/25 | Loss: 0.00106821
Iteration 19/25 | Loss: 0.00106821
Iteration 20/25 | Loss: 0.00106821
Iteration 21/25 | Loss: 0.00106821
Iteration 22/25 | Loss: 0.00106821
Iteration 23/25 | Loss: 0.00106821
Iteration 24/25 | Loss: 0.00106821
Iteration 25/25 | Loss: 0.00106821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106821
Iteration 2/1000 | Loss: 0.00014494
Iteration 3/1000 | Loss: 0.00008986
Iteration 4/1000 | Loss: 0.00006477
Iteration 5/1000 | Loss: 0.00016343
Iteration 6/1000 | Loss: 0.00005517
Iteration 7/1000 | Loss: 0.00019486
Iteration 8/1000 | Loss: 0.00005067
Iteration 9/1000 | Loss: 0.00004885
Iteration 10/1000 | Loss: 0.00004796
Iteration 11/1000 | Loss: 0.00014148
Iteration 12/1000 | Loss: 0.00007195
Iteration 13/1000 | Loss: 0.00005636
Iteration 14/1000 | Loss: 0.00004681
Iteration 15/1000 | Loss: 0.00004641
Iteration 16/1000 | Loss: 0.00004581
Iteration 17/1000 | Loss: 0.00004527
Iteration 18/1000 | Loss: 0.00004488
Iteration 19/1000 | Loss: 0.00004463
Iteration 20/1000 | Loss: 0.00004442
Iteration 21/1000 | Loss: 0.00004425
Iteration 22/1000 | Loss: 0.00004407
Iteration 23/1000 | Loss: 0.00004397
Iteration 24/1000 | Loss: 0.00004396
Iteration 25/1000 | Loss: 0.00004394
Iteration 26/1000 | Loss: 0.00004393
Iteration 27/1000 | Loss: 0.00004388
Iteration 28/1000 | Loss: 0.00004382
Iteration 29/1000 | Loss: 0.00004376
Iteration 30/1000 | Loss: 0.00004373
Iteration 31/1000 | Loss: 0.00004372
Iteration 32/1000 | Loss: 0.00004372
Iteration 33/1000 | Loss: 0.00004371
Iteration 34/1000 | Loss: 0.00004365
Iteration 35/1000 | Loss: 0.00004362
Iteration 36/1000 | Loss: 0.00004359
Iteration 37/1000 | Loss: 0.00004359
Iteration 38/1000 | Loss: 0.00004359
Iteration 39/1000 | Loss: 0.00004358
Iteration 40/1000 | Loss: 0.00004358
Iteration 41/1000 | Loss: 0.00004357
Iteration 42/1000 | Loss: 0.00004357
Iteration 43/1000 | Loss: 0.00004357
Iteration 44/1000 | Loss: 0.00004356
Iteration 45/1000 | Loss: 0.00004356
Iteration 46/1000 | Loss: 0.00004355
Iteration 47/1000 | Loss: 0.00004354
Iteration 48/1000 | Loss: 0.00004354
Iteration 49/1000 | Loss: 0.00004354
Iteration 50/1000 | Loss: 0.00004353
Iteration 51/1000 | Loss: 0.00004353
Iteration 52/1000 | Loss: 0.00004351
Iteration 53/1000 | Loss: 0.00004351
Iteration 54/1000 | Loss: 0.00004347
Iteration 55/1000 | Loss: 0.00004347
Iteration 56/1000 | Loss: 0.00004347
Iteration 57/1000 | Loss: 0.00004347
Iteration 58/1000 | Loss: 0.00004347
Iteration 59/1000 | Loss: 0.00004347
Iteration 60/1000 | Loss: 0.00004346
Iteration 61/1000 | Loss: 0.00004346
Iteration 62/1000 | Loss: 0.00004346
Iteration 63/1000 | Loss: 0.00004346
Iteration 64/1000 | Loss: 0.00004346
Iteration 65/1000 | Loss: 0.00004346
Iteration 66/1000 | Loss: 0.00004346
Iteration 67/1000 | Loss: 0.00004346
Iteration 68/1000 | Loss: 0.00004346
Iteration 69/1000 | Loss: 0.00004346
Iteration 70/1000 | Loss: 0.00004346
Iteration 71/1000 | Loss: 0.00004345
Iteration 72/1000 | Loss: 0.00004345
Iteration 73/1000 | Loss: 0.00004345
Iteration 74/1000 | Loss: 0.00004345
Iteration 75/1000 | Loss: 0.00004345
Iteration 76/1000 | Loss: 0.00004345
Iteration 77/1000 | Loss: 0.00004344
Iteration 78/1000 | Loss: 0.00004344
Iteration 79/1000 | Loss: 0.00004344
Iteration 80/1000 | Loss: 0.00004343
Iteration 81/1000 | Loss: 0.00004343
Iteration 82/1000 | Loss: 0.00004343
Iteration 83/1000 | Loss: 0.00004343
Iteration 84/1000 | Loss: 0.00004342
Iteration 85/1000 | Loss: 0.00004342
Iteration 86/1000 | Loss: 0.00004341
Iteration 87/1000 | Loss: 0.00004341
Iteration 88/1000 | Loss: 0.00004341
Iteration 89/1000 | Loss: 0.00004340
Iteration 90/1000 | Loss: 0.00004340
Iteration 91/1000 | Loss: 0.00004340
Iteration 92/1000 | Loss: 0.00004340
Iteration 93/1000 | Loss: 0.00004339
Iteration 94/1000 | Loss: 0.00004339
Iteration 95/1000 | Loss: 0.00004339
Iteration 96/1000 | Loss: 0.00004339
Iteration 97/1000 | Loss: 0.00004339
Iteration 98/1000 | Loss: 0.00004339
Iteration 99/1000 | Loss: 0.00004339
Iteration 100/1000 | Loss: 0.00004339
Iteration 101/1000 | Loss: 0.00004339
Iteration 102/1000 | Loss: 0.00004338
Iteration 103/1000 | Loss: 0.00004338
Iteration 104/1000 | Loss: 0.00004338
Iteration 105/1000 | Loss: 0.00004338
Iteration 106/1000 | Loss: 0.00004338
Iteration 107/1000 | Loss: 0.00004337
Iteration 108/1000 | Loss: 0.00004337
Iteration 109/1000 | Loss: 0.00004337
Iteration 110/1000 | Loss: 0.00004336
Iteration 111/1000 | Loss: 0.00004336
Iteration 112/1000 | Loss: 0.00004335
Iteration 113/1000 | Loss: 0.00004335
Iteration 114/1000 | Loss: 0.00004335
Iteration 115/1000 | Loss: 0.00004335
Iteration 116/1000 | Loss: 0.00004334
Iteration 117/1000 | Loss: 0.00004334
Iteration 118/1000 | Loss: 0.00004334
Iteration 119/1000 | Loss: 0.00004334
Iteration 120/1000 | Loss: 0.00004333
Iteration 121/1000 | Loss: 0.00004333
Iteration 122/1000 | Loss: 0.00004333
Iteration 123/1000 | Loss: 0.00004333
Iteration 124/1000 | Loss: 0.00004332
Iteration 125/1000 | Loss: 0.00004332
Iteration 126/1000 | Loss: 0.00004332
Iteration 127/1000 | Loss: 0.00004332
Iteration 128/1000 | Loss: 0.00004332
Iteration 129/1000 | Loss: 0.00004332
Iteration 130/1000 | Loss: 0.00004332
Iteration 131/1000 | Loss: 0.00004332
Iteration 132/1000 | Loss: 0.00004331
Iteration 133/1000 | Loss: 0.00004331
Iteration 134/1000 | Loss: 0.00004331
Iteration 135/1000 | Loss: 0.00004331
Iteration 136/1000 | Loss: 0.00004330
Iteration 137/1000 | Loss: 0.00004330
Iteration 138/1000 | Loss: 0.00004330
Iteration 139/1000 | Loss: 0.00004329
Iteration 140/1000 | Loss: 0.00004329
Iteration 141/1000 | Loss: 0.00004329
Iteration 142/1000 | Loss: 0.00004329
Iteration 143/1000 | Loss: 0.00004329
Iteration 144/1000 | Loss: 0.00004328
Iteration 145/1000 | Loss: 0.00004328
Iteration 146/1000 | Loss: 0.00004328
Iteration 147/1000 | Loss: 0.00004328
Iteration 148/1000 | Loss: 0.00004328
Iteration 149/1000 | Loss: 0.00004328
Iteration 150/1000 | Loss: 0.00004328
Iteration 151/1000 | Loss: 0.00004327
Iteration 152/1000 | Loss: 0.00004327
Iteration 153/1000 | Loss: 0.00004327
Iteration 154/1000 | Loss: 0.00004327
Iteration 155/1000 | Loss: 0.00004326
Iteration 156/1000 | Loss: 0.00004326
Iteration 157/1000 | Loss: 0.00004326
Iteration 158/1000 | Loss: 0.00004326
Iteration 159/1000 | Loss: 0.00004326
Iteration 160/1000 | Loss: 0.00004326
Iteration 161/1000 | Loss: 0.00004326
Iteration 162/1000 | Loss: 0.00004326
Iteration 163/1000 | Loss: 0.00004326
Iteration 164/1000 | Loss: 0.00004326
Iteration 165/1000 | Loss: 0.00004326
Iteration 166/1000 | Loss: 0.00004326
Iteration 167/1000 | Loss: 0.00004326
Iteration 168/1000 | Loss: 0.00004326
Iteration 169/1000 | Loss: 0.00004326
Iteration 170/1000 | Loss: 0.00004325
Iteration 171/1000 | Loss: 0.00004325
Iteration 172/1000 | Loss: 0.00004325
Iteration 173/1000 | Loss: 0.00004325
Iteration 174/1000 | Loss: 0.00004325
Iteration 175/1000 | Loss: 0.00004325
Iteration 176/1000 | Loss: 0.00004325
Iteration 177/1000 | Loss: 0.00004325
Iteration 178/1000 | Loss: 0.00004325
Iteration 179/1000 | Loss: 0.00004325
Iteration 180/1000 | Loss: 0.00004325
Iteration 181/1000 | Loss: 0.00004325
Iteration 182/1000 | Loss: 0.00004325
Iteration 183/1000 | Loss: 0.00004324
Iteration 184/1000 | Loss: 0.00004324
Iteration 185/1000 | Loss: 0.00004324
Iteration 186/1000 | Loss: 0.00004324
Iteration 187/1000 | Loss: 0.00004324
Iteration 188/1000 | Loss: 0.00004324
Iteration 189/1000 | Loss: 0.00004324
Iteration 190/1000 | Loss: 0.00004324
Iteration 191/1000 | Loss: 0.00004324
Iteration 192/1000 | Loss: 0.00004324
Iteration 193/1000 | Loss: 0.00004324
Iteration 194/1000 | Loss: 0.00004324
Iteration 195/1000 | Loss: 0.00004323
Iteration 196/1000 | Loss: 0.00004323
Iteration 197/1000 | Loss: 0.00004323
Iteration 198/1000 | Loss: 0.00004323
Iteration 199/1000 | Loss: 0.00004323
Iteration 200/1000 | Loss: 0.00004323
Iteration 201/1000 | Loss: 0.00004322
Iteration 202/1000 | Loss: 0.00004322
Iteration 203/1000 | Loss: 0.00004322
Iteration 204/1000 | Loss: 0.00004322
Iteration 205/1000 | Loss: 0.00004322
Iteration 206/1000 | Loss: 0.00004322
Iteration 207/1000 | Loss: 0.00004322
Iteration 208/1000 | Loss: 0.00004321
Iteration 209/1000 | Loss: 0.00004321
Iteration 210/1000 | Loss: 0.00004321
Iteration 211/1000 | Loss: 0.00004321
Iteration 212/1000 | Loss: 0.00004321
Iteration 213/1000 | Loss: 0.00004321
Iteration 214/1000 | Loss: 0.00004321
Iteration 215/1000 | Loss: 0.00004321
Iteration 216/1000 | Loss: 0.00004321
Iteration 217/1000 | Loss: 0.00004321
Iteration 218/1000 | Loss: 0.00004320
Iteration 219/1000 | Loss: 0.00004320
Iteration 220/1000 | Loss: 0.00004320
Iteration 221/1000 | Loss: 0.00004320
Iteration 222/1000 | Loss: 0.00004320
Iteration 223/1000 | Loss: 0.00004320
Iteration 224/1000 | Loss: 0.00004320
Iteration 225/1000 | Loss: 0.00004320
Iteration 226/1000 | Loss: 0.00004320
Iteration 227/1000 | Loss: 0.00004320
Iteration 228/1000 | Loss: 0.00004320
Iteration 229/1000 | Loss: 0.00004320
Iteration 230/1000 | Loss: 0.00004320
Iteration 231/1000 | Loss: 0.00004319
Iteration 232/1000 | Loss: 0.00004319
Iteration 233/1000 | Loss: 0.00004319
Iteration 234/1000 | Loss: 0.00004319
Iteration 235/1000 | Loss: 0.00004319
Iteration 236/1000 | Loss: 0.00004319
Iteration 237/1000 | Loss: 0.00004319
Iteration 238/1000 | Loss: 0.00004319
Iteration 239/1000 | Loss: 0.00004319
Iteration 240/1000 | Loss: 0.00004319
Iteration 241/1000 | Loss: 0.00004319
Iteration 242/1000 | Loss: 0.00004319
Iteration 243/1000 | Loss: 0.00004319
Iteration 244/1000 | Loss: 0.00004319
Iteration 245/1000 | Loss: 0.00004319
Iteration 246/1000 | Loss: 0.00004319
Iteration 247/1000 | Loss: 0.00004319
Iteration 248/1000 | Loss: 0.00004319
Iteration 249/1000 | Loss: 0.00004319
Iteration 250/1000 | Loss: 0.00004319
Iteration 251/1000 | Loss: 0.00004319
Iteration 252/1000 | Loss: 0.00004319
Iteration 253/1000 | Loss: 0.00004319
Iteration 254/1000 | Loss: 0.00004319
Iteration 255/1000 | Loss: 0.00004319
Iteration 256/1000 | Loss: 0.00004319
Iteration 257/1000 | Loss: 0.00004319
Iteration 258/1000 | Loss: 0.00004319
Iteration 259/1000 | Loss: 0.00004319
Iteration 260/1000 | Loss: 0.00004319
Iteration 261/1000 | Loss: 0.00004319
Iteration 262/1000 | Loss: 0.00004319
Iteration 263/1000 | Loss: 0.00004319
Iteration 264/1000 | Loss: 0.00004319
Iteration 265/1000 | Loss: 0.00004319
Iteration 266/1000 | Loss: 0.00004319
Iteration 267/1000 | Loss: 0.00004319
Iteration 268/1000 | Loss: 0.00004319
Iteration 269/1000 | Loss: 0.00004319
Iteration 270/1000 | Loss: 0.00004319
Iteration 271/1000 | Loss: 0.00004319
Iteration 272/1000 | Loss: 0.00004319
Iteration 273/1000 | Loss: 0.00004319
Iteration 274/1000 | Loss: 0.00004319
Iteration 275/1000 | Loss: 0.00004319
Iteration 276/1000 | Loss: 0.00004319
Iteration 277/1000 | Loss: 0.00004319
Iteration 278/1000 | Loss: 0.00004319
Iteration 279/1000 | Loss: 0.00004319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [4.3194537283852696e-05, 4.3194537283852696e-05, 4.3194537283852696e-05, 4.3194537283852696e-05, 4.3194537283852696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3194537283852696e-05

Optimization complete. Final v2v error: 5.3666582107543945 mm

Highest mean error: 10.81394100189209 mm for frame 135

Lowest mean error: 3.936084508895874 mm for frame 57

Saving results

Total time: 93.03475379943848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447418
Iteration 2/25 | Loss: 0.00148974
Iteration 3/25 | Loss: 0.00137835
Iteration 4/25 | Loss: 0.00136269
Iteration 5/25 | Loss: 0.00135786
Iteration 6/25 | Loss: 0.00135636
Iteration 7/25 | Loss: 0.00135604
Iteration 8/25 | Loss: 0.00135604
Iteration 9/25 | Loss: 0.00135604
Iteration 10/25 | Loss: 0.00135604
Iteration 11/25 | Loss: 0.00135604
Iteration 12/25 | Loss: 0.00135604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013560449006035924, 0.0013560449006035924, 0.0013560449006035924, 0.0013560449006035924, 0.0013560449006035924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013560449006035924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30621743
Iteration 2/25 | Loss: 0.00117864
Iteration 3/25 | Loss: 0.00117864
Iteration 4/25 | Loss: 0.00117864
Iteration 5/25 | Loss: 0.00117864
Iteration 6/25 | Loss: 0.00117864
Iteration 7/25 | Loss: 0.00117864
Iteration 8/25 | Loss: 0.00117864
Iteration 9/25 | Loss: 0.00117864
Iteration 10/25 | Loss: 0.00117864
Iteration 11/25 | Loss: 0.00117864
Iteration 12/25 | Loss: 0.00117864
Iteration 13/25 | Loss: 0.00117864
Iteration 14/25 | Loss: 0.00117864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00117863726336509, 0.00117863726336509, 0.00117863726336509, 0.00117863726336509, 0.00117863726336509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00117863726336509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117864
Iteration 2/1000 | Loss: 0.00007422
Iteration 3/1000 | Loss: 0.00004355
Iteration 4/1000 | Loss: 0.00003252
Iteration 5/1000 | Loss: 0.00002810
Iteration 6/1000 | Loss: 0.00002536
Iteration 7/1000 | Loss: 0.00002418
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002279
Iteration 10/1000 | Loss: 0.00002236
Iteration 11/1000 | Loss: 0.00002213
Iteration 12/1000 | Loss: 0.00002184
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002162
Iteration 15/1000 | Loss: 0.00002160
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00002155
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002152
Iteration 20/1000 | Loss: 0.00002149
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002147
Iteration 23/1000 | Loss: 0.00002146
Iteration 24/1000 | Loss: 0.00002145
Iteration 25/1000 | Loss: 0.00002145
Iteration 26/1000 | Loss: 0.00002144
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002140
Iteration 29/1000 | Loss: 0.00002140
Iteration 30/1000 | Loss: 0.00002140
Iteration 31/1000 | Loss: 0.00002139
Iteration 32/1000 | Loss: 0.00002139
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002137
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002136
Iteration 40/1000 | Loss: 0.00002136
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002135
Iteration 43/1000 | Loss: 0.00002135
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002135
Iteration 46/1000 | Loss: 0.00002135
Iteration 47/1000 | Loss: 0.00002134
Iteration 48/1000 | Loss: 0.00002134
Iteration 49/1000 | Loss: 0.00002134
Iteration 50/1000 | Loss: 0.00002134
Iteration 51/1000 | Loss: 0.00002133
Iteration 52/1000 | Loss: 0.00002133
Iteration 53/1000 | Loss: 0.00002132
Iteration 54/1000 | Loss: 0.00002132
Iteration 55/1000 | Loss: 0.00002132
Iteration 56/1000 | Loss: 0.00002132
Iteration 57/1000 | Loss: 0.00002132
Iteration 58/1000 | Loss: 0.00002132
Iteration 59/1000 | Loss: 0.00002131
Iteration 60/1000 | Loss: 0.00002131
Iteration 61/1000 | Loss: 0.00002131
Iteration 62/1000 | Loss: 0.00002131
Iteration 63/1000 | Loss: 0.00002130
Iteration 64/1000 | Loss: 0.00002130
Iteration 65/1000 | Loss: 0.00002129
Iteration 66/1000 | Loss: 0.00002129
Iteration 67/1000 | Loss: 0.00002129
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002128
Iteration 70/1000 | Loss: 0.00002128
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002127
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002127
Iteration 88/1000 | Loss: 0.00002127
Iteration 89/1000 | Loss: 0.00002127
Iteration 90/1000 | Loss: 0.00002127
Iteration 91/1000 | Loss: 0.00002127
Iteration 92/1000 | Loss: 0.00002126
Iteration 93/1000 | Loss: 0.00002126
Iteration 94/1000 | Loss: 0.00002126
Iteration 95/1000 | Loss: 0.00002126
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002124
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002123
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002122
Iteration 109/1000 | Loss: 0.00002122
Iteration 110/1000 | Loss: 0.00002122
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002122
Iteration 113/1000 | Loss: 0.00002121
Iteration 114/1000 | Loss: 0.00002121
Iteration 115/1000 | Loss: 0.00002121
Iteration 116/1000 | Loss: 0.00002121
Iteration 117/1000 | Loss: 0.00002121
Iteration 118/1000 | Loss: 0.00002121
Iteration 119/1000 | Loss: 0.00002120
Iteration 120/1000 | Loss: 0.00002120
Iteration 121/1000 | Loss: 0.00002120
Iteration 122/1000 | Loss: 0.00002120
Iteration 123/1000 | Loss: 0.00002120
Iteration 124/1000 | Loss: 0.00002120
Iteration 125/1000 | Loss: 0.00002120
Iteration 126/1000 | Loss: 0.00002120
Iteration 127/1000 | Loss: 0.00002119
Iteration 128/1000 | Loss: 0.00002119
Iteration 129/1000 | Loss: 0.00002119
Iteration 130/1000 | Loss: 0.00002119
Iteration 131/1000 | Loss: 0.00002119
Iteration 132/1000 | Loss: 0.00002119
Iteration 133/1000 | Loss: 0.00002119
Iteration 134/1000 | Loss: 0.00002119
Iteration 135/1000 | Loss: 0.00002119
Iteration 136/1000 | Loss: 0.00002119
Iteration 137/1000 | Loss: 0.00002119
Iteration 138/1000 | Loss: 0.00002119
Iteration 139/1000 | Loss: 0.00002119
Iteration 140/1000 | Loss: 0.00002119
Iteration 141/1000 | Loss: 0.00002119
Iteration 142/1000 | Loss: 0.00002118
Iteration 143/1000 | Loss: 0.00002118
Iteration 144/1000 | Loss: 0.00002118
Iteration 145/1000 | Loss: 0.00002118
Iteration 146/1000 | Loss: 0.00002118
Iteration 147/1000 | Loss: 0.00002118
Iteration 148/1000 | Loss: 0.00002118
Iteration 149/1000 | Loss: 0.00002118
Iteration 150/1000 | Loss: 0.00002118
Iteration 151/1000 | Loss: 0.00002118
Iteration 152/1000 | Loss: 0.00002118
Iteration 153/1000 | Loss: 0.00002118
Iteration 154/1000 | Loss: 0.00002118
Iteration 155/1000 | Loss: 0.00002118
Iteration 156/1000 | Loss: 0.00002118
Iteration 157/1000 | Loss: 0.00002118
Iteration 158/1000 | Loss: 0.00002118
Iteration 159/1000 | Loss: 0.00002118
Iteration 160/1000 | Loss: 0.00002118
Iteration 161/1000 | Loss: 0.00002118
Iteration 162/1000 | Loss: 0.00002117
Iteration 163/1000 | Loss: 0.00002117
Iteration 164/1000 | Loss: 0.00002117
Iteration 165/1000 | Loss: 0.00002117
Iteration 166/1000 | Loss: 0.00002117
Iteration 167/1000 | Loss: 0.00002117
Iteration 168/1000 | Loss: 0.00002117
Iteration 169/1000 | Loss: 0.00002117
Iteration 170/1000 | Loss: 0.00002117
Iteration 171/1000 | Loss: 0.00002117
Iteration 172/1000 | Loss: 0.00002117
Iteration 173/1000 | Loss: 0.00002117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.1173071218072437e-05, 2.1173071218072437e-05, 2.1173071218072437e-05, 2.1173071218072437e-05, 2.1173071218072437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1173071218072437e-05

Optimization complete. Final v2v error: 3.960523843765259 mm

Highest mean error: 4.443417072296143 mm for frame 25

Lowest mean error: 3.7085134983062744 mm for frame 80

Saving results

Total time: 40.69761896133423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678061
Iteration 2/25 | Loss: 0.00169486
Iteration 3/25 | Loss: 0.00150294
Iteration 4/25 | Loss: 0.00148765
Iteration 5/25 | Loss: 0.00148356
Iteration 6/25 | Loss: 0.00148301
Iteration 7/25 | Loss: 0.00148301
Iteration 8/25 | Loss: 0.00148301
Iteration 9/25 | Loss: 0.00148301
Iteration 10/25 | Loss: 0.00148301
Iteration 11/25 | Loss: 0.00148301
Iteration 12/25 | Loss: 0.00148301
Iteration 13/25 | Loss: 0.00148301
Iteration 14/25 | Loss: 0.00148301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014830075670033693, 0.0014830075670033693, 0.0014830075670033693, 0.0014830075670033693, 0.0014830075670033693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014830075670033693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61092550
Iteration 2/25 | Loss: 0.00095983
Iteration 3/25 | Loss: 0.00095983
Iteration 4/25 | Loss: 0.00095983
Iteration 5/25 | Loss: 0.00095983
Iteration 6/25 | Loss: 0.00095983
Iteration 7/25 | Loss: 0.00095982
Iteration 8/25 | Loss: 0.00095982
Iteration 9/25 | Loss: 0.00095982
Iteration 10/25 | Loss: 0.00095982
Iteration 11/25 | Loss: 0.00095982
Iteration 12/25 | Loss: 0.00095982
Iteration 13/25 | Loss: 0.00095982
Iteration 14/25 | Loss: 0.00095982
Iteration 15/25 | Loss: 0.00095982
Iteration 16/25 | Loss: 0.00095982
Iteration 17/25 | Loss: 0.00095982
Iteration 18/25 | Loss: 0.00095982
Iteration 19/25 | Loss: 0.00095982
Iteration 20/25 | Loss: 0.00095982
Iteration 21/25 | Loss: 0.00095982
Iteration 22/25 | Loss: 0.00095982
Iteration 23/25 | Loss: 0.00095982
Iteration 24/25 | Loss: 0.00095982
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009598226170055568, 0.0009598226170055568, 0.0009598226170055568, 0.0009598226170055568, 0.0009598226170055568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009598226170055568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095982
Iteration 2/1000 | Loss: 0.00006710
Iteration 3/1000 | Loss: 0.00005206
Iteration 4/1000 | Loss: 0.00004680
Iteration 5/1000 | Loss: 0.00004496
Iteration 6/1000 | Loss: 0.00004394
Iteration 7/1000 | Loss: 0.00004324
Iteration 8/1000 | Loss: 0.00004256
Iteration 9/1000 | Loss: 0.00004216
Iteration 10/1000 | Loss: 0.00004190
Iteration 11/1000 | Loss: 0.00004169
Iteration 12/1000 | Loss: 0.00004152
Iteration 13/1000 | Loss: 0.00004126
Iteration 14/1000 | Loss: 0.00004107
Iteration 15/1000 | Loss: 0.00004093
Iteration 16/1000 | Loss: 0.00004074
Iteration 17/1000 | Loss: 0.00004068
Iteration 18/1000 | Loss: 0.00004054
Iteration 19/1000 | Loss: 0.00004042
Iteration 20/1000 | Loss: 0.00004027
Iteration 21/1000 | Loss: 0.00004015
Iteration 22/1000 | Loss: 0.00004000
Iteration 23/1000 | Loss: 0.00003988
Iteration 24/1000 | Loss: 0.00003985
Iteration 25/1000 | Loss: 0.00003984
Iteration 26/1000 | Loss: 0.00003984
Iteration 27/1000 | Loss: 0.00003983
Iteration 28/1000 | Loss: 0.00003982
Iteration 29/1000 | Loss: 0.00003981
Iteration 30/1000 | Loss: 0.00003980
Iteration 31/1000 | Loss: 0.00003976
Iteration 32/1000 | Loss: 0.00003976
Iteration 33/1000 | Loss: 0.00003976
Iteration 34/1000 | Loss: 0.00003976
Iteration 35/1000 | Loss: 0.00003976
Iteration 36/1000 | Loss: 0.00003976
Iteration 37/1000 | Loss: 0.00003976
Iteration 38/1000 | Loss: 0.00003976
Iteration 39/1000 | Loss: 0.00003976
Iteration 40/1000 | Loss: 0.00003975
Iteration 41/1000 | Loss: 0.00003975
Iteration 42/1000 | Loss: 0.00003975
Iteration 43/1000 | Loss: 0.00003975
Iteration 44/1000 | Loss: 0.00003975
Iteration 45/1000 | Loss: 0.00003975
Iteration 46/1000 | Loss: 0.00003975
Iteration 47/1000 | Loss: 0.00003975
Iteration 48/1000 | Loss: 0.00003975
Iteration 49/1000 | Loss: 0.00003974
Iteration 50/1000 | Loss: 0.00003973
Iteration 51/1000 | Loss: 0.00003973
Iteration 52/1000 | Loss: 0.00003973
Iteration 53/1000 | Loss: 0.00003973
Iteration 54/1000 | Loss: 0.00003973
Iteration 55/1000 | Loss: 0.00003973
Iteration 56/1000 | Loss: 0.00003973
Iteration 57/1000 | Loss: 0.00003973
Iteration 58/1000 | Loss: 0.00003973
Iteration 59/1000 | Loss: 0.00003973
Iteration 60/1000 | Loss: 0.00003973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [3.9732258301228285e-05, 3.9732258301228285e-05, 3.9732258301228285e-05, 3.9732258301228285e-05, 3.9732258301228285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9732258301228285e-05

Optimization complete. Final v2v error: 5.21356725692749 mm

Highest mean error: 6.128546237945557 mm for frame 172

Lowest mean error: 4.901041030883789 mm for frame 14

Saving results

Total time: 48.23625707626343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915010
Iteration 2/25 | Loss: 0.00178823
Iteration 3/25 | Loss: 0.00150589
Iteration 4/25 | Loss: 0.00146761
Iteration 5/25 | Loss: 0.00147794
Iteration 6/25 | Loss: 0.00146127
Iteration 7/25 | Loss: 0.00144453
Iteration 8/25 | Loss: 0.00143551
Iteration 9/25 | Loss: 0.00143352
Iteration 10/25 | Loss: 0.00143299
Iteration 11/25 | Loss: 0.00143293
Iteration 12/25 | Loss: 0.00143293
Iteration 13/25 | Loss: 0.00143293
Iteration 14/25 | Loss: 0.00143293
Iteration 15/25 | Loss: 0.00143293
Iteration 16/25 | Loss: 0.00143293
Iteration 17/25 | Loss: 0.00143293
Iteration 18/25 | Loss: 0.00143293
Iteration 19/25 | Loss: 0.00143293
Iteration 20/25 | Loss: 0.00143293
Iteration 21/25 | Loss: 0.00143293
Iteration 22/25 | Loss: 0.00143293
Iteration 23/25 | Loss: 0.00143293
Iteration 24/25 | Loss: 0.00143293
Iteration 25/25 | Loss: 0.00143293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33957434
Iteration 2/25 | Loss: 0.00105798
Iteration 3/25 | Loss: 0.00105796
Iteration 4/25 | Loss: 0.00105796
Iteration 5/25 | Loss: 0.00105796
Iteration 6/25 | Loss: 0.00105796
Iteration 7/25 | Loss: 0.00105795
Iteration 8/25 | Loss: 0.00105795
Iteration 9/25 | Loss: 0.00105795
Iteration 10/25 | Loss: 0.00105795
Iteration 11/25 | Loss: 0.00105795
Iteration 12/25 | Loss: 0.00105795
Iteration 13/25 | Loss: 0.00105795
Iteration 14/25 | Loss: 0.00105795
Iteration 15/25 | Loss: 0.00105795
Iteration 16/25 | Loss: 0.00105795
Iteration 17/25 | Loss: 0.00105795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010579540394246578, 0.0010579540394246578, 0.0010579540394246578, 0.0010579540394246578, 0.0010579540394246578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010579540394246578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105795
Iteration 2/1000 | Loss: 0.00005974
Iteration 3/1000 | Loss: 0.00004424
Iteration 4/1000 | Loss: 0.00003616
Iteration 5/1000 | Loss: 0.00003279
Iteration 6/1000 | Loss: 0.00003074
Iteration 7/1000 | Loss: 0.00003000
Iteration 8/1000 | Loss: 0.00002947
Iteration 9/1000 | Loss: 0.00002903
Iteration 10/1000 | Loss: 0.00002872
Iteration 11/1000 | Loss: 0.00002830
Iteration 12/1000 | Loss: 0.00002801
Iteration 13/1000 | Loss: 0.00002776
Iteration 14/1000 | Loss: 0.00002757
Iteration 15/1000 | Loss: 0.00002754
Iteration 16/1000 | Loss: 0.00002753
Iteration 17/1000 | Loss: 0.00002750
Iteration 18/1000 | Loss: 0.00002749
Iteration 19/1000 | Loss: 0.00002744
Iteration 20/1000 | Loss: 0.00002741
Iteration 21/1000 | Loss: 0.00002740
Iteration 22/1000 | Loss: 0.00002740
Iteration 23/1000 | Loss: 0.00002739
Iteration 24/1000 | Loss: 0.00002739
Iteration 25/1000 | Loss: 0.00002739
Iteration 26/1000 | Loss: 0.00002739
Iteration 27/1000 | Loss: 0.00002738
Iteration 28/1000 | Loss: 0.00002738
Iteration 29/1000 | Loss: 0.00002737
Iteration 30/1000 | Loss: 0.00002737
Iteration 31/1000 | Loss: 0.00002736
Iteration 32/1000 | Loss: 0.00002736
Iteration 33/1000 | Loss: 0.00002735
Iteration 34/1000 | Loss: 0.00002735
Iteration 35/1000 | Loss: 0.00002734
Iteration 36/1000 | Loss: 0.00002733
Iteration 37/1000 | Loss: 0.00002731
Iteration 38/1000 | Loss: 0.00002731
Iteration 39/1000 | Loss: 0.00002731
Iteration 40/1000 | Loss: 0.00002730
Iteration 41/1000 | Loss: 0.00002730
Iteration 42/1000 | Loss: 0.00002730
Iteration 43/1000 | Loss: 0.00002729
Iteration 44/1000 | Loss: 0.00002728
Iteration 45/1000 | Loss: 0.00002728
Iteration 46/1000 | Loss: 0.00002728
Iteration 47/1000 | Loss: 0.00002728
Iteration 48/1000 | Loss: 0.00002728
Iteration 49/1000 | Loss: 0.00002728
Iteration 50/1000 | Loss: 0.00002728
Iteration 51/1000 | Loss: 0.00002728
Iteration 52/1000 | Loss: 0.00002728
Iteration 53/1000 | Loss: 0.00002728
Iteration 54/1000 | Loss: 0.00002727
Iteration 55/1000 | Loss: 0.00002727
Iteration 56/1000 | Loss: 0.00002727
Iteration 57/1000 | Loss: 0.00002727
Iteration 58/1000 | Loss: 0.00002727
Iteration 59/1000 | Loss: 0.00002727
Iteration 60/1000 | Loss: 0.00002727
Iteration 61/1000 | Loss: 0.00002727
Iteration 62/1000 | Loss: 0.00002726
Iteration 63/1000 | Loss: 0.00002726
Iteration 64/1000 | Loss: 0.00002726
Iteration 65/1000 | Loss: 0.00002726
Iteration 66/1000 | Loss: 0.00002726
Iteration 67/1000 | Loss: 0.00002726
Iteration 68/1000 | Loss: 0.00002725
Iteration 69/1000 | Loss: 0.00002725
Iteration 70/1000 | Loss: 0.00002725
Iteration 71/1000 | Loss: 0.00002725
Iteration 72/1000 | Loss: 0.00002725
Iteration 73/1000 | Loss: 0.00002725
Iteration 74/1000 | Loss: 0.00002725
Iteration 75/1000 | Loss: 0.00002724
Iteration 76/1000 | Loss: 0.00002724
Iteration 77/1000 | Loss: 0.00002724
Iteration 78/1000 | Loss: 0.00002724
Iteration 79/1000 | Loss: 0.00002724
Iteration 80/1000 | Loss: 0.00002724
Iteration 81/1000 | Loss: 0.00002724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.7244052034802735e-05, 2.7244052034802735e-05, 2.7244052034802735e-05, 2.7244052034802735e-05, 2.7244052034802735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7244052034802735e-05

Optimization complete. Final v2v error: 4.447188854217529 mm

Highest mean error: 5.240694522857666 mm for frame 179

Lowest mean error: 4.041303634643555 mm for frame 23

Saving results

Total time: 51.267274141311646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885577
Iteration 2/25 | Loss: 0.00180555
Iteration 3/25 | Loss: 0.00145019
Iteration 4/25 | Loss: 0.00140779
Iteration 5/25 | Loss: 0.00138769
Iteration 6/25 | Loss: 0.00138296
Iteration 7/25 | Loss: 0.00137297
Iteration 8/25 | Loss: 0.00136413
Iteration 9/25 | Loss: 0.00136652
Iteration 10/25 | Loss: 0.00137183
Iteration 11/25 | Loss: 0.00136609
Iteration 12/25 | Loss: 0.00136189
Iteration 13/25 | Loss: 0.00136752
Iteration 14/25 | Loss: 0.00136023
Iteration 15/25 | Loss: 0.00136618
Iteration 16/25 | Loss: 0.00136010
Iteration 17/25 | Loss: 0.00135984
Iteration 18/25 | Loss: 0.00135985
Iteration 19/25 | Loss: 0.00135984
Iteration 20/25 | Loss: 0.00135978
Iteration 21/25 | Loss: 0.00135979
Iteration 22/25 | Loss: 0.00135978
Iteration 23/25 | Loss: 0.00135979
Iteration 24/25 | Loss: 0.00135976
Iteration 25/25 | Loss: 0.00135979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54241776
Iteration 2/25 | Loss: 0.00084984
Iteration 3/25 | Loss: 0.00084981
Iteration 4/25 | Loss: 0.00084981
Iteration 5/25 | Loss: 0.00084981
Iteration 6/25 | Loss: 0.00084981
Iteration 7/25 | Loss: 0.00084981
Iteration 8/25 | Loss: 0.00084981
Iteration 9/25 | Loss: 0.00084981
Iteration 10/25 | Loss: 0.00084981
Iteration 11/25 | Loss: 0.00084981
Iteration 12/25 | Loss: 0.00084981
Iteration 13/25 | Loss: 0.00084981
Iteration 14/25 | Loss: 0.00084981
Iteration 15/25 | Loss: 0.00084981
Iteration 16/25 | Loss: 0.00084981
Iteration 17/25 | Loss: 0.00084981
Iteration 18/25 | Loss: 0.00084981
Iteration 19/25 | Loss: 0.00084981
Iteration 20/25 | Loss: 0.00084981
Iteration 21/25 | Loss: 0.00084981
Iteration 22/25 | Loss: 0.00084981
Iteration 23/25 | Loss: 0.00084981
Iteration 24/25 | Loss: 0.00084981
Iteration 25/25 | Loss: 0.00084981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084981
Iteration 2/1000 | Loss: 0.00005571
Iteration 3/1000 | Loss: 0.00003550
Iteration 4/1000 | Loss: 0.00002670
Iteration 5/1000 | Loss: 0.00002460
Iteration 6/1000 | Loss: 0.00002390
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002327
Iteration 9/1000 | Loss: 0.00002302
Iteration 10/1000 | Loss: 0.00002282
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002270
Iteration 13/1000 | Loss: 0.00002266
Iteration 14/1000 | Loss: 0.00002265
Iteration 15/1000 | Loss: 0.00002265
Iteration 16/1000 | Loss: 0.00002261
Iteration 17/1000 | Loss: 0.00002261
Iteration 18/1000 | Loss: 0.00002259
Iteration 19/1000 | Loss: 0.00002258
Iteration 20/1000 | Loss: 0.00002258
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002255
Iteration 23/1000 | Loss: 0.00002255
Iteration 24/1000 | Loss: 0.00002255
Iteration 25/1000 | Loss: 0.00002254
Iteration 26/1000 | Loss: 0.00002254
Iteration 27/1000 | Loss: 0.00002254
Iteration 28/1000 | Loss: 0.00002254
Iteration 29/1000 | Loss: 0.00002254
Iteration 30/1000 | Loss: 0.00002254
Iteration 31/1000 | Loss: 0.00002254
Iteration 32/1000 | Loss: 0.00002254
Iteration 33/1000 | Loss: 0.00002254
Iteration 34/1000 | Loss: 0.00002254
Iteration 35/1000 | Loss: 0.00002254
Iteration 36/1000 | Loss: 0.00002252
Iteration 37/1000 | Loss: 0.00002252
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002251
Iteration 40/1000 | Loss: 0.00002251
Iteration 41/1000 | Loss: 0.00002251
Iteration 42/1000 | Loss: 0.00002251
Iteration 43/1000 | Loss: 0.00002250
Iteration 44/1000 | Loss: 0.00002250
Iteration 45/1000 | Loss: 0.00002250
Iteration 46/1000 | Loss: 0.00002249
Iteration 47/1000 | Loss: 0.00002249
Iteration 48/1000 | Loss: 0.00002249
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002248
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002247
Iteration 55/1000 | Loss: 0.00002247
Iteration 56/1000 | Loss: 0.00002247
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002246
Iteration 60/1000 | Loss: 0.00002246
Iteration 61/1000 | Loss: 0.00002246
Iteration 62/1000 | Loss: 0.00002246
Iteration 63/1000 | Loss: 0.00002246
Iteration 64/1000 | Loss: 0.00002246
Iteration 65/1000 | Loss: 0.00002245
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002245
Iteration 68/1000 | Loss: 0.00002245
Iteration 69/1000 | Loss: 0.00002246
Iteration 70/1000 | Loss: 0.00002246
Iteration 71/1000 | Loss: 0.00002246
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002245
Iteration 76/1000 | Loss: 0.00002245
Iteration 77/1000 | Loss: 0.00002245
Iteration 78/1000 | Loss: 0.00002245
Iteration 79/1000 | Loss: 0.00002245
Iteration 80/1000 | Loss: 0.00002245
Iteration 81/1000 | Loss: 0.00002245
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002244
Iteration 85/1000 | Loss: 0.00002244
Iteration 86/1000 | Loss: 0.00002244
Iteration 87/1000 | Loss: 0.00002244
Iteration 88/1000 | Loss: 0.00002243
Iteration 89/1000 | Loss: 0.00002243
Iteration 90/1000 | Loss: 0.00002243
Iteration 91/1000 | Loss: 0.00002243
Iteration 92/1000 | Loss: 0.00002243
Iteration 93/1000 | Loss: 0.00002243
Iteration 94/1000 | Loss: 0.00002243
Iteration 95/1000 | Loss: 0.00002243
Iteration 96/1000 | Loss: 0.00002243
Iteration 97/1000 | Loss: 0.00002243
Iteration 98/1000 | Loss: 0.00002243
Iteration 99/1000 | Loss: 0.00002243
Iteration 100/1000 | Loss: 0.00002242
Iteration 101/1000 | Loss: 0.00002242
Iteration 102/1000 | Loss: 0.00002242
Iteration 103/1000 | Loss: 0.00002242
Iteration 104/1000 | Loss: 0.00002242
Iteration 105/1000 | Loss: 0.00002242
Iteration 106/1000 | Loss: 0.00002241
Iteration 107/1000 | Loss: 0.00002241
Iteration 108/1000 | Loss: 0.00002241
Iteration 109/1000 | Loss: 0.00002239
Iteration 110/1000 | Loss: 0.00002239
Iteration 111/1000 | Loss: 0.00002239
Iteration 112/1000 | Loss: 0.00002238
Iteration 113/1000 | Loss: 0.00002238
Iteration 114/1000 | Loss: 0.00002238
Iteration 115/1000 | Loss: 0.00002238
Iteration 116/1000 | Loss: 0.00002238
Iteration 117/1000 | Loss: 0.00002238
Iteration 118/1000 | Loss: 0.00002238
Iteration 119/1000 | Loss: 0.00002237
Iteration 120/1000 | Loss: 0.00002237
Iteration 121/1000 | Loss: 0.00002237
Iteration 122/1000 | Loss: 0.00002237
Iteration 123/1000 | Loss: 0.00002237
Iteration 124/1000 | Loss: 0.00002237
Iteration 125/1000 | Loss: 0.00002237
Iteration 126/1000 | Loss: 0.00002237
Iteration 127/1000 | Loss: 0.00002239
Iteration 128/1000 | Loss: 0.00002239
Iteration 129/1000 | Loss: 0.00002239
Iteration 130/1000 | Loss: 0.00002239
Iteration 131/1000 | Loss: 0.00002237
Iteration 132/1000 | Loss: 0.00002237
Iteration 133/1000 | Loss: 0.00002237
Iteration 134/1000 | Loss: 0.00002237
Iteration 135/1000 | Loss: 0.00002237
Iteration 136/1000 | Loss: 0.00002237
Iteration 137/1000 | Loss: 0.00002237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.2366024495568126e-05, 2.2366024495568126e-05, 2.2366024495568126e-05, 2.2366024495568126e-05, 2.2366024495568126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2366024495568126e-05

Optimization complete. Final v2v error: 4.079762935638428 mm

Highest mean error: 11.207564353942871 mm for frame 205

Lowest mean error: 3.537379741668701 mm for frame 236

Saving results

Total time: 75.88912391662598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920863
Iteration 2/25 | Loss: 0.00151198
Iteration 3/25 | Loss: 0.00138672
Iteration 4/25 | Loss: 0.00137332
Iteration 5/25 | Loss: 0.00136778
Iteration 6/25 | Loss: 0.00136670
Iteration 7/25 | Loss: 0.00136670
Iteration 8/25 | Loss: 0.00136670
Iteration 9/25 | Loss: 0.00136670
Iteration 10/25 | Loss: 0.00136670
Iteration 11/25 | Loss: 0.00136670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001366704236716032, 0.001366704236716032, 0.001366704236716032, 0.001366704236716032, 0.001366704236716032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001366704236716032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99450135
Iteration 2/25 | Loss: 0.00102736
Iteration 3/25 | Loss: 0.00102736
Iteration 4/25 | Loss: 0.00102736
Iteration 5/25 | Loss: 0.00102736
Iteration 6/25 | Loss: 0.00102736
Iteration 7/25 | Loss: 0.00102736
Iteration 8/25 | Loss: 0.00102735
Iteration 9/25 | Loss: 0.00102735
Iteration 10/25 | Loss: 0.00102735
Iteration 11/25 | Loss: 0.00102735
Iteration 12/25 | Loss: 0.00102735
Iteration 13/25 | Loss: 0.00102735
Iteration 14/25 | Loss: 0.00102735
Iteration 15/25 | Loss: 0.00102735
Iteration 16/25 | Loss: 0.00102735
Iteration 17/25 | Loss: 0.00102735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010273543884977698, 0.0010273543884977698, 0.0010273543884977698, 0.0010273543884977698, 0.0010273543884977698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010273543884977698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102735
Iteration 2/1000 | Loss: 0.00004723
Iteration 3/1000 | Loss: 0.00003353
Iteration 4/1000 | Loss: 0.00002843
Iteration 5/1000 | Loss: 0.00002551
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002369
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002291
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00002257
Iteration 12/1000 | Loss: 0.00002255
Iteration 13/1000 | Loss: 0.00002247
Iteration 14/1000 | Loss: 0.00002246
Iteration 15/1000 | Loss: 0.00002245
Iteration 16/1000 | Loss: 0.00002244
Iteration 17/1000 | Loss: 0.00002243
Iteration 18/1000 | Loss: 0.00002243
Iteration 19/1000 | Loss: 0.00002242
Iteration 20/1000 | Loss: 0.00002242
Iteration 21/1000 | Loss: 0.00002241
Iteration 22/1000 | Loss: 0.00002241
Iteration 23/1000 | Loss: 0.00002239
Iteration 24/1000 | Loss: 0.00002238
Iteration 25/1000 | Loss: 0.00002238
Iteration 26/1000 | Loss: 0.00002238
Iteration 27/1000 | Loss: 0.00002237
Iteration 28/1000 | Loss: 0.00002237
Iteration 29/1000 | Loss: 0.00002237
Iteration 30/1000 | Loss: 0.00002237
Iteration 31/1000 | Loss: 0.00002237
Iteration 32/1000 | Loss: 0.00002237
Iteration 33/1000 | Loss: 0.00002236
Iteration 34/1000 | Loss: 0.00002235
Iteration 35/1000 | Loss: 0.00002234
Iteration 36/1000 | Loss: 0.00002234
Iteration 37/1000 | Loss: 0.00002234
Iteration 38/1000 | Loss: 0.00002234
Iteration 39/1000 | Loss: 0.00002234
Iteration 40/1000 | Loss: 0.00002234
Iteration 41/1000 | Loss: 0.00002234
Iteration 42/1000 | Loss: 0.00002234
Iteration 43/1000 | Loss: 0.00002233
Iteration 44/1000 | Loss: 0.00002233
Iteration 45/1000 | Loss: 0.00002233
Iteration 46/1000 | Loss: 0.00002233
Iteration 47/1000 | Loss: 0.00002233
Iteration 48/1000 | Loss: 0.00002233
Iteration 49/1000 | Loss: 0.00002233
Iteration 50/1000 | Loss: 0.00002232
Iteration 51/1000 | Loss: 0.00002232
Iteration 52/1000 | Loss: 0.00002232
Iteration 53/1000 | Loss: 0.00002231
Iteration 54/1000 | Loss: 0.00002231
Iteration 55/1000 | Loss: 0.00002230
Iteration 56/1000 | Loss: 0.00002230
Iteration 57/1000 | Loss: 0.00002230
Iteration 58/1000 | Loss: 0.00002230
Iteration 59/1000 | Loss: 0.00002230
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002226
Iteration 66/1000 | Loss: 0.00002226
Iteration 67/1000 | Loss: 0.00002225
Iteration 68/1000 | Loss: 0.00002225
Iteration 69/1000 | Loss: 0.00002225
Iteration 70/1000 | Loss: 0.00002225
Iteration 71/1000 | Loss: 0.00002224
Iteration 72/1000 | Loss: 0.00002224
Iteration 73/1000 | Loss: 0.00002224
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002224
Iteration 76/1000 | Loss: 0.00002224
Iteration 77/1000 | Loss: 0.00002224
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002224
Iteration 80/1000 | Loss: 0.00002224
Iteration 81/1000 | Loss: 0.00002224
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002223
Iteration 84/1000 | Loss: 0.00002223
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002223
Iteration 89/1000 | Loss: 0.00002223
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002222
Iteration 92/1000 | Loss: 0.00002222
Iteration 93/1000 | Loss: 0.00002222
Iteration 94/1000 | Loss: 0.00002222
Iteration 95/1000 | Loss: 0.00002222
Iteration 96/1000 | Loss: 0.00002222
Iteration 97/1000 | Loss: 0.00002222
Iteration 98/1000 | Loss: 0.00002222
Iteration 99/1000 | Loss: 0.00002222
Iteration 100/1000 | Loss: 0.00002222
Iteration 101/1000 | Loss: 0.00002221
Iteration 102/1000 | Loss: 0.00002221
Iteration 103/1000 | Loss: 0.00002221
Iteration 104/1000 | Loss: 0.00002221
Iteration 105/1000 | Loss: 0.00002221
Iteration 106/1000 | Loss: 0.00002221
Iteration 107/1000 | Loss: 0.00002221
Iteration 108/1000 | Loss: 0.00002221
Iteration 109/1000 | Loss: 0.00002221
Iteration 110/1000 | Loss: 0.00002221
Iteration 111/1000 | Loss: 0.00002221
Iteration 112/1000 | Loss: 0.00002221
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002221
Iteration 115/1000 | Loss: 0.00002221
Iteration 116/1000 | Loss: 0.00002221
Iteration 117/1000 | Loss: 0.00002221
Iteration 118/1000 | Loss: 0.00002221
Iteration 119/1000 | Loss: 0.00002221
Iteration 120/1000 | Loss: 0.00002221
Iteration 121/1000 | Loss: 0.00002221
Iteration 122/1000 | Loss: 0.00002221
Iteration 123/1000 | Loss: 0.00002221
Iteration 124/1000 | Loss: 0.00002221
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.2214779164642096e-05, 2.2214779164642096e-05, 2.2214779164642096e-05, 2.2214779164642096e-05, 2.2214779164642096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2214779164642096e-05

Optimization complete. Final v2v error: 4.063488960266113 mm

Highest mean error: 5.5945539474487305 mm for frame 91

Lowest mean error: 3.8011746406555176 mm for frame 125

Saving results

Total time: 33.50805997848511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009744
Iteration 2/25 | Loss: 0.00198621
Iteration 3/25 | Loss: 0.00159063
Iteration 4/25 | Loss: 0.00155904
Iteration 5/25 | Loss: 0.00154602
Iteration 6/25 | Loss: 0.00154383
Iteration 7/25 | Loss: 0.00154383
Iteration 8/25 | Loss: 0.00154383
Iteration 9/25 | Loss: 0.00154383
Iteration 10/25 | Loss: 0.00154383
Iteration 11/25 | Loss: 0.00154383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015438311966136098, 0.0015438311966136098, 0.0015438311966136098, 0.0015438311966136098, 0.0015438311966136098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015438311966136098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93827844
Iteration 2/25 | Loss: 0.00125775
Iteration 3/25 | Loss: 0.00125775
Iteration 4/25 | Loss: 0.00125775
Iteration 5/25 | Loss: 0.00125775
Iteration 6/25 | Loss: 0.00125775
Iteration 7/25 | Loss: 0.00125775
Iteration 8/25 | Loss: 0.00125775
Iteration 9/25 | Loss: 0.00125775
Iteration 10/25 | Loss: 0.00125775
Iteration 11/25 | Loss: 0.00125775
Iteration 12/25 | Loss: 0.00125775
Iteration 13/25 | Loss: 0.00125775
Iteration 14/25 | Loss: 0.00125775
Iteration 15/25 | Loss: 0.00125775
Iteration 16/25 | Loss: 0.00125775
Iteration 17/25 | Loss: 0.00125775
Iteration 18/25 | Loss: 0.00125775
Iteration 19/25 | Loss: 0.00125775
Iteration 20/25 | Loss: 0.00125775
Iteration 21/25 | Loss: 0.00125775
Iteration 22/25 | Loss: 0.00125775
Iteration 23/25 | Loss: 0.00125775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001257750322110951, 0.001257750322110951, 0.001257750322110951, 0.001257750322110951, 0.001257750322110951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001257750322110951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125775
Iteration 2/1000 | Loss: 0.00009903
Iteration 3/1000 | Loss: 0.00007551
Iteration 4/1000 | Loss: 0.00006613
Iteration 5/1000 | Loss: 0.00006051
Iteration 6/1000 | Loss: 0.00005789
Iteration 7/1000 | Loss: 0.00005638
Iteration 8/1000 | Loss: 0.00005542
Iteration 9/1000 | Loss: 0.00005462
Iteration 10/1000 | Loss: 0.00005400
Iteration 11/1000 | Loss: 0.00005360
Iteration 12/1000 | Loss: 0.00005324
Iteration 13/1000 | Loss: 0.00005293
Iteration 14/1000 | Loss: 0.00005258
Iteration 15/1000 | Loss: 0.00005229
Iteration 16/1000 | Loss: 0.00005205
Iteration 17/1000 | Loss: 0.00005183
Iteration 18/1000 | Loss: 0.00005162
Iteration 19/1000 | Loss: 0.00005142
Iteration 20/1000 | Loss: 0.00005121
Iteration 21/1000 | Loss: 0.00005103
Iteration 22/1000 | Loss: 0.00005090
Iteration 23/1000 | Loss: 0.00005089
Iteration 24/1000 | Loss: 0.00005089
Iteration 25/1000 | Loss: 0.00005089
Iteration 26/1000 | Loss: 0.00005085
Iteration 27/1000 | Loss: 0.00005084
Iteration 28/1000 | Loss: 0.00005084
Iteration 29/1000 | Loss: 0.00005083
Iteration 30/1000 | Loss: 0.00005083
Iteration 31/1000 | Loss: 0.00005083
Iteration 32/1000 | Loss: 0.00005083
Iteration 33/1000 | Loss: 0.00005083
Iteration 34/1000 | Loss: 0.00005082
Iteration 35/1000 | Loss: 0.00005082
Iteration 36/1000 | Loss: 0.00005080
Iteration 37/1000 | Loss: 0.00005079
Iteration 38/1000 | Loss: 0.00005079
Iteration 39/1000 | Loss: 0.00005077
Iteration 40/1000 | Loss: 0.00005077
Iteration 41/1000 | Loss: 0.00005076
Iteration 42/1000 | Loss: 0.00005075
Iteration 43/1000 | Loss: 0.00005075
Iteration 44/1000 | Loss: 0.00005075
Iteration 45/1000 | Loss: 0.00005075
Iteration 46/1000 | Loss: 0.00005074
Iteration 47/1000 | Loss: 0.00005074
Iteration 48/1000 | Loss: 0.00005074
Iteration 49/1000 | Loss: 0.00005074
Iteration 50/1000 | Loss: 0.00005074
Iteration 51/1000 | Loss: 0.00005073
Iteration 52/1000 | Loss: 0.00005073
Iteration 53/1000 | Loss: 0.00005073
Iteration 54/1000 | Loss: 0.00005073
Iteration 55/1000 | Loss: 0.00005072
Iteration 56/1000 | Loss: 0.00005072
Iteration 57/1000 | Loss: 0.00005072
Iteration 58/1000 | Loss: 0.00005072
Iteration 59/1000 | Loss: 0.00005071
Iteration 60/1000 | Loss: 0.00005071
Iteration 61/1000 | Loss: 0.00005071
Iteration 62/1000 | Loss: 0.00005070
Iteration 63/1000 | Loss: 0.00005070
Iteration 64/1000 | Loss: 0.00005070
Iteration 65/1000 | Loss: 0.00005069
Iteration 66/1000 | Loss: 0.00005069
Iteration 67/1000 | Loss: 0.00005069
Iteration 68/1000 | Loss: 0.00005069
Iteration 69/1000 | Loss: 0.00005069
Iteration 70/1000 | Loss: 0.00005069
Iteration 71/1000 | Loss: 0.00005069
Iteration 72/1000 | Loss: 0.00005069
Iteration 73/1000 | Loss: 0.00005069
Iteration 74/1000 | Loss: 0.00005069
Iteration 75/1000 | Loss: 0.00005069
Iteration 76/1000 | Loss: 0.00005068
Iteration 77/1000 | Loss: 0.00005068
Iteration 78/1000 | Loss: 0.00005068
Iteration 79/1000 | Loss: 0.00005068
Iteration 80/1000 | Loss: 0.00005068
Iteration 81/1000 | Loss: 0.00005068
Iteration 82/1000 | Loss: 0.00005068
Iteration 83/1000 | Loss: 0.00005068
Iteration 84/1000 | Loss: 0.00005068
Iteration 85/1000 | Loss: 0.00005068
Iteration 86/1000 | Loss: 0.00005067
Iteration 87/1000 | Loss: 0.00005067
Iteration 88/1000 | Loss: 0.00005067
Iteration 89/1000 | Loss: 0.00005067
Iteration 90/1000 | Loss: 0.00005067
Iteration 91/1000 | Loss: 0.00005067
Iteration 92/1000 | Loss: 0.00005067
Iteration 93/1000 | Loss: 0.00005067
Iteration 94/1000 | Loss: 0.00005067
Iteration 95/1000 | Loss: 0.00005067
Iteration 96/1000 | Loss: 0.00005067
Iteration 97/1000 | Loss: 0.00005067
Iteration 98/1000 | Loss: 0.00005067
Iteration 99/1000 | Loss: 0.00005067
Iteration 100/1000 | Loss: 0.00005067
Iteration 101/1000 | Loss: 0.00005067
Iteration 102/1000 | Loss: 0.00005067
Iteration 103/1000 | Loss: 0.00005067
Iteration 104/1000 | Loss: 0.00005067
Iteration 105/1000 | Loss: 0.00005067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [5.067154415883124e-05, 5.067154415883124e-05, 5.067154415883124e-05, 5.067154415883124e-05, 5.067154415883124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.067154415883124e-05

Optimization complete. Final v2v error: 5.777037620544434 mm

Highest mean error: 7.03148078918457 mm for frame 124

Lowest mean error: 4.481738567352295 mm for frame 0

Saving results

Total time: 54.78490495681763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578370
Iteration 2/25 | Loss: 0.00162994
Iteration 3/25 | Loss: 0.00145033
Iteration 4/25 | Loss: 0.00142808
Iteration 5/25 | Loss: 0.00140081
Iteration 6/25 | Loss: 0.00138374
Iteration 7/25 | Loss: 0.00139521
Iteration 8/25 | Loss: 0.00138736
Iteration 9/25 | Loss: 0.00137619
Iteration 10/25 | Loss: 0.00137077
Iteration 11/25 | Loss: 0.00136802
Iteration 12/25 | Loss: 0.00136591
Iteration 13/25 | Loss: 0.00136480
Iteration 14/25 | Loss: 0.00136386
Iteration 15/25 | Loss: 0.00136357
Iteration 16/25 | Loss: 0.00136355
Iteration 17/25 | Loss: 0.00136355
Iteration 18/25 | Loss: 0.00136355
Iteration 19/25 | Loss: 0.00136355
Iteration 20/25 | Loss: 0.00136355
Iteration 21/25 | Loss: 0.00136355
Iteration 22/25 | Loss: 0.00136354
Iteration 23/25 | Loss: 0.00136354
Iteration 24/25 | Loss: 0.00136354
Iteration 25/25 | Loss: 0.00136354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34581316
Iteration 2/25 | Loss: 0.00106138
Iteration 3/25 | Loss: 0.00106138
Iteration 4/25 | Loss: 0.00106138
Iteration 5/25 | Loss: 0.00106138
Iteration 6/25 | Loss: 0.00106138
Iteration 7/25 | Loss: 0.00106138
Iteration 8/25 | Loss: 0.00106138
Iteration 9/25 | Loss: 0.00106138
Iteration 10/25 | Loss: 0.00106138
Iteration 11/25 | Loss: 0.00106138
Iteration 12/25 | Loss: 0.00106138
Iteration 13/25 | Loss: 0.00106138
Iteration 14/25 | Loss: 0.00106138
Iteration 15/25 | Loss: 0.00106138
Iteration 16/25 | Loss: 0.00106138
Iteration 17/25 | Loss: 0.00106138
Iteration 18/25 | Loss: 0.00106138
Iteration 19/25 | Loss: 0.00106138
Iteration 20/25 | Loss: 0.00106138
Iteration 21/25 | Loss: 0.00106138
Iteration 22/25 | Loss: 0.00106138
Iteration 23/25 | Loss: 0.00106138
Iteration 24/25 | Loss: 0.00106138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010613782797008753, 0.0010613782797008753, 0.0010613782797008753, 0.0010613782797008753, 0.0010613782797008753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010613782797008753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106138
Iteration 2/1000 | Loss: 0.00004480
Iteration 3/1000 | Loss: 0.00003235
Iteration 4/1000 | Loss: 0.00002808
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002394
Iteration 7/1000 | Loss: 0.00002315
Iteration 8/1000 | Loss: 0.00002272
Iteration 9/1000 | Loss: 0.00002240
Iteration 10/1000 | Loss: 0.00002212
Iteration 11/1000 | Loss: 0.00002198
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002197
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002193
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002190
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002188
Iteration 22/1000 | Loss: 0.00002187
Iteration 23/1000 | Loss: 0.00002187
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002186
Iteration 26/1000 | Loss: 0.00002186
Iteration 27/1000 | Loss: 0.00002185
Iteration 28/1000 | Loss: 0.00002185
Iteration 29/1000 | Loss: 0.00002185
Iteration 30/1000 | Loss: 0.00002185
Iteration 31/1000 | Loss: 0.00002185
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00002183
Iteration 40/1000 | Loss: 0.00002183
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00002183
Iteration 44/1000 | Loss: 0.00002182
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002181
Iteration 52/1000 | Loss: 0.00002181
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002181
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002180
Iteration 60/1000 | Loss: 0.00002180
Iteration 61/1000 | Loss: 0.00002180
Iteration 62/1000 | Loss: 0.00002180
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002179
Iteration 66/1000 | Loss: 0.00002179
Iteration 67/1000 | Loss: 0.00002179
Iteration 68/1000 | Loss: 0.00002179
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002177
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00002177
Iteration 80/1000 | Loss: 0.00002177
Iteration 81/1000 | Loss: 0.00002177
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002176
Iteration 85/1000 | Loss: 0.00002176
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002175
Iteration 89/1000 | Loss: 0.00002175
Iteration 90/1000 | Loss: 0.00002175
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002175
Iteration 93/1000 | Loss: 0.00002175
Iteration 94/1000 | Loss: 0.00002175
Iteration 95/1000 | Loss: 0.00002175
Iteration 96/1000 | Loss: 0.00002175
Iteration 97/1000 | Loss: 0.00002175
Iteration 98/1000 | Loss: 0.00002175
Iteration 99/1000 | Loss: 0.00002175
Iteration 100/1000 | Loss: 0.00002175
Iteration 101/1000 | Loss: 0.00002175
Iteration 102/1000 | Loss: 0.00002175
Iteration 103/1000 | Loss: 0.00002175
Iteration 104/1000 | Loss: 0.00002175
Iteration 105/1000 | Loss: 0.00002175
Iteration 106/1000 | Loss: 0.00002175
Iteration 107/1000 | Loss: 0.00002175
Iteration 108/1000 | Loss: 0.00002175
Iteration 109/1000 | Loss: 0.00002175
Iteration 110/1000 | Loss: 0.00002175
Iteration 111/1000 | Loss: 0.00002175
Iteration 112/1000 | Loss: 0.00002175
Iteration 113/1000 | Loss: 0.00002175
Iteration 114/1000 | Loss: 0.00002175
Iteration 115/1000 | Loss: 0.00002175
Iteration 116/1000 | Loss: 0.00002175
Iteration 117/1000 | Loss: 0.00002175
Iteration 118/1000 | Loss: 0.00002175
Iteration 119/1000 | Loss: 0.00002175
Iteration 120/1000 | Loss: 0.00002175
Iteration 121/1000 | Loss: 0.00002175
Iteration 122/1000 | Loss: 0.00002175
Iteration 123/1000 | Loss: 0.00002175
Iteration 124/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.174550354538951e-05, 2.174550354538951e-05, 2.174550354538951e-05, 2.174550354538951e-05, 2.174550354538951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.174550354538951e-05

Optimization complete. Final v2v error: 4.011066436767578 mm

Highest mean error: 4.426504135131836 mm for frame 40

Lowest mean error: 3.729254722595215 mm for frame 7

Saving results

Total time: 51.4272038936615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042117
Iteration 2/25 | Loss: 0.00265312
Iteration 3/25 | Loss: 0.00195171
Iteration 4/25 | Loss: 0.00182477
Iteration 5/25 | Loss: 0.00170328
Iteration 6/25 | Loss: 0.00165033
Iteration 7/25 | Loss: 0.00163333
Iteration 8/25 | Loss: 0.00163825
Iteration 9/25 | Loss: 0.00165480
Iteration 10/25 | Loss: 0.00164225
Iteration 11/25 | Loss: 0.00163723
Iteration 12/25 | Loss: 0.00163020
Iteration 13/25 | Loss: 0.00162584
Iteration 14/25 | Loss: 0.00162712
Iteration 15/25 | Loss: 0.00163432
Iteration 16/25 | Loss: 0.00163189
Iteration 17/25 | Loss: 0.00162244
Iteration 18/25 | Loss: 0.00162511
Iteration 19/25 | Loss: 0.00165450
Iteration 20/25 | Loss: 0.00161342
Iteration 21/25 | Loss: 0.00152458
Iteration 22/25 | Loss: 0.00149408
Iteration 23/25 | Loss: 0.00149107
Iteration 24/25 | Loss: 0.00148212
Iteration 25/25 | Loss: 0.00147673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31093764
Iteration 2/25 | Loss: 0.00172432
Iteration 3/25 | Loss: 0.00170458
Iteration 4/25 | Loss: 0.00170457
Iteration 5/25 | Loss: 0.00170457
Iteration 6/25 | Loss: 0.00170457
Iteration 7/25 | Loss: 0.00170457
Iteration 8/25 | Loss: 0.00170457
Iteration 9/25 | Loss: 0.00170457
Iteration 10/25 | Loss: 0.00170457
Iteration 11/25 | Loss: 0.00170457
Iteration 12/25 | Loss: 0.00170457
Iteration 13/25 | Loss: 0.00170457
Iteration 14/25 | Loss: 0.00170457
Iteration 15/25 | Loss: 0.00170457
Iteration 16/25 | Loss: 0.00170457
Iteration 17/25 | Loss: 0.00170457
Iteration 18/25 | Loss: 0.00170457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017045718850567937, 0.0017045718850567937, 0.0017045718850567937, 0.0017045718850567937, 0.0017045718850567937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017045718850567937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170457
Iteration 2/1000 | Loss: 0.00016592
Iteration 3/1000 | Loss: 0.00013610
Iteration 4/1000 | Loss: 0.00019127
Iteration 5/1000 | Loss: 0.00086578
Iteration 6/1000 | Loss: 0.00008953
Iteration 7/1000 | Loss: 0.00078867
Iteration 8/1000 | Loss: 0.00007613
Iteration 9/1000 | Loss: 0.00078814
Iteration 10/1000 | Loss: 0.00007068
Iteration 11/1000 | Loss: 0.00072500
Iteration 12/1000 | Loss: 0.00037880
Iteration 13/1000 | Loss: 0.00044530
Iteration 14/1000 | Loss: 0.00006629
Iteration 15/1000 | Loss: 0.00066224
Iteration 16/1000 | Loss: 0.00029926
Iteration 17/1000 | Loss: 0.00061287
Iteration 18/1000 | Loss: 0.00005913
Iteration 19/1000 | Loss: 0.00005833
Iteration 20/1000 | Loss: 0.00004973
Iteration 21/1000 | Loss: 0.00005548
Iteration 22/1000 | Loss: 0.00004294
Iteration 23/1000 | Loss: 0.00005466
Iteration 24/1000 | Loss: 0.00004069
Iteration 25/1000 | Loss: 0.00004005
Iteration 26/1000 | Loss: 0.00003939
Iteration 27/1000 | Loss: 0.00004030
Iteration 28/1000 | Loss: 0.00003894
Iteration 29/1000 | Loss: 0.00003822
Iteration 30/1000 | Loss: 0.00003790
Iteration 31/1000 | Loss: 0.00003760
Iteration 32/1000 | Loss: 0.00058035
Iteration 33/1000 | Loss: 0.00067733
Iteration 34/1000 | Loss: 0.00003811
Iteration 35/1000 | Loss: 0.00059323
Iteration 36/1000 | Loss: 0.00005817
Iteration 37/1000 | Loss: 0.00010990
Iteration 38/1000 | Loss: 0.00005121
Iteration 39/1000 | Loss: 0.00004043
Iteration 40/1000 | Loss: 0.00043426
Iteration 41/1000 | Loss: 0.00019770
Iteration 42/1000 | Loss: 0.00004162
Iteration 43/1000 | Loss: 0.00003958
Iteration 44/1000 | Loss: 0.00003915
Iteration 45/1000 | Loss: 0.00003885
Iteration 46/1000 | Loss: 0.00053154
Iteration 47/1000 | Loss: 0.00015140
Iteration 48/1000 | Loss: 0.00032377
Iteration 49/1000 | Loss: 0.00016509
Iteration 50/1000 | Loss: 0.00031228
Iteration 51/1000 | Loss: 0.00016815
Iteration 52/1000 | Loss: 0.00029360
Iteration 53/1000 | Loss: 0.00016658
Iteration 54/1000 | Loss: 0.00022740
Iteration 55/1000 | Loss: 0.00016103
Iteration 56/1000 | Loss: 0.00012239
Iteration 57/1000 | Loss: 0.00015251
Iteration 58/1000 | Loss: 0.00047738
Iteration 59/1000 | Loss: 0.00049339
Iteration 60/1000 | Loss: 0.00044192
Iteration 61/1000 | Loss: 0.00013870
Iteration 62/1000 | Loss: 0.00044987
Iteration 63/1000 | Loss: 0.00021030
Iteration 64/1000 | Loss: 0.00017722
Iteration 65/1000 | Loss: 0.00069042
Iteration 66/1000 | Loss: 0.00025361
Iteration 67/1000 | Loss: 0.00016640
Iteration 68/1000 | Loss: 0.00037913
Iteration 69/1000 | Loss: 0.00033555
Iteration 70/1000 | Loss: 0.00016003
Iteration 71/1000 | Loss: 0.00019316
Iteration 72/1000 | Loss: 0.00074491
Iteration 73/1000 | Loss: 0.00081734
Iteration 74/1000 | Loss: 0.00045680
Iteration 75/1000 | Loss: 0.00043245
Iteration 76/1000 | Loss: 0.00018879
Iteration 77/1000 | Loss: 0.00003953
Iteration 78/1000 | Loss: 0.00009895
Iteration 79/1000 | Loss: 0.00029422
Iteration 80/1000 | Loss: 0.00027212
Iteration 81/1000 | Loss: 0.00031395
Iteration 82/1000 | Loss: 0.00019862
Iteration 83/1000 | Loss: 0.00027401
Iteration 84/1000 | Loss: 0.00042021
Iteration 85/1000 | Loss: 0.00021932
Iteration 86/1000 | Loss: 0.00004699
Iteration 87/1000 | Loss: 0.00042157
Iteration 88/1000 | Loss: 0.00039410
Iteration 89/1000 | Loss: 0.00006237
Iteration 90/1000 | Loss: 0.00004829
Iteration 91/1000 | Loss: 0.00004443
Iteration 92/1000 | Loss: 0.00004333
Iteration 93/1000 | Loss: 0.00003778
Iteration 94/1000 | Loss: 0.00003623
Iteration 95/1000 | Loss: 0.00003509
Iteration 96/1000 | Loss: 0.00003426
Iteration 97/1000 | Loss: 0.00003396
Iteration 98/1000 | Loss: 0.00003371
Iteration 99/1000 | Loss: 0.00003370
Iteration 100/1000 | Loss: 0.00003368
Iteration 101/1000 | Loss: 0.00003364
Iteration 102/1000 | Loss: 0.00003362
Iteration 103/1000 | Loss: 0.00003361
Iteration 104/1000 | Loss: 0.00003361
Iteration 105/1000 | Loss: 0.00003360
Iteration 106/1000 | Loss: 0.00003355
Iteration 107/1000 | Loss: 0.00003353
Iteration 108/1000 | Loss: 0.00003339
Iteration 109/1000 | Loss: 0.00003339
Iteration 110/1000 | Loss: 0.00003337
Iteration 111/1000 | Loss: 0.00003336
Iteration 112/1000 | Loss: 0.00003336
Iteration 113/1000 | Loss: 0.00003336
Iteration 114/1000 | Loss: 0.00003336
Iteration 115/1000 | Loss: 0.00003335
Iteration 116/1000 | Loss: 0.00003335
Iteration 117/1000 | Loss: 0.00003335
Iteration 118/1000 | Loss: 0.00003334
Iteration 119/1000 | Loss: 0.00003334
Iteration 120/1000 | Loss: 0.00003334
Iteration 121/1000 | Loss: 0.00003333
Iteration 122/1000 | Loss: 0.00003333
Iteration 123/1000 | Loss: 0.00003331
Iteration 124/1000 | Loss: 0.00003331
Iteration 125/1000 | Loss: 0.00003330
Iteration 126/1000 | Loss: 0.00003330
Iteration 127/1000 | Loss: 0.00003329
Iteration 128/1000 | Loss: 0.00003329
Iteration 129/1000 | Loss: 0.00003328
Iteration 130/1000 | Loss: 0.00003328
Iteration 131/1000 | Loss: 0.00003328
Iteration 132/1000 | Loss: 0.00003327
Iteration 133/1000 | Loss: 0.00003327
Iteration 134/1000 | Loss: 0.00003326
Iteration 135/1000 | Loss: 0.00003326
Iteration 136/1000 | Loss: 0.00003326
Iteration 137/1000 | Loss: 0.00003325
Iteration 138/1000 | Loss: 0.00003325
Iteration 139/1000 | Loss: 0.00003324
Iteration 140/1000 | Loss: 0.00003324
Iteration 141/1000 | Loss: 0.00003324
Iteration 142/1000 | Loss: 0.00003323
Iteration 143/1000 | Loss: 0.00004589
Iteration 144/1000 | Loss: 0.00003659
Iteration 145/1000 | Loss: 0.00003399
Iteration 146/1000 | Loss: 0.00003320
Iteration 147/1000 | Loss: 0.00003320
Iteration 148/1000 | Loss: 0.00003319
Iteration 149/1000 | Loss: 0.00003319
Iteration 150/1000 | Loss: 0.00003319
Iteration 151/1000 | Loss: 0.00003319
Iteration 152/1000 | Loss: 0.00003319
Iteration 153/1000 | Loss: 0.00003319
Iteration 154/1000 | Loss: 0.00003319
Iteration 155/1000 | Loss: 0.00003319
Iteration 156/1000 | Loss: 0.00003319
Iteration 157/1000 | Loss: 0.00003319
Iteration 158/1000 | Loss: 0.00003319
Iteration 159/1000 | Loss: 0.00003319
Iteration 160/1000 | Loss: 0.00003319
Iteration 161/1000 | Loss: 0.00003319
Iteration 162/1000 | Loss: 0.00003318
Iteration 163/1000 | Loss: 0.00003318
Iteration 164/1000 | Loss: 0.00003318
Iteration 165/1000 | Loss: 0.00003318
Iteration 166/1000 | Loss: 0.00003860
Iteration 167/1000 | Loss: 0.00003670
Iteration 168/1000 | Loss: 0.00003368
Iteration 169/1000 | Loss: 0.00003646
Iteration 170/1000 | Loss: 0.00003563
Iteration 171/1000 | Loss: 0.00003435
Iteration 172/1000 | Loss: 0.00003359
Iteration 173/1000 | Loss: 0.00003437
Iteration 174/1000 | Loss: 0.00003726
Iteration 175/1000 | Loss: 0.00003318
Iteration 176/1000 | Loss: 0.00003318
Iteration 177/1000 | Loss: 0.00003318
Iteration 178/1000 | Loss: 0.00003318
Iteration 179/1000 | Loss: 0.00003318
Iteration 180/1000 | Loss: 0.00003318
Iteration 181/1000 | Loss: 0.00003318
Iteration 182/1000 | Loss: 0.00003318
Iteration 183/1000 | Loss: 0.00003318
Iteration 184/1000 | Loss: 0.00003324
Iteration 185/1000 | Loss: 0.00003323
Iteration 186/1000 | Loss: 0.00003320
Iteration 187/1000 | Loss: 0.00003432
Iteration 188/1000 | Loss: 0.00003348
Iteration 189/1000 | Loss: 0.00003348
Iteration 190/1000 | Loss: 0.00003318
Iteration 191/1000 | Loss: 0.00003317
Iteration 192/1000 | Loss: 0.00003317
Iteration 193/1000 | Loss: 0.00003317
Iteration 194/1000 | Loss: 0.00003317
Iteration 195/1000 | Loss: 0.00003316
Iteration 196/1000 | Loss: 0.00003316
Iteration 197/1000 | Loss: 0.00003316
Iteration 198/1000 | Loss: 0.00003316
Iteration 199/1000 | Loss: 0.00003316
Iteration 200/1000 | Loss: 0.00003316
Iteration 201/1000 | Loss: 0.00003343
Iteration 202/1000 | Loss: 0.00003323
Iteration 203/1000 | Loss: 0.00003317
Iteration 204/1000 | Loss: 0.00003317
Iteration 205/1000 | Loss: 0.00003317
Iteration 206/1000 | Loss: 0.00003316
Iteration 207/1000 | Loss: 0.00003316
Iteration 208/1000 | Loss: 0.00003316
Iteration 209/1000 | Loss: 0.00003316
Iteration 210/1000 | Loss: 0.00003316
Iteration 211/1000 | Loss: 0.00003316
Iteration 212/1000 | Loss: 0.00003316
Iteration 213/1000 | Loss: 0.00003316
Iteration 214/1000 | Loss: 0.00003316
Iteration 215/1000 | Loss: 0.00003316
Iteration 216/1000 | Loss: 0.00003316
Iteration 217/1000 | Loss: 0.00003317
Iteration 218/1000 | Loss: 0.00003317
Iteration 219/1000 | Loss: 0.00003317
Iteration 220/1000 | Loss: 0.00003317
Iteration 221/1000 | Loss: 0.00003316
Iteration 222/1000 | Loss: 0.00003316
Iteration 223/1000 | Loss: 0.00003316
Iteration 224/1000 | Loss: 0.00003316
Iteration 225/1000 | Loss: 0.00003316
Iteration 226/1000 | Loss: 0.00003316
Iteration 227/1000 | Loss: 0.00003316
Iteration 228/1000 | Loss: 0.00003316
Iteration 229/1000 | Loss: 0.00003316
Iteration 230/1000 | Loss: 0.00003316
Iteration 231/1000 | Loss: 0.00003316
Iteration 232/1000 | Loss: 0.00003316
Iteration 233/1000 | Loss: 0.00003316
Iteration 234/1000 | Loss: 0.00003316
Iteration 235/1000 | Loss: 0.00003316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [3.3161089959321544e-05, 3.3161089959321544e-05, 3.3161089959321544e-05, 3.3161089959321544e-05, 3.3161089959321544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3161089959321544e-05

Optimization complete. Final v2v error: 4.532196521759033 mm

Highest mean error: 12.023114204406738 mm for frame 21

Lowest mean error: 3.735872268676758 mm for frame 3

Saving results

Total time: 211.3556694984436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546444
Iteration 2/25 | Loss: 0.00153383
Iteration 3/25 | Loss: 0.00141800
Iteration 4/25 | Loss: 0.00139633
Iteration 5/25 | Loss: 0.00138718
Iteration 6/25 | Loss: 0.00138404
Iteration 7/25 | Loss: 0.00138226
Iteration 8/25 | Loss: 0.00138191
Iteration 9/25 | Loss: 0.00138191
Iteration 10/25 | Loss: 0.00138191
Iteration 11/25 | Loss: 0.00138191
Iteration 12/25 | Loss: 0.00138191
Iteration 13/25 | Loss: 0.00138191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013819128507748246, 0.0013819128507748246, 0.0013819128507748246, 0.0013819128507748246, 0.0013819128507748246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013819128507748246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.00642538
Iteration 2/25 | Loss: 0.00111053
Iteration 3/25 | Loss: 0.00111053
Iteration 4/25 | Loss: 0.00111053
Iteration 5/25 | Loss: 0.00111053
Iteration 6/25 | Loss: 0.00111053
Iteration 7/25 | Loss: 0.00111053
Iteration 8/25 | Loss: 0.00111053
Iteration 9/25 | Loss: 0.00111053
Iteration 10/25 | Loss: 0.00111053
Iteration 11/25 | Loss: 0.00111053
Iteration 12/25 | Loss: 0.00111053
Iteration 13/25 | Loss: 0.00111053
Iteration 14/25 | Loss: 0.00111053
Iteration 15/25 | Loss: 0.00111053
Iteration 16/25 | Loss: 0.00111053
Iteration 17/25 | Loss: 0.00111053
Iteration 18/25 | Loss: 0.00111053
Iteration 19/25 | Loss: 0.00111053
Iteration 20/25 | Loss: 0.00111053
Iteration 21/25 | Loss: 0.00111053
Iteration 22/25 | Loss: 0.00111053
Iteration 23/25 | Loss: 0.00111053
Iteration 24/25 | Loss: 0.00111053
Iteration 25/25 | Loss: 0.00111053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111053
Iteration 2/1000 | Loss: 0.00006022
Iteration 3/1000 | Loss: 0.00004287
Iteration 4/1000 | Loss: 0.00003616
Iteration 5/1000 | Loss: 0.00003166
Iteration 6/1000 | Loss: 0.00002926
Iteration 7/1000 | Loss: 0.00002849
Iteration 8/1000 | Loss: 0.00002801
Iteration 9/1000 | Loss: 0.00002772
Iteration 10/1000 | Loss: 0.00002741
Iteration 11/1000 | Loss: 0.00002711
Iteration 12/1000 | Loss: 0.00002692
Iteration 13/1000 | Loss: 0.00002689
Iteration 14/1000 | Loss: 0.00002685
Iteration 15/1000 | Loss: 0.00002670
Iteration 16/1000 | Loss: 0.00002666
Iteration 17/1000 | Loss: 0.00002666
Iteration 18/1000 | Loss: 0.00002666
Iteration 19/1000 | Loss: 0.00002666
Iteration 20/1000 | Loss: 0.00002665
Iteration 21/1000 | Loss: 0.00002665
Iteration 22/1000 | Loss: 0.00002661
Iteration 23/1000 | Loss: 0.00002661
Iteration 24/1000 | Loss: 0.00002661
Iteration 25/1000 | Loss: 0.00002659
Iteration 26/1000 | Loss: 0.00002659
Iteration 27/1000 | Loss: 0.00002659
Iteration 28/1000 | Loss: 0.00002656
Iteration 29/1000 | Loss: 0.00002655
Iteration 30/1000 | Loss: 0.00002655
Iteration 31/1000 | Loss: 0.00002655
Iteration 32/1000 | Loss: 0.00002654
Iteration 33/1000 | Loss: 0.00002654
Iteration 34/1000 | Loss: 0.00002653
Iteration 35/1000 | Loss: 0.00002653
Iteration 36/1000 | Loss: 0.00002652
Iteration 37/1000 | Loss: 0.00002652
Iteration 38/1000 | Loss: 0.00002652
Iteration 39/1000 | Loss: 0.00002651
Iteration 40/1000 | Loss: 0.00002651
Iteration 41/1000 | Loss: 0.00002651
Iteration 42/1000 | Loss: 0.00002650
Iteration 43/1000 | Loss: 0.00002650
Iteration 44/1000 | Loss: 0.00002650
Iteration 45/1000 | Loss: 0.00002650
Iteration 46/1000 | Loss: 0.00002649
Iteration 47/1000 | Loss: 0.00002649
Iteration 48/1000 | Loss: 0.00002649
Iteration 49/1000 | Loss: 0.00002648
Iteration 50/1000 | Loss: 0.00002648
Iteration 51/1000 | Loss: 0.00002648
Iteration 52/1000 | Loss: 0.00002647
Iteration 53/1000 | Loss: 0.00002647
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002646
Iteration 56/1000 | Loss: 0.00002646
Iteration 57/1000 | Loss: 0.00002645
Iteration 58/1000 | Loss: 0.00002645
Iteration 59/1000 | Loss: 0.00002644
Iteration 60/1000 | Loss: 0.00002644
Iteration 61/1000 | Loss: 0.00002644
Iteration 62/1000 | Loss: 0.00002644
Iteration 63/1000 | Loss: 0.00002644
Iteration 64/1000 | Loss: 0.00002644
Iteration 65/1000 | Loss: 0.00002644
Iteration 66/1000 | Loss: 0.00002644
Iteration 67/1000 | Loss: 0.00002644
Iteration 68/1000 | Loss: 0.00002643
Iteration 69/1000 | Loss: 0.00002643
Iteration 70/1000 | Loss: 0.00002643
Iteration 71/1000 | Loss: 0.00002643
Iteration 72/1000 | Loss: 0.00002643
Iteration 73/1000 | Loss: 0.00002643
Iteration 74/1000 | Loss: 0.00002643
Iteration 75/1000 | Loss: 0.00002643
Iteration 76/1000 | Loss: 0.00002643
Iteration 77/1000 | Loss: 0.00002642
Iteration 78/1000 | Loss: 0.00002642
Iteration 79/1000 | Loss: 0.00002642
Iteration 80/1000 | Loss: 0.00002642
Iteration 81/1000 | Loss: 0.00002642
Iteration 82/1000 | Loss: 0.00002641
Iteration 83/1000 | Loss: 0.00002641
Iteration 84/1000 | Loss: 0.00002641
Iteration 85/1000 | Loss: 0.00002641
Iteration 86/1000 | Loss: 0.00002641
Iteration 87/1000 | Loss: 0.00002641
Iteration 88/1000 | Loss: 0.00002641
Iteration 89/1000 | Loss: 0.00002640
Iteration 90/1000 | Loss: 0.00002640
Iteration 91/1000 | Loss: 0.00002640
Iteration 92/1000 | Loss: 0.00002640
Iteration 93/1000 | Loss: 0.00002640
Iteration 94/1000 | Loss: 0.00002640
Iteration 95/1000 | Loss: 0.00002639
Iteration 96/1000 | Loss: 0.00002639
Iteration 97/1000 | Loss: 0.00002639
Iteration 98/1000 | Loss: 0.00002639
Iteration 99/1000 | Loss: 0.00002639
Iteration 100/1000 | Loss: 0.00002639
Iteration 101/1000 | Loss: 0.00002639
Iteration 102/1000 | Loss: 0.00002639
Iteration 103/1000 | Loss: 0.00002638
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002638
Iteration 108/1000 | Loss: 0.00002638
Iteration 109/1000 | Loss: 0.00002638
Iteration 110/1000 | Loss: 0.00002638
Iteration 111/1000 | Loss: 0.00002637
Iteration 112/1000 | Loss: 0.00002637
Iteration 113/1000 | Loss: 0.00002637
Iteration 114/1000 | Loss: 0.00002637
Iteration 115/1000 | Loss: 0.00002637
Iteration 116/1000 | Loss: 0.00002637
Iteration 117/1000 | Loss: 0.00002637
Iteration 118/1000 | Loss: 0.00002637
Iteration 119/1000 | Loss: 0.00002637
Iteration 120/1000 | Loss: 0.00002637
Iteration 121/1000 | Loss: 0.00002636
Iteration 122/1000 | Loss: 0.00002636
Iteration 123/1000 | Loss: 0.00002636
Iteration 124/1000 | Loss: 0.00002636
Iteration 125/1000 | Loss: 0.00002636
Iteration 126/1000 | Loss: 0.00002636
Iteration 127/1000 | Loss: 0.00002636
Iteration 128/1000 | Loss: 0.00002635
Iteration 129/1000 | Loss: 0.00002635
Iteration 130/1000 | Loss: 0.00002635
Iteration 131/1000 | Loss: 0.00002635
Iteration 132/1000 | Loss: 0.00002635
Iteration 133/1000 | Loss: 0.00002635
Iteration 134/1000 | Loss: 0.00002635
Iteration 135/1000 | Loss: 0.00002635
Iteration 136/1000 | Loss: 0.00002635
Iteration 137/1000 | Loss: 0.00002635
Iteration 138/1000 | Loss: 0.00002635
Iteration 139/1000 | Loss: 0.00002634
Iteration 140/1000 | Loss: 0.00002634
Iteration 141/1000 | Loss: 0.00002634
Iteration 142/1000 | Loss: 0.00002634
Iteration 143/1000 | Loss: 0.00002633
Iteration 144/1000 | Loss: 0.00002633
Iteration 145/1000 | Loss: 0.00002633
Iteration 146/1000 | Loss: 0.00002633
Iteration 147/1000 | Loss: 0.00002632
Iteration 148/1000 | Loss: 0.00002632
Iteration 149/1000 | Loss: 0.00002632
Iteration 150/1000 | Loss: 0.00002632
Iteration 151/1000 | Loss: 0.00002632
Iteration 152/1000 | Loss: 0.00002632
Iteration 153/1000 | Loss: 0.00002632
Iteration 154/1000 | Loss: 0.00002632
Iteration 155/1000 | Loss: 0.00002632
Iteration 156/1000 | Loss: 0.00002631
Iteration 157/1000 | Loss: 0.00002631
Iteration 158/1000 | Loss: 0.00002631
Iteration 159/1000 | Loss: 0.00002631
Iteration 160/1000 | Loss: 0.00002631
Iteration 161/1000 | Loss: 0.00002631
Iteration 162/1000 | Loss: 0.00002631
Iteration 163/1000 | Loss: 0.00002631
Iteration 164/1000 | Loss: 0.00002631
Iteration 165/1000 | Loss: 0.00002631
Iteration 166/1000 | Loss: 0.00002631
Iteration 167/1000 | Loss: 0.00002631
Iteration 168/1000 | Loss: 0.00002630
Iteration 169/1000 | Loss: 0.00002630
Iteration 170/1000 | Loss: 0.00002630
Iteration 171/1000 | Loss: 0.00002630
Iteration 172/1000 | Loss: 0.00002630
Iteration 173/1000 | Loss: 0.00002630
Iteration 174/1000 | Loss: 0.00002630
Iteration 175/1000 | Loss: 0.00002630
Iteration 176/1000 | Loss: 0.00002630
Iteration 177/1000 | Loss: 0.00002630
Iteration 178/1000 | Loss: 0.00002630
Iteration 179/1000 | Loss: 0.00002630
Iteration 180/1000 | Loss: 0.00002630
Iteration 181/1000 | Loss: 0.00002630
Iteration 182/1000 | Loss: 0.00002630
Iteration 183/1000 | Loss: 0.00002629
Iteration 184/1000 | Loss: 0.00002629
Iteration 185/1000 | Loss: 0.00002629
Iteration 186/1000 | Loss: 0.00002629
Iteration 187/1000 | Loss: 0.00002629
Iteration 188/1000 | Loss: 0.00002629
Iteration 189/1000 | Loss: 0.00002629
Iteration 190/1000 | Loss: 0.00002629
Iteration 191/1000 | Loss: 0.00002629
Iteration 192/1000 | Loss: 0.00002629
Iteration 193/1000 | Loss: 0.00002629
Iteration 194/1000 | Loss: 0.00002629
Iteration 195/1000 | Loss: 0.00002629
Iteration 196/1000 | Loss: 0.00002629
Iteration 197/1000 | Loss: 0.00002629
Iteration 198/1000 | Loss: 0.00002629
Iteration 199/1000 | Loss: 0.00002629
Iteration 200/1000 | Loss: 0.00002629
Iteration 201/1000 | Loss: 0.00002629
Iteration 202/1000 | Loss: 0.00002629
Iteration 203/1000 | Loss: 0.00002629
Iteration 204/1000 | Loss: 0.00002629
Iteration 205/1000 | Loss: 0.00002629
Iteration 206/1000 | Loss: 0.00002629
Iteration 207/1000 | Loss: 0.00002629
Iteration 208/1000 | Loss: 0.00002629
Iteration 209/1000 | Loss: 0.00002629
Iteration 210/1000 | Loss: 0.00002629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.628792753966991e-05, 2.628792753966991e-05, 2.628792753966991e-05, 2.628792753966991e-05, 2.628792753966991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.628792753966991e-05

Optimization complete. Final v2v error: 4.392437934875488 mm

Highest mean error: 5.396759986877441 mm for frame 67

Lowest mean error: 3.871873140335083 mm for frame 8

Saving results

Total time: 44.436243772506714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647861
Iteration 2/25 | Loss: 0.00176516
Iteration 3/25 | Loss: 0.00144297
Iteration 4/25 | Loss: 0.00141418
Iteration 5/25 | Loss: 0.00140982
Iteration 6/25 | Loss: 0.00140877
Iteration 7/25 | Loss: 0.00140866
Iteration 8/25 | Loss: 0.00140866
Iteration 9/25 | Loss: 0.00140866
Iteration 10/25 | Loss: 0.00140866
Iteration 11/25 | Loss: 0.00140866
Iteration 12/25 | Loss: 0.00140866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014086627634242177, 0.0014086627634242177, 0.0014086627634242177, 0.0014086627634242177, 0.0014086627634242177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014086627634242177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17693233
Iteration 2/25 | Loss: 0.00091154
Iteration 3/25 | Loss: 0.00091154
Iteration 4/25 | Loss: 0.00091154
Iteration 5/25 | Loss: 0.00091154
Iteration 6/25 | Loss: 0.00091154
Iteration 7/25 | Loss: 0.00091154
Iteration 8/25 | Loss: 0.00091154
Iteration 9/25 | Loss: 0.00091154
Iteration 10/25 | Loss: 0.00091154
Iteration 11/25 | Loss: 0.00091154
Iteration 12/25 | Loss: 0.00091154
Iteration 13/25 | Loss: 0.00091154
Iteration 14/25 | Loss: 0.00091154
Iteration 15/25 | Loss: 0.00091154
Iteration 16/25 | Loss: 0.00091154
Iteration 17/25 | Loss: 0.00091154
Iteration 18/25 | Loss: 0.00091154
Iteration 19/25 | Loss: 0.00091154
Iteration 20/25 | Loss: 0.00091154
Iteration 21/25 | Loss: 0.00091154
Iteration 22/25 | Loss: 0.00091154
Iteration 23/25 | Loss: 0.00091154
Iteration 24/25 | Loss: 0.00091154
Iteration 25/25 | Loss: 0.00091154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091154
Iteration 2/1000 | Loss: 0.00006672
Iteration 3/1000 | Loss: 0.00004302
Iteration 4/1000 | Loss: 0.00003537
Iteration 5/1000 | Loss: 0.00003126
Iteration 6/1000 | Loss: 0.00002921
Iteration 7/1000 | Loss: 0.00002810
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002697
Iteration 10/1000 | Loss: 0.00002666
Iteration 11/1000 | Loss: 0.00002628
Iteration 12/1000 | Loss: 0.00002602
Iteration 13/1000 | Loss: 0.00002587
Iteration 14/1000 | Loss: 0.00002578
Iteration 15/1000 | Loss: 0.00002578
Iteration 16/1000 | Loss: 0.00002578
Iteration 17/1000 | Loss: 0.00002578
Iteration 18/1000 | Loss: 0.00002578
Iteration 19/1000 | Loss: 0.00002576
Iteration 20/1000 | Loss: 0.00002575
Iteration 21/1000 | Loss: 0.00002574
Iteration 22/1000 | Loss: 0.00002574
Iteration 23/1000 | Loss: 0.00002573
Iteration 24/1000 | Loss: 0.00002573
Iteration 25/1000 | Loss: 0.00002573
Iteration 26/1000 | Loss: 0.00002573
Iteration 27/1000 | Loss: 0.00002573
Iteration 28/1000 | Loss: 0.00002573
Iteration 29/1000 | Loss: 0.00002573
Iteration 30/1000 | Loss: 0.00002573
Iteration 31/1000 | Loss: 0.00002573
Iteration 32/1000 | Loss: 0.00002573
Iteration 33/1000 | Loss: 0.00002573
Iteration 34/1000 | Loss: 0.00002573
Iteration 35/1000 | Loss: 0.00002572
Iteration 36/1000 | Loss: 0.00002572
Iteration 37/1000 | Loss: 0.00002571
Iteration 38/1000 | Loss: 0.00002570
Iteration 39/1000 | Loss: 0.00002570
Iteration 40/1000 | Loss: 0.00002567
Iteration 41/1000 | Loss: 0.00002567
Iteration 42/1000 | Loss: 0.00002567
Iteration 43/1000 | Loss: 0.00002567
Iteration 44/1000 | Loss: 0.00002566
Iteration 45/1000 | Loss: 0.00002566
Iteration 46/1000 | Loss: 0.00002566
Iteration 47/1000 | Loss: 0.00002566
Iteration 48/1000 | Loss: 0.00002565
Iteration 49/1000 | Loss: 0.00002564
Iteration 50/1000 | Loss: 0.00002564
Iteration 51/1000 | Loss: 0.00002563
Iteration 52/1000 | Loss: 0.00002563
Iteration 53/1000 | Loss: 0.00002563
Iteration 54/1000 | Loss: 0.00002563
Iteration 55/1000 | Loss: 0.00002563
Iteration 56/1000 | Loss: 0.00002563
Iteration 57/1000 | Loss: 0.00002563
Iteration 58/1000 | Loss: 0.00002563
Iteration 59/1000 | Loss: 0.00002563
Iteration 60/1000 | Loss: 0.00002562
Iteration 61/1000 | Loss: 0.00002562
Iteration 62/1000 | Loss: 0.00002562
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002562
Iteration 65/1000 | Loss: 0.00002562
Iteration 66/1000 | Loss: 0.00002562
Iteration 67/1000 | Loss: 0.00002562
Iteration 68/1000 | Loss: 0.00002561
Iteration 69/1000 | Loss: 0.00002561
Iteration 70/1000 | Loss: 0.00002561
Iteration 71/1000 | Loss: 0.00002561
Iteration 72/1000 | Loss: 0.00002561
Iteration 73/1000 | Loss: 0.00002561
Iteration 74/1000 | Loss: 0.00002561
Iteration 75/1000 | Loss: 0.00002560
Iteration 76/1000 | Loss: 0.00002560
Iteration 77/1000 | Loss: 0.00002560
Iteration 78/1000 | Loss: 0.00002560
Iteration 79/1000 | Loss: 0.00002560
Iteration 80/1000 | Loss: 0.00002560
Iteration 81/1000 | Loss: 0.00002560
Iteration 82/1000 | Loss: 0.00002559
Iteration 83/1000 | Loss: 0.00002559
Iteration 84/1000 | Loss: 0.00002559
Iteration 85/1000 | Loss: 0.00002559
Iteration 86/1000 | Loss: 0.00002558
Iteration 87/1000 | Loss: 0.00002558
Iteration 88/1000 | Loss: 0.00002558
Iteration 89/1000 | Loss: 0.00002557
Iteration 90/1000 | Loss: 0.00002557
Iteration 91/1000 | Loss: 0.00002557
Iteration 92/1000 | Loss: 0.00002556
Iteration 93/1000 | Loss: 0.00002556
Iteration 94/1000 | Loss: 0.00002555
Iteration 95/1000 | Loss: 0.00002555
Iteration 96/1000 | Loss: 0.00002554
Iteration 97/1000 | Loss: 0.00002554
Iteration 98/1000 | Loss: 0.00002553
Iteration 99/1000 | Loss: 0.00002553
Iteration 100/1000 | Loss: 0.00002552
Iteration 101/1000 | Loss: 0.00002552
Iteration 102/1000 | Loss: 0.00002552
Iteration 103/1000 | Loss: 0.00002552
Iteration 104/1000 | Loss: 0.00002552
Iteration 105/1000 | Loss: 0.00002551
Iteration 106/1000 | Loss: 0.00002551
Iteration 107/1000 | Loss: 0.00002550
Iteration 108/1000 | Loss: 0.00002550
Iteration 109/1000 | Loss: 0.00002550
Iteration 110/1000 | Loss: 0.00002549
Iteration 111/1000 | Loss: 0.00002549
Iteration 112/1000 | Loss: 0.00002548
Iteration 113/1000 | Loss: 0.00002548
Iteration 114/1000 | Loss: 0.00002548
Iteration 115/1000 | Loss: 0.00002547
Iteration 116/1000 | Loss: 0.00002547
Iteration 117/1000 | Loss: 0.00002547
Iteration 118/1000 | Loss: 0.00002547
Iteration 119/1000 | Loss: 0.00002547
Iteration 120/1000 | Loss: 0.00002547
Iteration 121/1000 | Loss: 0.00002547
Iteration 122/1000 | Loss: 0.00002547
Iteration 123/1000 | Loss: 0.00002546
Iteration 124/1000 | Loss: 0.00002546
Iteration 125/1000 | Loss: 0.00002546
Iteration 126/1000 | Loss: 0.00002546
Iteration 127/1000 | Loss: 0.00002545
Iteration 128/1000 | Loss: 0.00002545
Iteration 129/1000 | Loss: 0.00002545
Iteration 130/1000 | Loss: 0.00002544
Iteration 131/1000 | Loss: 0.00002544
Iteration 132/1000 | Loss: 0.00002544
Iteration 133/1000 | Loss: 0.00002544
Iteration 134/1000 | Loss: 0.00002544
Iteration 135/1000 | Loss: 0.00002544
Iteration 136/1000 | Loss: 0.00002543
Iteration 137/1000 | Loss: 0.00002543
Iteration 138/1000 | Loss: 0.00002543
Iteration 139/1000 | Loss: 0.00002543
Iteration 140/1000 | Loss: 0.00002543
Iteration 141/1000 | Loss: 0.00002543
Iteration 142/1000 | Loss: 0.00002543
Iteration 143/1000 | Loss: 0.00002543
Iteration 144/1000 | Loss: 0.00002543
Iteration 145/1000 | Loss: 0.00002542
Iteration 146/1000 | Loss: 0.00002542
Iteration 147/1000 | Loss: 0.00002542
Iteration 148/1000 | Loss: 0.00002541
Iteration 149/1000 | Loss: 0.00002541
Iteration 150/1000 | Loss: 0.00002541
Iteration 151/1000 | Loss: 0.00002541
Iteration 152/1000 | Loss: 0.00002541
Iteration 153/1000 | Loss: 0.00002541
Iteration 154/1000 | Loss: 0.00002541
Iteration 155/1000 | Loss: 0.00002541
Iteration 156/1000 | Loss: 0.00002541
Iteration 157/1000 | Loss: 0.00002541
Iteration 158/1000 | Loss: 0.00002540
Iteration 159/1000 | Loss: 0.00002540
Iteration 160/1000 | Loss: 0.00002540
Iteration 161/1000 | Loss: 0.00002540
Iteration 162/1000 | Loss: 0.00002539
Iteration 163/1000 | Loss: 0.00002539
Iteration 164/1000 | Loss: 0.00002539
Iteration 165/1000 | Loss: 0.00002539
Iteration 166/1000 | Loss: 0.00002539
Iteration 167/1000 | Loss: 0.00002539
Iteration 168/1000 | Loss: 0.00002538
Iteration 169/1000 | Loss: 0.00002538
Iteration 170/1000 | Loss: 0.00002538
Iteration 171/1000 | Loss: 0.00002538
Iteration 172/1000 | Loss: 0.00002538
Iteration 173/1000 | Loss: 0.00002538
Iteration 174/1000 | Loss: 0.00002538
Iteration 175/1000 | Loss: 0.00002538
Iteration 176/1000 | Loss: 0.00002537
Iteration 177/1000 | Loss: 0.00002537
Iteration 178/1000 | Loss: 0.00002537
Iteration 179/1000 | Loss: 0.00002536
Iteration 180/1000 | Loss: 0.00002536
Iteration 181/1000 | Loss: 0.00002536
Iteration 182/1000 | Loss: 0.00002536
Iteration 183/1000 | Loss: 0.00002536
Iteration 184/1000 | Loss: 0.00002536
Iteration 185/1000 | Loss: 0.00002535
Iteration 186/1000 | Loss: 0.00002535
Iteration 187/1000 | Loss: 0.00002535
Iteration 188/1000 | Loss: 0.00002535
Iteration 189/1000 | Loss: 0.00002535
Iteration 190/1000 | Loss: 0.00002535
Iteration 191/1000 | Loss: 0.00002535
Iteration 192/1000 | Loss: 0.00002535
Iteration 193/1000 | Loss: 0.00002535
Iteration 194/1000 | Loss: 0.00002535
Iteration 195/1000 | Loss: 0.00002534
Iteration 196/1000 | Loss: 0.00002534
Iteration 197/1000 | Loss: 0.00002534
Iteration 198/1000 | Loss: 0.00002534
Iteration 199/1000 | Loss: 0.00002534
Iteration 200/1000 | Loss: 0.00002534
Iteration 201/1000 | Loss: 0.00002534
Iteration 202/1000 | Loss: 0.00002534
Iteration 203/1000 | Loss: 0.00002534
Iteration 204/1000 | Loss: 0.00002534
Iteration 205/1000 | Loss: 0.00002534
Iteration 206/1000 | Loss: 0.00002534
Iteration 207/1000 | Loss: 0.00002534
Iteration 208/1000 | Loss: 0.00002534
Iteration 209/1000 | Loss: 0.00002533
Iteration 210/1000 | Loss: 0.00002533
Iteration 211/1000 | Loss: 0.00002533
Iteration 212/1000 | Loss: 0.00002533
Iteration 213/1000 | Loss: 0.00002533
Iteration 214/1000 | Loss: 0.00002533
Iteration 215/1000 | Loss: 0.00002532
Iteration 216/1000 | Loss: 0.00002532
Iteration 217/1000 | Loss: 0.00002532
Iteration 218/1000 | Loss: 0.00002532
Iteration 219/1000 | Loss: 0.00002532
Iteration 220/1000 | Loss: 0.00002532
Iteration 221/1000 | Loss: 0.00002532
Iteration 222/1000 | Loss: 0.00002532
Iteration 223/1000 | Loss: 0.00002532
Iteration 224/1000 | Loss: 0.00002531
Iteration 225/1000 | Loss: 0.00002531
Iteration 226/1000 | Loss: 0.00002531
Iteration 227/1000 | Loss: 0.00002531
Iteration 228/1000 | Loss: 0.00002531
Iteration 229/1000 | Loss: 0.00002531
Iteration 230/1000 | Loss: 0.00002531
Iteration 231/1000 | Loss: 0.00002531
Iteration 232/1000 | Loss: 0.00002531
Iteration 233/1000 | Loss: 0.00002530
Iteration 234/1000 | Loss: 0.00002530
Iteration 235/1000 | Loss: 0.00002530
Iteration 236/1000 | Loss: 0.00002530
Iteration 237/1000 | Loss: 0.00002530
Iteration 238/1000 | Loss: 0.00002530
Iteration 239/1000 | Loss: 0.00002529
Iteration 240/1000 | Loss: 0.00002529
Iteration 241/1000 | Loss: 0.00002529
Iteration 242/1000 | Loss: 0.00002529
Iteration 243/1000 | Loss: 0.00002529
Iteration 244/1000 | Loss: 0.00002529
Iteration 245/1000 | Loss: 0.00002529
Iteration 246/1000 | Loss: 0.00002529
Iteration 247/1000 | Loss: 0.00002529
Iteration 248/1000 | Loss: 0.00002529
Iteration 249/1000 | Loss: 0.00002529
Iteration 250/1000 | Loss: 0.00002529
Iteration 251/1000 | Loss: 0.00002529
Iteration 252/1000 | Loss: 0.00002529
Iteration 253/1000 | Loss: 0.00002529
Iteration 254/1000 | Loss: 0.00002529
Iteration 255/1000 | Loss: 0.00002529
Iteration 256/1000 | Loss: 0.00002529
Iteration 257/1000 | Loss: 0.00002529
Iteration 258/1000 | Loss: 0.00002529
Iteration 259/1000 | Loss: 0.00002529
Iteration 260/1000 | Loss: 0.00002529
Iteration 261/1000 | Loss: 0.00002529
Iteration 262/1000 | Loss: 0.00002529
Iteration 263/1000 | Loss: 0.00002529
Iteration 264/1000 | Loss: 0.00002529
Iteration 265/1000 | Loss: 0.00002529
Iteration 266/1000 | Loss: 0.00002529
Iteration 267/1000 | Loss: 0.00002529
Iteration 268/1000 | Loss: 0.00002529
Iteration 269/1000 | Loss: 0.00002529
Iteration 270/1000 | Loss: 0.00002529
Iteration 271/1000 | Loss: 0.00002529
Iteration 272/1000 | Loss: 0.00002529
Iteration 273/1000 | Loss: 0.00002529
Iteration 274/1000 | Loss: 0.00002529
Iteration 275/1000 | Loss: 0.00002529
Iteration 276/1000 | Loss: 0.00002529
Iteration 277/1000 | Loss: 0.00002529
Iteration 278/1000 | Loss: 0.00002529
Iteration 279/1000 | Loss: 0.00002529
Iteration 280/1000 | Loss: 0.00002529
Iteration 281/1000 | Loss: 0.00002529
Iteration 282/1000 | Loss: 0.00002529
Iteration 283/1000 | Loss: 0.00002529
Iteration 284/1000 | Loss: 0.00002529
Iteration 285/1000 | Loss: 0.00002529
Iteration 286/1000 | Loss: 0.00002529
Iteration 287/1000 | Loss: 0.00002529
Iteration 288/1000 | Loss: 0.00002529
Iteration 289/1000 | Loss: 0.00002529
Iteration 290/1000 | Loss: 0.00002529
Iteration 291/1000 | Loss: 0.00002529
Iteration 292/1000 | Loss: 0.00002529
Iteration 293/1000 | Loss: 0.00002529
Iteration 294/1000 | Loss: 0.00002529
Iteration 295/1000 | Loss: 0.00002529
Iteration 296/1000 | Loss: 0.00002529
Iteration 297/1000 | Loss: 0.00002529
Iteration 298/1000 | Loss: 0.00002529
Iteration 299/1000 | Loss: 0.00002529
Iteration 300/1000 | Loss: 0.00002529
Iteration 301/1000 | Loss: 0.00002529
Iteration 302/1000 | Loss: 0.00002529
Iteration 303/1000 | Loss: 0.00002529
Iteration 304/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [2.5288802135037258e-05, 2.5288802135037258e-05, 2.5288802135037258e-05, 2.5288802135037258e-05, 2.5288802135037258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5288802135037258e-05

Optimization complete. Final v2v error: 4.132145881652832 mm

Highest mean error: 6.572815895080566 mm for frame 58

Lowest mean error: 3.5407872200012207 mm for frame 111

Saving results

Total time: 47.647786378860474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421082
Iteration 2/25 | Loss: 0.00159770
Iteration 3/25 | Loss: 0.00144127
Iteration 4/25 | Loss: 0.00142157
Iteration 5/25 | Loss: 0.00141583
Iteration 6/25 | Loss: 0.00141528
Iteration 7/25 | Loss: 0.00141528
Iteration 8/25 | Loss: 0.00141528
Iteration 9/25 | Loss: 0.00141528
Iteration 10/25 | Loss: 0.00141528
Iteration 11/25 | Loss: 0.00141528
Iteration 12/25 | Loss: 0.00141528
Iteration 13/25 | Loss: 0.00141528
Iteration 14/25 | Loss: 0.00141528
Iteration 15/25 | Loss: 0.00141528
Iteration 16/25 | Loss: 0.00141528
Iteration 17/25 | Loss: 0.00141528
Iteration 18/25 | Loss: 0.00141528
Iteration 19/25 | Loss: 0.00141528
Iteration 20/25 | Loss: 0.00141528
Iteration 21/25 | Loss: 0.00141528
Iteration 22/25 | Loss: 0.00141528
Iteration 23/25 | Loss: 0.00141528
Iteration 24/25 | Loss: 0.00141528
Iteration 25/25 | Loss: 0.00141528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76638269
Iteration 2/25 | Loss: 0.00089751
Iteration 3/25 | Loss: 0.00089751
Iteration 4/25 | Loss: 0.00089751
Iteration 5/25 | Loss: 0.00089751
Iteration 6/25 | Loss: 0.00089751
Iteration 7/25 | Loss: 0.00089751
Iteration 8/25 | Loss: 0.00089751
Iteration 9/25 | Loss: 0.00089751
Iteration 10/25 | Loss: 0.00089751
Iteration 11/25 | Loss: 0.00089751
Iteration 12/25 | Loss: 0.00089751
Iteration 13/25 | Loss: 0.00089751
Iteration 14/25 | Loss: 0.00089751
Iteration 15/25 | Loss: 0.00089751
Iteration 16/25 | Loss: 0.00089751
Iteration 17/25 | Loss: 0.00089751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008975107339210808, 0.0008975107339210808, 0.0008975107339210808, 0.0008975107339210808, 0.0008975107339210808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008975107339210808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089751
Iteration 2/1000 | Loss: 0.00005726
Iteration 3/1000 | Loss: 0.00003669
Iteration 4/1000 | Loss: 0.00003146
Iteration 5/1000 | Loss: 0.00002826
Iteration 6/1000 | Loss: 0.00002689
Iteration 7/1000 | Loss: 0.00002613
Iteration 8/1000 | Loss: 0.00002555
Iteration 9/1000 | Loss: 0.00002522
Iteration 10/1000 | Loss: 0.00002500
Iteration 11/1000 | Loss: 0.00002487
Iteration 12/1000 | Loss: 0.00002487
Iteration 13/1000 | Loss: 0.00002483
Iteration 14/1000 | Loss: 0.00002482
Iteration 15/1000 | Loss: 0.00002480
Iteration 16/1000 | Loss: 0.00002479
Iteration 17/1000 | Loss: 0.00002479
Iteration 18/1000 | Loss: 0.00002477
Iteration 19/1000 | Loss: 0.00002476
Iteration 20/1000 | Loss: 0.00002475
Iteration 21/1000 | Loss: 0.00002475
Iteration 22/1000 | Loss: 0.00002475
Iteration 23/1000 | Loss: 0.00002475
Iteration 24/1000 | Loss: 0.00002475
Iteration 25/1000 | Loss: 0.00002475
Iteration 26/1000 | Loss: 0.00002475
Iteration 27/1000 | Loss: 0.00002475
Iteration 28/1000 | Loss: 0.00002475
Iteration 29/1000 | Loss: 0.00002474
Iteration 30/1000 | Loss: 0.00002474
Iteration 31/1000 | Loss: 0.00002474
Iteration 32/1000 | Loss: 0.00002473
Iteration 33/1000 | Loss: 0.00002472
Iteration 34/1000 | Loss: 0.00002472
Iteration 35/1000 | Loss: 0.00002472
Iteration 36/1000 | Loss: 0.00002471
Iteration 37/1000 | Loss: 0.00002471
Iteration 38/1000 | Loss: 0.00002468
Iteration 39/1000 | Loss: 0.00002462
Iteration 40/1000 | Loss: 0.00002461
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002456
Iteration 43/1000 | Loss: 0.00002448
Iteration 44/1000 | Loss: 0.00002443
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002436
Iteration 47/1000 | Loss: 0.00002434
Iteration 48/1000 | Loss: 0.00002434
Iteration 49/1000 | Loss: 0.00002434
Iteration 50/1000 | Loss: 0.00002432
Iteration 51/1000 | Loss: 0.00002432
Iteration 52/1000 | Loss: 0.00002429
Iteration 53/1000 | Loss: 0.00002429
Iteration 54/1000 | Loss: 0.00002429
Iteration 55/1000 | Loss: 0.00002428
Iteration 56/1000 | Loss: 0.00002428
Iteration 57/1000 | Loss: 0.00002428
Iteration 58/1000 | Loss: 0.00002427
Iteration 59/1000 | Loss: 0.00002427
Iteration 60/1000 | Loss: 0.00002426
Iteration 61/1000 | Loss: 0.00002426
Iteration 62/1000 | Loss: 0.00002425
Iteration 63/1000 | Loss: 0.00002425
Iteration 64/1000 | Loss: 0.00002423
Iteration 65/1000 | Loss: 0.00002422
Iteration 66/1000 | Loss: 0.00002421
Iteration 67/1000 | Loss: 0.00002421
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002419
Iteration 70/1000 | Loss: 0.00002419
Iteration 71/1000 | Loss: 0.00002419
Iteration 72/1000 | Loss: 0.00002419
Iteration 73/1000 | Loss: 0.00002419
Iteration 74/1000 | Loss: 0.00002418
Iteration 75/1000 | Loss: 0.00002418
Iteration 76/1000 | Loss: 0.00002418
Iteration 77/1000 | Loss: 0.00002413
Iteration 78/1000 | Loss: 0.00002413
Iteration 79/1000 | Loss: 0.00002413
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002405
Iteration 82/1000 | Loss: 0.00002404
Iteration 83/1000 | Loss: 0.00002400
Iteration 84/1000 | Loss: 0.00002394
Iteration 85/1000 | Loss: 0.00002390
Iteration 86/1000 | Loss: 0.00002390
Iteration 87/1000 | Loss: 0.00002389
Iteration 88/1000 | Loss: 0.00002389
Iteration 89/1000 | Loss: 0.00002388
Iteration 90/1000 | Loss: 0.00002388
Iteration 91/1000 | Loss: 0.00002388
Iteration 92/1000 | Loss: 0.00002388
Iteration 93/1000 | Loss: 0.00002388
Iteration 94/1000 | Loss: 0.00002388
Iteration 95/1000 | Loss: 0.00002388
Iteration 96/1000 | Loss: 0.00002388
Iteration 97/1000 | Loss: 0.00002388
Iteration 98/1000 | Loss: 0.00002388
Iteration 99/1000 | Loss: 0.00002387
Iteration 100/1000 | Loss: 0.00002387
Iteration 101/1000 | Loss: 0.00002387
Iteration 102/1000 | Loss: 0.00002387
Iteration 103/1000 | Loss: 0.00002386
Iteration 104/1000 | Loss: 0.00002386
Iteration 105/1000 | Loss: 0.00002386
Iteration 106/1000 | Loss: 0.00002386
Iteration 107/1000 | Loss: 0.00002386
Iteration 108/1000 | Loss: 0.00002385
Iteration 109/1000 | Loss: 0.00002385
Iteration 110/1000 | Loss: 0.00002385
Iteration 111/1000 | Loss: 0.00002385
Iteration 112/1000 | Loss: 0.00002384
Iteration 113/1000 | Loss: 0.00002384
Iteration 114/1000 | Loss: 0.00002384
Iteration 115/1000 | Loss: 0.00002384
Iteration 116/1000 | Loss: 0.00002384
Iteration 117/1000 | Loss: 0.00002384
Iteration 118/1000 | Loss: 0.00002384
Iteration 119/1000 | Loss: 0.00002384
Iteration 120/1000 | Loss: 0.00002384
Iteration 121/1000 | Loss: 0.00002383
Iteration 122/1000 | Loss: 0.00002383
Iteration 123/1000 | Loss: 0.00002383
Iteration 124/1000 | Loss: 0.00002383
Iteration 125/1000 | Loss: 0.00002383
Iteration 126/1000 | Loss: 0.00002383
Iteration 127/1000 | Loss: 0.00002383
Iteration 128/1000 | Loss: 0.00002383
Iteration 129/1000 | Loss: 0.00002382
Iteration 130/1000 | Loss: 0.00002382
Iteration 131/1000 | Loss: 0.00002382
Iteration 132/1000 | Loss: 0.00002382
Iteration 133/1000 | Loss: 0.00002382
Iteration 134/1000 | Loss: 0.00002382
Iteration 135/1000 | Loss: 0.00002382
Iteration 136/1000 | Loss: 0.00002381
Iteration 137/1000 | Loss: 0.00002381
Iteration 138/1000 | Loss: 0.00002381
Iteration 139/1000 | Loss: 0.00002381
Iteration 140/1000 | Loss: 0.00002380
Iteration 141/1000 | Loss: 0.00002380
Iteration 142/1000 | Loss: 0.00002380
Iteration 143/1000 | Loss: 0.00002380
Iteration 144/1000 | Loss: 0.00002380
Iteration 145/1000 | Loss: 0.00002380
Iteration 146/1000 | Loss: 0.00002379
Iteration 147/1000 | Loss: 0.00002379
Iteration 148/1000 | Loss: 0.00002379
Iteration 149/1000 | Loss: 0.00002378
Iteration 150/1000 | Loss: 0.00002378
Iteration 151/1000 | Loss: 0.00002378
Iteration 152/1000 | Loss: 0.00002377
Iteration 153/1000 | Loss: 0.00002377
Iteration 154/1000 | Loss: 0.00002377
Iteration 155/1000 | Loss: 0.00002377
Iteration 156/1000 | Loss: 0.00002377
Iteration 157/1000 | Loss: 0.00002376
Iteration 158/1000 | Loss: 0.00002376
Iteration 159/1000 | Loss: 0.00002376
Iteration 160/1000 | Loss: 0.00002376
Iteration 161/1000 | Loss: 0.00002375
Iteration 162/1000 | Loss: 0.00002375
Iteration 163/1000 | Loss: 0.00002375
Iteration 164/1000 | Loss: 0.00002375
Iteration 165/1000 | Loss: 0.00002375
Iteration 166/1000 | Loss: 0.00002375
Iteration 167/1000 | Loss: 0.00002375
Iteration 168/1000 | Loss: 0.00002375
Iteration 169/1000 | Loss: 0.00002375
Iteration 170/1000 | Loss: 0.00002375
Iteration 171/1000 | Loss: 0.00002375
Iteration 172/1000 | Loss: 0.00002374
Iteration 173/1000 | Loss: 0.00002374
Iteration 174/1000 | Loss: 0.00002374
Iteration 175/1000 | Loss: 0.00002374
Iteration 176/1000 | Loss: 0.00002374
Iteration 177/1000 | Loss: 0.00002374
Iteration 178/1000 | Loss: 0.00002374
Iteration 179/1000 | Loss: 0.00002374
Iteration 180/1000 | Loss: 0.00002374
Iteration 181/1000 | Loss: 0.00002373
Iteration 182/1000 | Loss: 0.00002373
Iteration 183/1000 | Loss: 0.00002373
Iteration 184/1000 | Loss: 0.00002373
Iteration 185/1000 | Loss: 0.00002373
Iteration 186/1000 | Loss: 0.00002373
Iteration 187/1000 | Loss: 0.00002373
Iteration 188/1000 | Loss: 0.00002373
Iteration 189/1000 | Loss: 0.00002373
Iteration 190/1000 | Loss: 0.00002373
Iteration 191/1000 | Loss: 0.00002373
Iteration 192/1000 | Loss: 0.00002373
Iteration 193/1000 | Loss: 0.00002373
Iteration 194/1000 | Loss: 0.00002373
Iteration 195/1000 | Loss: 0.00002373
Iteration 196/1000 | Loss: 0.00002373
Iteration 197/1000 | Loss: 0.00002372
Iteration 198/1000 | Loss: 0.00002372
Iteration 199/1000 | Loss: 0.00002372
Iteration 200/1000 | Loss: 0.00002372
Iteration 201/1000 | Loss: 0.00002372
Iteration 202/1000 | Loss: 0.00002372
Iteration 203/1000 | Loss: 0.00002372
Iteration 204/1000 | Loss: 0.00002372
Iteration 205/1000 | Loss: 0.00002372
Iteration 206/1000 | Loss: 0.00002372
Iteration 207/1000 | Loss: 0.00002372
Iteration 208/1000 | Loss: 0.00002371
Iteration 209/1000 | Loss: 0.00002371
Iteration 210/1000 | Loss: 0.00002371
Iteration 211/1000 | Loss: 0.00002371
Iteration 212/1000 | Loss: 0.00002371
Iteration 213/1000 | Loss: 0.00002371
Iteration 214/1000 | Loss: 0.00002371
Iteration 215/1000 | Loss: 0.00002371
Iteration 216/1000 | Loss: 0.00002371
Iteration 217/1000 | Loss: 0.00002371
Iteration 218/1000 | Loss: 0.00002371
Iteration 219/1000 | Loss: 0.00002371
Iteration 220/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.3709508241154253e-05, 2.3709508241154253e-05, 2.3709508241154253e-05, 2.3709508241154253e-05, 2.3709508241154253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3709508241154253e-05

Optimization complete. Final v2v error: 4.175716400146484 mm

Highest mean error: 4.408905506134033 mm for frame 192

Lowest mean error: 4.095338344573975 mm for frame 156

Saving results

Total time: 56.19530177116394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00308145
Iteration 2/25 | Loss: 0.00155180
Iteration 3/25 | Loss: 0.00142043
Iteration 4/25 | Loss: 0.00140867
Iteration 5/25 | Loss: 0.00140495
Iteration 6/25 | Loss: 0.00140386
Iteration 7/25 | Loss: 0.00140336
Iteration 8/25 | Loss: 0.00140336
Iteration 9/25 | Loss: 0.00140336
Iteration 10/25 | Loss: 0.00140336
Iteration 11/25 | Loss: 0.00140336
Iteration 12/25 | Loss: 0.00140336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014033588813617826, 0.0014033588813617826, 0.0014033588813617826, 0.0014033588813617826, 0.0014033588813617826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014033588813617826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27722120
Iteration 2/25 | Loss: 0.00132927
Iteration 3/25 | Loss: 0.00132927
Iteration 4/25 | Loss: 0.00132926
Iteration 5/25 | Loss: 0.00132926
Iteration 6/25 | Loss: 0.00132926
Iteration 7/25 | Loss: 0.00132926
Iteration 8/25 | Loss: 0.00132926
Iteration 9/25 | Loss: 0.00132926
Iteration 10/25 | Loss: 0.00132926
Iteration 11/25 | Loss: 0.00132926
Iteration 12/25 | Loss: 0.00132926
Iteration 13/25 | Loss: 0.00132926
Iteration 14/25 | Loss: 0.00132926
Iteration 15/25 | Loss: 0.00132926
Iteration 16/25 | Loss: 0.00132926
Iteration 17/25 | Loss: 0.00132926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013292618095874786, 0.0013292618095874786, 0.0013292618095874786, 0.0013292618095874786, 0.0013292618095874786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013292618095874786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132926
Iteration 2/1000 | Loss: 0.00010336
Iteration 3/1000 | Loss: 0.00005062
Iteration 4/1000 | Loss: 0.00003768
Iteration 5/1000 | Loss: 0.00003204
Iteration 6/1000 | Loss: 0.00002885
Iteration 7/1000 | Loss: 0.00002713
Iteration 8/1000 | Loss: 0.00002610
Iteration 9/1000 | Loss: 0.00002536
Iteration 10/1000 | Loss: 0.00002491
Iteration 11/1000 | Loss: 0.00002451
Iteration 12/1000 | Loss: 0.00002409
Iteration 13/1000 | Loss: 0.00002371
Iteration 14/1000 | Loss: 0.00002333
Iteration 15/1000 | Loss: 0.00002309
Iteration 16/1000 | Loss: 0.00002291
Iteration 17/1000 | Loss: 0.00002291
Iteration 18/1000 | Loss: 0.00002283
Iteration 19/1000 | Loss: 0.00002277
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002269
Iteration 22/1000 | Loss: 0.00002269
Iteration 23/1000 | Loss: 0.00002268
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002266
Iteration 26/1000 | Loss: 0.00002265
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002261
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002258
Iteration 31/1000 | Loss: 0.00002258
Iteration 32/1000 | Loss: 0.00002256
Iteration 33/1000 | Loss: 0.00002255
Iteration 34/1000 | Loss: 0.00002255
Iteration 35/1000 | Loss: 0.00002255
Iteration 36/1000 | Loss: 0.00002254
Iteration 37/1000 | Loss: 0.00002253
Iteration 38/1000 | Loss: 0.00002253
Iteration 39/1000 | Loss: 0.00002253
Iteration 40/1000 | Loss: 0.00002253
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002252
Iteration 44/1000 | Loss: 0.00002251
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002250
Iteration 49/1000 | Loss: 0.00002250
Iteration 50/1000 | Loss: 0.00002250
Iteration 51/1000 | Loss: 0.00002250
Iteration 52/1000 | Loss: 0.00002249
Iteration 53/1000 | Loss: 0.00002249
Iteration 54/1000 | Loss: 0.00002249
Iteration 55/1000 | Loss: 0.00002249
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002248
Iteration 58/1000 | Loss: 0.00002248
Iteration 59/1000 | Loss: 0.00002248
Iteration 60/1000 | Loss: 0.00002248
Iteration 61/1000 | Loss: 0.00002248
Iteration 62/1000 | Loss: 0.00002248
Iteration 63/1000 | Loss: 0.00002248
Iteration 64/1000 | Loss: 0.00002248
Iteration 65/1000 | Loss: 0.00002248
Iteration 66/1000 | Loss: 0.00002248
Iteration 67/1000 | Loss: 0.00002247
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00002247
Iteration 70/1000 | Loss: 0.00002247
Iteration 71/1000 | Loss: 0.00002247
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002246
Iteration 76/1000 | Loss: 0.00002246
Iteration 77/1000 | Loss: 0.00002246
Iteration 78/1000 | Loss: 0.00002246
Iteration 79/1000 | Loss: 0.00002245
Iteration 80/1000 | Loss: 0.00002245
Iteration 81/1000 | Loss: 0.00002245
Iteration 82/1000 | Loss: 0.00002245
Iteration 83/1000 | Loss: 0.00002245
Iteration 84/1000 | Loss: 0.00002245
Iteration 85/1000 | Loss: 0.00002245
Iteration 86/1000 | Loss: 0.00002245
Iteration 87/1000 | Loss: 0.00002245
Iteration 88/1000 | Loss: 0.00002245
Iteration 89/1000 | Loss: 0.00002244
Iteration 90/1000 | Loss: 0.00002244
Iteration 91/1000 | Loss: 0.00002244
Iteration 92/1000 | Loss: 0.00002244
Iteration 93/1000 | Loss: 0.00002244
Iteration 94/1000 | Loss: 0.00002244
Iteration 95/1000 | Loss: 0.00002244
Iteration 96/1000 | Loss: 0.00002244
Iteration 97/1000 | Loss: 0.00002244
Iteration 98/1000 | Loss: 0.00002244
Iteration 99/1000 | Loss: 0.00002244
Iteration 100/1000 | Loss: 0.00002244
Iteration 101/1000 | Loss: 0.00002244
Iteration 102/1000 | Loss: 0.00002243
Iteration 103/1000 | Loss: 0.00002243
Iteration 104/1000 | Loss: 0.00002243
Iteration 105/1000 | Loss: 0.00002243
Iteration 106/1000 | Loss: 0.00002243
Iteration 107/1000 | Loss: 0.00002243
Iteration 108/1000 | Loss: 0.00002243
Iteration 109/1000 | Loss: 0.00002242
Iteration 110/1000 | Loss: 0.00002242
Iteration 111/1000 | Loss: 0.00002242
Iteration 112/1000 | Loss: 0.00002242
Iteration 113/1000 | Loss: 0.00002242
Iteration 114/1000 | Loss: 0.00002242
Iteration 115/1000 | Loss: 0.00002242
Iteration 116/1000 | Loss: 0.00002242
Iteration 117/1000 | Loss: 0.00002242
Iteration 118/1000 | Loss: 0.00002242
Iteration 119/1000 | Loss: 0.00002241
Iteration 120/1000 | Loss: 0.00002241
Iteration 121/1000 | Loss: 0.00002241
Iteration 122/1000 | Loss: 0.00002241
Iteration 123/1000 | Loss: 0.00002241
Iteration 124/1000 | Loss: 0.00002241
Iteration 125/1000 | Loss: 0.00002241
Iteration 126/1000 | Loss: 0.00002241
Iteration 127/1000 | Loss: 0.00002240
Iteration 128/1000 | Loss: 0.00002240
Iteration 129/1000 | Loss: 0.00002240
Iteration 130/1000 | Loss: 0.00002240
Iteration 131/1000 | Loss: 0.00002240
Iteration 132/1000 | Loss: 0.00002239
Iteration 133/1000 | Loss: 0.00002239
Iteration 134/1000 | Loss: 0.00002239
Iteration 135/1000 | Loss: 0.00002238
Iteration 136/1000 | Loss: 0.00002238
Iteration 137/1000 | Loss: 0.00002238
Iteration 138/1000 | Loss: 0.00002238
Iteration 139/1000 | Loss: 0.00002237
Iteration 140/1000 | Loss: 0.00002237
Iteration 141/1000 | Loss: 0.00002237
Iteration 142/1000 | Loss: 0.00002236
Iteration 143/1000 | Loss: 0.00002236
Iteration 144/1000 | Loss: 0.00002236
Iteration 145/1000 | Loss: 0.00002236
Iteration 146/1000 | Loss: 0.00002236
Iteration 147/1000 | Loss: 0.00002235
Iteration 148/1000 | Loss: 0.00002235
Iteration 149/1000 | Loss: 0.00002235
Iteration 150/1000 | Loss: 0.00002235
Iteration 151/1000 | Loss: 0.00002235
Iteration 152/1000 | Loss: 0.00002234
Iteration 153/1000 | Loss: 0.00002234
Iteration 154/1000 | Loss: 0.00002234
Iteration 155/1000 | Loss: 0.00002234
Iteration 156/1000 | Loss: 0.00002234
Iteration 157/1000 | Loss: 0.00002234
Iteration 158/1000 | Loss: 0.00002233
Iteration 159/1000 | Loss: 0.00002233
Iteration 160/1000 | Loss: 0.00002233
Iteration 161/1000 | Loss: 0.00002233
Iteration 162/1000 | Loss: 0.00002233
Iteration 163/1000 | Loss: 0.00002232
Iteration 164/1000 | Loss: 0.00002232
Iteration 165/1000 | Loss: 0.00002232
Iteration 166/1000 | Loss: 0.00002232
Iteration 167/1000 | Loss: 0.00002232
Iteration 168/1000 | Loss: 0.00002232
Iteration 169/1000 | Loss: 0.00002232
Iteration 170/1000 | Loss: 0.00002232
Iteration 171/1000 | Loss: 0.00002232
Iteration 172/1000 | Loss: 0.00002232
Iteration 173/1000 | Loss: 0.00002232
Iteration 174/1000 | Loss: 0.00002232
Iteration 175/1000 | Loss: 0.00002232
Iteration 176/1000 | Loss: 0.00002232
Iteration 177/1000 | Loss: 0.00002232
Iteration 178/1000 | Loss: 0.00002231
Iteration 179/1000 | Loss: 0.00002231
Iteration 180/1000 | Loss: 0.00002231
Iteration 181/1000 | Loss: 0.00002231
Iteration 182/1000 | Loss: 0.00002231
Iteration 183/1000 | Loss: 0.00002231
Iteration 184/1000 | Loss: 0.00002231
Iteration 185/1000 | Loss: 0.00002231
Iteration 186/1000 | Loss: 0.00002231
Iteration 187/1000 | Loss: 0.00002231
Iteration 188/1000 | Loss: 0.00002231
Iteration 189/1000 | Loss: 0.00002230
Iteration 190/1000 | Loss: 0.00002230
Iteration 191/1000 | Loss: 0.00002230
Iteration 192/1000 | Loss: 0.00002230
Iteration 193/1000 | Loss: 0.00002229
Iteration 194/1000 | Loss: 0.00002229
Iteration 195/1000 | Loss: 0.00002229
Iteration 196/1000 | Loss: 0.00002229
Iteration 197/1000 | Loss: 0.00002229
Iteration 198/1000 | Loss: 0.00002229
Iteration 199/1000 | Loss: 0.00002229
Iteration 200/1000 | Loss: 0.00002229
Iteration 201/1000 | Loss: 0.00002229
Iteration 202/1000 | Loss: 0.00002229
Iteration 203/1000 | Loss: 0.00002229
Iteration 204/1000 | Loss: 0.00002229
Iteration 205/1000 | Loss: 0.00002229
Iteration 206/1000 | Loss: 0.00002229
Iteration 207/1000 | Loss: 0.00002229
Iteration 208/1000 | Loss: 0.00002229
Iteration 209/1000 | Loss: 0.00002229
Iteration 210/1000 | Loss: 0.00002229
Iteration 211/1000 | Loss: 0.00002229
Iteration 212/1000 | Loss: 0.00002229
Iteration 213/1000 | Loss: 0.00002229
Iteration 214/1000 | Loss: 0.00002229
Iteration 215/1000 | Loss: 0.00002229
Iteration 216/1000 | Loss: 0.00002229
Iteration 217/1000 | Loss: 0.00002229
Iteration 218/1000 | Loss: 0.00002229
Iteration 219/1000 | Loss: 0.00002229
Iteration 220/1000 | Loss: 0.00002229
Iteration 221/1000 | Loss: 0.00002229
Iteration 222/1000 | Loss: 0.00002229
Iteration 223/1000 | Loss: 0.00002229
Iteration 224/1000 | Loss: 0.00002229
Iteration 225/1000 | Loss: 0.00002229
Iteration 226/1000 | Loss: 0.00002229
Iteration 227/1000 | Loss: 0.00002229
Iteration 228/1000 | Loss: 0.00002229
Iteration 229/1000 | Loss: 0.00002229
Iteration 230/1000 | Loss: 0.00002229
Iteration 231/1000 | Loss: 0.00002229
Iteration 232/1000 | Loss: 0.00002229
Iteration 233/1000 | Loss: 0.00002229
Iteration 234/1000 | Loss: 0.00002229
Iteration 235/1000 | Loss: 0.00002229
Iteration 236/1000 | Loss: 0.00002229
Iteration 237/1000 | Loss: 0.00002229
Iteration 238/1000 | Loss: 0.00002229
Iteration 239/1000 | Loss: 0.00002229
Iteration 240/1000 | Loss: 0.00002229
Iteration 241/1000 | Loss: 0.00002229
Iteration 242/1000 | Loss: 0.00002229
Iteration 243/1000 | Loss: 0.00002229
Iteration 244/1000 | Loss: 0.00002229
Iteration 245/1000 | Loss: 0.00002229
Iteration 246/1000 | Loss: 0.00002229
Iteration 247/1000 | Loss: 0.00002229
Iteration 248/1000 | Loss: 0.00002229
Iteration 249/1000 | Loss: 0.00002229
Iteration 250/1000 | Loss: 0.00002229
Iteration 251/1000 | Loss: 0.00002229
Iteration 252/1000 | Loss: 0.00002229
Iteration 253/1000 | Loss: 0.00002229
Iteration 254/1000 | Loss: 0.00002229
Iteration 255/1000 | Loss: 0.00002229
Iteration 256/1000 | Loss: 0.00002229
Iteration 257/1000 | Loss: 0.00002229
Iteration 258/1000 | Loss: 0.00002229
Iteration 259/1000 | Loss: 0.00002229
Iteration 260/1000 | Loss: 0.00002229
Iteration 261/1000 | Loss: 0.00002229
Iteration 262/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [2.229426354460884e-05, 2.229426354460884e-05, 2.229426354460884e-05, 2.229426354460884e-05, 2.229426354460884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.229426354460884e-05

Optimization complete. Final v2v error: 4.177445411682129 mm

Highest mean error: 4.562451362609863 mm for frame 5

Lowest mean error: 3.884582042694092 mm for frame 110

Saving results

Total time: 50.13038873672485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676986
Iteration 2/25 | Loss: 0.00173550
Iteration 3/25 | Loss: 0.00152387
Iteration 4/25 | Loss: 0.00150711
Iteration 5/25 | Loss: 0.00150250
Iteration 6/25 | Loss: 0.00150193
Iteration 7/25 | Loss: 0.00150193
Iteration 8/25 | Loss: 0.00150193
Iteration 9/25 | Loss: 0.00150193
Iteration 10/25 | Loss: 0.00150193
Iteration 11/25 | Loss: 0.00150193
Iteration 12/25 | Loss: 0.00150193
Iteration 13/25 | Loss: 0.00150193
Iteration 14/25 | Loss: 0.00150193
Iteration 15/25 | Loss: 0.00150193
Iteration 16/25 | Loss: 0.00150193
Iteration 17/25 | Loss: 0.00150193
Iteration 18/25 | Loss: 0.00150193
Iteration 19/25 | Loss: 0.00150193
Iteration 20/25 | Loss: 0.00150193
Iteration 21/25 | Loss: 0.00150193
Iteration 22/25 | Loss: 0.00150193
Iteration 23/25 | Loss: 0.00150193
Iteration 24/25 | Loss: 0.00150193
Iteration 25/25 | Loss: 0.00150193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35576379
Iteration 2/25 | Loss: 0.00094631
Iteration 3/25 | Loss: 0.00094629
Iteration 4/25 | Loss: 0.00094629
Iteration 5/25 | Loss: 0.00094629
Iteration 6/25 | Loss: 0.00094629
Iteration 7/25 | Loss: 0.00094628
Iteration 8/25 | Loss: 0.00094628
Iteration 9/25 | Loss: 0.00094628
Iteration 10/25 | Loss: 0.00094628
Iteration 11/25 | Loss: 0.00094628
Iteration 12/25 | Loss: 0.00094628
Iteration 13/25 | Loss: 0.00094628
Iteration 14/25 | Loss: 0.00094628
Iteration 15/25 | Loss: 0.00094628
Iteration 16/25 | Loss: 0.00094628
Iteration 17/25 | Loss: 0.00094628
Iteration 18/25 | Loss: 0.00094628
Iteration 19/25 | Loss: 0.00094628
Iteration 20/25 | Loss: 0.00094628
Iteration 21/25 | Loss: 0.00094628
Iteration 22/25 | Loss: 0.00094628
Iteration 23/25 | Loss: 0.00094628
Iteration 24/25 | Loss: 0.00094628
Iteration 25/25 | Loss: 0.00094628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094628
Iteration 2/1000 | Loss: 0.00008380
Iteration 3/1000 | Loss: 0.00005492
Iteration 4/1000 | Loss: 0.00004493
Iteration 5/1000 | Loss: 0.00004047
Iteration 6/1000 | Loss: 0.00003864
Iteration 7/1000 | Loss: 0.00003773
Iteration 8/1000 | Loss: 0.00003717
Iteration 9/1000 | Loss: 0.00003674
Iteration 10/1000 | Loss: 0.00003648
Iteration 11/1000 | Loss: 0.00003632
Iteration 12/1000 | Loss: 0.00003627
Iteration 13/1000 | Loss: 0.00003621
Iteration 14/1000 | Loss: 0.00003621
Iteration 15/1000 | Loss: 0.00003620
Iteration 16/1000 | Loss: 0.00003619
Iteration 17/1000 | Loss: 0.00003619
Iteration 18/1000 | Loss: 0.00003619
Iteration 19/1000 | Loss: 0.00003618
Iteration 20/1000 | Loss: 0.00003618
Iteration 21/1000 | Loss: 0.00003618
Iteration 22/1000 | Loss: 0.00003617
Iteration 23/1000 | Loss: 0.00003617
Iteration 24/1000 | Loss: 0.00003617
Iteration 25/1000 | Loss: 0.00003617
Iteration 26/1000 | Loss: 0.00003617
Iteration 27/1000 | Loss: 0.00003616
Iteration 28/1000 | Loss: 0.00003616
Iteration 29/1000 | Loss: 0.00003616
Iteration 30/1000 | Loss: 0.00003616
Iteration 31/1000 | Loss: 0.00003615
Iteration 32/1000 | Loss: 0.00003615
Iteration 33/1000 | Loss: 0.00003615
Iteration 34/1000 | Loss: 0.00003614
Iteration 35/1000 | Loss: 0.00003613
Iteration 36/1000 | Loss: 0.00003613
Iteration 37/1000 | Loss: 0.00003613
Iteration 38/1000 | Loss: 0.00003613
Iteration 39/1000 | Loss: 0.00003613
Iteration 40/1000 | Loss: 0.00003612
Iteration 41/1000 | Loss: 0.00003612
Iteration 42/1000 | Loss: 0.00003612
Iteration 43/1000 | Loss: 0.00003612
Iteration 44/1000 | Loss: 0.00003612
Iteration 45/1000 | Loss: 0.00003612
Iteration 46/1000 | Loss: 0.00003612
Iteration 47/1000 | Loss: 0.00003612
Iteration 48/1000 | Loss: 0.00003611
Iteration 49/1000 | Loss: 0.00003611
Iteration 50/1000 | Loss: 0.00003611
Iteration 51/1000 | Loss: 0.00003611
Iteration 52/1000 | Loss: 0.00003610
Iteration 53/1000 | Loss: 0.00003610
Iteration 54/1000 | Loss: 0.00003610
Iteration 55/1000 | Loss: 0.00003610
Iteration 56/1000 | Loss: 0.00003609
Iteration 57/1000 | Loss: 0.00003609
Iteration 58/1000 | Loss: 0.00003609
Iteration 59/1000 | Loss: 0.00003608
Iteration 60/1000 | Loss: 0.00003608
Iteration 61/1000 | Loss: 0.00003608
Iteration 62/1000 | Loss: 0.00003608
Iteration 63/1000 | Loss: 0.00003608
Iteration 64/1000 | Loss: 0.00003608
Iteration 65/1000 | Loss: 0.00003607
Iteration 66/1000 | Loss: 0.00003607
Iteration 67/1000 | Loss: 0.00003607
Iteration 68/1000 | Loss: 0.00003607
Iteration 69/1000 | Loss: 0.00003607
Iteration 70/1000 | Loss: 0.00003607
Iteration 71/1000 | Loss: 0.00003607
Iteration 72/1000 | Loss: 0.00003607
Iteration 73/1000 | Loss: 0.00003607
Iteration 74/1000 | Loss: 0.00003606
Iteration 75/1000 | Loss: 0.00003606
Iteration 76/1000 | Loss: 0.00003606
Iteration 77/1000 | Loss: 0.00003606
Iteration 78/1000 | Loss: 0.00003605
Iteration 79/1000 | Loss: 0.00003605
Iteration 80/1000 | Loss: 0.00003605
Iteration 81/1000 | Loss: 0.00003605
Iteration 82/1000 | Loss: 0.00003605
Iteration 83/1000 | Loss: 0.00003605
Iteration 84/1000 | Loss: 0.00003605
Iteration 85/1000 | Loss: 0.00003605
Iteration 86/1000 | Loss: 0.00003605
Iteration 87/1000 | Loss: 0.00003605
Iteration 88/1000 | Loss: 0.00003605
Iteration 89/1000 | Loss: 0.00003604
Iteration 90/1000 | Loss: 0.00003604
Iteration 91/1000 | Loss: 0.00003604
Iteration 92/1000 | Loss: 0.00003604
Iteration 93/1000 | Loss: 0.00003604
Iteration 94/1000 | Loss: 0.00003604
Iteration 95/1000 | Loss: 0.00003604
Iteration 96/1000 | Loss: 0.00003604
Iteration 97/1000 | Loss: 0.00003604
Iteration 98/1000 | Loss: 0.00003604
Iteration 99/1000 | Loss: 0.00003604
Iteration 100/1000 | Loss: 0.00003604
Iteration 101/1000 | Loss: 0.00003604
Iteration 102/1000 | Loss: 0.00003604
Iteration 103/1000 | Loss: 0.00003604
Iteration 104/1000 | Loss: 0.00003604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [3.6043300497112796e-05, 3.6043300497112796e-05, 3.6043300497112796e-05, 3.6043300497112796e-05, 3.6043300497112796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6043300497112796e-05

Optimization complete. Final v2v error: 5.072558403015137 mm

Highest mean error: 5.412403106689453 mm for frame 70

Lowest mean error: 4.509673118591309 mm for frame 232

Saving results

Total time: 35.74794363975525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444436
Iteration 2/25 | Loss: 0.00150068
Iteration 3/25 | Loss: 0.00140509
Iteration 4/25 | Loss: 0.00139188
Iteration 5/25 | Loss: 0.00138765
Iteration 6/25 | Loss: 0.00138639
Iteration 7/25 | Loss: 0.00138633
Iteration 8/25 | Loss: 0.00138633
Iteration 9/25 | Loss: 0.00138633
Iteration 10/25 | Loss: 0.00138633
Iteration 11/25 | Loss: 0.00138633
Iteration 12/25 | Loss: 0.00138633
Iteration 13/25 | Loss: 0.00138633
Iteration 14/25 | Loss: 0.00138633
Iteration 15/25 | Loss: 0.00138633
Iteration 16/25 | Loss: 0.00138633
Iteration 17/25 | Loss: 0.00138633
Iteration 18/25 | Loss: 0.00138633
Iteration 19/25 | Loss: 0.00138633
Iteration 20/25 | Loss: 0.00138633
Iteration 21/25 | Loss: 0.00138633
Iteration 22/25 | Loss: 0.00138633
Iteration 23/25 | Loss: 0.00138633
Iteration 24/25 | Loss: 0.00138633
Iteration 25/25 | Loss: 0.00138633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25533724
Iteration 2/25 | Loss: 0.00117480
Iteration 3/25 | Loss: 0.00117480
Iteration 4/25 | Loss: 0.00117480
Iteration 5/25 | Loss: 0.00117480
Iteration 6/25 | Loss: 0.00117480
Iteration 7/25 | Loss: 0.00117480
Iteration 8/25 | Loss: 0.00117480
Iteration 9/25 | Loss: 0.00117480
Iteration 10/25 | Loss: 0.00117480
Iteration 11/25 | Loss: 0.00117480
Iteration 12/25 | Loss: 0.00117480
Iteration 13/25 | Loss: 0.00117480
Iteration 14/25 | Loss: 0.00117480
Iteration 15/25 | Loss: 0.00117480
Iteration 16/25 | Loss: 0.00117480
Iteration 17/25 | Loss: 0.00117480
Iteration 18/25 | Loss: 0.00117480
Iteration 19/25 | Loss: 0.00117480
Iteration 20/25 | Loss: 0.00117480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011747978860512376, 0.0011747978860512376, 0.0011747978860512376, 0.0011747978860512376, 0.0011747978860512376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011747978860512376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117480
Iteration 2/1000 | Loss: 0.00007296
Iteration 3/1000 | Loss: 0.00004887
Iteration 4/1000 | Loss: 0.00004001
Iteration 5/1000 | Loss: 0.00003449
Iteration 6/1000 | Loss: 0.00003194
Iteration 7/1000 | Loss: 0.00003036
Iteration 8/1000 | Loss: 0.00002970
Iteration 9/1000 | Loss: 0.00002916
Iteration 10/1000 | Loss: 0.00002873
Iteration 11/1000 | Loss: 0.00002826
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002769
Iteration 14/1000 | Loss: 0.00002767
Iteration 15/1000 | Loss: 0.00002747
Iteration 16/1000 | Loss: 0.00002732
Iteration 17/1000 | Loss: 0.00002728
Iteration 18/1000 | Loss: 0.00002728
Iteration 19/1000 | Loss: 0.00002727
Iteration 20/1000 | Loss: 0.00002727
Iteration 21/1000 | Loss: 0.00002726
Iteration 22/1000 | Loss: 0.00002726
Iteration 23/1000 | Loss: 0.00002726
Iteration 24/1000 | Loss: 0.00002725
Iteration 25/1000 | Loss: 0.00002722
Iteration 26/1000 | Loss: 0.00002721
Iteration 27/1000 | Loss: 0.00002718
Iteration 28/1000 | Loss: 0.00002718
Iteration 29/1000 | Loss: 0.00002717
Iteration 30/1000 | Loss: 0.00002717
Iteration 31/1000 | Loss: 0.00002716
Iteration 32/1000 | Loss: 0.00002716
Iteration 33/1000 | Loss: 0.00002716
Iteration 34/1000 | Loss: 0.00002713
Iteration 35/1000 | Loss: 0.00002713
Iteration 36/1000 | Loss: 0.00002713
Iteration 37/1000 | Loss: 0.00002713
Iteration 38/1000 | Loss: 0.00002713
Iteration 39/1000 | Loss: 0.00002713
Iteration 40/1000 | Loss: 0.00002713
Iteration 41/1000 | Loss: 0.00002713
Iteration 42/1000 | Loss: 0.00002713
Iteration 43/1000 | Loss: 0.00002713
Iteration 44/1000 | Loss: 0.00002712
Iteration 45/1000 | Loss: 0.00002712
Iteration 46/1000 | Loss: 0.00002712
Iteration 47/1000 | Loss: 0.00002711
Iteration 48/1000 | Loss: 0.00002711
Iteration 49/1000 | Loss: 0.00002711
Iteration 50/1000 | Loss: 0.00002711
Iteration 51/1000 | Loss: 0.00002711
Iteration 52/1000 | Loss: 0.00002711
Iteration 53/1000 | Loss: 0.00002710
Iteration 54/1000 | Loss: 0.00002710
Iteration 55/1000 | Loss: 0.00002710
Iteration 56/1000 | Loss: 0.00002710
Iteration 57/1000 | Loss: 0.00002709
Iteration 58/1000 | Loss: 0.00002709
Iteration 59/1000 | Loss: 0.00002709
Iteration 60/1000 | Loss: 0.00002709
Iteration 61/1000 | Loss: 0.00002709
Iteration 62/1000 | Loss: 0.00002709
Iteration 63/1000 | Loss: 0.00002709
Iteration 64/1000 | Loss: 0.00002709
Iteration 65/1000 | Loss: 0.00002709
Iteration 66/1000 | Loss: 0.00002709
Iteration 67/1000 | Loss: 0.00002709
Iteration 68/1000 | Loss: 0.00002709
Iteration 69/1000 | Loss: 0.00002709
Iteration 70/1000 | Loss: 0.00002709
Iteration 71/1000 | Loss: 0.00002709
Iteration 72/1000 | Loss: 0.00002709
Iteration 73/1000 | Loss: 0.00002709
Iteration 74/1000 | Loss: 0.00002709
Iteration 75/1000 | Loss: 0.00002709
Iteration 76/1000 | Loss: 0.00002709
Iteration 77/1000 | Loss: 0.00002709
Iteration 78/1000 | Loss: 0.00002709
Iteration 79/1000 | Loss: 0.00002709
Iteration 80/1000 | Loss: 0.00002709
Iteration 81/1000 | Loss: 0.00002709
Iteration 82/1000 | Loss: 0.00002709
Iteration 83/1000 | Loss: 0.00002709
Iteration 84/1000 | Loss: 0.00002709
Iteration 85/1000 | Loss: 0.00002709
Iteration 86/1000 | Loss: 0.00002709
Iteration 87/1000 | Loss: 0.00002709
Iteration 88/1000 | Loss: 0.00002709
Iteration 89/1000 | Loss: 0.00002709
Iteration 90/1000 | Loss: 0.00002709
Iteration 91/1000 | Loss: 0.00002709
Iteration 92/1000 | Loss: 0.00002709
Iteration 93/1000 | Loss: 0.00002709
Iteration 94/1000 | Loss: 0.00002709
Iteration 95/1000 | Loss: 0.00002709
Iteration 96/1000 | Loss: 0.00002709
Iteration 97/1000 | Loss: 0.00002709
Iteration 98/1000 | Loss: 0.00002709
Iteration 99/1000 | Loss: 0.00002709
Iteration 100/1000 | Loss: 0.00002709
Iteration 101/1000 | Loss: 0.00002709
Iteration 102/1000 | Loss: 0.00002709
Iteration 103/1000 | Loss: 0.00002709
Iteration 104/1000 | Loss: 0.00002709
Iteration 105/1000 | Loss: 0.00002709
Iteration 106/1000 | Loss: 0.00002709
Iteration 107/1000 | Loss: 0.00002709
Iteration 108/1000 | Loss: 0.00002709
Iteration 109/1000 | Loss: 0.00002709
Iteration 110/1000 | Loss: 0.00002709
Iteration 111/1000 | Loss: 0.00002709
Iteration 112/1000 | Loss: 0.00002709
Iteration 113/1000 | Loss: 0.00002709
Iteration 114/1000 | Loss: 0.00002709
Iteration 115/1000 | Loss: 0.00002709
Iteration 116/1000 | Loss: 0.00002709
Iteration 117/1000 | Loss: 0.00002709
Iteration 118/1000 | Loss: 0.00002709
Iteration 119/1000 | Loss: 0.00002709
Iteration 120/1000 | Loss: 0.00002709
Iteration 121/1000 | Loss: 0.00002709
Iteration 122/1000 | Loss: 0.00002709
Iteration 123/1000 | Loss: 0.00002709
Iteration 124/1000 | Loss: 0.00002709
Iteration 125/1000 | Loss: 0.00002709
Iteration 126/1000 | Loss: 0.00002709
Iteration 127/1000 | Loss: 0.00002709
Iteration 128/1000 | Loss: 0.00002709
Iteration 129/1000 | Loss: 0.00002709
Iteration 130/1000 | Loss: 0.00002709
Iteration 131/1000 | Loss: 0.00002709
Iteration 132/1000 | Loss: 0.00002709
Iteration 133/1000 | Loss: 0.00002709
Iteration 134/1000 | Loss: 0.00002709
Iteration 135/1000 | Loss: 0.00002709
Iteration 136/1000 | Loss: 0.00002709
Iteration 137/1000 | Loss: 0.00002709
Iteration 138/1000 | Loss: 0.00002709
Iteration 139/1000 | Loss: 0.00002709
Iteration 140/1000 | Loss: 0.00002709
Iteration 141/1000 | Loss: 0.00002709
Iteration 142/1000 | Loss: 0.00002709
Iteration 143/1000 | Loss: 0.00002709
Iteration 144/1000 | Loss: 0.00002709
Iteration 145/1000 | Loss: 0.00002709
Iteration 146/1000 | Loss: 0.00002709
Iteration 147/1000 | Loss: 0.00002709
Iteration 148/1000 | Loss: 0.00002709
Iteration 149/1000 | Loss: 0.00002709
Iteration 150/1000 | Loss: 0.00002709
Iteration 151/1000 | Loss: 0.00002709
Iteration 152/1000 | Loss: 0.00002709
Iteration 153/1000 | Loss: 0.00002709
Iteration 154/1000 | Loss: 0.00002709
Iteration 155/1000 | Loss: 0.00002709
Iteration 156/1000 | Loss: 0.00002709
Iteration 157/1000 | Loss: 0.00002709
Iteration 158/1000 | Loss: 0.00002709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.7087919079349376e-05, 2.7087919079349376e-05, 2.7087919079349376e-05, 2.7087919079349376e-05, 2.7087919079349376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7087919079349376e-05

Optimization complete. Final v2v error: 4.346646308898926 mm

Highest mean error: 4.734610557556152 mm for frame 25

Lowest mean error: 3.934584617614746 mm for frame 133

Saving results

Total time: 38.90136766433716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141239
Iteration 2/25 | Loss: 0.01141239
Iteration 3/25 | Loss: 0.01141239
Iteration 4/25 | Loss: 0.01141239
Iteration 5/25 | Loss: 0.01141239
Iteration 6/25 | Loss: 0.01141239
Iteration 7/25 | Loss: 0.01141238
Iteration 8/25 | Loss: 0.01141238
Iteration 9/25 | Loss: 0.01141238
Iteration 10/25 | Loss: 0.01141238
Iteration 11/25 | Loss: 0.01141238
Iteration 12/25 | Loss: 0.01141238
Iteration 13/25 | Loss: 0.01141238
Iteration 14/25 | Loss: 0.01141237
Iteration 15/25 | Loss: 0.01141237
Iteration 16/25 | Loss: 0.01141237
Iteration 17/25 | Loss: 0.01141237
Iteration 18/25 | Loss: 0.01141237
Iteration 19/25 | Loss: 0.01141236
Iteration 20/25 | Loss: 0.01141236
Iteration 21/25 | Loss: 0.01141236
Iteration 22/25 | Loss: 0.01141236
Iteration 23/25 | Loss: 0.01141236
Iteration 24/25 | Loss: 0.01141236
Iteration 25/25 | Loss: 0.01141236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50511384
Iteration 2/25 | Loss: 0.12161698
Iteration 3/25 | Loss: 0.11747947
Iteration 4/25 | Loss: 0.11585217
Iteration 5/25 | Loss: 0.11585217
Iteration 6/25 | Loss: 0.11585216
Iteration 7/25 | Loss: 0.11585216
Iteration 8/25 | Loss: 0.11585215
Iteration 9/25 | Loss: 0.11585215
Iteration 10/25 | Loss: 0.11585215
Iteration 11/25 | Loss: 0.11585215
Iteration 12/25 | Loss: 0.11585215
Iteration 13/25 | Loss: 0.11585215
Iteration 14/25 | Loss: 0.11585215
Iteration 15/25 | Loss: 0.11585215
Iteration 16/25 | Loss: 0.11585215
Iteration 17/25 | Loss: 0.11585215
Iteration 18/25 | Loss: 0.11585215
Iteration 19/25 | Loss: 0.11585215
Iteration 20/25 | Loss: 0.11585215
Iteration 21/25 | Loss: 0.11585215
Iteration 22/25 | Loss: 0.11585215
Iteration 23/25 | Loss: 0.11585215
Iteration 24/25 | Loss: 0.11585215
Iteration 25/25 | Loss: 0.11585215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11585215
Iteration 2/1000 | Loss: 0.00246783
Iteration 3/1000 | Loss: 0.00072620
Iteration 4/1000 | Loss: 0.00037041
Iteration 5/1000 | Loss: 0.00014761
Iteration 6/1000 | Loss: 0.00011823
Iteration 7/1000 | Loss: 0.00006507
Iteration 8/1000 | Loss: 0.00005803
Iteration 9/1000 | Loss: 0.00004092
Iteration 10/1000 | Loss: 0.00003560
Iteration 11/1000 | Loss: 0.00004926
Iteration 12/1000 | Loss: 0.00003139
Iteration 13/1000 | Loss: 0.00002815
Iteration 14/1000 | Loss: 0.00002684
Iteration 15/1000 | Loss: 0.00003099
Iteration 16/1000 | Loss: 0.00002515
Iteration 17/1000 | Loss: 0.00002618
Iteration 18/1000 | Loss: 0.00004728
Iteration 19/1000 | Loss: 0.00002445
Iteration 20/1000 | Loss: 0.00002410
Iteration 21/1000 | Loss: 0.00002360
Iteration 22/1000 | Loss: 0.00002325
Iteration 23/1000 | Loss: 0.00002300
Iteration 24/1000 | Loss: 0.00002296
Iteration 25/1000 | Loss: 0.00002284
Iteration 26/1000 | Loss: 0.00002266
Iteration 27/1000 | Loss: 0.00002261
Iteration 28/1000 | Loss: 0.00002258
Iteration 29/1000 | Loss: 0.00002258
Iteration 30/1000 | Loss: 0.00002256
Iteration 31/1000 | Loss: 0.00003364
Iteration 32/1000 | Loss: 0.00002250
Iteration 33/1000 | Loss: 0.00002248
Iteration 34/1000 | Loss: 0.00002248
Iteration 35/1000 | Loss: 0.00002247
Iteration 36/1000 | Loss: 0.00002246
Iteration 37/1000 | Loss: 0.00002246
Iteration 38/1000 | Loss: 0.00002246
Iteration 39/1000 | Loss: 0.00002246
Iteration 40/1000 | Loss: 0.00002246
Iteration 41/1000 | Loss: 0.00002246
Iteration 42/1000 | Loss: 0.00002246
Iteration 43/1000 | Loss: 0.00002246
Iteration 44/1000 | Loss: 0.00002246
Iteration 45/1000 | Loss: 0.00002246
Iteration 46/1000 | Loss: 0.00002246
Iteration 47/1000 | Loss: 0.00002964
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002243
Iteration 50/1000 | Loss: 0.00002243
Iteration 51/1000 | Loss: 0.00002243
Iteration 52/1000 | Loss: 0.00002243
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00002243
Iteration 55/1000 | Loss: 0.00002243
Iteration 56/1000 | Loss: 0.00002243
Iteration 57/1000 | Loss: 0.00002242
Iteration 58/1000 | Loss: 0.00002242
Iteration 59/1000 | Loss: 0.00002242
Iteration 60/1000 | Loss: 0.00002242
Iteration 61/1000 | Loss: 0.00002242
Iteration 62/1000 | Loss: 0.00002242
Iteration 63/1000 | Loss: 0.00002242
Iteration 64/1000 | Loss: 0.00002242
Iteration 65/1000 | Loss: 0.00002242
Iteration 66/1000 | Loss: 0.00002242
Iteration 67/1000 | Loss: 0.00002242
Iteration 68/1000 | Loss: 0.00002241
Iteration 69/1000 | Loss: 0.00002241
Iteration 70/1000 | Loss: 0.00002241
Iteration 71/1000 | Loss: 0.00002241
Iteration 72/1000 | Loss: 0.00002240
Iteration 73/1000 | Loss: 0.00002240
Iteration 74/1000 | Loss: 0.00002240
Iteration 75/1000 | Loss: 0.00003362
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00002239
Iteration 82/1000 | Loss: 0.00002238
Iteration 83/1000 | Loss: 0.00002238
Iteration 84/1000 | Loss: 0.00002238
Iteration 85/1000 | Loss: 0.00002238
Iteration 86/1000 | Loss: 0.00002236
Iteration 87/1000 | Loss: 0.00002234
Iteration 88/1000 | Loss: 0.00002234
Iteration 89/1000 | Loss: 0.00002234
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002233
Iteration 93/1000 | Loss: 0.00002233
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002233
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002232
Iteration 98/1000 | Loss: 0.00002229
Iteration 99/1000 | Loss: 0.00002229
Iteration 100/1000 | Loss: 0.00002229
Iteration 101/1000 | Loss: 0.00002229
Iteration 102/1000 | Loss: 0.00002228
Iteration 103/1000 | Loss: 0.00002228
Iteration 104/1000 | Loss: 0.00002228
Iteration 105/1000 | Loss: 0.00002227
Iteration 106/1000 | Loss: 0.00002227
Iteration 107/1000 | Loss: 0.00002227
Iteration 108/1000 | Loss: 0.00002226
Iteration 109/1000 | Loss: 0.00002226
Iteration 110/1000 | Loss: 0.00002225
Iteration 111/1000 | Loss: 0.00002225
Iteration 112/1000 | Loss: 0.00002224
Iteration 113/1000 | Loss: 0.00002224
Iteration 114/1000 | Loss: 0.00002223
Iteration 115/1000 | Loss: 0.00002223
Iteration 116/1000 | Loss: 0.00002223
Iteration 117/1000 | Loss: 0.00002223
Iteration 118/1000 | Loss: 0.00002223
Iteration 119/1000 | Loss: 0.00002223
Iteration 120/1000 | Loss: 0.00002222
Iteration 121/1000 | Loss: 0.00002222
Iteration 122/1000 | Loss: 0.00002221
Iteration 123/1000 | Loss: 0.00002221
Iteration 124/1000 | Loss: 0.00002221
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Iteration 127/1000 | Loss: 0.00002221
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002220
Iteration 131/1000 | Loss: 0.00002220
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002219
Iteration 135/1000 | Loss: 0.00002219
Iteration 136/1000 | Loss: 0.00002219
Iteration 137/1000 | Loss: 0.00002219
Iteration 138/1000 | Loss: 0.00002219
Iteration 139/1000 | Loss: 0.00002219
Iteration 140/1000 | Loss: 0.00002218
Iteration 141/1000 | Loss: 0.00002218
Iteration 142/1000 | Loss: 0.00002218
Iteration 143/1000 | Loss: 0.00002218
Iteration 144/1000 | Loss: 0.00002218
Iteration 145/1000 | Loss: 0.00002217
Iteration 146/1000 | Loss: 0.00002217
Iteration 147/1000 | Loss: 0.00002216
Iteration 148/1000 | Loss: 0.00002216
Iteration 149/1000 | Loss: 0.00002216
Iteration 150/1000 | Loss: 0.00002216
Iteration 151/1000 | Loss: 0.00002215
Iteration 152/1000 | Loss: 0.00002215
Iteration 153/1000 | Loss: 0.00002215
Iteration 154/1000 | Loss: 0.00002215
Iteration 155/1000 | Loss: 0.00002215
Iteration 156/1000 | Loss: 0.00002215
Iteration 157/1000 | Loss: 0.00002215
Iteration 158/1000 | Loss: 0.00002215
Iteration 159/1000 | Loss: 0.00002215
Iteration 160/1000 | Loss: 0.00002214
Iteration 161/1000 | Loss: 0.00002214
Iteration 162/1000 | Loss: 0.00002214
Iteration 163/1000 | Loss: 0.00002214
Iteration 164/1000 | Loss: 0.00002214
Iteration 165/1000 | Loss: 0.00002213
Iteration 166/1000 | Loss: 0.00002213
Iteration 167/1000 | Loss: 0.00002213
Iteration 168/1000 | Loss: 0.00002213
Iteration 169/1000 | Loss: 0.00002213
Iteration 170/1000 | Loss: 0.00002213
Iteration 171/1000 | Loss: 0.00002213
Iteration 172/1000 | Loss: 0.00002213
Iteration 173/1000 | Loss: 0.00002213
Iteration 174/1000 | Loss: 0.00002213
Iteration 175/1000 | Loss: 0.00003717
Iteration 176/1000 | Loss: 0.00002211
Iteration 177/1000 | Loss: 0.00002210
Iteration 178/1000 | Loss: 0.00002210
Iteration 179/1000 | Loss: 0.00002210
Iteration 180/1000 | Loss: 0.00002209
Iteration 181/1000 | Loss: 0.00002209
Iteration 182/1000 | Loss: 0.00002209
Iteration 183/1000 | Loss: 0.00002209
Iteration 184/1000 | Loss: 0.00002209
Iteration 185/1000 | Loss: 0.00002209
Iteration 186/1000 | Loss: 0.00002209
Iteration 187/1000 | Loss: 0.00002209
Iteration 188/1000 | Loss: 0.00002209
Iteration 189/1000 | Loss: 0.00002209
Iteration 190/1000 | Loss: 0.00002209
Iteration 191/1000 | Loss: 0.00002209
Iteration 192/1000 | Loss: 0.00002209
Iteration 193/1000 | Loss: 0.00002209
Iteration 194/1000 | Loss: 0.00002209
Iteration 195/1000 | Loss: 0.00002209
Iteration 196/1000 | Loss: 0.00002209
Iteration 197/1000 | Loss: 0.00002209
Iteration 198/1000 | Loss: 0.00002209
Iteration 199/1000 | Loss: 0.00002209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.2088905097916722e-05, 2.2088905097916722e-05, 2.2088905097916722e-05, 2.2088905097916722e-05, 2.2088905097916722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2088905097916722e-05

Optimization complete. Final v2v error: 4.014456272125244 mm

Highest mean error: 10.008044242858887 mm for frame 226

Lowest mean error: 3.8213865756988525 mm for frame 230

Saving results

Total time: 69.71981859207153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109027
Iteration 2/25 | Loss: 0.00231294
Iteration 3/25 | Loss: 0.00152529
Iteration 4/25 | Loss: 0.00141252
Iteration 5/25 | Loss: 0.00139680
Iteration 6/25 | Loss: 0.00139256
Iteration 7/25 | Loss: 0.00139297
Iteration 8/25 | Loss: 0.00138981
Iteration 9/25 | Loss: 0.00138840
Iteration 10/25 | Loss: 0.00138575
Iteration 11/25 | Loss: 0.00138193
Iteration 12/25 | Loss: 0.00138064
Iteration 13/25 | Loss: 0.00141110
Iteration 14/25 | Loss: 0.00140412
Iteration 15/25 | Loss: 0.00137486
Iteration 16/25 | Loss: 0.00136839
Iteration 17/25 | Loss: 0.00136628
Iteration 18/25 | Loss: 0.00136286
Iteration 19/25 | Loss: 0.00136120
Iteration 20/25 | Loss: 0.00136097
Iteration 21/25 | Loss: 0.00136095
Iteration 22/25 | Loss: 0.00136095
Iteration 23/25 | Loss: 0.00136095
Iteration 24/25 | Loss: 0.00136095
Iteration 25/25 | Loss: 0.00136094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26881206
Iteration 2/25 | Loss: 0.00084625
Iteration 3/25 | Loss: 0.00084625
Iteration 4/25 | Loss: 0.00084625
Iteration 5/25 | Loss: 0.00084625
Iteration 6/25 | Loss: 0.00084625
Iteration 7/25 | Loss: 0.00084625
Iteration 8/25 | Loss: 0.00084625
Iteration 9/25 | Loss: 0.00084625
Iteration 10/25 | Loss: 0.00084625
Iteration 11/25 | Loss: 0.00084625
Iteration 12/25 | Loss: 0.00084625
Iteration 13/25 | Loss: 0.00084625
Iteration 14/25 | Loss: 0.00084625
Iteration 15/25 | Loss: 0.00084625
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008462456753477454, 0.0008462456753477454, 0.0008462456753477454, 0.0008462456753477454, 0.0008462456753477454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008462456753477454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084625
Iteration 2/1000 | Loss: 0.00003886
Iteration 3/1000 | Loss: 0.00002882
Iteration 4/1000 | Loss: 0.00002451
Iteration 5/1000 | Loss: 0.00002274
Iteration 6/1000 | Loss: 0.00002180
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002068
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00002014
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00002010
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001993
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001990
Iteration 31/1000 | Loss: 0.00001990
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001990
Iteration 34/1000 | Loss: 0.00001990
Iteration 35/1000 | Loss: 0.00001990
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001989
Iteration 42/1000 | Loss: 0.00001989
Iteration 43/1000 | Loss: 0.00001989
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.9873359633493237e-05, 1.9873359633493237e-05, 1.9873359633493237e-05, 1.9873359633493237e-05, 1.9873359633493237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9873359633493237e-05

Optimization complete. Final v2v error: 3.9187240600585938 mm

Highest mean error: 4.086270809173584 mm for frame 75

Lowest mean error: 3.689580202102661 mm for frame 2

Saving results

Total time: 57.18295454978943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450421
Iteration 2/25 | Loss: 0.00154407
Iteration 3/25 | Loss: 0.00140576
Iteration 4/25 | Loss: 0.00138793
Iteration 5/25 | Loss: 0.00138209
Iteration 6/25 | Loss: 0.00138047
Iteration 7/25 | Loss: 0.00138008
Iteration 8/25 | Loss: 0.00138008
Iteration 9/25 | Loss: 0.00138008
Iteration 10/25 | Loss: 0.00138008
Iteration 11/25 | Loss: 0.00138008
Iteration 12/25 | Loss: 0.00138008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013800798915326595, 0.0013800798915326595, 0.0013800798915326595, 0.0013800798915326595, 0.0013800798915326595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013800798915326595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34206021
Iteration 2/25 | Loss: 0.00114227
Iteration 3/25 | Loss: 0.00114225
Iteration 4/25 | Loss: 0.00114225
Iteration 5/25 | Loss: 0.00114225
Iteration 6/25 | Loss: 0.00114225
Iteration 7/25 | Loss: 0.00114225
Iteration 8/25 | Loss: 0.00114225
Iteration 9/25 | Loss: 0.00114225
Iteration 10/25 | Loss: 0.00114225
Iteration 11/25 | Loss: 0.00114225
Iteration 12/25 | Loss: 0.00114225
Iteration 13/25 | Loss: 0.00114225
Iteration 14/25 | Loss: 0.00114225
Iteration 15/25 | Loss: 0.00114225
Iteration 16/25 | Loss: 0.00114225
Iteration 17/25 | Loss: 0.00114225
Iteration 18/25 | Loss: 0.00114225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001142249209806323, 0.001142249209806323, 0.001142249209806323, 0.001142249209806323, 0.001142249209806323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001142249209806323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114225
Iteration 2/1000 | Loss: 0.00009679
Iteration 3/1000 | Loss: 0.00004767
Iteration 4/1000 | Loss: 0.00003716
Iteration 5/1000 | Loss: 0.00003222
Iteration 6/1000 | Loss: 0.00002877
Iteration 7/1000 | Loss: 0.00002718
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002578
Iteration 10/1000 | Loss: 0.00002530
Iteration 11/1000 | Loss: 0.00002505
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002453
Iteration 14/1000 | Loss: 0.00002434
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002425
Iteration 18/1000 | Loss: 0.00002423
Iteration 19/1000 | Loss: 0.00002423
Iteration 20/1000 | Loss: 0.00002423
Iteration 21/1000 | Loss: 0.00002423
Iteration 22/1000 | Loss: 0.00002419
Iteration 23/1000 | Loss: 0.00002419
Iteration 24/1000 | Loss: 0.00002419
Iteration 25/1000 | Loss: 0.00002418
Iteration 26/1000 | Loss: 0.00002418
Iteration 27/1000 | Loss: 0.00002418
Iteration 28/1000 | Loss: 0.00002417
Iteration 29/1000 | Loss: 0.00002417
Iteration 30/1000 | Loss: 0.00002416
Iteration 31/1000 | Loss: 0.00002414
Iteration 32/1000 | Loss: 0.00002413
Iteration 33/1000 | Loss: 0.00002413
Iteration 34/1000 | Loss: 0.00002412
Iteration 35/1000 | Loss: 0.00002412
Iteration 36/1000 | Loss: 0.00002411
Iteration 37/1000 | Loss: 0.00002411
Iteration 38/1000 | Loss: 0.00002409
Iteration 39/1000 | Loss: 0.00002407
Iteration 40/1000 | Loss: 0.00002406
Iteration 41/1000 | Loss: 0.00002406
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002405
Iteration 44/1000 | Loss: 0.00002405
Iteration 45/1000 | Loss: 0.00002404
Iteration 46/1000 | Loss: 0.00002404
Iteration 47/1000 | Loss: 0.00002403
Iteration 48/1000 | Loss: 0.00002403
Iteration 49/1000 | Loss: 0.00002402
Iteration 50/1000 | Loss: 0.00002402
Iteration 51/1000 | Loss: 0.00002402
Iteration 52/1000 | Loss: 0.00002402
Iteration 53/1000 | Loss: 0.00002402
Iteration 54/1000 | Loss: 0.00002402
Iteration 55/1000 | Loss: 0.00002402
Iteration 56/1000 | Loss: 0.00002402
Iteration 57/1000 | Loss: 0.00002401
Iteration 58/1000 | Loss: 0.00002401
Iteration 59/1000 | Loss: 0.00002401
Iteration 60/1000 | Loss: 0.00002401
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002400
Iteration 64/1000 | Loss: 0.00002400
Iteration 65/1000 | Loss: 0.00002400
Iteration 66/1000 | Loss: 0.00002400
Iteration 67/1000 | Loss: 0.00002399
Iteration 68/1000 | Loss: 0.00002399
Iteration 69/1000 | Loss: 0.00002399
Iteration 70/1000 | Loss: 0.00002398
Iteration 71/1000 | Loss: 0.00002398
Iteration 72/1000 | Loss: 0.00002398
Iteration 73/1000 | Loss: 0.00002398
Iteration 74/1000 | Loss: 0.00002398
Iteration 75/1000 | Loss: 0.00002397
Iteration 76/1000 | Loss: 0.00002397
Iteration 77/1000 | Loss: 0.00002396
Iteration 78/1000 | Loss: 0.00002396
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002395
Iteration 82/1000 | Loss: 0.00002395
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002395
Iteration 85/1000 | Loss: 0.00002395
Iteration 86/1000 | Loss: 0.00002395
Iteration 87/1000 | Loss: 0.00002394
Iteration 88/1000 | Loss: 0.00002394
Iteration 89/1000 | Loss: 0.00002394
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002393
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002392
Iteration 95/1000 | Loss: 0.00002391
Iteration 96/1000 | Loss: 0.00002391
Iteration 97/1000 | Loss: 0.00002391
Iteration 98/1000 | Loss: 0.00002391
Iteration 99/1000 | Loss: 0.00002391
Iteration 100/1000 | Loss: 0.00002391
Iteration 101/1000 | Loss: 0.00002391
Iteration 102/1000 | Loss: 0.00002390
Iteration 103/1000 | Loss: 0.00002390
Iteration 104/1000 | Loss: 0.00002390
Iteration 105/1000 | Loss: 0.00002390
Iteration 106/1000 | Loss: 0.00002389
Iteration 107/1000 | Loss: 0.00002389
Iteration 108/1000 | Loss: 0.00002389
Iteration 109/1000 | Loss: 0.00002388
Iteration 110/1000 | Loss: 0.00002388
Iteration 111/1000 | Loss: 0.00002388
Iteration 112/1000 | Loss: 0.00002388
Iteration 113/1000 | Loss: 0.00002387
Iteration 114/1000 | Loss: 0.00002387
Iteration 115/1000 | Loss: 0.00002387
Iteration 116/1000 | Loss: 0.00002387
Iteration 117/1000 | Loss: 0.00002387
Iteration 118/1000 | Loss: 0.00002387
Iteration 119/1000 | Loss: 0.00002387
Iteration 120/1000 | Loss: 0.00002387
Iteration 121/1000 | Loss: 0.00002387
Iteration 122/1000 | Loss: 0.00002387
Iteration 123/1000 | Loss: 0.00002387
Iteration 124/1000 | Loss: 0.00002386
Iteration 125/1000 | Loss: 0.00002386
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00002386
Iteration 128/1000 | Loss: 0.00002386
Iteration 129/1000 | Loss: 0.00002386
Iteration 130/1000 | Loss: 0.00002386
Iteration 131/1000 | Loss: 0.00002386
Iteration 132/1000 | Loss: 0.00002385
Iteration 133/1000 | Loss: 0.00002385
Iteration 134/1000 | Loss: 0.00002385
Iteration 135/1000 | Loss: 0.00002385
Iteration 136/1000 | Loss: 0.00002385
Iteration 137/1000 | Loss: 0.00002385
Iteration 138/1000 | Loss: 0.00002385
Iteration 139/1000 | Loss: 0.00002385
Iteration 140/1000 | Loss: 0.00002385
Iteration 141/1000 | Loss: 0.00002385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3851942387409508e-05, 2.3851942387409508e-05, 2.3851942387409508e-05, 2.3851942387409508e-05, 2.3851942387409508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3851942387409508e-05

Optimization complete. Final v2v error: 4.013345241546631 mm

Highest mean error: 5.953891754150391 mm for frame 75

Lowest mean error: 3.431915044784546 mm for frame 111

Saving results

Total time: 42.23336338996887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879918
Iteration 2/25 | Loss: 0.00162310
Iteration 3/25 | Loss: 0.00145928
Iteration 4/25 | Loss: 0.00143697
Iteration 5/25 | Loss: 0.00143229
Iteration 6/25 | Loss: 0.00143165
Iteration 7/25 | Loss: 0.00143165
Iteration 8/25 | Loss: 0.00143165
Iteration 9/25 | Loss: 0.00143165
Iteration 10/25 | Loss: 0.00143165
Iteration 11/25 | Loss: 0.00143165
Iteration 12/25 | Loss: 0.00143165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014316492015495896, 0.0014316492015495896, 0.0014316492015495896, 0.0014316492015495896, 0.0014316492015495896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014316492015495896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.59579158
Iteration 2/25 | Loss: 0.00121111
Iteration 3/25 | Loss: 0.00121111
Iteration 4/25 | Loss: 0.00121111
Iteration 5/25 | Loss: 0.00121111
Iteration 6/25 | Loss: 0.00121111
Iteration 7/25 | Loss: 0.00121111
Iteration 8/25 | Loss: 0.00121111
Iteration 9/25 | Loss: 0.00121111
Iteration 10/25 | Loss: 0.00121111
Iteration 11/25 | Loss: 0.00121111
Iteration 12/25 | Loss: 0.00121111
Iteration 13/25 | Loss: 0.00121111
Iteration 14/25 | Loss: 0.00121111
Iteration 15/25 | Loss: 0.00121111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012111096875742078, 0.0012111096875742078, 0.0012111096875742078, 0.0012111096875742078, 0.0012111096875742078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012111096875742078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121111
Iteration 2/1000 | Loss: 0.00005564
Iteration 3/1000 | Loss: 0.00004123
Iteration 4/1000 | Loss: 0.00003560
Iteration 5/1000 | Loss: 0.00003148
Iteration 6/1000 | Loss: 0.00002929
Iteration 7/1000 | Loss: 0.00002784
Iteration 8/1000 | Loss: 0.00002713
Iteration 9/1000 | Loss: 0.00002642
Iteration 10/1000 | Loss: 0.00002582
Iteration 11/1000 | Loss: 0.00002529
Iteration 12/1000 | Loss: 0.00002482
Iteration 13/1000 | Loss: 0.00002447
Iteration 14/1000 | Loss: 0.00002427
Iteration 15/1000 | Loss: 0.00002412
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002410
Iteration 18/1000 | Loss: 0.00002410
Iteration 19/1000 | Loss: 0.00002410
Iteration 20/1000 | Loss: 0.00002404
Iteration 21/1000 | Loss: 0.00002398
Iteration 22/1000 | Loss: 0.00002398
Iteration 23/1000 | Loss: 0.00002398
Iteration 24/1000 | Loss: 0.00002397
Iteration 25/1000 | Loss: 0.00002396
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002395
Iteration 30/1000 | Loss: 0.00002395
Iteration 31/1000 | Loss: 0.00002395
Iteration 32/1000 | Loss: 0.00002395
Iteration 33/1000 | Loss: 0.00002394
Iteration 34/1000 | Loss: 0.00002394
Iteration 35/1000 | Loss: 0.00002394
Iteration 36/1000 | Loss: 0.00002394
Iteration 37/1000 | Loss: 0.00002394
Iteration 38/1000 | Loss: 0.00002394
Iteration 39/1000 | Loss: 0.00002394
Iteration 40/1000 | Loss: 0.00002394
Iteration 41/1000 | Loss: 0.00002394
Iteration 42/1000 | Loss: 0.00002394
Iteration 43/1000 | Loss: 0.00002394
Iteration 44/1000 | Loss: 0.00002393
Iteration 45/1000 | Loss: 0.00002393
Iteration 46/1000 | Loss: 0.00002393
Iteration 47/1000 | Loss: 0.00002392
Iteration 48/1000 | Loss: 0.00002391
Iteration 49/1000 | Loss: 0.00002391
Iteration 50/1000 | Loss: 0.00002390
Iteration 51/1000 | Loss: 0.00002390
Iteration 52/1000 | Loss: 0.00002389
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002388
Iteration 56/1000 | Loss: 0.00002388
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002387
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002387
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002385
Iteration 69/1000 | Loss: 0.00002385
Iteration 70/1000 | Loss: 0.00002385
Iteration 71/1000 | Loss: 0.00002385
Iteration 72/1000 | Loss: 0.00002385
Iteration 73/1000 | Loss: 0.00002384
Iteration 74/1000 | Loss: 0.00002383
Iteration 75/1000 | Loss: 0.00002383
Iteration 76/1000 | Loss: 0.00002383
Iteration 77/1000 | Loss: 0.00002383
Iteration 78/1000 | Loss: 0.00002383
Iteration 79/1000 | Loss: 0.00002383
Iteration 80/1000 | Loss: 0.00002383
Iteration 81/1000 | Loss: 0.00002382
Iteration 82/1000 | Loss: 0.00002382
Iteration 83/1000 | Loss: 0.00002382
Iteration 84/1000 | Loss: 0.00002382
Iteration 85/1000 | Loss: 0.00002381
Iteration 86/1000 | Loss: 0.00002381
Iteration 87/1000 | Loss: 0.00002381
Iteration 88/1000 | Loss: 0.00002381
Iteration 89/1000 | Loss: 0.00002380
Iteration 90/1000 | Loss: 0.00002380
Iteration 91/1000 | Loss: 0.00002380
Iteration 92/1000 | Loss: 0.00002379
Iteration 93/1000 | Loss: 0.00002379
Iteration 94/1000 | Loss: 0.00002379
Iteration 95/1000 | Loss: 0.00002379
Iteration 96/1000 | Loss: 0.00002379
Iteration 97/1000 | Loss: 0.00002379
Iteration 98/1000 | Loss: 0.00002378
Iteration 99/1000 | Loss: 0.00002378
Iteration 100/1000 | Loss: 0.00002378
Iteration 101/1000 | Loss: 0.00002378
Iteration 102/1000 | Loss: 0.00002378
Iteration 103/1000 | Loss: 0.00002378
Iteration 104/1000 | Loss: 0.00002378
Iteration 105/1000 | Loss: 0.00002377
Iteration 106/1000 | Loss: 0.00002377
Iteration 107/1000 | Loss: 0.00002377
Iteration 108/1000 | Loss: 0.00002377
Iteration 109/1000 | Loss: 0.00002377
Iteration 110/1000 | Loss: 0.00002377
Iteration 111/1000 | Loss: 0.00002376
Iteration 112/1000 | Loss: 0.00002376
Iteration 113/1000 | Loss: 0.00002376
Iteration 114/1000 | Loss: 0.00002376
Iteration 115/1000 | Loss: 0.00002376
Iteration 116/1000 | Loss: 0.00002376
Iteration 117/1000 | Loss: 0.00002376
Iteration 118/1000 | Loss: 0.00002376
Iteration 119/1000 | Loss: 0.00002376
Iteration 120/1000 | Loss: 0.00002376
Iteration 121/1000 | Loss: 0.00002376
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002375
Iteration 126/1000 | Loss: 0.00002375
Iteration 127/1000 | Loss: 0.00002375
Iteration 128/1000 | Loss: 0.00002375
Iteration 129/1000 | Loss: 0.00002374
Iteration 130/1000 | Loss: 0.00002374
Iteration 131/1000 | Loss: 0.00002374
Iteration 132/1000 | Loss: 0.00002373
Iteration 133/1000 | Loss: 0.00002373
Iteration 134/1000 | Loss: 0.00002373
Iteration 135/1000 | Loss: 0.00002372
Iteration 136/1000 | Loss: 0.00002372
Iteration 137/1000 | Loss: 0.00002372
Iteration 138/1000 | Loss: 0.00002372
Iteration 139/1000 | Loss: 0.00002372
Iteration 140/1000 | Loss: 0.00002372
Iteration 141/1000 | Loss: 0.00002372
Iteration 142/1000 | Loss: 0.00002372
Iteration 143/1000 | Loss: 0.00002372
Iteration 144/1000 | Loss: 0.00002372
Iteration 145/1000 | Loss: 0.00002371
Iteration 146/1000 | Loss: 0.00002371
Iteration 147/1000 | Loss: 0.00002371
Iteration 148/1000 | Loss: 0.00002371
Iteration 149/1000 | Loss: 0.00002371
Iteration 150/1000 | Loss: 0.00002371
Iteration 151/1000 | Loss: 0.00002371
Iteration 152/1000 | Loss: 0.00002371
Iteration 153/1000 | Loss: 0.00002371
Iteration 154/1000 | Loss: 0.00002371
Iteration 155/1000 | Loss: 0.00002371
Iteration 156/1000 | Loss: 0.00002371
Iteration 157/1000 | Loss: 0.00002370
Iteration 158/1000 | Loss: 0.00002370
Iteration 159/1000 | Loss: 0.00002370
Iteration 160/1000 | Loss: 0.00002370
Iteration 161/1000 | Loss: 0.00002370
Iteration 162/1000 | Loss: 0.00002370
Iteration 163/1000 | Loss: 0.00002370
Iteration 164/1000 | Loss: 0.00002370
Iteration 165/1000 | Loss: 0.00002370
Iteration 166/1000 | Loss: 0.00002370
Iteration 167/1000 | Loss: 0.00002370
Iteration 168/1000 | Loss: 0.00002370
Iteration 169/1000 | Loss: 0.00002370
Iteration 170/1000 | Loss: 0.00002370
Iteration 171/1000 | Loss: 0.00002370
Iteration 172/1000 | Loss: 0.00002370
Iteration 173/1000 | Loss: 0.00002369
Iteration 174/1000 | Loss: 0.00002369
Iteration 175/1000 | Loss: 0.00002369
Iteration 176/1000 | Loss: 0.00002369
Iteration 177/1000 | Loss: 0.00002369
Iteration 178/1000 | Loss: 0.00002369
Iteration 179/1000 | Loss: 0.00002369
Iteration 180/1000 | Loss: 0.00002369
Iteration 181/1000 | Loss: 0.00002369
Iteration 182/1000 | Loss: 0.00002369
Iteration 183/1000 | Loss: 0.00002369
Iteration 184/1000 | Loss: 0.00002369
Iteration 185/1000 | Loss: 0.00002369
Iteration 186/1000 | Loss: 0.00002369
Iteration 187/1000 | Loss: 0.00002369
Iteration 188/1000 | Loss: 0.00002369
Iteration 189/1000 | Loss: 0.00002368
Iteration 190/1000 | Loss: 0.00002368
Iteration 191/1000 | Loss: 0.00002368
Iteration 192/1000 | Loss: 0.00002368
Iteration 193/1000 | Loss: 0.00002368
Iteration 194/1000 | Loss: 0.00002368
Iteration 195/1000 | Loss: 0.00002368
Iteration 196/1000 | Loss: 0.00002368
Iteration 197/1000 | Loss: 0.00002368
Iteration 198/1000 | Loss: 0.00002368
Iteration 199/1000 | Loss: 0.00002368
Iteration 200/1000 | Loss: 0.00002368
Iteration 201/1000 | Loss: 0.00002368
Iteration 202/1000 | Loss: 0.00002368
Iteration 203/1000 | Loss: 0.00002368
Iteration 204/1000 | Loss: 0.00002368
Iteration 205/1000 | Loss: 0.00002368
Iteration 206/1000 | Loss: 0.00002368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.368130662944168e-05, 2.368130662944168e-05, 2.368130662944168e-05, 2.368130662944168e-05, 2.368130662944168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.368130662944168e-05

Optimization complete. Final v2v error: 4.072280406951904 mm

Highest mean error: 4.684424877166748 mm for frame 130

Lowest mean error: 3.5813539028167725 mm for frame 73

Saving results

Total time: 49.92207360267639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886461
Iteration 2/25 | Loss: 0.00224030
Iteration 3/25 | Loss: 0.00154072
Iteration 4/25 | Loss: 0.00144942
Iteration 5/25 | Loss: 0.00143688
Iteration 6/25 | Loss: 0.00143558
Iteration 7/25 | Loss: 0.00143558
Iteration 8/25 | Loss: 0.00143558
Iteration 9/25 | Loss: 0.00143558
Iteration 10/25 | Loss: 0.00143558
Iteration 11/25 | Loss: 0.00143558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014355811290442944, 0.0014355811290442944, 0.0014355811290442944, 0.0014355811290442944, 0.0014355811290442944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014355811290442944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25280797
Iteration 2/25 | Loss: 0.00085745
Iteration 3/25 | Loss: 0.00085744
Iteration 4/25 | Loss: 0.00085744
Iteration 5/25 | Loss: 0.00085744
Iteration 6/25 | Loss: 0.00085744
Iteration 7/25 | Loss: 0.00085744
Iteration 8/25 | Loss: 0.00085744
Iteration 9/25 | Loss: 0.00085744
Iteration 10/25 | Loss: 0.00085744
Iteration 11/25 | Loss: 0.00085744
Iteration 12/25 | Loss: 0.00085744
Iteration 13/25 | Loss: 0.00085744
Iteration 14/25 | Loss: 0.00085744
Iteration 15/25 | Loss: 0.00085744
Iteration 16/25 | Loss: 0.00085744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008574422099627554, 0.0008574422099627554, 0.0008574422099627554, 0.0008574422099627554, 0.0008574422099627554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008574422099627554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085744
Iteration 2/1000 | Loss: 0.00005577
Iteration 3/1000 | Loss: 0.00004069
Iteration 4/1000 | Loss: 0.00003521
Iteration 5/1000 | Loss: 0.00003262
Iteration 6/1000 | Loss: 0.00003072
Iteration 7/1000 | Loss: 0.00002957
Iteration 8/1000 | Loss: 0.00002922
Iteration 9/1000 | Loss: 0.00002878
Iteration 10/1000 | Loss: 0.00002847
Iteration 11/1000 | Loss: 0.00002817
Iteration 12/1000 | Loss: 0.00002797
Iteration 13/1000 | Loss: 0.00002784
Iteration 14/1000 | Loss: 0.00002783
Iteration 15/1000 | Loss: 0.00002768
Iteration 16/1000 | Loss: 0.00002754
Iteration 17/1000 | Loss: 0.00002751
Iteration 18/1000 | Loss: 0.00002743
Iteration 19/1000 | Loss: 0.00002740
Iteration 20/1000 | Loss: 0.00002739
Iteration 21/1000 | Loss: 0.00002739
Iteration 22/1000 | Loss: 0.00002736
Iteration 23/1000 | Loss: 0.00002736
Iteration 24/1000 | Loss: 0.00002736
Iteration 25/1000 | Loss: 0.00002735
Iteration 26/1000 | Loss: 0.00002735
Iteration 27/1000 | Loss: 0.00002732
Iteration 28/1000 | Loss: 0.00002731
Iteration 29/1000 | Loss: 0.00002731
Iteration 30/1000 | Loss: 0.00002731
Iteration 31/1000 | Loss: 0.00002731
Iteration 32/1000 | Loss: 0.00002731
Iteration 33/1000 | Loss: 0.00002731
Iteration 34/1000 | Loss: 0.00002731
Iteration 35/1000 | Loss: 0.00002731
Iteration 36/1000 | Loss: 0.00002731
Iteration 37/1000 | Loss: 0.00002731
Iteration 38/1000 | Loss: 0.00002731
Iteration 39/1000 | Loss: 0.00002731
Iteration 40/1000 | Loss: 0.00002731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [2.731099448283203e-05, 2.731099448283203e-05, 2.731099448283203e-05, 2.731099448283203e-05, 2.731099448283203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.731099448283203e-05

Optimization complete. Final v2v error: 4.2483062744140625 mm

Highest mean error: 5.485558986663818 mm for frame 56

Lowest mean error: 3.525437355041504 mm for frame 101

Saving results

Total time: 38.88624548912048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432249
Iteration 2/25 | Loss: 0.00147970
Iteration 3/25 | Loss: 0.00136130
Iteration 4/25 | Loss: 0.00134767
Iteration 5/25 | Loss: 0.00134452
Iteration 6/25 | Loss: 0.00134381
Iteration 7/25 | Loss: 0.00134381
Iteration 8/25 | Loss: 0.00134381
Iteration 9/25 | Loss: 0.00134381
Iteration 10/25 | Loss: 0.00134381
Iteration 11/25 | Loss: 0.00134381
Iteration 12/25 | Loss: 0.00134381
Iteration 13/25 | Loss: 0.00134381
Iteration 14/25 | Loss: 0.00134381
Iteration 15/25 | Loss: 0.00134381
Iteration 16/25 | Loss: 0.00134381
Iteration 17/25 | Loss: 0.00134381
Iteration 18/25 | Loss: 0.00134381
Iteration 19/25 | Loss: 0.00134381
Iteration 20/25 | Loss: 0.00134381
Iteration 21/25 | Loss: 0.00134381
Iteration 22/25 | Loss: 0.00134381
Iteration 23/25 | Loss: 0.00134381
Iteration 24/25 | Loss: 0.00134381
Iteration 25/25 | Loss: 0.00134381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30341816
Iteration 2/25 | Loss: 0.00101361
Iteration 3/25 | Loss: 0.00101361
Iteration 4/25 | Loss: 0.00101361
Iteration 5/25 | Loss: 0.00101361
Iteration 6/25 | Loss: 0.00101361
Iteration 7/25 | Loss: 0.00101361
Iteration 8/25 | Loss: 0.00101361
Iteration 9/25 | Loss: 0.00101361
Iteration 10/25 | Loss: 0.00101361
Iteration 11/25 | Loss: 0.00101361
Iteration 12/25 | Loss: 0.00101361
Iteration 13/25 | Loss: 0.00101361
Iteration 14/25 | Loss: 0.00101361
Iteration 15/25 | Loss: 0.00101361
Iteration 16/25 | Loss: 0.00101361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001013610977679491, 0.001013610977679491, 0.001013610977679491, 0.001013610977679491, 0.001013610977679491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001013610977679491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101361
Iteration 2/1000 | Loss: 0.00004897
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002824
Iteration 5/1000 | Loss: 0.00002447
Iteration 6/1000 | Loss: 0.00002259
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002085
Iteration 9/1000 | Loss: 0.00002041
Iteration 10/1000 | Loss: 0.00002008
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001977
Iteration 13/1000 | Loss: 0.00001975
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001971
Iteration 16/1000 | Loss: 0.00001970
Iteration 17/1000 | Loss: 0.00001969
Iteration 18/1000 | Loss: 0.00001967
Iteration 19/1000 | Loss: 0.00001967
Iteration 20/1000 | Loss: 0.00001967
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001967
Iteration 23/1000 | Loss: 0.00001966
Iteration 24/1000 | Loss: 0.00001966
Iteration 25/1000 | Loss: 0.00001966
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001966
Iteration 29/1000 | Loss: 0.00001966
Iteration 30/1000 | Loss: 0.00001966
Iteration 31/1000 | Loss: 0.00001966
Iteration 32/1000 | Loss: 0.00001966
Iteration 33/1000 | Loss: 0.00001966
Iteration 34/1000 | Loss: 0.00001966
Iteration 35/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 35. Stopping optimization.
Last 5 losses: [1.9662233171402477e-05, 1.9662233171402477e-05, 1.9662233171402477e-05, 1.9662233171402477e-05, 1.9662233171402477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9662233171402477e-05

Optimization complete. Final v2v error: 3.796090841293335 mm

Highest mean error: 4.153001308441162 mm for frame 22

Lowest mean error: 3.4547812938690186 mm for frame 1

Saving results

Total time: 26.518611192703247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435156
Iteration 2/25 | Loss: 0.00146633
Iteration 3/25 | Loss: 0.00136240
Iteration 4/25 | Loss: 0.00134645
Iteration 5/25 | Loss: 0.00134079
Iteration 6/25 | Loss: 0.00133917
Iteration 7/25 | Loss: 0.00133917
Iteration 8/25 | Loss: 0.00133917
Iteration 9/25 | Loss: 0.00133917
Iteration 10/25 | Loss: 0.00133917
Iteration 11/25 | Loss: 0.00133917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013391660759225488, 0.0013391660759225488, 0.0013391660759225488, 0.0013391660759225488, 0.0013391660759225488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013391660759225488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69352221
Iteration 2/25 | Loss: 0.00112393
Iteration 3/25 | Loss: 0.00112392
Iteration 4/25 | Loss: 0.00112392
Iteration 5/25 | Loss: 0.00112392
Iteration 6/25 | Loss: 0.00112392
Iteration 7/25 | Loss: 0.00112392
Iteration 8/25 | Loss: 0.00112392
Iteration 9/25 | Loss: 0.00112392
Iteration 10/25 | Loss: 0.00112392
Iteration 11/25 | Loss: 0.00112392
Iteration 12/25 | Loss: 0.00112392
Iteration 13/25 | Loss: 0.00112392
Iteration 14/25 | Loss: 0.00112392
Iteration 15/25 | Loss: 0.00112392
Iteration 16/25 | Loss: 0.00112392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011239218292757869, 0.0011239218292757869, 0.0011239218292757869, 0.0011239218292757869, 0.0011239218292757869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011239218292757869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112392
Iteration 2/1000 | Loss: 0.00003979
Iteration 3/1000 | Loss: 0.00002954
Iteration 4/1000 | Loss: 0.00002428
Iteration 5/1000 | Loss: 0.00002207
Iteration 6/1000 | Loss: 0.00002096
Iteration 7/1000 | Loss: 0.00002060
Iteration 8/1000 | Loss: 0.00002012
Iteration 9/1000 | Loss: 0.00001983
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001961
Iteration 12/1000 | Loss: 0.00001955
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001946
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001943
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001941
Iteration 23/1000 | Loss: 0.00001941
Iteration 24/1000 | Loss: 0.00001940
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001939
Iteration 27/1000 | Loss: 0.00001939
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001938
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001937
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001936
Iteration 40/1000 | Loss: 0.00001936
Iteration 41/1000 | Loss: 0.00001935
Iteration 42/1000 | Loss: 0.00001935
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001935
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001934
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001934
Iteration 49/1000 | Loss: 0.00001933
Iteration 50/1000 | Loss: 0.00001933
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001932
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001931
Iteration 57/1000 | Loss: 0.00001931
Iteration 58/1000 | Loss: 0.00001930
Iteration 59/1000 | Loss: 0.00001930
Iteration 60/1000 | Loss: 0.00001930
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00001929
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001929
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001928
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001928
Iteration 76/1000 | Loss: 0.00001928
Iteration 77/1000 | Loss: 0.00001928
Iteration 78/1000 | Loss: 0.00001928
Iteration 79/1000 | Loss: 0.00001928
Iteration 80/1000 | Loss: 0.00001928
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001927
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001925
Iteration 117/1000 | Loss: 0.00001925
Iteration 118/1000 | Loss: 0.00001925
Iteration 119/1000 | Loss: 0.00001925
Iteration 120/1000 | Loss: 0.00001925
Iteration 121/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.9254783182987012e-05, 1.9254783182987012e-05, 1.9254783182987012e-05, 1.9254783182987012e-05, 1.9254783182987012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9254783182987012e-05

Optimization complete. Final v2v error: 3.766965627670288 mm

Highest mean error: 3.9898674488067627 mm for frame 148

Lowest mean error: 3.5824968814849854 mm for frame 18

Saving results

Total time: 34.815908908843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061240
Iteration 2/25 | Loss: 0.00174572
Iteration 3/25 | Loss: 0.00147403
Iteration 4/25 | Loss: 0.00144739
Iteration 5/25 | Loss: 0.00144071
Iteration 6/25 | Loss: 0.00143792
Iteration 7/25 | Loss: 0.00143698
Iteration 8/25 | Loss: 0.00143698
Iteration 9/25 | Loss: 0.00143698
Iteration 10/25 | Loss: 0.00143698
Iteration 11/25 | Loss: 0.00143697
Iteration 12/25 | Loss: 0.00143697
Iteration 13/25 | Loss: 0.00143697
Iteration 14/25 | Loss: 0.00143697
Iteration 15/25 | Loss: 0.00143697
Iteration 16/25 | Loss: 0.00143697
Iteration 17/25 | Loss: 0.00143697
Iteration 18/25 | Loss: 0.00143697
Iteration 19/25 | Loss: 0.00143697
Iteration 20/25 | Loss: 0.00143697
Iteration 21/25 | Loss: 0.00143697
Iteration 22/25 | Loss: 0.00143697
Iteration 23/25 | Loss: 0.00143697
Iteration 24/25 | Loss: 0.00143697
Iteration 25/25 | Loss: 0.00143697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28299236
Iteration 2/25 | Loss: 0.00112619
Iteration 3/25 | Loss: 0.00112619
Iteration 4/25 | Loss: 0.00112619
Iteration 5/25 | Loss: 0.00112619
Iteration 6/25 | Loss: 0.00112619
Iteration 7/25 | Loss: 0.00112619
Iteration 8/25 | Loss: 0.00112619
Iteration 9/25 | Loss: 0.00112619
Iteration 10/25 | Loss: 0.00112619
Iteration 11/25 | Loss: 0.00112619
Iteration 12/25 | Loss: 0.00112619
Iteration 13/25 | Loss: 0.00112619
Iteration 14/25 | Loss: 0.00112619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001126186572946608, 0.001126186572946608, 0.001126186572946608, 0.001126186572946608, 0.001126186572946608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001126186572946608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112619
Iteration 2/1000 | Loss: 0.00005795
Iteration 3/1000 | Loss: 0.00003900
Iteration 4/1000 | Loss: 0.00003233
Iteration 5/1000 | Loss: 0.00002913
Iteration 6/1000 | Loss: 0.00002775
Iteration 7/1000 | Loss: 0.00002684
Iteration 8/1000 | Loss: 0.00002609
Iteration 9/1000 | Loss: 0.00002550
Iteration 10/1000 | Loss: 0.00002524
Iteration 11/1000 | Loss: 0.00002503
Iteration 12/1000 | Loss: 0.00002497
Iteration 13/1000 | Loss: 0.00002496
Iteration 14/1000 | Loss: 0.00002489
Iteration 15/1000 | Loss: 0.00002482
Iteration 16/1000 | Loss: 0.00002482
Iteration 17/1000 | Loss: 0.00002481
Iteration 18/1000 | Loss: 0.00002481
Iteration 19/1000 | Loss: 0.00002481
Iteration 20/1000 | Loss: 0.00002481
Iteration 21/1000 | Loss: 0.00002481
Iteration 22/1000 | Loss: 0.00002481
Iteration 23/1000 | Loss: 0.00002481
Iteration 24/1000 | Loss: 0.00002481
Iteration 25/1000 | Loss: 0.00002481
Iteration 26/1000 | Loss: 0.00002480
Iteration 27/1000 | Loss: 0.00002480
Iteration 28/1000 | Loss: 0.00002479
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002478
Iteration 31/1000 | Loss: 0.00002478
Iteration 32/1000 | Loss: 0.00002478
Iteration 33/1000 | Loss: 0.00002478
Iteration 34/1000 | Loss: 0.00002478
Iteration 35/1000 | Loss: 0.00002478
Iteration 36/1000 | Loss: 0.00002478
Iteration 37/1000 | Loss: 0.00002478
Iteration 38/1000 | Loss: 0.00002478
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002477
Iteration 42/1000 | Loss: 0.00002477
Iteration 43/1000 | Loss: 0.00002477
Iteration 44/1000 | Loss: 0.00002477
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002477
Iteration 47/1000 | Loss: 0.00002477
Iteration 48/1000 | Loss: 0.00002477
Iteration 49/1000 | Loss: 0.00002476
Iteration 50/1000 | Loss: 0.00002476
Iteration 51/1000 | Loss: 0.00002476
Iteration 52/1000 | Loss: 0.00002476
Iteration 53/1000 | Loss: 0.00002476
Iteration 54/1000 | Loss: 0.00002476
Iteration 55/1000 | Loss: 0.00002476
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002476
Iteration 58/1000 | Loss: 0.00002475
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002475
Iteration 61/1000 | Loss: 0.00002475
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002474
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002474
Iteration 66/1000 | Loss: 0.00002474
Iteration 67/1000 | Loss: 0.00002474
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002473
Iteration 74/1000 | Loss: 0.00002473
Iteration 75/1000 | Loss: 0.00002473
Iteration 76/1000 | Loss: 0.00002473
Iteration 77/1000 | Loss: 0.00002473
Iteration 78/1000 | Loss: 0.00002473
Iteration 79/1000 | Loss: 0.00002473
Iteration 80/1000 | Loss: 0.00002473
Iteration 81/1000 | Loss: 0.00002472
Iteration 82/1000 | Loss: 0.00002472
Iteration 83/1000 | Loss: 0.00002472
Iteration 84/1000 | Loss: 0.00002472
Iteration 85/1000 | Loss: 0.00002472
Iteration 86/1000 | Loss: 0.00002472
Iteration 87/1000 | Loss: 0.00002472
Iteration 88/1000 | Loss: 0.00002472
Iteration 89/1000 | Loss: 0.00002472
Iteration 90/1000 | Loss: 0.00002472
Iteration 91/1000 | Loss: 0.00002472
Iteration 92/1000 | Loss: 0.00002472
Iteration 93/1000 | Loss: 0.00002472
Iteration 94/1000 | Loss: 0.00002472
Iteration 95/1000 | Loss: 0.00002472
Iteration 96/1000 | Loss: 0.00002472
Iteration 97/1000 | Loss: 0.00002472
Iteration 98/1000 | Loss: 0.00002471
Iteration 99/1000 | Loss: 0.00002471
Iteration 100/1000 | Loss: 0.00002471
Iteration 101/1000 | Loss: 0.00002471
Iteration 102/1000 | Loss: 0.00002471
Iteration 103/1000 | Loss: 0.00002471
Iteration 104/1000 | Loss: 0.00002471
Iteration 105/1000 | Loss: 0.00002471
Iteration 106/1000 | Loss: 0.00002471
Iteration 107/1000 | Loss: 0.00002471
Iteration 108/1000 | Loss: 0.00002471
Iteration 109/1000 | Loss: 0.00002471
Iteration 110/1000 | Loss: 0.00002471
Iteration 111/1000 | Loss: 0.00002471
Iteration 112/1000 | Loss: 0.00002471
Iteration 113/1000 | Loss: 0.00002471
Iteration 114/1000 | Loss: 0.00002471
Iteration 115/1000 | Loss: 0.00002471
Iteration 116/1000 | Loss: 0.00002471
Iteration 117/1000 | Loss: 0.00002471
Iteration 118/1000 | Loss: 0.00002471
Iteration 119/1000 | Loss: 0.00002471
Iteration 120/1000 | Loss: 0.00002471
Iteration 121/1000 | Loss: 0.00002471
Iteration 122/1000 | Loss: 0.00002471
Iteration 123/1000 | Loss: 0.00002471
Iteration 124/1000 | Loss: 0.00002471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.4711624064366333e-05, 2.4711624064366333e-05, 2.4711624064366333e-05, 2.4711624064366333e-05, 2.4711624064366333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4711624064366333e-05

Optimization complete. Final v2v error: 4.222621917724609 mm

Highest mean error: 4.347904682159424 mm for frame 116

Lowest mean error: 4.063397407531738 mm for frame 3

Saving results

Total time: 34.65418601036072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5213/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5213/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871171
Iteration 2/25 | Loss: 0.00150260
Iteration 3/25 | Loss: 0.00137985
Iteration 4/25 | Loss: 0.00136919
Iteration 5/25 | Loss: 0.00136532
Iteration 6/25 | Loss: 0.00136416
Iteration 7/25 | Loss: 0.00136410
Iteration 8/25 | Loss: 0.00136410
Iteration 9/25 | Loss: 0.00136410
Iteration 10/25 | Loss: 0.00136410
Iteration 11/25 | Loss: 0.00136410
Iteration 12/25 | Loss: 0.00136410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013641023542732, 0.0013641023542732, 0.0013641023542732, 0.0013641023542732, 0.0013641023542732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013641023542732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38021469
Iteration 2/25 | Loss: 0.00098277
Iteration 3/25 | Loss: 0.00098277
Iteration 4/25 | Loss: 0.00098277
Iteration 5/25 | Loss: 0.00098277
Iteration 6/25 | Loss: 0.00098277
Iteration 7/25 | Loss: 0.00098277
Iteration 8/25 | Loss: 0.00098277
Iteration 9/25 | Loss: 0.00098277
Iteration 10/25 | Loss: 0.00098277
Iteration 11/25 | Loss: 0.00098277
Iteration 12/25 | Loss: 0.00098277
Iteration 13/25 | Loss: 0.00098277
Iteration 14/25 | Loss: 0.00098277
Iteration 15/25 | Loss: 0.00098277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009827654575929046, 0.0009827654575929046, 0.0009827654575929046, 0.0009827654575929046, 0.0009827654575929046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009827654575929046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098277
Iteration 2/1000 | Loss: 0.00005170
Iteration 3/1000 | Loss: 0.00003435
Iteration 4/1000 | Loss: 0.00002855
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002382
Iteration 7/1000 | Loss: 0.00002301
Iteration 8/1000 | Loss: 0.00002249
Iteration 9/1000 | Loss: 0.00002209
Iteration 10/1000 | Loss: 0.00002179
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002156
Iteration 13/1000 | Loss: 0.00002142
Iteration 14/1000 | Loss: 0.00002136
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002135
Iteration 17/1000 | Loss: 0.00002132
Iteration 18/1000 | Loss: 0.00002131
Iteration 19/1000 | Loss: 0.00002131
Iteration 20/1000 | Loss: 0.00002130
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002127
Iteration 25/1000 | Loss: 0.00002127
Iteration 26/1000 | Loss: 0.00002126
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002126
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00002125
Iteration 31/1000 | Loss: 0.00002125
Iteration 32/1000 | Loss: 0.00002125
Iteration 33/1000 | Loss: 0.00002124
Iteration 34/1000 | Loss: 0.00002124
Iteration 35/1000 | Loss: 0.00002124
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002123
Iteration 38/1000 | Loss: 0.00002123
Iteration 39/1000 | Loss: 0.00002123
Iteration 40/1000 | Loss: 0.00002122
Iteration 41/1000 | Loss: 0.00002122
Iteration 42/1000 | Loss: 0.00002122
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002121
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00002118
Iteration 58/1000 | Loss: 0.00002118
Iteration 59/1000 | Loss: 0.00002118
Iteration 60/1000 | Loss: 0.00002118
Iteration 61/1000 | Loss: 0.00002118
Iteration 62/1000 | Loss: 0.00002118
Iteration 63/1000 | Loss: 0.00002118
Iteration 64/1000 | Loss: 0.00002118
Iteration 65/1000 | Loss: 0.00002118
Iteration 66/1000 | Loss: 0.00002118
Iteration 67/1000 | Loss: 0.00002118
Iteration 68/1000 | Loss: 0.00002118
Iteration 69/1000 | Loss: 0.00002118
Iteration 70/1000 | Loss: 0.00002118
Iteration 71/1000 | Loss: 0.00002118
Iteration 72/1000 | Loss: 0.00002118
Iteration 73/1000 | Loss: 0.00002118
Iteration 74/1000 | Loss: 0.00002118
Iteration 75/1000 | Loss: 0.00002118
Iteration 76/1000 | Loss: 0.00002118
Iteration 77/1000 | Loss: 0.00002118
Iteration 78/1000 | Loss: 0.00002118
Iteration 79/1000 | Loss: 0.00002118
Iteration 80/1000 | Loss: 0.00002118
Iteration 81/1000 | Loss: 0.00002118
Iteration 82/1000 | Loss: 0.00002118
Iteration 83/1000 | Loss: 0.00002118
Iteration 84/1000 | Loss: 0.00002118
Iteration 85/1000 | Loss: 0.00002118
Iteration 86/1000 | Loss: 0.00002118
Iteration 87/1000 | Loss: 0.00002118
Iteration 88/1000 | Loss: 0.00002118
Iteration 89/1000 | Loss: 0.00002118
Iteration 90/1000 | Loss: 0.00002118
Iteration 91/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.117858821293339e-05, 2.117858821293339e-05, 2.117858821293339e-05, 2.117858821293339e-05, 2.117858821293339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.117858821293339e-05

Optimization complete. Final v2v error: 3.9824512004852295 mm

Highest mean error: 4.637636184692383 mm for frame 59

Lowest mean error: 3.521066188812256 mm for frame 68

Saving results

Total time: 30.570957899093628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623141
Iteration 2/25 | Loss: 0.00091199
Iteration 3/25 | Loss: 0.00074221
Iteration 4/25 | Loss: 0.00071725
Iteration 5/25 | Loss: 0.00070794
Iteration 6/25 | Loss: 0.00070527
Iteration 7/25 | Loss: 0.00070447
Iteration 8/25 | Loss: 0.00070437
Iteration 9/25 | Loss: 0.00070437
Iteration 10/25 | Loss: 0.00070437
Iteration 11/25 | Loss: 0.00070437
Iteration 12/25 | Loss: 0.00070437
Iteration 13/25 | Loss: 0.00070437
Iteration 14/25 | Loss: 0.00070437
Iteration 15/25 | Loss: 0.00070437
Iteration 16/25 | Loss: 0.00070437
Iteration 17/25 | Loss: 0.00070437
Iteration 18/25 | Loss: 0.00070437
Iteration 19/25 | Loss: 0.00070437
Iteration 20/25 | Loss: 0.00070437
Iteration 21/25 | Loss: 0.00070437
Iteration 22/25 | Loss: 0.00070437
Iteration 23/25 | Loss: 0.00070437
Iteration 24/25 | Loss: 0.00070437
Iteration 25/25 | Loss: 0.00070437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.54471970
Iteration 2/25 | Loss: 0.00039515
Iteration 3/25 | Loss: 0.00039512
Iteration 4/25 | Loss: 0.00039512
Iteration 5/25 | Loss: 0.00039512
Iteration 6/25 | Loss: 0.00039512
Iteration 7/25 | Loss: 0.00039512
Iteration 8/25 | Loss: 0.00039512
Iteration 9/25 | Loss: 0.00039512
Iteration 10/25 | Loss: 0.00039512
Iteration 11/25 | Loss: 0.00039512
Iteration 12/25 | Loss: 0.00039512
Iteration 13/25 | Loss: 0.00039512
Iteration 14/25 | Loss: 0.00039512
Iteration 15/25 | Loss: 0.00039512
Iteration 16/25 | Loss: 0.00039512
Iteration 17/25 | Loss: 0.00039512
Iteration 18/25 | Loss: 0.00039512
Iteration 19/25 | Loss: 0.00039512
Iteration 20/25 | Loss: 0.00039512
Iteration 21/25 | Loss: 0.00039512
Iteration 22/25 | Loss: 0.00039512
Iteration 23/25 | Loss: 0.00039512
Iteration 24/25 | Loss: 0.00039512
Iteration 25/25 | Loss: 0.00039512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039512
Iteration 2/1000 | Loss: 0.00002693
Iteration 3/1000 | Loss: 0.00001814
Iteration 4/1000 | Loss: 0.00001586
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001385
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001372
Iteration 11/1000 | Loss: 0.00001371
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001343
Iteration 75/1000 | Loss: 0.00001343
Iteration 76/1000 | Loss: 0.00001343
Iteration 77/1000 | Loss: 0.00001343
Iteration 78/1000 | Loss: 0.00001343
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001342
Iteration 86/1000 | Loss: 0.00001342
Iteration 87/1000 | Loss: 0.00001342
Iteration 88/1000 | Loss: 0.00001342
Iteration 89/1000 | Loss: 0.00001342
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001342
Iteration 96/1000 | Loss: 0.00001342
Iteration 97/1000 | Loss: 0.00001342
Iteration 98/1000 | Loss: 0.00001342
Iteration 99/1000 | Loss: 0.00001342
Iteration 100/1000 | Loss: 0.00001342
Iteration 101/1000 | Loss: 0.00001342
Iteration 102/1000 | Loss: 0.00001342
Iteration 103/1000 | Loss: 0.00001342
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.3415476132649928e-05, 1.3415476132649928e-05, 1.3415476132649928e-05, 1.3415476132649928e-05, 1.3415476132649928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3415476132649928e-05

Optimization complete. Final v2v error: 3.1687469482421875 mm

Highest mean error: 3.5698745250701904 mm for frame 77

Lowest mean error: 2.816884994506836 mm for frame 0

Saving results

Total time: 32.71836876869202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404365
Iteration 2/25 | Loss: 0.00096829
Iteration 3/25 | Loss: 0.00080428
Iteration 4/25 | Loss: 0.00079356
Iteration 5/25 | Loss: 0.00078657
Iteration 6/25 | Loss: 0.00078470
Iteration 7/25 | Loss: 0.00078470
Iteration 8/25 | Loss: 0.00078470
Iteration 9/25 | Loss: 0.00078470
Iteration 10/25 | Loss: 0.00078470
Iteration 11/25 | Loss: 0.00078470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007847025990486145, 0.0007847025990486145, 0.0007847025990486145, 0.0007847025990486145, 0.0007847025990486145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007847025990486145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59164870
Iteration 2/25 | Loss: 0.00038223
Iteration 3/25 | Loss: 0.00038222
Iteration 4/25 | Loss: 0.00038222
Iteration 5/25 | Loss: 0.00038222
Iteration 6/25 | Loss: 0.00038222
Iteration 7/25 | Loss: 0.00038222
Iteration 8/25 | Loss: 0.00038222
Iteration 9/25 | Loss: 0.00038222
Iteration 10/25 | Loss: 0.00038222
Iteration 11/25 | Loss: 0.00038222
Iteration 12/25 | Loss: 0.00038222
Iteration 13/25 | Loss: 0.00038222
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00038222456350922585, 0.00038222456350922585, 0.00038222456350922585, 0.00038222456350922585, 0.00038222456350922585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038222456350922585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038222
Iteration 2/1000 | Loss: 0.00003533
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00001995
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001792
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001698
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001692
Iteration 15/1000 | Loss: 0.00001691
Iteration 16/1000 | Loss: 0.00001687
Iteration 17/1000 | Loss: 0.00001686
Iteration 18/1000 | Loss: 0.00001686
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001681
Iteration 21/1000 | Loss: 0.00001678
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001671
Iteration 26/1000 | Loss: 0.00001670
Iteration 27/1000 | Loss: 0.00001670
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001669
Iteration 30/1000 | Loss: 0.00001669
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001668
Iteration 33/1000 | Loss: 0.00001668
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001668
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001666
Iteration 44/1000 | Loss: 0.00001666
Iteration 45/1000 | Loss: 0.00001666
Iteration 46/1000 | Loss: 0.00001666
Iteration 47/1000 | Loss: 0.00001666
Iteration 48/1000 | Loss: 0.00001666
Iteration 49/1000 | Loss: 0.00001665
Iteration 50/1000 | Loss: 0.00001665
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001665
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001664
Iteration 58/1000 | Loss: 0.00001664
Iteration 59/1000 | Loss: 0.00001664
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00001664
Iteration 62/1000 | Loss: 0.00001664
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001663
Iteration 70/1000 | Loss: 0.00001663
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001663
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001662
Iteration 75/1000 | Loss: 0.00001662
Iteration 76/1000 | Loss: 0.00001662
Iteration 77/1000 | Loss: 0.00001662
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001662
Iteration 80/1000 | Loss: 0.00001662
Iteration 81/1000 | Loss: 0.00001662
Iteration 82/1000 | Loss: 0.00001661
Iteration 83/1000 | Loss: 0.00001661
Iteration 84/1000 | Loss: 0.00001661
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001660
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001659
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001659
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001657
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001656
Iteration 124/1000 | Loss: 0.00001656
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001656
Iteration 127/1000 | Loss: 0.00001656
Iteration 128/1000 | Loss: 0.00001656
Iteration 129/1000 | Loss: 0.00001656
Iteration 130/1000 | Loss: 0.00001656
Iteration 131/1000 | Loss: 0.00001656
Iteration 132/1000 | Loss: 0.00001656
Iteration 133/1000 | Loss: 0.00001656
Iteration 134/1000 | Loss: 0.00001656
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001656
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.6556006812606938e-05, 1.6556006812606938e-05, 1.6556006812606938e-05, 1.6556006812606938e-05, 1.6556006812606938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6556006812606938e-05

Optimization complete. Final v2v error: 3.557948589324951 mm

Highest mean error: 3.840026378631592 mm for frame 10

Lowest mean error: 3.3076744079589844 mm for frame 264

Saving results

Total time: 41.97068548202515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482946
Iteration 2/25 | Loss: 0.00091838
Iteration 3/25 | Loss: 0.00076180
Iteration 4/25 | Loss: 0.00073989
Iteration 5/25 | Loss: 0.00073566
Iteration 6/25 | Loss: 0.00073482
Iteration 7/25 | Loss: 0.00073480
Iteration 8/25 | Loss: 0.00073480
Iteration 9/25 | Loss: 0.00073480
Iteration 10/25 | Loss: 0.00073480
Iteration 11/25 | Loss: 0.00073480
Iteration 12/25 | Loss: 0.00073480
Iteration 13/25 | Loss: 0.00073480
Iteration 14/25 | Loss: 0.00073480
Iteration 15/25 | Loss: 0.00073480
Iteration 16/25 | Loss: 0.00073480
Iteration 17/25 | Loss: 0.00073480
Iteration 18/25 | Loss: 0.00073480
Iteration 19/25 | Loss: 0.00073480
Iteration 20/25 | Loss: 0.00073480
Iteration 21/25 | Loss: 0.00073480
Iteration 22/25 | Loss: 0.00073480
Iteration 23/25 | Loss: 0.00073480
Iteration 24/25 | Loss: 0.00073480
Iteration 25/25 | Loss: 0.00073480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49941206
Iteration 2/25 | Loss: 0.00033689
Iteration 3/25 | Loss: 0.00033688
Iteration 4/25 | Loss: 0.00033688
Iteration 5/25 | Loss: 0.00033688
Iteration 6/25 | Loss: 0.00033688
Iteration 7/25 | Loss: 0.00033688
Iteration 8/25 | Loss: 0.00033688
Iteration 9/25 | Loss: 0.00033688
Iteration 10/25 | Loss: 0.00033688
Iteration 11/25 | Loss: 0.00033688
Iteration 12/25 | Loss: 0.00033688
Iteration 13/25 | Loss: 0.00033688
Iteration 14/25 | Loss: 0.00033688
Iteration 15/25 | Loss: 0.00033688
Iteration 16/25 | Loss: 0.00033688
Iteration 17/25 | Loss: 0.00033688
Iteration 18/25 | Loss: 0.00033688
Iteration 19/25 | Loss: 0.00033688
Iteration 20/25 | Loss: 0.00033688
Iteration 21/25 | Loss: 0.00033688
Iteration 22/25 | Loss: 0.00033688
Iteration 23/25 | Loss: 0.00033688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003368766338098794, 0.0003368766338098794, 0.0003368766338098794, 0.0003368766338098794, 0.0003368766338098794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003368766338098794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033688
Iteration 2/1000 | Loss: 0.00003039
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001776
Iteration 5/1000 | Loss: 0.00001659
Iteration 6/1000 | Loss: 0.00001617
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001534
Iteration 12/1000 | Loss: 0.00001533
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001531
Iteration 16/1000 | Loss: 0.00001530
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001520
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001518
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001516
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001514
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001514
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001513
Iteration 57/1000 | Loss: 0.00001513
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.5120019270398188e-05, 1.5120019270398188e-05, 1.5120019270398188e-05, 1.5120019270398188e-05, 1.5120019270398188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5120019270398188e-05

Optimization complete. Final v2v error: 3.361368179321289 mm

Highest mean error: 3.700141668319702 mm for frame 30

Lowest mean error: 2.995891571044922 mm for frame 228

Saving results

Total time: 31.693780183792114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439192
Iteration 2/25 | Loss: 0.00120299
Iteration 3/25 | Loss: 0.00079468
Iteration 4/25 | Loss: 0.00072514
Iteration 5/25 | Loss: 0.00071405
Iteration 6/25 | Loss: 0.00070988
Iteration 7/25 | Loss: 0.00070886
Iteration 8/25 | Loss: 0.00070866
Iteration 9/25 | Loss: 0.00070866
Iteration 10/25 | Loss: 0.00070866
Iteration 11/25 | Loss: 0.00070866
Iteration 12/25 | Loss: 0.00070866
Iteration 13/25 | Loss: 0.00070866
Iteration 14/25 | Loss: 0.00070866
Iteration 15/25 | Loss: 0.00070866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007086588884703815, 0.0007086588884703815, 0.0007086588884703815, 0.0007086588884703815, 0.0007086588884703815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007086588884703815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.90257406
Iteration 2/25 | Loss: 0.00027699
Iteration 3/25 | Loss: 0.00027691
Iteration 4/25 | Loss: 0.00027691
Iteration 5/25 | Loss: 0.00027691
Iteration 6/25 | Loss: 0.00027691
Iteration 7/25 | Loss: 0.00027691
Iteration 8/25 | Loss: 0.00027691
Iteration 9/25 | Loss: 0.00027691
Iteration 10/25 | Loss: 0.00027691
Iteration 11/25 | Loss: 0.00027691
Iteration 12/25 | Loss: 0.00027691
Iteration 13/25 | Loss: 0.00027691
Iteration 14/25 | Loss: 0.00027691
Iteration 15/25 | Loss: 0.00027691
Iteration 16/25 | Loss: 0.00027691
Iteration 17/25 | Loss: 0.00027691
Iteration 18/25 | Loss: 0.00027691
Iteration 19/25 | Loss: 0.00027691
Iteration 20/25 | Loss: 0.00027691
Iteration 21/25 | Loss: 0.00027691
Iteration 22/25 | Loss: 0.00027691
Iteration 23/25 | Loss: 0.00027691
Iteration 24/25 | Loss: 0.00027691
Iteration 25/25 | Loss: 0.00027691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027691
Iteration 2/1000 | Loss: 0.00003000
Iteration 3/1000 | Loss: 0.00001950
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001456
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001440
Iteration 11/1000 | Loss: 0.00001438
Iteration 12/1000 | Loss: 0.00001433
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001429
Iteration 15/1000 | Loss: 0.00001428
Iteration 16/1000 | Loss: 0.00001427
Iteration 17/1000 | Loss: 0.00001427
Iteration 18/1000 | Loss: 0.00001426
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001423
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001401
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001396
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001395
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001395
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001394
Iteration 104/1000 | Loss: 0.00001394
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001394
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001393
Iteration 113/1000 | Loss: 0.00001393
Iteration 114/1000 | Loss: 0.00001393
Iteration 115/1000 | Loss: 0.00001393
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001392
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001392
Iteration 139/1000 | Loss: 0.00001392
Iteration 140/1000 | Loss: 0.00001392
Iteration 141/1000 | Loss: 0.00001392
Iteration 142/1000 | Loss: 0.00001392
Iteration 143/1000 | Loss: 0.00001392
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001392
Iteration 147/1000 | Loss: 0.00001392
Iteration 148/1000 | Loss: 0.00001392
Iteration 149/1000 | Loss: 0.00001392
Iteration 150/1000 | Loss: 0.00001392
Iteration 151/1000 | Loss: 0.00001392
Iteration 152/1000 | Loss: 0.00001392
Iteration 153/1000 | Loss: 0.00001392
Iteration 154/1000 | Loss: 0.00001392
Iteration 155/1000 | Loss: 0.00001392
Iteration 156/1000 | Loss: 0.00001392
Iteration 157/1000 | Loss: 0.00001392
Iteration 158/1000 | Loss: 0.00001392
Iteration 159/1000 | Loss: 0.00001392
Iteration 160/1000 | Loss: 0.00001392
Iteration 161/1000 | Loss: 0.00001392
Iteration 162/1000 | Loss: 0.00001392
Iteration 163/1000 | Loss: 0.00001392
Iteration 164/1000 | Loss: 0.00001392
Iteration 165/1000 | Loss: 0.00001392
Iteration 166/1000 | Loss: 0.00001392
Iteration 167/1000 | Loss: 0.00001392
Iteration 168/1000 | Loss: 0.00001392
Iteration 169/1000 | Loss: 0.00001392
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.3923194273957051e-05, 1.3923194273957051e-05, 1.3923194273957051e-05, 1.3923194273957051e-05, 1.3923194273957051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3923194273957051e-05

Optimization complete. Final v2v error: 3.155845880508423 mm

Highest mean error: 3.664595603942871 mm for frame 54

Lowest mean error: 2.848121166229248 mm for frame 120

Saving results

Total time: 36.590008020401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118692
Iteration 2/25 | Loss: 0.00362094
Iteration 3/25 | Loss: 0.00169942
Iteration 4/25 | Loss: 0.00141327
Iteration 5/25 | Loss: 0.00131260
Iteration 6/25 | Loss: 0.00124099
Iteration 7/25 | Loss: 0.00120671
Iteration 8/25 | Loss: 0.00108592
Iteration 9/25 | Loss: 0.00094378
Iteration 10/25 | Loss: 0.00088129
Iteration 11/25 | Loss: 0.00082234
Iteration 12/25 | Loss: 0.00080836
Iteration 13/25 | Loss: 0.00075553
Iteration 14/25 | Loss: 0.00076094
Iteration 15/25 | Loss: 0.00074569
Iteration 16/25 | Loss: 0.00076400
Iteration 17/25 | Loss: 0.00074531
Iteration 18/25 | Loss: 0.00074599
Iteration 19/25 | Loss: 0.00074858
Iteration 20/25 | Loss: 0.00074574
Iteration 21/25 | Loss: 0.00075568
Iteration 22/25 | Loss: 0.00074162
Iteration 23/25 | Loss: 0.00073648
Iteration 24/25 | Loss: 0.00073520
Iteration 25/25 | Loss: 0.00073447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76099539
Iteration 2/25 | Loss: 0.00050122
Iteration 3/25 | Loss: 0.00046831
Iteration 4/25 | Loss: 0.00046831
Iteration 5/25 | Loss: 0.00046831
Iteration 6/25 | Loss: 0.00046830
Iteration 7/25 | Loss: 0.00046830
Iteration 8/25 | Loss: 0.00046830
Iteration 9/25 | Loss: 0.00046830
Iteration 10/25 | Loss: 0.00046830
Iteration 11/25 | Loss: 0.00046830
Iteration 12/25 | Loss: 0.00046830
Iteration 13/25 | Loss: 0.00046830
Iteration 14/25 | Loss: 0.00046830
Iteration 15/25 | Loss: 0.00046830
Iteration 16/25 | Loss: 0.00046830
Iteration 17/25 | Loss: 0.00046830
Iteration 18/25 | Loss: 0.00046830
Iteration 19/25 | Loss: 0.00046830
Iteration 20/25 | Loss: 0.00046830
Iteration 21/25 | Loss: 0.00046830
Iteration 22/25 | Loss: 0.00046830
Iteration 23/25 | Loss: 0.00046830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00046830440987832844, 0.00046830440987832844, 0.00046830440987832844, 0.00046830440987832844, 0.00046830440987832844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046830440987832844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046830
Iteration 2/1000 | Loss: 0.00016711
Iteration 3/1000 | Loss: 0.00003979
Iteration 4/1000 | Loss: 0.00012127
Iteration 5/1000 | Loss: 0.00003055
Iteration 6/1000 | Loss: 0.00005748
Iteration 7/1000 | Loss: 0.00004000
Iteration 8/1000 | Loss: 0.00007390
Iteration 9/1000 | Loss: 0.00003269
Iteration 10/1000 | Loss: 0.00004125
Iteration 11/1000 | Loss: 0.00002523
Iteration 12/1000 | Loss: 0.00004630
Iteration 13/1000 | Loss: 0.00002421
Iteration 14/1000 | Loss: 0.00131334
Iteration 15/1000 | Loss: 0.00004248
Iteration 16/1000 | Loss: 0.00002193
Iteration 17/1000 | Loss: 0.00010356
Iteration 18/1000 | Loss: 0.00005047
Iteration 19/1000 | Loss: 0.00002487
Iteration 20/1000 | Loss: 0.00003141
Iteration 21/1000 | Loss: 0.00007894
Iteration 22/1000 | Loss: 0.00032795
Iteration 23/1000 | Loss: 0.00007823
Iteration 24/1000 | Loss: 0.00002659
Iteration 25/1000 | Loss: 0.00002824
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00006606
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00006017
Iteration 30/1000 | Loss: 0.00004076
Iteration 31/1000 | Loss: 0.00002239
Iteration 32/1000 | Loss: 0.00001814
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001524
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001510
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00005473
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001491
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001490
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001489
Iteration 74/1000 | Loss: 0.00001489
Iteration 75/1000 | Loss: 0.00001489
Iteration 76/1000 | Loss: 0.00001489
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001488
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00004087
Iteration 92/1000 | Loss: 0.00001919
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00003695
Iteration 96/1000 | Loss: 0.00001499
Iteration 97/1000 | Loss: 0.00001683
Iteration 98/1000 | Loss: 0.00004236
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001485
Iteration 102/1000 | Loss: 0.00001485
Iteration 103/1000 | Loss: 0.00001485
Iteration 104/1000 | Loss: 0.00001485
Iteration 105/1000 | Loss: 0.00001485
Iteration 106/1000 | Loss: 0.00001485
Iteration 107/1000 | Loss: 0.00001485
Iteration 108/1000 | Loss: 0.00001485
Iteration 109/1000 | Loss: 0.00001485
Iteration 110/1000 | Loss: 0.00001485
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001484
Iteration 113/1000 | Loss: 0.00001484
Iteration 114/1000 | Loss: 0.00001484
Iteration 115/1000 | Loss: 0.00001484
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001483
Iteration 119/1000 | Loss: 0.00001483
Iteration 120/1000 | Loss: 0.00001483
Iteration 121/1000 | Loss: 0.00001483
Iteration 122/1000 | Loss: 0.00001483
Iteration 123/1000 | Loss: 0.00001483
Iteration 124/1000 | Loss: 0.00001483
Iteration 125/1000 | Loss: 0.00001483
Iteration 126/1000 | Loss: 0.00001483
Iteration 127/1000 | Loss: 0.00001483
Iteration 128/1000 | Loss: 0.00001483
Iteration 129/1000 | Loss: 0.00001483
Iteration 130/1000 | Loss: 0.00001483
Iteration 131/1000 | Loss: 0.00001482
Iteration 132/1000 | Loss: 0.00001482
Iteration 133/1000 | Loss: 0.00001482
Iteration 134/1000 | Loss: 0.00001482
Iteration 135/1000 | Loss: 0.00001482
Iteration 136/1000 | Loss: 0.00001482
Iteration 137/1000 | Loss: 0.00001482
Iteration 138/1000 | Loss: 0.00001482
Iteration 139/1000 | Loss: 0.00001482
Iteration 140/1000 | Loss: 0.00001482
Iteration 141/1000 | Loss: 0.00001482
Iteration 142/1000 | Loss: 0.00001482
Iteration 143/1000 | Loss: 0.00001482
Iteration 144/1000 | Loss: 0.00001482
Iteration 145/1000 | Loss: 0.00001482
Iteration 146/1000 | Loss: 0.00001482
Iteration 147/1000 | Loss: 0.00001482
Iteration 148/1000 | Loss: 0.00001482
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001482
Iteration 156/1000 | Loss: 0.00001482
Iteration 157/1000 | Loss: 0.00001482
Iteration 158/1000 | Loss: 0.00001482
Iteration 159/1000 | Loss: 0.00001482
Iteration 160/1000 | Loss: 0.00001482
Iteration 161/1000 | Loss: 0.00001482
Iteration 162/1000 | Loss: 0.00001482
Iteration 163/1000 | Loss: 0.00001482
Iteration 164/1000 | Loss: 0.00001482
Iteration 165/1000 | Loss: 0.00001482
Iteration 166/1000 | Loss: 0.00001482
Iteration 167/1000 | Loss: 0.00001482
Iteration 168/1000 | Loss: 0.00001482
Iteration 169/1000 | Loss: 0.00001482
Iteration 170/1000 | Loss: 0.00001482
Iteration 171/1000 | Loss: 0.00001482
Iteration 172/1000 | Loss: 0.00001482
Iteration 173/1000 | Loss: 0.00001482
Iteration 174/1000 | Loss: 0.00001482
Iteration 175/1000 | Loss: 0.00001482
Iteration 176/1000 | Loss: 0.00001482
Iteration 177/1000 | Loss: 0.00001482
Iteration 178/1000 | Loss: 0.00001482
Iteration 179/1000 | Loss: 0.00001482
Iteration 180/1000 | Loss: 0.00001482
Iteration 181/1000 | Loss: 0.00001482
Iteration 182/1000 | Loss: 0.00001482
Iteration 183/1000 | Loss: 0.00001482
Iteration 184/1000 | Loss: 0.00001482
Iteration 185/1000 | Loss: 0.00001482
Iteration 186/1000 | Loss: 0.00001482
Iteration 187/1000 | Loss: 0.00001482
Iteration 188/1000 | Loss: 0.00001482
Iteration 189/1000 | Loss: 0.00001482
Iteration 190/1000 | Loss: 0.00001482
Iteration 191/1000 | Loss: 0.00001482
Iteration 192/1000 | Loss: 0.00001482
Iteration 193/1000 | Loss: 0.00001482
Iteration 194/1000 | Loss: 0.00001482
Iteration 195/1000 | Loss: 0.00001482
Iteration 196/1000 | Loss: 0.00001482
Iteration 197/1000 | Loss: 0.00001482
Iteration 198/1000 | Loss: 0.00001482
Iteration 199/1000 | Loss: 0.00001482
Iteration 200/1000 | Loss: 0.00001482
Iteration 201/1000 | Loss: 0.00001482
Iteration 202/1000 | Loss: 0.00001482
Iteration 203/1000 | Loss: 0.00001482
Iteration 204/1000 | Loss: 0.00001482
Iteration 205/1000 | Loss: 0.00001482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.4823880519543309e-05, 1.4823880519543309e-05, 1.4823880519543309e-05, 1.4823880519543309e-05, 1.4823880519543309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4823880519543309e-05

Optimization complete. Final v2v error: 3.2719361782073975 mm

Highest mean error: 4.488325119018555 mm for frame 79

Lowest mean error: 2.9034616947174072 mm for frame 106

Saving results

Total time: 115.1851978302002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857329
Iteration 2/25 | Loss: 0.00094053
Iteration 3/25 | Loss: 0.00072230
Iteration 4/25 | Loss: 0.00069478
Iteration 5/25 | Loss: 0.00068779
Iteration 6/25 | Loss: 0.00068600
Iteration 7/25 | Loss: 0.00068564
Iteration 8/25 | Loss: 0.00068564
Iteration 9/25 | Loss: 0.00068564
Iteration 10/25 | Loss: 0.00068564
Iteration 11/25 | Loss: 0.00068564
Iteration 12/25 | Loss: 0.00068564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006856425316073, 0.0006856425316073, 0.0006856425316073, 0.0006856425316073, 0.0006856425316073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006856425316073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46588850
Iteration 2/25 | Loss: 0.00030642
Iteration 3/25 | Loss: 0.00030642
Iteration 4/25 | Loss: 0.00030642
Iteration 5/25 | Loss: 0.00030642
Iteration 6/25 | Loss: 0.00030642
Iteration 7/25 | Loss: 0.00030642
Iteration 8/25 | Loss: 0.00030641
Iteration 9/25 | Loss: 0.00030641
Iteration 10/25 | Loss: 0.00030641
Iteration 11/25 | Loss: 0.00030641
Iteration 12/25 | Loss: 0.00030641
Iteration 13/25 | Loss: 0.00030641
Iteration 14/25 | Loss: 0.00030641
Iteration 15/25 | Loss: 0.00030641
Iteration 16/25 | Loss: 0.00030641
Iteration 17/25 | Loss: 0.00030641
Iteration 18/25 | Loss: 0.00030641
Iteration 19/25 | Loss: 0.00030641
Iteration 20/25 | Loss: 0.00030641
Iteration 21/25 | Loss: 0.00030641
Iteration 22/25 | Loss: 0.00030641
Iteration 23/25 | Loss: 0.00030641
Iteration 24/25 | Loss: 0.00030641
Iteration 25/25 | Loss: 0.00030641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030641
Iteration 2/1000 | Loss: 0.00002387
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001300
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001192
Iteration 12/1000 | Loss: 0.00001190
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001189
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001171
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001169
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.1690292922139633e-05, 1.1690292922139633e-05, 1.1690292922139633e-05, 1.1690292922139633e-05, 1.1690292922139633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1690292922139633e-05

Optimization complete. Final v2v error: 2.9332125186920166 mm

Highest mean error: 3.3102946281433105 mm for frame 44

Lowest mean error: 2.589125871658325 mm for frame 146

Saving results

Total time: 33.1349151134491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892625
Iteration 2/25 | Loss: 0.00106664
Iteration 3/25 | Loss: 0.00078960
Iteration 4/25 | Loss: 0.00072396
Iteration 5/25 | Loss: 0.00071446
Iteration 6/25 | Loss: 0.00071239
Iteration 7/25 | Loss: 0.00071006
Iteration 8/25 | Loss: 0.00070976
Iteration 9/25 | Loss: 0.00070969
Iteration 10/25 | Loss: 0.00070968
Iteration 11/25 | Loss: 0.00070968
Iteration 12/25 | Loss: 0.00070967
Iteration 13/25 | Loss: 0.00070967
Iteration 14/25 | Loss: 0.00070967
Iteration 15/25 | Loss: 0.00070967
Iteration 16/25 | Loss: 0.00070967
Iteration 17/25 | Loss: 0.00070967
Iteration 18/25 | Loss: 0.00070967
Iteration 19/25 | Loss: 0.00070967
Iteration 20/25 | Loss: 0.00070967
Iteration 21/25 | Loss: 0.00070967
Iteration 22/25 | Loss: 0.00070967
Iteration 23/25 | Loss: 0.00070967
Iteration 24/25 | Loss: 0.00070967
Iteration 25/25 | Loss: 0.00070967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80944276
Iteration 2/25 | Loss: 0.00033664
Iteration 3/25 | Loss: 0.00033011
Iteration 4/25 | Loss: 0.00033011
Iteration 5/25 | Loss: 0.00033011
Iteration 6/25 | Loss: 0.00033011
Iteration 7/25 | Loss: 0.00033011
Iteration 8/25 | Loss: 0.00033011
Iteration 9/25 | Loss: 0.00033011
Iteration 10/25 | Loss: 0.00033011
Iteration 11/25 | Loss: 0.00033011
Iteration 12/25 | Loss: 0.00033011
Iteration 13/25 | Loss: 0.00033011
Iteration 14/25 | Loss: 0.00033011
Iteration 15/25 | Loss: 0.00033011
Iteration 16/25 | Loss: 0.00033011
Iteration 17/25 | Loss: 0.00033011
Iteration 18/25 | Loss: 0.00033011
Iteration 19/25 | Loss: 0.00033011
Iteration 20/25 | Loss: 0.00033011
Iteration 21/25 | Loss: 0.00033011
Iteration 22/25 | Loss: 0.00033011
Iteration 23/25 | Loss: 0.00033011
Iteration 24/25 | Loss: 0.00033011
Iteration 25/25 | Loss: 0.00033011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033011
Iteration 2/1000 | Loss: 0.00002992
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001636
Iteration 8/1000 | Loss: 0.00006057
Iteration 9/1000 | Loss: 0.00002108
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001498
Iteration 29/1000 | Loss: 0.00002765
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001540
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001490
Iteration 49/1000 | Loss: 0.00001490
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001490
Iteration 52/1000 | Loss: 0.00001490
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001486
Iteration 78/1000 | Loss: 0.00001486
Iteration 79/1000 | Loss: 0.00001486
Iteration 80/1000 | Loss: 0.00001486
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001486
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001485
Iteration 92/1000 | Loss: 0.00001485
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001484
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001483
Iteration 108/1000 | Loss: 0.00001483
Iteration 109/1000 | Loss: 0.00001483
Iteration 110/1000 | Loss: 0.00001483
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001482
Iteration 117/1000 | Loss: 0.00001482
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001481
Iteration 120/1000 | Loss: 0.00001481
Iteration 121/1000 | Loss: 0.00001481
Iteration 122/1000 | Loss: 0.00001481
Iteration 123/1000 | Loss: 0.00001481
Iteration 124/1000 | Loss: 0.00001481
Iteration 125/1000 | Loss: 0.00001481
Iteration 126/1000 | Loss: 0.00001481
Iteration 127/1000 | Loss: 0.00001481
Iteration 128/1000 | Loss: 0.00001481
Iteration 129/1000 | Loss: 0.00001481
Iteration 130/1000 | Loss: 0.00001481
Iteration 131/1000 | Loss: 0.00001481
Iteration 132/1000 | Loss: 0.00001480
Iteration 133/1000 | Loss: 0.00001480
Iteration 134/1000 | Loss: 0.00001480
Iteration 135/1000 | Loss: 0.00001480
Iteration 136/1000 | Loss: 0.00001480
Iteration 137/1000 | Loss: 0.00001480
Iteration 138/1000 | Loss: 0.00001480
Iteration 139/1000 | Loss: 0.00001480
Iteration 140/1000 | Loss: 0.00001480
Iteration 141/1000 | Loss: 0.00001480
Iteration 142/1000 | Loss: 0.00001479
Iteration 143/1000 | Loss: 0.00001479
Iteration 144/1000 | Loss: 0.00001479
Iteration 145/1000 | Loss: 0.00001479
Iteration 146/1000 | Loss: 0.00001479
Iteration 147/1000 | Loss: 0.00001479
Iteration 148/1000 | Loss: 0.00001479
Iteration 149/1000 | Loss: 0.00001479
Iteration 150/1000 | Loss: 0.00001478
Iteration 151/1000 | Loss: 0.00001478
Iteration 152/1000 | Loss: 0.00001478
Iteration 153/1000 | Loss: 0.00001478
Iteration 154/1000 | Loss: 0.00001478
Iteration 155/1000 | Loss: 0.00001478
Iteration 156/1000 | Loss: 0.00001477
Iteration 157/1000 | Loss: 0.00001477
Iteration 158/1000 | Loss: 0.00001477
Iteration 159/1000 | Loss: 0.00001477
Iteration 160/1000 | Loss: 0.00001477
Iteration 161/1000 | Loss: 0.00001477
Iteration 162/1000 | Loss: 0.00001476
Iteration 163/1000 | Loss: 0.00001476
Iteration 164/1000 | Loss: 0.00001476
Iteration 165/1000 | Loss: 0.00001476
Iteration 166/1000 | Loss: 0.00001476
Iteration 167/1000 | Loss: 0.00001476
Iteration 168/1000 | Loss: 0.00001476
Iteration 169/1000 | Loss: 0.00001476
Iteration 170/1000 | Loss: 0.00001476
Iteration 171/1000 | Loss: 0.00001476
Iteration 172/1000 | Loss: 0.00001476
Iteration 173/1000 | Loss: 0.00001476
Iteration 174/1000 | Loss: 0.00001476
Iteration 175/1000 | Loss: 0.00001476
Iteration 176/1000 | Loss: 0.00001476
Iteration 177/1000 | Loss: 0.00001475
Iteration 178/1000 | Loss: 0.00001475
Iteration 179/1000 | Loss: 0.00001475
Iteration 180/1000 | Loss: 0.00001475
Iteration 181/1000 | Loss: 0.00001475
Iteration 182/1000 | Loss: 0.00001475
Iteration 183/1000 | Loss: 0.00001475
Iteration 184/1000 | Loss: 0.00001475
Iteration 185/1000 | Loss: 0.00001475
Iteration 186/1000 | Loss: 0.00001475
Iteration 187/1000 | Loss: 0.00001475
Iteration 188/1000 | Loss: 0.00001475
Iteration 189/1000 | Loss: 0.00001475
Iteration 190/1000 | Loss: 0.00001475
Iteration 191/1000 | Loss: 0.00001475
Iteration 192/1000 | Loss: 0.00001475
Iteration 193/1000 | Loss: 0.00001475
Iteration 194/1000 | Loss: 0.00001475
Iteration 195/1000 | Loss: 0.00001475
Iteration 196/1000 | Loss: 0.00001475
Iteration 197/1000 | Loss: 0.00001475
Iteration 198/1000 | Loss: 0.00001475
Iteration 199/1000 | Loss: 0.00001475
Iteration 200/1000 | Loss: 0.00001475
Iteration 201/1000 | Loss: 0.00001475
Iteration 202/1000 | Loss: 0.00001475
Iteration 203/1000 | Loss: 0.00001475
Iteration 204/1000 | Loss: 0.00001475
Iteration 205/1000 | Loss: 0.00001475
Iteration 206/1000 | Loss: 0.00001475
Iteration 207/1000 | Loss: 0.00001475
Iteration 208/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.4753824871149845e-05, 1.4753824871149845e-05, 1.4753824871149845e-05, 1.4753824871149845e-05, 1.4753824871149845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4753824871149845e-05

Optimization complete. Final v2v error: 3.296273708343506 mm

Highest mean error: 3.7926838397979736 mm for frame 16

Lowest mean error: 2.8724422454833984 mm for frame 137

Saving results

Total time: 54.40348029136658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00633330
Iteration 2/25 | Loss: 0.00131486
Iteration 3/25 | Loss: 0.00085822
Iteration 4/25 | Loss: 0.00079724
Iteration 5/25 | Loss: 0.00078363
Iteration 6/25 | Loss: 0.00078090
Iteration 7/25 | Loss: 0.00077993
Iteration 8/25 | Loss: 0.00077961
Iteration 9/25 | Loss: 0.00077957
Iteration 10/25 | Loss: 0.00077957
Iteration 11/25 | Loss: 0.00077957
Iteration 12/25 | Loss: 0.00077957
Iteration 13/25 | Loss: 0.00077957
Iteration 14/25 | Loss: 0.00077957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000779573165345937, 0.000779573165345937, 0.000779573165345937, 0.000779573165345937, 0.000779573165345937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000779573165345937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26358390
Iteration 2/25 | Loss: 0.00038318
Iteration 3/25 | Loss: 0.00038318
Iteration 4/25 | Loss: 0.00038318
Iteration 5/25 | Loss: 0.00038318
Iteration 6/25 | Loss: 0.00038318
Iteration 7/25 | Loss: 0.00038318
Iteration 8/25 | Loss: 0.00038318
Iteration 9/25 | Loss: 0.00038318
Iteration 10/25 | Loss: 0.00038318
Iteration 11/25 | Loss: 0.00038318
Iteration 12/25 | Loss: 0.00038318
Iteration 13/25 | Loss: 0.00038318
Iteration 14/25 | Loss: 0.00038318
Iteration 15/25 | Loss: 0.00038318
Iteration 16/25 | Loss: 0.00038318
Iteration 17/25 | Loss: 0.00038318
Iteration 18/25 | Loss: 0.00038318
Iteration 19/25 | Loss: 0.00038318
Iteration 20/25 | Loss: 0.00038318
Iteration 21/25 | Loss: 0.00038318
Iteration 22/25 | Loss: 0.00038318
Iteration 23/25 | Loss: 0.00038318
Iteration 24/25 | Loss: 0.00038318
Iteration 25/25 | Loss: 0.00038318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003831764333881438, 0.0003831764333881438, 0.0003831764333881438, 0.0003831764333881438, 0.0003831764333881438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003831764333881438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038318
Iteration 2/1000 | Loss: 0.00003771
Iteration 3/1000 | Loss: 0.00002729
Iteration 4/1000 | Loss: 0.00002341
Iteration 5/1000 | Loss: 0.00002204
Iteration 6/1000 | Loss: 0.00002124
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002038
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001970
Iteration 13/1000 | Loss: 0.00001966
Iteration 14/1000 | Loss: 0.00001965
Iteration 15/1000 | Loss: 0.00001965
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00001964
Iteration 18/1000 | Loss: 0.00001963
Iteration 19/1000 | Loss: 0.00001962
Iteration 20/1000 | Loss: 0.00001960
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001955
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001951
Iteration 39/1000 | Loss: 0.00001951
Iteration 40/1000 | Loss: 0.00001950
Iteration 41/1000 | Loss: 0.00001950
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001948
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001947
Iteration 51/1000 | Loss: 0.00001947
Iteration 52/1000 | Loss: 0.00001947
Iteration 53/1000 | Loss: 0.00001947
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Iteration 63/1000 | Loss: 0.00001945
Iteration 64/1000 | Loss: 0.00001945
Iteration 65/1000 | Loss: 0.00001945
Iteration 66/1000 | Loss: 0.00001945
Iteration 67/1000 | Loss: 0.00001945
Iteration 68/1000 | Loss: 0.00001945
Iteration 69/1000 | Loss: 0.00001945
Iteration 70/1000 | Loss: 0.00001945
Iteration 71/1000 | Loss: 0.00001945
Iteration 72/1000 | Loss: 0.00001945
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001944
Iteration 75/1000 | Loss: 0.00001944
Iteration 76/1000 | Loss: 0.00001944
Iteration 77/1000 | Loss: 0.00001944
Iteration 78/1000 | Loss: 0.00001943
Iteration 79/1000 | Loss: 0.00001943
Iteration 80/1000 | Loss: 0.00001943
Iteration 81/1000 | Loss: 0.00001943
Iteration 82/1000 | Loss: 0.00001943
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001943
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001943
Iteration 90/1000 | Loss: 0.00001943
Iteration 91/1000 | Loss: 0.00001943
Iteration 92/1000 | Loss: 0.00001943
Iteration 93/1000 | Loss: 0.00001943
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.9431285181781277e-05, 1.9431285181781277e-05, 1.9431285181781277e-05, 1.9431285181781277e-05, 1.9431285181781277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9431285181781277e-05

Optimization complete. Final v2v error: 3.67510724067688 mm

Highest mean error: 6.187963962554932 mm for frame 56

Lowest mean error: 3.0628068447113037 mm for frame 115

Saving results

Total time: 35.41412401199341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874241
Iteration 2/25 | Loss: 0.00123216
Iteration 3/25 | Loss: 0.00081601
Iteration 4/25 | Loss: 0.00073080
Iteration 5/25 | Loss: 0.00071358
Iteration 6/25 | Loss: 0.00070867
Iteration 7/25 | Loss: 0.00070686
Iteration 8/25 | Loss: 0.00070581
Iteration 9/25 | Loss: 0.00070507
Iteration 10/25 | Loss: 0.00070685
Iteration 11/25 | Loss: 0.00071012
Iteration 12/25 | Loss: 0.00070147
Iteration 13/25 | Loss: 0.00069800
Iteration 14/25 | Loss: 0.00070005
Iteration 15/25 | Loss: 0.00069846
Iteration 16/25 | Loss: 0.00069439
Iteration 17/25 | Loss: 0.00069241
Iteration 18/25 | Loss: 0.00069179
Iteration 19/25 | Loss: 0.00069128
Iteration 20/25 | Loss: 0.00069066
Iteration 21/25 | Loss: 0.00069028
Iteration 22/25 | Loss: 0.00069021
Iteration 23/25 | Loss: 0.00069020
Iteration 24/25 | Loss: 0.00069020
Iteration 25/25 | Loss: 0.00069020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45916057
Iteration 2/25 | Loss: 0.00028785
Iteration 3/25 | Loss: 0.00028785
Iteration 4/25 | Loss: 0.00028785
Iteration 5/25 | Loss: 0.00028785
Iteration 6/25 | Loss: 0.00028785
Iteration 7/25 | Loss: 0.00028785
Iteration 8/25 | Loss: 0.00028785
Iteration 9/25 | Loss: 0.00028785
Iteration 10/25 | Loss: 0.00028785
Iteration 11/25 | Loss: 0.00028785
Iteration 12/25 | Loss: 0.00028785
Iteration 13/25 | Loss: 0.00028785
Iteration 14/25 | Loss: 0.00028785
Iteration 15/25 | Loss: 0.00028785
Iteration 16/25 | Loss: 0.00028785
Iteration 17/25 | Loss: 0.00028785
Iteration 18/25 | Loss: 0.00028785
Iteration 19/25 | Loss: 0.00028785
Iteration 20/25 | Loss: 0.00028785
Iteration 21/25 | Loss: 0.00028785
Iteration 22/25 | Loss: 0.00028785
Iteration 23/25 | Loss: 0.00028785
Iteration 24/25 | Loss: 0.00028785
Iteration 25/25 | Loss: 0.00028785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028785
Iteration 2/1000 | Loss: 0.00001921
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001310
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001233
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001232
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001229
Iteration 33/1000 | Loss: 0.00001229
Iteration 34/1000 | Loss: 0.00001228
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001226
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001217
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001216
Iteration 85/1000 | Loss: 0.00001216
Iteration 86/1000 | Loss: 0.00001216
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001213
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001213
Iteration 110/1000 | Loss: 0.00001213
Iteration 111/1000 | Loss: 0.00001213
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001213
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001212
Iteration 121/1000 | Loss: 0.00001212
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.2117664482502732e-05, 1.2117664482502732e-05, 1.2117664482502732e-05, 1.2117664482502732e-05, 1.2117664482502732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2117664482502732e-05

Optimization complete. Final v2v error: 2.9576332569122314 mm

Highest mean error: 3.239619016647339 mm for frame 72

Lowest mean error: 2.645714044570923 mm for frame 181

Saving results

Total time: 59.216089487075806
