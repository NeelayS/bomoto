Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=92, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5152-5207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0001
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663430
Iteration 2/25 | Loss: 0.00109530
Iteration 3/25 | Loss: 0.00086219
Iteration 4/25 | Loss: 0.00081953
Iteration 5/25 | Loss: 0.00080845
Iteration 6/25 | Loss: 0.00080574
Iteration 7/25 | Loss: 0.00080571
Iteration 8/25 | Loss: 0.00080571
Iteration 9/25 | Loss: 0.00080571
Iteration 10/25 | Loss: 0.00080571
Iteration 11/25 | Loss: 0.00080571
Iteration 12/25 | Loss: 0.00080571
Iteration 13/25 | Loss: 0.00080571
Iteration 14/25 | Loss: 0.00080571
Iteration 15/25 | Loss: 0.00080571
Iteration 16/25 | Loss: 0.00080571
Iteration 17/25 | Loss: 0.00080571
Iteration 18/25 | Loss: 0.00080571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000805711664725095, 0.000805711664725095, 0.000805711664725095, 0.000805711664725095, 0.000805711664725095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000805711664725095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11031103
Iteration 2/25 | Loss: 0.00034639
Iteration 3/25 | Loss: 0.00034637
Iteration 4/25 | Loss: 0.00034637
Iteration 5/25 | Loss: 0.00034637
Iteration 6/25 | Loss: 0.00034637
Iteration 7/25 | Loss: 0.00034637
Iteration 8/25 | Loss: 0.00034637
Iteration 9/25 | Loss: 0.00034637
Iteration 10/25 | Loss: 0.00034637
Iteration 11/25 | Loss: 0.00034637
Iteration 12/25 | Loss: 0.00034637
Iteration 13/25 | Loss: 0.00034637
Iteration 14/25 | Loss: 0.00034637
Iteration 15/25 | Loss: 0.00034637
Iteration 16/25 | Loss: 0.00034637
Iteration 17/25 | Loss: 0.00034637
Iteration 18/25 | Loss: 0.00034637
Iteration 19/25 | Loss: 0.00034637
Iteration 20/25 | Loss: 0.00034637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00034637152566574514, 0.00034637152566574514, 0.00034637152566574514, 0.00034637152566574514, 0.00034637152566574514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034637152566574514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034637
Iteration 2/1000 | Loss: 0.00003124
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001903
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001621
Iteration 9/1000 | Loss: 0.00001596
Iteration 10/1000 | Loss: 0.00001586
Iteration 11/1000 | Loss: 0.00001571
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001566
Iteration 14/1000 | Loss: 0.00001566
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001565
Iteration 17/1000 | Loss: 0.00001563
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001561
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001557
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001556
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001555
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00001554
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001546
Iteration 91/1000 | Loss: 0.00001546
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001544
Iteration 98/1000 | Loss: 0.00001544
Iteration 99/1000 | Loss: 0.00001544
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00001543
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001542
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001541
Iteration 114/1000 | Loss: 0.00001541
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001537
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001536
Iteration 148/1000 | Loss: 0.00001536
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001535
Iteration 156/1000 | Loss: 0.00001535
Iteration 157/1000 | Loss: 0.00001535
Iteration 158/1000 | Loss: 0.00001535
Iteration 159/1000 | Loss: 0.00001535
Iteration 160/1000 | Loss: 0.00001535
Iteration 161/1000 | Loss: 0.00001535
Iteration 162/1000 | Loss: 0.00001535
Iteration 163/1000 | Loss: 0.00001535
Iteration 164/1000 | Loss: 0.00001535
Iteration 165/1000 | Loss: 0.00001535
Iteration 166/1000 | Loss: 0.00001535
Iteration 167/1000 | Loss: 0.00001535
Iteration 168/1000 | Loss: 0.00001535
Iteration 169/1000 | Loss: 0.00001535
Iteration 170/1000 | Loss: 0.00001535
Iteration 171/1000 | Loss: 0.00001535
Iteration 172/1000 | Loss: 0.00001535
Iteration 173/1000 | Loss: 0.00001535
Iteration 174/1000 | Loss: 0.00001535
Iteration 175/1000 | Loss: 0.00001535
Iteration 176/1000 | Loss: 0.00001535
Iteration 177/1000 | Loss: 0.00001535
Iteration 178/1000 | Loss: 0.00001535
Iteration 179/1000 | Loss: 0.00001535
Iteration 180/1000 | Loss: 0.00001535
Iteration 181/1000 | Loss: 0.00001535
Iteration 182/1000 | Loss: 0.00001535
Iteration 183/1000 | Loss: 0.00001535
Iteration 184/1000 | Loss: 0.00001535
Iteration 185/1000 | Loss: 0.00001535
Iteration 186/1000 | Loss: 0.00001535
Iteration 187/1000 | Loss: 0.00001535
Iteration 188/1000 | Loss: 0.00001535
Iteration 189/1000 | Loss: 0.00001535
Iteration 190/1000 | Loss: 0.00001535
Iteration 191/1000 | Loss: 0.00001535
Iteration 192/1000 | Loss: 0.00001535
Iteration 193/1000 | Loss: 0.00001535
Iteration 194/1000 | Loss: 0.00001535
Iteration 195/1000 | Loss: 0.00001535
Iteration 196/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.535267074359581e-05, 1.535267074359581e-05, 1.535267074359581e-05, 1.535267074359581e-05, 1.535267074359581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.535267074359581e-05

Optimization complete. Final v2v error: 3.2843048572540283 mm

Highest mean error: 3.999908447265625 mm for frame 227

Lowest mean error: 2.9670321941375732 mm for frame 80

Saving results

Total time: 507.1686336994171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0011
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869837
Iteration 2/25 | Loss: 0.00129470
Iteration 3/25 | Loss: 0.00102024
Iteration 4/25 | Loss: 0.00096632
Iteration 5/25 | Loss: 0.00094913
Iteration 6/25 | Loss: 0.00094486
Iteration 7/25 | Loss: 0.00094342
Iteration 8/25 | Loss: 0.00094307
Iteration 9/25 | Loss: 0.00094307
Iteration 10/25 | Loss: 0.00094307
Iteration 11/25 | Loss: 0.00094307
Iteration 12/25 | Loss: 0.00094307
Iteration 13/25 | Loss: 0.00094307
Iteration 14/25 | Loss: 0.00094307
Iteration 15/25 | Loss: 0.00094307
Iteration 16/25 | Loss: 0.00094307
Iteration 17/25 | Loss: 0.00094307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009430688805878162, 0.0009430688805878162, 0.0009430688805878162, 0.0009430688805878162, 0.0009430688805878162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009430688805878162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.61645603
Iteration 2/25 | Loss: 0.00048078
Iteration 3/25 | Loss: 0.00048078
Iteration 4/25 | Loss: 0.00048078
Iteration 5/25 | Loss: 0.00048078
Iteration 6/25 | Loss: 0.00048078
Iteration 7/25 | Loss: 0.00048078
Iteration 8/25 | Loss: 0.00048078
Iteration 9/25 | Loss: 0.00048078
Iteration 10/25 | Loss: 0.00048078
Iteration 11/25 | Loss: 0.00048078
Iteration 12/25 | Loss: 0.00048078
Iteration 13/25 | Loss: 0.00048078
Iteration 14/25 | Loss: 0.00048078
Iteration 15/25 | Loss: 0.00048078
Iteration 16/25 | Loss: 0.00048078
Iteration 17/25 | Loss: 0.00048078
Iteration 18/25 | Loss: 0.00048078
Iteration 19/25 | Loss: 0.00048078
Iteration 20/25 | Loss: 0.00048078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004807762452401221, 0.0004807762452401221, 0.0004807762452401221, 0.0004807762452401221, 0.0004807762452401221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004807762452401221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048078
Iteration 2/1000 | Loss: 0.00007910
Iteration 3/1000 | Loss: 0.00004875
Iteration 4/1000 | Loss: 0.00004018
Iteration 5/1000 | Loss: 0.00003643
Iteration 6/1000 | Loss: 0.00003441
Iteration 7/1000 | Loss: 0.00003250
Iteration 8/1000 | Loss: 0.00003141
Iteration 9/1000 | Loss: 0.00003058
Iteration 10/1000 | Loss: 0.00002994
Iteration 11/1000 | Loss: 0.00002956
Iteration 12/1000 | Loss: 0.00002929
Iteration 13/1000 | Loss: 0.00002907
Iteration 14/1000 | Loss: 0.00002884
Iteration 15/1000 | Loss: 0.00002871
Iteration 16/1000 | Loss: 0.00002866
Iteration 17/1000 | Loss: 0.00002861
Iteration 18/1000 | Loss: 0.00002858
Iteration 19/1000 | Loss: 0.00002857
Iteration 20/1000 | Loss: 0.00002856
Iteration 21/1000 | Loss: 0.00002856
Iteration 22/1000 | Loss: 0.00002856
Iteration 23/1000 | Loss: 0.00002855
Iteration 24/1000 | Loss: 0.00002852
Iteration 25/1000 | Loss: 0.00002852
Iteration 26/1000 | Loss: 0.00002849
Iteration 27/1000 | Loss: 0.00002849
Iteration 28/1000 | Loss: 0.00002849
Iteration 29/1000 | Loss: 0.00002849
Iteration 30/1000 | Loss: 0.00002849
Iteration 31/1000 | Loss: 0.00002849
Iteration 32/1000 | Loss: 0.00002848
Iteration 33/1000 | Loss: 0.00002848
Iteration 34/1000 | Loss: 0.00002848
Iteration 35/1000 | Loss: 0.00002848
Iteration 36/1000 | Loss: 0.00002848
Iteration 37/1000 | Loss: 0.00002848
Iteration 38/1000 | Loss: 0.00002848
Iteration 39/1000 | Loss: 0.00002846
Iteration 40/1000 | Loss: 0.00002846
Iteration 41/1000 | Loss: 0.00002845
Iteration 42/1000 | Loss: 0.00002845
Iteration 43/1000 | Loss: 0.00002845
Iteration 44/1000 | Loss: 0.00002844
Iteration 45/1000 | Loss: 0.00002844
Iteration 46/1000 | Loss: 0.00002844
Iteration 47/1000 | Loss: 0.00002844
Iteration 48/1000 | Loss: 0.00002843
Iteration 49/1000 | Loss: 0.00002843
Iteration 50/1000 | Loss: 0.00002843
Iteration 51/1000 | Loss: 0.00002843
Iteration 52/1000 | Loss: 0.00002842
Iteration 53/1000 | Loss: 0.00002842
Iteration 54/1000 | Loss: 0.00002842
Iteration 55/1000 | Loss: 0.00002841
Iteration 56/1000 | Loss: 0.00002841
Iteration 57/1000 | Loss: 0.00002841
Iteration 58/1000 | Loss: 0.00002840
Iteration 59/1000 | Loss: 0.00002840
Iteration 60/1000 | Loss: 0.00002840
Iteration 61/1000 | Loss: 0.00002840
Iteration 62/1000 | Loss: 0.00002839
Iteration 63/1000 | Loss: 0.00002839
Iteration 64/1000 | Loss: 0.00002839
Iteration 65/1000 | Loss: 0.00002839
Iteration 66/1000 | Loss: 0.00002838
Iteration 67/1000 | Loss: 0.00002838
Iteration 68/1000 | Loss: 0.00002838
Iteration 69/1000 | Loss: 0.00002837
Iteration 70/1000 | Loss: 0.00002837
Iteration 71/1000 | Loss: 0.00002837
Iteration 72/1000 | Loss: 0.00002836
Iteration 73/1000 | Loss: 0.00002836
Iteration 74/1000 | Loss: 0.00002836
Iteration 75/1000 | Loss: 0.00002836
Iteration 76/1000 | Loss: 0.00002836
Iteration 77/1000 | Loss: 0.00002835
Iteration 78/1000 | Loss: 0.00002835
Iteration 79/1000 | Loss: 0.00002835
Iteration 80/1000 | Loss: 0.00002835
Iteration 81/1000 | Loss: 0.00002835
Iteration 82/1000 | Loss: 0.00002835
Iteration 83/1000 | Loss: 0.00002835
Iteration 84/1000 | Loss: 0.00002834
Iteration 85/1000 | Loss: 0.00002834
Iteration 86/1000 | Loss: 0.00002834
Iteration 87/1000 | Loss: 0.00002834
Iteration 88/1000 | Loss: 0.00002834
Iteration 89/1000 | Loss: 0.00002834
Iteration 90/1000 | Loss: 0.00002834
Iteration 91/1000 | Loss: 0.00002834
Iteration 92/1000 | Loss: 0.00002834
Iteration 93/1000 | Loss: 0.00002834
Iteration 94/1000 | Loss: 0.00002834
Iteration 95/1000 | Loss: 0.00002834
Iteration 96/1000 | Loss: 0.00002834
Iteration 97/1000 | Loss: 0.00002834
Iteration 98/1000 | Loss: 0.00002833
Iteration 99/1000 | Loss: 0.00002833
Iteration 100/1000 | Loss: 0.00002833
Iteration 101/1000 | Loss: 0.00002833
Iteration 102/1000 | Loss: 0.00002832
Iteration 103/1000 | Loss: 0.00002832
Iteration 104/1000 | Loss: 0.00002832
Iteration 105/1000 | Loss: 0.00002831
Iteration 106/1000 | Loss: 0.00002831
Iteration 107/1000 | Loss: 0.00002831
Iteration 108/1000 | Loss: 0.00002831
Iteration 109/1000 | Loss: 0.00002831
Iteration 110/1000 | Loss: 0.00002830
Iteration 111/1000 | Loss: 0.00002830
Iteration 112/1000 | Loss: 0.00002830
Iteration 113/1000 | Loss: 0.00002830
Iteration 114/1000 | Loss: 0.00002830
Iteration 115/1000 | Loss: 0.00002829
Iteration 116/1000 | Loss: 0.00002829
Iteration 117/1000 | Loss: 0.00002829
Iteration 118/1000 | Loss: 0.00002829
Iteration 119/1000 | Loss: 0.00002828
Iteration 120/1000 | Loss: 0.00002828
Iteration 121/1000 | Loss: 0.00002828
Iteration 122/1000 | Loss: 0.00002828
Iteration 123/1000 | Loss: 0.00002828
Iteration 124/1000 | Loss: 0.00002828
Iteration 125/1000 | Loss: 0.00002828
Iteration 126/1000 | Loss: 0.00002828
Iteration 127/1000 | Loss: 0.00002828
Iteration 128/1000 | Loss: 0.00002827
Iteration 129/1000 | Loss: 0.00002827
Iteration 130/1000 | Loss: 0.00002827
Iteration 131/1000 | Loss: 0.00002826
Iteration 132/1000 | Loss: 0.00002826
Iteration 133/1000 | Loss: 0.00002826
Iteration 134/1000 | Loss: 0.00002826
Iteration 135/1000 | Loss: 0.00002826
Iteration 136/1000 | Loss: 0.00002825
Iteration 137/1000 | Loss: 0.00002825
Iteration 138/1000 | Loss: 0.00002825
Iteration 139/1000 | Loss: 0.00002825
Iteration 140/1000 | Loss: 0.00002825
Iteration 141/1000 | Loss: 0.00002824
Iteration 142/1000 | Loss: 0.00002824
Iteration 143/1000 | Loss: 0.00002824
Iteration 144/1000 | Loss: 0.00002823
Iteration 145/1000 | Loss: 0.00002823
Iteration 146/1000 | Loss: 0.00002823
Iteration 147/1000 | Loss: 0.00002823
Iteration 148/1000 | Loss: 0.00002823
Iteration 149/1000 | Loss: 0.00002823
Iteration 150/1000 | Loss: 0.00002823
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002822
Iteration 153/1000 | Loss: 0.00002822
Iteration 154/1000 | Loss: 0.00002822
Iteration 155/1000 | Loss: 0.00002822
Iteration 156/1000 | Loss: 0.00002822
Iteration 157/1000 | Loss: 0.00002822
Iteration 158/1000 | Loss: 0.00002822
Iteration 159/1000 | Loss: 0.00002822
Iteration 160/1000 | Loss: 0.00002821
Iteration 161/1000 | Loss: 0.00002821
Iteration 162/1000 | Loss: 0.00002821
Iteration 163/1000 | Loss: 0.00002821
Iteration 164/1000 | Loss: 0.00002821
Iteration 165/1000 | Loss: 0.00002821
Iteration 166/1000 | Loss: 0.00002821
Iteration 167/1000 | Loss: 0.00002821
Iteration 168/1000 | Loss: 0.00002821
Iteration 169/1000 | Loss: 0.00002821
Iteration 170/1000 | Loss: 0.00002821
Iteration 171/1000 | Loss: 0.00002821
Iteration 172/1000 | Loss: 0.00002821
Iteration 173/1000 | Loss: 0.00002821
Iteration 174/1000 | Loss: 0.00002821
Iteration 175/1000 | Loss: 0.00002821
Iteration 176/1000 | Loss: 0.00002821
Iteration 177/1000 | Loss: 0.00002821
Iteration 178/1000 | Loss: 0.00002821
Iteration 179/1000 | Loss: 0.00002821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.8214833946549334e-05, 2.8214833946549334e-05, 2.8214833946549334e-05, 2.8214833946549334e-05, 2.8214833946549334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8214833946549334e-05

Optimization complete. Final v2v error: 4.504234313964844 mm

Highest mean error: 6.378203868865967 mm for frame 95

Lowest mean error: 3.4123404026031494 mm for frame 28

Saving results

Total time: 235.08353233337402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881678
Iteration 2/25 | Loss: 0.00132452
Iteration 3/25 | Loss: 0.00093915
Iteration 4/25 | Loss: 0.00088476
Iteration 5/25 | Loss: 0.00087532
Iteration 6/25 | Loss: 0.00087393
Iteration 7/25 | Loss: 0.00087373
Iteration 8/25 | Loss: 0.00087373
Iteration 9/25 | Loss: 0.00087373
Iteration 10/25 | Loss: 0.00087373
Iteration 11/25 | Loss: 0.00087373
Iteration 12/25 | Loss: 0.00087373
Iteration 13/25 | Loss: 0.00087373
Iteration 14/25 | Loss: 0.00087373
Iteration 15/25 | Loss: 0.00087373
Iteration 16/25 | Loss: 0.00087373
Iteration 17/25 | Loss: 0.00087373
Iteration 18/25 | Loss: 0.00087373
Iteration 19/25 | Loss: 0.00087373
Iteration 20/25 | Loss: 0.00087373
Iteration 21/25 | Loss: 0.00087373
Iteration 22/25 | Loss: 0.00087373
Iteration 23/25 | Loss: 0.00087373
Iteration 24/25 | Loss: 0.00087373
Iteration 25/25 | Loss: 0.00087373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39728332
Iteration 2/25 | Loss: 0.00048726
Iteration 3/25 | Loss: 0.00048725
Iteration 4/25 | Loss: 0.00048725
Iteration 5/25 | Loss: 0.00048725
Iteration 6/25 | Loss: 0.00048725
Iteration 7/25 | Loss: 0.00048725
Iteration 8/25 | Loss: 0.00048725
Iteration 9/25 | Loss: 0.00048725
Iteration 10/25 | Loss: 0.00048725
Iteration 11/25 | Loss: 0.00048725
Iteration 12/25 | Loss: 0.00048725
Iteration 13/25 | Loss: 0.00048725
Iteration 14/25 | Loss: 0.00048725
Iteration 15/25 | Loss: 0.00048725
Iteration 16/25 | Loss: 0.00048725
Iteration 17/25 | Loss: 0.00048725
Iteration 18/25 | Loss: 0.00048725
Iteration 19/25 | Loss: 0.00048725
Iteration 20/25 | Loss: 0.00048725
Iteration 21/25 | Loss: 0.00048725
Iteration 22/25 | Loss: 0.00048725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00048724879161454737, 0.00048724879161454737, 0.00048724879161454737, 0.00048724879161454737, 0.00048724879161454737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048724879161454737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048725
Iteration 2/1000 | Loss: 0.00005342
Iteration 3/1000 | Loss: 0.00004114
Iteration 4/1000 | Loss: 0.00003666
Iteration 5/1000 | Loss: 0.00003497
Iteration 6/1000 | Loss: 0.00003368
Iteration 7/1000 | Loss: 0.00003275
Iteration 8/1000 | Loss: 0.00003226
Iteration 9/1000 | Loss: 0.00003189
Iteration 10/1000 | Loss: 0.00003169
Iteration 11/1000 | Loss: 0.00003153
Iteration 12/1000 | Loss: 0.00003149
Iteration 13/1000 | Loss: 0.00003149
Iteration 14/1000 | Loss: 0.00003148
Iteration 15/1000 | Loss: 0.00003148
Iteration 16/1000 | Loss: 0.00003148
Iteration 17/1000 | Loss: 0.00003148
Iteration 18/1000 | Loss: 0.00003148
Iteration 19/1000 | Loss: 0.00003148
Iteration 20/1000 | Loss: 0.00003148
Iteration 21/1000 | Loss: 0.00003148
Iteration 22/1000 | Loss: 0.00003147
Iteration 23/1000 | Loss: 0.00003147
Iteration 24/1000 | Loss: 0.00003145
Iteration 25/1000 | Loss: 0.00003136
Iteration 26/1000 | Loss: 0.00003136
Iteration 27/1000 | Loss: 0.00003134
Iteration 28/1000 | Loss: 0.00003131
Iteration 29/1000 | Loss: 0.00003131
Iteration 30/1000 | Loss: 0.00003130
Iteration 31/1000 | Loss: 0.00003130
Iteration 32/1000 | Loss: 0.00003127
Iteration 33/1000 | Loss: 0.00003127
Iteration 34/1000 | Loss: 0.00003127
Iteration 35/1000 | Loss: 0.00003127
Iteration 36/1000 | Loss: 0.00003127
Iteration 37/1000 | Loss: 0.00003127
Iteration 38/1000 | Loss: 0.00003127
Iteration 39/1000 | Loss: 0.00003123
Iteration 40/1000 | Loss: 0.00003123
Iteration 41/1000 | Loss: 0.00003123
Iteration 42/1000 | Loss: 0.00003123
Iteration 43/1000 | Loss: 0.00003123
Iteration 44/1000 | Loss: 0.00003122
Iteration 45/1000 | Loss: 0.00003120
Iteration 46/1000 | Loss: 0.00003119
Iteration 47/1000 | Loss: 0.00003119
Iteration 48/1000 | Loss: 0.00003118
Iteration 49/1000 | Loss: 0.00003118
Iteration 50/1000 | Loss: 0.00003118
Iteration 51/1000 | Loss: 0.00003118
Iteration 52/1000 | Loss: 0.00003118
Iteration 53/1000 | Loss: 0.00003118
Iteration 54/1000 | Loss: 0.00003118
Iteration 55/1000 | Loss: 0.00003118
Iteration 56/1000 | Loss: 0.00003118
Iteration 57/1000 | Loss: 0.00003118
Iteration 58/1000 | Loss: 0.00003117
Iteration 59/1000 | Loss: 0.00003117
Iteration 60/1000 | Loss: 0.00003117
Iteration 61/1000 | Loss: 0.00003110
Iteration 62/1000 | Loss: 0.00003110
Iteration 63/1000 | Loss: 0.00003109
Iteration 64/1000 | Loss: 0.00003109
Iteration 65/1000 | Loss: 0.00003109
Iteration 66/1000 | Loss: 0.00003109
Iteration 67/1000 | Loss: 0.00003108
Iteration 68/1000 | Loss: 0.00003108
Iteration 69/1000 | Loss: 0.00003107
Iteration 70/1000 | Loss: 0.00003107
Iteration 71/1000 | Loss: 0.00003107
Iteration 72/1000 | Loss: 0.00003106
Iteration 73/1000 | Loss: 0.00003106
Iteration 74/1000 | Loss: 0.00003106
Iteration 75/1000 | Loss: 0.00003106
Iteration 76/1000 | Loss: 0.00003106
Iteration 77/1000 | Loss: 0.00003106
Iteration 78/1000 | Loss: 0.00003105
Iteration 79/1000 | Loss: 0.00003105
Iteration 80/1000 | Loss: 0.00003105
Iteration 81/1000 | Loss: 0.00003105
Iteration 82/1000 | Loss: 0.00003104
Iteration 83/1000 | Loss: 0.00003104
Iteration 84/1000 | Loss: 0.00003104
Iteration 85/1000 | Loss: 0.00003103
Iteration 86/1000 | Loss: 0.00003103
Iteration 87/1000 | Loss: 0.00003103
Iteration 88/1000 | Loss: 0.00003103
Iteration 89/1000 | Loss: 0.00003103
Iteration 90/1000 | Loss: 0.00003103
Iteration 91/1000 | Loss: 0.00003103
Iteration 92/1000 | Loss: 0.00003103
Iteration 93/1000 | Loss: 0.00003103
Iteration 94/1000 | Loss: 0.00003103
Iteration 95/1000 | Loss: 0.00003103
Iteration 96/1000 | Loss: 0.00003103
Iteration 97/1000 | Loss: 0.00003103
Iteration 98/1000 | Loss: 0.00003103
Iteration 99/1000 | Loss: 0.00003103
Iteration 100/1000 | Loss: 0.00003103
Iteration 101/1000 | Loss: 0.00003103
Iteration 102/1000 | Loss: 0.00003103
Iteration 103/1000 | Loss: 0.00003103
Iteration 104/1000 | Loss: 0.00003103
Iteration 105/1000 | Loss: 0.00003103
Iteration 106/1000 | Loss: 0.00003103
Iteration 107/1000 | Loss: 0.00003102
Iteration 108/1000 | Loss: 0.00003102
Iteration 109/1000 | Loss: 0.00003102
Iteration 110/1000 | Loss: 0.00003102
Iteration 111/1000 | Loss: 0.00003102
Iteration 112/1000 | Loss: 0.00003102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [3.102472692262381e-05, 3.102472692262381e-05, 3.102472692262381e-05, 3.102472692262381e-05, 3.102472692262381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.102472692262381e-05

Optimization complete. Final v2v error: 4.600237846374512 mm

Highest mean error: 4.7937092781066895 mm for frame 103

Lowest mean error: 4.2156524658203125 mm for frame 50

Saving results

Total time: 194.2874345779419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0002
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913294
Iteration 2/25 | Loss: 0.00096566
Iteration 3/25 | Loss: 0.00080667
Iteration 4/25 | Loss: 0.00078437
Iteration 5/25 | Loss: 0.00077934
Iteration 6/25 | Loss: 0.00077814
Iteration 7/25 | Loss: 0.00077777
Iteration 8/25 | Loss: 0.00077776
Iteration 9/25 | Loss: 0.00077776
Iteration 10/25 | Loss: 0.00077776
Iteration 11/25 | Loss: 0.00077776
Iteration 12/25 | Loss: 0.00077776
Iteration 13/25 | Loss: 0.00077776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007777562132105231, 0.0007777562132105231, 0.0007777562132105231, 0.0007777562132105231, 0.0007777562132105231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007777562132105231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45861471
Iteration 2/25 | Loss: 0.00030905
Iteration 3/25 | Loss: 0.00030905
Iteration 4/25 | Loss: 0.00030905
Iteration 5/25 | Loss: 0.00030904
Iteration 6/25 | Loss: 0.00030904
Iteration 7/25 | Loss: 0.00030904
Iteration 8/25 | Loss: 0.00030904
Iteration 9/25 | Loss: 0.00030904
Iteration 10/25 | Loss: 0.00030904
Iteration 11/25 | Loss: 0.00030904
Iteration 12/25 | Loss: 0.00030904
Iteration 13/25 | Loss: 0.00030904
Iteration 14/25 | Loss: 0.00030904
Iteration 15/25 | Loss: 0.00030904
Iteration 16/25 | Loss: 0.00030904
Iteration 17/25 | Loss: 0.00030904
Iteration 18/25 | Loss: 0.00030904
Iteration 19/25 | Loss: 0.00030904
Iteration 20/25 | Loss: 0.00030904
Iteration 21/25 | Loss: 0.00030904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003090428072027862, 0.0003090428072027862, 0.0003090428072027862, 0.0003090428072027862, 0.0003090428072027862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003090428072027862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030904
Iteration 2/1000 | Loss: 0.00003620
Iteration 3/1000 | Loss: 0.00002125
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001724
Iteration 6/1000 | Loss: 0.00001636
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001514
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001513
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001493
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001488
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001481
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001478
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001477
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001474
Iteration 70/1000 | Loss: 0.00001474
Iteration 71/1000 | Loss: 0.00001474
Iteration 72/1000 | Loss: 0.00001474
Iteration 73/1000 | Loss: 0.00001474
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001474
Iteration 84/1000 | Loss: 0.00001474
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.4737345736648422e-05, 1.4737345736648422e-05, 1.4737345736648422e-05, 1.4737345736648422e-05, 1.4737345736648422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4737345736648422e-05

Optimization complete. Final v2v error: 3.247382640838623 mm

Highest mean error: 3.8892734050750732 mm for frame 47

Lowest mean error: 2.875042676925659 mm for frame 123

Saving results

Total time: 221.52070140838623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0022
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363780
Iteration 2/25 | Loss: 0.00089230
Iteration 3/25 | Loss: 0.00078311
Iteration 4/25 | Loss: 0.00076324
Iteration 5/25 | Loss: 0.00075928
Iteration 6/25 | Loss: 0.00075817
Iteration 7/25 | Loss: 0.00075808
Iteration 8/25 | Loss: 0.00075808
Iteration 9/25 | Loss: 0.00075808
Iteration 10/25 | Loss: 0.00075808
Iteration 11/25 | Loss: 0.00075808
Iteration 12/25 | Loss: 0.00075808
Iteration 13/25 | Loss: 0.00075808
Iteration 14/25 | Loss: 0.00075808
Iteration 15/25 | Loss: 0.00075808
Iteration 16/25 | Loss: 0.00075808
Iteration 17/25 | Loss: 0.00075808
Iteration 18/25 | Loss: 0.00075808
Iteration 19/25 | Loss: 0.00075808
Iteration 20/25 | Loss: 0.00075808
Iteration 21/25 | Loss: 0.00075808
Iteration 22/25 | Loss: 0.00075808
Iteration 23/25 | Loss: 0.00075808
Iteration 24/25 | Loss: 0.00075808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000758079462684691, 0.000758079462684691, 0.000758079462684691, 0.000758079462684691, 0.000758079462684691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000758079462684691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71081781
Iteration 2/25 | Loss: 0.00034873
Iteration 3/25 | Loss: 0.00034873
Iteration 4/25 | Loss: 0.00034872
Iteration 5/25 | Loss: 0.00034872
Iteration 6/25 | Loss: 0.00034872
Iteration 7/25 | Loss: 0.00034872
Iteration 8/25 | Loss: 0.00034872
Iteration 9/25 | Loss: 0.00034872
Iteration 10/25 | Loss: 0.00034872
Iteration 11/25 | Loss: 0.00034872
Iteration 12/25 | Loss: 0.00034872
Iteration 13/25 | Loss: 0.00034872
Iteration 14/25 | Loss: 0.00034872
Iteration 15/25 | Loss: 0.00034872
Iteration 16/25 | Loss: 0.00034872
Iteration 17/25 | Loss: 0.00034872
Iteration 18/25 | Loss: 0.00034872
Iteration 19/25 | Loss: 0.00034872
Iteration 20/25 | Loss: 0.00034872
Iteration 21/25 | Loss: 0.00034872
Iteration 22/25 | Loss: 0.00034872
Iteration 23/25 | Loss: 0.00034872
Iteration 24/25 | Loss: 0.00034872
Iteration 25/25 | Loss: 0.00034872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034872
Iteration 2/1000 | Loss: 0.00004439
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001544
Iteration 11/1000 | Loss: 0.00001527
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001505
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001498
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001496
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001495
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001494
Iteration 45/1000 | Loss: 0.00001493
Iteration 46/1000 | Loss: 0.00001493
Iteration 47/1000 | Loss: 0.00001493
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001488
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001487
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001486
Iteration 101/1000 | Loss: 0.00001486
Iteration 102/1000 | Loss: 0.00001486
Iteration 103/1000 | Loss: 0.00001486
Iteration 104/1000 | Loss: 0.00001486
Iteration 105/1000 | Loss: 0.00001486
Iteration 106/1000 | Loss: 0.00001486
Iteration 107/1000 | Loss: 0.00001486
Iteration 108/1000 | Loss: 0.00001486
Iteration 109/1000 | Loss: 0.00001485
Iteration 110/1000 | Loss: 0.00001485
Iteration 111/1000 | Loss: 0.00001485
Iteration 112/1000 | Loss: 0.00001485
Iteration 113/1000 | Loss: 0.00001485
Iteration 114/1000 | Loss: 0.00001485
Iteration 115/1000 | Loss: 0.00001485
Iteration 116/1000 | Loss: 0.00001485
Iteration 117/1000 | Loss: 0.00001485
Iteration 118/1000 | Loss: 0.00001485
Iteration 119/1000 | Loss: 0.00001484
Iteration 120/1000 | Loss: 0.00001484
Iteration 121/1000 | Loss: 0.00001484
Iteration 122/1000 | Loss: 0.00001484
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00001484
Iteration 125/1000 | Loss: 0.00001484
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001483
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00001483
Iteration 135/1000 | Loss: 0.00001483
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001482
Iteration 140/1000 | Loss: 0.00001482
Iteration 141/1000 | Loss: 0.00001482
Iteration 142/1000 | Loss: 0.00001482
Iteration 143/1000 | Loss: 0.00001482
Iteration 144/1000 | Loss: 0.00001482
Iteration 145/1000 | Loss: 0.00001481
Iteration 146/1000 | Loss: 0.00001481
Iteration 147/1000 | Loss: 0.00001481
Iteration 148/1000 | Loss: 0.00001481
Iteration 149/1000 | Loss: 0.00001481
Iteration 150/1000 | Loss: 0.00001481
Iteration 151/1000 | Loss: 0.00001481
Iteration 152/1000 | Loss: 0.00001481
Iteration 153/1000 | Loss: 0.00001481
Iteration 154/1000 | Loss: 0.00001481
Iteration 155/1000 | Loss: 0.00001481
Iteration 156/1000 | Loss: 0.00001481
Iteration 157/1000 | Loss: 0.00001480
Iteration 158/1000 | Loss: 0.00001480
Iteration 159/1000 | Loss: 0.00001480
Iteration 160/1000 | Loss: 0.00001480
Iteration 161/1000 | Loss: 0.00001480
Iteration 162/1000 | Loss: 0.00001479
Iteration 163/1000 | Loss: 0.00001479
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001479
Iteration 170/1000 | Loss: 0.00001479
Iteration 171/1000 | Loss: 0.00001479
Iteration 172/1000 | Loss: 0.00001479
Iteration 173/1000 | Loss: 0.00001479
Iteration 174/1000 | Loss: 0.00001479
Iteration 175/1000 | Loss: 0.00001479
Iteration 176/1000 | Loss: 0.00001479
Iteration 177/1000 | Loss: 0.00001479
Iteration 178/1000 | Loss: 0.00001478
Iteration 179/1000 | Loss: 0.00001478
Iteration 180/1000 | Loss: 0.00001478
Iteration 181/1000 | Loss: 0.00001478
Iteration 182/1000 | Loss: 0.00001478
Iteration 183/1000 | Loss: 0.00001478
Iteration 184/1000 | Loss: 0.00001478
Iteration 185/1000 | Loss: 0.00001478
Iteration 186/1000 | Loss: 0.00001478
Iteration 187/1000 | Loss: 0.00001478
Iteration 188/1000 | Loss: 0.00001478
Iteration 189/1000 | Loss: 0.00001478
Iteration 190/1000 | Loss: 0.00001478
Iteration 191/1000 | Loss: 0.00001478
Iteration 192/1000 | Loss: 0.00001478
Iteration 193/1000 | Loss: 0.00001478
Iteration 194/1000 | Loss: 0.00001478
Iteration 195/1000 | Loss: 0.00001478
Iteration 196/1000 | Loss: 0.00001478
Iteration 197/1000 | Loss: 0.00001478
Iteration 198/1000 | Loss: 0.00001478
Iteration 199/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4778478544030804e-05, 1.4778478544030804e-05, 1.4778478544030804e-05, 1.4778478544030804e-05, 1.4778478544030804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4778478544030804e-05

Optimization complete. Final v2v error: 3.3058323860168457 mm

Highest mean error: 3.8341917991638184 mm for frame 72

Lowest mean error: 2.9006896018981934 mm for frame 4

Saving results

Total time: 333.8530225753784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0021
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738932
Iteration 2/25 | Loss: 0.00119902
Iteration 3/25 | Loss: 0.00097954
Iteration 4/25 | Loss: 0.00093310
Iteration 5/25 | Loss: 0.00091687
Iteration 6/25 | Loss: 0.00091130
Iteration 7/25 | Loss: 0.00090817
Iteration 8/25 | Loss: 0.00090674
Iteration 9/25 | Loss: 0.00091419
Iteration 10/25 | Loss: 0.00091007
Iteration 11/25 | Loss: 0.00090942
Iteration 12/25 | Loss: 0.00090169
Iteration 13/25 | Loss: 0.00089941
Iteration 14/25 | Loss: 0.00089717
Iteration 15/25 | Loss: 0.00090005
Iteration 16/25 | Loss: 0.00089913
Iteration 17/25 | Loss: 0.00089961
Iteration 18/25 | Loss: 0.00089915
Iteration 19/25 | Loss: 0.00090017
Iteration 20/25 | Loss: 0.00089929
Iteration 21/25 | Loss: 0.00090024
Iteration 22/25 | Loss: 0.00089921
Iteration 23/25 | Loss: 0.00090009
Iteration 24/25 | Loss: 0.00089953
Iteration 25/25 | Loss: 0.00090020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57564723
Iteration 2/25 | Loss: 0.00049354
Iteration 3/25 | Loss: 0.00049354
Iteration 4/25 | Loss: 0.00049354
Iteration 5/25 | Loss: 0.00049354
Iteration 6/25 | Loss: 0.00049354
Iteration 7/25 | Loss: 0.00049354
Iteration 8/25 | Loss: 0.00049354
Iteration 9/25 | Loss: 0.00049354
Iteration 10/25 | Loss: 0.00049354
Iteration 11/25 | Loss: 0.00049354
Iteration 12/25 | Loss: 0.00049354
Iteration 13/25 | Loss: 0.00049354
Iteration 14/25 | Loss: 0.00049354
Iteration 15/25 | Loss: 0.00049354
Iteration 16/25 | Loss: 0.00049354
Iteration 17/25 | Loss: 0.00049354
Iteration 18/25 | Loss: 0.00049354
Iteration 19/25 | Loss: 0.00049354
Iteration 20/25 | Loss: 0.00049354
Iteration 21/25 | Loss: 0.00049354
Iteration 22/25 | Loss: 0.00049354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004935406614094973, 0.0004935406614094973, 0.0004935406614094973, 0.0004935406614094973, 0.0004935406614094973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004935406614094973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049354
Iteration 2/1000 | Loss: 0.00006547
Iteration 3/1000 | Loss: 0.00004459
Iteration 4/1000 | Loss: 0.00003739
Iteration 5/1000 | Loss: 0.00003311
Iteration 6/1000 | Loss: 0.00003009
Iteration 7/1000 | Loss: 0.00002802
Iteration 8/1000 | Loss: 0.00002637
Iteration 9/1000 | Loss: 0.00002549
Iteration 10/1000 | Loss: 0.00002504
Iteration 11/1000 | Loss: 0.00002478
Iteration 12/1000 | Loss: 0.00002452
Iteration 13/1000 | Loss: 0.00002423
Iteration 14/1000 | Loss: 0.00005729
Iteration 15/1000 | Loss: 0.00004403
Iteration 16/1000 | Loss: 0.00005402
Iteration 17/1000 | Loss: 0.00004222
Iteration 18/1000 | Loss: 0.00005630
Iteration 19/1000 | Loss: 0.00004332
Iteration 20/1000 | Loss: 0.00005619
Iteration 21/1000 | Loss: 0.00004001
Iteration 22/1000 | Loss: 0.00005540
Iteration 23/1000 | Loss: 0.00003674
Iteration 24/1000 | Loss: 0.00002457
Iteration 25/1000 | Loss: 0.00002360
Iteration 26/1000 | Loss: 0.00002353
Iteration 27/1000 | Loss: 0.00002347
Iteration 28/1000 | Loss: 0.00002347
Iteration 29/1000 | Loss: 0.00002344
Iteration 30/1000 | Loss: 0.00002342
Iteration 31/1000 | Loss: 0.00002342
Iteration 32/1000 | Loss: 0.00002341
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002340
Iteration 35/1000 | Loss: 0.00002340
Iteration 36/1000 | Loss: 0.00002340
Iteration 37/1000 | Loss: 0.00002339
Iteration 38/1000 | Loss: 0.00002338
Iteration 39/1000 | Loss: 0.00002338
Iteration 40/1000 | Loss: 0.00002338
Iteration 41/1000 | Loss: 0.00002337
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002336
Iteration 45/1000 | Loss: 0.00002336
Iteration 46/1000 | Loss: 0.00002336
Iteration 47/1000 | Loss: 0.00002335
Iteration 48/1000 | Loss: 0.00002335
Iteration 49/1000 | Loss: 0.00002332
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002329
Iteration 53/1000 | Loss: 0.00002329
Iteration 54/1000 | Loss: 0.00002328
Iteration 55/1000 | Loss: 0.00002328
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002327
Iteration 59/1000 | Loss: 0.00002327
Iteration 60/1000 | Loss: 0.00002326
Iteration 61/1000 | Loss: 0.00002326
Iteration 62/1000 | Loss: 0.00002326
Iteration 63/1000 | Loss: 0.00002325
Iteration 64/1000 | Loss: 0.00002325
Iteration 65/1000 | Loss: 0.00002325
Iteration 66/1000 | Loss: 0.00002325
Iteration 67/1000 | Loss: 0.00002325
Iteration 68/1000 | Loss: 0.00002324
Iteration 69/1000 | Loss: 0.00002324
Iteration 70/1000 | Loss: 0.00002324
Iteration 71/1000 | Loss: 0.00002324
Iteration 72/1000 | Loss: 0.00002324
Iteration 73/1000 | Loss: 0.00002324
Iteration 74/1000 | Loss: 0.00005806
Iteration 75/1000 | Loss: 0.00005806
Iteration 76/1000 | Loss: 0.00003979
Iteration 77/1000 | Loss: 0.00002347
Iteration 78/1000 | Loss: 0.00005556
Iteration 79/1000 | Loss: 0.00003791
Iteration 80/1000 | Loss: 0.00005752
Iteration 81/1000 | Loss: 0.00004465
Iteration 82/1000 | Loss: 0.00006073
Iteration 83/1000 | Loss: 0.00003604
Iteration 84/1000 | Loss: 0.00007072
Iteration 85/1000 | Loss: 0.00005413
Iteration 86/1000 | Loss: 0.00005673
Iteration 87/1000 | Loss: 0.00005356
Iteration 88/1000 | Loss: 0.00006454
Iteration 89/1000 | Loss: 0.00003344
Iteration 90/1000 | Loss: 0.00002948
Iteration 91/1000 | Loss: 0.00002733
Iteration 92/1000 | Loss: 0.00019383
Iteration 93/1000 | Loss: 0.00003497
Iteration 94/1000 | Loss: 0.00003848
Iteration 95/1000 | Loss: 0.00003581
Iteration 96/1000 | Loss: 0.00004765
Iteration 97/1000 | Loss: 0.00004949
Iteration 98/1000 | Loss: 0.00002949
Iteration 99/1000 | Loss: 0.00002711
Iteration 100/1000 | Loss: 0.00002653
Iteration 101/1000 | Loss: 0.00002609
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002396
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002355
Iteration 106/1000 | Loss: 0.00002346
Iteration 107/1000 | Loss: 0.00002344
Iteration 108/1000 | Loss: 0.00002343
Iteration 109/1000 | Loss: 0.00002343
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002341
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Iteration 116/1000 | Loss: 0.00002338
Iteration 117/1000 | Loss: 0.00002336
Iteration 118/1000 | Loss: 0.00002336
Iteration 119/1000 | Loss: 0.00002334
Iteration 120/1000 | Loss: 0.00002334
Iteration 121/1000 | Loss: 0.00002333
Iteration 122/1000 | Loss: 0.00002333
Iteration 123/1000 | Loss: 0.00002332
Iteration 124/1000 | Loss: 0.00002332
Iteration 125/1000 | Loss: 0.00002332
Iteration 126/1000 | Loss: 0.00002332
Iteration 127/1000 | Loss: 0.00002332
Iteration 128/1000 | Loss: 0.00002331
Iteration 129/1000 | Loss: 0.00002331
Iteration 130/1000 | Loss: 0.00002330
Iteration 131/1000 | Loss: 0.00002330
Iteration 132/1000 | Loss: 0.00002329
Iteration 133/1000 | Loss: 0.00002329
Iteration 134/1000 | Loss: 0.00002329
Iteration 135/1000 | Loss: 0.00002328
Iteration 136/1000 | Loss: 0.00002328
Iteration 137/1000 | Loss: 0.00002328
Iteration 138/1000 | Loss: 0.00002328
Iteration 139/1000 | Loss: 0.00002327
Iteration 140/1000 | Loss: 0.00002327
Iteration 141/1000 | Loss: 0.00002326
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002326
Iteration 144/1000 | Loss: 0.00002325
Iteration 145/1000 | Loss: 0.00002325
Iteration 146/1000 | Loss: 0.00002325
Iteration 147/1000 | Loss: 0.00002325
Iteration 148/1000 | Loss: 0.00002325
Iteration 149/1000 | Loss: 0.00002325
Iteration 150/1000 | Loss: 0.00002325
Iteration 151/1000 | Loss: 0.00002325
Iteration 152/1000 | Loss: 0.00002325
Iteration 153/1000 | Loss: 0.00002325
Iteration 154/1000 | Loss: 0.00002325
Iteration 155/1000 | Loss: 0.00002324
Iteration 156/1000 | Loss: 0.00002324
Iteration 157/1000 | Loss: 0.00002324
Iteration 158/1000 | Loss: 0.00002324
Iteration 159/1000 | Loss: 0.00002324
Iteration 160/1000 | Loss: 0.00002324
Iteration 161/1000 | Loss: 0.00002323
Iteration 162/1000 | Loss: 0.00002323
Iteration 163/1000 | Loss: 0.00002323
Iteration 164/1000 | Loss: 0.00002323
Iteration 165/1000 | Loss: 0.00002323
Iteration 166/1000 | Loss: 0.00002323
Iteration 167/1000 | Loss: 0.00002322
Iteration 168/1000 | Loss: 0.00002322
Iteration 169/1000 | Loss: 0.00002322
Iteration 170/1000 | Loss: 0.00002322
Iteration 171/1000 | Loss: 0.00002322
Iteration 172/1000 | Loss: 0.00002322
Iteration 173/1000 | Loss: 0.00002321
Iteration 174/1000 | Loss: 0.00002321
Iteration 175/1000 | Loss: 0.00002321
Iteration 176/1000 | Loss: 0.00002321
Iteration 177/1000 | Loss: 0.00002321
Iteration 178/1000 | Loss: 0.00002321
Iteration 179/1000 | Loss: 0.00002321
Iteration 180/1000 | Loss: 0.00002321
Iteration 181/1000 | Loss: 0.00002321
Iteration 182/1000 | Loss: 0.00002321
Iteration 183/1000 | Loss: 0.00002321
Iteration 184/1000 | Loss: 0.00002321
Iteration 185/1000 | Loss: 0.00002321
Iteration 186/1000 | Loss: 0.00002321
Iteration 187/1000 | Loss: 0.00002321
Iteration 188/1000 | Loss: 0.00002320
Iteration 189/1000 | Loss: 0.00002320
Iteration 190/1000 | Loss: 0.00002319
Iteration 191/1000 | Loss: 0.00002318
Iteration 192/1000 | Loss: 0.00002318
Iteration 193/1000 | Loss: 0.00002318
Iteration 194/1000 | Loss: 0.00002317
Iteration 195/1000 | Loss: 0.00002316
Iteration 196/1000 | Loss: 0.00002316
Iteration 197/1000 | Loss: 0.00002316
Iteration 198/1000 | Loss: 0.00002315
Iteration 199/1000 | Loss: 0.00002315
Iteration 200/1000 | Loss: 0.00002313
Iteration 201/1000 | Loss: 0.00002313
Iteration 202/1000 | Loss: 0.00002312
Iteration 203/1000 | Loss: 0.00002312
Iteration 204/1000 | Loss: 0.00002311
Iteration 205/1000 | Loss: 0.00002311
Iteration 206/1000 | Loss: 0.00002311
Iteration 207/1000 | Loss: 0.00002310
Iteration 208/1000 | Loss: 0.00002310
Iteration 209/1000 | Loss: 0.00002309
Iteration 210/1000 | Loss: 0.00002309
Iteration 211/1000 | Loss: 0.00002308
Iteration 212/1000 | Loss: 0.00002308
Iteration 213/1000 | Loss: 0.00002308
Iteration 214/1000 | Loss: 0.00002308
Iteration 215/1000 | Loss: 0.00002308
Iteration 216/1000 | Loss: 0.00002307
Iteration 217/1000 | Loss: 0.00002307
Iteration 218/1000 | Loss: 0.00002307
Iteration 219/1000 | Loss: 0.00002305
Iteration 220/1000 | Loss: 0.00002305
Iteration 221/1000 | Loss: 0.00002304
Iteration 222/1000 | Loss: 0.00002302
Iteration 223/1000 | Loss: 0.00002302
Iteration 224/1000 | Loss: 0.00002302
Iteration 225/1000 | Loss: 0.00002302
Iteration 226/1000 | Loss: 0.00002301
Iteration 227/1000 | Loss: 0.00002301
Iteration 228/1000 | Loss: 0.00002301
Iteration 229/1000 | Loss: 0.00002300
Iteration 230/1000 | Loss: 0.00002300
Iteration 231/1000 | Loss: 0.00002300
Iteration 232/1000 | Loss: 0.00002300
Iteration 233/1000 | Loss: 0.00002299
Iteration 234/1000 | Loss: 0.00002299
Iteration 235/1000 | Loss: 0.00002299
Iteration 236/1000 | Loss: 0.00002299
Iteration 237/1000 | Loss: 0.00002299
Iteration 238/1000 | Loss: 0.00002298
Iteration 239/1000 | Loss: 0.00002298
Iteration 240/1000 | Loss: 0.00002298
Iteration 241/1000 | Loss: 0.00002298
Iteration 242/1000 | Loss: 0.00002298
Iteration 243/1000 | Loss: 0.00002298
Iteration 244/1000 | Loss: 0.00002298
Iteration 245/1000 | Loss: 0.00002298
Iteration 246/1000 | Loss: 0.00002298
Iteration 247/1000 | Loss: 0.00002298
Iteration 248/1000 | Loss: 0.00002297
Iteration 249/1000 | Loss: 0.00002297
Iteration 250/1000 | Loss: 0.00002297
Iteration 251/1000 | Loss: 0.00002297
Iteration 252/1000 | Loss: 0.00002297
Iteration 253/1000 | Loss: 0.00002297
Iteration 254/1000 | Loss: 0.00002297
Iteration 255/1000 | Loss: 0.00002297
Iteration 256/1000 | Loss: 0.00002297
Iteration 257/1000 | Loss: 0.00002297
Iteration 258/1000 | Loss: 0.00002297
Iteration 259/1000 | Loss: 0.00002297
Iteration 260/1000 | Loss: 0.00002297
Iteration 261/1000 | Loss: 0.00002297
Iteration 262/1000 | Loss: 0.00002297
Iteration 263/1000 | Loss: 0.00002297
Iteration 264/1000 | Loss: 0.00002297
Iteration 265/1000 | Loss: 0.00002297
Iteration 266/1000 | Loss: 0.00002297
Iteration 267/1000 | Loss: 0.00002297
Iteration 268/1000 | Loss: 0.00002297
Iteration 269/1000 | Loss: 0.00002296
Iteration 270/1000 | Loss: 0.00002296
Iteration 271/1000 | Loss: 0.00002296
Iteration 272/1000 | Loss: 0.00002296
Iteration 273/1000 | Loss: 0.00002296
Iteration 274/1000 | Loss: 0.00002296
Iteration 275/1000 | Loss: 0.00002296
Iteration 276/1000 | Loss: 0.00002296
Iteration 277/1000 | Loss: 0.00002296
Iteration 278/1000 | Loss: 0.00002296
Iteration 279/1000 | Loss: 0.00002296
Iteration 280/1000 | Loss: 0.00002296
Iteration 281/1000 | Loss: 0.00002296
Iteration 282/1000 | Loss: 0.00002296
Iteration 283/1000 | Loss: 0.00002296
Iteration 284/1000 | Loss: 0.00002296
Iteration 285/1000 | Loss: 0.00002296
Iteration 286/1000 | Loss: 0.00002296
Iteration 287/1000 | Loss: 0.00002296
Iteration 288/1000 | Loss: 0.00002296
Iteration 289/1000 | Loss: 0.00002296
Iteration 290/1000 | Loss: 0.00002296
Iteration 291/1000 | Loss: 0.00002296
Iteration 292/1000 | Loss: 0.00002296
Iteration 293/1000 | Loss: 0.00002296
Iteration 294/1000 | Loss: 0.00002296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.295505692018196e-05, 2.295505692018196e-05, 2.295505692018196e-05, 2.295505692018196e-05, 2.295505692018196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.295505692018196e-05

Optimization complete. Final v2v error: 3.9628288745880127 mm

Highest mean error: 5.285053253173828 mm for frame 104

Lowest mean error: 3.223607063293457 mm for frame 1

Saving results

Total time: 846.723052740097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0013
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537083
Iteration 2/25 | Loss: 0.00121570
Iteration 3/25 | Loss: 0.00099012
Iteration 4/25 | Loss: 0.00095963
Iteration 5/25 | Loss: 0.00095161
Iteration 6/25 | Loss: 0.00095072
Iteration 7/25 | Loss: 0.00095072
Iteration 8/25 | Loss: 0.00095072
Iteration 9/25 | Loss: 0.00095072
Iteration 10/25 | Loss: 0.00095072
Iteration 11/25 | Loss: 0.00095072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009507171344012022, 0.0009507171344012022, 0.0009507171344012022, 0.0009507171344012022, 0.0009507171344012022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009507171344012022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63646036
Iteration 2/25 | Loss: 0.00049387
Iteration 3/25 | Loss: 0.00049386
Iteration 4/25 | Loss: 0.00049386
Iteration 5/25 | Loss: 0.00049386
Iteration 6/25 | Loss: 0.00049386
Iteration 7/25 | Loss: 0.00049386
Iteration 8/25 | Loss: 0.00049386
Iteration 9/25 | Loss: 0.00049386
Iteration 10/25 | Loss: 0.00049386
Iteration 11/25 | Loss: 0.00049386
Iteration 12/25 | Loss: 0.00049386
Iteration 13/25 | Loss: 0.00049386
Iteration 14/25 | Loss: 0.00049386
Iteration 15/25 | Loss: 0.00049386
Iteration 16/25 | Loss: 0.00049386
Iteration 17/25 | Loss: 0.00049386
Iteration 18/25 | Loss: 0.00049386
Iteration 19/25 | Loss: 0.00049386
Iteration 20/25 | Loss: 0.00049386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004938598140142858, 0.0004938598140142858, 0.0004938598140142858, 0.0004938598140142858, 0.0004938598140142858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004938598140142858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049386
Iteration 2/1000 | Loss: 0.00005994
Iteration 3/1000 | Loss: 0.00004410
Iteration 4/1000 | Loss: 0.00003819
Iteration 5/1000 | Loss: 0.00003529
Iteration 6/1000 | Loss: 0.00003411
Iteration 7/1000 | Loss: 0.00003334
Iteration 8/1000 | Loss: 0.00003290
Iteration 9/1000 | Loss: 0.00003219
Iteration 10/1000 | Loss: 0.00003172
Iteration 11/1000 | Loss: 0.00003142
Iteration 12/1000 | Loss: 0.00003120
Iteration 13/1000 | Loss: 0.00003105
Iteration 14/1000 | Loss: 0.00003087
Iteration 15/1000 | Loss: 0.00003069
Iteration 16/1000 | Loss: 0.00003052
Iteration 17/1000 | Loss: 0.00003048
Iteration 18/1000 | Loss: 0.00003043
Iteration 19/1000 | Loss: 0.00003042
Iteration 20/1000 | Loss: 0.00003042
Iteration 21/1000 | Loss: 0.00003041
Iteration 22/1000 | Loss: 0.00003040
Iteration 23/1000 | Loss: 0.00003040
Iteration 24/1000 | Loss: 0.00003039
Iteration 25/1000 | Loss: 0.00003039
Iteration 26/1000 | Loss: 0.00003039
Iteration 27/1000 | Loss: 0.00003038
Iteration 28/1000 | Loss: 0.00003038
Iteration 29/1000 | Loss: 0.00003038
Iteration 30/1000 | Loss: 0.00003038
Iteration 31/1000 | Loss: 0.00003038
Iteration 32/1000 | Loss: 0.00003037
Iteration 33/1000 | Loss: 0.00003037
Iteration 34/1000 | Loss: 0.00003036
Iteration 35/1000 | Loss: 0.00003036
Iteration 36/1000 | Loss: 0.00003036
Iteration 37/1000 | Loss: 0.00003036
Iteration 38/1000 | Loss: 0.00003035
Iteration 39/1000 | Loss: 0.00003035
Iteration 40/1000 | Loss: 0.00003035
Iteration 41/1000 | Loss: 0.00003034
Iteration 42/1000 | Loss: 0.00003034
Iteration 43/1000 | Loss: 0.00003034
Iteration 44/1000 | Loss: 0.00003033
Iteration 45/1000 | Loss: 0.00003032
Iteration 46/1000 | Loss: 0.00003032
Iteration 47/1000 | Loss: 0.00003032
Iteration 48/1000 | Loss: 0.00003031
Iteration 49/1000 | Loss: 0.00003031
Iteration 50/1000 | Loss: 0.00003031
Iteration 51/1000 | Loss: 0.00003030
Iteration 52/1000 | Loss: 0.00003030
Iteration 53/1000 | Loss: 0.00003030
Iteration 54/1000 | Loss: 0.00003030
Iteration 55/1000 | Loss: 0.00003030
Iteration 56/1000 | Loss: 0.00003030
Iteration 57/1000 | Loss: 0.00003029
Iteration 58/1000 | Loss: 0.00003029
Iteration 59/1000 | Loss: 0.00003029
Iteration 60/1000 | Loss: 0.00003029
Iteration 61/1000 | Loss: 0.00003029
Iteration 62/1000 | Loss: 0.00003029
Iteration 63/1000 | Loss: 0.00003029
Iteration 64/1000 | Loss: 0.00003028
Iteration 65/1000 | Loss: 0.00003028
Iteration 66/1000 | Loss: 0.00003028
Iteration 67/1000 | Loss: 0.00003028
Iteration 68/1000 | Loss: 0.00003028
Iteration 69/1000 | Loss: 0.00003028
Iteration 70/1000 | Loss: 0.00003028
Iteration 71/1000 | Loss: 0.00003028
Iteration 72/1000 | Loss: 0.00003028
Iteration 73/1000 | Loss: 0.00003028
Iteration 74/1000 | Loss: 0.00003028
Iteration 75/1000 | Loss: 0.00003028
Iteration 76/1000 | Loss: 0.00003028
Iteration 77/1000 | Loss: 0.00003027
Iteration 78/1000 | Loss: 0.00003027
Iteration 79/1000 | Loss: 0.00003027
Iteration 80/1000 | Loss: 0.00003027
Iteration 81/1000 | Loss: 0.00003027
Iteration 82/1000 | Loss: 0.00003027
Iteration 83/1000 | Loss: 0.00003027
Iteration 84/1000 | Loss: 0.00003026
Iteration 85/1000 | Loss: 0.00003026
Iteration 86/1000 | Loss: 0.00003026
Iteration 87/1000 | Loss: 0.00003026
Iteration 88/1000 | Loss: 0.00003026
Iteration 89/1000 | Loss: 0.00003026
Iteration 90/1000 | Loss: 0.00003026
Iteration 91/1000 | Loss: 0.00003026
Iteration 92/1000 | Loss: 0.00003026
Iteration 93/1000 | Loss: 0.00003025
Iteration 94/1000 | Loss: 0.00003025
Iteration 95/1000 | Loss: 0.00003024
Iteration 96/1000 | Loss: 0.00003024
Iteration 97/1000 | Loss: 0.00003024
Iteration 98/1000 | Loss: 0.00003024
Iteration 99/1000 | Loss: 0.00003023
Iteration 100/1000 | Loss: 0.00003023
Iteration 101/1000 | Loss: 0.00003023
Iteration 102/1000 | Loss: 0.00003023
Iteration 103/1000 | Loss: 0.00003022
Iteration 104/1000 | Loss: 0.00003022
Iteration 105/1000 | Loss: 0.00003022
Iteration 106/1000 | Loss: 0.00003022
Iteration 107/1000 | Loss: 0.00003022
Iteration 108/1000 | Loss: 0.00003022
Iteration 109/1000 | Loss: 0.00003021
Iteration 110/1000 | Loss: 0.00003021
Iteration 111/1000 | Loss: 0.00003021
Iteration 112/1000 | Loss: 0.00003021
Iteration 113/1000 | Loss: 0.00003021
Iteration 114/1000 | Loss: 0.00003021
Iteration 115/1000 | Loss: 0.00003020
Iteration 116/1000 | Loss: 0.00003020
Iteration 117/1000 | Loss: 0.00003020
Iteration 118/1000 | Loss: 0.00003020
Iteration 119/1000 | Loss: 0.00003020
Iteration 120/1000 | Loss: 0.00003020
Iteration 121/1000 | Loss: 0.00003019
Iteration 122/1000 | Loss: 0.00003019
Iteration 123/1000 | Loss: 0.00003019
Iteration 124/1000 | Loss: 0.00003019
Iteration 125/1000 | Loss: 0.00003019
Iteration 126/1000 | Loss: 0.00003019
Iteration 127/1000 | Loss: 0.00003019
Iteration 128/1000 | Loss: 0.00003019
Iteration 129/1000 | Loss: 0.00003019
Iteration 130/1000 | Loss: 0.00003018
Iteration 131/1000 | Loss: 0.00003018
Iteration 132/1000 | Loss: 0.00003018
Iteration 133/1000 | Loss: 0.00003018
Iteration 134/1000 | Loss: 0.00003017
Iteration 135/1000 | Loss: 0.00003017
Iteration 136/1000 | Loss: 0.00003016
Iteration 137/1000 | Loss: 0.00003016
Iteration 138/1000 | Loss: 0.00003016
Iteration 139/1000 | Loss: 0.00003016
Iteration 140/1000 | Loss: 0.00003016
Iteration 141/1000 | Loss: 0.00003016
Iteration 142/1000 | Loss: 0.00003016
Iteration 143/1000 | Loss: 0.00003015
Iteration 144/1000 | Loss: 0.00003015
Iteration 145/1000 | Loss: 0.00003014
Iteration 146/1000 | Loss: 0.00003014
Iteration 147/1000 | Loss: 0.00003014
Iteration 148/1000 | Loss: 0.00003014
Iteration 149/1000 | Loss: 0.00003014
Iteration 150/1000 | Loss: 0.00003014
Iteration 151/1000 | Loss: 0.00003013
Iteration 152/1000 | Loss: 0.00003013
Iteration 153/1000 | Loss: 0.00003013
Iteration 154/1000 | Loss: 0.00003013
Iteration 155/1000 | Loss: 0.00003013
Iteration 156/1000 | Loss: 0.00003012
Iteration 157/1000 | Loss: 0.00003012
Iteration 158/1000 | Loss: 0.00003012
Iteration 159/1000 | Loss: 0.00003012
Iteration 160/1000 | Loss: 0.00003012
Iteration 161/1000 | Loss: 0.00003012
Iteration 162/1000 | Loss: 0.00003012
Iteration 163/1000 | Loss: 0.00003011
Iteration 164/1000 | Loss: 0.00003011
Iteration 165/1000 | Loss: 0.00003011
Iteration 166/1000 | Loss: 0.00003011
Iteration 167/1000 | Loss: 0.00003011
Iteration 168/1000 | Loss: 0.00003010
Iteration 169/1000 | Loss: 0.00003010
Iteration 170/1000 | Loss: 0.00003010
Iteration 171/1000 | Loss: 0.00003010
Iteration 172/1000 | Loss: 0.00003010
Iteration 173/1000 | Loss: 0.00003010
Iteration 174/1000 | Loss: 0.00003010
Iteration 175/1000 | Loss: 0.00003010
Iteration 176/1000 | Loss: 0.00003010
Iteration 177/1000 | Loss: 0.00003010
Iteration 178/1000 | Loss: 0.00003010
Iteration 179/1000 | Loss: 0.00003009
Iteration 180/1000 | Loss: 0.00003009
Iteration 181/1000 | Loss: 0.00003009
Iteration 182/1000 | Loss: 0.00003009
Iteration 183/1000 | Loss: 0.00003009
Iteration 184/1000 | Loss: 0.00003009
Iteration 185/1000 | Loss: 0.00003009
Iteration 186/1000 | Loss: 0.00003009
Iteration 187/1000 | Loss: 0.00003009
Iteration 188/1000 | Loss: 0.00003009
Iteration 189/1000 | Loss: 0.00003009
Iteration 190/1000 | Loss: 0.00003009
Iteration 191/1000 | Loss: 0.00003009
Iteration 192/1000 | Loss: 0.00003009
Iteration 193/1000 | Loss: 0.00003008
Iteration 194/1000 | Loss: 0.00003008
Iteration 195/1000 | Loss: 0.00003008
Iteration 196/1000 | Loss: 0.00003008
Iteration 197/1000 | Loss: 0.00003008
Iteration 198/1000 | Loss: 0.00003008
Iteration 199/1000 | Loss: 0.00003008
Iteration 200/1000 | Loss: 0.00003008
Iteration 201/1000 | Loss: 0.00003008
Iteration 202/1000 | Loss: 0.00003008
Iteration 203/1000 | Loss: 0.00003008
Iteration 204/1000 | Loss: 0.00003008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.0081968361628242e-05, 3.0081968361628242e-05, 3.0081968361628242e-05, 3.0081968361628242e-05, 3.0081968361628242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0081968361628242e-05

Optimization complete. Final v2v error: 4.6938982009887695 mm

Highest mean error: 4.897734642028809 mm for frame 132

Lowest mean error: 4.275855541229248 mm for frame 12

Saving results

Total time: 484.6267206668854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0005
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393646
Iteration 2/25 | Loss: 0.00098066
Iteration 3/25 | Loss: 0.00086107
Iteration 4/25 | Loss: 0.00083470
Iteration 5/25 | Loss: 0.00082342
Iteration 6/25 | Loss: 0.00081996
Iteration 7/25 | Loss: 0.00081877
Iteration 8/25 | Loss: 0.00081846
Iteration 9/25 | Loss: 0.00081846
Iteration 10/25 | Loss: 0.00081846
Iteration 11/25 | Loss: 0.00081846
Iteration 12/25 | Loss: 0.00081846
Iteration 13/25 | Loss: 0.00081846
Iteration 14/25 | Loss: 0.00081846
Iteration 15/25 | Loss: 0.00081846
Iteration 16/25 | Loss: 0.00081846
Iteration 17/25 | Loss: 0.00081846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008184559992514551, 0.0008184559992514551, 0.0008184559992514551, 0.0008184559992514551, 0.0008184559992514551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008184559992514551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39570713
Iteration 2/25 | Loss: 0.00032378
Iteration 3/25 | Loss: 0.00032378
Iteration 4/25 | Loss: 0.00032378
Iteration 5/25 | Loss: 0.00032377
Iteration 6/25 | Loss: 0.00032377
Iteration 7/25 | Loss: 0.00032377
Iteration 8/25 | Loss: 0.00032377
Iteration 9/25 | Loss: 0.00032377
Iteration 10/25 | Loss: 0.00032377
Iteration 11/25 | Loss: 0.00032377
Iteration 12/25 | Loss: 0.00032377
Iteration 13/25 | Loss: 0.00032377
Iteration 14/25 | Loss: 0.00032377
Iteration 15/25 | Loss: 0.00032377
Iteration 16/25 | Loss: 0.00032377
Iteration 17/25 | Loss: 0.00032377
Iteration 18/25 | Loss: 0.00032377
Iteration 19/25 | Loss: 0.00032377
Iteration 20/25 | Loss: 0.00032377
Iteration 21/25 | Loss: 0.00032377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00032377245952375233, 0.00032377245952375233, 0.00032377245952375233, 0.00032377245952375233, 0.00032377245952375233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032377245952375233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032377
Iteration 2/1000 | Loss: 0.00005663
Iteration 3/1000 | Loss: 0.00003372
Iteration 4/1000 | Loss: 0.00002647
Iteration 5/1000 | Loss: 0.00002229
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001950
Iteration 8/1000 | Loss: 0.00001872
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00001747
Iteration 13/1000 | Loss: 0.00001747
Iteration 14/1000 | Loss: 0.00001742
Iteration 15/1000 | Loss: 0.00001741
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001716
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001714
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001712
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001698
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001692
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001691
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001690
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001689
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001687
Iteration 45/1000 | Loss: 0.00001687
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001685
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001683
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001682
Iteration 62/1000 | Loss: 0.00001682
Iteration 63/1000 | Loss: 0.00001682
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001680
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001680
Iteration 87/1000 | Loss: 0.00001680
Iteration 88/1000 | Loss: 0.00001680
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.6798008800833486e-05, 1.6798008800833486e-05, 1.6798008800833486e-05, 1.6798008800833486e-05, 1.6798008800833486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6798008800833486e-05

Optimization complete. Final v2v error: 3.431511402130127 mm

Highest mean error: 4.190803050994873 mm for frame 93

Lowest mean error: 2.800360918045044 mm for frame 107

Saving results

Total time: 256.6341791152954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0010
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088074
Iteration 2/25 | Loss: 0.01088074
Iteration 3/25 | Loss: 0.01088074
Iteration 4/25 | Loss: 0.01088074
Iteration 5/25 | Loss: 0.01088074
Iteration 6/25 | Loss: 0.01088073
Iteration 7/25 | Loss: 0.01088073
Iteration 8/25 | Loss: 0.01088073
Iteration 9/25 | Loss: 0.01088073
Iteration 10/25 | Loss: 0.01088073
Iteration 11/25 | Loss: 0.01088073
Iteration 12/25 | Loss: 0.01088073
Iteration 13/25 | Loss: 0.01088073
Iteration 14/25 | Loss: 0.01088073
Iteration 15/25 | Loss: 0.01088073
Iteration 16/25 | Loss: 0.01088073
Iteration 17/25 | Loss: 0.01088073
Iteration 18/25 | Loss: 0.01088073
Iteration 19/25 | Loss: 0.01088073
Iteration 20/25 | Loss: 0.01088072
Iteration 21/25 | Loss: 0.01088072
Iteration 22/25 | Loss: 0.01088072
Iteration 23/25 | Loss: 0.01088072
Iteration 24/25 | Loss: 0.01088072
Iteration 25/25 | Loss: 0.01088072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69434655
Iteration 2/25 | Loss: 0.14043494
Iteration 3/25 | Loss: 0.10420401
Iteration 4/25 | Loss: 0.10558656
Iteration 5/25 | Loss: 0.10301724
Iteration 6/25 | Loss: 0.10301723
Iteration 7/25 | Loss: 0.10301723
Iteration 8/25 | Loss: 0.10301721
Iteration 9/25 | Loss: 0.10301721
Iteration 10/25 | Loss: 0.10301721
Iteration 11/25 | Loss: 0.10301721
Iteration 12/25 | Loss: 0.10301723
Iteration 13/25 | Loss: 0.10301720
Iteration 14/25 | Loss: 0.10301720
Iteration 15/25 | Loss: 0.10301720
Iteration 16/25 | Loss: 0.10301720
Iteration 17/25 | Loss: 0.10301720
Iteration 18/25 | Loss: 0.10301720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.10301720350980759, 0.10301720350980759, 0.10301720350980759, 0.10301720350980759, 0.10301720350980759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10301720350980759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10301720
Iteration 2/1000 | Loss: 0.00422925
Iteration 3/1000 | Loss: 0.00445369
Iteration 4/1000 | Loss: 0.00169741
Iteration 5/1000 | Loss: 0.00078594
Iteration 6/1000 | Loss: 0.00068178
Iteration 7/1000 | Loss: 0.00013636
Iteration 8/1000 | Loss: 0.00014624
Iteration 9/1000 | Loss: 0.00030700
Iteration 10/1000 | Loss: 0.00055804
Iteration 11/1000 | Loss: 0.00120222
Iteration 12/1000 | Loss: 0.00007797
Iteration 13/1000 | Loss: 0.00009675
Iteration 14/1000 | Loss: 0.00201461
Iteration 15/1000 | Loss: 0.00332237
Iteration 16/1000 | Loss: 0.00007427
Iteration 17/1000 | Loss: 0.00059813
Iteration 18/1000 | Loss: 0.00212566
Iteration 19/1000 | Loss: 0.00016845
Iteration 20/1000 | Loss: 0.00040332
Iteration 21/1000 | Loss: 0.00007769
Iteration 22/1000 | Loss: 0.00020171
Iteration 23/1000 | Loss: 0.00020754
Iteration 24/1000 | Loss: 0.00037917
Iteration 25/1000 | Loss: 0.00004004
Iteration 26/1000 | Loss: 0.00020516
Iteration 27/1000 | Loss: 0.00004952
Iteration 28/1000 | Loss: 0.00023397
Iteration 29/1000 | Loss: 0.00081056
Iteration 30/1000 | Loss: 0.00009963
Iteration 31/1000 | Loss: 0.00011919
Iteration 32/1000 | Loss: 0.00025937
Iteration 33/1000 | Loss: 0.00015578
Iteration 34/1000 | Loss: 0.00003006
Iteration 35/1000 | Loss: 0.00003855
Iteration 36/1000 | Loss: 0.00008915
Iteration 37/1000 | Loss: 0.00004785
Iteration 38/1000 | Loss: 0.00010568
Iteration 39/1000 | Loss: 0.00003937
Iteration 40/1000 | Loss: 0.00004002
Iteration 41/1000 | Loss: 0.00003999
Iteration 42/1000 | Loss: 0.00003998
Iteration 43/1000 | Loss: 0.00011202
Iteration 44/1000 | Loss: 0.00011904
Iteration 45/1000 | Loss: 0.00003044
Iteration 46/1000 | Loss: 0.00004346
Iteration 47/1000 | Loss: 0.00002689
Iteration 48/1000 | Loss: 0.00003410
Iteration 49/1000 | Loss: 0.00002882
Iteration 50/1000 | Loss: 0.00002668
Iteration 51/1000 | Loss: 0.00002668
Iteration 52/1000 | Loss: 0.00002668
Iteration 53/1000 | Loss: 0.00002668
Iteration 54/1000 | Loss: 0.00002668
Iteration 55/1000 | Loss: 0.00002667
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00006761
Iteration 58/1000 | Loss: 0.00006229
Iteration 59/1000 | Loss: 0.00002960
Iteration 60/1000 | Loss: 0.00006576
Iteration 61/1000 | Loss: 0.00006576
Iteration 62/1000 | Loss: 0.00010924
Iteration 63/1000 | Loss: 0.00003862
Iteration 64/1000 | Loss: 0.00002784
Iteration 65/1000 | Loss: 0.00002662
Iteration 66/1000 | Loss: 0.00002641
Iteration 67/1000 | Loss: 0.00002640
Iteration 68/1000 | Loss: 0.00002640
Iteration 69/1000 | Loss: 0.00002640
Iteration 70/1000 | Loss: 0.00002640
Iteration 71/1000 | Loss: 0.00002640
Iteration 72/1000 | Loss: 0.00002640
Iteration 73/1000 | Loss: 0.00002640
Iteration 74/1000 | Loss: 0.00002640
Iteration 75/1000 | Loss: 0.00002640
Iteration 76/1000 | Loss: 0.00002640
Iteration 77/1000 | Loss: 0.00002640
Iteration 78/1000 | Loss: 0.00002640
Iteration 79/1000 | Loss: 0.00004714
Iteration 80/1000 | Loss: 0.00002639
Iteration 81/1000 | Loss: 0.00002639
Iteration 82/1000 | Loss: 0.00002639
Iteration 83/1000 | Loss: 0.00002639
Iteration 84/1000 | Loss: 0.00002639
Iteration 85/1000 | Loss: 0.00002639
Iteration 86/1000 | Loss: 0.00002638
Iteration 87/1000 | Loss: 0.00002638
Iteration 88/1000 | Loss: 0.00002638
Iteration 89/1000 | Loss: 0.00002638
Iteration 90/1000 | Loss: 0.00002638
Iteration 91/1000 | Loss: 0.00002638
Iteration 92/1000 | Loss: 0.00002638
Iteration 93/1000 | Loss: 0.00002637
Iteration 94/1000 | Loss: 0.00002637
Iteration 95/1000 | Loss: 0.00002637
Iteration 96/1000 | Loss: 0.00002636
Iteration 97/1000 | Loss: 0.00002636
Iteration 98/1000 | Loss: 0.00002636
Iteration 99/1000 | Loss: 0.00002636
Iteration 100/1000 | Loss: 0.00002636
Iteration 101/1000 | Loss: 0.00002636
Iteration 102/1000 | Loss: 0.00002636
Iteration 103/1000 | Loss: 0.00002636
Iteration 104/1000 | Loss: 0.00002636
Iteration 105/1000 | Loss: 0.00002636
Iteration 106/1000 | Loss: 0.00002636
Iteration 107/1000 | Loss: 0.00002636
Iteration 108/1000 | Loss: 0.00002636
Iteration 109/1000 | Loss: 0.00002636
Iteration 110/1000 | Loss: 0.00002636
Iteration 111/1000 | Loss: 0.00002635
Iteration 112/1000 | Loss: 0.00002635
Iteration 113/1000 | Loss: 0.00002635
Iteration 114/1000 | Loss: 0.00002635
Iteration 115/1000 | Loss: 0.00002635
Iteration 116/1000 | Loss: 0.00002635
Iteration 117/1000 | Loss: 0.00002635
Iteration 118/1000 | Loss: 0.00002635
Iteration 119/1000 | Loss: 0.00002635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.6352443455834873e-05, 2.6352443455834873e-05, 2.6352443455834873e-05, 2.6352443455834873e-05, 2.6352443455834873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6352443455834873e-05

Optimization complete. Final v2v error: 4.298680782318115 mm

Highest mean error: 4.6860456466674805 mm for frame 0

Lowest mean error: 4.015695095062256 mm for frame 90

Saving results

Total time: 697.2548243999481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0004
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855337
Iteration 2/25 | Loss: 0.00094866
Iteration 3/25 | Loss: 0.00079718
Iteration 4/25 | Loss: 0.00078143
Iteration 5/25 | Loss: 0.00077616
Iteration 6/25 | Loss: 0.00077503
Iteration 7/25 | Loss: 0.00077503
Iteration 8/25 | Loss: 0.00077503
Iteration 9/25 | Loss: 0.00077503
Iteration 10/25 | Loss: 0.00077503
Iteration 11/25 | Loss: 0.00077503
Iteration 12/25 | Loss: 0.00077503
Iteration 13/25 | Loss: 0.00077503
Iteration 14/25 | Loss: 0.00077503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000775027961935848, 0.000775027961935848, 0.000775027961935848, 0.000775027961935848, 0.000775027961935848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000775027961935848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41821730
Iteration 2/25 | Loss: 0.00038354
Iteration 3/25 | Loss: 0.00038354
Iteration 4/25 | Loss: 0.00038354
Iteration 5/25 | Loss: 0.00038354
Iteration 6/25 | Loss: 0.00038354
Iteration 7/25 | Loss: 0.00038354
Iteration 8/25 | Loss: 0.00038354
Iteration 9/25 | Loss: 0.00038354
Iteration 10/25 | Loss: 0.00038354
Iteration 11/25 | Loss: 0.00038354
Iteration 12/25 | Loss: 0.00038354
Iteration 13/25 | Loss: 0.00038354
Iteration 14/25 | Loss: 0.00038354
Iteration 15/25 | Loss: 0.00038354
Iteration 16/25 | Loss: 0.00038354
Iteration 17/25 | Loss: 0.00038354
Iteration 18/25 | Loss: 0.00038354
Iteration 19/25 | Loss: 0.00038354
Iteration 20/25 | Loss: 0.00038354
Iteration 21/25 | Loss: 0.00038354
Iteration 22/25 | Loss: 0.00038354
Iteration 23/25 | Loss: 0.00038354
Iteration 24/25 | Loss: 0.00038354
Iteration 25/25 | Loss: 0.00038354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038354
Iteration 2/1000 | Loss: 0.00003752
Iteration 3/1000 | Loss: 0.00002269
Iteration 4/1000 | Loss: 0.00001969
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001712
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001636
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001633
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001632
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001631
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001629
Iteration 64/1000 | Loss: 0.00001629
Iteration 65/1000 | Loss: 0.00001629
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001629
Iteration 69/1000 | Loss: 0.00001629
Iteration 70/1000 | Loss: 0.00001629
Iteration 71/1000 | Loss: 0.00001629
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001628
Iteration 75/1000 | Loss: 0.00001628
Iteration 76/1000 | Loss: 0.00001628
Iteration 77/1000 | Loss: 0.00001628
Iteration 78/1000 | Loss: 0.00001628
Iteration 79/1000 | Loss: 0.00001628
Iteration 80/1000 | Loss: 0.00001628
Iteration 81/1000 | Loss: 0.00001628
Iteration 82/1000 | Loss: 0.00001627
Iteration 83/1000 | Loss: 0.00001627
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001627
Iteration 86/1000 | Loss: 0.00001627
Iteration 87/1000 | Loss: 0.00001627
Iteration 88/1000 | Loss: 0.00001627
Iteration 89/1000 | Loss: 0.00001627
Iteration 90/1000 | Loss: 0.00001627
Iteration 91/1000 | Loss: 0.00001627
Iteration 92/1000 | Loss: 0.00001627
Iteration 93/1000 | Loss: 0.00001626
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001626
Iteration 96/1000 | Loss: 0.00001626
Iteration 97/1000 | Loss: 0.00001626
Iteration 98/1000 | Loss: 0.00001626
Iteration 99/1000 | Loss: 0.00001626
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001626
Iteration 102/1000 | Loss: 0.00001626
Iteration 103/1000 | Loss: 0.00001626
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001625
Iteration 106/1000 | Loss: 0.00001625
Iteration 107/1000 | Loss: 0.00001625
Iteration 108/1000 | Loss: 0.00001625
Iteration 109/1000 | Loss: 0.00001625
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001625
Iteration 112/1000 | Loss: 0.00001625
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001623
Iteration 133/1000 | Loss: 0.00001623
Iteration 134/1000 | Loss: 0.00001623
Iteration 135/1000 | Loss: 0.00001623
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001622
Iteration 142/1000 | Loss: 0.00001622
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.6176099961739965e-05, 1.6176099961739965e-05, 1.6176099961739965e-05, 1.6176099961739965e-05, 1.6176099961739965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6176099961739965e-05

Optimization complete. Final v2v error: 3.421826124191284 mm

Highest mean error: 3.9312350749969482 mm for frame 163

Lowest mean error: 3.204937696456909 mm for frame 22

Saving results

Total time: 247.35205078125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0017
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030356
Iteration 2/25 | Loss: 0.00202074
Iteration 3/25 | Loss: 0.00149375
Iteration 4/25 | Loss: 0.00136710
Iteration 5/25 | Loss: 0.00127440
Iteration 6/25 | Loss: 0.00125086
Iteration 7/25 | Loss: 0.00138640
Iteration 8/25 | Loss: 0.00114001
Iteration 9/25 | Loss: 0.00098043
Iteration 10/25 | Loss: 0.00093257
Iteration 11/25 | Loss: 0.00091478
Iteration 12/25 | Loss: 0.00087922
Iteration 13/25 | Loss: 0.00086636
Iteration 14/25 | Loss: 0.00087223
Iteration 15/25 | Loss: 0.00086896
Iteration 16/25 | Loss: 0.00087066
Iteration 17/25 | Loss: 0.00086511
Iteration 18/25 | Loss: 0.00085333
Iteration 19/25 | Loss: 0.00085603
Iteration 20/25 | Loss: 0.00086401
Iteration 21/25 | Loss: 0.00085256
Iteration 22/25 | Loss: 0.00084019
Iteration 23/25 | Loss: 0.00083311
Iteration 24/25 | Loss: 0.00083242
Iteration 25/25 | Loss: 0.00083078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42149973
Iteration 2/25 | Loss: 0.00076244
Iteration 3/25 | Loss: 0.00074320
Iteration 4/25 | Loss: 0.00074320
Iteration 5/25 | Loss: 0.00074320
Iteration 6/25 | Loss: 0.00074320
Iteration 7/25 | Loss: 0.00074320
Iteration 8/25 | Loss: 0.00074320
Iteration 9/25 | Loss: 0.00074320
Iteration 10/25 | Loss: 0.00074320
Iteration 11/25 | Loss: 0.00074320
Iteration 12/25 | Loss: 0.00074320
Iteration 13/25 | Loss: 0.00074320
Iteration 14/25 | Loss: 0.00074320
Iteration 15/25 | Loss: 0.00074320
Iteration 16/25 | Loss: 0.00074320
Iteration 17/25 | Loss: 0.00074320
Iteration 18/25 | Loss: 0.00074320
Iteration 19/25 | Loss: 0.00074320
Iteration 20/25 | Loss: 0.00074320
Iteration 21/25 | Loss: 0.00074320
Iteration 22/25 | Loss: 0.00074320
Iteration 23/25 | Loss: 0.00074320
Iteration 24/25 | Loss: 0.00074320
Iteration 25/25 | Loss: 0.00074320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074320
Iteration 2/1000 | Loss: 0.00018198
Iteration 3/1000 | Loss: 0.00022400
Iteration 4/1000 | Loss: 0.00017927
Iteration 5/1000 | Loss: 0.00022751
Iteration 6/1000 | Loss: 0.00015893
Iteration 7/1000 | Loss: 0.00077391
Iteration 8/1000 | Loss: 0.00045273
Iteration 9/1000 | Loss: 0.00061381
Iteration 10/1000 | Loss: 0.00084953
Iteration 11/1000 | Loss: 0.00063712
Iteration 12/1000 | Loss: 0.00094993
Iteration 13/1000 | Loss: 0.00019405
Iteration 14/1000 | Loss: 0.00006557
Iteration 15/1000 | Loss: 0.00187715
Iteration 16/1000 | Loss: 0.00007343
Iteration 17/1000 | Loss: 0.00004935
Iteration 18/1000 | Loss: 0.00004401
Iteration 19/1000 | Loss: 0.00003932
Iteration 20/1000 | Loss: 0.00023111
Iteration 21/1000 | Loss: 0.00008578
Iteration 22/1000 | Loss: 0.00005175
Iteration 23/1000 | Loss: 0.00045482
Iteration 24/1000 | Loss: 0.00004883
Iteration 25/1000 | Loss: 0.00003497
Iteration 26/1000 | Loss: 0.00003159
Iteration 27/1000 | Loss: 0.00002927
Iteration 28/1000 | Loss: 0.00002735
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002570
Iteration 31/1000 | Loss: 0.00002522
Iteration 32/1000 | Loss: 0.00002468
Iteration 33/1000 | Loss: 0.00036037
Iteration 34/1000 | Loss: 0.00007707
Iteration 35/1000 | Loss: 0.00022241
Iteration 36/1000 | Loss: 0.00026945
Iteration 37/1000 | Loss: 0.00003815
Iteration 38/1000 | Loss: 0.00002793
Iteration 39/1000 | Loss: 0.00002516
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00006536
Iteration 42/1000 | Loss: 0.00002185
Iteration 43/1000 | Loss: 0.00002131
Iteration 44/1000 | Loss: 0.00002101
Iteration 45/1000 | Loss: 0.00002091
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002090
Iteration 48/1000 | Loss: 0.00002086
Iteration 49/1000 | Loss: 0.00002084
Iteration 50/1000 | Loss: 0.00002083
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002076
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00002075
Iteration 63/1000 | Loss: 0.00002075
Iteration 64/1000 | Loss: 0.00002075
Iteration 65/1000 | Loss: 0.00002074
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002068
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002068
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002068
Iteration 96/1000 | Loss: 0.00002067
Iteration 97/1000 | Loss: 0.00002067
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002067
Iteration 101/1000 | Loss: 0.00002067
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002066
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002065
Iteration 115/1000 | Loss: 0.00002065
Iteration 116/1000 | Loss: 0.00002065
Iteration 117/1000 | Loss: 0.00002065
Iteration 118/1000 | Loss: 0.00002065
Iteration 119/1000 | Loss: 0.00002065
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002065
Iteration 123/1000 | Loss: 0.00002065
Iteration 124/1000 | Loss: 0.00002065
Iteration 125/1000 | Loss: 0.00002065
Iteration 126/1000 | Loss: 0.00002065
Iteration 127/1000 | Loss: 0.00002065
Iteration 128/1000 | Loss: 0.00002065
Iteration 129/1000 | Loss: 0.00002065
Iteration 130/1000 | Loss: 0.00002064
Iteration 131/1000 | Loss: 0.00002064
Iteration 132/1000 | Loss: 0.00002064
Iteration 133/1000 | Loss: 0.00002064
Iteration 134/1000 | Loss: 0.00002064
Iteration 135/1000 | Loss: 0.00002064
Iteration 136/1000 | Loss: 0.00002064
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002064
Iteration 139/1000 | Loss: 0.00002064
Iteration 140/1000 | Loss: 0.00002064
Iteration 141/1000 | Loss: 0.00002064
Iteration 142/1000 | Loss: 0.00002064
Iteration 143/1000 | Loss: 0.00002063
Iteration 144/1000 | Loss: 0.00002063
Iteration 145/1000 | Loss: 0.00002063
Iteration 146/1000 | Loss: 0.00002063
Iteration 147/1000 | Loss: 0.00002063
Iteration 148/1000 | Loss: 0.00002063
Iteration 149/1000 | Loss: 0.00002063
Iteration 150/1000 | Loss: 0.00002063
Iteration 151/1000 | Loss: 0.00002063
Iteration 152/1000 | Loss: 0.00002063
Iteration 153/1000 | Loss: 0.00002063
Iteration 154/1000 | Loss: 0.00002063
Iteration 155/1000 | Loss: 0.00002063
Iteration 156/1000 | Loss: 0.00002063
Iteration 157/1000 | Loss: 0.00002063
Iteration 158/1000 | Loss: 0.00002063
Iteration 159/1000 | Loss: 0.00002063
Iteration 160/1000 | Loss: 0.00002063
Iteration 161/1000 | Loss: 0.00002063
Iteration 162/1000 | Loss: 0.00002063
Iteration 163/1000 | Loss: 0.00002063
Iteration 164/1000 | Loss: 0.00002063
Iteration 165/1000 | Loss: 0.00002063
Iteration 166/1000 | Loss: 0.00002063
Iteration 167/1000 | Loss: 0.00002062
Iteration 168/1000 | Loss: 0.00002062
Iteration 169/1000 | Loss: 0.00002062
Iteration 170/1000 | Loss: 0.00002062
Iteration 171/1000 | Loss: 0.00002062
Iteration 172/1000 | Loss: 0.00002062
Iteration 173/1000 | Loss: 0.00002062
Iteration 174/1000 | Loss: 0.00002062
Iteration 175/1000 | Loss: 0.00002062
Iteration 176/1000 | Loss: 0.00002062
Iteration 177/1000 | Loss: 0.00002062
Iteration 178/1000 | Loss: 0.00002062
Iteration 179/1000 | Loss: 0.00002062
Iteration 180/1000 | Loss: 0.00002062
Iteration 181/1000 | Loss: 0.00002062
Iteration 182/1000 | Loss: 0.00002062
Iteration 183/1000 | Loss: 0.00002062
Iteration 184/1000 | Loss: 0.00002062
Iteration 185/1000 | Loss: 0.00002062
Iteration 186/1000 | Loss: 0.00002062
Iteration 187/1000 | Loss: 0.00002062
Iteration 188/1000 | Loss: 0.00002062
Iteration 189/1000 | Loss: 0.00002061
Iteration 190/1000 | Loss: 0.00002061
Iteration 191/1000 | Loss: 0.00002061
Iteration 192/1000 | Loss: 0.00002061
Iteration 193/1000 | Loss: 0.00002061
Iteration 194/1000 | Loss: 0.00002061
Iteration 195/1000 | Loss: 0.00002061
Iteration 196/1000 | Loss: 0.00002061
Iteration 197/1000 | Loss: 0.00002061
Iteration 198/1000 | Loss: 0.00002061
Iteration 199/1000 | Loss: 0.00002061
Iteration 200/1000 | Loss: 0.00002061
Iteration 201/1000 | Loss: 0.00002061
Iteration 202/1000 | Loss: 0.00002061
Iteration 203/1000 | Loss: 0.00002061
Iteration 204/1000 | Loss: 0.00002061
Iteration 205/1000 | Loss: 0.00002061
Iteration 206/1000 | Loss: 0.00002061
Iteration 207/1000 | Loss: 0.00002061
Iteration 208/1000 | Loss: 0.00002061
Iteration 209/1000 | Loss: 0.00002061
Iteration 210/1000 | Loss: 0.00002061
Iteration 211/1000 | Loss: 0.00002061
Iteration 212/1000 | Loss: 0.00002061
Iteration 213/1000 | Loss: 0.00002061
Iteration 214/1000 | Loss: 0.00002061
Iteration 215/1000 | Loss: 0.00002061
Iteration 216/1000 | Loss: 0.00002061
Iteration 217/1000 | Loss: 0.00002061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.060551560134627e-05, 2.060551560134627e-05, 2.060551560134627e-05, 2.060551560134627e-05, 2.060551560134627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.060551560134627e-05

Optimization complete. Final v2v error: 3.86271595954895 mm

Highest mean error: 4.715132713317871 mm for frame 48

Lowest mean error: 3.595242738723755 mm for frame 37

Saving results

Total time: 741.6807372570038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0020
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943997
Iteration 2/25 | Loss: 0.00142463
Iteration 3/25 | Loss: 0.00110882
Iteration 4/25 | Loss: 0.00112067
Iteration 5/25 | Loss: 0.00104071
Iteration 6/25 | Loss: 0.00102261
Iteration 7/25 | Loss: 0.00101266
Iteration 8/25 | Loss: 0.00100933
Iteration 9/25 | Loss: 0.00099738
Iteration 10/25 | Loss: 0.00099433
Iteration 11/25 | Loss: 0.00099467
Iteration 12/25 | Loss: 0.00099705
Iteration 13/25 | Loss: 0.00099061
Iteration 14/25 | Loss: 0.00099185
Iteration 15/25 | Loss: 0.00098810
Iteration 16/25 | Loss: 0.00098758
Iteration 17/25 | Loss: 0.00098897
Iteration 18/25 | Loss: 0.00098911
Iteration 19/25 | Loss: 0.00098345
Iteration 20/25 | Loss: 0.00098809
Iteration 21/25 | Loss: 0.00098580
Iteration 22/25 | Loss: 0.00098720
Iteration 23/25 | Loss: 0.00099439
Iteration 24/25 | Loss: 0.00098849
Iteration 25/25 | Loss: 0.00098314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77272701
Iteration 2/25 | Loss: 0.00107811
Iteration 3/25 | Loss: 0.00107808
Iteration 4/25 | Loss: 0.00107808
Iteration 5/25 | Loss: 0.00107808
Iteration 6/25 | Loss: 0.00107808
Iteration 7/25 | Loss: 0.00107808
Iteration 8/25 | Loss: 0.00107808
Iteration 9/25 | Loss: 0.00107808
Iteration 10/25 | Loss: 0.00107808
Iteration 11/25 | Loss: 0.00107807
Iteration 12/25 | Loss: 0.00107807
Iteration 13/25 | Loss: 0.00107807
Iteration 14/25 | Loss: 0.00107807
Iteration 15/25 | Loss: 0.00107807
Iteration 16/25 | Loss: 0.00107807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001078074797987938, 0.001078074797987938, 0.001078074797987938, 0.001078074797987938, 0.001078074797987938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001078074797987938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107807
Iteration 2/1000 | Loss: 0.00070956
Iteration 3/1000 | Loss: 0.00135836
Iteration 4/1000 | Loss: 0.00096663
Iteration 5/1000 | Loss: 0.00137231
Iteration 6/1000 | Loss: 0.00062239
Iteration 7/1000 | Loss: 0.00083566
Iteration 8/1000 | Loss: 0.00092610
Iteration 9/1000 | Loss: 0.00056098
Iteration 10/1000 | Loss: 0.00045959
Iteration 11/1000 | Loss: 0.00045953
Iteration 12/1000 | Loss: 0.00108697
Iteration 13/1000 | Loss: 0.00084386
Iteration 14/1000 | Loss: 0.00094073
Iteration 15/1000 | Loss: 0.00098088
Iteration 16/1000 | Loss: 0.00127023
Iteration 17/1000 | Loss: 0.00062764
Iteration 18/1000 | Loss: 0.00134632
Iteration 19/1000 | Loss: 0.00114307
Iteration 20/1000 | Loss: 0.00092799
Iteration 21/1000 | Loss: 0.00075558
Iteration 22/1000 | Loss: 0.00077619
Iteration 23/1000 | Loss: 0.00034105
Iteration 24/1000 | Loss: 0.00049389
Iteration 25/1000 | Loss: 0.00030214
Iteration 26/1000 | Loss: 0.00028938
Iteration 27/1000 | Loss: 0.00057880
Iteration 28/1000 | Loss: 0.00023315
Iteration 29/1000 | Loss: 0.00018199
Iteration 30/1000 | Loss: 0.00058531
Iteration 31/1000 | Loss: 0.00049264
Iteration 32/1000 | Loss: 0.00034606
Iteration 33/1000 | Loss: 0.00060642
Iteration 34/1000 | Loss: 0.00043259
Iteration 35/1000 | Loss: 0.00056376
Iteration 36/1000 | Loss: 0.00053233
Iteration 37/1000 | Loss: 0.00020240
Iteration 38/1000 | Loss: 0.00088202
Iteration 39/1000 | Loss: 0.00034370
Iteration 40/1000 | Loss: 0.00016494
Iteration 41/1000 | Loss: 0.00041552
Iteration 42/1000 | Loss: 0.00060617
Iteration 43/1000 | Loss: 0.00046149
Iteration 44/1000 | Loss: 0.00032324
Iteration 45/1000 | Loss: 0.00027316
Iteration 46/1000 | Loss: 0.00051694
Iteration 47/1000 | Loss: 0.00030546
Iteration 48/1000 | Loss: 0.00068836
Iteration 49/1000 | Loss: 0.00038947
Iteration 50/1000 | Loss: 0.00030867
Iteration 51/1000 | Loss: 0.00106916
Iteration 52/1000 | Loss: 0.00033795
Iteration 53/1000 | Loss: 0.00057108
Iteration 54/1000 | Loss: 0.00378657
Iteration 55/1000 | Loss: 0.00032292
Iteration 56/1000 | Loss: 0.00016474
Iteration 57/1000 | Loss: 0.00030364
Iteration 58/1000 | Loss: 0.00034026
Iteration 59/1000 | Loss: 0.00038335
Iteration 60/1000 | Loss: 0.00020070
Iteration 61/1000 | Loss: 0.00033774
Iteration 62/1000 | Loss: 0.00005136
Iteration 63/1000 | Loss: 0.00004388
Iteration 64/1000 | Loss: 0.00027670
Iteration 65/1000 | Loss: 0.00023698
Iteration 66/1000 | Loss: 0.00049536
Iteration 67/1000 | Loss: 0.00011741
Iteration 68/1000 | Loss: 0.00020378
Iteration 69/1000 | Loss: 0.00018433
Iteration 70/1000 | Loss: 0.00012957
Iteration 71/1000 | Loss: 0.00015270
Iteration 72/1000 | Loss: 0.00004347
Iteration 73/1000 | Loss: 0.00004032
Iteration 74/1000 | Loss: 0.00003924
Iteration 75/1000 | Loss: 0.00075129
Iteration 76/1000 | Loss: 0.00052187
Iteration 77/1000 | Loss: 0.00015351
Iteration 78/1000 | Loss: 0.00021336
Iteration 79/1000 | Loss: 0.00004123
Iteration 80/1000 | Loss: 0.00003799
Iteration 81/1000 | Loss: 0.00003598
Iteration 82/1000 | Loss: 0.00005506
Iteration 83/1000 | Loss: 0.00003524
Iteration 84/1000 | Loss: 0.00003415
Iteration 85/1000 | Loss: 0.00003346
Iteration 86/1000 | Loss: 0.00003273
Iteration 87/1000 | Loss: 0.00003236
Iteration 88/1000 | Loss: 0.00003212
Iteration 89/1000 | Loss: 0.00003209
Iteration 90/1000 | Loss: 0.00003190
Iteration 91/1000 | Loss: 0.00003186
Iteration 92/1000 | Loss: 0.00003180
Iteration 93/1000 | Loss: 0.00003179
Iteration 94/1000 | Loss: 0.00003179
Iteration 95/1000 | Loss: 0.00003178
Iteration 96/1000 | Loss: 0.00003175
Iteration 97/1000 | Loss: 0.00003171
Iteration 98/1000 | Loss: 0.00003171
Iteration 99/1000 | Loss: 0.00003170
Iteration 100/1000 | Loss: 0.00003170
Iteration 101/1000 | Loss: 0.00003170
Iteration 102/1000 | Loss: 0.00003169
Iteration 103/1000 | Loss: 0.00003169
Iteration 104/1000 | Loss: 0.00003169
Iteration 105/1000 | Loss: 0.00003168
Iteration 106/1000 | Loss: 0.00003168
Iteration 107/1000 | Loss: 0.00003167
Iteration 108/1000 | Loss: 0.00003167
Iteration 109/1000 | Loss: 0.00003167
Iteration 110/1000 | Loss: 0.00003167
Iteration 111/1000 | Loss: 0.00003167
Iteration 112/1000 | Loss: 0.00003166
Iteration 113/1000 | Loss: 0.00003166
Iteration 114/1000 | Loss: 0.00003166
Iteration 115/1000 | Loss: 0.00003166
Iteration 116/1000 | Loss: 0.00003166
Iteration 117/1000 | Loss: 0.00003166
Iteration 118/1000 | Loss: 0.00003165
Iteration 119/1000 | Loss: 0.00003165
Iteration 120/1000 | Loss: 0.00003164
Iteration 121/1000 | Loss: 0.00003162
Iteration 122/1000 | Loss: 0.00003162
Iteration 123/1000 | Loss: 0.00003161
Iteration 124/1000 | Loss: 0.00003161
Iteration 125/1000 | Loss: 0.00003161
Iteration 126/1000 | Loss: 0.00003160
Iteration 127/1000 | Loss: 0.00003160
Iteration 128/1000 | Loss: 0.00003159
Iteration 129/1000 | Loss: 0.00003158
Iteration 130/1000 | Loss: 0.00003158
Iteration 131/1000 | Loss: 0.00003157
Iteration 132/1000 | Loss: 0.00003157
Iteration 133/1000 | Loss: 0.00003156
Iteration 134/1000 | Loss: 0.00003156
Iteration 135/1000 | Loss: 0.00003155
Iteration 136/1000 | Loss: 0.00003155
Iteration 137/1000 | Loss: 0.00003155
Iteration 138/1000 | Loss: 0.00003154
Iteration 139/1000 | Loss: 0.00003154
Iteration 140/1000 | Loss: 0.00003153
Iteration 141/1000 | Loss: 0.00003153
Iteration 142/1000 | Loss: 0.00003153
Iteration 143/1000 | Loss: 0.00003153
Iteration 144/1000 | Loss: 0.00003153
Iteration 145/1000 | Loss: 0.00003153
Iteration 146/1000 | Loss: 0.00003153
Iteration 147/1000 | Loss: 0.00003153
Iteration 148/1000 | Loss: 0.00003153
Iteration 149/1000 | Loss: 0.00003152
Iteration 150/1000 | Loss: 0.00003151
Iteration 151/1000 | Loss: 0.00003151
Iteration 152/1000 | Loss: 0.00003151
Iteration 153/1000 | Loss: 0.00003151
Iteration 154/1000 | Loss: 0.00003150
Iteration 155/1000 | Loss: 0.00003150
Iteration 156/1000 | Loss: 0.00003150
Iteration 157/1000 | Loss: 0.00003150
Iteration 158/1000 | Loss: 0.00003150
Iteration 159/1000 | Loss: 0.00003150
Iteration 160/1000 | Loss: 0.00003149
Iteration 161/1000 | Loss: 0.00003149
Iteration 162/1000 | Loss: 0.00003149
Iteration 163/1000 | Loss: 0.00003149
Iteration 164/1000 | Loss: 0.00003148
Iteration 165/1000 | Loss: 0.00003148
Iteration 166/1000 | Loss: 0.00003148
Iteration 167/1000 | Loss: 0.00003148
Iteration 168/1000 | Loss: 0.00003147
Iteration 169/1000 | Loss: 0.00003147
Iteration 170/1000 | Loss: 0.00003147
Iteration 171/1000 | Loss: 0.00003146
Iteration 172/1000 | Loss: 0.00003146
Iteration 173/1000 | Loss: 0.00003146
Iteration 174/1000 | Loss: 0.00003146
Iteration 175/1000 | Loss: 0.00003146
Iteration 176/1000 | Loss: 0.00003146
Iteration 177/1000 | Loss: 0.00003146
Iteration 178/1000 | Loss: 0.00003145
Iteration 179/1000 | Loss: 0.00003145
Iteration 180/1000 | Loss: 0.00003145
Iteration 181/1000 | Loss: 0.00003145
Iteration 182/1000 | Loss: 0.00003145
Iteration 183/1000 | Loss: 0.00003145
Iteration 184/1000 | Loss: 0.00003145
Iteration 185/1000 | Loss: 0.00003144
Iteration 186/1000 | Loss: 0.00003144
Iteration 187/1000 | Loss: 0.00003144
Iteration 188/1000 | Loss: 0.00003144
Iteration 189/1000 | Loss: 0.00003144
Iteration 190/1000 | Loss: 0.00003144
Iteration 191/1000 | Loss: 0.00003144
Iteration 192/1000 | Loss: 0.00003144
Iteration 193/1000 | Loss: 0.00003143
Iteration 194/1000 | Loss: 0.00003143
Iteration 195/1000 | Loss: 0.00003143
Iteration 196/1000 | Loss: 0.00003143
Iteration 197/1000 | Loss: 0.00003143
Iteration 198/1000 | Loss: 0.00003143
Iteration 199/1000 | Loss: 0.00003142
Iteration 200/1000 | Loss: 0.00003142
Iteration 201/1000 | Loss: 0.00003142
Iteration 202/1000 | Loss: 0.00003142
Iteration 203/1000 | Loss: 0.00003141
Iteration 204/1000 | Loss: 0.00003141
Iteration 205/1000 | Loss: 0.00003141
Iteration 206/1000 | Loss: 0.00003141
Iteration 207/1000 | Loss: 0.00003141
Iteration 208/1000 | Loss: 0.00003141
Iteration 209/1000 | Loss: 0.00003141
Iteration 210/1000 | Loss: 0.00003141
Iteration 211/1000 | Loss: 0.00003141
Iteration 212/1000 | Loss: 0.00003140
Iteration 213/1000 | Loss: 0.00003140
Iteration 214/1000 | Loss: 0.00003140
Iteration 215/1000 | Loss: 0.00003140
Iteration 216/1000 | Loss: 0.00003139
Iteration 217/1000 | Loss: 0.00003139
Iteration 218/1000 | Loss: 0.00003139
Iteration 219/1000 | Loss: 0.00003139
Iteration 220/1000 | Loss: 0.00003139
Iteration 221/1000 | Loss: 0.00003139
Iteration 222/1000 | Loss: 0.00003139
Iteration 223/1000 | Loss: 0.00003138
Iteration 224/1000 | Loss: 0.00003138
Iteration 225/1000 | Loss: 0.00003138
Iteration 226/1000 | Loss: 0.00003138
Iteration 227/1000 | Loss: 0.00003138
Iteration 228/1000 | Loss: 0.00003138
Iteration 229/1000 | Loss: 0.00003138
Iteration 230/1000 | Loss: 0.00003138
Iteration 231/1000 | Loss: 0.00003138
Iteration 232/1000 | Loss: 0.00003138
Iteration 233/1000 | Loss: 0.00003138
Iteration 234/1000 | Loss: 0.00003138
Iteration 235/1000 | Loss: 0.00003138
Iteration 236/1000 | Loss: 0.00003138
Iteration 237/1000 | Loss: 0.00003137
Iteration 238/1000 | Loss: 0.00003137
Iteration 239/1000 | Loss: 0.00003137
Iteration 240/1000 | Loss: 0.00003137
Iteration 241/1000 | Loss: 0.00003137
Iteration 242/1000 | Loss: 0.00003137
Iteration 243/1000 | Loss: 0.00003137
Iteration 244/1000 | Loss: 0.00003136
Iteration 245/1000 | Loss: 0.00003136
Iteration 246/1000 | Loss: 0.00003136
Iteration 247/1000 | Loss: 0.00003136
Iteration 248/1000 | Loss: 0.00003135
Iteration 249/1000 | Loss: 0.00003135
Iteration 250/1000 | Loss: 0.00003135
Iteration 251/1000 | Loss: 0.00003135
Iteration 252/1000 | Loss: 0.00003134
Iteration 253/1000 | Loss: 0.00003134
Iteration 254/1000 | Loss: 0.00003134
Iteration 255/1000 | Loss: 0.00003134
Iteration 256/1000 | Loss: 0.00003134
Iteration 257/1000 | Loss: 0.00003134
Iteration 258/1000 | Loss: 0.00003133
Iteration 259/1000 | Loss: 0.00003133
Iteration 260/1000 | Loss: 0.00003133
Iteration 261/1000 | Loss: 0.00003133
Iteration 262/1000 | Loss: 0.00003133
Iteration 263/1000 | Loss: 0.00003133
Iteration 264/1000 | Loss: 0.00003133
Iteration 265/1000 | Loss: 0.00003133
Iteration 266/1000 | Loss: 0.00003133
Iteration 267/1000 | Loss: 0.00003132
Iteration 268/1000 | Loss: 0.00003132
Iteration 269/1000 | Loss: 0.00003132
Iteration 270/1000 | Loss: 0.00003132
Iteration 271/1000 | Loss: 0.00003132
Iteration 272/1000 | Loss: 0.00003132
Iteration 273/1000 | Loss: 0.00003132
Iteration 274/1000 | Loss: 0.00003132
Iteration 275/1000 | Loss: 0.00003132
Iteration 276/1000 | Loss: 0.00003131
Iteration 277/1000 | Loss: 0.00003131
Iteration 278/1000 | Loss: 0.00003131
Iteration 279/1000 | Loss: 0.00003131
Iteration 280/1000 | Loss: 0.00003131
Iteration 281/1000 | Loss: 0.00003131
Iteration 282/1000 | Loss: 0.00003131
Iteration 283/1000 | Loss: 0.00003130
Iteration 284/1000 | Loss: 0.00003130
Iteration 285/1000 | Loss: 0.00003130
Iteration 286/1000 | Loss: 0.00003130
Iteration 287/1000 | Loss: 0.00003129
Iteration 288/1000 | Loss: 0.00003129
Iteration 289/1000 | Loss: 0.00003129
Iteration 290/1000 | Loss: 0.00003128
Iteration 291/1000 | Loss: 0.00003128
Iteration 292/1000 | Loss: 0.00003127
Iteration 293/1000 | Loss: 0.00003127
Iteration 294/1000 | Loss: 0.00003127
Iteration 295/1000 | Loss: 0.00003127
Iteration 296/1000 | Loss: 0.00003127
Iteration 297/1000 | Loss: 0.00003127
Iteration 298/1000 | Loss: 0.00003127
Iteration 299/1000 | Loss: 0.00003126
Iteration 300/1000 | Loss: 0.00003125
Iteration 301/1000 | Loss: 0.00003125
Iteration 302/1000 | Loss: 0.00003125
Iteration 303/1000 | Loss: 0.00003125
Iteration 304/1000 | Loss: 0.00003125
Iteration 305/1000 | Loss: 0.00003125
Iteration 306/1000 | Loss: 0.00003125
Iteration 307/1000 | Loss: 0.00003125
Iteration 308/1000 | Loss: 0.00003124
Iteration 309/1000 | Loss: 0.00003124
Iteration 310/1000 | Loss: 0.00003124
Iteration 311/1000 | Loss: 0.00003124
Iteration 312/1000 | Loss: 0.00003124
Iteration 313/1000 | Loss: 0.00003124
Iteration 314/1000 | Loss: 0.00003124
Iteration 315/1000 | Loss: 0.00003124
Iteration 316/1000 | Loss: 0.00003124
Iteration 317/1000 | Loss: 0.00003124
Iteration 318/1000 | Loss: 0.00003124
Iteration 319/1000 | Loss: 0.00003124
Iteration 320/1000 | Loss: 0.00003124
Iteration 321/1000 | Loss: 0.00003124
Iteration 322/1000 | Loss: 0.00003124
Iteration 323/1000 | Loss: 0.00003124
Iteration 324/1000 | Loss: 0.00003124
Iteration 325/1000 | Loss: 0.00003124
Iteration 326/1000 | Loss: 0.00003124
Iteration 327/1000 | Loss: 0.00003124
Iteration 328/1000 | Loss: 0.00003124
Iteration 329/1000 | Loss: 0.00003124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [3.124014983768575e-05, 3.124014983768575e-05, 3.124014983768575e-05, 3.124014983768575e-05, 3.124014983768575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.124014983768575e-05

Optimization complete. Final v2v error: 4.6001386642456055 mm

Highest mean error: 7.115622520446777 mm for frame 42

Lowest mean error: 3.401712656021118 mm for frame 83

Saving results

Total time: 1792.1949048042297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0015
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929506
Iteration 2/25 | Loss: 0.00164224
Iteration 3/25 | Loss: 0.00103644
Iteration 4/25 | Loss: 0.00100141
Iteration 5/25 | Loss: 0.00098711
Iteration 6/25 | Loss: 0.00098347
Iteration 7/25 | Loss: 0.00098253
Iteration 8/25 | Loss: 0.00098253
Iteration 9/25 | Loss: 0.00098253
Iteration 10/25 | Loss: 0.00098253
Iteration 11/25 | Loss: 0.00098253
Iteration 12/25 | Loss: 0.00098253
Iteration 13/25 | Loss: 0.00098253
Iteration 14/25 | Loss: 0.00098253
Iteration 15/25 | Loss: 0.00098253
Iteration 16/25 | Loss: 0.00098253
Iteration 17/25 | Loss: 0.00098253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009825285524129868, 0.0009825285524129868, 0.0009825285524129868, 0.0009825285524129868, 0.0009825285524129868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009825285524129868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84650230
Iteration 2/25 | Loss: 0.00032308
Iteration 3/25 | Loss: 0.00032308
Iteration 4/25 | Loss: 0.00032308
Iteration 5/25 | Loss: 0.00032308
Iteration 6/25 | Loss: 0.00032307
Iteration 7/25 | Loss: 0.00032307
Iteration 8/25 | Loss: 0.00032307
Iteration 9/25 | Loss: 0.00032307
Iteration 10/25 | Loss: 0.00032307
Iteration 11/25 | Loss: 0.00032307
Iteration 12/25 | Loss: 0.00032307
Iteration 13/25 | Loss: 0.00032307
Iteration 14/25 | Loss: 0.00032307
Iteration 15/25 | Loss: 0.00032307
Iteration 16/25 | Loss: 0.00032307
Iteration 17/25 | Loss: 0.00032307
Iteration 18/25 | Loss: 0.00032307
Iteration 19/25 | Loss: 0.00032307
Iteration 20/25 | Loss: 0.00032307
Iteration 21/25 | Loss: 0.00032307
Iteration 22/25 | Loss: 0.00032307
Iteration 23/25 | Loss: 0.00032307
Iteration 24/25 | Loss: 0.00032307
Iteration 25/25 | Loss: 0.00032307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032307
Iteration 2/1000 | Loss: 0.00008134
Iteration 3/1000 | Loss: 0.00005280
Iteration 4/1000 | Loss: 0.00004390
Iteration 5/1000 | Loss: 0.00004078
Iteration 6/1000 | Loss: 0.00003868
Iteration 7/1000 | Loss: 0.00003741
Iteration 8/1000 | Loss: 0.00003641
Iteration 9/1000 | Loss: 0.00003590
Iteration 10/1000 | Loss: 0.00003555
Iteration 11/1000 | Loss: 0.00003524
Iteration 12/1000 | Loss: 0.00003500
Iteration 13/1000 | Loss: 0.00003482
Iteration 14/1000 | Loss: 0.00003469
Iteration 15/1000 | Loss: 0.00003467
Iteration 16/1000 | Loss: 0.00003459
Iteration 17/1000 | Loss: 0.00003457
Iteration 18/1000 | Loss: 0.00003457
Iteration 19/1000 | Loss: 0.00003457
Iteration 20/1000 | Loss: 0.00003457
Iteration 21/1000 | Loss: 0.00003456
Iteration 22/1000 | Loss: 0.00003456
Iteration 23/1000 | Loss: 0.00003454
Iteration 24/1000 | Loss: 0.00003453
Iteration 25/1000 | Loss: 0.00003453
Iteration 26/1000 | Loss: 0.00003452
Iteration 27/1000 | Loss: 0.00003452
Iteration 28/1000 | Loss: 0.00003450
Iteration 29/1000 | Loss: 0.00003450
Iteration 30/1000 | Loss: 0.00003449
Iteration 31/1000 | Loss: 0.00003444
Iteration 32/1000 | Loss: 0.00003444
Iteration 33/1000 | Loss: 0.00003438
Iteration 34/1000 | Loss: 0.00003438
Iteration 35/1000 | Loss: 0.00003436
Iteration 36/1000 | Loss: 0.00003436
Iteration 37/1000 | Loss: 0.00003435
Iteration 38/1000 | Loss: 0.00003435
Iteration 39/1000 | Loss: 0.00003435
Iteration 40/1000 | Loss: 0.00003435
Iteration 41/1000 | Loss: 0.00003434
Iteration 42/1000 | Loss: 0.00003432
Iteration 43/1000 | Loss: 0.00003431
Iteration 44/1000 | Loss: 0.00003431
Iteration 45/1000 | Loss: 0.00003428
Iteration 46/1000 | Loss: 0.00003428
Iteration 47/1000 | Loss: 0.00003428
Iteration 48/1000 | Loss: 0.00003428
Iteration 49/1000 | Loss: 0.00003428
Iteration 50/1000 | Loss: 0.00003427
Iteration 51/1000 | Loss: 0.00003426
Iteration 52/1000 | Loss: 0.00003425
Iteration 53/1000 | Loss: 0.00003424
Iteration 54/1000 | Loss: 0.00003424
Iteration 55/1000 | Loss: 0.00003424
Iteration 56/1000 | Loss: 0.00003423
Iteration 57/1000 | Loss: 0.00003422
Iteration 58/1000 | Loss: 0.00003422
Iteration 59/1000 | Loss: 0.00003422
Iteration 60/1000 | Loss: 0.00003422
Iteration 61/1000 | Loss: 0.00003422
Iteration 62/1000 | Loss: 0.00003422
Iteration 63/1000 | Loss: 0.00003422
Iteration 64/1000 | Loss: 0.00003421
Iteration 65/1000 | Loss: 0.00003421
Iteration 66/1000 | Loss: 0.00003420
Iteration 67/1000 | Loss: 0.00003420
Iteration 68/1000 | Loss: 0.00003419
Iteration 69/1000 | Loss: 0.00003419
Iteration 70/1000 | Loss: 0.00003418
Iteration 71/1000 | Loss: 0.00003418
Iteration 72/1000 | Loss: 0.00003417
Iteration 73/1000 | Loss: 0.00003417
Iteration 74/1000 | Loss: 0.00003417
Iteration 75/1000 | Loss: 0.00003416
Iteration 76/1000 | Loss: 0.00003416
Iteration 77/1000 | Loss: 0.00003416
Iteration 78/1000 | Loss: 0.00003416
Iteration 79/1000 | Loss: 0.00003416
Iteration 80/1000 | Loss: 0.00003416
Iteration 81/1000 | Loss: 0.00003416
Iteration 82/1000 | Loss: 0.00003416
Iteration 83/1000 | Loss: 0.00003416
Iteration 84/1000 | Loss: 0.00003415
Iteration 85/1000 | Loss: 0.00003415
Iteration 86/1000 | Loss: 0.00003415
Iteration 87/1000 | Loss: 0.00003415
Iteration 88/1000 | Loss: 0.00003414
Iteration 89/1000 | Loss: 0.00003414
Iteration 90/1000 | Loss: 0.00003414
Iteration 91/1000 | Loss: 0.00003414
Iteration 92/1000 | Loss: 0.00003414
Iteration 93/1000 | Loss: 0.00003413
Iteration 94/1000 | Loss: 0.00003413
Iteration 95/1000 | Loss: 0.00003413
Iteration 96/1000 | Loss: 0.00003413
Iteration 97/1000 | Loss: 0.00003413
Iteration 98/1000 | Loss: 0.00003413
Iteration 99/1000 | Loss: 0.00003413
Iteration 100/1000 | Loss: 0.00003413
Iteration 101/1000 | Loss: 0.00003413
Iteration 102/1000 | Loss: 0.00003413
Iteration 103/1000 | Loss: 0.00003412
Iteration 104/1000 | Loss: 0.00003412
Iteration 105/1000 | Loss: 0.00003412
Iteration 106/1000 | Loss: 0.00003412
Iteration 107/1000 | Loss: 0.00003412
Iteration 108/1000 | Loss: 0.00003412
Iteration 109/1000 | Loss: 0.00003411
Iteration 110/1000 | Loss: 0.00003411
Iteration 111/1000 | Loss: 0.00003411
Iteration 112/1000 | Loss: 0.00003411
Iteration 113/1000 | Loss: 0.00003411
Iteration 114/1000 | Loss: 0.00003411
Iteration 115/1000 | Loss: 0.00003411
Iteration 116/1000 | Loss: 0.00003411
Iteration 117/1000 | Loss: 0.00003410
Iteration 118/1000 | Loss: 0.00003410
Iteration 119/1000 | Loss: 0.00003410
Iteration 120/1000 | Loss: 0.00003410
Iteration 121/1000 | Loss: 0.00003410
Iteration 122/1000 | Loss: 0.00003410
Iteration 123/1000 | Loss: 0.00003410
Iteration 124/1000 | Loss: 0.00003409
Iteration 125/1000 | Loss: 0.00003409
Iteration 126/1000 | Loss: 0.00003409
Iteration 127/1000 | Loss: 0.00003409
Iteration 128/1000 | Loss: 0.00003409
Iteration 129/1000 | Loss: 0.00003409
Iteration 130/1000 | Loss: 0.00003409
Iteration 131/1000 | Loss: 0.00003409
Iteration 132/1000 | Loss: 0.00003409
Iteration 133/1000 | Loss: 0.00003409
Iteration 134/1000 | Loss: 0.00003409
Iteration 135/1000 | Loss: 0.00003409
Iteration 136/1000 | Loss: 0.00003408
Iteration 137/1000 | Loss: 0.00003408
Iteration 138/1000 | Loss: 0.00003408
Iteration 139/1000 | Loss: 0.00003408
Iteration 140/1000 | Loss: 0.00003408
Iteration 141/1000 | Loss: 0.00003408
Iteration 142/1000 | Loss: 0.00003408
Iteration 143/1000 | Loss: 0.00003408
Iteration 144/1000 | Loss: 0.00003407
Iteration 145/1000 | Loss: 0.00003407
Iteration 146/1000 | Loss: 0.00003407
Iteration 147/1000 | Loss: 0.00003407
Iteration 148/1000 | Loss: 0.00003407
Iteration 149/1000 | Loss: 0.00003407
Iteration 150/1000 | Loss: 0.00003407
Iteration 151/1000 | Loss: 0.00003406
Iteration 152/1000 | Loss: 0.00003406
Iteration 153/1000 | Loss: 0.00003406
Iteration 154/1000 | Loss: 0.00003406
Iteration 155/1000 | Loss: 0.00003406
Iteration 156/1000 | Loss: 0.00003406
Iteration 157/1000 | Loss: 0.00003406
Iteration 158/1000 | Loss: 0.00003406
Iteration 159/1000 | Loss: 0.00003406
Iteration 160/1000 | Loss: 0.00003406
Iteration 161/1000 | Loss: 0.00003406
Iteration 162/1000 | Loss: 0.00003406
Iteration 163/1000 | Loss: 0.00003406
Iteration 164/1000 | Loss: 0.00003406
Iteration 165/1000 | Loss: 0.00003406
Iteration 166/1000 | Loss: 0.00003406
Iteration 167/1000 | Loss: 0.00003405
Iteration 168/1000 | Loss: 0.00003405
Iteration 169/1000 | Loss: 0.00003405
Iteration 170/1000 | Loss: 0.00003405
Iteration 171/1000 | Loss: 0.00003405
Iteration 172/1000 | Loss: 0.00003405
Iteration 173/1000 | Loss: 0.00003405
Iteration 174/1000 | Loss: 0.00003404
Iteration 175/1000 | Loss: 0.00003404
Iteration 176/1000 | Loss: 0.00003404
Iteration 177/1000 | Loss: 0.00003404
Iteration 178/1000 | Loss: 0.00003404
Iteration 179/1000 | Loss: 0.00003404
Iteration 180/1000 | Loss: 0.00003403
Iteration 181/1000 | Loss: 0.00003403
Iteration 182/1000 | Loss: 0.00003403
Iteration 183/1000 | Loss: 0.00003403
Iteration 184/1000 | Loss: 0.00003403
Iteration 185/1000 | Loss: 0.00003403
Iteration 186/1000 | Loss: 0.00003403
Iteration 187/1000 | Loss: 0.00003402
Iteration 188/1000 | Loss: 0.00003402
Iteration 189/1000 | Loss: 0.00003402
Iteration 190/1000 | Loss: 0.00003402
Iteration 191/1000 | Loss: 0.00003402
Iteration 192/1000 | Loss: 0.00003402
Iteration 193/1000 | Loss: 0.00003402
Iteration 194/1000 | Loss: 0.00003401
Iteration 195/1000 | Loss: 0.00003401
Iteration 196/1000 | Loss: 0.00003401
Iteration 197/1000 | Loss: 0.00003401
Iteration 198/1000 | Loss: 0.00003401
Iteration 199/1000 | Loss: 0.00003401
Iteration 200/1000 | Loss: 0.00003401
Iteration 201/1000 | Loss: 0.00003401
Iteration 202/1000 | Loss: 0.00003401
Iteration 203/1000 | Loss: 0.00003401
Iteration 204/1000 | Loss: 0.00003400
Iteration 205/1000 | Loss: 0.00003400
Iteration 206/1000 | Loss: 0.00003400
Iteration 207/1000 | Loss: 0.00003400
Iteration 208/1000 | Loss: 0.00003400
Iteration 209/1000 | Loss: 0.00003400
Iteration 210/1000 | Loss: 0.00003400
Iteration 211/1000 | Loss: 0.00003399
Iteration 212/1000 | Loss: 0.00003399
Iteration 213/1000 | Loss: 0.00003399
Iteration 214/1000 | Loss: 0.00003399
Iteration 215/1000 | Loss: 0.00003399
Iteration 216/1000 | Loss: 0.00003399
Iteration 217/1000 | Loss: 0.00003399
Iteration 218/1000 | Loss: 0.00003399
Iteration 219/1000 | Loss: 0.00003399
Iteration 220/1000 | Loss: 0.00003399
Iteration 221/1000 | Loss: 0.00003399
Iteration 222/1000 | Loss: 0.00003399
Iteration 223/1000 | Loss: 0.00003398
Iteration 224/1000 | Loss: 0.00003398
Iteration 225/1000 | Loss: 0.00003398
Iteration 226/1000 | Loss: 0.00003398
Iteration 227/1000 | Loss: 0.00003398
Iteration 228/1000 | Loss: 0.00003398
Iteration 229/1000 | Loss: 0.00003398
Iteration 230/1000 | Loss: 0.00003398
Iteration 231/1000 | Loss: 0.00003398
Iteration 232/1000 | Loss: 0.00003398
Iteration 233/1000 | Loss: 0.00003398
Iteration 234/1000 | Loss: 0.00003398
Iteration 235/1000 | Loss: 0.00003398
Iteration 236/1000 | Loss: 0.00003398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [3.3984004403464496e-05, 3.3984004403464496e-05, 3.3984004403464496e-05, 3.3984004403464496e-05, 3.3984004403464496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3984004403464496e-05

Optimization complete. Final v2v error: 4.8067121505737305 mm

Highest mean error: 5.401157855987549 mm for frame 64

Lowest mean error: 3.7636709213256836 mm for frame 5

Saving results

Total time: 272.86389565467834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0016
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430840
Iteration 2/25 | Loss: 0.00103001
Iteration 3/25 | Loss: 0.00085116
Iteration 4/25 | Loss: 0.00082401
Iteration 5/25 | Loss: 0.00081586
Iteration 6/25 | Loss: 0.00081379
Iteration 7/25 | Loss: 0.00081371
Iteration 8/25 | Loss: 0.00081371
Iteration 9/25 | Loss: 0.00081371
Iteration 10/25 | Loss: 0.00081371
Iteration 11/25 | Loss: 0.00081371
Iteration 12/25 | Loss: 0.00081371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008137068361975253, 0.0008137068361975253, 0.0008137068361975253, 0.0008137068361975253, 0.0008137068361975253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008137068361975253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.09693861
Iteration 2/25 | Loss: 0.00034374
Iteration 3/25 | Loss: 0.00034372
Iteration 4/25 | Loss: 0.00034372
Iteration 5/25 | Loss: 0.00034372
Iteration 6/25 | Loss: 0.00034372
Iteration 7/25 | Loss: 0.00034372
Iteration 8/25 | Loss: 0.00034372
Iteration 9/25 | Loss: 0.00034372
Iteration 10/25 | Loss: 0.00034372
Iteration 11/25 | Loss: 0.00034372
Iteration 12/25 | Loss: 0.00034372
Iteration 13/25 | Loss: 0.00034372
Iteration 14/25 | Loss: 0.00034372
Iteration 15/25 | Loss: 0.00034372
Iteration 16/25 | Loss: 0.00034372
Iteration 17/25 | Loss: 0.00034372
Iteration 18/25 | Loss: 0.00034372
Iteration 19/25 | Loss: 0.00034372
Iteration 20/25 | Loss: 0.00034372
Iteration 21/25 | Loss: 0.00034372
Iteration 22/25 | Loss: 0.00034372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00034372007939964533, 0.00034372007939964533, 0.00034372007939964533, 0.00034372007939964533, 0.00034372007939964533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034372007939964533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034372
Iteration 2/1000 | Loss: 0.00004051
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002792
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002539
Iteration 7/1000 | Loss: 0.00002484
Iteration 8/1000 | Loss: 0.00002461
Iteration 9/1000 | Loss: 0.00002448
Iteration 10/1000 | Loss: 0.00002431
Iteration 11/1000 | Loss: 0.00002422
Iteration 12/1000 | Loss: 0.00002421
Iteration 13/1000 | Loss: 0.00002420
Iteration 14/1000 | Loss: 0.00002416
Iteration 15/1000 | Loss: 0.00002416
Iteration 16/1000 | Loss: 0.00002415
Iteration 17/1000 | Loss: 0.00002415
Iteration 18/1000 | Loss: 0.00002414
Iteration 19/1000 | Loss: 0.00002413
Iteration 20/1000 | Loss: 0.00002413
Iteration 21/1000 | Loss: 0.00002413
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002412
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002412
Iteration 26/1000 | Loss: 0.00002412
Iteration 27/1000 | Loss: 0.00002411
Iteration 28/1000 | Loss: 0.00002411
Iteration 29/1000 | Loss: 0.00002410
Iteration 30/1000 | Loss: 0.00002410
Iteration 31/1000 | Loss: 0.00002410
Iteration 32/1000 | Loss: 0.00002410
Iteration 33/1000 | Loss: 0.00002410
Iteration 34/1000 | Loss: 0.00002409
Iteration 35/1000 | Loss: 0.00002409
Iteration 36/1000 | Loss: 0.00002409
Iteration 37/1000 | Loss: 0.00002409
Iteration 38/1000 | Loss: 0.00002408
Iteration 39/1000 | Loss: 0.00002408
Iteration 40/1000 | Loss: 0.00002408
Iteration 41/1000 | Loss: 0.00002408
Iteration 42/1000 | Loss: 0.00002408
Iteration 43/1000 | Loss: 0.00002407
Iteration 44/1000 | Loss: 0.00002407
Iteration 45/1000 | Loss: 0.00002407
Iteration 46/1000 | Loss: 0.00002407
Iteration 47/1000 | Loss: 0.00002407
Iteration 48/1000 | Loss: 0.00002407
Iteration 49/1000 | Loss: 0.00002407
Iteration 50/1000 | Loss: 0.00002406
Iteration 51/1000 | Loss: 0.00002406
Iteration 52/1000 | Loss: 0.00002406
Iteration 53/1000 | Loss: 0.00002405
Iteration 54/1000 | Loss: 0.00002405
Iteration 55/1000 | Loss: 0.00002404
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002404
Iteration 59/1000 | Loss: 0.00002403
Iteration 60/1000 | Loss: 0.00002403
Iteration 61/1000 | Loss: 0.00002403
Iteration 62/1000 | Loss: 0.00002403
Iteration 63/1000 | Loss: 0.00002403
Iteration 64/1000 | Loss: 0.00002402
Iteration 65/1000 | Loss: 0.00002402
Iteration 66/1000 | Loss: 0.00002402
Iteration 67/1000 | Loss: 0.00002402
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002401
Iteration 70/1000 | Loss: 0.00002401
Iteration 71/1000 | Loss: 0.00002401
Iteration 72/1000 | Loss: 0.00002400
Iteration 73/1000 | Loss: 0.00002400
Iteration 74/1000 | Loss: 0.00002400
Iteration 75/1000 | Loss: 0.00002398
Iteration 76/1000 | Loss: 0.00002398
Iteration 77/1000 | Loss: 0.00002398
Iteration 78/1000 | Loss: 0.00002398
Iteration 79/1000 | Loss: 0.00002398
Iteration 80/1000 | Loss: 0.00002398
Iteration 81/1000 | Loss: 0.00002398
Iteration 82/1000 | Loss: 0.00002397
Iteration 83/1000 | Loss: 0.00002396
Iteration 84/1000 | Loss: 0.00002396
Iteration 85/1000 | Loss: 0.00002396
Iteration 86/1000 | Loss: 0.00002395
Iteration 87/1000 | Loss: 0.00002395
Iteration 88/1000 | Loss: 0.00002395
Iteration 89/1000 | Loss: 0.00002395
Iteration 90/1000 | Loss: 0.00002395
Iteration 91/1000 | Loss: 0.00002395
Iteration 92/1000 | Loss: 0.00002395
Iteration 93/1000 | Loss: 0.00002395
Iteration 94/1000 | Loss: 0.00002394
Iteration 95/1000 | Loss: 0.00002394
Iteration 96/1000 | Loss: 0.00002394
Iteration 97/1000 | Loss: 0.00002394
Iteration 98/1000 | Loss: 0.00002393
Iteration 99/1000 | Loss: 0.00002393
Iteration 100/1000 | Loss: 0.00002393
Iteration 101/1000 | Loss: 0.00002393
Iteration 102/1000 | Loss: 0.00002393
Iteration 103/1000 | Loss: 0.00002393
Iteration 104/1000 | Loss: 0.00002393
Iteration 105/1000 | Loss: 0.00002393
Iteration 106/1000 | Loss: 0.00002393
Iteration 107/1000 | Loss: 0.00002393
Iteration 108/1000 | Loss: 0.00002393
Iteration 109/1000 | Loss: 0.00002392
Iteration 110/1000 | Loss: 0.00002392
Iteration 111/1000 | Loss: 0.00002392
Iteration 112/1000 | Loss: 0.00002392
Iteration 113/1000 | Loss: 0.00002392
Iteration 114/1000 | Loss: 0.00002392
Iteration 115/1000 | Loss: 0.00002392
Iteration 116/1000 | Loss: 0.00002392
Iteration 117/1000 | Loss: 0.00002392
Iteration 118/1000 | Loss: 0.00002392
Iteration 119/1000 | Loss: 0.00002392
Iteration 120/1000 | Loss: 0.00002392
Iteration 121/1000 | Loss: 0.00002392
Iteration 122/1000 | Loss: 0.00002392
Iteration 123/1000 | Loss: 0.00002392
Iteration 124/1000 | Loss: 0.00002392
Iteration 125/1000 | Loss: 0.00002392
Iteration 126/1000 | Loss: 0.00002391
Iteration 127/1000 | Loss: 0.00002391
Iteration 128/1000 | Loss: 0.00002391
Iteration 129/1000 | Loss: 0.00002391
Iteration 130/1000 | Loss: 0.00002391
Iteration 131/1000 | Loss: 0.00002391
Iteration 132/1000 | Loss: 0.00002391
Iteration 133/1000 | Loss: 0.00002391
Iteration 134/1000 | Loss: 0.00002391
Iteration 135/1000 | Loss: 0.00002391
Iteration 136/1000 | Loss: 0.00002391
Iteration 137/1000 | Loss: 0.00002391
Iteration 138/1000 | Loss: 0.00002391
Iteration 139/1000 | Loss: 0.00002391
Iteration 140/1000 | Loss: 0.00002391
Iteration 141/1000 | Loss: 0.00002391
Iteration 142/1000 | Loss: 0.00002391
Iteration 143/1000 | Loss: 0.00002391
Iteration 144/1000 | Loss: 0.00002391
Iteration 145/1000 | Loss: 0.00002391
Iteration 146/1000 | Loss: 0.00002391
Iteration 147/1000 | Loss: 0.00002391
Iteration 148/1000 | Loss: 0.00002391
Iteration 149/1000 | Loss: 0.00002391
Iteration 150/1000 | Loss: 0.00002391
Iteration 151/1000 | Loss: 0.00002391
Iteration 152/1000 | Loss: 0.00002391
Iteration 153/1000 | Loss: 0.00002391
Iteration 154/1000 | Loss: 0.00002391
Iteration 155/1000 | Loss: 0.00002391
Iteration 156/1000 | Loss: 0.00002391
Iteration 157/1000 | Loss: 0.00002391
Iteration 158/1000 | Loss: 0.00002391
Iteration 159/1000 | Loss: 0.00002391
Iteration 160/1000 | Loss: 0.00002391
Iteration 161/1000 | Loss: 0.00002391
Iteration 162/1000 | Loss: 0.00002391
Iteration 163/1000 | Loss: 0.00002391
Iteration 164/1000 | Loss: 0.00002391
Iteration 165/1000 | Loss: 0.00002391
Iteration 166/1000 | Loss: 0.00002391
Iteration 167/1000 | Loss: 0.00002391
Iteration 168/1000 | Loss: 0.00002391
Iteration 169/1000 | Loss: 0.00002391
Iteration 170/1000 | Loss: 0.00002391
Iteration 171/1000 | Loss: 0.00002391
Iteration 172/1000 | Loss: 0.00002391
Iteration 173/1000 | Loss: 0.00002391
Iteration 174/1000 | Loss: 0.00002391
Iteration 175/1000 | Loss: 0.00002391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.391297857684549e-05, 2.391297857684549e-05, 2.391297857684549e-05, 2.391297857684549e-05, 2.391297857684549e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.391297857684549e-05

Optimization complete. Final v2v error: 4.132380962371826 mm

Highest mean error: 4.45836067199707 mm for frame 65

Lowest mean error: 3.592921495437622 mm for frame 0

Saving results

Total time: 213.41169238090515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0006
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895477
Iteration 2/25 | Loss: 0.00096903
Iteration 3/25 | Loss: 0.00087787
Iteration 4/25 | Loss: 0.00085227
Iteration 5/25 | Loss: 0.00084280
Iteration 6/25 | Loss: 0.00084112
Iteration 7/25 | Loss: 0.00084112
Iteration 8/25 | Loss: 0.00084112
Iteration 9/25 | Loss: 0.00084112
Iteration 10/25 | Loss: 0.00084112
Iteration 11/25 | Loss: 0.00084112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008411247399635613, 0.0008411247399635613, 0.0008411247399635613, 0.0008411247399635613, 0.0008411247399635613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008411247399635613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42968047
Iteration 2/25 | Loss: 0.00036114
Iteration 3/25 | Loss: 0.00036114
Iteration 4/25 | Loss: 0.00036114
Iteration 5/25 | Loss: 0.00036114
Iteration 6/25 | Loss: 0.00036114
Iteration 7/25 | Loss: 0.00036114
Iteration 8/25 | Loss: 0.00036114
Iteration 9/25 | Loss: 0.00036114
Iteration 10/25 | Loss: 0.00036114
Iteration 11/25 | Loss: 0.00036114
Iteration 12/25 | Loss: 0.00036114
Iteration 13/25 | Loss: 0.00036114
Iteration 14/25 | Loss: 0.00036114
Iteration 15/25 | Loss: 0.00036114
Iteration 16/25 | Loss: 0.00036114
Iteration 17/25 | Loss: 0.00036114
Iteration 18/25 | Loss: 0.00036114
Iteration 19/25 | Loss: 0.00036114
Iteration 20/25 | Loss: 0.00036114
Iteration 21/25 | Loss: 0.00036114
Iteration 22/25 | Loss: 0.00036114
Iteration 23/25 | Loss: 0.00036114
Iteration 24/25 | Loss: 0.00036114
Iteration 25/25 | Loss: 0.00036114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036114
Iteration 2/1000 | Loss: 0.00005890
Iteration 3/1000 | Loss: 0.00003904
Iteration 4/1000 | Loss: 0.00003369
Iteration 5/1000 | Loss: 0.00003149
Iteration 6/1000 | Loss: 0.00002998
Iteration 7/1000 | Loss: 0.00002918
Iteration 8/1000 | Loss: 0.00002848
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002773
Iteration 11/1000 | Loss: 0.00002751
Iteration 12/1000 | Loss: 0.00002741
Iteration 13/1000 | Loss: 0.00002740
Iteration 14/1000 | Loss: 0.00002740
Iteration 15/1000 | Loss: 0.00002739
Iteration 16/1000 | Loss: 0.00002737
Iteration 17/1000 | Loss: 0.00002737
Iteration 18/1000 | Loss: 0.00002736
Iteration 19/1000 | Loss: 0.00002736
Iteration 20/1000 | Loss: 0.00002736
Iteration 21/1000 | Loss: 0.00002735
Iteration 22/1000 | Loss: 0.00002734
Iteration 23/1000 | Loss: 0.00002733
Iteration 24/1000 | Loss: 0.00002733
Iteration 25/1000 | Loss: 0.00002732
Iteration 26/1000 | Loss: 0.00002732
Iteration 27/1000 | Loss: 0.00002732
Iteration 28/1000 | Loss: 0.00002731
Iteration 29/1000 | Loss: 0.00002731
Iteration 30/1000 | Loss: 0.00002731
Iteration 31/1000 | Loss: 0.00002731
Iteration 32/1000 | Loss: 0.00002731
Iteration 33/1000 | Loss: 0.00002731
Iteration 34/1000 | Loss: 0.00002731
Iteration 35/1000 | Loss: 0.00002731
Iteration 36/1000 | Loss: 0.00002730
Iteration 37/1000 | Loss: 0.00002730
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002730
Iteration 41/1000 | Loss: 0.00002730
Iteration 42/1000 | Loss: 0.00002730
Iteration 43/1000 | Loss: 0.00002729
Iteration 44/1000 | Loss: 0.00002729
Iteration 45/1000 | Loss: 0.00002729
Iteration 46/1000 | Loss: 0.00002729
Iteration 47/1000 | Loss: 0.00002729
Iteration 48/1000 | Loss: 0.00002728
Iteration 49/1000 | Loss: 0.00002728
Iteration 50/1000 | Loss: 0.00002728
Iteration 51/1000 | Loss: 0.00002728
Iteration 52/1000 | Loss: 0.00002728
Iteration 53/1000 | Loss: 0.00002727
Iteration 54/1000 | Loss: 0.00002727
Iteration 55/1000 | Loss: 0.00002727
Iteration 56/1000 | Loss: 0.00002726
Iteration 57/1000 | Loss: 0.00002726
Iteration 58/1000 | Loss: 0.00002726
Iteration 59/1000 | Loss: 0.00002726
Iteration 60/1000 | Loss: 0.00002726
Iteration 61/1000 | Loss: 0.00002725
Iteration 62/1000 | Loss: 0.00002725
Iteration 63/1000 | Loss: 0.00002725
Iteration 64/1000 | Loss: 0.00002725
Iteration 65/1000 | Loss: 0.00002724
Iteration 66/1000 | Loss: 0.00002724
Iteration 67/1000 | Loss: 0.00002723
Iteration 68/1000 | Loss: 0.00002723
Iteration 69/1000 | Loss: 0.00002723
Iteration 70/1000 | Loss: 0.00002722
Iteration 71/1000 | Loss: 0.00002722
Iteration 72/1000 | Loss: 0.00002721
Iteration 73/1000 | Loss: 0.00002721
Iteration 74/1000 | Loss: 0.00002721
Iteration 75/1000 | Loss: 0.00002720
Iteration 76/1000 | Loss: 0.00002720
Iteration 77/1000 | Loss: 0.00002720
Iteration 78/1000 | Loss: 0.00002719
Iteration 79/1000 | Loss: 0.00002719
Iteration 80/1000 | Loss: 0.00002719
Iteration 81/1000 | Loss: 0.00002719
Iteration 82/1000 | Loss: 0.00002719
Iteration 83/1000 | Loss: 0.00002719
Iteration 84/1000 | Loss: 0.00002719
Iteration 85/1000 | Loss: 0.00002719
Iteration 86/1000 | Loss: 0.00002719
Iteration 87/1000 | Loss: 0.00002718
Iteration 88/1000 | Loss: 0.00002718
Iteration 89/1000 | Loss: 0.00002718
Iteration 90/1000 | Loss: 0.00002718
Iteration 91/1000 | Loss: 0.00002718
Iteration 92/1000 | Loss: 0.00002717
Iteration 93/1000 | Loss: 0.00002717
Iteration 94/1000 | Loss: 0.00002717
Iteration 95/1000 | Loss: 0.00002717
Iteration 96/1000 | Loss: 0.00002717
Iteration 97/1000 | Loss: 0.00002717
Iteration 98/1000 | Loss: 0.00002716
Iteration 99/1000 | Loss: 0.00002716
Iteration 100/1000 | Loss: 0.00002716
Iteration 101/1000 | Loss: 0.00002715
Iteration 102/1000 | Loss: 0.00002715
Iteration 103/1000 | Loss: 0.00002715
Iteration 104/1000 | Loss: 0.00002715
Iteration 105/1000 | Loss: 0.00002714
Iteration 106/1000 | Loss: 0.00002714
Iteration 107/1000 | Loss: 0.00002714
Iteration 108/1000 | Loss: 0.00002714
Iteration 109/1000 | Loss: 0.00002713
Iteration 110/1000 | Loss: 0.00002713
Iteration 111/1000 | Loss: 0.00002713
Iteration 112/1000 | Loss: 0.00002713
Iteration 113/1000 | Loss: 0.00002713
Iteration 114/1000 | Loss: 0.00002713
Iteration 115/1000 | Loss: 0.00002713
Iteration 116/1000 | Loss: 0.00002713
Iteration 117/1000 | Loss: 0.00002713
Iteration 118/1000 | Loss: 0.00002713
Iteration 119/1000 | Loss: 0.00002712
Iteration 120/1000 | Loss: 0.00002712
Iteration 121/1000 | Loss: 0.00002712
Iteration 122/1000 | Loss: 0.00002712
Iteration 123/1000 | Loss: 0.00002712
Iteration 124/1000 | Loss: 0.00002711
Iteration 125/1000 | Loss: 0.00002711
Iteration 126/1000 | Loss: 0.00002711
Iteration 127/1000 | Loss: 0.00002711
Iteration 128/1000 | Loss: 0.00002711
Iteration 129/1000 | Loss: 0.00002711
Iteration 130/1000 | Loss: 0.00002711
Iteration 131/1000 | Loss: 0.00002711
Iteration 132/1000 | Loss: 0.00002711
Iteration 133/1000 | Loss: 0.00002711
Iteration 134/1000 | Loss: 0.00002711
Iteration 135/1000 | Loss: 0.00002711
Iteration 136/1000 | Loss: 0.00002710
Iteration 137/1000 | Loss: 0.00002710
Iteration 138/1000 | Loss: 0.00002710
Iteration 139/1000 | Loss: 0.00002710
Iteration 140/1000 | Loss: 0.00002710
Iteration 141/1000 | Loss: 0.00002709
Iteration 142/1000 | Loss: 0.00002709
Iteration 143/1000 | Loss: 0.00002709
Iteration 144/1000 | Loss: 0.00002709
Iteration 145/1000 | Loss: 0.00002709
Iteration 146/1000 | Loss: 0.00002709
Iteration 147/1000 | Loss: 0.00002709
Iteration 148/1000 | Loss: 0.00002709
Iteration 149/1000 | Loss: 0.00002709
Iteration 150/1000 | Loss: 0.00002709
Iteration 151/1000 | Loss: 0.00002709
Iteration 152/1000 | Loss: 0.00002709
Iteration 153/1000 | Loss: 0.00002709
Iteration 154/1000 | Loss: 0.00002709
Iteration 155/1000 | Loss: 0.00002709
Iteration 156/1000 | Loss: 0.00002709
Iteration 157/1000 | Loss: 0.00002709
Iteration 158/1000 | Loss: 0.00002708
Iteration 159/1000 | Loss: 0.00002708
Iteration 160/1000 | Loss: 0.00002708
Iteration 161/1000 | Loss: 0.00002708
Iteration 162/1000 | Loss: 0.00002708
Iteration 163/1000 | Loss: 0.00002708
Iteration 164/1000 | Loss: 0.00002708
Iteration 165/1000 | Loss: 0.00002708
Iteration 166/1000 | Loss: 0.00002708
Iteration 167/1000 | Loss: 0.00002708
Iteration 168/1000 | Loss: 0.00002708
Iteration 169/1000 | Loss: 0.00002708
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002708
Iteration 172/1000 | Loss: 0.00002708
Iteration 173/1000 | Loss: 0.00002708
Iteration 174/1000 | Loss: 0.00002708
Iteration 175/1000 | Loss: 0.00002708
Iteration 176/1000 | Loss: 0.00002708
Iteration 177/1000 | Loss: 0.00002708
Iteration 178/1000 | Loss: 0.00002708
Iteration 179/1000 | Loss: 0.00002708
Iteration 180/1000 | Loss: 0.00002708
Iteration 181/1000 | Loss: 0.00002707
Iteration 182/1000 | Loss: 0.00002707
Iteration 183/1000 | Loss: 0.00002707
Iteration 184/1000 | Loss: 0.00002707
Iteration 185/1000 | Loss: 0.00002707
Iteration 186/1000 | Loss: 0.00002707
Iteration 187/1000 | Loss: 0.00002707
Iteration 188/1000 | Loss: 0.00002707
Iteration 189/1000 | Loss: 0.00002707
Iteration 190/1000 | Loss: 0.00002707
Iteration 191/1000 | Loss: 0.00002707
Iteration 192/1000 | Loss: 0.00002707
Iteration 193/1000 | Loss: 0.00002707
Iteration 194/1000 | Loss: 0.00002707
Iteration 195/1000 | Loss: 0.00002707
Iteration 196/1000 | Loss: 0.00002707
Iteration 197/1000 | Loss: 0.00002707
Iteration 198/1000 | Loss: 0.00002707
Iteration 199/1000 | Loss: 0.00002707
Iteration 200/1000 | Loss: 0.00002707
Iteration 201/1000 | Loss: 0.00002707
Iteration 202/1000 | Loss: 0.00002707
Iteration 203/1000 | Loss: 0.00002707
Iteration 204/1000 | Loss: 0.00002707
Iteration 205/1000 | Loss: 0.00002707
Iteration 206/1000 | Loss: 0.00002707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.7072432203567587e-05, 2.7072432203567587e-05, 2.7072432203567587e-05, 2.7072432203567587e-05, 2.7072432203567587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7072432203567587e-05

Optimization complete. Final v2v error: 4.354377746582031 mm

Highest mean error: 4.72044038772583 mm for frame 99

Lowest mean error: 4.177608966827393 mm for frame 87

Saving results

Total time: 313.7105917930603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0023
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854966
Iteration 2/25 | Loss: 0.00158723
Iteration 3/25 | Loss: 0.00102003
Iteration 4/25 | Loss: 0.00092220
Iteration 5/25 | Loss: 0.00090913
Iteration 6/25 | Loss: 0.00090693
Iteration 7/25 | Loss: 0.00090653
Iteration 8/25 | Loss: 0.00090653
Iteration 9/25 | Loss: 0.00090653
Iteration 10/25 | Loss: 0.00090653
Iteration 11/25 | Loss: 0.00090653
Iteration 12/25 | Loss: 0.00090653
Iteration 13/25 | Loss: 0.00090653
Iteration 14/25 | Loss: 0.00090653
Iteration 15/25 | Loss: 0.00090653
Iteration 16/25 | Loss: 0.00090653
Iteration 17/25 | Loss: 0.00090653
Iteration 18/25 | Loss: 0.00090653
Iteration 19/25 | Loss: 0.00090653
Iteration 20/25 | Loss: 0.00090653
Iteration 21/25 | Loss: 0.00090653
Iteration 22/25 | Loss: 0.00090653
Iteration 23/25 | Loss: 0.00090653
Iteration 24/25 | Loss: 0.00090653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009065265185199678, 0.0009065265185199678, 0.0009065265185199678, 0.0009065265185199678, 0.0009065265185199678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009065265185199678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30945015
Iteration 2/25 | Loss: 0.00034844
Iteration 3/25 | Loss: 0.00034844
Iteration 4/25 | Loss: 0.00034843
Iteration 5/25 | Loss: 0.00034843
Iteration 6/25 | Loss: 0.00034843
Iteration 7/25 | Loss: 0.00034843
Iteration 8/25 | Loss: 0.00034843
Iteration 9/25 | Loss: 0.00034843
Iteration 10/25 | Loss: 0.00034843
Iteration 11/25 | Loss: 0.00034843
Iteration 12/25 | Loss: 0.00034843
Iteration 13/25 | Loss: 0.00034843
Iteration 14/25 | Loss: 0.00034843
Iteration 15/25 | Loss: 0.00034843
Iteration 16/25 | Loss: 0.00034843
Iteration 17/25 | Loss: 0.00034843
Iteration 18/25 | Loss: 0.00034843
Iteration 19/25 | Loss: 0.00034843
Iteration 20/25 | Loss: 0.00034843
Iteration 21/25 | Loss: 0.00034843
Iteration 22/25 | Loss: 0.00034843
Iteration 23/25 | Loss: 0.00034843
Iteration 24/25 | Loss: 0.00034843
Iteration 25/25 | Loss: 0.00034843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003484322514850646, 0.0003484322514850646, 0.0003484322514850646, 0.0003484322514850646, 0.0003484322514850646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003484322514850646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034843
Iteration 2/1000 | Loss: 0.00004970
Iteration 3/1000 | Loss: 0.00003241
Iteration 4/1000 | Loss: 0.00002610
Iteration 5/1000 | Loss: 0.00002295
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001851
Iteration 12/1000 | Loss: 0.00001821
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001806
Iteration 15/1000 | Loss: 0.00001792
Iteration 16/1000 | Loss: 0.00001790
Iteration 17/1000 | Loss: 0.00001790
Iteration 18/1000 | Loss: 0.00001788
Iteration 19/1000 | Loss: 0.00001787
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001779
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001779
Iteration 48/1000 | Loss: 0.00001779
Iteration 49/1000 | Loss: 0.00001779
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.7785539967007935e-05, 1.7785539967007935e-05, 1.7785539967007935e-05, 1.7785539967007935e-05, 1.7785539967007935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7785539967007935e-05

Optimization complete. Final v2v error: 3.6011013984680176 mm

Highest mean error: 4.074851989746094 mm for frame 0

Lowest mean error: 3.289473295211792 mm for frame 50

Saving results

Total time: 190.7103397846222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0024
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904427
Iteration 2/25 | Loss: 0.00138381
Iteration 3/25 | Loss: 0.00096563
Iteration 4/25 | Loss: 0.00092549
Iteration 5/25 | Loss: 0.00092223
Iteration 6/25 | Loss: 0.00092202
Iteration 7/25 | Loss: 0.00092202
Iteration 8/25 | Loss: 0.00092202
Iteration 9/25 | Loss: 0.00092202
Iteration 10/25 | Loss: 0.00092202
Iteration 11/25 | Loss: 0.00092202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009220162755809724, 0.0009220162755809724, 0.0009220162755809724, 0.0009220162755809724, 0.0009220162755809724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009220162755809724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33145308
Iteration 2/25 | Loss: 0.00040192
Iteration 3/25 | Loss: 0.00040192
Iteration 4/25 | Loss: 0.00040192
Iteration 5/25 | Loss: 0.00040192
Iteration 6/25 | Loss: 0.00040192
Iteration 7/25 | Loss: 0.00040192
Iteration 8/25 | Loss: 0.00040192
Iteration 9/25 | Loss: 0.00040192
Iteration 10/25 | Loss: 0.00040192
Iteration 11/25 | Loss: 0.00040192
Iteration 12/25 | Loss: 0.00040192
Iteration 13/25 | Loss: 0.00040192
Iteration 14/25 | Loss: 0.00040192
Iteration 15/25 | Loss: 0.00040192
Iteration 16/25 | Loss: 0.00040192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004019207553938031, 0.0004019207553938031, 0.0004019207553938031, 0.0004019207553938031, 0.0004019207553938031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004019207553938031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040192
Iteration 2/1000 | Loss: 0.00005871
Iteration 3/1000 | Loss: 0.00003950
Iteration 4/1000 | Loss: 0.00003405
Iteration 5/1000 | Loss: 0.00003181
Iteration 6/1000 | Loss: 0.00003079
Iteration 7/1000 | Loss: 0.00003026
Iteration 8/1000 | Loss: 0.00002979
Iteration 9/1000 | Loss: 0.00002949
Iteration 10/1000 | Loss: 0.00002942
Iteration 11/1000 | Loss: 0.00002940
Iteration 12/1000 | Loss: 0.00002932
Iteration 13/1000 | Loss: 0.00002922
Iteration 14/1000 | Loss: 0.00002920
Iteration 15/1000 | Loss: 0.00002920
Iteration 16/1000 | Loss: 0.00002920
Iteration 17/1000 | Loss: 0.00002920
Iteration 18/1000 | Loss: 0.00002919
Iteration 19/1000 | Loss: 0.00002919
Iteration 20/1000 | Loss: 0.00002919
Iteration 21/1000 | Loss: 0.00002918
Iteration 22/1000 | Loss: 0.00002918
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002918
Iteration 26/1000 | Loss: 0.00002916
Iteration 27/1000 | Loss: 0.00002916
Iteration 28/1000 | Loss: 0.00002916
Iteration 29/1000 | Loss: 0.00002915
Iteration 30/1000 | Loss: 0.00002915
Iteration 31/1000 | Loss: 0.00002915
Iteration 32/1000 | Loss: 0.00002915
Iteration 33/1000 | Loss: 0.00002915
Iteration 34/1000 | Loss: 0.00002915
Iteration 35/1000 | Loss: 0.00002915
Iteration 36/1000 | Loss: 0.00002914
Iteration 37/1000 | Loss: 0.00002914
Iteration 38/1000 | Loss: 0.00002914
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002914
Iteration 41/1000 | Loss: 0.00002914
Iteration 42/1000 | Loss: 0.00002914
Iteration 43/1000 | Loss: 0.00002914
Iteration 44/1000 | Loss: 0.00002914
Iteration 45/1000 | Loss: 0.00002914
Iteration 46/1000 | Loss: 0.00002914
Iteration 47/1000 | Loss: 0.00002914
Iteration 48/1000 | Loss: 0.00002914
Iteration 49/1000 | Loss: 0.00002914
Iteration 50/1000 | Loss: 0.00002914
Iteration 51/1000 | Loss: 0.00002914
Iteration 52/1000 | Loss: 0.00002914
Iteration 53/1000 | Loss: 0.00002914
Iteration 54/1000 | Loss: 0.00002914
Iteration 55/1000 | Loss: 0.00002914
Iteration 56/1000 | Loss: 0.00002914
Iteration 57/1000 | Loss: 0.00002914
Iteration 58/1000 | Loss: 0.00002914
Iteration 59/1000 | Loss: 0.00002914
Iteration 60/1000 | Loss: 0.00002914
Iteration 61/1000 | Loss: 0.00002914
Iteration 62/1000 | Loss: 0.00002914
Iteration 63/1000 | Loss: 0.00002914
Iteration 64/1000 | Loss: 0.00002914
Iteration 65/1000 | Loss: 0.00002914
Iteration 66/1000 | Loss: 0.00002914
Iteration 67/1000 | Loss: 0.00002914
Iteration 68/1000 | Loss: 0.00002914
Iteration 69/1000 | Loss: 0.00002914
Iteration 70/1000 | Loss: 0.00002914
Iteration 71/1000 | Loss: 0.00002914
Iteration 72/1000 | Loss: 0.00002914
Iteration 73/1000 | Loss: 0.00002914
Iteration 74/1000 | Loss: 0.00002914
Iteration 75/1000 | Loss: 0.00002914
Iteration 76/1000 | Loss: 0.00002914
Iteration 77/1000 | Loss: 0.00002914
Iteration 78/1000 | Loss: 0.00002914
Iteration 79/1000 | Loss: 0.00002914
Iteration 80/1000 | Loss: 0.00002914
Iteration 81/1000 | Loss: 0.00002914
Iteration 82/1000 | Loss: 0.00002914
Iteration 83/1000 | Loss: 0.00002914
Iteration 84/1000 | Loss: 0.00002914
Iteration 85/1000 | Loss: 0.00002914
Iteration 86/1000 | Loss: 0.00002914
Iteration 87/1000 | Loss: 0.00002913
Iteration 88/1000 | Loss: 0.00002913
Iteration 89/1000 | Loss: 0.00002913
Iteration 90/1000 | Loss: 0.00002913
Iteration 91/1000 | Loss: 0.00002913
Iteration 92/1000 | Loss: 0.00002913
Iteration 93/1000 | Loss: 0.00002913
Iteration 94/1000 | Loss: 0.00002913
Iteration 95/1000 | Loss: 0.00002913
Iteration 96/1000 | Loss: 0.00002913
Iteration 97/1000 | Loss: 0.00002913
Iteration 98/1000 | Loss: 0.00002913
Iteration 99/1000 | Loss: 0.00002913
Iteration 100/1000 | Loss: 0.00002913
Iteration 101/1000 | Loss: 0.00002913
Iteration 102/1000 | Loss: 0.00002913
Iteration 103/1000 | Loss: 0.00002913
Iteration 104/1000 | Loss: 0.00002913
Iteration 105/1000 | Loss: 0.00002913
Iteration 106/1000 | Loss: 0.00002913
Iteration 107/1000 | Loss: 0.00002913
Iteration 108/1000 | Loss: 0.00002913
Iteration 109/1000 | Loss: 0.00002913
Iteration 110/1000 | Loss: 0.00002913
Iteration 111/1000 | Loss: 0.00002913
Iteration 112/1000 | Loss: 0.00002913
Iteration 113/1000 | Loss: 0.00002913
Iteration 114/1000 | Loss: 0.00002913
Iteration 115/1000 | Loss: 0.00002913
Iteration 116/1000 | Loss: 0.00002913
Iteration 117/1000 | Loss: 0.00002913
Iteration 118/1000 | Loss: 0.00002913
Iteration 119/1000 | Loss: 0.00002913
Iteration 120/1000 | Loss: 0.00002913
Iteration 121/1000 | Loss: 0.00002913
Iteration 122/1000 | Loss: 0.00002913
Iteration 123/1000 | Loss: 0.00002913
Iteration 124/1000 | Loss: 0.00002913
Iteration 125/1000 | Loss: 0.00002913
Iteration 126/1000 | Loss: 0.00002913
Iteration 127/1000 | Loss: 0.00002913
Iteration 128/1000 | Loss: 0.00002913
Iteration 129/1000 | Loss: 0.00002913
Iteration 130/1000 | Loss: 0.00002913
Iteration 131/1000 | Loss: 0.00002913
Iteration 132/1000 | Loss: 0.00002913
Iteration 133/1000 | Loss: 0.00002913
Iteration 134/1000 | Loss: 0.00002913
Iteration 135/1000 | Loss: 0.00002913
Iteration 136/1000 | Loss: 0.00002913
Iteration 137/1000 | Loss: 0.00002913
Iteration 138/1000 | Loss: 0.00002913
Iteration 139/1000 | Loss: 0.00002913
Iteration 140/1000 | Loss: 0.00002913
Iteration 141/1000 | Loss: 0.00002913
Iteration 142/1000 | Loss: 0.00002913
Iteration 143/1000 | Loss: 0.00002913
Iteration 144/1000 | Loss: 0.00002913
Iteration 145/1000 | Loss: 0.00002913
Iteration 146/1000 | Loss: 0.00002913
Iteration 147/1000 | Loss: 0.00002913
Iteration 148/1000 | Loss: 0.00002913
Iteration 149/1000 | Loss: 0.00002913
Iteration 150/1000 | Loss: 0.00002913
Iteration 151/1000 | Loss: 0.00002913
Iteration 152/1000 | Loss: 0.00002913
Iteration 153/1000 | Loss: 0.00002913
Iteration 154/1000 | Loss: 0.00002913
Iteration 155/1000 | Loss: 0.00002913
Iteration 156/1000 | Loss: 0.00002913
Iteration 157/1000 | Loss: 0.00002913
Iteration 158/1000 | Loss: 0.00002913
Iteration 159/1000 | Loss: 0.00002913
Iteration 160/1000 | Loss: 0.00002913
Iteration 161/1000 | Loss: 0.00002913
Iteration 162/1000 | Loss: 0.00002913
Iteration 163/1000 | Loss: 0.00002913
Iteration 164/1000 | Loss: 0.00002913
Iteration 165/1000 | Loss: 0.00002913
Iteration 166/1000 | Loss: 0.00002913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.912884156103246e-05, 2.912884156103246e-05, 2.912884156103246e-05, 2.912884156103246e-05, 2.912884156103246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.912884156103246e-05

Optimization complete. Final v2v error: 4.509618282318115 mm

Highest mean error: 5.178436756134033 mm for frame 140

Lowest mean error: 3.749910831451416 mm for frame 2

Saving results

Total time: 379.39289140701294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0012
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959625
Iteration 2/25 | Loss: 0.00156479
Iteration 3/25 | Loss: 0.00107584
Iteration 4/25 | Loss: 0.00101007
Iteration 5/25 | Loss: 0.00099940
Iteration 6/25 | Loss: 0.00100753
Iteration 7/25 | Loss: 0.00100166
Iteration 8/25 | Loss: 0.00099000
Iteration 9/25 | Loss: 0.00098644
Iteration 10/25 | Loss: 0.00097928
Iteration 11/25 | Loss: 0.00096844
Iteration 12/25 | Loss: 0.00096601
Iteration 13/25 | Loss: 0.00096452
Iteration 14/25 | Loss: 0.00096375
Iteration 15/25 | Loss: 0.00096337
Iteration 16/25 | Loss: 0.00096307
Iteration 17/25 | Loss: 0.00096292
Iteration 18/25 | Loss: 0.00096283
Iteration 19/25 | Loss: 0.00096277
Iteration 20/25 | Loss: 0.00096277
Iteration 21/25 | Loss: 0.00096272
Iteration 22/25 | Loss: 0.00096272
Iteration 23/25 | Loss: 0.00096272
Iteration 24/25 | Loss: 0.00096272
Iteration 25/25 | Loss: 0.00096271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58547086
Iteration 2/25 | Loss: 0.00064298
Iteration 3/25 | Loss: 0.00064298
Iteration 4/25 | Loss: 0.00064298
Iteration 5/25 | Loss: 0.00064298
Iteration 6/25 | Loss: 0.00064298
Iteration 7/25 | Loss: 0.00064298
Iteration 8/25 | Loss: 0.00064298
Iteration 9/25 | Loss: 0.00064298
Iteration 10/25 | Loss: 0.00064298
Iteration 11/25 | Loss: 0.00064298
Iteration 12/25 | Loss: 0.00064298
Iteration 13/25 | Loss: 0.00064298
Iteration 14/25 | Loss: 0.00064298
Iteration 15/25 | Loss: 0.00064298
Iteration 16/25 | Loss: 0.00064298
Iteration 17/25 | Loss: 0.00064298
Iteration 18/25 | Loss: 0.00064298
Iteration 19/25 | Loss: 0.00064298
Iteration 20/25 | Loss: 0.00064298
Iteration 21/25 | Loss: 0.00064298
Iteration 22/25 | Loss: 0.00064298
Iteration 23/25 | Loss: 0.00064298
Iteration 24/25 | Loss: 0.00064298
Iteration 25/25 | Loss: 0.00064298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064298
Iteration 2/1000 | Loss: 0.00011562
Iteration 3/1000 | Loss: 0.00008281
Iteration 4/1000 | Loss: 0.00006810
Iteration 5/1000 | Loss: 0.00062556
Iteration 6/1000 | Loss: 0.00006992
Iteration 7/1000 | Loss: 0.00114251
Iteration 8/1000 | Loss: 0.00007201
Iteration 9/1000 | Loss: 0.00005956
Iteration 10/1000 | Loss: 0.00005453
Iteration 11/1000 | Loss: 0.00062268
Iteration 12/1000 | Loss: 0.00069161
Iteration 13/1000 | Loss: 0.00053195
Iteration 14/1000 | Loss: 0.00066469
Iteration 15/1000 | Loss: 0.00054259
Iteration 16/1000 | Loss: 0.00050112
Iteration 17/1000 | Loss: 0.00047168
Iteration 18/1000 | Loss: 0.00074106
Iteration 19/1000 | Loss: 0.00025599
Iteration 20/1000 | Loss: 0.00006158
Iteration 21/1000 | Loss: 0.00034307
Iteration 22/1000 | Loss: 0.00045114
Iteration 23/1000 | Loss: 0.00041620
Iteration 24/1000 | Loss: 0.00016671
Iteration 25/1000 | Loss: 0.00096300
Iteration 26/1000 | Loss: 0.00110770
Iteration 27/1000 | Loss: 0.00067483
Iteration 28/1000 | Loss: 0.00110637
Iteration 29/1000 | Loss: 0.00088440
Iteration 30/1000 | Loss: 0.00011254
Iteration 31/1000 | Loss: 0.00024658
Iteration 32/1000 | Loss: 0.00025650
Iteration 33/1000 | Loss: 0.00071925
Iteration 34/1000 | Loss: 0.00073350
Iteration 35/1000 | Loss: 0.00075278
Iteration 36/1000 | Loss: 0.00079986
Iteration 37/1000 | Loss: 0.00007079
Iteration 38/1000 | Loss: 0.00015616
Iteration 39/1000 | Loss: 0.00048188
Iteration 40/1000 | Loss: 0.00013579
Iteration 41/1000 | Loss: 0.00013609
Iteration 42/1000 | Loss: 0.00004413
Iteration 43/1000 | Loss: 0.00018340
Iteration 44/1000 | Loss: 0.00028598
Iteration 45/1000 | Loss: 0.00028383
Iteration 46/1000 | Loss: 0.00024241
Iteration 47/1000 | Loss: 0.00062552
Iteration 48/1000 | Loss: 0.00048953
Iteration 49/1000 | Loss: 0.00015493
Iteration 50/1000 | Loss: 0.00004984
Iteration 51/1000 | Loss: 0.00004517
Iteration 52/1000 | Loss: 0.00007050
Iteration 53/1000 | Loss: 0.00006206
Iteration 54/1000 | Loss: 0.00005840
Iteration 55/1000 | Loss: 0.00103854
Iteration 56/1000 | Loss: 0.00064163
Iteration 57/1000 | Loss: 0.00017600
Iteration 58/1000 | Loss: 0.00027067
Iteration 59/1000 | Loss: 0.00019801
Iteration 60/1000 | Loss: 0.00098343
Iteration 61/1000 | Loss: 0.00058530
Iteration 62/1000 | Loss: 0.00073404
Iteration 63/1000 | Loss: 0.00028357
Iteration 64/1000 | Loss: 0.00032922
Iteration 65/1000 | Loss: 0.00032692
Iteration 66/1000 | Loss: 0.00024997
Iteration 67/1000 | Loss: 0.00027497
Iteration 68/1000 | Loss: 0.00016910
Iteration 69/1000 | Loss: 0.00016364
Iteration 70/1000 | Loss: 0.00018386
Iteration 71/1000 | Loss: 0.00015469
Iteration 72/1000 | Loss: 0.00018626
Iteration 73/1000 | Loss: 0.00007954
Iteration 74/1000 | Loss: 0.00015031
Iteration 75/1000 | Loss: 0.00020840
Iteration 76/1000 | Loss: 0.00014325
Iteration 77/1000 | Loss: 0.00043415
Iteration 78/1000 | Loss: 0.00024394
Iteration 79/1000 | Loss: 0.00032314
Iteration 80/1000 | Loss: 0.00005660
Iteration 81/1000 | Loss: 0.00004923
Iteration 82/1000 | Loss: 0.00073160
Iteration 83/1000 | Loss: 0.00019137
Iteration 84/1000 | Loss: 0.00036585
Iteration 85/1000 | Loss: 0.00038123
Iteration 86/1000 | Loss: 0.00017557
Iteration 87/1000 | Loss: 0.00018677
Iteration 88/1000 | Loss: 0.00013930
Iteration 89/1000 | Loss: 0.00037900
Iteration 90/1000 | Loss: 0.00025793
Iteration 91/1000 | Loss: 0.00004790
Iteration 92/1000 | Loss: 0.00004191
Iteration 93/1000 | Loss: 0.00004008
Iteration 94/1000 | Loss: 0.00008650
Iteration 95/1000 | Loss: 0.00004365
Iteration 96/1000 | Loss: 0.00003876
Iteration 97/1000 | Loss: 0.00003719
Iteration 98/1000 | Loss: 0.00003642
Iteration 99/1000 | Loss: 0.00004788
Iteration 100/1000 | Loss: 0.00003569
Iteration 101/1000 | Loss: 0.00003502
Iteration 102/1000 | Loss: 0.00003463
Iteration 103/1000 | Loss: 0.00060610
Iteration 104/1000 | Loss: 0.00016524
Iteration 105/1000 | Loss: 0.00058498
Iteration 106/1000 | Loss: 0.00030988
Iteration 107/1000 | Loss: 0.00045471
Iteration 108/1000 | Loss: 0.00028005
Iteration 109/1000 | Loss: 0.00003704
Iteration 110/1000 | Loss: 0.00003489
Iteration 111/1000 | Loss: 0.00003426
Iteration 112/1000 | Loss: 0.00003406
Iteration 113/1000 | Loss: 0.00066009
Iteration 114/1000 | Loss: 0.00058587
Iteration 115/1000 | Loss: 0.00046141
Iteration 116/1000 | Loss: 0.00068285
Iteration 117/1000 | Loss: 0.00014739
Iteration 118/1000 | Loss: 0.00003984
Iteration 119/1000 | Loss: 0.00012301
Iteration 120/1000 | Loss: 0.00003645
Iteration 121/1000 | Loss: 0.00025631
Iteration 122/1000 | Loss: 0.00003721
Iteration 123/1000 | Loss: 0.00003575
Iteration 124/1000 | Loss: 0.00003508
Iteration 125/1000 | Loss: 0.00003468
Iteration 126/1000 | Loss: 0.00003441
Iteration 127/1000 | Loss: 0.00003408
Iteration 128/1000 | Loss: 0.00003401
Iteration 129/1000 | Loss: 0.00003400
Iteration 130/1000 | Loss: 0.00003400
Iteration 131/1000 | Loss: 0.00003391
Iteration 132/1000 | Loss: 0.00003388
Iteration 133/1000 | Loss: 0.00003388
Iteration 134/1000 | Loss: 0.00003388
Iteration 135/1000 | Loss: 0.00003388
Iteration 136/1000 | Loss: 0.00003388
Iteration 137/1000 | Loss: 0.00003388
Iteration 138/1000 | Loss: 0.00003388
Iteration 139/1000 | Loss: 0.00003388
Iteration 140/1000 | Loss: 0.00003388
Iteration 141/1000 | Loss: 0.00003387
Iteration 142/1000 | Loss: 0.00003387
Iteration 143/1000 | Loss: 0.00003387
Iteration 144/1000 | Loss: 0.00003387
Iteration 145/1000 | Loss: 0.00003387
Iteration 146/1000 | Loss: 0.00003387
Iteration 147/1000 | Loss: 0.00003386
Iteration 148/1000 | Loss: 0.00003377
Iteration 149/1000 | Loss: 0.00003377
Iteration 150/1000 | Loss: 0.00003377
Iteration 151/1000 | Loss: 0.00003377
Iteration 152/1000 | Loss: 0.00003376
Iteration 153/1000 | Loss: 0.00003376
Iteration 154/1000 | Loss: 0.00003375
Iteration 155/1000 | Loss: 0.00003375
Iteration 156/1000 | Loss: 0.00003375
Iteration 157/1000 | Loss: 0.00003375
Iteration 158/1000 | Loss: 0.00003375
Iteration 159/1000 | Loss: 0.00003375
Iteration 160/1000 | Loss: 0.00003375
Iteration 161/1000 | Loss: 0.00003375
Iteration 162/1000 | Loss: 0.00003375
Iteration 163/1000 | Loss: 0.00003375
Iteration 164/1000 | Loss: 0.00003375
Iteration 165/1000 | Loss: 0.00003374
Iteration 166/1000 | Loss: 0.00003374
Iteration 167/1000 | Loss: 0.00003374
Iteration 168/1000 | Loss: 0.00003374
Iteration 169/1000 | Loss: 0.00003374
Iteration 170/1000 | Loss: 0.00003374
Iteration 171/1000 | Loss: 0.00003374
Iteration 172/1000 | Loss: 0.00003374
Iteration 173/1000 | Loss: 0.00003374
Iteration 174/1000 | Loss: 0.00003374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.373638173798099e-05, 3.373638173798099e-05, 3.373638173798099e-05, 3.373638173798099e-05, 3.373638173798099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.373638173798099e-05

Optimization complete. Final v2v error: 4.485048294067383 mm

Highest mean error: 15.141944885253906 mm for frame 10

Lowest mean error: 4.147237777709961 mm for frame 202

Saving results

Total time: 1557.316897392273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0007
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086135
Iteration 2/25 | Loss: 0.01086135
Iteration 3/25 | Loss: 0.00240068
Iteration 4/25 | Loss: 0.00142767
Iteration 5/25 | Loss: 0.00116645
Iteration 6/25 | Loss: 0.00103448
Iteration 7/25 | Loss: 0.00096861
Iteration 8/25 | Loss: 0.00096396
Iteration 9/25 | Loss: 0.00094809
Iteration 10/25 | Loss: 0.00094428
Iteration 11/25 | Loss: 0.00094063
Iteration 12/25 | Loss: 0.00094867
Iteration 13/25 | Loss: 0.00094733
Iteration 14/25 | Loss: 0.00094958
Iteration 15/25 | Loss: 0.00094120
Iteration 16/25 | Loss: 0.00093779
Iteration 17/25 | Loss: 0.00093115
Iteration 18/25 | Loss: 0.00093300
Iteration 19/25 | Loss: 0.00092787
Iteration 20/25 | Loss: 0.00092733
Iteration 21/25 | Loss: 0.00097226
Iteration 22/25 | Loss: 0.00091198
Iteration 23/25 | Loss: 0.00087954
Iteration 24/25 | Loss: 0.00087256
Iteration 25/25 | Loss: 0.00086606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44807994
Iteration 2/25 | Loss: 0.00098184
Iteration 3/25 | Loss: 0.00098184
Iteration 4/25 | Loss: 0.00098183
Iteration 5/25 | Loss: 0.00098183
Iteration 6/25 | Loss: 0.00098183
Iteration 7/25 | Loss: 0.00098183
Iteration 8/25 | Loss: 0.00098183
Iteration 9/25 | Loss: 0.00098183
Iteration 10/25 | Loss: 0.00098183
Iteration 11/25 | Loss: 0.00098183
Iteration 12/25 | Loss: 0.00098183
Iteration 13/25 | Loss: 0.00098183
Iteration 14/25 | Loss: 0.00098183
Iteration 15/25 | Loss: 0.00098183
Iteration 16/25 | Loss: 0.00098183
Iteration 17/25 | Loss: 0.00098183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00098183355294168, 0.00098183355294168, 0.00098183355294168, 0.00098183355294168, 0.00098183355294168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00098183355294168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098183
Iteration 2/1000 | Loss: 0.00069858
Iteration 3/1000 | Loss: 0.00010611
Iteration 4/1000 | Loss: 0.00106600
Iteration 5/1000 | Loss: 0.00057146
Iteration 6/1000 | Loss: 0.00023851
Iteration 7/1000 | Loss: 0.00005953
Iteration 8/1000 | Loss: 0.00108364
Iteration 9/1000 | Loss: 0.00028291
Iteration 10/1000 | Loss: 0.00026857
Iteration 11/1000 | Loss: 0.00024744
Iteration 12/1000 | Loss: 0.00026408
Iteration 13/1000 | Loss: 0.00006302
Iteration 14/1000 | Loss: 0.00037930
Iteration 15/1000 | Loss: 0.00025981
Iteration 16/1000 | Loss: 0.00005205
Iteration 17/1000 | Loss: 0.00038556
Iteration 18/1000 | Loss: 0.00031990
Iteration 19/1000 | Loss: 0.00044014
Iteration 20/1000 | Loss: 0.00030254
Iteration 21/1000 | Loss: 0.00033991
Iteration 22/1000 | Loss: 0.00030144
Iteration 23/1000 | Loss: 0.00028991
Iteration 24/1000 | Loss: 0.00054990
Iteration 25/1000 | Loss: 0.00004600
Iteration 26/1000 | Loss: 0.00066921
Iteration 27/1000 | Loss: 0.00067760
Iteration 28/1000 | Loss: 0.00015855
Iteration 29/1000 | Loss: 0.00004336
Iteration 30/1000 | Loss: 0.00004694
Iteration 31/1000 | Loss: 0.00004360
Iteration 32/1000 | Loss: 0.00003646
Iteration 33/1000 | Loss: 0.00003851
Iteration 34/1000 | Loss: 0.00003380
Iteration 35/1000 | Loss: 0.00003141
Iteration 36/1000 | Loss: 0.00004159
Iteration 37/1000 | Loss: 0.00003851
Iteration 38/1000 | Loss: 0.00004199
Iteration 39/1000 | Loss: 0.00003981
Iteration 40/1000 | Loss: 0.00003959
Iteration 41/1000 | Loss: 0.00003864
Iteration 42/1000 | Loss: 0.00004252
Iteration 43/1000 | Loss: 0.00004041
Iteration 44/1000 | Loss: 0.00004197
Iteration 45/1000 | Loss: 0.00004346
Iteration 46/1000 | Loss: 0.00003915
Iteration 47/1000 | Loss: 0.00004171
Iteration 48/1000 | Loss: 0.00004175
Iteration 49/1000 | Loss: 0.00004154
Iteration 50/1000 | Loss: 0.00004260
Iteration 51/1000 | Loss: 0.00003736
Iteration 52/1000 | Loss: 0.00003985
Iteration 53/1000 | Loss: 0.00003985
Iteration 54/1000 | Loss: 0.00004082
Iteration 55/1000 | Loss: 0.00004208
Iteration 56/1000 | Loss: 0.00004102
Iteration 57/1000 | Loss: 0.00003902
Iteration 58/1000 | Loss: 0.00004083
Iteration 59/1000 | Loss: 0.00099161
Iteration 60/1000 | Loss: 0.00015011
Iteration 61/1000 | Loss: 0.00003717
Iteration 62/1000 | Loss: 0.00003210
Iteration 63/1000 | Loss: 0.00003052
Iteration 64/1000 | Loss: 0.00002978
Iteration 65/1000 | Loss: 0.00002928
Iteration 66/1000 | Loss: 0.00002891
Iteration 67/1000 | Loss: 0.00002866
Iteration 68/1000 | Loss: 0.00002862
Iteration 69/1000 | Loss: 0.00002841
Iteration 70/1000 | Loss: 0.00002823
Iteration 71/1000 | Loss: 0.00002819
Iteration 72/1000 | Loss: 0.00002818
Iteration 73/1000 | Loss: 0.00002817
Iteration 74/1000 | Loss: 0.00002817
Iteration 75/1000 | Loss: 0.00002816
Iteration 76/1000 | Loss: 0.00002814
Iteration 77/1000 | Loss: 0.00002805
Iteration 78/1000 | Loss: 0.00002799
Iteration 79/1000 | Loss: 0.00002793
Iteration 80/1000 | Loss: 0.00002793
Iteration 81/1000 | Loss: 0.00002791
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002790
Iteration 84/1000 | Loss: 0.00002790
Iteration 85/1000 | Loss: 0.00002789
Iteration 86/1000 | Loss: 0.00002789
Iteration 87/1000 | Loss: 0.00002788
Iteration 88/1000 | Loss: 0.00002787
Iteration 89/1000 | Loss: 0.00002787
Iteration 90/1000 | Loss: 0.00002785
Iteration 91/1000 | Loss: 0.00002784
Iteration 92/1000 | Loss: 0.00002782
Iteration 93/1000 | Loss: 0.00002782
Iteration 94/1000 | Loss: 0.00002781
Iteration 95/1000 | Loss: 0.00002780
Iteration 96/1000 | Loss: 0.00002780
Iteration 97/1000 | Loss: 0.00002779
Iteration 98/1000 | Loss: 0.00002777
Iteration 99/1000 | Loss: 0.00002776
Iteration 100/1000 | Loss: 0.00002775
Iteration 101/1000 | Loss: 0.00002775
Iteration 102/1000 | Loss: 0.00002774
Iteration 103/1000 | Loss: 0.00002774
Iteration 104/1000 | Loss: 0.00002774
Iteration 105/1000 | Loss: 0.00002773
Iteration 106/1000 | Loss: 0.00002773
Iteration 107/1000 | Loss: 0.00002773
Iteration 108/1000 | Loss: 0.00002772
Iteration 109/1000 | Loss: 0.00002772
Iteration 110/1000 | Loss: 0.00002771
Iteration 111/1000 | Loss: 0.00002770
Iteration 112/1000 | Loss: 0.00002770
Iteration 113/1000 | Loss: 0.00002769
Iteration 114/1000 | Loss: 0.00002769
Iteration 115/1000 | Loss: 0.00002767
Iteration 116/1000 | Loss: 0.00002767
Iteration 117/1000 | Loss: 0.00002766
Iteration 118/1000 | Loss: 0.00002766
Iteration 119/1000 | Loss: 0.00002765
Iteration 120/1000 | Loss: 0.00002765
Iteration 121/1000 | Loss: 0.00002763
Iteration 122/1000 | Loss: 0.00002763
Iteration 123/1000 | Loss: 0.00002763
Iteration 124/1000 | Loss: 0.00002763
Iteration 125/1000 | Loss: 0.00002763
Iteration 126/1000 | Loss: 0.00002763
Iteration 127/1000 | Loss: 0.00002763
Iteration 128/1000 | Loss: 0.00002763
Iteration 129/1000 | Loss: 0.00002763
Iteration 130/1000 | Loss: 0.00002762
Iteration 131/1000 | Loss: 0.00002761
Iteration 132/1000 | Loss: 0.00002761
Iteration 133/1000 | Loss: 0.00002760
Iteration 134/1000 | Loss: 0.00002760
Iteration 135/1000 | Loss: 0.00002760
Iteration 136/1000 | Loss: 0.00002760
Iteration 137/1000 | Loss: 0.00002760
Iteration 138/1000 | Loss: 0.00002760
Iteration 139/1000 | Loss: 0.00002760
Iteration 140/1000 | Loss: 0.00002760
Iteration 141/1000 | Loss: 0.00002760
Iteration 142/1000 | Loss: 0.00002760
Iteration 143/1000 | Loss: 0.00002760
Iteration 144/1000 | Loss: 0.00002760
Iteration 145/1000 | Loss: 0.00002760
Iteration 146/1000 | Loss: 0.00002760
Iteration 147/1000 | Loss: 0.00002759
Iteration 148/1000 | Loss: 0.00002759
Iteration 149/1000 | Loss: 0.00002759
Iteration 150/1000 | Loss: 0.00002759
Iteration 151/1000 | Loss: 0.00002759
Iteration 152/1000 | Loss: 0.00002759
Iteration 153/1000 | Loss: 0.00002759
Iteration 154/1000 | Loss: 0.00002759
Iteration 155/1000 | Loss: 0.00002759
Iteration 156/1000 | Loss: 0.00002758
Iteration 157/1000 | Loss: 0.00002758
Iteration 158/1000 | Loss: 0.00002758
Iteration 159/1000 | Loss: 0.00002758
Iteration 160/1000 | Loss: 0.00002758
Iteration 161/1000 | Loss: 0.00002758
Iteration 162/1000 | Loss: 0.00002758
Iteration 163/1000 | Loss: 0.00002758
Iteration 164/1000 | Loss: 0.00002758
Iteration 165/1000 | Loss: 0.00002758
Iteration 166/1000 | Loss: 0.00002757
Iteration 167/1000 | Loss: 0.00002757
Iteration 168/1000 | Loss: 0.00002757
Iteration 169/1000 | Loss: 0.00002757
Iteration 170/1000 | Loss: 0.00002757
Iteration 171/1000 | Loss: 0.00002757
Iteration 172/1000 | Loss: 0.00002757
Iteration 173/1000 | Loss: 0.00002757
Iteration 174/1000 | Loss: 0.00002757
Iteration 175/1000 | Loss: 0.00002757
Iteration 176/1000 | Loss: 0.00002757
Iteration 177/1000 | Loss: 0.00002757
Iteration 178/1000 | Loss: 0.00002757
Iteration 179/1000 | Loss: 0.00002757
Iteration 180/1000 | Loss: 0.00002757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.7572505132411607e-05, 2.7572505132411607e-05, 2.7572505132411607e-05, 2.7572505132411607e-05, 2.7572505132411607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7572505132411607e-05

Optimization complete. Final v2v error: 4.0127692222595215 mm

Highest mean error: 14.969704627990723 mm for frame 206

Lowest mean error: 3.4384801387786865 mm for frame 178

Saving results

Total time: 1566.5920026302338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0003
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930406
Iteration 2/25 | Loss: 0.00112995
Iteration 3/25 | Loss: 0.00086805
Iteration 4/25 | Loss: 0.00083923
Iteration 5/25 | Loss: 0.00082947
Iteration 6/25 | Loss: 0.00082720
Iteration 7/25 | Loss: 0.00082694
Iteration 8/25 | Loss: 0.00082694
Iteration 9/25 | Loss: 0.00082694
Iteration 10/25 | Loss: 0.00082694
Iteration 11/25 | Loss: 0.00082694
Iteration 12/25 | Loss: 0.00082694
Iteration 13/25 | Loss: 0.00082694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008269376703538001, 0.0008269376703538001, 0.0008269376703538001, 0.0008269376703538001, 0.0008269376703538001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008269376703538001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46442556
Iteration 2/25 | Loss: 0.00035080
Iteration 3/25 | Loss: 0.00035080
Iteration 4/25 | Loss: 0.00035080
Iteration 5/25 | Loss: 0.00035080
Iteration 6/25 | Loss: 0.00035080
Iteration 7/25 | Loss: 0.00035080
Iteration 8/25 | Loss: 0.00035080
Iteration 9/25 | Loss: 0.00035080
Iteration 10/25 | Loss: 0.00035080
Iteration 11/25 | Loss: 0.00035080
Iteration 12/25 | Loss: 0.00035080
Iteration 13/25 | Loss: 0.00035080
Iteration 14/25 | Loss: 0.00035080
Iteration 15/25 | Loss: 0.00035080
Iteration 16/25 | Loss: 0.00035080
Iteration 17/25 | Loss: 0.00035080
Iteration 18/25 | Loss: 0.00035080
Iteration 19/25 | Loss: 0.00035080
Iteration 20/25 | Loss: 0.00035080
Iteration 21/25 | Loss: 0.00035080
Iteration 22/25 | Loss: 0.00035080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003507963556330651, 0.0003507963556330651, 0.0003507963556330651, 0.0003507963556330651, 0.0003507963556330651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003507963556330651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035080
Iteration 2/1000 | Loss: 0.00006047
Iteration 3/1000 | Loss: 0.00003757
Iteration 4/1000 | Loss: 0.00003387
Iteration 5/1000 | Loss: 0.00003160
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00002959
Iteration 8/1000 | Loss: 0.00002903
Iteration 9/1000 | Loss: 0.00002871
Iteration 10/1000 | Loss: 0.00002840
Iteration 11/1000 | Loss: 0.00002832
Iteration 12/1000 | Loss: 0.00002826
Iteration 13/1000 | Loss: 0.00002816
Iteration 14/1000 | Loss: 0.00002814
Iteration 15/1000 | Loss: 0.00002812
Iteration 16/1000 | Loss: 0.00002812
Iteration 17/1000 | Loss: 0.00002812
Iteration 18/1000 | Loss: 0.00002812
Iteration 19/1000 | Loss: 0.00002811
Iteration 20/1000 | Loss: 0.00002811
Iteration 21/1000 | Loss: 0.00002810
Iteration 22/1000 | Loss: 0.00002810
Iteration 23/1000 | Loss: 0.00002810
Iteration 24/1000 | Loss: 0.00002810
Iteration 25/1000 | Loss: 0.00002810
Iteration 26/1000 | Loss: 0.00002810
Iteration 27/1000 | Loss: 0.00002810
Iteration 28/1000 | Loss: 0.00002810
Iteration 29/1000 | Loss: 0.00002809
Iteration 30/1000 | Loss: 0.00002809
Iteration 31/1000 | Loss: 0.00002809
Iteration 32/1000 | Loss: 0.00002809
Iteration 33/1000 | Loss: 0.00002808
Iteration 34/1000 | Loss: 0.00002808
Iteration 35/1000 | Loss: 0.00002808
Iteration 36/1000 | Loss: 0.00002807
Iteration 37/1000 | Loss: 0.00002806
Iteration 38/1000 | Loss: 0.00002806
Iteration 39/1000 | Loss: 0.00002806
Iteration 40/1000 | Loss: 0.00002806
Iteration 41/1000 | Loss: 0.00002805
Iteration 42/1000 | Loss: 0.00002805
Iteration 43/1000 | Loss: 0.00002805
Iteration 44/1000 | Loss: 0.00002804
Iteration 45/1000 | Loss: 0.00002803
Iteration 46/1000 | Loss: 0.00002803
Iteration 47/1000 | Loss: 0.00002803
Iteration 48/1000 | Loss: 0.00002803
Iteration 49/1000 | Loss: 0.00002803
Iteration 50/1000 | Loss: 0.00002803
Iteration 51/1000 | Loss: 0.00002802
Iteration 52/1000 | Loss: 0.00002802
Iteration 53/1000 | Loss: 0.00002800
Iteration 54/1000 | Loss: 0.00002800
Iteration 55/1000 | Loss: 0.00002799
Iteration 56/1000 | Loss: 0.00002799
Iteration 57/1000 | Loss: 0.00002799
Iteration 58/1000 | Loss: 0.00002798
Iteration 59/1000 | Loss: 0.00002798
Iteration 60/1000 | Loss: 0.00002797
Iteration 61/1000 | Loss: 0.00002797
Iteration 62/1000 | Loss: 0.00002797
Iteration 63/1000 | Loss: 0.00002797
Iteration 64/1000 | Loss: 0.00002796
Iteration 65/1000 | Loss: 0.00002796
Iteration 66/1000 | Loss: 0.00002796
Iteration 67/1000 | Loss: 0.00002796
Iteration 68/1000 | Loss: 0.00002795
Iteration 69/1000 | Loss: 0.00002795
Iteration 70/1000 | Loss: 0.00002794
Iteration 71/1000 | Loss: 0.00002794
Iteration 72/1000 | Loss: 0.00002794
Iteration 73/1000 | Loss: 0.00002793
Iteration 74/1000 | Loss: 0.00002793
Iteration 75/1000 | Loss: 0.00002793
Iteration 76/1000 | Loss: 0.00002792
Iteration 77/1000 | Loss: 0.00002792
Iteration 78/1000 | Loss: 0.00002791
Iteration 79/1000 | Loss: 0.00002791
Iteration 80/1000 | Loss: 0.00002791
Iteration 81/1000 | Loss: 0.00002791
Iteration 82/1000 | Loss: 0.00002790
Iteration 83/1000 | Loss: 0.00002790
Iteration 84/1000 | Loss: 0.00002790
Iteration 85/1000 | Loss: 0.00002790
Iteration 86/1000 | Loss: 0.00002790
Iteration 87/1000 | Loss: 0.00002790
Iteration 88/1000 | Loss: 0.00002790
Iteration 89/1000 | Loss: 0.00002790
Iteration 90/1000 | Loss: 0.00002790
Iteration 91/1000 | Loss: 0.00002790
Iteration 92/1000 | Loss: 0.00002790
Iteration 93/1000 | Loss: 0.00002790
Iteration 94/1000 | Loss: 0.00002790
Iteration 95/1000 | Loss: 0.00002789
Iteration 96/1000 | Loss: 0.00002789
Iteration 97/1000 | Loss: 0.00002789
Iteration 98/1000 | Loss: 0.00002789
Iteration 99/1000 | Loss: 0.00002788
Iteration 100/1000 | Loss: 0.00002788
Iteration 101/1000 | Loss: 0.00002788
Iteration 102/1000 | Loss: 0.00002788
Iteration 103/1000 | Loss: 0.00002787
Iteration 104/1000 | Loss: 0.00002787
Iteration 105/1000 | Loss: 0.00002787
Iteration 106/1000 | Loss: 0.00002787
Iteration 107/1000 | Loss: 0.00002787
Iteration 108/1000 | Loss: 0.00002787
Iteration 109/1000 | Loss: 0.00002787
Iteration 110/1000 | Loss: 0.00002787
Iteration 111/1000 | Loss: 0.00002786
Iteration 112/1000 | Loss: 0.00002786
Iteration 113/1000 | Loss: 0.00002786
Iteration 114/1000 | Loss: 0.00002786
Iteration 115/1000 | Loss: 0.00002786
Iteration 116/1000 | Loss: 0.00002786
Iteration 117/1000 | Loss: 0.00002786
Iteration 118/1000 | Loss: 0.00002786
Iteration 119/1000 | Loss: 0.00002786
Iteration 120/1000 | Loss: 0.00002785
Iteration 121/1000 | Loss: 0.00002785
Iteration 122/1000 | Loss: 0.00002785
Iteration 123/1000 | Loss: 0.00002785
Iteration 124/1000 | Loss: 0.00002785
Iteration 125/1000 | Loss: 0.00002785
Iteration 126/1000 | Loss: 0.00002785
Iteration 127/1000 | Loss: 0.00002785
Iteration 128/1000 | Loss: 0.00002785
Iteration 129/1000 | Loss: 0.00002785
Iteration 130/1000 | Loss: 0.00002785
Iteration 131/1000 | Loss: 0.00002785
Iteration 132/1000 | Loss: 0.00002785
Iteration 133/1000 | Loss: 0.00002785
Iteration 134/1000 | Loss: 0.00002785
Iteration 135/1000 | Loss: 0.00002785
Iteration 136/1000 | Loss: 0.00002785
Iteration 137/1000 | Loss: 0.00002785
Iteration 138/1000 | Loss: 0.00002784
Iteration 139/1000 | Loss: 0.00002784
Iteration 140/1000 | Loss: 0.00002784
Iteration 141/1000 | Loss: 0.00002784
Iteration 142/1000 | Loss: 0.00002784
Iteration 143/1000 | Loss: 0.00002784
Iteration 144/1000 | Loss: 0.00002784
Iteration 145/1000 | Loss: 0.00002784
Iteration 146/1000 | Loss: 0.00002784
Iteration 147/1000 | Loss: 0.00002784
Iteration 148/1000 | Loss: 0.00002784
Iteration 149/1000 | Loss: 0.00002784
Iteration 150/1000 | Loss: 0.00002784
Iteration 151/1000 | Loss: 0.00002784
Iteration 152/1000 | Loss: 0.00002784
Iteration 153/1000 | Loss: 0.00002784
Iteration 154/1000 | Loss: 0.00002784
Iteration 155/1000 | Loss: 0.00002784
Iteration 156/1000 | Loss: 0.00002784
Iteration 157/1000 | Loss: 0.00002784
Iteration 158/1000 | Loss: 0.00002784
Iteration 159/1000 | Loss: 0.00002784
Iteration 160/1000 | Loss: 0.00002784
Iteration 161/1000 | Loss: 0.00002784
Iteration 162/1000 | Loss: 0.00002784
Iteration 163/1000 | Loss: 0.00002784
Iteration 164/1000 | Loss: 0.00002784
Iteration 165/1000 | Loss: 0.00002784
Iteration 166/1000 | Loss: 0.00002784
Iteration 167/1000 | Loss: 0.00002784
Iteration 168/1000 | Loss: 0.00002784
Iteration 169/1000 | Loss: 0.00002784
Iteration 170/1000 | Loss: 0.00002784
Iteration 171/1000 | Loss: 0.00002784
Iteration 172/1000 | Loss: 0.00002784
Iteration 173/1000 | Loss: 0.00002784
Iteration 174/1000 | Loss: 0.00002784
Iteration 175/1000 | Loss: 0.00002784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.784092976071406e-05, 2.784092976071406e-05, 2.784092976071406e-05, 2.784092976071406e-05, 2.784092976071406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.784092976071406e-05

Optimization complete. Final v2v error: 4.342462539672852 mm

Highest mean error: 5.721008777618408 mm for frame 59

Lowest mean error: 3.7337303161621094 mm for frame 31

Saving results

Total time: 242.06655550003052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0018
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970741
Iteration 2/25 | Loss: 0.00147974
Iteration 3/25 | Loss: 0.00104428
Iteration 4/25 | Loss: 0.00087220
Iteration 5/25 | Loss: 0.00085328
Iteration 6/25 | Loss: 0.00084195
Iteration 7/25 | Loss: 0.00083888
Iteration 8/25 | Loss: 0.00082524
Iteration 9/25 | Loss: 0.00082617
Iteration 10/25 | Loss: 0.00082636
Iteration 11/25 | Loss: 0.00082249
Iteration 12/25 | Loss: 0.00081878
Iteration 13/25 | Loss: 0.00081839
Iteration 14/25 | Loss: 0.00082119
Iteration 15/25 | Loss: 0.00081742
Iteration 16/25 | Loss: 0.00082096
Iteration 17/25 | Loss: 0.00081727
Iteration 18/25 | Loss: 0.00081727
Iteration 19/25 | Loss: 0.00081727
Iteration 20/25 | Loss: 0.00081727
Iteration 21/25 | Loss: 0.00081726
Iteration 22/25 | Loss: 0.00081726
Iteration 23/25 | Loss: 0.00081726
Iteration 24/25 | Loss: 0.00081726
Iteration 25/25 | Loss: 0.00081726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41026449
Iteration 2/25 | Loss: 0.00042727
Iteration 3/25 | Loss: 0.00042727
Iteration 4/25 | Loss: 0.00042727
Iteration 5/25 | Loss: 0.00042727
Iteration 6/25 | Loss: 0.00042727
Iteration 7/25 | Loss: 0.00042727
Iteration 8/25 | Loss: 0.00042727
Iteration 9/25 | Loss: 0.00042727
Iteration 10/25 | Loss: 0.00042727
Iteration 11/25 | Loss: 0.00042727
Iteration 12/25 | Loss: 0.00042727
Iteration 13/25 | Loss: 0.00042727
Iteration 14/25 | Loss: 0.00042727
Iteration 15/25 | Loss: 0.00042727
Iteration 16/25 | Loss: 0.00042727
Iteration 17/25 | Loss: 0.00042727
Iteration 18/25 | Loss: 0.00042727
Iteration 19/25 | Loss: 0.00042727
Iteration 20/25 | Loss: 0.00042727
Iteration 21/25 | Loss: 0.00042727
Iteration 22/25 | Loss: 0.00042727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000427269987994805, 0.000427269987994805, 0.000427269987994805, 0.000427269987994805, 0.000427269987994805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000427269987994805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042727
Iteration 2/1000 | Loss: 0.00004929
Iteration 3/1000 | Loss: 0.00002980
Iteration 4/1000 | Loss: 0.00002529
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002234
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002167
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002157
Iteration 11/1000 | Loss: 0.00002153
Iteration 12/1000 | Loss: 0.00002148
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002144
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002142
Iteration 17/1000 | Loss: 0.00002141
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002139
Iteration 20/1000 | Loss: 0.00002138
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00002138
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002138
Iteration 26/1000 | Loss: 0.00002138
Iteration 27/1000 | Loss: 0.00002138
Iteration 28/1000 | Loss: 0.00002138
Iteration 29/1000 | Loss: 0.00002138
Iteration 30/1000 | Loss: 0.00002138
Iteration 31/1000 | Loss: 0.00002137
Iteration 32/1000 | Loss: 0.00002136
Iteration 33/1000 | Loss: 0.00002136
Iteration 34/1000 | Loss: 0.00002136
Iteration 35/1000 | Loss: 0.00002136
Iteration 36/1000 | Loss: 0.00002136
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002134
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002133
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002132
Iteration 47/1000 | Loss: 0.00002132
Iteration 48/1000 | Loss: 0.00002132
Iteration 49/1000 | Loss: 0.00002132
Iteration 50/1000 | Loss: 0.00002132
Iteration 51/1000 | Loss: 0.00002131
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002131
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002131
Iteration 56/1000 | Loss: 0.00002131
Iteration 57/1000 | Loss: 0.00002130
Iteration 58/1000 | Loss: 0.00002130
Iteration 59/1000 | Loss: 0.00002130
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002130
Iteration 62/1000 | Loss: 0.00002130
Iteration 63/1000 | Loss: 0.00002130
Iteration 64/1000 | Loss: 0.00002130
Iteration 65/1000 | Loss: 0.00002130
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002130
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002129
Iteration 74/1000 | Loss: 0.00002129
Iteration 75/1000 | Loss: 0.00002129
Iteration 76/1000 | Loss: 0.00002129
Iteration 77/1000 | Loss: 0.00002129
Iteration 78/1000 | Loss: 0.00002129
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002128
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002126
Iteration 92/1000 | Loss: 0.00002126
Iteration 93/1000 | Loss: 0.00002126
Iteration 94/1000 | Loss: 0.00002126
Iteration 95/1000 | Loss: 0.00002126
Iteration 96/1000 | Loss: 0.00002126
Iteration 97/1000 | Loss: 0.00002126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.1257574189803563e-05, 2.1257574189803563e-05, 2.1257574189803563e-05, 2.1257574189803563e-05, 2.1257574189803563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1257574189803563e-05

Optimization complete. Final v2v error: 3.8851070404052734 mm

Highest mean error: 8.485589981079102 mm for frame 89

Lowest mean error: 3.5989372730255127 mm for frame 77

Saving results

Total time: 870.5004818439484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0019
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930344
Iteration 2/25 | Loss: 0.00105774
Iteration 3/25 | Loss: 0.00085497
Iteration 4/25 | Loss: 0.00078697
Iteration 5/25 | Loss: 0.00076911
Iteration 6/25 | Loss: 0.00075913
Iteration 7/25 | Loss: 0.00075643
Iteration 8/25 | Loss: 0.00075734
Iteration 9/25 | Loss: 0.00075590
Iteration 10/25 | Loss: 0.00075433
Iteration 11/25 | Loss: 0.00075385
Iteration 12/25 | Loss: 0.00075348
Iteration 13/25 | Loss: 0.00075330
Iteration 14/25 | Loss: 0.00075307
Iteration 15/25 | Loss: 0.00075419
Iteration 16/25 | Loss: 0.00075277
Iteration 17/25 | Loss: 0.00075374
Iteration 18/25 | Loss: 0.00075271
Iteration 19/25 | Loss: 0.00075399
Iteration 20/25 | Loss: 0.00075300
Iteration 21/25 | Loss: 0.00075407
Iteration 22/25 | Loss: 0.00075301
Iteration 23/25 | Loss: 0.00075405
Iteration 24/25 | Loss: 0.00075317
Iteration 25/25 | Loss: 0.00075396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.61792469
Iteration 2/25 | Loss: 0.00036816
Iteration 3/25 | Loss: 0.00036811
Iteration 4/25 | Loss: 0.00036811
Iteration 5/25 | Loss: 0.00036811
Iteration 6/25 | Loss: 0.00036811
Iteration 7/25 | Loss: 0.00036811
Iteration 8/25 | Loss: 0.00036811
Iteration 9/25 | Loss: 0.00036811
Iteration 10/25 | Loss: 0.00036811
Iteration 11/25 | Loss: 0.00036811
Iteration 12/25 | Loss: 0.00036811
Iteration 13/25 | Loss: 0.00036811
Iteration 14/25 | Loss: 0.00036811
Iteration 15/25 | Loss: 0.00036811
Iteration 16/25 | Loss: 0.00036811
Iteration 17/25 | Loss: 0.00036811
Iteration 18/25 | Loss: 0.00036811
Iteration 19/25 | Loss: 0.00036811
Iteration 20/25 | Loss: 0.00036811
Iteration 21/25 | Loss: 0.00036811
Iteration 22/25 | Loss: 0.00036811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003681087982840836, 0.0003681087982840836, 0.0003681087982840836, 0.0003681087982840836, 0.0003681087982840836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003681087982840836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036811
Iteration 2/1000 | Loss: 0.00002998
Iteration 3/1000 | Loss: 0.00002076
Iteration 4/1000 | Loss: 0.00001837
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00001624
Iteration 7/1000 | Loss: 0.00001586
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001524
Iteration 14/1000 | Loss: 0.00001524
Iteration 15/1000 | Loss: 0.00001524
Iteration 16/1000 | Loss: 0.00001523
Iteration 17/1000 | Loss: 0.00001523
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001521
Iteration 23/1000 | Loss: 0.00001520
Iteration 24/1000 | Loss: 0.00001520
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001519
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001518
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001621
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001571
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001513
Iteration 52/1000 | Loss: 0.00001513
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001512
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001508
Iteration 83/1000 | Loss: 0.00001508
Iteration 84/1000 | Loss: 0.00001508
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001507
Iteration 91/1000 | Loss: 0.00001507
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001562
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001503
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001709
Iteration 135/1000 | Loss: 0.00001574
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001599
Iteration 140/1000 | Loss: 0.00001711
Iteration 141/1000 | Loss: 0.00001711
Iteration 142/1000 | Loss: 0.00001623
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001623
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001689
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001642
Iteration 149/1000 | Loss: 0.00001613
Iteration 150/1000 | Loss: 0.00001687
Iteration 151/1000 | Loss: 0.00001643
Iteration 152/1000 | Loss: 0.00001681
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001636
Iteration 155/1000 | Loss: 0.00001591
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001646
Iteration 160/1000 | Loss: 0.00001674
Iteration 161/1000 | Loss: 0.00001686
Iteration 162/1000 | Loss: 0.00001667
Iteration 163/1000 | Loss: 0.00001651
Iteration 164/1000 | Loss: 0.00001697
Iteration 165/1000 | Loss: 0.00001663
Iteration 166/1000 | Loss: 0.00001693
Iteration 167/1000 | Loss: 0.00001693
Iteration 168/1000 | Loss: 0.00001638
Iteration 169/1000 | Loss: 0.00001626
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001693
Iteration 172/1000 | Loss: 0.00001668
Iteration 173/1000 | Loss: 0.00001687
Iteration 174/1000 | Loss: 0.00001670
Iteration 175/1000 | Loss: 0.00001696
Iteration 176/1000 | Loss: 0.00001679
Iteration 177/1000 | Loss: 0.00001580
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001705
Iteration 181/1000 | Loss: 0.00001705
Iteration 182/1000 | Loss: 0.00001644
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001574
Iteration 185/1000 | Loss: 0.00001506
Iteration 186/1000 | Loss: 0.00001503
Iteration 187/1000 | Loss: 0.00001501
Iteration 188/1000 | Loss: 0.00001651
Iteration 189/1000 | Loss: 0.00001653
Iteration 190/1000 | Loss: 0.00001499
Iteration 191/1000 | Loss: 0.00001498
Iteration 192/1000 | Loss: 0.00001498
Iteration 193/1000 | Loss: 0.00001498
Iteration 194/1000 | Loss: 0.00001498
Iteration 195/1000 | Loss: 0.00001498
Iteration 196/1000 | Loss: 0.00001498
Iteration 197/1000 | Loss: 0.00001498
Iteration 198/1000 | Loss: 0.00001498
Iteration 199/1000 | Loss: 0.00001498
Iteration 200/1000 | Loss: 0.00001498
Iteration 201/1000 | Loss: 0.00001498
Iteration 202/1000 | Loss: 0.00001498
Iteration 203/1000 | Loss: 0.00001590
Iteration 204/1000 | Loss: 0.00001528
Iteration 205/1000 | Loss: 0.00001604
Iteration 206/1000 | Loss: 0.00001541
Iteration 207/1000 | Loss: 0.00001589
Iteration 208/1000 | Loss: 0.00001616
Iteration 209/1000 | Loss: 0.00001542
Iteration 210/1000 | Loss: 0.00001597
Iteration 211/1000 | Loss: 0.00001655
Iteration 212/1000 | Loss: 0.00001584
Iteration 213/1000 | Loss: 0.00001610
Iteration 214/1000 | Loss: 0.00001690
Iteration 215/1000 | Loss: 0.00001690
Iteration 216/1000 | Loss: 0.00001595
Iteration 217/1000 | Loss: 0.00001595
Iteration 218/1000 | Loss: 0.00001701
Iteration 219/1000 | Loss: 0.00001650
Iteration 220/1000 | Loss: 0.00001680
Iteration 221/1000 | Loss: 0.00001675
Iteration 222/1000 | Loss: 0.00001606
Iteration 223/1000 | Loss: 0.00001569
Iteration 224/1000 | Loss: 0.00001683
Iteration 225/1000 | Loss: 0.00001661
Iteration 226/1000 | Loss: 0.00001526
Iteration 227/1000 | Loss: 0.00001671
Iteration 228/1000 | Loss: 0.00001671
Iteration 229/1000 | Loss: 0.00001567
Iteration 230/1000 | Loss: 0.00001581
Iteration 231/1000 | Loss: 0.00001659
Iteration 232/1000 | Loss: 0.00001632
Iteration 233/1000 | Loss: 0.00001581
Iteration 234/1000 | Loss: 0.00001671
Iteration 235/1000 | Loss: 0.00001601
Iteration 236/1000 | Loss: 0.00001682
Iteration 237/1000 | Loss: 0.00001622
Iteration 238/1000 | Loss: 0.00001581
Iteration 239/1000 | Loss: 0.00001687
Iteration 240/1000 | Loss: 0.00001680
Iteration 241/1000 | Loss: 0.00001623
Iteration 242/1000 | Loss: 0.00001597
Iteration 243/1000 | Loss: 0.00001597
Iteration 244/1000 | Loss: 0.00001716
Iteration 245/1000 | Loss: 0.00001715
Iteration 246/1000 | Loss: 0.00001579
Iteration 247/1000 | Loss: 0.00001671
Iteration 248/1000 | Loss: 0.00001640
Iteration 249/1000 | Loss: 0.00001574
Iteration 250/1000 | Loss: 0.00001539
Iteration 251/1000 | Loss: 0.00001597
Iteration 252/1000 | Loss: 0.00001717
Iteration 253/1000 | Loss: 0.00001699
Iteration 254/1000 | Loss: 0.00001566
Iteration 255/1000 | Loss: 0.00001576
Iteration 256/1000 | Loss: 0.00001701
Iteration 257/1000 | Loss: 0.00001592
Iteration 258/1000 | Loss: 0.00001663
Iteration 259/1000 | Loss: 0.00001662
Iteration 260/1000 | Loss: 0.00001589
Iteration 261/1000 | Loss: 0.00001559
Iteration 262/1000 | Loss: 0.00001690
Iteration 263/1000 | Loss: 0.00001602
Iteration 264/1000 | Loss: 0.00001646
Iteration 265/1000 | Loss: 0.00001654
Iteration 266/1000 | Loss: 0.00001528
Iteration 267/1000 | Loss: 0.00001640
Iteration 268/1000 | Loss: 0.00001632
Iteration 269/1000 | Loss: 0.00001647
Iteration 270/1000 | Loss: 0.00001573
Iteration 271/1000 | Loss: 0.00001507
Iteration 272/1000 | Loss: 0.00001507
Iteration 273/1000 | Loss: 0.00001507
Iteration 274/1000 | Loss: 0.00001507
Iteration 275/1000 | Loss: 0.00001506
Iteration 276/1000 | Loss: 0.00001506
Iteration 277/1000 | Loss: 0.00001506
Iteration 278/1000 | Loss: 0.00001506
Iteration 279/1000 | Loss: 0.00001505
Iteration 280/1000 | Loss: 0.00001505
Iteration 281/1000 | Loss: 0.00001505
Iteration 282/1000 | Loss: 0.00001503
Iteration 283/1000 | Loss: 0.00001503
Iteration 284/1000 | Loss: 0.00001502
Iteration 285/1000 | Loss: 0.00001563
Iteration 286/1000 | Loss: 0.00001553
Iteration 287/1000 | Loss: 0.00001500
Iteration 288/1000 | Loss: 0.00001562
Iteration 289/1000 | Loss: 0.00001558
Iteration 290/1000 | Loss: 0.00001501
Iteration 291/1000 | Loss: 0.00001650
Iteration 292/1000 | Loss: 0.00001604
Iteration 293/1000 | Loss: 0.00001669
Iteration 294/1000 | Loss: 0.00001664
Iteration 295/1000 | Loss: 0.00001674
Iteration 296/1000 | Loss: 0.00001666
Iteration 297/1000 | Loss: 0.00001677
Iteration 298/1000 | Loss: 0.00001676
Iteration 299/1000 | Loss: 0.00001659
Iteration 300/1000 | Loss: 0.00001570
Iteration 301/1000 | Loss: 0.00001529
Iteration 302/1000 | Loss: 0.00001611
Iteration 303/1000 | Loss: 0.00001557
Iteration 304/1000 | Loss: 0.00001500
Iteration 305/1000 | Loss: 0.00001500
Iteration 306/1000 | Loss: 0.00001500
Iteration 307/1000 | Loss: 0.00001500
Iteration 308/1000 | Loss: 0.00001500
Iteration 309/1000 | Loss: 0.00001500
Iteration 310/1000 | Loss: 0.00001500
Iteration 311/1000 | Loss: 0.00001500
Iteration 312/1000 | Loss: 0.00001598
Iteration 313/1000 | Loss: 0.00001584
Iteration 314/1000 | Loss: 0.00001586
Iteration 315/1000 | Loss: 0.00001669
Iteration 316/1000 | Loss: 0.00001589
Iteration 317/1000 | Loss: 0.00001503
Iteration 318/1000 | Loss: 0.00001658
Iteration 319/1000 | Loss: 0.00001595
Iteration 320/1000 | Loss: 0.00001499
Iteration 321/1000 | Loss: 0.00001499
Iteration 322/1000 | Loss: 0.00001657
Iteration 323/1000 | Loss: 0.00001657
Iteration 324/1000 | Loss: 0.00001633
Iteration 325/1000 | Loss: 0.00001512
Iteration 326/1000 | Loss: 0.00001506
Iteration 327/1000 | Loss: 0.00001503
Iteration 328/1000 | Loss: 0.00001561
Iteration 329/1000 | Loss: 0.00001545
Iteration 330/1000 | Loss: 0.00001602
Iteration 331/1000 | Loss: 0.00001658
Iteration 332/1000 | Loss: 0.00001582
Iteration 333/1000 | Loss: 0.00001501
Iteration 334/1000 | Loss: 0.00001559
Iteration 335/1000 | Loss: 0.00001549
Iteration 336/1000 | Loss: 0.00001504
Iteration 337/1000 | Loss: 0.00001503
Iteration 338/1000 | Loss: 0.00001619
Iteration 339/1000 | Loss: 0.00001684
Iteration 340/1000 | Loss: 0.00001607
Iteration 341/1000 | Loss: 0.00001567
Iteration 342/1000 | Loss: 0.00001563
Iteration 343/1000 | Loss: 0.00001501
Iteration 344/1000 | Loss: 0.00001500
Iteration 345/1000 | Loss: 0.00001500
Iteration 346/1000 | Loss: 0.00001500
Iteration 347/1000 | Loss: 0.00001499
Iteration 348/1000 | Loss: 0.00001499
Iteration 349/1000 | Loss: 0.00001499
Iteration 350/1000 | Loss: 0.00001565
Iteration 351/1000 | Loss: 0.00001565
Iteration 352/1000 | Loss: 0.00001561
Iteration 353/1000 | Loss: 0.00001502
Iteration 354/1000 | Loss: 0.00001501
Iteration 355/1000 | Loss: 0.00001501
Iteration 356/1000 | Loss: 0.00001565
Iteration 357/1000 | Loss: 0.00001565
Iteration 358/1000 | Loss: 0.00001560
Iteration 359/1000 | Loss: 0.00001561
Iteration 360/1000 | Loss: 0.00001562
Iteration 361/1000 | Loss: 0.00001500
Iteration 362/1000 | Loss: 0.00001500
Iteration 363/1000 | Loss: 0.00001500
Iteration 364/1000 | Loss: 0.00001560
Iteration 365/1000 | Loss: 0.00001560
Iteration 366/1000 | Loss: 0.00001556
Iteration 367/1000 | Loss: 0.00001502
Iteration 368/1000 | Loss: 0.00001502
Iteration 369/1000 | Loss: 0.00001501
Iteration 370/1000 | Loss: 0.00001501
Iteration 371/1000 | Loss: 0.00001501
Iteration 372/1000 | Loss: 0.00001501
Iteration 373/1000 | Loss: 0.00001501
Iteration 374/1000 | Loss: 0.00001501
Iteration 375/1000 | Loss: 0.00001500
Iteration 376/1000 | Loss: 0.00001500
Iteration 377/1000 | Loss: 0.00001500
Iteration 378/1000 | Loss: 0.00001500
Iteration 379/1000 | Loss: 0.00001500
Iteration 380/1000 | Loss: 0.00001500
Iteration 381/1000 | Loss: 0.00001499
Iteration 382/1000 | Loss: 0.00001499
Iteration 383/1000 | Loss: 0.00001499
Iteration 384/1000 | Loss: 0.00001499
Iteration 385/1000 | Loss: 0.00001499
Iteration 386/1000 | Loss: 0.00001499
Iteration 387/1000 | Loss: 0.00001499
Iteration 388/1000 | Loss: 0.00001499
Iteration 389/1000 | Loss: 0.00001499
Iteration 390/1000 | Loss: 0.00001499
Iteration 391/1000 | Loss: 0.00001499
Iteration 392/1000 | Loss: 0.00001499
Iteration 393/1000 | Loss: 0.00001499
Iteration 394/1000 | Loss: 0.00001499
Iteration 395/1000 | Loss: 0.00001499
Iteration 396/1000 | Loss: 0.00001499
Iteration 397/1000 | Loss: 0.00001499
Iteration 398/1000 | Loss: 0.00001499
Iteration 399/1000 | Loss: 0.00001499
Iteration 400/1000 | Loss: 0.00001499
Iteration 401/1000 | Loss: 0.00001499
Iteration 402/1000 | Loss: 0.00001499
Iteration 403/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 403. Stopping optimization.
Last 5 losses: [1.4991722309787292e-05, 1.4991722309787292e-05, 1.4991722309787292e-05, 1.4991722309787292e-05, 1.4991722309787292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4991722309787292e-05

Optimization complete. Final v2v error: 3.172375202178955 mm

Highest mean error: 9.478731155395508 mm for frame 75

Lowest mean error: 2.7551181316375732 mm for frame 107

Saving results

Total time: 2614.0134398937225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0009
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490510
Iteration 2/25 | Loss: 0.00104243
Iteration 3/25 | Loss: 0.00091992
Iteration 4/25 | Loss: 0.00089592
Iteration 5/25 | Loss: 0.00088623
Iteration 6/25 | Loss: 0.00088390
Iteration 7/25 | Loss: 0.00088350
Iteration 8/25 | Loss: 0.00088350
Iteration 9/25 | Loss: 0.00088350
Iteration 10/25 | Loss: 0.00088350
Iteration 11/25 | Loss: 0.00088350
Iteration 12/25 | Loss: 0.00088350
Iteration 13/25 | Loss: 0.00088350
Iteration 14/25 | Loss: 0.00088350
Iteration 15/25 | Loss: 0.00088350
Iteration 16/25 | Loss: 0.00088350
Iteration 17/25 | Loss: 0.00088350
Iteration 18/25 | Loss: 0.00088350
Iteration 19/25 | Loss: 0.00088350
Iteration 20/25 | Loss: 0.00088350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008834985783323646, 0.0008834985783323646, 0.0008834985783323646, 0.0008834985783323646, 0.0008834985783323646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008834985783323646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13637424
Iteration 2/25 | Loss: 0.00034757
Iteration 3/25 | Loss: 0.00034756
Iteration 4/25 | Loss: 0.00034756
Iteration 5/25 | Loss: 0.00034756
Iteration 6/25 | Loss: 0.00034756
Iteration 7/25 | Loss: 0.00034756
Iteration 8/25 | Loss: 0.00034756
Iteration 9/25 | Loss: 0.00034756
Iteration 10/25 | Loss: 0.00034756
Iteration 11/25 | Loss: 0.00034756
Iteration 12/25 | Loss: 0.00034756
Iteration 13/25 | Loss: 0.00034756
Iteration 14/25 | Loss: 0.00034756
Iteration 15/25 | Loss: 0.00034756
Iteration 16/25 | Loss: 0.00034756
Iteration 17/25 | Loss: 0.00034756
Iteration 18/25 | Loss: 0.00034756
Iteration 19/25 | Loss: 0.00034756
Iteration 20/25 | Loss: 0.00034756
Iteration 21/25 | Loss: 0.00034756
Iteration 22/25 | Loss: 0.00034756
Iteration 23/25 | Loss: 0.00034756
Iteration 24/25 | Loss: 0.00034756
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003475593985058367, 0.0003475593985058367, 0.0003475593985058367, 0.0003475593985058367, 0.0003475593985058367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003475593985058367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034756
Iteration 2/1000 | Loss: 0.00005969
Iteration 3/1000 | Loss: 0.00003930
Iteration 4/1000 | Loss: 0.00003508
Iteration 5/1000 | Loss: 0.00003242
Iteration 6/1000 | Loss: 0.00003151
Iteration 7/1000 | Loss: 0.00003078
Iteration 8/1000 | Loss: 0.00003038
Iteration 9/1000 | Loss: 0.00002997
Iteration 10/1000 | Loss: 0.00002973
Iteration 11/1000 | Loss: 0.00002965
Iteration 12/1000 | Loss: 0.00002964
Iteration 13/1000 | Loss: 0.00002963
Iteration 14/1000 | Loss: 0.00002962
Iteration 15/1000 | Loss: 0.00002962
Iteration 16/1000 | Loss: 0.00002960
Iteration 17/1000 | Loss: 0.00002960
Iteration 18/1000 | Loss: 0.00002959
Iteration 19/1000 | Loss: 0.00002959
Iteration 20/1000 | Loss: 0.00002959
Iteration 21/1000 | Loss: 0.00002958
Iteration 22/1000 | Loss: 0.00002958
Iteration 23/1000 | Loss: 0.00002958
Iteration 24/1000 | Loss: 0.00002957
Iteration 25/1000 | Loss: 0.00002957
Iteration 26/1000 | Loss: 0.00002957
Iteration 27/1000 | Loss: 0.00002957
Iteration 28/1000 | Loss: 0.00002957
Iteration 29/1000 | Loss: 0.00002957
Iteration 30/1000 | Loss: 0.00002957
Iteration 31/1000 | Loss: 0.00002957
Iteration 32/1000 | Loss: 0.00002956
Iteration 33/1000 | Loss: 0.00002956
Iteration 34/1000 | Loss: 0.00002956
Iteration 35/1000 | Loss: 0.00002955
Iteration 36/1000 | Loss: 0.00002955
Iteration 37/1000 | Loss: 0.00002955
Iteration 38/1000 | Loss: 0.00002955
Iteration 39/1000 | Loss: 0.00002955
Iteration 40/1000 | Loss: 0.00002955
Iteration 41/1000 | Loss: 0.00002955
Iteration 42/1000 | Loss: 0.00002955
Iteration 43/1000 | Loss: 0.00002955
Iteration 44/1000 | Loss: 0.00002955
Iteration 45/1000 | Loss: 0.00002955
Iteration 46/1000 | Loss: 0.00002955
Iteration 47/1000 | Loss: 0.00002955
Iteration 48/1000 | Loss: 0.00002955
Iteration 49/1000 | Loss: 0.00002955
Iteration 50/1000 | Loss: 0.00002955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [2.9553897547884844e-05, 2.9553897547884844e-05, 2.9553897547884844e-05, 2.9553897547884844e-05, 2.9553897547884844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9553897547884844e-05

Optimization complete. Final v2v error: 4.587010383605957 mm

Highest mean error: 5.055815696716309 mm for frame 184

Lowest mean error: 4.12504768371582 mm for frame 137

Saving results

Total time: 318.54830026626587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1042
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089856
Iteration 2/25 | Loss: 0.00138878
Iteration 3/25 | Loss: 0.00100081
Iteration 4/25 | Loss: 0.00096516
Iteration 5/25 | Loss: 0.00094535
Iteration 6/25 | Loss: 0.00093457
Iteration 7/25 | Loss: 0.00093106
Iteration 8/25 | Loss: 0.00093068
Iteration 9/25 | Loss: 0.00093390
Iteration 10/25 | Loss: 0.00093045
Iteration 11/25 | Loss: 0.00093042
Iteration 12/25 | Loss: 0.00093042
Iteration 13/25 | Loss: 0.00093042
Iteration 14/25 | Loss: 0.00093042
Iteration 15/25 | Loss: 0.00093041
Iteration 16/25 | Loss: 0.00093041
Iteration 17/25 | Loss: 0.00093041
Iteration 18/25 | Loss: 0.00093041
Iteration 19/25 | Loss: 0.00093041
Iteration 20/25 | Loss: 0.00093041
Iteration 21/25 | Loss: 0.00093041
Iteration 22/25 | Loss: 0.00093041
Iteration 23/25 | Loss: 0.00093041
Iteration 24/25 | Loss: 0.00093041
Iteration 25/25 | Loss: 0.00093041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53612137
Iteration 2/25 | Loss: 0.00051076
Iteration 3/25 | Loss: 0.00046891
Iteration 4/25 | Loss: 0.00046891
Iteration 5/25 | Loss: 0.00046891
Iteration 6/25 | Loss: 0.00046891
Iteration 7/25 | Loss: 0.00046890
Iteration 8/25 | Loss: 0.00046890
Iteration 9/25 | Loss: 0.00046890
Iteration 10/25 | Loss: 0.00046890
Iteration 11/25 | Loss: 0.00046890
Iteration 12/25 | Loss: 0.00046890
Iteration 13/25 | Loss: 0.00046890
Iteration 14/25 | Loss: 0.00046890
Iteration 15/25 | Loss: 0.00046890
Iteration 16/25 | Loss: 0.00046890
Iteration 17/25 | Loss: 0.00046890
Iteration 18/25 | Loss: 0.00046890
Iteration 19/25 | Loss: 0.00046890
Iteration 20/25 | Loss: 0.00046890
Iteration 21/25 | Loss: 0.00046890
Iteration 22/25 | Loss: 0.00046890
Iteration 23/25 | Loss: 0.00046890
Iteration 24/25 | Loss: 0.00046890
Iteration 25/25 | Loss: 0.00046890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046890
Iteration 2/1000 | Loss: 0.00007697
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002212
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00001894
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00001797
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001715
Iteration 13/1000 | Loss: 0.00001708
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001706
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001696
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001684
Iteration 26/1000 | Loss: 0.00001681
Iteration 27/1000 | Loss: 0.00001681
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001679
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001679
Iteration 37/1000 | Loss: 0.00001679
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001678
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001677
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001676
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001676
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001674
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001674
Iteration 101/1000 | Loss: 0.00001674
Iteration 102/1000 | Loss: 0.00001674
Iteration 103/1000 | Loss: 0.00001674
Iteration 104/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.6737809346523136e-05, 1.6737809346523136e-05, 1.6737809346523136e-05, 1.6737809346523136e-05, 1.6737809346523136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6737809346523136e-05

Optimization complete. Final v2v error: 3.372922897338867 mm

Highest mean error: 3.659635066986084 mm for frame 12

Lowest mean error: 3.0907678604125977 mm for frame 52

Saving results

Total time: 298.5694863796234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1022
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426880
Iteration 2/25 | Loss: 0.00100929
Iteration 3/25 | Loss: 0.00087267
Iteration 4/25 | Loss: 0.00085665
Iteration 5/25 | Loss: 0.00085230
Iteration 6/25 | Loss: 0.00085117
Iteration 7/25 | Loss: 0.00085079
Iteration 8/25 | Loss: 0.00085079
Iteration 9/25 | Loss: 0.00085079
Iteration 10/25 | Loss: 0.00085079
Iteration 11/25 | Loss: 0.00085079
Iteration 12/25 | Loss: 0.00085079
Iteration 13/25 | Loss: 0.00085079
Iteration 14/25 | Loss: 0.00085079
Iteration 15/25 | Loss: 0.00085079
Iteration 16/25 | Loss: 0.00085079
Iteration 17/25 | Loss: 0.00085079
Iteration 18/25 | Loss: 0.00085079
Iteration 19/25 | Loss: 0.00085079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008507910533808172, 0.0008507910533808172, 0.0008507910533808172, 0.0008507910533808172, 0.0008507910533808172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008507910533808172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50997734
Iteration 2/25 | Loss: 0.00041956
Iteration 3/25 | Loss: 0.00041956
Iteration 4/25 | Loss: 0.00041956
Iteration 5/25 | Loss: 0.00041956
Iteration 6/25 | Loss: 0.00041956
Iteration 7/25 | Loss: 0.00041956
Iteration 8/25 | Loss: 0.00041956
Iteration 9/25 | Loss: 0.00041956
Iteration 10/25 | Loss: 0.00041956
Iteration 11/25 | Loss: 0.00041956
Iteration 12/25 | Loss: 0.00041956
Iteration 13/25 | Loss: 0.00041956
Iteration 14/25 | Loss: 0.00041956
Iteration 15/25 | Loss: 0.00041956
Iteration 16/25 | Loss: 0.00041956
Iteration 17/25 | Loss: 0.00041956
Iteration 18/25 | Loss: 0.00041956
Iteration 19/25 | Loss: 0.00041956
Iteration 20/25 | Loss: 0.00041956
Iteration 21/25 | Loss: 0.00041956
Iteration 22/25 | Loss: 0.00041956
Iteration 23/25 | Loss: 0.00041956
Iteration 24/25 | Loss: 0.00041956
Iteration 25/25 | Loss: 0.00041956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041956
Iteration 2/1000 | Loss: 0.00002381
Iteration 3/1000 | Loss: 0.00001373
Iteration 4/1000 | Loss: 0.00001198
Iteration 5/1000 | Loss: 0.00001126
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001071
Iteration 8/1000 | Loss: 0.00001054
Iteration 9/1000 | Loss: 0.00001040
Iteration 10/1000 | Loss: 0.00001035
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001031
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001031
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001029
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001026
Iteration 21/1000 | Loss: 0.00001025
Iteration 22/1000 | Loss: 0.00001025
Iteration 23/1000 | Loss: 0.00001025
Iteration 24/1000 | Loss: 0.00001024
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001023
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001019
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001017
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001017
Iteration 35/1000 | Loss: 0.00001017
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001015
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001015
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001014
Iteration 49/1000 | Loss: 0.00001013
Iteration 50/1000 | Loss: 0.00001013
Iteration 51/1000 | Loss: 0.00001013
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001012
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001011
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001011
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001010
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001009
Iteration 65/1000 | Loss: 0.00001009
Iteration 66/1000 | Loss: 0.00001009
Iteration 67/1000 | Loss: 0.00001009
Iteration 68/1000 | Loss: 0.00001009
Iteration 69/1000 | Loss: 0.00001009
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001009
Iteration 76/1000 | Loss: 0.00001008
Iteration 77/1000 | Loss: 0.00001008
Iteration 78/1000 | Loss: 0.00001008
Iteration 79/1000 | Loss: 0.00001008
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001008
Iteration 83/1000 | Loss: 0.00001008
Iteration 84/1000 | Loss: 0.00001008
Iteration 85/1000 | Loss: 0.00001008
Iteration 86/1000 | Loss: 0.00001008
Iteration 87/1000 | Loss: 0.00001008
Iteration 88/1000 | Loss: 0.00001008
Iteration 89/1000 | Loss: 0.00001008
Iteration 90/1000 | Loss: 0.00001008
Iteration 91/1000 | Loss: 0.00001007
Iteration 92/1000 | Loss: 0.00001007
Iteration 93/1000 | Loss: 0.00001007
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001007
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001007
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001006
Iteration 101/1000 | Loss: 0.00001006
Iteration 102/1000 | Loss: 0.00001006
Iteration 103/1000 | Loss: 0.00001005
Iteration 104/1000 | Loss: 0.00001005
Iteration 105/1000 | Loss: 0.00001005
Iteration 106/1000 | Loss: 0.00001005
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001005
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001004
Iteration 112/1000 | Loss: 0.00001004
Iteration 113/1000 | Loss: 0.00001004
Iteration 114/1000 | Loss: 0.00001004
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001003
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001001
Iteration 129/1000 | Loss: 0.00001001
Iteration 130/1000 | Loss: 0.00001001
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Iteration 143/1000 | Loss: 0.00001000
Iteration 144/1000 | Loss: 0.00001000
Iteration 145/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0002717317547649e-05, 1.0002717317547649e-05, 1.0002717317547649e-05, 1.0002717317547649e-05, 1.0002717317547649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0002717317547649e-05

Optimization complete. Final v2v error: 2.6331753730773926 mm

Highest mean error: 3.5430843830108643 mm for frame 51

Lowest mean error: 2.1788382530212402 mm for frame 97

Saving results

Total time: 175.25884318351746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015074
Iteration 2/25 | Loss: 0.00149635
Iteration 3/25 | Loss: 0.00112746
Iteration 4/25 | Loss: 0.00107869
Iteration 5/25 | Loss: 0.00107824
Iteration 6/25 | Loss: 0.00105983
Iteration 7/25 | Loss: 0.00106043
Iteration 8/25 | Loss: 0.00102972
Iteration 9/25 | Loss: 0.00103421
Iteration 10/25 | Loss: 0.00101924
Iteration 11/25 | Loss: 0.00101000
Iteration 12/25 | Loss: 0.00099060
Iteration 13/25 | Loss: 0.00098734
Iteration 14/25 | Loss: 0.00098332
Iteration 15/25 | Loss: 0.00097960
Iteration 16/25 | Loss: 0.00097506
Iteration 17/25 | Loss: 0.00097393
Iteration 18/25 | Loss: 0.00097192
Iteration 19/25 | Loss: 0.00097071
Iteration 20/25 | Loss: 0.00097014
Iteration 21/25 | Loss: 0.00096987
Iteration 22/25 | Loss: 0.00096978
Iteration 23/25 | Loss: 0.00096975
Iteration 24/25 | Loss: 0.00096975
Iteration 25/25 | Loss: 0.00096975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80185986
Iteration 2/25 | Loss: 0.00134115
Iteration 3/25 | Loss: 0.00134114
Iteration 4/25 | Loss: 0.00134114
Iteration 5/25 | Loss: 0.00134114
Iteration 6/25 | Loss: 0.00134114
Iteration 7/25 | Loss: 0.00134114
Iteration 8/25 | Loss: 0.00134114
Iteration 9/25 | Loss: 0.00134114
Iteration 10/25 | Loss: 0.00134114
Iteration 11/25 | Loss: 0.00134114
Iteration 12/25 | Loss: 0.00134114
Iteration 13/25 | Loss: 0.00134114
Iteration 14/25 | Loss: 0.00134114
Iteration 15/25 | Loss: 0.00134114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013411412946879864, 0.0013411412946879864, 0.0013411412946879864, 0.0013411412946879864, 0.0013411412946879864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013411412946879864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134114
Iteration 2/1000 | Loss: 0.00017541
Iteration 3/1000 | Loss: 0.00012061
Iteration 4/1000 | Loss: 0.00031265
Iteration 5/1000 | Loss: 0.00023146
Iteration 6/1000 | Loss: 0.00049953
Iteration 7/1000 | Loss: 0.00009567
Iteration 8/1000 | Loss: 0.00008105
Iteration 9/1000 | Loss: 0.00050315
Iteration 10/1000 | Loss: 0.00112575
Iteration 11/1000 | Loss: 0.00100632
Iteration 12/1000 | Loss: 0.00042630
Iteration 13/1000 | Loss: 0.00017251
Iteration 14/1000 | Loss: 0.00035072
Iteration 15/1000 | Loss: 0.00006797
Iteration 16/1000 | Loss: 0.00006288
Iteration 17/1000 | Loss: 0.00077102
Iteration 18/1000 | Loss: 0.00043159
Iteration 19/1000 | Loss: 0.00025633
Iteration 20/1000 | Loss: 0.00006808
Iteration 21/1000 | Loss: 0.00005587
Iteration 22/1000 | Loss: 0.00035699
Iteration 23/1000 | Loss: 0.00009932
Iteration 24/1000 | Loss: 0.00028294
Iteration 25/1000 | Loss: 0.00035776
Iteration 26/1000 | Loss: 0.00056272
Iteration 27/1000 | Loss: 0.00036169
Iteration 28/1000 | Loss: 0.00041024
Iteration 29/1000 | Loss: 0.00006390
Iteration 30/1000 | Loss: 0.00014020
Iteration 31/1000 | Loss: 0.00039319
Iteration 32/1000 | Loss: 0.00006408
Iteration 33/1000 | Loss: 0.00005365
Iteration 34/1000 | Loss: 0.00015569
Iteration 35/1000 | Loss: 0.00004468
Iteration 36/1000 | Loss: 0.00004200
Iteration 37/1000 | Loss: 0.00003999
Iteration 38/1000 | Loss: 0.00003822
Iteration 39/1000 | Loss: 0.00003736
Iteration 40/1000 | Loss: 0.00003664
Iteration 41/1000 | Loss: 0.00003619
Iteration 42/1000 | Loss: 0.00003590
Iteration 43/1000 | Loss: 0.00183602
Iteration 44/1000 | Loss: 0.00211298
Iteration 45/1000 | Loss: 0.00008735
Iteration 46/1000 | Loss: 0.00152411
Iteration 47/1000 | Loss: 0.00026999
Iteration 48/1000 | Loss: 0.00035132
Iteration 49/1000 | Loss: 0.00015966
Iteration 50/1000 | Loss: 0.00003965
Iteration 51/1000 | Loss: 0.00003525
Iteration 52/1000 | Loss: 0.00003203
Iteration 53/1000 | Loss: 0.00002905
Iteration 54/1000 | Loss: 0.00085391
Iteration 55/1000 | Loss: 0.00081034
Iteration 56/1000 | Loss: 0.00008824
Iteration 57/1000 | Loss: 0.00002881
Iteration 58/1000 | Loss: 0.00107424
Iteration 59/1000 | Loss: 0.00004938
Iteration 60/1000 | Loss: 0.00002983
Iteration 61/1000 | Loss: 0.00002615
Iteration 62/1000 | Loss: 0.00002379
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00001945
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001775
Iteration 68/1000 | Loss: 0.00001738
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001701
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001696
Iteration 85/1000 | Loss: 0.00001696
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001689
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001688
Iteration 137/1000 | Loss: 0.00001688
Iteration 138/1000 | Loss: 0.00001688
Iteration 139/1000 | Loss: 0.00001688
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001687
Iteration 147/1000 | Loss: 0.00001687
Iteration 148/1000 | Loss: 0.00001687
Iteration 149/1000 | Loss: 0.00001687
Iteration 150/1000 | Loss: 0.00001687
Iteration 151/1000 | Loss: 0.00001687
Iteration 152/1000 | Loss: 0.00001687
Iteration 153/1000 | Loss: 0.00001687
Iteration 154/1000 | Loss: 0.00001687
Iteration 155/1000 | Loss: 0.00001686
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001686
Iteration 161/1000 | Loss: 0.00001686
Iteration 162/1000 | Loss: 0.00001686
Iteration 163/1000 | Loss: 0.00001686
Iteration 164/1000 | Loss: 0.00001685
Iteration 165/1000 | Loss: 0.00001685
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001685
Iteration 168/1000 | Loss: 0.00001685
Iteration 169/1000 | Loss: 0.00001685
Iteration 170/1000 | Loss: 0.00001685
Iteration 171/1000 | Loss: 0.00001685
Iteration 172/1000 | Loss: 0.00001685
Iteration 173/1000 | Loss: 0.00001685
Iteration 174/1000 | Loss: 0.00001685
Iteration 175/1000 | Loss: 0.00001684
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001683
Iteration 181/1000 | Loss: 0.00001683
Iteration 182/1000 | Loss: 0.00001683
Iteration 183/1000 | Loss: 0.00001683
Iteration 184/1000 | Loss: 0.00001682
Iteration 185/1000 | Loss: 0.00001682
Iteration 186/1000 | Loss: 0.00001682
Iteration 187/1000 | Loss: 0.00001682
Iteration 188/1000 | Loss: 0.00001682
Iteration 189/1000 | Loss: 0.00001682
Iteration 190/1000 | Loss: 0.00001682
Iteration 191/1000 | Loss: 0.00001682
Iteration 192/1000 | Loss: 0.00001682
Iteration 193/1000 | Loss: 0.00001682
Iteration 194/1000 | Loss: 0.00001682
Iteration 195/1000 | Loss: 0.00001682
Iteration 196/1000 | Loss: 0.00001682
Iteration 197/1000 | Loss: 0.00001681
Iteration 198/1000 | Loss: 0.00001681
Iteration 199/1000 | Loss: 0.00001681
Iteration 200/1000 | Loss: 0.00001681
Iteration 201/1000 | Loss: 0.00001681
Iteration 202/1000 | Loss: 0.00001681
Iteration 203/1000 | Loss: 0.00001681
Iteration 204/1000 | Loss: 0.00001681
Iteration 205/1000 | Loss: 0.00001681
Iteration 206/1000 | Loss: 0.00001681
Iteration 207/1000 | Loss: 0.00001681
Iteration 208/1000 | Loss: 0.00001681
Iteration 209/1000 | Loss: 0.00001681
Iteration 210/1000 | Loss: 0.00001681
Iteration 211/1000 | Loss: 0.00001681
Iteration 212/1000 | Loss: 0.00001681
Iteration 213/1000 | Loss: 0.00001681
Iteration 214/1000 | Loss: 0.00001681
Iteration 215/1000 | Loss: 0.00001681
Iteration 216/1000 | Loss: 0.00001681
Iteration 217/1000 | Loss: 0.00001681
Iteration 218/1000 | Loss: 0.00001681
Iteration 219/1000 | Loss: 0.00001681
Iteration 220/1000 | Loss: 0.00001681
Iteration 221/1000 | Loss: 0.00001681
Iteration 222/1000 | Loss: 0.00001681
Iteration 223/1000 | Loss: 0.00001681
Iteration 224/1000 | Loss: 0.00001681
Iteration 225/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.680703462625388e-05, 1.680703462625388e-05, 1.680703462625388e-05, 1.680703462625388e-05, 1.680703462625388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680703462625388e-05

Optimization complete. Final v2v error: 3.4034552574157715 mm

Highest mean error: 5.250683307647705 mm for frame 29

Lowest mean error: 2.7414591312408447 mm for frame 14

Saving results

Total time: 1639.6355140209198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1014
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997996
Iteration 2/25 | Loss: 0.00138683
Iteration 3/25 | Loss: 0.00101685
Iteration 4/25 | Loss: 0.00093341
Iteration 5/25 | Loss: 0.00091568
Iteration 6/25 | Loss: 0.00090872
Iteration 7/25 | Loss: 0.00090606
Iteration 8/25 | Loss: 0.00089857
Iteration 9/25 | Loss: 0.00089903
Iteration 10/25 | Loss: 0.00089831
Iteration 11/25 | Loss: 0.00089692
Iteration 12/25 | Loss: 0.00090039
Iteration 13/25 | Loss: 0.00089788
Iteration 14/25 | Loss: 0.00090088
Iteration 15/25 | Loss: 0.00089756
Iteration 16/25 | Loss: 0.00089578
Iteration 17/25 | Loss: 0.00089324
Iteration 18/25 | Loss: 0.00089290
Iteration 19/25 | Loss: 0.00089098
Iteration 20/25 | Loss: 0.00089002
Iteration 21/25 | Loss: 0.00088965
Iteration 22/25 | Loss: 0.00088956
Iteration 23/25 | Loss: 0.00088956
Iteration 24/25 | Loss: 0.00088956
Iteration 25/25 | Loss: 0.00088956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49045146
Iteration 2/25 | Loss: 0.00068509
Iteration 3/25 | Loss: 0.00068508
Iteration 4/25 | Loss: 0.00068508
Iteration 5/25 | Loss: 0.00068508
Iteration 6/25 | Loss: 0.00068508
Iteration 7/25 | Loss: 0.00068508
Iteration 8/25 | Loss: 0.00068508
Iteration 9/25 | Loss: 0.00068508
Iteration 10/25 | Loss: 0.00068508
Iteration 11/25 | Loss: 0.00068508
Iteration 12/25 | Loss: 0.00068508
Iteration 13/25 | Loss: 0.00068508
Iteration 14/25 | Loss: 0.00068508
Iteration 15/25 | Loss: 0.00068508
Iteration 16/25 | Loss: 0.00068508
Iteration 17/25 | Loss: 0.00068508
Iteration 18/25 | Loss: 0.00068508
Iteration 19/25 | Loss: 0.00068508
Iteration 20/25 | Loss: 0.00068508
Iteration 21/25 | Loss: 0.00068508
Iteration 22/25 | Loss: 0.00068508
Iteration 23/25 | Loss: 0.00068508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006850762292742729, 0.0006850762292742729, 0.0006850762292742729, 0.0006850762292742729, 0.0006850762292742729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006850762292742729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068508
Iteration 2/1000 | Loss: 0.00002313
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001306
Iteration 5/1000 | Loss: 0.00001248
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001137
Iteration 10/1000 | Loss: 0.00001122
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001108
Iteration 13/1000 | Loss: 0.00001107
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001097
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001097
Iteration 48/1000 | Loss: 0.00001097
Iteration 49/1000 | Loss: 0.00001097
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [1.096513824450085e-05, 1.096513824450085e-05, 1.096513824450085e-05, 1.096513824450085e-05, 1.096513824450085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.096513824450085e-05

Optimization complete. Final v2v error: 2.791778326034546 mm

Highest mean error: 3.3617403507232666 mm for frame 175

Lowest mean error: 2.4449002742767334 mm for frame 26

Saving results

Total time: 934.684182882309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1084
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443546
Iteration 2/25 | Loss: 0.00098463
Iteration 3/25 | Loss: 0.00085552
Iteration 4/25 | Loss: 0.00084568
Iteration 5/25 | Loss: 0.00084302
Iteration 6/25 | Loss: 0.00084255
Iteration 7/25 | Loss: 0.00084255
Iteration 8/25 | Loss: 0.00084255
Iteration 9/25 | Loss: 0.00084255
Iteration 10/25 | Loss: 0.00084255
Iteration 11/25 | Loss: 0.00084255
Iteration 12/25 | Loss: 0.00084255
Iteration 13/25 | Loss: 0.00084255
Iteration 14/25 | Loss: 0.00084255
Iteration 15/25 | Loss: 0.00084255
Iteration 16/25 | Loss: 0.00084255
Iteration 17/25 | Loss: 0.00084255
Iteration 18/25 | Loss: 0.00084255
Iteration 19/25 | Loss: 0.00084255
Iteration 20/25 | Loss: 0.00084255
Iteration 21/25 | Loss: 0.00084255
Iteration 22/25 | Loss: 0.00084255
Iteration 23/25 | Loss: 0.00084255
Iteration 24/25 | Loss: 0.00084255
Iteration 25/25 | Loss: 0.00084255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30837584
Iteration 2/25 | Loss: 0.00047162
Iteration 3/25 | Loss: 0.00047162
Iteration 4/25 | Loss: 0.00047162
Iteration 5/25 | Loss: 0.00047161
Iteration 6/25 | Loss: 0.00047161
Iteration 7/25 | Loss: 0.00047161
Iteration 8/25 | Loss: 0.00047161
Iteration 9/25 | Loss: 0.00047161
Iteration 10/25 | Loss: 0.00047161
Iteration 11/25 | Loss: 0.00047161
Iteration 12/25 | Loss: 0.00047161
Iteration 13/25 | Loss: 0.00047161
Iteration 14/25 | Loss: 0.00047161
Iteration 15/25 | Loss: 0.00047161
Iteration 16/25 | Loss: 0.00047161
Iteration 17/25 | Loss: 0.00047161
Iteration 18/25 | Loss: 0.00047161
Iteration 19/25 | Loss: 0.00047161
Iteration 20/25 | Loss: 0.00047161
Iteration 21/25 | Loss: 0.00047161
Iteration 22/25 | Loss: 0.00047161
Iteration 23/25 | Loss: 0.00047161
Iteration 24/25 | Loss: 0.00047161
Iteration 25/25 | Loss: 0.00047161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047161
Iteration 2/1000 | Loss: 0.00001434
Iteration 3/1000 | Loss: 0.00001044
Iteration 4/1000 | Loss: 0.00000974
Iteration 5/1000 | Loss: 0.00000947
Iteration 6/1000 | Loss: 0.00000921
Iteration 7/1000 | Loss: 0.00000921
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000904
Iteration 10/1000 | Loss: 0.00000897
Iteration 11/1000 | Loss: 0.00000888
Iteration 12/1000 | Loss: 0.00000886
Iteration 13/1000 | Loss: 0.00000885
Iteration 14/1000 | Loss: 0.00000885
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000884
Iteration 17/1000 | Loss: 0.00000884
Iteration 18/1000 | Loss: 0.00000882
Iteration 19/1000 | Loss: 0.00000882
Iteration 20/1000 | Loss: 0.00000881
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000876
Iteration 23/1000 | Loss: 0.00000875
Iteration 24/1000 | Loss: 0.00000875
Iteration 25/1000 | Loss: 0.00000874
Iteration 26/1000 | Loss: 0.00000874
Iteration 27/1000 | Loss: 0.00000873
Iteration 28/1000 | Loss: 0.00000873
Iteration 29/1000 | Loss: 0.00000872
Iteration 30/1000 | Loss: 0.00000872
Iteration 31/1000 | Loss: 0.00000872
Iteration 32/1000 | Loss: 0.00000872
Iteration 33/1000 | Loss: 0.00000872
Iteration 34/1000 | Loss: 0.00000871
Iteration 35/1000 | Loss: 0.00000871
Iteration 36/1000 | Loss: 0.00000871
Iteration 37/1000 | Loss: 0.00000871
Iteration 38/1000 | Loss: 0.00000870
Iteration 39/1000 | Loss: 0.00000870
Iteration 40/1000 | Loss: 0.00000869
Iteration 41/1000 | Loss: 0.00000869
Iteration 42/1000 | Loss: 0.00000869
Iteration 43/1000 | Loss: 0.00000868
Iteration 44/1000 | Loss: 0.00000868
Iteration 45/1000 | Loss: 0.00000868
Iteration 46/1000 | Loss: 0.00000868
Iteration 47/1000 | Loss: 0.00000867
Iteration 48/1000 | Loss: 0.00000867
Iteration 49/1000 | Loss: 0.00000867
Iteration 50/1000 | Loss: 0.00000867
Iteration 51/1000 | Loss: 0.00000867
Iteration 52/1000 | Loss: 0.00000867
Iteration 53/1000 | Loss: 0.00000866
Iteration 54/1000 | Loss: 0.00000866
Iteration 55/1000 | Loss: 0.00000866
Iteration 56/1000 | Loss: 0.00000866
Iteration 57/1000 | Loss: 0.00000866
Iteration 58/1000 | Loss: 0.00000866
Iteration 59/1000 | Loss: 0.00000865
Iteration 60/1000 | Loss: 0.00000865
Iteration 61/1000 | Loss: 0.00000865
Iteration 62/1000 | Loss: 0.00000865
Iteration 63/1000 | Loss: 0.00000865
Iteration 64/1000 | Loss: 0.00000865
Iteration 65/1000 | Loss: 0.00000865
Iteration 66/1000 | Loss: 0.00000865
Iteration 67/1000 | Loss: 0.00000865
Iteration 68/1000 | Loss: 0.00000864
Iteration 69/1000 | Loss: 0.00000864
Iteration 70/1000 | Loss: 0.00000864
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000863
Iteration 74/1000 | Loss: 0.00000863
Iteration 75/1000 | Loss: 0.00000863
Iteration 76/1000 | Loss: 0.00000863
Iteration 77/1000 | Loss: 0.00000863
Iteration 78/1000 | Loss: 0.00000862
Iteration 79/1000 | Loss: 0.00000862
Iteration 80/1000 | Loss: 0.00000862
Iteration 81/1000 | Loss: 0.00000862
Iteration 82/1000 | Loss: 0.00000862
Iteration 83/1000 | Loss: 0.00000862
Iteration 84/1000 | Loss: 0.00000862
Iteration 85/1000 | Loss: 0.00000862
Iteration 86/1000 | Loss: 0.00000862
Iteration 87/1000 | Loss: 0.00000862
Iteration 88/1000 | Loss: 0.00000862
Iteration 89/1000 | Loss: 0.00000861
Iteration 90/1000 | Loss: 0.00000861
Iteration 91/1000 | Loss: 0.00000861
Iteration 92/1000 | Loss: 0.00000861
Iteration 93/1000 | Loss: 0.00000861
Iteration 94/1000 | Loss: 0.00000861
Iteration 95/1000 | Loss: 0.00000861
Iteration 96/1000 | Loss: 0.00000861
Iteration 97/1000 | Loss: 0.00000861
Iteration 98/1000 | Loss: 0.00000861
Iteration 99/1000 | Loss: 0.00000861
Iteration 100/1000 | Loss: 0.00000860
Iteration 101/1000 | Loss: 0.00000860
Iteration 102/1000 | Loss: 0.00000860
Iteration 103/1000 | Loss: 0.00000859
Iteration 104/1000 | Loss: 0.00000859
Iteration 105/1000 | Loss: 0.00000859
Iteration 106/1000 | Loss: 0.00000859
Iteration 107/1000 | Loss: 0.00000859
Iteration 108/1000 | Loss: 0.00000859
Iteration 109/1000 | Loss: 0.00000859
Iteration 110/1000 | Loss: 0.00000859
Iteration 111/1000 | Loss: 0.00000859
Iteration 112/1000 | Loss: 0.00000858
Iteration 113/1000 | Loss: 0.00000858
Iteration 114/1000 | Loss: 0.00000858
Iteration 115/1000 | Loss: 0.00000858
Iteration 116/1000 | Loss: 0.00000858
Iteration 117/1000 | Loss: 0.00000858
Iteration 118/1000 | Loss: 0.00000858
Iteration 119/1000 | Loss: 0.00000858
Iteration 120/1000 | Loss: 0.00000858
Iteration 121/1000 | Loss: 0.00000858
Iteration 122/1000 | Loss: 0.00000858
Iteration 123/1000 | Loss: 0.00000858
Iteration 124/1000 | Loss: 0.00000858
Iteration 125/1000 | Loss: 0.00000858
Iteration 126/1000 | Loss: 0.00000858
Iteration 127/1000 | Loss: 0.00000857
Iteration 128/1000 | Loss: 0.00000857
Iteration 129/1000 | Loss: 0.00000857
Iteration 130/1000 | Loss: 0.00000857
Iteration 131/1000 | Loss: 0.00000857
Iteration 132/1000 | Loss: 0.00000857
Iteration 133/1000 | Loss: 0.00000856
Iteration 134/1000 | Loss: 0.00000856
Iteration 135/1000 | Loss: 0.00000856
Iteration 136/1000 | Loss: 0.00000856
Iteration 137/1000 | Loss: 0.00000856
Iteration 138/1000 | Loss: 0.00000856
Iteration 139/1000 | Loss: 0.00000856
Iteration 140/1000 | Loss: 0.00000856
Iteration 141/1000 | Loss: 0.00000855
Iteration 142/1000 | Loss: 0.00000855
Iteration 143/1000 | Loss: 0.00000855
Iteration 144/1000 | Loss: 0.00000855
Iteration 145/1000 | Loss: 0.00000855
Iteration 146/1000 | Loss: 0.00000855
Iteration 147/1000 | Loss: 0.00000855
Iteration 148/1000 | Loss: 0.00000855
Iteration 149/1000 | Loss: 0.00000855
Iteration 150/1000 | Loss: 0.00000855
Iteration 151/1000 | Loss: 0.00000855
Iteration 152/1000 | Loss: 0.00000855
Iteration 153/1000 | Loss: 0.00000855
Iteration 154/1000 | Loss: 0.00000854
Iteration 155/1000 | Loss: 0.00000854
Iteration 156/1000 | Loss: 0.00000854
Iteration 157/1000 | Loss: 0.00000854
Iteration 158/1000 | Loss: 0.00000854
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000854
Iteration 161/1000 | Loss: 0.00000854
Iteration 162/1000 | Loss: 0.00000854
Iteration 163/1000 | Loss: 0.00000854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.543510375602636e-06, 8.543510375602636e-06, 8.543510375602636e-06, 8.543510375602636e-06, 8.543510375602636e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.543510375602636e-06

Optimization complete. Final v2v error: 2.500148057937622 mm

Highest mean error: 2.780156373977661 mm for frame 89

Lowest mean error: 2.303905963897705 mm for frame 51

Saving results

Total time: 381.3216326236725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1048
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092173
Iteration 2/25 | Loss: 0.00150216
Iteration 3/25 | Loss: 0.00107998
Iteration 4/25 | Loss: 0.00104890
Iteration 5/25 | Loss: 0.00104187
Iteration 6/25 | Loss: 0.00103980
Iteration 7/25 | Loss: 0.00103980
Iteration 8/25 | Loss: 0.00103980
Iteration 9/25 | Loss: 0.00103980
Iteration 10/25 | Loss: 0.00103980
Iteration 11/25 | Loss: 0.00103980
Iteration 12/25 | Loss: 0.00103980
Iteration 13/25 | Loss: 0.00103980
Iteration 14/25 | Loss: 0.00103980
Iteration 15/25 | Loss: 0.00103980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010398023296147585, 0.0010398023296147585, 0.0010398023296147585, 0.0010398023296147585, 0.0010398023296147585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010398023296147585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94993389
Iteration 2/25 | Loss: 0.00052606
Iteration 3/25 | Loss: 0.00052606
Iteration 4/25 | Loss: 0.00052606
Iteration 5/25 | Loss: 0.00052606
Iteration 6/25 | Loss: 0.00052606
Iteration 7/25 | Loss: 0.00052606
Iteration 8/25 | Loss: 0.00052606
Iteration 9/25 | Loss: 0.00052606
Iteration 10/25 | Loss: 0.00052606
Iteration 11/25 | Loss: 0.00052606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005260574398562312, 0.0005260574398562312, 0.0005260574398562312, 0.0005260574398562312, 0.0005260574398562312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005260574398562312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052606
Iteration 2/1000 | Loss: 0.00004760
Iteration 3/1000 | Loss: 0.00003319
Iteration 4/1000 | Loss: 0.00002813
Iteration 5/1000 | Loss: 0.00002710
Iteration 6/1000 | Loss: 0.00002603
Iteration 7/1000 | Loss: 0.00002554
Iteration 8/1000 | Loss: 0.00002505
Iteration 9/1000 | Loss: 0.00002475
Iteration 10/1000 | Loss: 0.00002448
Iteration 11/1000 | Loss: 0.00002424
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002388
Iteration 14/1000 | Loss: 0.00002373
Iteration 15/1000 | Loss: 0.00002371
Iteration 16/1000 | Loss: 0.00002355
Iteration 17/1000 | Loss: 0.00002338
Iteration 18/1000 | Loss: 0.00002335
Iteration 19/1000 | Loss: 0.00002329
Iteration 20/1000 | Loss: 0.00002329
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002326
Iteration 23/1000 | Loss: 0.00002325
Iteration 24/1000 | Loss: 0.00002325
Iteration 25/1000 | Loss: 0.00002324
Iteration 26/1000 | Loss: 0.00002323
Iteration 27/1000 | Loss: 0.00002323
Iteration 28/1000 | Loss: 0.00002323
Iteration 29/1000 | Loss: 0.00002323
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002322
Iteration 32/1000 | Loss: 0.00002322
Iteration 33/1000 | Loss: 0.00002317
Iteration 34/1000 | Loss: 0.00002317
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002311
Iteration 37/1000 | Loss: 0.00002310
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002308
Iteration 41/1000 | Loss: 0.00002308
Iteration 42/1000 | Loss: 0.00002307
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00002307
Iteration 45/1000 | Loss: 0.00002307
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002307
Iteration 48/1000 | Loss: 0.00002307
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002306
Iteration 53/1000 | Loss: 0.00002304
Iteration 54/1000 | Loss: 0.00002303
Iteration 55/1000 | Loss: 0.00002303
Iteration 56/1000 | Loss: 0.00002303
Iteration 57/1000 | Loss: 0.00002303
Iteration 58/1000 | Loss: 0.00002302
Iteration 59/1000 | Loss: 0.00002301
Iteration 60/1000 | Loss: 0.00002301
Iteration 61/1000 | Loss: 0.00002300
Iteration 62/1000 | Loss: 0.00002300
Iteration 63/1000 | Loss: 0.00002299
Iteration 64/1000 | Loss: 0.00002299
Iteration 65/1000 | Loss: 0.00002299
Iteration 66/1000 | Loss: 0.00002299
Iteration 67/1000 | Loss: 0.00002298
Iteration 68/1000 | Loss: 0.00002298
Iteration 69/1000 | Loss: 0.00002298
Iteration 70/1000 | Loss: 0.00002298
Iteration 71/1000 | Loss: 0.00002297
Iteration 72/1000 | Loss: 0.00002297
Iteration 73/1000 | Loss: 0.00002297
Iteration 74/1000 | Loss: 0.00002297
Iteration 75/1000 | Loss: 0.00002297
Iteration 76/1000 | Loss: 0.00002297
Iteration 77/1000 | Loss: 0.00002297
Iteration 78/1000 | Loss: 0.00002297
Iteration 79/1000 | Loss: 0.00002297
Iteration 80/1000 | Loss: 0.00002297
Iteration 81/1000 | Loss: 0.00002297
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002296
Iteration 84/1000 | Loss: 0.00002295
Iteration 85/1000 | Loss: 0.00002295
Iteration 86/1000 | Loss: 0.00002295
Iteration 87/1000 | Loss: 0.00002295
Iteration 88/1000 | Loss: 0.00002294
Iteration 89/1000 | Loss: 0.00002294
Iteration 90/1000 | Loss: 0.00002294
Iteration 91/1000 | Loss: 0.00002294
Iteration 92/1000 | Loss: 0.00002294
Iteration 93/1000 | Loss: 0.00002293
Iteration 94/1000 | Loss: 0.00002293
Iteration 95/1000 | Loss: 0.00002293
Iteration 96/1000 | Loss: 0.00002293
Iteration 97/1000 | Loss: 0.00002293
Iteration 98/1000 | Loss: 0.00002293
Iteration 99/1000 | Loss: 0.00002292
Iteration 100/1000 | Loss: 0.00002292
Iteration 101/1000 | Loss: 0.00002292
Iteration 102/1000 | Loss: 0.00002292
Iteration 103/1000 | Loss: 0.00002292
Iteration 104/1000 | Loss: 0.00002292
Iteration 105/1000 | Loss: 0.00002292
Iteration 106/1000 | Loss: 0.00002291
Iteration 107/1000 | Loss: 0.00002291
Iteration 108/1000 | Loss: 0.00002291
Iteration 109/1000 | Loss: 0.00002291
Iteration 110/1000 | Loss: 0.00002291
Iteration 111/1000 | Loss: 0.00002291
Iteration 112/1000 | Loss: 0.00002291
Iteration 113/1000 | Loss: 0.00002290
Iteration 114/1000 | Loss: 0.00002290
Iteration 115/1000 | Loss: 0.00002290
Iteration 116/1000 | Loss: 0.00002290
Iteration 117/1000 | Loss: 0.00002290
Iteration 118/1000 | Loss: 0.00002290
Iteration 119/1000 | Loss: 0.00002290
Iteration 120/1000 | Loss: 0.00002290
Iteration 121/1000 | Loss: 0.00002290
Iteration 122/1000 | Loss: 0.00002290
Iteration 123/1000 | Loss: 0.00002289
Iteration 124/1000 | Loss: 0.00002289
Iteration 125/1000 | Loss: 0.00002289
Iteration 126/1000 | Loss: 0.00002289
Iteration 127/1000 | Loss: 0.00002289
Iteration 128/1000 | Loss: 0.00002289
Iteration 129/1000 | Loss: 0.00002289
Iteration 130/1000 | Loss: 0.00002289
Iteration 131/1000 | Loss: 0.00002289
Iteration 132/1000 | Loss: 0.00002289
Iteration 133/1000 | Loss: 0.00002289
Iteration 134/1000 | Loss: 0.00002288
Iteration 135/1000 | Loss: 0.00002288
Iteration 136/1000 | Loss: 0.00002288
Iteration 137/1000 | Loss: 0.00002288
Iteration 138/1000 | Loss: 0.00002288
Iteration 139/1000 | Loss: 0.00002288
Iteration 140/1000 | Loss: 0.00002288
Iteration 141/1000 | Loss: 0.00002288
Iteration 142/1000 | Loss: 0.00002288
Iteration 143/1000 | Loss: 0.00002288
Iteration 144/1000 | Loss: 0.00002288
Iteration 145/1000 | Loss: 0.00002288
Iteration 146/1000 | Loss: 0.00002288
Iteration 147/1000 | Loss: 0.00002288
Iteration 148/1000 | Loss: 0.00002288
Iteration 149/1000 | Loss: 0.00002288
Iteration 150/1000 | Loss: 0.00002288
Iteration 151/1000 | Loss: 0.00002288
Iteration 152/1000 | Loss: 0.00002288
Iteration 153/1000 | Loss: 0.00002288
Iteration 154/1000 | Loss: 0.00002288
Iteration 155/1000 | Loss: 0.00002288
Iteration 156/1000 | Loss: 0.00002288
Iteration 157/1000 | Loss: 0.00002288
Iteration 158/1000 | Loss: 0.00002288
Iteration 159/1000 | Loss: 0.00002288
Iteration 160/1000 | Loss: 0.00002288
Iteration 161/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.2875488866702653e-05, 2.2875488866702653e-05, 2.2875488866702653e-05, 2.2875488866702653e-05, 2.2875488866702653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2875488866702653e-05

Optimization complete. Final v2v error: 3.8903517723083496 mm

Highest mean error: 4.858299255371094 mm for frame 139

Lowest mean error: 3.193474769592285 mm for frame 24

Saving results

Total time: 378.3130724430084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1016
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912551
Iteration 2/25 | Loss: 0.00134034
Iteration 3/25 | Loss: 0.00104439
Iteration 4/25 | Loss: 0.00100536
Iteration 5/25 | Loss: 0.00099953
Iteration 6/25 | Loss: 0.00098455
Iteration 7/25 | Loss: 0.00097783
Iteration 8/25 | Loss: 0.00097026
Iteration 9/25 | Loss: 0.00096624
Iteration 10/25 | Loss: 0.00096562
Iteration 11/25 | Loss: 0.00096979
Iteration 12/25 | Loss: 0.00096679
Iteration 13/25 | Loss: 0.00096475
Iteration 14/25 | Loss: 0.00096405
Iteration 15/25 | Loss: 0.00096385
Iteration 16/25 | Loss: 0.00096384
Iteration 17/25 | Loss: 0.00096384
Iteration 18/25 | Loss: 0.00096384
Iteration 19/25 | Loss: 0.00096384
Iteration 20/25 | Loss: 0.00096384
Iteration 21/25 | Loss: 0.00096384
Iteration 22/25 | Loss: 0.00096384
Iteration 23/25 | Loss: 0.00096384
Iteration 24/25 | Loss: 0.00096383
Iteration 25/25 | Loss: 0.00096383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37847090
Iteration 2/25 | Loss: 0.00047668
Iteration 3/25 | Loss: 0.00047667
Iteration 4/25 | Loss: 0.00047667
Iteration 5/25 | Loss: 0.00047667
Iteration 6/25 | Loss: 0.00047667
Iteration 7/25 | Loss: 0.00047667
Iteration 8/25 | Loss: 0.00047667
Iteration 9/25 | Loss: 0.00047667
Iteration 10/25 | Loss: 0.00047667
Iteration 11/25 | Loss: 0.00047667
Iteration 12/25 | Loss: 0.00047667
Iteration 13/25 | Loss: 0.00047667
Iteration 14/25 | Loss: 0.00047667
Iteration 15/25 | Loss: 0.00047667
Iteration 16/25 | Loss: 0.00047667
Iteration 17/25 | Loss: 0.00047667
Iteration 18/25 | Loss: 0.00047667
Iteration 19/25 | Loss: 0.00047667
Iteration 20/25 | Loss: 0.00047667
Iteration 21/25 | Loss: 0.00047667
Iteration 22/25 | Loss: 0.00047667
Iteration 23/25 | Loss: 0.00047667
Iteration 24/25 | Loss: 0.00047667
Iteration 25/25 | Loss: 0.00047667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047667
Iteration 2/1000 | Loss: 0.00013977
Iteration 3/1000 | Loss: 0.00004293
Iteration 4/1000 | Loss: 0.00003160
Iteration 5/1000 | Loss: 0.00002802
Iteration 6/1000 | Loss: 0.00002572
Iteration 7/1000 | Loss: 0.00002452
Iteration 8/1000 | Loss: 0.00024786
Iteration 9/1000 | Loss: 0.00028264
Iteration 10/1000 | Loss: 0.00005170
Iteration 11/1000 | Loss: 0.00002640
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00019811
Iteration 14/1000 | Loss: 0.00003008
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002161
Iteration 17/1000 | Loss: 0.00002084
Iteration 18/1000 | Loss: 0.00002043
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001962
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001937
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001931
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001926
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001925
Iteration 53/1000 | Loss: 0.00001925
Iteration 54/1000 | Loss: 0.00001925
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001923
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001921
Iteration 71/1000 | Loss: 0.00001921
Iteration 72/1000 | Loss: 0.00001920
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001919
Iteration 75/1000 | Loss: 0.00001919
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001911
Iteration 91/1000 | Loss: 0.00001911
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001909
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001906
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001906
Iteration 119/1000 | Loss: 0.00001906
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001905
Iteration 123/1000 | Loss: 0.00001905
Iteration 124/1000 | Loss: 0.00001905
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001905
Iteration 128/1000 | Loss: 0.00001905
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001904
Iteration 135/1000 | Loss: 0.00001904
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001904
Iteration 138/1000 | Loss: 0.00001904
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001903
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001903
Iteration 153/1000 | Loss: 0.00001903
Iteration 154/1000 | Loss: 0.00001903
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001902
Iteration 162/1000 | Loss: 0.00001902
Iteration 163/1000 | Loss: 0.00001902
Iteration 164/1000 | Loss: 0.00001902
Iteration 165/1000 | Loss: 0.00001902
Iteration 166/1000 | Loss: 0.00001902
Iteration 167/1000 | Loss: 0.00001902
Iteration 168/1000 | Loss: 0.00001901
Iteration 169/1000 | Loss: 0.00001901
Iteration 170/1000 | Loss: 0.00001901
Iteration 171/1000 | Loss: 0.00001901
Iteration 172/1000 | Loss: 0.00001901
Iteration 173/1000 | Loss: 0.00001901
Iteration 174/1000 | Loss: 0.00001901
Iteration 175/1000 | Loss: 0.00001900
Iteration 176/1000 | Loss: 0.00001900
Iteration 177/1000 | Loss: 0.00001900
Iteration 178/1000 | Loss: 0.00001900
Iteration 179/1000 | Loss: 0.00001900
Iteration 180/1000 | Loss: 0.00001900
Iteration 181/1000 | Loss: 0.00001900
Iteration 182/1000 | Loss: 0.00001900
Iteration 183/1000 | Loss: 0.00001900
Iteration 184/1000 | Loss: 0.00001900
Iteration 185/1000 | Loss: 0.00001900
Iteration 186/1000 | Loss: 0.00001900
Iteration 187/1000 | Loss: 0.00001900
Iteration 188/1000 | Loss: 0.00001900
Iteration 189/1000 | Loss: 0.00001900
Iteration 190/1000 | Loss: 0.00001900
Iteration 191/1000 | Loss: 0.00001900
Iteration 192/1000 | Loss: 0.00001900
Iteration 193/1000 | Loss: 0.00001900
Iteration 194/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.9002474800799973e-05, 1.9002474800799973e-05, 1.9002474800799973e-05, 1.9002474800799973e-05, 1.9002474800799973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9002474800799973e-05

Optimization complete. Final v2v error: 3.5733723640441895 mm

Highest mean error: 5.197875499725342 mm for frame 222

Lowest mean error: 2.9697272777557373 mm for frame 231

Saving results

Total time: 1069.248833656311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1073
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556625
Iteration 2/25 | Loss: 0.00098059
Iteration 3/25 | Loss: 0.00087152
Iteration 4/25 | Loss: 0.00085844
Iteration 5/25 | Loss: 0.00085586
Iteration 6/25 | Loss: 0.00085570
Iteration 7/25 | Loss: 0.00085570
Iteration 8/25 | Loss: 0.00085570
Iteration 9/25 | Loss: 0.00085570
Iteration 10/25 | Loss: 0.00085570
Iteration 11/25 | Loss: 0.00085570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008556954562664032, 0.0008556954562664032, 0.0008556954562664032, 0.0008556954562664032, 0.0008556954562664032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008556954562664032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36535847
Iteration 2/25 | Loss: 0.00041308
Iteration 3/25 | Loss: 0.00041303
Iteration 4/25 | Loss: 0.00041303
Iteration 5/25 | Loss: 0.00041303
Iteration 6/25 | Loss: 0.00041303
Iteration 7/25 | Loss: 0.00041303
Iteration 8/25 | Loss: 0.00041303
Iteration 9/25 | Loss: 0.00041303
Iteration 10/25 | Loss: 0.00041303
Iteration 11/25 | Loss: 0.00041303
Iteration 12/25 | Loss: 0.00041303
Iteration 13/25 | Loss: 0.00041303
Iteration 14/25 | Loss: 0.00041303
Iteration 15/25 | Loss: 0.00041303
Iteration 16/25 | Loss: 0.00041303
Iteration 17/25 | Loss: 0.00041303
Iteration 18/25 | Loss: 0.00041303
Iteration 19/25 | Loss: 0.00041303
Iteration 20/25 | Loss: 0.00041303
Iteration 21/25 | Loss: 0.00041303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00041302895988337696, 0.00041302895988337696, 0.00041302895988337696, 0.00041302895988337696, 0.00041302895988337696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041302895988337696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041303
Iteration 2/1000 | Loss: 0.00001745
Iteration 3/1000 | Loss: 0.00001178
Iteration 4/1000 | Loss: 0.00001060
Iteration 5/1000 | Loss: 0.00000983
Iteration 6/1000 | Loss: 0.00000945
Iteration 7/1000 | Loss: 0.00000903
Iteration 8/1000 | Loss: 0.00000885
Iteration 9/1000 | Loss: 0.00000876
Iteration 10/1000 | Loss: 0.00000876
Iteration 11/1000 | Loss: 0.00000869
Iteration 12/1000 | Loss: 0.00000863
Iteration 13/1000 | Loss: 0.00000856
Iteration 14/1000 | Loss: 0.00000854
Iteration 15/1000 | Loss: 0.00000853
Iteration 16/1000 | Loss: 0.00000853
Iteration 17/1000 | Loss: 0.00000852
Iteration 18/1000 | Loss: 0.00000852
Iteration 19/1000 | Loss: 0.00000851
Iteration 20/1000 | Loss: 0.00000851
Iteration 21/1000 | Loss: 0.00000850
Iteration 22/1000 | Loss: 0.00000850
Iteration 23/1000 | Loss: 0.00000849
Iteration 24/1000 | Loss: 0.00000849
Iteration 25/1000 | Loss: 0.00000848
Iteration 26/1000 | Loss: 0.00000848
Iteration 27/1000 | Loss: 0.00000847
Iteration 28/1000 | Loss: 0.00000846
Iteration 29/1000 | Loss: 0.00000846
Iteration 30/1000 | Loss: 0.00000845
Iteration 31/1000 | Loss: 0.00000845
Iteration 32/1000 | Loss: 0.00000843
Iteration 33/1000 | Loss: 0.00000843
Iteration 34/1000 | Loss: 0.00000843
Iteration 35/1000 | Loss: 0.00000842
Iteration 36/1000 | Loss: 0.00000841
Iteration 37/1000 | Loss: 0.00000840
Iteration 38/1000 | Loss: 0.00000840
Iteration 39/1000 | Loss: 0.00000839
Iteration 40/1000 | Loss: 0.00000839
Iteration 41/1000 | Loss: 0.00000839
Iteration 42/1000 | Loss: 0.00000839
Iteration 43/1000 | Loss: 0.00000839
Iteration 44/1000 | Loss: 0.00000839
Iteration 45/1000 | Loss: 0.00000839
Iteration 46/1000 | Loss: 0.00000838
Iteration 47/1000 | Loss: 0.00000838
Iteration 48/1000 | Loss: 0.00000838
Iteration 49/1000 | Loss: 0.00000837
Iteration 50/1000 | Loss: 0.00000837
Iteration 51/1000 | Loss: 0.00000837
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000837
Iteration 54/1000 | Loss: 0.00000836
Iteration 55/1000 | Loss: 0.00000836
Iteration 56/1000 | Loss: 0.00000836
Iteration 57/1000 | Loss: 0.00000836
Iteration 58/1000 | Loss: 0.00000836
Iteration 59/1000 | Loss: 0.00000836
Iteration 60/1000 | Loss: 0.00000836
Iteration 61/1000 | Loss: 0.00000836
Iteration 62/1000 | Loss: 0.00000836
Iteration 63/1000 | Loss: 0.00000836
Iteration 64/1000 | Loss: 0.00000836
Iteration 65/1000 | Loss: 0.00000835
Iteration 66/1000 | Loss: 0.00000835
Iteration 67/1000 | Loss: 0.00000835
Iteration 68/1000 | Loss: 0.00000835
Iteration 69/1000 | Loss: 0.00000835
Iteration 70/1000 | Loss: 0.00000835
Iteration 71/1000 | Loss: 0.00000835
Iteration 72/1000 | Loss: 0.00000835
Iteration 73/1000 | Loss: 0.00000835
Iteration 74/1000 | Loss: 0.00000835
Iteration 75/1000 | Loss: 0.00000835
Iteration 76/1000 | Loss: 0.00000835
Iteration 77/1000 | Loss: 0.00000835
Iteration 78/1000 | Loss: 0.00000835
Iteration 79/1000 | Loss: 0.00000835
Iteration 80/1000 | Loss: 0.00000835
Iteration 81/1000 | Loss: 0.00000835
Iteration 82/1000 | Loss: 0.00000835
Iteration 83/1000 | Loss: 0.00000835
Iteration 84/1000 | Loss: 0.00000835
Iteration 85/1000 | Loss: 0.00000835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [8.34600086818682e-06, 8.34600086818682e-06, 8.34600086818682e-06, 8.34600086818682e-06, 8.34600086818682e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.34600086818682e-06

Optimization complete. Final v2v error: 2.444289445877075 mm

Highest mean error: 2.5764050483703613 mm for frame 149

Lowest mean error: 2.349506139755249 mm for frame 90

Saving results

Total time: 269.618191242218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1081
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144963
Iteration 2/25 | Loss: 0.00170702
Iteration 3/25 | Loss: 0.00118098
Iteration 4/25 | Loss: 0.00113173
Iteration 5/25 | Loss: 0.00112353
Iteration 6/25 | Loss: 0.00112148
Iteration 7/25 | Loss: 0.00112068
Iteration 8/25 | Loss: 0.00111628
Iteration 9/25 | Loss: 0.00111477
Iteration 10/25 | Loss: 0.00111395
Iteration 11/25 | Loss: 0.00111108
Iteration 12/25 | Loss: 0.00110641
Iteration 13/25 | Loss: 0.00110663
Iteration 14/25 | Loss: 0.00111063
Iteration 15/25 | Loss: 0.00111206
Iteration 16/25 | Loss: 0.00111253
Iteration 17/25 | Loss: 0.00111129
Iteration 18/25 | Loss: 0.00110854
Iteration 19/25 | Loss: 0.00110750
Iteration 20/25 | Loss: 0.00110442
Iteration 21/25 | Loss: 0.00110498
Iteration 22/25 | Loss: 0.00110675
Iteration 23/25 | Loss: 0.00110538
Iteration 24/25 | Loss: 0.00110555
Iteration 25/25 | Loss: 0.00110102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30911601
Iteration 2/25 | Loss: 0.00126052
Iteration 3/25 | Loss: 0.00126052
Iteration 4/25 | Loss: 0.00126052
Iteration 5/25 | Loss: 0.00126052
Iteration 6/25 | Loss: 0.00126051
Iteration 7/25 | Loss: 0.00126051
Iteration 8/25 | Loss: 0.00126051
Iteration 9/25 | Loss: 0.00126051
Iteration 10/25 | Loss: 0.00126051
Iteration 11/25 | Loss: 0.00126051
Iteration 12/25 | Loss: 0.00126051
Iteration 13/25 | Loss: 0.00126051
Iteration 14/25 | Loss: 0.00126051
Iteration 15/25 | Loss: 0.00126051
Iteration 16/25 | Loss: 0.00126051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012605140218511224, 0.0012605140218511224, 0.0012605140218511224, 0.0012605140218511224, 0.0012605140218511224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012605140218511224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126051
Iteration 2/1000 | Loss: 0.00024933
Iteration 3/1000 | Loss: 0.00111297
Iteration 4/1000 | Loss: 0.00058360
Iteration 5/1000 | Loss: 0.00048864
Iteration 6/1000 | Loss: 0.00048371
Iteration 7/1000 | Loss: 0.00040474
Iteration 8/1000 | Loss: 0.00034149
Iteration 9/1000 | Loss: 0.00065924
Iteration 10/1000 | Loss: 0.00049894
Iteration 11/1000 | Loss: 0.00046065
Iteration 12/1000 | Loss: 0.00045360
Iteration 13/1000 | Loss: 0.00087226
Iteration 14/1000 | Loss: 0.00038291
Iteration 15/1000 | Loss: 0.00095011
Iteration 16/1000 | Loss: 0.00058600
Iteration 17/1000 | Loss: 0.00055952
Iteration 18/1000 | Loss: 0.00090111
Iteration 19/1000 | Loss: 0.00064165
Iteration 20/1000 | Loss: 0.00047478
Iteration 21/1000 | Loss: 0.00071637
Iteration 22/1000 | Loss: 0.00058661
Iteration 23/1000 | Loss: 0.00057075
Iteration 24/1000 | Loss: 0.00056091
Iteration 25/1000 | Loss: 0.00058989
Iteration 26/1000 | Loss: 0.00056355
Iteration 27/1000 | Loss: 0.00058690
Iteration 28/1000 | Loss: 0.00085906
Iteration 29/1000 | Loss: 0.00065093
Iteration 30/1000 | Loss: 0.00048753
Iteration 31/1000 | Loss: 0.00082997
Iteration 32/1000 | Loss: 0.00050841
Iteration 33/1000 | Loss: 0.00069017
Iteration 34/1000 | Loss: 0.00045146
Iteration 35/1000 | Loss: 0.00039570
Iteration 36/1000 | Loss: 0.00090493
Iteration 37/1000 | Loss: 0.00114204
Iteration 38/1000 | Loss: 0.00072952
Iteration 39/1000 | Loss: 0.00074796
Iteration 40/1000 | Loss: 0.00040772
Iteration 41/1000 | Loss: 0.00044341
Iteration 42/1000 | Loss: 0.00048558
Iteration 43/1000 | Loss: 0.00037439
Iteration 44/1000 | Loss: 0.00042457
Iteration 45/1000 | Loss: 0.00053053
Iteration 46/1000 | Loss: 0.00070139
Iteration 47/1000 | Loss: 0.00065812
Iteration 48/1000 | Loss: 0.00050873
Iteration 49/1000 | Loss: 0.00043776
Iteration 50/1000 | Loss: 0.00052800
Iteration 51/1000 | Loss: 0.00087498
Iteration 52/1000 | Loss: 0.00054617
Iteration 53/1000 | Loss: 0.00050986
Iteration 54/1000 | Loss: 0.00047916
Iteration 55/1000 | Loss: 0.00079614
Iteration 56/1000 | Loss: 0.00053631
Iteration 57/1000 | Loss: 0.00041782
Iteration 58/1000 | Loss: 0.00027079
Iteration 59/1000 | Loss: 0.00053120
Iteration 60/1000 | Loss: 0.00064493
Iteration 61/1000 | Loss: 0.00023210
Iteration 62/1000 | Loss: 0.00011284
Iteration 63/1000 | Loss: 0.00015697
Iteration 64/1000 | Loss: 0.00007324
Iteration 65/1000 | Loss: 0.00022045
Iteration 66/1000 | Loss: 0.00030849
Iteration 67/1000 | Loss: 0.00053456
Iteration 68/1000 | Loss: 0.00037270
Iteration 69/1000 | Loss: 0.00070883
Iteration 70/1000 | Loss: 0.00049374
Iteration 71/1000 | Loss: 0.00063638
Iteration 72/1000 | Loss: 0.00092153
Iteration 73/1000 | Loss: 0.00007219
Iteration 74/1000 | Loss: 0.00041661
Iteration 75/1000 | Loss: 0.00034466
Iteration 76/1000 | Loss: 0.00043309
Iteration 77/1000 | Loss: 0.00024086
Iteration 78/1000 | Loss: 0.00084862
Iteration 79/1000 | Loss: 0.00054101
Iteration 80/1000 | Loss: 0.00006965
Iteration 81/1000 | Loss: 0.00052048
Iteration 82/1000 | Loss: 0.00053240
Iteration 83/1000 | Loss: 0.00010817
Iteration 84/1000 | Loss: 0.00029243
Iteration 85/1000 | Loss: 0.00041305
Iteration 86/1000 | Loss: 0.00004575
Iteration 87/1000 | Loss: 0.00019089
Iteration 88/1000 | Loss: 0.00035643
Iteration 89/1000 | Loss: 0.00005701
Iteration 90/1000 | Loss: 0.00005252
Iteration 91/1000 | Loss: 0.00004737
Iteration 92/1000 | Loss: 0.00004869
Iteration 93/1000 | Loss: 0.00037136
Iteration 94/1000 | Loss: 0.00017287
Iteration 95/1000 | Loss: 0.00004860
Iteration 96/1000 | Loss: 0.00056013
Iteration 97/1000 | Loss: 0.00005569
Iteration 98/1000 | Loss: 0.00005039
Iteration 99/1000 | Loss: 0.00004673
Iteration 100/1000 | Loss: 0.00003872
Iteration 101/1000 | Loss: 0.00003453
Iteration 102/1000 | Loss: 0.00004192
Iteration 103/1000 | Loss: 0.00003545
Iteration 104/1000 | Loss: 0.00003370
Iteration 105/1000 | Loss: 0.00004252
Iteration 106/1000 | Loss: 0.00003908
Iteration 107/1000 | Loss: 0.00003700
Iteration 108/1000 | Loss: 0.00004749
Iteration 109/1000 | Loss: 0.00003849
Iteration 110/1000 | Loss: 0.00003724
Iteration 111/1000 | Loss: 0.00003922
Iteration 112/1000 | Loss: 0.00004505
Iteration 113/1000 | Loss: 0.00003831
Iteration 114/1000 | Loss: 0.00004063
Iteration 115/1000 | Loss: 0.00003856
Iteration 116/1000 | Loss: 0.00004084
Iteration 117/1000 | Loss: 0.00004164
Iteration 118/1000 | Loss: 0.00024832
Iteration 119/1000 | Loss: 0.00008536
Iteration 120/1000 | Loss: 0.00003720
Iteration 121/1000 | Loss: 0.00003897
Iteration 122/1000 | Loss: 0.00004224
Iteration 123/1000 | Loss: 0.00003402
Iteration 124/1000 | Loss: 0.00023362
Iteration 125/1000 | Loss: 0.00036595
Iteration 126/1000 | Loss: 0.00005982
Iteration 127/1000 | Loss: 0.00011455
Iteration 128/1000 | Loss: 0.00010230
Iteration 129/1000 | Loss: 0.00004905
Iteration 130/1000 | Loss: 0.00029380
Iteration 131/1000 | Loss: 0.00017618
Iteration 132/1000 | Loss: 0.00003503
Iteration 133/1000 | Loss: 0.00003292
Iteration 134/1000 | Loss: 0.00023801
Iteration 135/1000 | Loss: 0.00004261
Iteration 136/1000 | Loss: 0.00003347
Iteration 137/1000 | Loss: 0.00003096
Iteration 138/1000 | Loss: 0.00002971
Iteration 139/1000 | Loss: 0.00002904
Iteration 140/1000 | Loss: 0.00002871
Iteration 141/1000 | Loss: 0.00002849
Iteration 142/1000 | Loss: 0.00002830
Iteration 143/1000 | Loss: 0.00002813
Iteration 144/1000 | Loss: 0.00002808
Iteration 145/1000 | Loss: 0.00002802
Iteration 146/1000 | Loss: 0.00002798
Iteration 147/1000 | Loss: 0.00002792
Iteration 148/1000 | Loss: 0.00002783
Iteration 149/1000 | Loss: 0.00002782
Iteration 150/1000 | Loss: 0.00014224
Iteration 151/1000 | Loss: 0.00006448
Iteration 152/1000 | Loss: 0.00042911
Iteration 153/1000 | Loss: 0.00025798
Iteration 154/1000 | Loss: 0.00004240
Iteration 155/1000 | Loss: 0.00017092
Iteration 156/1000 | Loss: 0.00002972
Iteration 157/1000 | Loss: 0.00005627
Iteration 158/1000 | Loss: 0.00005025
Iteration 159/1000 | Loss: 0.00011351
Iteration 160/1000 | Loss: 0.00003061
Iteration 161/1000 | Loss: 0.00002834
Iteration 162/1000 | Loss: 0.00002786
Iteration 163/1000 | Loss: 0.00002779
Iteration 164/1000 | Loss: 0.00002776
Iteration 165/1000 | Loss: 0.00002776
Iteration 166/1000 | Loss: 0.00002776
Iteration 167/1000 | Loss: 0.00002776
Iteration 168/1000 | Loss: 0.00002776
Iteration 169/1000 | Loss: 0.00002775
Iteration 170/1000 | Loss: 0.00002775
Iteration 171/1000 | Loss: 0.00002775
Iteration 172/1000 | Loss: 0.00002775
Iteration 173/1000 | Loss: 0.00002769
Iteration 174/1000 | Loss: 0.00002764
Iteration 175/1000 | Loss: 0.00002759
Iteration 176/1000 | Loss: 0.00002758
Iteration 177/1000 | Loss: 0.00002758
Iteration 178/1000 | Loss: 0.00002757
Iteration 179/1000 | Loss: 0.00002757
Iteration 180/1000 | Loss: 0.00018423
Iteration 181/1000 | Loss: 0.00020401
Iteration 182/1000 | Loss: 0.00006033
Iteration 183/1000 | Loss: 0.00025598
Iteration 184/1000 | Loss: 0.00030675
Iteration 185/1000 | Loss: 0.00008627
Iteration 186/1000 | Loss: 0.00003050
Iteration 187/1000 | Loss: 0.00030190
Iteration 188/1000 | Loss: 0.00003370
Iteration 189/1000 | Loss: 0.00003081
Iteration 190/1000 | Loss: 0.00002946
Iteration 191/1000 | Loss: 0.00002850
Iteration 192/1000 | Loss: 0.00002724
Iteration 193/1000 | Loss: 0.00002636
Iteration 194/1000 | Loss: 0.00002585
Iteration 195/1000 | Loss: 0.00002572
Iteration 196/1000 | Loss: 0.00002572
Iteration 197/1000 | Loss: 0.00002563
Iteration 198/1000 | Loss: 0.00002562
Iteration 199/1000 | Loss: 0.00002562
Iteration 200/1000 | Loss: 0.00002561
Iteration 201/1000 | Loss: 0.00002561
Iteration 202/1000 | Loss: 0.00002560
Iteration 203/1000 | Loss: 0.00002560
Iteration 204/1000 | Loss: 0.00002560
Iteration 205/1000 | Loss: 0.00002559
Iteration 206/1000 | Loss: 0.00002559
Iteration 207/1000 | Loss: 0.00002559
Iteration 208/1000 | Loss: 0.00002559
Iteration 209/1000 | Loss: 0.00002559
Iteration 210/1000 | Loss: 0.00002558
Iteration 211/1000 | Loss: 0.00002558
Iteration 212/1000 | Loss: 0.00002558
Iteration 213/1000 | Loss: 0.00002558
Iteration 214/1000 | Loss: 0.00002558
Iteration 215/1000 | Loss: 0.00002558
Iteration 216/1000 | Loss: 0.00002558
Iteration 217/1000 | Loss: 0.00002558
Iteration 218/1000 | Loss: 0.00002558
Iteration 219/1000 | Loss: 0.00002558
Iteration 220/1000 | Loss: 0.00002558
Iteration 221/1000 | Loss: 0.00002558
Iteration 222/1000 | Loss: 0.00002557
Iteration 223/1000 | Loss: 0.00002557
Iteration 224/1000 | Loss: 0.00002557
Iteration 225/1000 | Loss: 0.00002557
Iteration 226/1000 | Loss: 0.00002557
Iteration 227/1000 | Loss: 0.00002556
Iteration 228/1000 | Loss: 0.00002556
Iteration 229/1000 | Loss: 0.00002556
Iteration 230/1000 | Loss: 0.00002556
Iteration 231/1000 | Loss: 0.00002556
Iteration 232/1000 | Loss: 0.00002556
Iteration 233/1000 | Loss: 0.00002556
Iteration 234/1000 | Loss: 0.00002556
Iteration 235/1000 | Loss: 0.00002556
Iteration 236/1000 | Loss: 0.00002556
Iteration 237/1000 | Loss: 0.00002556
Iteration 238/1000 | Loss: 0.00002556
Iteration 239/1000 | Loss: 0.00002556
Iteration 240/1000 | Loss: 0.00002555
Iteration 241/1000 | Loss: 0.00002555
Iteration 242/1000 | Loss: 0.00002555
Iteration 243/1000 | Loss: 0.00002554
Iteration 244/1000 | Loss: 0.00002554
Iteration 245/1000 | Loss: 0.00002553
Iteration 246/1000 | Loss: 0.00002553
Iteration 247/1000 | Loss: 0.00002553
Iteration 248/1000 | Loss: 0.00002553
Iteration 249/1000 | Loss: 0.00002552
Iteration 250/1000 | Loss: 0.00002552
Iteration 251/1000 | Loss: 0.00002552
Iteration 252/1000 | Loss: 0.00002551
Iteration 253/1000 | Loss: 0.00002551
Iteration 254/1000 | Loss: 0.00002551
Iteration 255/1000 | Loss: 0.00002551
Iteration 256/1000 | Loss: 0.00002551
Iteration 257/1000 | Loss: 0.00002551
Iteration 258/1000 | Loss: 0.00002551
Iteration 259/1000 | Loss: 0.00002551
Iteration 260/1000 | Loss: 0.00002551
Iteration 261/1000 | Loss: 0.00002551
Iteration 262/1000 | Loss: 0.00002551
Iteration 263/1000 | Loss: 0.00002551
Iteration 264/1000 | Loss: 0.00002551
Iteration 265/1000 | Loss: 0.00002551
Iteration 266/1000 | Loss: 0.00002551
Iteration 267/1000 | Loss: 0.00002551
Iteration 268/1000 | Loss: 0.00002551
Iteration 269/1000 | Loss: 0.00002551
Iteration 270/1000 | Loss: 0.00002551
Iteration 271/1000 | Loss: 0.00002550
Iteration 272/1000 | Loss: 0.00002550
Iteration 273/1000 | Loss: 0.00002550
Iteration 274/1000 | Loss: 0.00002550
Iteration 275/1000 | Loss: 0.00002550
Iteration 276/1000 | Loss: 0.00002550
Iteration 277/1000 | Loss: 0.00002550
Iteration 278/1000 | Loss: 0.00002550
Iteration 279/1000 | Loss: 0.00002550
Iteration 280/1000 | Loss: 0.00002550
Iteration 281/1000 | Loss: 0.00002550
Iteration 282/1000 | Loss: 0.00002550
Iteration 283/1000 | Loss: 0.00002549
Iteration 284/1000 | Loss: 0.00002549
Iteration 285/1000 | Loss: 0.00002549
Iteration 286/1000 | Loss: 0.00002549
Iteration 287/1000 | Loss: 0.00002549
Iteration 288/1000 | Loss: 0.00002549
Iteration 289/1000 | Loss: 0.00002549
Iteration 290/1000 | Loss: 0.00002549
Iteration 291/1000 | Loss: 0.00002549
Iteration 292/1000 | Loss: 0.00002549
Iteration 293/1000 | Loss: 0.00002549
Iteration 294/1000 | Loss: 0.00002548
Iteration 295/1000 | Loss: 0.00002548
Iteration 296/1000 | Loss: 0.00002548
Iteration 297/1000 | Loss: 0.00002548
Iteration 298/1000 | Loss: 0.00002548
Iteration 299/1000 | Loss: 0.00002548
Iteration 300/1000 | Loss: 0.00002548
Iteration 301/1000 | Loss: 0.00002548
Iteration 302/1000 | Loss: 0.00002548
Iteration 303/1000 | Loss: 0.00002548
Iteration 304/1000 | Loss: 0.00002548
Iteration 305/1000 | Loss: 0.00002548
Iteration 306/1000 | Loss: 0.00002548
Iteration 307/1000 | Loss: 0.00002548
Iteration 308/1000 | Loss: 0.00002548
Iteration 309/1000 | Loss: 0.00002548
Iteration 310/1000 | Loss: 0.00002548
Iteration 311/1000 | Loss: 0.00002548
Iteration 312/1000 | Loss: 0.00002548
Iteration 313/1000 | Loss: 0.00002548
Iteration 314/1000 | Loss: 0.00002547
Iteration 315/1000 | Loss: 0.00002547
Iteration 316/1000 | Loss: 0.00002547
Iteration 317/1000 | Loss: 0.00002547
Iteration 318/1000 | Loss: 0.00002547
Iteration 319/1000 | Loss: 0.00002547
Iteration 320/1000 | Loss: 0.00002547
Iteration 321/1000 | Loss: 0.00002547
Iteration 322/1000 | Loss: 0.00002547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [2.5473553250776604e-05, 2.5473553250776604e-05, 2.5473553250776604e-05, 2.5473553250776604e-05, 2.5473553250776604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5473553250776604e-05

Optimization complete. Final v2v error: 4.026100158691406 mm

Highest mean error: 5.223021507263184 mm for frame 89

Lowest mean error: 3.1331727504730225 mm for frame 34

Saving results

Total time: 2920.378438472748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1038
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099865
Iteration 2/25 | Loss: 0.00183115
Iteration 3/25 | Loss: 0.00114637
Iteration 4/25 | Loss: 0.00102085
Iteration 5/25 | Loss: 0.00098139
Iteration 6/25 | Loss: 0.00096935
Iteration 7/25 | Loss: 0.00093168
Iteration 8/25 | Loss: 0.00089134
Iteration 9/25 | Loss: 0.00087907
Iteration 10/25 | Loss: 0.00087410
Iteration 11/25 | Loss: 0.00087094
Iteration 12/25 | Loss: 0.00086481
Iteration 13/25 | Loss: 0.00086368
Iteration 14/25 | Loss: 0.00086364
Iteration 15/25 | Loss: 0.00086363
Iteration 16/25 | Loss: 0.00086362
Iteration 17/25 | Loss: 0.00086361
Iteration 18/25 | Loss: 0.00086361
Iteration 19/25 | Loss: 0.00086361
Iteration 20/25 | Loss: 0.00086361
Iteration 21/25 | Loss: 0.00086361
Iteration 22/25 | Loss: 0.00086361
Iteration 23/25 | Loss: 0.00086361
Iteration 24/25 | Loss: 0.00086361
Iteration 25/25 | Loss: 0.00086361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54843986
Iteration 2/25 | Loss: 0.00054046
Iteration 3/25 | Loss: 0.00054046
Iteration 4/25 | Loss: 0.00054046
Iteration 5/25 | Loss: 0.00054046
Iteration 6/25 | Loss: 0.00054046
Iteration 7/25 | Loss: 0.00054046
Iteration 8/25 | Loss: 0.00054046
Iteration 9/25 | Loss: 0.00054046
Iteration 10/25 | Loss: 0.00054046
Iteration 11/25 | Loss: 0.00054046
Iteration 12/25 | Loss: 0.00054046
Iteration 13/25 | Loss: 0.00054046
Iteration 14/25 | Loss: 0.00054046
Iteration 15/25 | Loss: 0.00054046
Iteration 16/25 | Loss: 0.00054046
Iteration 17/25 | Loss: 0.00054046
Iteration 18/25 | Loss: 0.00054046
Iteration 19/25 | Loss: 0.00054046
Iteration 20/25 | Loss: 0.00054046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005404599360190332, 0.0005404599360190332, 0.0005404599360190332, 0.0005404599360190332, 0.0005404599360190332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005404599360190332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054046
Iteration 2/1000 | Loss: 0.00004477
Iteration 3/1000 | Loss: 0.00005744
Iteration 4/1000 | Loss: 0.00005628
Iteration 5/1000 | Loss: 0.00005874
Iteration 6/1000 | Loss: 0.00004255
Iteration 7/1000 | Loss: 0.00010385
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00007876
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00004222
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00005380
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001428
Iteration 17/1000 | Loss: 0.00001427
Iteration 18/1000 | Loss: 0.00001427
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001424
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00003316
Iteration 26/1000 | Loss: 0.00001407
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001401
Iteration 29/1000 | Loss: 0.00002252
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001378
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001375
Iteration 61/1000 | Loss: 0.00001375
Iteration 62/1000 | Loss: 0.00001375
Iteration 63/1000 | Loss: 0.00001375
Iteration 64/1000 | Loss: 0.00001375
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001374
Iteration 67/1000 | Loss: 0.00001374
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001373
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001365
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001365
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001364
Iteration 119/1000 | Loss: 0.00001364
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.3644055798067711e-05, 1.3644055798067711e-05, 1.3644055798067711e-05, 1.3644055798067711e-05, 1.3644055798067711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3644055798067711e-05

Optimization complete. Final v2v error: 3.0541555881500244 mm

Highest mean error: 4.5279860496521 mm for frame 81

Lowest mean error: 2.4457926750183105 mm for frame 50

Saving results

Total time: 499.3558337688446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1001
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424401
Iteration 2/25 | Loss: 0.00097721
Iteration 3/25 | Loss: 0.00084727
Iteration 4/25 | Loss: 0.00083191
Iteration 5/25 | Loss: 0.00082788
Iteration 6/25 | Loss: 0.00082700
Iteration 7/25 | Loss: 0.00082700
Iteration 8/25 | Loss: 0.00082700
Iteration 9/25 | Loss: 0.00082700
Iteration 10/25 | Loss: 0.00082700
Iteration 11/25 | Loss: 0.00082700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008269965182989836, 0.0008269965182989836, 0.0008269965182989836, 0.0008269965182989836, 0.0008269965182989836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008269965182989836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.42061806
Iteration 2/25 | Loss: 0.00036389
Iteration 3/25 | Loss: 0.00036387
Iteration 4/25 | Loss: 0.00036387
Iteration 5/25 | Loss: 0.00036387
Iteration 6/25 | Loss: 0.00036387
Iteration 7/25 | Loss: 0.00036387
Iteration 8/25 | Loss: 0.00036387
Iteration 9/25 | Loss: 0.00036387
Iteration 10/25 | Loss: 0.00036387
Iteration 11/25 | Loss: 0.00036387
Iteration 12/25 | Loss: 0.00036387
Iteration 13/25 | Loss: 0.00036387
Iteration 14/25 | Loss: 0.00036387
Iteration 15/25 | Loss: 0.00036387
Iteration 16/25 | Loss: 0.00036387
Iteration 17/25 | Loss: 0.00036387
Iteration 18/25 | Loss: 0.00036387
Iteration 19/25 | Loss: 0.00036387
Iteration 20/25 | Loss: 0.00036387
Iteration 21/25 | Loss: 0.00036387
Iteration 22/25 | Loss: 0.00036387
Iteration 23/25 | Loss: 0.00036387
Iteration 24/25 | Loss: 0.00036387
Iteration 25/25 | Loss: 0.00036387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036387
Iteration 2/1000 | Loss: 0.00002073
Iteration 3/1000 | Loss: 0.00001289
Iteration 4/1000 | Loss: 0.00001080
Iteration 5/1000 | Loss: 0.00001007
Iteration 6/1000 | Loss: 0.00000970
Iteration 7/1000 | Loss: 0.00000944
Iteration 8/1000 | Loss: 0.00000937
Iteration 9/1000 | Loss: 0.00000937
Iteration 10/1000 | Loss: 0.00000934
Iteration 11/1000 | Loss: 0.00000931
Iteration 12/1000 | Loss: 0.00000930
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000928
Iteration 15/1000 | Loss: 0.00000921
Iteration 16/1000 | Loss: 0.00000920
Iteration 17/1000 | Loss: 0.00000919
Iteration 18/1000 | Loss: 0.00000915
Iteration 19/1000 | Loss: 0.00000913
Iteration 20/1000 | Loss: 0.00000913
Iteration 21/1000 | Loss: 0.00000912
Iteration 22/1000 | Loss: 0.00000910
Iteration 23/1000 | Loss: 0.00000909
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000907
Iteration 26/1000 | Loss: 0.00000905
Iteration 27/1000 | Loss: 0.00000904
Iteration 28/1000 | Loss: 0.00000904
Iteration 29/1000 | Loss: 0.00000902
Iteration 30/1000 | Loss: 0.00000901
Iteration 31/1000 | Loss: 0.00000901
Iteration 32/1000 | Loss: 0.00000901
Iteration 33/1000 | Loss: 0.00000900
Iteration 34/1000 | Loss: 0.00000900
Iteration 35/1000 | Loss: 0.00000900
Iteration 36/1000 | Loss: 0.00000899
Iteration 37/1000 | Loss: 0.00000899
Iteration 38/1000 | Loss: 0.00000899
Iteration 39/1000 | Loss: 0.00000899
Iteration 40/1000 | Loss: 0.00000898
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000898
Iteration 43/1000 | Loss: 0.00000898
Iteration 44/1000 | Loss: 0.00000897
Iteration 45/1000 | Loss: 0.00000897
Iteration 46/1000 | Loss: 0.00000897
Iteration 47/1000 | Loss: 0.00000896
Iteration 48/1000 | Loss: 0.00000896
Iteration 49/1000 | Loss: 0.00000896
Iteration 50/1000 | Loss: 0.00000895
Iteration 51/1000 | Loss: 0.00000895
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000894
Iteration 55/1000 | Loss: 0.00000894
Iteration 56/1000 | Loss: 0.00000893
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000892
Iteration 60/1000 | Loss: 0.00000891
Iteration 61/1000 | Loss: 0.00000891
Iteration 62/1000 | Loss: 0.00000891
Iteration 63/1000 | Loss: 0.00000890
Iteration 64/1000 | Loss: 0.00000890
Iteration 65/1000 | Loss: 0.00000890
Iteration 66/1000 | Loss: 0.00000889
Iteration 67/1000 | Loss: 0.00000889
Iteration 68/1000 | Loss: 0.00000889
Iteration 69/1000 | Loss: 0.00000889
Iteration 70/1000 | Loss: 0.00000889
Iteration 71/1000 | Loss: 0.00000888
Iteration 72/1000 | Loss: 0.00000888
Iteration 73/1000 | Loss: 0.00000888
Iteration 74/1000 | Loss: 0.00000888
Iteration 75/1000 | Loss: 0.00000888
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000887
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000886
Iteration 89/1000 | Loss: 0.00000886
Iteration 90/1000 | Loss: 0.00000886
Iteration 91/1000 | Loss: 0.00000886
Iteration 92/1000 | Loss: 0.00000886
Iteration 93/1000 | Loss: 0.00000886
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000885
Iteration 98/1000 | Loss: 0.00000885
Iteration 99/1000 | Loss: 0.00000885
Iteration 100/1000 | Loss: 0.00000884
Iteration 101/1000 | Loss: 0.00000884
Iteration 102/1000 | Loss: 0.00000884
Iteration 103/1000 | Loss: 0.00000884
Iteration 104/1000 | Loss: 0.00000884
Iteration 105/1000 | Loss: 0.00000884
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000883
Iteration 108/1000 | Loss: 0.00000883
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000883
Iteration 113/1000 | Loss: 0.00000883
Iteration 114/1000 | Loss: 0.00000883
Iteration 115/1000 | Loss: 0.00000883
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000883
Iteration 121/1000 | Loss: 0.00000883
Iteration 122/1000 | Loss: 0.00000882
Iteration 123/1000 | Loss: 0.00000882
Iteration 124/1000 | Loss: 0.00000882
Iteration 125/1000 | Loss: 0.00000882
Iteration 126/1000 | Loss: 0.00000881
Iteration 127/1000 | Loss: 0.00000881
Iteration 128/1000 | Loss: 0.00000881
Iteration 129/1000 | Loss: 0.00000881
Iteration 130/1000 | Loss: 0.00000880
Iteration 131/1000 | Loss: 0.00000880
Iteration 132/1000 | Loss: 0.00000880
Iteration 133/1000 | Loss: 0.00000880
Iteration 134/1000 | Loss: 0.00000879
Iteration 135/1000 | Loss: 0.00000879
Iteration 136/1000 | Loss: 0.00000879
Iteration 137/1000 | Loss: 0.00000879
Iteration 138/1000 | Loss: 0.00000879
Iteration 139/1000 | Loss: 0.00000879
Iteration 140/1000 | Loss: 0.00000878
Iteration 141/1000 | Loss: 0.00000878
Iteration 142/1000 | Loss: 0.00000878
Iteration 143/1000 | Loss: 0.00000878
Iteration 144/1000 | Loss: 0.00000878
Iteration 145/1000 | Loss: 0.00000878
Iteration 146/1000 | Loss: 0.00000877
Iteration 147/1000 | Loss: 0.00000877
Iteration 148/1000 | Loss: 0.00000877
Iteration 149/1000 | Loss: 0.00000877
Iteration 150/1000 | Loss: 0.00000877
Iteration 151/1000 | Loss: 0.00000877
Iteration 152/1000 | Loss: 0.00000877
Iteration 153/1000 | Loss: 0.00000877
Iteration 154/1000 | Loss: 0.00000876
Iteration 155/1000 | Loss: 0.00000876
Iteration 156/1000 | Loss: 0.00000876
Iteration 157/1000 | Loss: 0.00000876
Iteration 158/1000 | Loss: 0.00000875
Iteration 159/1000 | Loss: 0.00000875
Iteration 160/1000 | Loss: 0.00000875
Iteration 161/1000 | Loss: 0.00000874
Iteration 162/1000 | Loss: 0.00000874
Iteration 163/1000 | Loss: 0.00000874
Iteration 164/1000 | Loss: 0.00000873
Iteration 165/1000 | Loss: 0.00000873
Iteration 166/1000 | Loss: 0.00000873
Iteration 167/1000 | Loss: 0.00000873
Iteration 168/1000 | Loss: 0.00000872
Iteration 169/1000 | Loss: 0.00000872
Iteration 170/1000 | Loss: 0.00000872
Iteration 171/1000 | Loss: 0.00000872
Iteration 172/1000 | Loss: 0.00000872
Iteration 173/1000 | Loss: 0.00000872
Iteration 174/1000 | Loss: 0.00000871
Iteration 175/1000 | Loss: 0.00000871
Iteration 176/1000 | Loss: 0.00000871
Iteration 177/1000 | Loss: 0.00000871
Iteration 178/1000 | Loss: 0.00000871
Iteration 179/1000 | Loss: 0.00000871
Iteration 180/1000 | Loss: 0.00000871
Iteration 181/1000 | Loss: 0.00000871
Iteration 182/1000 | Loss: 0.00000871
Iteration 183/1000 | Loss: 0.00000871
Iteration 184/1000 | Loss: 0.00000871
Iteration 185/1000 | Loss: 0.00000871
Iteration 186/1000 | Loss: 0.00000871
Iteration 187/1000 | Loss: 0.00000871
Iteration 188/1000 | Loss: 0.00000871
Iteration 189/1000 | Loss: 0.00000871
Iteration 190/1000 | Loss: 0.00000871
Iteration 191/1000 | Loss: 0.00000871
Iteration 192/1000 | Loss: 0.00000871
Iteration 193/1000 | Loss: 0.00000871
Iteration 194/1000 | Loss: 0.00000871
Iteration 195/1000 | Loss: 0.00000871
Iteration 196/1000 | Loss: 0.00000871
Iteration 197/1000 | Loss: 0.00000871
Iteration 198/1000 | Loss: 0.00000871
Iteration 199/1000 | Loss: 0.00000871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [8.71104930411093e-06, 8.71104930411093e-06, 8.71104930411093e-06, 8.71104930411093e-06, 8.71104930411093e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.71104930411093e-06

Optimization complete. Final v2v error: 2.4957964420318604 mm

Highest mean error: 2.9280271530151367 mm for frame 88

Lowest mean error: 2.253877878189087 mm for frame 63

Saving results

Total time: 291.94205021858215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1002
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845982
Iteration 2/25 | Loss: 0.00106209
Iteration 3/25 | Loss: 0.00089869
Iteration 4/25 | Loss: 0.00086584
Iteration 5/25 | Loss: 0.00085269
Iteration 6/25 | Loss: 0.00084930
Iteration 7/25 | Loss: 0.00084865
Iteration 8/25 | Loss: 0.00084865
Iteration 9/25 | Loss: 0.00084865
Iteration 10/25 | Loss: 0.00084865
Iteration 11/25 | Loss: 0.00084865
Iteration 12/25 | Loss: 0.00084865
Iteration 13/25 | Loss: 0.00084865
Iteration 14/25 | Loss: 0.00084865
Iteration 15/25 | Loss: 0.00084865
Iteration 16/25 | Loss: 0.00084865
Iteration 17/25 | Loss: 0.00084865
Iteration 18/25 | Loss: 0.00084865
Iteration 19/25 | Loss: 0.00084865
Iteration 20/25 | Loss: 0.00084865
Iteration 21/25 | Loss: 0.00084865
Iteration 22/25 | Loss: 0.00084865
Iteration 23/25 | Loss: 0.00084865
Iteration 24/25 | Loss: 0.00084865
Iteration 25/25 | Loss: 0.00084865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54625773
Iteration 2/25 | Loss: 0.00048979
Iteration 3/25 | Loss: 0.00048979
Iteration 4/25 | Loss: 0.00048979
Iteration 5/25 | Loss: 0.00048979
Iteration 6/25 | Loss: 0.00048979
Iteration 7/25 | Loss: 0.00048979
Iteration 8/25 | Loss: 0.00048978
Iteration 9/25 | Loss: 0.00048978
Iteration 10/25 | Loss: 0.00048978
Iteration 11/25 | Loss: 0.00048978
Iteration 12/25 | Loss: 0.00048978
Iteration 13/25 | Loss: 0.00048978
Iteration 14/25 | Loss: 0.00048978
Iteration 15/25 | Loss: 0.00048978
Iteration 16/25 | Loss: 0.00048978
Iteration 17/25 | Loss: 0.00048978
Iteration 18/25 | Loss: 0.00048978
Iteration 19/25 | Loss: 0.00048978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004897842300124466, 0.0004897842300124466, 0.0004897842300124466, 0.0004897842300124466, 0.0004897842300124466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004897842300124466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048978
Iteration 2/1000 | Loss: 0.00004772
Iteration 3/1000 | Loss: 0.00002898
Iteration 4/1000 | Loss: 0.00002337
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001771
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001739
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001733
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001722
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001707
Iteration 132/1000 | Loss: 0.00001707
Iteration 133/1000 | Loss: 0.00001707
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001706
Iteration 138/1000 | Loss: 0.00001706
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001706
Iteration 141/1000 | Loss: 0.00001706
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Iteration 151/1000 | Loss: 0.00001705
Iteration 152/1000 | Loss: 0.00001705
Iteration 153/1000 | Loss: 0.00001705
Iteration 154/1000 | Loss: 0.00001705
Iteration 155/1000 | Loss: 0.00001705
Iteration 156/1000 | Loss: 0.00001705
Iteration 157/1000 | Loss: 0.00001705
Iteration 158/1000 | Loss: 0.00001705
Iteration 159/1000 | Loss: 0.00001705
Iteration 160/1000 | Loss: 0.00001705
Iteration 161/1000 | Loss: 0.00001705
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001704
Iteration 164/1000 | Loss: 0.00001704
Iteration 165/1000 | Loss: 0.00001704
Iteration 166/1000 | Loss: 0.00001704
Iteration 167/1000 | Loss: 0.00001704
Iteration 168/1000 | Loss: 0.00001704
Iteration 169/1000 | Loss: 0.00001704
Iteration 170/1000 | Loss: 0.00001704
Iteration 171/1000 | Loss: 0.00001704
Iteration 172/1000 | Loss: 0.00001704
Iteration 173/1000 | Loss: 0.00001704
Iteration 174/1000 | Loss: 0.00001704
Iteration 175/1000 | Loss: 0.00001704
Iteration 176/1000 | Loss: 0.00001704
Iteration 177/1000 | Loss: 0.00001704
Iteration 178/1000 | Loss: 0.00001704
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001704
Iteration 182/1000 | Loss: 0.00001704
Iteration 183/1000 | Loss: 0.00001704
Iteration 184/1000 | Loss: 0.00001704
Iteration 185/1000 | Loss: 0.00001703
Iteration 186/1000 | Loss: 0.00001703
Iteration 187/1000 | Loss: 0.00001703
Iteration 188/1000 | Loss: 0.00001703
Iteration 189/1000 | Loss: 0.00001703
Iteration 190/1000 | Loss: 0.00001703
Iteration 191/1000 | Loss: 0.00001703
Iteration 192/1000 | Loss: 0.00001703
Iteration 193/1000 | Loss: 0.00001703
Iteration 194/1000 | Loss: 0.00001703
Iteration 195/1000 | Loss: 0.00001703
Iteration 196/1000 | Loss: 0.00001703
Iteration 197/1000 | Loss: 0.00001703
Iteration 198/1000 | Loss: 0.00001703
Iteration 199/1000 | Loss: 0.00001703
Iteration 200/1000 | Loss: 0.00001703
Iteration 201/1000 | Loss: 0.00001703
Iteration 202/1000 | Loss: 0.00001703
Iteration 203/1000 | Loss: 0.00001703
Iteration 204/1000 | Loss: 0.00001703
Iteration 205/1000 | Loss: 0.00001703
Iteration 206/1000 | Loss: 0.00001702
Iteration 207/1000 | Loss: 0.00001702
Iteration 208/1000 | Loss: 0.00001702
Iteration 209/1000 | Loss: 0.00001702
Iteration 210/1000 | Loss: 0.00001702
Iteration 211/1000 | Loss: 0.00001702
Iteration 212/1000 | Loss: 0.00001702
Iteration 213/1000 | Loss: 0.00001702
Iteration 214/1000 | Loss: 0.00001702
Iteration 215/1000 | Loss: 0.00001702
Iteration 216/1000 | Loss: 0.00001702
Iteration 217/1000 | Loss: 0.00001702
Iteration 218/1000 | Loss: 0.00001702
Iteration 219/1000 | Loss: 0.00001702
Iteration 220/1000 | Loss: 0.00001702
Iteration 221/1000 | Loss: 0.00001702
Iteration 222/1000 | Loss: 0.00001702
Iteration 223/1000 | Loss: 0.00001701
Iteration 224/1000 | Loss: 0.00001701
Iteration 225/1000 | Loss: 0.00001701
Iteration 226/1000 | Loss: 0.00001701
Iteration 227/1000 | Loss: 0.00001701
Iteration 228/1000 | Loss: 0.00001701
Iteration 229/1000 | Loss: 0.00001701
Iteration 230/1000 | Loss: 0.00001701
Iteration 231/1000 | Loss: 0.00001701
Iteration 232/1000 | Loss: 0.00001701
Iteration 233/1000 | Loss: 0.00001701
Iteration 234/1000 | Loss: 0.00001701
Iteration 235/1000 | Loss: 0.00001701
Iteration 236/1000 | Loss: 0.00001701
Iteration 237/1000 | Loss: 0.00001701
Iteration 238/1000 | Loss: 0.00001701
Iteration 239/1000 | Loss: 0.00001701
Iteration 240/1000 | Loss: 0.00001701
Iteration 241/1000 | Loss: 0.00001701
Iteration 242/1000 | Loss: 0.00001701
Iteration 243/1000 | Loss: 0.00001701
Iteration 244/1000 | Loss: 0.00001701
Iteration 245/1000 | Loss: 0.00001700
Iteration 246/1000 | Loss: 0.00001700
Iteration 247/1000 | Loss: 0.00001700
Iteration 248/1000 | Loss: 0.00001700
Iteration 249/1000 | Loss: 0.00001700
Iteration 250/1000 | Loss: 0.00001700
Iteration 251/1000 | Loss: 0.00001700
Iteration 252/1000 | Loss: 0.00001700
Iteration 253/1000 | Loss: 0.00001700
Iteration 254/1000 | Loss: 0.00001700
Iteration 255/1000 | Loss: 0.00001700
Iteration 256/1000 | Loss: 0.00001700
Iteration 257/1000 | Loss: 0.00001700
Iteration 258/1000 | Loss: 0.00001700
Iteration 259/1000 | Loss: 0.00001700
Iteration 260/1000 | Loss: 0.00001700
Iteration 261/1000 | Loss: 0.00001700
Iteration 262/1000 | Loss: 0.00001700
Iteration 263/1000 | Loss: 0.00001700
Iteration 264/1000 | Loss: 0.00001700
Iteration 265/1000 | Loss: 0.00001700
Iteration 266/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.7001233572955243e-05, 1.7001233572955243e-05, 1.7001233572955243e-05, 1.7001233572955243e-05, 1.7001233572955243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7001233572955243e-05

Optimization complete. Final v2v error: 3.283766031265259 mm

Highest mean error: 5.697433948516846 mm for frame 59

Lowest mean error: 2.2143311500549316 mm for frame 46

Saving results

Total time: 345.4028332233429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1031
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816723
Iteration 2/25 | Loss: 0.00168631
Iteration 3/25 | Loss: 0.00109269
Iteration 4/25 | Loss: 0.00100657
Iteration 5/25 | Loss: 0.00099768
Iteration 6/25 | Loss: 0.00099668
Iteration 7/25 | Loss: 0.00099642
Iteration 8/25 | Loss: 0.00099642
Iteration 9/25 | Loss: 0.00099642
Iteration 10/25 | Loss: 0.00099642
Iteration 11/25 | Loss: 0.00099642
Iteration 12/25 | Loss: 0.00099642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009964172495529056, 0.0009964172495529056, 0.0009964172495529056, 0.0009964172495529056, 0.0009964172495529056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009964172495529056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25501013
Iteration 2/25 | Loss: 0.00029241
Iteration 3/25 | Loss: 0.00029238
Iteration 4/25 | Loss: 0.00029238
Iteration 5/25 | Loss: 0.00029238
Iteration 6/25 | Loss: 0.00029238
Iteration 7/25 | Loss: 0.00029238
Iteration 8/25 | Loss: 0.00029238
Iteration 9/25 | Loss: 0.00029238
Iteration 10/25 | Loss: 0.00029238
Iteration 11/25 | Loss: 0.00029238
Iteration 12/25 | Loss: 0.00029238
Iteration 13/25 | Loss: 0.00029238
Iteration 14/25 | Loss: 0.00029238
Iteration 15/25 | Loss: 0.00029238
Iteration 16/25 | Loss: 0.00029238
Iteration 17/25 | Loss: 0.00029238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00029237644048407674, 0.00029237644048407674, 0.00029237644048407674, 0.00029237644048407674, 0.00029237644048407674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029237644048407674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029238
Iteration 2/1000 | Loss: 0.00003645
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001798
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001650
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001604
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001572
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001571
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001557
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001555
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001553
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001549
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001549
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001548
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001547
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.5453832020284608e-05, 1.5453832020284608e-05, 1.5453832020284608e-05, 1.5453832020284608e-05, 1.5453832020284608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5453832020284608e-05

Optimization complete. Final v2v error: 3.282886028289795 mm

Highest mean error: 3.5829293727874756 mm for frame 112

Lowest mean error: 3.0197982788085938 mm for frame 45

Saving results

Total time: 233.32692956924438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1039
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076142
Iteration 2/25 | Loss: 0.01076142
Iteration 3/25 | Loss: 0.01076142
Iteration 4/25 | Loss: 0.01076142
Iteration 5/25 | Loss: 0.01076142
Iteration 6/25 | Loss: 0.01076142
Iteration 7/25 | Loss: 0.01076142
Iteration 8/25 | Loss: 0.01076141
Iteration 9/25 | Loss: 0.01076141
Iteration 10/25 | Loss: 0.01076141
Iteration 11/25 | Loss: 0.01076141
Iteration 12/25 | Loss: 0.01076141
Iteration 13/25 | Loss: 0.01076141
Iteration 14/25 | Loss: 0.01076141
Iteration 15/25 | Loss: 0.01076141
Iteration 16/25 | Loss: 0.01076141
Iteration 17/25 | Loss: 0.01076141
Iteration 18/25 | Loss: 0.01076141
Iteration 19/25 | Loss: 0.01076141
Iteration 20/25 | Loss: 0.01076140
Iteration 21/25 | Loss: 0.01076140
Iteration 22/25 | Loss: 0.01076140
Iteration 23/25 | Loss: 0.01076140
Iteration 24/25 | Loss: 0.01076140
Iteration 25/25 | Loss: 0.01076140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35708487
Iteration 2/25 | Loss: 0.16789503
Iteration 3/25 | Loss: 0.16646439
Iteration 4/25 | Loss: 0.16562800
Iteration 5/25 | Loss: 0.16540591
Iteration 6/25 | Loss: 0.16537455
Iteration 7/25 | Loss: 0.16537455
Iteration 8/25 | Loss: 0.16537455
Iteration 9/25 | Loss: 0.16537453
Iteration 10/25 | Loss: 0.16537452
Iteration 11/25 | Loss: 0.16537452
Iteration 12/25 | Loss: 0.16537450
Iteration 13/25 | Loss: 0.16537450
Iteration 14/25 | Loss: 0.16537450
Iteration 15/25 | Loss: 0.16537450
Iteration 16/25 | Loss: 0.16537450
Iteration 17/25 | Loss: 0.16537450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.1653745025396347, 0.1653745025396347, 0.1653745025396347, 0.1653745025396347, 0.1653745025396347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1653745025396347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16537450
Iteration 2/1000 | Loss: 0.00360145
Iteration 3/1000 | Loss: 0.00029854
Iteration 4/1000 | Loss: 0.00071331
Iteration 5/1000 | Loss: 0.00006489
Iteration 6/1000 | Loss: 0.00004276
Iteration 7/1000 | Loss: 0.00003301
Iteration 8/1000 | Loss: 0.00002750
Iteration 9/1000 | Loss: 0.00002243
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001759
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001097
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001090
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001086
Iteration 27/1000 | Loss: 0.00001078
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001074
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001074
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001073
Iteration 37/1000 | Loss: 0.00001073
Iteration 38/1000 | Loss: 0.00001072
Iteration 39/1000 | Loss: 0.00001071
Iteration 40/1000 | Loss: 0.00001070
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001066
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001063
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001063
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001061
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001059
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001057
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001055
Iteration 76/1000 | Loss: 0.00001055
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001054
Iteration 79/1000 | Loss: 0.00001054
Iteration 80/1000 | Loss: 0.00001054
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001052
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001052
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001050
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001049
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001046
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001045
Iteration 106/1000 | Loss: 0.00001045
Iteration 107/1000 | Loss: 0.00001045
Iteration 108/1000 | Loss: 0.00001044
Iteration 109/1000 | Loss: 0.00001044
Iteration 110/1000 | Loss: 0.00001044
Iteration 111/1000 | Loss: 0.00001044
Iteration 112/1000 | Loss: 0.00001043
Iteration 113/1000 | Loss: 0.00001042
Iteration 114/1000 | Loss: 0.00001042
Iteration 115/1000 | Loss: 0.00001042
Iteration 116/1000 | Loss: 0.00001042
Iteration 117/1000 | Loss: 0.00001042
Iteration 118/1000 | Loss: 0.00001042
Iteration 119/1000 | Loss: 0.00001042
Iteration 120/1000 | Loss: 0.00001042
Iteration 121/1000 | Loss: 0.00001042
Iteration 122/1000 | Loss: 0.00001042
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001041
Iteration 127/1000 | Loss: 0.00001040
Iteration 128/1000 | Loss: 0.00001040
Iteration 129/1000 | Loss: 0.00001040
Iteration 130/1000 | Loss: 0.00001040
Iteration 131/1000 | Loss: 0.00001040
Iteration 132/1000 | Loss: 0.00001040
Iteration 133/1000 | Loss: 0.00001040
Iteration 134/1000 | Loss: 0.00001040
Iteration 135/1000 | Loss: 0.00001039
Iteration 136/1000 | Loss: 0.00001039
Iteration 137/1000 | Loss: 0.00001039
Iteration 138/1000 | Loss: 0.00001039
Iteration 139/1000 | Loss: 0.00001038
Iteration 140/1000 | Loss: 0.00001038
Iteration 141/1000 | Loss: 0.00001038
Iteration 142/1000 | Loss: 0.00001038
Iteration 143/1000 | Loss: 0.00001038
Iteration 144/1000 | Loss: 0.00001038
Iteration 145/1000 | Loss: 0.00001038
Iteration 146/1000 | Loss: 0.00001038
Iteration 147/1000 | Loss: 0.00001038
Iteration 148/1000 | Loss: 0.00001038
Iteration 149/1000 | Loss: 0.00001038
Iteration 150/1000 | Loss: 0.00001038
Iteration 151/1000 | Loss: 0.00001038
Iteration 152/1000 | Loss: 0.00001038
Iteration 153/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.0381179890828207e-05, 1.0381179890828207e-05, 1.0381179890828207e-05, 1.0381179890828207e-05, 1.0381179890828207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0381179890828207e-05

Optimization complete. Final v2v error: 2.6777377128601074 mm

Highest mean error: 2.953237771987915 mm for frame 128

Lowest mean error: 2.318110466003418 mm for frame 222

Saving results

Total time: 389.7230749130249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1092
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755839
Iteration 2/25 | Loss: 0.00139663
Iteration 3/25 | Loss: 0.00101450
Iteration 4/25 | Loss: 0.00094966
Iteration 5/25 | Loss: 0.00092862
Iteration 6/25 | Loss: 0.00091939
Iteration 7/25 | Loss: 0.00091091
Iteration 8/25 | Loss: 0.00090423
Iteration 9/25 | Loss: 0.00090194
Iteration 10/25 | Loss: 0.00090073
Iteration 11/25 | Loss: 0.00089949
Iteration 12/25 | Loss: 0.00089882
Iteration 13/25 | Loss: 0.00089848
Iteration 14/25 | Loss: 0.00089836
Iteration 15/25 | Loss: 0.00089835
Iteration 16/25 | Loss: 0.00089834
Iteration 17/25 | Loss: 0.00089834
Iteration 18/25 | Loss: 0.00089834
Iteration 19/25 | Loss: 0.00089834
Iteration 20/25 | Loss: 0.00089834
Iteration 21/25 | Loss: 0.00089834
Iteration 22/25 | Loss: 0.00089834
Iteration 23/25 | Loss: 0.00089833
Iteration 24/25 | Loss: 0.00089833
Iteration 25/25 | Loss: 0.00089833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.86362743
Iteration 2/25 | Loss: 0.00060774
Iteration 3/25 | Loss: 0.00060762
Iteration 4/25 | Loss: 0.00060762
Iteration 5/25 | Loss: 0.00060762
Iteration 6/25 | Loss: 0.00060762
Iteration 7/25 | Loss: 0.00060762
Iteration 8/25 | Loss: 0.00060762
Iteration 9/25 | Loss: 0.00060762
Iteration 10/25 | Loss: 0.00060762
Iteration 11/25 | Loss: 0.00060762
Iteration 12/25 | Loss: 0.00060762
Iteration 13/25 | Loss: 0.00060762
Iteration 14/25 | Loss: 0.00060762
Iteration 15/25 | Loss: 0.00060762
Iteration 16/25 | Loss: 0.00060762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006076153367757797, 0.0006076153367757797, 0.0006076153367757797, 0.0006076153367757797, 0.0006076153367757797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006076153367757797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060762
Iteration 2/1000 | Loss: 0.00009596
Iteration 3/1000 | Loss: 0.00007137
Iteration 4/1000 | Loss: 0.00009196
Iteration 5/1000 | Loss: 0.00017005
Iteration 6/1000 | Loss: 0.00009162
Iteration 7/1000 | Loss: 0.00003006
Iteration 8/1000 | Loss: 0.00002472
Iteration 9/1000 | Loss: 0.00007353
Iteration 10/1000 | Loss: 0.00010592
Iteration 11/1000 | Loss: 0.00002297
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00006438
Iteration 15/1000 | Loss: 0.00003535
Iteration 16/1000 | Loss: 0.00002602
Iteration 17/1000 | Loss: 0.00002706
Iteration 18/1000 | Loss: 0.00003350
Iteration 19/1000 | Loss: 0.00002301
Iteration 20/1000 | Loss: 0.00002078
Iteration 21/1000 | Loss: 0.00002484
Iteration 22/1000 | Loss: 0.00002529
Iteration 23/1000 | Loss: 0.00002728
Iteration 24/1000 | Loss: 0.00002666
Iteration 25/1000 | Loss: 0.00003693
Iteration 26/1000 | Loss: 0.00002708
Iteration 27/1000 | Loss: 0.00002430
Iteration 28/1000 | Loss: 0.00003603
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00002971
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00002865
Iteration 33/1000 | Loss: 0.00004390
Iteration 34/1000 | Loss: 0.00003670
Iteration 35/1000 | Loss: 0.00004047
Iteration 36/1000 | Loss: 0.00002982
Iteration 37/1000 | Loss: 0.00003295
Iteration 38/1000 | Loss: 0.00003157
Iteration 39/1000 | Loss: 0.00003136
Iteration 40/1000 | Loss: 0.00003309
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00004104
Iteration 43/1000 | Loss: 0.00003471
Iteration 44/1000 | Loss: 0.00002641
Iteration 45/1000 | Loss: 0.00004118
Iteration 46/1000 | Loss: 0.00002517
Iteration 47/1000 | Loss: 0.00004082
Iteration 48/1000 | Loss: 0.00004186
Iteration 49/1000 | Loss: 0.00004258
Iteration 50/1000 | Loss: 0.00004350
Iteration 51/1000 | Loss: 0.00004705
Iteration 52/1000 | Loss: 0.00004286
Iteration 53/1000 | Loss: 0.00003169
Iteration 54/1000 | Loss: 0.00002272
Iteration 55/1000 | Loss: 0.00003658
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002010
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001534
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001500
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001499
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001498
Iteration 80/1000 | Loss: 0.00001497
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001497
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001495
Iteration 91/1000 | Loss: 0.00001495
Iteration 92/1000 | Loss: 0.00001495
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001494
Iteration 95/1000 | Loss: 0.00001494
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001493
Iteration 100/1000 | Loss: 0.00001493
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001485
Iteration 109/1000 | Loss: 0.00001480
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001465
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001462
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001461
Iteration 130/1000 | Loss: 0.00001461
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001460
Iteration 133/1000 | Loss: 0.00001460
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001459
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001457
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001456
Iteration 151/1000 | Loss: 0.00001456
Iteration 152/1000 | Loss: 0.00001456
Iteration 153/1000 | Loss: 0.00001456
Iteration 154/1000 | Loss: 0.00001456
Iteration 155/1000 | Loss: 0.00001456
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001455
Iteration 160/1000 | Loss: 0.00001455
Iteration 161/1000 | Loss: 0.00001455
Iteration 162/1000 | Loss: 0.00001455
Iteration 163/1000 | Loss: 0.00001455
Iteration 164/1000 | Loss: 0.00001455
Iteration 165/1000 | Loss: 0.00001455
Iteration 166/1000 | Loss: 0.00001455
Iteration 167/1000 | Loss: 0.00001455
Iteration 168/1000 | Loss: 0.00001454
Iteration 169/1000 | Loss: 0.00001454
Iteration 170/1000 | Loss: 0.00001454
Iteration 171/1000 | Loss: 0.00001454
Iteration 172/1000 | Loss: 0.00001454
Iteration 173/1000 | Loss: 0.00001454
Iteration 174/1000 | Loss: 0.00001453
Iteration 175/1000 | Loss: 0.00001453
Iteration 176/1000 | Loss: 0.00001453
Iteration 177/1000 | Loss: 0.00001453
Iteration 178/1000 | Loss: 0.00001453
Iteration 179/1000 | Loss: 0.00001453
Iteration 180/1000 | Loss: 0.00001453
Iteration 181/1000 | Loss: 0.00001452
Iteration 182/1000 | Loss: 0.00001452
Iteration 183/1000 | Loss: 0.00001452
Iteration 184/1000 | Loss: 0.00001452
Iteration 185/1000 | Loss: 0.00001452
Iteration 186/1000 | Loss: 0.00001452
Iteration 187/1000 | Loss: 0.00001452
Iteration 188/1000 | Loss: 0.00001452
Iteration 189/1000 | Loss: 0.00001451
Iteration 190/1000 | Loss: 0.00001451
Iteration 191/1000 | Loss: 0.00001451
Iteration 192/1000 | Loss: 0.00001451
Iteration 193/1000 | Loss: 0.00001451
Iteration 194/1000 | Loss: 0.00001451
Iteration 195/1000 | Loss: 0.00001451
Iteration 196/1000 | Loss: 0.00001451
Iteration 197/1000 | Loss: 0.00001451
Iteration 198/1000 | Loss: 0.00001451
Iteration 199/1000 | Loss: 0.00001451
Iteration 200/1000 | Loss: 0.00001451
Iteration 201/1000 | Loss: 0.00001451
Iteration 202/1000 | Loss: 0.00001451
Iteration 203/1000 | Loss: 0.00001451
Iteration 204/1000 | Loss: 0.00001451
Iteration 205/1000 | Loss: 0.00001451
Iteration 206/1000 | Loss: 0.00001450
Iteration 207/1000 | Loss: 0.00001450
Iteration 208/1000 | Loss: 0.00001450
Iteration 209/1000 | Loss: 0.00001450
Iteration 210/1000 | Loss: 0.00001450
Iteration 211/1000 | Loss: 0.00001450
Iteration 212/1000 | Loss: 0.00001450
Iteration 213/1000 | Loss: 0.00001450
Iteration 214/1000 | Loss: 0.00001450
Iteration 215/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.4504294995276723e-05, 1.4504294995276723e-05, 1.4504294995276723e-05, 1.4504294995276723e-05, 1.4504294995276723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4504294995276723e-05

Optimization complete. Final v2v error: 3.0989928245544434 mm

Highest mean error: 5.112680912017822 mm for frame 194

Lowest mean error: 2.268242835998535 mm for frame 117

Saving results

Total time: 1340.382928609848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1091
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713173
Iteration 2/25 | Loss: 0.00121898
Iteration 3/25 | Loss: 0.00090646
Iteration 4/25 | Loss: 0.00086287
Iteration 5/25 | Loss: 0.00085722
Iteration 6/25 | Loss: 0.00085563
Iteration 7/25 | Loss: 0.00085563
Iteration 8/25 | Loss: 0.00085563
Iteration 9/25 | Loss: 0.00085563
Iteration 10/25 | Loss: 0.00085563
Iteration 11/25 | Loss: 0.00085563
Iteration 12/25 | Loss: 0.00085563
Iteration 13/25 | Loss: 0.00085563
Iteration 14/25 | Loss: 0.00085563
Iteration 15/25 | Loss: 0.00085563
Iteration 16/25 | Loss: 0.00085563
Iteration 17/25 | Loss: 0.00085563
Iteration 18/25 | Loss: 0.00085563
Iteration 19/25 | Loss: 0.00085563
Iteration 20/25 | Loss: 0.00085563
Iteration 21/25 | Loss: 0.00085563
Iteration 22/25 | Loss: 0.00085563
Iteration 23/25 | Loss: 0.00085563
Iteration 24/25 | Loss: 0.00085563
Iteration 25/25 | Loss: 0.00085563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008556316024623811, 0.0008556316024623811, 0.0008556316024623811, 0.0008556316024623811, 0.0008556316024623811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008556316024623811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40018547
Iteration 2/25 | Loss: 0.00043354
Iteration 3/25 | Loss: 0.00043353
Iteration 4/25 | Loss: 0.00043353
Iteration 5/25 | Loss: 0.00043353
Iteration 6/25 | Loss: 0.00043353
Iteration 7/25 | Loss: 0.00043353
Iteration 8/25 | Loss: 0.00043353
Iteration 9/25 | Loss: 0.00043352
Iteration 10/25 | Loss: 0.00043352
Iteration 11/25 | Loss: 0.00043352
Iteration 12/25 | Loss: 0.00043352
Iteration 13/25 | Loss: 0.00043352
Iteration 14/25 | Loss: 0.00043352
Iteration 15/25 | Loss: 0.00043352
Iteration 16/25 | Loss: 0.00043352
Iteration 17/25 | Loss: 0.00043352
Iteration 18/25 | Loss: 0.00043352
Iteration 19/25 | Loss: 0.00043352
Iteration 20/25 | Loss: 0.00043352
Iteration 21/25 | Loss: 0.00043352
Iteration 22/25 | Loss: 0.00043352
Iteration 23/25 | Loss: 0.00043352
Iteration 24/25 | Loss: 0.00043352
Iteration 25/25 | Loss: 0.00043352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043352
Iteration 2/1000 | Loss: 0.00002122
Iteration 3/1000 | Loss: 0.00001429
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001208
Iteration 6/1000 | Loss: 0.00001140
Iteration 7/1000 | Loss: 0.00001108
Iteration 8/1000 | Loss: 0.00001083
Iteration 9/1000 | Loss: 0.00001061
Iteration 10/1000 | Loss: 0.00001050
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001044
Iteration 13/1000 | Loss: 0.00001043
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001041
Iteration 18/1000 | Loss: 0.00001039
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001032
Iteration 21/1000 | Loss: 0.00001031
Iteration 22/1000 | Loss: 0.00001030
Iteration 23/1000 | Loss: 0.00001030
Iteration 24/1000 | Loss: 0.00001029
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001028
Iteration 27/1000 | Loss: 0.00001028
Iteration 28/1000 | Loss: 0.00001028
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001027
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001026
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001025
Iteration 35/1000 | Loss: 0.00001025
Iteration 36/1000 | Loss: 0.00001025
Iteration 37/1000 | Loss: 0.00001025
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001025
Iteration 42/1000 | Loss: 0.00001025
Iteration 43/1000 | Loss: 0.00001025
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001025
Iteration 46/1000 | Loss: 0.00001025
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001025
Iteration 50/1000 | Loss: 0.00001025
Iteration 51/1000 | Loss: 0.00001025
Iteration 52/1000 | Loss: 0.00001025
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001025
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001025
Iteration 58/1000 | Loss: 0.00001025
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001025
Iteration 62/1000 | Loss: 0.00001025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.02494914244744e-05, 1.02494914244744e-05, 1.02494914244744e-05, 1.02494914244744e-05, 1.02494914244744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.02494914244744e-05

Optimization complete. Final v2v error: 2.763143539428711 mm

Highest mean error: 3.2766318321228027 mm for frame 233

Lowest mean error: 2.4467530250549316 mm for frame 166

Saving results

Total time: 376.7848074436188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1040
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378360
Iteration 2/25 | Loss: 0.00085746
Iteration 3/25 | Loss: 0.00079489
Iteration 4/25 | Loss: 0.00078883
Iteration 5/25 | Loss: 0.00078689
Iteration 6/25 | Loss: 0.00078650
Iteration 7/25 | Loss: 0.00078650
Iteration 8/25 | Loss: 0.00078650
Iteration 9/25 | Loss: 0.00078650
Iteration 10/25 | Loss: 0.00078650
Iteration 11/25 | Loss: 0.00078650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007865042425692081, 0.0007865042425692081, 0.0007865042425692081, 0.0007865042425692081, 0.0007865042425692081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007865042425692081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38001907
Iteration 2/25 | Loss: 0.00038544
Iteration 3/25 | Loss: 0.00038544
Iteration 4/25 | Loss: 0.00038544
Iteration 5/25 | Loss: 0.00038544
Iteration 6/25 | Loss: 0.00038544
Iteration 7/25 | Loss: 0.00038544
Iteration 8/25 | Loss: 0.00038544
Iteration 9/25 | Loss: 0.00038544
Iteration 10/25 | Loss: 0.00038544
Iteration 11/25 | Loss: 0.00038544
Iteration 12/25 | Loss: 0.00038544
Iteration 13/25 | Loss: 0.00038544
Iteration 14/25 | Loss: 0.00038544
Iteration 15/25 | Loss: 0.00038544
Iteration 16/25 | Loss: 0.00038544
Iteration 17/25 | Loss: 0.00038544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000385437102522701, 0.000385437102522701, 0.000385437102522701, 0.000385437102522701, 0.000385437102522701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000385437102522701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038544
Iteration 2/1000 | Loss: 0.00001813
Iteration 3/1000 | Loss: 0.00001299
Iteration 4/1000 | Loss: 0.00001129
Iteration 5/1000 | Loss: 0.00001062
Iteration 6/1000 | Loss: 0.00001009
Iteration 7/1000 | Loss: 0.00000974
Iteration 8/1000 | Loss: 0.00000957
Iteration 9/1000 | Loss: 0.00000946
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000932
Iteration 12/1000 | Loss: 0.00000930
Iteration 13/1000 | Loss: 0.00000930
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000926
Iteration 16/1000 | Loss: 0.00000925
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000925
Iteration 19/1000 | Loss: 0.00000924
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000921
Iteration 22/1000 | Loss: 0.00000920
Iteration 23/1000 | Loss: 0.00000920
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000919
Iteration 26/1000 | Loss: 0.00000918
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000916
Iteration 29/1000 | Loss: 0.00000915
Iteration 30/1000 | Loss: 0.00000915
Iteration 31/1000 | Loss: 0.00000915
Iteration 32/1000 | Loss: 0.00000915
Iteration 33/1000 | Loss: 0.00000915
Iteration 34/1000 | Loss: 0.00000914
Iteration 35/1000 | Loss: 0.00000914
Iteration 36/1000 | Loss: 0.00000914
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000914
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000914
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000913
Iteration 43/1000 | Loss: 0.00000913
Iteration 44/1000 | Loss: 0.00000913
Iteration 45/1000 | Loss: 0.00000913
Iteration 46/1000 | Loss: 0.00000913
Iteration 47/1000 | Loss: 0.00000913
Iteration 48/1000 | Loss: 0.00000912
Iteration 49/1000 | Loss: 0.00000912
Iteration 50/1000 | Loss: 0.00000912
Iteration 51/1000 | Loss: 0.00000912
Iteration 52/1000 | Loss: 0.00000912
Iteration 53/1000 | Loss: 0.00000912
Iteration 54/1000 | Loss: 0.00000912
Iteration 55/1000 | Loss: 0.00000911
Iteration 56/1000 | Loss: 0.00000911
Iteration 57/1000 | Loss: 0.00000911
Iteration 58/1000 | Loss: 0.00000911
Iteration 59/1000 | Loss: 0.00000911
Iteration 60/1000 | Loss: 0.00000910
Iteration 61/1000 | Loss: 0.00000909
Iteration 62/1000 | Loss: 0.00000909
Iteration 63/1000 | Loss: 0.00000909
Iteration 64/1000 | Loss: 0.00000909
Iteration 65/1000 | Loss: 0.00000909
Iteration 66/1000 | Loss: 0.00000909
Iteration 67/1000 | Loss: 0.00000909
Iteration 68/1000 | Loss: 0.00000909
Iteration 69/1000 | Loss: 0.00000909
Iteration 70/1000 | Loss: 0.00000909
Iteration 71/1000 | Loss: 0.00000909
Iteration 72/1000 | Loss: 0.00000908
Iteration 73/1000 | Loss: 0.00000908
Iteration 74/1000 | Loss: 0.00000908
Iteration 75/1000 | Loss: 0.00000908
Iteration 76/1000 | Loss: 0.00000908
Iteration 77/1000 | Loss: 0.00000908
Iteration 78/1000 | Loss: 0.00000908
Iteration 79/1000 | Loss: 0.00000908
Iteration 80/1000 | Loss: 0.00000908
Iteration 81/1000 | Loss: 0.00000908
Iteration 82/1000 | Loss: 0.00000908
Iteration 83/1000 | Loss: 0.00000908
Iteration 84/1000 | Loss: 0.00000908
Iteration 85/1000 | Loss: 0.00000908
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000908
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [9.079481060325634e-06, 9.079481060325634e-06, 9.079481060325634e-06, 9.079481060325634e-06, 9.079481060325634e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.079481060325634e-06

Optimization complete. Final v2v error: 2.553420066833496 mm

Highest mean error: 2.748997211456299 mm for frame 135

Lowest mean error: 2.4196009635925293 mm for frame 38

Saving results

Total time: 187.02191710472107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1060
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01140899
Iteration 2/25 | Loss: 0.00179147
Iteration 3/25 | Loss: 0.00110428
Iteration 4/25 | Loss: 0.00105735
Iteration 5/25 | Loss: 0.00104680
Iteration 6/25 | Loss: 0.00104337
Iteration 7/25 | Loss: 0.00104275
Iteration 8/25 | Loss: 0.00104275
Iteration 9/25 | Loss: 0.00104275
Iteration 10/25 | Loss: 0.00104275
Iteration 11/25 | Loss: 0.00104275
Iteration 12/25 | Loss: 0.00104275
Iteration 13/25 | Loss: 0.00104275
Iteration 14/25 | Loss: 0.00104275
Iteration 15/25 | Loss: 0.00104275
Iteration 16/25 | Loss: 0.00104275
Iteration 17/25 | Loss: 0.00104275
Iteration 18/25 | Loss: 0.00104275
Iteration 19/25 | Loss: 0.00104275
Iteration 20/25 | Loss: 0.00104275
Iteration 21/25 | Loss: 0.00104275
Iteration 22/25 | Loss: 0.00104275
Iteration 23/25 | Loss: 0.00104275
Iteration 24/25 | Loss: 0.00104275
Iteration 25/25 | Loss: 0.00104275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97302067
Iteration 2/25 | Loss: 0.00058882
Iteration 3/25 | Loss: 0.00058878
Iteration 4/25 | Loss: 0.00058878
Iteration 5/25 | Loss: 0.00058878
Iteration 6/25 | Loss: 0.00058878
Iteration 7/25 | Loss: 0.00058878
Iteration 8/25 | Loss: 0.00058878
Iteration 9/25 | Loss: 0.00058878
Iteration 10/25 | Loss: 0.00058878
Iteration 11/25 | Loss: 0.00058878
Iteration 12/25 | Loss: 0.00058878
Iteration 13/25 | Loss: 0.00058878
Iteration 14/25 | Loss: 0.00058878
Iteration 15/25 | Loss: 0.00058878
Iteration 16/25 | Loss: 0.00058878
Iteration 17/25 | Loss: 0.00058878
Iteration 18/25 | Loss: 0.00058878
Iteration 19/25 | Loss: 0.00058878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005887766601517797, 0.0005887766601517797, 0.0005887766601517797, 0.0005887766601517797, 0.0005887766601517797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005887766601517797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058878
Iteration 2/1000 | Loss: 0.00005255
Iteration 3/1000 | Loss: 0.00003400
Iteration 4/1000 | Loss: 0.00003012
Iteration 5/1000 | Loss: 0.00002843
Iteration 6/1000 | Loss: 0.00002761
Iteration 7/1000 | Loss: 0.00002697
Iteration 8/1000 | Loss: 0.00002655
Iteration 9/1000 | Loss: 0.00002617
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002567
Iteration 12/1000 | Loss: 0.00002548
Iteration 13/1000 | Loss: 0.00002533
Iteration 14/1000 | Loss: 0.00002524
Iteration 15/1000 | Loss: 0.00002520
Iteration 16/1000 | Loss: 0.00002519
Iteration 17/1000 | Loss: 0.00002519
Iteration 18/1000 | Loss: 0.00002514
Iteration 19/1000 | Loss: 0.00002502
Iteration 20/1000 | Loss: 0.00002499
Iteration 21/1000 | Loss: 0.00002494
Iteration 22/1000 | Loss: 0.00002493
Iteration 23/1000 | Loss: 0.00002486
Iteration 24/1000 | Loss: 0.00002483
Iteration 25/1000 | Loss: 0.00002482
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002480
Iteration 28/1000 | Loss: 0.00002479
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002479
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002478
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002474
Iteration 37/1000 | Loss: 0.00002474
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002473
Iteration 40/1000 | Loss: 0.00002471
Iteration 41/1000 | Loss: 0.00002471
Iteration 42/1000 | Loss: 0.00002471
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00002471
Iteration 45/1000 | Loss: 0.00002471
Iteration 46/1000 | Loss: 0.00002471
Iteration 47/1000 | Loss: 0.00002471
Iteration 48/1000 | Loss: 0.00002470
Iteration 49/1000 | Loss: 0.00002470
Iteration 50/1000 | Loss: 0.00002470
Iteration 51/1000 | Loss: 0.00002470
Iteration 52/1000 | Loss: 0.00002467
Iteration 53/1000 | Loss: 0.00002467
Iteration 54/1000 | Loss: 0.00002467
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002464
Iteration 58/1000 | Loss: 0.00002463
Iteration 59/1000 | Loss: 0.00002463
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002461
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002461
Iteration 66/1000 | Loss: 0.00002461
Iteration 67/1000 | Loss: 0.00002461
Iteration 68/1000 | Loss: 0.00002461
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002461
Iteration 72/1000 | Loss: 0.00002460
Iteration 73/1000 | Loss: 0.00002460
Iteration 74/1000 | Loss: 0.00002460
Iteration 75/1000 | Loss: 0.00002459
Iteration 76/1000 | Loss: 0.00002459
Iteration 77/1000 | Loss: 0.00002459
Iteration 78/1000 | Loss: 0.00002459
Iteration 79/1000 | Loss: 0.00002459
Iteration 80/1000 | Loss: 0.00002459
Iteration 81/1000 | Loss: 0.00002459
Iteration 82/1000 | Loss: 0.00002459
Iteration 83/1000 | Loss: 0.00002459
Iteration 84/1000 | Loss: 0.00002459
Iteration 85/1000 | Loss: 0.00002459
Iteration 86/1000 | Loss: 0.00002459
Iteration 87/1000 | Loss: 0.00002459
Iteration 88/1000 | Loss: 0.00002459
Iteration 89/1000 | Loss: 0.00002458
Iteration 90/1000 | Loss: 0.00002458
Iteration 91/1000 | Loss: 0.00002457
Iteration 92/1000 | Loss: 0.00002457
Iteration 93/1000 | Loss: 0.00002456
Iteration 94/1000 | Loss: 0.00002456
Iteration 95/1000 | Loss: 0.00002456
Iteration 96/1000 | Loss: 0.00002456
Iteration 97/1000 | Loss: 0.00002455
Iteration 98/1000 | Loss: 0.00002455
Iteration 99/1000 | Loss: 0.00002455
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002453
Iteration 105/1000 | Loss: 0.00002453
Iteration 106/1000 | Loss: 0.00002453
Iteration 107/1000 | Loss: 0.00002453
Iteration 108/1000 | Loss: 0.00002453
Iteration 109/1000 | Loss: 0.00002453
Iteration 110/1000 | Loss: 0.00002453
Iteration 111/1000 | Loss: 0.00002453
Iteration 112/1000 | Loss: 0.00002453
Iteration 113/1000 | Loss: 0.00002453
Iteration 114/1000 | Loss: 0.00002453
Iteration 115/1000 | Loss: 0.00002453
Iteration 116/1000 | Loss: 0.00002453
Iteration 117/1000 | Loss: 0.00002453
Iteration 118/1000 | Loss: 0.00002453
Iteration 119/1000 | Loss: 0.00002453
Iteration 120/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.4528691938030533e-05, 2.4528691938030533e-05, 2.4528691938030533e-05, 2.4528691938030533e-05, 2.4528691938030533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4528691938030533e-05

Optimization complete. Final v2v error: 3.991676092147827 mm

Highest mean error: 5.547776699066162 mm for frame 72

Lowest mean error: 3.100532293319702 mm for frame 32

Saving results

Total time: 519.9603703022003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1003
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930762
Iteration 2/25 | Loss: 0.00275142
Iteration 3/25 | Loss: 0.00206904
Iteration 4/25 | Loss: 0.00167765
Iteration 5/25 | Loss: 0.00149099
Iteration 6/25 | Loss: 0.00151745
Iteration 7/25 | Loss: 0.00148933
Iteration 8/25 | Loss: 0.00139576
Iteration 9/25 | Loss: 0.00134952
Iteration 10/25 | Loss: 0.00134634
Iteration 11/25 | Loss: 0.00131045
Iteration 12/25 | Loss: 0.00129423
Iteration 13/25 | Loss: 0.00132337
Iteration 14/25 | Loss: 0.00128646
Iteration 15/25 | Loss: 0.00126215
Iteration 16/25 | Loss: 0.00127873
Iteration 17/25 | Loss: 0.00125952
Iteration 18/25 | Loss: 0.00125455
Iteration 19/25 | Loss: 0.00125395
Iteration 20/25 | Loss: 0.00125170
Iteration 21/25 | Loss: 0.00124169
Iteration 22/25 | Loss: 0.00125018
Iteration 23/25 | Loss: 0.00122906
Iteration 24/25 | Loss: 0.00122685
Iteration 25/25 | Loss: 0.00123771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.34328365
Iteration 2/25 | Loss: 0.00360135
Iteration 3/25 | Loss: 0.00360134
Iteration 4/25 | Loss: 0.00360134
Iteration 5/25 | Loss: 0.00360134
Iteration 6/25 | Loss: 0.00360134
Iteration 7/25 | Loss: 0.00360134
Iteration 8/25 | Loss: 0.00360134
Iteration 9/25 | Loss: 0.00360134
Iteration 10/25 | Loss: 0.00360134
Iteration 11/25 | Loss: 0.00360134
Iteration 12/25 | Loss: 0.00360134
Iteration 13/25 | Loss: 0.00360134
Iteration 14/25 | Loss: 0.00360134
Iteration 15/25 | Loss: 0.00360134
Iteration 16/25 | Loss: 0.00360134
Iteration 17/25 | Loss: 0.00360134
Iteration 18/25 | Loss: 0.00360134
Iteration 19/25 | Loss: 0.00360134
Iteration 20/25 | Loss: 0.00360134
Iteration 21/25 | Loss: 0.00360134
Iteration 22/25 | Loss: 0.00360134
Iteration 23/25 | Loss: 0.00360134
Iteration 24/25 | Loss: 0.00360134
Iteration 25/25 | Loss: 0.00360134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360134
Iteration 2/1000 | Loss: 0.00061402
Iteration 3/1000 | Loss: 0.00094834
Iteration 4/1000 | Loss: 0.00177917
Iteration 5/1000 | Loss: 0.00329891
Iteration 6/1000 | Loss: 0.00703179
Iteration 7/1000 | Loss: 0.00059948
Iteration 8/1000 | Loss: 0.00278135
Iteration 9/1000 | Loss: 0.00536789
Iteration 10/1000 | Loss: 0.00041554
Iteration 11/1000 | Loss: 0.00877278
Iteration 12/1000 | Loss: 0.01630681
Iteration 13/1000 | Loss: 0.00624427
Iteration 14/1000 | Loss: 0.00400435
Iteration 15/1000 | Loss: 0.00264368
Iteration 16/1000 | Loss: 0.00330411
Iteration 17/1000 | Loss: 0.00116312
Iteration 18/1000 | Loss: 0.00339656
Iteration 19/1000 | Loss: 0.00403762
Iteration 20/1000 | Loss: 0.00297108
Iteration 21/1000 | Loss: 0.00191315
Iteration 22/1000 | Loss: 0.00194706
Iteration 23/1000 | Loss: 0.00106730
Iteration 24/1000 | Loss: 0.00028126
Iteration 25/1000 | Loss: 0.00018808
Iteration 26/1000 | Loss: 0.00103107
Iteration 27/1000 | Loss: 0.00020740
Iteration 28/1000 | Loss: 0.00084562
Iteration 29/1000 | Loss: 0.00052964
Iteration 30/1000 | Loss: 0.00033370
Iteration 31/1000 | Loss: 0.00047717
Iteration 32/1000 | Loss: 0.00052324
Iteration 33/1000 | Loss: 0.00049392
Iteration 34/1000 | Loss: 0.00030189
Iteration 35/1000 | Loss: 0.00079506
Iteration 36/1000 | Loss: 0.00024935
Iteration 37/1000 | Loss: 0.00056856
Iteration 38/1000 | Loss: 0.00260055
Iteration 39/1000 | Loss: 0.00554293
Iteration 40/1000 | Loss: 0.00225837
Iteration 41/1000 | Loss: 0.00303465
Iteration 42/1000 | Loss: 0.00392269
Iteration 43/1000 | Loss: 0.00274569
Iteration 44/1000 | Loss: 0.00613799
Iteration 45/1000 | Loss: 0.00482575
Iteration 46/1000 | Loss: 0.00295253
Iteration 47/1000 | Loss: 0.00215627
Iteration 48/1000 | Loss: 0.00045203
Iteration 49/1000 | Loss: 0.00017791
Iteration 50/1000 | Loss: 0.00014339
Iteration 51/1000 | Loss: 0.00043265
Iteration 52/1000 | Loss: 0.00030828
Iteration 53/1000 | Loss: 0.00032305
Iteration 54/1000 | Loss: 0.00047516
Iteration 55/1000 | Loss: 0.00144231
Iteration 56/1000 | Loss: 0.00105378
Iteration 57/1000 | Loss: 0.00073900
Iteration 58/1000 | Loss: 0.00042401
Iteration 59/1000 | Loss: 0.00061216
Iteration 60/1000 | Loss: 0.00043587
Iteration 61/1000 | Loss: 0.00030303
Iteration 62/1000 | Loss: 0.00013477
Iteration 63/1000 | Loss: 0.00011591
Iteration 64/1000 | Loss: 0.00049926
Iteration 65/1000 | Loss: 0.00120969
Iteration 66/1000 | Loss: 0.00065696
Iteration 67/1000 | Loss: 0.00112800
Iteration 68/1000 | Loss: 0.00089635
Iteration 69/1000 | Loss: 0.00072115
Iteration 70/1000 | Loss: 0.00042468
Iteration 71/1000 | Loss: 0.00062684
Iteration 72/1000 | Loss: 0.00024006
Iteration 73/1000 | Loss: 0.00120993
Iteration 74/1000 | Loss: 0.00050501
Iteration 75/1000 | Loss: 0.00248964
Iteration 76/1000 | Loss: 0.00146839
Iteration 77/1000 | Loss: 0.00229283
Iteration 78/1000 | Loss: 0.00357333
Iteration 79/1000 | Loss: 0.00153006
Iteration 80/1000 | Loss: 0.00136042
Iteration 81/1000 | Loss: 0.00173580
Iteration 82/1000 | Loss: 0.00171529
Iteration 83/1000 | Loss: 0.00186870
Iteration 84/1000 | Loss: 0.00175143
Iteration 85/1000 | Loss: 0.00065808
Iteration 86/1000 | Loss: 0.00056622
Iteration 87/1000 | Loss: 0.00032271
Iteration 88/1000 | Loss: 0.00088461
Iteration 89/1000 | Loss: 0.00137691
Iteration 90/1000 | Loss: 0.00070476
Iteration 91/1000 | Loss: 0.00090238
Iteration 92/1000 | Loss: 0.00079738
Iteration 93/1000 | Loss: 0.00055350
Iteration 94/1000 | Loss: 0.00031816
Iteration 95/1000 | Loss: 0.00092433
Iteration 96/1000 | Loss: 0.00062794
Iteration 97/1000 | Loss: 0.00032040
Iteration 98/1000 | Loss: 0.00034417
Iteration 99/1000 | Loss: 0.00009891
Iteration 100/1000 | Loss: 0.00107512
Iteration 101/1000 | Loss: 0.00459657
Iteration 102/1000 | Loss: 0.00266228
Iteration 103/1000 | Loss: 0.00304014
Iteration 104/1000 | Loss: 0.00233575
Iteration 105/1000 | Loss: 0.00192526
Iteration 106/1000 | Loss: 0.00039816
Iteration 107/1000 | Loss: 0.00023673
Iteration 108/1000 | Loss: 0.00098062
Iteration 109/1000 | Loss: 0.00110201
Iteration 110/1000 | Loss: 0.00189825
Iteration 111/1000 | Loss: 0.00186898
Iteration 112/1000 | Loss: 0.00167743
Iteration 113/1000 | Loss: 0.00145643
Iteration 114/1000 | Loss: 0.00534714
Iteration 115/1000 | Loss: 0.00392888
Iteration 116/1000 | Loss: 0.00379207
Iteration 117/1000 | Loss: 0.00414017
Iteration 118/1000 | Loss: 0.00355326
Iteration 119/1000 | Loss: 0.00147798
Iteration 120/1000 | Loss: 0.00094817
Iteration 121/1000 | Loss: 0.00238227
Iteration 122/1000 | Loss: 0.00030926
Iteration 123/1000 | Loss: 0.00054473
Iteration 124/1000 | Loss: 0.00105236
Iteration 125/1000 | Loss: 0.00180299
Iteration 126/1000 | Loss: 0.00181682
Iteration 127/1000 | Loss: 0.00117411
Iteration 128/1000 | Loss: 0.00164788
Iteration 129/1000 | Loss: 0.00200703
Iteration 130/1000 | Loss: 0.00123525
Iteration 131/1000 | Loss: 0.00076386
Iteration 132/1000 | Loss: 0.00390208
Iteration 133/1000 | Loss: 0.00296388
Iteration 134/1000 | Loss: 0.00184923
Iteration 135/1000 | Loss: 0.00172686
Iteration 136/1000 | Loss: 0.00458899
Iteration 137/1000 | Loss: 0.00247334
Iteration 138/1000 | Loss: 0.00247988
Iteration 139/1000 | Loss: 0.00083397
Iteration 140/1000 | Loss: 0.00088952
Iteration 141/1000 | Loss: 0.00085560
Iteration 142/1000 | Loss: 0.00031650
Iteration 143/1000 | Loss: 0.00013749
Iteration 144/1000 | Loss: 0.00053855
Iteration 145/1000 | Loss: 0.00170720
Iteration 146/1000 | Loss: 0.00100029
Iteration 147/1000 | Loss: 0.00089350
Iteration 148/1000 | Loss: 0.00054410
Iteration 149/1000 | Loss: 0.00036441
Iteration 150/1000 | Loss: 0.00094815
Iteration 151/1000 | Loss: 0.00036494
Iteration 152/1000 | Loss: 0.00075970
Iteration 153/1000 | Loss: 0.00048997
Iteration 154/1000 | Loss: 0.00064997
Iteration 155/1000 | Loss: 0.00033617
Iteration 156/1000 | Loss: 0.00026265
Iteration 157/1000 | Loss: 0.00018545
Iteration 158/1000 | Loss: 0.00026429
Iteration 159/1000 | Loss: 0.00005950
Iteration 160/1000 | Loss: 0.00005255
Iteration 161/1000 | Loss: 0.00030861
Iteration 162/1000 | Loss: 0.00046544
Iteration 163/1000 | Loss: 0.00030714
Iteration 164/1000 | Loss: 0.00042412
Iteration 165/1000 | Loss: 0.00023033
Iteration 166/1000 | Loss: 0.00015358
Iteration 167/1000 | Loss: 0.00042738
Iteration 168/1000 | Loss: 0.00053757
Iteration 169/1000 | Loss: 0.00060236
Iteration 170/1000 | Loss: 0.00044827
Iteration 171/1000 | Loss: 0.00044496
Iteration 172/1000 | Loss: 0.00043054
Iteration 173/1000 | Loss: 0.00040109
Iteration 174/1000 | Loss: 0.00030577
Iteration 175/1000 | Loss: 0.00048956
Iteration 176/1000 | Loss: 0.00022546
Iteration 177/1000 | Loss: 0.00012686
Iteration 178/1000 | Loss: 0.00010484
Iteration 179/1000 | Loss: 0.00004071
Iteration 180/1000 | Loss: 0.00003697
Iteration 181/1000 | Loss: 0.00003468
Iteration 182/1000 | Loss: 0.00003237
Iteration 183/1000 | Loss: 0.00003097
Iteration 184/1000 | Loss: 0.00007498
Iteration 185/1000 | Loss: 0.00032781
Iteration 186/1000 | Loss: 0.00011434
Iteration 187/1000 | Loss: 0.00003011
Iteration 188/1000 | Loss: 0.00008769
Iteration 189/1000 | Loss: 0.00054556
Iteration 190/1000 | Loss: 0.00005630
Iteration 191/1000 | Loss: 0.00003346
Iteration 192/1000 | Loss: 0.00022704
Iteration 193/1000 | Loss: 0.00060175
Iteration 194/1000 | Loss: 0.00042223
Iteration 195/1000 | Loss: 0.00039958
Iteration 196/1000 | Loss: 0.00052697
Iteration 197/1000 | Loss: 0.00004825
Iteration 198/1000 | Loss: 0.00003437
Iteration 199/1000 | Loss: 0.00003053
Iteration 200/1000 | Loss: 0.00002830
Iteration 201/1000 | Loss: 0.00002638
Iteration 202/1000 | Loss: 0.00002460
Iteration 203/1000 | Loss: 0.00002359
Iteration 204/1000 | Loss: 0.00002279
Iteration 205/1000 | Loss: 0.00051988
Iteration 206/1000 | Loss: 0.00084658
Iteration 207/1000 | Loss: 0.00027080
Iteration 208/1000 | Loss: 0.00056369
Iteration 209/1000 | Loss: 0.00002791
Iteration 210/1000 | Loss: 0.00002309
Iteration 211/1000 | Loss: 0.00026213
Iteration 212/1000 | Loss: 0.00002113
Iteration 213/1000 | Loss: 0.00001980
Iteration 214/1000 | Loss: 0.00001892
Iteration 215/1000 | Loss: 0.00001829
Iteration 216/1000 | Loss: 0.00039136
Iteration 217/1000 | Loss: 0.00118157
Iteration 218/1000 | Loss: 0.00012950
Iteration 219/1000 | Loss: 0.00006245
Iteration 220/1000 | Loss: 0.00001822
Iteration 221/1000 | Loss: 0.00001781
Iteration 222/1000 | Loss: 0.00037294
Iteration 223/1000 | Loss: 0.00016563
Iteration 224/1000 | Loss: 0.00005509
Iteration 225/1000 | Loss: 0.00001858
Iteration 226/1000 | Loss: 0.00001769
Iteration 227/1000 | Loss: 0.00001748
Iteration 228/1000 | Loss: 0.00001732
Iteration 229/1000 | Loss: 0.00037854
Iteration 230/1000 | Loss: 0.00009125
Iteration 231/1000 | Loss: 0.00058912
Iteration 232/1000 | Loss: 0.00028941
Iteration 233/1000 | Loss: 0.00040056
Iteration 234/1000 | Loss: 0.00016142
Iteration 235/1000 | Loss: 0.00035892
Iteration 236/1000 | Loss: 0.00034098
Iteration 237/1000 | Loss: 0.00010834
Iteration 238/1000 | Loss: 0.00018136
Iteration 239/1000 | Loss: 0.00003353
Iteration 240/1000 | Loss: 0.00002569
Iteration 241/1000 | Loss: 0.00001912
Iteration 242/1000 | Loss: 0.00001808
Iteration 243/1000 | Loss: 0.00001781
Iteration 244/1000 | Loss: 0.00001755
Iteration 245/1000 | Loss: 0.00001738
Iteration 246/1000 | Loss: 0.00001736
Iteration 247/1000 | Loss: 0.00001730
Iteration 248/1000 | Loss: 0.00001729
Iteration 249/1000 | Loss: 0.00001729
Iteration 250/1000 | Loss: 0.00001729
Iteration 251/1000 | Loss: 0.00001728
Iteration 252/1000 | Loss: 0.00001726
Iteration 253/1000 | Loss: 0.00001726
Iteration 254/1000 | Loss: 0.00001726
Iteration 255/1000 | Loss: 0.00001726
Iteration 256/1000 | Loss: 0.00001725
Iteration 257/1000 | Loss: 0.00001725
Iteration 258/1000 | Loss: 0.00001725
Iteration 259/1000 | Loss: 0.00001725
Iteration 260/1000 | Loss: 0.00001725
Iteration 261/1000 | Loss: 0.00001725
Iteration 262/1000 | Loss: 0.00001724
Iteration 263/1000 | Loss: 0.00001723
Iteration 264/1000 | Loss: 0.00001723
Iteration 265/1000 | Loss: 0.00001722
Iteration 266/1000 | Loss: 0.00001709
Iteration 267/1000 | Loss: 0.00001695
Iteration 268/1000 | Loss: 0.00037740
Iteration 269/1000 | Loss: 0.00038691
Iteration 270/1000 | Loss: 0.00070810
Iteration 271/1000 | Loss: 0.00032561
Iteration 272/1000 | Loss: 0.00064620
Iteration 273/1000 | Loss: 0.00029220
Iteration 274/1000 | Loss: 0.00019160
Iteration 275/1000 | Loss: 0.00038329
Iteration 276/1000 | Loss: 0.00036851
Iteration 277/1000 | Loss: 0.00002365
Iteration 278/1000 | Loss: 0.00004648
Iteration 279/1000 | Loss: 0.00001817
Iteration 280/1000 | Loss: 0.00016318
Iteration 281/1000 | Loss: 0.00001777
Iteration 282/1000 | Loss: 0.00001722
Iteration 283/1000 | Loss: 0.00001712
Iteration 284/1000 | Loss: 0.00001707
Iteration 285/1000 | Loss: 0.00001707
Iteration 286/1000 | Loss: 0.00001707
Iteration 287/1000 | Loss: 0.00001707
Iteration 288/1000 | Loss: 0.00001707
Iteration 289/1000 | Loss: 0.00001706
Iteration 290/1000 | Loss: 0.00001706
Iteration 291/1000 | Loss: 0.00001706
Iteration 292/1000 | Loss: 0.00001706
Iteration 293/1000 | Loss: 0.00001706
Iteration 294/1000 | Loss: 0.00001706
Iteration 295/1000 | Loss: 0.00001706
Iteration 296/1000 | Loss: 0.00001706
Iteration 297/1000 | Loss: 0.00001705
Iteration 298/1000 | Loss: 0.00001705
Iteration 299/1000 | Loss: 0.00001702
Iteration 300/1000 | Loss: 0.00001701
Iteration 301/1000 | Loss: 0.00001701
Iteration 302/1000 | Loss: 0.00001701
Iteration 303/1000 | Loss: 0.00001701
Iteration 304/1000 | Loss: 0.00001701
Iteration 305/1000 | Loss: 0.00001700
Iteration 306/1000 | Loss: 0.00001700
Iteration 307/1000 | Loss: 0.00001700
Iteration 308/1000 | Loss: 0.00001700
Iteration 309/1000 | Loss: 0.00001700
Iteration 310/1000 | Loss: 0.00001700
Iteration 311/1000 | Loss: 0.00001700
Iteration 312/1000 | Loss: 0.00001700
Iteration 313/1000 | Loss: 0.00001699
Iteration 314/1000 | Loss: 0.00001699
Iteration 315/1000 | Loss: 0.00001699
Iteration 316/1000 | Loss: 0.00001699
Iteration 317/1000 | Loss: 0.00001699
Iteration 318/1000 | Loss: 0.00001699
Iteration 319/1000 | Loss: 0.00001698
Iteration 320/1000 | Loss: 0.00001698
Iteration 321/1000 | Loss: 0.00001696
Iteration 322/1000 | Loss: 0.00001692
Iteration 323/1000 | Loss: 0.00001690
Iteration 324/1000 | Loss: 0.00001690
Iteration 325/1000 | Loss: 0.00001689
Iteration 326/1000 | Loss: 0.00001689
Iteration 327/1000 | Loss: 0.00001688
Iteration 328/1000 | Loss: 0.00001686
Iteration 329/1000 | Loss: 0.00001685
Iteration 330/1000 | Loss: 0.00034402
Iteration 331/1000 | Loss: 0.00029785
Iteration 332/1000 | Loss: 0.00035255
Iteration 333/1000 | Loss: 0.00017434
Iteration 334/1000 | Loss: 0.00002023
Iteration 335/1000 | Loss: 0.00037538
Iteration 336/1000 | Loss: 0.00030778
Iteration 337/1000 | Loss: 0.00036455
Iteration 338/1000 | Loss: 0.00029984
Iteration 339/1000 | Loss: 0.00027293
Iteration 340/1000 | Loss: 0.00066895
Iteration 341/1000 | Loss: 0.00100398
Iteration 342/1000 | Loss: 0.00003334
Iteration 343/1000 | Loss: 0.00001878
Iteration 344/1000 | Loss: 0.00032004
Iteration 345/1000 | Loss: 0.00024928
Iteration 346/1000 | Loss: 0.00025786
Iteration 347/1000 | Loss: 0.00062985
Iteration 348/1000 | Loss: 0.00039953
Iteration 349/1000 | Loss: 0.00002730
Iteration 350/1000 | Loss: 0.00002068
Iteration 351/1000 | Loss: 0.00001755
Iteration 352/1000 | Loss: 0.00001624
Iteration 353/1000 | Loss: 0.00001572
Iteration 354/1000 | Loss: 0.00001530
Iteration 355/1000 | Loss: 0.00001496
Iteration 356/1000 | Loss: 0.00001492
Iteration 357/1000 | Loss: 0.00001485
Iteration 358/1000 | Loss: 0.00001464
Iteration 359/1000 | Loss: 0.00001460
Iteration 360/1000 | Loss: 0.00001458
Iteration 361/1000 | Loss: 0.00001452
Iteration 362/1000 | Loss: 0.00001452
Iteration 363/1000 | Loss: 0.00001451
Iteration 364/1000 | Loss: 0.00001451
Iteration 365/1000 | Loss: 0.00001450
Iteration 366/1000 | Loss: 0.00001450
Iteration 367/1000 | Loss: 0.00001449
Iteration 368/1000 | Loss: 0.00001449
Iteration 369/1000 | Loss: 0.00001446
Iteration 370/1000 | Loss: 0.00001446
Iteration 371/1000 | Loss: 0.00001444
Iteration 372/1000 | Loss: 0.00001444
Iteration 373/1000 | Loss: 0.00001444
Iteration 374/1000 | Loss: 0.00001444
Iteration 375/1000 | Loss: 0.00001444
Iteration 376/1000 | Loss: 0.00001444
Iteration 377/1000 | Loss: 0.00001443
Iteration 378/1000 | Loss: 0.00001443
Iteration 379/1000 | Loss: 0.00001443
Iteration 380/1000 | Loss: 0.00001443
Iteration 381/1000 | Loss: 0.00001443
Iteration 382/1000 | Loss: 0.00001442
Iteration 383/1000 | Loss: 0.00001441
Iteration 384/1000 | Loss: 0.00001441
Iteration 385/1000 | Loss: 0.00001441
Iteration 386/1000 | Loss: 0.00001441
Iteration 387/1000 | Loss: 0.00001441
Iteration 388/1000 | Loss: 0.00001440
Iteration 389/1000 | Loss: 0.00001440
Iteration 390/1000 | Loss: 0.00001440
Iteration 391/1000 | Loss: 0.00001440
Iteration 392/1000 | Loss: 0.00001440
Iteration 393/1000 | Loss: 0.00001440
Iteration 394/1000 | Loss: 0.00001440
Iteration 395/1000 | Loss: 0.00001440
Iteration 396/1000 | Loss: 0.00001440
Iteration 397/1000 | Loss: 0.00001440
Iteration 398/1000 | Loss: 0.00001439
Iteration 399/1000 | Loss: 0.00001439
Iteration 400/1000 | Loss: 0.00001439
Iteration 401/1000 | Loss: 0.00001439
Iteration 402/1000 | Loss: 0.00001439
Iteration 403/1000 | Loss: 0.00001439
Iteration 404/1000 | Loss: 0.00001439
Iteration 405/1000 | Loss: 0.00001439
Iteration 406/1000 | Loss: 0.00001439
Iteration 407/1000 | Loss: 0.00001438
Iteration 408/1000 | Loss: 0.00001438
Iteration 409/1000 | Loss: 0.00001438
Iteration 410/1000 | Loss: 0.00001438
Iteration 411/1000 | Loss: 0.00001438
Iteration 412/1000 | Loss: 0.00001438
Iteration 413/1000 | Loss: 0.00001438
Iteration 414/1000 | Loss: 0.00001438
Iteration 415/1000 | Loss: 0.00001438
Iteration 416/1000 | Loss: 0.00001438
Iteration 417/1000 | Loss: 0.00001438
Iteration 418/1000 | Loss: 0.00001438
Iteration 419/1000 | Loss: 0.00001437
Iteration 420/1000 | Loss: 0.00001437
Iteration 421/1000 | Loss: 0.00001437
Iteration 422/1000 | Loss: 0.00001437
Iteration 423/1000 | Loss: 0.00001437
Iteration 424/1000 | Loss: 0.00001437
Iteration 425/1000 | Loss: 0.00001437
Iteration 426/1000 | Loss: 0.00001437
Iteration 427/1000 | Loss: 0.00001437
Iteration 428/1000 | Loss: 0.00001437
Iteration 429/1000 | Loss: 0.00001437
Iteration 430/1000 | Loss: 0.00001436
Iteration 431/1000 | Loss: 0.00001436
Iteration 432/1000 | Loss: 0.00001436
Iteration 433/1000 | Loss: 0.00001436
Iteration 434/1000 | Loss: 0.00001436
Iteration 435/1000 | Loss: 0.00001436
Iteration 436/1000 | Loss: 0.00001436
Iteration 437/1000 | Loss: 0.00001436
Iteration 438/1000 | Loss: 0.00001436
Iteration 439/1000 | Loss: 0.00001436
Iteration 440/1000 | Loss: 0.00001435
Iteration 441/1000 | Loss: 0.00001435
Iteration 442/1000 | Loss: 0.00001435
Iteration 443/1000 | Loss: 0.00001435
Iteration 444/1000 | Loss: 0.00001435
Iteration 445/1000 | Loss: 0.00001435
Iteration 446/1000 | Loss: 0.00001435
Iteration 447/1000 | Loss: 0.00001435
Iteration 448/1000 | Loss: 0.00001435
Iteration 449/1000 | Loss: 0.00001435
Iteration 450/1000 | Loss: 0.00001435
Iteration 451/1000 | Loss: 0.00001435
Iteration 452/1000 | Loss: 0.00001435
Iteration 453/1000 | Loss: 0.00001435
Iteration 454/1000 | Loss: 0.00001435
Iteration 455/1000 | Loss: 0.00001435
Iteration 456/1000 | Loss: 0.00001435
Iteration 457/1000 | Loss: 0.00001435
Iteration 458/1000 | Loss: 0.00001435
Iteration 459/1000 | Loss: 0.00001434
Iteration 460/1000 | Loss: 0.00001434
Iteration 461/1000 | Loss: 0.00001434
Iteration 462/1000 | Loss: 0.00001434
Iteration 463/1000 | Loss: 0.00001434
Iteration 464/1000 | Loss: 0.00001434
Iteration 465/1000 | Loss: 0.00001434
Iteration 466/1000 | Loss: 0.00001434
Iteration 467/1000 | Loss: 0.00001434
Iteration 468/1000 | Loss: 0.00001434
Iteration 469/1000 | Loss: 0.00001434
Iteration 470/1000 | Loss: 0.00001434
Iteration 471/1000 | Loss: 0.00001434
Iteration 472/1000 | Loss: 0.00001434
Iteration 473/1000 | Loss: 0.00001434
Iteration 474/1000 | Loss: 0.00001434
Iteration 475/1000 | Loss: 0.00001434
Iteration 476/1000 | Loss: 0.00001434
Iteration 477/1000 | Loss: 0.00001434
Iteration 478/1000 | Loss: 0.00001433
Iteration 479/1000 | Loss: 0.00001433
Iteration 480/1000 | Loss: 0.00001433
Iteration 481/1000 | Loss: 0.00001433
Iteration 482/1000 | Loss: 0.00001433
Iteration 483/1000 | Loss: 0.00001433
Iteration 484/1000 | Loss: 0.00001433
Iteration 485/1000 | Loss: 0.00001433
Iteration 486/1000 | Loss: 0.00001433
Iteration 487/1000 | Loss: 0.00001433
Iteration 488/1000 | Loss: 0.00001433
Iteration 489/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 489. Stopping optimization.
Last 5 losses: [1.433323177479906e-05, 1.433323177479906e-05, 1.433323177479906e-05, 1.433323177479906e-05, 1.433323177479906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.433323177479906e-05

Optimization complete. Final v2v error: 3.1067354679107666 mm

Highest mean error: 4.810598850250244 mm for frame 72

Lowest mean error: 2.241037368774414 mm for frame 21

Saving results

Total time: 3445.4839918613434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1072
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496026
Iteration 2/25 | Loss: 0.00112412
Iteration 3/25 | Loss: 0.00090455
Iteration 4/25 | Loss: 0.00087855
Iteration 5/25 | Loss: 0.00087134
Iteration 6/25 | Loss: 0.00086954
Iteration 7/25 | Loss: 0.00086954
Iteration 8/25 | Loss: 0.00086954
Iteration 9/25 | Loss: 0.00086954
Iteration 10/25 | Loss: 0.00086954
Iteration 11/25 | Loss: 0.00086954
Iteration 12/25 | Loss: 0.00086954
Iteration 13/25 | Loss: 0.00086954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008695359574630857, 0.0008695359574630857, 0.0008695359574630857, 0.0008695359574630857, 0.0008695359574630857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008695359574630857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81798929
Iteration 2/25 | Loss: 0.00048755
Iteration 3/25 | Loss: 0.00048755
Iteration 4/25 | Loss: 0.00048755
Iteration 5/25 | Loss: 0.00048755
Iteration 6/25 | Loss: 0.00048755
Iteration 7/25 | Loss: 0.00048755
Iteration 8/25 | Loss: 0.00048755
Iteration 9/25 | Loss: 0.00048755
Iteration 10/25 | Loss: 0.00048755
Iteration 11/25 | Loss: 0.00048755
Iteration 12/25 | Loss: 0.00048755
Iteration 13/25 | Loss: 0.00048755
Iteration 14/25 | Loss: 0.00048755
Iteration 15/25 | Loss: 0.00048755
Iteration 16/25 | Loss: 0.00048755
Iteration 17/25 | Loss: 0.00048755
Iteration 18/25 | Loss: 0.00048755
Iteration 19/25 | Loss: 0.00048755
Iteration 20/25 | Loss: 0.00048755
Iteration 21/25 | Loss: 0.00048755
Iteration 22/25 | Loss: 0.00048755
Iteration 23/25 | Loss: 0.00048755
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004875464946962893, 0.0004875464946962893, 0.0004875464946962893, 0.0004875464946962893, 0.0004875464946962893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004875464946962893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048755
Iteration 2/1000 | Loss: 0.00003442
Iteration 3/1000 | Loss: 0.00002176
Iteration 4/1000 | Loss: 0.00001979
Iteration 5/1000 | Loss: 0.00001860
Iteration 6/1000 | Loss: 0.00001818
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001699
Iteration 10/1000 | Loss: 0.00001676
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001592
Iteration 21/1000 | Loss: 0.00001592
Iteration 22/1000 | Loss: 0.00001592
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001592
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001589
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001574
Iteration 40/1000 | Loss: 0.00001574
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001574
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001564
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001558
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001550
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001544
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001539
Iteration 104/1000 | Loss: 0.00001539
Iteration 105/1000 | Loss: 0.00001539
Iteration 106/1000 | Loss: 0.00001539
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001539
Iteration 114/1000 | Loss: 0.00001539
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001535
Iteration 121/1000 | Loss: 0.00001535
Iteration 122/1000 | Loss: 0.00001535
Iteration 123/1000 | Loss: 0.00001535
Iteration 124/1000 | Loss: 0.00001535
Iteration 125/1000 | Loss: 0.00001535
Iteration 126/1000 | Loss: 0.00001535
Iteration 127/1000 | Loss: 0.00001535
Iteration 128/1000 | Loss: 0.00001534
Iteration 129/1000 | Loss: 0.00001534
Iteration 130/1000 | Loss: 0.00001534
Iteration 131/1000 | Loss: 0.00001534
Iteration 132/1000 | Loss: 0.00001534
Iteration 133/1000 | Loss: 0.00001534
Iteration 134/1000 | Loss: 0.00001534
Iteration 135/1000 | Loss: 0.00001534
Iteration 136/1000 | Loss: 0.00001534
Iteration 137/1000 | Loss: 0.00001534
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001533
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001532
Iteration 144/1000 | Loss: 0.00001532
Iteration 145/1000 | Loss: 0.00001532
Iteration 146/1000 | Loss: 0.00001532
Iteration 147/1000 | Loss: 0.00001532
Iteration 148/1000 | Loss: 0.00001532
Iteration 149/1000 | Loss: 0.00001532
Iteration 150/1000 | Loss: 0.00001532
Iteration 151/1000 | Loss: 0.00001532
Iteration 152/1000 | Loss: 0.00001531
Iteration 153/1000 | Loss: 0.00001531
Iteration 154/1000 | Loss: 0.00001531
Iteration 155/1000 | Loss: 0.00001531
Iteration 156/1000 | Loss: 0.00001531
Iteration 157/1000 | Loss: 0.00001530
Iteration 158/1000 | Loss: 0.00001530
Iteration 159/1000 | Loss: 0.00001530
Iteration 160/1000 | Loss: 0.00001530
Iteration 161/1000 | Loss: 0.00001530
Iteration 162/1000 | Loss: 0.00001530
Iteration 163/1000 | Loss: 0.00001529
Iteration 164/1000 | Loss: 0.00001529
Iteration 165/1000 | Loss: 0.00001529
Iteration 166/1000 | Loss: 0.00001529
Iteration 167/1000 | Loss: 0.00001528
Iteration 168/1000 | Loss: 0.00001528
Iteration 169/1000 | Loss: 0.00001528
Iteration 170/1000 | Loss: 0.00001528
Iteration 171/1000 | Loss: 0.00001528
Iteration 172/1000 | Loss: 0.00001528
Iteration 173/1000 | Loss: 0.00001527
Iteration 174/1000 | Loss: 0.00001527
Iteration 175/1000 | Loss: 0.00001527
Iteration 176/1000 | Loss: 0.00001527
Iteration 177/1000 | Loss: 0.00001527
Iteration 178/1000 | Loss: 0.00001527
Iteration 179/1000 | Loss: 0.00001527
Iteration 180/1000 | Loss: 0.00001527
Iteration 181/1000 | Loss: 0.00001527
Iteration 182/1000 | Loss: 0.00001527
Iteration 183/1000 | Loss: 0.00001527
Iteration 184/1000 | Loss: 0.00001527
Iteration 185/1000 | Loss: 0.00001527
Iteration 186/1000 | Loss: 0.00001527
Iteration 187/1000 | Loss: 0.00001527
Iteration 188/1000 | Loss: 0.00001527
Iteration 189/1000 | Loss: 0.00001527
Iteration 190/1000 | Loss: 0.00001527
Iteration 191/1000 | Loss: 0.00001527
Iteration 192/1000 | Loss: 0.00001527
Iteration 193/1000 | Loss: 0.00001527
Iteration 194/1000 | Loss: 0.00001527
Iteration 195/1000 | Loss: 0.00001527
Iteration 196/1000 | Loss: 0.00001527
Iteration 197/1000 | Loss: 0.00001527
Iteration 198/1000 | Loss: 0.00001527
Iteration 199/1000 | Loss: 0.00001527
Iteration 200/1000 | Loss: 0.00001527
Iteration 201/1000 | Loss: 0.00001527
Iteration 202/1000 | Loss: 0.00001527
Iteration 203/1000 | Loss: 0.00001527
Iteration 204/1000 | Loss: 0.00001527
Iteration 205/1000 | Loss: 0.00001527
Iteration 206/1000 | Loss: 0.00001527
Iteration 207/1000 | Loss: 0.00001527
Iteration 208/1000 | Loss: 0.00001527
Iteration 209/1000 | Loss: 0.00001527
Iteration 210/1000 | Loss: 0.00001527
Iteration 211/1000 | Loss: 0.00001527
Iteration 212/1000 | Loss: 0.00001527
Iteration 213/1000 | Loss: 0.00001527
Iteration 214/1000 | Loss: 0.00001527
Iteration 215/1000 | Loss: 0.00001527
Iteration 216/1000 | Loss: 0.00001527
Iteration 217/1000 | Loss: 0.00001527
Iteration 218/1000 | Loss: 0.00001527
Iteration 219/1000 | Loss: 0.00001527
Iteration 220/1000 | Loss: 0.00001527
Iteration 221/1000 | Loss: 0.00001527
Iteration 222/1000 | Loss: 0.00001527
Iteration 223/1000 | Loss: 0.00001527
Iteration 224/1000 | Loss: 0.00001527
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001527
Iteration 227/1000 | Loss: 0.00001527
Iteration 228/1000 | Loss: 0.00001527
Iteration 229/1000 | Loss: 0.00001527
Iteration 230/1000 | Loss: 0.00001527
Iteration 231/1000 | Loss: 0.00001527
Iteration 232/1000 | Loss: 0.00001527
Iteration 233/1000 | Loss: 0.00001527
Iteration 234/1000 | Loss: 0.00001527
Iteration 235/1000 | Loss: 0.00001527
Iteration 236/1000 | Loss: 0.00001527
Iteration 237/1000 | Loss: 0.00001527
Iteration 238/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.5268404240487143e-05, 1.5268404240487143e-05, 1.5268404240487143e-05, 1.5268404240487143e-05, 1.5268404240487143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5268404240487143e-05

Optimization complete. Final v2v error: 3.2453112602233887 mm

Highest mean error: 3.9643585681915283 mm for frame 263

Lowest mean error: 3.144521474838257 mm for frame 81

Saving results

Total time: 576.9150352478027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1030
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502484
Iteration 2/25 | Loss: 0.00128539
Iteration 3/25 | Loss: 0.00096036
Iteration 4/25 | Loss: 0.00093511
Iteration 5/25 | Loss: 0.00092717
Iteration 6/25 | Loss: 0.00092497
Iteration 7/25 | Loss: 0.00092497
Iteration 8/25 | Loss: 0.00092497
Iteration 9/25 | Loss: 0.00092497
Iteration 10/25 | Loss: 0.00092497
Iteration 11/25 | Loss: 0.00092497
Iteration 12/25 | Loss: 0.00092497
Iteration 13/25 | Loss: 0.00092497
Iteration 14/25 | Loss: 0.00092497
Iteration 15/25 | Loss: 0.00092497
Iteration 16/25 | Loss: 0.00092497
Iteration 17/25 | Loss: 0.00092497
Iteration 18/25 | Loss: 0.00092497
Iteration 19/25 | Loss: 0.00092497
Iteration 20/25 | Loss: 0.00092497
Iteration 21/25 | Loss: 0.00092497
Iteration 22/25 | Loss: 0.00092497
Iteration 23/25 | Loss: 0.00092497
Iteration 24/25 | Loss: 0.00092497
Iteration 25/25 | Loss: 0.00092497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10448241
Iteration 2/25 | Loss: 0.00053411
Iteration 3/25 | Loss: 0.00053411
Iteration 4/25 | Loss: 0.00053411
Iteration 5/25 | Loss: 0.00053411
Iteration 6/25 | Loss: 0.00053411
Iteration 7/25 | Loss: 0.00053410
Iteration 8/25 | Loss: 0.00053410
Iteration 9/25 | Loss: 0.00053410
Iteration 10/25 | Loss: 0.00053410
Iteration 11/25 | Loss: 0.00053410
Iteration 12/25 | Loss: 0.00053410
Iteration 13/25 | Loss: 0.00053410
Iteration 14/25 | Loss: 0.00053410
Iteration 15/25 | Loss: 0.00053410
Iteration 16/25 | Loss: 0.00053410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005341042997315526, 0.0005341042997315526, 0.0005341042997315526, 0.0005341042997315526, 0.0005341042997315526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005341042997315526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053410
Iteration 2/1000 | Loss: 0.00005116
Iteration 3/1000 | Loss: 0.00003530
Iteration 4/1000 | Loss: 0.00002655
Iteration 5/1000 | Loss: 0.00002467
Iteration 6/1000 | Loss: 0.00002353
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002205
Iteration 9/1000 | Loss: 0.00002150
Iteration 10/1000 | Loss: 0.00002110
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002030
Iteration 14/1000 | Loss: 0.00002017
Iteration 15/1000 | Loss: 0.00002001
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001980
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001967
Iteration 20/1000 | Loss: 0.00001966
Iteration 21/1000 | Loss: 0.00001963
Iteration 22/1000 | Loss: 0.00001962
Iteration 23/1000 | Loss: 0.00001961
Iteration 24/1000 | Loss: 0.00001961
Iteration 25/1000 | Loss: 0.00001960
Iteration 26/1000 | Loss: 0.00001960
Iteration 27/1000 | Loss: 0.00001960
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001956
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001954
Iteration 40/1000 | Loss: 0.00001954
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001954
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001953
Iteration 48/1000 | Loss: 0.00001953
Iteration 49/1000 | Loss: 0.00001953
Iteration 50/1000 | Loss: 0.00001953
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001951
Iteration 55/1000 | Loss: 0.00001951
Iteration 56/1000 | Loss: 0.00001950
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001948
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001948
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001948
Iteration 69/1000 | Loss: 0.00001948
Iteration 70/1000 | Loss: 0.00001948
Iteration 71/1000 | Loss: 0.00001948
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001947
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001943
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001943
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00001942
Iteration 104/1000 | Loss: 0.00001942
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001940
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001939
Iteration 124/1000 | Loss: 0.00001939
Iteration 125/1000 | Loss: 0.00001939
Iteration 126/1000 | Loss: 0.00001938
Iteration 127/1000 | Loss: 0.00001938
Iteration 128/1000 | Loss: 0.00001938
Iteration 129/1000 | Loss: 0.00001938
Iteration 130/1000 | Loss: 0.00001937
Iteration 131/1000 | Loss: 0.00001937
Iteration 132/1000 | Loss: 0.00001937
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001936
Iteration 135/1000 | Loss: 0.00001936
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001936
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001935
Iteration 140/1000 | Loss: 0.00001935
Iteration 141/1000 | Loss: 0.00001935
Iteration 142/1000 | Loss: 0.00001935
Iteration 143/1000 | Loss: 0.00001935
Iteration 144/1000 | Loss: 0.00001935
Iteration 145/1000 | Loss: 0.00001935
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001935
Iteration 148/1000 | Loss: 0.00001935
Iteration 149/1000 | Loss: 0.00001935
Iteration 150/1000 | Loss: 0.00001935
Iteration 151/1000 | Loss: 0.00001935
Iteration 152/1000 | Loss: 0.00001935
Iteration 153/1000 | Loss: 0.00001935
Iteration 154/1000 | Loss: 0.00001935
Iteration 155/1000 | Loss: 0.00001935
Iteration 156/1000 | Loss: 0.00001935
Iteration 157/1000 | Loss: 0.00001935
Iteration 158/1000 | Loss: 0.00001935
Iteration 159/1000 | Loss: 0.00001935
Iteration 160/1000 | Loss: 0.00001935
Iteration 161/1000 | Loss: 0.00001935
Iteration 162/1000 | Loss: 0.00001935
Iteration 163/1000 | Loss: 0.00001935
Iteration 164/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.9348390196682885e-05, 1.9348390196682885e-05, 1.9348390196682885e-05, 1.9348390196682885e-05, 1.9348390196682885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9348390196682885e-05

Optimization complete. Final v2v error: 3.513697862625122 mm

Highest mean error: 5.061258792877197 mm for frame 161

Lowest mean error: 2.2461562156677246 mm for frame 197

Saving results

Total time: 364.91951870918274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1061
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401610
Iteration 2/25 | Loss: 0.00096491
Iteration 3/25 | Loss: 0.00088985
Iteration 4/25 | Loss: 0.00087711
Iteration 5/25 | Loss: 0.00087283
Iteration 6/25 | Loss: 0.00087206
Iteration 7/25 | Loss: 0.00087206
Iteration 8/25 | Loss: 0.00087206
Iteration 9/25 | Loss: 0.00087206
Iteration 10/25 | Loss: 0.00087206
Iteration 11/25 | Loss: 0.00087206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008720554178580642, 0.0008720554178580642, 0.0008720554178580642, 0.0008720554178580642, 0.0008720554178580642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008720554178580642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32302046
Iteration 2/25 | Loss: 0.00044128
Iteration 3/25 | Loss: 0.00044127
Iteration 4/25 | Loss: 0.00044127
Iteration 5/25 | Loss: 0.00044127
Iteration 6/25 | Loss: 0.00044127
Iteration 7/25 | Loss: 0.00044127
Iteration 8/25 | Loss: 0.00044127
Iteration 9/25 | Loss: 0.00044127
Iteration 10/25 | Loss: 0.00044127
Iteration 11/25 | Loss: 0.00044127
Iteration 12/25 | Loss: 0.00044127
Iteration 13/25 | Loss: 0.00044127
Iteration 14/25 | Loss: 0.00044127
Iteration 15/25 | Loss: 0.00044127
Iteration 16/25 | Loss: 0.00044127
Iteration 17/25 | Loss: 0.00044127
Iteration 18/25 | Loss: 0.00044127
Iteration 19/25 | Loss: 0.00044127
Iteration 20/25 | Loss: 0.00044127
Iteration 21/25 | Loss: 0.00044127
Iteration 22/25 | Loss: 0.00044127
Iteration 23/25 | Loss: 0.00044127
Iteration 24/25 | Loss: 0.00044127
Iteration 25/25 | Loss: 0.00044127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044127
Iteration 2/1000 | Loss: 0.00002730
Iteration 3/1000 | Loss: 0.00001857
Iteration 4/1000 | Loss: 0.00001671
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001484
Iteration 12/1000 | Loss: 0.00001475
Iteration 13/1000 | Loss: 0.00001474
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001473
Iteration 16/1000 | Loss: 0.00001473
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001462
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001461
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001459
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001455
Iteration 59/1000 | Loss: 0.00001455
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001450
Iteration 80/1000 | Loss: 0.00001450
Iteration 81/1000 | Loss: 0.00001450
Iteration 82/1000 | Loss: 0.00001450
Iteration 83/1000 | Loss: 0.00001449
Iteration 84/1000 | Loss: 0.00001449
Iteration 85/1000 | Loss: 0.00001449
Iteration 86/1000 | Loss: 0.00001449
Iteration 87/1000 | Loss: 0.00001449
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001448
Iteration 91/1000 | Loss: 0.00001448
Iteration 92/1000 | Loss: 0.00001448
Iteration 93/1000 | Loss: 0.00001448
Iteration 94/1000 | Loss: 0.00001448
Iteration 95/1000 | Loss: 0.00001448
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001447
Iteration 100/1000 | Loss: 0.00001447
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001446
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001445
Iteration 119/1000 | Loss: 0.00001445
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001443
Iteration 126/1000 | Loss: 0.00001443
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001442
Iteration 136/1000 | Loss: 0.00001442
Iteration 137/1000 | Loss: 0.00001442
Iteration 138/1000 | Loss: 0.00001441
Iteration 139/1000 | Loss: 0.00001441
Iteration 140/1000 | Loss: 0.00001441
Iteration 141/1000 | Loss: 0.00001441
Iteration 142/1000 | Loss: 0.00001441
Iteration 143/1000 | Loss: 0.00001441
Iteration 144/1000 | Loss: 0.00001441
Iteration 145/1000 | Loss: 0.00001441
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001440
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001440
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001438
Iteration 158/1000 | Loss: 0.00001438
Iteration 159/1000 | Loss: 0.00001438
Iteration 160/1000 | Loss: 0.00001438
Iteration 161/1000 | Loss: 0.00001438
Iteration 162/1000 | Loss: 0.00001438
Iteration 163/1000 | Loss: 0.00001438
Iteration 164/1000 | Loss: 0.00001438
Iteration 165/1000 | Loss: 0.00001438
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001437
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Iteration 170/1000 | Loss: 0.00001437
Iteration 171/1000 | Loss: 0.00001436
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001436
Iteration 175/1000 | Loss: 0.00001436
Iteration 176/1000 | Loss: 0.00001436
Iteration 177/1000 | Loss: 0.00001436
Iteration 178/1000 | Loss: 0.00001436
Iteration 179/1000 | Loss: 0.00001436
Iteration 180/1000 | Loss: 0.00001436
Iteration 181/1000 | Loss: 0.00001436
Iteration 182/1000 | Loss: 0.00001436
Iteration 183/1000 | Loss: 0.00001436
Iteration 184/1000 | Loss: 0.00001436
Iteration 185/1000 | Loss: 0.00001436
Iteration 186/1000 | Loss: 0.00001436
Iteration 187/1000 | Loss: 0.00001436
Iteration 188/1000 | Loss: 0.00001435
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001435
Iteration 191/1000 | Loss: 0.00001435
Iteration 192/1000 | Loss: 0.00001435
Iteration 193/1000 | Loss: 0.00001435
Iteration 194/1000 | Loss: 0.00001435
Iteration 195/1000 | Loss: 0.00001435
Iteration 196/1000 | Loss: 0.00001435
Iteration 197/1000 | Loss: 0.00001435
Iteration 198/1000 | Loss: 0.00001435
Iteration 199/1000 | Loss: 0.00001435
Iteration 200/1000 | Loss: 0.00001435
Iteration 201/1000 | Loss: 0.00001435
Iteration 202/1000 | Loss: 0.00001435
Iteration 203/1000 | Loss: 0.00001435
Iteration 204/1000 | Loss: 0.00001435
Iteration 205/1000 | Loss: 0.00001435
Iteration 206/1000 | Loss: 0.00001435
Iteration 207/1000 | Loss: 0.00001435
Iteration 208/1000 | Loss: 0.00001435
Iteration 209/1000 | Loss: 0.00001435
Iteration 210/1000 | Loss: 0.00001435
Iteration 211/1000 | Loss: 0.00001435
Iteration 212/1000 | Loss: 0.00001435
Iteration 213/1000 | Loss: 0.00001435
Iteration 214/1000 | Loss: 0.00001435
Iteration 215/1000 | Loss: 0.00001435
Iteration 216/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.4346327589009888e-05, 1.4346327589009888e-05, 1.4346327589009888e-05, 1.4346327589009888e-05, 1.4346327589009888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4346327589009888e-05

Optimization complete. Final v2v error: 2.9990220069885254 mm

Highest mean error: 3.758913516998291 mm for frame 120

Lowest mean error: 2.5039591789245605 mm for frame 67

Saving results

Total time: 318.63552069664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1082
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600893
Iteration 2/25 | Loss: 0.00091738
Iteration 3/25 | Loss: 0.00081404
Iteration 4/25 | Loss: 0.00079692
Iteration 5/25 | Loss: 0.00079121
Iteration 6/25 | Loss: 0.00078933
Iteration 7/25 | Loss: 0.00078933
Iteration 8/25 | Loss: 0.00078933
Iteration 9/25 | Loss: 0.00078933
Iteration 10/25 | Loss: 0.00078933
Iteration 11/25 | Loss: 0.00078933
Iteration 12/25 | Loss: 0.00078933
Iteration 13/25 | Loss: 0.00078933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007893334259279072, 0.0007893334259279072, 0.0007893334259279072, 0.0007893334259279072, 0.0007893334259279072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007893334259279072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95124292
Iteration 2/25 | Loss: 0.00045591
Iteration 3/25 | Loss: 0.00045591
Iteration 4/25 | Loss: 0.00045591
Iteration 5/25 | Loss: 0.00045591
Iteration 6/25 | Loss: 0.00045591
Iteration 7/25 | Loss: 0.00045591
Iteration 8/25 | Loss: 0.00045591
Iteration 9/25 | Loss: 0.00045591
Iteration 10/25 | Loss: 0.00045591
Iteration 11/25 | Loss: 0.00045591
Iteration 12/25 | Loss: 0.00045591
Iteration 13/25 | Loss: 0.00045591
Iteration 14/25 | Loss: 0.00045591
Iteration 15/25 | Loss: 0.00045591
Iteration 16/25 | Loss: 0.00045591
Iteration 17/25 | Loss: 0.00045591
Iteration 18/25 | Loss: 0.00045591
Iteration 19/25 | Loss: 0.00045591
Iteration 20/25 | Loss: 0.00045591
Iteration 21/25 | Loss: 0.00045591
Iteration 22/25 | Loss: 0.00045591
Iteration 23/25 | Loss: 0.00045591
Iteration 24/25 | Loss: 0.00045591
Iteration 25/25 | Loss: 0.00045591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045591
Iteration 2/1000 | Loss: 0.00001878
Iteration 3/1000 | Loss: 0.00000999
Iteration 4/1000 | Loss: 0.00000913
Iteration 5/1000 | Loss: 0.00000858
Iteration 6/1000 | Loss: 0.00000824
Iteration 7/1000 | Loss: 0.00000802
Iteration 8/1000 | Loss: 0.00000789
Iteration 9/1000 | Loss: 0.00000785
Iteration 10/1000 | Loss: 0.00000784
Iteration 11/1000 | Loss: 0.00000784
Iteration 12/1000 | Loss: 0.00000784
Iteration 13/1000 | Loss: 0.00000776
Iteration 14/1000 | Loss: 0.00000775
Iteration 15/1000 | Loss: 0.00000774
Iteration 16/1000 | Loss: 0.00000773
Iteration 17/1000 | Loss: 0.00000772
Iteration 18/1000 | Loss: 0.00000771
Iteration 19/1000 | Loss: 0.00000770
Iteration 20/1000 | Loss: 0.00000770
Iteration 21/1000 | Loss: 0.00000768
Iteration 22/1000 | Loss: 0.00000768
Iteration 23/1000 | Loss: 0.00000766
Iteration 24/1000 | Loss: 0.00000763
Iteration 25/1000 | Loss: 0.00000763
Iteration 26/1000 | Loss: 0.00000762
Iteration 27/1000 | Loss: 0.00000761
Iteration 28/1000 | Loss: 0.00000761
Iteration 29/1000 | Loss: 0.00000761
Iteration 30/1000 | Loss: 0.00000760
Iteration 31/1000 | Loss: 0.00000759
Iteration 32/1000 | Loss: 0.00000759
Iteration 33/1000 | Loss: 0.00000758
Iteration 34/1000 | Loss: 0.00000758
Iteration 35/1000 | Loss: 0.00000757
Iteration 36/1000 | Loss: 0.00000757
Iteration 37/1000 | Loss: 0.00000757
Iteration 38/1000 | Loss: 0.00000756
Iteration 39/1000 | Loss: 0.00000755
Iteration 40/1000 | Loss: 0.00000755
Iteration 41/1000 | Loss: 0.00000755
Iteration 42/1000 | Loss: 0.00000754
Iteration 43/1000 | Loss: 0.00000754
Iteration 44/1000 | Loss: 0.00000753
Iteration 45/1000 | Loss: 0.00000753
Iteration 46/1000 | Loss: 0.00000753
Iteration 47/1000 | Loss: 0.00000752
Iteration 48/1000 | Loss: 0.00000752
Iteration 49/1000 | Loss: 0.00000751
Iteration 50/1000 | Loss: 0.00000751
Iteration 51/1000 | Loss: 0.00000751
Iteration 52/1000 | Loss: 0.00000750
Iteration 53/1000 | Loss: 0.00000750
Iteration 54/1000 | Loss: 0.00000749
Iteration 55/1000 | Loss: 0.00000749
Iteration 56/1000 | Loss: 0.00000749
Iteration 57/1000 | Loss: 0.00000748
Iteration 58/1000 | Loss: 0.00000748
Iteration 59/1000 | Loss: 0.00000748
Iteration 60/1000 | Loss: 0.00000747
Iteration 61/1000 | Loss: 0.00000747
Iteration 62/1000 | Loss: 0.00000747
Iteration 63/1000 | Loss: 0.00000747
Iteration 64/1000 | Loss: 0.00000747
Iteration 65/1000 | Loss: 0.00000747
Iteration 66/1000 | Loss: 0.00000747
Iteration 67/1000 | Loss: 0.00000746
Iteration 68/1000 | Loss: 0.00000746
Iteration 69/1000 | Loss: 0.00000746
Iteration 70/1000 | Loss: 0.00000746
Iteration 71/1000 | Loss: 0.00000746
Iteration 72/1000 | Loss: 0.00000746
Iteration 73/1000 | Loss: 0.00000745
Iteration 74/1000 | Loss: 0.00000745
Iteration 75/1000 | Loss: 0.00000745
Iteration 76/1000 | Loss: 0.00000745
Iteration 77/1000 | Loss: 0.00000744
Iteration 78/1000 | Loss: 0.00000744
Iteration 79/1000 | Loss: 0.00000744
Iteration 80/1000 | Loss: 0.00000744
Iteration 81/1000 | Loss: 0.00000744
Iteration 82/1000 | Loss: 0.00000744
Iteration 83/1000 | Loss: 0.00000743
Iteration 84/1000 | Loss: 0.00000743
Iteration 85/1000 | Loss: 0.00000743
Iteration 86/1000 | Loss: 0.00000743
Iteration 87/1000 | Loss: 0.00000742
Iteration 88/1000 | Loss: 0.00000741
Iteration 89/1000 | Loss: 0.00000741
Iteration 90/1000 | Loss: 0.00000741
Iteration 91/1000 | Loss: 0.00000741
Iteration 92/1000 | Loss: 0.00000741
Iteration 93/1000 | Loss: 0.00000740
Iteration 94/1000 | Loss: 0.00000740
Iteration 95/1000 | Loss: 0.00000740
Iteration 96/1000 | Loss: 0.00000740
Iteration 97/1000 | Loss: 0.00000740
Iteration 98/1000 | Loss: 0.00000740
Iteration 99/1000 | Loss: 0.00000739
Iteration 100/1000 | Loss: 0.00000739
Iteration 101/1000 | Loss: 0.00000739
Iteration 102/1000 | Loss: 0.00000738
Iteration 103/1000 | Loss: 0.00000738
Iteration 104/1000 | Loss: 0.00000738
Iteration 105/1000 | Loss: 0.00000738
Iteration 106/1000 | Loss: 0.00000738
Iteration 107/1000 | Loss: 0.00000738
Iteration 108/1000 | Loss: 0.00000738
Iteration 109/1000 | Loss: 0.00000738
Iteration 110/1000 | Loss: 0.00000738
Iteration 111/1000 | Loss: 0.00000738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [7.383896445389837e-06, 7.383896445389837e-06, 7.383896445389837e-06, 7.383896445389837e-06, 7.383896445389837e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.383896445389837e-06

Optimization complete. Final v2v error: 2.343688488006592 mm

Highest mean error: 2.601675033569336 mm for frame 142

Lowest mean error: 2.0841732025146484 mm for frame 0

Saving results

Total time: 250.83723044395447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1067
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507327
Iteration 2/25 | Loss: 0.00097953
Iteration 3/25 | Loss: 0.00088621
Iteration 4/25 | Loss: 0.00086356
Iteration 5/25 | Loss: 0.00085907
Iteration 6/25 | Loss: 0.00085841
Iteration 7/25 | Loss: 0.00085841
Iteration 8/25 | Loss: 0.00085841
Iteration 9/25 | Loss: 0.00085841
Iteration 10/25 | Loss: 0.00085841
Iteration 11/25 | Loss: 0.00085841
Iteration 12/25 | Loss: 0.00085841
Iteration 13/25 | Loss: 0.00085841
Iteration 14/25 | Loss: 0.00085841
Iteration 15/25 | Loss: 0.00085841
Iteration 16/25 | Loss: 0.00085841
Iteration 17/25 | Loss: 0.00085841
Iteration 18/25 | Loss: 0.00085841
Iteration 19/25 | Loss: 0.00085841
Iteration 20/25 | Loss: 0.00085841
Iteration 21/25 | Loss: 0.00085841
Iteration 22/25 | Loss: 0.00085841
Iteration 23/25 | Loss: 0.00085841
Iteration 24/25 | Loss: 0.00085841
Iteration 25/25 | Loss: 0.00085841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81709218
Iteration 2/25 | Loss: 0.00041940
Iteration 3/25 | Loss: 0.00041939
Iteration 4/25 | Loss: 0.00041939
Iteration 5/25 | Loss: 0.00041939
Iteration 6/25 | Loss: 0.00041939
Iteration 7/25 | Loss: 0.00041939
Iteration 8/25 | Loss: 0.00041939
Iteration 9/25 | Loss: 0.00041939
Iteration 10/25 | Loss: 0.00041939
Iteration 11/25 | Loss: 0.00041939
Iteration 12/25 | Loss: 0.00041939
Iteration 13/25 | Loss: 0.00041939
Iteration 14/25 | Loss: 0.00041939
Iteration 15/25 | Loss: 0.00041939
Iteration 16/25 | Loss: 0.00041939
Iteration 17/25 | Loss: 0.00041939
Iteration 18/25 | Loss: 0.00041939
Iteration 19/25 | Loss: 0.00041939
Iteration 20/25 | Loss: 0.00041939
Iteration 21/25 | Loss: 0.00041939
Iteration 22/25 | Loss: 0.00041939
Iteration 23/25 | Loss: 0.00041939
Iteration 24/25 | Loss: 0.00041939
Iteration 25/25 | Loss: 0.00041939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041939
Iteration 2/1000 | Loss: 0.00002344
Iteration 3/1000 | Loss: 0.00001695
Iteration 4/1000 | Loss: 0.00001581
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001353
Iteration 14/1000 | Loss: 0.00001348
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001344
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001336
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001332
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001331
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001327
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001325
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001320
Iteration 82/1000 | Loss: 0.00001320
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001319
Iteration 85/1000 | Loss: 0.00001319
Iteration 86/1000 | Loss: 0.00001319
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001318
Iteration 89/1000 | Loss: 0.00001318
Iteration 90/1000 | Loss: 0.00001318
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001318
Iteration 93/1000 | Loss: 0.00001318
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001317
Iteration 96/1000 | Loss: 0.00001317
Iteration 97/1000 | Loss: 0.00001317
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001316
Iteration 102/1000 | Loss: 0.00001316
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001315
Iteration 105/1000 | Loss: 0.00001315
Iteration 106/1000 | Loss: 0.00001315
Iteration 107/1000 | Loss: 0.00001315
Iteration 108/1000 | Loss: 0.00001315
Iteration 109/1000 | Loss: 0.00001315
Iteration 110/1000 | Loss: 0.00001315
Iteration 111/1000 | Loss: 0.00001315
Iteration 112/1000 | Loss: 0.00001315
Iteration 113/1000 | Loss: 0.00001315
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001314
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001313
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001313
Iteration 125/1000 | Loss: 0.00001313
Iteration 126/1000 | Loss: 0.00001313
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.3130320439813659e-05, 1.3130320439813659e-05, 1.3130320439813659e-05, 1.3130320439813659e-05, 1.3130320439813659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3130320439813659e-05

Optimization complete. Final v2v error: 3.036612033843994 mm

Highest mean error: 3.4443914890289307 mm for frame 95

Lowest mean error: 2.68992018699646 mm for frame 122

Saving results

Total time: 421.06189918518066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1057
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894458
Iteration 2/25 | Loss: 0.00210199
Iteration 3/25 | Loss: 0.00115714
Iteration 4/25 | Loss: 0.00111762
Iteration 5/25 | Loss: 0.00111023
Iteration 6/25 | Loss: 0.00110776
Iteration 7/25 | Loss: 0.00110776
Iteration 8/25 | Loss: 0.00110776
Iteration 9/25 | Loss: 0.00110776
Iteration 10/25 | Loss: 0.00110776
Iteration 11/25 | Loss: 0.00110776
Iteration 12/25 | Loss: 0.00110776
Iteration 13/25 | Loss: 0.00110776
Iteration 14/25 | Loss: 0.00110776
Iteration 15/25 | Loss: 0.00110776
Iteration 16/25 | Loss: 0.00110776
Iteration 17/25 | Loss: 0.00110776
Iteration 18/25 | Loss: 0.00110776
Iteration 19/25 | Loss: 0.00110776
Iteration 20/25 | Loss: 0.00110776
Iteration 21/25 | Loss: 0.00110776
Iteration 22/25 | Loss: 0.00110776
Iteration 23/25 | Loss: 0.00110776
Iteration 24/25 | Loss: 0.00110776
Iteration 25/25 | Loss: 0.00110776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92142969
Iteration 2/25 | Loss: 0.00048892
Iteration 3/25 | Loss: 0.00048892
Iteration 4/25 | Loss: 0.00048892
Iteration 5/25 | Loss: 0.00048891
Iteration 6/25 | Loss: 0.00048891
Iteration 7/25 | Loss: 0.00048891
Iteration 8/25 | Loss: 0.00048891
Iteration 9/25 | Loss: 0.00048891
Iteration 10/25 | Loss: 0.00048891
Iteration 11/25 | Loss: 0.00048891
Iteration 12/25 | Loss: 0.00048891
Iteration 13/25 | Loss: 0.00048891
Iteration 14/25 | Loss: 0.00048891
Iteration 15/25 | Loss: 0.00048891
Iteration 16/25 | Loss: 0.00048891
Iteration 17/25 | Loss: 0.00048891
Iteration 18/25 | Loss: 0.00048891
Iteration 19/25 | Loss: 0.00048891
Iteration 20/25 | Loss: 0.00048891
Iteration 21/25 | Loss: 0.00048891
Iteration 22/25 | Loss: 0.00048891
Iteration 23/25 | Loss: 0.00048891
Iteration 24/25 | Loss: 0.00048891
Iteration 25/25 | Loss: 0.00048891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048891
Iteration 2/1000 | Loss: 0.00007315
Iteration 3/1000 | Loss: 0.00005533
Iteration 4/1000 | Loss: 0.00005008
Iteration 5/1000 | Loss: 0.00004821
Iteration 6/1000 | Loss: 0.00004705
Iteration 7/1000 | Loss: 0.00004594
Iteration 8/1000 | Loss: 0.00004492
Iteration 9/1000 | Loss: 0.00004404
Iteration 10/1000 | Loss: 0.00004310
Iteration 11/1000 | Loss: 0.00004262
Iteration 12/1000 | Loss: 0.00004226
Iteration 13/1000 | Loss: 0.00004177
Iteration 14/1000 | Loss: 0.00004135
Iteration 15/1000 | Loss: 0.00004098
Iteration 16/1000 | Loss: 0.00004064
Iteration 17/1000 | Loss: 0.00004023
Iteration 18/1000 | Loss: 0.00003989
Iteration 19/1000 | Loss: 0.00003962
Iteration 20/1000 | Loss: 0.00003942
Iteration 21/1000 | Loss: 0.00003926
Iteration 22/1000 | Loss: 0.00003906
Iteration 23/1000 | Loss: 0.00003889
Iteration 24/1000 | Loss: 0.00003877
Iteration 25/1000 | Loss: 0.00003867
Iteration 26/1000 | Loss: 0.00003862
Iteration 27/1000 | Loss: 0.00003861
Iteration 28/1000 | Loss: 0.00003857
Iteration 29/1000 | Loss: 0.00003856
Iteration 30/1000 | Loss: 0.00003853
Iteration 31/1000 | Loss: 0.00003851
Iteration 32/1000 | Loss: 0.00003850
Iteration 33/1000 | Loss: 0.00003849
Iteration 34/1000 | Loss: 0.00003849
Iteration 35/1000 | Loss: 0.00003849
Iteration 36/1000 | Loss: 0.00003849
Iteration 37/1000 | Loss: 0.00003849
Iteration 38/1000 | Loss: 0.00003849
Iteration 39/1000 | Loss: 0.00003849
Iteration 40/1000 | Loss: 0.00003849
Iteration 41/1000 | Loss: 0.00003848
Iteration 42/1000 | Loss: 0.00003848
Iteration 43/1000 | Loss: 0.00003848
Iteration 44/1000 | Loss: 0.00003844
Iteration 45/1000 | Loss: 0.00003843
Iteration 46/1000 | Loss: 0.00003842
Iteration 47/1000 | Loss: 0.00003839
Iteration 48/1000 | Loss: 0.00003839
Iteration 49/1000 | Loss: 0.00003839
Iteration 50/1000 | Loss: 0.00003836
Iteration 51/1000 | Loss: 0.00003831
Iteration 52/1000 | Loss: 0.00003831
Iteration 53/1000 | Loss: 0.00003830
Iteration 54/1000 | Loss: 0.00003830
Iteration 55/1000 | Loss: 0.00003830
Iteration 56/1000 | Loss: 0.00003830
Iteration 57/1000 | Loss: 0.00003828
Iteration 58/1000 | Loss: 0.00003828
Iteration 59/1000 | Loss: 0.00003828
Iteration 60/1000 | Loss: 0.00003828
Iteration 61/1000 | Loss: 0.00003827
Iteration 62/1000 | Loss: 0.00003827
Iteration 63/1000 | Loss: 0.00003827
Iteration 64/1000 | Loss: 0.00003826
Iteration 65/1000 | Loss: 0.00003826
Iteration 66/1000 | Loss: 0.00003826
Iteration 67/1000 | Loss: 0.00003826
Iteration 68/1000 | Loss: 0.00003825
Iteration 69/1000 | Loss: 0.00003825
Iteration 70/1000 | Loss: 0.00003825
Iteration 71/1000 | Loss: 0.00003825
Iteration 72/1000 | Loss: 0.00003825
Iteration 73/1000 | Loss: 0.00003824
Iteration 74/1000 | Loss: 0.00003824
Iteration 75/1000 | Loss: 0.00003824
Iteration 76/1000 | Loss: 0.00003824
Iteration 77/1000 | Loss: 0.00003824
Iteration 78/1000 | Loss: 0.00003824
Iteration 79/1000 | Loss: 0.00003824
Iteration 80/1000 | Loss: 0.00003824
Iteration 81/1000 | Loss: 0.00003824
Iteration 82/1000 | Loss: 0.00003824
Iteration 83/1000 | Loss: 0.00003824
Iteration 84/1000 | Loss: 0.00003824
Iteration 85/1000 | Loss: 0.00003824
Iteration 86/1000 | Loss: 0.00003823
Iteration 87/1000 | Loss: 0.00003823
Iteration 88/1000 | Loss: 0.00003823
Iteration 89/1000 | Loss: 0.00003823
Iteration 90/1000 | Loss: 0.00003823
Iteration 91/1000 | Loss: 0.00003823
Iteration 92/1000 | Loss: 0.00003822
Iteration 93/1000 | Loss: 0.00003822
Iteration 94/1000 | Loss: 0.00003822
Iteration 95/1000 | Loss: 0.00003822
Iteration 96/1000 | Loss: 0.00003822
Iteration 97/1000 | Loss: 0.00003822
Iteration 98/1000 | Loss: 0.00003822
Iteration 99/1000 | Loss: 0.00003822
Iteration 100/1000 | Loss: 0.00003822
Iteration 101/1000 | Loss: 0.00003821
Iteration 102/1000 | Loss: 0.00003821
Iteration 103/1000 | Loss: 0.00003821
Iteration 104/1000 | Loss: 0.00003821
Iteration 105/1000 | Loss: 0.00003821
Iteration 106/1000 | Loss: 0.00003821
Iteration 107/1000 | Loss: 0.00003821
Iteration 108/1000 | Loss: 0.00003821
Iteration 109/1000 | Loss: 0.00003821
Iteration 110/1000 | Loss: 0.00003821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.821233622147702e-05, 3.821233622147702e-05, 3.821233622147702e-05, 3.821233622147702e-05, 3.821233622147702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.821233622147702e-05

Optimization complete. Final v2v error: 5.0343475341796875 mm

Highest mean error: 5.637641429901123 mm for frame 16

Lowest mean error: 4.441184043884277 mm for frame 183

Saving results

Total time: 616.8345572948456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1025
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456008
Iteration 2/25 | Loss: 0.00094271
Iteration 3/25 | Loss: 0.00085546
Iteration 4/25 | Loss: 0.00084558
Iteration 5/25 | Loss: 0.00084297
Iteration 6/25 | Loss: 0.00084270
Iteration 7/25 | Loss: 0.00084270
Iteration 8/25 | Loss: 0.00084270
Iteration 9/25 | Loss: 0.00084270
Iteration 10/25 | Loss: 0.00084270
Iteration 11/25 | Loss: 0.00084270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008426977437920868, 0.0008426977437920868, 0.0008426977437920868, 0.0008426977437920868, 0.0008426977437920868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008426977437920868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47429848
Iteration 2/25 | Loss: 0.00043237
Iteration 3/25 | Loss: 0.00043237
Iteration 4/25 | Loss: 0.00043237
Iteration 5/25 | Loss: 0.00043237
Iteration 6/25 | Loss: 0.00043237
Iteration 7/25 | Loss: 0.00043237
Iteration 8/25 | Loss: 0.00043237
Iteration 9/25 | Loss: 0.00043237
Iteration 10/25 | Loss: 0.00043237
Iteration 11/25 | Loss: 0.00043237
Iteration 12/25 | Loss: 0.00043237
Iteration 13/25 | Loss: 0.00043237
Iteration 14/25 | Loss: 0.00043237
Iteration 15/25 | Loss: 0.00043237
Iteration 16/25 | Loss: 0.00043237
Iteration 17/25 | Loss: 0.00043237
Iteration 18/25 | Loss: 0.00043237
Iteration 19/25 | Loss: 0.00043237
Iteration 20/25 | Loss: 0.00043237
Iteration 21/25 | Loss: 0.00043237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004323661560192704, 0.0004323661560192704, 0.0004323661560192704, 0.0004323661560192704, 0.0004323661560192704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004323661560192704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043237
Iteration 2/1000 | Loss: 0.00001446
Iteration 3/1000 | Loss: 0.00001106
Iteration 4/1000 | Loss: 0.00001032
Iteration 5/1000 | Loss: 0.00000993
Iteration 6/1000 | Loss: 0.00000973
Iteration 7/1000 | Loss: 0.00000957
Iteration 8/1000 | Loss: 0.00000940
Iteration 9/1000 | Loss: 0.00000934
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000922
Iteration 14/1000 | Loss: 0.00000920
Iteration 15/1000 | Loss: 0.00000920
Iteration 16/1000 | Loss: 0.00000920
Iteration 17/1000 | Loss: 0.00000919
Iteration 18/1000 | Loss: 0.00000919
Iteration 19/1000 | Loss: 0.00000919
Iteration 20/1000 | Loss: 0.00000919
Iteration 21/1000 | Loss: 0.00000919
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000917
Iteration 26/1000 | Loss: 0.00000917
Iteration 27/1000 | Loss: 0.00000917
Iteration 28/1000 | Loss: 0.00000916
Iteration 29/1000 | Loss: 0.00000916
Iteration 30/1000 | Loss: 0.00000916
Iteration 31/1000 | Loss: 0.00000914
Iteration 32/1000 | Loss: 0.00000914
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000914
Iteration 35/1000 | Loss: 0.00000914
Iteration 36/1000 | Loss: 0.00000914
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000914
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000913
Iteration 43/1000 | Loss: 0.00000912
Iteration 44/1000 | Loss: 0.00000912
Iteration 45/1000 | Loss: 0.00000911
Iteration 46/1000 | Loss: 0.00000911
Iteration 47/1000 | Loss: 0.00000911
Iteration 48/1000 | Loss: 0.00000911
Iteration 49/1000 | Loss: 0.00000911
Iteration 50/1000 | Loss: 0.00000911
Iteration 51/1000 | Loss: 0.00000910
Iteration 52/1000 | Loss: 0.00000910
Iteration 53/1000 | Loss: 0.00000910
Iteration 54/1000 | Loss: 0.00000910
Iteration 55/1000 | Loss: 0.00000910
Iteration 56/1000 | Loss: 0.00000910
Iteration 57/1000 | Loss: 0.00000910
Iteration 58/1000 | Loss: 0.00000910
Iteration 59/1000 | Loss: 0.00000910
Iteration 60/1000 | Loss: 0.00000910
Iteration 61/1000 | Loss: 0.00000910
Iteration 62/1000 | Loss: 0.00000910
Iteration 63/1000 | Loss: 0.00000910
Iteration 64/1000 | Loss: 0.00000910
Iteration 65/1000 | Loss: 0.00000910
Iteration 66/1000 | Loss: 0.00000910
Iteration 67/1000 | Loss: 0.00000910
Iteration 68/1000 | Loss: 0.00000910
Iteration 69/1000 | Loss: 0.00000910
Iteration 70/1000 | Loss: 0.00000910
Iteration 71/1000 | Loss: 0.00000910
Iteration 72/1000 | Loss: 0.00000910
Iteration 73/1000 | Loss: 0.00000910
Iteration 74/1000 | Loss: 0.00000910
Iteration 75/1000 | Loss: 0.00000910
Iteration 76/1000 | Loss: 0.00000910
Iteration 77/1000 | Loss: 0.00000910
Iteration 78/1000 | Loss: 0.00000910
Iteration 79/1000 | Loss: 0.00000910
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000910
Iteration 82/1000 | Loss: 0.00000910
Iteration 83/1000 | Loss: 0.00000910
Iteration 84/1000 | Loss: 0.00000910
Iteration 85/1000 | Loss: 0.00000910
Iteration 86/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [9.102268450078554e-06, 9.102268450078554e-06, 9.102268450078554e-06, 9.102268450078554e-06, 9.102268450078554e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.102268450078554e-06

Optimization complete. Final v2v error: 2.583913803100586 mm

Highest mean error: 2.7835707664489746 mm for frame 92

Lowest mean error: 2.403129816055298 mm for frame 266

Saving results

Total time: 425.1846797466278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1055
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876206
Iteration 2/25 | Loss: 0.00122766
Iteration 3/25 | Loss: 0.00101022
Iteration 4/25 | Loss: 0.00098183
Iteration 5/25 | Loss: 0.00097607
Iteration 6/25 | Loss: 0.00097476
Iteration 7/25 | Loss: 0.00097476
Iteration 8/25 | Loss: 0.00097446
Iteration 9/25 | Loss: 0.00097446
Iteration 10/25 | Loss: 0.00097446
Iteration 11/25 | Loss: 0.00097446
Iteration 12/25 | Loss: 0.00097446
Iteration 13/25 | Loss: 0.00097446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009744598646648228, 0.0009744598646648228, 0.0009744598646648228, 0.0009744598646648228, 0.0009744598646648228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009744598646648228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34243882
Iteration 2/25 | Loss: 0.00042201
Iteration 3/25 | Loss: 0.00042201
Iteration 4/25 | Loss: 0.00042201
Iteration 5/25 | Loss: 0.00042201
Iteration 6/25 | Loss: 0.00042201
Iteration 7/25 | Loss: 0.00042201
Iteration 8/25 | Loss: 0.00042201
Iteration 9/25 | Loss: 0.00042201
Iteration 10/25 | Loss: 0.00042201
Iteration 11/25 | Loss: 0.00042201
Iteration 12/25 | Loss: 0.00042201
Iteration 13/25 | Loss: 0.00042201
Iteration 14/25 | Loss: 0.00042201
Iteration 15/25 | Loss: 0.00042201
Iteration 16/25 | Loss: 0.00042201
Iteration 17/25 | Loss: 0.00042201
Iteration 18/25 | Loss: 0.00042201
Iteration 19/25 | Loss: 0.00042201
Iteration 20/25 | Loss: 0.00042201
Iteration 21/25 | Loss: 0.00042201
Iteration 22/25 | Loss: 0.00042201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004220086266286671, 0.0004220086266286671, 0.0004220086266286671, 0.0004220086266286671, 0.0004220086266286671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004220086266286671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042201
Iteration 2/1000 | Loss: 0.00005525
Iteration 3/1000 | Loss: 0.00003879
Iteration 4/1000 | Loss: 0.00003304
Iteration 5/1000 | Loss: 0.00003122
Iteration 6/1000 | Loss: 0.00003000
Iteration 7/1000 | Loss: 0.00002903
Iteration 8/1000 | Loss: 0.00002855
Iteration 9/1000 | Loss: 0.00002802
Iteration 10/1000 | Loss: 0.00002772
Iteration 11/1000 | Loss: 0.00002746
Iteration 12/1000 | Loss: 0.00002725
Iteration 13/1000 | Loss: 0.00002723
Iteration 14/1000 | Loss: 0.00002710
Iteration 15/1000 | Loss: 0.00002695
Iteration 16/1000 | Loss: 0.00002685
Iteration 17/1000 | Loss: 0.00002682
Iteration 18/1000 | Loss: 0.00002679
Iteration 19/1000 | Loss: 0.00002678
Iteration 20/1000 | Loss: 0.00002678
Iteration 21/1000 | Loss: 0.00002677
Iteration 22/1000 | Loss: 0.00002677
Iteration 23/1000 | Loss: 0.00002672
Iteration 24/1000 | Loss: 0.00002672
Iteration 25/1000 | Loss: 0.00002672
Iteration 26/1000 | Loss: 0.00002671
Iteration 27/1000 | Loss: 0.00002671
Iteration 28/1000 | Loss: 0.00002670
Iteration 29/1000 | Loss: 0.00002670
Iteration 30/1000 | Loss: 0.00002669
Iteration 31/1000 | Loss: 0.00002668
Iteration 32/1000 | Loss: 0.00002667
Iteration 33/1000 | Loss: 0.00002666
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002662
Iteration 36/1000 | Loss: 0.00002662
Iteration 37/1000 | Loss: 0.00002661
Iteration 38/1000 | Loss: 0.00002661
Iteration 39/1000 | Loss: 0.00002660
Iteration 40/1000 | Loss: 0.00002660
Iteration 41/1000 | Loss: 0.00002660
Iteration 42/1000 | Loss: 0.00002659
Iteration 43/1000 | Loss: 0.00002659
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00002659
Iteration 46/1000 | Loss: 0.00002658
Iteration 47/1000 | Loss: 0.00002658
Iteration 48/1000 | Loss: 0.00002658
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002657
Iteration 51/1000 | Loss: 0.00002657
Iteration 52/1000 | Loss: 0.00002657
Iteration 53/1000 | Loss: 0.00002657
Iteration 54/1000 | Loss: 0.00002657
Iteration 55/1000 | Loss: 0.00002657
Iteration 56/1000 | Loss: 0.00002656
Iteration 57/1000 | Loss: 0.00002656
Iteration 58/1000 | Loss: 0.00002656
Iteration 59/1000 | Loss: 0.00002656
Iteration 60/1000 | Loss: 0.00002656
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00002655
Iteration 63/1000 | Loss: 0.00002655
Iteration 64/1000 | Loss: 0.00002655
Iteration 65/1000 | Loss: 0.00002655
Iteration 66/1000 | Loss: 0.00002655
Iteration 67/1000 | Loss: 0.00002655
Iteration 68/1000 | Loss: 0.00002655
Iteration 69/1000 | Loss: 0.00002655
Iteration 70/1000 | Loss: 0.00002654
Iteration 71/1000 | Loss: 0.00002654
Iteration 72/1000 | Loss: 0.00002654
Iteration 73/1000 | Loss: 0.00002654
Iteration 74/1000 | Loss: 0.00002654
Iteration 75/1000 | Loss: 0.00002654
Iteration 76/1000 | Loss: 0.00002654
Iteration 77/1000 | Loss: 0.00002654
Iteration 78/1000 | Loss: 0.00002654
Iteration 79/1000 | Loss: 0.00002654
Iteration 80/1000 | Loss: 0.00002653
Iteration 81/1000 | Loss: 0.00002653
Iteration 82/1000 | Loss: 0.00002653
Iteration 83/1000 | Loss: 0.00002653
Iteration 84/1000 | Loss: 0.00002652
Iteration 85/1000 | Loss: 0.00002652
Iteration 86/1000 | Loss: 0.00002652
Iteration 87/1000 | Loss: 0.00002651
Iteration 88/1000 | Loss: 0.00002651
Iteration 89/1000 | Loss: 0.00002651
Iteration 90/1000 | Loss: 0.00002650
Iteration 91/1000 | Loss: 0.00002650
Iteration 92/1000 | Loss: 0.00002650
Iteration 93/1000 | Loss: 0.00002650
Iteration 94/1000 | Loss: 0.00002649
Iteration 95/1000 | Loss: 0.00002649
Iteration 96/1000 | Loss: 0.00002649
Iteration 97/1000 | Loss: 0.00002649
Iteration 98/1000 | Loss: 0.00002649
Iteration 99/1000 | Loss: 0.00002649
Iteration 100/1000 | Loss: 0.00002648
Iteration 101/1000 | Loss: 0.00002648
Iteration 102/1000 | Loss: 0.00002648
Iteration 103/1000 | Loss: 0.00002648
Iteration 104/1000 | Loss: 0.00002648
Iteration 105/1000 | Loss: 0.00002648
Iteration 106/1000 | Loss: 0.00002648
Iteration 107/1000 | Loss: 0.00002648
Iteration 108/1000 | Loss: 0.00002648
Iteration 109/1000 | Loss: 0.00002648
Iteration 110/1000 | Loss: 0.00002648
Iteration 111/1000 | Loss: 0.00002648
Iteration 112/1000 | Loss: 0.00002648
Iteration 113/1000 | Loss: 0.00002648
Iteration 114/1000 | Loss: 0.00002648
Iteration 115/1000 | Loss: 0.00002648
Iteration 116/1000 | Loss: 0.00002648
Iteration 117/1000 | Loss: 0.00002648
Iteration 118/1000 | Loss: 0.00002647
Iteration 119/1000 | Loss: 0.00002647
Iteration 120/1000 | Loss: 0.00002647
Iteration 121/1000 | Loss: 0.00002647
Iteration 122/1000 | Loss: 0.00002647
Iteration 123/1000 | Loss: 0.00002647
Iteration 124/1000 | Loss: 0.00002647
Iteration 125/1000 | Loss: 0.00002647
Iteration 126/1000 | Loss: 0.00002646
Iteration 127/1000 | Loss: 0.00002646
Iteration 128/1000 | Loss: 0.00002646
Iteration 129/1000 | Loss: 0.00002646
Iteration 130/1000 | Loss: 0.00002646
Iteration 131/1000 | Loss: 0.00002646
Iteration 132/1000 | Loss: 0.00002646
Iteration 133/1000 | Loss: 0.00002646
Iteration 134/1000 | Loss: 0.00002646
Iteration 135/1000 | Loss: 0.00002645
Iteration 136/1000 | Loss: 0.00002645
Iteration 137/1000 | Loss: 0.00002645
Iteration 138/1000 | Loss: 0.00002645
Iteration 139/1000 | Loss: 0.00002645
Iteration 140/1000 | Loss: 0.00002645
Iteration 141/1000 | Loss: 0.00002645
Iteration 142/1000 | Loss: 0.00002645
Iteration 143/1000 | Loss: 0.00002645
Iteration 144/1000 | Loss: 0.00002645
Iteration 145/1000 | Loss: 0.00002645
Iteration 146/1000 | Loss: 0.00002645
Iteration 147/1000 | Loss: 0.00002645
Iteration 148/1000 | Loss: 0.00002645
Iteration 149/1000 | Loss: 0.00002644
Iteration 150/1000 | Loss: 0.00002644
Iteration 151/1000 | Loss: 0.00002644
Iteration 152/1000 | Loss: 0.00002644
Iteration 153/1000 | Loss: 0.00002644
Iteration 154/1000 | Loss: 0.00002644
Iteration 155/1000 | Loss: 0.00002644
Iteration 156/1000 | Loss: 0.00002643
Iteration 157/1000 | Loss: 0.00002643
Iteration 158/1000 | Loss: 0.00002643
Iteration 159/1000 | Loss: 0.00002643
Iteration 160/1000 | Loss: 0.00002643
Iteration 161/1000 | Loss: 0.00002643
Iteration 162/1000 | Loss: 0.00002643
Iteration 163/1000 | Loss: 0.00002643
Iteration 164/1000 | Loss: 0.00002643
Iteration 165/1000 | Loss: 0.00002643
Iteration 166/1000 | Loss: 0.00002643
Iteration 167/1000 | Loss: 0.00002643
Iteration 168/1000 | Loss: 0.00002643
Iteration 169/1000 | Loss: 0.00002642
Iteration 170/1000 | Loss: 0.00002642
Iteration 171/1000 | Loss: 0.00002642
Iteration 172/1000 | Loss: 0.00002642
Iteration 173/1000 | Loss: 0.00002642
Iteration 174/1000 | Loss: 0.00002642
Iteration 175/1000 | Loss: 0.00002642
Iteration 176/1000 | Loss: 0.00002641
Iteration 177/1000 | Loss: 0.00002641
Iteration 178/1000 | Loss: 0.00002641
Iteration 179/1000 | Loss: 0.00002641
Iteration 180/1000 | Loss: 0.00002641
Iteration 181/1000 | Loss: 0.00002641
Iteration 182/1000 | Loss: 0.00002641
Iteration 183/1000 | Loss: 0.00002641
Iteration 184/1000 | Loss: 0.00002641
Iteration 185/1000 | Loss: 0.00002641
Iteration 186/1000 | Loss: 0.00002641
Iteration 187/1000 | Loss: 0.00002641
Iteration 188/1000 | Loss: 0.00002641
Iteration 189/1000 | Loss: 0.00002641
Iteration 190/1000 | Loss: 0.00002641
Iteration 191/1000 | Loss: 0.00002641
Iteration 192/1000 | Loss: 0.00002641
Iteration 193/1000 | Loss: 0.00002641
Iteration 194/1000 | Loss: 0.00002641
Iteration 195/1000 | Loss: 0.00002641
Iteration 196/1000 | Loss: 0.00002641
Iteration 197/1000 | Loss: 0.00002641
Iteration 198/1000 | Loss: 0.00002641
Iteration 199/1000 | Loss: 0.00002641
Iteration 200/1000 | Loss: 0.00002641
Iteration 201/1000 | Loss: 0.00002641
Iteration 202/1000 | Loss: 0.00002641
Iteration 203/1000 | Loss: 0.00002641
Iteration 204/1000 | Loss: 0.00002641
Iteration 205/1000 | Loss: 0.00002641
Iteration 206/1000 | Loss: 0.00002641
Iteration 207/1000 | Loss: 0.00002641
Iteration 208/1000 | Loss: 0.00002641
Iteration 209/1000 | Loss: 0.00002641
Iteration 210/1000 | Loss: 0.00002641
Iteration 211/1000 | Loss: 0.00002641
Iteration 212/1000 | Loss: 0.00002641
Iteration 213/1000 | Loss: 0.00002641
Iteration 214/1000 | Loss: 0.00002641
Iteration 215/1000 | Loss: 0.00002641
Iteration 216/1000 | Loss: 0.00002641
Iteration 217/1000 | Loss: 0.00002641
Iteration 218/1000 | Loss: 0.00002641
Iteration 219/1000 | Loss: 0.00002641
Iteration 220/1000 | Loss: 0.00002641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.6411864382680506e-05, 2.6411864382680506e-05, 2.6411864382680506e-05, 2.6411864382680506e-05, 2.6411864382680506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6411864382680506e-05

Optimization complete. Final v2v error: 4.216653347015381 mm

Highest mean error: 4.622812747955322 mm for frame 20

Lowest mean error: 3.7366645336151123 mm for frame 87

Saving results

Total time: 225.24858736991882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1013
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456587
Iteration 2/25 | Loss: 0.00100812
Iteration 3/25 | Loss: 0.00090288
Iteration 4/25 | Loss: 0.00088550
Iteration 5/25 | Loss: 0.00088061
Iteration 6/25 | Loss: 0.00087995
Iteration 7/25 | Loss: 0.00087995
Iteration 8/25 | Loss: 0.00087995
Iteration 9/25 | Loss: 0.00087995
Iteration 10/25 | Loss: 0.00087995
Iteration 11/25 | Loss: 0.00087995
Iteration 12/25 | Loss: 0.00087995
Iteration 13/25 | Loss: 0.00087995
Iteration 14/25 | Loss: 0.00087995
Iteration 15/25 | Loss: 0.00087995
Iteration 16/25 | Loss: 0.00087995
Iteration 17/25 | Loss: 0.00087995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008799476199783385, 0.0008799476199783385, 0.0008799476199783385, 0.0008799476199783385, 0.0008799476199783385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008799476199783385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.82548380
Iteration 2/25 | Loss: 0.00060335
Iteration 3/25 | Loss: 0.00060334
Iteration 4/25 | Loss: 0.00060334
Iteration 5/25 | Loss: 0.00060334
Iteration 6/25 | Loss: 0.00060334
Iteration 7/25 | Loss: 0.00060334
Iteration 8/25 | Loss: 0.00060334
Iteration 9/25 | Loss: 0.00060334
Iteration 10/25 | Loss: 0.00060334
Iteration 11/25 | Loss: 0.00060334
Iteration 12/25 | Loss: 0.00060334
Iteration 13/25 | Loss: 0.00060334
Iteration 14/25 | Loss: 0.00060334
Iteration 15/25 | Loss: 0.00060334
Iteration 16/25 | Loss: 0.00060334
Iteration 17/25 | Loss: 0.00060334
Iteration 18/25 | Loss: 0.00060334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006033405661582947, 0.0006033405661582947, 0.0006033405661582947, 0.0006033405661582947, 0.0006033405661582947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006033405661582947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060334
Iteration 2/1000 | Loss: 0.00002232
Iteration 3/1000 | Loss: 0.00001785
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001501
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001487
Iteration 19/1000 | Loss: 0.00001485
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001485
Iteration 22/1000 | Loss: 0.00001485
Iteration 23/1000 | Loss: 0.00001485
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001484
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001481
Iteration 33/1000 | Loss: 0.00001481
Iteration 34/1000 | Loss: 0.00001481
Iteration 35/1000 | Loss: 0.00001480
Iteration 36/1000 | Loss: 0.00001480
Iteration 37/1000 | Loss: 0.00001480
Iteration 38/1000 | Loss: 0.00001480
Iteration 39/1000 | Loss: 0.00001479
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.4785089661017992e-05, 1.4785089661017992e-05, 1.4785089661017992e-05, 1.4785089661017992e-05, 1.4785089661017992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4785089661017992e-05

Optimization complete. Final v2v error: 3.228386402130127 mm

Highest mean error: 3.7685484886169434 mm for frame 228

Lowest mean error: 2.8740742206573486 mm for frame 252

Saving results

Total time: 394.10306763648987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1056
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408152
Iteration 2/25 | Loss: 0.00091468
Iteration 3/25 | Loss: 0.00083447
Iteration 4/25 | Loss: 0.00082444
Iteration 5/25 | Loss: 0.00082120
Iteration 6/25 | Loss: 0.00082067
Iteration 7/25 | Loss: 0.00082067
Iteration 8/25 | Loss: 0.00082067
Iteration 9/25 | Loss: 0.00082067
Iteration 10/25 | Loss: 0.00082067
Iteration 11/25 | Loss: 0.00082067
Iteration 12/25 | Loss: 0.00082067
Iteration 13/25 | Loss: 0.00082067
Iteration 14/25 | Loss: 0.00082067
Iteration 15/25 | Loss: 0.00082067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008206694619730115, 0.0008206694619730115, 0.0008206694619730115, 0.0008206694619730115, 0.0008206694619730115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008206694619730115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.47866178
Iteration 2/25 | Loss: 0.00046517
Iteration 3/25 | Loss: 0.00046517
Iteration 4/25 | Loss: 0.00046517
Iteration 5/25 | Loss: 0.00046517
Iteration 6/25 | Loss: 0.00046517
Iteration 7/25 | Loss: 0.00046517
Iteration 8/25 | Loss: 0.00046517
Iteration 9/25 | Loss: 0.00046517
Iteration 10/25 | Loss: 0.00046517
Iteration 11/25 | Loss: 0.00046517
Iteration 12/25 | Loss: 0.00046517
Iteration 13/25 | Loss: 0.00046517
Iteration 14/25 | Loss: 0.00046517
Iteration 15/25 | Loss: 0.00046517
Iteration 16/25 | Loss: 0.00046517
Iteration 17/25 | Loss: 0.00046517
Iteration 18/25 | Loss: 0.00046517
Iteration 19/25 | Loss: 0.00046517
Iteration 20/25 | Loss: 0.00046517
Iteration 21/25 | Loss: 0.00046517
Iteration 22/25 | Loss: 0.00046517
Iteration 23/25 | Loss: 0.00046517
Iteration 24/25 | Loss: 0.00046517
Iteration 25/25 | Loss: 0.00046517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046517
Iteration 2/1000 | Loss: 0.00001680
Iteration 3/1000 | Loss: 0.00001081
Iteration 4/1000 | Loss: 0.00000981
Iteration 5/1000 | Loss: 0.00000939
Iteration 6/1000 | Loss: 0.00000910
Iteration 7/1000 | Loss: 0.00000898
Iteration 8/1000 | Loss: 0.00000891
Iteration 9/1000 | Loss: 0.00000891
Iteration 10/1000 | Loss: 0.00000890
Iteration 11/1000 | Loss: 0.00000889
Iteration 12/1000 | Loss: 0.00000888
Iteration 13/1000 | Loss: 0.00000881
Iteration 14/1000 | Loss: 0.00000876
Iteration 15/1000 | Loss: 0.00000863
Iteration 16/1000 | Loss: 0.00000860
Iteration 17/1000 | Loss: 0.00000859
Iteration 18/1000 | Loss: 0.00000858
Iteration 19/1000 | Loss: 0.00000856
Iteration 20/1000 | Loss: 0.00000855
Iteration 21/1000 | Loss: 0.00000855
Iteration 22/1000 | Loss: 0.00000855
Iteration 23/1000 | Loss: 0.00000855
Iteration 24/1000 | Loss: 0.00000855
Iteration 25/1000 | Loss: 0.00000854
Iteration 26/1000 | Loss: 0.00000854
Iteration 27/1000 | Loss: 0.00000854
Iteration 28/1000 | Loss: 0.00000854
Iteration 29/1000 | Loss: 0.00000854
Iteration 30/1000 | Loss: 0.00000853
Iteration 31/1000 | Loss: 0.00000852
Iteration 32/1000 | Loss: 0.00000852
Iteration 33/1000 | Loss: 0.00000851
Iteration 34/1000 | Loss: 0.00000851
Iteration 35/1000 | Loss: 0.00000851
Iteration 36/1000 | Loss: 0.00000851
Iteration 37/1000 | Loss: 0.00000850
Iteration 38/1000 | Loss: 0.00000849
Iteration 39/1000 | Loss: 0.00000849
Iteration 40/1000 | Loss: 0.00000848
Iteration 41/1000 | Loss: 0.00000847
Iteration 42/1000 | Loss: 0.00000846
Iteration 43/1000 | Loss: 0.00000846
Iteration 44/1000 | Loss: 0.00000846
Iteration 45/1000 | Loss: 0.00000845
Iteration 46/1000 | Loss: 0.00000845
Iteration 47/1000 | Loss: 0.00000845
Iteration 48/1000 | Loss: 0.00000844
Iteration 49/1000 | Loss: 0.00000844
Iteration 50/1000 | Loss: 0.00000844
Iteration 51/1000 | Loss: 0.00000843
Iteration 52/1000 | Loss: 0.00000843
Iteration 53/1000 | Loss: 0.00000843
Iteration 54/1000 | Loss: 0.00000843
Iteration 55/1000 | Loss: 0.00000843
Iteration 56/1000 | Loss: 0.00000843
Iteration 57/1000 | Loss: 0.00000843
Iteration 58/1000 | Loss: 0.00000843
Iteration 59/1000 | Loss: 0.00000843
Iteration 60/1000 | Loss: 0.00000842
Iteration 61/1000 | Loss: 0.00000842
Iteration 62/1000 | Loss: 0.00000841
Iteration 63/1000 | Loss: 0.00000841
Iteration 64/1000 | Loss: 0.00000841
Iteration 65/1000 | Loss: 0.00000840
Iteration 66/1000 | Loss: 0.00000840
Iteration 67/1000 | Loss: 0.00000840
Iteration 68/1000 | Loss: 0.00000840
Iteration 69/1000 | Loss: 0.00000840
Iteration 70/1000 | Loss: 0.00000840
Iteration 71/1000 | Loss: 0.00000840
Iteration 72/1000 | Loss: 0.00000840
Iteration 73/1000 | Loss: 0.00000840
Iteration 74/1000 | Loss: 0.00000840
Iteration 75/1000 | Loss: 0.00000840
Iteration 76/1000 | Loss: 0.00000840
Iteration 77/1000 | Loss: 0.00000840
Iteration 78/1000 | Loss: 0.00000840
Iteration 79/1000 | Loss: 0.00000840
Iteration 80/1000 | Loss: 0.00000840
Iteration 81/1000 | Loss: 0.00000839
Iteration 82/1000 | Loss: 0.00000839
Iteration 83/1000 | Loss: 0.00000839
Iteration 84/1000 | Loss: 0.00000839
Iteration 85/1000 | Loss: 0.00000838
Iteration 86/1000 | Loss: 0.00000838
Iteration 87/1000 | Loss: 0.00000838
Iteration 88/1000 | Loss: 0.00000838
Iteration 89/1000 | Loss: 0.00000837
Iteration 90/1000 | Loss: 0.00000837
Iteration 91/1000 | Loss: 0.00000837
Iteration 92/1000 | Loss: 0.00000837
Iteration 93/1000 | Loss: 0.00000837
Iteration 94/1000 | Loss: 0.00000837
Iteration 95/1000 | Loss: 0.00000837
Iteration 96/1000 | Loss: 0.00000837
Iteration 97/1000 | Loss: 0.00000837
Iteration 98/1000 | Loss: 0.00000837
Iteration 99/1000 | Loss: 0.00000837
Iteration 100/1000 | Loss: 0.00000837
Iteration 101/1000 | Loss: 0.00000837
Iteration 102/1000 | Loss: 0.00000837
Iteration 103/1000 | Loss: 0.00000836
Iteration 104/1000 | Loss: 0.00000836
Iteration 105/1000 | Loss: 0.00000836
Iteration 106/1000 | Loss: 0.00000836
Iteration 107/1000 | Loss: 0.00000836
Iteration 108/1000 | Loss: 0.00000836
Iteration 109/1000 | Loss: 0.00000836
Iteration 110/1000 | Loss: 0.00000836
Iteration 111/1000 | Loss: 0.00000836
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000835
Iteration 115/1000 | Loss: 0.00000835
Iteration 116/1000 | Loss: 0.00000835
Iteration 117/1000 | Loss: 0.00000835
Iteration 118/1000 | Loss: 0.00000835
Iteration 119/1000 | Loss: 0.00000834
Iteration 120/1000 | Loss: 0.00000834
Iteration 121/1000 | Loss: 0.00000834
Iteration 122/1000 | Loss: 0.00000834
Iteration 123/1000 | Loss: 0.00000834
Iteration 124/1000 | Loss: 0.00000833
Iteration 125/1000 | Loss: 0.00000833
Iteration 126/1000 | Loss: 0.00000833
Iteration 127/1000 | Loss: 0.00000833
Iteration 128/1000 | Loss: 0.00000833
Iteration 129/1000 | Loss: 0.00000833
Iteration 130/1000 | Loss: 0.00000833
Iteration 131/1000 | Loss: 0.00000833
Iteration 132/1000 | Loss: 0.00000833
Iteration 133/1000 | Loss: 0.00000832
Iteration 134/1000 | Loss: 0.00000832
Iteration 135/1000 | Loss: 0.00000832
Iteration 136/1000 | Loss: 0.00000832
Iteration 137/1000 | Loss: 0.00000832
Iteration 138/1000 | Loss: 0.00000832
Iteration 139/1000 | Loss: 0.00000832
Iteration 140/1000 | Loss: 0.00000832
Iteration 141/1000 | Loss: 0.00000832
Iteration 142/1000 | Loss: 0.00000832
Iteration 143/1000 | Loss: 0.00000832
Iteration 144/1000 | Loss: 0.00000832
Iteration 145/1000 | Loss: 0.00000832
Iteration 146/1000 | Loss: 0.00000832
Iteration 147/1000 | Loss: 0.00000832
Iteration 148/1000 | Loss: 0.00000832
Iteration 149/1000 | Loss: 0.00000832
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000831
Iteration 152/1000 | Loss: 0.00000831
Iteration 153/1000 | Loss: 0.00000831
Iteration 154/1000 | Loss: 0.00000831
Iteration 155/1000 | Loss: 0.00000831
Iteration 156/1000 | Loss: 0.00000831
Iteration 157/1000 | Loss: 0.00000831
Iteration 158/1000 | Loss: 0.00000831
Iteration 159/1000 | Loss: 0.00000831
Iteration 160/1000 | Loss: 0.00000831
Iteration 161/1000 | Loss: 0.00000831
Iteration 162/1000 | Loss: 0.00000830
Iteration 163/1000 | Loss: 0.00000830
Iteration 164/1000 | Loss: 0.00000830
Iteration 165/1000 | Loss: 0.00000830
Iteration 166/1000 | Loss: 0.00000830
Iteration 167/1000 | Loss: 0.00000830
Iteration 168/1000 | Loss: 0.00000830
Iteration 169/1000 | Loss: 0.00000830
Iteration 170/1000 | Loss: 0.00000830
Iteration 171/1000 | Loss: 0.00000830
Iteration 172/1000 | Loss: 0.00000830
Iteration 173/1000 | Loss: 0.00000830
Iteration 174/1000 | Loss: 0.00000830
Iteration 175/1000 | Loss: 0.00000830
Iteration 176/1000 | Loss: 0.00000830
Iteration 177/1000 | Loss: 0.00000829
Iteration 178/1000 | Loss: 0.00000829
Iteration 179/1000 | Loss: 0.00000829
Iteration 180/1000 | Loss: 0.00000829
Iteration 181/1000 | Loss: 0.00000829
Iteration 182/1000 | Loss: 0.00000829
Iteration 183/1000 | Loss: 0.00000829
Iteration 184/1000 | Loss: 0.00000829
Iteration 185/1000 | Loss: 0.00000829
Iteration 186/1000 | Loss: 0.00000829
Iteration 187/1000 | Loss: 0.00000829
Iteration 188/1000 | Loss: 0.00000828
Iteration 189/1000 | Loss: 0.00000828
Iteration 190/1000 | Loss: 0.00000828
Iteration 191/1000 | Loss: 0.00000828
Iteration 192/1000 | Loss: 0.00000828
Iteration 193/1000 | Loss: 0.00000828
Iteration 194/1000 | Loss: 0.00000828
Iteration 195/1000 | Loss: 0.00000828
Iteration 196/1000 | Loss: 0.00000828
Iteration 197/1000 | Loss: 0.00000828
Iteration 198/1000 | Loss: 0.00000828
Iteration 199/1000 | Loss: 0.00000828
Iteration 200/1000 | Loss: 0.00000828
Iteration 201/1000 | Loss: 0.00000828
Iteration 202/1000 | Loss: 0.00000828
Iteration 203/1000 | Loss: 0.00000828
Iteration 204/1000 | Loss: 0.00000828
Iteration 205/1000 | Loss: 0.00000828
Iteration 206/1000 | Loss: 0.00000828
Iteration 207/1000 | Loss: 0.00000828
Iteration 208/1000 | Loss: 0.00000828
Iteration 209/1000 | Loss: 0.00000828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [8.279173925984651e-06, 8.279173925984651e-06, 8.279173925984651e-06, 8.279173925984651e-06, 8.279173925984651e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.279173925984651e-06

Optimization complete. Final v2v error: 2.4205079078674316 mm

Highest mean error: 2.7372941970825195 mm for frame 156

Lowest mean error: 2.226848840713501 mm for frame 134

Saving results

Total time: 366.1177008152008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1099
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526714
Iteration 2/25 | Loss: 0.00118062
Iteration 3/25 | Loss: 0.00105198
Iteration 4/25 | Loss: 0.00103482
Iteration 5/25 | Loss: 0.00102939
Iteration 6/25 | Loss: 0.00102939
Iteration 7/25 | Loss: 0.00102939
Iteration 8/25 | Loss: 0.00102939
Iteration 9/25 | Loss: 0.00102939
Iteration 10/25 | Loss: 0.00102939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00102938711643219, 0.00102938711643219, 0.00102938711643219, 0.00102938711643219, 0.00102938711643219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00102938711643219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78022450
Iteration 2/25 | Loss: 0.00042697
Iteration 3/25 | Loss: 0.00042696
Iteration 4/25 | Loss: 0.00042696
Iteration 5/25 | Loss: 0.00042696
Iteration 6/25 | Loss: 0.00042696
Iteration 7/25 | Loss: 0.00042696
Iteration 8/25 | Loss: 0.00042696
Iteration 9/25 | Loss: 0.00042696
Iteration 10/25 | Loss: 0.00042696
Iteration 11/25 | Loss: 0.00042696
Iteration 12/25 | Loss: 0.00042696
Iteration 13/25 | Loss: 0.00042696
Iteration 14/25 | Loss: 0.00042696
Iteration 15/25 | Loss: 0.00042696
Iteration 16/25 | Loss: 0.00042696
Iteration 17/25 | Loss: 0.00042696
Iteration 18/25 | Loss: 0.00042696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004269612254574895, 0.0004269612254574895, 0.0004269612254574895, 0.0004269612254574895, 0.0004269612254574895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004269612254574895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042696
Iteration 2/1000 | Loss: 0.00004142
Iteration 3/1000 | Loss: 0.00003779
Iteration 4/1000 | Loss: 0.00003614
Iteration 5/1000 | Loss: 0.00003514
Iteration 6/1000 | Loss: 0.00003470
Iteration 7/1000 | Loss: 0.00003416
Iteration 8/1000 | Loss: 0.00003381
Iteration 9/1000 | Loss: 0.00003353
Iteration 10/1000 | Loss: 0.00003329
Iteration 11/1000 | Loss: 0.00003311
Iteration 12/1000 | Loss: 0.00003279
Iteration 13/1000 | Loss: 0.00003256
Iteration 14/1000 | Loss: 0.00003240
Iteration 15/1000 | Loss: 0.00003224
Iteration 16/1000 | Loss: 0.00003220
Iteration 17/1000 | Loss: 0.00003220
Iteration 18/1000 | Loss: 0.00003220
Iteration 19/1000 | Loss: 0.00003220
Iteration 20/1000 | Loss: 0.00003220
Iteration 21/1000 | Loss: 0.00003220
Iteration 22/1000 | Loss: 0.00003219
Iteration 23/1000 | Loss: 0.00003219
Iteration 24/1000 | Loss: 0.00003219
Iteration 25/1000 | Loss: 0.00003219
Iteration 26/1000 | Loss: 0.00003219
Iteration 27/1000 | Loss: 0.00003219
Iteration 28/1000 | Loss: 0.00003219
Iteration 29/1000 | Loss: 0.00003219
Iteration 30/1000 | Loss: 0.00003219
Iteration 31/1000 | Loss: 0.00003219
Iteration 32/1000 | Loss: 0.00003219
Iteration 33/1000 | Loss: 0.00003218
Iteration 34/1000 | Loss: 0.00003217
Iteration 35/1000 | Loss: 0.00003212
Iteration 36/1000 | Loss: 0.00003209
Iteration 37/1000 | Loss: 0.00003198
Iteration 38/1000 | Loss: 0.00003196
Iteration 39/1000 | Loss: 0.00003194
Iteration 40/1000 | Loss: 0.00003194
Iteration 41/1000 | Loss: 0.00003194
Iteration 42/1000 | Loss: 0.00003194
Iteration 43/1000 | Loss: 0.00003194
Iteration 44/1000 | Loss: 0.00003194
Iteration 45/1000 | Loss: 0.00003194
Iteration 46/1000 | Loss: 0.00003194
Iteration 47/1000 | Loss: 0.00003194
Iteration 48/1000 | Loss: 0.00003194
Iteration 49/1000 | Loss: 0.00003194
Iteration 50/1000 | Loss: 0.00003194
Iteration 51/1000 | Loss: 0.00003194
Iteration 52/1000 | Loss: 0.00003194
Iteration 53/1000 | Loss: 0.00003194
Iteration 54/1000 | Loss: 0.00003194
Iteration 55/1000 | Loss: 0.00003194
Iteration 56/1000 | Loss: 0.00003194
Iteration 57/1000 | Loss: 0.00003194
Iteration 58/1000 | Loss: 0.00003194
Iteration 59/1000 | Loss: 0.00003194
Iteration 60/1000 | Loss: 0.00003194
Iteration 61/1000 | Loss: 0.00003194
Iteration 62/1000 | Loss: 0.00003194
Iteration 63/1000 | Loss: 0.00003194
Iteration 64/1000 | Loss: 0.00003194
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00003194
Iteration 67/1000 | Loss: 0.00003194
Iteration 68/1000 | Loss: 0.00003194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [3.1936568120727316e-05, 3.1936568120727316e-05, 3.1936568120727316e-05, 3.1936568120727316e-05, 3.1936568120727316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1936568120727316e-05

Optimization complete. Final v2v error: 4.388833045959473 mm

Highest mean error: 4.4207682609558105 mm for frame 224

Lowest mean error: 4.359570026397705 mm for frame 263

Saving results

Total time: 441.9779887199402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1083
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376845
Iteration 2/25 | Loss: 0.00090620
Iteration 3/25 | Loss: 0.00082184
Iteration 4/25 | Loss: 0.00081001
Iteration 5/25 | Loss: 0.00080709
Iteration 6/25 | Loss: 0.00080709
Iteration 7/25 | Loss: 0.00080709
Iteration 8/25 | Loss: 0.00080709
Iteration 9/25 | Loss: 0.00080709
Iteration 10/25 | Loss: 0.00080709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008070891490206122, 0.0008070891490206122, 0.0008070891490206122, 0.0008070891490206122, 0.0008070891490206122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008070891490206122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37786484
Iteration 2/25 | Loss: 0.00040050
Iteration 3/25 | Loss: 0.00040050
Iteration 4/25 | Loss: 0.00040050
Iteration 5/25 | Loss: 0.00040050
Iteration 6/25 | Loss: 0.00040050
Iteration 7/25 | Loss: 0.00040050
Iteration 8/25 | Loss: 0.00040050
Iteration 9/25 | Loss: 0.00040050
Iteration 10/25 | Loss: 0.00040050
Iteration 11/25 | Loss: 0.00040050
Iteration 12/25 | Loss: 0.00040050
Iteration 13/25 | Loss: 0.00040050
Iteration 14/25 | Loss: 0.00040050
Iteration 15/25 | Loss: 0.00040050
Iteration 16/25 | Loss: 0.00040050
Iteration 17/25 | Loss: 0.00040050
Iteration 18/25 | Loss: 0.00040050
Iteration 19/25 | Loss: 0.00040050
Iteration 20/25 | Loss: 0.00040050
Iteration 21/25 | Loss: 0.00040050
Iteration 22/25 | Loss: 0.00040050
Iteration 23/25 | Loss: 0.00040050
Iteration 24/25 | Loss: 0.00040050
Iteration 25/25 | Loss: 0.00040050

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040050
Iteration 2/1000 | Loss: 0.00001234
Iteration 3/1000 | Loss: 0.00000969
Iteration 4/1000 | Loss: 0.00000890
Iteration 5/1000 | Loss: 0.00000853
Iteration 6/1000 | Loss: 0.00000832
Iteration 7/1000 | Loss: 0.00000819
Iteration 8/1000 | Loss: 0.00000807
Iteration 9/1000 | Loss: 0.00000798
Iteration 10/1000 | Loss: 0.00000798
Iteration 11/1000 | Loss: 0.00000798
Iteration 12/1000 | Loss: 0.00000798
Iteration 13/1000 | Loss: 0.00000797
Iteration 14/1000 | Loss: 0.00000797
Iteration 15/1000 | Loss: 0.00000797
Iteration 16/1000 | Loss: 0.00000797
Iteration 17/1000 | Loss: 0.00000797
Iteration 18/1000 | Loss: 0.00000797
Iteration 19/1000 | Loss: 0.00000789
Iteration 20/1000 | Loss: 0.00000789
Iteration 21/1000 | Loss: 0.00000789
Iteration 22/1000 | Loss: 0.00000789
Iteration 23/1000 | Loss: 0.00000789
Iteration 24/1000 | Loss: 0.00000789
Iteration 25/1000 | Loss: 0.00000789
Iteration 26/1000 | Loss: 0.00000789
Iteration 27/1000 | Loss: 0.00000788
Iteration 28/1000 | Loss: 0.00000788
Iteration 29/1000 | Loss: 0.00000786
Iteration 30/1000 | Loss: 0.00000783
Iteration 31/1000 | Loss: 0.00000783
Iteration 32/1000 | Loss: 0.00000782
Iteration 33/1000 | Loss: 0.00000780
Iteration 34/1000 | Loss: 0.00000779
Iteration 35/1000 | Loss: 0.00000779
Iteration 36/1000 | Loss: 0.00000779
Iteration 37/1000 | Loss: 0.00000779
Iteration 38/1000 | Loss: 0.00000779
Iteration 39/1000 | Loss: 0.00000779
Iteration 40/1000 | Loss: 0.00000779
Iteration 41/1000 | Loss: 0.00000779
Iteration 42/1000 | Loss: 0.00000779
Iteration 43/1000 | Loss: 0.00000778
Iteration 44/1000 | Loss: 0.00000778
Iteration 45/1000 | Loss: 0.00000777
Iteration 46/1000 | Loss: 0.00000777
Iteration 47/1000 | Loss: 0.00000777
Iteration 48/1000 | Loss: 0.00000776
Iteration 49/1000 | Loss: 0.00000776
Iteration 50/1000 | Loss: 0.00000775
Iteration 51/1000 | Loss: 0.00000775
Iteration 52/1000 | Loss: 0.00000775
Iteration 53/1000 | Loss: 0.00000775
Iteration 54/1000 | Loss: 0.00000774
Iteration 55/1000 | Loss: 0.00000774
Iteration 56/1000 | Loss: 0.00000774
Iteration 57/1000 | Loss: 0.00000774
Iteration 58/1000 | Loss: 0.00000774
Iteration 59/1000 | Loss: 0.00000773
Iteration 60/1000 | Loss: 0.00000773
Iteration 61/1000 | Loss: 0.00000773
Iteration 62/1000 | Loss: 0.00000772
Iteration 63/1000 | Loss: 0.00000772
Iteration 64/1000 | Loss: 0.00000771
Iteration 65/1000 | Loss: 0.00000771
Iteration 66/1000 | Loss: 0.00000771
Iteration 67/1000 | Loss: 0.00000771
Iteration 68/1000 | Loss: 0.00000771
Iteration 69/1000 | Loss: 0.00000771
Iteration 70/1000 | Loss: 0.00000771
Iteration 71/1000 | Loss: 0.00000770
Iteration 72/1000 | Loss: 0.00000770
Iteration 73/1000 | Loss: 0.00000770
Iteration 74/1000 | Loss: 0.00000769
Iteration 75/1000 | Loss: 0.00000768
Iteration 76/1000 | Loss: 0.00000768
Iteration 77/1000 | Loss: 0.00000768
Iteration 78/1000 | Loss: 0.00000768
Iteration 79/1000 | Loss: 0.00000768
Iteration 80/1000 | Loss: 0.00000768
Iteration 81/1000 | Loss: 0.00000768
Iteration 82/1000 | Loss: 0.00000768
Iteration 83/1000 | Loss: 0.00000768
Iteration 84/1000 | Loss: 0.00000767
Iteration 85/1000 | Loss: 0.00000767
Iteration 86/1000 | Loss: 0.00000767
Iteration 87/1000 | Loss: 0.00000766
Iteration 88/1000 | Loss: 0.00000765
Iteration 89/1000 | Loss: 0.00000764
Iteration 90/1000 | Loss: 0.00000764
Iteration 91/1000 | Loss: 0.00000764
Iteration 92/1000 | Loss: 0.00000764
Iteration 93/1000 | Loss: 0.00000764
Iteration 94/1000 | Loss: 0.00000764
Iteration 95/1000 | Loss: 0.00000764
Iteration 96/1000 | Loss: 0.00000764
Iteration 97/1000 | Loss: 0.00000764
Iteration 98/1000 | Loss: 0.00000763
Iteration 99/1000 | Loss: 0.00000763
Iteration 100/1000 | Loss: 0.00000763
Iteration 101/1000 | Loss: 0.00000763
Iteration 102/1000 | Loss: 0.00000762
Iteration 103/1000 | Loss: 0.00000762
Iteration 104/1000 | Loss: 0.00000762
Iteration 105/1000 | Loss: 0.00000762
Iteration 106/1000 | Loss: 0.00000762
Iteration 107/1000 | Loss: 0.00000762
Iteration 108/1000 | Loss: 0.00000761
Iteration 109/1000 | Loss: 0.00000761
Iteration 110/1000 | Loss: 0.00000761
Iteration 111/1000 | Loss: 0.00000761
Iteration 112/1000 | Loss: 0.00000761
Iteration 113/1000 | Loss: 0.00000761
Iteration 114/1000 | Loss: 0.00000761
Iteration 115/1000 | Loss: 0.00000761
Iteration 116/1000 | Loss: 0.00000761
Iteration 117/1000 | Loss: 0.00000761
Iteration 118/1000 | Loss: 0.00000761
Iteration 119/1000 | Loss: 0.00000761
Iteration 120/1000 | Loss: 0.00000761
Iteration 121/1000 | Loss: 0.00000761
Iteration 122/1000 | Loss: 0.00000761
Iteration 123/1000 | Loss: 0.00000760
Iteration 124/1000 | Loss: 0.00000760
Iteration 125/1000 | Loss: 0.00000760
Iteration 126/1000 | Loss: 0.00000760
Iteration 127/1000 | Loss: 0.00000759
Iteration 128/1000 | Loss: 0.00000759
Iteration 129/1000 | Loss: 0.00000759
Iteration 130/1000 | Loss: 0.00000759
Iteration 131/1000 | Loss: 0.00000759
Iteration 132/1000 | Loss: 0.00000759
Iteration 133/1000 | Loss: 0.00000759
Iteration 134/1000 | Loss: 0.00000759
Iteration 135/1000 | Loss: 0.00000759
Iteration 136/1000 | Loss: 0.00000758
Iteration 137/1000 | Loss: 0.00000758
Iteration 138/1000 | Loss: 0.00000758
Iteration 139/1000 | Loss: 0.00000758
Iteration 140/1000 | Loss: 0.00000758
Iteration 141/1000 | Loss: 0.00000758
Iteration 142/1000 | Loss: 0.00000758
Iteration 143/1000 | Loss: 0.00000758
Iteration 144/1000 | Loss: 0.00000758
Iteration 145/1000 | Loss: 0.00000757
Iteration 146/1000 | Loss: 0.00000757
Iteration 147/1000 | Loss: 0.00000757
Iteration 148/1000 | Loss: 0.00000757
Iteration 149/1000 | Loss: 0.00000757
Iteration 150/1000 | Loss: 0.00000757
Iteration 151/1000 | Loss: 0.00000757
Iteration 152/1000 | Loss: 0.00000757
Iteration 153/1000 | Loss: 0.00000757
Iteration 154/1000 | Loss: 0.00000757
Iteration 155/1000 | Loss: 0.00000757
Iteration 156/1000 | Loss: 0.00000757
Iteration 157/1000 | Loss: 0.00000757
Iteration 158/1000 | Loss: 0.00000757
Iteration 159/1000 | Loss: 0.00000757
Iteration 160/1000 | Loss: 0.00000757
Iteration 161/1000 | Loss: 0.00000757
Iteration 162/1000 | Loss: 0.00000757
Iteration 163/1000 | Loss: 0.00000757
Iteration 164/1000 | Loss: 0.00000757
Iteration 165/1000 | Loss: 0.00000757
Iteration 166/1000 | Loss: 0.00000757
Iteration 167/1000 | Loss: 0.00000757
Iteration 168/1000 | Loss: 0.00000757
Iteration 169/1000 | Loss: 0.00000757
Iteration 170/1000 | Loss: 0.00000757
Iteration 171/1000 | Loss: 0.00000757
Iteration 172/1000 | Loss: 0.00000757
Iteration 173/1000 | Loss: 0.00000757
Iteration 174/1000 | Loss: 0.00000757
Iteration 175/1000 | Loss: 0.00000757
Iteration 176/1000 | Loss: 0.00000757
Iteration 177/1000 | Loss: 0.00000757
Iteration 178/1000 | Loss: 0.00000757
Iteration 179/1000 | Loss: 0.00000757
Iteration 180/1000 | Loss: 0.00000757
Iteration 181/1000 | Loss: 0.00000757
Iteration 182/1000 | Loss: 0.00000757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [7.567380180262262e-06, 7.567380180262262e-06, 7.567380180262262e-06, 7.567380180262262e-06, 7.567380180262262e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.567380180262262e-06

Optimization complete. Final v2v error: 2.3522093296051025 mm

Highest mean error: 2.4244065284729004 mm for frame 266

Lowest mean error: 2.3255863189697266 mm for frame 39

Saving results

Total time: 418.62003326416016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1078
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437178
Iteration 2/25 | Loss: 0.00099523
Iteration 3/25 | Loss: 0.00087193
Iteration 4/25 | Loss: 0.00085654
Iteration 5/25 | Loss: 0.00085148
Iteration 6/25 | Loss: 0.00085073
Iteration 7/25 | Loss: 0.00085073
Iteration 8/25 | Loss: 0.00085073
Iteration 9/25 | Loss: 0.00085073
Iteration 10/25 | Loss: 0.00085073
Iteration 11/25 | Loss: 0.00085073
Iteration 12/25 | Loss: 0.00085073
Iteration 13/25 | Loss: 0.00085073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008507275488227606, 0.0008507275488227606, 0.0008507275488227606, 0.0008507275488227606, 0.0008507275488227606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008507275488227606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40562117
Iteration 2/25 | Loss: 0.00052550
Iteration 3/25 | Loss: 0.00052550
Iteration 4/25 | Loss: 0.00052550
Iteration 5/25 | Loss: 0.00052550
Iteration 6/25 | Loss: 0.00052550
Iteration 7/25 | Loss: 0.00052550
Iteration 8/25 | Loss: 0.00052550
Iteration 9/25 | Loss: 0.00052550
Iteration 10/25 | Loss: 0.00052550
Iteration 11/25 | Loss: 0.00052550
Iteration 12/25 | Loss: 0.00052550
Iteration 13/25 | Loss: 0.00052550
Iteration 14/25 | Loss: 0.00052550
Iteration 15/25 | Loss: 0.00052550
Iteration 16/25 | Loss: 0.00052550
Iteration 17/25 | Loss: 0.00052550
Iteration 18/25 | Loss: 0.00052550
Iteration 19/25 | Loss: 0.00052550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005254954448901117, 0.0005254954448901117, 0.0005254954448901117, 0.0005254954448901117, 0.0005254954448901117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005254954448901117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052550
Iteration 2/1000 | Loss: 0.00002015
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001463
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001333
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001317
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001312
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001311
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001310
Iteration 43/1000 | Loss: 0.00001310
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001306
Iteration 65/1000 | Loss: 0.00001306
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001305
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001302
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001302
Iteration 84/1000 | Loss: 0.00001302
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001301
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001299
Iteration 97/1000 | Loss: 0.00001299
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001289
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001289
Iteration 151/1000 | Loss: 0.00001289
Iteration 152/1000 | Loss: 0.00001289
Iteration 153/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.288551720790565e-05, 1.288551720790565e-05, 1.288551720790565e-05, 1.288551720790565e-05, 1.288551720790565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.288551720790565e-05

Optimization complete. Final v2v error: 2.9480977058410645 mm

Highest mean error: 3.5234262943267822 mm for frame 130

Lowest mean error: 2.605431079864502 mm for frame 17

Saving results

Total time: 361.8604052066803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1028
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911414
Iteration 2/25 | Loss: 0.00100822
Iteration 3/25 | Loss: 0.00088929
Iteration 4/25 | Loss: 0.00085594
Iteration 5/25 | Loss: 0.00084881
Iteration 6/25 | Loss: 0.00084736
Iteration 7/25 | Loss: 0.00084736
Iteration 8/25 | Loss: 0.00084736
Iteration 9/25 | Loss: 0.00084736
Iteration 10/25 | Loss: 0.00084736
Iteration 11/25 | Loss: 0.00084736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008473633206449449, 0.0008473633206449449, 0.0008473633206449449, 0.0008473633206449449, 0.0008473633206449449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008473633206449449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32972014
Iteration 2/25 | Loss: 0.00048870
Iteration 3/25 | Loss: 0.00048870
Iteration 4/25 | Loss: 0.00048870
Iteration 5/25 | Loss: 0.00048870
Iteration 6/25 | Loss: 0.00048870
Iteration 7/25 | Loss: 0.00048870
Iteration 8/25 | Loss: 0.00048870
Iteration 9/25 | Loss: 0.00048870
Iteration 10/25 | Loss: 0.00048870
Iteration 11/25 | Loss: 0.00048870
Iteration 12/25 | Loss: 0.00048870
Iteration 13/25 | Loss: 0.00048870
Iteration 14/25 | Loss: 0.00048870
Iteration 15/25 | Loss: 0.00048870
Iteration 16/25 | Loss: 0.00048870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000488700345158577, 0.000488700345158577, 0.000488700345158577, 0.000488700345158577, 0.000488700345158577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000488700345158577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048870
Iteration 2/1000 | Loss: 0.00002500
Iteration 3/1000 | Loss: 0.00001717
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001274
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001253
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001242
Iteration 55/1000 | Loss: 0.00001242
Iteration 56/1000 | Loss: 0.00001242
Iteration 57/1000 | Loss: 0.00001242
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001238
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001237
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001237
Iteration 98/1000 | Loss: 0.00001237
Iteration 99/1000 | Loss: 0.00001237
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001236
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001236
Iteration 107/1000 | Loss: 0.00001236
Iteration 108/1000 | Loss: 0.00001236
Iteration 109/1000 | Loss: 0.00001236
Iteration 110/1000 | Loss: 0.00001236
Iteration 111/1000 | Loss: 0.00001236
Iteration 112/1000 | Loss: 0.00001236
Iteration 113/1000 | Loss: 0.00001236
Iteration 114/1000 | Loss: 0.00001235
Iteration 115/1000 | Loss: 0.00001235
Iteration 116/1000 | Loss: 0.00001235
Iteration 117/1000 | Loss: 0.00001235
Iteration 118/1000 | Loss: 0.00001235
Iteration 119/1000 | Loss: 0.00001235
Iteration 120/1000 | Loss: 0.00001235
Iteration 121/1000 | Loss: 0.00001235
Iteration 122/1000 | Loss: 0.00001235
Iteration 123/1000 | Loss: 0.00001235
Iteration 124/1000 | Loss: 0.00001235
Iteration 125/1000 | Loss: 0.00001235
Iteration 126/1000 | Loss: 0.00001235
Iteration 127/1000 | Loss: 0.00001235
Iteration 128/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.2349187272775453e-05, 1.2349187272775453e-05, 1.2349187272775453e-05, 1.2349187272775453e-05, 1.2349187272775453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2349187272775453e-05

Optimization complete. Final v2v error: 2.947824716567993 mm

Highest mean error: 3.3253579139709473 mm for frame 165

Lowest mean error: 2.3895137310028076 mm for frame 216

Saving results

Total time: 368.3089015483856
