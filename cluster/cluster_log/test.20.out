Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=20, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1120-1175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860249
Iteration 2/25 | Loss: 0.00119642
Iteration 3/25 | Loss: 0.00111506
Iteration 4/25 | Loss: 0.00109635
Iteration 5/25 | Loss: 0.00109048
Iteration 6/25 | Loss: 0.00108964
Iteration 7/25 | Loss: 0.00108964
Iteration 8/25 | Loss: 0.00108964
Iteration 9/25 | Loss: 0.00108964
Iteration 10/25 | Loss: 0.00108964
Iteration 11/25 | Loss: 0.00108964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010896429885178804, 0.0010896429885178804, 0.0010896429885178804, 0.0010896429885178804, 0.0010896429885178804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010896429885178804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38631499
Iteration 2/25 | Loss: 0.00082480
Iteration 3/25 | Loss: 0.00082479
Iteration 4/25 | Loss: 0.00082478
Iteration 5/25 | Loss: 0.00082478
Iteration 6/25 | Loss: 0.00082478
Iteration 7/25 | Loss: 0.00082478
Iteration 8/25 | Loss: 0.00082478
Iteration 9/25 | Loss: 0.00082478
Iteration 10/25 | Loss: 0.00082478
Iteration 11/25 | Loss: 0.00082478
Iteration 12/25 | Loss: 0.00082478
Iteration 13/25 | Loss: 0.00082478
Iteration 14/25 | Loss: 0.00082478
Iteration 15/25 | Loss: 0.00082478
Iteration 16/25 | Loss: 0.00082478
Iteration 17/25 | Loss: 0.00082478
Iteration 18/25 | Loss: 0.00082478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008247816003859043, 0.0008247816003859043, 0.0008247816003859043, 0.0008247816003859043, 0.0008247816003859043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008247816003859043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082478
Iteration 2/1000 | Loss: 0.00002484
Iteration 3/1000 | Loss: 0.00001762
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001237
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001233
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001217
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001209
Iteration 30/1000 | Loss: 0.00001209
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001176
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001173
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001168
Iteration 147/1000 | Loss: 0.00001168
Iteration 148/1000 | Loss: 0.00001168
Iteration 149/1000 | Loss: 0.00001167
Iteration 150/1000 | Loss: 0.00001167
Iteration 151/1000 | Loss: 0.00001167
Iteration 152/1000 | Loss: 0.00001167
Iteration 153/1000 | Loss: 0.00001167
Iteration 154/1000 | Loss: 0.00001167
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001165
Iteration 158/1000 | Loss: 0.00001165
Iteration 159/1000 | Loss: 0.00001165
Iteration 160/1000 | Loss: 0.00001165
Iteration 161/1000 | Loss: 0.00001165
Iteration 162/1000 | Loss: 0.00001165
Iteration 163/1000 | Loss: 0.00001164
Iteration 164/1000 | Loss: 0.00001164
Iteration 165/1000 | Loss: 0.00001164
Iteration 166/1000 | Loss: 0.00001164
Iteration 167/1000 | Loss: 0.00001164
Iteration 168/1000 | Loss: 0.00001164
Iteration 169/1000 | Loss: 0.00001164
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001164
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001163
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001161
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001161
Iteration 191/1000 | Loss: 0.00001161
Iteration 192/1000 | Loss: 0.00001161
Iteration 193/1000 | Loss: 0.00001161
Iteration 194/1000 | Loss: 0.00001161
Iteration 195/1000 | Loss: 0.00001161
Iteration 196/1000 | Loss: 0.00001161
Iteration 197/1000 | Loss: 0.00001161
Iteration 198/1000 | Loss: 0.00001161
Iteration 199/1000 | Loss: 0.00001160
Iteration 200/1000 | Loss: 0.00001160
Iteration 201/1000 | Loss: 0.00001160
Iteration 202/1000 | Loss: 0.00001160
Iteration 203/1000 | Loss: 0.00001160
Iteration 204/1000 | Loss: 0.00001160
Iteration 205/1000 | Loss: 0.00001160
Iteration 206/1000 | Loss: 0.00001160
Iteration 207/1000 | Loss: 0.00001160
Iteration 208/1000 | Loss: 0.00001160
Iteration 209/1000 | Loss: 0.00001160
Iteration 210/1000 | Loss: 0.00001159
Iteration 211/1000 | Loss: 0.00001159
Iteration 212/1000 | Loss: 0.00001159
Iteration 213/1000 | Loss: 0.00001159
Iteration 214/1000 | Loss: 0.00001158
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001158
Iteration 219/1000 | Loss: 0.00001158
Iteration 220/1000 | Loss: 0.00001158
Iteration 221/1000 | Loss: 0.00001158
Iteration 222/1000 | Loss: 0.00001158
Iteration 223/1000 | Loss: 0.00001158
Iteration 224/1000 | Loss: 0.00001158
Iteration 225/1000 | Loss: 0.00001158
Iteration 226/1000 | Loss: 0.00001158
Iteration 227/1000 | Loss: 0.00001158
Iteration 228/1000 | Loss: 0.00001157
Iteration 229/1000 | Loss: 0.00001157
Iteration 230/1000 | Loss: 0.00001157
Iteration 231/1000 | Loss: 0.00001157
Iteration 232/1000 | Loss: 0.00001157
Iteration 233/1000 | Loss: 0.00001157
Iteration 234/1000 | Loss: 0.00001157
Iteration 235/1000 | Loss: 0.00001157
Iteration 236/1000 | Loss: 0.00001157
Iteration 237/1000 | Loss: 0.00001157
Iteration 238/1000 | Loss: 0.00001157
Iteration 239/1000 | Loss: 0.00001157
Iteration 240/1000 | Loss: 0.00001157
Iteration 241/1000 | Loss: 0.00001157
Iteration 242/1000 | Loss: 0.00001157
Iteration 243/1000 | Loss: 0.00001157
Iteration 244/1000 | Loss: 0.00001157
Iteration 245/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.1573803021747153e-05, 1.1573803021747153e-05, 1.1573803021747153e-05, 1.1573803021747153e-05, 1.1573803021747153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1573803021747153e-05

Optimization complete. Final v2v error: 2.8889307975769043 mm

Highest mean error: 3.823168992996216 mm for frame 92

Lowest mean error: 2.658107042312622 mm for frame 155

Saving results

Total time: 47.52513837814331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515933
Iteration 2/25 | Loss: 0.00123599
Iteration 3/25 | Loss: 0.00113866
Iteration 4/25 | Loss: 0.00112552
Iteration 5/25 | Loss: 0.00112398
Iteration 6/25 | Loss: 0.00112398
Iteration 7/25 | Loss: 0.00112398
Iteration 8/25 | Loss: 0.00112398
Iteration 9/25 | Loss: 0.00112398
Iteration 10/25 | Loss: 0.00112398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011239792220294476, 0.0011239792220294476, 0.0011239792220294476, 0.0011239792220294476, 0.0011239792220294476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011239792220294476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93767071
Iteration 2/25 | Loss: 0.00084999
Iteration 3/25 | Loss: 0.00084999
Iteration 4/25 | Loss: 0.00084999
Iteration 5/25 | Loss: 0.00084999
Iteration 6/25 | Loss: 0.00084999
Iteration 7/25 | Loss: 0.00084999
Iteration 8/25 | Loss: 0.00084999
Iteration 9/25 | Loss: 0.00084999
Iteration 10/25 | Loss: 0.00084999
Iteration 11/25 | Loss: 0.00084999
Iteration 12/25 | Loss: 0.00084999
Iteration 13/25 | Loss: 0.00084999
Iteration 14/25 | Loss: 0.00084999
Iteration 15/25 | Loss: 0.00084999
Iteration 16/25 | Loss: 0.00084999
Iteration 17/25 | Loss: 0.00084999
Iteration 18/25 | Loss: 0.00084999
Iteration 19/25 | Loss: 0.00084999
Iteration 20/25 | Loss: 0.00084999
Iteration 21/25 | Loss: 0.00084999
Iteration 22/25 | Loss: 0.00084999
Iteration 23/25 | Loss: 0.00084999
Iteration 24/25 | Loss: 0.00084999
Iteration 25/25 | Loss: 0.00084999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084999
Iteration 2/1000 | Loss: 0.00002577
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001766
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001611
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001391
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001363
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001355
Iteration 60/1000 | Loss: 0.00001355
Iteration 61/1000 | Loss: 0.00001355
Iteration 62/1000 | Loss: 0.00001355
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001351
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001350
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001348
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001346
Iteration 109/1000 | Loss: 0.00001346
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001344
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001340
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Iteration 159/1000 | Loss: 0.00001340
Iteration 160/1000 | Loss: 0.00001340
Iteration 161/1000 | Loss: 0.00001340
Iteration 162/1000 | Loss: 0.00001340
Iteration 163/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.3399468116404023e-05, 1.3399468116404023e-05, 1.3399468116404023e-05, 1.3399468116404023e-05, 1.3399468116404023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3399468116404023e-05

Optimization complete. Final v2v error: 3.0981335639953613 mm

Highest mean error: 3.609431028366089 mm for frame 74

Lowest mean error: 2.7907841205596924 mm for frame 217

Saving results

Total time: 43.32012057304382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876444
Iteration 2/25 | Loss: 0.00118486
Iteration 3/25 | Loss: 0.00109887
Iteration 4/25 | Loss: 0.00109272
Iteration 5/25 | Loss: 0.00109094
Iteration 6/25 | Loss: 0.00109060
Iteration 7/25 | Loss: 0.00109060
Iteration 8/25 | Loss: 0.00109060
Iteration 9/25 | Loss: 0.00109060
Iteration 10/25 | Loss: 0.00109060
Iteration 11/25 | Loss: 0.00109060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010906036477535963, 0.0010906036477535963, 0.0010906036477535963, 0.0010906036477535963, 0.0010906036477535963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010906036477535963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11468697
Iteration 2/25 | Loss: 0.00086613
Iteration 3/25 | Loss: 0.00086613
Iteration 4/25 | Loss: 0.00086613
Iteration 5/25 | Loss: 0.00086613
Iteration 6/25 | Loss: 0.00086612
Iteration 7/25 | Loss: 0.00086612
Iteration 8/25 | Loss: 0.00086612
Iteration 9/25 | Loss: 0.00086612
Iteration 10/25 | Loss: 0.00086612
Iteration 11/25 | Loss: 0.00086612
Iteration 12/25 | Loss: 0.00086612
Iteration 13/25 | Loss: 0.00086612
Iteration 14/25 | Loss: 0.00086612
Iteration 15/25 | Loss: 0.00086612
Iteration 16/25 | Loss: 0.00086612
Iteration 17/25 | Loss: 0.00086612
Iteration 18/25 | Loss: 0.00086612
Iteration 19/25 | Loss: 0.00086612
Iteration 20/25 | Loss: 0.00086612
Iteration 21/25 | Loss: 0.00086612
Iteration 22/25 | Loss: 0.00086612
Iteration 23/25 | Loss: 0.00086612
Iteration 24/25 | Loss: 0.00086612
Iteration 25/25 | Loss: 0.00086612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086612
Iteration 2/1000 | Loss: 0.00002133
Iteration 3/1000 | Loss: 0.00001343
Iteration 4/1000 | Loss: 0.00001212
Iteration 5/1000 | Loss: 0.00001134
Iteration 6/1000 | Loss: 0.00001094
Iteration 7/1000 | Loss: 0.00001066
Iteration 8/1000 | Loss: 0.00001045
Iteration 9/1000 | Loss: 0.00001040
Iteration 10/1000 | Loss: 0.00001027
Iteration 11/1000 | Loss: 0.00001026
Iteration 12/1000 | Loss: 0.00001010
Iteration 13/1000 | Loss: 0.00001001
Iteration 14/1000 | Loss: 0.00001000
Iteration 15/1000 | Loss: 0.00000999
Iteration 16/1000 | Loss: 0.00000998
Iteration 17/1000 | Loss: 0.00000997
Iteration 18/1000 | Loss: 0.00000996
Iteration 19/1000 | Loss: 0.00000995
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000990
Iteration 24/1000 | Loss: 0.00000989
Iteration 25/1000 | Loss: 0.00000988
Iteration 26/1000 | Loss: 0.00000988
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000976
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000976
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000975
Iteration 34/1000 | Loss: 0.00000975
Iteration 35/1000 | Loss: 0.00000975
Iteration 36/1000 | Loss: 0.00000974
Iteration 37/1000 | Loss: 0.00000974
Iteration 38/1000 | Loss: 0.00000969
Iteration 39/1000 | Loss: 0.00000969
Iteration 40/1000 | Loss: 0.00000969
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000969
Iteration 44/1000 | Loss: 0.00000968
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000968
Iteration 47/1000 | Loss: 0.00000968
Iteration 48/1000 | Loss: 0.00000968
Iteration 49/1000 | Loss: 0.00000968
Iteration 50/1000 | Loss: 0.00000965
Iteration 51/1000 | Loss: 0.00000965
Iteration 52/1000 | Loss: 0.00000964
Iteration 53/1000 | Loss: 0.00000964
Iteration 54/1000 | Loss: 0.00000963
Iteration 55/1000 | Loss: 0.00000963
Iteration 56/1000 | Loss: 0.00000963
Iteration 57/1000 | Loss: 0.00000960
Iteration 58/1000 | Loss: 0.00000960
Iteration 59/1000 | Loss: 0.00000960
Iteration 60/1000 | Loss: 0.00000959
Iteration 61/1000 | Loss: 0.00000959
Iteration 62/1000 | Loss: 0.00000958
Iteration 63/1000 | Loss: 0.00000958
Iteration 64/1000 | Loss: 0.00000958
Iteration 65/1000 | Loss: 0.00000957
Iteration 66/1000 | Loss: 0.00000957
Iteration 67/1000 | Loss: 0.00000957
Iteration 68/1000 | Loss: 0.00000957
Iteration 69/1000 | Loss: 0.00000957
Iteration 70/1000 | Loss: 0.00000957
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000956
Iteration 76/1000 | Loss: 0.00000956
Iteration 77/1000 | Loss: 0.00000956
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000956
Iteration 80/1000 | Loss: 0.00000956
Iteration 81/1000 | Loss: 0.00000956
Iteration 82/1000 | Loss: 0.00000956
Iteration 83/1000 | Loss: 0.00000956
Iteration 84/1000 | Loss: 0.00000955
Iteration 85/1000 | Loss: 0.00000955
Iteration 86/1000 | Loss: 0.00000955
Iteration 87/1000 | Loss: 0.00000955
Iteration 88/1000 | Loss: 0.00000955
Iteration 89/1000 | Loss: 0.00000955
Iteration 90/1000 | Loss: 0.00000955
Iteration 91/1000 | Loss: 0.00000954
Iteration 92/1000 | Loss: 0.00000954
Iteration 93/1000 | Loss: 0.00000953
Iteration 94/1000 | Loss: 0.00000953
Iteration 95/1000 | Loss: 0.00000953
Iteration 96/1000 | Loss: 0.00000953
Iteration 97/1000 | Loss: 0.00000952
Iteration 98/1000 | Loss: 0.00000952
Iteration 99/1000 | Loss: 0.00000952
Iteration 100/1000 | Loss: 0.00000952
Iteration 101/1000 | Loss: 0.00000952
Iteration 102/1000 | Loss: 0.00000952
Iteration 103/1000 | Loss: 0.00000951
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000951
Iteration 106/1000 | Loss: 0.00000951
Iteration 107/1000 | Loss: 0.00000951
Iteration 108/1000 | Loss: 0.00000950
Iteration 109/1000 | Loss: 0.00000950
Iteration 110/1000 | Loss: 0.00000950
Iteration 111/1000 | Loss: 0.00000950
Iteration 112/1000 | Loss: 0.00000950
Iteration 113/1000 | Loss: 0.00000949
Iteration 114/1000 | Loss: 0.00000949
Iteration 115/1000 | Loss: 0.00000949
Iteration 116/1000 | Loss: 0.00000949
Iteration 117/1000 | Loss: 0.00000949
Iteration 118/1000 | Loss: 0.00000949
Iteration 119/1000 | Loss: 0.00000948
Iteration 120/1000 | Loss: 0.00000948
Iteration 121/1000 | Loss: 0.00000948
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000948
Iteration 127/1000 | Loss: 0.00000948
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000944
Iteration 138/1000 | Loss: 0.00000944
Iteration 139/1000 | Loss: 0.00000944
Iteration 140/1000 | Loss: 0.00000943
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000943
Iteration 146/1000 | Loss: 0.00000943
Iteration 147/1000 | Loss: 0.00000943
Iteration 148/1000 | Loss: 0.00000942
Iteration 149/1000 | Loss: 0.00000942
Iteration 150/1000 | Loss: 0.00000942
Iteration 151/1000 | Loss: 0.00000942
Iteration 152/1000 | Loss: 0.00000941
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000941
Iteration 155/1000 | Loss: 0.00000940
Iteration 156/1000 | Loss: 0.00000940
Iteration 157/1000 | Loss: 0.00000940
Iteration 158/1000 | Loss: 0.00000940
Iteration 159/1000 | Loss: 0.00000940
Iteration 160/1000 | Loss: 0.00000940
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000940
Iteration 163/1000 | Loss: 0.00000939
Iteration 164/1000 | Loss: 0.00000939
Iteration 165/1000 | Loss: 0.00000939
Iteration 166/1000 | Loss: 0.00000939
Iteration 167/1000 | Loss: 0.00000939
Iteration 168/1000 | Loss: 0.00000938
Iteration 169/1000 | Loss: 0.00000938
Iteration 170/1000 | Loss: 0.00000938
Iteration 171/1000 | Loss: 0.00000938
Iteration 172/1000 | Loss: 0.00000937
Iteration 173/1000 | Loss: 0.00000937
Iteration 174/1000 | Loss: 0.00000937
Iteration 175/1000 | Loss: 0.00000937
Iteration 176/1000 | Loss: 0.00000937
Iteration 177/1000 | Loss: 0.00000937
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000936
Iteration 181/1000 | Loss: 0.00000936
Iteration 182/1000 | Loss: 0.00000936
Iteration 183/1000 | Loss: 0.00000935
Iteration 184/1000 | Loss: 0.00000935
Iteration 185/1000 | Loss: 0.00000935
Iteration 186/1000 | Loss: 0.00000935
Iteration 187/1000 | Loss: 0.00000935
Iteration 188/1000 | Loss: 0.00000935
Iteration 189/1000 | Loss: 0.00000935
Iteration 190/1000 | Loss: 0.00000935
Iteration 191/1000 | Loss: 0.00000935
Iteration 192/1000 | Loss: 0.00000935
Iteration 193/1000 | Loss: 0.00000935
Iteration 194/1000 | Loss: 0.00000935
Iteration 195/1000 | Loss: 0.00000935
Iteration 196/1000 | Loss: 0.00000935
Iteration 197/1000 | Loss: 0.00000935
Iteration 198/1000 | Loss: 0.00000935
Iteration 199/1000 | Loss: 0.00000934
Iteration 200/1000 | Loss: 0.00000934
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000934
Iteration 205/1000 | Loss: 0.00000934
Iteration 206/1000 | Loss: 0.00000934
Iteration 207/1000 | Loss: 0.00000934
Iteration 208/1000 | Loss: 0.00000934
Iteration 209/1000 | Loss: 0.00000934
Iteration 210/1000 | Loss: 0.00000934
Iteration 211/1000 | Loss: 0.00000933
Iteration 212/1000 | Loss: 0.00000933
Iteration 213/1000 | Loss: 0.00000933
Iteration 214/1000 | Loss: 0.00000933
Iteration 215/1000 | Loss: 0.00000933
Iteration 216/1000 | Loss: 0.00000933
Iteration 217/1000 | Loss: 0.00000933
Iteration 218/1000 | Loss: 0.00000933
Iteration 219/1000 | Loss: 0.00000933
Iteration 220/1000 | Loss: 0.00000933
Iteration 221/1000 | Loss: 0.00000933
Iteration 222/1000 | Loss: 0.00000933
Iteration 223/1000 | Loss: 0.00000933
Iteration 224/1000 | Loss: 0.00000933
Iteration 225/1000 | Loss: 0.00000933
Iteration 226/1000 | Loss: 0.00000933
Iteration 227/1000 | Loss: 0.00000933
Iteration 228/1000 | Loss: 0.00000933
Iteration 229/1000 | Loss: 0.00000933
Iteration 230/1000 | Loss: 0.00000933
Iteration 231/1000 | Loss: 0.00000932
Iteration 232/1000 | Loss: 0.00000932
Iteration 233/1000 | Loss: 0.00000932
Iteration 234/1000 | Loss: 0.00000932
Iteration 235/1000 | Loss: 0.00000932
Iteration 236/1000 | Loss: 0.00000932
Iteration 237/1000 | Loss: 0.00000932
Iteration 238/1000 | Loss: 0.00000932
Iteration 239/1000 | Loss: 0.00000932
Iteration 240/1000 | Loss: 0.00000932
Iteration 241/1000 | Loss: 0.00000932
Iteration 242/1000 | Loss: 0.00000932
Iteration 243/1000 | Loss: 0.00000932
Iteration 244/1000 | Loss: 0.00000932
Iteration 245/1000 | Loss: 0.00000932
Iteration 246/1000 | Loss: 0.00000932
Iteration 247/1000 | Loss: 0.00000932
Iteration 248/1000 | Loss: 0.00000932
Iteration 249/1000 | Loss: 0.00000931
Iteration 250/1000 | Loss: 0.00000931
Iteration 251/1000 | Loss: 0.00000931
Iteration 252/1000 | Loss: 0.00000931
Iteration 253/1000 | Loss: 0.00000931
Iteration 254/1000 | Loss: 0.00000931
Iteration 255/1000 | Loss: 0.00000931
Iteration 256/1000 | Loss: 0.00000931
Iteration 257/1000 | Loss: 0.00000931
Iteration 258/1000 | Loss: 0.00000931
Iteration 259/1000 | Loss: 0.00000931
Iteration 260/1000 | Loss: 0.00000931
Iteration 261/1000 | Loss: 0.00000931
Iteration 262/1000 | Loss: 0.00000931
Iteration 263/1000 | Loss: 0.00000931
Iteration 264/1000 | Loss: 0.00000931
Iteration 265/1000 | Loss: 0.00000931
Iteration 266/1000 | Loss: 0.00000931
Iteration 267/1000 | Loss: 0.00000931
Iteration 268/1000 | Loss: 0.00000931
Iteration 269/1000 | Loss: 0.00000931
Iteration 270/1000 | Loss: 0.00000931
Iteration 271/1000 | Loss: 0.00000931
Iteration 272/1000 | Loss: 0.00000930
Iteration 273/1000 | Loss: 0.00000930
Iteration 274/1000 | Loss: 0.00000930
Iteration 275/1000 | Loss: 0.00000930
Iteration 276/1000 | Loss: 0.00000930
Iteration 277/1000 | Loss: 0.00000930
Iteration 278/1000 | Loss: 0.00000930
Iteration 279/1000 | Loss: 0.00000930
Iteration 280/1000 | Loss: 0.00000930
Iteration 281/1000 | Loss: 0.00000930
Iteration 282/1000 | Loss: 0.00000930
Iteration 283/1000 | Loss: 0.00000930
Iteration 284/1000 | Loss: 0.00000930
Iteration 285/1000 | Loss: 0.00000930
Iteration 286/1000 | Loss: 0.00000930
Iteration 287/1000 | Loss: 0.00000930
Iteration 288/1000 | Loss: 0.00000930
Iteration 289/1000 | Loss: 0.00000930
Iteration 290/1000 | Loss: 0.00000930
Iteration 291/1000 | Loss: 0.00000930
Iteration 292/1000 | Loss: 0.00000930
Iteration 293/1000 | Loss: 0.00000930
Iteration 294/1000 | Loss: 0.00000930
Iteration 295/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [9.298263648815919e-06, 9.298263648815919e-06, 9.298263648815919e-06, 9.298263648815919e-06, 9.298263648815919e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.298263648815919e-06

Optimization complete. Final v2v error: 2.6171047687530518 mm

Highest mean error: 2.9069478511810303 mm for frame 45

Lowest mean error: 2.3401706218719482 mm for frame 129

Saving results

Total time: 42.560547828674316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855054
Iteration 2/25 | Loss: 0.00154720
Iteration 3/25 | Loss: 0.00127848
Iteration 4/25 | Loss: 0.00118489
Iteration 5/25 | Loss: 0.00118528
Iteration 6/25 | Loss: 0.00117737
Iteration 7/25 | Loss: 0.00113759
Iteration 8/25 | Loss: 0.00114089
Iteration 9/25 | Loss: 0.00112959
Iteration 10/25 | Loss: 0.00112118
Iteration 11/25 | Loss: 0.00112098
Iteration 12/25 | Loss: 0.00111788
Iteration 13/25 | Loss: 0.00111057
Iteration 14/25 | Loss: 0.00110978
Iteration 15/25 | Loss: 0.00110953
Iteration 16/25 | Loss: 0.00110946
Iteration 17/25 | Loss: 0.00110945
Iteration 18/25 | Loss: 0.00110945
Iteration 19/25 | Loss: 0.00110944
Iteration 20/25 | Loss: 0.00110944
Iteration 21/25 | Loss: 0.00110944
Iteration 22/25 | Loss: 0.00110944
Iteration 23/25 | Loss: 0.00110944
Iteration 24/25 | Loss: 0.00110944
Iteration 25/25 | Loss: 0.00110944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.85288906
Iteration 2/25 | Loss: 0.00092769
Iteration 3/25 | Loss: 0.00092704
Iteration 4/25 | Loss: 0.00092704
Iteration 5/25 | Loss: 0.00092703
Iteration 6/25 | Loss: 0.00092703
Iteration 7/25 | Loss: 0.00092703
Iteration 8/25 | Loss: 0.00092703
Iteration 9/25 | Loss: 0.00092703
Iteration 10/25 | Loss: 0.00092703
Iteration 11/25 | Loss: 0.00092703
Iteration 12/25 | Loss: 0.00092703
Iteration 13/25 | Loss: 0.00092703
Iteration 14/25 | Loss: 0.00092703
Iteration 15/25 | Loss: 0.00092703
Iteration 16/25 | Loss: 0.00092703
Iteration 17/25 | Loss: 0.00092703
Iteration 18/25 | Loss: 0.00092703
Iteration 19/25 | Loss: 0.00092703
Iteration 20/25 | Loss: 0.00092703
Iteration 21/25 | Loss: 0.00092703
Iteration 22/25 | Loss: 0.00092703
Iteration 23/25 | Loss: 0.00092703
Iteration 24/25 | Loss: 0.00092703
Iteration 25/25 | Loss: 0.00092703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092703
Iteration 2/1000 | Loss: 0.00002137
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00004931
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00002342
Iteration 23/1000 | Loss: 0.00004501
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00002427
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00001183
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001192
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001179
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.1777378858823795e-05, 1.1777378858823795e-05, 1.1777378858823795e-05, 1.1777378858823795e-05, 1.1777378858823795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1777378858823795e-05

Optimization complete. Final v2v error: 2.9023637771606445 mm

Highest mean error: 3.289846897125244 mm for frame 154

Lowest mean error: 2.6631088256835938 mm for frame 138

Saving results

Total time: 63.210166931152344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865128
Iteration 2/25 | Loss: 0.00136594
Iteration 3/25 | Loss: 0.00119192
Iteration 4/25 | Loss: 0.00117628
Iteration 5/25 | Loss: 0.00117413
Iteration 6/25 | Loss: 0.00117413
Iteration 7/25 | Loss: 0.00117413
Iteration 8/25 | Loss: 0.00117413
Iteration 9/25 | Loss: 0.00117413
Iteration 10/25 | Loss: 0.00117413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011741340858861804, 0.0011741340858861804, 0.0011741340858861804, 0.0011741340858861804, 0.0011741340858861804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011741340858861804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35342610
Iteration 2/25 | Loss: 0.00075463
Iteration 3/25 | Loss: 0.00075463
Iteration 4/25 | Loss: 0.00075463
Iteration 5/25 | Loss: 0.00075463
Iteration 6/25 | Loss: 0.00075463
Iteration 7/25 | Loss: 0.00075463
Iteration 8/25 | Loss: 0.00075463
Iteration 9/25 | Loss: 0.00075463
Iteration 10/25 | Loss: 0.00075463
Iteration 11/25 | Loss: 0.00075463
Iteration 12/25 | Loss: 0.00075463
Iteration 13/25 | Loss: 0.00075463
Iteration 14/25 | Loss: 0.00075463
Iteration 15/25 | Loss: 0.00075463
Iteration 16/25 | Loss: 0.00075463
Iteration 17/25 | Loss: 0.00075463
Iteration 18/25 | Loss: 0.00075463
Iteration 19/25 | Loss: 0.00075463
Iteration 20/25 | Loss: 0.00075463
Iteration 21/25 | Loss: 0.00075463
Iteration 22/25 | Loss: 0.00075463
Iteration 23/25 | Loss: 0.00075463
Iteration 24/25 | Loss: 0.00075463
Iteration 25/25 | Loss: 0.00075463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075463
Iteration 2/1000 | Loss: 0.00002414
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001530
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001453
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001451
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001437
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001433
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001419
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001418
Iteration 149/1000 | Loss: 0.00001418
Iteration 150/1000 | Loss: 0.00001418
Iteration 151/1000 | Loss: 0.00001417
Iteration 152/1000 | Loss: 0.00001417
Iteration 153/1000 | Loss: 0.00001417
Iteration 154/1000 | Loss: 0.00001417
Iteration 155/1000 | Loss: 0.00001417
Iteration 156/1000 | Loss: 0.00001417
Iteration 157/1000 | Loss: 0.00001417
Iteration 158/1000 | Loss: 0.00001417
Iteration 159/1000 | Loss: 0.00001416
Iteration 160/1000 | Loss: 0.00001416
Iteration 161/1000 | Loss: 0.00001416
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001416
Iteration 164/1000 | Loss: 0.00001416
Iteration 165/1000 | Loss: 0.00001416
Iteration 166/1000 | Loss: 0.00001416
Iteration 167/1000 | Loss: 0.00001416
Iteration 168/1000 | Loss: 0.00001416
Iteration 169/1000 | Loss: 0.00001416
Iteration 170/1000 | Loss: 0.00001416
Iteration 171/1000 | Loss: 0.00001416
Iteration 172/1000 | Loss: 0.00001416
Iteration 173/1000 | Loss: 0.00001416
Iteration 174/1000 | Loss: 0.00001416
Iteration 175/1000 | Loss: 0.00001416
Iteration 176/1000 | Loss: 0.00001416
Iteration 177/1000 | Loss: 0.00001416
Iteration 178/1000 | Loss: 0.00001416
Iteration 179/1000 | Loss: 0.00001416
Iteration 180/1000 | Loss: 0.00001416
Iteration 181/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.4162520528770983e-05, 1.4162520528770983e-05, 1.4162520528770983e-05, 1.4162520528770983e-05, 1.4162520528770983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4162520528770983e-05

Optimization complete. Final v2v error: 3.182620048522949 mm

Highest mean error: 3.70265531539917 mm for frame 151

Lowest mean error: 2.90409517288208 mm for frame 98

Saving results

Total time: 40.84187078475952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954351
Iteration 2/25 | Loss: 0.00137421
Iteration 3/25 | Loss: 0.00115805
Iteration 4/25 | Loss: 0.00113915
Iteration 5/25 | Loss: 0.00113413
Iteration 6/25 | Loss: 0.00113287
Iteration 7/25 | Loss: 0.00113287
Iteration 8/25 | Loss: 0.00113287
Iteration 9/25 | Loss: 0.00113287
Iteration 10/25 | Loss: 0.00113287
Iteration 11/25 | Loss: 0.00113287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001132869510911405, 0.001132869510911405, 0.001132869510911405, 0.001132869510911405, 0.001132869510911405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001132869510911405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64925075
Iteration 2/25 | Loss: 0.00081084
Iteration 3/25 | Loss: 0.00081084
Iteration 4/25 | Loss: 0.00081083
Iteration 5/25 | Loss: 0.00081083
Iteration 6/25 | Loss: 0.00081083
Iteration 7/25 | Loss: 0.00081083
Iteration 8/25 | Loss: 0.00081083
Iteration 9/25 | Loss: 0.00081083
Iteration 10/25 | Loss: 0.00081083
Iteration 11/25 | Loss: 0.00081083
Iteration 12/25 | Loss: 0.00081083
Iteration 13/25 | Loss: 0.00081083
Iteration 14/25 | Loss: 0.00081083
Iteration 15/25 | Loss: 0.00081083
Iteration 16/25 | Loss: 0.00081083
Iteration 17/25 | Loss: 0.00081083
Iteration 18/25 | Loss: 0.00081083
Iteration 19/25 | Loss: 0.00081083
Iteration 20/25 | Loss: 0.00081083
Iteration 21/25 | Loss: 0.00081083
Iteration 22/25 | Loss: 0.00081083
Iteration 23/25 | Loss: 0.00081083
Iteration 24/25 | Loss: 0.00081083
Iteration 25/25 | Loss: 0.00081083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081083
Iteration 2/1000 | Loss: 0.00002288
Iteration 3/1000 | Loss: 0.00001570
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001318
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001310
Iteration 12/1000 | Loss: 0.00001308
Iteration 13/1000 | Loss: 0.00001304
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001295
Iteration 19/1000 | Loss: 0.00001293
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001287
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001285
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001283
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001282
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001279
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001278
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001277
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001261
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001261
Iteration 54/1000 | Loss: 0.00001260
Iteration 55/1000 | Loss: 0.00001260
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001250
Iteration 103/1000 | Loss: 0.00001250
Iteration 104/1000 | Loss: 0.00001250
Iteration 105/1000 | Loss: 0.00001250
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001249
Iteration 111/1000 | Loss: 0.00001249
Iteration 112/1000 | Loss: 0.00001249
Iteration 113/1000 | Loss: 0.00001249
Iteration 114/1000 | Loss: 0.00001249
Iteration 115/1000 | Loss: 0.00001249
Iteration 116/1000 | Loss: 0.00001248
Iteration 117/1000 | Loss: 0.00001248
Iteration 118/1000 | Loss: 0.00001248
Iteration 119/1000 | Loss: 0.00001248
Iteration 120/1000 | Loss: 0.00001248
Iteration 121/1000 | Loss: 0.00001248
Iteration 122/1000 | Loss: 0.00001247
Iteration 123/1000 | Loss: 0.00001247
Iteration 124/1000 | Loss: 0.00001247
Iteration 125/1000 | Loss: 0.00001247
Iteration 126/1000 | Loss: 0.00001247
Iteration 127/1000 | Loss: 0.00001247
Iteration 128/1000 | Loss: 0.00001247
Iteration 129/1000 | Loss: 0.00001247
Iteration 130/1000 | Loss: 0.00001247
Iteration 131/1000 | Loss: 0.00001247
Iteration 132/1000 | Loss: 0.00001247
Iteration 133/1000 | Loss: 0.00001247
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001246
Iteration 137/1000 | Loss: 0.00001246
Iteration 138/1000 | Loss: 0.00001245
Iteration 139/1000 | Loss: 0.00001245
Iteration 140/1000 | Loss: 0.00001245
Iteration 141/1000 | Loss: 0.00001245
Iteration 142/1000 | Loss: 0.00001245
Iteration 143/1000 | Loss: 0.00001245
Iteration 144/1000 | Loss: 0.00001245
Iteration 145/1000 | Loss: 0.00001245
Iteration 146/1000 | Loss: 0.00001244
Iteration 147/1000 | Loss: 0.00001244
Iteration 148/1000 | Loss: 0.00001244
Iteration 149/1000 | Loss: 0.00001244
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001242
Iteration 154/1000 | Loss: 0.00001242
Iteration 155/1000 | Loss: 0.00001242
Iteration 156/1000 | Loss: 0.00001242
Iteration 157/1000 | Loss: 0.00001242
Iteration 158/1000 | Loss: 0.00001242
Iteration 159/1000 | Loss: 0.00001241
Iteration 160/1000 | Loss: 0.00001241
Iteration 161/1000 | Loss: 0.00001241
Iteration 162/1000 | Loss: 0.00001241
Iteration 163/1000 | Loss: 0.00001241
Iteration 164/1000 | Loss: 0.00001241
Iteration 165/1000 | Loss: 0.00001241
Iteration 166/1000 | Loss: 0.00001241
Iteration 167/1000 | Loss: 0.00001241
Iteration 168/1000 | Loss: 0.00001240
Iteration 169/1000 | Loss: 0.00001240
Iteration 170/1000 | Loss: 0.00001240
Iteration 171/1000 | Loss: 0.00001240
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001238
Iteration 176/1000 | Loss: 0.00001238
Iteration 177/1000 | Loss: 0.00001238
Iteration 178/1000 | Loss: 0.00001238
Iteration 179/1000 | Loss: 0.00001238
Iteration 180/1000 | Loss: 0.00001238
Iteration 181/1000 | Loss: 0.00001238
Iteration 182/1000 | Loss: 0.00001238
Iteration 183/1000 | Loss: 0.00001238
Iteration 184/1000 | Loss: 0.00001238
Iteration 185/1000 | Loss: 0.00001238
Iteration 186/1000 | Loss: 0.00001238
Iteration 187/1000 | Loss: 0.00001237
Iteration 188/1000 | Loss: 0.00001237
Iteration 189/1000 | Loss: 0.00001237
Iteration 190/1000 | Loss: 0.00001237
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Iteration 199/1000 | Loss: 0.00001235
Iteration 200/1000 | Loss: 0.00001235
Iteration 201/1000 | Loss: 0.00001235
Iteration 202/1000 | Loss: 0.00001235
Iteration 203/1000 | Loss: 0.00001235
Iteration 204/1000 | Loss: 0.00001235
Iteration 205/1000 | Loss: 0.00001235
Iteration 206/1000 | Loss: 0.00001235
Iteration 207/1000 | Loss: 0.00001235
Iteration 208/1000 | Loss: 0.00001235
Iteration 209/1000 | Loss: 0.00001235
Iteration 210/1000 | Loss: 0.00001235
Iteration 211/1000 | Loss: 0.00001235
Iteration 212/1000 | Loss: 0.00001235
Iteration 213/1000 | Loss: 0.00001235
Iteration 214/1000 | Loss: 0.00001235
Iteration 215/1000 | Loss: 0.00001235
Iteration 216/1000 | Loss: 0.00001235
Iteration 217/1000 | Loss: 0.00001235
Iteration 218/1000 | Loss: 0.00001235
Iteration 219/1000 | Loss: 0.00001235
Iteration 220/1000 | Loss: 0.00001235
Iteration 221/1000 | Loss: 0.00001235
Iteration 222/1000 | Loss: 0.00001235
Iteration 223/1000 | Loss: 0.00001235
Iteration 224/1000 | Loss: 0.00001235
Iteration 225/1000 | Loss: 0.00001235
Iteration 226/1000 | Loss: 0.00001235
Iteration 227/1000 | Loss: 0.00001235
Iteration 228/1000 | Loss: 0.00001235
Iteration 229/1000 | Loss: 0.00001235
Iteration 230/1000 | Loss: 0.00001235
Iteration 231/1000 | Loss: 0.00001235
Iteration 232/1000 | Loss: 0.00001235
Iteration 233/1000 | Loss: 0.00001235
Iteration 234/1000 | Loss: 0.00001235
Iteration 235/1000 | Loss: 0.00001235
Iteration 236/1000 | Loss: 0.00001235
Iteration 237/1000 | Loss: 0.00001235
Iteration 238/1000 | Loss: 0.00001235
Iteration 239/1000 | Loss: 0.00001235
Iteration 240/1000 | Loss: 0.00001235
Iteration 241/1000 | Loss: 0.00001235
Iteration 242/1000 | Loss: 0.00001235
Iteration 243/1000 | Loss: 0.00001235
Iteration 244/1000 | Loss: 0.00001235
Iteration 245/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.2350174074526876e-05, 1.2350174074526876e-05, 1.2350174074526876e-05, 1.2350174074526876e-05, 1.2350174074526876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2350174074526876e-05

Optimization complete. Final v2v error: 2.9772515296936035 mm

Highest mean error: 3.134660482406616 mm for frame 89

Lowest mean error: 2.798546552658081 mm for frame 110

Saving results

Total time: 38.49883985519409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472105
Iteration 2/25 | Loss: 0.00132424
Iteration 3/25 | Loss: 0.00116345
Iteration 4/25 | Loss: 0.00114643
Iteration 5/25 | Loss: 0.00114274
Iteration 6/25 | Loss: 0.00114216
Iteration 7/25 | Loss: 0.00114216
Iteration 8/25 | Loss: 0.00114216
Iteration 9/25 | Loss: 0.00114216
Iteration 10/25 | Loss: 0.00114216
Iteration 11/25 | Loss: 0.00114216
Iteration 12/25 | Loss: 0.00114216
Iteration 13/25 | Loss: 0.00114216
Iteration 14/25 | Loss: 0.00114216
Iteration 15/25 | Loss: 0.00114216
Iteration 16/25 | Loss: 0.00114216
Iteration 17/25 | Loss: 0.00114216
Iteration 18/25 | Loss: 0.00114216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011421634117141366, 0.0011421634117141366, 0.0011421634117141366, 0.0011421634117141366, 0.0011421634117141366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011421634117141366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37229693
Iteration 2/25 | Loss: 0.00092173
Iteration 3/25 | Loss: 0.00092173
Iteration 4/25 | Loss: 0.00092173
Iteration 5/25 | Loss: 0.00092173
Iteration 6/25 | Loss: 0.00092173
Iteration 7/25 | Loss: 0.00092173
Iteration 8/25 | Loss: 0.00092173
Iteration 9/25 | Loss: 0.00092173
Iteration 10/25 | Loss: 0.00092173
Iteration 11/25 | Loss: 0.00092173
Iteration 12/25 | Loss: 0.00092173
Iteration 13/25 | Loss: 0.00092173
Iteration 14/25 | Loss: 0.00092173
Iteration 15/25 | Loss: 0.00092173
Iteration 16/25 | Loss: 0.00092173
Iteration 17/25 | Loss: 0.00092173
Iteration 18/25 | Loss: 0.00092173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009217256447300315, 0.0009217256447300315, 0.0009217256447300315, 0.0009217256447300315, 0.0009217256447300315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009217256447300315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092173
Iteration 2/1000 | Loss: 0.00003144
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001870
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001737
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001719
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001694
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001693
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001692
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001692
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001690
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001689
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001689
Iteration 104/1000 | Loss: 0.00001689
Iteration 105/1000 | Loss: 0.00001689
Iteration 106/1000 | Loss: 0.00001689
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001686
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001685
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001684
Iteration 122/1000 | Loss: 0.00001684
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001683
Iteration 125/1000 | Loss: 0.00001683
Iteration 126/1000 | Loss: 0.00001683
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001683
Iteration 129/1000 | Loss: 0.00001683
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00001683
Iteration 132/1000 | Loss: 0.00001683
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001682
Iteration 139/1000 | Loss: 0.00001682
Iteration 140/1000 | Loss: 0.00001682
Iteration 141/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.6823227269924246e-05, 1.6823227269924246e-05, 1.6823227269924246e-05, 1.6823227269924246e-05, 1.6823227269924246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6823227269924246e-05

Optimization complete. Final v2v error: 3.4374289512634277 mm

Highest mean error: 4.365665912628174 mm for frame 57

Lowest mean error: 2.860710620880127 mm for frame 209

Saving results

Total time: 43.16579055786133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495773
Iteration 2/25 | Loss: 0.00133937
Iteration 3/25 | Loss: 0.00114366
Iteration 4/25 | Loss: 0.00113223
Iteration 5/25 | Loss: 0.00113012
Iteration 6/25 | Loss: 0.00112986
Iteration 7/25 | Loss: 0.00112986
Iteration 8/25 | Loss: 0.00112986
Iteration 9/25 | Loss: 0.00112986
Iteration 10/25 | Loss: 0.00112986
Iteration 11/25 | Loss: 0.00112986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011298551689833403, 0.0011298551689833403, 0.0011298551689833403, 0.0011298551689833403, 0.0011298551689833403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011298551689833403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37078440
Iteration 2/25 | Loss: 0.00077904
Iteration 3/25 | Loss: 0.00077904
Iteration 4/25 | Loss: 0.00077904
Iteration 5/25 | Loss: 0.00077904
Iteration 6/25 | Loss: 0.00077904
Iteration 7/25 | Loss: 0.00077904
Iteration 8/25 | Loss: 0.00077904
Iteration 9/25 | Loss: 0.00077904
Iteration 10/25 | Loss: 0.00077904
Iteration 11/25 | Loss: 0.00077904
Iteration 12/25 | Loss: 0.00077904
Iteration 13/25 | Loss: 0.00077904
Iteration 14/25 | Loss: 0.00077904
Iteration 15/25 | Loss: 0.00077904
Iteration 16/25 | Loss: 0.00077904
Iteration 17/25 | Loss: 0.00077904
Iteration 18/25 | Loss: 0.00077904
Iteration 19/25 | Loss: 0.00077904
Iteration 20/25 | Loss: 0.00077904
Iteration 21/25 | Loss: 0.00077904
Iteration 22/25 | Loss: 0.00077904
Iteration 23/25 | Loss: 0.00077904
Iteration 24/25 | Loss: 0.00077904
Iteration 25/25 | Loss: 0.00077904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077904
Iteration 2/1000 | Loss: 0.00003390
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001718
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001597
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001527
Iteration 10/1000 | Loss: 0.00001509
Iteration 11/1000 | Loss: 0.00001495
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001473
Iteration 15/1000 | Loss: 0.00001471
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001470
Iteration 18/1000 | Loss: 0.00001469
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001467
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001457
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001452
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001448
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001446
Iteration 71/1000 | Loss: 0.00001446
Iteration 72/1000 | Loss: 0.00001446
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001441
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001440
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001439
Iteration 92/1000 | Loss: 0.00001439
Iteration 93/1000 | Loss: 0.00001439
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001438
Iteration 101/1000 | Loss: 0.00001437
Iteration 102/1000 | Loss: 0.00001436
Iteration 103/1000 | Loss: 0.00001436
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001430
Iteration 129/1000 | Loss: 0.00001430
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001427
Iteration 140/1000 | Loss: 0.00001427
Iteration 141/1000 | Loss: 0.00001427
Iteration 142/1000 | Loss: 0.00001427
Iteration 143/1000 | Loss: 0.00001427
Iteration 144/1000 | Loss: 0.00001427
Iteration 145/1000 | Loss: 0.00001427
Iteration 146/1000 | Loss: 0.00001427
Iteration 147/1000 | Loss: 0.00001427
Iteration 148/1000 | Loss: 0.00001427
Iteration 149/1000 | Loss: 0.00001427
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001425
Iteration 169/1000 | Loss: 0.00001425
Iteration 170/1000 | Loss: 0.00001424
Iteration 171/1000 | Loss: 0.00001424
Iteration 172/1000 | Loss: 0.00001424
Iteration 173/1000 | Loss: 0.00001424
Iteration 174/1000 | Loss: 0.00001424
Iteration 175/1000 | Loss: 0.00001424
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001423
Iteration 184/1000 | Loss: 0.00001422
Iteration 185/1000 | Loss: 0.00001422
Iteration 186/1000 | Loss: 0.00001421
Iteration 187/1000 | Loss: 0.00001421
Iteration 188/1000 | Loss: 0.00001421
Iteration 189/1000 | Loss: 0.00001421
Iteration 190/1000 | Loss: 0.00001421
Iteration 191/1000 | Loss: 0.00001421
Iteration 192/1000 | Loss: 0.00001421
Iteration 193/1000 | Loss: 0.00001420
Iteration 194/1000 | Loss: 0.00001420
Iteration 195/1000 | Loss: 0.00001420
Iteration 196/1000 | Loss: 0.00001420
Iteration 197/1000 | Loss: 0.00001420
Iteration 198/1000 | Loss: 0.00001420
Iteration 199/1000 | Loss: 0.00001420
Iteration 200/1000 | Loss: 0.00001419
Iteration 201/1000 | Loss: 0.00001419
Iteration 202/1000 | Loss: 0.00001419
Iteration 203/1000 | Loss: 0.00001419
Iteration 204/1000 | Loss: 0.00001419
Iteration 205/1000 | Loss: 0.00001419
Iteration 206/1000 | Loss: 0.00001419
Iteration 207/1000 | Loss: 0.00001419
Iteration 208/1000 | Loss: 0.00001419
Iteration 209/1000 | Loss: 0.00001419
Iteration 210/1000 | Loss: 0.00001419
Iteration 211/1000 | Loss: 0.00001419
Iteration 212/1000 | Loss: 0.00001418
Iteration 213/1000 | Loss: 0.00001418
Iteration 214/1000 | Loss: 0.00001418
Iteration 215/1000 | Loss: 0.00001418
Iteration 216/1000 | Loss: 0.00001418
Iteration 217/1000 | Loss: 0.00001418
Iteration 218/1000 | Loss: 0.00001418
Iteration 219/1000 | Loss: 0.00001418
Iteration 220/1000 | Loss: 0.00001418
Iteration 221/1000 | Loss: 0.00001418
Iteration 222/1000 | Loss: 0.00001418
Iteration 223/1000 | Loss: 0.00001418
Iteration 224/1000 | Loss: 0.00001417
Iteration 225/1000 | Loss: 0.00001417
Iteration 226/1000 | Loss: 0.00001417
Iteration 227/1000 | Loss: 0.00001417
Iteration 228/1000 | Loss: 0.00001417
Iteration 229/1000 | Loss: 0.00001417
Iteration 230/1000 | Loss: 0.00001417
Iteration 231/1000 | Loss: 0.00001417
Iteration 232/1000 | Loss: 0.00001417
Iteration 233/1000 | Loss: 0.00001417
Iteration 234/1000 | Loss: 0.00001417
Iteration 235/1000 | Loss: 0.00001417
Iteration 236/1000 | Loss: 0.00001417
Iteration 237/1000 | Loss: 0.00001417
Iteration 238/1000 | Loss: 0.00001417
Iteration 239/1000 | Loss: 0.00001417
Iteration 240/1000 | Loss: 0.00001417
Iteration 241/1000 | Loss: 0.00001417
Iteration 242/1000 | Loss: 0.00001417
Iteration 243/1000 | Loss: 0.00001417
Iteration 244/1000 | Loss: 0.00001417
Iteration 245/1000 | Loss: 0.00001417
Iteration 246/1000 | Loss: 0.00001417
Iteration 247/1000 | Loss: 0.00001417
Iteration 248/1000 | Loss: 0.00001417
Iteration 249/1000 | Loss: 0.00001417
Iteration 250/1000 | Loss: 0.00001417
Iteration 251/1000 | Loss: 0.00001417
Iteration 252/1000 | Loss: 0.00001417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.4165528227749746e-05, 1.4165528227749746e-05, 1.4165528227749746e-05, 1.4165528227749746e-05, 1.4165528227749746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4165528227749746e-05

Optimization complete. Final v2v error: 2.9985742568969727 mm

Highest mean error: 4.282378673553467 mm for frame 69

Lowest mean error: 2.545755624771118 mm for frame 19

Saving results

Total time: 41.19637203216553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050177
Iteration 2/25 | Loss: 0.00208158
Iteration 3/25 | Loss: 0.00169511
Iteration 4/25 | Loss: 0.00162712
Iteration 5/25 | Loss: 0.00142550
Iteration 6/25 | Loss: 0.00140868
Iteration 7/25 | Loss: 0.00133341
Iteration 8/25 | Loss: 0.00129812
Iteration 9/25 | Loss: 0.00125236
Iteration 10/25 | Loss: 0.00123097
Iteration 11/25 | Loss: 0.00121029
Iteration 12/25 | Loss: 0.00120130
Iteration 13/25 | Loss: 0.00119872
Iteration 14/25 | Loss: 0.00119943
Iteration 15/25 | Loss: 0.00119815
Iteration 16/25 | Loss: 0.00120802
Iteration 17/25 | Loss: 0.00119072
Iteration 18/25 | Loss: 0.00118792
Iteration 19/25 | Loss: 0.00119015
Iteration 20/25 | Loss: 0.00118830
Iteration 21/25 | Loss: 0.00118732
Iteration 22/25 | Loss: 0.00118413
Iteration 23/25 | Loss: 0.00118454
Iteration 24/25 | Loss: 0.00118660
Iteration 25/25 | Loss: 0.00118655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39882648
Iteration 2/25 | Loss: 0.00176039
Iteration 3/25 | Loss: 0.00169241
Iteration 4/25 | Loss: 0.00169240
Iteration 5/25 | Loss: 0.00169240
Iteration 6/25 | Loss: 0.00169240
Iteration 7/25 | Loss: 0.00169240
Iteration 8/25 | Loss: 0.00169240
Iteration 9/25 | Loss: 0.00169240
Iteration 10/25 | Loss: 0.00169240
Iteration 11/25 | Loss: 0.00169240
Iteration 12/25 | Loss: 0.00169240
Iteration 13/25 | Loss: 0.00169240
Iteration 14/25 | Loss: 0.00169240
Iteration 15/25 | Loss: 0.00169240
Iteration 16/25 | Loss: 0.00169240
Iteration 17/25 | Loss: 0.00169240
Iteration 18/25 | Loss: 0.00169240
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016924006631597877, 0.0016924006631597877, 0.0016924006631597877, 0.0016924006631597877, 0.0016924006631597877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016924006631597877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169240
Iteration 2/1000 | Loss: 0.00075789
Iteration 3/1000 | Loss: 0.00022756
Iteration 4/1000 | Loss: 0.00055400
Iteration 5/1000 | Loss: 0.00037040
Iteration 6/1000 | Loss: 0.00040989
Iteration 7/1000 | Loss: 0.00044202
Iteration 8/1000 | Loss: 0.00035559
Iteration 9/1000 | Loss: 0.00033177
Iteration 10/1000 | Loss: 0.00013321
Iteration 11/1000 | Loss: 0.00020580
Iteration 12/1000 | Loss: 0.00024515
Iteration 13/1000 | Loss: 0.00089026
Iteration 14/1000 | Loss: 0.00037621
Iteration 15/1000 | Loss: 0.00017012
Iteration 16/1000 | Loss: 0.00022531
Iteration 17/1000 | Loss: 0.00068464
Iteration 18/1000 | Loss: 0.00023542
Iteration 19/1000 | Loss: 0.00019915
Iteration 20/1000 | Loss: 0.00023759
Iteration 21/1000 | Loss: 0.00023973
Iteration 22/1000 | Loss: 0.00024807
Iteration 23/1000 | Loss: 0.00019912
Iteration 24/1000 | Loss: 0.00022996
Iteration 25/1000 | Loss: 0.00040704
Iteration 26/1000 | Loss: 0.00009121
Iteration 27/1000 | Loss: 0.00042318
Iteration 28/1000 | Loss: 0.00043902
Iteration 29/1000 | Loss: 0.00125175
Iteration 30/1000 | Loss: 0.00058239
Iteration 31/1000 | Loss: 0.00038127
Iteration 32/1000 | Loss: 0.00033534
Iteration 33/1000 | Loss: 0.00011199
Iteration 34/1000 | Loss: 0.00016499
Iteration 35/1000 | Loss: 0.00016849
Iteration 36/1000 | Loss: 0.00027840
Iteration 37/1000 | Loss: 0.00019940
Iteration 38/1000 | Loss: 0.00018947
Iteration 39/1000 | Loss: 0.00023303
Iteration 40/1000 | Loss: 0.00008964
Iteration 41/1000 | Loss: 0.00035826
Iteration 42/1000 | Loss: 0.00027891
Iteration 43/1000 | Loss: 0.00046069
Iteration 44/1000 | Loss: 0.00015193
Iteration 45/1000 | Loss: 0.00010197
Iteration 46/1000 | Loss: 0.00007427
Iteration 47/1000 | Loss: 0.00016555
Iteration 48/1000 | Loss: 0.00125970
Iteration 49/1000 | Loss: 0.00046446
Iteration 50/1000 | Loss: 0.00069684
Iteration 51/1000 | Loss: 0.00024902
Iteration 52/1000 | Loss: 0.00018374
Iteration 53/1000 | Loss: 0.00020356
Iteration 54/1000 | Loss: 0.00020716
Iteration 55/1000 | Loss: 0.00026728
Iteration 56/1000 | Loss: 0.00021262
Iteration 57/1000 | Loss: 0.00020229
Iteration 58/1000 | Loss: 0.00018596
Iteration 59/1000 | Loss: 0.00036140
Iteration 60/1000 | Loss: 0.00028527
Iteration 61/1000 | Loss: 0.00017004
Iteration 62/1000 | Loss: 0.00028778
Iteration 63/1000 | Loss: 0.00030115
Iteration 64/1000 | Loss: 0.00032457
Iteration 65/1000 | Loss: 0.00032643
Iteration 66/1000 | Loss: 0.00019468
Iteration 67/1000 | Loss: 0.00022976
Iteration 68/1000 | Loss: 0.00028923
Iteration 69/1000 | Loss: 0.00035379
Iteration 70/1000 | Loss: 0.00042665
Iteration 71/1000 | Loss: 0.00041118
Iteration 72/1000 | Loss: 0.00025282
Iteration 73/1000 | Loss: 0.00010044
Iteration 74/1000 | Loss: 0.00018221
Iteration 75/1000 | Loss: 0.00025764
Iteration 76/1000 | Loss: 0.00014926
Iteration 77/1000 | Loss: 0.00009455
Iteration 78/1000 | Loss: 0.00008578
Iteration 79/1000 | Loss: 0.00008078
Iteration 80/1000 | Loss: 0.00015508
Iteration 81/1000 | Loss: 0.00012864
Iteration 82/1000 | Loss: 0.00008658
Iteration 83/1000 | Loss: 0.00011466
Iteration 84/1000 | Loss: 0.00007782
Iteration 85/1000 | Loss: 0.00013994
Iteration 86/1000 | Loss: 0.00011486
Iteration 87/1000 | Loss: 0.00009046
Iteration 88/1000 | Loss: 0.00016298
Iteration 89/1000 | Loss: 0.00009128
Iteration 90/1000 | Loss: 0.00077140
Iteration 91/1000 | Loss: 0.00033000
Iteration 92/1000 | Loss: 0.00017790
Iteration 93/1000 | Loss: 0.00024074
Iteration 94/1000 | Loss: 0.00017955
Iteration 95/1000 | Loss: 0.00045418
Iteration 96/1000 | Loss: 0.00018323
Iteration 97/1000 | Loss: 0.00008399
Iteration 98/1000 | Loss: 0.00013833
Iteration 99/1000 | Loss: 0.00011364
Iteration 100/1000 | Loss: 0.00021735
Iteration 101/1000 | Loss: 0.00019377
Iteration 102/1000 | Loss: 0.00012080
Iteration 103/1000 | Loss: 0.00003451
Iteration 104/1000 | Loss: 0.00016568
Iteration 105/1000 | Loss: 0.00026943
Iteration 106/1000 | Loss: 0.00050497
Iteration 107/1000 | Loss: 0.00039662
Iteration 108/1000 | Loss: 0.00003415
Iteration 109/1000 | Loss: 0.00004375
Iteration 110/1000 | Loss: 0.00013904
Iteration 111/1000 | Loss: 0.00011431
Iteration 112/1000 | Loss: 0.00005970
Iteration 113/1000 | Loss: 0.00003567
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00013337
Iteration 117/1000 | Loss: 0.00006883
Iteration 118/1000 | Loss: 0.00002428
Iteration 119/1000 | Loss: 0.00014474
Iteration 120/1000 | Loss: 0.00021814
Iteration 121/1000 | Loss: 0.00015076
Iteration 122/1000 | Loss: 0.00027599
Iteration 123/1000 | Loss: 0.00044163
Iteration 124/1000 | Loss: 0.00021988
Iteration 125/1000 | Loss: 0.00003294
Iteration 126/1000 | Loss: 0.00012431
Iteration 127/1000 | Loss: 0.00028795
Iteration 128/1000 | Loss: 0.00022911
Iteration 129/1000 | Loss: 0.00014861
Iteration 130/1000 | Loss: 0.00003504
Iteration 131/1000 | Loss: 0.00055657
Iteration 132/1000 | Loss: 0.00012681
Iteration 133/1000 | Loss: 0.00039054
Iteration 134/1000 | Loss: 0.00016929
Iteration 135/1000 | Loss: 0.00016985
Iteration 136/1000 | Loss: 0.00012167
Iteration 137/1000 | Loss: 0.00017441
Iteration 138/1000 | Loss: 0.00062857
Iteration 139/1000 | Loss: 0.00009407
Iteration 140/1000 | Loss: 0.00003903
Iteration 141/1000 | Loss: 0.00022089
Iteration 142/1000 | Loss: 0.00005112
Iteration 143/1000 | Loss: 0.00017029
Iteration 144/1000 | Loss: 0.00004139
Iteration 145/1000 | Loss: 0.00003123
Iteration 146/1000 | Loss: 0.00002648
Iteration 147/1000 | Loss: 0.00046729
Iteration 148/1000 | Loss: 0.00008872
Iteration 149/1000 | Loss: 0.00002645
Iteration 150/1000 | Loss: 0.00002316
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00019710
Iteration 153/1000 | Loss: 0.00013803
Iteration 154/1000 | Loss: 0.00015239
Iteration 155/1000 | Loss: 0.00002456
Iteration 156/1000 | Loss: 0.00019875
Iteration 157/1000 | Loss: 0.00015879
Iteration 158/1000 | Loss: 0.00002321
Iteration 159/1000 | Loss: 0.00018098
Iteration 160/1000 | Loss: 0.00002961
Iteration 161/1000 | Loss: 0.00002576
Iteration 162/1000 | Loss: 0.00012062
Iteration 163/1000 | Loss: 0.00060983
Iteration 164/1000 | Loss: 0.00023756
Iteration 165/1000 | Loss: 0.00002953
Iteration 166/1000 | Loss: 0.00002338
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002026
Iteration 169/1000 | Loss: 0.00001919
Iteration 170/1000 | Loss: 0.00001881
Iteration 171/1000 | Loss: 0.00043617
Iteration 172/1000 | Loss: 0.00002123
Iteration 173/1000 | Loss: 0.00001890
Iteration 174/1000 | Loss: 0.00001856
Iteration 175/1000 | Loss: 0.00001835
Iteration 176/1000 | Loss: 0.00002205
Iteration 177/1000 | Loss: 0.00001848
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001802
Iteration 180/1000 | Loss: 0.00001800
Iteration 181/1000 | Loss: 0.00001777
Iteration 182/1000 | Loss: 0.00001773
Iteration 183/1000 | Loss: 0.00001771
Iteration 184/1000 | Loss: 0.00001771
Iteration 185/1000 | Loss: 0.00001770
Iteration 186/1000 | Loss: 0.00001770
Iteration 187/1000 | Loss: 0.00001770
Iteration 188/1000 | Loss: 0.00001770
Iteration 189/1000 | Loss: 0.00064162
Iteration 190/1000 | Loss: 0.00004483
Iteration 191/1000 | Loss: 0.00002718
Iteration 192/1000 | Loss: 0.00002240
Iteration 193/1000 | Loss: 0.00002370
Iteration 194/1000 | Loss: 0.00001851
Iteration 195/1000 | Loss: 0.00001744
Iteration 196/1000 | Loss: 0.00001668
Iteration 197/1000 | Loss: 0.00001604
Iteration 198/1000 | Loss: 0.00001595
Iteration 199/1000 | Loss: 0.00001584
Iteration 200/1000 | Loss: 0.00001583
Iteration 201/1000 | Loss: 0.00001568
Iteration 202/1000 | Loss: 0.00001564
Iteration 203/1000 | Loss: 0.00001563
Iteration 204/1000 | Loss: 0.00001562
Iteration 205/1000 | Loss: 0.00001562
Iteration 206/1000 | Loss: 0.00001562
Iteration 207/1000 | Loss: 0.00001562
Iteration 208/1000 | Loss: 0.00001562
Iteration 209/1000 | Loss: 0.00001562
Iteration 210/1000 | Loss: 0.00001562
Iteration 211/1000 | Loss: 0.00001562
Iteration 212/1000 | Loss: 0.00001562
Iteration 213/1000 | Loss: 0.00001562
Iteration 214/1000 | Loss: 0.00001561
Iteration 215/1000 | Loss: 0.00001561
Iteration 216/1000 | Loss: 0.00001561
Iteration 217/1000 | Loss: 0.00001560
Iteration 218/1000 | Loss: 0.00001559
Iteration 219/1000 | Loss: 0.00001559
Iteration 220/1000 | Loss: 0.00001558
Iteration 221/1000 | Loss: 0.00001558
Iteration 222/1000 | Loss: 0.00001558
Iteration 223/1000 | Loss: 0.00001557
Iteration 224/1000 | Loss: 0.00001557
Iteration 225/1000 | Loss: 0.00001557
Iteration 226/1000 | Loss: 0.00001556
Iteration 227/1000 | Loss: 0.00001556
Iteration 228/1000 | Loss: 0.00001556
Iteration 229/1000 | Loss: 0.00001555
Iteration 230/1000 | Loss: 0.00001554
Iteration 231/1000 | Loss: 0.00001554
Iteration 232/1000 | Loss: 0.00001553
Iteration 233/1000 | Loss: 0.00001553
Iteration 234/1000 | Loss: 0.00001553
Iteration 235/1000 | Loss: 0.00001553
Iteration 236/1000 | Loss: 0.00001552
Iteration 237/1000 | Loss: 0.00001552
Iteration 238/1000 | Loss: 0.00001552
Iteration 239/1000 | Loss: 0.00001552
Iteration 240/1000 | Loss: 0.00001552
Iteration 241/1000 | Loss: 0.00001551
Iteration 242/1000 | Loss: 0.00001551
Iteration 243/1000 | Loss: 0.00001551
Iteration 244/1000 | Loss: 0.00001550
Iteration 245/1000 | Loss: 0.00001550
Iteration 246/1000 | Loss: 0.00001548
Iteration 247/1000 | Loss: 0.00001548
Iteration 248/1000 | Loss: 0.00001547
Iteration 249/1000 | Loss: 0.00001547
Iteration 250/1000 | Loss: 0.00001547
Iteration 251/1000 | Loss: 0.00001546
Iteration 252/1000 | Loss: 0.00001546
Iteration 253/1000 | Loss: 0.00001546
Iteration 254/1000 | Loss: 0.00001545
Iteration 255/1000 | Loss: 0.00001545
Iteration 256/1000 | Loss: 0.00001543
Iteration 257/1000 | Loss: 0.00001543
Iteration 258/1000 | Loss: 0.00001542
Iteration 259/1000 | Loss: 0.00001541
Iteration 260/1000 | Loss: 0.00001541
Iteration 261/1000 | Loss: 0.00001541
Iteration 262/1000 | Loss: 0.00001541
Iteration 263/1000 | Loss: 0.00001541
Iteration 264/1000 | Loss: 0.00001541
Iteration 265/1000 | Loss: 0.00001541
Iteration 266/1000 | Loss: 0.00001541
Iteration 267/1000 | Loss: 0.00001541
Iteration 268/1000 | Loss: 0.00001541
Iteration 269/1000 | Loss: 0.00001541
Iteration 270/1000 | Loss: 0.00001540
Iteration 271/1000 | Loss: 0.00001540
Iteration 272/1000 | Loss: 0.00001540
Iteration 273/1000 | Loss: 0.00001540
Iteration 274/1000 | Loss: 0.00001540
Iteration 275/1000 | Loss: 0.00001540
Iteration 276/1000 | Loss: 0.00001539
Iteration 277/1000 | Loss: 0.00001539
Iteration 278/1000 | Loss: 0.00001538
Iteration 279/1000 | Loss: 0.00001538
Iteration 280/1000 | Loss: 0.00001538
Iteration 281/1000 | Loss: 0.00001537
Iteration 282/1000 | Loss: 0.00001537
Iteration 283/1000 | Loss: 0.00001537
Iteration 284/1000 | Loss: 0.00001536
Iteration 285/1000 | Loss: 0.00001536
Iteration 286/1000 | Loss: 0.00001536
Iteration 287/1000 | Loss: 0.00001536
Iteration 288/1000 | Loss: 0.00001536
Iteration 289/1000 | Loss: 0.00001536
Iteration 290/1000 | Loss: 0.00001535
Iteration 291/1000 | Loss: 0.00001535
Iteration 292/1000 | Loss: 0.00001535
Iteration 293/1000 | Loss: 0.00001535
Iteration 294/1000 | Loss: 0.00001534
Iteration 295/1000 | Loss: 0.00001534
Iteration 296/1000 | Loss: 0.00001533
Iteration 297/1000 | Loss: 0.00001533
Iteration 298/1000 | Loss: 0.00001533
Iteration 299/1000 | Loss: 0.00001533
Iteration 300/1000 | Loss: 0.00001532
Iteration 301/1000 | Loss: 0.00001532
Iteration 302/1000 | Loss: 0.00001532
Iteration 303/1000 | Loss: 0.00001532
Iteration 304/1000 | Loss: 0.00001532
Iteration 305/1000 | Loss: 0.00001532
Iteration 306/1000 | Loss: 0.00001532
Iteration 307/1000 | Loss: 0.00001532
Iteration 308/1000 | Loss: 0.00001532
Iteration 309/1000 | Loss: 0.00001532
Iteration 310/1000 | Loss: 0.00001532
Iteration 311/1000 | Loss: 0.00001531
Iteration 312/1000 | Loss: 0.00001531
Iteration 313/1000 | Loss: 0.00001531
Iteration 314/1000 | Loss: 0.00001530
Iteration 315/1000 | Loss: 0.00001530
Iteration 316/1000 | Loss: 0.00001530
Iteration 317/1000 | Loss: 0.00001530
Iteration 318/1000 | Loss: 0.00001530
Iteration 319/1000 | Loss: 0.00001530
Iteration 320/1000 | Loss: 0.00001530
Iteration 321/1000 | Loss: 0.00001530
Iteration 322/1000 | Loss: 0.00001530
Iteration 323/1000 | Loss: 0.00001530
Iteration 324/1000 | Loss: 0.00001530
Iteration 325/1000 | Loss: 0.00001529
Iteration 326/1000 | Loss: 0.00001529
Iteration 327/1000 | Loss: 0.00001529
Iteration 328/1000 | Loss: 0.00001529
Iteration 329/1000 | Loss: 0.00001529
Iteration 330/1000 | Loss: 0.00001529
Iteration 331/1000 | Loss: 0.00001529
Iteration 332/1000 | Loss: 0.00001529
Iteration 333/1000 | Loss: 0.00001529
Iteration 334/1000 | Loss: 0.00001529
Iteration 335/1000 | Loss: 0.00001529
Iteration 336/1000 | Loss: 0.00001529
Iteration 337/1000 | Loss: 0.00001529
Iteration 338/1000 | Loss: 0.00001528
Iteration 339/1000 | Loss: 0.00001528
Iteration 340/1000 | Loss: 0.00001528
Iteration 341/1000 | Loss: 0.00001528
Iteration 342/1000 | Loss: 0.00001528
Iteration 343/1000 | Loss: 0.00001528
Iteration 344/1000 | Loss: 0.00001528
Iteration 345/1000 | Loss: 0.00001528
Iteration 346/1000 | Loss: 0.00001528
Iteration 347/1000 | Loss: 0.00001528
Iteration 348/1000 | Loss: 0.00001528
Iteration 349/1000 | Loss: 0.00001528
Iteration 350/1000 | Loss: 0.00001528
Iteration 351/1000 | Loss: 0.00001528
Iteration 352/1000 | Loss: 0.00001527
Iteration 353/1000 | Loss: 0.00001527
Iteration 354/1000 | Loss: 0.00001527
Iteration 355/1000 | Loss: 0.00001527
Iteration 356/1000 | Loss: 0.00001527
Iteration 357/1000 | Loss: 0.00001527
Iteration 358/1000 | Loss: 0.00001527
Iteration 359/1000 | Loss: 0.00001527
Iteration 360/1000 | Loss: 0.00001527
Iteration 361/1000 | Loss: 0.00001527
Iteration 362/1000 | Loss: 0.00001527
Iteration 363/1000 | Loss: 0.00001527
Iteration 364/1000 | Loss: 0.00001527
Iteration 365/1000 | Loss: 0.00001527
Iteration 366/1000 | Loss: 0.00001527
Iteration 367/1000 | Loss: 0.00001527
Iteration 368/1000 | Loss: 0.00001527
Iteration 369/1000 | Loss: 0.00001527
Iteration 370/1000 | Loss: 0.00001527
Iteration 371/1000 | Loss: 0.00001527
Iteration 372/1000 | Loss: 0.00001527
Iteration 373/1000 | Loss: 0.00001527
Iteration 374/1000 | Loss: 0.00001527
Iteration 375/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 375. Stopping optimization.
Last 5 losses: [1.5274144971044734e-05, 1.5274144971044734e-05, 1.5274144971044734e-05, 1.5274144971044734e-05, 1.5274144971044734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5274144971044734e-05

Optimization complete. Final v2v error: 3.3207409381866455 mm

Highest mean error: 4.973441123962402 mm for frame 239

Lowest mean error: 2.8872268199920654 mm for frame 232

Saving results

Total time: 366.7235686779022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820341
Iteration 2/25 | Loss: 0.00128976
Iteration 3/25 | Loss: 0.00116004
Iteration 4/25 | Loss: 0.00113036
Iteration 5/25 | Loss: 0.00112646
Iteration 6/25 | Loss: 0.00112678
Iteration 7/25 | Loss: 0.00113901
Iteration 8/25 | Loss: 0.00114038
Iteration 9/25 | Loss: 0.00113509
Iteration 10/25 | Loss: 0.00112704
Iteration 11/25 | Loss: 0.00112320
Iteration 12/25 | Loss: 0.00111922
Iteration 13/25 | Loss: 0.00111038
Iteration 14/25 | Loss: 0.00110659
Iteration 15/25 | Loss: 0.00110646
Iteration 16/25 | Loss: 0.00110422
Iteration 17/25 | Loss: 0.00110425
Iteration 18/25 | Loss: 0.00110493
Iteration 19/25 | Loss: 0.00110510
Iteration 20/25 | Loss: 0.00110475
Iteration 21/25 | Loss: 0.00110538
Iteration 22/25 | Loss: 0.00110462
Iteration 23/25 | Loss: 0.00110491
Iteration 24/25 | Loss: 0.00110503
Iteration 25/25 | Loss: 0.00110409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34477365
Iteration 2/25 | Loss: 0.00114967
Iteration 3/25 | Loss: 0.00114965
Iteration 4/25 | Loss: 0.00114964
Iteration 5/25 | Loss: 0.00114964
Iteration 6/25 | Loss: 0.00114964
Iteration 7/25 | Loss: 0.00114964
Iteration 8/25 | Loss: 0.00114964
Iteration 9/25 | Loss: 0.00114964
Iteration 10/25 | Loss: 0.00114964
Iteration 11/25 | Loss: 0.00114964
Iteration 12/25 | Loss: 0.00114964
Iteration 13/25 | Loss: 0.00114964
Iteration 14/25 | Loss: 0.00114964
Iteration 15/25 | Loss: 0.00114964
Iteration 16/25 | Loss: 0.00114964
Iteration 17/25 | Loss: 0.00114964
Iteration 18/25 | Loss: 0.00114964
Iteration 19/25 | Loss: 0.00114964
Iteration 20/25 | Loss: 0.00114964
Iteration 21/25 | Loss: 0.00114964
Iteration 22/25 | Loss: 0.00114964
Iteration 23/25 | Loss: 0.00114964
Iteration 24/25 | Loss: 0.00114964
Iteration 25/25 | Loss: 0.00114964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114964
Iteration 2/1000 | Loss: 0.00006558
Iteration 3/1000 | Loss: 0.00003543
Iteration 4/1000 | Loss: 0.00006814
Iteration 5/1000 | Loss: 0.00003379
Iteration 6/1000 | Loss: 0.00002636
Iteration 7/1000 | Loss: 0.00003207
Iteration 8/1000 | Loss: 0.00025452
Iteration 9/1000 | Loss: 0.00006098
Iteration 10/1000 | Loss: 0.00018277
Iteration 11/1000 | Loss: 0.00004157
Iteration 12/1000 | Loss: 0.00002972
Iteration 13/1000 | Loss: 0.00003206
Iteration 14/1000 | Loss: 0.00002411
Iteration 15/1000 | Loss: 0.00002911
Iteration 16/1000 | Loss: 0.00002517
Iteration 17/1000 | Loss: 0.00002763
Iteration 18/1000 | Loss: 0.00002358
Iteration 19/1000 | Loss: 0.00002316
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002186
Iteration 22/1000 | Loss: 0.00002999
Iteration 23/1000 | Loss: 0.00002035
Iteration 24/1000 | Loss: 0.00002830
Iteration 25/1000 | Loss: 0.00002293
Iteration 26/1000 | Loss: 0.00002934
Iteration 27/1000 | Loss: 0.00003028
Iteration 28/1000 | Loss: 0.00002728
Iteration 29/1000 | Loss: 0.00002889
Iteration 30/1000 | Loss: 0.00002632
Iteration 31/1000 | Loss: 0.00002887
Iteration 32/1000 | Loss: 0.00002580
Iteration 33/1000 | Loss: 0.00002719
Iteration 34/1000 | Loss: 0.00002624
Iteration 35/1000 | Loss: 0.00002542
Iteration 36/1000 | Loss: 0.00002610
Iteration 37/1000 | Loss: 0.00002471
Iteration 38/1000 | Loss: 0.00002471
Iteration 39/1000 | Loss: 0.00003368
Iteration 40/1000 | Loss: 0.00002195
Iteration 41/1000 | Loss: 0.00001850
Iteration 42/1000 | Loss: 0.00002612
Iteration 43/1000 | Loss: 0.00002867
Iteration 44/1000 | Loss: 0.00002985
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001965
Iteration 47/1000 | Loss: 0.00002374
Iteration 48/1000 | Loss: 0.00002424
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00003685
Iteration 52/1000 | Loss: 0.00002869
Iteration 53/1000 | Loss: 0.00002326
Iteration 54/1000 | Loss: 0.00002726
Iteration 55/1000 | Loss: 0.00001826
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00003020
Iteration 58/1000 | Loss: 0.00002416
Iteration 59/1000 | Loss: 0.00003006
Iteration 60/1000 | Loss: 0.00002435
Iteration 61/1000 | Loss: 0.00002851
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00003885
Iteration 64/1000 | Loss: 0.00002022
Iteration 65/1000 | Loss: 0.00002970
Iteration 66/1000 | Loss: 0.00003553
Iteration 67/1000 | Loss: 0.00002235
Iteration 68/1000 | Loss: 0.00002989
Iteration 69/1000 | Loss: 0.00002419
Iteration 70/1000 | Loss: 0.00003006
Iteration 71/1000 | Loss: 0.00002418
Iteration 72/1000 | Loss: 0.00002971
Iteration 73/1000 | Loss: 0.00002535
Iteration 74/1000 | Loss: 0.00003061
Iteration 75/1000 | Loss: 0.00002465
Iteration 76/1000 | Loss: 0.00002934
Iteration 77/1000 | Loss: 0.00002453
Iteration 78/1000 | Loss: 0.00002981
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00002136
Iteration 82/1000 | Loss: 0.00003334
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001594
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001534
Iteration 88/1000 | Loss: 0.00001534
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001531
Iteration 93/1000 | Loss: 0.00001530
Iteration 94/1000 | Loss: 0.00001529
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001524
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001518
Iteration 112/1000 | Loss: 0.00001518
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001512
Iteration 123/1000 | Loss: 0.00001512
Iteration 124/1000 | Loss: 0.00001510
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001506
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001494
Iteration 133/1000 | Loss: 0.00001494
Iteration 134/1000 | Loss: 0.00001494
Iteration 135/1000 | Loss: 0.00001494
Iteration 136/1000 | Loss: 0.00001494
Iteration 137/1000 | Loss: 0.00001494
Iteration 138/1000 | Loss: 0.00001494
Iteration 139/1000 | Loss: 0.00001494
Iteration 140/1000 | Loss: 0.00001494
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001493
Iteration 143/1000 | Loss: 0.00001493
Iteration 144/1000 | Loss: 0.00001493
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001492
Iteration 153/1000 | Loss: 0.00001492
Iteration 154/1000 | Loss: 0.00001492
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001491
Iteration 158/1000 | Loss: 0.00001491
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001491
Iteration 162/1000 | Loss: 0.00001491
Iteration 163/1000 | Loss: 0.00001491
Iteration 164/1000 | Loss: 0.00001491
Iteration 165/1000 | Loss: 0.00001490
Iteration 166/1000 | Loss: 0.00001490
Iteration 167/1000 | Loss: 0.00001490
Iteration 168/1000 | Loss: 0.00001490
Iteration 169/1000 | Loss: 0.00001490
Iteration 170/1000 | Loss: 0.00001490
Iteration 171/1000 | Loss: 0.00001490
Iteration 172/1000 | Loss: 0.00001490
Iteration 173/1000 | Loss: 0.00001490
Iteration 174/1000 | Loss: 0.00001490
Iteration 175/1000 | Loss: 0.00001490
Iteration 176/1000 | Loss: 0.00001490
Iteration 177/1000 | Loss: 0.00001490
Iteration 178/1000 | Loss: 0.00001490
Iteration 179/1000 | Loss: 0.00001490
Iteration 180/1000 | Loss: 0.00001490
Iteration 181/1000 | Loss: 0.00001490
Iteration 182/1000 | Loss: 0.00001490
Iteration 183/1000 | Loss: 0.00001490
Iteration 184/1000 | Loss: 0.00001490
Iteration 185/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.490100385126425e-05, 1.490100385126425e-05, 1.490100385126425e-05, 1.490100385126425e-05, 1.490100385126425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.490100385126425e-05

Optimization complete. Final v2v error: 3.1932318210601807 mm

Highest mean error: 5.2104034423828125 mm for frame 65

Lowest mean error: 2.484822988510132 mm for frame 12

Saving results

Total time: 197.28094458580017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927981
Iteration 2/25 | Loss: 0.00162619
Iteration 3/25 | Loss: 0.00125432
Iteration 4/25 | Loss: 0.00122933
Iteration 5/25 | Loss: 0.00122446
Iteration 6/25 | Loss: 0.00122306
Iteration 7/25 | Loss: 0.00122306
Iteration 8/25 | Loss: 0.00122306
Iteration 9/25 | Loss: 0.00122306
Iteration 10/25 | Loss: 0.00122306
Iteration 11/25 | Loss: 0.00122306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012230558786541224, 0.0012230558786541224, 0.0012230558786541224, 0.0012230558786541224, 0.0012230558786541224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012230558786541224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95797813
Iteration 2/25 | Loss: 0.00081483
Iteration 3/25 | Loss: 0.00081482
Iteration 4/25 | Loss: 0.00081482
Iteration 5/25 | Loss: 0.00081482
Iteration 6/25 | Loss: 0.00081482
Iteration 7/25 | Loss: 0.00081482
Iteration 8/25 | Loss: 0.00081482
Iteration 9/25 | Loss: 0.00081482
Iteration 10/25 | Loss: 0.00081482
Iteration 11/25 | Loss: 0.00081482
Iteration 12/25 | Loss: 0.00081482
Iteration 13/25 | Loss: 0.00081482
Iteration 14/25 | Loss: 0.00081482
Iteration 15/25 | Loss: 0.00081482
Iteration 16/25 | Loss: 0.00081482
Iteration 17/25 | Loss: 0.00081482
Iteration 18/25 | Loss: 0.00081482
Iteration 19/25 | Loss: 0.00081482
Iteration 20/25 | Loss: 0.00081482
Iteration 21/25 | Loss: 0.00081482
Iteration 22/25 | Loss: 0.00081482
Iteration 23/25 | Loss: 0.00081482
Iteration 24/25 | Loss: 0.00081482
Iteration 25/25 | Loss: 0.00081482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081482
Iteration 2/1000 | Loss: 0.00004248
Iteration 3/1000 | Loss: 0.00002958
Iteration 4/1000 | Loss: 0.00002461
Iteration 5/1000 | Loss: 0.00002316
Iteration 6/1000 | Loss: 0.00002219
Iteration 7/1000 | Loss: 0.00002162
Iteration 8/1000 | Loss: 0.00002098
Iteration 9/1000 | Loss: 0.00002067
Iteration 10/1000 | Loss: 0.00002047
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001974
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001959
Iteration 17/1000 | Loss: 0.00001956
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001950
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001938
Iteration 25/1000 | Loss: 0.00001935
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001929
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001927
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001920
Iteration 51/1000 | Loss: 0.00001920
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001918
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001917
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001917
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001917
Iteration 66/1000 | Loss: 0.00001917
Iteration 67/1000 | Loss: 0.00001917
Iteration 68/1000 | Loss: 0.00001917
Iteration 69/1000 | Loss: 0.00001916
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001911
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001911
Iteration 89/1000 | Loss: 0.00001911
Iteration 90/1000 | Loss: 0.00001910
Iteration 91/1000 | Loss: 0.00001910
Iteration 92/1000 | Loss: 0.00001910
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001910
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00001909
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001905
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001905
Iteration 128/1000 | Loss: 0.00001905
Iteration 129/1000 | Loss: 0.00001905
Iteration 130/1000 | Loss: 0.00001905
Iteration 131/1000 | Loss: 0.00001905
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001904
Iteration 135/1000 | Loss: 0.00001904
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.9027536836802028e-05, 1.9027536836802028e-05, 1.9027536836802028e-05, 1.9027536836802028e-05, 1.9027536836802028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9027536836802028e-05

Optimization complete. Final v2v error: 3.6387648582458496 mm

Highest mean error: 4.201470851898193 mm for frame 137

Lowest mean error: 2.918823003768921 mm for frame 27

Saving results

Total time: 42.595179080963135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003583
Iteration 2/25 | Loss: 0.00188649
Iteration 3/25 | Loss: 0.00155406
Iteration 4/25 | Loss: 0.00143919
Iteration 5/25 | Loss: 0.00132974
Iteration 6/25 | Loss: 0.00139691
Iteration 7/25 | Loss: 0.00135879
Iteration 8/25 | Loss: 0.00141287
Iteration 9/25 | Loss: 0.00133203
Iteration 10/25 | Loss: 0.00121054
Iteration 11/25 | Loss: 0.00119598
Iteration 12/25 | Loss: 0.00119090
Iteration 13/25 | Loss: 0.00119561
Iteration 14/25 | Loss: 0.00118856
Iteration 15/25 | Loss: 0.00116760
Iteration 16/25 | Loss: 0.00116362
Iteration 17/25 | Loss: 0.00115783
Iteration 18/25 | Loss: 0.00115604
Iteration 19/25 | Loss: 0.00114674
Iteration 20/25 | Loss: 0.00114264
Iteration 21/25 | Loss: 0.00113630
Iteration 22/25 | Loss: 0.00113416
Iteration 23/25 | Loss: 0.00113666
Iteration 24/25 | Loss: 0.00113501
Iteration 25/25 | Loss: 0.00114041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36026084
Iteration 2/25 | Loss: 0.00092467
Iteration 3/25 | Loss: 0.00092467
Iteration 4/25 | Loss: 0.00092467
Iteration 5/25 | Loss: 0.00092467
Iteration 6/25 | Loss: 0.00092466
Iteration 7/25 | Loss: 0.00092466
Iteration 8/25 | Loss: 0.00092466
Iteration 9/25 | Loss: 0.00092466
Iteration 10/25 | Loss: 0.00092466
Iteration 11/25 | Loss: 0.00092466
Iteration 12/25 | Loss: 0.00092466
Iteration 13/25 | Loss: 0.00092466
Iteration 14/25 | Loss: 0.00092466
Iteration 15/25 | Loss: 0.00092466
Iteration 16/25 | Loss: 0.00092466
Iteration 17/25 | Loss: 0.00092466
Iteration 18/25 | Loss: 0.00092466
Iteration 19/25 | Loss: 0.00092466
Iteration 20/25 | Loss: 0.00092466
Iteration 21/25 | Loss: 0.00092466
Iteration 22/25 | Loss: 0.00092466
Iteration 23/25 | Loss: 0.00092466
Iteration 24/25 | Loss: 0.00092466
Iteration 25/25 | Loss: 0.00092466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092466
Iteration 2/1000 | Loss: 0.00003300
Iteration 3/1000 | Loss: 0.00004998
Iteration 4/1000 | Loss: 0.00005143
Iteration 5/1000 | Loss: 0.00017246
Iteration 6/1000 | Loss: 0.00021554
Iteration 7/1000 | Loss: 0.00022688
Iteration 8/1000 | Loss: 0.00021234
Iteration 9/1000 | Loss: 0.00020441
Iteration 10/1000 | Loss: 0.00006180
Iteration 11/1000 | Loss: 0.00002089
Iteration 12/1000 | Loss: 0.00011133
Iteration 13/1000 | Loss: 0.00023078
Iteration 14/1000 | Loss: 0.00016986
Iteration 15/1000 | Loss: 0.00019595
Iteration 16/1000 | Loss: 0.00019789
Iteration 17/1000 | Loss: 0.00019788
Iteration 18/1000 | Loss: 0.00018653
Iteration 19/1000 | Loss: 0.00022328
Iteration 20/1000 | Loss: 0.00010829
Iteration 21/1000 | Loss: 0.00005247
Iteration 22/1000 | Loss: 0.00013731
Iteration 23/1000 | Loss: 0.00041815
Iteration 24/1000 | Loss: 0.00003022
Iteration 25/1000 | Loss: 0.00002363
Iteration 26/1000 | Loss: 0.00009952
Iteration 27/1000 | Loss: 0.00002158
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00003436
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00003395
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001666
Iteration 36/1000 | Loss: 0.00001979
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001623
Iteration 40/1000 | Loss: 0.00001622
Iteration 41/1000 | Loss: 0.00001622
Iteration 42/1000 | Loss: 0.00001618
Iteration 43/1000 | Loss: 0.00030019
Iteration 44/1000 | Loss: 0.00022857
Iteration 45/1000 | Loss: 0.00026470
Iteration 46/1000 | Loss: 0.00039274
Iteration 47/1000 | Loss: 0.00026832
Iteration 48/1000 | Loss: 0.00027886
Iteration 49/1000 | Loss: 0.00027174
Iteration 50/1000 | Loss: 0.00012092
Iteration 51/1000 | Loss: 0.00024638
Iteration 52/1000 | Loss: 0.00002894
Iteration 53/1000 | Loss: 0.00002603
Iteration 54/1000 | Loss: 0.00003740
Iteration 55/1000 | Loss: 0.00022381
Iteration 56/1000 | Loss: 0.00018589
Iteration 57/1000 | Loss: 0.00022731
Iteration 58/1000 | Loss: 0.00018672
Iteration 59/1000 | Loss: 0.00018841
Iteration 60/1000 | Loss: 0.00003185
Iteration 61/1000 | Loss: 0.00002091
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00014175
Iteration 64/1000 | Loss: 0.00008085
Iteration 65/1000 | Loss: 0.00002561
Iteration 66/1000 | Loss: 0.00033626
Iteration 67/1000 | Loss: 0.00011619
Iteration 68/1000 | Loss: 0.00006221
Iteration 69/1000 | Loss: 0.00034082
Iteration 70/1000 | Loss: 0.00002602
Iteration 71/1000 | Loss: 0.00028593
Iteration 72/1000 | Loss: 0.00002744
Iteration 73/1000 | Loss: 0.00002201
Iteration 74/1000 | Loss: 0.00002002
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00029005
Iteration 78/1000 | Loss: 0.00017255
Iteration 79/1000 | Loss: 0.00018646
Iteration 80/1000 | Loss: 0.00019238
Iteration 81/1000 | Loss: 0.00015931
Iteration 82/1000 | Loss: 0.00021344
Iteration 83/1000 | Loss: 0.00021975
Iteration 84/1000 | Loss: 0.00016745
Iteration 85/1000 | Loss: 0.00003788
Iteration 86/1000 | Loss: 0.00003709
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00044825
Iteration 91/1000 | Loss: 0.00036741
Iteration 92/1000 | Loss: 0.00043477
Iteration 93/1000 | Loss: 0.00036268
Iteration 94/1000 | Loss: 0.00002231
Iteration 95/1000 | Loss: 0.00002535
Iteration 96/1000 | Loss: 0.00044190
Iteration 97/1000 | Loss: 0.00005707
Iteration 98/1000 | Loss: 0.00024919
Iteration 99/1000 | Loss: 0.00008889
Iteration 100/1000 | Loss: 0.00022443
Iteration 101/1000 | Loss: 0.00011662
Iteration 102/1000 | Loss: 0.00040473
Iteration 103/1000 | Loss: 0.00003552
Iteration 104/1000 | Loss: 0.00002317
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00004238
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00004557
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001865
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001485
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00001453
Iteration 116/1000 | Loss: 0.00002290
Iteration 117/1000 | Loss: 0.00002043
Iteration 118/1000 | Loss: 0.00002401
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00002101
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001439
Iteration 136/1000 | Loss: 0.00001439
Iteration 137/1000 | Loss: 0.00001439
Iteration 138/1000 | Loss: 0.00001439
Iteration 139/1000 | Loss: 0.00001439
Iteration 140/1000 | Loss: 0.00001439
Iteration 141/1000 | Loss: 0.00001439
Iteration 142/1000 | Loss: 0.00001439
Iteration 143/1000 | Loss: 0.00001439
Iteration 144/1000 | Loss: 0.00001439
Iteration 145/1000 | Loss: 0.00001439
Iteration 146/1000 | Loss: 0.00001439
Iteration 147/1000 | Loss: 0.00001439
Iteration 148/1000 | Loss: 0.00001439
Iteration 149/1000 | Loss: 0.00001439
Iteration 150/1000 | Loss: 0.00001439
Iteration 151/1000 | Loss: 0.00001439
Iteration 152/1000 | Loss: 0.00001439
Iteration 153/1000 | Loss: 0.00001439
Iteration 154/1000 | Loss: 0.00001439
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001439
Iteration 168/1000 | Loss: 0.00001439
Iteration 169/1000 | Loss: 0.00001439
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001439
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00001439
Iteration 174/1000 | Loss: 0.00001439
Iteration 175/1000 | Loss: 0.00001439
Iteration 176/1000 | Loss: 0.00001439
Iteration 177/1000 | Loss: 0.00001439
Iteration 178/1000 | Loss: 0.00001439
Iteration 179/1000 | Loss: 0.00001439
Iteration 180/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.439193329133559e-05, 1.439193329133559e-05, 1.439193329133559e-05, 1.439193329133559e-05, 1.439193329133559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.439193329133559e-05

Optimization complete. Final v2v error: 3.1880617141723633 mm

Highest mean error: 4.499505043029785 mm for frame 44

Lowest mean error: 2.6400976181030273 mm for frame 21

Saving results

Total time: 203.81913900375366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008381
Iteration 2/25 | Loss: 0.00317766
Iteration 3/25 | Loss: 0.00192755
Iteration 4/25 | Loss: 0.00176823
Iteration 5/25 | Loss: 0.00200426
Iteration 6/25 | Loss: 0.00183042
Iteration 7/25 | Loss: 0.00147429
Iteration 8/25 | Loss: 0.00125474
Iteration 9/25 | Loss: 0.00120321
Iteration 10/25 | Loss: 0.00115232
Iteration 11/25 | Loss: 0.00116459
Iteration 12/25 | Loss: 0.00118415
Iteration 13/25 | Loss: 0.00116165
Iteration 14/25 | Loss: 0.00117639
Iteration 15/25 | Loss: 0.00114112
Iteration 16/25 | Loss: 0.00114070
Iteration 17/25 | Loss: 0.00114730
Iteration 18/25 | Loss: 0.00115029
Iteration 19/25 | Loss: 0.00115017
Iteration 20/25 | Loss: 0.00114098
Iteration 21/25 | Loss: 0.00113695
Iteration 22/25 | Loss: 0.00113736
Iteration 23/25 | Loss: 0.00113800
Iteration 24/25 | Loss: 0.00113799
Iteration 25/25 | Loss: 0.00113682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36866724
Iteration 2/25 | Loss: 0.00144764
Iteration 3/25 | Loss: 0.00106839
Iteration 4/25 | Loss: 0.00106839
Iteration 5/25 | Loss: 0.00106839
Iteration 6/25 | Loss: 0.00106839
Iteration 7/25 | Loss: 0.00106839
Iteration 8/25 | Loss: 0.00106839
Iteration 9/25 | Loss: 0.00106839
Iteration 10/25 | Loss: 0.00106839
Iteration 11/25 | Loss: 0.00106839
Iteration 12/25 | Loss: 0.00106839
Iteration 13/25 | Loss: 0.00106839
Iteration 14/25 | Loss: 0.00106839
Iteration 15/25 | Loss: 0.00106839
Iteration 16/25 | Loss: 0.00106839
Iteration 17/25 | Loss: 0.00106839
Iteration 18/25 | Loss: 0.00106839
Iteration 19/25 | Loss: 0.00106839
Iteration 20/25 | Loss: 0.00106839
Iteration 21/25 | Loss: 0.00106839
Iteration 22/25 | Loss: 0.00106839
Iteration 23/25 | Loss: 0.00106839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010683888103812933, 0.0010683888103812933, 0.0010683888103812933, 0.0010683888103812933, 0.0010683888103812933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010683888103812933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106839
Iteration 2/1000 | Loss: 0.00035897
Iteration 3/1000 | Loss: 0.00003752
Iteration 4/1000 | Loss: 0.00003999
Iteration 5/1000 | Loss: 0.00003142
Iteration 6/1000 | Loss: 0.00002745
Iteration 7/1000 | Loss: 0.00002493
Iteration 8/1000 | Loss: 0.00002349
Iteration 9/1000 | Loss: 0.00002258
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002134
Iteration 12/1000 | Loss: 0.00002096
Iteration 13/1000 | Loss: 0.00177652
Iteration 14/1000 | Loss: 0.00002517
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001295
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001142
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001119
Iteration 28/1000 | Loss: 0.00001116
Iteration 29/1000 | Loss: 0.00001115
Iteration 30/1000 | Loss: 0.00001115
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001111
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001101
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001100
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001099
Iteration 66/1000 | Loss: 0.00001099
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001098
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001096
Iteration 83/1000 | Loss: 0.00001096
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001095
Iteration 94/1000 | Loss: 0.00001095
Iteration 95/1000 | Loss: 0.00001095
Iteration 96/1000 | Loss: 0.00001095
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001094
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001094
Iteration 106/1000 | Loss: 0.00001094
Iteration 107/1000 | Loss: 0.00001094
Iteration 108/1000 | Loss: 0.00001094
Iteration 109/1000 | Loss: 0.00001094
Iteration 110/1000 | Loss: 0.00001094
Iteration 111/1000 | Loss: 0.00001094
Iteration 112/1000 | Loss: 0.00001094
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001093
Iteration 115/1000 | Loss: 0.00001093
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001093
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001092
Iteration 123/1000 | Loss: 0.00001092
Iteration 124/1000 | Loss: 0.00001092
Iteration 125/1000 | Loss: 0.00001092
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001092
Iteration 134/1000 | Loss: 0.00001092
Iteration 135/1000 | Loss: 0.00001092
Iteration 136/1000 | Loss: 0.00001092
Iteration 137/1000 | Loss: 0.00001092
Iteration 138/1000 | Loss: 0.00001092
Iteration 139/1000 | Loss: 0.00001092
Iteration 140/1000 | Loss: 0.00001092
Iteration 141/1000 | Loss: 0.00001091
Iteration 142/1000 | Loss: 0.00001091
Iteration 143/1000 | Loss: 0.00001091
Iteration 144/1000 | Loss: 0.00001091
Iteration 145/1000 | Loss: 0.00001091
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001091
Iteration 162/1000 | Loss: 0.00001091
Iteration 163/1000 | Loss: 0.00001091
Iteration 164/1000 | Loss: 0.00001091
Iteration 165/1000 | Loss: 0.00001091
Iteration 166/1000 | Loss: 0.00001091
Iteration 167/1000 | Loss: 0.00001091
Iteration 168/1000 | Loss: 0.00001091
Iteration 169/1000 | Loss: 0.00001091
Iteration 170/1000 | Loss: 0.00001091
Iteration 171/1000 | Loss: 0.00001091
Iteration 172/1000 | Loss: 0.00001091
Iteration 173/1000 | Loss: 0.00001091
Iteration 174/1000 | Loss: 0.00001091
Iteration 175/1000 | Loss: 0.00001091
Iteration 176/1000 | Loss: 0.00001091
Iteration 177/1000 | Loss: 0.00001091
Iteration 178/1000 | Loss: 0.00001091
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001091
Iteration 183/1000 | Loss: 0.00001091
Iteration 184/1000 | Loss: 0.00001091
Iteration 185/1000 | Loss: 0.00001091
Iteration 186/1000 | Loss: 0.00001091
Iteration 187/1000 | Loss: 0.00001091
Iteration 188/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.090674504666822e-05, 1.090674504666822e-05, 1.090674504666822e-05, 1.090674504666822e-05, 1.090674504666822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.090674504666822e-05

Optimization complete. Final v2v error: 2.8174843788146973 mm

Highest mean error: 3.5366978645324707 mm for frame 75

Lowest mean error: 2.571887493133545 mm for frame 34

Saving results

Total time: 92.91986012458801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739882
Iteration 2/25 | Loss: 0.00190302
Iteration 3/25 | Loss: 0.00159444
Iteration 4/25 | Loss: 0.00155309
Iteration 5/25 | Loss: 0.00151706
Iteration 6/25 | Loss: 0.00148651
Iteration 7/25 | Loss: 0.00145904
Iteration 8/25 | Loss: 0.00145224
Iteration 9/25 | Loss: 0.00145367
Iteration 10/25 | Loss: 0.00146182
Iteration 11/25 | Loss: 0.00145087
Iteration 12/25 | Loss: 0.00141482
Iteration 13/25 | Loss: 0.00141246
Iteration 14/25 | Loss: 0.00144048
Iteration 15/25 | Loss: 0.00147997
Iteration 16/25 | Loss: 0.00144196
Iteration 17/25 | Loss: 0.00139472
Iteration 18/25 | Loss: 0.00137032
Iteration 19/25 | Loss: 0.00136682
Iteration 20/25 | Loss: 0.00136090
Iteration 21/25 | Loss: 0.00136099
Iteration 22/25 | Loss: 0.00136092
Iteration 23/25 | Loss: 0.00136087
Iteration 24/25 | Loss: 0.00136080
Iteration 25/25 | Loss: 0.00136038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66493642
Iteration 2/25 | Loss: 0.00158091
Iteration 3/25 | Loss: 0.00158077
Iteration 4/25 | Loss: 0.00158077
Iteration 5/25 | Loss: 0.00158077
Iteration 6/25 | Loss: 0.00158076
Iteration 7/25 | Loss: 0.00158076
Iteration 8/25 | Loss: 0.00158076
Iteration 9/25 | Loss: 0.00158076
Iteration 10/25 | Loss: 0.00158076
Iteration 11/25 | Loss: 0.00158076
Iteration 12/25 | Loss: 0.00158076
Iteration 13/25 | Loss: 0.00158076
Iteration 14/25 | Loss: 0.00158076
Iteration 15/25 | Loss: 0.00158076
Iteration 16/25 | Loss: 0.00158076
Iteration 17/25 | Loss: 0.00158076
Iteration 18/25 | Loss: 0.00158076
Iteration 19/25 | Loss: 0.00158076
Iteration 20/25 | Loss: 0.00158076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015807620948180556, 0.0015807620948180556, 0.0015807620948180556, 0.0015807620948180556, 0.0015807620948180556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015807620948180556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158076
Iteration 2/1000 | Loss: 0.00274591
Iteration 3/1000 | Loss: 0.00010562
Iteration 4/1000 | Loss: 0.00006737
Iteration 5/1000 | Loss: 0.00005174
Iteration 6/1000 | Loss: 0.00004497
Iteration 7/1000 | Loss: 0.00004120
Iteration 8/1000 | Loss: 0.00003952
Iteration 9/1000 | Loss: 0.00004057
Iteration 10/1000 | Loss: 0.00003868
Iteration 11/1000 | Loss: 0.00003811
Iteration 12/1000 | Loss: 0.00003614
Iteration 13/1000 | Loss: 0.00003753
Iteration 14/1000 | Loss: 0.00003693
Iteration 15/1000 | Loss: 0.00003623
Iteration 16/1000 | Loss: 0.00003586
Iteration 17/1000 | Loss: 0.00003875
Iteration 18/1000 | Loss: 0.00003863
Iteration 19/1000 | Loss: 0.00003485
Iteration 20/1000 | Loss: 0.00003643
Iteration 21/1000 | Loss: 0.00003714
Iteration 22/1000 | Loss: 0.00004156
Iteration 23/1000 | Loss: 0.00003991
Iteration 24/1000 | Loss: 0.00004057
Iteration 25/1000 | Loss: 0.00003468
Iteration 26/1000 | Loss: 0.00003941
Iteration 27/1000 | Loss: 0.00003453
Iteration 28/1000 | Loss: 0.00003954
Iteration 29/1000 | Loss: 0.00003701
Iteration 30/1000 | Loss: 0.00004783
Iteration 31/1000 | Loss: 0.00005105
Iteration 32/1000 | Loss: 0.00004814
Iteration 33/1000 | Loss: 0.00004869
Iteration 34/1000 | Loss: 0.00004108
Iteration 35/1000 | Loss: 0.00003411
Iteration 36/1000 | Loss: 0.00003352
Iteration 37/1000 | Loss: 0.00003312
Iteration 38/1000 | Loss: 0.00003294
Iteration 39/1000 | Loss: 0.00003292
Iteration 40/1000 | Loss: 0.00003280
Iteration 41/1000 | Loss: 0.00003277
Iteration 42/1000 | Loss: 0.00003273
Iteration 43/1000 | Loss: 0.00003273
Iteration 44/1000 | Loss: 0.00003270
Iteration 45/1000 | Loss: 0.00003270
Iteration 46/1000 | Loss: 0.00003269
Iteration 47/1000 | Loss: 0.00003269
Iteration 48/1000 | Loss: 0.00003268
Iteration 49/1000 | Loss: 0.00003268
Iteration 50/1000 | Loss: 0.00003267
Iteration 51/1000 | Loss: 0.00003267
Iteration 52/1000 | Loss: 0.00003265
Iteration 53/1000 | Loss: 0.00003264
Iteration 54/1000 | Loss: 0.00003261
Iteration 55/1000 | Loss: 0.00003261
Iteration 56/1000 | Loss: 0.00003261
Iteration 57/1000 | Loss: 0.00003261
Iteration 58/1000 | Loss: 0.00003261
Iteration 59/1000 | Loss: 0.00003261
Iteration 60/1000 | Loss: 0.00003261
Iteration 61/1000 | Loss: 0.00003261
Iteration 62/1000 | Loss: 0.00003261
Iteration 63/1000 | Loss: 0.00003260
Iteration 64/1000 | Loss: 0.00003260
Iteration 65/1000 | Loss: 0.00003260
Iteration 66/1000 | Loss: 0.00003260
Iteration 67/1000 | Loss: 0.00003259
Iteration 68/1000 | Loss: 0.00003259
Iteration 69/1000 | Loss: 0.00003258
Iteration 70/1000 | Loss: 0.00003258
Iteration 71/1000 | Loss: 0.00003258
Iteration 72/1000 | Loss: 0.00003258
Iteration 73/1000 | Loss: 0.00003257
Iteration 74/1000 | Loss: 0.00003257
Iteration 75/1000 | Loss: 0.00003257
Iteration 76/1000 | Loss: 0.00003256
Iteration 77/1000 | Loss: 0.00003256
Iteration 78/1000 | Loss: 0.00003256
Iteration 79/1000 | Loss: 0.00003255
Iteration 80/1000 | Loss: 0.00003255
Iteration 81/1000 | Loss: 0.00003255
Iteration 82/1000 | Loss: 0.00004668
Iteration 83/1000 | Loss: 0.00004668
Iteration 84/1000 | Loss: 0.00004668
Iteration 85/1000 | Loss: 0.00004667
Iteration 86/1000 | Loss: 0.00004667
Iteration 87/1000 | Loss: 0.00004311
Iteration 88/1000 | Loss: 0.00003531
Iteration 89/1000 | Loss: 0.00004070
Iteration 90/1000 | Loss: 0.00003249
Iteration 91/1000 | Loss: 0.00003249
Iteration 92/1000 | Loss: 0.00003249
Iteration 93/1000 | Loss: 0.00003249
Iteration 94/1000 | Loss: 0.00003249
Iteration 95/1000 | Loss: 0.00003249
Iteration 96/1000 | Loss: 0.00003249
Iteration 97/1000 | Loss: 0.00003249
Iteration 98/1000 | Loss: 0.00003249
Iteration 99/1000 | Loss: 0.00003249
Iteration 100/1000 | Loss: 0.00003249
Iteration 101/1000 | Loss: 0.00003249
Iteration 102/1000 | Loss: 0.00003249
Iteration 103/1000 | Loss: 0.00003249
Iteration 104/1000 | Loss: 0.00003249
Iteration 105/1000 | Loss: 0.00003249
Iteration 106/1000 | Loss: 0.00003249
Iteration 107/1000 | Loss: 0.00003249
Iteration 108/1000 | Loss: 0.00003249
Iteration 109/1000 | Loss: 0.00003249
Iteration 110/1000 | Loss: 0.00003249
Iteration 111/1000 | Loss: 0.00003249
Iteration 112/1000 | Loss: 0.00003249
Iteration 113/1000 | Loss: 0.00003249
Iteration 114/1000 | Loss: 0.00003249
Iteration 115/1000 | Loss: 0.00003249
Iteration 116/1000 | Loss: 0.00003249
Iteration 117/1000 | Loss: 0.00003249
Iteration 118/1000 | Loss: 0.00003249
Iteration 119/1000 | Loss: 0.00003249
Iteration 120/1000 | Loss: 0.00003249
Iteration 121/1000 | Loss: 0.00003249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.248573557357304e-05, 3.248573557357304e-05, 3.248573557357304e-05, 3.248573557357304e-05, 3.248573557357304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.248573557357304e-05

Optimization complete. Final v2v error: 4.491763591766357 mm

Highest mean error: 7.539464950561523 mm for frame 179

Lowest mean error: 2.913525342941284 mm for frame 239

Saving results

Total time: 127.32004046440125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969370
Iteration 2/25 | Loss: 0.00226445
Iteration 3/25 | Loss: 0.00152192
Iteration 4/25 | Loss: 0.00133100
Iteration 5/25 | Loss: 0.00131753
Iteration 6/25 | Loss: 0.00126360
Iteration 7/25 | Loss: 0.00125310
Iteration 8/25 | Loss: 0.00123678
Iteration 9/25 | Loss: 0.00122297
Iteration 10/25 | Loss: 0.00119694
Iteration 11/25 | Loss: 0.00120699
Iteration 12/25 | Loss: 0.00122914
Iteration 13/25 | Loss: 0.00122164
Iteration 14/25 | Loss: 0.00119998
Iteration 15/25 | Loss: 0.00117828
Iteration 16/25 | Loss: 0.00117269
Iteration 17/25 | Loss: 0.00116918
Iteration 18/25 | Loss: 0.00117204
Iteration 19/25 | Loss: 0.00116547
Iteration 20/25 | Loss: 0.00116409
Iteration 21/25 | Loss: 0.00116893
Iteration 22/25 | Loss: 0.00117021
Iteration 23/25 | Loss: 0.00116588
Iteration 24/25 | Loss: 0.00116261
Iteration 25/25 | Loss: 0.00116466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24710739
Iteration 2/25 | Loss: 0.00082963
Iteration 3/25 | Loss: 0.00073425
Iteration 4/25 | Loss: 0.00082961
Iteration 5/25 | Loss: 0.00082961
Iteration 6/25 | Loss: 0.00082961
Iteration 7/25 | Loss: 0.00082961
Iteration 8/25 | Loss: 0.00082961
Iteration 9/25 | Loss: 0.00082961
Iteration 10/25 | Loss: 0.00082961
Iteration 11/25 | Loss: 0.00082961
Iteration 12/25 | Loss: 0.00082961
Iteration 13/25 | Loss: 0.00082961
Iteration 14/25 | Loss: 0.00082961
Iteration 15/25 | Loss: 0.00082961
Iteration 16/25 | Loss: 0.00082961
Iteration 17/25 | Loss: 0.00082961
Iteration 18/25 | Loss: 0.00082961
Iteration 19/25 | Loss: 0.00082961
Iteration 20/25 | Loss: 0.00082961
Iteration 21/25 | Loss: 0.00082961
Iteration 22/25 | Loss: 0.00082961
Iteration 23/25 | Loss: 0.00082961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008296078303828835, 0.0008296078303828835, 0.0008296078303828835, 0.0008296078303828835, 0.0008296078303828835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008296078303828835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082961
Iteration 2/1000 | Loss: 0.00056022
Iteration 3/1000 | Loss: 0.00079798
Iteration 4/1000 | Loss: 0.00005013
Iteration 5/1000 | Loss: 0.00020422
Iteration 6/1000 | Loss: 0.00011073
Iteration 7/1000 | Loss: 0.00003953
Iteration 8/1000 | Loss: 0.00003511
Iteration 9/1000 | Loss: 0.00003333
Iteration 10/1000 | Loss: 0.00003166
Iteration 11/1000 | Loss: 0.00003009
Iteration 12/1000 | Loss: 0.00002880
Iteration 13/1000 | Loss: 0.00002757
Iteration 14/1000 | Loss: 0.00003778
Iteration 15/1000 | Loss: 0.00015681
Iteration 16/1000 | Loss: 0.00026034
Iteration 17/1000 | Loss: 0.00003200
Iteration 18/1000 | Loss: 0.00006432
Iteration 19/1000 | Loss: 0.00002934
Iteration 20/1000 | Loss: 0.00002768
Iteration 21/1000 | Loss: 0.00002708
Iteration 22/1000 | Loss: 0.00010745
Iteration 23/1000 | Loss: 0.00002694
Iteration 24/1000 | Loss: 0.00002623
Iteration 25/1000 | Loss: 0.00018289
Iteration 26/1000 | Loss: 0.00009624
Iteration 27/1000 | Loss: 0.00007916
Iteration 28/1000 | Loss: 0.00009166
Iteration 29/1000 | Loss: 0.00006626
Iteration 30/1000 | Loss: 0.00002803
Iteration 31/1000 | Loss: 0.00002658
Iteration 32/1000 | Loss: 0.00002579
Iteration 33/1000 | Loss: 0.00010556
Iteration 34/1000 | Loss: 0.00016082
Iteration 35/1000 | Loss: 0.00007992
Iteration 36/1000 | Loss: 0.00026440
Iteration 37/1000 | Loss: 0.00071291
Iteration 38/1000 | Loss: 0.00005667
Iteration 39/1000 | Loss: 0.00002924
Iteration 40/1000 | Loss: 0.00007792
Iteration 41/1000 | Loss: 0.00015563
Iteration 42/1000 | Loss: 0.00003218
Iteration 43/1000 | Loss: 0.00002752
Iteration 44/1000 | Loss: 0.00003733
Iteration 45/1000 | Loss: 0.00010151
Iteration 46/1000 | Loss: 0.00003024
Iteration 47/1000 | Loss: 0.00002663
Iteration 48/1000 | Loss: 0.00002613
Iteration 49/1000 | Loss: 0.00002588
Iteration 50/1000 | Loss: 0.00002567
Iteration 51/1000 | Loss: 0.00008605
Iteration 52/1000 | Loss: 0.00003328
Iteration 53/1000 | Loss: 0.00005234
Iteration 54/1000 | Loss: 0.00002519
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00002493
Iteration 57/1000 | Loss: 0.00002470
Iteration 58/1000 | Loss: 0.00008471
Iteration 59/1000 | Loss: 0.00002456
Iteration 60/1000 | Loss: 0.00002423
Iteration 61/1000 | Loss: 0.00002397
Iteration 62/1000 | Loss: 0.00002374
Iteration 63/1000 | Loss: 0.00002358
Iteration 64/1000 | Loss: 0.00002339
Iteration 65/1000 | Loss: 0.00020213
Iteration 66/1000 | Loss: 0.00106536
Iteration 67/1000 | Loss: 0.00042739
Iteration 68/1000 | Loss: 0.00006097
Iteration 69/1000 | Loss: 0.00020486
Iteration 70/1000 | Loss: 0.00031858
Iteration 71/1000 | Loss: 0.00024031
Iteration 72/1000 | Loss: 0.00019352
Iteration 73/1000 | Loss: 0.00016002
Iteration 74/1000 | Loss: 0.00023626
Iteration 75/1000 | Loss: 0.00013984
Iteration 76/1000 | Loss: 0.00005225
Iteration 77/1000 | Loss: 0.00003711
Iteration 78/1000 | Loss: 0.00004403
Iteration 79/1000 | Loss: 0.00006639
Iteration 80/1000 | Loss: 0.00003254
Iteration 81/1000 | Loss: 0.00012965
Iteration 82/1000 | Loss: 0.00003025
Iteration 83/1000 | Loss: 0.00008473
Iteration 84/1000 | Loss: 0.00003558
Iteration 85/1000 | Loss: 0.00003081
Iteration 86/1000 | Loss: 0.00002910
Iteration 87/1000 | Loss: 0.00005453
Iteration 88/1000 | Loss: 0.00007287
Iteration 89/1000 | Loss: 0.00003295
Iteration 90/1000 | Loss: 0.00002654
Iteration 91/1000 | Loss: 0.00004309
Iteration 92/1000 | Loss: 0.00002658
Iteration 93/1000 | Loss: 0.00002556
Iteration 94/1000 | Loss: 0.00002520
Iteration 95/1000 | Loss: 0.00002499
Iteration 96/1000 | Loss: 0.00002495
Iteration 97/1000 | Loss: 0.00002492
Iteration 98/1000 | Loss: 0.00002477
Iteration 99/1000 | Loss: 0.00002476
Iteration 100/1000 | Loss: 0.00002475
Iteration 101/1000 | Loss: 0.00002475
Iteration 102/1000 | Loss: 0.00002473
Iteration 103/1000 | Loss: 0.00002470
Iteration 104/1000 | Loss: 0.00002470
Iteration 105/1000 | Loss: 0.00002470
Iteration 106/1000 | Loss: 0.00002469
Iteration 107/1000 | Loss: 0.00002460
Iteration 108/1000 | Loss: 0.00015810
Iteration 109/1000 | Loss: 0.00002477
Iteration 110/1000 | Loss: 0.00002446
Iteration 111/1000 | Loss: 0.00002446
Iteration 112/1000 | Loss: 0.00002446
Iteration 113/1000 | Loss: 0.00002444
Iteration 114/1000 | Loss: 0.00002444
Iteration 115/1000 | Loss: 0.00002443
Iteration 116/1000 | Loss: 0.00002443
Iteration 117/1000 | Loss: 0.00002439
Iteration 118/1000 | Loss: 0.00002438
Iteration 119/1000 | Loss: 0.00002438
Iteration 120/1000 | Loss: 0.00002436
Iteration 121/1000 | Loss: 0.00002435
Iteration 122/1000 | Loss: 0.00002435
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002434
Iteration 125/1000 | Loss: 0.00002434
Iteration 126/1000 | Loss: 0.00002433
Iteration 127/1000 | Loss: 0.00002433
Iteration 128/1000 | Loss: 0.00002431
Iteration 129/1000 | Loss: 0.00002430
Iteration 130/1000 | Loss: 0.00002428
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00011039
Iteration 133/1000 | Loss: 0.00019674
Iteration 134/1000 | Loss: 0.00002852
Iteration 135/1000 | Loss: 0.00002458
Iteration 136/1000 | Loss: 0.00002409
Iteration 137/1000 | Loss: 0.00002401
Iteration 138/1000 | Loss: 0.00002392
Iteration 139/1000 | Loss: 0.00002392
Iteration 140/1000 | Loss: 0.00002391
Iteration 141/1000 | Loss: 0.00002391
Iteration 142/1000 | Loss: 0.00002389
Iteration 143/1000 | Loss: 0.00002377
Iteration 144/1000 | Loss: 0.00002373
Iteration 145/1000 | Loss: 0.00002373
Iteration 146/1000 | Loss: 0.00002372
Iteration 147/1000 | Loss: 0.00002360
Iteration 148/1000 | Loss: 0.00002358
Iteration 149/1000 | Loss: 0.00002358
Iteration 150/1000 | Loss: 0.00002357
Iteration 151/1000 | Loss: 0.00002355
Iteration 152/1000 | Loss: 0.00002352
Iteration 153/1000 | Loss: 0.00002350
Iteration 154/1000 | Loss: 0.00002349
Iteration 155/1000 | Loss: 0.00002349
Iteration 156/1000 | Loss: 0.00002343
Iteration 157/1000 | Loss: 0.00002342
Iteration 158/1000 | Loss: 0.00002340
Iteration 159/1000 | Loss: 0.00002664
Iteration 160/1000 | Loss: 0.00002359
Iteration 161/1000 | Loss: 0.00002331
Iteration 162/1000 | Loss: 0.00002301
Iteration 163/1000 | Loss: 0.00024825
Iteration 164/1000 | Loss: 0.00027541
Iteration 165/1000 | Loss: 0.00014164
Iteration 166/1000 | Loss: 0.00008933
Iteration 167/1000 | Loss: 0.00016528
Iteration 168/1000 | Loss: 0.00003516
Iteration 169/1000 | Loss: 0.00014795
Iteration 170/1000 | Loss: 0.00014547
Iteration 171/1000 | Loss: 0.00022600
Iteration 172/1000 | Loss: 0.00036579
Iteration 173/1000 | Loss: 0.00022810
Iteration 174/1000 | Loss: 0.00023153
Iteration 175/1000 | Loss: 0.00015656
Iteration 176/1000 | Loss: 0.00014775
Iteration 177/1000 | Loss: 0.00004478
Iteration 178/1000 | Loss: 0.00004412
Iteration 179/1000 | Loss: 0.00003235
Iteration 180/1000 | Loss: 0.00017112
Iteration 181/1000 | Loss: 0.00006570
Iteration 182/1000 | Loss: 0.00003164
Iteration 183/1000 | Loss: 0.00004765
Iteration 184/1000 | Loss: 0.00002692
Iteration 185/1000 | Loss: 0.00002585
Iteration 186/1000 | Loss: 0.00002481
Iteration 187/1000 | Loss: 0.00004915
Iteration 188/1000 | Loss: 0.00002385
Iteration 189/1000 | Loss: 0.00002365
Iteration 190/1000 | Loss: 0.00002339
Iteration 191/1000 | Loss: 0.00015192
Iteration 192/1000 | Loss: 0.00036727
Iteration 193/1000 | Loss: 0.00016457
Iteration 194/1000 | Loss: 0.00011066
Iteration 195/1000 | Loss: 0.00005440
Iteration 196/1000 | Loss: 0.00004253
Iteration 197/1000 | Loss: 0.00002723
Iteration 198/1000 | Loss: 0.00002322
Iteration 199/1000 | Loss: 0.00002309
Iteration 200/1000 | Loss: 0.00002304
Iteration 201/1000 | Loss: 0.00002298
Iteration 202/1000 | Loss: 0.00002294
Iteration 203/1000 | Loss: 0.00002287
Iteration 204/1000 | Loss: 0.00002285
Iteration 205/1000 | Loss: 0.00002283
Iteration 206/1000 | Loss: 0.00002282
Iteration 207/1000 | Loss: 0.00002282
Iteration 208/1000 | Loss: 0.00002281
Iteration 209/1000 | Loss: 0.00002280
Iteration 210/1000 | Loss: 0.00002280
Iteration 211/1000 | Loss: 0.00002280
Iteration 212/1000 | Loss: 0.00002280
Iteration 213/1000 | Loss: 0.00002279
Iteration 214/1000 | Loss: 0.00002279
Iteration 215/1000 | Loss: 0.00002278
Iteration 216/1000 | Loss: 0.00002278
Iteration 217/1000 | Loss: 0.00002278
Iteration 218/1000 | Loss: 0.00002277
Iteration 219/1000 | Loss: 0.00002276
Iteration 220/1000 | Loss: 0.00011476
Iteration 221/1000 | Loss: 0.00012968
Iteration 222/1000 | Loss: 0.00009133
Iteration 223/1000 | Loss: 0.00017939
Iteration 224/1000 | Loss: 0.00006833
Iteration 225/1000 | Loss: 0.00011568
Iteration 226/1000 | Loss: 0.00002358
Iteration 227/1000 | Loss: 0.00010634
Iteration 228/1000 | Loss: 0.00010276
Iteration 229/1000 | Loss: 0.00039952
Iteration 230/1000 | Loss: 0.00002750
Iteration 231/1000 | Loss: 0.00002538
Iteration 232/1000 | Loss: 0.00002455
Iteration 233/1000 | Loss: 0.00002412
Iteration 234/1000 | Loss: 0.00002389
Iteration 235/1000 | Loss: 0.00002373
Iteration 236/1000 | Loss: 0.00002372
Iteration 237/1000 | Loss: 0.00002371
Iteration 238/1000 | Loss: 0.00002370
Iteration 239/1000 | Loss: 0.00002369
Iteration 240/1000 | Loss: 0.00002369
Iteration 241/1000 | Loss: 0.00002369
Iteration 242/1000 | Loss: 0.00002368
Iteration 243/1000 | Loss: 0.00002368
Iteration 244/1000 | Loss: 0.00002368
Iteration 245/1000 | Loss: 0.00002368
Iteration 246/1000 | Loss: 0.00002368
Iteration 247/1000 | Loss: 0.00002368
Iteration 248/1000 | Loss: 0.00002367
Iteration 249/1000 | Loss: 0.00002367
Iteration 250/1000 | Loss: 0.00002367
Iteration 251/1000 | Loss: 0.00002367
Iteration 252/1000 | Loss: 0.00002366
Iteration 253/1000 | Loss: 0.00002366
Iteration 254/1000 | Loss: 0.00002365
Iteration 255/1000 | Loss: 0.00017965
Iteration 256/1000 | Loss: 0.00036657
Iteration 257/1000 | Loss: 0.00006045
Iteration 258/1000 | Loss: 0.00020565
Iteration 259/1000 | Loss: 0.00034337
Iteration 260/1000 | Loss: 0.00024455
Iteration 261/1000 | Loss: 0.00021462
Iteration 262/1000 | Loss: 0.00017597
Iteration 263/1000 | Loss: 0.00019495
Iteration 264/1000 | Loss: 0.00012122
Iteration 265/1000 | Loss: 0.00016923
Iteration 266/1000 | Loss: 0.00011393
Iteration 267/1000 | Loss: 0.00017606
Iteration 268/1000 | Loss: 0.00010622
Iteration 269/1000 | Loss: 0.00017346
Iteration 270/1000 | Loss: 0.00011051
Iteration 271/1000 | Loss: 0.00004064
Iteration 272/1000 | Loss: 0.00002498
Iteration 273/1000 | Loss: 0.00007017
Iteration 274/1000 | Loss: 0.00005425
Iteration 275/1000 | Loss: 0.00006849
Iteration 276/1000 | Loss: 0.00005691
Iteration 277/1000 | Loss: 0.00008105
Iteration 278/1000 | Loss: 0.00006986
Iteration 279/1000 | Loss: 0.00005965
Iteration 280/1000 | Loss: 0.00006958
Iteration 281/1000 | Loss: 0.00005791
Iteration 282/1000 | Loss: 0.00003521
Iteration 283/1000 | Loss: 0.00008157
Iteration 284/1000 | Loss: 0.00005973
Iteration 285/1000 | Loss: 0.00002535
Iteration 286/1000 | Loss: 0.00011355
Iteration 287/1000 | Loss: 0.00003437
Iteration 288/1000 | Loss: 0.00002771
Iteration 289/1000 | Loss: 0.00003321
Iteration 290/1000 | Loss: 0.00002299
Iteration 291/1000 | Loss: 0.00020683
Iteration 292/1000 | Loss: 0.00004669
Iteration 293/1000 | Loss: 0.00002278
Iteration 294/1000 | Loss: 0.00014710
Iteration 295/1000 | Loss: 0.00008074
Iteration 296/1000 | Loss: 0.00007720
Iteration 297/1000 | Loss: 0.00002594
Iteration 298/1000 | Loss: 0.00006133
Iteration 299/1000 | Loss: 0.00022914
Iteration 300/1000 | Loss: 0.00003932
Iteration 301/1000 | Loss: 0.00002479
Iteration 302/1000 | Loss: 0.00002340
Iteration 303/1000 | Loss: 0.00002284
Iteration 304/1000 | Loss: 0.00002281
Iteration 305/1000 | Loss: 0.00002263
Iteration 306/1000 | Loss: 0.00002242
Iteration 307/1000 | Loss: 0.00002225
Iteration 308/1000 | Loss: 0.00002206
Iteration 309/1000 | Loss: 0.00014337
Iteration 310/1000 | Loss: 0.00002240
Iteration 311/1000 | Loss: 0.00002187
Iteration 312/1000 | Loss: 0.00002186
Iteration 313/1000 | Loss: 0.00002186
Iteration 314/1000 | Loss: 0.00002186
Iteration 315/1000 | Loss: 0.00002186
Iteration 316/1000 | Loss: 0.00002186
Iteration 317/1000 | Loss: 0.00002186
Iteration 318/1000 | Loss: 0.00002185
Iteration 319/1000 | Loss: 0.00002185
Iteration 320/1000 | Loss: 0.00002185
Iteration 321/1000 | Loss: 0.00002185
Iteration 322/1000 | Loss: 0.00002185
Iteration 323/1000 | Loss: 0.00002185
Iteration 324/1000 | Loss: 0.00002185
Iteration 325/1000 | Loss: 0.00002185
Iteration 326/1000 | Loss: 0.00002185
Iteration 327/1000 | Loss: 0.00002184
Iteration 328/1000 | Loss: 0.00002184
Iteration 329/1000 | Loss: 0.00002184
Iteration 330/1000 | Loss: 0.00002184
Iteration 331/1000 | Loss: 0.00002184
Iteration 332/1000 | Loss: 0.00002184
Iteration 333/1000 | Loss: 0.00002184
Iteration 334/1000 | Loss: 0.00002184
Iteration 335/1000 | Loss: 0.00002184
Iteration 336/1000 | Loss: 0.00002184
Iteration 337/1000 | Loss: 0.00002184
Iteration 338/1000 | Loss: 0.00002183
Iteration 339/1000 | Loss: 0.00002181
Iteration 340/1000 | Loss: 0.00002180
Iteration 341/1000 | Loss: 0.00002179
Iteration 342/1000 | Loss: 0.00002179
Iteration 343/1000 | Loss: 0.00002178
Iteration 344/1000 | Loss: 0.00002177
Iteration 345/1000 | Loss: 0.00002177
Iteration 346/1000 | Loss: 0.00002176
Iteration 347/1000 | Loss: 0.00002176
Iteration 348/1000 | Loss: 0.00002175
Iteration 349/1000 | Loss: 0.00002175
Iteration 350/1000 | Loss: 0.00002175
Iteration 351/1000 | Loss: 0.00002175
Iteration 352/1000 | Loss: 0.00002175
Iteration 353/1000 | Loss: 0.00002174
Iteration 354/1000 | Loss: 0.00002174
Iteration 355/1000 | Loss: 0.00002174
Iteration 356/1000 | Loss: 0.00002174
Iteration 357/1000 | Loss: 0.00002174
Iteration 358/1000 | Loss: 0.00002173
Iteration 359/1000 | Loss: 0.00002173
Iteration 360/1000 | Loss: 0.00002173
Iteration 361/1000 | Loss: 0.00002171
Iteration 362/1000 | Loss: 0.00002171
Iteration 363/1000 | Loss: 0.00002171
Iteration 364/1000 | Loss: 0.00002171
Iteration 365/1000 | Loss: 0.00002171
Iteration 366/1000 | Loss: 0.00002171
Iteration 367/1000 | Loss: 0.00002171
Iteration 368/1000 | Loss: 0.00002171
Iteration 369/1000 | Loss: 0.00002171
Iteration 370/1000 | Loss: 0.00002171
Iteration 371/1000 | Loss: 0.00002171
Iteration 372/1000 | Loss: 0.00002171
Iteration 373/1000 | Loss: 0.00002171
Iteration 374/1000 | Loss: 0.00002171
Iteration 375/1000 | Loss: 0.00002171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 375. Stopping optimization.
Last 5 losses: [2.170555671909824e-05, 2.170555671909824e-05, 2.170555671909824e-05, 2.170555671909824e-05, 2.170555671909824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.170555671909824e-05

Optimization complete. Final v2v error: 3.8689544200897217 mm

Highest mean error: 6.362090110778809 mm for frame 9

Lowest mean error: 2.9245433807373047 mm for frame 60

Saving results

Total time: 425.5430648326874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830046
Iteration 2/25 | Loss: 0.00134394
Iteration 3/25 | Loss: 0.00119076
Iteration 4/25 | Loss: 0.00117783
Iteration 5/25 | Loss: 0.00117463
Iteration 6/25 | Loss: 0.00117387
Iteration 7/25 | Loss: 0.00117387
Iteration 8/25 | Loss: 0.00117387
Iteration 9/25 | Loss: 0.00117387
Iteration 10/25 | Loss: 0.00117387
Iteration 11/25 | Loss: 0.00117387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011738695902749896, 0.0011738695902749896, 0.0011738695902749896, 0.0011738695902749896, 0.0011738695902749896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011738695902749896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31462765
Iteration 2/25 | Loss: 0.00080085
Iteration 3/25 | Loss: 0.00080079
Iteration 4/25 | Loss: 0.00080079
Iteration 5/25 | Loss: 0.00080079
Iteration 6/25 | Loss: 0.00080079
Iteration 7/25 | Loss: 0.00080079
Iteration 8/25 | Loss: 0.00080079
Iteration 9/25 | Loss: 0.00080079
Iteration 10/25 | Loss: 0.00080079
Iteration 11/25 | Loss: 0.00080079
Iteration 12/25 | Loss: 0.00080079
Iteration 13/25 | Loss: 0.00080079
Iteration 14/25 | Loss: 0.00080079
Iteration 15/25 | Loss: 0.00080079
Iteration 16/25 | Loss: 0.00080079
Iteration 17/25 | Loss: 0.00080079
Iteration 18/25 | Loss: 0.00080079
Iteration 19/25 | Loss: 0.00080079
Iteration 20/25 | Loss: 0.00080079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008007881697267294, 0.0008007881697267294, 0.0008007881697267294, 0.0008007881697267294, 0.0008007881697267294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008007881697267294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080079
Iteration 2/1000 | Loss: 0.00003710
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001922
Iteration 5/1000 | Loss: 0.00001829
Iteration 6/1000 | Loss: 0.00001751
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001599
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001578
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001568
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001538
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001531
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001530
Iteration 70/1000 | Loss: 0.00001529
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001529
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001528
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001526
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001525
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001518
Iteration 113/1000 | Loss: 0.00001518
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001514
Iteration 126/1000 | Loss: 0.00001514
Iteration 127/1000 | Loss: 0.00001514
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001513
Iteration 131/1000 | Loss: 0.00001513
Iteration 132/1000 | Loss: 0.00001513
Iteration 133/1000 | Loss: 0.00001513
Iteration 134/1000 | Loss: 0.00001513
Iteration 135/1000 | Loss: 0.00001513
Iteration 136/1000 | Loss: 0.00001512
Iteration 137/1000 | Loss: 0.00001512
Iteration 138/1000 | Loss: 0.00001512
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001510
Iteration 143/1000 | Loss: 0.00001510
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001509
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001508
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001507
Iteration 155/1000 | Loss: 0.00001507
Iteration 156/1000 | Loss: 0.00001507
Iteration 157/1000 | Loss: 0.00001506
Iteration 158/1000 | Loss: 0.00001506
Iteration 159/1000 | Loss: 0.00001506
Iteration 160/1000 | Loss: 0.00001506
Iteration 161/1000 | Loss: 0.00001506
Iteration 162/1000 | Loss: 0.00001506
Iteration 163/1000 | Loss: 0.00001506
Iteration 164/1000 | Loss: 0.00001506
Iteration 165/1000 | Loss: 0.00001506
Iteration 166/1000 | Loss: 0.00001505
Iteration 167/1000 | Loss: 0.00001505
Iteration 168/1000 | Loss: 0.00001505
Iteration 169/1000 | Loss: 0.00001505
Iteration 170/1000 | Loss: 0.00001504
Iteration 171/1000 | Loss: 0.00001504
Iteration 172/1000 | Loss: 0.00001504
Iteration 173/1000 | Loss: 0.00001504
Iteration 174/1000 | Loss: 0.00001504
Iteration 175/1000 | Loss: 0.00001504
Iteration 176/1000 | Loss: 0.00001504
Iteration 177/1000 | Loss: 0.00001504
Iteration 178/1000 | Loss: 0.00001504
Iteration 179/1000 | Loss: 0.00001504
Iteration 180/1000 | Loss: 0.00001504
Iteration 181/1000 | Loss: 0.00001504
Iteration 182/1000 | Loss: 0.00001503
Iteration 183/1000 | Loss: 0.00001503
Iteration 184/1000 | Loss: 0.00001503
Iteration 185/1000 | Loss: 0.00001503
Iteration 186/1000 | Loss: 0.00001503
Iteration 187/1000 | Loss: 0.00001503
Iteration 188/1000 | Loss: 0.00001503
Iteration 189/1000 | Loss: 0.00001502
Iteration 190/1000 | Loss: 0.00001502
Iteration 191/1000 | Loss: 0.00001502
Iteration 192/1000 | Loss: 0.00001502
Iteration 193/1000 | Loss: 0.00001502
Iteration 194/1000 | Loss: 0.00001502
Iteration 195/1000 | Loss: 0.00001502
Iteration 196/1000 | Loss: 0.00001502
Iteration 197/1000 | Loss: 0.00001502
Iteration 198/1000 | Loss: 0.00001502
Iteration 199/1000 | Loss: 0.00001502
Iteration 200/1000 | Loss: 0.00001502
Iteration 201/1000 | Loss: 0.00001502
Iteration 202/1000 | Loss: 0.00001502
Iteration 203/1000 | Loss: 0.00001502
Iteration 204/1000 | Loss: 0.00001501
Iteration 205/1000 | Loss: 0.00001501
Iteration 206/1000 | Loss: 0.00001501
Iteration 207/1000 | Loss: 0.00001501
Iteration 208/1000 | Loss: 0.00001501
Iteration 209/1000 | Loss: 0.00001501
Iteration 210/1000 | Loss: 0.00001501
Iteration 211/1000 | Loss: 0.00001501
Iteration 212/1000 | Loss: 0.00001501
Iteration 213/1000 | Loss: 0.00001501
Iteration 214/1000 | Loss: 0.00001501
Iteration 215/1000 | Loss: 0.00001501
Iteration 216/1000 | Loss: 0.00001501
Iteration 217/1000 | Loss: 0.00001501
Iteration 218/1000 | Loss: 0.00001501
Iteration 219/1000 | Loss: 0.00001501
Iteration 220/1000 | Loss: 0.00001501
Iteration 221/1000 | Loss: 0.00001501
Iteration 222/1000 | Loss: 0.00001501
Iteration 223/1000 | Loss: 0.00001501
Iteration 224/1000 | Loss: 0.00001501
Iteration 225/1000 | Loss: 0.00001501
Iteration 226/1000 | Loss: 0.00001501
Iteration 227/1000 | Loss: 0.00001501
Iteration 228/1000 | Loss: 0.00001501
Iteration 229/1000 | Loss: 0.00001501
Iteration 230/1000 | Loss: 0.00001501
Iteration 231/1000 | Loss: 0.00001501
Iteration 232/1000 | Loss: 0.00001501
Iteration 233/1000 | Loss: 0.00001501
Iteration 234/1000 | Loss: 0.00001501
Iteration 235/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.5014685232017655e-05, 1.5014685232017655e-05, 1.5014685232017655e-05, 1.5014685232017655e-05, 1.5014685232017655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5014685232017655e-05

Optimization complete. Final v2v error: 3.2715976238250732 mm

Highest mean error: 4.0642571449279785 mm for frame 2

Lowest mean error: 2.8384783267974854 mm for frame 186

Saving results

Total time: 50.097232818603516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415795
Iteration 2/25 | Loss: 0.00116097
Iteration 3/25 | Loss: 0.00109368
Iteration 4/25 | Loss: 0.00108272
Iteration 5/25 | Loss: 0.00107870
Iteration 6/25 | Loss: 0.00107772
Iteration 7/25 | Loss: 0.00107738
Iteration 8/25 | Loss: 0.00107730
Iteration 9/25 | Loss: 0.00107730
Iteration 10/25 | Loss: 0.00107730
Iteration 11/25 | Loss: 0.00107730
Iteration 12/25 | Loss: 0.00107730
Iteration 13/25 | Loss: 0.00107730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010773026151582599, 0.0010773026151582599, 0.0010773026151582599, 0.0010773026151582599, 0.0010773026151582599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010773026151582599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69772983
Iteration 2/25 | Loss: 0.00089235
Iteration 3/25 | Loss: 0.00089235
Iteration 4/25 | Loss: 0.00089235
Iteration 5/25 | Loss: 0.00089234
Iteration 6/25 | Loss: 0.00089234
Iteration 7/25 | Loss: 0.00089234
Iteration 8/25 | Loss: 0.00089234
Iteration 9/25 | Loss: 0.00089234
Iteration 10/25 | Loss: 0.00089234
Iteration 11/25 | Loss: 0.00089234
Iteration 12/25 | Loss: 0.00089234
Iteration 13/25 | Loss: 0.00089234
Iteration 14/25 | Loss: 0.00089234
Iteration 15/25 | Loss: 0.00089234
Iteration 16/25 | Loss: 0.00089234
Iteration 17/25 | Loss: 0.00089234
Iteration 18/25 | Loss: 0.00089234
Iteration 19/25 | Loss: 0.00089234
Iteration 20/25 | Loss: 0.00089234
Iteration 21/25 | Loss: 0.00089234
Iteration 22/25 | Loss: 0.00089234
Iteration 23/25 | Loss: 0.00089234
Iteration 24/25 | Loss: 0.00089234
Iteration 25/25 | Loss: 0.00089234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089234
Iteration 2/1000 | Loss: 0.00002803
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001407
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001231
Iteration 7/1000 | Loss: 0.00001187
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001140
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001111
Iteration 12/1000 | Loss: 0.00001094
Iteration 13/1000 | Loss: 0.00001090
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001086
Iteration 16/1000 | Loss: 0.00001085
Iteration 17/1000 | Loss: 0.00001084
Iteration 18/1000 | Loss: 0.00001082
Iteration 19/1000 | Loss: 0.00001081
Iteration 20/1000 | Loss: 0.00001072
Iteration 21/1000 | Loss: 0.00001069
Iteration 22/1000 | Loss: 0.00001068
Iteration 23/1000 | Loss: 0.00001064
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001060
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001060
Iteration 28/1000 | Loss: 0.00001059
Iteration 29/1000 | Loss: 0.00001059
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001059
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001056
Iteration 35/1000 | Loss: 0.00001056
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001055
Iteration 38/1000 | Loss: 0.00001055
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001054
Iteration 41/1000 | Loss: 0.00001054
Iteration 42/1000 | Loss: 0.00001054
Iteration 43/1000 | Loss: 0.00001053
Iteration 44/1000 | Loss: 0.00001053
Iteration 45/1000 | Loss: 0.00001053
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001052
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001052
Iteration 53/1000 | Loss: 0.00001052
Iteration 54/1000 | Loss: 0.00001051
Iteration 55/1000 | Loss: 0.00001051
Iteration 56/1000 | Loss: 0.00001051
Iteration 57/1000 | Loss: 0.00001051
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001050
Iteration 60/1000 | Loss: 0.00001050
Iteration 61/1000 | Loss: 0.00001050
Iteration 62/1000 | Loss: 0.00001049
Iteration 63/1000 | Loss: 0.00001049
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001049
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001048
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001045
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001045
Iteration 89/1000 | Loss: 0.00001045
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001044
Iteration 92/1000 | Loss: 0.00001044
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001043
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001042
Iteration 102/1000 | Loss: 0.00001042
Iteration 103/1000 | Loss: 0.00001042
Iteration 104/1000 | Loss: 0.00001042
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001041
Iteration 111/1000 | Loss: 0.00001041
Iteration 112/1000 | Loss: 0.00001041
Iteration 113/1000 | Loss: 0.00001041
Iteration 114/1000 | Loss: 0.00001041
Iteration 115/1000 | Loss: 0.00001041
Iteration 116/1000 | Loss: 0.00001041
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001040
Iteration 119/1000 | Loss: 0.00001040
Iteration 120/1000 | Loss: 0.00001040
Iteration 121/1000 | Loss: 0.00001039
Iteration 122/1000 | Loss: 0.00001039
Iteration 123/1000 | Loss: 0.00001039
Iteration 124/1000 | Loss: 0.00001039
Iteration 125/1000 | Loss: 0.00001038
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001038
Iteration 132/1000 | Loss: 0.00001038
Iteration 133/1000 | Loss: 0.00001038
Iteration 134/1000 | Loss: 0.00001038
Iteration 135/1000 | Loss: 0.00001038
Iteration 136/1000 | Loss: 0.00001035
Iteration 137/1000 | Loss: 0.00001035
Iteration 138/1000 | Loss: 0.00001035
Iteration 139/1000 | Loss: 0.00001034
Iteration 140/1000 | Loss: 0.00001034
Iteration 141/1000 | Loss: 0.00001033
Iteration 142/1000 | Loss: 0.00001033
Iteration 143/1000 | Loss: 0.00001033
Iteration 144/1000 | Loss: 0.00001032
Iteration 145/1000 | Loss: 0.00001032
Iteration 146/1000 | Loss: 0.00001032
Iteration 147/1000 | Loss: 0.00001032
Iteration 148/1000 | Loss: 0.00001031
Iteration 149/1000 | Loss: 0.00001031
Iteration 150/1000 | Loss: 0.00001031
Iteration 151/1000 | Loss: 0.00001031
Iteration 152/1000 | Loss: 0.00001031
Iteration 153/1000 | Loss: 0.00001031
Iteration 154/1000 | Loss: 0.00001031
Iteration 155/1000 | Loss: 0.00001031
Iteration 156/1000 | Loss: 0.00001031
Iteration 157/1000 | Loss: 0.00001030
Iteration 158/1000 | Loss: 0.00001030
Iteration 159/1000 | Loss: 0.00001030
Iteration 160/1000 | Loss: 0.00001030
Iteration 161/1000 | Loss: 0.00001029
Iteration 162/1000 | Loss: 0.00001029
Iteration 163/1000 | Loss: 0.00001029
Iteration 164/1000 | Loss: 0.00001029
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Iteration 167/1000 | Loss: 0.00001028
Iteration 168/1000 | Loss: 0.00001028
Iteration 169/1000 | Loss: 0.00001028
Iteration 170/1000 | Loss: 0.00001028
Iteration 171/1000 | Loss: 0.00001028
Iteration 172/1000 | Loss: 0.00001028
Iteration 173/1000 | Loss: 0.00001028
Iteration 174/1000 | Loss: 0.00001028
Iteration 175/1000 | Loss: 0.00001028
Iteration 176/1000 | Loss: 0.00001028
Iteration 177/1000 | Loss: 0.00001028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.028096357913455e-05, 1.028096357913455e-05, 1.028096357913455e-05, 1.028096357913455e-05, 1.028096357913455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.028096357913455e-05

Optimization complete. Final v2v error: 2.7413547039031982 mm

Highest mean error: 3.678976535797119 mm for frame 74

Lowest mean error: 2.432718276977539 mm for frame 96

Saving results

Total time: 42.306846141815186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739630
Iteration 2/25 | Loss: 0.00157796
Iteration 3/25 | Loss: 0.00127951
Iteration 4/25 | Loss: 0.00122255
Iteration 5/25 | Loss: 0.00121099
Iteration 6/25 | Loss: 0.00120844
Iteration 7/25 | Loss: 0.00120708
Iteration 8/25 | Loss: 0.00120681
Iteration 9/25 | Loss: 0.00120670
Iteration 10/25 | Loss: 0.00120668
Iteration 11/25 | Loss: 0.00120668
Iteration 12/25 | Loss: 0.00120668
Iteration 13/25 | Loss: 0.00120668
Iteration 14/25 | Loss: 0.00120668
Iteration 15/25 | Loss: 0.00120668
Iteration 16/25 | Loss: 0.00120667
Iteration 17/25 | Loss: 0.00120667
Iteration 18/25 | Loss: 0.00120667
Iteration 19/25 | Loss: 0.00120667
Iteration 20/25 | Loss: 0.00120667
Iteration 21/25 | Loss: 0.00120667
Iteration 22/25 | Loss: 0.00120667
Iteration 23/25 | Loss: 0.00120667
Iteration 24/25 | Loss: 0.00120667
Iteration 25/25 | Loss: 0.00120667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28786922
Iteration 2/25 | Loss: 0.00110080
Iteration 3/25 | Loss: 0.00110079
Iteration 4/25 | Loss: 0.00110079
Iteration 5/25 | Loss: 0.00110079
Iteration 6/25 | Loss: 0.00110079
Iteration 7/25 | Loss: 0.00110079
Iteration 8/25 | Loss: 0.00110079
Iteration 9/25 | Loss: 0.00110079
Iteration 10/25 | Loss: 0.00110079
Iteration 11/25 | Loss: 0.00110079
Iteration 12/25 | Loss: 0.00110079
Iteration 13/25 | Loss: 0.00110078
Iteration 14/25 | Loss: 0.00110078
Iteration 15/25 | Loss: 0.00110078
Iteration 16/25 | Loss: 0.00110078
Iteration 17/25 | Loss: 0.00110078
Iteration 18/25 | Loss: 0.00110078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011007849825546145, 0.0011007849825546145, 0.0011007849825546145, 0.0011007849825546145, 0.0011007849825546145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011007849825546145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110078
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00002331
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002089
Iteration 6/1000 | Loss: 0.00004341
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00002002
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001947
Iteration 13/1000 | Loss: 0.00001944
Iteration 14/1000 | Loss: 0.00001933
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001932
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001925
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001917
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001915
Iteration 27/1000 | Loss: 0.00001912
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001907
Iteration 35/1000 | Loss: 0.00001902
Iteration 36/1000 | Loss: 0.00001902
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00001899
Iteration 39/1000 | Loss: 0.00001899
Iteration 40/1000 | Loss: 0.00001898
Iteration 41/1000 | Loss: 0.00001898
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001894
Iteration 50/1000 | Loss: 0.00001894
Iteration 51/1000 | Loss: 0.00001893
Iteration 52/1000 | Loss: 0.00001893
Iteration 53/1000 | Loss: 0.00001893
Iteration 54/1000 | Loss: 0.00001892
Iteration 55/1000 | Loss: 0.00001892
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001879
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001877
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001877
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001876
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001875
Iteration 122/1000 | Loss: 0.00001875
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001874
Iteration 125/1000 | Loss: 0.00001874
Iteration 126/1000 | Loss: 0.00001874
Iteration 127/1000 | Loss: 0.00001873
Iteration 128/1000 | Loss: 0.00001873
Iteration 129/1000 | Loss: 0.00001873
Iteration 130/1000 | Loss: 0.00001873
Iteration 131/1000 | Loss: 0.00001873
Iteration 132/1000 | Loss: 0.00001872
Iteration 133/1000 | Loss: 0.00001872
Iteration 134/1000 | Loss: 0.00001872
Iteration 135/1000 | Loss: 0.00001872
Iteration 136/1000 | Loss: 0.00001872
Iteration 137/1000 | Loss: 0.00001872
Iteration 138/1000 | Loss: 0.00001872
Iteration 139/1000 | Loss: 0.00001872
Iteration 140/1000 | Loss: 0.00001872
Iteration 141/1000 | Loss: 0.00001872
Iteration 142/1000 | Loss: 0.00001872
Iteration 143/1000 | Loss: 0.00001872
Iteration 144/1000 | Loss: 0.00001872
Iteration 145/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.8716256818152033e-05, 1.8716256818152033e-05, 1.8716256818152033e-05, 1.8716256818152033e-05, 1.8716256818152033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8716256818152033e-05

Optimization complete. Final v2v error: 3.6016886234283447 mm

Highest mean error: 4.004386901855469 mm for frame 212

Lowest mean error: 2.9452152252197266 mm for frame 144

Saving results

Total time: 50.8650689125061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830263
Iteration 2/25 | Loss: 0.00116845
Iteration 3/25 | Loss: 0.00108201
Iteration 4/25 | Loss: 0.00107160
Iteration 5/25 | Loss: 0.00106793
Iteration 6/25 | Loss: 0.00106707
Iteration 7/25 | Loss: 0.00106707
Iteration 8/25 | Loss: 0.00106707
Iteration 9/25 | Loss: 0.00106707
Iteration 10/25 | Loss: 0.00106707
Iteration 11/25 | Loss: 0.00106707
Iteration 12/25 | Loss: 0.00106707
Iteration 13/25 | Loss: 0.00106707
Iteration 14/25 | Loss: 0.00106707
Iteration 15/25 | Loss: 0.00106707
Iteration 16/25 | Loss: 0.00106707
Iteration 17/25 | Loss: 0.00106707
Iteration 18/25 | Loss: 0.00106707
Iteration 19/25 | Loss: 0.00106707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010670693591237068, 0.0010670693591237068, 0.0010670693591237068, 0.0010670693591237068, 0.0010670693591237068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670693591237068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55659568
Iteration 2/25 | Loss: 0.00088796
Iteration 3/25 | Loss: 0.00088796
Iteration 4/25 | Loss: 0.00088796
Iteration 5/25 | Loss: 0.00088796
Iteration 6/25 | Loss: 0.00088796
Iteration 7/25 | Loss: 0.00088795
Iteration 8/25 | Loss: 0.00088795
Iteration 9/25 | Loss: 0.00088795
Iteration 10/25 | Loss: 0.00088795
Iteration 11/25 | Loss: 0.00088795
Iteration 12/25 | Loss: 0.00088795
Iteration 13/25 | Loss: 0.00088795
Iteration 14/25 | Loss: 0.00088795
Iteration 15/25 | Loss: 0.00088795
Iteration 16/25 | Loss: 0.00088795
Iteration 17/25 | Loss: 0.00088795
Iteration 18/25 | Loss: 0.00088795
Iteration 19/25 | Loss: 0.00088795
Iteration 20/25 | Loss: 0.00088795
Iteration 21/25 | Loss: 0.00088795
Iteration 22/25 | Loss: 0.00088795
Iteration 23/25 | Loss: 0.00088795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008879535016603768, 0.0008879535016603768, 0.0008879535016603768, 0.0008879535016603768, 0.0008879535016603768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008879535016603768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088795
Iteration 2/1000 | Loss: 0.00001724
Iteration 3/1000 | Loss: 0.00001157
Iteration 4/1000 | Loss: 0.00001044
Iteration 5/1000 | Loss: 0.00000999
Iteration 6/1000 | Loss: 0.00000954
Iteration 7/1000 | Loss: 0.00000922
Iteration 8/1000 | Loss: 0.00000911
Iteration 9/1000 | Loss: 0.00000904
Iteration 10/1000 | Loss: 0.00000902
Iteration 11/1000 | Loss: 0.00000885
Iteration 12/1000 | Loss: 0.00000885
Iteration 13/1000 | Loss: 0.00000875
Iteration 14/1000 | Loss: 0.00000864
Iteration 15/1000 | Loss: 0.00000861
Iteration 16/1000 | Loss: 0.00000860
Iteration 17/1000 | Loss: 0.00000860
Iteration 18/1000 | Loss: 0.00000859
Iteration 19/1000 | Loss: 0.00000859
Iteration 20/1000 | Loss: 0.00000859
Iteration 21/1000 | Loss: 0.00000858
Iteration 22/1000 | Loss: 0.00000854
Iteration 23/1000 | Loss: 0.00000853
Iteration 24/1000 | Loss: 0.00000851
Iteration 25/1000 | Loss: 0.00000851
Iteration 26/1000 | Loss: 0.00000847
Iteration 27/1000 | Loss: 0.00000847
Iteration 28/1000 | Loss: 0.00000847
Iteration 29/1000 | Loss: 0.00000847
Iteration 30/1000 | Loss: 0.00000846
Iteration 31/1000 | Loss: 0.00000846
Iteration 32/1000 | Loss: 0.00000846
Iteration 33/1000 | Loss: 0.00000846
Iteration 34/1000 | Loss: 0.00000846
Iteration 35/1000 | Loss: 0.00000846
Iteration 36/1000 | Loss: 0.00000845
Iteration 37/1000 | Loss: 0.00000845
Iteration 38/1000 | Loss: 0.00000844
Iteration 39/1000 | Loss: 0.00000844
Iteration 40/1000 | Loss: 0.00000843
Iteration 41/1000 | Loss: 0.00000843
Iteration 42/1000 | Loss: 0.00000843
Iteration 43/1000 | Loss: 0.00000842
Iteration 44/1000 | Loss: 0.00000841
Iteration 45/1000 | Loss: 0.00000840
Iteration 46/1000 | Loss: 0.00000840
Iteration 47/1000 | Loss: 0.00000840
Iteration 48/1000 | Loss: 0.00000839
Iteration 49/1000 | Loss: 0.00000839
Iteration 50/1000 | Loss: 0.00000839
Iteration 51/1000 | Loss: 0.00000839
Iteration 52/1000 | Loss: 0.00000839
Iteration 53/1000 | Loss: 0.00000839
Iteration 54/1000 | Loss: 0.00000838
Iteration 55/1000 | Loss: 0.00000838
Iteration 56/1000 | Loss: 0.00000838
Iteration 57/1000 | Loss: 0.00000837
Iteration 58/1000 | Loss: 0.00000837
Iteration 59/1000 | Loss: 0.00000836
Iteration 60/1000 | Loss: 0.00000836
Iteration 61/1000 | Loss: 0.00000836
Iteration 62/1000 | Loss: 0.00000836
Iteration 63/1000 | Loss: 0.00000836
Iteration 64/1000 | Loss: 0.00000835
Iteration 65/1000 | Loss: 0.00000835
Iteration 66/1000 | Loss: 0.00000834
Iteration 67/1000 | Loss: 0.00000834
Iteration 68/1000 | Loss: 0.00000833
Iteration 69/1000 | Loss: 0.00000833
Iteration 70/1000 | Loss: 0.00000833
Iteration 71/1000 | Loss: 0.00000833
Iteration 72/1000 | Loss: 0.00000832
Iteration 73/1000 | Loss: 0.00000832
Iteration 74/1000 | Loss: 0.00000832
Iteration 75/1000 | Loss: 0.00000832
Iteration 76/1000 | Loss: 0.00000832
Iteration 77/1000 | Loss: 0.00000831
Iteration 78/1000 | Loss: 0.00000831
Iteration 79/1000 | Loss: 0.00000831
Iteration 80/1000 | Loss: 0.00000831
Iteration 81/1000 | Loss: 0.00000831
Iteration 82/1000 | Loss: 0.00000831
Iteration 83/1000 | Loss: 0.00000830
Iteration 84/1000 | Loss: 0.00000830
Iteration 85/1000 | Loss: 0.00000830
Iteration 86/1000 | Loss: 0.00000830
Iteration 87/1000 | Loss: 0.00000829
Iteration 88/1000 | Loss: 0.00000829
Iteration 89/1000 | Loss: 0.00000829
Iteration 90/1000 | Loss: 0.00000828
Iteration 91/1000 | Loss: 0.00000828
Iteration 92/1000 | Loss: 0.00000828
Iteration 93/1000 | Loss: 0.00000828
Iteration 94/1000 | Loss: 0.00000828
Iteration 95/1000 | Loss: 0.00000828
Iteration 96/1000 | Loss: 0.00000828
Iteration 97/1000 | Loss: 0.00000828
Iteration 98/1000 | Loss: 0.00000828
Iteration 99/1000 | Loss: 0.00000828
Iteration 100/1000 | Loss: 0.00000827
Iteration 101/1000 | Loss: 0.00000827
Iteration 102/1000 | Loss: 0.00000827
Iteration 103/1000 | Loss: 0.00000827
Iteration 104/1000 | Loss: 0.00000827
Iteration 105/1000 | Loss: 0.00000826
Iteration 106/1000 | Loss: 0.00000826
Iteration 107/1000 | Loss: 0.00000826
Iteration 108/1000 | Loss: 0.00000826
Iteration 109/1000 | Loss: 0.00000826
Iteration 110/1000 | Loss: 0.00000825
Iteration 111/1000 | Loss: 0.00000825
Iteration 112/1000 | Loss: 0.00000825
Iteration 113/1000 | Loss: 0.00000825
Iteration 114/1000 | Loss: 0.00000825
Iteration 115/1000 | Loss: 0.00000825
Iteration 116/1000 | Loss: 0.00000825
Iteration 117/1000 | Loss: 0.00000825
Iteration 118/1000 | Loss: 0.00000824
Iteration 119/1000 | Loss: 0.00000824
Iteration 120/1000 | Loss: 0.00000824
Iteration 121/1000 | Loss: 0.00000824
Iteration 122/1000 | Loss: 0.00000824
Iteration 123/1000 | Loss: 0.00000824
Iteration 124/1000 | Loss: 0.00000823
Iteration 125/1000 | Loss: 0.00000823
Iteration 126/1000 | Loss: 0.00000823
Iteration 127/1000 | Loss: 0.00000823
Iteration 128/1000 | Loss: 0.00000822
Iteration 129/1000 | Loss: 0.00000822
Iteration 130/1000 | Loss: 0.00000822
Iteration 131/1000 | Loss: 0.00000822
Iteration 132/1000 | Loss: 0.00000822
Iteration 133/1000 | Loss: 0.00000821
Iteration 134/1000 | Loss: 0.00000821
Iteration 135/1000 | Loss: 0.00000821
Iteration 136/1000 | Loss: 0.00000821
Iteration 137/1000 | Loss: 0.00000821
Iteration 138/1000 | Loss: 0.00000821
Iteration 139/1000 | Loss: 0.00000821
Iteration 140/1000 | Loss: 0.00000821
Iteration 141/1000 | Loss: 0.00000821
Iteration 142/1000 | Loss: 0.00000821
Iteration 143/1000 | Loss: 0.00000821
Iteration 144/1000 | Loss: 0.00000821
Iteration 145/1000 | Loss: 0.00000821
Iteration 146/1000 | Loss: 0.00000821
Iteration 147/1000 | Loss: 0.00000821
Iteration 148/1000 | Loss: 0.00000821
Iteration 149/1000 | Loss: 0.00000820
Iteration 150/1000 | Loss: 0.00000820
Iteration 151/1000 | Loss: 0.00000819
Iteration 152/1000 | Loss: 0.00000819
Iteration 153/1000 | Loss: 0.00000819
Iteration 154/1000 | Loss: 0.00000819
Iteration 155/1000 | Loss: 0.00000819
Iteration 156/1000 | Loss: 0.00000819
Iteration 157/1000 | Loss: 0.00000819
Iteration 158/1000 | Loss: 0.00000819
Iteration 159/1000 | Loss: 0.00000819
Iteration 160/1000 | Loss: 0.00000818
Iteration 161/1000 | Loss: 0.00000818
Iteration 162/1000 | Loss: 0.00000818
Iteration 163/1000 | Loss: 0.00000818
Iteration 164/1000 | Loss: 0.00000818
Iteration 165/1000 | Loss: 0.00000818
Iteration 166/1000 | Loss: 0.00000818
Iteration 167/1000 | Loss: 0.00000818
Iteration 168/1000 | Loss: 0.00000818
Iteration 169/1000 | Loss: 0.00000818
Iteration 170/1000 | Loss: 0.00000818
Iteration 171/1000 | Loss: 0.00000818
Iteration 172/1000 | Loss: 0.00000818
Iteration 173/1000 | Loss: 0.00000817
Iteration 174/1000 | Loss: 0.00000817
Iteration 175/1000 | Loss: 0.00000817
Iteration 176/1000 | Loss: 0.00000817
Iteration 177/1000 | Loss: 0.00000817
Iteration 178/1000 | Loss: 0.00000817
Iteration 179/1000 | Loss: 0.00000817
Iteration 180/1000 | Loss: 0.00000817
Iteration 181/1000 | Loss: 0.00000817
Iteration 182/1000 | Loss: 0.00000817
Iteration 183/1000 | Loss: 0.00000817
Iteration 184/1000 | Loss: 0.00000817
Iteration 185/1000 | Loss: 0.00000816
Iteration 186/1000 | Loss: 0.00000816
Iteration 187/1000 | Loss: 0.00000816
Iteration 188/1000 | Loss: 0.00000816
Iteration 189/1000 | Loss: 0.00000816
Iteration 190/1000 | Loss: 0.00000816
Iteration 191/1000 | Loss: 0.00000816
Iteration 192/1000 | Loss: 0.00000816
Iteration 193/1000 | Loss: 0.00000816
Iteration 194/1000 | Loss: 0.00000816
Iteration 195/1000 | Loss: 0.00000816
Iteration 196/1000 | Loss: 0.00000816
Iteration 197/1000 | Loss: 0.00000816
Iteration 198/1000 | Loss: 0.00000816
Iteration 199/1000 | Loss: 0.00000816
Iteration 200/1000 | Loss: 0.00000816
Iteration 201/1000 | Loss: 0.00000816
Iteration 202/1000 | Loss: 0.00000816
Iteration 203/1000 | Loss: 0.00000816
Iteration 204/1000 | Loss: 0.00000816
Iteration 205/1000 | Loss: 0.00000816
Iteration 206/1000 | Loss: 0.00000816
Iteration 207/1000 | Loss: 0.00000816
Iteration 208/1000 | Loss: 0.00000816
Iteration 209/1000 | Loss: 0.00000816
Iteration 210/1000 | Loss: 0.00000816
Iteration 211/1000 | Loss: 0.00000816
Iteration 212/1000 | Loss: 0.00000816
Iteration 213/1000 | Loss: 0.00000816
Iteration 214/1000 | Loss: 0.00000816
Iteration 215/1000 | Loss: 0.00000816
Iteration 216/1000 | Loss: 0.00000816
Iteration 217/1000 | Loss: 0.00000816
Iteration 218/1000 | Loss: 0.00000816
Iteration 219/1000 | Loss: 0.00000816
Iteration 220/1000 | Loss: 0.00000816
Iteration 221/1000 | Loss: 0.00000816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [8.157055162882898e-06, 8.157055162882898e-06, 8.157055162882898e-06, 8.157055162882898e-06, 8.157055162882898e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.157055162882898e-06

Optimization complete. Final v2v error: 2.466062545776367 mm

Highest mean error: 2.864293336868286 mm for frame 52

Lowest mean error: 2.318204879760742 mm for frame 111

Saving results

Total time: 37.23261594772339
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798116
Iteration 2/25 | Loss: 0.00156563
Iteration 3/25 | Loss: 0.00123579
Iteration 4/25 | Loss: 0.00120623
Iteration 5/25 | Loss: 0.00119767
Iteration 6/25 | Loss: 0.00119477
Iteration 7/25 | Loss: 0.00119435
Iteration 8/25 | Loss: 0.00119435
Iteration 9/25 | Loss: 0.00119435
Iteration 10/25 | Loss: 0.00119435
Iteration 11/25 | Loss: 0.00119435
Iteration 12/25 | Loss: 0.00119435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011943524004891515, 0.0011943524004891515, 0.0011943524004891515, 0.0011943524004891515, 0.0011943524004891515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011943524004891515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16264737
Iteration 2/25 | Loss: 0.00098069
Iteration 3/25 | Loss: 0.00098069
Iteration 4/25 | Loss: 0.00098068
Iteration 5/25 | Loss: 0.00098068
Iteration 6/25 | Loss: 0.00098068
Iteration 7/25 | Loss: 0.00098068
Iteration 8/25 | Loss: 0.00098068
Iteration 9/25 | Loss: 0.00098068
Iteration 10/25 | Loss: 0.00098068
Iteration 11/25 | Loss: 0.00098068
Iteration 12/25 | Loss: 0.00098068
Iteration 13/25 | Loss: 0.00098068
Iteration 14/25 | Loss: 0.00098068
Iteration 15/25 | Loss: 0.00098068
Iteration 16/25 | Loss: 0.00098068
Iteration 17/25 | Loss: 0.00098068
Iteration 18/25 | Loss: 0.00098068
Iteration 19/25 | Loss: 0.00098068
Iteration 20/25 | Loss: 0.00098068
Iteration 21/25 | Loss: 0.00098068
Iteration 22/25 | Loss: 0.00098068
Iteration 23/25 | Loss: 0.00098068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009806813905015588, 0.0009806813905015588, 0.0009806813905015588, 0.0009806813905015588, 0.0009806813905015588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009806813905015588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098068
Iteration 2/1000 | Loss: 0.00008015
Iteration 3/1000 | Loss: 0.00005144
Iteration 4/1000 | Loss: 0.00003916
Iteration 5/1000 | Loss: 0.00003512
Iteration 6/1000 | Loss: 0.00003355
Iteration 7/1000 | Loss: 0.00003199
Iteration 8/1000 | Loss: 0.00003128
Iteration 9/1000 | Loss: 0.00003047
Iteration 10/1000 | Loss: 0.00003012
Iteration 11/1000 | Loss: 0.00002971
Iteration 12/1000 | Loss: 0.00002931
Iteration 13/1000 | Loss: 0.00002899
Iteration 14/1000 | Loss: 0.00002881
Iteration 15/1000 | Loss: 0.00002859
Iteration 16/1000 | Loss: 0.00002856
Iteration 17/1000 | Loss: 0.00002850
Iteration 18/1000 | Loss: 0.00002839
Iteration 19/1000 | Loss: 0.00002839
Iteration 20/1000 | Loss: 0.00002836
Iteration 21/1000 | Loss: 0.00002828
Iteration 22/1000 | Loss: 0.00002824
Iteration 23/1000 | Loss: 0.00002819
Iteration 24/1000 | Loss: 0.00002818
Iteration 25/1000 | Loss: 0.00002817
Iteration 26/1000 | Loss: 0.00002813
Iteration 27/1000 | Loss: 0.00002813
Iteration 28/1000 | Loss: 0.00002810
Iteration 29/1000 | Loss: 0.00002809
Iteration 30/1000 | Loss: 0.00002809
Iteration 31/1000 | Loss: 0.00002808
Iteration 32/1000 | Loss: 0.00002805
Iteration 33/1000 | Loss: 0.00002804
Iteration 34/1000 | Loss: 0.00002804
Iteration 35/1000 | Loss: 0.00002802
Iteration 36/1000 | Loss: 0.00002802
Iteration 37/1000 | Loss: 0.00002802
Iteration 38/1000 | Loss: 0.00002802
Iteration 39/1000 | Loss: 0.00002801
Iteration 40/1000 | Loss: 0.00002801
Iteration 41/1000 | Loss: 0.00002801
Iteration 42/1000 | Loss: 0.00002801
Iteration 43/1000 | Loss: 0.00002801
Iteration 44/1000 | Loss: 0.00002801
Iteration 45/1000 | Loss: 0.00002801
Iteration 46/1000 | Loss: 0.00002801
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002801
Iteration 49/1000 | Loss: 0.00002801
Iteration 50/1000 | Loss: 0.00002800
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002800
Iteration 53/1000 | Loss: 0.00002800
Iteration 54/1000 | Loss: 0.00002800
Iteration 55/1000 | Loss: 0.00002800
Iteration 56/1000 | Loss: 0.00002800
Iteration 57/1000 | Loss: 0.00002800
Iteration 58/1000 | Loss: 0.00002800
Iteration 59/1000 | Loss: 0.00002800
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002799
Iteration 62/1000 | Loss: 0.00002799
Iteration 63/1000 | Loss: 0.00002799
Iteration 64/1000 | Loss: 0.00002799
Iteration 65/1000 | Loss: 0.00002799
Iteration 66/1000 | Loss: 0.00002799
Iteration 67/1000 | Loss: 0.00002799
Iteration 68/1000 | Loss: 0.00002799
Iteration 69/1000 | Loss: 0.00002799
Iteration 70/1000 | Loss: 0.00002798
Iteration 71/1000 | Loss: 0.00002797
Iteration 72/1000 | Loss: 0.00002797
Iteration 73/1000 | Loss: 0.00002796
Iteration 74/1000 | Loss: 0.00002796
Iteration 75/1000 | Loss: 0.00002795
Iteration 76/1000 | Loss: 0.00002794
Iteration 77/1000 | Loss: 0.00002794
Iteration 78/1000 | Loss: 0.00002793
Iteration 79/1000 | Loss: 0.00002792
Iteration 80/1000 | Loss: 0.00002792
Iteration 81/1000 | Loss: 0.00002792
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002791
Iteration 84/1000 | Loss: 0.00002791
Iteration 85/1000 | Loss: 0.00002790
Iteration 86/1000 | Loss: 0.00002790
Iteration 87/1000 | Loss: 0.00002790
Iteration 88/1000 | Loss: 0.00002790
Iteration 89/1000 | Loss: 0.00002790
Iteration 90/1000 | Loss: 0.00002790
Iteration 91/1000 | Loss: 0.00002790
Iteration 92/1000 | Loss: 0.00002790
Iteration 93/1000 | Loss: 0.00002790
Iteration 94/1000 | Loss: 0.00002790
Iteration 95/1000 | Loss: 0.00002790
Iteration 96/1000 | Loss: 0.00002790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.7895728635485284e-05, 2.7895728635485284e-05, 2.7895728635485284e-05, 2.7895728635485284e-05, 2.7895728635485284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7895728635485284e-05

Optimization complete. Final v2v error: 4.241074562072754 mm

Highest mean error: 5.1551618576049805 mm for frame 158

Lowest mean error: 2.7371561527252197 mm for frame 198

Saving results

Total time: 42.69525647163391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427474
Iteration 2/25 | Loss: 0.00121863
Iteration 3/25 | Loss: 0.00111967
Iteration 4/25 | Loss: 0.00110919
Iteration 5/25 | Loss: 0.00110706
Iteration 6/25 | Loss: 0.00110652
Iteration 7/25 | Loss: 0.00110635
Iteration 8/25 | Loss: 0.00110635
Iteration 9/25 | Loss: 0.00110635
Iteration 10/25 | Loss: 0.00110635
Iteration 11/25 | Loss: 0.00110635
Iteration 12/25 | Loss: 0.00110635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011063533602282405, 0.0011063533602282405, 0.0011063533602282405, 0.0011063533602282405, 0.0011063533602282405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011063533602282405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45678294
Iteration 2/25 | Loss: 0.00087274
Iteration 3/25 | Loss: 0.00087274
Iteration 4/25 | Loss: 0.00087274
Iteration 5/25 | Loss: 0.00087274
Iteration 6/25 | Loss: 0.00087274
Iteration 7/25 | Loss: 0.00087273
Iteration 8/25 | Loss: 0.00087273
Iteration 9/25 | Loss: 0.00087273
Iteration 10/25 | Loss: 0.00087273
Iteration 11/25 | Loss: 0.00087273
Iteration 12/25 | Loss: 0.00087273
Iteration 13/25 | Loss: 0.00087273
Iteration 14/25 | Loss: 0.00087273
Iteration 15/25 | Loss: 0.00087273
Iteration 16/25 | Loss: 0.00087273
Iteration 17/25 | Loss: 0.00087273
Iteration 18/25 | Loss: 0.00087273
Iteration 19/25 | Loss: 0.00087273
Iteration 20/25 | Loss: 0.00087273
Iteration 21/25 | Loss: 0.00087273
Iteration 22/25 | Loss: 0.00087273
Iteration 23/25 | Loss: 0.00087273
Iteration 24/25 | Loss: 0.00087273
Iteration 25/25 | Loss: 0.00087273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087273
Iteration 2/1000 | Loss: 0.00003429
Iteration 3/1000 | Loss: 0.00002228
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001432
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00001285
Iteration 44/1000 | Loss: 0.00001285
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001284
Iteration 48/1000 | Loss: 0.00001284
Iteration 49/1000 | Loss: 0.00001283
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001282
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001281
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001277
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001273
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001271
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001270
Iteration 88/1000 | Loss: 0.00001270
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001268
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001266
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001265
Iteration 114/1000 | Loss: 0.00001265
Iteration 115/1000 | Loss: 0.00001265
Iteration 116/1000 | Loss: 0.00001265
Iteration 117/1000 | Loss: 0.00001265
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001264
Iteration 122/1000 | Loss: 0.00001264
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001263
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001262
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001260
Iteration 142/1000 | Loss: 0.00001260
Iteration 143/1000 | Loss: 0.00001260
Iteration 144/1000 | Loss: 0.00001260
Iteration 145/1000 | Loss: 0.00001260
Iteration 146/1000 | Loss: 0.00001260
Iteration 147/1000 | Loss: 0.00001260
Iteration 148/1000 | Loss: 0.00001260
Iteration 149/1000 | Loss: 0.00001260
Iteration 150/1000 | Loss: 0.00001260
Iteration 151/1000 | Loss: 0.00001260
Iteration 152/1000 | Loss: 0.00001259
Iteration 153/1000 | Loss: 0.00001259
Iteration 154/1000 | Loss: 0.00001259
Iteration 155/1000 | Loss: 0.00001259
Iteration 156/1000 | Loss: 0.00001259
Iteration 157/1000 | Loss: 0.00001259
Iteration 158/1000 | Loss: 0.00001259
Iteration 159/1000 | Loss: 0.00001259
Iteration 160/1000 | Loss: 0.00001259
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001258
Iteration 163/1000 | Loss: 0.00001258
Iteration 164/1000 | Loss: 0.00001258
Iteration 165/1000 | Loss: 0.00001258
Iteration 166/1000 | Loss: 0.00001258
Iteration 167/1000 | Loss: 0.00001258
Iteration 168/1000 | Loss: 0.00001258
Iteration 169/1000 | Loss: 0.00001258
Iteration 170/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.2582616363943089e-05, 1.2582616363943089e-05, 1.2582616363943089e-05, 1.2582616363943089e-05, 1.2582616363943089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2582616363943089e-05

Optimization complete. Final v2v error: 2.99448561668396 mm

Highest mean error: 4.3767194747924805 mm for frame 47

Lowest mean error: 2.636953830718994 mm for frame 86

Saving results

Total time: 42.49402737617493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374986
Iteration 2/25 | Loss: 0.00129794
Iteration 3/25 | Loss: 0.00115025
Iteration 4/25 | Loss: 0.00111890
Iteration 5/25 | Loss: 0.00111131
Iteration 6/25 | Loss: 0.00110862
Iteration 7/25 | Loss: 0.00110820
Iteration 8/25 | Loss: 0.00110820
Iteration 9/25 | Loss: 0.00110820
Iteration 10/25 | Loss: 0.00110820
Iteration 11/25 | Loss: 0.00110820
Iteration 12/25 | Loss: 0.00110820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011081958655267954, 0.0011081958655267954, 0.0011081958655267954, 0.0011081958655267954, 0.0011081958655267954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011081958655267954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27493739
Iteration 2/25 | Loss: 0.00098721
Iteration 3/25 | Loss: 0.00098720
Iteration 4/25 | Loss: 0.00098720
Iteration 5/25 | Loss: 0.00098720
Iteration 6/25 | Loss: 0.00098720
Iteration 7/25 | Loss: 0.00098720
Iteration 8/25 | Loss: 0.00098720
Iteration 9/25 | Loss: 0.00098720
Iteration 10/25 | Loss: 0.00098720
Iteration 11/25 | Loss: 0.00098720
Iteration 12/25 | Loss: 0.00098720
Iteration 13/25 | Loss: 0.00098720
Iteration 14/25 | Loss: 0.00098720
Iteration 15/25 | Loss: 0.00098720
Iteration 16/25 | Loss: 0.00098720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009871968068182468, 0.0009871968068182468, 0.0009871968068182468, 0.0009871968068182468, 0.0009871968068182468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009871968068182468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098720
Iteration 2/1000 | Loss: 0.00003818
Iteration 3/1000 | Loss: 0.00002435
Iteration 4/1000 | Loss: 0.00002026
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001817
Iteration 7/1000 | Loss: 0.00001753
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001685
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001635
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001593
Iteration 34/1000 | Loss: 0.00001592
Iteration 35/1000 | Loss: 0.00001592
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001589
Iteration 41/1000 | Loss: 0.00001589
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001588
Iteration 44/1000 | Loss: 0.00001588
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001583
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001582
Iteration 57/1000 | Loss: 0.00001582
Iteration 58/1000 | Loss: 0.00001582
Iteration 59/1000 | Loss: 0.00001582
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001582
Iteration 63/1000 | Loss: 0.00001581
Iteration 64/1000 | Loss: 0.00001580
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001579
Iteration 67/1000 | Loss: 0.00001579
Iteration 68/1000 | Loss: 0.00001578
Iteration 69/1000 | Loss: 0.00001578
Iteration 70/1000 | Loss: 0.00001577
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001576
Iteration 75/1000 | Loss: 0.00001576
Iteration 76/1000 | Loss: 0.00001576
Iteration 77/1000 | Loss: 0.00001575
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001575
Iteration 81/1000 | Loss: 0.00001575
Iteration 82/1000 | Loss: 0.00001575
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001573
Iteration 88/1000 | Loss: 0.00001573
Iteration 89/1000 | Loss: 0.00001573
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001572
Iteration 98/1000 | Loss: 0.00001572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.571915709064342e-05, 1.571915709064342e-05, 1.571915709064342e-05, 1.571915709064342e-05, 1.571915709064342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.571915709064342e-05

Optimization complete. Final v2v error: 3.3467726707458496 mm

Highest mean error: 3.947901964187622 mm for frame 179

Lowest mean error: 2.511220932006836 mm for frame 81

Saving results

Total time: 39.78386211395264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660720
Iteration 2/25 | Loss: 0.00123821
Iteration 3/25 | Loss: 0.00110855
Iteration 4/25 | Loss: 0.00109460
Iteration 5/25 | Loss: 0.00109030
Iteration 6/25 | Loss: 0.00108911
Iteration 7/25 | Loss: 0.00108902
Iteration 8/25 | Loss: 0.00108902
Iteration 9/25 | Loss: 0.00108902
Iteration 10/25 | Loss: 0.00108902
Iteration 11/25 | Loss: 0.00108902
Iteration 12/25 | Loss: 0.00108902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010890158591791987, 0.0010890158591791987, 0.0010890158591791987, 0.0010890158591791987, 0.0010890158591791987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010890158591791987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43078554
Iteration 2/25 | Loss: 0.00087121
Iteration 3/25 | Loss: 0.00087121
Iteration 4/25 | Loss: 0.00087121
Iteration 5/25 | Loss: 0.00087121
Iteration 6/25 | Loss: 0.00087121
Iteration 7/25 | Loss: 0.00087121
Iteration 8/25 | Loss: 0.00087121
Iteration 9/25 | Loss: 0.00087121
Iteration 10/25 | Loss: 0.00087121
Iteration 11/25 | Loss: 0.00087121
Iteration 12/25 | Loss: 0.00087121
Iteration 13/25 | Loss: 0.00087121
Iteration 14/25 | Loss: 0.00087121
Iteration 15/25 | Loss: 0.00087121
Iteration 16/25 | Loss: 0.00087121
Iteration 17/25 | Loss: 0.00087121
Iteration 18/25 | Loss: 0.00087121
Iteration 19/25 | Loss: 0.00087121
Iteration 20/25 | Loss: 0.00087121
Iteration 21/25 | Loss: 0.00087121
Iteration 22/25 | Loss: 0.00087121
Iteration 23/25 | Loss: 0.00087121
Iteration 24/25 | Loss: 0.00087121
Iteration 25/25 | Loss: 0.00087121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087121
Iteration 2/1000 | Loss: 0.00002374
Iteration 3/1000 | Loss: 0.00001522
Iteration 4/1000 | Loss: 0.00001324
Iteration 5/1000 | Loss: 0.00001236
Iteration 6/1000 | Loss: 0.00001188
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001122
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001113
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001100
Iteration 13/1000 | Loss: 0.00001078
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001053
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001040
Iteration 23/1000 | Loss: 0.00001039
Iteration 24/1000 | Loss: 0.00001039
Iteration 25/1000 | Loss: 0.00001038
Iteration 26/1000 | Loss: 0.00001038
Iteration 27/1000 | Loss: 0.00001037
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001034
Iteration 30/1000 | Loss: 0.00001029
Iteration 31/1000 | Loss: 0.00001029
Iteration 32/1000 | Loss: 0.00001028
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00001025
Iteration 36/1000 | Loss: 0.00001025
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001023
Iteration 40/1000 | Loss: 0.00001023
Iteration 41/1000 | Loss: 0.00001022
Iteration 42/1000 | Loss: 0.00001022
Iteration 43/1000 | Loss: 0.00001022
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001021
Iteration 46/1000 | Loss: 0.00001021
Iteration 47/1000 | Loss: 0.00001021
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001020
Iteration 51/1000 | Loss: 0.00001020
Iteration 52/1000 | Loss: 0.00001020
Iteration 53/1000 | Loss: 0.00001020
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001019
Iteration 59/1000 | Loss: 0.00001019
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001018
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001017
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001016
Iteration 68/1000 | Loss: 0.00001016
Iteration 69/1000 | Loss: 0.00001016
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001013
Iteration 84/1000 | Loss: 0.00001013
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001010
Iteration 95/1000 | Loss: 0.00001010
Iteration 96/1000 | Loss: 0.00001010
Iteration 97/1000 | Loss: 0.00001010
Iteration 98/1000 | Loss: 0.00001010
Iteration 99/1000 | Loss: 0.00001010
Iteration 100/1000 | Loss: 0.00001010
Iteration 101/1000 | Loss: 0.00001009
Iteration 102/1000 | Loss: 0.00001009
Iteration 103/1000 | Loss: 0.00001009
Iteration 104/1000 | Loss: 0.00001009
Iteration 105/1000 | Loss: 0.00001009
Iteration 106/1000 | Loss: 0.00001008
Iteration 107/1000 | Loss: 0.00001008
Iteration 108/1000 | Loss: 0.00001008
Iteration 109/1000 | Loss: 0.00001008
Iteration 110/1000 | Loss: 0.00001008
Iteration 111/1000 | Loss: 0.00001008
Iteration 112/1000 | Loss: 0.00001008
Iteration 113/1000 | Loss: 0.00001008
Iteration 114/1000 | Loss: 0.00001007
Iteration 115/1000 | Loss: 0.00001007
Iteration 116/1000 | Loss: 0.00001007
Iteration 117/1000 | Loss: 0.00001007
Iteration 118/1000 | Loss: 0.00001007
Iteration 119/1000 | Loss: 0.00001007
Iteration 120/1000 | Loss: 0.00001007
Iteration 121/1000 | Loss: 0.00001007
Iteration 122/1000 | Loss: 0.00001007
Iteration 123/1000 | Loss: 0.00001007
Iteration 124/1000 | Loss: 0.00001007
Iteration 125/1000 | Loss: 0.00001007
Iteration 126/1000 | Loss: 0.00001007
Iteration 127/1000 | Loss: 0.00001007
Iteration 128/1000 | Loss: 0.00001006
Iteration 129/1000 | Loss: 0.00001006
Iteration 130/1000 | Loss: 0.00001006
Iteration 131/1000 | Loss: 0.00001006
Iteration 132/1000 | Loss: 0.00001006
Iteration 133/1000 | Loss: 0.00001005
Iteration 134/1000 | Loss: 0.00001005
Iteration 135/1000 | Loss: 0.00001005
Iteration 136/1000 | Loss: 0.00001005
Iteration 137/1000 | Loss: 0.00001005
Iteration 138/1000 | Loss: 0.00001005
Iteration 139/1000 | Loss: 0.00001005
Iteration 140/1000 | Loss: 0.00001005
Iteration 141/1000 | Loss: 0.00001005
Iteration 142/1000 | Loss: 0.00001005
Iteration 143/1000 | Loss: 0.00001005
Iteration 144/1000 | Loss: 0.00001004
Iteration 145/1000 | Loss: 0.00001004
Iteration 146/1000 | Loss: 0.00001004
Iteration 147/1000 | Loss: 0.00001004
Iteration 148/1000 | Loss: 0.00001003
Iteration 149/1000 | Loss: 0.00001003
Iteration 150/1000 | Loss: 0.00001003
Iteration 151/1000 | Loss: 0.00001003
Iteration 152/1000 | Loss: 0.00001003
Iteration 153/1000 | Loss: 0.00001003
Iteration 154/1000 | Loss: 0.00001003
Iteration 155/1000 | Loss: 0.00001003
Iteration 156/1000 | Loss: 0.00001003
Iteration 157/1000 | Loss: 0.00001003
Iteration 158/1000 | Loss: 0.00001003
Iteration 159/1000 | Loss: 0.00001002
Iteration 160/1000 | Loss: 0.00001002
Iteration 161/1000 | Loss: 0.00001002
Iteration 162/1000 | Loss: 0.00001002
Iteration 163/1000 | Loss: 0.00001002
Iteration 164/1000 | Loss: 0.00001002
Iteration 165/1000 | Loss: 0.00001002
Iteration 166/1000 | Loss: 0.00001002
Iteration 167/1000 | Loss: 0.00001002
Iteration 168/1000 | Loss: 0.00001002
Iteration 169/1000 | Loss: 0.00001002
Iteration 170/1000 | Loss: 0.00001002
Iteration 171/1000 | Loss: 0.00001002
Iteration 172/1000 | Loss: 0.00001001
Iteration 173/1000 | Loss: 0.00001001
Iteration 174/1000 | Loss: 0.00001001
Iteration 175/1000 | Loss: 0.00001001
Iteration 176/1000 | Loss: 0.00001001
Iteration 177/1000 | Loss: 0.00001001
Iteration 178/1000 | Loss: 0.00001001
Iteration 179/1000 | Loss: 0.00001001
Iteration 180/1000 | Loss: 0.00001001
Iteration 181/1000 | Loss: 0.00001001
Iteration 182/1000 | Loss: 0.00001001
Iteration 183/1000 | Loss: 0.00001001
Iteration 184/1000 | Loss: 0.00001001
Iteration 185/1000 | Loss: 0.00001001
Iteration 186/1000 | Loss: 0.00001001
Iteration 187/1000 | Loss: 0.00001001
Iteration 188/1000 | Loss: 0.00001001
Iteration 189/1000 | Loss: 0.00001001
Iteration 190/1000 | Loss: 0.00001000
Iteration 191/1000 | Loss: 0.00001000
Iteration 192/1000 | Loss: 0.00001000
Iteration 193/1000 | Loss: 0.00001000
Iteration 194/1000 | Loss: 0.00001000
Iteration 195/1000 | Loss: 0.00001000
Iteration 196/1000 | Loss: 0.00001000
Iteration 197/1000 | Loss: 0.00001000
Iteration 198/1000 | Loss: 0.00001000
Iteration 199/1000 | Loss: 0.00001000
Iteration 200/1000 | Loss: 0.00001000
Iteration 201/1000 | Loss: 0.00001000
Iteration 202/1000 | Loss: 0.00001000
Iteration 203/1000 | Loss: 0.00001000
Iteration 204/1000 | Loss: 0.00001000
Iteration 205/1000 | Loss: 0.00001000
Iteration 206/1000 | Loss: 0.00001000
Iteration 207/1000 | Loss: 0.00001000
Iteration 208/1000 | Loss: 0.00001000
Iteration 209/1000 | Loss: 0.00000999
Iteration 210/1000 | Loss: 0.00000999
Iteration 211/1000 | Loss: 0.00000999
Iteration 212/1000 | Loss: 0.00000999
Iteration 213/1000 | Loss: 0.00000999
Iteration 214/1000 | Loss: 0.00000999
Iteration 215/1000 | Loss: 0.00000999
Iteration 216/1000 | Loss: 0.00000999
Iteration 217/1000 | Loss: 0.00000999
Iteration 218/1000 | Loss: 0.00000999
Iteration 219/1000 | Loss: 0.00000999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [9.992863851948641e-06, 9.992863851948641e-06, 9.992863851948641e-06, 9.992863851948641e-06, 9.992863851948641e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.992863851948641e-06

Optimization complete. Final v2v error: 2.665391683578491 mm

Highest mean error: 3.6675961017608643 mm for frame 73

Lowest mean error: 2.287290334701538 mm for frame 3

Saving results

Total time: 41.02038860321045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088239
Iteration 2/25 | Loss: 0.01088238
Iteration 3/25 | Loss: 0.01088237
Iteration 4/25 | Loss: 0.01088237
Iteration 5/25 | Loss: 0.00281096
Iteration 6/25 | Loss: 0.00211236
Iteration 7/25 | Loss: 0.00193275
Iteration 8/25 | Loss: 0.00171987
Iteration 9/25 | Loss: 0.00157477
Iteration 10/25 | Loss: 0.00155198
Iteration 11/25 | Loss: 0.00154735
Iteration 12/25 | Loss: 0.00154452
Iteration 13/25 | Loss: 0.00154346
Iteration 14/25 | Loss: 0.00154179
Iteration 15/25 | Loss: 0.00154154
Iteration 16/25 | Loss: 0.00154135
Iteration 17/25 | Loss: 0.00154120
Iteration 18/25 | Loss: 0.00154114
Iteration 19/25 | Loss: 0.00154114
Iteration 20/25 | Loss: 0.00154114
Iteration 21/25 | Loss: 0.00154113
Iteration 22/25 | Loss: 0.00154113
Iteration 23/25 | Loss: 0.00154113
Iteration 24/25 | Loss: 0.00154113
Iteration 25/25 | Loss: 0.00154113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20396757
Iteration 2/25 | Loss: 0.00225338
Iteration 3/25 | Loss: 0.00225337
Iteration 4/25 | Loss: 0.00225337
Iteration 5/25 | Loss: 0.00225337
Iteration 6/25 | Loss: 0.00225337
Iteration 7/25 | Loss: 0.00225337
Iteration 8/25 | Loss: 0.00225337
Iteration 9/25 | Loss: 0.00225337
Iteration 10/25 | Loss: 0.00225337
Iteration 11/25 | Loss: 0.00225337
Iteration 12/25 | Loss: 0.00225337
Iteration 13/25 | Loss: 0.00225337
Iteration 14/25 | Loss: 0.00225337
Iteration 15/25 | Loss: 0.00225337
Iteration 16/25 | Loss: 0.00225337
Iteration 17/25 | Loss: 0.00225337
Iteration 18/25 | Loss: 0.00225337
Iteration 19/25 | Loss: 0.00225337
Iteration 20/25 | Loss: 0.00225337
Iteration 21/25 | Loss: 0.00225337
Iteration 22/25 | Loss: 0.00225337
Iteration 23/25 | Loss: 0.00225337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002253366634249687, 0.002253366634249687, 0.002253366634249687, 0.002253366634249687, 0.002253366634249687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002253366634249687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225337
Iteration 2/1000 | Loss: 0.00026772
Iteration 3/1000 | Loss: 0.00021331
Iteration 4/1000 | Loss: 0.00019411
Iteration 5/1000 | Loss: 0.00018205
Iteration 6/1000 | Loss: 0.00017191
Iteration 7/1000 | Loss: 0.00016504
Iteration 8/1000 | Loss: 0.00015887
Iteration 9/1000 | Loss: 0.00015516
Iteration 10/1000 | Loss: 0.00015124
Iteration 11/1000 | Loss: 0.00014845
Iteration 12/1000 | Loss: 0.00014635
Iteration 13/1000 | Loss: 0.00014505
Iteration 14/1000 | Loss: 0.00014415
Iteration 15/1000 | Loss: 0.00014317
Iteration 16/1000 | Loss: 0.00014200
Iteration 17/1000 | Loss: 0.00014146
Iteration 18/1000 | Loss: 0.00014107
Iteration 19/1000 | Loss: 0.00014075
Iteration 20/1000 | Loss: 0.00014049
Iteration 21/1000 | Loss: 0.00014021
Iteration 22/1000 | Loss: 0.00013993
Iteration 23/1000 | Loss: 0.00013984
Iteration 24/1000 | Loss: 0.00013972
Iteration 25/1000 | Loss: 0.00013968
Iteration 26/1000 | Loss: 0.00013966
Iteration 27/1000 | Loss: 0.00013966
Iteration 28/1000 | Loss: 0.00013958
Iteration 29/1000 | Loss: 0.00013950
Iteration 30/1000 | Loss: 0.00013945
Iteration 31/1000 | Loss: 0.00013937
Iteration 32/1000 | Loss: 0.00013935
Iteration 33/1000 | Loss: 0.00013924
Iteration 34/1000 | Loss: 0.00013923
Iteration 35/1000 | Loss: 0.00013916
Iteration 36/1000 | Loss: 0.00013915
Iteration 37/1000 | Loss: 0.00013913
Iteration 38/1000 | Loss: 0.00013912
Iteration 39/1000 | Loss: 0.00013912
Iteration 40/1000 | Loss: 0.00013911
Iteration 41/1000 | Loss: 0.00013911
Iteration 42/1000 | Loss: 0.00013910
Iteration 43/1000 | Loss: 0.00013910
Iteration 44/1000 | Loss: 0.00013909
Iteration 45/1000 | Loss: 0.00013909
Iteration 46/1000 | Loss: 0.00013909
Iteration 47/1000 | Loss: 0.00013909
Iteration 48/1000 | Loss: 0.00013909
Iteration 49/1000 | Loss: 0.00013908
Iteration 50/1000 | Loss: 0.00013907
Iteration 51/1000 | Loss: 0.00013906
Iteration 52/1000 | Loss: 0.00013906
Iteration 53/1000 | Loss: 0.00013906
Iteration 54/1000 | Loss: 0.00013906
Iteration 55/1000 | Loss: 0.00013906
Iteration 56/1000 | Loss: 0.00013906
Iteration 57/1000 | Loss: 0.00013905
Iteration 58/1000 | Loss: 0.00013905
Iteration 59/1000 | Loss: 0.00013905
Iteration 60/1000 | Loss: 0.00013905
Iteration 61/1000 | Loss: 0.00013905
Iteration 62/1000 | Loss: 0.00013905
Iteration 63/1000 | Loss: 0.00013905
Iteration 64/1000 | Loss: 0.00013905
Iteration 65/1000 | Loss: 0.00013904
Iteration 66/1000 | Loss: 0.00013904
Iteration 67/1000 | Loss: 0.00013903
Iteration 68/1000 | Loss: 0.00013903
Iteration 69/1000 | Loss: 0.00013902
Iteration 70/1000 | Loss: 0.00013902
Iteration 71/1000 | Loss: 0.00013902
Iteration 72/1000 | Loss: 0.00013902
Iteration 73/1000 | Loss: 0.00013902
Iteration 74/1000 | Loss: 0.00013902
Iteration 75/1000 | Loss: 0.00013901
Iteration 76/1000 | Loss: 0.00013901
Iteration 77/1000 | Loss: 0.00013901
Iteration 78/1000 | Loss: 0.00013901
Iteration 79/1000 | Loss: 0.00013901
Iteration 80/1000 | Loss: 0.00013901
Iteration 81/1000 | Loss: 0.00013901
Iteration 82/1000 | Loss: 0.00013901
Iteration 83/1000 | Loss: 0.00013901
Iteration 84/1000 | Loss: 0.00013901
Iteration 85/1000 | Loss: 0.00013900
Iteration 86/1000 | Loss: 0.00013900
Iteration 87/1000 | Loss: 0.00013900
Iteration 88/1000 | Loss: 0.00013900
Iteration 89/1000 | Loss: 0.00013900
Iteration 90/1000 | Loss: 0.00013900
Iteration 91/1000 | Loss: 0.00013900
Iteration 92/1000 | Loss: 0.00013900
Iteration 93/1000 | Loss: 0.00013899
Iteration 94/1000 | Loss: 0.00013899
Iteration 95/1000 | Loss: 0.00013899
Iteration 96/1000 | Loss: 0.00013899
Iteration 97/1000 | Loss: 0.00013899
Iteration 98/1000 | Loss: 0.00013898
Iteration 99/1000 | Loss: 0.00013898
Iteration 100/1000 | Loss: 0.00013898
Iteration 101/1000 | Loss: 0.00013898
Iteration 102/1000 | Loss: 0.00013898
Iteration 103/1000 | Loss: 0.00013898
Iteration 104/1000 | Loss: 0.00013897
Iteration 105/1000 | Loss: 0.00013897
Iteration 106/1000 | Loss: 0.00013897
Iteration 107/1000 | Loss: 0.00013897
Iteration 108/1000 | Loss: 0.00013897
Iteration 109/1000 | Loss: 0.00013897
Iteration 110/1000 | Loss: 0.00013897
Iteration 111/1000 | Loss: 0.00013896
Iteration 112/1000 | Loss: 0.00013896
Iteration 113/1000 | Loss: 0.00013896
Iteration 114/1000 | Loss: 0.00013896
Iteration 115/1000 | Loss: 0.00013896
Iteration 116/1000 | Loss: 0.00013896
Iteration 117/1000 | Loss: 0.00013896
Iteration 118/1000 | Loss: 0.00013896
Iteration 119/1000 | Loss: 0.00013896
Iteration 120/1000 | Loss: 0.00013895
Iteration 121/1000 | Loss: 0.00013895
Iteration 122/1000 | Loss: 0.00013895
Iteration 123/1000 | Loss: 0.00013895
Iteration 124/1000 | Loss: 0.00013895
Iteration 125/1000 | Loss: 0.00013895
Iteration 126/1000 | Loss: 0.00013894
Iteration 127/1000 | Loss: 0.00013894
Iteration 128/1000 | Loss: 0.00013894
Iteration 129/1000 | Loss: 0.00013894
Iteration 130/1000 | Loss: 0.00013894
Iteration 131/1000 | Loss: 0.00013894
Iteration 132/1000 | Loss: 0.00013894
Iteration 133/1000 | Loss: 0.00013893
Iteration 134/1000 | Loss: 0.00013893
Iteration 135/1000 | Loss: 0.00013893
Iteration 136/1000 | Loss: 0.00013893
Iteration 137/1000 | Loss: 0.00013892
Iteration 138/1000 | Loss: 0.00013892
Iteration 139/1000 | Loss: 0.00013892
Iteration 140/1000 | Loss: 0.00013892
Iteration 141/1000 | Loss: 0.00013892
Iteration 142/1000 | Loss: 0.00013892
Iteration 143/1000 | Loss: 0.00013892
Iteration 144/1000 | Loss: 0.00013892
Iteration 145/1000 | Loss: 0.00013892
Iteration 146/1000 | Loss: 0.00013892
Iteration 147/1000 | Loss: 0.00013892
Iteration 148/1000 | Loss: 0.00013891
Iteration 149/1000 | Loss: 0.00013891
Iteration 150/1000 | Loss: 0.00013891
Iteration 151/1000 | Loss: 0.00013891
Iteration 152/1000 | Loss: 0.00013891
Iteration 153/1000 | Loss: 0.00013890
Iteration 154/1000 | Loss: 0.00013890
Iteration 155/1000 | Loss: 0.00013890
Iteration 156/1000 | Loss: 0.00013890
Iteration 157/1000 | Loss: 0.00013890
Iteration 158/1000 | Loss: 0.00013890
Iteration 159/1000 | Loss: 0.00013890
Iteration 160/1000 | Loss: 0.00013890
Iteration 161/1000 | Loss: 0.00013890
Iteration 162/1000 | Loss: 0.00013889
Iteration 163/1000 | Loss: 0.00013889
Iteration 164/1000 | Loss: 0.00013889
Iteration 165/1000 | Loss: 0.00013889
Iteration 166/1000 | Loss: 0.00013889
Iteration 167/1000 | Loss: 0.00013889
Iteration 168/1000 | Loss: 0.00013889
Iteration 169/1000 | Loss: 0.00013889
Iteration 170/1000 | Loss: 0.00013888
Iteration 171/1000 | Loss: 0.00013888
Iteration 172/1000 | Loss: 0.00013888
Iteration 173/1000 | Loss: 0.00013888
Iteration 174/1000 | Loss: 0.00013888
Iteration 175/1000 | Loss: 0.00013888
Iteration 176/1000 | Loss: 0.00013888
Iteration 177/1000 | Loss: 0.00013888
Iteration 178/1000 | Loss: 0.00013888
Iteration 179/1000 | Loss: 0.00013888
Iteration 180/1000 | Loss: 0.00013888
Iteration 181/1000 | Loss: 0.00013888
Iteration 182/1000 | Loss: 0.00013888
Iteration 183/1000 | Loss: 0.00013888
Iteration 184/1000 | Loss: 0.00013888
Iteration 185/1000 | Loss: 0.00013888
Iteration 186/1000 | Loss: 0.00013888
Iteration 187/1000 | Loss: 0.00013888
Iteration 188/1000 | Loss: 0.00013888
Iteration 189/1000 | Loss: 0.00013888
Iteration 190/1000 | Loss: 0.00013888
Iteration 191/1000 | Loss: 0.00013888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [0.0001388782256981358, 0.0001388782256981358, 0.0001388782256981358, 0.0001388782256981358, 0.0001388782256981358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001388782256981358

Optimization complete. Final v2v error: 6.869550704956055 mm

Highest mean error: 10.971158027648926 mm for frame 106

Lowest mean error: 4.740686416625977 mm for frame 67

Saving results

Total time: 87.98013615608215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074220
Iteration 2/25 | Loss: 0.01074220
Iteration 3/25 | Loss: 0.01074220
Iteration 4/25 | Loss: 0.01074219
Iteration 5/25 | Loss: 0.01074219
Iteration 6/25 | Loss: 0.01074219
Iteration 7/25 | Loss: 0.00621437
Iteration 8/25 | Loss: 0.00371682
Iteration 9/25 | Loss: 0.00328095
Iteration 10/25 | Loss: 0.00250471
Iteration 11/25 | Loss: 0.00211515
Iteration 12/25 | Loss: 0.00185573
Iteration 13/25 | Loss: 0.00161974
Iteration 14/25 | Loss: 0.00153355
Iteration 15/25 | Loss: 0.00144886
Iteration 16/25 | Loss: 0.00134774
Iteration 17/25 | Loss: 0.00132633
Iteration 18/25 | Loss: 0.00130981
Iteration 19/25 | Loss: 0.00130470
Iteration 20/25 | Loss: 0.00129115
Iteration 21/25 | Loss: 0.00128858
Iteration 22/25 | Loss: 0.00128706
Iteration 23/25 | Loss: 0.00127788
Iteration 24/25 | Loss: 0.00127756
Iteration 25/25 | Loss: 0.00127754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55791515
Iteration 2/25 | Loss: 0.00088370
Iteration 3/25 | Loss: 0.00084713
Iteration 4/25 | Loss: 0.00084703
Iteration 5/25 | Loss: 0.00084662
Iteration 6/25 | Loss: 0.00084661
Iteration 7/25 | Loss: 0.00084661
Iteration 8/25 | Loss: 0.00084661
Iteration 9/25 | Loss: 0.00084661
Iteration 10/25 | Loss: 0.00084661
Iteration 11/25 | Loss: 0.00084661
Iteration 12/25 | Loss: 0.00084661
Iteration 13/25 | Loss: 0.00084661
Iteration 14/25 | Loss: 0.00084661
Iteration 15/25 | Loss: 0.00084661
Iteration 16/25 | Loss: 0.00084661
Iteration 17/25 | Loss: 0.00084661
Iteration 18/25 | Loss: 0.00084661
Iteration 19/25 | Loss: 0.00084661
Iteration 20/25 | Loss: 0.00084661
Iteration 21/25 | Loss: 0.00084661
Iteration 22/25 | Loss: 0.00084661
Iteration 23/25 | Loss: 0.00084661
Iteration 24/25 | Loss: 0.00084661
Iteration 25/25 | Loss: 0.00084661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084661
Iteration 2/1000 | Loss: 0.00013604
Iteration 3/1000 | Loss: 0.00004475
Iteration 4/1000 | Loss: 0.00003120
Iteration 5/1000 | Loss: 0.00002939
Iteration 6/1000 | Loss: 0.00002841
Iteration 7/1000 | Loss: 0.00002789
Iteration 8/1000 | Loss: 0.00002739
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00002672
Iteration 11/1000 | Loss: 0.00002639
Iteration 12/1000 | Loss: 0.00206339
Iteration 13/1000 | Loss: 0.00003375
Iteration 14/1000 | Loss: 0.00004725
Iteration 15/1000 | Loss: 0.00003560
Iteration 16/1000 | Loss: 0.00002317
Iteration 17/1000 | Loss: 0.00002138
Iteration 18/1000 | Loss: 0.00002052
Iteration 19/1000 | Loss: 0.00001989
Iteration 20/1000 | Loss: 0.00001936
Iteration 21/1000 | Loss: 0.00004576
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001833
Iteration 24/1000 | Loss: 0.00001796
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001743
Iteration 39/1000 | Loss: 0.00001743
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001742
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001735
Iteration 47/1000 | Loss: 0.00001735
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001734
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001727
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001723
Iteration 115/1000 | Loss: 0.00001723
Iteration 116/1000 | Loss: 0.00001723
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001723
Iteration 124/1000 | Loss: 0.00001723
Iteration 125/1000 | Loss: 0.00001723
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001723
Iteration 130/1000 | Loss: 0.00001723
Iteration 131/1000 | Loss: 0.00001723
Iteration 132/1000 | Loss: 0.00001723
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001723
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001723
Iteration 146/1000 | Loss: 0.00001723
Iteration 147/1000 | Loss: 0.00001723
Iteration 148/1000 | Loss: 0.00001723
Iteration 149/1000 | Loss: 0.00001723
Iteration 150/1000 | Loss: 0.00001723
Iteration 151/1000 | Loss: 0.00001723
Iteration 152/1000 | Loss: 0.00001723
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001723
Iteration 155/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.722631714073941e-05, 1.722631714073941e-05, 1.722631714073941e-05, 1.722631714073941e-05, 1.722631714073941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.722631714073941e-05

Optimization complete. Final v2v error: 3.572631597518921 mm

Highest mean error: 3.82885479927063 mm for frame 119

Lowest mean error: 3.460468053817749 mm for frame 37

Saving results

Total time: 92.24655866622925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369583
Iteration 2/25 | Loss: 0.00121056
Iteration 3/25 | Loss: 0.00110756
Iteration 4/25 | Loss: 0.00109313
Iteration 5/25 | Loss: 0.00108873
Iteration 6/25 | Loss: 0.00108839
Iteration 7/25 | Loss: 0.00108839
Iteration 8/25 | Loss: 0.00108839
Iteration 9/25 | Loss: 0.00108839
Iteration 10/25 | Loss: 0.00108839
Iteration 11/25 | Loss: 0.00108839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010883938521146774, 0.0010883938521146774, 0.0010883938521146774, 0.0010883938521146774, 0.0010883938521146774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010883938521146774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77092922
Iteration 2/25 | Loss: 0.00093331
Iteration 3/25 | Loss: 0.00093330
Iteration 4/25 | Loss: 0.00093330
Iteration 5/25 | Loss: 0.00093330
Iteration 6/25 | Loss: 0.00093330
Iteration 7/25 | Loss: 0.00093330
Iteration 8/25 | Loss: 0.00093330
Iteration 9/25 | Loss: 0.00093330
Iteration 10/25 | Loss: 0.00093330
Iteration 11/25 | Loss: 0.00093330
Iteration 12/25 | Loss: 0.00093330
Iteration 13/25 | Loss: 0.00093330
Iteration 14/25 | Loss: 0.00093330
Iteration 15/25 | Loss: 0.00093330
Iteration 16/25 | Loss: 0.00093330
Iteration 17/25 | Loss: 0.00093330
Iteration 18/25 | Loss: 0.00093330
Iteration 19/25 | Loss: 0.00093330
Iteration 20/25 | Loss: 0.00093330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009332994814030826, 0.0009332994814030826, 0.0009332994814030826, 0.0009332994814030826, 0.0009332994814030826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009332994814030826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093330
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001523
Iteration 4/1000 | Loss: 0.00001387
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001143
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001089
Iteration 11/1000 | Loss: 0.00001069
Iteration 12/1000 | Loss: 0.00001055
Iteration 13/1000 | Loss: 0.00001048
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001040
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001025
Iteration 24/1000 | Loss: 0.00001025
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001023
Iteration 29/1000 | Loss: 0.00001022
Iteration 30/1000 | Loss: 0.00001022
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001019
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001014
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001013
Iteration 41/1000 | Loss: 0.00001013
Iteration 42/1000 | Loss: 0.00001012
Iteration 43/1000 | Loss: 0.00001011
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001010
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001005
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001003
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001002
Iteration 69/1000 | Loss: 0.00001002
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000997
Iteration 85/1000 | Loss: 0.00000997
Iteration 86/1000 | Loss: 0.00000997
Iteration 87/1000 | Loss: 0.00000997
Iteration 88/1000 | Loss: 0.00000997
Iteration 89/1000 | Loss: 0.00000997
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000996
Iteration 92/1000 | Loss: 0.00000996
Iteration 93/1000 | Loss: 0.00000996
Iteration 94/1000 | Loss: 0.00000996
Iteration 95/1000 | Loss: 0.00000996
Iteration 96/1000 | Loss: 0.00000996
Iteration 97/1000 | Loss: 0.00000996
Iteration 98/1000 | Loss: 0.00000996
Iteration 99/1000 | Loss: 0.00000995
Iteration 100/1000 | Loss: 0.00000995
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000995
Iteration 108/1000 | Loss: 0.00000995
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000994
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000994
Iteration 117/1000 | Loss: 0.00000994
Iteration 118/1000 | Loss: 0.00000994
Iteration 119/1000 | Loss: 0.00000994
Iteration 120/1000 | Loss: 0.00000994
Iteration 121/1000 | Loss: 0.00000994
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000993
Iteration 125/1000 | Loss: 0.00000993
Iteration 126/1000 | Loss: 0.00000993
Iteration 127/1000 | Loss: 0.00000993
Iteration 128/1000 | Loss: 0.00000993
Iteration 129/1000 | Loss: 0.00000993
Iteration 130/1000 | Loss: 0.00000993
Iteration 131/1000 | Loss: 0.00000993
Iteration 132/1000 | Loss: 0.00000993
Iteration 133/1000 | Loss: 0.00000993
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000993
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000993
Iteration 139/1000 | Loss: 0.00000993
Iteration 140/1000 | Loss: 0.00000993
Iteration 141/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [9.930930900736712e-06, 9.930930900736712e-06, 9.930930900736712e-06, 9.930930900736712e-06, 9.930930900736712e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.930930900736712e-06

Optimization complete. Final v2v error: 2.7358381748199463 mm

Highest mean error: 3.21317195892334 mm for frame 75

Lowest mean error: 2.3161487579345703 mm for frame 10

Saving results

Total time: 41.1384494304657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761350
Iteration 2/25 | Loss: 0.00163199
Iteration 3/25 | Loss: 0.00120934
Iteration 4/25 | Loss: 0.00117494
Iteration 5/25 | Loss: 0.00118122
Iteration 6/25 | Loss: 0.00117987
Iteration 7/25 | Loss: 0.00116794
Iteration 8/25 | Loss: 0.00116493
Iteration 9/25 | Loss: 0.00116205
Iteration 10/25 | Loss: 0.00116306
Iteration 11/25 | Loss: 0.00116131
Iteration 12/25 | Loss: 0.00115949
Iteration 13/25 | Loss: 0.00115765
Iteration 14/25 | Loss: 0.00115739
Iteration 15/25 | Loss: 0.00116120
Iteration 16/25 | Loss: 0.00115981
Iteration 17/25 | Loss: 0.00115872
Iteration 18/25 | Loss: 0.00115675
Iteration 19/25 | Loss: 0.00115630
Iteration 20/25 | Loss: 0.00115616
Iteration 21/25 | Loss: 0.00115616
Iteration 22/25 | Loss: 0.00115616
Iteration 23/25 | Loss: 0.00115616
Iteration 24/25 | Loss: 0.00115616
Iteration 25/25 | Loss: 0.00115616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.47142220
Iteration 2/25 | Loss: 0.00057281
Iteration 3/25 | Loss: 0.00057254
Iteration 4/25 | Loss: 0.00057254
Iteration 5/25 | Loss: 0.00057254
Iteration 6/25 | Loss: 0.00057254
Iteration 7/25 | Loss: 0.00057254
Iteration 8/25 | Loss: 0.00057254
Iteration 9/25 | Loss: 0.00057254
Iteration 10/25 | Loss: 0.00057254
Iteration 11/25 | Loss: 0.00057254
Iteration 12/25 | Loss: 0.00057254
Iteration 13/25 | Loss: 0.00057254
Iteration 14/25 | Loss: 0.00057254
Iteration 15/25 | Loss: 0.00057254
Iteration 16/25 | Loss: 0.00057254
Iteration 17/25 | Loss: 0.00057254
Iteration 18/25 | Loss: 0.00057254
Iteration 19/25 | Loss: 0.00057254
Iteration 20/25 | Loss: 0.00057254
Iteration 21/25 | Loss: 0.00057254
Iteration 22/25 | Loss: 0.00057254
Iteration 23/25 | Loss: 0.00057254
Iteration 24/25 | Loss: 0.00057254
Iteration 25/25 | Loss: 0.00057254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057254
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00001549
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001402
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001398
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001384
Iteration 68/1000 | Loss: 0.00001384
Iteration 69/1000 | Loss: 0.00001384
Iteration 70/1000 | Loss: 0.00001384
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001384
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001381
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001376
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001371
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.370313657389488e-05, 1.370313657389488e-05, 1.370313657389488e-05, 1.370313657389488e-05, 1.370313657389488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.370313657389488e-05

Optimization complete. Final v2v error: 3.1691200733184814 mm

Highest mean error: 3.542595386505127 mm for frame 4

Lowest mean error: 2.8879542350769043 mm for frame 135

Saving results

Total time: 65.89577603340149
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514791
Iteration 2/25 | Loss: 0.00127127
Iteration 3/25 | Loss: 0.00117166
Iteration 4/25 | Loss: 0.00116397
Iteration 5/25 | Loss: 0.00116312
Iteration 6/25 | Loss: 0.00116312
Iteration 7/25 | Loss: 0.00116312
Iteration 8/25 | Loss: 0.00116312
Iteration 9/25 | Loss: 0.00116312
Iteration 10/25 | Loss: 0.00116312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001163121429271996, 0.001163121429271996, 0.001163121429271996, 0.001163121429271996, 0.001163121429271996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163121429271996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81236881
Iteration 2/25 | Loss: 0.00057578
Iteration 3/25 | Loss: 0.00057577
Iteration 4/25 | Loss: 0.00057577
Iteration 5/25 | Loss: 0.00057577
Iteration 6/25 | Loss: 0.00057577
Iteration 7/25 | Loss: 0.00057577
Iteration 8/25 | Loss: 0.00057577
Iteration 9/25 | Loss: 0.00057577
Iteration 10/25 | Loss: 0.00057577
Iteration 11/25 | Loss: 0.00057577
Iteration 12/25 | Loss: 0.00057577
Iteration 13/25 | Loss: 0.00057577
Iteration 14/25 | Loss: 0.00057577
Iteration 15/25 | Loss: 0.00057577
Iteration 16/25 | Loss: 0.00057577
Iteration 17/25 | Loss: 0.00057577
Iteration 18/25 | Loss: 0.00057577
Iteration 19/25 | Loss: 0.00057577
Iteration 20/25 | Loss: 0.00057577
Iteration 21/25 | Loss: 0.00057577
Iteration 22/25 | Loss: 0.00057577
Iteration 23/25 | Loss: 0.00057577
Iteration 24/25 | Loss: 0.00057577
Iteration 25/25 | Loss: 0.00057577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057577
Iteration 2/1000 | Loss: 0.00002094
Iteration 3/1000 | Loss: 0.00001606
Iteration 4/1000 | Loss: 0.00001497
Iteration 5/1000 | Loss: 0.00001437
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001343
Iteration 9/1000 | Loss: 0.00001328
Iteration 10/1000 | Loss: 0.00001308
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001279
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001277
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001271
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001257
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001253
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001244
Iteration 81/1000 | Loss: 0.00001244
Iteration 82/1000 | Loss: 0.00001244
Iteration 83/1000 | Loss: 0.00001244
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001237
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001229
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001229
Iteration 147/1000 | Loss: 0.00001229
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001228
Iteration 154/1000 | Loss: 0.00001228
Iteration 155/1000 | Loss: 0.00001228
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001228
Iteration 163/1000 | Loss: 0.00001228
Iteration 164/1000 | Loss: 0.00001228
Iteration 165/1000 | Loss: 0.00001228
Iteration 166/1000 | Loss: 0.00001228
Iteration 167/1000 | Loss: 0.00001228
Iteration 168/1000 | Loss: 0.00001228
Iteration 169/1000 | Loss: 0.00001228
Iteration 170/1000 | Loss: 0.00001228
Iteration 171/1000 | Loss: 0.00001228
Iteration 172/1000 | Loss: 0.00001228
Iteration 173/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.2280468581593595e-05, 1.2280468581593595e-05, 1.2280468581593595e-05, 1.2280468581593595e-05, 1.2280468581593595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2280468581593595e-05

Optimization complete. Final v2v error: 2.959015369415283 mm

Highest mean error: 2.9943180084228516 mm for frame 0

Lowest mean error: 2.8969666957855225 mm for frame 248

Saving results

Total time: 41.21389436721802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774873
Iteration 2/25 | Loss: 0.00133889
Iteration 3/25 | Loss: 0.00116193
Iteration 4/25 | Loss: 0.00113154
Iteration 5/25 | Loss: 0.00112356
Iteration 6/25 | Loss: 0.00112095
Iteration 7/25 | Loss: 0.00112062
Iteration 8/25 | Loss: 0.00112062
Iteration 9/25 | Loss: 0.00112062
Iteration 10/25 | Loss: 0.00112062
Iteration 11/25 | Loss: 0.00112062
Iteration 12/25 | Loss: 0.00112062
Iteration 13/25 | Loss: 0.00112062
Iteration 14/25 | Loss: 0.00112062
Iteration 15/25 | Loss: 0.00112062
Iteration 16/25 | Loss: 0.00112062
Iteration 17/25 | Loss: 0.00112062
Iteration 18/25 | Loss: 0.00112062
Iteration 19/25 | Loss: 0.00112062
Iteration 20/25 | Loss: 0.00112062
Iteration 21/25 | Loss: 0.00112062
Iteration 22/25 | Loss: 0.00112062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011206208728253841, 0.0011206208728253841, 0.0011206208728253841, 0.0011206208728253841, 0.0011206208728253841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011206208728253841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33400834
Iteration 2/25 | Loss: 0.00096009
Iteration 3/25 | Loss: 0.00096008
Iteration 4/25 | Loss: 0.00096008
Iteration 5/25 | Loss: 0.00096008
Iteration 6/25 | Loss: 0.00096008
Iteration 7/25 | Loss: 0.00096008
Iteration 8/25 | Loss: 0.00096008
Iteration 9/25 | Loss: 0.00096008
Iteration 10/25 | Loss: 0.00096008
Iteration 11/25 | Loss: 0.00096008
Iteration 12/25 | Loss: 0.00096008
Iteration 13/25 | Loss: 0.00096008
Iteration 14/25 | Loss: 0.00096008
Iteration 15/25 | Loss: 0.00096008
Iteration 16/25 | Loss: 0.00096008
Iteration 17/25 | Loss: 0.00096008
Iteration 18/25 | Loss: 0.00096008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009600790799595416, 0.0009600790799595416, 0.0009600790799595416, 0.0009600790799595416, 0.0009600790799595416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009600790799595416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096008
Iteration 2/1000 | Loss: 0.00004360
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002624
Iteration 5/1000 | Loss: 0.00002451
Iteration 6/1000 | Loss: 0.00002318
Iteration 7/1000 | Loss: 0.00002234
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002127
Iteration 10/1000 | Loss: 0.00002088
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002037
Iteration 15/1000 | Loss: 0.00002030
Iteration 16/1000 | Loss: 0.00002025
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00002021
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00002020
Iteration 21/1000 | Loss: 0.00002019
Iteration 22/1000 | Loss: 0.00002018
Iteration 23/1000 | Loss: 0.00002017
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002015
Iteration 27/1000 | Loss: 0.00002015
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002013
Iteration 31/1000 | Loss: 0.00002012
Iteration 32/1000 | Loss: 0.00002012
Iteration 33/1000 | Loss: 0.00002011
Iteration 34/1000 | Loss: 0.00002011
Iteration 35/1000 | Loss: 0.00002010
Iteration 36/1000 | Loss: 0.00002010
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002008
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002004
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002003
Iteration 43/1000 | Loss: 0.00002002
Iteration 44/1000 | Loss: 0.00002002
Iteration 45/1000 | Loss: 0.00002001
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00002000
Iteration 48/1000 | Loss: 0.00002000
Iteration 49/1000 | Loss: 0.00002000
Iteration 50/1000 | Loss: 0.00001999
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001999
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001998
Iteration 56/1000 | Loss: 0.00001998
Iteration 57/1000 | Loss: 0.00001998
Iteration 58/1000 | Loss: 0.00001998
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001997
Iteration 61/1000 | Loss: 0.00001997
Iteration 62/1000 | Loss: 0.00001997
Iteration 63/1000 | Loss: 0.00001997
Iteration 64/1000 | Loss: 0.00001997
Iteration 65/1000 | Loss: 0.00001997
Iteration 66/1000 | Loss: 0.00001996
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001996
Iteration 70/1000 | Loss: 0.00001996
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001996
Iteration 74/1000 | Loss: 0.00001996
Iteration 75/1000 | Loss: 0.00001995
Iteration 76/1000 | Loss: 0.00001995
Iteration 77/1000 | Loss: 0.00001995
Iteration 78/1000 | Loss: 0.00001995
Iteration 79/1000 | Loss: 0.00001995
Iteration 80/1000 | Loss: 0.00001995
Iteration 81/1000 | Loss: 0.00001995
Iteration 82/1000 | Loss: 0.00001994
Iteration 83/1000 | Loss: 0.00001994
Iteration 84/1000 | Loss: 0.00001994
Iteration 85/1000 | Loss: 0.00001994
Iteration 86/1000 | Loss: 0.00001994
Iteration 87/1000 | Loss: 0.00001994
Iteration 88/1000 | Loss: 0.00001994
Iteration 89/1000 | Loss: 0.00001994
Iteration 90/1000 | Loss: 0.00001994
Iteration 91/1000 | Loss: 0.00001994
Iteration 92/1000 | Loss: 0.00001994
Iteration 93/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.9937193428631872e-05, 1.9937193428631872e-05, 1.9937193428631872e-05, 1.9937193428631872e-05, 1.9937193428631872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9937193428631872e-05

Optimization complete. Final v2v error: 3.673048496246338 mm

Highest mean error: 4.840584754943848 mm for frame 153

Lowest mean error: 2.903282642364502 mm for frame 10

Saving results

Total time: 36.537140130996704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552424
Iteration 2/25 | Loss: 0.00122936
Iteration 3/25 | Loss: 0.00114135
Iteration 4/25 | Loss: 0.00113310
Iteration 5/25 | Loss: 0.00113106
Iteration 6/25 | Loss: 0.00113106
Iteration 7/25 | Loss: 0.00113106
Iteration 8/25 | Loss: 0.00113106
Iteration 9/25 | Loss: 0.00113106
Iteration 10/25 | Loss: 0.00113106
Iteration 11/25 | Loss: 0.00113106
Iteration 12/25 | Loss: 0.00113106
Iteration 13/25 | Loss: 0.00113106
Iteration 14/25 | Loss: 0.00113106
Iteration 15/25 | Loss: 0.00113106
Iteration 16/25 | Loss: 0.00113106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011310637928545475, 0.0011310637928545475, 0.0011310637928545475, 0.0011310637928545475, 0.0011310637928545475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011310637928545475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31840765
Iteration 2/25 | Loss: 0.00084817
Iteration 3/25 | Loss: 0.00084812
Iteration 4/25 | Loss: 0.00084812
Iteration 5/25 | Loss: 0.00084812
Iteration 6/25 | Loss: 0.00084812
Iteration 7/25 | Loss: 0.00084812
Iteration 8/25 | Loss: 0.00084812
Iteration 9/25 | Loss: 0.00084812
Iteration 10/25 | Loss: 0.00084812
Iteration 11/25 | Loss: 0.00084812
Iteration 12/25 | Loss: 0.00084812
Iteration 13/25 | Loss: 0.00084812
Iteration 14/25 | Loss: 0.00084812
Iteration 15/25 | Loss: 0.00084812
Iteration 16/25 | Loss: 0.00084812
Iteration 17/25 | Loss: 0.00084812
Iteration 18/25 | Loss: 0.00084812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008481214754283428, 0.0008481214754283428, 0.0008481214754283428, 0.0008481214754283428, 0.0008481214754283428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008481214754283428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084812
Iteration 2/1000 | Loss: 0.00002480
Iteration 3/1000 | Loss: 0.00001559
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001275
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001158
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001123
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001082
Iteration 18/1000 | Loss: 0.00001082
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001079
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001076
Iteration 26/1000 | Loss: 0.00001075
Iteration 27/1000 | Loss: 0.00001075
Iteration 28/1000 | Loss: 0.00001074
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001073
Iteration 33/1000 | Loss: 0.00001073
Iteration 34/1000 | Loss: 0.00001073
Iteration 35/1000 | Loss: 0.00001072
Iteration 36/1000 | Loss: 0.00001072
Iteration 37/1000 | Loss: 0.00001072
Iteration 38/1000 | Loss: 0.00001072
Iteration 39/1000 | Loss: 0.00001072
Iteration 40/1000 | Loss: 0.00001072
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001071
Iteration 43/1000 | Loss: 0.00001071
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001071
Iteration 48/1000 | Loss: 0.00001071
Iteration 49/1000 | Loss: 0.00001071
Iteration 50/1000 | Loss: 0.00001071
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.0709080925153103e-05, 1.0709080925153103e-05, 1.0709080925153103e-05, 1.0709080925153103e-05, 1.0709080925153103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0709080925153103e-05

Optimization complete. Final v2v error: 2.7935752868652344 mm

Highest mean error: 2.9368627071380615 mm for frame 149

Lowest mean error: 2.6918857097625732 mm for frame 71

Saving results

Total time: 27.509974002838135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064044
Iteration 2/25 | Loss: 0.01064043
Iteration 3/25 | Loss: 0.00197245
Iteration 4/25 | Loss: 0.00132020
Iteration 5/25 | Loss: 0.00121283
Iteration 6/25 | Loss: 0.00117933
Iteration 7/25 | Loss: 0.00117336
Iteration 8/25 | Loss: 0.00118552
Iteration 9/25 | Loss: 0.00117279
Iteration 10/25 | Loss: 0.00115536
Iteration 11/25 | Loss: 0.00114122
Iteration 12/25 | Loss: 0.00114421
Iteration 13/25 | Loss: 0.00113964
Iteration 14/25 | Loss: 0.00113891
Iteration 15/25 | Loss: 0.00113945
Iteration 16/25 | Loss: 0.00113051
Iteration 17/25 | Loss: 0.00112693
Iteration 18/25 | Loss: 0.00112988
Iteration 19/25 | Loss: 0.00112433
Iteration 20/25 | Loss: 0.00112118
Iteration 21/25 | Loss: 0.00112533
Iteration 22/25 | Loss: 0.00112542
Iteration 23/25 | Loss: 0.00112975
Iteration 24/25 | Loss: 0.00112675
Iteration 25/25 | Loss: 0.00112773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.48311329
Iteration 2/25 | Loss: 0.00092649
Iteration 3/25 | Loss: 0.00092649
Iteration 4/25 | Loss: 0.00092649
Iteration 5/25 | Loss: 0.00092648
Iteration 6/25 | Loss: 0.00092648
Iteration 7/25 | Loss: 0.00092648
Iteration 8/25 | Loss: 0.00092648
Iteration 9/25 | Loss: 0.00092648
Iteration 10/25 | Loss: 0.00092648
Iteration 11/25 | Loss: 0.00092648
Iteration 12/25 | Loss: 0.00092648
Iteration 13/25 | Loss: 0.00092648
Iteration 14/25 | Loss: 0.00092648
Iteration 15/25 | Loss: 0.00092648
Iteration 16/25 | Loss: 0.00092648
Iteration 17/25 | Loss: 0.00092648
Iteration 18/25 | Loss: 0.00092648
Iteration 19/25 | Loss: 0.00092648
Iteration 20/25 | Loss: 0.00092648
Iteration 21/25 | Loss: 0.00092648
Iteration 22/25 | Loss: 0.00092648
Iteration 23/25 | Loss: 0.00092648
Iteration 24/25 | Loss: 0.00092648
Iteration 25/25 | Loss: 0.00092648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092648
Iteration 2/1000 | Loss: 0.00015460
Iteration 3/1000 | Loss: 0.00041611
Iteration 4/1000 | Loss: 0.00029567
Iteration 5/1000 | Loss: 0.00034082
Iteration 6/1000 | Loss: 0.00008361
Iteration 7/1000 | Loss: 0.00010711
Iteration 8/1000 | Loss: 0.00005158
Iteration 9/1000 | Loss: 0.00023088
Iteration 10/1000 | Loss: 0.00012340
Iteration 11/1000 | Loss: 0.00017957
Iteration 12/1000 | Loss: 0.00019250
Iteration 13/1000 | Loss: 0.00018201
Iteration 14/1000 | Loss: 0.00005665
Iteration 15/1000 | Loss: 0.00024383
Iteration 16/1000 | Loss: 0.00020448
Iteration 17/1000 | Loss: 0.00012265
Iteration 18/1000 | Loss: 0.00016403
Iteration 19/1000 | Loss: 0.00008290
Iteration 20/1000 | Loss: 0.00027065
Iteration 21/1000 | Loss: 0.00030493
Iteration 22/1000 | Loss: 0.00022940
Iteration 23/1000 | Loss: 0.00005193
Iteration 24/1000 | Loss: 0.00002969
Iteration 25/1000 | Loss: 0.00010750
Iteration 26/1000 | Loss: 0.00033862
Iteration 27/1000 | Loss: 0.00028848
Iteration 28/1000 | Loss: 0.00024721
Iteration 29/1000 | Loss: 0.00029391
Iteration 30/1000 | Loss: 0.00035631
Iteration 31/1000 | Loss: 0.00012315
Iteration 32/1000 | Loss: 0.00014588
Iteration 33/1000 | Loss: 0.00007909
Iteration 34/1000 | Loss: 0.00005755
Iteration 35/1000 | Loss: 0.00005487
Iteration 36/1000 | Loss: 0.00007047
Iteration 37/1000 | Loss: 0.00019380
Iteration 38/1000 | Loss: 0.00007673
Iteration 39/1000 | Loss: 0.00004027
Iteration 40/1000 | Loss: 0.00016447
Iteration 41/1000 | Loss: 0.00012384
Iteration 42/1000 | Loss: 0.00006893
Iteration 43/1000 | Loss: 0.00011821
Iteration 44/1000 | Loss: 0.00015746
Iteration 45/1000 | Loss: 0.00003263
Iteration 46/1000 | Loss: 0.00002378
Iteration 47/1000 | Loss: 0.00026812
Iteration 48/1000 | Loss: 0.00042053
Iteration 49/1000 | Loss: 0.00038349
Iteration 50/1000 | Loss: 0.00032468
Iteration 51/1000 | Loss: 0.00038519
Iteration 52/1000 | Loss: 0.00032412
Iteration 53/1000 | Loss: 0.00007541
Iteration 54/1000 | Loss: 0.00003905
Iteration 55/1000 | Loss: 0.00006209
Iteration 56/1000 | Loss: 0.00003310
Iteration 57/1000 | Loss: 0.00002128
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00010456
Iteration 61/1000 | Loss: 0.00002274
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00006457
Iteration 64/1000 | Loss: 0.00002090
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00006230
Iteration 72/1000 | Loss: 0.00015145
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001528
Iteration 78/1000 | Loss: 0.00012000
Iteration 79/1000 | Loss: 0.00010481
Iteration 80/1000 | Loss: 0.00015045
Iteration 81/1000 | Loss: 0.00012294
Iteration 82/1000 | Loss: 0.00012649
Iteration 83/1000 | Loss: 0.00007814
Iteration 84/1000 | Loss: 0.00005634
Iteration 85/1000 | Loss: 0.00002475
Iteration 86/1000 | Loss: 0.00016281
Iteration 87/1000 | Loss: 0.00011691
Iteration 88/1000 | Loss: 0.00001531
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00009959
Iteration 91/1000 | Loss: 0.00017202
Iteration 92/1000 | Loss: 0.00006009
Iteration 93/1000 | Loss: 0.00004387
Iteration 94/1000 | Loss: 0.00003752
Iteration 95/1000 | Loss: 0.00001408
Iteration 96/1000 | Loss: 0.00001395
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00010300
Iteration 102/1000 | Loss: 0.00021407
Iteration 103/1000 | Loss: 0.00010714
Iteration 104/1000 | Loss: 0.00004698
Iteration 105/1000 | Loss: 0.00008811
Iteration 106/1000 | Loss: 0.00004045
Iteration 107/1000 | Loss: 0.00012876
Iteration 108/1000 | Loss: 0.00006324
Iteration 109/1000 | Loss: 0.00012890
Iteration 110/1000 | Loss: 0.00005813
Iteration 111/1000 | Loss: 0.00001849
Iteration 112/1000 | Loss: 0.00003626
Iteration 113/1000 | Loss: 0.00007311
Iteration 114/1000 | Loss: 0.00001747
Iteration 115/1000 | Loss: 0.00001730
Iteration 116/1000 | Loss: 0.00001399
Iteration 117/1000 | Loss: 0.00001387
Iteration 118/1000 | Loss: 0.00013165
Iteration 119/1000 | Loss: 0.00007546
Iteration 120/1000 | Loss: 0.00004765
Iteration 121/1000 | Loss: 0.00011592
Iteration 122/1000 | Loss: 0.00005350
Iteration 123/1000 | Loss: 0.00003429
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00007016
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00014324
Iteration 129/1000 | Loss: 0.00004140
Iteration 130/1000 | Loss: 0.00024196
Iteration 131/1000 | Loss: 0.00020011
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00002258
Iteration 135/1000 | Loss: 0.00001564
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001363
Iteration 138/1000 | Loss: 0.00001362
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001335
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001322
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001397
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001316
Iteration 156/1000 | Loss: 0.00001316
Iteration 157/1000 | Loss: 0.00001316
Iteration 158/1000 | Loss: 0.00001316
Iteration 159/1000 | Loss: 0.00001316
Iteration 160/1000 | Loss: 0.00001316
Iteration 161/1000 | Loss: 0.00001316
Iteration 162/1000 | Loss: 0.00001316
Iteration 163/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.315634017373668e-05, 1.315634017373668e-05, 1.315634017373668e-05, 1.315634017373668e-05, 1.315634017373668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.315634017373668e-05

Optimization complete. Final v2v error: 3.0851175785064697 mm

Highest mean error: 6.2980194091796875 mm for frame 154

Lowest mean error: 2.7073187828063965 mm for frame 213

Saving results

Total time: 269.62402057647705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789974
Iteration 2/25 | Loss: 0.00132886
Iteration 3/25 | Loss: 0.00115743
Iteration 4/25 | Loss: 0.00113464
Iteration 5/25 | Loss: 0.00112586
Iteration 6/25 | Loss: 0.00112437
Iteration 7/25 | Loss: 0.00112437
Iteration 8/25 | Loss: 0.00112437
Iteration 9/25 | Loss: 0.00112437
Iteration 10/25 | Loss: 0.00112437
Iteration 11/25 | Loss: 0.00112437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001124367001466453, 0.001124367001466453, 0.001124367001466453, 0.001124367001466453, 0.001124367001466453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001124367001466453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71979022
Iteration 2/25 | Loss: 0.00067333
Iteration 3/25 | Loss: 0.00067333
Iteration 4/25 | Loss: 0.00067333
Iteration 5/25 | Loss: 0.00067333
Iteration 6/25 | Loss: 0.00067333
Iteration 7/25 | Loss: 0.00067333
Iteration 8/25 | Loss: 0.00067333
Iteration 9/25 | Loss: 0.00067333
Iteration 10/25 | Loss: 0.00067333
Iteration 11/25 | Loss: 0.00067333
Iteration 12/25 | Loss: 0.00067333
Iteration 13/25 | Loss: 0.00067333
Iteration 14/25 | Loss: 0.00067333
Iteration 15/25 | Loss: 0.00067333
Iteration 16/25 | Loss: 0.00067333
Iteration 17/25 | Loss: 0.00067333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006733294576406479, 0.0006733294576406479, 0.0006733294576406479, 0.0006733294576406479, 0.0006733294576406479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006733294576406479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067333
Iteration 2/1000 | Loss: 0.00003544
Iteration 3/1000 | Loss: 0.00002340
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001799
Iteration 8/1000 | Loss: 0.00001766
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001683
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001648
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001644
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001643
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001635
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001635
Iteration 46/1000 | Loss: 0.00001634
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001634
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00001632
Iteration 60/1000 | Loss: 0.00001632
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001631
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001630
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001629
Iteration 80/1000 | Loss: 0.00001629
Iteration 81/1000 | Loss: 0.00001629
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001629
Iteration 85/1000 | Loss: 0.00001629
Iteration 86/1000 | Loss: 0.00001629
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001627
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001627
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001626
Iteration 105/1000 | Loss: 0.00001626
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001626
Iteration 115/1000 | Loss: 0.00001626
Iteration 116/1000 | Loss: 0.00001626
Iteration 117/1000 | Loss: 0.00001626
Iteration 118/1000 | Loss: 0.00001626
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001626
Iteration 121/1000 | Loss: 0.00001625
Iteration 122/1000 | Loss: 0.00001625
Iteration 123/1000 | Loss: 0.00001625
Iteration 124/1000 | Loss: 0.00001625
Iteration 125/1000 | Loss: 0.00001625
Iteration 126/1000 | Loss: 0.00001625
Iteration 127/1000 | Loss: 0.00001625
Iteration 128/1000 | Loss: 0.00001625
Iteration 129/1000 | Loss: 0.00001624
Iteration 130/1000 | Loss: 0.00001624
Iteration 131/1000 | Loss: 0.00001624
Iteration 132/1000 | Loss: 0.00001624
Iteration 133/1000 | Loss: 0.00001624
Iteration 134/1000 | Loss: 0.00001624
Iteration 135/1000 | Loss: 0.00001624
Iteration 136/1000 | Loss: 0.00001624
Iteration 137/1000 | Loss: 0.00001624
Iteration 138/1000 | Loss: 0.00001624
Iteration 139/1000 | Loss: 0.00001624
Iteration 140/1000 | Loss: 0.00001624
Iteration 141/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.6241716366494074e-05, 1.6241716366494074e-05, 1.6241716366494074e-05, 1.6241716366494074e-05, 1.6241716366494074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6241716366494074e-05

Optimization complete. Final v2v error: 3.446720600128174 mm

Highest mean error: 3.830911159515381 mm for frame 41

Lowest mean error: 3.122474431991577 mm for frame 165

Saving results

Total time: 40.83056116104126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755066
Iteration 2/25 | Loss: 0.00154018
Iteration 3/25 | Loss: 0.00132473
Iteration 4/25 | Loss: 0.00128695
Iteration 5/25 | Loss: 0.00127759
Iteration 6/25 | Loss: 0.00127504
Iteration 7/25 | Loss: 0.00127504
Iteration 8/25 | Loss: 0.00127504
Iteration 9/25 | Loss: 0.00127504
Iteration 10/25 | Loss: 0.00127504
Iteration 11/25 | Loss: 0.00127504
Iteration 12/25 | Loss: 0.00127504
Iteration 13/25 | Loss: 0.00127504
Iteration 14/25 | Loss: 0.00127504
Iteration 15/25 | Loss: 0.00127504
Iteration 16/25 | Loss: 0.00127504
Iteration 17/25 | Loss: 0.00127504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012750430032610893, 0.0012750430032610893, 0.0012750430032610893, 0.0012750430032610893, 0.0012750430032610893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012750430032610893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18696284
Iteration 2/25 | Loss: 0.00129268
Iteration 3/25 | Loss: 0.00129268
Iteration 4/25 | Loss: 0.00129268
Iteration 5/25 | Loss: 0.00129268
Iteration 6/25 | Loss: 0.00129268
Iteration 7/25 | Loss: 0.00129268
Iteration 8/25 | Loss: 0.00129268
Iteration 9/25 | Loss: 0.00129268
Iteration 10/25 | Loss: 0.00129268
Iteration 11/25 | Loss: 0.00129268
Iteration 12/25 | Loss: 0.00129268
Iteration 13/25 | Loss: 0.00129268
Iteration 14/25 | Loss: 0.00129268
Iteration 15/25 | Loss: 0.00129268
Iteration 16/25 | Loss: 0.00129268
Iteration 17/25 | Loss: 0.00129268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001292680623009801, 0.001292680623009801, 0.001292680623009801, 0.001292680623009801, 0.001292680623009801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001292680623009801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129268
Iteration 2/1000 | Loss: 0.00008123
Iteration 3/1000 | Loss: 0.00005103
Iteration 4/1000 | Loss: 0.00004196
Iteration 5/1000 | Loss: 0.00003861
Iteration 6/1000 | Loss: 0.00003651
Iteration 7/1000 | Loss: 0.00003536
Iteration 8/1000 | Loss: 0.00003436
Iteration 9/1000 | Loss: 0.00003381
Iteration 10/1000 | Loss: 0.00003334
Iteration 11/1000 | Loss: 0.00003287
Iteration 12/1000 | Loss: 0.00003250
Iteration 13/1000 | Loss: 0.00003223
Iteration 14/1000 | Loss: 0.00003197
Iteration 15/1000 | Loss: 0.00003178
Iteration 16/1000 | Loss: 0.00003162
Iteration 17/1000 | Loss: 0.00003149
Iteration 18/1000 | Loss: 0.00003147
Iteration 19/1000 | Loss: 0.00003144
Iteration 20/1000 | Loss: 0.00003141
Iteration 21/1000 | Loss: 0.00003138
Iteration 22/1000 | Loss: 0.00003133
Iteration 23/1000 | Loss: 0.00003130
Iteration 24/1000 | Loss: 0.00003129
Iteration 25/1000 | Loss: 0.00003126
Iteration 26/1000 | Loss: 0.00003126
Iteration 27/1000 | Loss: 0.00003125
Iteration 28/1000 | Loss: 0.00003124
Iteration 29/1000 | Loss: 0.00003123
Iteration 30/1000 | Loss: 0.00003123
Iteration 31/1000 | Loss: 0.00003123
Iteration 32/1000 | Loss: 0.00003122
Iteration 33/1000 | Loss: 0.00003121
Iteration 34/1000 | Loss: 0.00003118
Iteration 35/1000 | Loss: 0.00003118
Iteration 36/1000 | Loss: 0.00003118
Iteration 37/1000 | Loss: 0.00003117
Iteration 38/1000 | Loss: 0.00003117
Iteration 39/1000 | Loss: 0.00003111
Iteration 40/1000 | Loss: 0.00003111
Iteration 41/1000 | Loss: 0.00003109
Iteration 42/1000 | Loss: 0.00003109
Iteration 43/1000 | Loss: 0.00003108
Iteration 44/1000 | Loss: 0.00003108
Iteration 45/1000 | Loss: 0.00003108
Iteration 46/1000 | Loss: 0.00003107
Iteration 47/1000 | Loss: 0.00003107
Iteration 48/1000 | Loss: 0.00003107
Iteration 49/1000 | Loss: 0.00003106
Iteration 50/1000 | Loss: 0.00003106
Iteration 51/1000 | Loss: 0.00003105
Iteration 52/1000 | Loss: 0.00003105
Iteration 53/1000 | Loss: 0.00003105
Iteration 54/1000 | Loss: 0.00003105
Iteration 55/1000 | Loss: 0.00003105
Iteration 56/1000 | Loss: 0.00003104
Iteration 57/1000 | Loss: 0.00003104
Iteration 58/1000 | Loss: 0.00003104
Iteration 59/1000 | Loss: 0.00003104
Iteration 60/1000 | Loss: 0.00003103
Iteration 61/1000 | Loss: 0.00003103
Iteration 62/1000 | Loss: 0.00003103
Iteration 63/1000 | Loss: 0.00003102
Iteration 64/1000 | Loss: 0.00003102
Iteration 65/1000 | Loss: 0.00003102
Iteration 66/1000 | Loss: 0.00003102
Iteration 67/1000 | Loss: 0.00003101
Iteration 68/1000 | Loss: 0.00003101
Iteration 69/1000 | Loss: 0.00003101
Iteration 70/1000 | Loss: 0.00003101
Iteration 71/1000 | Loss: 0.00003100
Iteration 72/1000 | Loss: 0.00003100
Iteration 73/1000 | Loss: 0.00003099
Iteration 74/1000 | Loss: 0.00003099
Iteration 75/1000 | Loss: 0.00003099
Iteration 76/1000 | Loss: 0.00003099
Iteration 77/1000 | Loss: 0.00003099
Iteration 78/1000 | Loss: 0.00003099
Iteration 79/1000 | Loss: 0.00003099
Iteration 80/1000 | Loss: 0.00003099
Iteration 81/1000 | Loss: 0.00003099
Iteration 82/1000 | Loss: 0.00003099
Iteration 83/1000 | Loss: 0.00003099
Iteration 84/1000 | Loss: 0.00003098
Iteration 85/1000 | Loss: 0.00003098
Iteration 86/1000 | Loss: 0.00003098
Iteration 87/1000 | Loss: 0.00003098
Iteration 88/1000 | Loss: 0.00003097
Iteration 89/1000 | Loss: 0.00003097
Iteration 90/1000 | Loss: 0.00003097
Iteration 91/1000 | Loss: 0.00003097
Iteration 92/1000 | Loss: 0.00003097
Iteration 93/1000 | Loss: 0.00003097
Iteration 94/1000 | Loss: 0.00003097
Iteration 95/1000 | Loss: 0.00003096
Iteration 96/1000 | Loss: 0.00003096
Iteration 97/1000 | Loss: 0.00003096
Iteration 98/1000 | Loss: 0.00003096
Iteration 99/1000 | Loss: 0.00003096
Iteration 100/1000 | Loss: 0.00003096
Iteration 101/1000 | Loss: 0.00003096
Iteration 102/1000 | Loss: 0.00003096
Iteration 103/1000 | Loss: 0.00003096
Iteration 104/1000 | Loss: 0.00003096
Iteration 105/1000 | Loss: 0.00003096
Iteration 106/1000 | Loss: 0.00003096
Iteration 107/1000 | Loss: 0.00003095
Iteration 108/1000 | Loss: 0.00003095
Iteration 109/1000 | Loss: 0.00003095
Iteration 110/1000 | Loss: 0.00003095
Iteration 111/1000 | Loss: 0.00003095
Iteration 112/1000 | Loss: 0.00003095
Iteration 113/1000 | Loss: 0.00003095
Iteration 114/1000 | Loss: 0.00003095
Iteration 115/1000 | Loss: 0.00003094
Iteration 116/1000 | Loss: 0.00003094
Iteration 117/1000 | Loss: 0.00003094
Iteration 118/1000 | Loss: 0.00003094
Iteration 119/1000 | Loss: 0.00003094
Iteration 120/1000 | Loss: 0.00003094
Iteration 121/1000 | Loss: 0.00003094
Iteration 122/1000 | Loss: 0.00003094
Iteration 123/1000 | Loss: 0.00003094
Iteration 124/1000 | Loss: 0.00003094
Iteration 125/1000 | Loss: 0.00003094
Iteration 126/1000 | Loss: 0.00003094
Iteration 127/1000 | Loss: 0.00003094
Iteration 128/1000 | Loss: 0.00003094
Iteration 129/1000 | Loss: 0.00003094
Iteration 130/1000 | Loss: 0.00003094
Iteration 131/1000 | Loss: 0.00003094
Iteration 132/1000 | Loss: 0.00003094
Iteration 133/1000 | Loss: 0.00003094
Iteration 134/1000 | Loss: 0.00003094
Iteration 135/1000 | Loss: 0.00003094
Iteration 136/1000 | Loss: 0.00003094
Iteration 137/1000 | Loss: 0.00003094
Iteration 138/1000 | Loss: 0.00003094
Iteration 139/1000 | Loss: 0.00003094
Iteration 140/1000 | Loss: 0.00003094
Iteration 141/1000 | Loss: 0.00003094
Iteration 142/1000 | Loss: 0.00003094
Iteration 143/1000 | Loss: 0.00003094
Iteration 144/1000 | Loss: 0.00003094
Iteration 145/1000 | Loss: 0.00003094
Iteration 146/1000 | Loss: 0.00003094
Iteration 147/1000 | Loss: 0.00003094
Iteration 148/1000 | Loss: 0.00003094
Iteration 149/1000 | Loss: 0.00003094
Iteration 150/1000 | Loss: 0.00003094
Iteration 151/1000 | Loss: 0.00003094
Iteration 152/1000 | Loss: 0.00003094
Iteration 153/1000 | Loss: 0.00003094
Iteration 154/1000 | Loss: 0.00003094
Iteration 155/1000 | Loss: 0.00003094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.09391034534201e-05, 3.09391034534201e-05, 3.09391034534201e-05, 3.09391034534201e-05, 3.09391034534201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.09391034534201e-05

Optimization complete. Final v2v error: 4.545044898986816 mm

Highest mean error: 6.041025161743164 mm for frame 55

Lowest mean error: 3.4676787853240967 mm for frame 192

Saving results

Total time: 50.47083377838135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828885
Iteration 2/25 | Loss: 0.00128044
Iteration 3/25 | Loss: 0.00113041
Iteration 4/25 | Loss: 0.00111168
Iteration 5/25 | Loss: 0.00110688
Iteration 6/25 | Loss: 0.00110688
Iteration 7/25 | Loss: 0.00110688
Iteration 8/25 | Loss: 0.00110688
Iteration 9/25 | Loss: 0.00110688
Iteration 10/25 | Loss: 0.00110688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011068794410675764, 0.0011068794410675764, 0.0011068794410675764, 0.0011068794410675764, 0.0011068794410675764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011068794410675764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96468449
Iteration 2/25 | Loss: 0.00045421
Iteration 3/25 | Loss: 0.00045420
Iteration 4/25 | Loss: 0.00045420
Iteration 5/25 | Loss: 0.00045420
Iteration 6/25 | Loss: 0.00045420
Iteration 7/25 | Loss: 0.00045420
Iteration 8/25 | Loss: 0.00045420
Iteration 9/25 | Loss: 0.00045420
Iteration 10/25 | Loss: 0.00045420
Iteration 11/25 | Loss: 0.00045420
Iteration 12/25 | Loss: 0.00045420
Iteration 13/25 | Loss: 0.00045420
Iteration 14/25 | Loss: 0.00045420
Iteration 15/25 | Loss: 0.00045420
Iteration 16/25 | Loss: 0.00045420
Iteration 17/25 | Loss: 0.00045420
Iteration 18/25 | Loss: 0.00045420
Iteration 19/25 | Loss: 0.00045420
Iteration 20/25 | Loss: 0.00045420
Iteration 21/25 | Loss: 0.00045420
Iteration 22/25 | Loss: 0.00045420
Iteration 23/25 | Loss: 0.00045420
Iteration 24/25 | Loss: 0.00045420
Iteration 25/25 | Loss: 0.00045420

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045420
Iteration 2/1000 | Loss: 0.00003123
Iteration 3/1000 | Loss: 0.00002515
Iteration 4/1000 | Loss: 0.00002357
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002083
Iteration 8/1000 | Loss: 0.00002043
Iteration 9/1000 | Loss: 0.00002002
Iteration 10/1000 | Loss: 0.00001978
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001935
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001925
Iteration 20/1000 | Loss: 0.00001919
Iteration 21/1000 | Loss: 0.00001918
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001916
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001916
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001916
Iteration 29/1000 | Loss: 0.00001916
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001916
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001916
Iteration 34/1000 | Loss: 0.00001915
Iteration 35/1000 | Loss: 0.00001915
Iteration 36/1000 | Loss: 0.00001915
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001914
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001914
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001913
Iteration 43/1000 | Loss: 0.00001913
Iteration 44/1000 | Loss: 0.00001913
Iteration 45/1000 | Loss: 0.00001912
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001911
Iteration 49/1000 | Loss: 0.00001911
Iteration 50/1000 | Loss: 0.00001910
Iteration 51/1000 | Loss: 0.00001910
Iteration 52/1000 | Loss: 0.00001910
Iteration 53/1000 | Loss: 0.00001910
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00001910
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001909
Iteration 58/1000 | Loss: 0.00001909
Iteration 59/1000 | Loss: 0.00001909
Iteration 60/1000 | Loss: 0.00001909
Iteration 61/1000 | Loss: 0.00001909
Iteration 62/1000 | Loss: 0.00001909
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001909
Iteration 73/1000 | Loss: 0.00001909
Iteration 74/1000 | Loss: 0.00001909
Iteration 75/1000 | Loss: 0.00001909
Iteration 76/1000 | Loss: 0.00001909
Iteration 77/1000 | Loss: 0.00001909
Iteration 78/1000 | Loss: 0.00001909
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001909
Iteration 82/1000 | Loss: 0.00001909
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001909
Iteration 85/1000 | Loss: 0.00001909
Iteration 86/1000 | Loss: 0.00001909
Iteration 87/1000 | Loss: 0.00001909
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001909
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00001909
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9087543478235602e-05, 1.9087543478235602e-05, 1.9087543478235602e-05, 1.9087543478235602e-05, 1.9087543478235602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9087543478235602e-05

Optimization complete. Final v2v error: 3.7617764472961426 mm

Highest mean error: 4.394223213195801 mm for frame 239

Lowest mean error: 3.345407485961914 mm for frame 3

Saving results

Total time: 36.341301679611206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060101
Iteration 2/25 | Loss: 0.00326135
Iteration 3/25 | Loss: 0.00226411
Iteration 4/25 | Loss: 0.00215340
Iteration 5/25 | Loss: 0.00175066
Iteration 6/25 | Loss: 0.00139640
Iteration 7/25 | Loss: 0.00129348
Iteration 8/25 | Loss: 0.00124413
Iteration 9/25 | Loss: 0.00115217
Iteration 10/25 | Loss: 0.00113331
Iteration 11/25 | Loss: 0.00112149
Iteration 12/25 | Loss: 0.00112048
Iteration 13/25 | Loss: 0.00112016
Iteration 14/25 | Loss: 0.00112009
Iteration 15/25 | Loss: 0.00112008
Iteration 16/25 | Loss: 0.00112007
Iteration 17/25 | Loss: 0.00112007
Iteration 18/25 | Loss: 0.00112007
Iteration 19/25 | Loss: 0.00112007
Iteration 20/25 | Loss: 0.00112007
Iteration 21/25 | Loss: 0.00112007
Iteration 22/25 | Loss: 0.00112007
Iteration 23/25 | Loss: 0.00112006
Iteration 24/25 | Loss: 0.00112006
Iteration 25/25 | Loss: 0.00112006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44498754
Iteration 2/25 | Loss: 0.00161837
Iteration 3/25 | Loss: 0.00093978
Iteration 4/25 | Loss: 0.00093975
Iteration 5/25 | Loss: 0.00093975
Iteration 6/25 | Loss: 0.00093975
Iteration 7/25 | Loss: 0.00093975
Iteration 8/25 | Loss: 0.00093975
Iteration 9/25 | Loss: 0.00093974
Iteration 10/25 | Loss: 0.00093974
Iteration 11/25 | Loss: 0.00093974
Iteration 12/25 | Loss: 0.00093974
Iteration 13/25 | Loss: 0.00093974
Iteration 14/25 | Loss: 0.00093974
Iteration 15/25 | Loss: 0.00093974
Iteration 16/25 | Loss: 0.00093974
Iteration 17/25 | Loss: 0.00093974
Iteration 18/25 | Loss: 0.00093974
Iteration 19/25 | Loss: 0.00093974
Iteration 20/25 | Loss: 0.00093974
Iteration 21/25 | Loss: 0.00093974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009397435933351517, 0.0009397435933351517, 0.0009397435933351517, 0.0009397435933351517, 0.0009397435933351517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009397435933351517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093974
Iteration 2/1000 | Loss: 0.00081566
Iteration 3/1000 | Loss: 0.00006071
Iteration 4/1000 | Loss: 0.00003553
Iteration 5/1000 | Loss: 0.00004174
Iteration 6/1000 | Loss: 0.00006874
Iteration 7/1000 | Loss: 0.00003133
Iteration 8/1000 | Loss: 0.00003055
Iteration 9/1000 | Loss: 0.00002995
Iteration 10/1000 | Loss: 0.00002912
Iteration 11/1000 | Loss: 0.00339107
Iteration 12/1000 | Loss: 0.00005362
Iteration 13/1000 | Loss: 0.00003682
Iteration 14/1000 | Loss: 0.00001907
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001633
Iteration 17/1000 | Loss: 0.00001540
Iteration 18/1000 | Loss: 0.00001463
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001246
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001228
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001228
Iteration 154/1000 | Loss: 0.00001228
Iteration 155/1000 | Loss: 0.00001228
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001228
Iteration 163/1000 | Loss: 0.00001228
Iteration 164/1000 | Loss: 0.00001228
Iteration 165/1000 | Loss: 0.00001228
Iteration 166/1000 | Loss: 0.00001228
Iteration 167/1000 | Loss: 0.00001228
Iteration 168/1000 | Loss: 0.00001228
Iteration 169/1000 | Loss: 0.00001228
Iteration 170/1000 | Loss: 0.00001228
Iteration 171/1000 | Loss: 0.00001228
Iteration 172/1000 | Loss: 0.00001228
Iteration 173/1000 | Loss: 0.00001228
Iteration 174/1000 | Loss: 0.00001228
Iteration 175/1000 | Loss: 0.00001228
Iteration 176/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.2282234820304438e-05, 1.2282234820304438e-05, 1.2282234820304438e-05, 1.2282234820304438e-05, 1.2282234820304438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2282234820304438e-05

Optimization complete. Final v2v error: 2.9884777069091797 mm

Highest mean error: 3.2691428661346436 mm for frame 87

Lowest mean error: 2.802402973175049 mm for frame 64

Saving results

Total time: 68.72161388397217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002286
Iteration 2/25 | Loss: 0.01002286
Iteration 3/25 | Loss: 0.00259088
Iteration 4/25 | Loss: 0.00190763
Iteration 5/25 | Loss: 0.00176427
Iteration 6/25 | Loss: 0.00179848
Iteration 7/25 | Loss: 0.00179891
Iteration 8/25 | Loss: 0.00159814
Iteration 9/25 | Loss: 0.00149294
Iteration 10/25 | Loss: 0.00146779
Iteration 11/25 | Loss: 0.00147089
Iteration 12/25 | Loss: 0.00147958
Iteration 13/25 | Loss: 0.00140930
Iteration 14/25 | Loss: 0.00136411
Iteration 15/25 | Loss: 0.00135075
Iteration 16/25 | Loss: 0.00133771
Iteration 17/25 | Loss: 0.00132811
Iteration 18/25 | Loss: 0.00132018
Iteration 19/25 | Loss: 0.00131731
Iteration 20/25 | Loss: 0.00131344
Iteration 21/25 | Loss: 0.00130775
Iteration 22/25 | Loss: 0.00131204
Iteration 23/25 | Loss: 0.00130962
Iteration 24/25 | Loss: 0.00130811
Iteration 25/25 | Loss: 0.00130769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33884132
Iteration 2/25 | Loss: 0.00193382
Iteration 3/25 | Loss: 0.00187538
Iteration 4/25 | Loss: 0.00187537
Iteration 5/25 | Loss: 0.00187537
Iteration 6/25 | Loss: 0.00187537
Iteration 7/25 | Loss: 0.00187537
Iteration 8/25 | Loss: 0.00187537
Iteration 9/25 | Loss: 0.00187537
Iteration 10/25 | Loss: 0.00187537
Iteration 11/25 | Loss: 0.00187537
Iteration 12/25 | Loss: 0.00187537
Iteration 13/25 | Loss: 0.00187537
Iteration 14/25 | Loss: 0.00187537
Iteration 15/25 | Loss: 0.00187537
Iteration 16/25 | Loss: 0.00187537
Iteration 17/25 | Loss: 0.00187537
Iteration 18/25 | Loss: 0.00187537
Iteration 19/25 | Loss: 0.00187537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018753709737211466, 0.0018753709737211466, 0.0018753709737211466, 0.0018753709737211466, 0.0018753709737211466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018753709737211466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187537
Iteration 2/1000 | Loss: 0.00102063
Iteration 3/1000 | Loss: 0.00078091
Iteration 4/1000 | Loss: 0.00035155
Iteration 5/1000 | Loss: 0.00028415
Iteration 6/1000 | Loss: 0.00025492
Iteration 7/1000 | Loss: 0.00018236
Iteration 8/1000 | Loss: 0.00057028
Iteration 9/1000 | Loss: 0.00057736
Iteration 10/1000 | Loss: 0.00023498
Iteration 11/1000 | Loss: 0.00023725
Iteration 12/1000 | Loss: 0.00043341
Iteration 13/1000 | Loss: 0.00014525
Iteration 14/1000 | Loss: 0.00013775
Iteration 15/1000 | Loss: 0.00045099
Iteration 16/1000 | Loss: 0.00010971
Iteration 17/1000 | Loss: 0.00047859
Iteration 18/1000 | Loss: 0.00630870
Iteration 19/1000 | Loss: 0.00211411
Iteration 20/1000 | Loss: 0.00040637
Iteration 21/1000 | Loss: 0.00032960
Iteration 22/1000 | Loss: 0.00014410
Iteration 23/1000 | Loss: 0.00018711
Iteration 24/1000 | Loss: 0.00055130
Iteration 25/1000 | Loss: 0.00015815
Iteration 26/1000 | Loss: 0.00041737
Iteration 27/1000 | Loss: 0.00020456
Iteration 28/1000 | Loss: 0.00010565
Iteration 29/1000 | Loss: 0.00019488
Iteration 30/1000 | Loss: 0.00037862
Iteration 31/1000 | Loss: 0.00005574
Iteration 32/1000 | Loss: 0.00004197
Iteration 33/1000 | Loss: 0.00003423
Iteration 34/1000 | Loss: 0.00005097
Iteration 35/1000 | Loss: 0.00002717
Iteration 36/1000 | Loss: 0.00002499
Iteration 37/1000 | Loss: 0.00019499
Iteration 38/1000 | Loss: 0.00006945
Iteration 39/1000 | Loss: 0.00020152
Iteration 40/1000 | Loss: 0.00002819
Iteration 41/1000 | Loss: 0.00002375
Iteration 42/1000 | Loss: 0.00029981
Iteration 43/1000 | Loss: 0.00034486
Iteration 44/1000 | Loss: 0.00002803
Iteration 45/1000 | Loss: 0.00017726
Iteration 46/1000 | Loss: 0.00021784
Iteration 47/1000 | Loss: 0.00025063
Iteration 48/1000 | Loss: 0.00009452
Iteration 49/1000 | Loss: 0.00015063
Iteration 50/1000 | Loss: 0.00003748
Iteration 51/1000 | Loss: 0.00017190
Iteration 52/1000 | Loss: 0.00033923
Iteration 53/1000 | Loss: 0.00015562
Iteration 54/1000 | Loss: 0.00019749
Iteration 55/1000 | Loss: 0.00002712
Iteration 56/1000 | Loss: 0.00002330
Iteration 57/1000 | Loss: 0.00002097
Iteration 58/1000 | Loss: 0.00022035
Iteration 59/1000 | Loss: 0.00016950
Iteration 60/1000 | Loss: 0.00002772
Iteration 61/1000 | Loss: 0.00004137
Iteration 62/1000 | Loss: 0.00002137
Iteration 63/1000 | Loss: 0.00003569
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00002433
Iteration 67/1000 | Loss: 0.00002910
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001748
Iteration 70/1000 | Loss: 0.00001737
Iteration 71/1000 | Loss: 0.00012555
Iteration 72/1000 | Loss: 0.00013736
Iteration 73/1000 | Loss: 0.00002576
Iteration 74/1000 | Loss: 0.00017568
Iteration 75/1000 | Loss: 0.00002102
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00002072
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001848
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001566
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001542
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001542
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00002211
Iteration 120/1000 | Loss: 0.00001565
Iteration 121/1000 | Loss: 0.00001537
Iteration 122/1000 | Loss: 0.00001537
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001535
Iteration 127/1000 | Loss: 0.00001534
Iteration 128/1000 | Loss: 0.00001565
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00002345
Iteration 132/1000 | Loss: 0.00001574
Iteration 133/1000 | Loss: 0.00001531
Iteration 134/1000 | Loss: 0.00001520
Iteration 135/1000 | Loss: 0.00001520
Iteration 136/1000 | Loss: 0.00001520
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001520
Iteration 139/1000 | Loss: 0.00001520
Iteration 140/1000 | Loss: 0.00001520
Iteration 141/1000 | Loss: 0.00001520
Iteration 142/1000 | Loss: 0.00001520
Iteration 143/1000 | Loss: 0.00001520
Iteration 144/1000 | Loss: 0.00001520
Iteration 145/1000 | Loss: 0.00001520
Iteration 146/1000 | Loss: 0.00001520
Iteration 147/1000 | Loss: 0.00001520
Iteration 148/1000 | Loss: 0.00001520
Iteration 149/1000 | Loss: 0.00001520
Iteration 150/1000 | Loss: 0.00001520
Iteration 151/1000 | Loss: 0.00001520
Iteration 152/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.5198110304481816e-05, 1.5198110304481816e-05, 1.5198110304481816e-05, 1.5198110304481816e-05, 1.5198110304481816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5198110304481816e-05

Optimization complete. Final v2v error: 3.0770630836486816 mm

Highest mean error: 13.535805702209473 mm for frame 3

Lowest mean error: 2.7875888347625732 mm for frame 221

Saving results

Total time: 195.72382974624634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908808
Iteration 2/25 | Loss: 0.00161029
Iteration 3/25 | Loss: 0.00134272
Iteration 4/25 | Loss: 0.00131541
Iteration 5/25 | Loss: 0.00131068
Iteration 6/25 | Loss: 0.00130927
Iteration 7/25 | Loss: 0.00130887
Iteration 8/25 | Loss: 0.00130887
Iteration 9/25 | Loss: 0.00130887
Iteration 10/25 | Loss: 0.00130887
Iteration 11/25 | Loss: 0.00130887
Iteration 12/25 | Loss: 0.00130887
Iteration 13/25 | Loss: 0.00130887
Iteration 14/25 | Loss: 0.00130887
Iteration 15/25 | Loss: 0.00130887
Iteration 16/25 | Loss: 0.00130887
Iteration 17/25 | Loss: 0.00130887
Iteration 18/25 | Loss: 0.00130887
Iteration 19/25 | Loss: 0.00130887
Iteration 20/25 | Loss: 0.00130887
Iteration 21/25 | Loss: 0.00130887
Iteration 22/25 | Loss: 0.00130887
Iteration 23/25 | Loss: 0.00130887
Iteration 24/25 | Loss: 0.00130887
Iteration 25/25 | Loss: 0.00130887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55572343
Iteration 2/25 | Loss: 0.00099080
Iteration 3/25 | Loss: 0.00099080
Iteration 4/25 | Loss: 0.00099080
Iteration 5/25 | Loss: 0.00099080
Iteration 6/25 | Loss: 0.00099080
Iteration 7/25 | Loss: 0.00099080
Iteration 8/25 | Loss: 0.00099080
Iteration 9/25 | Loss: 0.00099080
Iteration 10/25 | Loss: 0.00099080
Iteration 11/25 | Loss: 0.00099080
Iteration 12/25 | Loss: 0.00099080
Iteration 13/25 | Loss: 0.00099080
Iteration 14/25 | Loss: 0.00099080
Iteration 15/25 | Loss: 0.00099080
Iteration 16/25 | Loss: 0.00099080
Iteration 17/25 | Loss: 0.00099080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009908020729199052, 0.0009908020729199052, 0.0009908020729199052, 0.0009908020729199052, 0.0009908020729199052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009908020729199052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099080
Iteration 2/1000 | Loss: 0.00006931
Iteration 3/1000 | Loss: 0.00003837
Iteration 4/1000 | Loss: 0.00002901
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002571
Iteration 7/1000 | Loss: 0.00002509
Iteration 8/1000 | Loss: 0.00002464
Iteration 9/1000 | Loss: 0.00002411
Iteration 10/1000 | Loss: 0.00002383
Iteration 11/1000 | Loss: 0.00002360
Iteration 12/1000 | Loss: 0.00002343
Iteration 13/1000 | Loss: 0.00002332
Iteration 14/1000 | Loss: 0.00002324
Iteration 15/1000 | Loss: 0.00002319
Iteration 16/1000 | Loss: 0.00002315
Iteration 17/1000 | Loss: 0.00002312
Iteration 18/1000 | Loss: 0.00002311
Iteration 19/1000 | Loss: 0.00002310
Iteration 20/1000 | Loss: 0.00002308
Iteration 21/1000 | Loss: 0.00002303
Iteration 22/1000 | Loss: 0.00002299
Iteration 23/1000 | Loss: 0.00002299
Iteration 24/1000 | Loss: 0.00002297
Iteration 25/1000 | Loss: 0.00002297
Iteration 26/1000 | Loss: 0.00002297
Iteration 27/1000 | Loss: 0.00002297
Iteration 28/1000 | Loss: 0.00002296
Iteration 29/1000 | Loss: 0.00002296
Iteration 30/1000 | Loss: 0.00002295
Iteration 31/1000 | Loss: 0.00002295
Iteration 32/1000 | Loss: 0.00002295
Iteration 33/1000 | Loss: 0.00002294
Iteration 34/1000 | Loss: 0.00002294
Iteration 35/1000 | Loss: 0.00002294
Iteration 36/1000 | Loss: 0.00002294
Iteration 37/1000 | Loss: 0.00002294
Iteration 38/1000 | Loss: 0.00002294
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00002294
Iteration 41/1000 | Loss: 0.00002294
Iteration 42/1000 | Loss: 0.00002294
Iteration 43/1000 | Loss: 0.00002294
Iteration 44/1000 | Loss: 0.00002293
Iteration 45/1000 | Loss: 0.00002293
Iteration 46/1000 | Loss: 0.00002293
Iteration 47/1000 | Loss: 0.00002293
Iteration 48/1000 | Loss: 0.00002293
Iteration 49/1000 | Loss: 0.00002292
Iteration 50/1000 | Loss: 0.00002292
Iteration 51/1000 | Loss: 0.00002292
Iteration 52/1000 | Loss: 0.00002291
Iteration 53/1000 | Loss: 0.00002291
Iteration 54/1000 | Loss: 0.00002291
Iteration 55/1000 | Loss: 0.00002291
Iteration 56/1000 | Loss: 0.00002291
Iteration 57/1000 | Loss: 0.00002291
Iteration 58/1000 | Loss: 0.00002291
Iteration 59/1000 | Loss: 0.00002291
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002291
Iteration 63/1000 | Loss: 0.00002291
Iteration 64/1000 | Loss: 0.00002291
Iteration 65/1000 | Loss: 0.00002291
Iteration 66/1000 | Loss: 0.00002291
Iteration 67/1000 | Loss: 0.00002291
Iteration 68/1000 | Loss: 0.00002291
Iteration 69/1000 | Loss: 0.00002291
Iteration 70/1000 | Loss: 0.00002291
Iteration 71/1000 | Loss: 0.00002291
Iteration 72/1000 | Loss: 0.00002291
Iteration 73/1000 | Loss: 0.00002291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.290508746227715e-05, 2.290508746227715e-05, 2.290508746227715e-05, 2.290508746227715e-05, 2.290508746227715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.290508746227715e-05

Optimization complete. Final v2v error: 4.028375625610352 mm

Highest mean error: 4.509978294372559 mm for frame 19

Lowest mean error: 3.6229944229125977 mm for frame 42

Saving results

Total time: 33.950252294540405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470209
Iteration 2/25 | Loss: 0.00129320
Iteration 3/25 | Loss: 0.00120704
Iteration 4/25 | Loss: 0.00120165
Iteration 5/25 | Loss: 0.00120016
Iteration 6/25 | Loss: 0.00120016
Iteration 7/25 | Loss: 0.00120016
Iteration 8/25 | Loss: 0.00120016
Iteration 9/25 | Loss: 0.00120016
Iteration 10/25 | Loss: 0.00120016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012001553550362587, 0.0012001553550362587, 0.0012001553550362587, 0.0012001553550362587, 0.0012001553550362587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012001553550362587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33074868
Iteration 2/25 | Loss: 0.00106502
Iteration 3/25 | Loss: 0.00106500
Iteration 4/25 | Loss: 0.00106500
Iteration 5/25 | Loss: 0.00106500
Iteration 6/25 | Loss: 0.00106500
Iteration 7/25 | Loss: 0.00106500
Iteration 8/25 | Loss: 0.00106499
Iteration 9/25 | Loss: 0.00106499
Iteration 10/25 | Loss: 0.00106499
Iteration 11/25 | Loss: 0.00106499
Iteration 12/25 | Loss: 0.00106499
Iteration 13/25 | Loss: 0.00106499
Iteration 14/25 | Loss: 0.00106499
Iteration 15/25 | Loss: 0.00106499
Iteration 16/25 | Loss: 0.00106499
Iteration 17/25 | Loss: 0.00106499
Iteration 18/25 | Loss: 0.00106499
Iteration 19/25 | Loss: 0.00106499
Iteration 20/25 | Loss: 0.00106499
Iteration 21/25 | Loss: 0.00106499
Iteration 22/25 | Loss: 0.00106499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001064994023181498, 0.001064994023181498, 0.001064994023181498, 0.001064994023181498, 0.001064994023181498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001064994023181498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106499
Iteration 2/1000 | Loss: 0.00003315
Iteration 3/1000 | Loss: 0.00002298
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00001748
Iteration 7/1000 | Loss: 0.00001694
Iteration 8/1000 | Loss: 0.00001651
Iteration 9/1000 | Loss: 0.00001619
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001565
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001555
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001540
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001528
Iteration 23/1000 | Loss: 0.00001525
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001516
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001512
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001503
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001502
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001497
Iteration 70/1000 | Loss: 0.00001497
Iteration 71/1000 | Loss: 0.00001496
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001492
Iteration 77/1000 | Loss: 0.00001492
Iteration 78/1000 | Loss: 0.00001492
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001488
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001488
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.4872416613798123e-05, 1.4872416613798123e-05, 1.4872416613798123e-05, 1.4872416613798123e-05, 1.4872416613798123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4872416613798123e-05

Optimization complete. Final v2v error: 3.2504618167877197 mm

Highest mean error: 3.671731948852539 mm for frame 71

Lowest mean error: 2.8389744758605957 mm for frame 189

Saving results

Total time: 40.84675335884094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684896
Iteration 2/25 | Loss: 0.00149326
Iteration 3/25 | Loss: 0.00122530
Iteration 4/25 | Loss: 0.00118477
Iteration 5/25 | Loss: 0.00117753
Iteration 6/25 | Loss: 0.00117627
Iteration 7/25 | Loss: 0.00116335
Iteration 8/25 | Loss: 0.00115914
Iteration 9/25 | Loss: 0.00115402
Iteration 10/25 | Loss: 0.00114812
Iteration 11/25 | Loss: 0.00114644
Iteration 12/25 | Loss: 0.00114633
Iteration 13/25 | Loss: 0.00114633
Iteration 14/25 | Loss: 0.00114632
Iteration 15/25 | Loss: 0.00114632
Iteration 16/25 | Loss: 0.00114632
Iteration 17/25 | Loss: 0.00114632
Iteration 18/25 | Loss: 0.00114632
Iteration 19/25 | Loss: 0.00114632
Iteration 20/25 | Loss: 0.00114632
Iteration 21/25 | Loss: 0.00114632
Iteration 22/25 | Loss: 0.00114632
Iteration 23/25 | Loss: 0.00114632
Iteration 24/25 | Loss: 0.00114632
Iteration 25/25 | Loss: 0.00114632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06565285
Iteration 2/25 | Loss: 0.00103781
Iteration 3/25 | Loss: 0.00103756
Iteration 4/25 | Loss: 0.00103756
Iteration 5/25 | Loss: 0.00103756
Iteration 6/25 | Loss: 0.00103756
Iteration 7/25 | Loss: 0.00103756
Iteration 8/25 | Loss: 0.00103756
Iteration 9/25 | Loss: 0.00103756
Iteration 10/25 | Loss: 0.00103756
Iteration 11/25 | Loss: 0.00103756
Iteration 12/25 | Loss: 0.00103756
Iteration 13/25 | Loss: 0.00103756
Iteration 14/25 | Loss: 0.00103756
Iteration 15/25 | Loss: 0.00103756
Iteration 16/25 | Loss: 0.00103756
Iteration 17/25 | Loss: 0.00103756
Iteration 18/25 | Loss: 0.00103756
Iteration 19/25 | Loss: 0.00103756
Iteration 20/25 | Loss: 0.00103756
Iteration 21/25 | Loss: 0.00103756
Iteration 22/25 | Loss: 0.00103756
Iteration 23/25 | Loss: 0.00103756
Iteration 24/25 | Loss: 0.00103756
Iteration 25/25 | Loss: 0.00103756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103756
Iteration 2/1000 | Loss: 0.00005831
Iteration 3/1000 | Loss: 0.00003681
Iteration 4/1000 | Loss: 0.00002990
Iteration 5/1000 | Loss: 0.00002737
Iteration 6/1000 | Loss: 0.00002572
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002370
Iteration 9/1000 | Loss: 0.00002312
Iteration 10/1000 | Loss: 0.00002269
Iteration 11/1000 | Loss: 0.00002232
Iteration 12/1000 | Loss: 0.00002202
Iteration 13/1000 | Loss: 0.00002179
Iteration 14/1000 | Loss: 0.00002164
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002147
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002135
Iteration 21/1000 | Loss: 0.00002134
Iteration 22/1000 | Loss: 0.00002133
Iteration 23/1000 | Loss: 0.00002129
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002121
Iteration 26/1000 | Loss: 0.00002121
Iteration 27/1000 | Loss: 0.00002121
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002120
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002119
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002116
Iteration 36/1000 | Loss: 0.00002115
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00002108
Iteration 47/1000 | Loss: 0.00002104
Iteration 48/1000 | Loss: 0.00002103
Iteration 49/1000 | Loss: 0.00002100
Iteration 50/1000 | Loss: 0.00002098
Iteration 51/1000 | Loss: 0.00002098
Iteration 52/1000 | Loss: 0.00002097
Iteration 53/1000 | Loss: 0.00002095
Iteration 54/1000 | Loss: 0.00002095
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002093
Iteration 60/1000 | Loss: 0.00002093
Iteration 61/1000 | Loss: 0.00002093
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002090
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002084
Iteration 81/1000 | Loss: 0.00002084
Iteration 82/1000 | Loss: 0.00002084
Iteration 83/1000 | Loss: 0.00002084
Iteration 84/1000 | Loss: 0.00002084
Iteration 85/1000 | Loss: 0.00002083
Iteration 86/1000 | Loss: 0.00002083
Iteration 87/1000 | Loss: 0.00002083
Iteration 88/1000 | Loss: 0.00002082
Iteration 89/1000 | Loss: 0.00002082
Iteration 90/1000 | Loss: 0.00002082
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00002081
Iteration 93/1000 | Loss: 0.00002080
Iteration 94/1000 | Loss: 0.00002080
Iteration 95/1000 | Loss: 0.00002080
Iteration 96/1000 | Loss: 0.00002079
Iteration 97/1000 | Loss: 0.00002079
Iteration 98/1000 | Loss: 0.00002079
Iteration 99/1000 | Loss: 0.00002078
Iteration 100/1000 | Loss: 0.00002078
Iteration 101/1000 | Loss: 0.00002077
Iteration 102/1000 | Loss: 0.00002077
Iteration 103/1000 | Loss: 0.00002077
Iteration 104/1000 | Loss: 0.00002077
Iteration 105/1000 | Loss: 0.00002076
Iteration 106/1000 | Loss: 0.00002076
Iteration 107/1000 | Loss: 0.00002076
Iteration 108/1000 | Loss: 0.00002076
Iteration 109/1000 | Loss: 0.00002075
Iteration 110/1000 | Loss: 0.00002075
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002074
Iteration 114/1000 | Loss: 0.00002073
Iteration 115/1000 | Loss: 0.00002073
Iteration 116/1000 | Loss: 0.00002073
Iteration 117/1000 | Loss: 0.00002073
Iteration 118/1000 | Loss: 0.00002072
Iteration 119/1000 | Loss: 0.00002072
Iteration 120/1000 | Loss: 0.00002072
Iteration 121/1000 | Loss: 0.00002071
Iteration 122/1000 | Loss: 0.00002071
Iteration 123/1000 | Loss: 0.00002071
Iteration 124/1000 | Loss: 0.00002071
Iteration 125/1000 | Loss: 0.00002070
Iteration 126/1000 | Loss: 0.00002070
Iteration 127/1000 | Loss: 0.00002069
Iteration 128/1000 | Loss: 0.00002069
Iteration 129/1000 | Loss: 0.00002069
Iteration 130/1000 | Loss: 0.00002069
Iteration 131/1000 | Loss: 0.00002069
Iteration 132/1000 | Loss: 0.00002069
Iteration 133/1000 | Loss: 0.00002069
Iteration 134/1000 | Loss: 0.00002068
Iteration 135/1000 | Loss: 0.00002068
Iteration 136/1000 | Loss: 0.00002067
Iteration 137/1000 | Loss: 0.00002067
Iteration 138/1000 | Loss: 0.00002067
Iteration 139/1000 | Loss: 0.00002067
Iteration 140/1000 | Loss: 0.00002067
Iteration 141/1000 | Loss: 0.00002067
Iteration 142/1000 | Loss: 0.00002066
Iteration 143/1000 | Loss: 0.00002066
Iteration 144/1000 | Loss: 0.00002066
Iteration 145/1000 | Loss: 0.00002066
Iteration 146/1000 | Loss: 0.00002065
Iteration 147/1000 | Loss: 0.00002065
Iteration 148/1000 | Loss: 0.00002065
Iteration 149/1000 | Loss: 0.00002065
Iteration 150/1000 | Loss: 0.00002065
Iteration 151/1000 | Loss: 0.00002064
Iteration 152/1000 | Loss: 0.00002064
Iteration 153/1000 | Loss: 0.00002064
Iteration 154/1000 | Loss: 0.00002064
Iteration 155/1000 | Loss: 0.00002064
Iteration 156/1000 | Loss: 0.00002064
Iteration 157/1000 | Loss: 0.00002063
Iteration 158/1000 | Loss: 0.00002063
Iteration 159/1000 | Loss: 0.00002063
Iteration 160/1000 | Loss: 0.00002063
Iteration 161/1000 | Loss: 0.00002063
Iteration 162/1000 | Loss: 0.00002063
Iteration 163/1000 | Loss: 0.00002063
Iteration 164/1000 | Loss: 0.00002063
Iteration 165/1000 | Loss: 0.00002063
Iteration 166/1000 | Loss: 0.00002063
Iteration 167/1000 | Loss: 0.00002063
Iteration 168/1000 | Loss: 0.00002063
Iteration 169/1000 | Loss: 0.00002062
Iteration 170/1000 | Loss: 0.00002062
Iteration 171/1000 | Loss: 0.00002062
Iteration 172/1000 | Loss: 0.00002062
Iteration 173/1000 | Loss: 0.00002062
Iteration 174/1000 | Loss: 0.00002062
Iteration 175/1000 | Loss: 0.00002062
Iteration 176/1000 | Loss: 0.00002061
Iteration 177/1000 | Loss: 0.00002061
Iteration 178/1000 | Loss: 0.00002061
Iteration 179/1000 | Loss: 0.00002060
Iteration 180/1000 | Loss: 0.00002060
Iteration 181/1000 | Loss: 0.00002060
Iteration 182/1000 | Loss: 0.00002060
Iteration 183/1000 | Loss: 0.00002060
Iteration 184/1000 | Loss: 0.00002060
Iteration 185/1000 | Loss: 0.00002060
Iteration 186/1000 | Loss: 0.00002060
Iteration 187/1000 | Loss: 0.00002060
Iteration 188/1000 | Loss: 0.00002060
Iteration 189/1000 | Loss: 0.00002060
Iteration 190/1000 | Loss: 0.00002060
Iteration 191/1000 | Loss: 0.00002060
Iteration 192/1000 | Loss: 0.00002060
Iteration 193/1000 | Loss: 0.00002060
Iteration 194/1000 | Loss: 0.00002060
Iteration 195/1000 | Loss: 0.00002060
Iteration 196/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.0601764845196158e-05, 2.0601764845196158e-05, 2.0601764845196158e-05, 2.0601764845196158e-05, 2.0601764845196158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0601764845196158e-05

Optimization complete. Final v2v error: 3.6361653804779053 mm

Highest mean error: 5.891295909881592 mm for frame 131

Lowest mean error: 2.729456901550293 mm for frame 205

Saving results

Total time: 67.21092963218689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680359
Iteration 2/25 | Loss: 0.00129675
Iteration 3/25 | Loss: 0.00116962
Iteration 4/25 | Loss: 0.00110141
Iteration 5/25 | Loss: 0.00109364
Iteration 6/25 | Loss: 0.00109518
Iteration 7/25 | Loss: 0.00108796
Iteration 8/25 | Loss: 0.00108970
Iteration 9/25 | Loss: 0.00108430
Iteration 10/25 | Loss: 0.00108427
Iteration 11/25 | Loss: 0.00108427
Iteration 12/25 | Loss: 0.00108427
Iteration 13/25 | Loss: 0.00108427
Iteration 14/25 | Loss: 0.00108427
Iteration 15/25 | Loss: 0.00108427
Iteration 16/25 | Loss: 0.00108427
Iteration 17/25 | Loss: 0.00108427
Iteration 18/25 | Loss: 0.00108427
Iteration 19/25 | Loss: 0.00108426
Iteration 20/25 | Loss: 0.00108426
Iteration 21/25 | Loss: 0.00108426
Iteration 22/25 | Loss: 0.00108426
Iteration 23/25 | Loss: 0.00108426
Iteration 24/25 | Loss: 0.00108426
Iteration 25/25 | Loss: 0.00108426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61549950
Iteration 2/25 | Loss: 0.00090746
Iteration 3/25 | Loss: 0.00090745
Iteration 4/25 | Loss: 0.00090745
Iteration 5/25 | Loss: 0.00090745
Iteration 6/25 | Loss: 0.00090745
Iteration 7/25 | Loss: 0.00090745
Iteration 8/25 | Loss: 0.00090745
Iteration 9/25 | Loss: 0.00090745
Iteration 10/25 | Loss: 0.00090745
Iteration 11/25 | Loss: 0.00090745
Iteration 12/25 | Loss: 0.00090745
Iteration 13/25 | Loss: 0.00090745
Iteration 14/25 | Loss: 0.00090745
Iteration 15/25 | Loss: 0.00090745
Iteration 16/25 | Loss: 0.00090745
Iteration 17/25 | Loss: 0.00090745
Iteration 18/25 | Loss: 0.00090745
Iteration 19/25 | Loss: 0.00090745
Iteration 20/25 | Loss: 0.00090745
Iteration 21/25 | Loss: 0.00090745
Iteration 22/25 | Loss: 0.00090745
Iteration 23/25 | Loss: 0.00090745
Iteration 24/25 | Loss: 0.00090745
Iteration 25/25 | Loss: 0.00090745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090745
Iteration 2/1000 | Loss: 0.00001713
Iteration 3/1000 | Loss: 0.00001293
Iteration 4/1000 | Loss: 0.00001189
Iteration 5/1000 | Loss: 0.00001137
Iteration 6/1000 | Loss: 0.00001098
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001074
Iteration 9/1000 | Loss: 0.00001048
Iteration 10/1000 | Loss: 0.00001038
Iteration 11/1000 | Loss: 0.00001028
Iteration 12/1000 | Loss: 0.00001014
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001011
Iteration 15/1000 | Loss: 0.00001008
Iteration 16/1000 | Loss: 0.00001008
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001008
Iteration 19/1000 | Loss: 0.00001008
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001008
Iteration 25/1000 | Loss: 0.00001007
Iteration 26/1000 | Loss: 0.00001007
Iteration 27/1000 | Loss: 0.00001007
Iteration 28/1000 | Loss: 0.00001006
Iteration 29/1000 | Loss: 0.00001006
Iteration 30/1000 | Loss: 0.00001003
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001003
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001001
Iteration 35/1000 | Loss: 0.00001001
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000998
Iteration 38/1000 | Loss: 0.00000998
Iteration 39/1000 | Loss: 0.00000997
Iteration 40/1000 | Loss: 0.00000997
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000993
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000993
Iteration 45/1000 | Loss: 0.00000993
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000992
Iteration 50/1000 | Loss: 0.00000989
Iteration 51/1000 | Loss: 0.00000989
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000987
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000976
Iteration 85/1000 | Loss: 0.00000976
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000973
Iteration 97/1000 | Loss: 0.00000973
Iteration 98/1000 | Loss: 0.00000973
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000972
Iteration 101/1000 | Loss: 0.00000972
Iteration 102/1000 | Loss: 0.00000972
Iteration 103/1000 | Loss: 0.00000972
Iteration 104/1000 | Loss: 0.00000972
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000971
Iteration 107/1000 | Loss: 0.00000971
Iteration 108/1000 | Loss: 0.00000971
Iteration 109/1000 | Loss: 0.00000970
Iteration 110/1000 | Loss: 0.00000970
Iteration 111/1000 | Loss: 0.00000970
Iteration 112/1000 | Loss: 0.00000970
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000969
Iteration 117/1000 | Loss: 0.00000969
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000969
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000968
Iteration 125/1000 | Loss: 0.00000968
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000968
Iteration 143/1000 | Loss: 0.00000968
Iteration 144/1000 | Loss: 0.00000968
Iteration 145/1000 | Loss: 0.00000968
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000968
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000968
Iteration 151/1000 | Loss: 0.00000968
Iteration 152/1000 | Loss: 0.00000968
Iteration 153/1000 | Loss: 0.00000968
Iteration 154/1000 | Loss: 0.00000968
Iteration 155/1000 | Loss: 0.00000968
Iteration 156/1000 | Loss: 0.00000968
Iteration 157/1000 | Loss: 0.00000968
Iteration 158/1000 | Loss: 0.00000968
Iteration 159/1000 | Loss: 0.00000968
Iteration 160/1000 | Loss: 0.00000968
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000968
Iteration 164/1000 | Loss: 0.00000968
Iteration 165/1000 | Loss: 0.00000968
Iteration 166/1000 | Loss: 0.00000968
Iteration 167/1000 | Loss: 0.00000968
Iteration 168/1000 | Loss: 0.00000968
Iteration 169/1000 | Loss: 0.00000968
Iteration 170/1000 | Loss: 0.00000968
Iteration 171/1000 | Loss: 0.00000968
Iteration 172/1000 | Loss: 0.00000968
Iteration 173/1000 | Loss: 0.00000968
Iteration 174/1000 | Loss: 0.00000968
Iteration 175/1000 | Loss: 0.00000968
Iteration 176/1000 | Loss: 0.00000968
Iteration 177/1000 | Loss: 0.00000968
Iteration 178/1000 | Loss: 0.00000968
Iteration 179/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [9.676540685177315e-06, 9.676540685177315e-06, 9.676540685177315e-06, 9.676540685177315e-06, 9.676540685177315e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.676540685177315e-06

Optimization complete. Final v2v error: 2.669492721557617 mm

Highest mean error: 2.9989678859710693 mm for frame 158

Lowest mean error: 2.4421114921569824 mm for frame 39

Saving results

Total time: 48.8703670501709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987203
Iteration 2/25 | Loss: 0.00198626
Iteration 3/25 | Loss: 0.00124721
Iteration 4/25 | Loss: 0.00121412
Iteration 5/25 | Loss: 0.00121092
Iteration 6/25 | Loss: 0.00121048
Iteration 7/25 | Loss: 0.00121048
Iteration 8/25 | Loss: 0.00121048
Iteration 9/25 | Loss: 0.00121048
Iteration 10/25 | Loss: 0.00121048
Iteration 11/25 | Loss: 0.00121048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001210477901622653, 0.001210477901622653, 0.001210477901622653, 0.001210477901622653, 0.001210477901622653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210477901622653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31075621
Iteration 2/25 | Loss: 0.00104855
Iteration 3/25 | Loss: 0.00104855
Iteration 4/25 | Loss: 0.00104855
Iteration 5/25 | Loss: 0.00104855
Iteration 6/25 | Loss: 0.00104855
Iteration 7/25 | Loss: 0.00104855
Iteration 8/25 | Loss: 0.00104855
Iteration 9/25 | Loss: 0.00104855
Iteration 10/25 | Loss: 0.00104855
Iteration 11/25 | Loss: 0.00104855
Iteration 12/25 | Loss: 0.00104855
Iteration 13/25 | Loss: 0.00104855
Iteration 14/25 | Loss: 0.00104855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010485462844371796, 0.0010485462844371796, 0.0010485462844371796, 0.0010485462844371796, 0.0010485462844371796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010485462844371796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104855
Iteration 2/1000 | Loss: 0.00003404
Iteration 3/1000 | Loss: 0.00002235
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001564
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001411
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001389
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001388
Iteration 28/1000 | Loss: 0.00001388
Iteration 29/1000 | Loss: 0.00001387
Iteration 30/1000 | Loss: 0.00001387
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001385
Iteration 40/1000 | Loss: 0.00001385
Iteration 41/1000 | Loss: 0.00001385
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001384
Iteration 44/1000 | Loss: 0.00001384
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001379
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001370
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001368
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001367
Iteration 103/1000 | Loss: 0.00001367
Iteration 104/1000 | Loss: 0.00001367
Iteration 105/1000 | Loss: 0.00001367
Iteration 106/1000 | Loss: 0.00001367
Iteration 107/1000 | Loss: 0.00001367
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001366
Iteration 110/1000 | Loss: 0.00001366
Iteration 111/1000 | Loss: 0.00001366
Iteration 112/1000 | Loss: 0.00001366
Iteration 113/1000 | Loss: 0.00001366
Iteration 114/1000 | Loss: 0.00001366
Iteration 115/1000 | Loss: 0.00001366
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00001365
Iteration 118/1000 | Loss: 0.00001365
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001363
Iteration 130/1000 | Loss: 0.00001363
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001363
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001363
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001363
Iteration 140/1000 | Loss: 0.00001363
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001363
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001363
Iteration 147/1000 | Loss: 0.00001363
Iteration 148/1000 | Loss: 0.00001363
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001363
Iteration 152/1000 | Loss: 0.00001363
Iteration 153/1000 | Loss: 0.00001363
Iteration 154/1000 | Loss: 0.00001363
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001363
Iteration 157/1000 | Loss: 0.00001363
Iteration 158/1000 | Loss: 0.00001363
Iteration 159/1000 | Loss: 0.00001363
Iteration 160/1000 | Loss: 0.00001363
Iteration 161/1000 | Loss: 0.00001363
Iteration 162/1000 | Loss: 0.00001363
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.362524926662445e-05, 1.362524926662445e-05, 1.362524926662445e-05, 1.362524926662445e-05, 1.362524926662445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.362524926662445e-05

Optimization complete. Final v2v error: 3.0858187675476074 mm

Highest mean error: 3.9237985610961914 mm for frame 234

Lowest mean error: 2.6456387042999268 mm for frame 173

Saving results

Total time: 39.8832528591156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883581
Iteration 2/25 | Loss: 0.00125156
Iteration 3/25 | Loss: 0.00114561
Iteration 4/25 | Loss: 0.00113015
Iteration 5/25 | Loss: 0.00112489
Iteration 6/25 | Loss: 0.00112449
Iteration 7/25 | Loss: 0.00112449
Iteration 8/25 | Loss: 0.00112449
Iteration 9/25 | Loss: 0.00112449
Iteration 10/25 | Loss: 0.00112449
Iteration 11/25 | Loss: 0.00112449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011244866764172912, 0.0011244866764172912, 0.0011244866764172912, 0.0011244866764172912, 0.0011244866764172912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011244866764172912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35667455
Iteration 2/25 | Loss: 0.00094209
Iteration 3/25 | Loss: 0.00094208
Iteration 4/25 | Loss: 0.00094208
Iteration 5/25 | Loss: 0.00094208
Iteration 6/25 | Loss: 0.00094208
Iteration 7/25 | Loss: 0.00094208
Iteration 8/25 | Loss: 0.00094208
Iteration 9/25 | Loss: 0.00094208
Iteration 10/25 | Loss: 0.00094208
Iteration 11/25 | Loss: 0.00094208
Iteration 12/25 | Loss: 0.00094208
Iteration 13/25 | Loss: 0.00094208
Iteration 14/25 | Loss: 0.00094208
Iteration 15/25 | Loss: 0.00094208
Iteration 16/25 | Loss: 0.00094208
Iteration 17/25 | Loss: 0.00094208
Iteration 18/25 | Loss: 0.00094208
Iteration 19/25 | Loss: 0.00094208
Iteration 20/25 | Loss: 0.00094208
Iteration 21/25 | Loss: 0.00094208
Iteration 22/25 | Loss: 0.00094208
Iteration 23/25 | Loss: 0.00094208
Iteration 24/25 | Loss: 0.00094208
Iteration 25/25 | Loss: 0.00094208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094208
Iteration 2/1000 | Loss: 0.00003531
Iteration 3/1000 | Loss: 0.00001807
Iteration 4/1000 | Loss: 0.00001557
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001344
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001316
Iteration 10/1000 | Loss: 0.00001293
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001239
Iteration 64/1000 | Loss: 0.00001239
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001233
Iteration 80/1000 | Loss: 0.00001233
Iteration 81/1000 | Loss: 0.00001233
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001228
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001226
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.2264560609764885e-05, 1.2264560609764885e-05, 1.2264560609764885e-05, 1.2264560609764885e-05, 1.2264560609764885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2264560609764885e-05

Optimization complete. Final v2v error: 2.9782252311706543 mm

Highest mean error: 3.5413222312927246 mm for frame 37

Lowest mean error: 2.721935272216797 mm for frame 29

Saving results

Total time: 37.442039489746094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615476
Iteration 2/25 | Loss: 0.00156463
Iteration 3/25 | Loss: 0.00137497
Iteration 4/25 | Loss: 0.00133176
Iteration 5/25 | Loss: 0.00134682
Iteration 6/25 | Loss: 0.00131709
Iteration 7/25 | Loss: 0.00126821
Iteration 8/25 | Loss: 0.00125704
Iteration 9/25 | Loss: 0.00125273
Iteration 10/25 | Loss: 0.00125397
Iteration 11/25 | Loss: 0.00125572
Iteration 12/25 | Loss: 0.00124922
Iteration 13/25 | Loss: 0.00124644
Iteration 14/25 | Loss: 0.00124669
Iteration 15/25 | Loss: 0.00124571
Iteration 16/25 | Loss: 0.00124572
Iteration 17/25 | Loss: 0.00124673
Iteration 18/25 | Loss: 0.00124591
Iteration 19/25 | Loss: 0.00124515
Iteration 20/25 | Loss: 0.00124565
Iteration 21/25 | Loss: 0.00124610
Iteration 22/25 | Loss: 0.00124581
Iteration 23/25 | Loss: 0.00124630
Iteration 24/25 | Loss: 0.00124601
Iteration 25/25 | Loss: 0.00124606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26208174
Iteration 2/25 | Loss: 0.00137941
Iteration 3/25 | Loss: 0.00137934
Iteration 4/25 | Loss: 0.00137934
Iteration 5/25 | Loss: 0.00137934
Iteration 6/25 | Loss: 0.00137934
Iteration 7/25 | Loss: 0.00137934
Iteration 8/25 | Loss: 0.00137934
Iteration 9/25 | Loss: 0.00137934
Iteration 10/25 | Loss: 0.00137934
Iteration 11/25 | Loss: 0.00137934
Iteration 12/25 | Loss: 0.00137934
Iteration 13/25 | Loss: 0.00137934
Iteration 14/25 | Loss: 0.00137934
Iteration 15/25 | Loss: 0.00137934
Iteration 16/25 | Loss: 0.00137934
Iteration 17/25 | Loss: 0.00137934
Iteration 18/25 | Loss: 0.00137934
Iteration 19/25 | Loss: 0.00137934
Iteration 20/25 | Loss: 0.00137934
Iteration 21/25 | Loss: 0.00137934
Iteration 22/25 | Loss: 0.00137934
Iteration 23/25 | Loss: 0.00137934
Iteration 24/25 | Loss: 0.00137934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013793354155495763, 0.0013793354155495763, 0.0013793354155495763, 0.0013793354155495763, 0.0013793354155495763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013793354155495763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137934
Iteration 2/1000 | Loss: 0.00014428
Iteration 3/1000 | Loss: 0.00008639
Iteration 4/1000 | Loss: 0.00004958
Iteration 5/1000 | Loss: 0.00004183
Iteration 6/1000 | Loss: 0.00005664
Iteration 7/1000 | Loss: 0.00003457
Iteration 8/1000 | Loss: 0.00004578
Iteration 9/1000 | Loss: 0.00005091
Iteration 10/1000 | Loss: 0.00003576
Iteration 11/1000 | Loss: 0.00003217
Iteration 12/1000 | Loss: 0.00003523
Iteration 13/1000 | Loss: 0.00004514
Iteration 14/1000 | Loss: 0.00004280
Iteration 15/1000 | Loss: 0.00004628
Iteration 16/1000 | Loss: 0.00004744
Iteration 17/1000 | Loss: 0.00004108
Iteration 18/1000 | Loss: 0.00004726
Iteration 19/1000 | Loss: 0.00004143
Iteration 20/1000 | Loss: 0.00005737
Iteration 21/1000 | Loss: 0.00004304
Iteration 22/1000 | Loss: 0.00005778
Iteration 23/1000 | Loss: 0.00002933
Iteration 24/1000 | Loss: 0.00002821
Iteration 25/1000 | Loss: 0.00002702
Iteration 26/1000 | Loss: 0.00002624
Iteration 27/1000 | Loss: 0.00002577
Iteration 28/1000 | Loss: 0.00002539
Iteration 29/1000 | Loss: 0.00002514
Iteration 30/1000 | Loss: 0.00002494
Iteration 31/1000 | Loss: 0.00002487
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002478
Iteration 35/1000 | Loss: 0.00002478
Iteration 36/1000 | Loss: 0.00002478
Iteration 37/1000 | Loss: 0.00002477
Iteration 38/1000 | Loss: 0.00002477
Iteration 39/1000 | Loss: 0.00002476
Iteration 40/1000 | Loss: 0.00002476
Iteration 41/1000 | Loss: 0.00002476
Iteration 42/1000 | Loss: 0.00002471
Iteration 43/1000 | Loss: 0.00002470
Iteration 44/1000 | Loss: 0.00002469
Iteration 45/1000 | Loss: 0.00002468
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002466
Iteration 48/1000 | Loss: 0.00002465
Iteration 49/1000 | Loss: 0.00002465
Iteration 50/1000 | Loss: 0.00002465
Iteration 51/1000 | Loss: 0.00002464
Iteration 52/1000 | Loss: 0.00002464
Iteration 53/1000 | Loss: 0.00002464
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002463
Iteration 56/1000 | Loss: 0.00002462
Iteration 57/1000 | Loss: 0.00002461
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002446
Iteration 60/1000 | Loss: 0.00002445
Iteration 61/1000 | Loss: 0.00002443
Iteration 62/1000 | Loss: 0.00002442
Iteration 63/1000 | Loss: 0.00002442
Iteration 64/1000 | Loss: 0.00002441
Iteration 65/1000 | Loss: 0.00002439
Iteration 66/1000 | Loss: 0.00002438
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002432
Iteration 69/1000 | Loss: 0.00002431
Iteration 70/1000 | Loss: 0.00002430
Iteration 71/1000 | Loss: 0.00002430
Iteration 72/1000 | Loss: 0.00002429
Iteration 73/1000 | Loss: 0.00002425
Iteration 74/1000 | Loss: 0.00002425
Iteration 75/1000 | Loss: 0.00002422
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00002419
Iteration 78/1000 | Loss: 0.00002419
Iteration 79/1000 | Loss: 0.00002418
Iteration 80/1000 | Loss: 0.00002418
Iteration 81/1000 | Loss: 0.00002417
Iteration 82/1000 | Loss: 0.00002417
Iteration 83/1000 | Loss: 0.00002416
Iteration 84/1000 | Loss: 0.00002416
Iteration 85/1000 | Loss: 0.00002415
Iteration 86/1000 | Loss: 0.00002415
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002415
Iteration 90/1000 | Loss: 0.00002415
Iteration 91/1000 | Loss: 0.00002415
Iteration 92/1000 | Loss: 0.00002414
Iteration 93/1000 | Loss: 0.00002414
Iteration 94/1000 | Loss: 0.00002414
Iteration 95/1000 | Loss: 0.00002414
Iteration 96/1000 | Loss: 0.00002414
Iteration 97/1000 | Loss: 0.00002414
Iteration 98/1000 | Loss: 0.00002413
Iteration 99/1000 | Loss: 0.00002413
Iteration 100/1000 | Loss: 0.00002413
Iteration 101/1000 | Loss: 0.00002412
Iteration 102/1000 | Loss: 0.00002412
Iteration 103/1000 | Loss: 0.00002412
Iteration 104/1000 | Loss: 0.00002411
Iteration 105/1000 | Loss: 0.00002411
Iteration 106/1000 | Loss: 0.00002411
Iteration 107/1000 | Loss: 0.00002411
Iteration 108/1000 | Loss: 0.00002411
Iteration 109/1000 | Loss: 0.00002410
Iteration 110/1000 | Loss: 0.00002410
Iteration 111/1000 | Loss: 0.00002410
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002410
Iteration 114/1000 | Loss: 0.00002410
Iteration 115/1000 | Loss: 0.00002410
Iteration 116/1000 | Loss: 0.00002410
Iteration 117/1000 | Loss: 0.00002409
Iteration 118/1000 | Loss: 0.00002409
Iteration 119/1000 | Loss: 0.00002409
Iteration 120/1000 | Loss: 0.00002409
Iteration 121/1000 | Loss: 0.00002409
Iteration 122/1000 | Loss: 0.00002409
Iteration 123/1000 | Loss: 0.00002409
Iteration 124/1000 | Loss: 0.00002409
Iteration 125/1000 | Loss: 0.00002408
Iteration 126/1000 | Loss: 0.00002408
Iteration 127/1000 | Loss: 0.00002408
Iteration 128/1000 | Loss: 0.00002408
Iteration 129/1000 | Loss: 0.00002408
Iteration 130/1000 | Loss: 0.00002407
Iteration 131/1000 | Loss: 0.00002407
Iteration 132/1000 | Loss: 0.00002407
Iteration 133/1000 | Loss: 0.00002407
Iteration 134/1000 | Loss: 0.00002407
Iteration 135/1000 | Loss: 0.00002407
Iteration 136/1000 | Loss: 0.00002407
Iteration 137/1000 | Loss: 0.00002407
Iteration 138/1000 | Loss: 0.00002406
Iteration 139/1000 | Loss: 0.00002406
Iteration 140/1000 | Loss: 0.00002406
Iteration 141/1000 | Loss: 0.00002406
Iteration 142/1000 | Loss: 0.00002406
Iteration 143/1000 | Loss: 0.00002405
Iteration 144/1000 | Loss: 0.00002405
Iteration 145/1000 | Loss: 0.00002405
Iteration 146/1000 | Loss: 0.00002405
Iteration 147/1000 | Loss: 0.00002405
Iteration 148/1000 | Loss: 0.00002405
Iteration 149/1000 | Loss: 0.00002404
Iteration 150/1000 | Loss: 0.00002404
Iteration 151/1000 | Loss: 0.00002404
Iteration 152/1000 | Loss: 0.00002404
Iteration 153/1000 | Loss: 0.00002404
Iteration 154/1000 | Loss: 0.00002404
Iteration 155/1000 | Loss: 0.00002404
Iteration 156/1000 | Loss: 0.00002403
Iteration 157/1000 | Loss: 0.00002403
Iteration 158/1000 | Loss: 0.00002403
Iteration 159/1000 | Loss: 0.00002403
Iteration 160/1000 | Loss: 0.00002403
Iteration 161/1000 | Loss: 0.00002403
Iteration 162/1000 | Loss: 0.00002403
Iteration 163/1000 | Loss: 0.00002403
Iteration 164/1000 | Loss: 0.00002403
Iteration 165/1000 | Loss: 0.00002403
Iteration 166/1000 | Loss: 0.00002403
Iteration 167/1000 | Loss: 0.00002403
Iteration 168/1000 | Loss: 0.00002403
Iteration 169/1000 | Loss: 0.00002403
Iteration 170/1000 | Loss: 0.00002402
Iteration 171/1000 | Loss: 0.00002402
Iteration 172/1000 | Loss: 0.00002402
Iteration 173/1000 | Loss: 0.00002402
Iteration 174/1000 | Loss: 0.00002402
Iteration 175/1000 | Loss: 0.00002402
Iteration 176/1000 | Loss: 0.00002402
Iteration 177/1000 | Loss: 0.00002402
Iteration 178/1000 | Loss: 0.00002402
Iteration 179/1000 | Loss: 0.00002402
Iteration 180/1000 | Loss: 0.00002401
Iteration 181/1000 | Loss: 0.00002401
Iteration 182/1000 | Loss: 0.00002401
Iteration 183/1000 | Loss: 0.00002401
Iteration 184/1000 | Loss: 0.00002401
Iteration 185/1000 | Loss: 0.00002401
Iteration 186/1000 | Loss: 0.00002400
Iteration 187/1000 | Loss: 0.00002400
Iteration 188/1000 | Loss: 0.00002400
Iteration 189/1000 | Loss: 0.00002400
Iteration 190/1000 | Loss: 0.00002400
Iteration 191/1000 | Loss: 0.00002400
Iteration 192/1000 | Loss: 0.00002400
Iteration 193/1000 | Loss: 0.00002400
Iteration 194/1000 | Loss: 0.00002399
Iteration 195/1000 | Loss: 0.00002399
Iteration 196/1000 | Loss: 0.00002399
Iteration 197/1000 | Loss: 0.00002399
Iteration 198/1000 | Loss: 0.00002399
Iteration 199/1000 | Loss: 0.00002399
Iteration 200/1000 | Loss: 0.00002399
Iteration 201/1000 | Loss: 0.00002399
Iteration 202/1000 | Loss: 0.00002399
Iteration 203/1000 | Loss: 0.00002399
Iteration 204/1000 | Loss: 0.00002399
Iteration 205/1000 | Loss: 0.00002399
Iteration 206/1000 | Loss: 0.00002399
Iteration 207/1000 | Loss: 0.00002398
Iteration 208/1000 | Loss: 0.00002398
Iteration 209/1000 | Loss: 0.00002398
Iteration 210/1000 | Loss: 0.00002398
Iteration 211/1000 | Loss: 0.00002398
Iteration 212/1000 | Loss: 0.00002398
Iteration 213/1000 | Loss: 0.00002398
Iteration 214/1000 | Loss: 0.00002398
Iteration 215/1000 | Loss: 0.00002397
Iteration 216/1000 | Loss: 0.00002397
Iteration 217/1000 | Loss: 0.00002397
Iteration 218/1000 | Loss: 0.00002397
Iteration 219/1000 | Loss: 0.00002397
Iteration 220/1000 | Loss: 0.00002397
Iteration 221/1000 | Loss: 0.00002397
Iteration 222/1000 | Loss: 0.00002397
Iteration 223/1000 | Loss: 0.00002397
Iteration 224/1000 | Loss: 0.00002397
Iteration 225/1000 | Loss: 0.00002397
Iteration 226/1000 | Loss: 0.00002397
Iteration 227/1000 | Loss: 0.00002397
Iteration 228/1000 | Loss: 0.00002397
Iteration 229/1000 | Loss: 0.00002397
Iteration 230/1000 | Loss: 0.00002397
Iteration 231/1000 | Loss: 0.00002397
Iteration 232/1000 | Loss: 0.00002397
Iteration 233/1000 | Loss: 0.00002397
Iteration 234/1000 | Loss: 0.00002397
Iteration 235/1000 | Loss: 0.00002397
Iteration 236/1000 | Loss: 0.00002397
Iteration 237/1000 | Loss: 0.00002397
Iteration 238/1000 | Loss: 0.00002397
Iteration 239/1000 | Loss: 0.00002397
Iteration 240/1000 | Loss: 0.00002397
Iteration 241/1000 | Loss: 0.00002397
Iteration 242/1000 | Loss: 0.00002397
Iteration 243/1000 | Loss: 0.00002397
Iteration 244/1000 | Loss: 0.00002397
Iteration 245/1000 | Loss: 0.00002397
Iteration 246/1000 | Loss: 0.00002397
Iteration 247/1000 | Loss: 0.00002397
Iteration 248/1000 | Loss: 0.00002397
Iteration 249/1000 | Loss: 0.00002397
Iteration 250/1000 | Loss: 0.00002397
Iteration 251/1000 | Loss: 0.00002397
Iteration 252/1000 | Loss: 0.00002397
Iteration 253/1000 | Loss: 0.00002397
Iteration 254/1000 | Loss: 0.00002397
Iteration 255/1000 | Loss: 0.00002397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.3969185349415056e-05, 2.3969185349415056e-05, 2.3969185349415056e-05, 2.3969185349415056e-05, 2.3969185349415056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3969185349415056e-05

Optimization complete. Final v2v error: 3.653571605682373 mm

Highest mean error: 10.790998458862305 mm for frame 0

Lowest mean error: 2.8098387718200684 mm for frame 172

Saving results

Total time: 113.31080913543701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956506
Iteration 2/25 | Loss: 0.00956506
Iteration 3/25 | Loss: 0.00956506
Iteration 4/25 | Loss: 0.00377311
Iteration 5/25 | Loss: 0.00266437
Iteration 6/25 | Loss: 0.00222834
Iteration 7/25 | Loss: 0.00212413
Iteration 8/25 | Loss: 0.00207597
Iteration 9/25 | Loss: 0.00175122
Iteration 10/25 | Loss: 0.00161240
Iteration 11/25 | Loss: 0.00157531
Iteration 12/25 | Loss: 0.00149622
Iteration 13/25 | Loss: 0.00144544
Iteration 14/25 | Loss: 0.00142280
Iteration 15/25 | Loss: 0.00140325
Iteration 16/25 | Loss: 0.00140263
Iteration 17/25 | Loss: 0.00140176
Iteration 18/25 | Loss: 0.00139642
Iteration 19/25 | Loss: 0.00139387
Iteration 20/25 | Loss: 0.00139428
Iteration 21/25 | Loss: 0.00139183
Iteration 22/25 | Loss: 0.00138932
Iteration 23/25 | Loss: 0.00138694
Iteration 24/25 | Loss: 0.00138639
Iteration 25/25 | Loss: 0.00138633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33020449
Iteration 2/25 | Loss: 0.00182416
Iteration 3/25 | Loss: 0.00182416
Iteration 4/25 | Loss: 0.00182416
Iteration 5/25 | Loss: 0.00182416
Iteration 6/25 | Loss: 0.00182416
Iteration 7/25 | Loss: 0.00182416
Iteration 8/25 | Loss: 0.00182416
Iteration 9/25 | Loss: 0.00182416
Iteration 10/25 | Loss: 0.00182416
Iteration 11/25 | Loss: 0.00182416
Iteration 12/25 | Loss: 0.00182416
Iteration 13/25 | Loss: 0.00182416
Iteration 14/25 | Loss: 0.00182416
Iteration 15/25 | Loss: 0.00182416
Iteration 16/25 | Loss: 0.00182416
Iteration 17/25 | Loss: 0.00182416
Iteration 18/25 | Loss: 0.00182416
Iteration 19/25 | Loss: 0.00182416
Iteration 20/25 | Loss: 0.00182416
Iteration 21/25 | Loss: 0.00182416
Iteration 22/25 | Loss: 0.00182416
Iteration 23/25 | Loss: 0.00182416
Iteration 24/25 | Loss: 0.00182416
Iteration 25/25 | Loss: 0.00182416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182416
Iteration 2/1000 | Loss: 0.00109522
Iteration 3/1000 | Loss: 0.00048217
Iteration 4/1000 | Loss: 0.00066375
Iteration 5/1000 | Loss: 0.00061365
Iteration 6/1000 | Loss: 0.00049681
Iteration 7/1000 | Loss: 0.00019478
Iteration 8/1000 | Loss: 0.00015871
Iteration 9/1000 | Loss: 0.00029896
Iteration 10/1000 | Loss: 0.00014614
Iteration 11/1000 | Loss: 0.00013092
Iteration 12/1000 | Loss: 0.00012382
Iteration 13/1000 | Loss: 0.00035415
Iteration 14/1000 | Loss: 0.00135634
Iteration 15/1000 | Loss: 0.00332230
Iteration 16/1000 | Loss: 0.00074084
Iteration 17/1000 | Loss: 0.00021881
Iteration 18/1000 | Loss: 0.00017225
Iteration 19/1000 | Loss: 0.00011680
Iteration 20/1000 | Loss: 0.00009062
Iteration 21/1000 | Loss: 0.00005712
Iteration 22/1000 | Loss: 0.00007357
Iteration 23/1000 | Loss: 0.00004038
Iteration 24/1000 | Loss: 0.00003356
Iteration 25/1000 | Loss: 0.00002751
Iteration 26/1000 | Loss: 0.00002430
Iteration 27/1000 | Loss: 0.00002217
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001501
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001410
Iteration 43/1000 | Loss: 0.00001410
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001409
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001406
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001397
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001395
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001392
Iteration 82/1000 | Loss: 0.00001392
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001391
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001391
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001390
Iteration 94/1000 | Loss: 0.00001390
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001389
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001388
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001388
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001388
Iteration 113/1000 | Loss: 0.00001388
Iteration 114/1000 | Loss: 0.00001388
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001387
Iteration 118/1000 | Loss: 0.00001387
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001386
Iteration 134/1000 | Loss: 0.00001386
Iteration 135/1000 | Loss: 0.00001386
Iteration 136/1000 | Loss: 0.00001386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.3864067113900091e-05, 1.3864067113900091e-05, 1.3864067113900091e-05, 1.3864067113900091e-05, 1.3864067113900091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3864067113900091e-05

Optimization complete. Final v2v error: 3.1937012672424316 mm

Highest mean error: 3.384338617324829 mm for frame 33

Lowest mean error: 3.012726306915283 mm for frame 197

Saving results

Total time: 113.14341330528259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946997
Iteration 2/25 | Loss: 0.00222129
Iteration 3/25 | Loss: 0.00166598
Iteration 4/25 | Loss: 0.00158052
Iteration 5/25 | Loss: 0.00139564
Iteration 6/25 | Loss: 0.00137331
Iteration 7/25 | Loss: 0.00134946
Iteration 8/25 | Loss: 0.00136016
Iteration 9/25 | Loss: 0.00129328
Iteration 10/25 | Loss: 0.00128138
Iteration 11/25 | Loss: 0.00125235
Iteration 12/25 | Loss: 0.00122352
Iteration 13/25 | Loss: 0.00126071
Iteration 14/25 | Loss: 0.00124609
Iteration 15/25 | Loss: 0.00122875
Iteration 16/25 | Loss: 0.00122732
Iteration 17/25 | Loss: 0.00121687
Iteration 18/25 | Loss: 0.00121043
Iteration 19/25 | Loss: 0.00120471
Iteration 20/25 | Loss: 0.00119063
Iteration 21/25 | Loss: 0.00118853
Iteration 22/25 | Loss: 0.00118920
Iteration 23/25 | Loss: 0.00118873
Iteration 24/25 | Loss: 0.00118820
Iteration 25/25 | Loss: 0.00118839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33389556
Iteration 2/25 | Loss: 0.00099983
Iteration 3/25 | Loss: 0.00099983
Iteration 4/25 | Loss: 0.00099983
Iteration 5/25 | Loss: 0.00099983
Iteration 6/25 | Loss: 0.00099983
Iteration 7/25 | Loss: 0.00099983
Iteration 8/25 | Loss: 0.00099983
Iteration 9/25 | Loss: 0.00099983
Iteration 10/25 | Loss: 0.00099983
Iteration 11/25 | Loss: 0.00099983
Iteration 12/25 | Loss: 0.00099983
Iteration 13/25 | Loss: 0.00099983
Iteration 14/25 | Loss: 0.00099983
Iteration 15/25 | Loss: 0.00099983
Iteration 16/25 | Loss: 0.00099983
Iteration 17/25 | Loss: 0.00099983
Iteration 18/25 | Loss: 0.00099983
Iteration 19/25 | Loss: 0.00099983
Iteration 20/25 | Loss: 0.00099983
Iteration 21/25 | Loss: 0.00099983
Iteration 22/25 | Loss: 0.00099983
Iteration 23/25 | Loss: 0.00099983
Iteration 24/25 | Loss: 0.00099983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009998285677284002, 0.0009998285677284002, 0.0009998285677284002, 0.0009998285677284002, 0.0009998285677284002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998285677284002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099983
Iteration 2/1000 | Loss: 0.00010243
Iteration 3/1000 | Loss: 0.00005744
Iteration 4/1000 | Loss: 0.00005071
Iteration 5/1000 | Loss: 0.00006715
Iteration 6/1000 | Loss: 0.00006216
Iteration 7/1000 | Loss: 0.00006011
Iteration 8/1000 | Loss: 0.00006228
Iteration 9/1000 | Loss: 0.00007023
Iteration 10/1000 | Loss: 0.00006227
Iteration 11/1000 | Loss: 0.00006418
Iteration 12/1000 | Loss: 0.00007302
Iteration 13/1000 | Loss: 0.00006756
Iteration 14/1000 | Loss: 0.00007124
Iteration 15/1000 | Loss: 0.00006250
Iteration 16/1000 | Loss: 0.00006504
Iteration 17/1000 | Loss: 0.00006500
Iteration 18/1000 | Loss: 0.00006525
Iteration 19/1000 | Loss: 0.00007943
Iteration 20/1000 | Loss: 0.00006591
Iteration 21/1000 | Loss: 0.00004907
Iteration 22/1000 | Loss: 0.00005689
Iteration 23/1000 | Loss: 0.00006766
Iteration 24/1000 | Loss: 0.00007148
Iteration 25/1000 | Loss: 0.00008823
Iteration 26/1000 | Loss: 0.00007298
Iteration 27/1000 | Loss: 0.00007627
Iteration 28/1000 | Loss: 0.00007008
Iteration 29/1000 | Loss: 0.00007204
Iteration 30/1000 | Loss: 0.00007358
Iteration 31/1000 | Loss: 0.00006682
Iteration 32/1000 | Loss: 0.00007767
Iteration 33/1000 | Loss: 0.00006899
Iteration 34/1000 | Loss: 0.00007424
Iteration 35/1000 | Loss: 0.00007732
Iteration 36/1000 | Loss: 0.00007247
Iteration 37/1000 | Loss: 0.00007245
Iteration 38/1000 | Loss: 0.00008340
Iteration 39/1000 | Loss: 0.00007229
Iteration 40/1000 | Loss: 0.00007653
Iteration 41/1000 | Loss: 0.00007684
Iteration 42/1000 | Loss: 0.00006917
Iteration 43/1000 | Loss: 0.00007170
Iteration 44/1000 | Loss: 0.00007054
Iteration 45/1000 | Loss: 0.00004783
Iteration 46/1000 | Loss: 0.00007840
Iteration 47/1000 | Loss: 0.00008455
Iteration 48/1000 | Loss: 0.00007765
Iteration 49/1000 | Loss: 0.00007702
Iteration 50/1000 | Loss: 0.00007457
Iteration 51/1000 | Loss: 0.00007822
Iteration 52/1000 | Loss: 0.00007660
Iteration 53/1000 | Loss: 0.00006990
Iteration 54/1000 | Loss: 0.00005131
Iteration 55/1000 | Loss: 0.00008389
Iteration 56/1000 | Loss: 0.00006706
Iteration 57/1000 | Loss: 0.00007291
Iteration 58/1000 | Loss: 0.00007103
Iteration 59/1000 | Loss: 0.00007200
Iteration 60/1000 | Loss: 0.00007068
Iteration 61/1000 | Loss: 0.00007497
Iteration 62/1000 | Loss: 0.00006793
Iteration 63/1000 | Loss: 0.00007121
Iteration 64/1000 | Loss: 0.00014701
Iteration 65/1000 | Loss: 0.00008379
Iteration 66/1000 | Loss: 0.00009068
Iteration 67/1000 | Loss: 0.00007193
Iteration 68/1000 | Loss: 0.00005045
Iteration 69/1000 | Loss: 0.00007202
Iteration 70/1000 | Loss: 0.00004847
Iteration 71/1000 | Loss: 0.00007059
Iteration 72/1000 | Loss: 0.00003766
Iteration 73/1000 | Loss: 0.00005302
Iteration 74/1000 | Loss: 0.00007454
Iteration 75/1000 | Loss: 0.00006023
Iteration 76/1000 | Loss: 0.00005572
Iteration 77/1000 | Loss: 0.00007644
Iteration 78/1000 | Loss: 0.00006477
Iteration 79/1000 | Loss: 0.00006339
Iteration 80/1000 | Loss: 0.00008064
Iteration 81/1000 | Loss: 0.00008145
Iteration 82/1000 | Loss: 0.00007483
Iteration 83/1000 | Loss: 0.00007789
Iteration 84/1000 | Loss: 0.00007280
Iteration 85/1000 | Loss: 0.00007403
Iteration 86/1000 | Loss: 0.00010832
Iteration 87/1000 | Loss: 0.00007398
Iteration 88/1000 | Loss: 0.00005348
Iteration 89/1000 | Loss: 0.00012835
Iteration 90/1000 | Loss: 0.00005091
Iteration 91/1000 | Loss: 0.00006744
Iteration 92/1000 | Loss: 0.00007087
Iteration 93/1000 | Loss: 0.00006645
Iteration 94/1000 | Loss: 0.00003421
Iteration 95/1000 | Loss: 0.00007732
Iteration 96/1000 | Loss: 0.00005738
Iteration 97/1000 | Loss: 0.00004103
Iteration 98/1000 | Loss: 0.00003476
Iteration 99/1000 | Loss: 0.00003182
Iteration 100/1000 | Loss: 0.00003029
Iteration 101/1000 | Loss: 0.00002904
Iteration 102/1000 | Loss: 0.00002837
Iteration 103/1000 | Loss: 0.00002764
Iteration 104/1000 | Loss: 0.00002717
Iteration 105/1000 | Loss: 0.00002686
Iteration 106/1000 | Loss: 0.00002674
Iteration 107/1000 | Loss: 0.00002665
Iteration 108/1000 | Loss: 0.00002663
Iteration 109/1000 | Loss: 0.00002648
Iteration 110/1000 | Loss: 0.00002634
Iteration 111/1000 | Loss: 0.00002620
Iteration 112/1000 | Loss: 0.00002604
Iteration 113/1000 | Loss: 0.00002601
Iteration 114/1000 | Loss: 0.00002597
Iteration 115/1000 | Loss: 0.00002591
Iteration 116/1000 | Loss: 0.00002590
Iteration 117/1000 | Loss: 0.00002587
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002585
Iteration 120/1000 | Loss: 0.00002585
Iteration 121/1000 | Loss: 0.00002585
Iteration 122/1000 | Loss: 0.00002584
Iteration 123/1000 | Loss: 0.00002584
Iteration 124/1000 | Loss: 0.00002584
Iteration 125/1000 | Loss: 0.00002583
Iteration 126/1000 | Loss: 0.00002583
Iteration 127/1000 | Loss: 0.00002582
Iteration 128/1000 | Loss: 0.00002582
Iteration 129/1000 | Loss: 0.00002581
Iteration 130/1000 | Loss: 0.00002581
Iteration 131/1000 | Loss: 0.00002580
Iteration 132/1000 | Loss: 0.00002579
Iteration 133/1000 | Loss: 0.00002579
Iteration 134/1000 | Loss: 0.00002579
Iteration 135/1000 | Loss: 0.00002578
Iteration 136/1000 | Loss: 0.00002578
Iteration 137/1000 | Loss: 0.00002578
Iteration 138/1000 | Loss: 0.00002577
Iteration 139/1000 | Loss: 0.00002576
Iteration 140/1000 | Loss: 0.00002576
Iteration 141/1000 | Loss: 0.00002576
Iteration 142/1000 | Loss: 0.00002576
Iteration 143/1000 | Loss: 0.00002575
Iteration 144/1000 | Loss: 0.00002574
Iteration 145/1000 | Loss: 0.00002573
Iteration 146/1000 | Loss: 0.00002573
Iteration 147/1000 | Loss: 0.00002573
Iteration 148/1000 | Loss: 0.00002572
Iteration 149/1000 | Loss: 0.00002572
Iteration 150/1000 | Loss: 0.00002571
Iteration 151/1000 | Loss: 0.00002571
Iteration 152/1000 | Loss: 0.00002571
Iteration 153/1000 | Loss: 0.00002571
Iteration 154/1000 | Loss: 0.00002571
Iteration 155/1000 | Loss: 0.00002571
Iteration 156/1000 | Loss: 0.00002571
Iteration 157/1000 | Loss: 0.00002571
Iteration 158/1000 | Loss: 0.00002571
Iteration 159/1000 | Loss: 0.00002571
Iteration 160/1000 | Loss: 0.00002571
Iteration 161/1000 | Loss: 0.00002570
Iteration 162/1000 | Loss: 0.00002570
Iteration 163/1000 | Loss: 0.00002570
Iteration 164/1000 | Loss: 0.00003700
Iteration 165/1000 | Loss: 0.00002568
Iteration 166/1000 | Loss: 0.00002566
Iteration 167/1000 | Loss: 0.00002565
Iteration 168/1000 | Loss: 0.00002565
Iteration 169/1000 | Loss: 0.00002565
Iteration 170/1000 | Loss: 0.00002565
Iteration 171/1000 | Loss: 0.00002565
Iteration 172/1000 | Loss: 0.00002565
Iteration 173/1000 | Loss: 0.00002565
Iteration 174/1000 | Loss: 0.00002565
Iteration 175/1000 | Loss: 0.00002565
Iteration 176/1000 | Loss: 0.00002564
Iteration 177/1000 | Loss: 0.00002564
Iteration 178/1000 | Loss: 0.00002564
Iteration 179/1000 | Loss: 0.00002564
Iteration 180/1000 | Loss: 0.00002563
Iteration 181/1000 | Loss: 0.00002563
Iteration 182/1000 | Loss: 0.00002563
Iteration 183/1000 | Loss: 0.00002563
Iteration 184/1000 | Loss: 0.00002562
Iteration 185/1000 | Loss: 0.00002562
Iteration 186/1000 | Loss: 0.00002562
Iteration 187/1000 | Loss: 0.00002561
Iteration 188/1000 | Loss: 0.00002561
Iteration 189/1000 | Loss: 0.00002561
Iteration 190/1000 | Loss: 0.00002561
Iteration 191/1000 | Loss: 0.00002561
Iteration 192/1000 | Loss: 0.00002561
Iteration 193/1000 | Loss: 0.00002561
Iteration 194/1000 | Loss: 0.00002560
Iteration 195/1000 | Loss: 0.00002560
Iteration 196/1000 | Loss: 0.00002560
Iteration 197/1000 | Loss: 0.00002560
Iteration 198/1000 | Loss: 0.00002560
Iteration 199/1000 | Loss: 0.00002560
Iteration 200/1000 | Loss: 0.00002560
Iteration 201/1000 | Loss: 0.00002560
Iteration 202/1000 | Loss: 0.00002560
Iteration 203/1000 | Loss: 0.00002560
Iteration 204/1000 | Loss: 0.00002560
Iteration 205/1000 | Loss: 0.00002560
Iteration 206/1000 | Loss: 0.00002560
Iteration 207/1000 | Loss: 0.00002560
Iteration 208/1000 | Loss: 0.00002560
Iteration 209/1000 | Loss: 0.00002560
Iteration 210/1000 | Loss: 0.00002560
Iteration 211/1000 | Loss: 0.00002560
Iteration 212/1000 | Loss: 0.00002560
Iteration 213/1000 | Loss: 0.00002560
Iteration 214/1000 | Loss: 0.00002560
Iteration 215/1000 | Loss: 0.00002560
Iteration 216/1000 | Loss: 0.00002560
Iteration 217/1000 | Loss: 0.00002560
Iteration 218/1000 | Loss: 0.00002560
Iteration 219/1000 | Loss: 0.00002560
Iteration 220/1000 | Loss: 0.00002560
Iteration 221/1000 | Loss: 0.00002560
Iteration 222/1000 | Loss: 0.00002560
Iteration 223/1000 | Loss: 0.00002560
Iteration 224/1000 | Loss: 0.00002560
Iteration 225/1000 | Loss: 0.00002560
Iteration 226/1000 | Loss: 0.00002560
Iteration 227/1000 | Loss: 0.00002560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.5601180823286995e-05, 2.5601180823286995e-05, 2.5601180823286995e-05, 2.5601180823286995e-05, 2.5601180823286995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5601180823286995e-05

Optimization complete. Final v2v error: 3.7291243076324463 mm

Highest mean error: 10.697965621948242 mm for frame 21

Lowest mean error: 2.453413486480713 mm for frame 12

Saving results

Total time: 205.40444135665894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473551
Iteration 2/25 | Loss: 0.00123527
Iteration 3/25 | Loss: 0.00112744
Iteration 4/25 | Loss: 0.00111397
Iteration 5/25 | Loss: 0.00111115
Iteration 6/25 | Loss: 0.00111115
Iteration 7/25 | Loss: 0.00111115
Iteration 8/25 | Loss: 0.00111115
Iteration 9/25 | Loss: 0.00111115
Iteration 10/25 | Loss: 0.00111115
Iteration 11/25 | Loss: 0.00111115
Iteration 12/25 | Loss: 0.00111115
Iteration 13/25 | Loss: 0.00111115
Iteration 14/25 | Loss: 0.00111115
Iteration 15/25 | Loss: 0.00111115
Iteration 16/25 | Loss: 0.00111115
Iteration 17/25 | Loss: 0.00111115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011111523490399122, 0.0011111523490399122, 0.0011111523490399122, 0.0011111523490399122, 0.0011111523490399122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011111523490399122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.99870586
Iteration 2/25 | Loss: 0.00075143
Iteration 3/25 | Loss: 0.00075143
Iteration 4/25 | Loss: 0.00075143
Iteration 5/25 | Loss: 0.00075143
Iteration 6/25 | Loss: 0.00075143
Iteration 7/25 | Loss: 0.00075143
Iteration 8/25 | Loss: 0.00075143
Iteration 9/25 | Loss: 0.00075143
Iteration 10/25 | Loss: 0.00075142
Iteration 11/25 | Loss: 0.00075142
Iteration 12/25 | Loss: 0.00075142
Iteration 13/25 | Loss: 0.00075142
Iteration 14/25 | Loss: 0.00075142
Iteration 15/25 | Loss: 0.00075142
Iteration 16/25 | Loss: 0.00075142
Iteration 17/25 | Loss: 0.00075142
Iteration 18/25 | Loss: 0.00075142
Iteration 19/25 | Loss: 0.00075142
Iteration 20/25 | Loss: 0.00075142
Iteration 21/25 | Loss: 0.00075142
Iteration 22/25 | Loss: 0.00075142
Iteration 23/25 | Loss: 0.00075142
Iteration 24/25 | Loss: 0.00075142
Iteration 25/25 | Loss: 0.00075142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075142
Iteration 2/1000 | Loss: 0.00002028
Iteration 3/1000 | Loss: 0.00001600
Iteration 4/1000 | Loss: 0.00001495
Iteration 5/1000 | Loss: 0.00001411
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001237
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001207
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001192
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001180
Iteration 32/1000 | Loss: 0.00001180
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001150
Iteration 113/1000 | Loss: 0.00001150
Iteration 114/1000 | Loss: 0.00001150
Iteration 115/1000 | Loss: 0.00001150
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001150
Iteration 124/1000 | Loss: 0.00001149
Iteration 125/1000 | Loss: 0.00001149
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001149
Iteration 129/1000 | Loss: 0.00001149
Iteration 130/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.1494938917167019e-05, 1.1494938917167019e-05, 1.1494938917167019e-05, 1.1494938917167019e-05, 1.1494938917167019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1494938917167019e-05

Optimization complete. Final v2v error: 2.8962221145629883 mm

Highest mean error: 3.201688766479492 mm for frame 212

Lowest mean error: 2.7472708225250244 mm for frame 47

Saving results

Total time: 38.90970253944397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812646
Iteration 2/25 | Loss: 0.00121656
Iteration 3/25 | Loss: 0.00113222
Iteration 4/25 | Loss: 0.00112062
Iteration 5/25 | Loss: 0.00111756
Iteration 6/25 | Loss: 0.00111756
Iteration 7/25 | Loss: 0.00111756
Iteration 8/25 | Loss: 0.00111756
Iteration 9/25 | Loss: 0.00111756
Iteration 10/25 | Loss: 0.00111756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001117559033446014, 0.001117559033446014, 0.001117559033446014, 0.001117559033446014, 0.001117559033446014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001117559033446014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.79826260
Iteration 2/25 | Loss: 0.00085754
Iteration 3/25 | Loss: 0.00085754
Iteration 4/25 | Loss: 0.00085753
Iteration 5/25 | Loss: 0.00085753
Iteration 6/25 | Loss: 0.00085753
Iteration 7/25 | Loss: 0.00085753
Iteration 8/25 | Loss: 0.00085753
Iteration 9/25 | Loss: 0.00085753
Iteration 10/25 | Loss: 0.00085753
Iteration 11/25 | Loss: 0.00085753
Iteration 12/25 | Loss: 0.00085753
Iteration 13/25 | Loss: 0.00085753
Iteration 14/25 | Loss: 0.00085753
Iteration 15/25 | Loss: 0.00085753
Iteration 16/25 | Loss: 0.00085753
Iteration 17/25 | Loss: 0.00085753
Iteration 18/25 | Loss: 0.00085753
Iteration 19/25 | Loss: 0.00085753
Iteration 20/25 | Loss: 0.00085753
Iteration 21/25 | Loss: 0.00085753
Iteration 22/25 | Loss: 0.00085753
Iteration 23/25 | Loss: 0.00085753
Iteration 24/25 | Loss: 0.00085753
Iteration 25/25 | Loss: 0.00085753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085753
Iteration 2/1000 | Loss: 0.00002628
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001808
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001616
Iteration 8/1000 | Loss: 0.00001564
Iteration 9/1000 | Loss: 0.00001521
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001470
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001462
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001450
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001445
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001435
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001425
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001417
Iteration 100/1000 | Loss: 0.00001417
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001416
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001414
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001412
Iteration 116/1000 | Loss: 0.00001412
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001410
Iteration 130/1000 | Loss: 0.00001410
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001409
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001409
Iteration 137/1000 | Loss: 0.00001409
Iteration 138/1000 | Loss: 0.00001409
Iteration 139/1000 | Loss: 0.00001409
Iteration 140/1000 | Loss: 0.00001409
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4089091564528644e-05, 1.4089091564528644e-05, 1.4089091564528644e-05, 1.4089091564528644e-05, 1.4089091564528644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4089091564528644e-05

Optimization complete. Final v2v error: 3.1827316284179688 mm

Highest mean error: 3.4441633224487305 mm for frame 161

Lowest mean error: 2.983752727508545 mm for frame 26

Saving results

Total time: 41.63528776168823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643638
Iteration 2/25 | Loss: 0.00146476
Iteration 3/25 | Loss: 0.00133684
Iteration 4/25 | Loss: 0.00132921
Iteration 5/25 | Loss: 0.00132846
Iteration 6/25 | Loss: 0.00132846
Iteration 7/25 | Loss: 0.00132846
Iteration 8/25 | Loss: 0.00132846
Iteration 9/25 | Loss: 0.00132846
Iteration 10/25 | Loss: 0.00132846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013284612214192748, 0.0013284612214192748, 0.0013284612214192748, 0.0013284612214192748, 0.0013284612214192748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013284612214192748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62606704
Iteration 2/25 | Loss: 0.00082712
Iteration 3/25 | Loss: 0.00082711
Iteration 4/25 | Loss: 0.00082711
Iteration 5/25 | Loss: 0.00082711
Iteration 6/25 | Loss: 0.00082711
Iteration 7/25 | Loss: 0.00082711
Iteration 8/25 | Loss: 0.00082711
Iteration 9/25 | Loss: 0.00082711
Iteration 10/25 | Loss: 0.00082711
Iteration 11/25 | Loss: 0.00082711
Iteration 12/25 | Loss: 0.00082711
Iteration 13/25 | Loss: 0.00082711
Iteration 14/25 | Loss: 0.00082711
Iteration 15/25 | Loss: 0.00082711
Iteration 16/25 | Loss: 0.00082711
Iteration 17/25 | Loss: 0.00082711
Iteration 18/25 | Loss: 0.00082711
Iteration 19/25 | Loss: 0.00082711
Iteration 20/25 | Loss: 0.00082711
Iteration 21/25 | Loss: 0.00082711
Iteration 22/25 | Loss: 0.00082711
Iteration 23/25 | Loss: 0.00082711
Iteration 24/25 | Loss: 0.00082711
Iteration 25/25 | Loss: 0.00082711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082711
Iteration 2/1000 | Loss: 0.00004175
Iteration 3/1000 | Loss: 0.00003145
Iteration 4/1000 | Loss: 0.00002860
Iteration 5/1000 | Loss: 0.00002758
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002667
Iteration 8/1000 | Loss: 0.00002627
Iteration 9/1000 | Loss: 0.00002598
Iteration 10/1000 | Loss: 0.00002566
Iteration 11/1000 | Loss: 0.00002548
Iteration 12/1000 | Loss: 0.00002530
Iteration 13/1000 | Loss: 0.00002514
Iteration 14/1000 | Loss: 0.00002498
Iteration 15/1000 | Loss: 0.00002494
Iteration 16/1000 | Loss: 0.00002491
Iteration 17/1000 | Loss: 0.00002479
Iteration 18/1000 | Loss: 0.00002477
Iteration 19/1000 | Loss: 0.00002468
Iteration 20/1000 | Loss: 0.00002468
Iteration 21/1000 | Loss: 0.00002464
Iteration 22/1000 | Loss: 0.00002464
Iteration 23/1000 | Loss: 0.00002463
Iteration 24/1000 | Loss: 0.00002463
Iteration 25/1000 | Loss: 0.00002462
Iteration 26/1000 | Loss: 0.00002462
Iteration 27/1000 | Loss: 0.00002461
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00002458
Iteration 30/1000 | Loss: 0.00002458
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002453
Iteration 33/1000 | Loss: 0.00002453
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002453
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002453
Iteration 38/1000 | Loss: 0.00002453
Iteration 39/1000 | Loss: 0.00002453
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002451
Iteration 42/1000 | Loss: 0.00002451
Iteration 43/1000 | Loss: 0.00002451
Iteration 44/1000 | Loss: 0.00002450
Iteration 45/1000 | Loss: 0.00002450
Iteration 46/1000 | Loss: 0.00002450
Iteration 47/1000 | Loss: 0.00002449
Iteration 48/1000 | Loss: 0.00002449
Iteration 49/1000 | Loss: 0.00002449
Iteration 50/1000 | Loss: 0.00002449
Iteration 51/1000 | Loss: 0.00002449
Iteration 52/1000 | Loss: 0.00002449
Iteration 53/1000 | Loss: 0.00002448
Iteration 54/1000 | Loss: 0.00002448
Iteration 55/1000 | Loss: 0.00002448
Iteration 56/1000 | Loss: 0.00002448
Iteration 57/1000 | Loss: 0.00002448
Iteration 58/1000 | Loss: 0.00002448
Iteration 59/1000 | Loss: 0.00002447
Iteration 60/1000 | Loss: 0.00002447
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002447
Iteration 64/1000 | Loss: 0.00002446
Iteration 65/1000 | Loss: 0.00002446
Iteration 66/1000 | Loss: 0.00002446
Iteration 67/1000 | Loss: 0.00002446
Iteration 68/1000 | Loss: 0.00002446
Iteration 69/1000 | Loss: 0.00002446
Iteration 70/1000 | Loss: 0.00002445
Iteration 71/1000 | Loss: 0.00002445
Iteration 72/1000 | Loss: 0.00002445
Iteration 73/1000 | Loss: 0.00002445
Iteration 74/1000 | Loss: 0.00002445
Iteration 75/1000 | Loss: 0.00002445
Iteration 76/1000 | Loss: 0.00002445
Iteration 77/1000 | Loss: 0.00002445
Iteration 78/1000 | Loss: 0.00002445
Iteration 79/1000 | Loss: 0.00002445
Iteration 80/1000 | Loss: 0.00002445
Iteration 81/1000 | Loss: 0.00002445
Iteration 82/1000 | Loss: 0.00002445
Iteration 83/1000 | Loss: 0.00002445
Iteration 84/1000 | Loss: 0.00002445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.4449311240459792e-05, 2.4449311240459792e-05, 2.4449311240459792e-05, 2.4449311240459792e-05, 2.4449311240459792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4449311240459792e-05

Optimization complete. Final v2v error: 3.9353199005126953 mm

Highest mean error: 4.70573091506958 mm for frame 172

Lowest mean error: 3.7160820960998535 mm for frame 106

Saving results

Total time: 40.137373208999634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373660
Iteration 2/25 | Loss: 0.00114910
Iteration 3/25 | Loss: 0.00108261
Iteration 4/25 | Loss: 0.00107558
Iteration 5/25 | Loss: 0.00107351
Iteration 6/25 | Loss: 0.00107307
Iteration 7/25 | Loss: 0.00107307
Iteration 8/25 | Loss: 0.00107307
Iteration 9/25 | Loss: 0.00107307
Iteration 10/25 | Loss: 0.00107307
Iteration 11/25 | Loss: 0.00107307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001073070103302598, 0.001073070103302598, 0.001073070103302598, 0.001073070103302598, 0.001073070103302598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001073070103302598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65817475
Iteration 2/25 | Loss: 0.00087905
Iteration 3/25 | Loss: 0.00087904
Iteration 4/25 | Loss: 0.00087904
Iteration 5/25 | Loss: 0.00087904
Iteration 6/25 | Loss: 0.00087904
Iteration 7/25 | Loss: 0.00087904
Iteration 8/25 | Loss: 0.00087904
Iteration 9/25 | Loss: 0.00087904
Iteration 10/25 | Loss: 0.00087904
Iteration 11/25 | Loss: 0.00087904
Iteration 12/25 | Loss: 0.00087904
Iteration 13/25 | Loss: 0.00087904
Iteration 14/25 | Loss: 0.00087904
Iteration 15/25 | Loss: 0.00087904
Iteration 16/25 | Loss: 0.00087904
Iteration 17/25 | Loss: 0.00087904
Iteration 18/25 | Loss: 0.00087904
Iteration 19/25 | Loss: 0.00087904
Iteration 20/25 | Loss: 0.00087904
Iteration 21/25 | Loss: 0.00087904
Iteration 22/25 | Loss: 0.00087904
Iteration 23/25 | Loss: 0.00087904
Iteration 24/25 | Loss: 0.00087904
Iteration 25/25 | Loss: 0.00087904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087904
Iteration 2/1000 | Loss: 0.00002029
Iteration 3/1000 | Loss: 0.00001281
Iteration 4/1000 | Loss: 0.00001111
Iteration 5/1000 | Loss: 0.00001030
Iteration 6/1000 | Loss: 0.00000980
Iteration 7/1000 | Loss: 0.00000942
Iteration 8/1000 | Loss: 0.00000922
Iteration 9/1000 | Loss: 0.00000907
Iteration 10/1000 | Loss: 0.00000890
Iteration 11/1000 | Loss: 0.00000887
Iteration 12/1000 | Loss: 0.00000880
Iteration 13/1000 | Loss: 0.00000879
Iteration 14/1000 | Loss: 0.00000876
Iteration 15/1000 | Loss: 0.00000876
Iteration 16/1000 | Loss: 0.00000876
Iteration 17/1000 | Loss: 0.00000875
Iteration 18/1000 | Loss: 0.00000874
Iteration 19/1000 | Loss: 0.00000870
Iteration 20/1000 | Loss: 0.00000870
Iteration 21/1000 | Loss: 0.00000869
Iteration 22/1000 | Loss: 0.00000869
Iteration 23/1000 | Loss: 0.00000864
Iteration 24/1000 | Loss: 0.00000861
Iteration 25/1000 | Loss: 0.00000860
Iteration 26/1000 | Loss: 0.00000860
Iteration 27/1000 | Loss: 0.00000857
Iteration 28/1000 | Loss: 0.00000856
Iteration 29/1000 | Loss: 0.00000856
Iteration 30/1000 | Loss: 0.00000856
Iteration 31/1000 | Loss: 0.00000855
Iteration 32/1000 | Loss: 0.00000854
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000852
Iteration 35/1000 | Loss: 0.00000852
Iteration 36/1000 | Loss: 0.00000851
Iteration 37/1000 | Loss: 0.00000851
Iteration 38/1000 | Loss: 0.00000851
Iteration 39/1000 | Loss: 0.00000850
Iteration 40/1000 | Loss: 0.00000850
Iteration 41/1000 | Loss: 0.00000850
Iteration 42/1000 | Loss: 0.00000849
Iteration 43/1000 | Loss: 0.00000849
Iteration 44/1000 | Loss: 0.00000849
Iteration 45/1000 | Loss: 0.00000848
Iteration 46/1000 | Loss: 0.00000848
Iteration 47/1000 | Loss: 0.00000848
Iteration 48/1000 | Loss: 0.00000847
Iteration 49/1000 | Loss: 0.00000847
Iteration 50/1000 | Loss: 0.00000846
Iteration 51/1000 | Loss: 0.00000846
Iteration 52/1000 | Loss: 0.00000845
Iteration 53/1000 | Loss: 0.00000844
Iteration 54/1000 | Loss: 0.00000844
Iteration 55/1000 | Loss: 0.00000843
Iteration 56/1000 | Loss: 0.00000842
Iteration 57/1000 | Loss: 0.00000842
Iteration 58/1000 | Loss: 0.00000842
Iteration 59/1000 | Loss: 0.00000842
Iteration 60/1000 | Loss: 0.00000842
Iteration 61/1000 | Loss: 0.00000842
Iteration 62/1000 | Loss: 0.00000842
Iteration 63/1000 | Loss: 0.00000841
Iteration 64/1000 | Loss: 0.00000841
Iteration 65/1000 | Loss: 0.00000841
Iteration 66/1000 | Loss: 0.00000839
Iteration 67/1000 | Loss: 0.00000839
Iteration 68/1000 | Loss: 0.00000839
Iteration 69/1000 | Loss: 0.00000839
Iteration 70/1000 | Loss: 0.00000839
Iteration 71/1000 | Loss: 0.00000839
Iteration 72/1000 | Loss: 0.00000839
Iteration 73/1000 | Loss: 0.00000839
Iteration 74/1000 | Loss: 0.00000839
Iteration 75/1000 | Loss: 0.00000838
Iteration 76/1000 | Loss: 0.00000838
Iteration 77/1000 | Loss: 0.00000838
Iteration 78/1000 | Loss: 0.00000837
Iteration 79/1000 | Loss: 0.00000837
Iteration 80/1000 | Loss: 0.00000836
Iteration 81/1000 | Loss: 0.00000836
Iteration 82/1000 | Loss: 0.00000836
Iteration 83/1000 | Loss: 0.00000836
Iteration 84/1000 | Loss: 0.00000835
Iteration 85/1000 | Loss: 0.00000835
Iteration 86/1000 | Loss: 0.00000835
Iteration 87/1000 | Loss: 0.00000835
Iteration 88/1000 | Loss: 0.00000835
Iteration 89/1000 | Loss: 0.00000835
Iteration 90/1000 | Loss: 0.00000834
Iteration 91/1000 | Loss: 0.00000834
Iteration 92/1000 | Loss: 0.00000834
Iteration 93/1000 | Loss: 0.00000834
Iteration 94/1000 | Loss: 0.00000834
Iteration 95/1000 | Loss: 0.00000834
Iteration 96/1000 | Loss: 0.00000833
Iteration 97/1000 | Loss: 0.00000833
Iteration 98/1000 | Loss: 0.00000833
Iteration 99/1000 | Loss: 0.00000833
Iteration 100/1000 | Loss: 0.00000833
Iteration 101/1000 | Loss: 0.00000833
Iteration 102/1000 | Loss: 0.00000833
Iteration 103/1000 | Loss: 0.00000833
Iteration 104/1000 | Loss: 0.00000832
Iteration 105/1000 | Loss: 0.00000832
Iteration 106/1000 | Loss: 0.00000832
Iteration 107/1000 | Loss: 0.00000832
Iteration 108/1000 | Loss: 0.00000832
Iteration 109/1000 | Loss: 0.00000831
Iteration 110/1000 | Loss: 0.00000831
Iteration 111/1000 | Loss: 0.00000831
Iteration 112/1000 | Loss: 0.00000831
Iteration 113/1000 | Loss: 0.00000830
Iteration 114/1000 | Loss: 0.00000830
Iteration 115/1000 | Loss: 0.00000830
Iteration 116/1000 | Loss: 0.00000830
Iteration 117/1000 | Loss: 0.00000830
Iteration 118/1000 | Loss: 0.00000829
Iteration 119/1000 | Loss: 0.00000829
Iteration 120/1000 | Loss: 0.00000829
Iteration 121/1000 | Loss: 0.00000829
Iteration 122/1000 | Loss: 0.00000829
Iteration 123/1000 | Loss: 0.00000829
Iteration 124/1000 | Loss: 0.00000829
Iteration 125/1000 | Loss: 0.00000829
Iteration 126/1000 | Loss: 0.00000829
Iteration 127/1000 | Loss: 0.00000829
Iteration 128/1000 | Loss: 0.00000829
Iteration 129/1000 | Loss: 0.00000829
Iteration 130/1000 | Loss: 0.00000828
Iteration 131/1000 | Loss: 0.00000828
Iteration 132/1000 | Loss: 0.00000828
Iteration 133/1000 | Loss: 0.00000827
Iteration 134/1000 | Loss: 0.00000827
Iteration 135/1000 | Loss: 0.00000827
Iteration 136/1000 | Loss: 0.00000827
Iteration 137/1000 | Loss: 0.00000827
Iteration 138/1000 | Loss: 0.00000826
Iteration 139/1000 | Loss: 0.00000826
Iteration 140/1000 | Loss: 0.00000826
Iteration 141/1000 | Loss: 0.00000826
Iteration 142/1000 | Loss: 0.00000826
Iteration 143/1000 | Loss: 0.00000826
Iteration 144/1000 | Loss: 0.00000826
Iteration 145/1000 | Loss: 0.00000826
Iteration 146/1000 | Loss: 0.00000825
Iteration 147/1000 | Loss: 0.00000825
Iteration 148/1000 | Loss: 0.00000825
Iteration 149/1000 | Loss: 0.00000825
Iteration 150/1000 | Loss: 0.00000825
Iteration 151/1000 | Loss: 0.00000825
Iteration 152/1000 | Loss: 0.00000824
Iteration 153/1000 | Loss: 0.00000824
Iteration 154/1000 | Loss: 0.00000824
Iteration 155/1000 | Loss: 0.00000824
Iteration 156/1000 | Loss: 0.00000824
Iteration 157/1000 | Loss: 0.00000824
Iteration 158/1000 | Loss: 0.00000824
Iteration 159/1000 | Loss: 0.00000823
Iteration 160/1000 | Loss: 0.00000823
Iteration 161/1000 | Loss: 0.00000823
Iteration 162/1000 | Loss: 0.00000823
Iteration 163/1000 | Loss: 0.00000823
Iteration 164/1000 | Loss: 0.00000823
Iteration 165/1000 | Loss: 0.00000823
Iteration 166/1000 | Loss: 0.00000822
Iteration 167/1000 | Loss: 0.00000822
Iteration 168/1000 | Loss: 0.00000822
Iteration 169/1000 | Loss: 0.00000822
Iteration 170/1000 | Loss: 0.00000822
Iteration 171/1000 | Loss: 0.00000822
Iteration 172/1000 | Loss: 0.00000822
Iteration 173/1000 | Loss: 0.00000822
Iteration 174/1000 | Loss: 0.00000822
Iteration 175/1000 | Loss: 0.00000821
Iteration 176/1000 | Loss: 0.00000821
Iteration 177/1000 | Loss: 0.00000821
Iteration 178/1000 | Loss: 0.00000821
Iteration 179/1000 | Loss: 0.00000821
Iteration 180/1000 | Loss: 0.00000820
Iteration 181/1000 | Loss: 0.00000820
Iteration 182/1000 | Loss: 0.00000820
Iteration 183/1000 | Loss: 0.00000820
Iteration 184/1000 | Loss: 0.00000820
Iteration 185/1000 | Loss: 0.00000820
Iteration 186/1000 | Loss: 0.00000820
Iteration 187/1000 | Loss: 0.00000820
Iteration 188/1000 | Loss: 0.00000820
Iteration 189/1000 | Loss: 0.00000820
Iteration 190/1000 | Loss: 0.00000819
Iteration 191/1000 | Loss: 0.00000819
Iteration 192/1000 | Loss: 0.00000819
Iteration 193/1000 | Loss: 0.00000819
Iteration 194/1000 | Loss: 0.00000819
Iteration 195/1000 | Loss: 0.00000819
Iteration 196/1000 | Loss: 0.00000819
Iteration 197/1000 | Loss: 0.00000819
Iteration 198/1000 | Loss: 0.00000818
Iteration 199/1000 | Loss: 0.00000818
Iteration 200/1000 | Loss: 0.00000818
Iteration 201/1000 | Loss: 0.00000818
Iteration 202/1000 | Loss: 0.00000818
Iteration 203/1000 | Loss: 0.00000818
Iteration 204/1000 | Loss: 0.00000818
Iteration 205/1000 | Loss: 0.00000818
Iteration 206/1000 | Loss: 0.00000818
Iteration 207/1000 | Loss: 0.00000818
Iteration 208/1000 | Loss: 0.00000818
Iteration 209/1000 | Loss: 0.00000818
Iteration 210/1000 | Loss: 0.00000818
Iteration 211/1000 | Loss: 0.00000818
Iteration 212/1000 | Loss: 0.00000818
Iteration 213/1000 | Loss: 0.00000818
Iteration 214/1000 | Loss: 0.00000818
Iteration 215/1000 | Loss: 0.00000818
Iteration 216/1000 | Loss: 0.00000818
Iteration 217/1000 | Loss: 0.00000818
Iteration 218/1000 | Loss: 0.00000818
Iteration 219/1000 | Loss: 0.00000818
Iteration 220/1000 | Loss: 0.00000818
Iteration 221/1000 | Loss: 0.00000818
Iteration 222/1000 | Loss: 0.00000818
Iteration 223/1000 | Loss: 0.00000818
Iteration 224/1000 | Loss: 0.00000818
Iteration 225/1000 | Loss: 0.00000818
Iteration 226/1000 | Loss: 0.00000818
Iteration 227/1000 | Loss: 0.00000818
Iteration 228/1000 | Loss: 0.00000818
Iteration 229/1000 | Loss: 0.00000818
Iteration 230/1000 | Loss: 0.00000818
Iteration 231/1000 | Loss: 0.00000818
Iteration 232/1000 | Loss: 0.00000818
Iteration 233/1000 | Loss: 0.00000818
Iteration 234/1000 | Loss: 0.00000818
Iteration 235/1000 | Loss: 0.00000818
Iteration 236/1000 | Loss: 0.00000818
Iteration 237/1000 | Loss: 0.00000818
Iteration 238/1000 | Loss: 0.00000818
Iteration 239/1000 | Loss: 0.00000818
Iteration 240/1000 | Loss: 0.00000818
Iteration 241/1000 | Loss: 0.00000818
Iteration 242/1000 | Loss: 0.00000818
Iteration 243/1000 | Loss: 0.00000818
Iteration 244/1000 | Loss: 0.00000818
Iteration 245/1000 | Loss: 0.00000818
Iteration 246/1000 | Loss: 0.00000818
Iteration 247/1000 | Loss: 0.00000818
Iteration 248/1000 | Loss: 0.00000818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [8.176384653779678e-06, 8.176384653779678e-06, 8.176384653779678e-06, 8.176384653779678e-06, 8.176384653779678e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.176384653779678e-06

Optimization complete. Final v2v error: 2.4548230171203613 mm

Highest mean error: 2.853383779525757 mm for frame 83

Lowest mean error: 2.3724188804626465 mm for frame 67

Saving results

Total time: 40.434683084487915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786594
Iteration 2/25 | Loss: 0.00143039
Iteration 3/25 | Loss: 0.00120071
Iteration 4/25 | Loss: 0.00118687
Iteration 5/25 | Loss: 0.00118599
Iteration 6/25 | Loss: 0.00118599
Iteration 7/25 | Loss: 0.00118599
Iteration 8/25 | Loss: 0.00118599
Iteration 9/25 | Loss: 0.00118599
Iteration 10/25 | Loss: 0.00118599
Iteration 11/25 | Loss: 0.00118599
Iteration 12/25 | Loss: 0.00118599
Iteration 13/25 | Loss: 0.00118599
Iteration 14/25 | Loss: 0.00118599
Iteration 15/25 | Loss: 0.00118599
Iteration 16/25 | Loss: 0.00118599
Iteration 17/25 | Loss: 0.00118599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011859943624585867, 0.0011859943624585867, 0.0011859943624585867, 0.0011859943624585867, 0.0011859943624585867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859943624585867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32352197
Iteration 2/25 | Loss: 0.00058101
Iteration 3/25 | Loss: 0.00058100
Iteration 4/25 | Loss: 0.00058100
Iteration 5/25 | Loss: 0.00058100
Iteration 6/25 | Loss: 0.00058100
Iteration 7/25 | Loss: 0.00058100
Iteration 8/25 | Loss: 0.00058100
Iteration 9/25 | Loss: 0.00058100
Iteration 10/25 | Loss: 0.00058100
Iteration 11/25 | Loss: 0.00058100
Iteration 12/25 | Loss: 0.00058100
Iteration 13/25 | Loss: 0.00058100
Iteration 14/25 | Loss: 0.00058100
Iteration 15/25 | Loss: 0.00058100
Iteration 16/25 | Loss: 0.00058100
Iteration 17/25 | Loss: 0.00058100
Iteration 18/25 | Loss: 0.00058100
Iteration 19/25 | Loss: 0.00058100
Iteration 20/25 | Loss: 0.00058100
Iteration 21/25 | Loss: 0.00058100
Iteration 22/25 | Loss: 0.00058100
Iteration 23/25 | Loss: 0.00058100
Iteration 24/25 | Loss: 0.00058100
Iteration 25/25 | Loss: 0.00058100

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058100
Iteration 2/1000 | Loss: 0.00002929
Iteration 3/1000 | Loss: 0.00002276
Iteration 4/1000 | Loss: 0.00002126
Iteration 5/1000 | Loss: 0.00002033
Iteration 6/1000 | Loss: 0.00001959
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001802
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001792
Iteration 21/1000 | Loss: 0.00001791
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001787
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001785
Iteration 28/1000 | Loss: 0.00001780
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001777
Iteration 35/1000 | Loss: 0.00001777
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001774
Iteration 44/1000 | Loss: 0.00001774
Iteration 45/1000 | Loss: 0.00001774
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001774
Iteration 49/1000 | Loss: 0.00001774
Iteration 50/1000 | Loss: 0.00001774
Iteration 51/1000 | Loss: 0.00001773
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001766
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001766
Iteration 68/1000 | Loss: 0.00001766
Iteration 69/1000 | Loss: 0.00001766
Iteration 70/1000 | Loss: 0.00001766
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001765
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001765
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001765
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001765
Iteration 90/1000 | Loss: 0.00001765
Iteration 91/1000 | Loss: 0.00001765
Iteration 92/1000 | Loss: 0.00001765
Iteration 93/1000 | Loss: 0.00001765
Iteration 94/1000 | Loss: 0.00001765
Iteration 95/1000 | Loss: 0.00001765
Iteration 96/1000 | Loss: 0.00001765
Iteration 97/1000 | Loss: 0.00001765
Iteration 98/1000 | Loss: 0.00001765
Iteration 99/1000 | Loss: 0.00001765
Iteration 100/1000 | Loss: 0.00001765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.7650183508521877e-05, 1.7650183508521877e-05, 1.7650183508521877e-05, 1.7650183508521877e-05, 1.7650183508521877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7650183508521877e-05

Optimization complete. Final v2v error: 3.499807119369507 mm

Highest mean error: 3.7583870887756348 mm for frame 55

Lowest mean error: 3.283684253692627 mm for frame 119

Saving results

Total time: 32.64060854911804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770568
Iteration 2/25 | Loss: 0.00152908
Iteration 3/25 | Loss: 0.00130055
Iteration 4/25 | Loss: 0.00126583
Iteration 5/25 | Loss: 0.00127405
Iteration 6/25 | Loss: 0.00126451
Iteration 7/25 | Loss: 0.00123314
Iteration 8/25 | Loss: 0.00118398
Iteration 9/25 | Loss: 0.00117374
Iteration 10/25 | Loss: 0.00117266
Iteration 11/25 | Loss: 0.00117250
Iteration 12/25 | Loss: 0.00117244
Iteration 13/25 | Loss: 0.00117243
Iteration 14/25 | Loss: 0.00117243
Iteration 15/25 | Loss: 0.00117243
Iteration 16/25 | Loss: 0.00117243
Iteration 17/25 | Loss: 0.00117243
Iteration 18/25 | Loss: 0.00117243
Iteration 19/25 | Loss: 0.00117243
Iteration 20/25 | Loss: 0.00117242
Iteration 21/25 | Loss: 0.00117242
Iteration 22/25 | Loss: 0.00117242
Iteration 23/25 | Loss: 0.00117242
Iteration 24/25 | Loss: 0.00117242
Iteration 25/25 | Loss: 0.00117242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29658294
Iteration 2/25 | Loss: 0.00069301
Iteration 3/25 | Loss: 0.00069298
Iteration 4/25 | Loss: 0.00069297
Iteration 5/25 | Loss: 0.00069297
Iteration 6/25 | Loss: 0.00069297
Iteration 7/25 | Loss: 0.00069297
Iteration 8/25 | Loss: 0.00069297
Iteration 9/25 | Loss: 0.00069297
Iteration 10/25 | Loss: 0.00069297
Iteration 11/25 | Loss: 0.00069297
Iteration 12/25 | Loss: 0.00069297
Iteration 13/25 | Loss: 0.00069297
Iteration 14/25 | Loss: 0.00069297
Iteration 15/25 | Loss: 0.00069297
Iteration 16/25 | Loss: 0.00069297
Iteration 17/25 | Loss: 0.00069297
Iteration 18/25 | Loss: 0.00069297
Iteration 19/25 | Loss: 0.00069297
Iteration 20/25 | Loss: 0.00069297
Iteration 21/25 | Loss: 0.00069297
Iteration 22/25 | Loss: 0.00069297
Iteration 23/25 | Loss: 0.00069297
Iteration 24/25 | Loss: 0.00069297
Iteration 25/25 | Loss: 0.00069297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069297
Iteration 2/1000 | Loss: 0.00004934
Iteration 3/1000 | Loss: 0.00003066
Iteration 4/1000 | Loss: 0.00002528
Iteration 5/1000 | Loss: 0.00002323
Iteration 6/1000 | Loss: 0.00002223
Iteration 7/1000 | Loss: 0.00002169
Iteration 8/1000 | Loss: 0.00002122
Iteration 9/1000 | Loss: 0.00002089
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002046
Iteration 12/1000 | Loss: 0.00002024
Iteration 13/1000 | Loss: 0.00002024
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002008
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00002004
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00001994
Iteration 20/1000 | Loss: 0.00001984
Iteration 21/1000 | Loss: 0.00001975
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001970
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001970
Iteration 26/1000 | Loss: 0.00001970
Iteration 27/1000 | Loss: 0.00001969
Iteration 28/1000 | Loss: 0.00001968
Iteration 29/1000 | Loss: 0.00001967
Iteration 30/1000 | Loss: 0.00001967
Iteration 31/1000 | Loss: 0.00001967
Iteration 32/1000 | Loss: 0.00001967
Iteration 33/1000 | Loss: 0.00001967
Iteration 34/1000 | Loss: 0.00001966
Iteration 35/1000 | Loss: 0.00001966
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001965
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001963
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001962
Iteration 44/1000 | Loss: 0.00001962
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001961
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001958
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001957
Iteration 61/1000 | Loss: 0.00001957
Iteration 62/1000 | Loss: 0.00001957
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001955
Iteration 68/1000 | Loss: 0.00001955
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001954
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00001954
Iteration 73/1000 | Loss: 0.00001954
Iteration 74/1000 | Loss: 0.00001954
Iteration 75/1000 | Loss: 0.00001953
Iteration 76/1000 | Loss: 0.00001953
Iteration 77/1000 | Loss: 0.00001953
Iteration 78/1000 | Loss: 0.00001953
Iteration 79/1000 | Loss: 0.00001953
Iteration 80/1000 | Loss: 0.00001953
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001952
Iteration 84/1000 | Loss: 0.00001952
Iteration 85/1000 | Loss: 0.00001952
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001951
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001950
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001950
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001950
Iteration 115/1000 | Loss: 0.00001950
Iteration 116/1000 | Loss: 0.00001950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.9504026568029076e-05, 1.9504026568029076e-05, 1.9504026568029076e-05, 1.9504026568029076e-05, 1.9504026568029076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9504026568029076e-05

Optimization complete. Final v2v error: 3.585613250732422 mm

Highest mean error: 4.332875728607178 mm for frame 127

Lowest mean error: 2.9620513916015625 mm for frame 9

Saving results

Total time: 47.645652770996094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827178
Iteration 2/25 | Loss: 0.00168608
Iteration 3/25 | Loss: 0.00147799
Iteration 4/25 | Loss: 0.00140445
Iteration 5/25 | Loss: 0.00140458
Iteration 6/25 | Loss: 0.00139047
Iteration 7/25 | Loss: 0.00137303
Iteration 8/25 | Loss: 0.00140073
Iteration 9/25 | Loss: 0.00135306
Iteration 10/25 | Loss: 0.00130210
Iteration 11/25 | Loss: 0.00130947
Iteration 12/25 | Loss: 0.00129863
Iteration 13/25 | Loss: 0.00127947
Iteration 14/25 | Loss: 0.00130068
Iteration 15/25 | Loss: 0.00126483
Iteration 16/25 | Loss: 0.00123683
Iteration 17/25 | Loss: 0.00122021
Iteration 18/25 | Loss: 0.00122015
Iteration 19/25 | Loss: 0.00121449
Iteration 20/25 | Loss: 0.00120992
Iteration 21/25 | Loss: 0.00120486
Iteration 22/25 | Loss: 0.00120060
Iteration 23/25 | Loss: 0.00119871
Iteration 24/25 | Loss: 0.00119870
Iteration 25/25 | Loss: 0.00119799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31024003
Iteration 2/25 | Loss: 0.00088829
Iteration 3/25 | Loss: 0.00088829
Iteration 4/25 | Loss: 0.00088829
Iteration 5/25 | Loss: 0.00088829
Iteration 6/25 | Loss: 0.00088829
Iteration 7/25 | Loss: 0.00088829
Iteration 8/25 | Loss: 0.00088829
Iteration 9/25 | Loss: 0.00088829
Iteration 10/25 | Loss: 0.00088829
Iteration 11/25 | Loss: 0.00088829
Iteration 12/25 | Loss: 0.00088829
Iteration 13/25 | Loss: 0.00088829
Iteration 14/25 | Loss: 0.00088829
Iteration 15/25 | Loss: 0.00088829
Iteration 16/25 | Loss: 0.00088829
Iteration 17/25 | Loss: 0.00088829
Iteration 18/25 | Loss: 0.00088829
Iteration 19/25 | Loss: 0.00088829
Iteration 20/25 | Loss: 0.00088829
Iteration 21/25 | Loss: 0.00088829
Iteration 22/25 | Loss: 0.00088829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008882866823114455, 0.0008882866823114455, 0.0008882866823114455, 0.0008882866823114455, 0.0008882866823114455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008882866823114455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088829
Iteration 2/1000 | Loss: 0.00008861
Iteration 3/1000 | Loss: 0.00005311
Iteration 4/1000 | Loss: 0.00004359
Iteration 5/1000 | Loss: 0.00003736
Iteration 6/1000 | Loss: 0.00004454
Iteration 7/1000 | Loss: 0.00065810
Iteration 8/1000 | Loss: 0.00035565
Iteration 9/1000 | Loss: 0.00028256
Iteration 10/1000 | Loss: 0.00003757
Iteration 11/1000 | Loss: 0.00003244
Iteration 12/1000 | Loss: 0.00003053
Iteration 13/1000 | Loss: 0.00002948
Iteration 14/1000 | Loss: 0.00002860
Iteration 15/1000 | Loss: 0.00044303
Iteration 16/1000 | Loss: 0.00003575
Iteration 17/1000 | Loss: 0.00002854
Iteration 18/1000 | Loss: 0.00002696
Iteration 19/1000 | Loss: 0.00002611
Iteration 20/1000 | Loss: 0.00050056
Iteration 21/1000 | Loss: 0.00047748
Iteration 22/1000 | Loss: 0.00033808
Iteration 23/1000 | Loss: 0.00002679
Iteration 24/1000 | Loss: 0.00042560
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002310
Iteration 28/1000 | Loss: 0.00002218
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00002106
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002064
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002054
Iteration 38/1000 | Loss: 0.00002050
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002034
Iteration 41/1000 | Loss: 0.00002034
Iteration 42/1000 | Loss: 0.00002034
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002032
Iteration 45/1000 | Loss: 0.00002031
Iteration 46/1000 | Loss: 0.00002028
Iteration 47/1000 | Loss: 0.00002028
Iteration 48/1000 | Loss: 0.00002027
Iteration 49/1000 | Loss: 0.00002027
Iteration 50/1000 | Loss: 0.00002027
Iteration 51/1000 | Loss: 0.00002027
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002026
Iteration 55/1000 | Loss: 0.00002024
Iteration 56/1000 | Loss: 0.00002024
Iteration 57/1000 | Loss: 0.00002024
Iteration 58/1000 | Loss: 0.00002024
Iteration 59/1000 | Loss: 0.00002024
Iteration 60/1000 | Loss: 0.00002024
Iteration 61/1000 | Loss: 0.00002024
Iteration 62/1000 | Loss: 0.00002024
Iteration 63/1000 | Loss: 0.00002024
Iteration 64/1000 | Loss: 0.00002023
Iteration 65/1000 | Loss: 0.00002023
Iteration 66/1000 | Loss: 0.00002023
Iteration 67/1000 | Loss: 0.00002022
Iteration 68/1000 | Loss: 0.00002021
Iteration 69/1000 | Loss: 0.00002021
Iteration 70/1000 | Loss: 0.00002021
Iteration 71/1000 | Loss: 0.00002021
Iteration 72/1000 | Loss: 0.00002020
Iteration 73/1000 | Loss: 0.00002020
Iteration 74/1000 | Loss: 0.00002020
Iteration 75/1000 | Loss: 0.00002020
Iteration 76/1000 | Loss: 0.00002019
Iteration 77/1000 | Loss: 0.00002019
Iteration 78/1000 | Loss: 0.00002019
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002018
Iteration 82/1000 | Loss: 0.00002018
Iteration 83/1000 | Loss: 0.00002018
Iteration 84/1000 | Loss: 0.00002018
Iteration 85/1000 | Loss: 0.00002018
Iteration 86/1000 | Loss: 0.00002018
Iteration 87/1000 | Loss: 0.00002018
Iteration 88/1000 | Loss: 0.00002017
Iteration 89/1000 | Loss: 0.00002017
Iteration 90/1000 | Loss: 0.00002017
Iteration 91/1000 | Loss: 0.00002017
Iteration 92/1000 | Loss: 0.00002017
Iteration 93/1000 | Loss: 0.00002017
Iteration 94/1000 | Loss: 0.00002016
Iteration 95/1000 | Loss: 0.00002016
Iteration 96/1000 | Loss: 0.00002016
Iteration 97/1000 | Loss: 0.00002016
Iteration 98/1000 | Loss: 0.00002016
Iteration 99/1000 | Loss: 0.00002016
Iteration 100/1000 | Loss: 0.00002016
Iteration 101/1000 | Loss: 0.00002015
Iteration 102/1000 | Loss: 0.00002015
Iteration 103/1000 | Loss: 0.00002015
Iteration 104/1000 | Loss: 0.00002015
Iteration 105/1000 | Loss: 0.00002015
Iteration 106/1000 | Loss: 0.00002015
Iteration 107/1000 | Loss: 0.00002015
Iteration 108/1000 | Loss: 0.00002014
Iteration 109/1000 | Loss: 0.00002014
Iteration 110/1000 | Loss: 0.00002014
Iteration 111/1000 | Loss: 0.00002014
Iteration 112/1000 | Loss: 0.00002014
Iteration 113/1000 | Loss: 0.00002014
Iteration 114/1000 | Loss: 0.00002013
Iteration 115/1000 | Loss: 0.00002013
Iteration 116/1000 | Loss: 0.00002013
Iteration 117/1000 | Loss: 0.00002013
Iteration 118/1000 | Loss: 0.00002013
Iteration 119/1000 | Loss: 0.00002013
Iteration 120/1000 | Loss: 0.00002013
Iteration 121/1000 | Loss: 0.00002013
Iteration 122/1000 | Loss: 0.00002013
Iteration 123/1000 | Loss: 0.00002012
Iteration 124/1000 | Loss: 0.00002012
Iteration 125/1000 | Loss: 0.00002012
Iteration 126/1000 | Loss: 0.00002012
Iteration 127/1000 | Loss: 0.00002012
Iteration 128/1000 | Loss: 0.00002012
Iteration 129/1000 | Loss: 0.00002012
Iteration 130/1000 | Loss: 0.00002012
Iteration 131/1000 | Loss: 0.00002012
Iteration 132/1000 | Loss: 0.00002012
Iteration 133/1000 | Loss: 0.00002012
Iteration 134/1000 | Loss: 0.00002012
Iteration 135/1000 | Loss: 0.00002012
Iteration 136/1000 | Loss: 0.00002012
Iteration 137/1000 | Loss: 0.00002012
Iteration 138/1000 | Loss: 0.00002012
Iteration 139/1000 | Loss: 0.00002011
Iteration 140/1000 | Loss: 0.00002011
Iteration 141/1000 | Loss: 0.00002011
Iteration 142/1000 | Loss: 0.00002011
Iteration 143/1000 | Loss: 0.00002011
Iteration 144/1000 | Loss: 0.00002011
Iteration 145/1000 | Loss: 0.00002011
Iteration 146/1000 | Loss: 0.00002011
Iteration 147/1000 | Loss: 0.00002011
Iteration 148/1000 | Loss: 0.00002011
Iteration 149/1000 | Loss: 0.00002010
Iteration 150/1000 | Loss: 0.00002010
Iteration 151/1000 | Loss: 0.00002010
Iteration 152/1000 | Loss: 0.00002010
Iteration 153/1000 | Loss: 0.00002010
Iteration 154/1000 | Loss: 0.00002010
Iteration 155/1000 | Loss: 0.00002009
Iteration 156/1000 | Loss: 0.00002009
Iteration 157/1000 | Loss: 0.00002009
Iteration 158/1000 | Loss: 0.00002009
Iteration 159/1000 | Loss: 0.00002008
Iteration 160/1000 | Loss: 0.00002008
Iteration 161/1000 | Loss: 0.00002008
Iteration 162/1000 | Loss: 0.00002008
Iteration 163/1000 | Loss: 0.00002008
Iteration 164/1000 | Loss: 0.00002008
Iteration 165/1000 | Loss: 0.00002008
Iteration 166/1000 | Loss: 0.00002008
Iteration 167/1000 | Loss: 0.00002008
Iteration 168/1000 | Loss: 0.00002008
Iteration 169/1000 | Loss: 0.00002008
Iteration 170/1000 | Loss: 0.00002008
Iteration 171/1000 | Loss: 0.00002008
Iteration 172/1000 | Loss: 0.00002008
Iteration 173/1000 | Loss: 0.00002008
Iteration 174/1000 | Loss: 0.00002008
Iteration 175/1000 | Loss: 0.00002008
Iteration 176/1000 | Loss: 0.00002008
Iteration 177/1000 | Loss: 0.00002008
Iteration 178/1000 | Loss: 0.00002008
Iteration 179/1000 | Loss: 0.00002008
Iteration 180/1000 | Loss: 0.00002008
Iteration 181/1000 | Loss: 0.00002008
Iteration 182/1000 | Loss: 0.00002008
Iteration 183/1000 | Loss: 0.00002008
Iteration 184/1000 | Loss: 0.00002008
Iteration 185/1000 | Loss: 0.00002008
Iteration 186/1000 | Loss: 0.00002008
Iteration 187/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.007567854889203e-05, 2.007567854889203e-05, 2.007567854889203e-05, 2.007567854889203e-05, 2.007567854889203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.007567854889203e-05

Optimization complete. Final v2v error: 3.7289061546325684 mm

Highest mean error: 4.439868927001953 mm for frame 21

Lowest mean error: 3.4252562522888184 mm for frame 31

Saving results

Total time: 116.8258764743805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821664
Iteration 2/25 | Loss: 0.00119149
Iteration 3/25 | Loss: 0.00107870
Iteration 4/25 | Loss: 0.00106478
Iteration 5/25 | Loss: 0.00106174
Iteration 6/25 | Loss: 0.00106174
Iteration 7/25 | Loss: 0.00106174
Iteration 8/25 | Loss: 0.00106174
Iteration 9/25 | Loss: 0.00106174
Iteration 10/25 | Loss: 0.00106174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010617367224767804, 0.0010617367224767804, 0.0010617367224767804, 0.0010617367224767804, 0.0010617367224767804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010617367224767804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34658253
Iteration 2/25 | Loss: 0.00077000
Iteration 3/25 | Loss: 0.00076999
Iteration 4/25 | Loss: 0.00076999
Iteration 5/25 | Loss: 0.00076999
Iteration 6/25 | Loss: 0.00076999
Iteration 7/25 | Loss: 0.00076999
Iteration 8/25 | Loss: 0.00076999
Iteration 9/25 | Loss: 0.00076999
Iteration 10/25 | Loss: 0.00076999
Iteration 11/25 | Loss: 0.00076999
Iteration 12/25 | Loss: 0.00076999
Iteration 13/25 | Loss: 0.00076999
Iteration 14/25 | Loss: 0.00076999
Iteration 15/25 | Loss: 0.00076999
Iteration 16/25 | Loss: 0.00076999
Iteration 17/25 | Loss: 0.00076999
Iteration 18/25 | Loss: 0.00076999
Iteration 19/25 | Loss: 0.00076999
Iteration 20/25 | Loss: 0.00076999
Iteration 21/25 | Loss: 0.00076999
Iteration 22/25 | Loss: 0.00076999
Iteration 23/25 | Loss: 0.00076999
Iteration 24/25 | Loss: 0.00076999
Iteration 25/25 | Loss: 0.00076999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076999
Iteration 2/1000 | Loss: 0.00001873
Iteration 3/1000 | Loss: 0.00001344
Iteration 4/1000 | Loss: 0.00001199
Iteration 5/1000 | Loss: 0.00001120
Iteration 6/1000 | Loss: 0.00001051
Iteration 7/1000 | Loss: 0.00001012
Iteration 8/1000 | Loss: 0.00000985
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000953
Iteration 11/1000 | Loss: 0.00000952
Iteration 12/1000 | Loss: 0.00000947
Iteration 13/1000 | Loss: 0.00000941
Iteration 14/1000 | Loss: 0.00000940
Iteration 15/1000 | Loss: 0.00000939
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000933
Iteration 18/1000 | Loss: 0.00000933
Iteration 19/1000 | Loss: 0.00000932
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000929
Iteration 22/1000 | Loss: 0.00000929
Iteration 23/1000 | Loss: 0.00000929
Iteration 24/1000 | Loss: 0.00000929
Iteration 25/1000 | Loss: 0.00000929
Iteration 26/1000 | Loss: 0.00000929
Iteration 27/1000 | Loss: 0.00000929
Iteration 28/1000 | Loss: 0.00000929
Iteration 29/1000 | Loss: 0.00000929
Iteration 30/1000 | Loss: 0.00000928
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000927
Iteration 34/1000 | Loss: 0.00000925
Iteration 35/1000 | Loss: 0.00000924
Iteration 36/1000 | Loss: 0.00000924
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000924
Iteration 41/1000 | Loss: 0.00000923
Iteration 42/1000 | Loss: 0.00000923
Iteration 43/1000 | Loss: 0.00000923
Iteration 44/1000 | Loss: 0.00000922
Iteration 45/1000 | Loss: 0.00000922
Iteration 46/1000 | Loss: 0.00000922
Iteration 47/1000 | Loss: 0.00000920
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000919
Iteration 50/1000 | Loss: 0.00000919
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000918
Iteration 53/1000 | Loss: 0.00000918
Iteration 54/1000 | Loss: 0.00000918
Iteration 55/1000 | Loss: 0.00000918
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000915
Iteration 58/1000 | Loss: 0.00000915
Iteration 59/1000 | Loss: 0.00000915
Iteration 60/1000 | Loss: 0.00000915
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000915
Iteration 63/1000 | Loss: 0.00000915
Iteration 64/1000 | Loss: 0.00000915
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000914
Iteration 71/1000 | Loss: 0.00000914
Iteration 72/1000 | Loss: 0.00000914
Iteration 73/1000 | Loss: 0.00000914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [9.143447641690727e-06, 9.143447641690727e-06, 9.143447641690727e-06, 9.143447641690727e-06, 9.143447641690727e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.143447641690727e-06

Optimization complete. Final v2v error: 2.6208221912384033 mm

Highest mean error: 2.849400281906128 mm for frame 104

Lowest mean error: 2.455918073654175 mm for frame 5

Saving results

Total time: 31.694462299346924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00338461
Iteration 2/25 | Loss: 0.00119433
Iteration 3/25 | Loss: 0.00107888
Iteration 4/25 | Loss: 0.00106793
Iteration 5/25 | Loss: 0.00106402
Iteration 6/25 | Loss: 0.00106299
Iteration 7/25 | Loss: 0.00106299
Iteration 8/25 | Loss: 0.00106299
Iteration 9/25 | Loss: 0.00106299
Iteration 10/25 | Loss: 0.00106299
Iteration 11/25 | Loss: 0.00106299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010629931930452585, 0.0010629931930452585, 0.0010629931930452585, 0.0010629931930452585, 0.0010629931930452585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010629931930452585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31929076
Iteration 2/25 | Loss: 0.00092388
Iteration 3/25 | Loss: 0.00092388
Iteration 4/25 | Loss: 0.00092388
Iteration 5/25 | Loss: 0.00092388
Iteration 6/25 | Loss: 0.00092388
Iteration 7/25 | Loss: 0.00092388
Iteration 8/25 | Loss: 0.00092388
Iteration 9/25 | Loss: 0.00092388
Iteration 10/25 | Loss: 0.00092388
Iteration 11/25 | Loss: 0.00092388
Iteration 12/25 | Loss: 0.00092388
Iteration 13/25 | Loss: 0.00092388
Iteration 14/25 | Loss: 0.00092388
Iteration 15/25 | Loss: 0.00092388
Iteration 16/25 | Loss: 0.00092388
Iteration 17/25 | Loss: 0.00092388
Iteration 18/25 | Loss: 0.00092388
Iteration 19/25 | Loss: 0.00092388
Iteration 20/25 | Loss: 0.00092388
Iteration 21/25 | Loss: 0.00092388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000923875835724175, 0.000923875835724175, 0.000923875835724175, 0.000923875835724175, 0.000923875835724175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000923875835724175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092388
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001210
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001151
Iteration 16/1000 | Loss: 0.00001149
Iteration 17/1000 | Loss: 0.00001149
Iteration 18/1000 | Loss: 0.00001147
Iteration 19/1000 | Loss: 0.00001147
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001131
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001128
Iteration 48/1000 | Loss: 0.00001127
Iteration 49/1000 | Loss: 0.00001127
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001123
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001120
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001117
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001115
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001104
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001103
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001099
Iteration 137/1000 | Loss: 0.00001099
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001098
Iteration 140/1000 | Loss: 0.00001098
Iteration 141/1000 | Loss: 0.00001098
Iteration 142/1000 | Loss: 0.00001098
Iteration 143/1000 | Loss: 0.00001098
Iteration 144/1000 | Loss: 0.00001098
Iteration 145/1000 | Loss: 0.00001097
Iteration 146/1000 | Loss: 0.00001097
Iteration 147/1000 | Loss: 0.00001097
Iteration 148/1000 | Loss: 0.00001097
Iteration 149/1000 | Loss: 0.00001097
Iteration 150/1000 | Loss: 0.00001097
Iteration 151/1000 | Loss: 0.00001097
Iteration 152/1000 | Loss: 0.00001096
Iteration 153/1000 | Loss: 0.00001096
Iteration 154/1000 | Loss: 0.00001096
Iteration 155/1000 | Loss: 0.00001096
Iteration 156/1000 | Loss: 0.00001096
Iteration 157/1000 | Loss: 0.00001096
Iteration 158/1000 | Loss: 0.00001095
Iteration 159/1000 | Loss: 0.00001095
Iteration 160/1000 | Loss: 0.00001095
Iteration 161/1000 | Loss: 0.00001095
Iteration 162/1000 | Loss: 0.00001095
Iteration 163/1000 | Loss: 0.00001095
Iteration 164/1000 | Loss: 0.00001095
Iteration 165/1000 | Loss: 0.00001095
Iteration 166/1000 | Loss: 0.00001095
Iteration 167/1000 | Loss: 0.00001095
Iteration 168/1000 | Loss: 0.00001095
Iteration 169/1000 | Loss: 0.00001095
Iteration 170/1000 | Loss: 0.00001095
Iteration 171/1000 | Loss: 0.00001095
Iteration 172/1000 | Loss: 0.00001095
Iteration 173/1000 | Loss: 0.00001095
Iteration 174/1000 | Loss: 0.00001095
Iteration 175/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.0947401278826874e-05, 1.0947401278826874e-05, 1.0947401278826874e-05, 1.0947401278826874e-05, 1.0947401278826874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0947401278826874e-05

Optimization complete. Final v2v error: 2.842660903930664 mm

Highest mean error: 3.442725419998169 mm for frame 22

Lowest mean error: 2.5349605083465576 mm for frame 255

Saving results

Total time: 45.5670690536499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829915
Iteration 2/25 | Loss: 0.00118371
Iteration 3/25 | Loss: 0.00108680
Iteration 4/25 | Loss: 0.00107825
Iteration 5/25 | Loss: 0.00107666
Iteration 6/25 | Loss: 0.00107666
Iteration 7/25 | Loss: 0.00107666
Iteration 8/25 | Loss: 0.00107666
Iteration 9/25 | Loss: 0.00107666
Iteration 10/25 | Loss: 0.00107666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010766646591946483, 0.0010766646591946483, 0.0010766646591946483, 0.0010766646591946483, 0.0010766646591946483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010766646591946483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34215820
Iteration 2/25 | Loss: 0.00075768
Iteration 3/25 | Loss: 0.00075768
Iteration 4/25 | Loss: 0.00075767
Iteration 5/25 | Loss: 0.00075767
Iteration 6/25 | Loss: 0.00075767
Iteration 7/25 | Loss: 0.00075767
Iteration 8/25 | Loss: 0.00075767
Iteration 9/25 | Loss: 0.00075767
Iteration 10/25 | Loss: 0.00075767
Iteration 11/25 | Loss: 0.00075767
Iteration 12/25 | Loss: 0.00075767
Iteration 13/25 | Loss: 0.00075767
Iteration 14/25 | Loss: 0.00075767
Iteration 15/25 | Loss: 0.00075767
Iteration 16/25 | Loss: 0.00075767
Iteration 17/25 | Loss: 0.00075767
Iteration 18/25 | Loss: 0.00075767
Iteration 19/25 | Loss: 0.00075767
Iteration 20/25 | Loss: 0.00075767
Iteration 21/25 | Loss: 0.00075767
Iteration 22/25 | Loss: 0.00075767
Iteration 23/25 | Loss: 0.00075767
Iteration 24/25 | Loss: 0.00075767
Iteration 25/25 | Loss: 0.00075767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075767
Iteration 2/1000 | Loss: 0.00001712
Iteration 3/1000 | Loss: 0.00001320
Iteration 4/1000 | Loss: 0.00001203
Iteration 5/1000 | Loss: 0.00001123
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001015
Iteration 10/1000 | Loss: 0.00001004
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00000992
Iteration 13/1000 | Loss: 0.00000992
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000988
Iteration 16/1000 | Loss: 0.00000986
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000979
Iteration 19/1000 | Loss: 0.00000978
Iteration 20/1000 | Loss: 0.00000977
Iteration 21/1000 | Loss: 0.00000975
Iteration 22/1000 | Loss: 0.00000971
Iteration 23/1000 | Loss: 0.00000966
Iteration 24/1000 | Loss: 0.00000966
Iteration 25/1000 | Loss: 0.00000958
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000945
Iteration 30/1000 | Loss: 0.00000944
Iteration 31/1000 | Loss: 0.00000943
Iteration 32/1000 | Loss: 0.00000943
Iteration 33/1000 | Loss: 0.00000941
Iteration 34/1000 | Loss: 0.00000941
Iteration 35/1000 | Loss: 0.00000941
Iteration 36/1000 | Loss: 0.00000941
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000936
Iteration 48/1000 | Loss: 0.00000936
Iteration 49/1000 | Loss: 0.00000936
Iteration 50/1000 | Loss: 0.00000935
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000935
Iteration 53/1000 | Loss: 0.00000935
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000934
Iteration 56/1000 | Loss: 0.00000933
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000933
Iteration 60/1000 | Loss: 0.00000932
Iteration 61/1000 | Loss: 0.00000932
Iteration 62/1000 | Loss: 0.00000932
Iteration 63/1000 | Loss: 0.00000932
Iteration 64/1000 | Loss: 0.00000931
Iteration 65/1000 | Loss: 0.00000931
Iteration 66/1000 | Loss: 0.00000930
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000929
Iteration 70/1000 | Loss: 0.00000928
Iteration 71/1000 | Loss: 0.00000928
Iteration 72/1000 | Loss: 0.00000928
Iteration 73/1000 | Loss: 0.00000927
Iteration 74/1000 | Loss: 0.00000927
Iteration 75/1000 | Loss: 0.00000926
Iteration 76/1000 | Loss: 0.00000926
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000923
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000921
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000921
Iteration 105/1000 | Loss: 0.00000921
Iteration 106/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [9.207539733324666e-06, 9.207539733324666e-06, 9.207539733324666e-06, 9.207539733324666e-06, 9.207539733324666e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.207539733324666e-06

Optimization complete. Final v2v error: 2.5979230403900146 mm

Highest mean error: 2.7623989582061768 mm for frame 0

Lowest mean error: 2.408794403076172 mm for frame 222

Saving results

Total time: 38.02707600593567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379918
Iteration 2/25 | Loss: 0.00153609
Iteration 3/25 | Loss: 0.00124594
Iteration 4/25 | Loss: 0.00121377
Iteration 5/25 | Loss: 0.00120574
Iteration 6/25 | Loss: 0.00120404
Iteration 7/25 | Loss: 0.00120362
Iteration 8/25 | Loss: 0.00120362
Iteration 9/25 | Loss: 0.00120362
Iteration 10/25 | Loss: 0.00120362
Iteration 11/25 | Loss: 0.00120362
Iteration 12/25 | Loss: 0.00120362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012036231346428394, 0.0012036231346428394, 0.0012036231346428394, 0.0012036231346428394, 0.0012036231346428394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012036231346428394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26471257
Iteration 2/25 | Loss: 0.00108458
Iteration 3/25 | Loss: 0.00108458
Iteration 4/25 | Loss: 0.00108458
Iteration 5/25 | Loss: 0.00108458
Iteration 6/25 | Loss: 0.00108458
Iteration 7/25 | Loss: 0.00108458
Iteration 8/25 | Loss: 0.00108458
Iteration 9/25 | Loss: 0.00108458
Iteration 10/25 | Loss: 0.00108458
Iteration 11/25 | Loss: 0.00108458
Iteration 12/25 | Loss: 0.00108458
Iteration 13/25 | Loss: 0.00108458
Iteration 14/25 | Loss: 0.00108458
Iteration 15/25 | Loss: 0.00108458
Iteration 16/25 | Loss: 0.00108458
Iteration 17/25 | Loss: 0.00108458
Iteration 18/25 | Loss: 0.00108458
Iteration 19/25 | Loss: 0.00108458
Iteration 20/25 | Loss: 0.00108458
Iteration 21/25 | Loss: 0.00108458
Iteration 22/25 | Loss: 0.00108458
Iteration 23/25 | Loss: 0.00108458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010845809010788798, 0.0010845809010788798, 0.0010845809010788798, 0.0010845809010788798, 0.0010845809010788798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010845809010788798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108458
Iteration 2/1000 | Loss: 0.00003789
Iteration 3/1000 | Loss: 0.00002100
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001509
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001283
Iteration 22/1000 | Loss: 0.00001282
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001282
Iteration 26/1000 | Loss: 0.00001282
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001280
Iteration 31/1000 | Loss: 0.00001280
Iteration 32/1000 | Loss: 0.00001280
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001279
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001278
Iteration 37/1000 | Loss: 0.00001278
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001277
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001277
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001276
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001276
Iteration 48/1000 | Loss: 0.00001276
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001274
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001274
Iteration 72/1000 | Loss: 0.00001274
Iteration 73/1000 | Loss: 0.00001274
Iteration 74/1000 | Loss: 0.00001273
Iteration 75/1000 | Loss: 0.00001273
Iteration 76/1000 | Loss: 0.00001273
Iteration 77/1000 | Loss: 0.00001273
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001273
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001273
Iteration 85/1000 | Loss: 0.00001273
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001273
Iteration 88/1000 | Loss: 0.00001273
Iteration 89/1000 | Loss: 0.00001273
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.2725314263661858e-05, 1.2725314263661858e-05, 1.2725314263661858e-05, 1.2725314263661858e-05, 1.2725314263661858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2725314263661858e-05

Optimization complete. Final v2v error: 3.0358893871307373 mm

Highest mean error: 3.246168375015259 mm for frame 103

Lowest mean error: 2.877761125564575 mm for frame 25

Saving results

Total time: 33.56060314178467
