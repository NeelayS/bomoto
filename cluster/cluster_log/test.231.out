Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=231, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12936-12991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549654
Iteration 2/25 | Loss: 0.00152919
Iteration 3/25 | Loss: 0.00142212
Iteration 4/25 | Loss: 0.00140441
Iteration 5/25 | Loss: 0.00139658
Iteration 6/25 | Loss: 0.00139404
Iteration 7/25 | Loss: 0.00139404
Iteration 8/25 | Loss: 0.00139404
Iteration 9/25 | Loss: 0.00139404
Iteration 10/25 | Loss: 0.00139404
Iteration 11/25 | Loss: 0.00139404
Iteration 12/25 | Loss: 0.00139404
Iteration 13/25 | Loss: 0.00139404
Iteration 14/25 | Loss: 0.00139404
Iteration 15/25 | Loss: 0.00139404
Iteration 16/25 | Loss: 0.00139404
Iteration 17/25 | Loss: 0.00139404
Iteration 18/25 | Loss: 0.00139404
Iteration 19/25 | Loss: 0.00139404
Iteration 20/25 | Loss: 0.00139404
Iteration 21/25 | Loss: 0.00139404
Iteration 22/25 | Loss: 0.00139404
Iteration 23/25 | Loss: 0.00139404
Iteration 24/25 | Loss: 0.00139404
Iteration 25/25 | Loss: 0.00139404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54482484
Iteration 2/25 | Loss: 0.00135551
Iteration 3/25 | Loss: 0.00135551
Iteration 4/25 | Loss: 0.00135551
Iteration 5/25 | Loss: 0.00135551
Iteration 6/25 | Loss: 0.00135551
Iteration 7/25 | Loss: 0.00135551
Iteration 8/25 | Loss: 0.00135551
Iteration 9/25 | Loss: 0.00135551
Iteration 10/25 | Loss: 0.00135551
Iteration 11/25 | Loss: 0.00135551
Iteration 12/25 | Loss: 0.00135551
Iteration 13/25 | Loss: 0.00135551
Iteration 14/25 | Loss: 0.00135551
Iteration 15/25 | Loss: 0.00135551
Iteration 16/25 | Loss: 0.00135551
Iteration 17/25 | Loss: 0.00135551
Iteration 18/25 | Loss: 0.00135551
Iteration 19/25 | Loss: 0.00135551
Iteration 20/25 | Loss: 0.00135551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001355511019937694, 0.001355511019937694, 0.001355511019937694, 0.001355511019937694, 0.001355511019937694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355511019937694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135551
Iteration 2/1000 | Loss: 0.00005735
Iteration 3/1000 | Loss: 0.00003405
Iteration 4/1000 | Loss: 0.00002839
Iteration 5/1000 | Loss: 0.00002630
Iteration 6/1000 | Loss: 0.00002504
Iteration 7/1000 | Loss: 0.00002439
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002375
Iteration 10/1000 | Loss: 0.00002369
Iteration 11/1000 | Loss: 0.00002363
Iteration 12/1000 | Loss: 0.00002353
Iteration 13/1000 | Loss: 0.00002353
Iteration 14/1000 | Loss: 0.00002351
Iteration 15/1000 | Loss: 0.00002348
Iteration 16/1000 | Loss: 0.00002347
Iteration 17/1000 | Loss: 0.00002347
Iteration 18/1000 | Loss: 0.00002347
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002346
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00002345
Iteration 23/1000 | Loss: 0.00002345
Iteration 24/1000 | Loss: 0.00002344
Iteration 25/1000 | Loss: 0.00002343
Iteration 26/1000 | Loss: 0.00002343
Iteration 27/1000 | Loss: 0.00002343
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002341
Iteration 30/1000 | Loss: 0.00002341
Iteration 31/1000 | Loss: 0.00002341
Iteration 32/1000 | Loss: 0.00002340
Iteration 33/1000 | Loss: 0.00002340
Iteration 34/1000 | Loss: 0.00002339
Iteration 35/1000 | Loss: 0.00002339
Iteration 36/1000 | Loss: 0.00002339
Iteration 37/1000 | Loss: 0.00002339
Iteration 38/1000 | Loss: 0.00002339
Iteration 39/1000 | Loss: 0.00002339
Iteration 40/1000 | Loss: 0.00002339
Iteration 41/1000 | Loss: 0.00002339
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002338
Iteration 45/1000 | Loss: 0.00002338
Iteration 46/1000 | Loss: 0.00002337
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002337
Iteration 49/1000 | Loss: 0.00002337
Iteration 50/1000 | Loss: 0.00002337
Iteration 51/1000 | Loss: 0.00002336
Iteration 52/1000 | Loss: 0.00002336
Iteration 53/1000 | Loss: 0.00002336
Iteration 54/1000 | Loss: 0.00002335
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00002335
Iteration 57/1000 | Loss: 0.00002334
Iteration 58/1000 | Loss: 0.00002333
Iteration 59/1000 | Loss: 0.00002332
Iteration 60/1000 | Loss: 0.00002332
Iteration 61/1000 | Loss: 0.00002332
Iteration 62/1000 | Loss: 0.00002331
Iteration 63/1000 | Loss: 0.00002331
Iteration 64/1000 | Loss: 0.00002331
Iteration 65/1000 | Loss: 0.00002330
Iteration 66/1000 | Loss: 0.00002330
Iteration 67/1000 | Loss: 0.00002329
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002325
Iteration 72/1000 | Loss: 0.00002325
Iteration 73/1000 | Loss: 0.00002325
Iteration 74/1000 | Loss: 0.00002325
Iteration 75/1000 | Loss: 0.00002325
Iteration 76/1000 | Loss: 0.00002325
Iteration 77/1000 | Loss: 0.00002324
Iteration 78/1000 | Loss: 0.00002324
Iteration 79/1000 | Loss: 0.00002324
Iteration 80/1000 | Loss: 0.00002323
Iteration 81/1000 | Loss: 0.00002323
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002323
Iteration 84/1000 | Loss: 0.00002322
Iteration 85/1000 | Loss: 0.00002322
Iteration 86/1000 | Loss: 0.00002322
Iteration 87/1000 | Loss: 0.00002322
Iteration 88/1000 | Loss: 0.00002322
Iteration 89/1000 | Loss: 0.00002322
Iteration 90/1000 | Loss: 0.00002322
Iteration 91/1000 | Loss: 0.00002321
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002321
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002319
Iteration 98/1000 | Loss: 0.00002319
Iteration 99/1000 | Loss: 0.00002319
Iteration 100/1000 | Loss: 0.00002319
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002318
Iteration 103/1000 | Loss: 0.00002318
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002318
Iteration 107/1000 | Loss: 0.00002318
Iteration 108/1000 | Loss: 0.00002318
Iteration 109/1000 | Loss: 0.00002318
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002317
Iteration 115/1000 | Loss: 0.00002317
Iteration 116/1000 | Loss: 0.00002317
Iteration 117/1000 | Loss: 0.00002317
Iteration 118/1000 | Loss: 0.00002317
Iteration 119/1000 | Loss: 0.00002317
Iteration 120/1000 | Loss: 0.00002317
Iteration 121/1000 | Loss: 0.00002317
Iteration 122/1000 | Loss: 0.00002317
Iteration 123/1000 | Loss: 0.00002317
Iteration 124/1000 | Loss: 0.00002317
Iteration 125/1000 | Loss: 0.00002317
Iteration 126/1000 | Loss: 0.00002317
Iteration 127/1000 | Loss: 0.00002317
Iteration 128/1000 | Loss: 0.00002317
Iteration 129/1000 | Loss: 0.00002317
Iteration 130/1000 | Loss: 0.00002317
Iteration 131/1000 | Loss: 0.00002317
Iteration 132/1000 | Loss: 0.00002317
Iteration 133/1000 | Loss: 0.00002317
Iteration 134/1000 | Loss: 0.00002317
Iteration 135/1000 | Loss: 0.00002317
Iteration 136/1000 | Loss: 0.00002317
Iteration 137/1000 | Loss: 0.00002317
Iteration 138/1000 | Loss: 0.00002317
Iteration 139/1000 | Loss: 0.00002317
Iteration 140/1000 | Loss: 0.00002317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.3166825485532172e-05, 2.3166825485532172e-05, 2.3166825485532172e-05, 2.3166825485532172e-05, 2.3166825485532172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3166825485532172e-05

Optimization complete. Final v2v error: 4.204848289489746 mm

Highest mean error: 4.565428733825684 mm for frame 171

Lowest mean error: 3.8315603733062744 mm for frame 80

Saving results

Total time: 36.1166455745697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476600
Iteration 2/25 | Loss: 0.00144890
Iteration 3/25 | Loss: 0.00137792
Iteration 4/25 | Loss: 0.00136258
Iteration 5/25 | Loss: 0.00135825
Iteration 6/25 | Loss: 0.00135690
Iteration 7/25 | Loss: 0.00135677
Iteration 8/25 | Loss: 0.00135677
Iteration 9/25 | Loss: 0.00135677
Iteration 10/25 | Loss: 0.00135677
Iteration 11/25 | Loss: 0.00135677
Iteration 12/25 | Loss: 0.00135677
Iteration 13/25 | Loss: 0.00135677
Iteration 14/25 | Loss: 0.00135677
Iteration 15/25 | Loss: 0.00135677
Iteration 16/25 | Loss: 0.00135677
Iteration 17/25 | Loss: 0.00135677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013567651621997356, 0.0013567651621997356, 0.0013567651621997356, 0.0013567651621997356, 0.0013567651621997356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013567651621997356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63980770
Iteration 2/25 | Loss: 0.00113446
Iteration 3/25 | Loss: 0.00113446
Iteration 4/25 | Loss: 0.00113446
Iteration 5/25 | Loss: 0.00113446
Iteration 6/25 | Loss: 0.00113445
Iteration 7/25 | Loss: 0.00113445
Iteration 8/25 | Loss: 0.00113445
Iteration 9/25 | Loss: 0.00113445
Iteration 10/25 | Loss: 0.00113445
Iteration 11/25 | Loss: 0.00113445
Iteration 12/25 | Loss: 0.00113445
Iteration 13/25 | Loss: 0.00113445
Iteration 14/25 | Loss: 0.00113445
Iteration 15/25 | Loss: 0.00113445
Iteration 16/25 | Loss: 0.00113445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001134453690610826, 0.001134453690610826, 0.001134453690610826, 0.001134453690610826, 0.001134453690610826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001134453690610826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113445
Iteration 2/1000 | Loss: 0.00005462
Iteration 3/1000 | Loss: 0.00003496
Iteration 4/1000 | Loss: 0.00003155
Iteration 5/1000 | Loss: 0.00002997
Iteration 6/1000 | Loss: 0.00002900
Iteration 7/1000 | Loss: 0.00002824
Iteration 8/1000 | Loss: 0.00002768
Iteration 9/1000 | Loss: 0.00002733
Iteration 10/1000 | Loss: 0.00002714
Iteration 11/1000 | Loss: 0.00002713
Iteration 12/1000 | Loss: 0.00002701
Iteration 13/1000 | Loss: 0.00002694
Iteration 14/1000 | Loss: 0.00002689
Iteration 15/1000 | Loss: 0.00002686
Iteration 16/1000 | Loss: 0.00002686
Iteration 17/1000 | Loss: 0.00002686
Iteration 18/1000 | Loss: 0.00002685
Iteration 19/1000 | Loss: 0.00002685
Iteration 20/1000 | Loss: 0.00002685
Iteration 21/1000 | Loss: 0.00002684
Iteration 22/1000 | Loss: 0.00002684
Iteration 23/1000 | Loss: 0.00002684
Iteration 24/1000 | Loss: 0.00002684
Iteration 25/1000 | Loss: 0.00002683
Iteration 26/1000 | Loss: 0.00002683
Iteration 27/1000 | Loss: 0.00002683
Iteration 28/1000 | Loss: 0.00002683
Iteration 29/1000 | Loss: 0.00002683
Iteration 30/1000 | Loss: 0.00002683
Iteration 31/1000 | Loss: 0.00002683
Iteration 32/1000 | Loss: 0.00002682
Iteration 33/1000 | Loss: 0.00002682
Iteration 34/1000 | Loss: 0.00002682
Iteration 35/1000 | Loss: 0.00002682
Iteration 36/1000 | Loss: 0.00002682
Iteration 37/1000 | Loss: 0.00002682
Iteration 38/1000 | Loss: 0.00002681
Iteration 39/1000 | Loss: 0.00002681
Iteration 40/1000 | Loss: 0.00002681
Iteration 41/1000 | Loss: 0.00002681
Iteration 42/1000 | Loss: 0.00002680
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002680
Iteration 45/1000 | Loss: 0.00002680
Iteration 46/1000 | Loss: 0.00002680
Iteration 47/1000 | Loss: 0.00002679
Iteration 48/1000 | Loss: 0.00002679
Iteration 49/1000 | Loss: 0.00002679
Iteration 50/1000 | Loss: 0.00002679
Iteration 51/1000 | Loss: 0.00002679
Iteration 52/1000 | Loss: 0.00002679
Iteration 53/1000 | Loss: 0.00002679
Iteration 54/1000 | Loss: 0.00002679
Iteration 55/1000 | Loss: 0.00002679
Iteration 56/1000 | Loss: 0.00002679
Iteration 57/1000 | Loss: 0.00002679
Iteration 58/1000 | Loss: 0.00002679
Iteration 59/1000 | Loss: 0.00002679
Iteration 60/1000 | Loss: 0.00002679
Iteration 61/1000 | Loss: 0.00002679
Iteration 62/1000 | Loss: 0.00002679
Iteration 63/1000 | Loss: 0.00002679
Iteration 64/1000 | Loss: 0.00002679
Iteration 65/1000 | Loss: 0.00002679
Iteration 66/1000 | Loss: 0.00002679
Iteration 67/1000 | Loss: 0.00002679
Iteration 68/1000 | Loss: 0.00002679
Iteration 69/1000 | Loss: 0.00002679
Iteration 70/1000 | Loss: 0.00002679
Iteration 71/1000 | Loss: 0.00002679
Iteration 72/1000 | Loss: 0.00002679
Iteration 73/1000 | Loss: 0.00002679
Iteration 74/1000 | Loss: 0.00002679
Iteration 75/1000 | Loss: 0.00002679
Iteration 76/1000 | Loss: 0.00002679
Iteration 77/1000 | Loss: 0.00002679
Iteration 78/1000 | Loss: 0.00002679
Iteration 79/1000 | Loss: 0.00002679
Iteration 80/1000 | Loss: 0.00002679
Iteration 81/1000 | Loss: 0.00002679
Iteration 82/1000 | Loss: 0.00002679
Iteration 83/1000 | Loss: 0.00002679
Iteration 84/1000 | Loss: 0.00002679
Iteration 85/1000 | Loss: 0.00002679
Iteration 86/1000 | Loss: 0.00002679
Iteration 87/1000 | Loss: 0.00002679
Iteration 88/1000 | Loss: 0.00002679
Iteration 89/1000 | Loss: 0.00002679
Iteration 90/1000 | Loss: 0.00002679
Iteration 91/1000 | Loss: 0.00002679
Iteration 92/1000 | Loss: 0.00002679
Iteration 93/1000 | Loss: 0.00002679
Iteration 94/1000 | Loss: 0.00002679
Iteration 95/1000 | Loss: 0.00002679
Iteration 96/1000 | Loss: 0.00002679
Iteration 97/1000 | Loss: 0.00002679
Iteration 98/1000 | Loss: 0.00002679
Iteration 99/1000 | Loss: 0.00002679
Iteration 100/1000 | Loss: 0.00002679
Iteration 101/1000 | Loss: 0.00002679
Iteration 102/1000 | Loss: 0.00002679
Iteration 103/1000 | Loss: 0.00002679
Iteration 104/1000 | Loss: 0.00002679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.6792267817654647e-05, 2.6792267817654647e-05, 2.6792267817654647e-05, 2.6792267817654647e-05, 2.6792267817654647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6792267817654647e-05

Optimization complete. Final v2v error: 4.468562602996826 mm

Highest mean error: 4.909422874450684 mm for frame 89

Lowest mean error: 4.034513473510742 mm for frame 130

Saving results

Total time: 30.42378854751587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097780
Iteration 2/25 | Loss: 0.00168898
Iteration 3/25 | Loss: 0.00154961
Iteration 4/25 | Loss: 0.00144752
Iteration 5/25 | Loss: 0.00142319
Iteration 6/25 | Loss: 0.00140827
Iteration 7/25 | Loss: 0.00140546
Iteration 8/25 | Loss: 0.00140473
Iteration 9/25 | Loss: 0.00140628
Iteration 10/25 | Loss: 0.00140518
Iteration 11/25 | Loss: 0.00140384
Iteration 12/25 | Loss: 0.00140321
Iteration 13/25 | Loss: 0.00140286
Iteration 14/25 | Loss: 0.00140277
Iteration 15/25 | Loss: 0.00140276
Iteration 16/25 | Loss: 0.00140276
Iteration 17/25 | Loss: 0.00140276
Iteration 18/25 | Loss: 0.00140276
Iteration 19/25 | Loss: 0.00140275
Iteration 20/25 | Loss: 0.00140275
Iteration 21/25 | Loss: 0.00140275
Iteration 22/25 | Loss: 0.00140275
Iteration 23/25 | Loss: 0.00140275
Iteration 24/25 | Loss: 0.00140275
Iteration 25/25 | Loss: 0.00140275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44241273
Iteration 2/25 | Loss: 0.00129020
Iteration 3/25 | Loss: 0.00129019
Iteration 4/25 | Loss: 0.00129019
Iteration 5/25 | Loss: 0.00129019
Iteration 6/25 | Loss: 0.00129019
Iteration 7/25 | Loss: 0.00129019
Iteration 8/25 | Loss: 0.00129019
Iteration 9/25 | Loss: 0.00129019
Iteration 10/25 | Loss: 0.00129019
Iteration 11/25 | Loss: 0.00129019
Iteration 12/25 | Loss: 0.00129019
Iteration 13/25 | Loss: 0.00129019
Iteration 14/25 | Loss: 0.00129019
Iteration 15/25 | Loss: 0.00129019
Iteration 16/25 | Loss: 0.00129019
Iteration 17/25 | Loss: 0.00129019
Iteration 18/25 | Loss: 0.00129019
Iteration 19/25 | Loss: 0.00129019
Iteration 20/25 | Loss: 0.00129019
Iteration 21/25 | Loss: 0.00129019
Iteration 22/25 | Loss: 0.00129019
Iteration 23/25 | Loss: 0.00129019
Iteration 24/25 | Loss: 0.00129019
Iteration 25/25 | Loss: 0.00129019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012901934096589684, 0.0012901934096589684, 0.0012901934096589684, 0.0012901934096589684, 0.0012901934096589684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012901934096589684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129019
Iteration 2/1000 | Loss: 0.00006021
Iteration 3/1000 | Loss: 0.00004233
Iteration 4/1000 | Loss: 0.00003777
Iteration 5/1000 | Loss: 0.00003504
Iteration 6/1000 | Loss: 0.00003341
Iteration 7/1000 | Loss: 0.00003234
Iteration 8/1000 | Loss: 0.00003148
Iteration 9/1000 | Loss: 0.00003121
Iteration 10/1000 | Loss: 0.00003096
Iteration 11/1000 | Loss: 0.00003086
Iteration 12/1000 | Loss: 0.00003082
Iteration 13/1000 | Loss: 0.00003082
Iteration 14/1000 | Loss: 0.00003082
Iteration 15/1000 | Loss: 0.00003080
Iteration 16/1000 | Loss: 0.00003080
Iteration 17/1000 | Loss: 0.00003079
Iteration 18/1000 | Loss: 0.00003078
Iteration 19/1000 | Loss: 0.00003078
Iteration 20/1000 | Loss: 0.00003076
Iteration 21/1000 | Loss: 0.00003076
Iteration 22/1000 | Loss: 0.00003076
Iteration 23/1000 | Loss: 0.00003076
Iteration 24/1000 | Loss: 0.00003076
Iteration 25/1000 | Loss: 0.00003075
Iteration 26/1000 | Loss: 0.00003075
Iteration 27/1000 | Loss: 0.00003075
Iteration 28/1000 | Loss: 0.00003075
Iteration 29/1000 | Loss: 0.00003075
Iteration 30/1000 | Loss: 0.00003075
Iteration 31/1000 | Loss: 0.00003075
Iteration 32/1000 | Loss: 0.00003075
Iteration 33/1000 | Loss: 0.00003075
Iteration 34/1000 | Loss: 0.00003074
Iteration 35/1000 | Loss: 0.00003074
Iteration 36/1000 | Loss: 0.00003073
Iteration 37/1000 | Loss: 0.00003073
Iteration 38/1000 | Loss: 0.00003073
Iteration 39/1000 | Loss: 0.00003073
Iteration 40/1000 | Loss: 0.00003072
Iteration 41/1000 | Loss: 0.00003072
Iteration 42/1000 | Loss: 0.00003072
Iteration 43/1000 | Loss: 0.00003072
Iteration 44/1000 | Loss: 0.00003071
Iteration 45/1000 | Loss: 0.00003071
Iteration 46/1000 | Loss: 0.00003071
Iteration 47/1000 | Loss: 0.00003070
Iteration 48/1000 | Loss: 0.00003070
Iteration 49/1000 | Loss: 0.00003069
Iteration 50/1000 | Loss: 0.00003069
Iteration 51/1000 | Loss: 0.00003069
Iteration 52/1000 | Loss: 0.00003068
Iteration 53/1000 | Loss: 0.00003068
Iteration 54/1000 | Loss: 0.00003067
Iteration 55/1000 | Loss: 0.00003067
Iteration 56/1000 | Loss: 0.00003067
Iteration 57/1000 | Loss: 0.00003066
Iteration 58/1000 | Loss: 0.00003066
Iteration 59/1000 | Loss: 0.00003066
Iteration 60/1000 | Loss: 0.00003065
Iteration 61/1000 | Loss: 0.00003065
Iteration 62/1000 | Loss: 0.00003065
Iteration 63/1000 | Loss: 0.00003064
Iteration 64/1000 | Loss: 0.00003064
Iteration 65/1000 | Loss: 0.00003064
Iteration 66/1000 | Loss: 0.00003063
Iteration 67/1000 | Loss: 0.00003063
Iteration 68/1000 | Loss: 0.00003063
Iteration 69/1000 | Loss: 0.00003062
Iteration 70/1000 | Loss: 0.00003062
Iteration 71/1000 | Loss: 0.00003062
Iteration 72/1000 | Loss: 0.00003061
Iteration 73/1000 | Loss: 0.00003061
Iteration 74/1000 | Loss: 0.00003060
Iteration 75/1000 | Loss: 0.00003060
Iteration 76/1000 | Loss: 0.00003060
Iteration 77/1000 | Loss: 0.00003060
Iteration 78/1000 | Loss: 0.00003059
Iteration 79/1000 | Loss: 0.00003059
Iteration 80/1000 | Loss: 0.00003059
Iteration 81/1000 | Loss: 0.00003059
Iteration 82/1000 | Loss: 0.00003059
Iteration 83/1000 | Loss: 0.00003059
Iteration 84/1000 | Loss: 0.00003058
Iteration 85/1000 | Loss: 0.00003058
Iteration 86/1000 | Loss: 0.00003058
Iteration 87/1000 | Loss: 0.00003058
Iteration 88/1000 | Loss: 0.00003058
Iteration 89/1000 | Loss: 0.00003058
Iteration 90/1000 | Loss: 0.00003057
Iteration 91/1000 | Loss: 0.00003057
Iteration 92/1000 | Loss: 0.00003057
Iteration 93/1000 | Loss: 0.00003057
Iteration 94/1000 | Loss: 0.00003057
Iteration 95/1000 | Loss: 0.00003057
Iteration 96/1000 | Loss: 0.00003057
Iteration 97/1000 | Loss: 0.00003056
Iteration 98/1000 | Loss: 0.00003056
Iteration 99/1000 | Loss: 0.00003056
Iteration 100/1000 | Loss: 0.00003056
Iteration 101/1000 | Loss: 0.00003056
Iteration 102/1000 | Loss: 0.00003056
Iteration 103/1000 | Loss: 0.00003056
Iteration 104/1000 | Loss: 0.00003056
Iteration 105/1000 | Loss: 0.00003056
Iteration 106/1000 | Loss: 0.00003056
Iteration 107/1000 | Loss: 0.00003056
Iteration 108/1000 | Loss: 0.00003056
Iteration 109/1000 | Loss: 0.00003056
Iteration 110/1000 | Loss: 0.00003056
Iteration 111/1000 | Loss: 0.00003056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.056238710996695e-05, 3.056238710996695e-05, 3.056238710996695e-05, 3.056238710996695e-05, 3.056238710996695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.056238710996695e-05

Optimization complete. Final v2v error: 4.7997870445251465 mm

Highest mean error: 5.169210910797119 mm for frame 108

Lowest mean error: 4.505927085876465 mm for frame 164

Saving results

Total time: 55.05786085128784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921735
Iteration 2/25 | Loss: 0.00163652
Iteration 3/25 | Loss: 0.00139626
Iteration 4/25 | Loss: 0.00135557
Iteration 5/25 | Loss: 0.00134583
Iteration 6/25 | Loss: 0.00134253
Iteration 7/25 | Loss: 0.00134132
Iteration 8/25 | Loss: 0.00134132
Iteration 9/25 | Loss: 0.00134132
Iteration 10/25 | Loss: 0.00134132
Iteration 11/25 | Loss: 0.00134132
Iteration 12/25 | Loss: 0.00134132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013413240667432547, 0.0013413240667432547, 0.0013413240667432547, 0.0013413240667432547, 0.0013413240667432547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013413240667432547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50997245
Iteration 2/25 | Loss: 0.00137854
Iteration 3/25 | Loss: 0.00137854
Iteration 4/25 | Loss: 0.00137854
Iteration 5/25 | Loss: 0.00137854
Iteration 6/25 | Loss: 0.00137854
Iteration 7/25 | Loss: 0.00137854
Iteration 8/25 | Loss: 0.00137854
Iteration 9/25 | Loss: 0.00137854
Iteration 10/25 | Loss: 0.00137854
Iteration 11/25 | Loss: 0.00137854
Iteration 12/25 | Loss: 0.00137854
Iteration 13/25 | Loss: 0.00137854
Iteration 14/25 | Loss: 0.00137854
Iteration 15/25 | Loss: 0.00137854
Iteration 16/25 | Loss: 0.00137854
Iteration 17/25 | Loss: 0.00137854
Iteration 18/25 | Loss: 0.00137854
Iteration 19/25 | Loss: 0.00137854
Iteration 20/25 | Loss: 0.00137854
Iteration 21/25 | Loss: 0.00137854
Iteration 22/25 | Loss: 0.00137854
Iteration 23/25 | Loss: 0.00137854
Iteration 24/25 | Loss: 0.00137854
Iteration 25/25 | Loss: 0.00137854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137854
Iteration 2/1000 | Loss: 0.00005935
Iteration 3/1000 | Loss: 0.00003583
Iteration 4/1000 | Loss: 0.00003010
Iteration 5/1000 | Loss: 0.00002607
Iteration 6/1000 | Loss: 0.00002415
Iteration 7/1000 | Loss: 0.00002261
Iteration 8/1000 | Loss: 0.00002164
Iteration 9/1000 | Loss: 0.00002078
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00001980
Iteration 12/1000 | Loss: 0.00001951
Iteration 13/1000 | Loss: 0.00001924
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001888
Iteration 24/1000 | Loss: 0.00001888
Iteration 25/1000 | Loss: 0.00001887
Iteration 26/1000 | Loss: 0.00001887
Iteration 27/1000 | Loss: 0.00001886
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001884
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001882
Iteration 35/1000 | Loss: 0.00001882
Iteration 36/1000 | Loss: 0.00001882
Iteration 37/1000 | Loss: 0.00001881
Iteration 38/1000 | Loss: 0.00001881
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001879
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001879
Iteration 46/1000 | Loss: 0.00001878
Iteration 47/1000 | Loss: 0.00001878
Iteration 48/1000 | Loss: 0.00001878
Iteration 49/1000 | Loss: 0.00001878
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001878
Iteration 53/1000 | Loss: 0.00001878
Iteration 54/1000 | Loss: 0.00001877
Iteration 55/1000 | Loss: 0.00001877
Iteration 56/1000 | Loss: 0.00001876
Iteration 57/1000 | Loss: 0.00001876
Iteration 58/1000 | Loss: 0.00001876
Iteration 59/1000 | Loss: 0.00001876
Iteration 60/1000 | Loss: 0.00001876
Iteration 61/1000 | Loss: 0.00001875
Iteration 62/1000 | Loss: 0.00001875
Iteration 63/1000 | Loss: 0.00001875
Iteration 64/1000 | Loss: 0.00001875
Iteration 65/1000 | Loss: 0.00001875
Iteration 66/1000 | Loss: 0.00001875
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001874
Iteration 69/1000 | Loss: 0.00001874
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001874
Iteration 72/1000 | Loss: 0.00001874
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001873
Iteration 75/1000 | Loss: 0.00001873
Iteration 76/1000 | Loss: 0.00001873
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001873
Iteration 82/1000 | Loss: 0.00001873
Iteration 83/1000 | Loss: 0.00001873
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001872
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001872
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001872
Iteration 93/1000 | Loss: 0.00001872
Iteration 94/1000 | Loss: 0.00001872
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001872
Iteration 115/1000 | Loss: 0.00001872
Iteration 116/1000 | Loss: 0.00001872
Iteration 117/1000 | Loss: 0.00001872
Iteration 118/1000 | Loss: 0.00001872
Iteration 119/1000 | Loss: 0.00001872
Iteration 120/1000 | Loss: 0.00001872
Iteration 121/1000 | Loss: 0.00001872
Iteration 122/1000 | Loss: 0.00001872
Iteration 123/1000 | Loss: 0.00001872
Iteration 124/1000 | Loss: 0.00001872
Iteration 125/1000 | Loss: 0.00001872
Iteration 126/1000 | Loss: 0.00001872
Iteration 127/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.871789208962582e-05, 1.871789208962582e-05, 1.871789208962582e-05, 1.871789208962582e-05, 1.871789208962582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.871789208962582e-05

Optimization complete. Final v2v error: 3.7688913345336914 mm

Highest mean error: 4.1530961990356445 mm for frame 86

Lowest mean error: 3.492191791534424 mm for frame 184

Saving results

Total time: 40.260945081710815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964615
Iteration 2/25 | Loss: 0.00160858
Iteration 3/25 | Loss: 0.00139027
Iteration 4/25 | Loss: 0.00137078
Iteration 5/25 | Loss: 0.00137395
Iteration 6/25 | Loss: 0.00136647
Iteration 7/25 | Loss: 0.00136325
Iteration 8/25 | Loss: 0.00135487
Iteration 9/25 | Loss: 0.00135394
Iteration 10/25 | Loss: 0.00135309
Iteration 11/25 | Loss: 0.00135211
Iteration 12/25 | Loss: 0.00135182
Iteration 13/25 | Loss: 0.00135138
Iteration 14/25 | Loss: 0.00135101
Iteration 15/25 | Loss: 0.00135074
Iteration 16/25 | Loss: 0.00135051
Iteration 17/25 | Loss: 0.00134981
Iteration 18/25 | Loss: 0.00135530
Iteration 19/25 | Loss: 0.00134672
Iteration 20/25 | Loss: 0.00134540
Iteration 21/25 | Loss: 0.00134525
Iteration 22/25 | Loss: 0.00134522
Iteration 23/25 | Loss: 0.00134522
Iteration 24/25 | Loss: 0.00134522
Iteration 25/25 | Loss: 0.00134521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59518516
Iteration 2/25 | Loss: 0.00121305
Iteration 3/25 | Loss: 0.00121304
Iteration 4/25 | Loss: 0.00121304
Iteration 5/25 | Loss: 0.00121304
Iteration 6/25 | Loss: 0.00121304
Iteration 7/25 | Loss: 0.00121304
Iteration 8/25 | Loss: 0.00121304
Iteration 9/25 | Loss: 0.00121304
Iteration 10/25 | Loss: 0.00121304
Iteration 11/25 | Loss: 0.00121304
Iteration 12/25 | Loss: 0.00121304
Iteration 13/25 | Loss: 0.00121304
Iteration 14/25 | Loss: 0.00121304
Iteration 15/25 | Loss: 0.00121304
Iteration 16/25 | Loss: 0.00121304
Iteration 17/25 | Loss: 0.00121304
Iteration 18/25 | Loss: 0.00121304
Iteration 19/25 | Loss: 0.00121304
Iteration 20/25 | Loss: 0.00121304
Iteration 21/25 | Loss: 0.00121304
Iteration 22/25 | Loss: 0.00121304
Iteration 23/25 | Loss: 0.00121304
Iteration 24/25 | Loss: 0.00121304
Iteration 25/25 | Loss: 0.00121304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121304
Iteration 2/1000 | Loss: 0.00004687
Iteration 3/1000 | Loss: 0.00003203
Iteration 4/1000 | Loss: 0.00002848
Iteration 5/1000 | Loss: 0.00002735
Iteration 6/1000 | Loss: 0.00002652
Iteration 7/1000 | Loss: 0.00002601
Iteration 8/1000 | Loss: 0.00002562
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002522
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00002519
Iteration 13/1000 | Loss: 0.00002518
Iteration 14/1000 | Loss: 0.00002503
Iteration 15/1000 | Loss: 0.00002493
Iteration 16/1000 | Loss: 0.00002488
Iteration 17/1000 | Loss: 0.00002485
Iteration 18/1000 | Loss: 0.00002485
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002483
Iteration 21/1000 | Loss: 0.00002483
Iteration 22/1000 | Loss: 0.00002483
Iteration 23/1000 | Loss: 0.00002482
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002481
Iteration 26/1000 | Loss: 0.00002481
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002481
Iteration 34/1000 | Loss: 0.00002481
Iteration 35/1000 | Loss: 0.00002481
Iteration 36/1000 | Loss: 0.00002481
Iteration 37/1000 | Loss: 0.00002481
Iteration 38/1000 | Loss: 0.00002481
Iteration 39/1000 | Loss: 0.00002481
Iteration 40/1000 | Loss: 0.00002481
Iteration 41/1000 | Loss: 0.00002481
Iteration 42/1000 | Loss: 0.00002481
Iteration 43/1000 | Loss: 0.00002481
Iteration 44/1000 | Loss: 0.00002481
Iteration 45/1000 | Loss: 0.00002481
Iteration 46/1000 | Loss: 0.00002481
Iteration 47/1000 | Loss: 0.00002481
Iteration 48/1000 | Loss: 0.00002481
Iteration 49/1000 | Loss: 0.00002481
Iteration 50/1000 | Loss: 0.00002481
Iteration 51/1000 | Loss: 0.00002481
Iteration 52/1000 | Loss: 0.00002481
Iteration 53/1000 | Loss: 0.00002481
Iteration 54/1000 | Loss: 0.00002481
Iteration 55/1000 | Loss: 0.00002481
Iteration 56/1000 | Loss: 0.00002481
Iteration 57/1000 | Loss: 0.00002481
Iteration 58/1000 | Loss: 0.00002481
Iteration 59/1000 | Loss: 0.00002481
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002481
Iteration 62/1000 | Loss: 0.00002481
Iteration 63/1000 | Loss: 0.00002481
Iteration 64/1000 | Loss: 0.00002481
Iteration 65/1000 | Loss: 0.00002481
Iteration 66/1000 | Loss: 0.00002481
Iteration 67/1000 | Loss: 0.00002481
Iteration 68/1000 | Loss: 0.00002481
Iteration 69/1000 | Loss: 0.00002481
Iteration 70/1000 | Loss: 0.00002481
Iteration 71/1000 | Loss: 0.00002481
Iteration 72/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.480531475157477e-05, 2.480531475157477e-05, 2.480531475157477e-05, 2.480531475157477e-05, 2.480531475157477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.480531475157477e-05

Optimization complete. Final v2v error: 4.327988624572754 mm

Highest mean error: 5.167898654937744 mm for frame 95

Lowest mean error: 3.921553611755371 mm for frame 65

Saving results

Total time: 58.440048933029175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00933405
Iteration 2/25 | Loss: 0.00174253
Iteration 3/25 | Loss: 0.00142727
Iteration 4/25 | Loss: 0.00138685
Iteration 5/25 | Loss: 0.00137885
Iteration 6/25 | Loss: 0.00137731
Iteration 7/25 | Loss: 0.00137731
Iteration 8/25 | Loss: 0.00137731
Iteration 9/25 | Loss: 0.00137731
Iteration 10/25 | Loss: 0.00137731
Iteration 11/25 | Loss: 0.00137731
Iteration 12/25 | Loss: 0.00137731
Iteration 13/25 | Loss: 0.00137731
Iteration 14/25 | Loss: 0.00137731
Iteration 15/25 | Loss: 0.00137731
Iteration 16/25 | Loss: 0.00137731
Iteration 17/25 | Loss: 0.00137731
Iteration 18/25 | Loss: 0.00137731
Iteration 19/25 | Loss: 0.00137731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013773082755506039, 0.0013773082755506039, 0.0013773082755506039, 0.0013773082755506039, 0.0013773082755506039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013773082755506039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.21248035
Iteration 2/25 | Loss: 0.00162381
Iteration 3/25 | Loss: 0.00162379
Iteration 4/25 | Loss: 0.00162379
Iteration 5/25 | Loss: 0.00162379
Iteration 6/25 | Loss: 0.00162379
Iteration 7/25 | Loss: 0.00162379
Iteration 8/25 | Loss: 0.00162379
Iteration 9/25 | Loss: 0.00162379
Iteration 10/25 | Loss: 0.00162379
Iteration 11/25 | Loss: 0.00162379
Iteration 12/25 | Loss: 0.00162379
Iteration 13/25 | Loss: 0.00162379
Iteration 14/25 | Loss: 0.00162379
Iteration 15/25 | Loss: 0.00162379
Iteration 16/25 | Loss: 0.00162379
Iteration 17/25 | Loss: 0.00162379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001623789663426578, 0.001623789663426578, 0.001623789663426578, 0.001623789663426578, 0.001623789663426578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001623789663426578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162379
Iteration 2/1000 | Loss: 0.00004416
Iteration 3/1000 | Loss: 0.00003059
Iteration 4/1000 | Loss: 0.00002648
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002376
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002235
Iteration 10/1000 | Loss: 0.00002208
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002193
Iteration 13/1000 | Loss: 0.00002192
Iteration 14/1000 | Loss: 0.00002190
Iteration 15/1000 | Loss: 0.00002171
Iteration 16/1000 | Loss: 0.00002171
Iteration 17/1000 | Loss: 0.00002171
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00002169
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002168
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002161
Iteration 29/1000 | Loss: 0.00002161
Iteration 30/1000 | Loss: 0.00002157
Iteration 31/1000 | Loss: 0.00002157
Iteration 32/1000 | Loss: 0.00002157
Iteration 33/1000 | Loss: 0.00002157
Iteration 34/1000 | Loss: 0.00002156
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002155
Iteration 37/1000 | Loss: 0.00002155
Iteration 38/1000 | Loss: 0.00002154
Iteration 39/1000 | Loss: 0.00002154
Iteration 40/1000 | Loss: 0.00002153
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00002153
Iteration 43/1000 | Loss: 0.00002152
Iteration 44/1000 | Loss: 0.00002152
Iteration 45/1000 | Loss: 0.00002152
Iteration 46/1000 | Loss: 0.00002151
Iteration 47/1000 | Loss: 0.00002151
Iteration 48/1000 | Loss: 0.00002151
Iteration 49/1000 | Loss: 0.00002151
Iteration 50/1000 | Loss: 0.00002150
Iteration 51/1000 | Loss: 0.00002150
Iteration 52/1000 | Loss: 0.00002149
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00002148
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00002147
Iteration 57/1000 | Loss: 0.00002147
Iteration 58/1000 | Loss: 0.00002146
Iteration 59/1000 | Loss: 0.00002146
Iteration 60/1000 | Loss: 0.00002146
Iteration 61/1000 | Loss: 0.00002146
Iteration 62/1000 | Loss: 0.00002146
Iteration 63/1000 | Loss: 0.00002146
Iteration 64/1000 | Loss: 0.00002146
Iteration 65/1000 | Loss: 0.00002146
Iteration 66/1000 | Loss: 0.00002146
Iteration 67/1000 | Loss: 0.00002146
Iteration 68/1000 | Loss: 0.00002146
Iteration 69/1000 | Loss: 0.00002146
Iteration 70/1000 | Loss: 0.00002145
Iteration 71/1000 | Loss: 0.00002145
Iteration 72/1000 | Loss: 0.00002145
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002144
Iteration 76/1000 | Loss: 0.00002144
Iteration 77/1000 | Loss: 0.00002144
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002144
Iteration 81/1000 | Loss: 0.00002144
Iteration 82/1000 | Loss: 0.00002144
Iteration 83/1000 | Loss: 0.00002144
Iteration 84/1000 | Loss: 0.00002144
Iteration 85/1000 | Loss: 0.00002144
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002144
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002144
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002144
Iteration 99/1000 | Loss: 0.00002144
Iteration 100/1000 | Loss: 0.00002144
Iteration 101/1000 | Loss: 0.00002144
Iteration 102/1000 | Loss: 0.00002144
Iteration 103/1000 | Loss: 0.00002144
Iteration 104/1000 | Loss: 0.00002144
Iteration 105/1000 | Loss: 0.00002144
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002144
Iteration 110/1000 | Loss: 0.00002144
Iteration 111/1000 | Loss: 0.00002144
Iteration 112/1000 | Loss: 0.00002144
Iteration 113/1000 | Loss: 0.00002144
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.1438230760395527e-05, 2.1438230760395527e-05, 2.1438230760395527e-05, 2.1438230760395527e-05, 2.1438230760395527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1438230760395527e-05

Optimization complete. Final v2v error: 3.9811227321624756 mm

Highest mean error: 4.345883846282959 mm for frame 54

Lowest mean error: 3.512683629989624 mm for frame 95

Saving results

Total time: 38.30748414993286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915833
Iteration 2/25 | Loss: 0.00164388
Iteration 3/25 | Loss: 0.00147751
Iteration 4/25 | Loss: 0.00145213
Iteration 5/25 | Loss: 0.00144575
Iteration 6/25 | Loss: 0.00144317
Iteration 7/25 | Loss: 0.00144205
Iteration 8/25 | Loss: 0.00144443
Iteration 9/25 | Loss: 0.00144207
Iteration 10/25 | Loss: 0.00144027
Iteration 11/25 | Loss: 0.00143772
Iteration 12/25 | Loss: 0.00143693
Iteration 13/25 | Loss: 0.00143662
Iteration 14/25 | Loss: 0.00143656
Iteration 15/25 | Loss: 0.00143656
Iteration 16/25 | Loss: 0.00143656
Iteration 17/25 | Loss: 0.00143656
Iteration 18/25 | Loss: 0.00143656
Iteration 19/25 | Loss: 0.00143656
Iteration 20/25 | Loss: 0.00143655
Iteration 21/25 | Loss: 0.00143655
Iteration 22/25 | Loss: 0.00143655
Iteration 23/25 | Loss: 0.00143655
Iteration 24/25 | Loss: 0.00143655
Iteration 25/25 | Loss: 0.00143655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46330464
Iteration 2/25 | Loss: 0.00226974
Iteration 3/25 | Loss: 0.00226974
Iteration 4/25 | Loss: 0.00226973
Iteration 5/25 | Loss: 0.00226973
Iteration 6/25 | Loss: 0.00226973
Iteration 7/25 | Loss: 0.00226973
Iteration 8/25 | Loss: 0.00226973
Iteration 9/25 | Loss: 0.00226973
Iteration 10/25 | Loss: 0.00226973
Iteration 11/25 | Loss: 0.00226973
Iteration 12/25 | Loss: 0.00226973
Iteration 13/25 | Loss: 0.00226973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0022697336971759796, 0.0022697336971759796, 0.0022697336971759796, 0.0022697336971759796, 0.0022697336971759796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022697336971759796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226973
Iteration 2/1000 | Loss: 0.00020960
Iteration 3/1000 | Loss: 0.00015127
Iteration 4/1000 | Loss: 0.00012166
Iteration 5/1000 | Loss: 0.00059077
Iteration 6/1000 | Loss: 0.00064892
Iteration 7/1000 | Loss: 0.00035864
Iteration 8/1000 | Loss: 0.00203335
Iteration 9/1000 | Loss: 0.00611989
Iteration 10/1000 | Loss: 0.00077181
Iteration 11/1000 | Loss: 0.00052805
Iteration 12/1000 | Loss: 0.00026663
Iteration 13/1000 | Loss: 0.00049515
Iteration 14/1000 | Loss: 0.00010676
Iteration 15/1000 | Loss: 0.00037238
Iteration 16/1000 | Loss: 0.00098305
Iteration 17/1000 | Loss: 0.00008715
Iteration 18/1000 | Loss: 0.00071833
Iteration 19/1000 | Loss: 0.00025499
Iteration 20/1000 | Loss: 0.00005972
Iteration 21/1000 | Loss: 0.00152719
Iteration 22/1000 | Loss: 0.00066690
Iteration 23/1000 | Loss: 0.00145436
Iteration 24/1000 | Loss: 0.00052662
Iteration 25/1000 | Loss: 0.00005044
Iteration 26/1000 | Loss: 0.00004419
Iteration 27/1000 | Loss: 0.00066360
Iteration 28/1000 | Loss: 0.00027698
Iteration 29/1000 | Loss: 0.00004376
Iteration 30/1000 | Loss: 0.00068105
Iteration 31/1000 | Loss: 0.00022261
Iteration 32/1000 | Loss: 0.00004029
Iteration 33/1000 | Loss: 0.00088702
Iteration 34/1000 | Loss: 0.00103408
Iteration 35/1000 | Loss: 0.00016921
Iteration 36/1000 | Loss: 0.00049506
Iteration 37/1000 | Loss: 0.00010967
Iteration 38/1000 | Loss: 0.00027613
Iteration 39/1000 | Loss: 0.00003965
Iteration 40/1000 | Loss: 0.00003549
Iteration 41/1000 | Loss: 0.00003360
Iteration 42/1000 | Loss: 0.00003181
Iteration 43/1000 | Loss: 0.00003101
Iteration 44/1000 | Loss: 0.00003032
Iteration 45/1000 | Loss: 0.00090566
Iteration 46/1000 | Loss: 0.00066352
Iteration 47/1000 | Loss: 0.00012727
Iteration 48/1000 | Loss: 0.00002989
Iteration 49/1000 | Loss: 0.00002944
Iteration 50/1000 | Loss: 0.00002931
Iteration 51/1000 | Loss: 0.00002901
Iteration 52/1000 | Loss: 0.00089142
Iteration 53/1000 | Loss: 0.00004982
Iteration 54/1000 | Loss: 0.00003376
Iteration 55/1000 | Loss: 0.00002963
Iteration 56/1000 | Loss: 0.00002784
Iteration 57/1000 | Loss: 0.00002694
Iteration 58/1000 | Loss: 0.00002637
Iteration 59/1000 | Loss: 0.00002599
Iteration 60/1000 | Loss: 0.00002564
Iteration 61/1000 | Loss: 0.00002551
Iteration 62/1000 | Loss: 0.00002539
Iteration 63/1000 | Loss: 0.00002538
Iteration 64/1000 | Loss: 0.00002538
Iteration 65/1000 | Loss: 0.00002537
Iteration 66/1000 | Loss: 0.00002537
Iteration 67/1000 | Loss: 0.00002536
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002535
Iteration 70/1000 | Loss: 0.00002534
Iteration 71/1000 | Loss: 0.00002531
Iteration 72/1000 | Loss: 0.00002530
Iteration 73/1000 | Loss: 0.00002530
Iteration 74/1000 | Loss: 0.00002529
Iteration 75/1000 | Loss: 0.00002529
Iteration 76/1000 | Loss: 0.00002529
Iteration 77/1000 | Loss: 0.00002528
Iteration 78/1000 | Loss: 0.00002528
Iteration 79/1000 | Loss: 0.00002528
Iteration 80/1000 | Loss: 0.00002528
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002527
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002527
Iteration 87/1000 | Loss: 0.00002527
Iteration 88/1000 | Loss: 0.00002527
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002526
Iteration 92/1000 | Loss: 0.00002526
Iteration 93/1000 | Loss: 0.00002526
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002524
Iteration 98/1000 | Loss: 0.00002524
Iteration 99/1000 | Loss: 0.00002524
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002523
Iteration 102/1000 | Loss: 0.00002523
Iteration 103/1000 | Loss: 0.00002523
Iteration 104/1000 | Loss: 0.00002522
Iteration 105/1000 | Loss: 0.00002522
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002521
Iteration 109/1000 | Loss: 0.00002521
Iteration 110/1000 | Loss: 0.00002521
Iteration 111/1000 | Loss: 0.00002521
Iteration 112/1000 | Loss: 0.00002521
Iteration 113/1000 | Loss: 0.00002521
Iteration 114/1000 | Loss: 0.00002520
Iteration 115/1000 | Loss: 0.00002520
Iteration 116/1000 | Loss: 0.00002520
Iteration 117/1000 | Loss: 0.00002520
Iteration 118/1000 | Loss: 0.00002520
Iteration 119/1000 | Loss: 0.00002520
Iteration 120/1000 | Loss: 0.00002520
Iteration 121/1000 | Loss: 0.00002520
Iteration 122/1000 | Loss: 0.00002520
Iteration 123/1000 | Loss: 0.00002520
Iteration 124/1000 | Loss: 0.00002519
Iteration 125/1000 | Loss: 0.00002519
Iteration 126/1000 | Loss: 0.00002519
Iteration 127/1000 | Loss: 0.00002519
Iteration 128/1000 | Loss: 0.00002519
Iteration 129/1000 | Loss: 0.00002519
Iteration 130/1000 | Loss: 0.00002519
Iteration 131/1000 | Loss: 0.00002519
Iteration 132/1000 | Loss: 0.00002518
Iteration 133/1000 | Loss: 0.00002518
Iteration 134/1000 | Loss: 0.00002518
Iteration 135/1000 | Loss: 0.00002518
Iteration 136/1000 | Loss: 0.00002518
Iteration 137/1000 | Loss: 0.00002518
Iteration 138/1000 | Loss: 0.00002518
Iteration 139/1000 | Loss: 0.00002517
Iteration 140/1000 | Loss: 0.00002517
Iteration 141/1000 | Loss: 0.00002517
Iteration 142/1000 | Loss: 0.00002517
Iteration 143/1000 | Loss: 0.00002517
Iteration 144/1000 | Loss: 0.00002516
Iteration 145/1000 | Loss: 0.00002516
Iteration 146/1000 | Loss: 0.00002516
Iteration 147/1000 | Loss: 0.00002516
Iteration 148/1000 | Loss: 0.00002516
Iteration 149/1000 | Loss: 0.00002516
Iteration 150/1000 | Loss: 0.00002516
Iteration 151/1000 | Loss: 0.00002516
Iteration 152/1000 | Loss: 0.00002516
Iteration 153/1000 | Loss: 0.00002516
Iteration 154/1000 | Loss: 0.00002516
Iteration 155/1000 | Loss: 0.00002516
Iteration 156/1000 | Loss: 0.00002516
Iteration 157/1000 | Loss: 0.00002516
Iteration 158/1000 | Loss: 0.00002516
Iteration 159/1000 | Loss: 0.00002516
Iteration 160/1000 | Loss: 0.00002516
Iteration 161/1000 | Loss: 0.00002516
Iteration 162/1000 | Loss: 0.00002516
Iteration 163/1000 | Loss: 0.00002516
Iteration 164/1000 | Loss: 0.00002516
Iteration 165/1000 | Loss: 0.00002516
Iteration 166/1000 | Loss: 0.00002516
Iteration 167/1000 | Loss: 0.00002516
Iteration 168/1000 | Loss: 0.00002516
Iteration 169/1000 | Loss: 0.00002516
Iteration 170/1000 | Loss: 0.00002516
Iteration 171/1000 | Loss: 0.00002516
Iteration 172/1000 | Loss: 0.00002516
Iteration 173/1000 | Loss: 0.00002516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.5155284674838185e-05, 2.5155284674838185e-05, 2.5155284674838185e-05, 2.5155284674838185e-05, 2.5155284674838185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5155284674838185e-05

Optimization complete. Final v2v error: 4.360085964202881 mm

Highest mean error: 5.660410404205322 mm for frame 85

Lowest mean error: 3.899172067642212 mm for frame 168

Saving results

Total time: 125.5674660205841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081755
Iteration 2/25 | Loss: 0.00210470
Iteration 3/25 | Loss: 0.00161425
Iteration 4/25 | Loss: 0.00156611
Iteration 5/25 | Loss: 0.00154609
Iteration 6/25 | Loss: 0.00154242
Iteration 7/25 | Loss: 0.00154222
Iteration 8/25 | Loss: 0.00154222
Iteration 9/25 | Loss: 0.00154222
Iteration 10/25 | Loss: 0.00154222
Iteration 11/25 | Loss: 0.00154222
Iteration 12/25 | Loss: 0.00154222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015422211727127433, 0.0015422211727127433, 0.0015422211727127433, 0.0015422211727127433, 0.0015422211727127433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015422211727127433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96479845
Iteration 2/25 | Loss: 0.00130858
Iteration 3/25 | Loss: 0.00130858
Iteration 4/25 | Loss: 0.00130858
Iteration 5/25 | Loss: 0.00130858
Iteration 6/25 | Loss: 0.00130858
Iteration 7/25 | Loss: 0.00130858
Iteration 8/25 | Loss: 0.00130857
Iteration 9/25 | Loss: 0.00130857
Iteration 10/25 | Loss: 0.00130857
Iteration 11/25 | Loss: 0.00130857
Iteration 12/25 | Loss: 0.00130857
Iteration 13/25 | Loss: 0.00130857
Iteration 14/25 | Loss: 0.00130857
Iteration 15/25 | Loss: 0.00130857
Iteration 16/25 | Loss: 0.00130857
Iteration 17/25 | Loss: 0.00130857
Iteration 18/25 | Loss: 0.00130857
Iteration 19/25 | Loss: 0.00130857
Iteration 20/25 | Loss: 0.00130857
Iteration 21/25 | Loss: 0.00130857
Iteration 22/25 | Loss: 0.00130857
Iteration 23/25 | Loss: 0.00130858
Iteration 24/25 | Loss: 0.00130858
Iteration 25/25 | Loss: 0.00130858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130858
Iteration 2/1000 | Loss: 0.00011869
Iteration 3/1000 | Loss: 0.00008118
Iteration 4/1000 | Loss: 0.00007233
Iteration 5/1000 | Loss: 0.00006822
Iteration 6/1000 | Loss: 0.00006509
Iteration 7/1000 | Loss: 0.00006327
Iteration 8/1000 | Loss: 0.00006199
Iteration 9/1000 | Loss: 0.00006095
Iteration 10/1000 | Loss: 0.00006027
Iteration 11/1000 | Loss: 0.00005984
Iteration 12/1000 | Loss: 0.00005964
Iteration 13/1000 | Loss: 0.00005944
Iteration 14/1000 | Loss: 0.00005926
Iteration 15/1000 | Loss: 0.00005913
Iteration 16/1000 | Loss: 0.00005910
Iteration 17/1000 | Loss: 0.00005904
Iteration 18/1000 | Loss: 0.00005903
Iteration 19/1000 | Loss: 0.00005900
Iteration 20/1000 | Loss: 0.00005900
Iteration 21/1000 | Loss: 0.00005899
Iteration 22/1000 | Loss: 0.00005899
Iteration 23/1000 | Loss: 0.00005899
Iteration 24/1000 | Loss: 0.00005896
Iteration 25/1000 | Loss: 0.00005895
Iteration 26/1000 | Loss: 0.00005894
Iteration 27/1000 | Loss: 0.00005891
Iteration 28/1000 | Loss: 0.00005891
Iteration 29/1000 | Loss: 0.00005891
Iteration 30/1000 | Loss: 0.00005891
Iteration 31/1000 | Loss: 0.00005891
Iteration 32/1000 | Loss: 0.00005891
Iteration 33/1000 | Loss: 0.00005890
Iteration 34/1000 | Loss: 0.00005890
Iteration 35/1000 | Loss: 0.00005890
Iteration 36/1000 | Loss: 0.00005890
Iteration 37/1000 | Loss: 0.00005889
Iteration 38/1000 | Loss: 0.00005889
Iteration 39/1000 | Loss: 0.00005889
Iteration 40/1000 | Loss: 0.00005887
Iteration 41/1000 | Loss: 0.00005887
Iteration 42/1000 | Loss: 0.00005886
Iteration 43/1000 | Loss: 0.00005886
Iteration 44/1000 | Loss: 0.00005886
Iteration 45/1000 | Loss: 0.00005886
Iteration 46/1000 | Loss: 0.00005886
Iteration 47/1000 | Loss: 0.00005886
Iteration 48/1000 | Loss: 0.00005886
Iteration 49/1000 | Loss: 0.00005886
Iteration 50/1000 | Loss: 0.00005886
Iteration 51/1000 | Loss: 0.00005886
Iteration 52/1000 | Loss: 0.00005886
Iteration 53/1000 | Loss: 0.00005885
Iteration 54/1000 | Loss: 0.00005885
Iteration 55/1000 | Loss: 0.00005885
Iteration 56/1000 | Loss: 0.00005884
Iteration 57/1000 | Loss: 0.00005884
Iteration 58/1000 | Loss: 0.00005884
Iteration 59/1000 | Loss: 0.00005884
Iteration 60/1000 | Loss: 0.00005884
Iteration 61/1000 | Loss: 0.00005884
Iteration 62/1000 | Loss: 0.00005884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [5.884369238629006e-05, 5.884369238629006e-05, 5.884369238629006e-05, 5.884369238629006e-05, 5.884369238629006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.884369238629006e-05

Optimization complete. Final v2v error: 6.342864036560059 mm

Highest mean error: 7.226009368896484 mm for frame 94

Lowest mean error: 5.8485918045043945 mm for frame 122

Saving results

Total time: 42.471882820129395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882965
Iteration 2/25 | Loss: 0.00157158
Iteration 3/25 | Loss: 0.00137385
Iteration 4/25 | Loss: 0.00136413
Iteration 5/25 | Loss: 0.00136042
Iteration 6/25 | Loss: 0.00135922
Iteration 7/25 | Loss: 0.00135922
Iteration 8/25 | Loss: 0.00135922
Iteration 9/25 | Loss: 0.00135922
Iteration 10/25 | Loss: 0.00135922
Iteration 11/25 | Loss: 0.00135922
Iteration 12/25 | Loss: 0.00135922
Iteration 13/25 | Loss: 0.00135922
Iteration 14/25 | Loss: 0.00135922
Iteration 15/25 | Loss: 0.00135922
Iteration 16/25 | Loss: 0.00135922
Iteration 17/25 | Loss: 0.00135922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013592203613370657, 0.0013592203613370657, 0.0013592203613370657, 0.0013592203613370657, 0.0013592203613370657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013592203613370657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45933652
Iteration 2/25 | Loss: 0.00144152
Iteration 3/25 | Loss: 0.00144152
Iteration 4/25 | Loss: 0.00144152
Iteration 5/25 | Loss: 0.00144152
Iteration 6/25 | Loss: 0.00144151
Iteration 7/25 | Loss: 0.00144151
Iteration 8/25 | Loss: 0.00144151
Iteration 9/25 | Loss: 0.00144151
Iteration 10/25 | Loss: 0.00144151
Iteration 11/25 | Loss: 0.00144151
Iteration 12/25 | Loss: 0.00144151
Iteration 13/25 | Loss: 0.00144151
Iteration 14/25 | Loss: 0.00144151
Iteration 15/25 | Loss: 0.00144151
Iteration 16/25 | Loss: 0.00144151
Iteration 17/25 | Loss: 0.00144151
Iteration 18/25 | Loss: 0.00144151
Iteration 19/25 | Loss: 0.00144151
Iteration 20/25 | Loss: 0.00144151
Iteration 21/25 | Loss: 0.00144151
Iteration 22/25 | Loss: 0.00144151
Iteration 23/25 | Loss: 0.00144151
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014415138866752386, 0.0014415138866752386, 0.0014415138866752386, 0.0014415138866752386, 0.0014415138866752386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014415138866752386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144151
Iteration 2/1000 | Loss: 0.00005036
Iteration 3/1000 | Loss: 0.00003442
Iteration 4/1000 | Loss: 0.00003194
Iteration 5/1000 | Loss: 0.00003074
Iteration 6/1000 | Loss: 0.00002996
Iteration 7/1000 | Loss: 0.00002933
Iteration 8/1000 | Loss: 0.00002884
Iteration 9/1000 | Loss: 0.00002853
Iteration 10/1000 | Loss: 0.00002836
Iteration 11/1000 | Loss: 0.00002816
Iteration 12/1000 | Loss: 0.00002798
Iteration 13/1000 | Loss: 0.00002797
Iteration 14/1000 | Loss: 0.00002784
Iteration 15/1000 | Loss: 0.00002773
Iteration 16/1000 | Loss: 0.00002770
Iteration 17/1000 | Loss: 0.00002770
Iteration 18/1000 | Loss: 0.00002769
Iteration 19/1000 | Loss: 0.00002767
Iteration 20/1000 | Loss: 0.00002767
Iteration 21/1000 | Loss: 0.00002766
Iteration 22/1000 | Loss: 0.00002765
Iteration 23/1000 | Loss: 0.00002762
Iteration 24/1000 | Loss: 0.00002761
Iteration 25/1000 | Loss: 0.00002761
Iteration 26/1000 | Loss: 0.00002761
Iteration 27/1000 | Loss: 0.00002760
Iteration 28/1000 | Loss: 0.00002760
Iteration 29/1000 | Loss: 0.00002759
Iteration 30/1000 | Loss: 0.00002759
Iteration 31/1000 | Loss: 0.00002758
Iteration 32/1000 | Loss: 0.00002758
Iteration 33/1000 | Loss: 0.00002758
Iteration 34/1000 | Loss: 0.00002758
Iteration 35/1000 | Loss: 0.00002758
Iteration 36/1000 | Loss: 0.00002758
Iteration 37/1000 | Loss: 0.00002758
Iteration 38/1000 | Loss: 0.00002758
Iteration 39/1000 | Loss: 0.00002758
Iteration 40/1000 | Loss: 0.00002758
Iteration 41/1000 | Loss: 0.00002757
Iteration 42/1000 | Loss: 0.00002757
Iteration 43/1000 | Loss: 0.00002756
Iteration 44/1000 | Loss: 0.00002756
Iteration 45/1000 | Loss: 0.00002756
Iteration 46/1000 | Loss: 0.00002756
Iteration 47/1000 | Loss: 0.00002756
Iteration 48/1000 | Loss: 0.00002756
Iteration 49/1000 | Loss: 0.00002756
Iteration 50/1000 | Loss: 0.00002756
Iteration 51/1000 | Loss: 0.00002756
Iteration 52/1000 | Loss: 0.00002756
Iteration 53/1000 | Loss: 0.00002756
Iteration 54/1000 | Loss: 0.00002755
Iteration 55/1000 | Loss: 0.00002755
Iteration 56/1000 | Loss: 0.00002755
Iteration 57/1000 | Loss: 0.00002755
Iteration 58/1000 | Loss: 0.00002755
Iteration 59/1000 | Loss: 0.00002754
Iteration 60/1000 | Loss: 0.00002754
Iteration 61/1000 | Loss: 0.00002754
Iteration 62/1000 | Loss: 0.00002754
Iteration 63/1000 | Loss: 0.00002753
Iteration 64/1000 | Loss: 0.00002753
Iteration 65/1000 | Loss: 0.00002753
Iteration 66/1000 | Loss: 0.00002753
Iteration 67/1000 | Loss: 0.00002753
Iteration 68/1000 | Loss: 0.00002753
Iteration 69/1000 | Loss: 0.00002753
Iteration 70/1000 | Loss: 0.00002753
Iteration 71/1000 | Loss: 0.00002753
Iteration 72/1000 | Loss: 0.00002753
Iteration 73/1000 | Loss: 0.00002752
Iteration 74/1000 | Loss: 0.00002752
Iteration 75/1000 | Loss: 0.00002752
Iteration 76/1000 | Loss: 0.00002752
Iteration 77/1000 | Loss: 0.00002752
Iteration 78/1000 | Loss: 0.00002752
Iteration 79/1000 | Loss: 0.00002752
Iteration 80/1000 | Loss: 0.00002751
Iteration 81/1000 | Loss: 0.00002751
Iteration 82/1000 | Loss: 0.00002751
Iteration 83/1000 | Loss: 0.00002751
Iteration 84/1000 | Loss: 0.00002750
Iteration 85/1000 | Loss: 0.00002750
Iteration 86/1000 | Loss: 0.00002750
Iteration 87/1000 | Loss: 0.00002750
Iteration 88/1000 | Loss: 0.00002750
Iteration 89/1000 | Loss: 0.00002750
Iteration 90/1000 | Loss: 0.00002750
Iteration 91/1000 | Loss: 0.00002750
Iteration 92/1000 | Loss: 0.00002750
Iteration 93/1000 | Loss: 0.00002750
Iteration 94/1000 | Loss: 0.00002749
Iteration 95/1000 | Loss: 0.00002749
Iteration 96/1000 | Loss: 0.00002749
Iteration 97/1000 | Loss: 0.00002749
Iteration 98/1000 | Loss: 0.00002749
Iteration 99/1000 | Loss: 0.00002749
Iteration 100/1000 | Loss: 0.00002749
Iteration 101/1000 | Loss: 0.00002749
Iteration 102/1000 | Loss: 0.00002748
Iteration 103/1000 | Loss: 0.00002748
Iteration 104/1000 | Loss: 0.00002748
Iteration 105/1000 | Loss: 0.00002748
Iteration 106/1000 | Loss: 0.00002748
Iteration 107/1000 | Loss: 0.00002748
Iteration 108/1000 | Loss: 0.00002748
Iteration 109/1000 | Loss: 0.00002748
Iteration 110/1000 | Loss: 0.00002747
Iteration 111/1000 | Loss: 0.00002747
Iteration 112/1000 | Loss: 0.00002747
Iteration 113/1000 | Loss: 0.00002747
Iteration 114/1000 | Loss: 0.00002747
Iteration 115/1000 | Loss: 0.00002747
Iteration 116/1000 | Loss: 0.00002747
Iteration 117/1000 | Loss: 0.00002747
Iteration 118/1000 | Loss: 0.00002747
Iteration 119/1000 | Loss: 0.00002747
Iteration 120/1000 | Loss: 0.00002747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.747245889622718e-05, 2.747245889622718e-05, 2.747245889622718e-05, 2.747245889622718e-05, 2.747245889622718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.747245889622718e-05

Optimization complete. Final v2v error: 4.6259684562683105 mm

Highest mean error: 4.851615905761719 mm for frame 160

Lowest mean error: 4.314116954803467 mm for frame 201

Saving results

Total time: 42.37230062484741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149976
Iteration 2/25 | Loss: 0.00252100
Iteration 3/25 | Loss: 0.00195543
Iteration 4/25 | Loss: 0.00188650
Iteration 5/25 | Loss: 0.00175628
Iteration 6/25 | Loss: 0.00160495
Iteration 7/25 | Loss: 0.00156869
Iteration 8/25 | Loss: 0.00156738
Iteration 9/25 | Loss: 0.00154106
Iteration 10/25 | Loss: 0.00153518
Iteration 11/25 | Loss: 0.00151963
Iteration 12/25 | Loss: 0.00151680
Iteration 13/25 | Loss: 0.00151047
Iteration 14/25 | Loss: 0.00150911
Iteration 15/25 | Loss: 0.00150859
Iteration 16/25 | Loss: 0.00150781
Iteration 17/25 | Loss: 0.00150718
Iteration 18/25 | Loss: 0.00150698
Iteration 19/25 | Loss: 0.00150688
Iteration 20/25 | Loss: 0.00150688
Iteration 21/25 | Loss: 0.00150688
Iteration 22/25 | Loss: 0.00150688
Iteration 23/25 | Loss: 0.00150688
Iteration 24/25 | Loss: 0.00150688
Iteration 25/25 | Loss: 0.00150688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45222569
Iteration 2/25 | Loss: 0.00168128
Iteration 3/25 | Loss: 0.00168127
Iteration 4/25 | Loss: 0.00168127
Iteration 5/25 | Loss: 0.00168127
Iteration 6/25 | Loss: 0.00168127
Iteration 7/25 | Loss: 0.00168127
Iteration 8/25 | Loss: 0.00168127
Iteration 9/25 | Loss: 0.00168127
Iteration 10/25 | Loss: 0.00168127
Iteration 11/25 | Loss: 0.00168127
Iteration 12/25 | Loss: 0.00168127
Iteration 13/25 | Loss: 0.00168127
Iteration 14/25 | Loss: 0.00168127
Iteration 15/25 | Loss: 0.00168127
Iteration 16/25 | Loss: 0.00168127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001681271125562489, 0.001681271125562489, 0.001681271125562489, 0.001681271125562489, 0.001681271125562489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001681271125562489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168127
Iteration 2/1000 | Loss: 0.00016659
Iteration 3/1000 | Loss: 0.00007535
Iteration 4/1000 | Loss: 0.00005298
Iteration 5/1000 | Loss: 0.00004620
Iteration 6/1000 | Loss: 0.00003990
Iteration 7/1000 | Loss: 0.00003694
Iteration 8/1000 | Loss: 0.00003556
Iteration 9/1000 | Loss: 0.00003496
Iteration 10/1000 | Loss: 0.00003454
Iteration 11/1000 | Loss: 0.00003412
Iteration 12/1000 | Loss: 0.00003391
Iteration 13/1000 | Loss: 0.00003391
Iteration 14/1000 | Loss: 0.00003387
Iteration 15/1000 | Loss: 0.00003383
Iteration 16/1000 | Loss: 0.00003373
Iteration 17/1000 | Loss: 0.00003371
Iteration 18/1000 | Loss: 0.00003370
Iteration 19/1000 | Loss: 0.00003366
Iteration 20/1000 | Loss: 0.00003362
Iteration 21/1000 | Loss: 0.00003358
Iteration 22/1000 | Loss: 0.00003353
Iteration 23/1000 | Loss: 0.00003353
Iteration 24/1000 | Loss: 0.00003353
Iteration 25/1000 | Loss: 0.00003352
Iteration 26/1000 | Loss: 0.00003351
Iteration 27/1000 | Loss: 0.00003350
Iteration 28/1000 | Loss: 0.00003350
Iteration 29/1000 | Loss: 0.00003350
Iteration 30/1000 | Loss: 0.00003347
Iteration 31/1000 | Loss: 0.00003347
Iteration 32/1000 | Loss: 0.00003347
Iteration 33/1000 | Loss: 0.00003346
Iteration 34/1000 | Loss: 0.00003346
Iteration 35/1000 | Loss: 0.00003346
Iteration 36/1000 | Loss: 0.00003346
Iteration 37/1000 | Loss: 0.00003345
Iteration 38/1000 | Loss: 0.00003345
Iteration 39/1000 | Loss: 0.00003345
Iteration 40/1000 | Loss: 0.00003345
Iteration 41/1000 | Loss: 0.00003345
Iteration 42/1000 | Loss: 0.00003345
Iteration 43/1000 | Loss: 0.00003345
Iteration 44/1000 | Loss: 0.00003344
Iteration 45/1000 | Loss: 0.00003343
Iteration 46/1000 | Loss: 0.00003343
Iteration 47/1000 | Loss: 0.00003343
Iteration 48/1000 | Loss: 0.00003343
Iteration 49/1000 | Loss: 0.00003343
Iteration 50/1000 | Loss: 0.00003343
Iteration 51/1000 | Loss: 0.00003343
Iteration 52/1000 | Loss: 0.00003343
Iteration 53/1000 | Loss: 0.00003342
Iteration 54/1000 | Loss: 0.00003342
Iteration 55/1000 | Loss: 0.00003342
Iteration 56/1000 | Loss: 0.00003342
Iteration 57/1000 | Loss: 0.00003342
Iteration 58/1000 | Loss: 0.00003342
Iteration 59/1000 | Loss: 0.00003342
Iteration 60/1000 | Loss: 0.00003341
Iteration 61/1000 | Loss: 0.00003341
Iteration 62/1000 | Loss: 0.00003341
Iteration 63/1000 | Loss: 0.00003341
Iteration 64/1000 | Loss: 0.00003341
Iteration 65/1000 | Loss: 0.00003341
Iteration 66/1000 | Loss: 0.00003340
Iteration 67/1000 | Loss: 0.00003340
Iteration 68/1000 | Loss: 0.00003340
Iteration 69/1000 | Loss: 0.00003340
Iteration 70/1000 | Loss: 0.00003340
Iteration 71/1000 | Loss: 0.00003340
Iteration 72/1000 | Loss: 0.00003340
Iteration 73/1000 | Loss: 0.00003340
Iteration 74/1000 | Loss: 0.00003340
Iteration 75/1000 | Loss: 0.00003340
Iteration 76/1000 | Loss: 0.00003340
Iteration 77/1000 | Loss: 0.00003340
Iteration 78/1000 | Loss: 0.00003340
Iteration 79/1000 | Loss: 0.00003340
Iteration 80/1000 | Loss: 0.00003339
Iteration 81/1000 | Loss: 0.00003339
Iteration 82/1000 | Loss: 0.00003339
Iteration 83/1000 | Loss: 0.00003339
Iteration 84/1000 | Loss: 0.00003339
Iteration 85/1000 | Loss: 0.00003339
Iteration 86/1000 | Loss: 0.00003339
Iteration 87/1000 | Loss: 0.00003339
Iteration 88/1000 | Loss: 0.00003339
Iteration 89/1000 | Loss: 0.00003339
Iteration 90/1000 | Loss: 0.00003338
Iteration 91/1000 | Loss: 0.00003338
Iteration 92/1000 | Loss: 0.00003338
Iteration 93/1000 | Loss: 0.00003338
Iteration 94/1000 | Loss: 0.00003338
Iteration 95/1000 | Loss: 0.00003338
Iteration 96/1000 | Loss: 0.00003338
Iteration 97/1000 | Loss: 0.00003338
Iteration 98/1000 | Loss: 0.00003338
Iteration 99/1000 | Loss: 0.00003338
Iteration 100/1000 | Loss: 0.00003337
Iteration 101/1000 | Loss: 0.00003337
Iteration 102/1000 | Loss: 0.00003337
Iteration 103/1000 | Loss: 0.00003337
Iteration 104/1000 | Loss: 0.00003337
Iteration 105/1000 | Loss: 0.00003337
Iteration 106/1000 | Loss: 0.00003337
Iteration 107/1000 | Loss: 0.00003337
Iteration 108/1000 | Loss: 0.00003337
Iteration 109/1000 | Loss: 0.00003337
Iteration 110/1000 | Loss: 0.00003337
Iteration 111/1000 | Loss: 0.00003337
Iteration 112/1000 | Loss: 0.00003337
Iteration 113/1000 | Loss: 0.00003337
Iteration 114/1000 | Loss: 0.00003337
Iteration 115/1000 | Loss: 0.00003337
Iteration 116/1000 | Loss: 0.00003337
Iteration 117/1000 | Loss: 0.00003337
Iteration 118/1000 | Loss: 0.00003337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.336645386298187e-05, 3.336645386298187e-05, 3.336645386298187e-05, 3.336645386298187e-05, 3.336645386298187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.336645386298187e-05

Optimization complete. Final v2v error: 5.007844924926758 mm

Highest mean error: 5.197347640991211 mm for frame 133

Lowest mean error: 4.781522274017334 mm for frame 62

Saving results

Total time: 69.6798198223114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890832
Iteration 2/25 | Loss: 0.00174669
Iteration 3/25 | Loss: 0.00152808
Iteration 4/25 | Loss: 0.00149284
Iteration 5/25 | Loss: 0.00148614
Iteration 6/25 | Loss: 0.00148487
Iteration 7/25 | Loss: 0.00148484
Iteration 8/25 | Loss: 0.00148484
Iteration 9/25 | Loss: 0.00148484
Iteration 10/25 | Loss: 0.00148484
Iteration 11/25 | Loss: 0.00148484
Iteration 12/25 | Loss: 0.00148484
Iteration 13/25 | Loss: 0.00148484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014848444843664765, 0.0014848444843664765, 0.0014848444843664765, 0.0014848444843664765, 0.0014848444843664765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014848444843664765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36593974
Iteration 2/25 | Loss: 0.00164052
Iteration 3/25 | Loss: 0.00164052
Iteration 4/25 | Loss: 0.00164052
Iteration 5/25 | Loss: 0.00164052
Iteration 6/25 | Loss: 0.00164052
Iteration 7/25 | Loss: 0.00164052
Iteration 8/25 | Loss: 0.00164052
Iteration 9/25 | Loss: 0.00164052
Iteration 10/25 | Loss: 0.00164052
Iteration 11/25 | Loss: 0.00164052
Iteration 12/25 | Loss: 0.00164052
Iteration 13/25 | Loss: 0.00164052
Iteration 14/25 | Loss: 0.00164052
Iteration 15/25 | Loss: 0.00164052
Iteration 16/25 | Loss: 0.00164052
Iteration 17/25 | Loss: 0.00164052
Iteration 18/25 | Loss: 0.00164052
Iteration 19/25 | Loss: 0.00164052
Iteration 20/25 | Loss: 0.00164052
Iteration 21/25 | Loss: 0.00164052
Iteration 22/25 | Loss: 0.00164052
Iteration 23/25 | Loss: 0.00164052
Iteration 24/25 | Loss: 0.00164052
Iteration 25/25 | Loss: 0.00164052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164052
Iteration 2/1000 | Loss: 0.00009109
Iteration 3/1000 | Loss: 0.00006318
Iteration 4/1000 | Loss: 0.00004589
Iteration 5/1000 | Loss: 0.00004151
Iteration 6/1000 | Loss: 0.00003969
Iteration 7/1000 | Loss: 0.00003822
Iteration 8/1000 | Loss: 0.00003723
Iteration 9/1000 | Loss: 0.00003629
Iteration 10/1000 | Loss: 0.00003576
Iteration 11/1000 | Loss: 0.00003539
Iteration 12/1000 | Loss: 0.00003508
Iteration 13/1000 | Loss: 0.00003489
Iteration 14/1000 | Loss: 0.00003485
Iteration 15/1000 | Loss: 0.00003478
Iteration 16/1000 | Loss: 0.00003478
Iteration 17/1000 | Loss: 0.00003476
Iteration 18/1000 | Loss: 0.00003476
Iteration 19/1000 | Loss: 0.00003476
Iteration 20/1000 | Loss: 0.00003476
Iteration 21/1000 | Loss: 0.00003476
Iteration 22/1000 | Loss: 0.00003476
Iteration 23/1000 | Loss: 0.00003476
Iteration 24/1000 | Loss: 0.00003475
Iteration 25/1000 | Loss: 0.00003475
Iteration 26/1000 | Loss: 0.00003475
Iteration 27/1000 | Loss: 0.00003475
Iteration 28/1000 | Loss: 0.00003475
Iteration 29/1000 | Loss: 0.00003474
Iteration 30/1000 | Loss: 0.00003474
Iteration 31/1000 | Loss: 0.00003474
Iteration 32/1000 | Loss: 0.00003474
Iteration 33/1000 | Loss: 0.00003474
Iteration 34/1000 | Loss: 0.00003474
Iteration 35/1000 | Loss: 0.00003474
Iteration 36/1000 | Loss: 0.00003473
Iteration 37/1000 | Loss: 0.00003473
Iteration 38/1000 | Loss: 0.00003473
Iteration 39/1000 | Loss: 0.00003473
Iteration 40/1000 | Loss: 0.00003473
Iteration 41/1000 | Loss: 0.00003473
Iteration 42/1000 | Loss: 0.00003472
Iteration 43/1000 | Loss: 0.00003472
Iteration 44/1000 | Loss: 0.00003472
Iteration 45/1000 | Loss: 0.00003471
Iteration 46/1000 | Loss: 0.00003471
Iteration 47/1000 | Loss: 0.00003471
Iteration 48/1000 | Loss: 0.00003470
Iteration 49/1000 | Loss: 0.00003470
Iteration 50/1000 | Loss: 0.00003470
Iteration 51/1000 | Loss: 0.00003469
Iteration 52/1000 | Loss: 0.00003468
Iteration 53/1000 | Loss: 0.00003467
Iteration 54/1000 | Loss: 0.00003467
Iteration 55/1000 | Loss: 0.00003467
Iteration 56/1000 | Loss: 0.00003467
Iteration 57/1000 | Loss: 0.00003467
Iteration 58/1000 | Loss: 0.00003467
Iteration 59/1000 | Loss: 0.00003467
Iteration 60/1000 | Loss: 0.00003467
Iteration 61/1000 | Loss: 0.00003467
Iteration 62/1000 | Loss: 0.00003467
Iteration 63/1000 | Loss: 0.00003467
Iteration 64/1000 | Loss: 0.00003466
Iteration 65/1000 | Loss: 0.00003466
Iteration 66/1000 | Loss: 0.00003466
Iteration 67/1000 | Loss: 0.00003466
Iteration 68/1000 | Loss: 0.00003466
Iteration 69/1000 | Loss: 0.00003466
Iteration 70/1000 | Loss: 0.00003466
Iteration 71/1000 | Loss: 0.00003465
Iteration 72/1000 | Loss: 0.00003465
Iteration 73/1000 | Loss: 0.00003465
Iteration 74/1000 | Loss: 0.00003465
Iteration 75/1000 | Loss: 0.00003465
Iteration 76/1000 | Loss: 0.00003465
Iteration 77/1000 | Loss: 0.00003465
Iteration 78/1000 | Loss: 0.00003465
Iteration 79/1000 | Loss: 0.00003464
Iteration 80/1000 | Loss: 0.00003464
Iteration 81/1000 | Loss: 0.00003464
Iteration 82/1000 | Loss: 0.00003464
Iteration 83/1000 | Loss: 0.00003464
Iteration 84/1000 | Loss: 0.00003464
Iteration 85/1000 | Loss: 0.00003464
Iteration 86/1000 | Loss: 0.00003463
Iteration 87/1000 | Loss: 0.00003463
Iteration 88/1000 | Loss: 0.00003463
Iteration 89/1000 | Loss: 0.00003462
Iteration 90/1000 | Loss: 0.00003462
Iteration 91/1000 | Loss: 0.00003462
Iteration 92/1000 | Loss: 0.00003462
Iteration 93/1000 | Loss: 0.00003461
Iteration 94/1000 | Loss: 0.00003461
Iteration 95/1000 | Loss: 0.00003461
Iteration 96/1000 | Loss: 0.00003461
Iteration 97/1000 | Loss: 0.00003461
Iteration 98/1000 | Loss: 0.00003461
Iteration 99/1000 | Loss: 0.00003461
Iteration 100/1000 | Loss: 0.00003461
Iteration 101/1000 | Loss: 0.00003461
Iteration 102/1000 | Loss: 0.00003461
Iteration 103/1000 | Loss: 0.00003461
Iteration 104/1000 | Loss: 0.00003461
Iteration 105/1000 | Loss: 0.00003461
Iteration 106/1000 | Loss: 0.00003460
Iteration 107/1000 | Loss: 0.00003460
Iteration 108/1000 | Loss: 0.00003460
Iteration 109/1000 | Loss: 0.00003460
Iteration 110/1000 | Loss: 0.00003460
Iteration 111/1000 | Loss: 0.00003460
Iteration 112/1000 | Loss: 0.00003460
Iteration 113/1000 | Loss: 0.00003460
Iteration 114/1000 | Loss: 0.00003460
Iteration 115/1000 | Loss: 0.00003460
Iteration 116/1000 | Loss: 0.00003460
Iteration 117/1000 | Loss: 0.00003460
Iteration 118/1000 | Loss: 0.00003460
Iteration 119/1000 | Loss: 0.00003460
Iteration 120/1000 | Loss: 0.00003460
Iteration 121/1000 | Loss: 0.00003459
Iteration 122/1000 | Loss: 0.00003459
Iteration 123/1000 | Loss: 0.00003459
Iteration 124/1000 | Loss: 0.00003459
Iteration 125/1000 | Loss: 0.00003459
Iteration 126/1000 | Loss: 0.00003459
Iteration 127/1000 | Loss: 0.00003459
Iteration 128/1000 | Loss: 0.00003459
Iteration 129/1000 | Loss: 0.00003459
Iteration 130/1000 | Loss: 0.00003459
Iteration 131/1000 | Loss: 0.00003459
Iteration 132/1000 | Loss: 0.00003459
Iteration 133/1000 | Loss: 0.00003458
Iteration 134/1000 | Loss: 0.00003458
Iteration 135/1000 | Loss: 0.00003458
Iteration 136/1000 | Loss: 0.00003458
Iteration 137/1000 | Loss: 0.00003458
Iteration 138/1000 | Loss: 0.00003458
Iteration 139/1000 | Loss: 0.00003458
Iteration 140/1000 | Loss: 0.00003458
Iteration 141/1000 | Loss: 0.00003458
Iteration 142/1000 | Loss: 0.00003457
Iteration 143/1000 | Loss: 0.00003457
Iteration 144/1000 | Loss: 0.00003457
Iteration 145/1000 | Loss: 0.00003457
Iteration 146/1000 | Loss: 0.00003456
Iteration 147/1000 | Loss: 0.00003456
Iteration 148/1000 | Loss: 0.00003456
Iteration 149/1000 | Loss: 0.00003456
Iteration 150/1000 | Loss: 0.00003456
Iteration 151/1000 | Loss: 0.00003456
Iteration 152/1000 | Loss: 0.00003456
Iteration 153/1000 | Loss: 0.00003456
Iteration 154/1000 | Loss: 0.00003456
Iteration 155/1000 | Loss: 0.00003456
Iteration 156/1000 | Loss: 0.00003455
Iteration 157/1000 | Loss: 0.00003455
Iteration 158/1000 | Loss: 0.00003454
Iteration 159/1000 | Loss: 0.00003454
Iteration 160/1000 | Loss: 0.00003454
Iteration 161/1000 | Loss: 0.00003454
Iteration 162/1000 | Loss: 0.00003454
Iteration 163/1000 | Loss: 0.00003454
Iteration 164/1000 | Loss: 0.00003454
Iteration 165/1000 | Loss: 0.00003454
Iteration 166/1000 | Loss: 0.00003454
Iteration 167/1000 | Loss: 0.00003454
Iteration 168/1000 | Loss: 0.00003454
Iteration 169/1000 | Loss: 0.00003454
Iteration 170/1000 | Loss: 0.00003453
Iteration 171/1000 | Loss: 0.00003453
Iteration 172/1000 | Loss: 0.00003453
Iteration 173/1000 | Loss: 0.00003453
Iteration 174/1000 | Loss: 0.00003453
Iteration 175/1000 | Loss: 0.00003453
Iteration 176/1000 | Loss: 0.00003453
Iteration 177/1000 | Loss: 0.00003453
Iteration 178/1000 | Loss: 0.00003453
Iteration 179/1000 | Loss: 0.00003453
Iteration 180/1000 | Loss: 0.00003453
Iteration 181/1000 | Loss: 0.00003452
Iteration 182/1000 | Loss: 0.00003452
Iteration 183/1000 | Loss: 0.00003452
Iteration 184/1000 | Loss: 0.00003452
Iteration 185/1000 | Loss: 0.00003452
Iteration 186/1000 | Loss: 0.00003452
Iteration 187/1000 | Loss: 0.00003452
Iteration 188/1000 | Loss: 0.00003452
Iteration 189/1000 | Loss: 0.00003452
Iteration 190/1000 | Loss: 0.00003452
Iteration 191/1000 | Loss: 0.00003452
Iteration 192/1000 | Loss: 0.00003452
Iteration 193/1000 | Loss: 0.00003452
Iteration 194/1000 | Loss: 0.00003452
Iteration 195/1000 | Loss: 0.00003452
Iteration 196/1000 | Loss: 0.00003452
Iteration 197/1000 | Loss: 0.00003452
Iteration 198/1000 | Loss: 0.00003452
Iteration 199/1000 | Loss: 0.00003452
Iteration 200/1000 | Loss: 0.00003452
Iteration 201/1000 | Loss: 0.00003452
Iteration 202/1000 | Loss: 0.00003452
Iteration 203/1000 | Loss: 0.00003452
Iteration 204/1000 | Loss: 0.00003452
Iteration 205/1000 | Loss: 0.00003452
Iteration 206/1000 | Loss: 0.00003452
Iteration 207/1000 | Loss: 0.00003452
Iteration 208/1000 | Loss: 0.00003452
Iteration 209/1000 | Loss: 0.00003452
Iteration 210/1000 | Loss: 0.00003452
Iteration 211/1000 | Loss: 0.00003452
Iteration 212/1000 | Loss: 0.00003452
Iteration 213/1000 | Loss: 0.00003452
Iteration 214/1000 | Loss: 0.00003452
Iteration 215/1000 | Loss: 0.00003452
Iteration 216/1000 | Loss: 0.00003452
Iteration 217/1000 | Loss: 0.00003452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [3.451584780123085e-05, 3.451584780123085e-05, 3.451584780123085e-05, 3.451584780123085e-05, 3.451584780123085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.451584780123085e-05

Optimization complete. Final v2v error: 5.046567916870117 mm

Highest mean error: 5.447543621063232 mm for frame 92

Lowest mean error: 4.329066753387451 mm for frame 73

Saving results

Total time: 40.8827543258667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00252901
Iteration 2/25 | Loss: 0.00156515
Iteration 3/25 | Loss: 0.00146977
Iteration 4/25 | Loss: 0.00143088
Iteration 5/25 | Loss: 0.00142241
Iteration 6/25 | Loss: 0.00141989
Iteration 7/25 | Loss: 0.00141833
Iteration 8/25 | Loss: 0.00141797
Iteration 9/25 | Loss: 0.00141797
Iteration 10/25 | Loss: 0.00141797
Iteration 11/25 | Loss: 0.00141797
Iteration 12/25 | Loss: 0.00141797
Iteration 13/25 | Loss: 0.00141797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001417968305759132, 0.001417968305759132, 0.001417968305759132, 0.001417968305759132, 0.001417968305759132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001417968305759132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51008284
Iteration 2/25 | Loss: 0.00145031
Iteration 3/25 | Loss: 0.00145031
Iteration 4/25 | Loss: 0.00145031
Iteration 5/25 | Loss: 0.00145031
Iteration 6/25 | Loss: 0.00145031
Iteration 7/25 | Loss: 0.00145031
Iteration 8/25 | Loss: 0.00145031
Iteration 9/25 | Loss: 0.00145030
Iteration 10/25 | Loss: 0.00145030
Iteration 11/25 | Loss: 0.00145030
Iteration 12/25 | Loss: 0.00145030
Iteration 13/25 | Loss: 0.00145030
Iteration 14/25 | Loss: 0.00145030
Iteration 15/25 | Loss: 0.00145030
Iteration 16/25 | Loss: 0.00145030
Iteration 17/25 | Loss: 0.00145030
Iteration 18/25 | Loss: 0.00145030
Iteration 19/25 | Loss: 0.00145030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001450304756872356, 0.001450304756872356, 0.001450304756872356, 0.001450304756872356, 0.001450304756872356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001450304756872356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145030
Iteration 2/1000 | Loss: 0.00008234
Iteration 3/1000 | Loss: 0.00004493
Iteration 4/1000 | Loss: 0.00003343
Iteration 5/1000 | Loss: 0.00003074
Iteration 6/1000 | Loss: 0.00002927
Iteration 7/1000 | Loss: 0.00002811
Iteration 8/1000 | Loss: 0.00002682
Iteration 9/1000 | Loss: 0.00002601
Iteration 10/1000 | Loss: 0.00002549
Iteration 11/1000 | Loss: 0.00002511
Iteration 12/1000 | Loss: 0.00002479
Iteration 13/1000 | Loss: 0.00002476
Iteration 14/1000 | Loss: 0.00002456
Iteration 15/1000 | Loss: 0.00002456
Iteration 16/1000 | Loss: 0.00002441
Iteration 17/1000 | Loss: 0.00002426
Iteration 18/1000 | Loss: 0.00002423
Iteration 19/1000 | Loss: 0.00002423
Iteration 20/1000 | Loss: 0.00002421
Iteration 21/1000 | Loss: 0.00002421
Iteration 22/1000 | Loss: 0.00002420
Iteration 23/1000 | Loss: 0.00002418
Iteration 24/1000 | Loss: 0.00002417
Iteration 25/1000 | Loss: 0.00002417
Iteration 26/1000 | Loss: 0.00002416
Iteration 27/1000 | Loss: 0.00002416
Iteration 28/1000 | Loss: 0.00002415
Iteration 29/1000 | Loss: 0.00002415
Iteration 30/1000 | Loss: 0.00002414
Iteration 31/1000 | Loss: 0.00002414
Iteration 32/1000 | Loss: 0.00002412
Iteration 33/1000 | Loss: 0.00002412
Iteration 34/1000 | Loss: 0.00002412
Iteration 35/1000 | Loss: 0.00002412
Iteration 36/1000 | Loss: 0.00002412
Iteration 37/1000 | Loss: 0.00002412
Iteration 38/1000 | Loss: 0.00002411
Iteration 39/1000 | Loss: 0.00002411
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002409
Iteration 42/1000 | Loss: 0.00002408
Iteration 43/1000 | Loss: 0.00002404
Iteration 44/1000 | Loss: 0.00002403
Iteration 45/1000 | Loss: 0.00002403
Iteration 46/1000 | Loss: 0.00002403
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002402
Iteration 49/1000 | Loss: 0.00002399
Iteration 50/1000 | Loss: 0.00002398
Iteration 51/1000 | Loss: 0.00002398
Iteration 52/1000 | Loss: 0.00002398
Iteration 53/1000 | Loss: 0.00002397
Iteration 54/1000 | Loss: 0.00002397
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002393
Iteration 57/1000 | Loss: 0.00002393
Iteration 58/1000 | Loss: 0.00002392
Iteration 59/1000 | Loss: 0.00002392
Iteration 60/1000 | Loss: 0.00002392
Iteration 61/1000 | Loss: 0.00002392
Iteration 62/1000 | Loss: 0.00002391
Iteration 63/1000 | Loss: 0.00002391
Iteration 64/1000 | Loss: 0.00002391
Iteration 65/1000 | Loss: 0.00002390
Iteration 66/1000 | Loss: 0.00002390
Iteration 67/1000 | Loss: 0.00002390
Iteration 68/1000 | Loss: 0.00002390
Iteration 69/1000 | Loss: 0.00002390
Iteration 70/1000 | Loss: 0.00002389
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002389
Iteration 73/1000 | Loss: 0.00002389
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00002389
Iteration 76/1000 | Loss: 0.00002389
Iteration 77/1000 | Loss: 0.00002389
Iteration 78/1000 | Loss: 0.00002389
Iteration 79/1000 | Loss: 0.00002389
Iteration 80/1000 | Loss: 0.00002388
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002387
Iteration 83/1000 | Loss: 0.00002387
Iteration 84/1000 | Loss: 0.00002387
Iteration 85/1000 | Loss: 0.00002387
Iteration 86/1000 | Loss: 0.00002387
Iteration 87/1000 | Loss: 0.00002387
Iteration 88/1000 | Loss: 0.00002387
Iteration 89/1000 | Loss: 0.00002387
Iteration 90/1000 | Loss: 0.00002387
Iteration 91/1000 | Loss: 0.00002387
Iteration 92/1000 | Loss: 0.00002387
Iteration 93/1000 | Loss: 0.00002386
Iteration 94/1000 | Loss: 0.00002386
Iteration 95/1000 | Loss: 0.00002386
Iteration 96/1000 | Loss: 0.00002386
Iteration 97/1000 | Loss: 0.00002386
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002383
Iteration 101/1000 | Loss: 0.00002383
Iteration 102/1000 | Loss: 0.00002383
Iteration 103/1000 | Loss: 0.00002382
Iteration 104/1000 | Loss: 0.00002382
Iteration 105/1000 | Loss: 0.00002382
Iteration 106/1000 | Loss: 0.00002381
Iteration 107/1000 | Loss: 0.00002381
Iteration 108/1000 | Loss: 0.00002381
Iteration 109/1000 | Loss: 0.00002379
Iteration 110/1000 | Loss: 0.00002379
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002378
Iteration 113/1000 | Loss: 0.00002377
Iteration 114/1000 | Loss: 0.00002377
Iteration 115/1000 | Loss: 0.00002377
Iteration 116/1000 | Loss: 0.00002376
Iteration 117/1000 | Loss: 0.00002376
Iteration 118/1000 | Loss: 0.00002376
Iteration 119/1000 | Loss: 0.00002376
Iteration 120/1000 | Loss: 0.00002376
Iteration 121/1000 | Loss: 0.00002375
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002375
Iteration 126/1000 | Loss: 0.00002375
Iteration 127/1000 | Loss: 0.00002375
Iteration 128/1000 | Loss: 0.00002375
Iteration 129/1000 | Loss: 0.00002375
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002375
Iteration 132/1000 | Loss: 0.00002375
Iteration 133/1000 | Loss: 0.00002375
Iteration 134/1000 | Loss: 0.00002375
Iteration 135/1000 | Loss: 0.00002375
Iteration 136/1000 | Loss: 0.00002375
Iteration 137/1000 | Loss: 0.00002375
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002375
Iteration 140/1000 | Loss: 0.00002375
Iteration 141/1000 | Loss: 0.00002375
Iteration 142/1000 | Loss: 0.00002375
Iteration 143/1000 | Loss: 0.00002375
Iteration 144/1000 | Loss: 0.00002375
Iteration 145/1000 | Loss: 0.00002375
Iteration 146/1000 | Loss: 0.00002375
Iteration 147/1000 | Loss: 0.00002375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.3745951693854295e-05, 2.3745951693854295e-05, 2.3745951693854295e-05, 2.3745951693854295e-05, 2.3745951693854295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3745951693854295e-05

Optimization complete. Final v2v error: 4.269115924835205 mm

Highest mean error: 4.567684173583984 mm for frame 78

Lowest mean error: 4.017794609069824 mm for frame 118

Saving results

Total time: 44.34811615943909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109808
Iteration 2/25 | Loss: 0.00179109
Iteration 3/25 | Loss: 0.00147888
Iteration 4/25 | Loss: 0.00141256
Iteration 5/25 | Loss: 0.00142851
Iteration 6/25 | Loss: 0.00137924
Iteration 7/25 | Loss: 0.00136538
Iteration 8/25 | Loss: 0.00135984
Iteration 9/25 | Loss: 0.00135474
Iteration 10/25 | Loss: 0.00135025
Iteration 11/25 | Loss: 0.00134584
Iteration 12/25 | Loss: 0.00134488
Iteration 13/25 | Loss: 0.00134428
Iteration 14/25 | Loss: 0.00134405
Iteration 15/25 | Loss: 0.00134460
Iteration 16/25 | Loss: 0.00134299
Iteration 17/25 | Loss: 0.00134282
Iteration 18/25 | Loss: 0.00134094
Iteration 19/25 | Loss: 0.00134111
Iteration 20/25 | Loss: 0.00134189
Iteration 21/25 | Loss: 0.00133996
Iteration 22/25 | Loss: 0.00134009
Iteration 23/25 | Loss: 0.00134206
Iteration 24/25 | Loss: 0.00133881
Iteration 25/25 | Loss: 0.00133687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.36182976
Iteration 2/25 | Loss: 0.00139079
Iteration 3/25 | Loss: 0.00139070
Iteration 4/25 | Loss: 0.00139070
Iteration 5/25 | Loss: 0.00139070
Iteration 6/25 | Loss: 0.00139070
Iteration 7/25 | Loss: 0.00139070
Iteration 8/25 | Loss: 0.00139070
Iteration 9/25 | Loss: 0.00139070
Iteration 10/25 | Loss: 0.00139070
Iteration 11/25 | Loss: 0.00139070
Iteration 12/25 | Loss: 0.00139070
Iteration 13/25 | Loss: 0.00139070
Iteration 14/25 | Loss: 0.00139070
Iteration 15/25 | Loss: 0.00139070
Iteration 16/25 | Loss: 0.00139070
Iteration 17/25 | Loss: 0.00139070
Iteration 18/25 | Loss: 0.00139070
Iteration 19/25 | Loss: 0.00139070
Iteration 20/25 | Loss: 0.00139070
Iteration 21/25 | Loss: 0.00139070
Iteration 22/25 | Loss: 0.00139070
Iteration 23/25 | Loss: 0.00139070
Iteration 24/25 | Loss: 0.00139070
Iteration 25/25 | Loss: 0.00139070

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139070
Iteration 2/1000 | Loss: 0.00008446
Iteration 3/1000 | Loss: 0.00004979
Iteration 4/1000 | Loss: 0.00006885
Iteration 5/1000 | Loss: 0.00003864
Iteration 6/1000 | Loss: 0.00004865
Iteration 7/1000 | Loss: 0.00003718
Iteration 8/1000 | Loss: 0.00005071
Iteration 9/1000 | Loss: 0.00007865
Iteration 10/1000 | Loss: 0.00005424
Iteration 11/1000 | Loss: 0.00007373
Iteration 12/1000 | Loss: 0.00004826
Iteration 13/1000 | Loss: 0.00007465
Iteration 14/1000 | Loss: 0.00005323
Iteration 15/1000 | Loss: 0.00006048
Iteration 16/1000 | Loss: 0.00007125
Iteration 17/1000 | Loss: 0.00007871
Iteration 18/1000 | Loss: 0.00003642
Iteration 19/1000 | Loss: 0.00003230
Iteration 20/1000 | Loss: 0.00003081
Iteration 21/1000 | Loss: 0.00002958
Iteration 22/1000 | Loss: 0.00002857
Iteration 23/1000 | Loss: 0.00002772
Iteration 24/1000 | Loss: 0.00002696
Iteration 25/1000 | Loss: 0.00002639
Iteration 26/1000 | Loss: 0.00002608
Iteration 27/1000 | Loss: 0.00002598
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002593
Iteration 30/1000 | Loss: 0.00002591
Iteration 31/1000 | Loss: 0.00002591
Iteration 32/1000 | Loss: 0.00002590
Iteration 33/1000 | Loss: 0.00002590
Iteration 34/1000 | Loss: 0.00002589
Iteration 35/1000 | Loss: 0.00002589
Iteration 36/1000 | Loss: 0.00002588
Iteration 37/1000 | Loss: 0.00002587
Iteration 38/1000 | Loss: 0.00002586
Iteration 39/1000 | Loss: 0.00002585
Iteration 40/1000 | Loss: 0.00002585
Iteration 41/1000 | Loss: 0.00002584
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002584
Iteration 45/1000 | Loss: 0.00002583
Iteration 46/1000 | Loss: 0.00002583
Iteration 47/1000 | Loss: 0.00002583
Iteration 48/1000 | Loss: 0.00002583
Iteration 49/1000 | Loss: 0.00002583
Iteration 50/1000 | Loss: 0.00002583
Iteration 51/1000 | Loss: 0.00002582
Iteration 52/1000 | Loss: 0.00002582
Iteration 53/1000 | Loss: 0.00002582
Iteration 54/1000 | Loss: 0.00002582
Iteration 55/1000 | Loss: 0.00002582
Iteration 56/1000 | Loss: 0.00002582
Iteration 57/1000 | Loss: 0.00002582
Iteration 58/1000 | Loss: 0.00002581
Iteration 59/1000 | Loss: 0.00002581
Iteration 60/1000 | Loss: 0.00002581
Iteration 61/1000 | Loss: 0.00002581
Iteration 62/1000 | Loss: 0.00002581
Iteration 63/1000 | Loss: 0.00002581
Iteration 64/1000 | Loss: 0.00002581
Iteration 65/1000 | Loss: 0.00002581
Iteration 66/1000 | Loss: 0.00002581
Iteration 67/1000 | Loss: 0.00002581
Iteration 68/1000 | Loss: 0.00002581
Iteration 69/1000 | Loss: 0.00002581
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002580
Iteration 73/1000 | Loss: 0.00002580
Iteration 74/1000 | Loss: 0.00002580
Iteration 75/1000 | Loss: 0.00002580
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002580
Iteration 78/1000 | Loss: 0.00002580
Iteration 79/1000 | Loss: 0.00002579
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002579
Iteration 82/1000 | Loss: 0.00002579
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002579
Iteration 85/1000 | Loss: 0.00002579
Iteration 86/1000 | Loss: 0.00002579
Iteration 87/1000 | Loss: 0.00002579
Iteration 88/1000 | Loss: 0.00002579
Iteration 89/1000 | Loss: 0.00002579
Iteration 90/1000 | Loss: 0.00002579
Iteration 91/1000 | Loss: 0.00002579
Iteration 92/1000 | Loss: 0.00002579
Iteration 93/1000 | Loss: 0.00002578
Iteration 94/1000 | Loss: 0.00002578
Iteration 95/1000 | Loss: 0.00002578
Iteration 96/1000 | Loss: 0.00002578
Iteration 97/1000 | Loss: 0.00002578
Iteration 98/1000 | Loss: 0.00002578
Iteration 99/1000 | Loss: 0.00002578
Iteration 100/1000 | Loss: 0.00002578
Iteration 101/1000 | Loss: 0.00002578
Iteration 102/1000 | Loss: 0.00002578
Iteration 103/1000 | Loss: 0.00002578
Iteration 104/1000 | Loss: 0.00002578
Iteration 105/1000 | Loss: 0.00002578
Iteration 106/1000 | Loss: 0.00002578
Iteration 107/1000 | Loss: 0.00002578
Iteration 108/1000 | Loss: 0.00002578
Iteration 109/1000 | Loss: 0.00002578
Iteration 110/1000 | Loss: 0.00002578
Iteration 111/1000 | Loss: 0.00002578
Iteration 112/1000 | Loss: 0.00002578
Iteration 113/1000 | Loss: 0.00002578
Iteration 114/1000 | Loss: 0.00002578
Iteration 115/1000 | Loss: 0.00002578
Iteration 116/1000 | Loss: 0.00002578
Iteration 117/1000 | Loss: 0.00002578
Iteration 118/1000 | Loss: 0.00002578
Iteration 119/1000 | Loss: 0.00002578
Iteration 120/1000 | Loss: 0.00002578
Iteration 121/1000 | Loss: 0.00002578
Iteration 122/1000 | Loss: 0.00002578
Iteration 123/1000 | Loss: 0.00002578
Iteration 124/1000 | Loss: 0.00002578
Iteration 125/1000 | Loss: 0.00002578
Iteration 126/1000 | Loss: 0.00002578
Iteration 127/1000 | Loss: 0.00002578
Iteration 128/1000 | Loss: 0.00002578
Iteration 129/1000 | Loss: 0.00002578
Iteration 130/1000 | Loss: 0.00002578
Iteration 131/1000 | Loss: 0.00002578
Iteration 132/1000 | Loss: 0.00002578
Iteration 133/1000 | Loss: 0.00002578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.578004205133766e-05, 2.578004205133766e-05, 2.578004205133766e-05, 2.578004205133766e-05, 2.578004205133766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.578004205133766e-05

Optimization complete. Final v2v error: 4.434837341308594 mm

Highest mean error: 5.722443103790283 mm for frame 34

Lowest mean error: 4.195826053619385 mm for frame 156

Saving results

Total time: 91.4402551651001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834142
Iteration 2/25 | Loss: 0.00195733
Iteration 3/25 | Loss: 0.00151461
Iteration 4/25 | Loss: 0.00144262
Iteration 5/25 | Loss: 0.00142471
Iteration 6/25 | Loss: 0.00141687
Iteration 7/25 | Loss: 0.00140615
Iteration 8/25 | Loss: 0.00140548
Iteration 9/25 | Loss: 0.00140165
Iteration 10/25 | Loss: 0.00139946
Iteration 11/25 | Loss: 0.00139928
Iteration 12/25 | Loss: 0.00139913
Iteration 13/25 | Loss: 0.00139911
Iteration 14/25 | Loss: 0.00139911
Iteration 15/25 | Loss: 0.00139911
Iteration 16/25 | Loss: 0.00139911
Iteration 17/25 | Loss: 0.00139911
Iteration 18/25 | Loss: 0.00139911
Iteration 19/25 | Loss: 0.00139910
Iteration 20/25 | Loss: 0.00139910
Iteration 21/25 | Loss: 0.00139910
Iteration 22/25 | Loss: 0.00139910
Iteration 23/25 | Loss: 0.00139910
Iteration 24/25 | Loss: 0.00139910
Iteration 25/25 | Loss: 0.00139910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.98324585
Iteration 2/25 | Loss: 0.00147271
Iteration 3/25 | Loss: 0.00147259
Iteration 4/25 | Loss: 0.00147259
Iteration 5/25 | Loss: 0.00147259
Iteration 6/25 | Loss: 0.00147259
Iteration 7/25 | Loss: 0.00147259
Iteration 8/25 | Loss: 0.00147259
Iteration 9/25 | Loss: 0.00147259
Iteration 10/25 | Loss: 0.00147259
Iteration 11/25 | Loss: 0.00147259
Iteration 12/25 | Loss: 0.00147259
Iteration 13/25 | Loss: 0.00147259
Iteration 14/25 | Loss: 0.00147259
Iteration 15/25 | Loss: 0.00147259
Iteration 16/25 | Loss: 0.00147259
Iteration 17/25 | Loss: 0.00147259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014725864166393876, 0.0014725864166393876, 0.0014725864166393876, 0.0014725864166393876, 0.0014725864166393876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014725864166393876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147259
Iteration 2/1000 | Loss: 0.00006656
Iteration 3/1000 | Loss: 0.00004146
Iteration 4/1000 | Loss: 0.00003432
Iteration 5/1000 | Loss: 0.00003163
Iteration 6/1000 | Loss: 0.00003003
Iteration 7/1000 | Loss: 0.00002887
Iteration 8/1000 | Loss: 0.00002816
Iteration 9/1000 | Loss: 0.00002759
Iteration 10/1000 | Loss: 0.00002716
Iteration 11/1000 | Loss: 0.00002682
Iteration 12/1000 | Loss: 0.00002660
Iteration 13/1000 | Loss: 0.00002640
Iteration 14/1000 | Loss: 0.00002637
Iteration 15/1000 | Loss: 0.00002634
Iteration 16/1000 | Loss: 0.00002629
Iteration 17/1000 | Loss: 0.00002626
Iteration 18/1000 | Loss: 0.00002625
Iteration 19/1000 | Loss: 0.00002622
Iteration 20/1000 | Loss: 0.00002622
Iteration 21/1000 | Loss: 0.00002620
Iteration 22/1000 | Loss: 0.00002620
Iteration 23/1000 | Loss: 0.00002618
Iteration 24/1000 | Loss: 0.00002614
Iteration 25/1000 | Loss: 0.00002614
Iteration 26/1000 | Loss: 0.00002613
Iteration 27/1000 | Loss: 0.00002612
Iteration 28/1000 | Loss: 0.00002612
Iteration 29/1000 | Loss: 0.00002611
Iteration 30/1000 | Loss: 0.00002610
Iteration 31/1000 | Loss: 0.00002610
Iteration 32/1000 | Loss: 0.00002609
Iteration 33/1000 | Loss: 0.00002609
Iteration 34/1000 | Loss: 0.00002608
Iteration 35/1000 | Loss: 0.00002607
Iteration 36/1000 | Loss: 0.00002606
Iteration 37/1000 | Loss: 0.00002606
Iteration 38/1000 | Loss: 0.00002605
Iteration 39/1000 | Loss: 0.00002604
Iteration 40/1000 | Loss: 0.00002604
Iteration 41/1000 | Loss: 0.00002603
Iteration 42/1000 | Loss: 0.00002602
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002601
Iteration 45/1000 | Loss: 0.00002600
Iteration 46/1000 | Loss: 0.00002600
Iteration 47/1000 | Loss: 0.00002599
Iteration 48/1000 | Loss: 0.00002599
Iteration 49/1000 | Loss: 0.00002599
Iteration 50/1000 | Loss: 0.00002598
Iteration 51/1000 | Loss: 0.00002598
Iteration 52/1000 | Loss: 0.00002598
Iteration 53/1000 | Loss: 0.00002597
Iteration 54/1000 | Loss: 0.00002597
Iteration 55/1000 | Loss: 0.00002597
Iteration 56/1000 | Loss: 0.00002597
Iteration 57/1000 | Loss: 0.00002596
Iteration 58/1000 | Loss: 0.00002596
Iteration 59/1000 | Loss: 0.00002596
Iteration 60/1000 | Loss: 0.00002596
Iteration 61/1000 | Loss: 0.00002595
Iteration 62/1000 | Loss: 0.00002595
Iteration 63/1000 | Loss: 0.00002595
Iteration 64/1000 | Loss: 0.00002594
Iteration 65/1000 | Loss: 0.00002594
Iteration 66/1000 | Loss: 0.00002594
Iteration 67/1000 | Loss: 0.00002594
Iteration 68/1000 | Loss: 0.00002593
Iteration 69/1000 | Loss: 0.00002593
Iteration 70/1000 | Loss: 0.00002593
Iteration 71/1000 | Loss: 0.00002593
Iteration 72/1000 | Loss: 0.00002593
Iteration 73/1000 | Loss: 0.00002593
Iteration 74/1000 | Loss: 0.00002592
Iteration 75/1000 | Loss: 0.00002592
Iteration 76/1000 | Loss: 0.00002592
Iteration 77/1000 | Loss: 0.00002592
Iteration 78/1000 | Loss: 0.00002591
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002591
Iteration 82/1000 | Loss: 0.00002591
Iteration 83/1000 | Loss: 0.00002591
Iteration 84/1000 | Loss: 0.00002591
Iteration 85/1000 | Loss: 0.00002591
Iteration 86/1000 | Loss: 0.00002591
Iteration 87/1000 | Loss: 0.00002591
Iteration 88/1000 | Loss: 0.00002590
Iteration 89/1000 | Loss: 0.00002590
Iteration 90/1000 | Loss: 0.00002590
Iteration 91/1000 | Loss: 0.00002590
Iteration 92/1000 | Loss: 0.00002589
Iteration 93/1000 | Loss: 0.00002589
Iteration 94/1000 | Loss: 0.00002589
Iteration 95/1000 | Loss: 0.00002589
Iteration 96/1000 | Loss: 0.00002588
Iteration 97/1000 | Loss: 0.00002588
Iteration 98/1000 | Loss: 0.00002588
Iteration 99/1000 | Loss: 0.00002588
Iteration 100/1000 | Loss: 0.00002588
Iteration 101/1000 | Loss: 0.00002588
Iteration 102/1000 | Loss: 0.00002588
Iteration 103/1000 | Loss: 0.00002588
Iteration 104/1000 | Loss: 0.00002588
Iteration 105/1000 | Loss: 0.00002587
Iteration 106/1000 | Loss: 0.00002587
Iteration 107/1000 | Loss: 0.00002587
Iteration 108/1000 | Loss: 0.00002587
Iteration 109/1000 | Loss: 0.00002587
Iteration 110/1000 | Loss: 0.00002587
Iteration 111/1000 | Loss: 0.00002586
Iteration 112/1000 | Loss: 0.00002586
Iteration 113/1000 | Loss: 0.00002586
Iteration 114/1000 | Loss: 0.00002586
Iteration 115/1000 | Loss: 0.00002586
Iteration 116/1000 | Loss: 0.00002586
Iteration 117/1000 | Loss: 0.00002586
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002586
Iteration 120/1000 | Loss: 0.00002586
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002585
Iteration 123/1000 | Loss: 0.00002585
Iteration 124/1000 | Loss: 0.00002585
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002585
Iteration 128/1000 | Loss: 0.00002585
Iteration 129/1000 | Loss: 0.00002585
Iteration 130/1000 | Loss: 0.00002585
Iteration 131/1000 | Loss: 0.00002585
Iteration 132/1000 | Loss: 0.00002585
Iteration 133/1000 | Loss: 0.00002585
Iteration 134/1000 | Loss: 0.00002584
Iteration 135/1000 | Loss: 0.00002584
Iteration 136/1000 | Loss: 0.00002584
Iteration 137/1000 | Loss: 0.00002584
Iteration 138/1000 | Loss: 0.00002584
Iteration 139/1000 | Loss: 0.00002584
Iteration 140/1000 | Loss: 0.00002584
Iteration 141/1000 | Loss: 0.00002584
Iteration 142/1000 | Loss: 0.00002584
Iteration 143/1000 | Loss: 0.00002584
Iteration 144/1000 | Loss: 0.00002584
Iteration 145/1000 | Loss: 0.00002584
Iteration 146/1000 | Loss: 0.00002584
Iteration 147/1000 | Loss: 0.00002584
Iteration 148/1000 | Loss: 0.00002584
Iteration 149/1000 | Loss: 0.00002584
Iteration 150/1000 | Loss: 0.00002584
Iteration 151/1000 | Loss: 0.00002584
Iteration 152/1000 | Loss: 0.00002584
Iteration 153/1000 | Loss: 0.00002584
Iteration 154/1000 | Loss: 0.00002584
Iteration 155/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.5840563466772437e-05, 2.5840563466772437e-05, 2.5840563466772437e-05, 2.5840563466772437e-05, 2.5840563466772437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5840563466772437e-05

Optimization complete. Final v2v error: 4.345746040344238 mm

Highest mean error: 6.7402729988098145 mm for frame 195

Lowest mean error: 3.762756824493408 mm for frame 117

Saving results

Total time: 59.959394216537476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548981
Iteration 2/25 | Loss: 0.00141602
Iteration 3/25 | Loss: 0.00126864
Iteration 4/25 | Loss: 0.00124879
Iteration 5/25 | Loss: 0.00124654
Iteration 6/25 | Loss: 0.00124511
Iteration 7/25 | Loss: 0.00124392
Iteration 8/25 | Loss: 0.00124536
Iteration 9/25 | Loss: 0.00124383
Iteration 10/25 | Loss: 0.00124382
Iteration 11/25 | Loss: 0.00124382
Iteration 12/25 | Loss: 0.00124382
Iteration 13/25 | Loss: 0.00124382
Iteration 14/25 | Loss: 0.00124382
Iteration 15/25 | Loss: 0.00124382
Iteration 16/25 | Loss: 0.00124382
Iteration 17/25 | Loss: 0.00124382
Iteration 18/25 | Loss: 0.00124381
Iteration 19/25 | Loss: 0.00124381
Iteration 20/25 | Loss: 0.00124381
Iteration 21/25 | Loss: 0.00124381
Iteration 22/25 | Loss: 0.00124381
Iteration 23/25 | Loss: 0.00124381
Iteration 24/25 | Loss: 0.00124381
Iteration 25/25 | Loss: 0.00124381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00780630
Iteration 2/25 | Loss: 0.00086542
Iteration 3/25 | Loss: 0.00086541
Iteration 4/25 | Loss: 0.00081932
Iteration 5/25 | Loss: 0.00081932
Iteration 6/25 | Loss: 0.00081932
Iteration 7/25 | Loss: 0.00081932
Iteration 8/25 | Loss: 0.00081932
Iteration 9/25 | Loss: 0.00081932
Iteration 10/25 | Loss: 0.00081932
Iteration 11/25 | Loss: 0.00081932
Iteration 12/25 | Loss: 0.00081932
Iteration 13/25 | Loss: 0.00081932
Iteration 14/25 | Loss: 0.00081932
Iteration 15/25 | Loss: 0.00081932
Iteration 16/25 | Loss: 0.00081932
Iteration 17/25 | Loss: 0.00081932
Iteration 18/25 | Loss: 0.00081932
Iteration 19/25 | Loss: 0.00081932
Iteration 20/25 | Loss: 0.00081932
Iteration 21/25 | Loss: 0.00081932
Iteration 22/25 | Loss: 0.00081932
Iteration 23/25 | Loss: 0.00081932
Iteration 24/25 | Loss: 0.00081932
Iteration 25/25 | Loss: 0.00081932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081932
Iteration 2/1000 | Loss: 0.00008299
Iteration 3/1000 | Loss: 0.00006726
Iteration 4/1000 | Loss: 0.00002564
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00003584
Iteration 7/1000 | Loss: 0.00012231
Iteration 8/1000 | Loss: 0.00002309
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00004643
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001547
Iteration 15/1000 | Loss: 0.00001535
Iteration 16/1000 | Loss: 0.00001855
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00003529
Iteration 19/1000 | Loss: 0.00001516
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00001504
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001493
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001490
Iteration 49/1000 | Loss: 0.00001490
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00005632
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001476
Iteration 76/1000 | Loss: 0.00001476
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001471
Iteration 102/1000 | Loss: 0.00001471
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001470
Iteration 112/1000 | Loss: 0.00004185
Iteration 113/1000 | Loss: 0.00006220
Iteration 114/1000 | Loss: 0.00003242
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001470
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001469
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001468
Iteration 129/1000 | Loss: 0.00001468
Iteration 130/1000 | Loss: 0.00001468
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001467
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00002643
Iteration 136/1000 | Loss: 0.00009994
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001461
Iteration 148/1000 | Loss: 0.00001461
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001461
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001460
Iteration 157/1000 | Loss: 0.00001460
Iteration 158/1000 | Loss: 0.00001460
Iteration 159/1000 | Loss: 0.00001460
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001459
Iteration 173/1000 | Loss: 0.00001459
Iteration 174/1000 | Loss: 0.00001459
Iteration 175/1000 | Loss: 0.00001459
Iteration 176/1000 | Loss: 0.00001459
Iteration 177/1000 | Loss: 0.00001459
Iteration 178/1000 | Loss: 0.00001459
Iteration 179/1000 | Loss: 0.00001459
Iteration 180/1000 | Loss: 0.00001459
Iteration 181/1000 | Loss: 0.00001459
Iteration 182/1000 | Loss: 0.00001459
Iteration 183/1000 | Loss: 0.00001459
Iteration 184/1000 | Loss: 0.00001459
Iteration 185/1000 | Loss: 0.00001459
Iteration 186/1000 | Loss: 0.00001459
Iteration 187/1000 | Loss: 0.00001459
Iteration 188/1000 | Loss: 0.00001459
Iteration 189/1000 | Loss: 0.00001458
Iteration 190/1000 | Loss: 0.00001458
Iteration 191/1000 | Loss: 0.00001458
Iteration 192/1000 | Loss: 0.00001458
Iteration 193/1000 | Loss: 0.00001458
Iteration 194/1000 | Loss: 0.00001458
Iteration 195/1000 | Loss: 0.00001458
Iteration 196/1000 | Loss: 0.00001458
Iteration 197/1000 | Loss: 0.00001458
Iteration 198/1000 | Loss: 0.00001458
Iteration 199/1000 | Loss: 0.00001458
Iteration 200/1000 | Loss: 0.00001457
Iteration 201/1000 | Loss: 0.00001457
Iteration 202/1000 | Loss: 0.00001457
Iteration 203/1000 | Loss: 0.00001457
Iteration 204/1000 | Loss: 0.00001457
Iteration 205/1000 | Loss: 0.00001457
Iteration 206/1000 | Loss: 0.00001457
Iteration 207/1000 | Loss: 0.00001457
Iteration 208/1000 | Loss: 0.00001457
Iteration 209/1000 | Loss: 0.00001457
Iteration 210/1000 | Loss: 0.00001457
Iteration 211/1000 | Loss: 0.00001457
Iteration 212/1000 | Loss: 0.00001457
Iteration 213/1000 | Loss: 0.00001457
Iteration 214/1000 | Loss: 0.00001457
Iteration 215/1000 | Loss: 0.00001457
Iteration 216/1000 | Loss: 0.00001457
Iteration 217/1000 | Loss: 0.00001457
Iteration 218/1000 | Loss: 0.00001456
Iteration 219/1000 | Loss: 0.00001456
Iteration 220/1000 | Loss: 0.00001456
Iteration 221/1000 | Loss: 0.00001456
Iteration 222/1000 | Loss: 0.00001456
Iteration 223/1000 | Loss: 0.00001456
Iteration 224/1000 | Loss: 0.00001456
Iteration 225/1000 | Loss: 0.00001456
Iteration 226/1000 | Loss: 0.00001456
Iteration 227/1000 | Loss: 0.00001456
Iteration 228/1000 | Loss: 0.00001456
Iteration 229/1000 | Loss: 0.00001456
Iteration 230/1000 | Loss: 0.00001456
Iteration 231/1000 | Loss: 0.00001456
Iteration 232/1000 | Loss: 0.00001456
Iteration 233/1000 | Loss: 0.00001456
Iteration 234/1000 | Loss: 0.00001456
Iteration 235/1000 | Loss: 0.00001456
Iteration 236/1000 | Loss: 0.00001456
Iteration 237/1000 | Loss: 0.00001456
Iteration 238/1000 | Loss: 0.00001456
Iteration 239/1000 | Loss: 0.00001456
Iteration 240/1000 | Loss: 0.00001456
Iteration 241/1000 | Loss: 0.00001456
Iteration 242/1000 | Loss: 0.00001456
Iteration 243/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.4558282600773964e-05, 1.4558282600773964e-05, 1.4558282600773964e-05, 1.4558282600773964e-05, 1.4558282600773964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4558282600773964e-05

Optimization complete. Final v2v error: 3.2117505073547363 mm

Highest mean error: 3.5787858963012695 mm for frame 195

Lowest mean error: 2.958998441696167 mm for frame 230

Saving results

Total time: 81.43946981430054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989848
Iteration 2/25 | Loss: 0.00205511
Iteration 3/25 | Loss: 0.00161478
Iteration 4/25 | Loss: 0.00157249
Iteration 5/25 | Loss: 0.00150351
Iteration 6/25 | Loss: 0.00152108
Iteration 7/25 | Loss: 0.00163740
Iteration 8/25 | Loss: 0.00143451
Iteration 9/25 | Loss: 0.00138370
Iteration 10/25 | Loss: 0.00135354
Iteration 11/25 | Loss: 0.00133365
Iteration 12/25 | Loss: 0.00132277
Iteration 13/25 | Loss: 0.00131957
Iteration 14/25 | Loss: 0.00131835
Iteration 15/25 | Loss: 0.00131682
Iteration 16/25 | Loss: 0.00131618
Iteration 17/25 | Loss: 0.00132030
Iteration 18/25 | Loss: 0.00131623
Iteration 19/25 | Loss: 0.00131191
Iteration 20/25 | Loss: 0.00130994
Iteration 21/25 | Loss: 0.00130772
Iteration 22/25 | Loss: 0.00130615
Iteration 23/25 | Loss: 0.00130553
Iteration 24/25 | Loss: 0.00130533
Iteration 25/25 | Loss: 0.00130528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98577464
Iteration 2/25 | Loss: 0.00060737
Iteration 3/25 | Loss: 0.00060737
Iteration 4/25 | Loss: 0.00060736
Iteration 5/25 | Loss: 0.00060736
Iteration 6/25 | Loss: 0.00060736
Iteration 7/25 | Loss: 0.00060736
Iteration 8/25 | Loss: 0.00060736
Iteration 9/25 | Loss: 0.00060736
Iteration 10/25 | Loss: 0.00060736
Iteration 11/25 | Loss: 0.00060736
Iteration 12/25 | Loss: 0.00060736
Iteration 13/25 | Loss: 0.00060736
Iteration 14/25 | Loss: 0.00060736
Iteration 15/25 | Loss: 0.00060736
Iteration 16/25 | Loss: 0.00060736
Iteration 17/25 | Loss: 0.00060736
Iteration 18/25 | Loss: 0.00060736
Iteration 19/25 | Loss: 0.00060736
Iteration 20/25 | Loss: 0.00060736
Iteration 21/25 | Loss: 0.00060736
Iteration 22/25 | Loss: 0.00060736
Iteration 23/25 | Loss: 0.00060736
Iteration 24/25 | Loss: 0.00060736
Iteration 25/25 | Loss: 0.00060736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060736
Iteration 2/1000 | Loss: 0.00003405
Iteration 3/1000 | Loss: 0.00025776
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002324
Iteration 6/1000 | Loss: 0.00002225
Iteration 7/1000 | Loss: 0.00002156
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002028
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001977
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001948
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001925
Iteration 20/1000 | Loss: 0.00001925
Iteration 21/1000 | Loss: 0.00001924
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001918
Iteration 24/1000 | Loss: 0.00001915
Iteration 25/1000 | Loss: 0.00001915
Iteration 26/1000 | Loss: 0.00001915
Iteration 27/1000 | Loss: 0.00001915
Iteration 28/1000 | Loss: 0.00001914
Iteration 29/1000 | Loss: 0.00001913
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001913
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001908
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001904
Iteration 58/1000 | Loss: 0.00001904
Iteration 59/1000 | Loss: 0.00001904
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001898
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001897
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001897
Iteration 74/1000 | Loss: 0.00001897
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001895
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001894
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001890
Iteration 92/1000 | Loss: 0.00001890
Iteration 93/1000 | Loss: 0.00001890
Iteration 94/1000 | Loss: 0.00001889
Iteration 95/1000 | Loss: 0.00001889
Iteration 96/1000 | Loss: 0.00001889
Iteration 97/1000 | Loss: 0.00001888
Iteration 98/1000 | Loss: 0.00001888
Iteration 99/1000 | Loss: 0.00001888
Iteration 100/1000 | Loss: 0.00001888
Iteration 101/1000 | Loss: 0.00001888
Iteration 102/1000 | Loss: 0.00001888
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001887
Iteration 106/1000 | Loss: 0.00001887
Iteration 107/1000 | Loss: 0.00001887
Iteration 108/1000 | Loss: 0.00001886
Iteration 109/1000 | Loss: 0.00001886
Iteration 110/1000 | Loss: 0.00001886
Iteration 111/1000 | Loss: 0.00001886
Iteration 112/1000 | Loss: 0.00001886
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00001886
Iteration 115/1000 | Loss: 0.00001886
Iteration 116/1000 | Loss: 0.00001886
Iteration 117/1000 | Loss: 0.00001886
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001886
Iteration 122/1000 | Loss: 0.00001886
Iteration 123/1000 | Loss: 0.00001886
Iteration 124/1000 | Loss: 0.00001886
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.8857990653486922e-05, 1.8857990653486922e-05, 1.8857990653486922e-05, 1.8857990653486922e-05, 1.8857990653486922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8857990653486922e-05

Optimization complete. Final v2v error: 3.6521623134613037 mm

Highest mean error: 4.413669586181641 mm for frame 94

Lowest mean error: 3.306729316711426 mm for frame 16

Saving results

Total time: 82.7541332244873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001696
Iteration 2/25 | Loss: 0.01001696
Iteration 3/25 | Loss: 0.00287269
Iteration 4/25 | Loss: 0.00205204
Iteration 5/25 | Loss: 0.00180142
Iteration 6/25 | Loss: 0.00175060
Iteration 7/25 | Loss: 0.00164648
Iteration 8/25 | Loss: 0.00157184
Iteration 9/25 | Loss: 0.00152332
Iteration 10/25 | Loss: 0.00149275
Iteration 11/25 | Loss: 0.00146348
Iteration 12/25 | Loss: 0.00143516
Iteration 13/25 | Loss: 0.00144079
Iteration 14/25 | Loss: 0.00143312
Iteration 15/25 | Loss: 0.00142241
Iteration 16/25 | Loss: 0.00141885
Iteration 17/25 | Loss: 0.00141936
Iteration 18/25 | Loss: 0.00140986
Iteration 19/25 | Loss: 0.00139596
Iteration 20/25 | Loss: 0.00140083
Iteration 21/25 | Loss: 0.00139713
Iteration 22/25 | Loss: 0.00138790
Iteration 23/25 | Loss: 0.00138656
Iteration 24/25 | Loss: 0.00139224
Iteration 25/25 | Loss: 0.00138579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44968915
Iteration 2/25 | Loss: 0.00350968
Iteration 3/25 | Loss: 0.00187487
Iteration 4/25 | Loss: 0.00187486
Iteration 5/25 | Loss: 0.00187486
Iteration 6/25 | Loss: 0.00187486
Iteration 7/25 | Loss: 0.00187486
Iteration 8/25 | Loss: 0.00187486
Iteration 9/25 | Loss: 0.00187486
Iteration 10/25 | Loss: 0.00187486
Iteration 11/25 | Loss: 0.00187486
Iteration 12/25 | Loss: 0.00187486
Iteration 13/25 | Loss: 0.00187486
Iteration 14/25 | Loss: 0.00187486
Iteration 15/25 | Loss: 0.00187486
Iteration 16/25 | Loss: 0.00187486
Iteration 17/25 | Loss: 0.00187486
Iteration 18/25 | Loss: 0.00187486
Iteration 19/25 | Loss: 0.00187486
Iteration 20/25 | Loss: 0.00187486
Iteration 21/25 | Loss: 0.00187486
Iteration 22/25 | Loss: 0.00187486
Iteration 23/25 | Loss: 0.00187486
Iteration 24/25 | Loss: 0.00187486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001874860841780901, 0.001874860841780901, 0.001874860841780901, 0.001874860841780901, 0.001874860841780901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001874860841780901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187486
Iteration 2/1000 | Loss: 0.00200295
Iteration 3/1000 | Loss: 0.00045202
Iteration 4/1000 | Loss: 0.00033911
Iteration 5/1000 | Loss: 0.00019574
Iteration 6/1000 | Loss: 0.00026468
Iteration 7/1000 | Loss: 0.00025181
Iteration 8/1000 | Loss: 0.00024112
Iteration 9/1000 | Loss: 0.00043603
Iteration 10/1000 | Loss: 0.00018153
Iteration 11/1000 | Loss: 0.00013425
Iteration 12/1000 | Loss: 0.00013065
Iteration 13/1000 | Loss: 0.00018958
Iteration 14/1000 | Loss: 0.00019954
Iteration 15/1000 | Loss: 0.00076520
Iteration 16/1000 | Loss: 0.00054086
Iteration 17/1000 | Loss: 0.00025666
Iteration 18/1000 | Loss: 0.00064068
Iteration 19/1000 | Loss: 0.00184711
Iteration 20/1000 | Loss: 0.00047574
Iteration 21/1000 | Loss: 0.00011290
Iteration 22/1000 | Loss: 0.00016745
Iteration 23/1000 | Loss: 0.00061469
Iteration 24/1000 | Loss: 0.00019926
Iteration 25/1000 | Loss: 0.00016628
Iteration 26/1000 | Loss: 0.00030341
Iteration 27/1000 | Loss: 0.00020321
Iteration 28/1000 | Loss: 0.00014932
Iteration 29/1000 | Loss: 0.00103863
Iteration 30/1000 | Loss: 0.00060073
Iteration 31/1000 | Loss: 0.00009718
Iteration 32/1000 | Loss: 0.00008353
Iteration 33/1000 | Loss: 0.00045622
Iteration 34/1000 | Loss: 0.00033146
Iteration 35/1000 | Loss: 0.00030325
Iteration 36/1000 | Loss: 0.00013726
Iteration 37/1000 | Loss: 0.00014057
Iteration 38/1000 | Loss: 0.00007829
Iteration 39/1000 | Loss: 0.00007828
Iteration 40/1000 | Loss: 0.00019584
Iteration 41/1000 | Loss: 0.00021004
Iteration 42/1000 | Loss: 0.00035561
Iteration 43/1000 | Loss: 0.00029313
Iteration 44/1000 | Loss: 0.00034946
Iteration 45/1000 | Loss: 0.00016980
Iteration 46/1000 | Loss: 0.00033513
Iteration 47/1000 | Loss: 0.00008536
Iteration 48/1000 | Loss: 0.00012445
Iteration 49/1000 | Loss: 0.00009363
Iteration 50/1000 | Loss: 0.00007921
Iteration 51/1000 | Loss: 0.00009445
Iteration 52/1000 | Loss: 0.00010712
Iteration 53/1000 | Loss: 0.00008265
Iteration 54/1000 | Loss: 0.00008443
Iteration 55/1000 | Loss: 0.00008567
Iteration 56/1000 | Loss: 0.00010156
Iteration 57/1000 | Loss: 0.00027304
Iteration 58/1000 | Loss: 0.00008896
Iteration 59/1000 | Loss: 0.00008726
Iteration 60/1000 | Loss: 0.00007945
Iteration 61/1000 | Loss: 0.00007533
Iteration 62/1000 | Loss: 0.00009071
Iteration 63/1000 | Loss: 0.00007456
Iteration 64/1000 | Loss: 0.00007509
Iteration 65/1000 | Loss: 0.00007842
Iteration 66/1000 | Loss: 0.00010879
Iteration 67/1000 | Loss: 0.00049552
Iteration 68/1000 | Loss: 0.00042947
Iteration 69/1000 | Loss: 0.00025045
Iteration 70/1000 | Loss: 0.00028873
Iteration 71/1000 | Loss: 0.00016019
Iteration 72/1000 | Loss: 0.00007005
Iteration 73/1000 | Loss: 0.00018790
Iteration 74/1000 | Loss: 0.00014882
Iteration 75/1000 | Loss: 0.00027396
Iteration 76/1000 | Loss: 0.00039272
Iteration 77/1000 | Loss: 0.00029173
Iteration 78/1000 | Loss: 0.00024791
Iteration 79/1000 | Loss: 0.00022639
Iteration 80/1000 | Loss: 0.00017215
Iteration 81/1000 | Loss: 0.00006639
Iteration 82/1000 | Loss: 0.00015407
Iteration 83/1000 | Loss: 0.00005538
Iteration 84/1000 | Loss: 0.00009127
Iteration 85/1000 | Loss: 0.00006144
Iteration 86/1000 | Loss: 0.00005570
Iteration 87/1000 | Loss: 0.00005900
Iteration 88/1000 | Loss: 0.00007292
Iteration 89/1000 | Loss: 0.00005215
Iteration 90/1000 | Loss: 0.00005237
Iteration 91/1000 | Loss: 0.00004950
Iteration 92/1000 | Loss: 0.00005787
Iteration 93/1000 | Loss: 0.00005298
Iteration 94/1000 | Loss: 0.00005339
Iteration 95/1000 | Loss: 0.00008956
Iteration 96/1000 | Loss: 0.00011780
Iteration 97/1000 | Loss: 0.00006076
Iteration 98/1000 | Loss: 0.00005681
Iteration 99/1000 | Loss: 0.00005257
Iteration 100/1000 | Loss: 0.00007631
Iteration 101/1000 | Loss: 0.00025238
Iteration 102/1000 | Loss: 0.00005814
Iteration 103/1000 | Loss: 0.00006420
Iteration 104/1000 | Loss: 0.00004343
Iteration 105/1000 | Loss: 0.00008972
Iteration 106/1000 | Loss: 0.00005777
Iteration 107/1000 | Loss: 0.00007723
Iteration 108/1000 | Loss: 0.00005616
Iteration 109/1000 | Loss: 0.00007774
Iteration 110/1000 | Loss: 0.00004572
Iteration 111/1000 | Loss: 0.00004009
Iteration 112/1000 | Loss: 0.00005050
Iteration 113/1000 | Loss: 0.00003913
Iteration 114/1000 | Loss: 0.00004575
Iteration 115/1000 | Loss: 0.00003841
Iteration 116/1000 | Loss: 0.00003797
Iteration 117/1000 | Loss: 0.00003777
Iteration 118/1000 | Loss: 0.00036760
Iteration 119/1000 | Loss: 0.00003944
Iteration 120/1000 | Loss: 0.00003745
Iteration 121/1000 | Loss: 0.00003690
Iteration 122/1000 | Loss: 0.00038828
Iteration 123/1000 | Loss: 0.00003805
Iteration 124/1000 | Loss: 0.00003598
Iteration 125/1000 | Loss: 0.00003512
Iteration 126/1000 | Loss: 0.00003453
Iteration 127/1000 | Loss: 0.00024294
Iteration 128/1000 | Loss: 0.00089024
Iteration 129/1000 | Loss: 0.00058222
Iteration 130/1000 | Loss: 0.00090448
Iteration 131/1000 | Loss: 0.00072442
Iteration 132/1000 | Loss: 0.00050371
Iteration 133/1000 | Loss: 0.00042921
Iteration 134/1000 | Loss: 0.00005029
Iteration 135/1000 | Loss: 0.00042386
Iteration 136/1000 | Loss: 0.00003709
Iteration 137/1000 | Loss: 0.00003673
Iteration 138/1000 | Loss: 0.00003119
Iteration 139/1000 | Loss: 0.00041543
Iteration 140/1000 | Loss: 0.00099107
Iteration 141/1000 | Loss: 0.00183711
Iteration 142/1000 | Loss: 0.00006928
Iteration 143/1000 | Loss: 0.00048932
Iteration 144/1000 | Loss: 0.00004139
Iteration 145/1000 | Loss: 0.00019771
Iteration 146/1000 | Loss: 0.00003104
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00027012
Iteration 149/1000 | Loss: 0.00002752
Iteration 150/1000 | Loss: 0.00002572
Iteration 151/1000 | Loss: 0.00002467
Iteration 152/1000 | Loss: 0.00002410
Iteration 153/1000 | Loss: 0.00038840
Iteration 154/1000 | Loss: 0.00002547
Iteration 155/1000 | Loss: 0.00002320
Iteration 156/1000 | Loss: 0.00002227
Iteration 157/1000 | Loss: 0.00002159
Iteration 158/1000 | Loss: 0.00002099
Iteration 159/1000 | Loss: 0.00002064
Iteration 160/1000 | Loss: 0.00002042
Iteration 161/1000 | Loss: 0.00002040
Iteration 162/1000 | Loss: 0.00002038
Iteration 163/1000 | Loss: 0.00002030
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002025
Iteration 166/1000 | Loss: 0.00002020
Iteration 167/1000 | Loss: 0.00002019
Iteration 168/1000 | Loss: 0.00002019
Iteration 169/1000 | Loss: 0.00002018
Iteration 170/1000 | Loss: 0.00002018
Iteration 171/1000 | Loss: 0.00002014
Iteration 172/1000 | Loss: 0.00002014
Iteration 173/1000 | Loss: 0.00002013
Iteration 174/1000 | Loss: 0.00002013
Iteration 175/1000 | Loss: 0.00002013
Iteration 176/1000 | Loss: 0.00002012
Iteration 177/1000 | Loss: 0.00002012
Iteration 178/1000 | Loss: 0.00002011
Iteration 179/1000 | Loss: 0.00002009
Iteration 180/1000 | Loss: 0.00002007
Iteration 181/1000 | Loss: 0.00002006
Iteration 182/1000 | Loss: 0.00002006
Iteration 183/1000 | Loss: 0.00002005
Iteration 184/1000 | Loss: 0.00002004
Iteration 185/1000 | Loss: 0.00002004
Iteration 186/1000 | Loss: 0.00002003
Iteration 187/1000 | Loss: 0.00002002
Iteration 188/1000 | Loss: 0.00002002
Iteration 189/1000 | Loss: 0.00002002
Iteration 190/1000 | Loss: 0.00002001
Iteration 191/1000 | Loss: 0.00002001
Iteration 192/1000 | Loss: 0.00002001
Iteration 193/1000 | Loss: 0.00002001
Iteration 194/1000 | Loss: 0.00002000
Iteration 195/1000 | Loss: 0.00002000
Iteration 196/1000 | Loss: 0.00002000
Iteration 197/1000 | Loss: 0.00001999
Iteration 198/1000 | Loss: 0.00001999
Iteration 199/1000 | Loss: 0.00001999
Iteration 200/1000 | Loss: 0.00001999
Iteration 201/1000 | Loss: 0.00001998
Iteration 202/1000 | Loss: 0.00001998
Iteration 203/1000 | Loss: 0.00001998
Iteration 204/1000 | Loss: 0.00001998
Iteration 205/1000 | Loss: 0.00001998
Iteration 206/1000 | Loss: 0.00001997
Iteration 207/1000 | Loss: 0.00001997
Iteration 208/1000 | Loss: 0.00001997
Iteration 209/1000 | Loss: 0.00001996
Iteration 210/1000 | Loss: 0.00001996
Iteration 211/1000 | Loss: 0.00001996
Iteration 212/1000 | Loss: 0.00001995
Iteration 213/1000 | Loss: 0.00001995
Iteration 214/1000 | Loss: 0.00001995
Iteration 215/1000 | Loss: 0.00001995
Iteration 216/1000 | Loss: 0.00001995
Iteration 217/1000 | Loss: 0.00001994
Iteration 218/1000 | Loss: 0.00001994
Iteration 219/1000 | Loss: 0.00001994
Iteration 220/1000 | Loss: 0.00001994
Iteration 221/1000 | Loss: 0.00001994
Iteration 222/1000 | Loss: 0.00001994
Iteration 223/1000 | Loss: 0.00001994
Iteration 224/1000 | Loss: 0.00001994
Iteration 225/1000 | Loss: 0.00001994
Iteration 226/1000 | Loss: 0.00001994
Iteration 227/1000 | Loss: 0.00001993
Iteration 228/1000 | Loss: 0.00001993
Iteration 229/1000 | Loss: 0.00001993
Iteration 230/1000 | Loss: 0.00001992
Iteration 231/1000 | Loss: 0.00001992
Iteration 232/1000 | Loss: 0.00001992
Iteration 233/1000 | Loss: 0.00001992
Iteration 234/1000 | Loss: 0.00001992
Iteration 235/1000 | Loss: 0.00001992
Iteration 236/1000 | Loss: 0.00001992
Iteration 237/1000 | Loss: 0.00001992
Iteration 238/1000 | Loss: 0.00001992
Iteration 239/1000 | Loss: 0.00001992
Iteration 240/1000 | Loss: 0.00001992
Iteration 241/1000 | Loss: 0.00001992
Iteration 242/1000 | Loss: 0.00001992
Iteration 243/1000 | Loss: 0.00001992
Iteration 244/1000 | Loss: 0.00001992
Iteration 245/1000 | Loss: 0.00001992
Iteration 246/1000 | Loss: 0.00001991
Iteration 247/1000 | Loss: 0.00001991
Iteration 248/1000 | Loss: 0.00001991
Iteration 249/1000 | Loss: 0.00001991
Iteration 250/1000 | Loss: 0.00001991
Iteration 251/1000 | Loss: 0.00001991
Iteration 252/1000 | Loss: 0.00001991
Iteration 253/1000 | Loss: 0.00001991
Iteration 254/1000 | Loss: 0.00001991
Iteration 255/1000 | Loss: 0.00001991
Iteration 256/1000 | Loss: 0.00001990
Iteration 257/1000 | Loss: 0.00001990
Iteration 258/1000 | Loss: 0.00001990
Iteration 259/1000 | Loss: 0.00001990
Iteration 260/1000 | Loss: 0.00001990
Iteration 261/1000 | Loss: 0.00001990
Iteration 262/1000 | Loss: 0.00001990
Iteration 263/1000 | Loss: 0.00001990
Iteration 264/1000 | Loss: 0.00001989
Iteration 265/1000 | Loss: 0.00001989
Iteration 266/1000 | Loss: 0.00001989
Iteration 267/1000 | Loss: 0.00001989
Iteration 268/1000 | Loss: 0.00001989
Iteration 269/1000 | Loss: 0.00001989
Iteration 270/1000 | Loss: 0.00001989
Iteration 271/1000 | Loss: 0.00001989
Iteration 272/1000 | Loss: 0.00001989
Iteration 273/1000 | Loss: 0.00001989
Iteration 274/1000 | Loss: 0.00001989
Iteration 275/1000 | Loss: 0.00001989
Iteration 276/1000 | Loss: 0.00001989
Iteration 277/1000 | Loss: 0.00001989
Iteration 278/1000 | Loss: 0.00001989
Iteration 279/1000 | Loss: 0.00001989
Iteration 280/1000 | Loss: 0.00001989
Iteration 281/1000 | Loss: 0.00001989
Iteration 282/1000 | Loss: 0.00001989
Iteration 283/1000 | Loss: 0.00001989
Iteration 284/1000 | Loss: 0.00001989
Iteration 285/1000 | Loss: 0.00001989
Iteration 286/1000 | Loss: 0.00001989
Iteration 287/1000 | Loss: 0.00001989
Iteration 288/1000 | Loss: 0.00001989
Iteration 289/1000 | Loss: 0.00001989
Iteration 290/1000 | Loss: 0.00001989
Iteration 291/1000 | Loss: 0.00001989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.9888897440978326e-05, 1.9888897440978326e-05, 1.9888897440978326e-05, 1.9888897440978326e-05, 1.9888897440978326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9888897440978326e-05

Optimization complete. Final v2v error: 3.5922927856445312 mm

Highest mean error: 10.902623176574707 mm for frame 200

Lowest mean error: 3.2805685997009277 mm for frame 222

Saving results

Total time: 326.3376851081848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366509
Iteration 2/25 | Loss: 0.00134819
Iteration 3/25 | Loss: 0.00122637
Iteration 4/25 | Loss: 0.00119996
Iteration 5/25 | Loss: 0.00119115
Iteration 6/25 | Loss: 0.00118898
Iteration 7/25 | Loss: 0.00118889
Iteration 8/25 | Loss: 0.00118889
Iteration 9/25 | Loss: 0.00118889
Iteration 10/25 | Loss: 0.00118889
Iteration 11/25 | Loss: 0.00118889
Iteration 12/25 | Loss: 0.00118889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011888936860486865, 0.0011888936860486865, 0.0011888936860486865, 0.0011888936860486865, 0.0011888936860486865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011888936860486865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46144378
Iteration 2/25 | Loss: 0.00070966
Iteration 3/25 | Loss: 0.00070966
Iteration 4/25 | Loss: 0.00070966
Iteration 5/25 | Loss: 0.00070966
Iteration 6/25 | Loss: 0.00070966
Iteration 7/25 | Loss: 0.00070966
Iteration 8/25 | Loss: 0.00070965
Iteration 9/25 | Loss: 0.00070965
Iteration 10/25 | Loss: 0.00070965
Iteration 11/25 | Loss: 0.00070965
Iteration 12/25 | Loss: 0.00070965
Iteration 13/25 | Loss: 0.00070965
Iteration 14/25 | Loss: 0.00070965
Iteration 15/25 | Loss: 0.00070965
Iteration 16/25 | Loss: 0.00070965
Iteration 17/25 | Loss: 0.00070965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007096539484336972, 0.0007096539484336972, 0.0007096539484336972, 0.0007096539484336972, 0.0007096539484336972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007096539484336972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070965
Iteration 2/1000 | Loss: 0.00005043
Iteration 3/1000 | Loss: 0.00003269
Iteration 4/1000 | Loss: 0.00002505
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001960
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00001833
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001791
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001750
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001747
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001747
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001744
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001742
Iteration 40/1000 | Loss: 0.00001742
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001741
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001740
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001738
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001737
Iteration 67/1000 | Loss: 0.00001737
Iteration 68/1000 | Loss: 0.00001737
Iteration 69/1000 | Loss: 0.00001737
Iteration 70/1000 | Loss: 0.00001737
Iteration 71/1000 | Loss: 0.00001737
Iteration 72/1000 | Loss: 0.00001736
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001734
Iteration 78/1000 | Loss: 0.00001734
Iteration 79/1000 | Loss: 0.00001734
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001730
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001729
Iteration 101/1000 | Loss: 0.00001729
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001728
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001728
Iteration 113/1000 | Loss: 0.00001727
Iteration 114/1000 | Loss: 0.00001727
Iteration 115/1000 | Loss: 0.00001727
Iteration 116/1000 | Loss: 0.00001727
Iteration 117/1000 | Loss: 0.00001727
Iteration 118/1000 | Loss: 0.00001727
Iteration 119/1000 | Loss: 0.00001727
Iteration 120/1000 | Loss: 0.00001727
Iteration 121/1000 | Loss: 0.00001726
Iteration 122/1000 | Loss: 0.00001726
Iteration 123/1000 | Loss: 0.00001726
Iteration 124/1000 | Loss: 0.00001726
Iteration 125/1000 | Loss: 0.00001726
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001726
Iteration 130/1000 | Loss: 0.00001726
Iteration 131/1000 | Loss: 0.00001726
Iteration 132/1000 | Loss: 0.00001726
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00001726
Iteration 135/1000 | Loss: 0.00001726
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001726
Iteration 140/1000 | Loss: 0.00001726
Iteration 141/1000 | Loss: 0.00001726
Iteration 142/1000 | Loss: 0.00001726
Iteration 143/1000 | Loss: 0.00001726
Iteration 144/1000 | Loss: 0.00001726
Iteration 145/1000 | Loss: 0.00001726
Iteration 146/1000 | Loss: 0.00001726
Iteration 147/1000 | Loss: 0.00001726
Iteration 148/1000 | Loss: 0.00001726
Iteration 149/1000 | Loss: 0.00001726
Iteration 150/1000 | Loss: 0.00001726
Iteration 151/1000 | Loss: 0.00001726
Iteration 152/1000 | Loss: 0.00001726
Iteration 153/1000 | Loss: 0.00001726
Iteration 154/1000 | Loss: 0.00001726
Iteration 155/1000 | Loss: 0.00001726
Iteration 156/1000 | Loss: 0.00001726
Iteration 157/1000 | Loss: 0.00001726
Iteration 158/1000 | Loss: 0.00001726
Iteration 159/1000 | Loss: 0.00001726
Iteration 160/1000 | Loss: 0.00001726
Iteration 161/1000 | Loss: 0.00001726
Iteration 162/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.7264548660023138e-05, 1.7264548660023138e-05, 1.7264548660023138e-05, 1.7264548660023138e-05, 1.7264548660023138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7264548660023138e-05

Optimization complete. Final v2v error: 3.458460569381714 mm

Highest mean error: 5.002146244049072 mm for frame 151

Lowest mean error: 2.535468816757202 mm for frame 3

Saving results

Total time: 41.329193353652954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773772
Iteration 2/25 | Loss: 0.00138720
Iteration 3/25 | Loss: 0.00122500
Iteration 4/25 | Loss: 0.00121566
Iteration 5/25 | Loss: 0.00121304
Iteration 6/25 | Loss: 0.00121266
Iteration 7/25 | Loss: 0.00121266
Iteration 8/25 | Loss: 0.00121266
Iteration 9/25 | Loss: 0.00121266
Iteration 10/25 | Loss: 0.00121266
Iteration 11/25 | Loss: 0.00121266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001212659408338368, 0.001212659408338368, 0.001212659408338368, 0.001212659408338368, 0.001212659408338368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001212659408338368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43719006
Iteration 2/25 | Loss: 0.00075823
Iteration 3/25 | Loss: 0.00075822
Iteration 4/25 | Loss: 0.00075822
Iteration 5/25 | Loss: 0.00075822
Iteration 6/25 | Loss: 0.00075822
Iteration 7/25 | Loss: 0.00075822
Iteration 8/25 | Loss: 0.00075822
Iteration 9/25 | Loss: 0.00075822
Iteration 10/25 | Loss: 0.00075822
Iteration 11/25 | Loss: 0.00075822
Iteration 12/25 | Loss: 0.00075822
Iteration 13/25 | Loss: 0.00075822
Iteration 14/25 | Loss: 0.00075822
Iteration 15/25 | Loss: 0.00075822
Iteration 16/25 | Loss: 0.00075822
Iteration 17/25 | Loss: 0.00075822
Iteration 18/25 | Loss: 0.00075822
Iteration 19/25 | Loss: 0.00075822
Iteration 20/25 | Loss: 0.00075822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007582217804156244, 0.0007582217804156244, 0.0007582217804156244, 0.0007582217804156244, 0.0007582217804156244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007582217804156244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075822
Iteration 2/1000 | Loss: 0.00002770
Iteration 3/1000 | Loss: 0.00001786
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001440
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001276
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001245
Iteration 13/1000 | Loss: 0.00001219
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001192
Iteration 23/1000 | Loss: 0.00001192
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001190
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001184
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001182
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001181
Iteration 69/1000 | Loss: 0.00001181
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001180
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001179
Iteration 77/1000 | Loss: 0.00001179
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001173
Iteration 101/1000 | Loss: 0.00001173
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001168
Iteration 127/1000 | Loss: 0.00001168
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001167
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001167
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001167
Iteration 139/1000 | Loss: 0.00001167
Iteration 140/1000 | Loss: 0.00001167
Iteration 141/1000 | Loss: 0.00001167
Iteration 142/1000 | Loss: 0.00001167
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.1673356311803218e-05, 1.1673356311803218e-05, 1.1673356311803218e-05, 1.1673356311803218e-05, 1.1673356311803218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1673356311803218e-05

Optimization complete. Final v2v error: 2.929513931274414 mm

Highest mean error: 3.17901873588562 mm for frame 104

Lowest mean error: 2.7771553993225098 mm for frame 21

Saving results

Total time: 40.10523009300232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841087
Iteration 2/25 | Loss: 0.00165002
Iteration 3/25 | Loss: 0.00139139
Iteration 4/25 | Loss: 0.00135276
Iteration 5/25 | Loss: 0.00133529
Iteration 6/25 | Loss: 0.00132817
Iteration 7/25 | Loss: 0.00133466
Iteration 8/25 | Loss: 0.00131908
Iteration 9/25 | Loss: 0.00130007
Iteration 10/25 | Loss: 0.00129770
Iteration 11/25 | Loss: 0.00128852
Iteration 12/25 | Loss: 0.00128697
Iteration 13/25 | Loss: 0.00128664
Iteration 14/25 | Loss: 0.00128958
Iteration 15/25 | Loss: 0.00128560
Iteration 16/25 | Loss: 0.00128288
Iteration 17/25 | Loss: 0.00127151
Iteration 18/25 | Loss: 0.00126370
Iteration 19/25 | Loss: 0.00126241
Iteration 20/25 | Loss: 0.00126226
Iteration 21/25 | Loss: 0.00126225
Iteration 22/25 | Loss: 0.00126225
Iteration 23/25 | Loss: 0.00126225
Iteration 24/25 | Loss: 0.00126224
Iteration 25/25 | Loss: 0.00126224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98760265
Iteration 2/25 | Loss: 0.00058722
Iteration 3/25 | Loss: 0.00058721
Iteration 4/25 | Loss: 0.00058721
Iteration 5/25 | Loss: 0.00058721
Iteration 6/25 | Loss: 0.00058720
Iteration 7/25 | Loss: 0.00058720
Iteration 8/25 | Loss: 0.00058720
Iteration 9/25 | Loss: 0.00058720
Iteration 10/25 | Loss: 0.00058720
Iteration 11/25 | Loss: 0.00058720
Iteration 12/25 | Loss: 0.00058720
Iteration 13/25 | Loss: 0.00058720
Iteration 14/25 | Loss: 0.00058720
Iteration 15/25 | Loss: 0.00058720
Iteration 16/25 | Loss: 0.00058720
Iteration 17/25 | Loss: 0.00058720
Iteration 18/25 | Loss: 0.00058720
Iteration 19/25 | Loss: 0.00058720
Iteration 20/25 | Loss: 0.00058720
Iteration 21/25 | Loss: 0.00058720
Iteration 22/25 | Loss: 0.00058720
Iteration 23/25 | Loss: 0.00058720
Iteration 24/25 | Loss: 0.00058720
Iteration 25/25 | Loss: 0.00058720

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058720
Iteration 2/1000 | Loss: 0.00004653
Iteration 3/1000 | Loss: 0.00003614
Iteration 4/1000 | Loss: 0.00003192
Iteration 5/1000 | Loss: 0.00003082
Iteration 6/1000 | Loss: 0.00002922
Iteration 7/1000 | Loss: 0.00002817
Iteration 8/1000 | Loss: 0.00002746
Iteration 9/1000 | Loss: 0.00002704
Iteration 10/1000 | Loss: 0.00002658
Iteration 11/1000 | Loss: 0.00002601
Iteration 12/1000 | Loss: 0.00083460
Iteration 13/1000 | Loss: 0.00010131
Iteration 14/1000 | Loss: 0.00005140
Iteration 15/1000 | Loss: 0.00003514
Iteration 16/1000 | Loss: 0.00002945
Iteration 17/1000 | Loss: 0.00002681
Iteration 18/1000 | Loss: 0.00002437
Iteration 19/1000 | Loss: 0.00002263
Iteration 20/1000 | Loss: 0.00002163
Iteration 21/1000 | Loss: 0.00002126
Iteration 22/1000 | Loss: 0.00002113
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002090
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002084
Iteration 27/1000 | Loss: 0.00002073
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00002071
Iteration 31/1000 | Loss: 0.00002071
Iteration 32/1000 | Loss: 0.00002070
Iteration 33/1000 | Loss: 0.00002070
Iteration 34/1000 | Loss: 0.00002070
Iteration 35/1000 | Loss: 0.00002069
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002068
Iteration 39/1000 | Loss: 0.00002068
Iteration 40/1000 | Loss: 0.00002068
Iteration 41/1000 | Loss: 0.00002068
Iteration 42/1000 | Loss: 0.00002068
Iteration 43/1000 | Loss: 0.00002068
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00002067
Iteration 49/1000 | Loss: 0.00002067
Iteration 50/1000 | Loss: 0.00002067
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00002067
Iteration 53/1000 | Loss: 0.00002067
Iteration 54/1000 | Loss: 0.00002067
Iteration 55/1000 | Loss: 0.00002067
Iteration 56/1000 | Loss: 0.00002067
Iteration 57/1000 | Loss: 0.00002067
Iteration 58/1000 | Loss: 0.00002067
Iteration 59/1000 | Loss: 0.00002066
Iteration 60/1000 | Loss: 0.00002066
Iteration 61/1000 | Loss: 0.00002066
Iteration 62/1000 | Loss: 0.00002066
Iteration 63/1000 | Loss: 0.00002065
Iteration 64/1000 | Loss: 0.00002065
Iteration 65/1000 | Loss: 0.00002065
Iteration 66/1000 | Loss: 0.00002065
Iteration 67/1000 | Loss: 0.00002065
Iteration 68/1000 | Loss: 0.00002065
Iteration 69/1000 | Loss: 0.00002065
Iteration 70/1000 | Loss: 0.00002064
Iteration 71/1000 | Loss: 0.00002064
Iteration 72/1000 | Loss: 0.00002064
Iteration 73/1000 | Loss: 0.00002063
Iteration 74/1000 | Loss: 0.00002063
Iteration 75/1000 | Loss: 0.00002062
Iteration 76/1000 | Loss: 0.00002062
Iteration 77/1000 | Loss: 0.00002062
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002061
Iteration 81/1000 | Loss: 0.00002061
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002059
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002059
Iteration 92/1000 | Loss: 0.00002059
Iteration 93/1000 | Loss: 0.00002059
Iteration 94/1000 | Loss: 0.00002059
Iteration 95/1000 | Loss: 0.00002059
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002058
Iteration 101/1000 | Loss: 0.00002058
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002058
Iteration 110/1000 | Loss: 0.00002058
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002057
Iteration 114/1000 | Loss: 0.00002057
Iteration 115/1000 | Loss: 0.00002057
Iteration 116/1000 | Loss: 0.00002057
Iteration 117/1000 | Loss: 0.00002057
Iteration 118/1000 | Loss: 0.00002057
Iteration 119/1000 | Loss: 0.00002057
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002057
Iteration 122/1000 | Loss: 0.00002057
Iteration 123/1000 | Loss: 0.00002056
Iteration 124/1000 | Loss: 0.00002056
Iteration 125/1000 | Loss: 0.00002056
Iteration 126/1000 | Loss: 0.00002056
Iteration 127/1000 | Loss: 0.00002056
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002056
Iteration 130/1000 | Loss: 0.00002056
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002055
Iteration 135/1000 | Loss: 0.00002055
Iteration 136/1000 | Loss: 0.00002055
Iteration 137/1000 | Loss: 0.00002055
Iteration 138/1000 | Loss: 0.00002055
Iteration 139/1000 | Loss: 0.00002055
Iteration 140/1000 | Loss: 0.00002055
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002055
Iteration 145/1000 | Loss: 0.00002055
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002054
Iteration 148/1000 | Loss: 0.00002054
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002054
Iteration 155/1000 | Loss: 0.00002054
Iteration 156/1000 | Loss: 0.00002054
Iteration 157/1000 | Loss: 0.00002054
Iteration 158/1000 | Loss: 0.00002054
Iteration 159/1000 | Loss: 0.00002054
Iteration 160/1000 | Loss: 0.00002054
Iteration 161/1000 | Loss: 0.00002054
Iteration 162/1000 | Loss: 0.00002054
Iteration 163/1000 | Loss: 0.00002054
Iteration 164/1000 | Loss: 0.00002054
Iteration 165/1000 | Loss: 0.00002054
Iteration 166/1000 | Loss: 0.00002054
Iteration 167/1000 | Loss: 0.00002054
Iteration 168/1000 | Loss: 0.00002054
Iteration 169/1000 | Loss: 0.00002054
Iteration 170/1000 | Loss: 0.00002054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.0537358068395406e-05, 2.0537358068395406e-05, 2.0537358068395406e-05, 2.0537358068395406e-05, 2.0537358068395406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0537358068395406e-05

Optimization complete. Final v2v error: 3.792363166809082 mm

Highest mean error: 4.101871490478516 mm for frame 137

Lowest mean error: 3.652143955230713 mm for frame 2

Saving results

Total time: 78.14854216575623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499372
Iteration 2/25 | Loss: 0.00131963
Iteration 3/25 | Loss: 0.00123771
Iteration 4/25 | Loss: 0.00122387
Iteration 5/25 | Loss: 0.00122128
Iteration 6/25 | Loss: 0.00122027
Iteration 7/25 | Loss: 0.00122017
Iteration 8/25 | Loss: 0.00122017
Iteration 9/25 | Loss: 0.00122017
Iteration 10/25 | Loss: 0.00122017
Iteration 11/25 | Loss: 0.00122017
Iteration 12/25 | Loss: 0.00122017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012201748322695494, 0.0012201748322695494, 0.0012201748322695494, 0.0012201748322695494, 0.0012201748322695494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012201748322695494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05147159
Iteration 2/25 | Loss: 0.00057231
Iteration 3/25 | Loss: 0.00057226
Iteration 4/25 | Loss: 0.00057226
Iteration 5/25 | Loss: 0.00057226
Iteration 6/25 | Loss: 0.00057226
Iteration 7/25 | Loss: 0.00057226
Iteration 8/25 | Loss: 0.00057226
Iteration 9/25 | Loss: 0.00057226
Iteration 10/25 | Loss: 0.00057226
Iteration 11/25 | Loss: 0.00057226
Iteration 12/25 | Loss: 0.00057226
Iteration 13/25 | Loss: 0.00057226
Iteration 14/25 | Loss: 0.00057226
Iteration 15/25 | Loss: 0.00057226
Iteration 16/25 | Loss: 0.00057226
Iteration 17/25 | Loss: 0.00057226
Iteration 18/25 | Loss: 0.00057226
Iteration 19/25 | Loss: 0.00057226
Iteration 20/25 | Loss: 0.00057226
Iteration 21/25 | Loss: 0.00057226
Iteration 22/25 | Loss: 0.00057226
Iteration 23/25 | Loss: 0.00057226
Iteration 24/25 | Loss: 0.00057226
Iteration 25/25 | Loss: 0.00057226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057226
Iteration 2/1000 | Loss: 0.00004648
Iteration 3/1000 | Loss: 0.00003097
Iteration 4/1000 | Loss: 0.00002633
Iteration 5/1000 | Loss: 0.00002475
Iteration 6/1000 | Loss: 0.00002392
Iteration 7/1000 | Loss: 0.00002281
Iteration 8/1000 | Loss: 0.00002236
Iteration 9/1000 | Loss: 0.00002186
Iteration 10/1000 | Loss: 0.00002161
Iteration 11/1000 | Loss: 0.00002123
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002068
Iteration 14/1000 | Loss: 0.00002045
Iteration 15/1000 | Loss: 0.00002017
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001996
Iteration 18/1000 | Loss: 0.00001992
Iteration 19/1000 | Loss: 0.00001989
Iteration 20/1000 | Loss: 0.00001988
Iteration 21/1000 | Loss: 0.00001987
Iteration 22/1000 | Loss: 0.00001980
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001978
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001976
Iteration 29/1000 | Loss: 0.00001976
Iteration 30/1000 | Loss: 0.00001976
Iteration 31/1000 | Loss: 0.00001975
Iteration 32/1000 | Loss: 0.00001975
Iteration 33/1000 | Loss: 0.00001975
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001974
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001963
Iteration 39/1000 | Loss: 0.00001963
Iteration 40/1000 | Loss: 0.00001962
Iteration 41/1000 | Loss: 0.00001962
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001962
Iteration 44/1000 | Loss: 0.00001961
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001960
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001959
Iteration 50/1000 | Loss: 0.00001959
Iteration 51/1000 | Loss: 0.00001959
Iteration 52/1000 | Loss: 0.00001959
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001959
Iteration 56/1000 | Loss: 0.00001959
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001959
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00001958
Iteration 62/1000 | Loss: 0.00001958
Iteration 63/1000 | Loss: 0.00001958
Iteration 64/1000 | Loss: 0.00001958
Iteration 65/1000 | Loss: 0.00001957
Iteration 66/1000 | Loss: 0.00001957
Iteration 67/1000 | Loss: 0.00001957
Iteration 68/1000 | Loss: 0.00001957
Iteration 69/1000 | Loss: 0.00001957
Iteration 70/1000 | Loss: 0.00001957
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001956
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001956
Iteration 79/1000 | Loss: 0.00001956
Iteration 80/1000 | Loss: 0.00001956
Iteration 81/1000 | Loss: 0.00001956
Iteration 82/1000 | Loss: 0.00001956
Iteration 83/1000 | Loss: 0.00001956
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001956
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001955
Iteration 91/1000 | Loss: 0.00001955
Iteration 92/1000 | Loss: 0.00001955
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001953
Iteration 109/1000 | Loss: 0.00001953
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001953
Iteration 117/1000 | Loss: 0.00001953
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001952
Iteration 129/1000 | Loss: 0.00001952
Iteration 130/1000 | Loss: 0.00001952
Iteration 131/1000 | Loss: 0.00001952
Iteration 132/1000 | Loss: 0.00001952
Iteration 133/1000 | Loss: 0.00001952
Iteration 134/1000 | Loss: 0.00001952
Iteration 135/1000 | Loss: 0.00001952
Iteration 136/1000 | Loss: 0.00001952
Iteration 137/1000 | Loss: 0.00001952
Iteration 138/1000 | Loss: 0.00001952
Iteration 139/1000 | Loss: 0.00001952
Iteration 140/1000 | Loss: 0.00001952
Iteration 141/1000 | Loss: 0.00001952
Iteration 142/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.9515418898663484e-05, 1.9515418898663484e-05, 1.9515418898663484e-05, 1.9515418898663484e-05, 1.9515418898663484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9515418898663484e-05

Optimization complete. Final v2v error: 3.7250959873199463 mm

Highest mean error: 3.753787040710449 mm for frame 4

Lowest mean error: 3.6982758045196533 mm for frame 40

Saving results

Total time: 40.992724895477295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789759
Iteration 2/25 | Loss: 0.00168965
Iteration 3/25 | Loss: 0.00136483
Iteration 4/25 | Loss: 0.00133700
Iteration 5/25 | Loss: 0.00133372
Iteration 6/25 | Loss: 0.00133372
Iteration 7/25 | Loss: 0.00133372
Iteration 8/25 | Loss: 0.00133372
Iteration 9/25 | Loss: 0.00133372
Iteration 10/25 | Loss: 0.00133372
Iteration 11/25 | Loss: 0.00133372
Iteration 12/25 | Loss: 0.00133372
Iteration 13/25 | Loss: 0.00133372
Iteration 14/25 | Loss: 0.00133372
Iteration 15/25 | Loss: 0.00133372
Iteration 16/25 | Loss: 0.00133372
Iteration 17/25 | Loss: 0.00133372
Iteration 18/25 | Loss: 0.00133372
Iteration 19/25 | Loss: 0.00133372
Iteration 20/25 | Loss: 0.00133372
Iteration 21/25 | Loss: 0.00133372
Iteration 22/25 | Loss: 0.00133372
Iteration 23/25 | Loss: 0.00133372
Iteration 24/25 | Loss: 0.00133372
Iteration 25/25 | Loss: 0.00133372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48570180
Iteration 2/25 | Loss: 0.00071040
Iteration 3/25 | Loss: 0.00071039
Iteration 4/25 | Loss: 0.00071039
Iteration 5/25 | Loss: 0.00071039
Iteration 6/25 | Loss: 0.00071039
Iteration 7/25 | Loss: 0.00071039
Iteration 8/25 | Loss: 0.00071039
Iteration 9/25 | Loss: 0.00071038
Iteration 10/25 | Loss: 0.00071038
Iteration 11/25 | Loss: 0.00071038
Iteration 12/25 | Loss: 0.00071038
Iteration 13/25 | Loss: 0.00071038
Iteration 14/25 | Loss: 0.00071038
Iteration 15/25 | Loss: 0.00071038
Iteration 16/25 | Loss: 0.00071038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007103846874088049, 0.0007103846874088049, 0.0007103846874088049, 0.0007103846874088049, 0.0007103846874088049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007103846874088049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071038
Iteration 2/1000 | Loss: 0.00006917
Iteration 3/1000 | Loss: 0.00003684
Iteration 4/1000 | Loss: 0.00003383
Iteration 5/1000 | Loss: 0.00003248
Iteration 6/1000 | Loss: 0.00003146
Iteration 7/1000 | Loss: 0.00003059
Iteration 8/1000 | Loss: 0.00002993
Iteration 9/1000 | Loss: 0.00002929
Iteration 10/1000 | Loss: 0.00002899
Iteration 11/1000 | Loss: 0.00002878
Iteration 12/1000 | Loss: 0.00002859
Iteration 13/1000 | Loss: 0.00002853
Iteration 14/1000 | Loss: 0.00002850
Iteration 15/1000 | Loss: 0.00002844
Iteration 16/1000 | Loss: 0.00002843
Iteration 17/1000 | Loss: 0.00002843
Iteration 18/1000 | Loss: 0.00002842
Iteration 19/1000 | Loss: 0.00002842
Iteration 20/1000 | Loss: 0.00002841
Iteration 21/1000 | Loss: 0.00002840
Iteration 22/1000 | Loss: 0.00002839
Iteration 23/1000 | Loss: 0.00002839
Iteration 24/1000 | Loss: 0.00002839
Iteration 25/1000 | Loss: 0.00002836
Iteration 26/1000 | Loss: 0.00002836
Iteration 27/1000 | Loss: 0.00002835
Iteration 28/1000 | Loss: 0.00002835
Iteration 29/1000 | Loss: 0.00002835
Iteration 30/1000 | Loss: 0.00002834
Iteration 31/1000 | Loss: 0.00002834
Iteration 32/1000 | Loss: 0.00002834
Iteration 33/1000 | Loss: 0.00002834
Iteration 34/1000 | Loss: 0.00002834
Iteration 35/1000 | Loss: 0.00002834
Iteration 36/1000 | Loss: 0.00002834
Iteration 37/1000 | Loss: 0.00002834
Iteration 38/1000 | Loss: 0.00002834
Iteration 39/1000 | Loss: 0.00002834
Iteration 40/1000 | Loss: 0.00002834
Iteration 41/1000 | Loss: 0.00002834
Iteration 42/1000 | Loss: 0.00002833
Iteration 43/1000 | Loss: 0.00002833
Iteration 44/1000 | Loss: 0.00002833
Iteration 45/1000 | Loss: 0.00002833
Iteration 46/1000 | Loss: 0.00002832
Iteration 47/1000 | Loss: 0.00002832
Iteration 48/1000 | Loss: 0.00002832
Iteration 49/1000 | Loss: 0.00002831
Iteration 50/1000 | Loss: 0.00002831
Iteration 51/1000 | Loss: 0.00002831
Iteration 52/1000 | Loss: 0.00002831
Iteration 53/1000 | Loss: 0.00002831
Iteration 54/1000 | Loss: 0.00002831
Iteration 55/1000 | Loss: 0.00002830
Iteration 56/1000 | Loss: 0.00002830
Iteration 57/1000 | Loss: 0.00002830
Iteration 58/1000 | Loss: 0.00002830
Iteration 59/1000 | Loss: 0.00002830
Iteration 60/1000 | Loss: 0.00002829
Iteration 61/1000 | Loss: 0.00002829
Iteration 62/1000 | Loss: 0.00002829
Iteration 63/1000 | Loss: 0.00002828
Iteration 64/1000 | Loss: 0.00002828
Iteration 65/1000 | Loss: 0.00002828
Iteration 66/1000 | Loss: 0.00002828
Iteration 67/1000 | Loss: 0.00002828
Iteration 68/1000 | Loss: 0.00002828
Iteration 69/1000 | Loss: 0.00002828
Iteration 70/1000 | Loss: 0.00002828
Iteration 71/1000 | Loss: 0.00002827
Iteration 72/1000 | Loss: 0.00002827
Iteration 73/1000 | Loss: 0.00002827
Iteration 74/1000 | Loss: 0.00002827
Iteration 75/1000 | Loss: 0.00002827
Iteration 76/1000 | Loss: 0.00002827
Iteration 77/1000 | Loss: 0.00002827
Iteration 78/1000 | Loss: 0.00002826
Iteration 79/1000 | Loss: 0.00002826
Iteration 80/1000 | Loss: 0.00002826
Iteration 81/1000 | Loss: 0.00002826
Iteration 82/1000 | Loss: 0.00002825
Iteration 83/1000 | Loss: 0.00002825
Iteration 84/1000 | Loss: 0.00002825
Iteration 85/1000 | Loss: 0.00002825
Iteration 86/1000 | Loss: 0.00002825
Iteration 87/1000 | Loss: 0.00002824
Iteration 88/1000 | Loss: 0.00002824
Iteration 89/1000 | Loss: 0.00002824
Iteration 90/1000 | Loss: 0.00002824
Iteration 91/1000 | Loss: 0.00002824
Iteration 92/1000 | Loss: 0.00002823
Iteration 93/1000 | Loss: 0.00002823
Iteration 94/1000 | Loss: 0.00002823
Iteration 95/1000 | Loss: 0.00002823
Iteration 96/1000 | Loss: 0.00002823
Iteration 97/1000 | Loss: 0.00002822
Iteration 98/1000 | Loss: 0.00002822
Iteration 99/1000 | Loss: 0.00002822
Iteration 100/1000 | Loss: 0.00002822
Iteration 101/1000 | Loss: 0.00002822
Iteration 102/1000 | Loss: 0.00002822
Iteration 103/1000 | Loss: 0.00002822
Iteration 104/1000 | Loss: 0.00002822
Iteration 105/1000 | Loss: 0.00002822
Iteration 106/1000 | Loss: 0.00002822
Iteration 107/1000 | Loss: 0.00002821
Iteration 108/1000 | Loss: 0.00002821
Iteration 109/1000 | Loss: 0.00002821
Iteration 110/1000 | Loss: 0.00002821
Iteration 111/1000 | Loss: 0.00002821
Iteration 112/1000 | Loss: 0.00002821
Iteration 113/1000 | Loss: 0.00002821
Iteration 114/1000 | Loss: 0.00002821
Iteration 115/1000 | Loss: 0.00002821
Iteration 116/1000 | Loss: 0.00002821
Iteration 117/1000 | Loss: 0.00002821
Iteration 118/1000 | Loss: 0.00002821
Iteration 119/1000 | Loss: 0.00002821
Iteration 120/1000 | Loss: 0.00002820
Iteration 121/1000 | Loss: 0.00002820
Iteration 122/1000 | Loss: 0.00002820
Iteration 123/1000 | Loss: 0.00002820
Iteration 124/1000 | Loss: 0.00002820
Iteration 125/1000 | Loss: 0.00002820
Iteration 126/1000 | Loss: 0.00002820
Iteration 127/1000 | Loss: 0.00002820
Iteration 128/1000 | Loss: 0.00002820
Iteration 129/1000 | Loss: 0.00002820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.8204845875734463e-05, 2.8204845875734463e-05, 2.8204845875734463e-05, 2.8204845875734463e-05, 2.8204845875734463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8204845875734463e-05

Optimization complete. Final v2v error: 4.5052595138549805 mm

Highest mean error: 5.122954368591309 mm for frame 8

Lowest mean error: 3.7374775409698486 mm for frame 238

Saving results

Total time: 39.44835042953491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624274
Iteration 2/25 | Loss: 0.00171809
Iteration 3/25 | Loss: 0.00136431
Iteration 4/25 | Loss: 0.00132050
Iteration 5/25 | Loss: 0.00128076
Iteration 6/25 | Loss: 0.00128521
Iteration 7/25 | Loss: 0.00129276
Iteration 8/25 | Loss: 0.00126195
Iteration 9/25 | Loss: 0.00124623
Iteration 10/25 | Loss: 0.00124390
Iteration 11/25 | Loss: 0.00124068
Iteration 12/25 | Loss: 0.00124009
Iteration 13/25 | Loss: 0.00124088
Iteration 14/25 | Loss: 0.00124116
Iteration 15/25 | Loss: 0.00123731
Iteration 16/25 | Loss: 0.00123573
Iteration 17/25 | Loss: 0.00123446
Iteration 18/25 | Loss: 0.00123551
Iteration 19/25 | Loss: 0.00123804
Iteration 20/25 | Loss: 0.00123752
Iteration 21/25 | Loss: 0.00123573
Iteration 22/25 | Loss: 0.00123538
Iteration 23/25 | Loss: 0.00123615
Iteration 24/25 | Loss: 0.00123386
Iteration 25/25 | Loss: 0.00123299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81503820
Iteration 2/25 | Loss: 0.00094646
Iteration 3/25 | Loss: 0.00090628
Iteration 4/25 | Loss: 0.00090628
Iteration 5/25 | Loss: 0.00090628
Iteration 6/25 | Loss: 0.00090628
Iteration 7/25 | Loss: 0.00090628
Iteration 8/25 | Loss: 0.00090628
Iteration 9/25 | Loss: 0.00090628
Iteration 10/25 | Loss: 0.00090628
Iteration 11/25 | Loss: 0.00090628
Iteration 12/25 | Loss: 0.00090628
Iteration 13/25 | Loss: 0.00090628
Iteration 14/25 | Loss: 0.00090628
Iteration 15/25 | Loss: 0.00090628
Iteration 16/25 | Loss: 0.00090628
Iteration 17/25 | Loss: 0.00090628
Iteration 18/25 | Loss: 0.00090628
Iteration 19/25 | Loss: 0.00090628
Iteration 20/25 | Loss: 0.00090628
Iteration 21/25 | Loss: 0.00090628
Iteration 22/25 | Loss: 0.00090628
Iteration 23/25 | Loss: 0.00090628
Iteration 24/25 | Loss: 0.00090628
Iteration 25/25 | Loss: 0.00090628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090628
Iteration 2/1000 | Loss: 0.00004938
Iteration 3/1000 | Loss: 0.00003341
Iteration 4/1000 | Loss: 0.00002896
Iteration 5/1000 | Loss: 0.00002624
Iteration 6/1000 | Loss: 0.00002471
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00004167
Iteration 9/1000 | Loss: 0.00002300
Iteration 10/1000 | Loss: 0.00002204
Iteration 11/1000 | Loss: 0.00002169
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002134
Iteration 14/1000 | Loss: 0.00002124
Iteration 15/1000 | Loss: 0.00002120
Iteration 16/1000 | Loss: 0.00002106
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002093
Iteration 20/1000 | Loss: 0.00002091
Iteration 21/1000 | Loss: 0.00002088
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002085
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002084
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00002083
Iteration 29/1000 | Loss: 0.00002082
Iteration 30/1000 | Loss: 0.00002082
Iteration 31/1000 | Loss: 0.00002081
Iteration 32/1000 | Loss: 0.00002081
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002079
Iteration 37/1000 | Loss: 0.00002079
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002076
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002075
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002074
Iteration 51/1000 | Loss: 0.00002074
Iteration 52/1000 | Loss: 0.00002074
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002073
Iteration 56/1000 | Loss: 0.00002073
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002072
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002072
Iteration 61/1000 | Loss: 0.00002072
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002071
Iteration 64/1000 | Loss: 0.00002071
Iteration 65/1000 | Loss: 0.00002070
Iteration 66/1000 | Loss: 0.00002070
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002069
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002067
Iteration 77/1000 | Loss: 0.00002067
Iteration 78/1000 | Loss: 0.00002067
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002066
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002064
Iteration 89/1000 | Loss: 0.00002064
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002063
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002062
Iteration 95/1000 | Loss: 0.00002062
Iteration 96/1000 | Loss: 0.00002062
Iteration 97/1000 | Loss: 0.00002061
Iteration 98/1000 | Loss: 0.00002061
Iteration 99/1000 | Loss: 0.00002061
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002060
Iteration 104/1000 | Loss: 0.00002059
Iteration 105/1000 | Loss: 0.00002059
Iteration 106/1000 | Loss: 0.00002059
Iteration 107/1000 | Loss: 0.00002059
Iteration 108/1000 | Loss: 0.00002059
Iteration 109/1000 | Loss: 0.00002058
Iteration 110/1000 | Loss: 0.00002058
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002058
Iteration 114/1000 | Loss: 0.00002058
Iteration 115/1000 | Loss: 0.00002058
Iteration 116/1000 | Loss: 0.00002058
Iteration 117/1000 | Loss: 0.00002058
Iteration 118/1000 | Loss: 0.00002058
Iteration 119/1000 | Loss: 0.00002058
Iteration 120/1000 | Loss: 0.00002058
Iteration 121/1000 | Loss: 0.00002058
Iteration 122/1000 | Loss: 0.00002058
Iteration 123/1000 | Loss: 0.00002058
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002058
Iteration 126/1000 | Loss: 0.00002058
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002058
Iteration 133/1000 | Loss: 0.00002058
Iteration 134/1000 | Loss: 0.00002058
Iteration 135/1000 | Loss: 0.00002058
Iteration 136/1000 | Loss: 0.00002058
Iteration 137/1000 | Loss: 0.00002058
Iteration 138/1000 | Loss: 0.00002058
Iteration 139/1000 | Loss: 0.00002058
Iteration 140/1000 | Loss: 0.00002058
Iteration 141/1000 | Loss: 0.00002058
Iteration 142/1000 | Loss: 0.00002058
Iteration 143/1000 | Loss: 0.00002058
Iteration 144/1000 | Loss: 0.00002058
Iteration 145/1000 | Loss: 0.00002058
Iteration 146/1000 | Loss: 0.00002058
Iteration 147/1000 | Loss: 0.00002058
Iteration 148/1000 | Loss: 0.00002058
Iteration 149/1000 | Loss: 0.00002058
Iteration 150/1000 | Loss: 0.00002058
Iteration 151/1000 | Loss: 0.00002058
Iteration 152/1000 | Loss: 0.00002058
Iteration 153/1000 | Loss: 0.00002058
Iteration 154/1000 | Loss: 0.00002058
Iteration 155/1000 | Loss: 0.00002058
Iteration 156/1000 | Loss: 0.00002058
Iteration 157/1000 | Loss: 0.00002058
Iteration 158/1000 | Loss: 0.00002058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.057576784864068e-05, 2.057576784864068e-05, 2.057576784864068e-05, 2.057576784864068e-05, 2.057576784864068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.057576784864068e-05

Optimization complete. Final v2v error: 3.7535645961761475 mm

Highest mean error: 6.805370807647705 mm for frame 64

Lowest mean error: 2.880488634109497 mm for frame 181

Saving results

Total time: 90.16076588630676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801820
Iteration 2/25 | Loss: 0.00157273
Iteration 3/25 | Loss: 0.00135310
Iteration 4/25 | Loss: 0.00132547
Iteration 5/25 | Loss: 0.00131837
Iteration 6/25 | Loss: 0.00131738
Iteration 7/25 | Loss: 0.00131738
Iteration 8/25 | Loss: 0.00131738
Iteration 9/25 | Loss: 0.00131738
Iteration 10/25 | Loss: 0.00131738
Iteration 11/25 | Loss: 0.00131738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013173813931643963, 0.0013173813931643963, 0.0013173813931643963, 0.0013173813931643963, 0.0013173813931643963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013173813931643963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09971654
Iteration 2/25 | Loss: 0.00071613
Iteration 3/25 | Loss: 0.00071613
Iteration 4/25 | Loss: 0.00071613
Iteration 5/25 | Loss: 0.00071613
Iteration 6/25 | Loss: 0.00071612
Iteration 7/25 | Loss: 0.00071612
Iteration 8/25 | Loss: 0.00071612
Iteration 9/25 | Loss: 0.00071612
Iteration 10/25 | Loss: 0.00071612
Iteration 11/25 | Loss: 0.00071612
Iteration 12/25 | Loss: 0.00071612
Iteration 13/25 | Loss: 0.00071612
Iteration 14/25 | Loss: 0.00071612
Iteration 15/25 | Loss: 0.00071612
Iteration 16/25 | Loss: 0.00071612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007161237299442291, 0.0007161237299442291, 0.0007161237299442291, 0.0007161237299442291, 0.0007161237299442291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007161237299442291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071612
Iteration 2/1000 | Loss: 0.00008290
Iteration 3/1000 | Loss: 0.00005412
Iteration 4/1000 | Loss: 0.00004340
Iteration 5/1000 | Loss: 0.00003935
Iteration 6/1000 | Loss: 0.00003750
Iteration 7/1000 | Loss: 0.00003571
Iteration 8/1000 | Loss: 0.00003455
Iteration 9/1000 | Loss: 0.00003353
Iteration 10/1000 | Loss: 0.00003295
Iteration 11/1000 | Loss: 0.00003245
Iteration 12/1000 | Loss: 0.00003202
Iteration 13/1000 | Loss: 0.00003176
Iteration 14/1000 | Loss: 0.00003147
Iteration 15/1000 | Loss: 0.00003119
Iteration 16/1000 | Loss: 0.00003100
Iteration 17/1000 | Loss: 0.00003076
Iteration 18/1000 | Loss: 0.00003060
Iteration 19/1000 | Loss: 0.00003053
Iteration 20/1000 | Loss: 0.00003050
Iteration 21/1000 | Loss: 0.00003044
Iteration 22/1000 | Loss: 0.00003042
Iteration 23/1000 | Loss: 0.00003037
Iteration 24/1000 | Loss: 0.00003036
Iteration 25/1000 | Loss: 0.00003034
Iteration 26/1000 | Loss: 0.00003034
Iteration 27/1000 | Loss: 0.00003032
Iteration 28/1000 | Loss: 0.00003032
Iteration 29/1000 | Loss: 0.00003031
Iteration 30/1000 | Loss: 0.00003031
Iteration 31/1000 | Loss: 0.00003030
Iteration 32/1000 | Loss: 0.00003030
Iteration 33/1000 | Loss: 0.00003029
Iteration 34/1000 | Loss: 0.00003028
Iteration 35/1000 | Loss: 0.00003028
Iteration 36/1000 | Loss: 0.00003025
Iteration 37/1000 | Loss: 0.00003024
Iteration 38/1000 | Loss: 0.00003024
Iteration 39/1000 | Loss: 0.00003023
Iteration 40/1000 | Loss: 0.00003023
Iteration 41/1000 | Loss: 0.00003023
Iteration 42/1000 | Loss: 0.00003022
Iteration 43/1000 | Loss: 0.00003022
Iteration 44/1000 | Loss: 0.00003021
Iteration 45/1000 | Loss: 0.00003021
Iteration 46/1000 | Loss: 0.00003020
Iteration 47/1000 | Loss: 0.00003019
Iteration 48/1000 | Loss: 0.00003019
Iteration 49/1000 | Loss: 0.00003019
Iteration 50/1000 | Loss: 0.00003018
Iteration 51/1000 | Loss: 0.00003017
Iteration 52/1000 | Loss: 0.00003016
Iteration 53/1000 | Loss: 0.00003016
Iteration 54/1000 | Loss: 0.00003016
Iteration 55/1000 | Loss: 0.00003016
Iteration 56/1000 | Loss: 0.00003015
Iteration 57/1000 | Loss: 0.00003015
Iteration 58/1000 | Loss: 0.00003015
Iteration 59/1000 | Loss: 0.00003014
Iteration 60/1000 | Loss: 0.00003014
Iteration 61/1000 | Loss: 0.00003012
Iteration 62/1000 | Loss: 0.00003012
Iteration 63/1000 | Loss: 0.00003012
Iteration 64/1000 | Loss: 0.00003012
Iteration 65/1000 | Loss: 0.00003012
Iteration 66/1000 | Loss: 0.00003012
Iteration 67/1000 | Loss: 0.00003012
Iteration 68/1000 | Loss: 0.00003012
Iteration 69/1000 | Loss: 0.00003012
Iteration 70/1000 | Loss: 0.00003012
Iteration 71/1000 | Loss: 0.00003012
Iteration 72/1000 | Loss: 0.00003012
Iteration 73/1000 | Loss: 0.00003011
Iteration 74/1000 | Loss: 0.00003011
Iteration 75/1000 | Loss: 0.00003010
Iteration 76/1000 | Loss: 0.00003010
Iteration 77/1000 | Loss: 0.00003010
Iteration 78/1000 | Loss: 0.00003009
Iteration 79/1000 | Loss: 0.00003009
Iteration 80/1000 | Loss: 0.00003009
Iteration 81/1000 | Loss: 0.00003009
Iteration 82/1000 | Loss: 0.00003008
Iteration 83/1000 | Loss: 0.00003008
Iteration 84/1000 | Loss: 0.00003008
Iteration 85/1000 | Loss: 0.00003008
Iteration 86/1000 | Loss: 0.00003008
Iteration 87/1000 | Loss: 0.00003008
Iteration 88/1000 | Loss: 0.00003007
Iteration 89/1000 | Loss: 0.00003007
Iteration 90/1000 | Loss: 0.00003007
Iteration 91/1000 | Loss: 0.00003007
Iteration 92/1000 | Loss: 0.00003006
Iteration 93/1000 | Loss: 0.00003006
Iteration 94/1000 | Loss: 0.00003006
Iteration 95/1000 | Loss: 0.00003005
Iteration 96/1000 | Loss: 0.00003005
Iteration 97/1000 | Loss: 0.00003005
Iteration 98/1000 | Loss: 0.00003005
Iteration 99/1000 | Loss: 0.00003005
Iteration 100/1000 | Loss: 0.00003005
Iteration 101/1000 | Loss: 0.00003005
Iteration 102/1000 | Loss: 0.00003005
Iteration 103/1000 | Loss: 0.00003005
Iteration 104/1000 | Loss: 0.00003005
Iteration 105/1000 | Loss: 0.00003005
Iteration 106/1000 | Loss: 0.00003005
Iteration 107/1000 | Loss: 0.00003005
Iteration 108/1000 | Loss: 0.00003005
Iteration 109/1000 | Loss: 0.00003005
Iteration 110/1000 | Loss: 0.00003004
Iteration 111/1000 | Loss: 0.00003004
Iteration 112/1000 | Loss: 0.00003004
Iteration 113/1000 | Loss: 0.00003004
Iteration 114/1000 | Loss: 0.00003004
Iteration 115/1000 | Loss: 0.00003004
Iteration 116/1000 | Loss: 0.00003004
Iteration 117/1000 | Loss: 0.00003003
Iteration 118/1000 | Loss: 0.00003003
Iteration 119/1000 | Loss: 0.00003003
Iteration 120/1000 | Loss: 0.00003003
Iteration 121/1000 | Loss: 0.00003003
Iteration 122/1000 | Loss: 0.00003003
Iteration 123/1000 | Loss: 0.00003003
Iteration 124/1000 | Loss: 0.00003003
Iteration 125/1000 | Loss: 0.00003003
Iteration 126/1000 | Loss: 0.00003002
Iteration 127/1000 | Loss: 0.00003002
Iteration 128/1000 | Loss: 0.00003002
Iteration 129/1000 | Loss: 0.00003002
Iteration 130/1000 | Loss: 0.00003001
Iteration 131/1000 | Loss: 0.00003001
Iteration 132/1000 | Loss: 0.00003001
Iteration 133/1000 | Loss: 0.00003001
Iteration 134/1000 | Loss: 0.00003001
Iteration 135/1000 | Loss: 0.00003000
Iteration 136/1000 | Loss: 0.00003000
Iteration 137/1000 | Loss: 0.00003000
Iteration 138/1000 | Loss: 0.00003000
Iteration 139/1000 | Loss: 0.00003000
Iteration 140/1000 | Loss: 0.00003000
Iteration 141/1000 | Loss: 0.00002999
Iteration 142/1000 | Loss: 0.00002999
Iteration 143/1000 | Loss: 0.00002998
Iteration 144/1000 | Loss: 0.00002998
Iteration 145/1000 | Loss: 0.00002998
Iteration 146/1000 | Loss: 0.00002997
Iteration 147/1000 | Loss: 0.00002997
Iteration 148/1000 | Loss: 0.00002997
Iteration 149/1000 | Loss: 0.00002997
Iteration 150/1000 | Loss: 0.00002997
Iteration 151/1000 | Loss: 0.00002997
Iteration 152/1000 | Loss: 0.00002997
Iteration 153/1000 | Loss: 0.00002997
Iteration 154/1000 | Loss: 0.00002997
Iteration 155/1000 | Loss: 0.00002996
Iteration 156/1000 | Loss: 0.00002996
Iteration 157/1000 | Loss: 0.00002996
Iteration 158/1000 | Loss: 0.00002996
Iteration 159/1000 | Loss: 0.00002996
Iteration 160/1000 | Loss: 0.00002996
Iteration 161/1000 | Loss: 0.00002995
Iteration 162/1000 | Loss: 0.00002995
Iteration 163/1000 | Loss: 0.00002995
Iteration 164/1000 | Loss: 0.00002995
Iteration 165/1000 | Loss: 0.00002995
Iteration 166/1000 | Loss: 0.00002995
Iteration 167/1000 | Loss: 0.00002995
Iteration 168/1000 | Loss: 0.00002995
Iteration 169/1000 | Loss: 0.00002995
Iteration 170/1000 | Loss: 0.00002995
Iteration 171/1000 | Loss: 0.00002995
Iteration 172/1000 | Loss: 0.00002995
Iteration 173/1000 | Loss: 0.00002995
Iteration 174/1000 | Loss: 0.00002995
Iteration 175/1000 | Loss: 0.00002995
Iteration 176/1000 | Loss: 0.00002995
Iteration 177/1000 | Loss: 0.00002995
Iteration 178/1000 | Loss: 0.00002995
Iteration 179/1000 | Loss: 0.00002995
Iteration 180/1000 | Loss: 0.00002995
Iteration 181/1000 | Loss: 0.00002995
Iteration 182/1000 | Loss: 0.00002995
Iteration 183/1000 | Loss: 0.00002995
Iteration 184/1000 | Loss: 0.00002995
Iteration 185/1000 | Loss: 0.00002995
Iteration 186/1000 | Loss: 0.00002995
Iteration 187/1000 | Loss: 0.00002995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.9953327612020075e-05, 2.9953327612020075e-05, 2.9953327612020075e-05, 2.9953327612020075e-05, 2.9953327612020075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9953327612020075e-05

Optimization complete. Final v2v error: 4.429749011993408 mm

Highest mean error: 5.66816520690918 mm for frame 35

Lowest mean error: 3.380434989929199 mm for frame 0

Saving results

Total time: 55.602627754211426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433931
Iteration 2/25 | Loss: 0.00128515
Iteration 3/25 | Loss: 0.00120940
Iteration 4/25 | Loss: 0.00119972
Iteration 5/25 | Loss: 0.00119727
Iteration 6/25 | Loss: 0.00119678
Iteration 7/25 | Loss: 0.00119678
Iteration 8/25 | Loss: 0.00119678
Iteration 9/25 | Loss: 0.00119678
Iteration 10/25 | Loss: 0.00119678
Iteration 11/25 | Loss: 0.00119678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011967831524088979, 0.0011967831524088979, 0.0011967831524088979, 0.0011967831524088979, 0.0011967831524088979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011967831524088979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17683434
Iteration 2/25 | Loss: 0.00054791
Iteration 3/25 | Loss: 0.00054790
Iteration 4/25 | Loss: 0.00054790
Iteration 5/25 | Loss: 0.00054790
Iteration 6/25 | Loss: 0.00054790
Iteration 7/25 | Loss: 0.00054790
Iteration 8/25 | Loss: 0.00054790
Iteration 9/25 | Loss: 0.00054790
Iteration 10/25 | Loss: 0.00054790
Iteration 11/25 | Loss: 0.00054790
Iteration 12/25 | Loss: 0.00054790
Iteration 13/25 | Loss: 0.00054790
Iteration 14/25 | Loss: 0.00054790
Iteration 15/25 | Loss: 0.00054790
Iteration 16/25 | Loss: 0.00054790
Iteration 17/25 | Loss: 0.00054790
Iteration 18/25 | Loss: 0.00054790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005478998646140099, 0.0005478998646140099, 0.0005478998646140099, 0.0005478998646140099, 0.0005478998646140099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005478998646140099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054790
Iteration 2/1000 | Loss: 0.00004115
Iteration 3/1000 | Loss: 0.00002471
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001798
Iteration 7/1000 | Loss: 0.00001706
Iteration 8/1000 | Loss: 0.00001669
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001587
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001539
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001509
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001509
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001509
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001509
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001509
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001509
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001509
Iteration 42/1000 | Loss: 0.00001509
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001508
Iteration 47/1000 | Loss: 0.00001508
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001508
Iteration 51/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [1.5084880942595191e-05, 1.5084880942595191e-05, 1.5084880942595191e-05, 1.5084880942595191e-05, 1.5084880942595191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5084880942595191e-05

Optimization complete. Final v2v error: 3.3343958854675293 mm

Highest mean error: 3.390312433242798 mm for frame 95

Lowest mean error: 3.2978556156158447 mm for frame 34

Saving results

Total time: 28.104339599609375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746569
Iteration 2/25 | Loss: 0.00159455
Iteration 3/25 | Loss: 0.00144958
Iteration 4/25 | Loss: 0.00143750
Iteration 5/25 | Loss: 0.00143439
Iteration 6/25 | Loss: 0.00143439
Iteration 7/25 | Loss: 0.00143439
Iteration 8/25 | Loss: 0.00143439
Iteration 9/25 | Loss: 0.00143439
Iteration 10/25 | Loss: 0.00143439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014343896182253957, 0.0014343896182253957, 0.0014343896182253957, 0.0014343896182253957, 0.0014343896182253957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014343896182253957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.36485624
Iteration 2/25 | Loss: 0.00085329
Iteration 3/25 | Loss: 0.00085329
Iteration 4/25 | Loss: 0.00085329
Iteration 5/25 | Loss: 0.00085329
Iteration 6/25 | Loss: 0.00085329
Iteration 7/25 | Loss: 0.00085329
Iteration 8/25 | Loss: 0.00085329
Iteration 9/25 | Loss: 0.00085329
Iteration 10/25 | Loss: 0.00085329
Iteration 11/25 | Loss: 0.00085329
Iteration 12/25 | Loss: 0.00085329
Iteration 13/25 | Loss: 0.00085329
Iteration 14/25 | Loss: 0.00085329
Iteration 15/25 | Loss: 0.00085329
Iteration 16/25 | Loss: 0.00085329
Iteration 17/25 | Loss: 0.00085329
Iteration 18/25 | Loss: 0.00085329
Iteration 19/25 | Loss: 0.00085329
Iteration 20/25 | Loss: 0.00085329
Iteration 21/25 | Loss: 0.00085329
Iteration 22/25 | Loss: 0.00085329
Iteration 23/25 | Loss: 0.00085329
Iteration 24/25 | Loss: 0.00085329
Iteration 25/25 | Loss: 0.00085329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000853291479870677, 0.000853291479870677, 0.000853291479870677, 0.000853291479870677, 0.000853291479870677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000853291479870677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085329
Iteration 2/1000 | Loss: 0.00007358
Iteration 3/1000 | Loss: 0.00003961
Iteration 4/1000 | Loss: 0.00003367
Iteration 5/1000 | Loss: 0.00003141
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00002957
Iteration 8/1000 | Loss: 0.00002903
Iteration 9/1000 | Loss: 0.00002856
Iteration 10/1000 | Loss: 0.00002825
Iteration 11/1000 | Loss: 0.00002799
Iteration 12/1000 | Loss: 0.00002779
Iteration 13/1000 | Loss: 0.00002772
Iteration 14/1000 | Loss: 0.00002768
Iteration 15/1000 | Loss: 0.00002767
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002763
Iteration 18/1000 | Loss: 0.00002746
Iteration 19/1000 | Loss: 0.00002739
Iteration 20/1000 | Loss: 0.00002729
Iteration 21/1000 | Loss: 0.00002725
Iteration 22/1000 | Loss: 0.00002725
Iteration 23/1000 | Loss: 0.00002723
Iteration 24/1000 | Loss: 0.00002723
Iteration 25/1000 | Loss: 0.00002723
Iteration 26/1000 | Loss: 0.00002722
Iteration 27/1000 | Loss: 0.00002721
Iteration 28/1000 | Loss: 0.00002720
Iteration 29/1000 | Loss: 0.00002720
Iteration 30/1000 | Loss: 0.00002718
Iteration 31/1000 | Loss: 0.00002715
Iteration 32/1000 | Loss: 0.00002714
Iteration 33/1000 | Loss: 0.00002711
Iteration 34/1000 | Loss: 0.00002711
Iteration 35/1000 | Loss: 0.00002710
Iteration 36/1000 | Loss: 0.00002710
Iteration 37/1000 | Loss: 0.00002710
Iteration 38/1000 | Loss: 0.00002709
Iteration 39/1000 | Loss: 0.00002709
Iteration 40/1000 | Loss: 0.00002708
Iteration 41/1000 | Loss: 0.00002708
Iteration 42/1000 | Loss: 0.00002707
Iteration 43/1000 | Loss: 0.00002707
Iteration 44/1000 | Loss: 0.00002706
Iteration 45/1000 | Loss: 0.00002706
Iteration 46/1000 | Loss: 0.00002706
Iteration 47/1000 | Loss: 0.00002706
Iteration 48/1000 | Loss: 0.00002706
Iteration 49/1000 | Loss: 0.00002706
Iteration 50/1000 | Loss: 0.00002706
Iteration 51/1000 | Loss: 0.00002706
Iteration 52/1000 | Loss: 0.00002705
Iteration 53/1000 | Loss: 0.00002705
Iteration 54/1000 | Loss: 0.00002704
Iteration 55/1000 | Loss: 0.00002704
Iteration 56/1000 | Loss: 0.00002704
Iteration 57/1000 | Loss: 0.00002703
Iteration 58/1000 | Loss: 0.00002703
Iteration 59/1000 | Loss: 0.00002703
Iteration 60/1000 | Loss: 0.00002703
Iteration 61/1000 | Loss: 0.00002700
Iteration 62/1000 | Loss: 0.00002700
Iteration 63/1000 | Loss: 0.00002700
Iteration 64/1000 | Loss: 0.00002699
Iteration 65/1000 | Loss: 0.00002699
Iteration 66/1000 | Loss: 0.00002699
Iteration 67/1000 | Loss: 0.00002699
Iteration 68/1000 | Loss: 0.00002699
Iteration 69/1000 | Loss: 0.00002698
Iteration 70/1000 | Loss: 0.00002698
Iteration 71/1000 | Loss: 0.00002698
Iteration 72/1000 | Loss: 0.00002698
Iteration 73/1000 | Loss: 0.00002697
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002697
Iteration 76/1000 | Loss: 0.00002697
Iteration 77/1000 | Loss: 0.00002697
Iteration 78/1000 | Loss: 0.00002697
Iteration 79/1000 | Loss: 0.00002697
Iteration 80/1000 | Loss: 0.00002696
Iteration 81/1000 | Loss: 0.00002696
Iteration 82/1000 | Loss: 0.00002696
Iteration 83/1000 | Loss: 0.00002696
Iteration 84/1000 | Loss: 0.00002696
Iteration 85/1000 | Loss: 0.00002696
Iteration 86/1000 | Loss: 0.00002696
Iteration 87/1000 | Loss: 0.00002696
Iteration 88/1000 | Loss: 0.00002696
Iteration 89/1000 | Loss: 0.00002695
Iteration 90/1000 | Loss: 0.00002695
Iteration 91/1000 | Loss: 0.00002695
Iteration 92/1000 | Loss: 0.00002694
Iteration 93/1000 | Loss: 0.00002694
Iteration 94/1000 | Loss: 0.00002694
Iteration 95/1000 | Loss: 0.00002693
Iteration 96/1000 | Loss: 0.00002693
Iteration 97/1000 | Loss: 0.00002693
Iteration 98/1000 | Loss: 0.00002693
Iteration 99/1000 | Loss: 0.00002693
Iteration 100/1000 | Loss: 0.00002692
Iteration 101/1000 | Loss: 0.00002692
Iteration 102/1000 | Loss: 0.00002692
Iteration 103/1000 | Loss: 0.00002692
Iteration 104/1000 | Loss: 0.00002692
Iteration 105/1000 | Loss: 0.00002692
Iteration 106/1000 | Loss: 0.00002692
Iteration 107/1000 | Loss: 0.00002691
Iteration 108/1000 | Loss: 0.00002691
Iteration 109/1000 | Loss: 0.00002691
Iteration 110/1000 | Loss: 0.00002691
Iteration 111/1000 | Loss: 0.00002691
Iteration 112/1000 | Loss: 0.00002691
Iteration 113/1000 | Loss: 0.00002691
Iteration 114/1000 | Loss: 0.00002691
Iteration 115/1000 | Loss: 0.00002691
Iteration 116/1000 | Loss: 0.00002691
Iteration 117/1000 | Loss: 0.00002691
Iteration 118/1000 | Loss: 0.00002691
Iteration 119/1000 | Loss: 0.00002691
Iteration 120/1000 | Loss: 0.00002691
Iteration 121/1000 | Loss: 0.00002691
Iteration 122/1000 | Loss: 0.00002691
Iteration 123/1000 | Loss: 0.00002691
Iteration 124/1000 | Loss: 0.00002691
Iteration 125/1000 | Loss: 0.00002691
Iteration 126/1000 | Loss: 0.00002691
Iteration 127/1000 | Loss: 0.00002691
Iteration 128/1000 | Loss: 0.00002691
Iteration 129/1000 | Loss: 0.00002691
Iteration 130/1000 | Loss: 0.00002691
Iteration 131/1000 | Loss: 0.00002691
Iteration 132/1000 | Loss: 0.00002691
Iteration 133/1000 | Loss: 0.00002691
Iteration 134/1000 | Loss: 0.00002691
Iteration 135/1000 | Loss: 0.00002691
Iteration 136/1000 | Loss: 0.00002691
Iteration 137/1000 | Loss: 0.00002691
Iteration 138/1000 | Loss: 0.00002691
Iteration 139/1000 | Loss: 0.00002691
Iteration 140/1000 | Loss: 0.00002691
Iteration 141/1000 | Loss: 0.00002691
Iteration 142/1000 | Loss: 0.00002691
Iteration 143/1000 | Loss: 0.00002691
Iteration 144/1000 | Loss: 0.00002691
Iteration 145/1000 | Loss: 0.00002691
Iteration 146/1000 | Loss: 0.00002691
Iteration 147/1000 | Loss: 0.00002691
Iteration 148/1000 | Loss: 0.00002691
Iteration 149/1000 | Loss: 0.00002691
Iteration 150/1000 | Loss: 0.00002691
Iteration 151/1000 | Loss: 0.00002691
Iteration 152/1000 | Loss: 0.00002691
Iteration 153/1000 | Loss: 0.00002691
Iteration 154/1000 | Loss: 0.00002691
Iteration 155/1000 | Loss: 0.00002691
Iteration 156/1000 | Loss: 0.00002691
Iteration 157/1000 | Loss: 0.00002691
Iteration 158/1000 | Loss: 0.00002691
Iteration 159/1000 | Loss: 0.00002691
Iteration 160/1000 | Loss: 0.00002691
Iteration 161/1000 | Loss: 0.00002691
Iteration 162/1000 | Loss: 0.00002691
Iteration 163/1000 | Loss: 0.00002691
Iteration 164/1000 | Loss: 0.00002691
Iteration 165/1000 | Loss: 0.00002691
Iteration 166/1000 | Loss: 0.00002691
Iteration 167/1000 | Loss: 0.00002691
Iteration 168/1000 | Loss: 0.00002691
Iteration 169/1000 | Loss: 0.00002691
Iteration 170/1000 | Loss: 0.00002691
Iteration 171/1000 | Loss: 0.00002691
Iteration 172/1000 | Loss: 0.00002691
Iteration 173/1000 | Loss: 0.00002691
Iteration 174/1000 | Loss: 0.00002691
Iteration 175/1000 | Loss: 0.00002691
Iteration 176/1000 | Loss: 0.00002691
Iteration 177/1000 | Loss: 0.00002691
Iteration 178/1000 | Loss: 0.00002691
Iteration 179/1000 | Loss: 0.00002691
Iteration 180/1000 | Loss: 0.00002691
Iteration 181/1000 | Loss: 0.00002691
Iteration 182/1000 | Loss: 0.00002691
Iteration 183/1000 | Loss: 0.00002691
Iteration 184/1000 | Loss: 0.00002691
Iteration 185/1000 | Loss: 0.00002691
Iteration 186/1000 | Loss: 0.00002691
Iteration 187/1000 | Loss: 0.00002691
Iteration 188/1000 | Loss: 0.00002691
Iteration 189/1000 | Loss: 0.00002691
Iteration 190/1000 | Loss: 0.00002691
Iteration 191/1000 | Loss: 0.00002691
Iteration 192/1000 | Loss: 0.00002691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.6907489882432856e-05, 2.6907489882432856e-05, 2.6907489882432856e-05, 2.6907489882432856e-05, 2.6907489882432856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6907489882432856e-05

Optimization complete. Final v2v error: 4.187638759613037 mm

Highest mean error: 4.962911605834961 mm for frame 16

Lowest mean error: 3.477031707763672 mm for frame 82

Saving results

Total time: 41.74277853965759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422478
Iteration 2/25 | Loss: 0.00133054
Iteration 3/25 | Loss: 0.00122559
Iteration 4/25 | Loss: 0.00121278
Iteration 5/25 | Loss: 0.00120916
Iteration 6/25 | Loss: 0.00120836
Iteration 7/25 | Loss: 0.00120836
Iteration 8/25 | Loss: 0.00120836
Iteration 9/25 | Loss: 0.00120836
Iteration 10/25 | Loss: 0.00120836
Iteration 11/25 | Loss: 0.00120836
Iteration 12/25 | Loss: 0.00120836
Iteration 13/25 | Loss: 0.00120836
Iteration 14/25 | Loss: 0.00120836
Iteration 15/25 | Loss: 0.00120836
Iteration 16/25 | Loss: 0.00120836
Iteration 17/25 | Loss: 0.00120836
Iteration 18/25 | Loss: 0.00120836
Iteration 19/25 | Loss: 0.00120836
Iteration 20/25 | Loss: 0.00120836
Iteration 21/25 | Loss: 0.00120836
Iteration 22/25 | Loss: 0.00120836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012083614710718393, 0.0012083614710718393, 0.0012083614710718393, 0.0012083614710718393, 0.0012083614710718393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012083614710718393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.82562685
Iteration 2/25 | Loss: 0.00076881
Iteration 3/25 | Loss: 0.00076879
Iteration 4/25 | Loss: 0.00076879
Iteration 5/25 | Loss: 0.00076879
Iteration 6/25 | Loss: 0.00076879
Iteration 7/25 | Loss: 0.00076879
Iteration 8/25 | Loss: 0.00076879
Iteration 9/25 | Loss: 0.00076879
Iteration 10/25 | Loss: 0.00076879
Iteration 11/25 | Loss: 0.00076879
Iteration 12/25 | Loss: 0.00076879
Iteration 13/25 | Loss: 0.00076879
Iteration 14/25 | Loss: 0.00076879
Iteration 15/25 | Loss: 0.00076879
Iteration 16/25 | Loss: 0.00076879
Iteration 17/25 | Loss: 0.00076879
Iteration 18/25 | Loss: 0.00076879
Iteration 19/25 | Loss: 0.00076879
Iteration 20/25 | Loss: 0.00076879
Iteration 21/25 | Loss: 0.00076879
Iteration 22/25 | Loss: 0.00076879
Iteration 23/25 | Loss: 0.00076879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007687876932322979, 0.0007687876932322979, 0.0007687876932322979, 0.0007687876932322979, 0.0007687876932322979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007687876932322979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076879
Iteration 2/1000 | Loss: 0.00002846
Iteration 3/1000 | Loss: 0.00002031
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001542
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001456
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001352
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001345
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001333
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001331
Iteration 109/1000 | Loss: 0.00001331
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001330
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001326
Iteration 133/1000 | Loss: 0.00001326
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001322
Iteration 152/1000 | Loss: 0.00001322
Iteration 153/1000 | Loss: 0.00001322
Iteration 154/1000 | Loss: 0.00001322
Iteration 155/1000 | Loss: 0.00001322
Iteration 156/1000 | Loss: 0.00001322
Iteration 157/1000 | Loss: 0.00001322
Iteration 158/1000 | Loss: 0.00001322
Iteration 159/1000 | Loss: 0.00001322
Iteration 160/1000 | Loss: 0.00001322
Iteration 161/1000 | Loss: 0.00001321
Iteration 162/1000 | Loss: 0.00001321
Iteration 163/1000 | Loss: 0.00001321
Iteration 164/1000 | Loss: 0.00001321
Iteration 165/1000 | Loss: 0.00001321
Iteration 166/1000 | Loss: 0.00001321
Iteration 167/1000 | Loss: 0.00001321
Iteration 168/1000 | Loss: 0.00001321
Iteration 169/1000 | Loss: 0.00001321
Iteration 170/1000 | Loss: 0.00001321
Iteration 171/1000 | Loss: 0.00001321
Iteration 172/1000 | Loss: 0.00001321
Iteration 173/1000 | Loss: 0.00001321
Iteration 174/1000 | Loss: 0.00001321
Iteration 175/1000 | Loss: 0.00001321
Iteration 176/1000 | Loss: 0.00001321
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001320
Iteration 179/1000 | Loss: 0.00001320
Iteration 180/1000 | Loss: 0.00001320
Iteration 181/1000 | Loss: 0.00001320
Iteration 182/1000 | Loss: 0.00001320
Iteration 183/1000 | Loss: 0.00001320
Iteration 184/1000 | Loss: 0.00001320
Iteration 185/1000 | Loss: 0.00001320
Iteration 186/1000 | Loss: 0.00001320
Iteration 187/1000 | Loss: 0.00001320
Iteration 188/1000 | Loss: 0.00001320
Iteration 189/1000 | Loss: 0.00001320
Iteration 190/1000 | Loss: 0.00001320
Iteration 191/1000 | Loss: 0.00001320
Iteration 192/1000 | Loss: 0.00001320
Iteration 193/1000 | Loss: 0.00001320
Iteration 194/1000 | Loss: 0.00001320
Iteration 195/1000 | Loss: 0.00001320
Iteration 196/1000 | Loss: 0.00001320
Iteration 197/1000 | Loss: 0.00001320
Iteration 198/1000 | Loss: 0.00001320
Iteration 199/1000 | Loss: 0.00001320
Iteration 200/1000 | Loss: 0.00001320
Iteration 201/1000 | Loss: 0.00001320
Iteration 202/1000 | Loss: 0.00001320
Iteration 203/1000 | Loss: 0.00001320
Iteration 204/1000 | Loss: 0.00001320
Iteration 205/1000 | Loss: 0.00001320
Iteration 206/1000 | Loss: 0.00001320
Iteration 207/1000 | Loss: 0.00001320
Iteration 208/1000 | Loss: 0.00001320
Iteration 209/1000 | Loss: 0.00001320
Iteration 210/1000 | Loss: 0.00001320
Iteration 211/1000 | Loss: 0.00001320
Iteration 212/1000 | Loss: 0.00001320
Iteration 213/1000 | Loss: 0.00001320
Iteration 214/1000 | Loss: 0.00001320
Iteration 215/1000 | Loss: 0.00001320
Iteration 216/1000 | Loss: 0.00001320
Iteration 217/1000 | Loss: 0.00001320
Iteration 218/1000 | Loss: 0.00001320
Iteration 219/1000 | Loss: 0.00001320
Iteration 220/1000 | Loss: 0.00001320
Iteration 221/1000 | Loss: 0.00001320
Iteration 222/1000 | Loss: 0.00001320
Iteration 223/1000 | Loss: 0.00001320
Iteration 224/1000 | Loss: 0.00001320
Iteration 225/1000 | Loss: 0.00001320
Iteration 226/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3203585695009679e-05, 1.3203585695009679e-05, 1.3203585695009679e-05, 1.3203585695009679e-05, 1.3203585695009679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3203585695009679e-05

Optimization complete. Final v2v error: 3.0904345512390137 mm

Highest mean error: 3.763612747192383 mm for frame 85

Lowest mean error: 2.7759108543395996 mm for frame 29

Saving results

Total time: 42.21140122413635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825643
Iteration 2/25 | Loss: 0.00176462
Iteration 3/25 | Loss: 0.00139442
Iteration 4/25 | Loss: 0.00136815
Iteration 5/25 | Loss: 0.00136291
Iteration 6/25 | Loss: 0.00136015
Iteration 7/25 | Loss: 0.00135341
Iteration 8/25 | Loss: 0.00135296
Iteration 9/25 | Loss: 0.00135284
Iteration 10/25 | Loss: 0.00135284
Iteration 11/25 | Loss: 0.00135284
Iteration 12/25 | Loss: 0.00135284
Iteration 13/25 | Loss: 0.00135284
Iteration 14/25 | Loss: 0.00135284
Iteration 15/25 | Loss: 0.00135284
Iteration 16/25 | Loss: 0.00135284
Iteration 17/25 | Loss: 0.00135283
Iteration 18/25 | Loss: 0.00135283
Iteration 19/25 | Loss: 0.00135283
Iteration 20/25 | Loss: 0.00135283
Iteration 21/25 | Loss: 0.00135283
Iteration 22/25 | Loss: 0.00135283
Iteration 23/25 | Loss: 0.00135283
Iteration 24/25 | Loss: 0.00135283
Iteration 25/25 | Loss: 0.00135283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27710235
Iteration 2/25 | Loss: 0.00104381
Iteration 3/25 | Loss: 0.00104381
Iteration 4/25 | Loss: 0.00104381
Iteration 5/25 | Loss: 0.00104381
Iteration 6/25 | Loss: 0.00104381
Iteration 7/25 | Loss: 0.00104381
Iteration 8/25 | Loss: 0.00104381
Iteration 9/25 | Loss: 0.00104381
Iteration 10/25 | Loss: 0.00104381
Iteration 11/25 | Loss: 0.00104381
Iteration 12/25 | Loss: 0.00104381
Iteration 13/25 | Loss: 0.00104381
Iteration 14/25 | Loss: 0.00104381
Iteration 15/25 | Loss: 0.00104381
Iteration 16/25 | Loss: 0.00104381
Iteration 17/25 | Loss: 0.00104381
Iteration 18/25 | Loss: 0.00104381
Iteration 19/25 | Loss: 0.00104381
Iteration 20/25 | Loss: 0.00104381
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010438082972541451, 0.0010438082972541451, 0.0010438082972541451, 0.0010438082972541451, 0.0010438082972541451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010438082972541451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104381
Iteration 2/1000 | Loss: 0.00003971
Iteration 3/1000 | Loss: 0.00002742
Iteration 4/1000 | Loss: 0.00002472
Iteration 5/1000 | Loss: 0.00002302
Iteration 6/1000 | Loss: 0.00002203
Iteration 7/1000 | Loss: 0.00002162
Iteration 8/1000 | Loss: 0.00002113
Iteration 9/1000 | Loss: 0.00002073
Iteration 10/1000 | Loss: 0.00002051
Iteration 11/1000 | Loss: 0.00002033
Iteration 12/1000 | Loss: 0.00002016
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002014
Iteration 16/1000 | Loss: 0.00002014
Iteration 17/1000 | Loss: 0.00002013
Iteration 18/1000 | Loss: 0.00002013
Iteration 19/1000 | Loss: 0.00002013
Iteration 20/1000 | Loss: 0.00002013
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002008
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00002005
Iteration 26/1000 | Loss: 0.00002005
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00002001
Iteration 29/1000 | Loss: 0.00002001
Iteration 30/1000 | Loss: 0.00002001
Iteration 31/1000 | Loss: 0.00002001
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00002001
Iteration 35/1000 | Loss: 0.00002001
Iteration 36/1000 | Loss: 0.00002001
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00002000
Iteration 40/1000 | Loss: 0.00001999
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001998
Iteration 45/1000 | Loss: 0.00001997
Iteration 46/1000 | Loss: 0.00001997
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001996
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001996
Iteration 53/1000 | Loss: 0.00001996
Iteration 54/1000 | Loss: 0.00001996
Iteration 55/1000 | Loss: 0.00001996
Iteration 56/1000 | Loss: 0.00001996
Iteration 57/1000 | Loss: 0.00001996
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001995
Iteration 60/1000 | Loss: 0.00001995
Iteration 61/1000 | Loss: 0.00001995
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001995
Iteration 64/1000 | Loss: 0.00001995
Iteration 65/1000 | Loss: 0.00001995
Iteration 66/1000 | Loss: 0.00001994
Iteration 67/1000 | Loss: 0.00001994
Iteration 68/1000 | Loss: 0.00001994
Iteration 69/1000 | Loss: 0.00001994
Iteration 70/1000 | Loss: 0.00001994
Iteration 71/1000 | Loss: 0.00001994
Iteration 72/1000 | Loss: 0.00001994
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001993
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00001993
Iteration 78/1000 | Loss: 0.00001993
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001993
Iteration 81/1000 | Loss: 0.00001993
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001993
Iteration 84/1000 | Loss: 0.00001992
Iteration 85/1000 | Loss: 0.00001992
Iteration 86/1000 | Loss: 0.00001992
Iteration 87/1000 | Loss: 0.00001992
Iteration 88/1000 | Loss: 0.00001992
Iteration 89/1000 | Loss: 0.00001992
Iteration 90/1000 | Loss: 0.00001992
Iteration 91/1000 | Loss: 0.00001992
Iteration 92/1000 | Loss: 0.00001992
Iteration 93/1000 | Loss: 0.00001992
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001991
Iteration 97/1000 | Loss: 0.00001991
Iteration 98/1000 | Loss: 0.00001991
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001990
Iteration 101/1000 | Loss: 0.00001990
Iteration 102/1000 | Loss: 0.00001990
Iteration 103/1000 | Loss: 0.00001990
Iteration 104/1000 | Loss: 0.00001990
Iteration 105/1000 | Loss: 0.00001990
Iteration 106/1000 | Loss: 0.00001990
Iteration 107/1000 | Loss: 0.00001990
Iteration 108/1000 | Loss: 0.00001990
Iteration 109/1000 | Loss: 0.00001989
Iteration 110/1000 | Loss: 0.00001989
Iteration 111/1000 | Loss: 0.00001989
Iteration 112/1000 | Loss: 0.00001989
Iteration 113/1000 | Loss: 0.00001989
Iteration 114/1000 | Loss: 0.00001989
Iteration 115/1000 | Loss: 0.00001989
Iteration 116/1000 | Loss: 0.00001989
Iteration 117/1000 | Loss: 0.00001989
Iteration 118/1000 | Loss: 0.00001989
Iteration 119/1000 | Loss: 0.00001989
Iteration 120/1000 | Loss: 0.00001989
Iteration 121/1000 | Loss: 0.00001989
Iteration 122/1000 | Loss: 0.00001989
Iteration 123/1000 | Loss: 0.00001989
Iteration 124/1000 | Loss: 0.00001988
Iteration 125/1000 | Loss: 0.00001988
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001988
Iteration 131/1000 | Loss: 0.00001988
Iteration 132/1000 | Loss: 0.00001988
Iteration 133/1000 | Loss: 0.00001987
Iteration 134/1000 | Loss: 0.00001987
Iteration 135/1000 | Loss: 0.00001987
Iteration 136/1000 | Loss: 0.00001987
Iteration 137/1000 | Loss: 0.00001986
Iteration 138/1000 | Loss: 0.00001986
Iteration 139/1000 | Loss: 0.00001985
Iteration 140/1000 | Loss: 0.00001985
Iteration 141/1000 | Loss: 0.00001985
Iteration 142/1000 | Loss: 0.00001985
Iteration 143/1000 | Loss: 0.00001985
Iteration 144/1000 | Loss: 0.00001985
Iteration 145/1000 | Loss: 0.00001985
Iteration 146/1000 | Loss: 0.00001985
Iteration 147/1000 | Loss: 0.00001985
Iteration 148/1000 | Loss: 0.00001985
Iteration 149/1000 | Loss: 0.00001985
Iteration 150/1000 | Loss: 0.00001984
Iteration 151/1000 | Loss: 0.00001984
Iteration 152/1000 | Loss: 0.00001984
Iteration 153/1000 | Loss: 0.00001984
Iteration 154/1000 | Loss: 0.00001984
Iteration 155/1000 | Loss: 0.00001984
Iteration 156/1000 | Loss: 0.00001984
Iteration 157/1000 | Loss: 0.00001984
Iteration 158/1000 | Loss: 0.00001984
Iteration 159/1000 | Loss: 0.00001984
Iteration 160/1000 | Loss: 0.00001984
Iteration 161/1000 | Loss: 0.00001983
Iteration 162/1000 | Loss: 0.00001983
Iteration 163/1000 | Loss: 0.00001983
Iteration 164/1000 | Loss: 0.00001983
Iteration 165/1000 | Loss: 0.00001983
Iteration 166/1000 | Loss: 0.00001983
Iteration 167/1000 | Loss: 0.00001983
Iteration 168/1000 | Loss: 0.00001983
Iteration 169/1000 | Loss: 0.00001983
Iteration 170/1000 | Loss: 0.00001983
Iteration 171/1000 | Loss: 0.00001983
Iteration 172/1000 | Loss: 0.00001983
Iteration 173/1000 | Loss: 0.00001983
Iteration 174/1000 | Loss: 0.00001983
Iteration 175/1000 | Loss: 0.00001983
Iteration 176/1000 | Loss: 0.00001983
Iteration 177/1000 | Loss: 0.00001983
Iteration 178/1000 | Loss: 0.00001983
Iteration 179/1000 | Loss: 0.00001983
Iteration 180/1000 | Loss: 0.00001982
Iteration 181/1000 | Loss: 0.00001982
Iteration 182/1000 | Loss: 0.00001982
Iteration 183/1000 | Loss: 0.00001982
Iteration 184/1000 | Loss: 0.00001982
Iteration 185/1000 | Loss: 0.00001982
Iteration 186/1000 | Loss: 0.00001982
Iteration 187/1000 | Loss: 0.00001982
Iteration 188/1000 | Loss: 0.00001982
Iteration 189/1000 | Loss: 0.00001982
Iteration 190/1000 | Loss: 0.00001982
Iteration 191/1000 | Loss: 0.00001982
Iteration 192/1000 | Loss: 0.00001982
Iteration 193/1000 | Loss: 0.00001982
Iteration 194/1000 | Loss: 0.00001982
Iteration 195/1000 | Loss: 0.00001982
Iteration 196/1000 | Loss: 0.00001982
Iteration 197/1000 | Loss: 0.00001982
Iteration 198/1000 | Loss: 0.00001981
Iteration 199/1000 | Loss: 0.00001981
Iteration 200/1000 | Loss: 0.00001981
Iteration 201/1000 | Loss: 0.00001981
Iteration 202/1000 | Loss: 0.00001981
Iteration 203/1000 | Loss: 0.00001981
Iteration 204/1000 | Loss: 0.00001981
Iteration 205/1000 | Loss: 0.00001981
Iteration 206/1000 | Loss: 0.00001981
Iteration 207/1000 | Loss: 0.00001981
Iteration 208/1000 | Loss: 0.00001981
Iteration 209/1000 | Loss: 0.00001981
Iteration 210/1000 | Loss: 0.00001981
Iteration 211/1000 | Loss: 0.00001981
Iteration 212/1000 | Loss: 0.00001981
Iteration 213/1000 | Loss: 0.00001981
Iteration 214/1000 | Loss: 0.00001981
Iteration 215/1000 | Loss: 0.00001981
Iteration 216/1000 | Loss: 0.00001981
Iteration 217/1000 | Loss: 0.00001981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.9813616745523177e-05, 1.9813616745523177e-05, 1.9813616745523177e-05, 1.9813616745523177e-05, 1.9813616745523177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9813616745523177e-05

Optimization complete. Final v2v error: 3.7469122409820557 mm

Highest mean error: 4.423730850219727 mm for frame 107

Lowest mean error: 3.3903403282165527 mm for frame 144

Saving results

Total time: 53.08466124534607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007219
Iteration 2/25 | Loss: 0.00155136
Iteration 3/25 | Loss: 0.00130872
Iteration 4/25 | Loss: 0.00127445
Iteration 5/25 | Loss: 0.00127078
Iteration 6/25 | Loss: 0.00127078
Iteration 7/25 | Loss: 0.00127078
Iteration 8/25 | Loss: 0.00127078
Iteration 9/25 | Loss: 0.00127078
Iteration 10/25 | Loss: 0.00127078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012707775458693504, 0.0012707775458693504, 0.0012707775458693504, 0.0012707775458693504, 0.0012707775458693504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012707775458693504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56106269
Iteration 2/25 | Loss: 0.00086095
Iteration 3/25 | Loss: 0.00086095
Iteration 4/25 | Loss: 0.00086095
Iteration 5/25 | Loss: 0.00086095
Iteration 6/25 | Loss: 0.00086095
Iteration 7/25 | Loss: 0.00086095
Iteration 8/25 | Loss: 0.00086095
Iteration 9/25 | Loss: 0.00086095
Iteration 10/25 | Loss: 0.00086095
Iteration 11/25 | Loss: 0.00086095
Iteration 12/25 | Loss: 0.00086095
Iteration 13/25 | Loss: 0.00086095
Iteration 14/25 | Loss: 0.00086095
Iteration 15/25 | Loss: 0.00086095
Iteration 16/25 | Loss: 0.00086095
Iteration 17/25 | Loss: 0.00086095
Iteration 18/25 | Loss: 0.00086095
Iteration 19/25 | Loss: 0.00086095
Iteration 20/25 | Loss: 0.00086095
Iteration 21/25 | Loss: 0.00086095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000860951142385602, 0.000860951142385602, 0.000860951142385602, 0.000860951142385602, 0.000860951142385602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000860951142385602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086095
Iteration 2/1000 | Loss: 0.00003651
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002177
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002014
Iteration 8/1000 | Loss: 0.00001973
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001898
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001836
Iteration 15/1000 | Loss: 0.00001836
Iteration 16/1000 | Loss: 0.00001836
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001832
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001831
Iteration 22/1000 | Loss: 0.00001826
Iteration 23/1000 | Loss: 0.00001825
Iteration 24/1000 | Loss: 0.00001823
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001818
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001818
Iteration 31/1000 | Loss: 0.00001818
Iteration 32/1000 | Loss: 0.00001814
Iteration 33/1000 | Loss: 0.00001814
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001812
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001811
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001809
Iteration 50/1000 | Loss: 0.00001809
Iteration 51/1000 | Loss: 0.00001809
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001807
Iteration 67/1000 | Loss: 0.00001807
Iteration 68/1000 | Loss: 0.00001807
Iteration 69/1000 | Loss: 0.00001807
Iteration 70/1000 | Loss: 0.00001807
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.8073205865221098e-05, 1.8073205865221098e-05, 1.8073205865221098e-05, 1.8073205865221098e-05, 1.8073205865221098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8073205865221098e-05

Optimization complete. Final v2v error: 3.63429594039917 mm

Highest mean error: 4.13103723526001 mm for frame 170

Lowest mean error: 3.182450771331787 mm for frame 200

Saving results

Total time: 34.100051403045654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451173
Iteration 2/25 | Loss: 0.00134239
Iteration 3/25 | Loss: 0.00126926
Iteration 4/25 | Loss: 0.00125764
Iteration 5/25 | Loss: 0.00125309
Iteration 6/25 | Loss: 0.00125238
Iteration 7/25 | Loss: 0.00125238
Iteration 8/25 | Loss: 0.00125238
Iteration 9/25 | Loss: 0.00125238
Iteration 10/25 | Loss: 0.00125238
Iteration 11/25 | Loss: 0.00125238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001252382411621511, 0.001252382411621511, 0.001252382411621511, 0.001252382411621511, 0.001252382411621511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001252382411621511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53928268
Iteration 2/25 | Loss: 0.00083604
Iteration 3/25 | Loss: 0.00083603
Iteration 4/25 | Loss: 0.00083603
Iteration 5/25 | Loss: 0.00083603
Iteration 6/25 | Loss: 0.00083602
Iteration 7/25 | Loss: 0.00083602
Iteration 8/25 | Loss: 0.00083602
Iteration 9/25 | Loss: 0.00083602
Iteration 10/25 | Loss: 0.00083602
Iteration 11/25 | Loss: 0.00083602
Iteration 12/25 | Loss: 0.00083602
Iteration 13/25 | Loss: 0.00083602
Iteration 14/25 | Loss: 0.00083602
Iteration 15/25 | Loss: 0.00083602
Iteration 16/25 | Loss: 0.00083602
Iteration 17/25 | Loss: 0.00083602
Iteration 18/25 | Loss: 0.00083602
Iteration 19/25 | Loss: 0.00083602
Iteration 20/25 | Loss: 0.00083602
Iteration 21/25 | Loss: 0.00083602
Iteration 22/25 | Loss: 0.00083602
Iteration 23/25 | Loss: 0.00083602
Iteration 24/25 | Loss: 0.00083602
Iteration 25/25 | Loss: 0.00083602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083602
Iteration 2/1000 | Loss: 0.00003060
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001654
Iteration 8/1000 | Loss: 0.00001651
Iteration 9/1000 | Loss: 0.00001648
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001584
Iteration 14/1000 | Loss: 0.00001579
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001556
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001551
Iteration 31/1000 | Loss: 0.00001551
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001551
Iteration 34/1000 | Loss: 0.00001550
Iteration 35/1000 | Loss: 0.00001550
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001549
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001545
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001538
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001535
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001525
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001516
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001516
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001515
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001514
Iteration 87/1000 | Loss: 0.00001514
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001511
Iteration 106/1000 | Loss: 0.00001511
Iteration 107/1000 | Loss: 0.00001511
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00001511
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001510
Iteration 113/1000 | Loss: 0.00001510
Iteration 114/1000 | Loss: 0.00001510
Iteration 115/1000 | Loss: 0.00001510
Iteration 116/1000 | Loss: 0.00001510
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001510
Iteration 120/1000 | Loss: 0.00001510
Iteration 121/1000 | Loss: 0.00001510
Iteration 122/1000 | Loss: 0.00001510
Iteration 123/1000 | Loss: 0.00001510
Iteration 124/1000 | Loss: 0.00001510
Iteration 125/1000 | Loss: 0.00001510
Iteration 126/1000 | Loss: 0.00001510
Iteration 127/1000 | Loss: 0.00001510
Iteration 128/1000 | Loss: 0.00001510
Iteration 129/1000 | Loss: 0.00001510
Iteration 130/1000 | Loss: 0.00001510
Iteration 131/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.5098057701834477e-05, 1.5098057701834477e-05, 1.5098057701834477e-05, 1.5098057701834477e-05, 1.5098057701834477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5098057701834477e-05

Optimization complete. Final v2v error: 3.28696608543396 mm

Highest mean error: 3.5521645545959473 mm for frame 174

Lowest mean error: 2.97731351852417 mm for frame 97

Saving results

Total time: 40.53835129737854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426916
Iteration 2/25 | Loss: 0.00129735
Iteration 3/25 | Loss: 0.00122838
Iteration 4/25 | Loss: 0.00121121
Iteration 5/25 | Loss: 0.00120592
Iteration 6/25 | Loss: 0.00120488
Iteration 7/25 | Loss: 0.00120477
Iteration 8/25 | Loss: 0.00120477
Iteration 9/25 | Loss: 0.00120477
Iteration 10/25 | Loss: 0.00120477
Iteration 11/25 | Loss: 0.00120477
Iteration 12/25 | Loss: 0.00120477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012047655181959271, 0.0012047655181959271, 0.0012047655181959271, 0.0012047655181959271, 0.0012047655181959271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012047655181959271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60856688
Iteration 2/25 | Loss: 0.00075299
Iteration 3/25 | Loss: 0.00075298
Iteration 4/25 | Loss: 0.00075298
Iteration 5/25 | Loss: 0.00075298
Iteration 6/25 | Loss: 0.00075298
Iteration 7/25 | Loss: 0.00075298
Iteration 8/25 | Loss: 0.00075298
Iteration 9/25 | Loss: 0.00075298
Iteration 10/25 | Loss: 0.00075298
Iteration 11/25 | Loss: 0.00075298
Iteration 12/25 | Loss: 0.00075298
Iteration 13/25 | Loss: 0.00075298
Iteration 14/25 | Loss: 0.00075298
Iteration 15/25 | Loss: 0.00075298
Iteration 16/25 | Loss: 0.00075298
Iteration 17/25 | Loss: 0.00075298
Iteration 18/25 | Loss: 0.00075298
Iteration 19/25 | Loss: 0.00075298
Iteration 20/25 | Loss: 0.00075298
Iteration 21/25 | Loss: 0.00075298
Iteration 22/25 | Loss: 0.00075298
Iteration 23/25 | Loss: 0.00075298
Iteration 24/25 | Loss: 0.00075298
Iteration 25/25 | Loss: 0.00075298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075298
Iteration 2/1000 | Loss: 0.00003018
Iteration 3/1000 | Loss: 0.00002065
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001680
Iteration 6/1000 | Loss: 0.00001623
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001451
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001450
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001442
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001434
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001432
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001418
Iteration 85/1000 | Loss: 0.00001418
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001414
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001413
Iteration 97/1000 | Loss: 0.00001413
Iteration 98/1000 | Loss: 0.00001413
Iteration 99/1000 | Loss: 0.00001413
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001412
Iteration 102/1000 | Loss: 0.00001412
Iteration 103/1000 | Loss: 0.00001412
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001408
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001408
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001408
Iteration 117/1000 | Loss: 0.00001408
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001406
Iteration 127/1000 | Loss: 0.00001406
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001405
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001405
Iteration 133/1000 | Loss: 0.00001404
Iteration 134/1000 | Loss: 0.00001404
Iteration 135/1000 | Loss: 0.00001404
Iteration 136/1000 | Loss: 0.00001404
Iteration 137/1000 | Loss: 0.00001404
Iteration 138/1000 | Loss: 0.00001403
Iteration 139/1000 | Loss: 0.00001403
Iteration 140/1000 | Loss: 0.00001403
Iteration 141/1000 | Loss: 0.00001403
Iteration 142/1000 | Loss: 0.00001403
Iteration 143/1000 | Loss: 0.00001403
Iteration 144/1000 | Loss: 0.00001403
Iteration 145/1000 | Loss: 0.00001402
Iteration 146/1000 | Loss: 0.00001402
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001402
Iteration 149/1000 | Loss: 0.00001402
Iteration 150/1000 | Loss: 0.00001402
Iteration 151/1000 | Loss: 0.00001402
Iteration 152/1000 | Loss: 0.00001402
Iteration 153/1000 | Loss: 0.00001402
Iteration 154/1000 | Loss: 0.00001401
Iteration 155/1000 | Loss: 0.00001401
Iteration 156/1000 | Loss: 0.00001401
Iteration 157/1000 | Loss: 0.00001401
Iteration 158/1000 | Loss: 0.00001401
Iteration 159/1000 | Loss: 0.00001401
Iteration 160/1000 | Loss: 0.00001401
Iteration 161/1000 | Loss: 0.00001401
Iteration 162/1000 | Loss: 0.00001401
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001399
Iteration 168/1000 | Loss: 0.00001399
Iteration 169/1000 | Loss: 0.00001399
Iteration 170/1000 | Loss: 0.00001399
Iteration 171/1000 | Loss: 0.00001399
Iteration 172/1000 | Loss: 0.00001399
Iteration 173/1000 | Loss: 0.00001399
Iteration 174/1000 | Loss: 0.00001399
Iteration 175/1000 | Loss: 0.00001399
Iteration 176/1000 | Loss: 0.00001399
Iteration 177/1000 | Loss: 0.00001399
Iteration 178/1000 | Loss: 0.00001398
Iteration 179/1000 | Loss: 0.00001398
Iteration 180/1000 | Loss: 0.00001398
Iteration 181/1000 | Loss: 0.00001398
Iteration 182/1000 | Loss: 0.00001398
Iteration 183/1000 | Loss: 0.00001398
Iteration 184/1000 | Loss: 0.00001398
Iteration 185/1000 | Loss: 0.00001398
Iteration 186/1000 | Loss: 0.00001398
Iteration 187/1000 | Loss: 0.00001398
Iteration 188/1000 | Loss: 0.00001398
Iteration 189/1000 | Loss: 0.00001398
Iteration 190/1000 | Loss: 0.00001398
Iteration 191/1000 | Loss: 0.00001398
Iteration 192/1000 | Loss: 0.00001398
Iteration 193/1000 | Loss: 0.00001398
Iteration 194/1000 | Loss: 0.00001398
Iteration 195/1000 | Loss: 0.00001398
Iteration 196/1000 | Loss: 0.00001398
Iteration 197/1000 | Loss: 0.00001398
Iteration 198/1000 | Loss: 0.00001398
Iteration 199/1000 | Loss: 0.00001398
Iteration 200/1000 | Loss: 0.00001397
Iteration 201/1000 | Loss: 0.00001397
Iteration 202/1000 | Loss: 0.00001397
Iteration 203/1000 | Loss: 0.00001397
Iteration 204/1000 | Loss: 0.00001397
Iteration 205/1000 | Loss: 0.00001397
Iteration 206/1000 | Loss: 0.00001397
Iteration 207/1000 | Loss: 0.00001397
Iteration 208/1000 | Loss: 0.00001397
Iteration 209/1000 | Loss: 0.00001397
Iteration 210/1000 | Loss: 0.00001397
Iteration 211/1000 | Loss: 0.00001397
Iteration 212/1000 | Loss: 0.00001397
Iteration 213/1000 | Loss: 0.00001397
Iteration 214/1000 | Loss: 0.00001397
Iteration 215/1000 | Loss: 0.00001397
Iteration 216/1000 | Loss: 0.00001397
Iteration 217/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.397117375745438e-05, 1.397117375745438e-05, 1.397117375745438e-05, 1.397117375745438e-05, 1.397117375745438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.397117375745438e-05

Optimization complete. Final v2v error: 3.203735589981079 mm

Highest mean error: 3.6106202602386475 mm for frame 90

Lowest mean error: 3.0692262649536133 mm for frame 123

Saving results

Total time: 43.637269258499146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021571
Iteration 2/25 | Loss: 0.00168644
Iteration 3/25 | Loss: 0.00141019
Iteration 4/25 | Loss: 0.00138553
Iteration 5/25 | Loss: 0.00137693
Iteration 6/25 | Loss: 0.00137520
Iteration 7/25 | Loss: 0.00137520
Iteration 8/25 | Loss: 0.00137520
Iteration 9/25 | Loss: 0.00137520
Iteration 10/25 | Loss: 0.00137520
Iteration 11/25 | Loss: 0.00137520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013752044178545475, 0.0013752044178545475, 0.0013752044178545475, 0.0013752044178545475, 0.0013752044178545475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013752044178545475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94202131
Iteration 2/25 | Loss: 0.00101119
Iteration 3/25 | Loss: 0.00101117
Iteration 4/25 | Loss: 0.00101117
Iteration 5/25 | Loss: 0.00101117
Iteration 6/25 | Loss: 0.00101117
Iteration 7/25 | Loss: 0.00101117
Iteration 8/25 | Loss: 0.00101117
Iteration 9/25 | Loss: 0.00101117
Iteration 10/25 | Loss: 0.00101117
Iteration 11/25 | Loss: 0.00101117
Iteration 12/25 | Loss: 0.00101117
Iteration 13/25 | Loss: 0.00101117
Iteration 14/25 | Loss: 0.00101117
Iteration 15/25 | Loss: 0.00101117
Iteration 16/25 | Loss: 0.00101117
Iteration 17/25 | Loss: 0.00101117
Iteration 18/25 | Loss: 0.00101117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010111673036590219, 0.0010111673036590219, 0.0010111673036590219, 0.0010111673036590219, 0.0010111673036590219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010111673036590219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101117
Iteration 2/1000 | Loss: 0.00008179
Iteration 3/1000 | Loss: 0.00004286
Iteration 4/1000 | Loss: 0.00003420
Iteration 5/1000 | Loss: 0.00003212
Iteration 6/1000 | Loss: 0.00003093
Iteration 7/1000 | Loss: 0.00002987
Iteration 8/1000 | Loss: 0.00002912
Iteration 9/1000 | Loss: 0.00002859
Iteration 10/1000 | Loss: 0.00002816
Iteration 11/1000 | Loss: 0.00002778
Iteration 12/1000 | Loss: 0.00002753
Iteration 13/1000 | Loss: 0.00002730
Iteration 14/1000 | Loss: 0.00002710
Iteration 15/1000 | Loss: 0.00002693
Iteration 16/1000 | Loss: 0.00002692
Iteration 17/1000 | Loss: 0.00002686
Iteration 18/1000 | Loss: 0.00002682
Iteration 19/1000 | Loss: 0.00002682
Iteration 20/1000 | Loss: 0.00002672
Iteration 21/1000 | Loss: 0.00002664
Iteration 22/1000 | Loss: 0.00002664
Iteration 23/1000 | Loss: 0.00002663
Iteration 24/1000 | Loss: 0.00002663
Iteration 25/1000 | Loss: 0.00002662
Iteration 26/1000 | Loss: 0.00002661
Iteration 27/1000 | Loss: 0.00002661
Iteration 28/1000 | Loss: 0.00002660
Iteration 29/1000 | Loss: 0.00002660
Iteration 30/1000 | Loss: 0.00002659
Iteration 31/1000 | Loss: 0.00002659
Iteration 32/1000 | Loss: 0.00002659
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002654
Iteration 35/1000 | Loss: 0.00002652
Iteration 36/1000 | Loss: 0.00002651
Iteration 37/1000 | Loss: 0.00002650
Iteration 38/1000 | Loss: 0.00002646
Iteration 39/1000 | Loss: 0.00002640
Iteration 40/1000 | Loss: 0.00002636
Iteration 41/1000 | Loss: 0.00002636
Iteration 42/1000 | Loss: 0.00002636
Iteration 43/1000 | Loss: 0.00002636
Iteration 44/1000 | Loss: 0.00002635
Iteration 45/1000 | Loss: 0.00002635
Iteration 46/1000 | Loss: 0.00002635
Iteration 47/1000 | Loss: 0.00002635
Iteration 48/1000 | Loss: 0.00002635
Iteration 49/1000 | Loss: 0.00002635
Iteration 50/1000 | Loss: 0.00002635
Iteration 51/1000 | Loss: 0.00002635
Iteration 52/1000 | Loss: 0.00002635
Iteration 53/1000 | Loss: 0.00002634
Iteration 54/1000 | Loss: 0.00002634
Iteration 55/1000 | Loss: 0.00002633
Iteration 56/1000 | Loss: 0.00002633
Iteration 57/1000 | Loss: 0.00002633
Iteration 58/1000 | Loss: 0.00002633
Iteration 59/1000 | Loss: 0.00002632
Iteration 60/1000 | Loss: 0.00002632
Iteration 61/1000 | Loss: 0.00002632
Iteration 62/1000 | Loss: 0.00002632
Iteration 63/1000 | Loss: 0.00002632
Iteration 64/1000 | Loss: 0.00002632
Iteration 65/1000 | Loss: 0.00002632
Iteration 66/1000 | Loss: 0.00002632
Iteration 67/1000 | Loss: 0.00002632
Iteration 68/1000 | Loss: 0.00002632
Iteration 69/1000 | Loss: 0.00002631
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002629
Iteration 74/1000 | Loss: 0.00002629
Iteration 75/1000 | Loss: 0.00002629
Iteration 76/1000 | Loss: 0.00002628
Iteration 77/1000 | Loss: 0.00002626
Iteration 78/1000 | Loss: 0.00002626
Iteration 79/1000 | Loss: 0.00002626
Iteration 80/1000 | Loss: 0.00002626
Iteration 81/1000 | Loss: 0.00002626
Iteration 82/1000 | Loss: 0.00002625
Iteration 83/1000 | Loss: 0.00002625
Iteration 84/1000 | Loss: 0.00002625
Iteration 85/1000 | Loss: 0.00002625
Iteration 86/1000 | Loss: 0.00002625
Iteration 87/1000 | Loss: 0.00002623
Iteration 88/1000 | Loss: 0.00002623
Iteration 89/1000 | Loss: 0.00002623
Iteration 90/1000 | Loss: 0.00002623
Iteration 91/1000 | Loss: 0.00002623
Iteration 92/1000 | Loss: 0.00002623
Iteration 93/1000 | Loss: 0.00002623
Iteration 94/1000 | Loss: 0.00002623
Iteration 95/1000 | Loss: 0.00002623
Iteration 96/1000 | Loss: 0.00002623
Iteration 97/1000 | Loss: 0.00002623
Iteration 98/1000 | Loss: 0.00002623
Iteration 99/1000 | Loss: 0.00002622
Iteration 100/1000 | Loss: 0.00002621
Iteration 101/1000 | Loss: 0.00002621
Iteration 102/1000 | Loss: 0.00002621
Iteration 103/1000 | Loss: 0.00002621
Iteration 104/1000 | Loss: 0.00002620
Iteration 105/1000 | Loss: 0.00002620
Iteration 106/1000 | Loss: 0.00002620
Iteration 107/1000 | Loss: 0.00002620
Iteration 108/1000 | Loss: 0.00002620
Iteration 109/1000 | Loss: 0.00002619
Iteration 110/1000 | Loss: 0.00002619
Iteration 111/1000 | Loss: 0.00002619
Iteration 112/1000 | Loss: 0.00002619
Iteration 113/1000 | Loss: 0.00002619
Iteration 114/1000 | Loss: 0.00002619
Iteration 115/1000 | Loss: 0.00002619
Iteration 116/1000 | Loss: 0.00002619
Iteration 117/1000 | Loss: 0.00002619
Iteration 118/1000 | Loss: 0.00002618
Iteration 119/1000 | Loss: 0.00002618
Iteration 120/1000 | Loss: 0.00002618
Iteration 121/1000 | Loss: 0.00002618
Iteration 122/1000 | Loss: 0.00002618
Iteration 123/1000 | Loss: 0.00002618
Iteration 124/1000 | Loss: 0.00002618
Iteration 125/1000 | Loss: 0.00002617
Iteration 126/1000 | Loss: 0.00002617
Iteration 127/1000 | Loss: 0.00002617
Iteration 128/1000 | Loss: 0.00002617
Iteration 129/1000 | Loss: 0.00002617
Iteration 130/1000 | Loss: 0.00002617
Iteration 131/1000 | Loss: 0.00002617
Iteration 132/1000 | Loss: 0.00002617
Iteration 133/1000 | Loss: 0.00002616
Iteration 134/1000 | Loss: 0.00002616
Iteration 135/1000 | Loss: 0.00002616
Iteration 136/1000 | Loss: 0.00002616
Iteration 137/1000 | Loss: 0.00002616
Iteration 138/1000 | Loss: 0.00002616
Iteration 139/1000 | Loss: 0.00002616
Iteration 140/1000 | Loss: 0.00002616
Iteration 141/1000 | Loss: 0.00002616
Iteration 142/1000 | Loss: 0.00002616
Iteration 143/1000 | Loss: 0.00002616
Iteration 144/1000 | Loss: 0.00002616
Iteration 145/1000 | Loss: 0.00002616
Iteration 146/1000 | Loss: 0.00002616
Iteration 147/1000 | Loss: 0.00002616
Iteration 148/1000 | Loss: 0.00002616
Iteration 149/1000 | Loss: 0.00002616
Iteration 150/1000 | Loss: 0.00002616
Iteration 151/1000 | Loss: 0.00002616
Iteration 152/1000 | Loss: 0.00002616
Iteration 153/1000 | Loss: 0.00002616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.6156953026656993e-05, 2.6156953026656993e-05, 2.6156953026656993e-05, 2.6156953026656993e-05, 2.6156953026656993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6156953026656993e-05

Optimization complete. Final v2v error: 4.270369529724121 mm

Highest mean error: 5.061262607574463 mm for frame 67

Lowest mean error: 3.664011240005493 mm for frame 28

Saving results

Total time: 50.294310092926025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014074
Iteration 2/25 | Loss: 0.00180955
Iteration 3/25 | Loss: 0.00143704
Iteration 4/25 | Loss: 0.00135066
Iteration 5/25 | Loss: 0.00134770
Iteration 6/25 | Loss: 0.00132386
Iteration 7/25 | Loss: 0.00131377
Iteration 8/25 | Loss: 0.00131134
Iteration 9/25 | Loss: 0.00131080
Iteration 10/25 | Loss: 0.00131065
Iteration 11/25 | Loss: 0.00131062
Iteration 12/25 | Loss: 0.00131062
Iteration 13/25 | Loss: 0.00131062
Iteration 14/25 | Loss: 0.00131062
Iteration 15/25 | Loss: 0.00131062
Iteration 16/25 | Loss: 0.00131062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013106176629662514, 0.0013106176629662514, 0.0013106176629662514, 0.0013106176629662514, 0.0013106176629662514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013106176629662514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37714791
Iteration 2/25 | Loss: 0.00086717
Iteration 3/25 | Loss: 0.00086717
Iteration 4/25 | Loss: 0.00086717
Iteration 5/25 | Loss: 0.00086717
Iteration 6/25 | Loss: 0.00086717
Iteration 7/25 | Loss: 0.00086717
Iteration 8/25 | Loss: 0.00086717
Iteration 9/25 | Loss: 0.00086717
Iteration 10/25 | Loss: 0.00086717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008671687101013958, 0.0008671687101013958, 0.0008671687101013958, 0.0008671687101013958, 0.0008671687101013958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008671687101013958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086717
Iteration 2/1000 | Loss: 0.00003643
Iteration 3/1000 | Loss: 0.00002687
Iteration 4/1000 | Loss: 0.00002488
Iteration 5/1000 | Loss: 0.00002421
Iteration 6/1000 | Loss: 0.00002400
Iteration 7/1000 | Loss: 0.00002337
Iteration 8/1000 | Loss: 0.00002315
Iteration 9/1000 | Loss: 0.00002283
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002249
Iteration 12/1000 | Loss: 0.00002226
Iteration 13/1000 | Loss: 0.00002199
Iteration 14/1000 | Loss: 0.00002184
Iteration 15/1000 | Loss: 0.00002177
Iteration 16/1000 | Loss: 0.00002174
Iteration 17/1000 | Loss: 0.00002174
Iteration 18/1000 | Loss: 0.00002167
Iteration 19/1000 | Loss: 0.00002167
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00002163
Iteration 23/1000 | Loss: 0.00002162
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002154
Iteration 26/1000 | Loss: 0.00002153
Iteration 27/1000 | Loss: 0.00002147
Iteration 28/1000 | Loss: 0.00002147
Iteration 29/1000 | Loss: 0.00002146
Iteration 30/1000 | Loss: 0.00002146
Iteration 31/1000 | Loss: 0.00002146
Iteration 32/1000 | Loss: 0.00002146
Iteration 33/1000 | Loss: 0.00002146
Iteration 34/1000 | Loss: 0.00002146
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002141
Iteration 41/1000 | Loss: 0.00002140
Iteration 42/1000 | Loss: 0.00002140
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002136
Iteration 57/1000 | Loss: 0.00002136
Iteration 58/1000 | Loss: 0.00002136
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002135
Iteration 63/1000 | Loss: 0.00002135
Iteration 64/1000 | Loss: 0.00002135
Iteration 65/1000 | Loss: 0.00002135
Iteration 66/1000 | Loss: 0.00002134
Iteration 67/1000 | Loss: 0.00002134
Iteration 68/1000 | Loss: 0.00002134
Iteration 69/1000 | Loss: 0.00002134
Iteration 70/1000 | Loss: 0.00002134
Iteration 71/1000 | Loss: 0.00002134
Iteration 72/1000 | Loss: 0.00002133
Iteration 73/1000 | Loss: 0.00002133
Iteration 74/1000 | Loss: 0.00002133
Iteration 75/1000 | Loss: 0.00002133
Iteration 76/1000 | Loss: 0.00002133
Iteration 77/1000 | Loss: 0.00002133
Iteration 78/1000 | Loss: 0.00002133
Iteration 79/1000 | Loss: 0.00002133
Iteration 80/1000 | Loss: 0.00002132
Iteration 81/1000 | Loss: 0.00002132
Iteration 82/1000 | Loss: 0.00002132
Iteration 83/1000 | Loss: 0.00002132
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002131
Iteration 87/1000 | Loss: 0.00002131
Iteration 88/1000 | Loss: 0.00002131
Iteration 89/1000 | Loss: 0.00002131
Iteration 90/1000 | Loss: 0.00002131
Iteration 91/1000 | Loss: 0.00002131
Iteration 92/1000 | Loss: 0.00002131
Iteration 93/1000 | Loss: 0.00002130
Iteration 94/1000 | Loss: 0.00002130
Iteration 95/1000 | Loss: 0.00002130
Iteration 96/1000 | Loss: 0.00002130
Iteration 97/1000 | Loss: 0.00002130
Iteration 98/1000 | Loss: 0.00002130
Iteration 99/1000 | Loss: 0.00002130
Iteration 100/1000 | Loss: 0.00002130
Iteration 101/1000 | Loss: 0.00002130
Iteration 102/1000 | Loss: 0.00002130
Iteration 103/1000 | Loss: 0.00002130
Iteration 104/1000 | Loss: 0.00002130
Iteration 105/1000 | Loss: 0.00002130
Iteration 106/1000 | Loss: 0.00002130
Iteration 107/1000 | Loss: 0.00002130
Iteration 108/1000 | Loss: 0.00002130
Iteration 109/1000 | Loss: 0.00002130
Iteration 110/1000 | Loss: 0.00002130
Iteration 111/1000 | Loss: 0.00002130
Iteration 112/1000 | Loss: 0.00002130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.12974009627942e-05, 2.12974009627942e-05, 2.12974009627942e-05, 2.12974009627942e-05, 2.12974009627942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.12974009627942e-05

Optimization complete. Final v2v error: 3.8882501125335693 mm

Highest mean error: 4.007964611053467 mm for frame 0

Lowest mean error: 3.7059743404388428 mm for frame 175

Saving results

Total time: 45.65440583229065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394172
Iteration 2/25 | Loss: 0.00130853
Iteration 3/25 | Loss: 0.00123100
Iteration 4/25 | Loss: 0.00120947
Iteration 5/25 | Loss: 0.00120159
Iteration 6/25 | Loss: 0.00120050
Iteration 7/25 | Loss: 0.00120050
Iteration 8/25 | Loss: 0.00120050
Iteration 9/25 | Loss: 0.00120050
Iteration 10/25 | Loss: 0.00120050
Iteration 11/25 | Loss: 0.00120050
Iteration 12/25 | Loss: 0.00120050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012004959862679243, 0.0012004959862679243, 0.0012004959862679243, 0.0012004959862679243, 0.0012004959862679243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012004959862679243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43150890
Iteration 2/25 | Loss: 0.00067714
Iteration 3/25 | Loss: 0.00067714
Iteration 4/25 | Loss: 0.00067714
Iteration 5/25 | Loss: 0.00067714
Iteration 6/25 | Loss: 0.00067714
Iteration 7/25 | Loss: 0.00067714
Iteration 8/25 | Loss: 0.00067714
Iteration 9/25 | Loss: 0.00067714
Iteration 10/25 | Loss: 0.00067714
Iteration 11/25 | Loss: 0.00067714
Iteration 12/25 | Loss: 0.00067714
Iteration 13/25 | Loss: 0.00067714
Iteration 14/25 | Loss: 0.00067714
Iteration 15/25 | Loss: 0.00067714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000677136704325676, 0.000677136704325676, 0.000677136704325676, 0.000677136704325676, 0.000677136704325676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000677136704325676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067714
Iteration 2/1000 | Loss: 0.00004618
Iteration 3/1000 | Loss: 0.00003427
Iteration 4/1000 | Loss: 0.00002916
Iteration 5/1000 | Loss: 0.00002663
Iteration 6/1000 | Loss: 0.00002508
Iteration 7/1000 | Loss: 0.00002411
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002305
Iteration 10/1000 | Loss: 0.00002266
Iteration 11/1000 | Loss: 0.00002237
Iteration 12/1000 | Loss: 0.00002228
Iteration 13/1000 | Loss: 0.00002210
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002189
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002188
Iteration 22/1000 | Loss: 0.00002187
Iteration 23/1000 | Loss: 0.00002187
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002184
Iteration 28/1000 | Loss: 0.00002184
Iteration 29/1000 | Loss: 0.00002184
Iteration 30/1000 | Loss: 0.00002184
Iteration 31/1000 | Loss: 0.00002183
Iteration 32/1000 | Loss: 0.00002183
Iteration 33/1000 | Loss: 0.00002182
Iteration 34/1000 | Loss: 0.00002182
Iteration 35/1000 | Loss: 0.00002182
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002179
Iteration 39/1000 | Loss: 0.00002179
Iteration 40/1000 | Loss: 0.00002179
Iteration 41/1000 | Loss: 0.00002178
Iteration 42/1000 | Loss: 0.00002178
Iteration 43/1000 | Loss: 0.00002176
Iteration 44/1000 | Loss: 0.00002176
Iteration 45/1000 | Loss: 0.00002176
Iteration 46/1000 | Loss: 0.00002176
Iteration 47/1000 | Loss: 0.00002176
Iteration 48/1000 | Loss: 0.00002176
Iteration 49/1000 | Loss: 0.00002176
Iteration 50/1000 | Loss: 0.00002176
Iteration 51/1000 | Loss: 0.00002175
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002173
Iteration 54/1000 | Loss: 0.00002173
Iteration 55/1000 | Loss: 0.00002173
Iteration 56/1000 | Loss: 0.00002172
Iteration 57/1000 | Loss: 0.00002171
Iteration 58/1000 | Loss: 0.00002171
Iteration 59/1000 | Loss: 0.00002171
Iteration 60/1000 | Loss: 0.00002170
Iteration 61/1000 | Loss: 0.00002170
Iteration 62/1000 | Loss: 0.00002169
Iteration 63/1000 | Loss: 0.00002169
Iteration 64/1000 | Loss: 0.00002169
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002166
Iteration 71/1000 | Loss: 0.00002165
Iteration 72/1000 | Loss: 0.00002165
Iteration 73/1000 | Loss: 0.00002165
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002164
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002163
Iteration 87/1000 | Loss: 0.00002163
Iteration 88/1000 | Loss: 0.00002163
Iteration 89/1000 | Loss: 0.00002163
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002162
Iteration 93/1000 | Loss: 0.00002162
Iteration 94/1000 | Loss: 0.00002162
Iteration 95/1000 | Loss: 0.00002162
Iteration 96/1000 | Loss: 0.00002162
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002161
Iteration 99/1000 | Loss: 0.00002161
Iteration 100/1000 | Loss: 0.00002161
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002161
Iteration 103/1000 | Loss: 0.00002160
Iteration 104/1000 | Loss: 0.00002160
Iteration 105/1000 | Loss: 0.00002160
Iteration 106/1000 | Loss: 0.00002160
Iteration 107/1000 | Loss: 0.00002160
Iteration 108/1000 | Loss: 0.00002160
Iteration 109/1000 | Loss: 0.00002160
Iteration 110/1000 | Loss: 0.00002159
Iteration 111/1000 | Loss: 0.00002159
Iteration 112/1000 | Loss: 0.00002159
Iteration 113/1000 | Loss: 0.00002159
Iteration 114/1000 | Loss: 0.00002159
Iteration 115/1000 | Loss: 0.00002159
Iteration 116/1000 | Loss: 0.00002158
Iteration 117/1000 | Loss: 0.00002158
Iteration 118/1000 | Loss: 0.00002158
Iteration 119/1000 | Loss: 0.00002158
Iteration 120/1000 | Loss: 0.00002158
Iteration 121/1000 | Loss: 0.00002158
Iteration 122/1000 | Loss: 0.00002157
Iteration 123/1000 | Loss: 0.00002157
Iteration 124/1000 | Loss: 0.00002157
Iteration 125/1000 | Loss: 0.00002157
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00002156
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00002156
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00002156
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002155
Iteration 136/1000 | Loss: 0.00002155
Iteration 137/1000 | Loss: 0.00002155
Iteration 138/1000 | Loss: 0.00002155
Iteration 139/1000 | Loss: 0.00002155
Iteration 140/1000 | Loss: 0.00002155
Iteration 141/1000 | Loss: 0.00002155
Iteration 142/1000 | Loss: 0.00002154
Iteration 143/1000 | Loss: 0.00002154
Iteration 144/1000 | Loss: 0.00002154
Iteration 145/1000 | Loss: 0.00002154
Iteration 146/1000 | Loss: 0.00002154
Iteration 147/1000 | Loss: 0.00002154
Iteration 148/1000 | Loss: 0.00002154
Iteration 149/1000 | Loss: 0.00002154
Iteration 150/1000 | Loss: 0.00002153
Iteration 151/1000 | Loss: 0.00002153
Iteration 152/1000 | Loss: 0.00002153
Iteration 153/1000 | Loss: 0.00002153
Iteration 154/1000 | Loss: 0.00002153
Iteration 155/1000 | Loss: 0.00002153
Iteration 156/1000 | Loss: 0.00002153
Iteration 157/1000 | Loss: 0.00002153
Iteration 158/1000 | Loss: 0.00002153
Iteration 159/1000 | Loss: 0.00002153
Iteration 160/1000 | Loss: 0.00002153
Iteration 161/1000 | Loss: 0.00002153
Iteration 162/1000 | Loss: 0.00002153
Iteration 163/1000 | Loss: 0.00002153
Iteration 164/1000 | Loss: 0.00002153
Iteration 165/1000 | Loss: 0.00002153
Iteration 166/1000 | Loss: 0.00002152
Iteration 167/1000 | Loss: 0.00002152
Iteration 168/1000 | Loss: 0.00002152
Iteration 169/1000 | Loss: 0.00002152
Iteration 170/1000 | Loss: 0.00002152
Iteration 171/1000 | Loss: 0.00002152
Iteration 172/1000 | Loss: 0.00002152
Iteration 173/1000 | Loss: 0.00002152
Iteration 174/1000 | Loss: 0.00002152
Iteration 175/1000 | Loss: 0.00002152
Iteration 176/1000 | Loss: 0.00002152
Iteration 177/1000 | Loss: 0.00002152
Iteration 178/1000 | Loss: 0.00002152
Iteration 179/1000 | Loss: 0.00002152
Iteration 180/1000 | Loss: 0.00002152
Iteration 181/1000 | Loss: 0.00002152
Iteration 182/1000 | Loss: 0.00002152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.1517631466849707e-05, 2.1517631466849707e-05, 2.1517631466849707e-05, 2.1517631466849707e-05, 2.1517631466849707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1517631466849707e-05

Optimization complete. Final v2v error: 3.791961193084717 mm

Highest mean error: 4.789183139801025 mm for frame 103

Lowest mean error: 2.8537328243255615 mm for frame 0

Saving results

Total time: 46.245420932769775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796642
Iteration 2/25 | Loss: 0.00161948
Iteration 3/25 | Loss: 0.00126832
Iteration 4/25 | Loss: 0.00124379
Iteration 5/25 | Loss: 0.00124144
Iteration 6/25 | Loss: 0.00124144
Iteration 7/25 | Loss: 0.00124144
Iteration 8/25 | Loss: 0.00124144
Iteration 9/25 | Loss: 0.00124144
Iteration 10/25 | Loss: 0.00124144
Iteration 11/25 | Loss: 0.00124144
Iteration 12/25 | Loss: 0.00124144
Iteration 13/25 | Loss: 0.00124144
Iteration 14/25 | Loss: 0.00124144
Iteration 15/25 | Loss: 0.00124144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012414352968335152, 0.0012414352968335152, 0.0012414352968335152, 0.0012414352968335152, 0.0012414352968335152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012414352968335152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46516609
Iteration 2/25 | Loss: 0.00055318
Iteration 3/25 | Loss: 0.00055318
Iteration 4/25 | Loss: 0.00055318
Iteration 5/25 | Loss: 0.00055318
Iteration 6/25 | Loss: 0.00055318
Iteration 7/25 | Loss: 0.00055318
Iteration 8/25 | Loss: 0.00055318
Iteration 9/25 | Loss: 0.00055318
Iteration 10/25 | Loss: 0.00055318
Iteration 11/25 | Loss: 0.00055318
Iteration 12/25 | Loss: 0.00055318
Iteration 13/25 | Loss: 0.00055318
Iteration 14/25 | Loss: 0.00055318
Iteration 15/25 | Loss: 0.00055318
Iteration 16/25 | Loss: 0.00055318
Iteration 17/25 | Loss: 0.00055318
Iteration 18/25 | Loss: 0.00055318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005531780188903213, 0.0005531780188903213, 0.0005531780188903213, 0.0005531780188903213, 0.0005531780188903213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005531780188903213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055318
Iteration 2/1000 | Loss: 0.00004333
Iteration 3/1000 | Loss: 0.00002823
Iteration 4/1000 | Loss: 0.00002555
Iteration 5/1000 | Loss: 0.00002386
Iteration 6/1000 | Loss: 0.00002211
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001941
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001912
Iteration 14/1000 | Loss: 0.00001905
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001888
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001883
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001882
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001881
Iteration 34/1000 | Loss: 0.00001880
Iteration 35/1000 | Loss: 0.00001880
Iteration 36/1000 | Loss: 0.00001880
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001879
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001870
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001869
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001864
Iteration 89/1000 | Loss: 0.00001864
Iteration 90/1000 | Loss: 0.00001864
Iteration 91/1000 | Loss: 0.00001864
Iteration 92/1000 | Loss: 0.00001864
Iteration 93/1000 | Loss: 0.00001864
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001863
Iteration 96/1000 | Loss: 0.00001863
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001862
Iteration 101/1000 | Loss: 0.00001862
Iteration 102/1000 | Loss: 0.00001862
Iteration 103/1000 | Loss: 0.00001862
Iteration 104/1000 | Loss: 0.00001862
Iteration 105/1000 | Loss: 0.00001862
Iteration 106/1000 | Loss: 0.00001862
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001860
Iteration 118/1000 | Loss: 0.00001860
Iteration 119/1000 | Loss: 0.00001860
Iteration 120/1000 | Loss: 0.00001860
Iteration 121/1000 | Loss: 0.00001860
Iteration 122/1000 | Loss: 0.00001860
Iteration 123/1000 | Loss: 0.00001860
Iteration 124/1000 | Loss: 0.00001860
Iteration 125/1000 | Loss: 0.00001860
Iteration 126/1000 | Loss: 0.00001860
Iteration 127/1000 | Loss: 0.00001860
Iteration 128/1000 | Loss: 0.00001860
Iteration 129/1000 | Loss: 0.00001860
Iteration 130/1000 | Loss: 0.00001860
Iteration 131/1000 | Loss: 0.00001860
Iteration 132/1000 | Loss: 0.00001860
Iteration 133/1000 | Loss: 0.00001860
Iteration 134/1000 | Loss: 0.00001860
Iteration 135/1000 | Loss: 0.00001860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.859509211499244e-05, 1.859509211499244e-05, 1.859509211499244e-05, 1.859509211499244e-05, 1.859509211499244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.859509211499244e-05

Optimization complete. Final v2v error: 3.618290901184082 mm

Highest mean error: 4.317498683929443 mm for frame 23

Lowest mean error: 3.364086151123047 mm for frame 0

Saving results

Total time: 42.834075689315796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828768
Iteration 2/25 | Loss: 0.00157081
Iteration 3/25 | Loss: 0.00125359
Iteration 4/25 | Loss: 0.00123344
Iteration 5/25 | Loss: 0.00122980
Iteration 6/25 | Loss: 0.00122956
Iteration 7/25 | Loss: 0.00122956
Iteration 8/25 | Loss: 0.00122956
Iteration 9/25 | Loss: 0.00122956
Iteration 10/25 | Loss: 0.00122956
Iteration 11/25 | Loss: 0.00122956
Iteration 12/25 | Loss: 0.00122956
Iteration 13/25 | Loss: 0.00122956
Iteration 14/25 | Loss: 0.00122956
Iteration 15/25 | Loss: 0.00122956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012295568594709039, 0.0012295568594709039, 0.0012295568594709039, 0.0012295568594709039, 0.0012295568594709039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012295568594709039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43392110
Iteration 2/25 | Loss: 0.00074401
Iteration 3/25 | Loss: 0.00074399
Iteration 4/25 | Loss: 0.00074399
Iteration 5/25 | Loss: 0.00074399
Iteration 6/25 | Loss: 0.00074399
Iteration 7/25 | Loss: 0.00074398
Iteration 8/25 | Loss: 0.00074398
Iteration 9/25 | Loss: 0.00074398
Iteration 10/25 | Loss: 0.00074398
Iteration 11/25 | Loss: 0.00074398
Iteration 12/25 | Loss: 0.00074398
Iteration 13/25 | Loss: 0.00074398
Iteration 14/25 | Loss: 0.00074398
Iteration 15/25 | Loss: 0.00074398
Iteration 16/25 | Loss: 0.00074398
Iteration 17/25 | Loss: 0.00074398
Iteration 18/25 | Loss: 0.00074398
Iteration 19/25 | Loss: 0.00074398
Iteration 20/25 | Loss: 0.00074398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007439839537255466, 0.0007439839537255466, 0.0007439839537255466, 0.0007439839537255466, 0.0007439839537255466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007439839537255466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074398
Iteration 2/1000 | Loss: 0.00002151
Iteration 3/1000 | Loss: 0.00001682
Iteration 4/1000 | Loss: 0.00001566
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001331
Iteration 17/1000 | Loss: 0.00001329
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00001323
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001319
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001317
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001314
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001314
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001313
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001302
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001293
Iteration 84/1000 | Loss: 0.00001293
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001292
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001290
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001290
Iteration 100/1000 | Loss: 0.00001290
Iteration 101/1000 | Loss: 0.00001290
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001289
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001288
Iteration 123/1000 | Loss: 0.00001288
Iteration 124/1000 | Loss: 0.00001288
Iteration 125/1000 | Loss: 0.00001288
Iteration 126/1000 | Loss: 0.00001288
Iteration 127/1000 | Loss: 0.00001288
Iteration 128/1000 | Loss: 0.00001288
Iteration 129/1000 | Loss: 0.00001287
Iteration 130/1000 | Loss: 0.00001287
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001287
Iteration 141/1000 | Loss: 0.00001287
Iteration 142/1000 | Loss: 0.00001287
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001287
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001287
Iteration 163/1000 | Loss: 0.00001287
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2872043953393586e-05, 1.2872043953393586e-05, 1.2872043953393586e-05, 1.2872043953393586e-05, 1.2872043953393586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2872043953393586e-05

Optimization complete. Final v2v error: 3.074537754058838 mm

Highest mean error: 3.3899593353271484 mm for frame 116

Lowest mean error: 2.6774868965148926 mm for frame 36

Saving results

Total time: 41.601113080978394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976146
Iteration 2/25 | Loss: 0.00178049
Iteration 3/25 | Loss: 0.00145595
Iteration 4/25 | Loss: 0.00143400
Iteration 5/25 | Loss: 0.00142693
Iteration 6/25 | Loss: 0.00142582
Iteration 7/25 | Loss: 0.00142582
Iteration 8/25 | Loss: 0.00142582
Iteration 9/25 | Loss: 0.00142582
Iteration 10/25 | Loss: 0.00142582
Iteration 11/25 | Loss: 0.00142582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014258197043091059, 0.0014258197043091059, 0.0014258197043091059, 0.0014258197043091059, 0.0014258197043091059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014258197043091059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54430872
Iteration 2/25 | Loss: 0.00083176
Iteration 3/25 | Loss: 0.00083175
Iteration 4/25 | Loss: 0.00083175
Iteration 5/25 | Loss: 0.00083175
Iteration 6/25 | Loss: 0.00083175
Iteration 7/25 | Loss: 0.00083175
Iteration 8/25 | Loss: 0.00083175
Iteration 9/25 | Loss: 0.00083175
Iteration 10/25 | Loss: 0.00083175
Iteration 11/25 | Loss: 0.00083175
Iteration 12/25 | Loss: 0.00083175
Iteration 13/25 | Loss: 0.00083175
Iteration 14/25 | Loss: 0.00083175
Iteration 15/25 | Loss: 0.00083175
Iteration 16/25 | Loss: 0.00083175
Iteration 17/25 | Loss: 0.00083175
Iteration 18/25 | Loss: 0.00083175
Iteration 19/25 | Loss: 0.00083175
Iteration 20/25 | Loss: 0.00083175
Iteration 21/25 | Loss: 0.00083175
Iteration 22/25 | Loss: 0.00083175
Iteration 23/25 | Loss: 0.00083175
Iteration 24/25 | Loss: 0.00083175
Iteration 25/25 | Loss: 0.00083175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083175
Iteration 2/1000 | Loss: 0.00007227
Iteration 3/1000 | Loss: 0.00005037
Iteration 4/1000 | Loss: 0.00004352
Iteration 5/1000 | Loss: 0.00004144
Iteration 6/1000 | Loss: 0.00004018
Iteration 7/1000 | Loss: 0.00003922
Iteration 8/1000 | Loss: 0.00003830
Iteration 9/1000 | Loss: 0.00003749
Iteration 10/1000 | Loss: 0.00003717
Iteration 11/1000 | Loss: 0.00003659
Iteration 12/1000 | Loss: 0.00003602
Iteration 13/1000 | Loss: 0.00003559
Iteration 14/1000 | Loss: 0.00003518
Iteration 15/1000 | Loss: 0.00003483
Iteration 16/1000 | Loss: 0.00003434
Iteration 17/1000 | Loss: 0.00003404
Iteration 18/1000 | Loss: 0.00003380
Iteration 19/1000 | Loss: 0.00003354
Iteration 20/1000 | Loss: 0.00003336
Iteration 21/1000 | Loss: 0.00003320
Iteration 22/1000 | Loss: 0.00003307
Iteration 23/1000 | Loss: 0.00003298
Iteration 24/1000 | Loss: 0.00003294
Iteration 25/1000 | Loss: 0.00003290
Iteration 26/1000 | Loss: 0.00003287
Iteration 27/1000 | Loss: 0.00003287
Iteration 28/1000 | Loss: 0.00003286
Iteration 29/1000 | Loss: 0.00003286
Iteration 30/1000 | Loss: 0.00003285
Iteration 31/1000 | Loss: 0.00003284
Iteration 32/1000 | Loss: 0.00003279
Iteration 33/1000 | Loss: 0.00003278
Iteration 34/1000 | Loss: 0.00003278
Iteration 35/1000 | Loss: 0.00003277
Iteration 36/1000 | Loss: 0.00003276
Iteration 37/1000 | Loss: 0.00003276
Iteration 38/1000 | Loss: 0.00003275
Iteration 39/1000 | Loss: 0.00003275
Iteration 40/1000 | Loss: 0.00003275
Iteration 41/1000 | Loss: 0.00003275
Iteration 42/1000 | Loss: 0.00003274
Iteration 43/1000 | Loss: 0.00003274
Iteration 44/1000 | Loss: 0.00003274
Iteration 45/1000 | Loss: 0.00003273
Iteration 46/1000 | Loss: 0.00003273
Iteration 47/1000 | Loss: 0.00003273
Iteration 48/1000 | Loss: 0.00003273
Iteration 49/1000 | Loss: 0.00003273
Iteration 50/1000 | Loss: 0.00003273
Iteration 51/1000 | Loss: 0.00003273
Iteration 52/1000 | Loss: 0.00003273
Iteration 53/1000 | Loss: 0.00003272
Iteration 54/1000 | Loss: 0.00003272
Iteration 55/1000 | Loss: 0.00003272
Iteration 56/1000 | Loss: 0.00003270
Iteration 57/1000 | Loss: 0.00003270
Iteration 58/1000 | Loss: 0.00003270
Iteration 59/1000 | Loss: 0.00003270
Iteration 60/1000 | Loss: 0.00003270
Iteration 61/1000 | Loss: 0.00003270
Iteration 62/1000 | Loss: 0.00003270
Iteration 63/1000 | Loss: 0.00003270
Iteration 64/1000 | Loss: 0.00003270
Iteration 65/1000 | Loss: 0.00003270
Iteration 66/1000 | Loss: 0.00003270
Iteration 67/1000 | Loss: 0.00003270
Iteration 68/1000 | Loss: 0.00003269
Iteration 69/1000 | Loss: 0.00003269
Iteration 70/1000 | Loss: 0.00003269
Iteration 71/1000 | Loss: 0.00003269
Iteration 72/1000 | Loss: 0.00003268
Iteration 73/1000 | Loss: 0.00003268
Iteration 74/1000 | Loss: 0.00003267
Iteration 75/1000 | Loss: 0.00003267
Iteration 76/1000 | Loss: 0.00003266
Iteration 77/1000 | Loss: 0.00003266
Iteration 78/1000 | Loss: 0.00003266
Iteration 79/1000 | Loss: 0.00003266
Iteration 80/1000 | Loss: 0.00003266
Iteration 81/1000 | Loss: 0.00003266
Iteration 82/1000 | Loss: 0.00003266
Iteration 83/1000 | Loss: 0.00003266
Iteration 84/1000 | Loss: 0.00003266
Iteration 85/1000 | Loss: 0.00003266
Iteration 86/1000 | Loss: 0.00003266
Iteration 87/1000 | Loss: 0.00003266
Iteration 88/1000 | Loss: 0.00003266
Iteration 89/1000 | Loss: 0.00003264
Iteration 90/1000 | Loss: 0.00003264
Iteration 91/1000 | Loss: 0.00003264
Iteration 92/1000 | Loss: 0.00003264
Iteration 93/1000 | Loss: 0.00003263
Iteration 94/1000 | Loss: 0.00003263
Iteration 95/1000 | Loss: 0.00003263
Iteration 96/1000 | Loss: 0.00003262
Iteration 97/1000 | Loss: 0.00003262
Iteration 98/1000 | Loss: 0.00003262
Iteration 99/1000 | Loss: 0.00003261
Iteration 100/1000 | Loss: 0.00003261
Iteration 101/1000 | Loss: 0.00003261
Iteration 102/1000 | Loss: 0.00003260
Iteration 103/1000 | Loss: 0.00003260
Iteration 104/1000 | Loss: 0.00003260
Iteration 105/1000 | Loss: 0.00003260
Iteration 106/1000 | Loss: 0.00003260
Iteration 107/1000 | Loss: 0.00003260
Iteration 108/1000 | Loss: 0.00003260
Iteration 109/1000 | Loss: 0.00003260
Iteration 110/1000 | Loss: 0.00003259
Iteration 111/1000 | Loss: 0.00003259
Iteration 112/1000 | Loss: 0.00003259
Iteration 113/1000 | Loss: 0.00003259
Iteration 114/1000 | Loss: 0.00003259
Iteration 115/1000 | Loss: 0.00003259
Iteration 116/1000 | Loss: 0.00003259
Iteration 117/1000 | Loss: 0.00003259
Iteration 118/1000 | Loss: 0.00003259
Iteration 119/1000 | Loss: 0.00003259
Iteration 120/1000 | Loss: 0.00003259
Iteration 121/1000 | Loss: 0.00003259
Iteration 122/1000 | Loss: 0.00003259
Iteration 123/1000 | Loss: 0.00003259
Iteration 124/1000 | Loss: 0.00003259
Iteration 125/1000 | Loss: 0.00003259
Iteration 126/1000 | Loss: 0.00003259
Iteration 127/1000 | Loss: 0.00003259
Iteration 128/1000 | Loss: 0.00003259
Iteration 129/1000 | Loss: 0.00003259
Iteration 130/1000 | Loss: 0.00003259
Iteration 131/1000 | Loss: 0.00003258
Iteration 132/1000 | Loss: 0.00003258
Iteration 133/1000 | Loss: 0.00003258
Iteration 134/1000 | Loss: 0.00003258
Iteration 135/1000 | Loss: 0.00003258
Iteration 136/1000 | Loss: 0.00003258
Iteration 137/1000 | Loss: 0.00003258
Iteration 138/1000 | Loss: 0.00003258
Iteration 139/1000 | Loss: 0.00003258
Iteration 140/1000 | Loss: 0.00003258
Iteration 141/1000 | Loss: 0.00003258
Iteration 142/1000 | Loss: 0.00003258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.258401920902543e-05, 3.258401920902543e-05, 3.258401920902543e-05, 3.258401920902543e-05, 3.258401920902543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.258401920902543e-05

Optimization complete. Final v2v error: 4.844043731689453 mm

Highest mean error: 5.056306838989258 mm for frame 72

Lowest mean error: 4.325003147125244 mm for frame 40

Saving results

Total time: 52.89047837257385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532059
Iteration 2/25 | Loss: 0.00133976
Iteration 3/25 | Loss: 0.00127395
Iteration 4/25 | Loss: 0.00126635
Iteration 5/25 | Loss: 0.00126396
Iteration 6/25 | Loss: 0.00126396
Iteration 7/25 | Loss: 0.00126396
Iteration 8/25 | Loss: 0.00126396
Iteration 9/25 | Loss: 0.00126396
Iteration 10/25 | Loss: 0.00126396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012639580527320504, 0.0012639580527320504, 0.0012639580527320504, 0.0012639580527320504, 0.0012639580527320504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012639580527320504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.34985161
Iteration 2/25 | Loss: 0.00077123
Iteration 3/25 | Loss: 0.00077122
Iteration 4/25 | Loss: 0.00077122
Iteration 5/25 | Loss: 0.00077121
Iteration 6/25 | Loss: 0.00077121
Iteration 7/25 | Loss: 0.00077121
Iteration 8/25 | Loss: 0.00077121
Iteration 9/25 | Loss: 0.00077121
Iteration 10/25 | Loss: 0.00077121
Iteration 11/25 | Loss: 0.00077121
Iteration 12/25 | Loss: 0.00077121
Iteration 13/25 | Loss: 0.00077121
Iteration 14/25 | Loss: 0.00077121
Iteration 15/25 | Loss: 0.00077121
Iteration 16/25 | Loss: 0.00077121
Iteration 17/25 | Loss: 0.00077121
Iteration 18/25 | Loss: 0.00077121
Iteration 19/25 | Loss: 0.00077121
Iteration 20/25 | Loss: 0.00077121
Iteration 21/25 | Loss: 0.00077121
Iteration 22/25 | Loss: 0.00077121
Iteration 23/25 | Loss: 0.00077121
Iteration 24/25 | Loss: 0.00077121
Iteration 25/25 | Loss: 0.00077121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077121
Iteration 2/1000 | Loss: 0.00002642
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001728
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001542
Iteration 12/1000 | Loss: 0.00001533
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001530
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001507
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00001503
Iteration 22/1000 | Loss: 0.00001502
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001495
Iteration 30/1000 | Loss: 0.00001492
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001457
Iteration 40/1000 | Loss: 0.00001457
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001454
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001445
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001441
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001439
Iteration 92/1000 | Loss: 0.00001439
Iteration 93/1000 | Loss: 0.00001439
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001439
Iteration 98/1000 | Loss: 0.00001439
Iteration 99/1000 | Loss: 0.00001439
Iteration 100/1000 | Loss: 0.00001439
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.438973777112551e-05, 1.438973777112551e-05, 1.438973777112551e-05, 1.438973777112551e-05, 1.438973777112551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.438973777112551e-05

Optimization complete. Final v2v error: 3.216172218322754 mm

Highest mean error: 3.4429380893707275 mm for frame 42

Lowest mean error: 3.0247066020965576 mm for frame 30

Saving results

Total time: 41.64792442321777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736425
Iteration 2/25 | Loss: 0.00204011
Iteration 3/25 | Loss: 0.00144762
Iteration 4/25 | Loss: 0.00136987
Iteration 5/25 | Loss: 0.00134338
Iteration 6/25 | Loss: 0.00132222
Iteration 7/25 | Loss: 0.00131230
Iteration 8/25 | Loss: 0.00131349
Iteration 9/25 | Loss: 0.00131247
Iteration 10/25 | Loss: 0.00130946
Iteration 11/25 | Loss: 0.00130863
Iteration 12/25 | Loss: 0.00130851
Iteration 13/25 | Loss: 0.00130849
Iteration 14/25 | Loss: 0.00130849
Iteration 15/25 | Loss: 0.00130849
Iteration 16/25 | Loss: 0.00130849
Iteration 17/25 | Loss: 0.00130849
Iteration 18/25 | Loss: 0.00130849
Iteration 19/25 | Loss: 0.00130849
Iteration 20/25 | Loss: 0.00130848
Iteration 21/25 | Loss: 0.00130848
Iteration 22/25 | Loss: 0.00130848
Iteration 23/25 | Loss: 0.00130848
Iteration 24/25 | Loss: 0.00130848
Iteration 25/25 | Loss: 0.00130848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14595222
Iteration 2/25 | Loss: 0.00069815
Iteration 3/25 | Loss: 0.00069810
Iteration 4/25 | Loss: 0.00069810
Iteration 5/25 | Loss: 0.00069810
Iteration 6/25 | Loss: 0.00069810
Iteration 7/25 | Loss: 0.00069810
Iteration 8/25 | Loss: 0.00069810
Iteration 9/25 | Loss: 0.00069810
Iteration 10/25 | Loss: 0.00069809
Iteration 11/25 | Loss: 0.00069809
Iteration 12/25 | Loss: 0.00069809
Iteration 13/25 | Loss: 0.00069809
Iteration 14/25 | Loss: 0.00069809
Iteration 15/25 | Loss: 0.00069809
Iteration 16/25 | Loss: 0.00069809
Iteration 17/25 | Loss: 0.00069809
Iteration 18/25 | Loss: 0.00069809
Iteration 19/25 | Loss: 0.00069809
Iteration 20/25 | Loss: 0.00069809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006980942562222481, 0.0006980942562222481, 0.0006980942562222481, 0.0006980942562222481, 0.0006980942562222481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006980942562222481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069809
Iteration 2/1000 | Loss: 0.00004225
Iteration 3/1000 | Loss: 0.00002715
Iteration 4/1000 | Loss: 0.00002388
Iteration 5/1000 | Loss: 0.00002251
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002085
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002057
Iteration 11/1000 | Loss: 0.00002048
Iteration 12/1000 | Loss: 0.00002047
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00002025
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002024
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00002016
Iteration 27/1000 | Loss: 0.00002016
Iteration 28/1000 | Loss: 0.00002016
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002016
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002009
Iteration 35/1000 | Loss: 0.00002009
Iteration 36/1000 | Loss: 0.00002008
Iteration 37/1000 | Loss: 0.00002008
Iteration 38/1000 | Loss: 0.00002006
Iteration 39/1000 | Loss: 0.00002006
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002003
Iteration 52/1000 | Loss: 0.00002003
Iteration 53/1000 | Loss: 0.00002003
Iteration 54/1000 | Loss: 0.00002003
Iteration 55/1000 | Loss: 0.00002003
Iteration 56/1000 | Loss: 0.00002003
Iteration 57/1000 | Loss: 0.00002003
Iteration 58/1000 | Loss: 0.00002002
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00002000
Iteration 72/1000 | Loss: 0.00002000
Iteration 73/1000 | Loss: 0.00002000
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001998
Iteration 83/1000 | Loss: 0.00001998
Iteration 84/1000 | Loss: 0.00001998
Iteration 85/1000 | Loss: 0.00001998
Iteration 86/1000 | Loss: 0.00001997
Iteration 87/1000 | Loss: 0.00001997
Iteration 88/1000 | Loss: 0.00001996
Iteration 89/1000 | Loss: 0.00001996
Iteration 90/1000 | Loss: 0.00001996
Iteration 91/1000 | Loss: 0.00001995
Iteration 92/1000 | Loss: 0.00001995
Iteration 93/1000 | Loss: 0.00001995
Iteration 94/1000 | Loss: 0.00001995
Iteration 95/1000 | Loss: 0.00001995
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001993
Iteration 102/1000 | Loss: 0.00001993
Iteration 103/1000 | Loss: 0.00001993
Iteration 104/1000 | Loss: 0.00001992
Iteration 105/1000 | Loss: 0.00001992
Iteration 106/1000 | Loss: 0.00001992
Iteration 107/1000 | Loss: 0.00001992
Iteration 108/1000 | Loss: 0.00001991
Iteration 109/1000 | Loss: 0.00001991
Iteration 110/1000 | Loss: 0.00001991
Iteration 111/1000 | Loss: 0.00001990
Iteration 112/1000 | Loss: 0.00001990
Iteration 113/1000 | Loss: 0.00001990
Iteration 114/1000 | Loss: 0.00001990
Iteration 115/1000 | Loss: 0.00001990
Iteration 116/1000 | Loss: 0.00001990
Iteration 117/1000 | Loss: 0.00001990
Iteration 118/1000 | Loss: 0.00001990
Iteration 119/1000 | Loss: 0.00001990
Iteration 120/1000 | Loss: 0.00001990
Iteration 121/1000 | Loss: 0.00001990
Iteration 122/1000 | Loss: 0.00001990
Iteration 123/1000 | Loss: 0.00001989
Iteration 124/1000 | Loss: 0.00001989
Iteration 125/1000 | Loss: 0.00001989
Iteration 126/1000 | Loss: 0.00001989
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001987
Iteration 131/1000 | Loss: 0.00001987
Iteration 132/1000 | Loss: 0.00001987
Iteration 133/1000 | Loss: 0.00001987
Iteration 134/1000 | Loss: 0.00001986
Iteration 135/1000 | Loss: 0.00001986
Iteration 136/1000 | Loss: 0.00001986
Iteration 137/1000 | Loss: 0.00001986
Iteration 138/1000 | Loss: 0.00001986
Iteration 139/1000 | Loss: 0.00001986
Iteration 140/1000 | Loss: 0.00001986
Iteration 141/1000 | Loss: 0.00001986
Iteration 142/1000 | Loss: 0.00001985
Iteration 143/1000 | Loss: 0.00001985
Iteration 144/1000 | Loss: 0.00001985
Iteration 145/1000 | Loss: 0.00001985
Iteration 146/1000 | Loss: 0.00001985
Iteration 147/1000 | Loss: 0.00001985
Iteration 148/1000 | Loss: 0.00001985
Iteration 149/1000 | Loss: 0.00001985
Iteration 150/1000 | Loss: 0.00001985
Iteration 151/1000 | Loss: 0.00001985
Iteration 152/1000 | Loss: 0.00001985
Iteration 153/1000 | Loss: 0.00001985
Iteration 154/1000 | Loss: 0.00001985
Iteration 155/1000 | Loss: 0.00001985
Iteration 156/1000 | Loss: 0.00001985
Iteration 157/1000 | Loss: 0.00001985
Iteration 158/1000 | Loss: 0.00001985
Iteration 159/1000 | Loss: 0.00001985
Iteration 160/1000 | Loss: 0.00001985
Iteration 161/1000 | Loss: 0.00001985
Iteration 162/1000 | Loss: 0.00001985
Iteration 163/1000 | Loss: 0.00001985
Iteration 164/1000 | Loss: 0.00001985
Iteration 165/1000 | Loss: 0.00001985
Iteration 166/1000 | Loss: 0.00001985
Iteration 167/1000 | Loss: 0.00001985
Iteration 168/1000 | Loss: 0.00001985
Iteration 169/1000 | Loss: 0.00001985
Iteration 170/1000 | Loss: 0.00001985
Iteration 171/1000 | Loss: 0.00001985
Iteration 172/1000 | Loss: 0.00001985
Iteration 173/1000 | Loss: 0.00001985
Iteration 174/1000 | Loss: 0.00001985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.9845952920150012e-05, 1.9845952920150012e-05, 1.9845952920150012e-05, 1.9845952920150012e-05, 1.9845952920150012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9845952920150012e-05

Optimization complete. Final v2v error: 3.744488000869751 mm

Highest mean error: 4.174233913421631 mm for frame 112

Lowest mean error: 3.4376230239868164 mm for frame 226

Saving results

Total time: 56.85316205024719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440124
Iteration 2/25 | Loss: 0.00130523
Iteration 3/25 | Loss: 0.00121750
Iteration 4/25 | Loss: 0.00120321
Iteration 5/25 | Loss: 0.00119801
Iteration 6/25 | Loss: 0.00119704
Iteration 7/25 | Loss: 0.00119704
Iteration 8/25 | Loss: 0.00119704
Iteration 9/25 | Loss: 0.00119704
Iteration 10/25 | Loss: 0.00119704
Iteration 11/25 | Loss: 0.00119704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00119704008102417, 0.00119704008102417, 0.00119704008102417, 0.00119704008102417, 0.00119704008102417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00119704008102417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.67312336
Iteration 2/25 | Loss: 0.00078225
Iteration 3/25 | Loss: 0.00078223
Iteration 4/25 | Loss: 0.00078223
Iteration 5/25 | Loss: 0.00078223
Iteration 6/25 | Loss: 0.00078223
Iteration 7/25 | Loss: 0.00078223
Iteration 8/25 | Loss: 0.00078223
Iteration 9/25 | Loss: 0.00078223
Iteration 10/25 | Loss: 0.00078223
Iteration 11/25 | Loss: 0.00078223
Iteration 12/25 | Loss: 0.00078223
Iteration 13/25 | Loss: 0.00078223
Iteration 14/25 | Loss: 0.00078223
Iteration 15/25 | Loss: 0.00078223
Iteration 16/25 | Loss: 0.00078223
Iteration 17/25 | Loss: 0.00078223
Iteration 18/25 | Loss: 0.00078223
Iteration 19/25 | Loss: 0.00078223
Iteration 20/25 | Loss: 0.00078223
Iteration 21/25 | Loss: 0.00078223
Iteration 22/25 | Loss: 0.00078223
Iteration 23/25 | Loss: 0.00078223
Iteration 24/25 | Loss: 0.00078223
Iteration 25/25 | Loss: 0.00078223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078223
Iteration 2/1000 | Loss: 0.00002815
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001672
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001431
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001385
Iteration 11/1000 | Loss: 0.00001371
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001315
Iteration 25/1000 | Loss: 0.00001308
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001297
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001294
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001277
Iteration 125/1000 | Loss: 0.00001277
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001277
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001276
Iteration 132/1000 | Loss: 0.00001276
Iteration 133/1000 | Loss: 0.00001276
Iteration 134/1000 | Loss: 0.00001276
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001275
Iteration 138/1000 | Loss: 0.00001275
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001274
Iteration 145/1000 | Loss: 0.00001274
Iteration 146/1000 | Loss: 0.00001274
Iteration 147/1000 | Loss: 0.00001274
Iteration 148/1000 | Loss: 0.00001274
Iteration 149/1000 | Loss: 0.00001274
Iteration 150/1000 | Loss: 0.00001274
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001273
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Iteration 164/1000 | Loss: 0.00001273
Iteration 165/1000 | Loss: 0.00001273
Iteration 166/1000 | Loss: 0.00001273
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001273
Iteration 177/1000 | Loss: 0.00001273
Iteration 178/1000 | Loss: 0.00001273
Iteration 179/1000 | Loss: 0.00001273
Iteration 180/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.27310813695658e-05, 1.27310813695658e-05, 1.27310813695658e-05, 1.27310813695658e-05, 1.27310813695658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.27310813695658e-05

Optimization complete. Final v2v error: 3.050717353820801 mm

Highest mean error: 3.6146888732910156 mm for frame 92

Lowest mean error: 2.746527910232544 mm for frame 206

Saving results

Total time: 47.67259931564331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396771
Iteration 2/25 | Loss: 0.00120882
Iteration 3/25 | Loss: 0.00117032
Iteration 4/25 | Loss: 0.00116529
Iteration 5/25 | Loss: 0.00116366
Iteration 6/25 | Loss: 0.00116362
Iteration 7/25 | Loss: 0.00116362
Iteration 8/25 | Loss: 0.00116362
Iteration 9/25 | Loss: 0.00116362
Iteration 10/25 | Loss: 0.00116362
Iteration 11/25 | Loss: 0.00116362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011636216659098864, 0.0011636216659098864, 0.0011636216659098864, 0.0011636216659098864, 0.0011636216659098864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011636216659098864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43313718
Iteration 2/25 | Loss: 0.00060726
Iteration 3/25 | Loss: 0.00060726
Iteration 4/25 | Loss: 0.00060726
Iteration 5/25 | Loss: 0.00060726
Iteration 6/25 | Loss: 0.00060726
Iteration 7/25 | Loss: 0.00060726
Iteration 8/25 | Loss: 0.00060726
Iteration 9/25 | Loss: 0.00060726
Iteration 10/25 | Loss: 0.00060726
Iteration 11/25 | Loss: 0.00060726
Iteration 12/25 | Loss: 0.00060726
Iteration 13/25 | Loss: 0.00060726
Iteration 14/25 | Loss: 0.00060726
Iteration 15/25 | Loss: 0.00060725
Iteration 16/25 | Loss: 0.00060725
Iteration 17/25 | Loss: 0.00060725
Iteration 18/25 | Loss: 0.00060725
Iteration 19/25 | Loss: 0.00060725
Iteration 20/25 | Loss: 0.00060726
Iteration 21/25 | Loss: 0.00060725
Iteration 22/25 | Loss: 0.00060725
Iteration 23/25 | Loss: 0.00060725
Iteration 24/25 | Loss: 0.00060726
Iteration 25/25 | Loss: 0.00060726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060726
Iteration 2/1000 | Loss: 0.00002469
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001240
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001172
Iteration 19/1000 | Loss: 0.00001171
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001165
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001152
Iteration 46/1000 | Loss: 0.00001152
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001150
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001149
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001149
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001149
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001148
Iteration 63/1000 | Loss: 0.00001148
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001147
Iteration 67/1000 | Loss: 0.00001147
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001142
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001136
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001130
Iteration 94/1000 | Loss: 0.00001130
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001128
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001128
Iteration 104/1000 | Loss: 0.00001128
Iteration 105/1000 | Loss: 0.00001128
Iteration 106/1000 | Loss: 0.00001128
Iteration 107/1000 | Loss: 0.00001128
Iteration 108/1000 | Loss: 0.00001128
Iteration 109/1000 | Loss: 0.00001128
Iteration 110/1000 | Loss: 0.00001128
Iteration 111/1000 | Loss: 0.00001127
Iteration 112/1000 | Loss: 0.00001127
Iteration 113/1000 | Loss: 0.00001127
Iteration 114/1000 | Loss: 0.00001127
Iteration 115/1000 | Loss: 0.00001127
Iteration 116/1000 | Loss: 0.00001127
Iteration 117/1000 | Loss: 0.00001126
Iteration 118/1000 | Loss: 0.00001126
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001123
Iteration 125/1000 | Loss: 0.00001123
Iteration 126/1000 | Loss: 0.00001122
Iteration 127/1000 | Loss: 0.00001122
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001119
Iteration 135/1000 | Loss: 0.00001119
Iteration 136/1000 | Loss: 0.00001119
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001118
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001118
Iteration 142/1000 | Loss: 0.00001118
Iteration 143/1000 | Loss: 0.00001118
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001117
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001116
Iteration 150/1000 | Loss: 0.00001116
Iteration 151/1000 | Loss: 0.00001116
Iteration 152/1000 | Loss: 0.00001116
Iteration 153/1000 | Loss: 0.00001116
Iteration 154/1000 | Loss: 0.00001116
Iteration 155/1000 | Loss: 0.00001116
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001116
Iteration 161/1000 | Loss: 0.00001116
Iteration 162/1000 | Loss: 0.00001116
Iteration 163/1000 | Loss: 0.00001116
Iteration 164/1000 | Loss: 0.00001116
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001115
Iteration 172/1000 | Loss: 0.00001115
Iteration 173/1000 | Loss: 0.00001115
Iteration 174/1000 | Loss: 0.00001115
Iteration 175/1000 | Loss: 0.00001115
Iteration 176/1000 | Loss: 0.00001115
Iteration 177/1000 | Loss: 0.00001115
Iteration 178/1000 | Loss: 0.00001114
Iteration 179/1000 | Loss: 0.00001114
Iteration 180/1000 | Loss: 0.00001114
Iteration 181/1000 | Loss: 0.00001114
Iteration 182/1000 | Loss: 0.00001114
Iteration 183/1000 | Loss: 0.00001114
Iteration 184/1000 | Loss: 0.00001114
Iteration 185/1000 | Loss: 0.00001114
Iteration 186/1000 | Loss: 0.00001114
Iteration 187/1000 | Loss: 0.00001114
Iteration 188/1000 | Loss: 0.00001114
Iteration 189/1000 | Loss: 0.00001114
Iteration 190/1000 | Loss: 0.00001114
Iteration 191/1000 | Loss: 0.00001114
Iteration 192/1000 | Loss: 0.00001114
Iteration 193/1000 | Loss: 0.00001114
Iteration 194/1000 | Loss: 0.00001113
Iteration 195/1000 | Loss: 0.00001113
Iteration 196/1000 | Loss: 0.00001113
Iteration 197/1000 | Loss: 0.00001113
Iteration 198/1000 | Loss: 0.00001113
Iteration 199/1000 | Loss: 0.00001113
Iteration 200/1000 | Loss: 0.00001113
Iteration 201/1000 | Loss: 0.00001113
Iteration 202/1000 | Loss: 0.00001113
Iteration 203/1000 | Loss: 0.00001113
Iteration 204/1000 | Loss: 0.00001113
Iteration 205/1000 | Loss: 0.00001113
Iteration 206/1000 | Loss: 0.00001113
Iteration 207/1000 | Loss: 0.00001113
Iteration 208/1000 | Loss: 0.00001113
Iteration 209/1000 | Loss: 0.00001112
Iteration 210/1000 | Loss: 0.00001112
Iteration 211/1000 | Loss: 0.00001112
Iteration 212/1000 | Loss: 0.00001112
Iteration 213/1000 | Loss: 0.00001112
Iteration 214/1000 | Loss: 0.00001112
Iteration 215/1000 | Loss: 0.00001112
Iteration 216/1000 | Loss: 0.00001112
Iteration 217/1000 | Loss: 0.00001112
Iteration 218/1000 | Loss: 0.00001112
Iteration 219/1000 | Loss: 0.00001112
Iteration 220/1000 | Loss: 0.00001112
Iteration 221/1000 | Loss: 0.00001112
Iteration 222/1000 | Loss: 0.00001111
Iteration 223/1000 | Loss: 0.00001111
Iteration 224/1000 | Loss: 0.00001111
Iteration 225/1000 | Loss: 0.00001111
Iteration 226/1000 | Loss: 0.00001111
Iteration 227/1000 | Loss: 0.00001111
Iteration 228/1000 | Loss: 0.00001111
Iteration 229/1000 | Loss: 0.00001111
Iteration 230/1000 | Loss: 0.00001111
Iteration 231/1000 | Loss: 0.00001111
Iteration 232/1000 | Loss: 0.00001111
Iteration 233/1000 | Loss: 0.00001111
Iteration 234/1000 | Loss: 0.00001111
Iteration 235/1000 | Loss: 0.00001111
Iteration 236/1000 | Loss: 0.00001111
Iteration 237/1000 | Loss: 0.00001111
Iteration 238/1000 | Loss: 0.00001111
Iteration 239/1000 | Loss: 0.00001111
Iteration 240/1000 | Loss: 0.00001111
Iteration 241/1000 | Loss: 0.00001110
Iteration 242/1000 | Loss: 0.00001110
Iteration 243/1000 | Loss: 0.00001110
Iteration 244/1000 | Loss: 0.00001110
Iteration 245/1000 | Loss: 0.00001110
Iteration 246/1000 | Loss: 0.00001110
Iteration 247/1000 | Loss: 0.00001110
Iteration 248/1000 | Loss: 0.00001110
Iteration 249/1000 | Loss: 0.00001110
Iteration 250/1000 | Loss: 0.00001110
Iteration 251/1000 | Loss: 0.00001110
Iteration 252/1000 | Loss: 0.00001110
Iteration 253/1000 | Loss: 0.00001110
Iteration 254/1000 | Loss: 0.00001110
Iteration 255/1000 | Loss: 0.00001110
Iteration 256/1000 | Loss: 0.00001110
Iteration 257/1000 | Loss: 0.00001110
Iteration 258/1000 | Loss: 0.00001110
Iteration 259/1000 | Loss: 0.00001110
Iteration 260/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.1095742593170144e-05, 1.1095742593170144e-05, 1.1095742593170144e-05, 1.1095742593170144e-05, 1.1095742593170144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1095742593170144e-05

Optimization complete. Final v2v error: 2.8479878902435303 mm

Highest mean error: 3.02085542678833 mm for frame 135

Lowest mean error: 2.754871368408203 mm for frame 38

Saving results

Total time: 43.440741300582886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453672
Iteration 2/25 | Loss: 0.00148791
Iteration 3/25 | Loss: 0.00129161
Iteration 4/25 | Loss: 0.00125222
Iteration 5/25 | Loss: 0.00124437
Iteration 6/25 | Loss: 0.00124345
Iteration 7/25 | Loss: 0.00124345
Iteration 8/25 | Loss: 0.00124345
Iteration 9/25 | Loss: 0.00124345
Iteration 10/25 | Loss: 0.00124345
Iteration 11/25 | Loss: 0.00124345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001243449398316443, 0.001243449398316443, 0.001243449398316443, 0.001243449398316443, 0.001243449398316443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243449398316443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35033703
Iteration 2/25 | Loss: 0.00079310
Iteration 3/25 | Loss: 0.00079308
Iteration 4/25 | Loss: 0.00079308
Iteration 5/25 | Loss: 0.00079308
Iteration 6/25 | Loss: 0.00079308
Iteration 7/25 | Loss: 0.00079308
Iteration 8/25 | Loss: 0.00079308
Iteration 9/25 | Loss: 0.00079308
Iteration 10/25 | Loss: 0.00079308
Iteration 11/25 | Loss: 0.00079308
Iteration 12/25 | Loss: 0.00079308
Iteration 13/25 | Loss: 0.00079308
Iteration 14/25 | Loss: 0.00079308
Iteration 15/25 | Loss: 0.00079308
Iteration 16/25 | Loss: 0.00079308
Iteration 17/25 | Loss: 0.00079308
Iteration 18/25 | Loss: 0.00079308
Iteration 19/25 | Loss: 0.00079308
Iteration 20/25 | Loss: 0.00079308
Iteration 21/25 | Loss: 0.00079308
Iteration 22/25 | Loss: 0.00079308
Iteration 23/25 | Loss: 0.00079308
Iteration 24/25 | Loss: 0.00079308
Iteration 25/25 | Loss: 0.00079308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079308
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00003021
Iteration 4/1000 | Loss: 0.00002626
Iteration 5/1000 | Loss: 0.00002424
Iteration 6/1000 | Loss: 0.00002260
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002093
Iteration 9/1000 | Loss: 0.00002035
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001942
Iteration 12/1000 | Loss: 0.00001941
Iteration 13/1000 | Loss: 0.00001918
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001882
Iteration 17/1000 | Loss: 0.00001874
Iteration 18/1000 | Loss: 0.00001872
Iteration 19/1000 | Loss: 0.00001872
Iteration 20/1000 | Loss: 0.00001868
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001862
Iteration 27/1000 | Loss: 0.00001862
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001860
Iteration 34/1000 | Loss: 0.00001860
Iteration 35/1000 | Loss: 0.00001859
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001859
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001858
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001858
Iteration 46/1000 | Loss: 0.00001858
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001855
Iteration 52/1000 | Loss: 0.00001855
Iteration 53/1000 | Loss: 0.00001855
Iteration 54/1000 | Loss: 0.00001855
Iteration 55/1000 | Loss: 0.00001855
Iteration 56/1000 | Loss: 0.00001854
Iteration 57/1000 | Loss: 0.00001854
Iteration 58/1000 | Loss: 0.00001853
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001850
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001849
Iteration 70/1000 | Loss: 0.00001849
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001846
Iteration 85/1000 | Loss: 0.00001846
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001846
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001846
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001845
Iteration 95/1000 | Loss: 0.00001845
Iteration 96/1000 | Loss: 0.00001845
Iteration 97/1000 | Loss: 0.00001845
Iteration 98/1000 | Loss: 0.00001845
Iteration 99/1000 | Loss: 0.00001844
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001841
Iteration 111/1000 | Loss: 0.00001841
Iteration 112/1000 | Loss: 0.00001841
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001840
Iteration 115/1000 | Loss: 0.00001840
Iteration 116/1000 | Loss: 0.00001840
Iteration 117/1000 | Loss: 0.00001840
Iteration 118/1000 | Loss: 0.00001840
Iteration 119/1000 | Loss: 0.00001840
Iteration 120/1000 | Loss: 0.00001840
Iteration 121/1000 | Loss: 0.00001840
Iteration 122/1000 | Loss: 0.00001840
Iteration 123/1000 | Loss: 0.00001840
Iteration 124/1000 | Loss: 0.00001840
Iteration 125/1000 | Loss: 0.00001840
Iteration 126/1000 | Loss: 0.00001839
Iteration 127/1000 | Loss: 0.00001839
Iteration 128/1000 | Loss: 0.00001839
Iteration 129/1000 | Loss: 0.00001839
Iteration 130/1000 | Loss: 0.00001839
Iteration 131/1000 | Loss: 0.00001839
Iteration 132/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.8389544493402354e-05, 1.8389544493402354e-05, 1.8389544493402354e-05, 1.8389544493402354e-05, 1.8389544493402354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8389544493402354e-05

Optimization complete. Final v2v error: 3.609426259994507 mm

Highest mean error: 4.225499629974365 mm for frame 249

Lowest mean error: 3.1919963359832764 mm for frame 49

Saving results

Total time: 46.334636211395264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081561
Iteration 2/25 | Loss: 0.00246740
Iteration 3/25 | Loss: 0.00193948
Iteration 4/25 | Loss: 0.00211269
Iteration 5/25 | Loss: 0.00209187
Iteration 6/25 | Loss: 0.00202653
Iteration 7/25 | Loss: 0.00189118
Iteration 8/25 | Loss: 0.00174086
Iteration 9/25 | Loss: 0.00169076
Iteration 10/25 | Loss: 0.00166798
Iteration 11/25 | Loss: 0.00164503
Iteration 12/25 | Loss: 0.00160729
Iteration 13/25 | Loss: 0.00157851
Iteration 14/25 | Loss: 0.00154575
Iteration 15/25 | Loss: 0.00154149
Iteration 16/25 | Loss: 0.00153997
Iteration 17/25 | Loss: 0.00155083
Iteration 18/25 | Loss: 0.00153404
Iteration 19/25 | Loss: 0.00152798
Iteration 20/25 | Loss: 0.00152254
Iteration 21/25 | Loss: 0.00153171
Iteration 22/25 | Loss: 0.00153377
Iteration 23/25 | Loss: 0.00151225
Iteration 24/25 | Loss: 0.00150758
Iteration 25/25 | Loss: 0.00150707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70228988
Iteration 2/25 | Loss: 0.00159213
Iteration 3/25 | Loss: 0.00159213
Iteration 4/25 | Loss: 0.00159213
Iteration 5/25 | Loss: 0.00159213
Iteration 6/25 | Loss: 0.00159213
Iteration 7/25 | Loss: 0.00159213
Iteration 8/25 | Loss: 0.00159213
Iteration 9/25 | Loss: 0.00159213
Iteration 10/25 | Loss: 0.00159213
Iteration 11/25 | Loss: 0.00159213
Iteration 12/25 | Loss: 0.00159213
Iteration 13/25 | Loss: 0.00159213
Iteration 14/25 | Loss: 0.00159213
Iteration 15/25 | Loss: 0.00159213
Iteration 16/25 | Loss: 0.00159213
Iteration 17/25 | Loss: 0.00159213
Iteration 18/25 | Loss: 0.00159213
Iteration 19/25 | Loss: 0.00159213
Iteration 20/25 | Loss: 0.00159213
Iteration 21/25 | Loss: 0.00159213
Iteration 22/25 | Loss: 0.00159213
Iteration 23/25 | Loss: 0.00159213
Iteration 24/25 | Loss: 0.00159213
Iteration 25/25 | Loss: 0.00159213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159213
Iteration 2/1000 | Loss: 0.00050948
Iteration 3/1000 | Loss: 0.00052155
Iteration 4/1000 | Loss: 0.00040877
Iteration 5/1000 | Loss: 0.00042823
Iteration 6/1000 | Loss: 0.00050739
Iteration 7/1000 | Loss: 0.00066430
Iteration 8/1000 | Loss: 0.00127726
Iteration 9/1000 | Loss: 0.00060452
Iteration 10/1000 | Loss: 0.00046588
Iteration 11/1000 | Loss: 0.00023267
Iteration 12/1000 | Loss: 0.00014359
Iteration 13/1000 | Loss: 0.00011264
Iteration 14/1000 | Loss: 0.00043830
Iteration 15/1000 | Loss: 0.00052256
Iteration 16/1000 | Loss: 0.00057509
Iteration 17/1000 | Loss: 0.00028012
Iteration 18/1000 | Loss: 0.00024755
Iteration 19/1000 | Loss: 0.00029329
Iteration 20/1000 | Loss: 0.00020547
Iteration 21/1000 | Loss: 0.00033797
Iteration 22/1000 | Loss: 0.00085127
Iteration 23/1000 | Loss: 0.00049169
Iteration 24/1000 | Loss: 0.00034960
Iteration 25/1000 | Loss: 0.00041824
Iteration 26/1000 | Loss: 0.00028433
Iteration 27/1000 | Loss: 0.00025572
Iteration 28/1000 | Loss: 0.00021594
Iteration 29/1000 | Loss: 0.00018550
Iteration 30/1000 | Loss: 0.00012137
Iteration 31/1000 | Loss: 0.00040909
Iteration 32/1000 | Loss: 0.00005860
Iteration 33/1000 | Loss: 0.00005037
Iteration 34/1000 | Loss: 0.00048568
Iteration 35/1000 | Loss: 0.00051566
Iteration 36/1000 | Loss: 0.00042790
Iteration 37/1000 | Loss: 0.00047339
Iteration 38/1000 | Loss: 0.00056191
Iteration 39/1000 | Loss: 0.00042624
Iteration 40/1000 | Loss: 0.00084119
Iteration 41/1000 | Loss: 0.00077279
Iteration 42/1000 | Loss: 0.00064995
Iteration 43/1000 | Loss: 0.00060292
Iteration 44/1000 | Loss: 0.00057490
Iteration 45/1000 | Loss: 0.00035697
Iteration 46/1000 | Loss: 0.00050456
Iteration 47/1000 | Loss: 0.00093907
Iteration 48/1000 | Loss: 0.00035731
Iteration 49/1000 | Loss: 0.00021146
Iteration 50/1000 | Loss: 0.00073953
Iteration 51/1000 | Loss: 0.00067840
Iteration 52/1000 | Loss: 0.00019794
Iteration 53/1000 | Loss: 0.00039773
Iteration 54/1000 | Loss: 0.00070235
Iteration 55/1000 | Loss: 0.00100740
Iteration 56/1000 | Loss: 0.00065380
Iteration 57/1000 | Loss: 0.00037126
Iteration 58/1000 | Loss: 0.00016352
Iteration 59/1000 | Loss: 0.00005208
Iteration 60/1000 | Loss: 0.00004630
Iteration 61/1000 | Loss: 0.00004198
Iteration 62/1000 | Loss: 0.00021247
Iteration 63/1000 | Loss: 0.00004144
Iteration 64/1000 | Loss: 0.00024701
Iteration 65/1000 | Loss: 0.00014192
Iteration 66/1000 | Loss: 0.00004367
Iteration 67/1000 | Loss: 0.00003490
Iteration 68/1000 | Loss: 0.00003548
Iteration 69/1000 | Loss: 0.00003329
Iteration 70/1000 | Loss: 0.00003091
Iteration 71/1000 | Loss: 0.00003040
Iteration 72/1000 | Loss: 0.00002957
Iteration 73/1000 | Loss: 0.00030733
Iteration 74/1000 | Loss: 0.00012582
Iteration 75/1000 | Loss: 0.00003553
Iteration 76/1000 | Loss: 0.00015023
Iteration 77/1000 | Loss: 0.00019301
Iteration 78/1000 | Loss: 0.00015772
Iteration 79/1000 | Loss: 0.00012833
Iteration 80/1000 | Loss: 0.00014873
Iteration 81/1000 | Loss: 0.00013916
Iteration 82/1000 | Loss: 0.00015892
Iteration 83/1000 | Loss: 0.00005786
Iteration 84/1000 | Loss: 0.00003989
Iteration 85/1000 | Loss: 0.00009527
Iteration 86/1000 | Loss: 0.00006234
Iteration 87/1000 | Loss: 0.00006445
Iteration 88/1000 | Loss: 0.00003581
Iteration 89/1000 | Loss: 0.00002984
Iteration 90/1000 | Loss: 0.00002753
Iteration 91/1000 | Loss: 0.00002631
Iteration 92/1000 | Loss: 0.00002599
Iteration 93/1000 | Loss: 0.00002576
Iteration 94/1000 | Loss: 0.00002569
Iteration 95/1000 | Loss: 0.00002563
Iteration 96/1000 | Loss: 0.00002563
Iteration 97/1000 | Loss: 0.00002563
Iteration 98/1000 | Loss: 0.00002551
Iteration 99/1000 | Loss: 0.00002547
Iteration 100/1000 | Loss: 0.00002546
Iteration 101/1000 | Loss: 0.00002533
Iteration 102/1000 | Loss: 0.00002525
Iteration 103/1000 | Loss: 0.00002571
Iteration 104/1000 | Loss: 0.00002509
Iteration 105/1000 | Loss: 0.00002498
Iteration 106/1000 | Loss: 0.00002495
Iteration 107/1000 | Loss: 0.00002495
Iteration 108/1000 | Loss: 0.00002484
Iteration 109/1000 | Loss: 0.00002484
Iteration 110/1000 | Loss: 0.00002483
Iteration 111/1000 | Loss: 0.00002483
Iteration 112/1000 | Loss: 0.00002478
Iteration 113/1000 | Loss: 0.00002478
Iteration 114/1000 | Loss: 0.00002477
Iteration 115/1000 | Loss: 0.00002477
Iteration 116/1000 | Loss: 0.00002475
Iteration 117/1000 | Loss: 0.00002475
Iteration 118/1000 | Loss: 0.00002475
Iteration 119/1000 | Loss: 0.00002475
Iteration 120/1000 | Loss: 0.00002474
Iteration 121/1000 | Loss: 0.00002474
Iteration 122/1000 | Loss: 0.00002474
Iteration 123/1000 | Loss: 0.00002474
Iteration 124/1000 | Loss: 0.00002474
Iteration 125/1000 | Loss: 0.00002474
Iteration 126/1000 | Loss: 0.00002473
Iteration 127/1000 | Loss: 0.00002473
Iteration 128/1000 | Loss: 0.00002473
Iteration 129/1000 | Loss: 0.00002473
Iteration 130/1000 | Loss: 0.00002473
Iteration 131/1000 | Loss: 0.00002473
Iteration 132/1000 | Loss: 0.00002472
Iteration 133/1000 | Loss: 0.00002472
Iteration 134/1000 | Loss: 0.00002472
Iteration 135/1000 | Loss: 0.00002472
Iteration 136/1000 | Loss: 0.00002472
Iteration 137/1000 | Loss: 0.00002471
Iteration 138/1000 | Loss: 0.00002471
Iteration 139/1000 | Loss: 0.00002471
Iteration 140/1000 | Loss: 0.00002470
Iteration 141/1000 | Loss: 0.00002470
Iteration 142/1000 | Loss: 0.00002470
Iteration 143/1000 | Loss: 0.00002469
Iteration 144/1000 | Loss: 0.00002469
Iteration 145/1000 | Loss: 0.00002468
Iteration 146/1000 | Loss: 0.00002468
Iteration 147/1000 | Loss: 0.00002468
Iteration 148/1000 | Loss: 0.00002468
Iteration 149/1000 | Loss: 0.00002468
Iteration 150/1000 | Loss: 0.00002468
Iteration 151/1000 | Loss: 0.00002468
Iteration 152/1000 | Loss: 0.00002468
Iteration 153/1000 | Loss: 0.00002468
Iteration 154/1000 | Loss: 0.00002468
Iteration 155/1000 | Loss: 0.00002468
Iteration 156/1000 | Loss: 0.00002468
Iteration 157/1000 | Loss: 0.00002467
Iteration 158/1000 | Loss: 0.00002467
Iteration 159/1000 | Loss: 0.00002467
Iteration 160/1000 | Loss: 0.00002467
Iteration 161/1000 | Loss: 0.00002467
Iteration 162/1000 | Loss: 0.00002467
Iteration 163/1000 | Loss: 0.00002466
Iteration 164/1000 | Loss: 0.00002466
Iteration 165/1000 | Loss: 0.00002466
Iteration 166/1000 | Loss: 0.00002466
Iteration 167/1000 | Loss: 0.00002466
Iteration 168/1000 | Loss: 0.00002466
Iteration 169/1000 | Loss: 0.00002466
Iteration 170/1000 | Loss: 0.00002466
Iteration 171/1000 | Loss: 0.00002466
Iteration 172/1000 | Loss: 0.00002466
Iteration 173/1000 | Loss: 0.00002466
Iteration 174/1000 | Loss: 0.00002466
Iteration 175/1000 | Loss: 0.00002466
Iteration 176/1000 | Loss: 0.00002466
Iteration 177/1000 | Loss: 0.00002466
Iteration 178/1000 | Loss: 0.00002466
Iteration 179/1000 | Loss: 0.00002466
Iteration 180/1000 | Loss: 0.00002466
Iteration 181/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.4660563212819397e-05, 2.4660563212819397e-05, 2.4660563212819397e-05, 2.4660563212819397e-05, 2.4660563212819397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4660563212819397e-05

Optimization complete. Final v2v error: 4.086044788360596 mm

Highest mean error: 5.474534034729004 mm for frame 138

Lowest mean error: 3.6970086097717285 mm for frame 26

Saving results

Total time: 197.7683117389679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555759
Iteration 2/25 | Loss: 0.00139793
Iteration 3/25 | Loss: 0.00131580
Iteration 4/25 | Loss: 0.00130640
Iteration 5/25 | Loss: 0.00130497
Iteration 6/25 | Loss: 0.00130497
Iteration 7/25 | Loss: 0.00130497
Iteration 8/25 | Loss: 0.00130497
Iteration 9/25 | Loss: 0.00130497
Iteration 10/25 | Loss: 0.00130497
Iteration 11/25 | Loss: 0.00130497
Iteration 12/25 | Loss: 0.00130497
Iteration 13/25 | Loss: 0.00130497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013049655826762319, 0.0013049655826762319, 0.0013049655826762319, 0.0013049655826762319, 0.0013049655826762319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013049655826762319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46848297
Iteration 2/25 | Loss: 0.00087734
Iteration 3/25 | Loss: 0.00087732
Iteration 4/25 | Loss: 0.00087732
Iteration 5/25 | Loss: 0.00087732
Iteration 6/25 | Loss: 0.00087732
Iteration 7/25 | Loss: 0.00087732
Iteration 8/25 | Loss: 0.00087732
Iteration 9/25 | Loss: 0.00087732
Iteration 10/25 | Loss: 0.00087732
Iteration 11/25 | Loss: 0.00087732
Iteration 12/25 | Loss: 0.00087732
Iteration 13/25 | Loss: 0.00087732
Iteration 14/25 | Loss: 0.00087732
Iteration 15/25 | Loss: 0.00087732
Iteration 16/25 | Loss: 0.00087732
Iteration 17/25 | Loss: 0.00087732
Iteration 18/25 | Loss: 0.00087732
Iteration 19/25 | Loss: 0.00087732
Iteration 20/25 | Loss: 0.00087732
Iteration 21/25 | Loss: 0.00087732
Iteration 22/25 | Loss: 0.00087732
Iteration 23/25 | Loss: 0.00087732
Iteration 24/25 | Loss: 0.00087732
Iteration 25/25 | Loss: 0.00087732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087732
Iteration 2/1000 | Loss: 0.00003863
Iteration 3/1000 | Loss: 0.00003115
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002883
Iteration 6/1000 | Loss: 0.00002826
Iteration 7/1000 | Loss: 0.00002778
Iteration 8/1000 | Loss: 0.00002741
Iteration 9/1000 | Loss: 0.00002711
Iteration 10/1000 | Loss: 0.00002688
Iteration 11/1000 | Loss: 0.00002661
Iteration 12/1000 | Loss: 0.00002635
Iteration 13/1000 | Loss: 0.00002612
Iteration 14/1000 | Loss: 0.00002594
Iteration 15/1000 | Loss: 0.00002583
Iteration 16/1000 | Loss: 0.00002583
Iteration 17/1000 | Loss: 0.00002577
Iteration 18/1000 | Loss: 0.00002575
Iteration 19/1000 | Loss: 0.00002575
Iteration 20/1000 | Loss: 0.00002571
Iteration 21/1000 | Loss: 0.00002571
Iteration 22/1000 | Loss: 0.00002570
Iteration 23/1000 | Loss: 0.00002570
Iteration 24/1000 | Loss: 0.00002570
Iteration 25/1000 | Loss: 0.00002570
Iteration 26/1000 | Loss: 0.00002570
Iteration 27/1000 | Loss: 0.00002570
Iteration 28/1000 | Loss: 0.00002570
Iteration 29/1000 | Loss: 0.00002570
Iteration 30/1000 | Loss: 0.00002569
Iteration 31/1000 | Loss: 0.00002567
Iteration 32/1000 | Loss: 0.00002567
Iteration 33/1000 | Loss: 0.00002567
Iteration 34/1000 | Loss: 0.00002567
Iteration 35/1000 | Loss: 0.00002566
Iteration 36/1000 | Loss: 0.00002566
Iteration 37/1000 | Loss: 0.00002562
Iteration 38/1000 | Loss: 0.00002561
Iteration 39/1000 | Loss: 0.00002557
Iteration 40/1000 | Loss: 0.00002557
Iteration 41/1000 | Loss: 0.00002556
Iteration 42/1000 | Loss: 0.00002556
Iteration 43/1000 | Loss: 0.00002556
Iteration 44/1000 | Loss: 0.00002556
Iteration 45/1000 | Loss: 0.00002555
Iteration 46/1000 | Loss: 0.00002553
Iteration 47/1000 | Loss: 0.00002553
Iteration 48/1000 | Loss: 0.00002553
Iteration 49/1000 | Loss: 0.00002551
Iteration 50/1000 | Loss: 0.00002551
Iteration 51/1000 | Loss: 0.00002550
Iteration 52/1000 | Loss: 0.00002549
Iteration 53/1000 | Loss: 0.00002549
Iteration 54/1000 | Loss: 0.00002549
Iteration 55/1000 | Loss: 0.00002549
Iteration 56/1000 | Loss: 0.00002549
Iteration 57/1000 | Loss: 0.00002549
Iteration 58/1000 | Loss: 0.00002549
Iteration 59/1000 | Loss: 0.00002549
Iteration 60/1000 | Loss: 0.00002548
Iteration 61/1000 | Loss: 0.00002548
Iteration 62/1000 | Loss: 0.00002548
Iteration 63/1000 | Loss: 0.00002548
Iteration 64/1000 | Loss: 0.00002546
Iteration 65/1000 | Loss: 0.00002546
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002546
Iteration 68/1000 | Loss: 0.00002546
Iteration 69/1000 | Loss: 0.00002546
Iteration 70/1000 | Loss: 0.00002546
Iteration 71/1000 | Loss: 0.00002546
Iteration 72/1000 | Loss: 0.00002546
Iteration 73/1000 | Loss: 0.00002546
Iteration 74/1000 | Loss: 0.00002546
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002545
Iteration 77/1000 | Loss: 0.00002545
Iteration 78/1000 | Loss: 0.00002545
Iteration 79/1000 | Loss: 0.00002545
Iteration 80/1000 | Loss: 0.00002545
Iteration 81/1000 | Loss: 0.00002545
Iteration 82/1000 | Loss: 0.00002545
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002544
Iteration 85/1000 | Loss: 0.00002544
Iteration 86/1000 | Loss: 0.00002544
Iteration 87/1000 | Loss: 0.00002544
Iteration 88/1000 | Loss: 0.00002544
Iteration 89/1000 | Loss: 0.00002544
Iteration 90/1000 | Loss: 0.00002543
Iteration 91/1000 | Loss: 0.00002543
Iteration 92/1000 | Loss: 0.00002543
Iteration 93/1000 | Loss: 0.00002542
Iteration 94/1000 | Loss: 0.00002542
Iteration 95/1000 | Loss: 0.00002542
Iteration 96/1000 | Loss: 0.00002542
Iteration 97/1000 | Loss: 0.00002542
Iteration 98/1000 | Loss: 0.00002542
Iteration 99/1000 | Loss: 0.00002542
Iteration 100/1000 | Loss: 0.00002542
Iteration 101/1000 | Loss: 0.00002542
Iteration 102/1000 | Loss: 0.00002542
Iteration 103/1000 | Loss: 0.00002542
Iteration 104/1000 | Loss: 0.00002542
Iteration 105/1000 | Loss: 0.00002541
Iteration 106/1000 | Loss: 0.00002541
Iteration 107/1000 | Loss: 0.00002541
Iteration 108/1000 | Loss: 0.00002541
Iteration 109/1000 | Loss: 0.00002541
Iteration 110/1000 | Loss: 0.00002541
Iteration 111/1000 | Loss: 0.00002541
Iteration 112/1000 | Loss: 0.00002541
Iteration 113/1000 | Loss: 0.00002541
Iteration 114/1000 | Loss: 0.00002540
Iteration 115/1000 | Loss: 0.00002540
Iteration 116/1000 | Loss: 0.00002540
Iteration 117/1000 | Loss: 0.00002540
Iteration 118/1000 | Loss: 0.00002540
Iteration 119/1000 | Loss: 0.00002540
Iteration 120/1000 | Loss: 0.00002540
Iteration 121/1000 | Loss: 0.00002540
Iteration 122/1000 | Loss: 0.00002540
Iteration 123/1000 | Loss: 0.00002540
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002539
Iteration 134/1000 | Loss: 0.00002539
Iteration 135/1000 | Loss: 0.00002539
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002539
Iteration 138/1000 | Loss: 0.00002539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.539497654652223e-05, 2.539497654652223e-05, 2.539497654652223e-05, 2.539497654652223e-05, 2.539497654652223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.539497654652223e-05

Optimization complete. Final v2v error: 4.110363006591797 mm

Highest mean error: 4.50570011138916 mm for frame 132

Lowest mean error: 3.65621018409729 mm for frame 81

Saving results

Total time: 45.53322505950928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035388
Iteration 2/25 | Loss: 0.00258281
Iteration 3/25 | Loss: 0.00184515
Iteration 4/25 | Loss: 0.00179814
Iteration 5/25 | Loss: 0.00176748
Iteration 6/25 | Loss: 0.00178122
Iteration 7/25 | Loss: 0.00167584
Iteration 8/25 | Loss: 0.00162444
Iteration 9/25 | Loss: 0.00157631
Iteration 10/25 | Loss: 0.00156503
Iteration 11/25 | Loss: 0.00156944
Iteration 12/25 | Loss: 0.00156310
Iteration 13/25 | Loss: 0.00154868
Iteration 14/25 | Loss: 0.00153772
Iteration 15/25 | Loss: 0.00153206
Iteration 16/25 | Loss: 0.00153434
Iteration 17/25 | Loss: 0.00153081
Iteration 18/25 | Loss: 0.00152698
Iteration 19/25 | Loss: 0.00152466
Iteration 20/25 | Loss: 0.00152572
Iteration 21/25 | Loss: 0.00152637
Iteration 22/25 | Loss: 0.00152579
Iteration 23/25 | Loss: 0.00152681
Iteration 24/25 | Loss: 0.00152400
Iteration 25/25 | Loss: 0.00152358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39524519
Iteration 2/25 | Loss: 0.00352280
Iteration 3/25 | Loss: 0.00352279
Iteration 4/25 | Loss: 0.00352279
Iteration 5/25 | Loss: 0.00352279
Iteration 6/25 | Loss: 0.00352279
Iteration 7/25 | Loss: 0.00352279
Iteration 8/25 | Loss: 0.00352279
Iteration 9/25 | Loss: 0.00352279
Iteration 10/25 | Loss: 0.00352279
Iteration 11/25 | Loss: 0.00352279
Iteration 12/25 | Loss: 0.00316753
Iteration 13/25 | Loss: 0.00306324
Iteration 14/25 | Loss: 0.00307612
Iteration 15/25 | Loss: 0.00306068
Iteration 16/25 | Loss: 0.00306068
Iteration 17/25 | Loss: 0.00306067
Iteration 18/25 | Loss: 0.00306067
Iteration 19/25 | Loss: 0.00306067
Iteration 20/25 | Loss: 0.00306067
Iteration 21/25 | Loss: 0.00306067
Iteration 22/25 | Loss: 0.00306067
Iteration 23/25 | Loss: 0.00306067
Iteration 24/25 | Loss: 0.00306067
Iteration 25/25 | Loss: 0.00306067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306067
Iteration 2/1000 | Loss: 0.00501992
Iteration 3/1000 | Loss: 0.00252437
Iteration 4/1000 | Loss: 0.00063047
Iteration 5/1000 | Loss: 0.00134242
Iteration 6/1000 | Loss: 0.00584281
Iteration 7/1000 | Loss: 0.00504020
Iteration 8/1000 | Loss: 0.00062906
Iteration 9/1000 | Loss: 0.00382721
Iteration 10/1000 | Loss: 0.00187019
Iteration 11/1000 | Loss: 0.00205830
Iteration 12/1000 | Loss: 0.00243548
Iteration 13/1000 | Loss: 0.00097817
Iteration 14/1000 | Loss: 0.00261206
Iteration 15/1000 | Loss: 0.00032498
Iteration 16/1000 | Loss: 0.00031389
Iteration 17/1000 | Loss: 0.00037749
Iteration 18/1000 | Loss: 0.00025171
Iteration 19/1000 | Loss: 0.00078486
Iteration 20/1000 | Loss: 0.00041164
Iteration 21/1000 | Loss: 0.00063865
Iteration 22/1000 | Loss: 0.00017463
Iteration 23/1000 | Loss: 0.00193590
Iteration 24/1000 | Loss: 0.00913186
Iteration 25/1000 | Loss: 0.00879147
Iteration 26/1000 | Loss: 0.00374819
Iteration 27/1000 | Loss: 0.00144974
Iteration 28/1000 | Loss: 0.00191443
Iteration 29/1000 | Loss: 0.00866681
Iteration 30/1000 | Loss: 0.00340040
Iteration 31/1000 | Loss: 0.00293050
Iteration 32/1000 | Loss: 0.00219935
Iteration 33/1000 | Loss: 0.00236041
Iteration 34/1000 | Loss: 0.00202206
Iteration 35/1000 | Loss: 0.00140796
Iteration 36/1000 | Loss: 0.00176088
Iteration 37/1000 | Loss: 0.00089093
Iteration 38/1000 | Loss: 0.00077785
Iteration 39/1000 | Loss: 0.00045009
Iteration 40/1000 | Loss: 0.00012841
Iteration 41/1000 | Loss: 0.00059524
Iteration 42/1000 | Loss: 0.00021294
Iteration 43/1000 | Loss: 0.00036164
Iteration 44/1000 | Loss: 0.00073556
Iteration 45/1000 | Loss: 0.00054037
Iteration 46/1000 | Loss: 0.00034204
Iteration 47/1000 | Loss: 0.00029934
Iteration 48/1000 | Loss: 0.00076849
Iteration 49/1000 | Loss: 0.00130584
Iteration 50/1000 | Loss: 0.00160009
Iteration 51/1000 | Loss: 0.00115177
Iteration 52/1000 | Loss: 0.00156148
Iteration 53/1000 | Loss: 0.00139009
Iteration 54/1000 | Loss: 0.00130225
Iteration 55/1000 | Loss: 0.00091253
Iteration 56/1000 | Loss: 0.00048842
Iteration 57/1000 | Loss: 0.00037071
Iteration 58/1000 | Loss: 0.00009667
Iteration 59/1000 | Loss: 0.00066808
Iteration 60/1000 | Loss: 0.00062322
Iteration 61/1000 | Loss: 0.00038134
Iteration 62/1000 | Loss: 0.00043434
Iteration 63/1000 | Loss: 0.00050740
Iteration 64/1000 | Loss: 0.00163872
Iteration 65/1000 | Loss: 0.00060603
Iteration 66/1000 | Loss: 0.00039592
Iteration 67/1000 | Loss: 0.00027815
Iteration 68/1000 | Loss: 0.00031391
Iteration 69/1000 | Loss: 0.00088758
Iteration 70/1000 | Loss: 0.00073225
Iteration 71/1000 | Loss: 0.00016314
Iteration 72/1000 | Loss: 0.00028912
Iteration 73/1000 | Loss: 0.00029509
Iteration 74/1000 | Loss: 0.00016191
Iteration 75/1000 | Loss: 0.00111961
Iteration 76/1000 | Loss: 0.00080920
Iteration 77/1000 | Loss: 0.00081774
Iteration 78/1000 | Loss: 0.00009468
Iteration 79/1000 | Loss: 0.00007126
Iteration 80/1000 | Loss: 0.00010938
Iteration 81/1000 | Loss: 0.00005411
Iteration 82/1000 | Loss: 0.00007830
Iteration 83/1000 | Loss: 0.00007420
Iteration 84/1000 | Loss: 0.00007469
Iteration 85/1000 | Loss: 0.00006469
Iteration 86/1000 | Loss: 0.00026253
Iteration 87/1000 | Loss: 0.00008635
Iteration 88/1000 | Loss: 0.00009915
Iteration 89/1000 | Loss: 0.00007775
Iteration 90/1000 | Loss: 0.00017874
Iteration 91/1000 | Loss: 0.00005537
Iteration 92/1000 | Loss: 0.00003850
Iteration 93/1000 | Loss: 0.00008985
Iteration 94/1000 | Loss: 0.00007535
Iteration 95/1000 | Loss: 0.00016595
Iteration 96/1000 | Loss: 0.00007501
Iteration 97/1000 | Loss: 0.00007487
Iteration 98/1000 | Loss: 0.00009978
Iteration 99/1000 | Loss: 0.00007697
Iteration 100/1000 | Loss: 0.00004823
Iteration 101/1000 | Loss: 0.00003091
Iteration 102/1000 | Loss: 0.00004167
Iteration 103/1000 | Loss: 0.00003566
Iteration 104/1000 | Loss: 0.00002573
Iteration 105/1000 | Loss: 0.00008073
Iteration 106/1000 | Loss: 0.00003278
Iteration 107/1000 | Loss: 0.00005943
Iteration 108/1000 | Loss: 0.00002077
Iteration 109/1000 | Loss: 0.00002750
Iteration 110/1000 | Loss: 0.00005484
Iteration 111/1000 | Loss: 0.00004600
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00003431
Iteration 114/1000 | Loss: 0.00002160
Iteration 115/1000 | Loss: 0.00001909
Iteration 116/1000 | Loss: 0.00001829
Iteration 117/1000 | Loss: 0.00001888
Iteration 118/1000 | Loss: 0.00004090
Iteration 119/1000 | Loss: 0.00001788
Iteration 120/1000 | Loss: 0.00002054
Iteration 121/1000 | Loss: 0.00002096
Iteration 122/1000 | Loss: 0.00003353
Iteration 123/1000 | Loss: 0.00004165
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001758
Iteration 126/1000 | Loss: 0.00001752
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00001752
Iteration 131/1000 | Loss: 0.00001752
Iteration 132/1000 | Loss: 0.00001752
Iteration 133/1000 | Loss: 0.00001752
Iteration 134/1000 | Loss: 0.00001752
Iteration 135/1000 | Loss: 0.00001752
Iteration 136/1000 | Loss: 0.00001752
Iteration 137/1000 | Loss: 0.00001752
Iteration 138/1000 | Loss: 0.00002043
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001747
Iteration 147/1000 | Loss: 0.00004040
Iteration 148/1000 | Loss: 0.00013178
Iteration 149/1000 | Loss: 0.00003133
Iteration 150/1000 | Loss: 0.00002655
Iteration 151/1000 | Loss: 0.00001838
Iteration 152/1000 | Loss: 0.00001732
Iteration 153/1000 | Loss: 0.00001732
Iteration 154/1000 | Loss: 0.00001732
Iteration 155/1000 | Loss: 0.00001731
Iteration 156/1000 | Loss: 0.00001731
Iteration 157/1000 | Loss: 0.00001731
Iteration 158/1000 | Loss: 0.00001731
Iteration 159/1000 | Loss: 0.00001731
Iteration 160/1000 | Loss: 0.00001731
Iteration 161/1000 | Loss: 0.00001731
Iteration 162/1000 | Loss: 0.00001731
Iteration 163/1000 | Loss: 0.00001731
Iteration 164/1000 | Loss: 0.00001731
Iteration 165/1000 | Loss: 0.00001731
Iteration 166/1000 | Loss: 0.00001731
Iteration 167/1000 | Loss: 0.00001731
Iteration 168/1000 | Loss: 0.00001731
Iteration 169/1000 | Loss: 0.00001731
Iteration 170/1000 | Loss: 0.00001731
Iteration 171/1000 | Loss: 0.00001730
Iteration 172/1000 | Loss: 0.00001730
Iteration 173/1000 | Loss: 0.00001730
Iteration 174/1000 | Loss: 0.00001730
Iteration 175/1000 | Loss: 0.00001730
Iteration 176/1000 | Loss: 0.00001730
Iteration 177/1000 | Loss: 0.00001730
Iteration 178/1000 | Loss: 0.00001730
Iteration 179/1000 | Loss: 0.00001730
Iteration 180/1000 | Loss: 0.00001730
Iteration 181/1000 | Loss: 0.00001730
Iteration 182/1000 | Loss: 0.00003016
Iteration 183/1000 | Loss: 0.00002094
Iteration 184/1000 | Loss: 0.00008048
Iteration 185/1000 | Loss: 0.00002733
Iteration 186/1000 | Loss: 0.00001853
Iteration 187/1000 | Loss: 0.00002243
Iteration 188/1000 | Loss: 0.00001772
Iteration 189/1000 | Loss: 0.00001727
Iteration 190/1000 | Loss: 0.00001727
Iteration 191/1000 | Loss: 0.00001726
Iteration 192/1000 | Loss: 0.00001725
Iteration 193/1000 | Loss: 0.00001724
Iteration 194/1000 | Loss: 0.00001724
Iteration 195/1000 | Loss: 0.00002026
Iteration 196/1000 | Loss: 0.00001742
Iteration 197/1000 | Loss: 0.00001721
Iteration 198/1000 | Loss: 0.00001721
Iteration 199/1000 | Loss: 0.00001721
Iteration 200/1000 | Loss: 0.00001721
Iteration 201/1000 | Loss: 0.00001721
Iteration 202/1000 | Loss: 0.00001721
Iteration 203/1000 | Loss: 0.00001721
Iteration 204/1000 | Loss: 0.00001721
Iteration 205/1000 | Loss: 0.00001721
Iteration 206/1000 | Loss: 0.00001721
Iteration 207/1000 | Loss: 0.00001721
Iteration 208/1000 | Loss: 0.00001721
Iteration 209/1000 | Loss: 0.00001721
Iteration 210/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.7210797523148358e-05, 1.7210797523148358e-05, 1.7210797523148358e-05, 1.7210797523148358e-05, 1.7210797523148358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7210797523148358e-05

Optimization complete. Final v2v error: 3.3894855976104736 mm

Highest mean error: 5.757823944091797 mm for frame 72

Lowest mean error: 2.9217166900634766 mm for frame 81

Saving results

Total time: 262.0126516819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013235
Iteration 2/25 | Loss: 0.00301069
Iteration 3/25 | Loss: 0.00216617
Iteration 4/25 | Loss: 0.00211595
Iteration 5/25 | Loss: 0.00192083
Iteration 6/25 | Loss: 0.00175646
Iteration 7/25 | Loss: 0.00173304
Iteration 8/25 | Loss: 0.00168444
Iteration 9/25 | Loss: 0.00166282
Iteration 10/25 | Loss: 0.00163043
Iteration 11/25 | Loss: 0.00158973
Iteration 12/25 | Loss: 0.00158609
Iteration 13/25 | Loss: 0.00158320
Iteration 14/25 | Loss: 0.00157867
Iteration 15/25 | Loss: 0.00156795
Iteration 16/25 | Loss: 0.00156785
Iteration 17/25 | Loss: 0.00156749
Iteration 18/25 | Loss: 0.00156742
Iteration 19/25 | Loss: 0.00157066
Iteration 20/25 | Loss: 0.00156300
Iteration 21/25 | Loss: 0.00156231
Iteration 22/25 | Loss: 0.00156133
Iteration 23/25 | Loss: 0.00156229
Iteration 24/25 | Loss: 0.00156179
Iteration 25/25 | Loss: 0.00156004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42927182
Iteration 2/25 | Loss: 0.00299274
Iteration 3/25 | Loss: 0.00270107
Iteration 4/25 | Loss: 0.00270106
Iteration 5/25 | Loss: 0.00270106
Iteration 6/25 | Loss: 0.00270106
Iteration 7/25 | Loss: 0.00270106
Iteration 8/25 | Loss: 0.00270106
Iteration 9/25 | Loss: 0.00270106
Iteration 10/25 | Loss: 0.00270106
Iteration 11/25 | Loss: 0.00270106
Iteration 12/25 | Loss: 0.00270106
Iteration 13/25 | Loss: 0.00270106
Iteration 14/25 | Loss: 0.00270106
Iteration 15/25 | Loss: 0.00270106
Iteration 16/25 | Loss: 0.00270106
Iteration 17/25 | Loss: 0.00270106
Iteration 18/25 | Loss: 0.00270106
Iteration 19/25 | Loss: 0.00270106
Iteration 20/25 | Loss: 0.00270106
Iteration 21/25 | Loss: 0.00270106
Iteration 22/25 | Loss: 0.00270106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00270105991512537, 0.00270105991512537, 0.00270105991512537, 0.00270105991512537, 0.00270105991512537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00270105991512537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270106
Iteration 2/1000 | Loss: 0.00052026
Iteration 3/1000 | Loss: 0.00051562
Iteration 4/1000 | Loss: 0.00281814
Iteration 5/1000 | Loss: 0.00121556
Iteration 6/1000 | Loss: 0.00035146
Iteration 7/1000 | Loss: 0.00075551
Iteration 8/1000 | Loss: 0.00149016
Iteration 9/1000 | Loss: 0.00087500
Iteration 10/1000 | Loss: 0.00046335
Iteration 11/1000 | Loss: 0.00022902
Iteration 12/1000 | Loss: 0.00022201
Iteration 13/1000 | Loss: 0.00250877
Iteration 14/1000 | Loss: 0.00228733
Iteration 15/1000 | Loss: 0.00155371
Iteration 16/1000 | Loss: 0.00160562
Iteration 17/1000 | Loss: 0.00690613
Iteration 18/1000 | Loss: 0.00579610
Iteration 19/1000 | Loss: 0.00259385
Iteration 20/1000 | Loss: 0.00137124
Iteration 21/1000 | Loss: 0.00149077
Iteration 22/1000 | Loss: 0.00207215
Iteration 23/1000 | Loss: 0.00160096
Iteration 24/1000 | Loss: 0.00222367
Iteration 25/1000 | Loss: 0.00144392
Iteration 26/1000 | Loss: 0.00136898
Iteration 27/1000 | Loss: 0.00093093
Iteration 28/1000 | Loss: 0.00044900
Iteration 29/1000 | Loss: 0.00014178
Iteration 30/1000 | Loss: 0.00051152
Iteration 31/1000 | Loss: 0.00030002
Iteration 32/1000 | Loss: 0.00022707
Iteration 33/1000 | Loss: 0.00008903
Iteration 34/1000 | Loss: 0.00108137
Iteration 35/1000 | Loss: 0.00055208
Iteration 36/1000 | Loss: 0.00034009
Iteration 37/1000 | Loss: 0.00109369
Iteration 38/1000 | Loss: 0.00011488
Iteration 39/1000 | Loss: 0.00007925
Iteration 40/1000 | Loss: 0.00050954
Iteration 41/1000 | Loss: 0.00046715
Iteration 42/1000 | Loss: 0.00056466
Iteration 43/1000 | Loss: 0.00029398
Iteration 44/1000 | Loss: 0.00046728
Iteration 45/1000 | Loss: 0.00018141
Iteration 46/1000 | Loss: 0.00029165
Iteration 47/1000 | Loss: 0.00083033
Iteration 48/1000 | Loss: 0.00033053
Iteration 49/1000 | Loss: 0.00044458
Iteration 50/1000 | Loss: 0.00080914
Iteration 51/1000 | Loss: 0.00008302
Iteration 52/1000 | Loss: 0.00006985
Iteration 53/1000 | Loss: 0.00076305
Iteration 54/1000 | Loss: 0.00030574
Iteration 55/1000 | Loss: 0.00062074
Iteration 56/1000 | Loss: 0.00085747
Iteration 57/1000 | Loss: 0.00043353
Iteration 58/1000 | Loss: 0.00057072
Iteration 59/1000 | Loss: 0.00065521
Iteration 60/1000 | Loss: 0.00055399
Iteration 61/1000 | Loss: 0.00033513
Iteration 62/1000 | Loss: 0.00018988
Iteration 63/1000 | Loss: 0.00015939
Iteration 64/1000 | Loss: 0.00011927
Iteration 65/1000 | Loss: 0.00018668
Iteration 66/1000 | Loss: 0.00094689
Iteration 67/1000 | Loss: 0.00041651
Iteration 68/1000 | Loss: 0.00069868
Iteration 69/1000 | Loss: 0.00041906
Iteration 70/1000 | Loss: 0.00007207
Iteration 71/1000 | Loss: 0.00016439
Iteration 72/1000 | Loss: 0.00008890
Iteration 73/1000 | Loss: 0.00047481
Iteration 74/1000 | Loss: 0.00143780
Iteration 75/1000 | Loss: 0.00064801
Iteration 76/1000 | Loss: 0.00035462
Iteration 77/1000 | Loss: 0.00032077
Iteration 78/1000 | Loss: 0.00024094
Iteration 79/1000 | Loss: 0.00023315
Iteration 80/1000 | Loss: 0.00024280
Iteration 81/1000 | Loss: 0.00010545
Iteration 82/1000 | Loss: 0.00005057
Iteration 83/1000 | Loss: 0.00043068
Iteration 84/1000 | Loss: 0.00007668
Iteration 85/1000 | Loss: 0.00010438
Iteration 86/1000 | Loss: 0.00012723
Iteration 87/1000 | Loss: 0.00004291
Iteration 88/1000 | Loss: 0.00016846
Iteration 89/1000 | Loss: 0.00004503
Iteration 90/1000 | Loss: 0.00004837
Iteration 91/1000 | Loss: 0.00044324
Iteration 92/1000 | Loss: 0.00024480
Iteration 93/1000 | Loss: 0.00069679
Iteration 94/1000 | Loss: 0.00061819
Iteration 95/1000 | Loss: 0.00060611
Iteration 96/1000 | Loss: 0.00012262
Iteration 97/1000 | Loss: 0.00005037
Iteration 98/1000 | Loss: 0.00005982
Iteration 99/1000 | Loss: 0.00004300
Iteration 100/1000 | Loss: 0.00013097
Iteration 101/1000 | Loss: 0.00034925
Iteration 102/1000 | Loss: 0.00032829
Iteration 103/1000 | Loss: 0.00032797
Iteration 104/1000 | Loss: 0.00037934
Iteration 105/1000 | Loss: 0.00017213
Iteration 106/1000 | Loss: 0.00023678
Iteration 107/1000 | Loss: 0.00015762
Iteration 108/1000 | Loss: 0.00024089
Iteration 109/1000 | Loss: 0.00007057
Iteration 110/1000 | Loss: 0.00004653
Iteration 111/1000 | Loss: 0.00003240
Iteration 112/1000 | Loss: 0.00003926
Iteration 113/1000 | Loss: 0.00003008
Iteration 114/1000 | Loss: 0.00006772
Iteration 115/1000 | Loss: 0.00002880
Iteration 116/1000 | Loss: 0.00010893
Iteration 117/1000 | Loss: 0.00006919
Iteration 118/1000 | Loss: 0.00051097
Iteration 119/1000 | Loss: 0.00018585
Iteration 120/1000 | Loss: 0.00006526
Iteration 121/1000 | Loss: 0.00003387
Iteration 122/1000 | Loss: 0.00002818
Iteration 123/1000 | Loss: 0.00016192
Iteration 124/1000 | Loss: 0.00006460
Iteration 125/1000 | Loss: 0.00002796
Iteration 126/1000 | Loss: 0.00016851
Iteration 127/1000 | Loss: 0.00005679
Iteration 128/1000 | Loss: 0.00002947
Iteration 129/1000 | Loss: 0.00012571
Iteration 130/1000 | Loss: 0.00004253
Iteration 131/1000 | Loss: 0.00004203
Iteration 132/1000 | Loss: 0.00022818
Iteration 133/1000 | Loss: 0.00003041
Iteration 134/1000 | Loss: 0.00003274
Iteration 135/1000 | Loss: 0.00018221
Iteration 136/1000 | Loss: 0.00020649
Iteration 137/1000 | Loss: 0.00019733
Iteration 138/1000 | Loss: 0.00006148
Iteration 139/1000 | Loss: 0.00030858
Iteration 140/1000 | Loss: 0.00009268
Iteration 141/1000 | Loss: 0.00009777
Iteration 142/1000 | Loss: 0.00002830
Iteration 143/1000 | Loss: 0.00002787
Iteration 144/1000 | Loss: 0.00006080
Iteration 145/1000 | Loss: 0.00003064
Iteration 146/1000 | Loss: 0.00003294
Iteration 147/1000 | Loss: 0.00002704
Iteration 148/1000 | Loss: 0.00002698
Iteration 149/1000 | Loss: 0.00008255
Iteration 150/1000 | Loss: 0.00050661
Iteration 151/1000 | Loss: 0.00007291
Iteration 152/1000 | Loss: 0.00002654
Iteration 153/1000 | Loss: 0.00006290
Iteration 154/1000 | Loss: 0.00002554
Iteration 155/1000 | Loss: 0.00004509
Iteration 156/1000 | Loss: 0.00002471
Iteration 157/1000 | Loss: 0.00005985
Iteration 158/1000 | Loss: 0.00037780
Iteration 159/1000 | Loss: 0.00226689
Iteration 160/1000 | Loss: 0.00030830
Iteration 161/1000 | Loss: 0.00059244
Iteration 162/1000 | Loss: 0.00027976
Iteration 163/1000 | Loss: 0.00033851
Iteration 164/1000 | Loss: 0.00024667
Iteration 165/1000 | Loss: 0.00019325
Iteration 166/1000 | Loss: 0.00024302
Iteration 167/1000 | Loss: 0.00021994
Iteration 168/1000 | Loss: 0.00025443
Iteration 169/1000 | Loss: 0.00039442
Iteration 170/1000 | Loss: 0.00013282
Iteration 171/1000 | Loss: 0.00030667
Iteration 172/1000 | Loss: 0.00029546
Iteration 173/1000 | Loss: 0.00002791
Iteration 174/1000 | Loss: 0.00002490
Iteration 175/1000 | Loss: 0.00002530
Iteration 176/1000 | Loss: 0.00002448
Iteration 177/1000 | Loss: 0.00002416
Iteration 178/1000 | Loss: 0.00002416
Iteration 179/1000 | Loss: 0.00002416
Iteration 180/1000 | Loss: 0.00002416
Iteration 181/1000 | Loss: 0.00002416
Iteration 182/1000 | Loss: 0.00002416
Iteration 183/1000 | Loss: 0.00002416
Iteration 184/1000 | Loss: 0.00002416
Iteration 185/1000 | Loss: 0.00002415
Iteration 186/1000 | Loss: 0.00002414
Iteration 187/1000 | Loss: 0.00027952
Iteration 188/1000 | Loss: 0.00011510
Iteration 189/1000 | Loss: 0.00018925
Iteration 190/1000 | Loss: 0.00004731
Iteration 191/1000 | Loss: 0.00011924
Iteration 192/1000 | Loss: 0.00002511
Iteration 193/1000 | Loss: 0.00003611
Iteration 194/1000 | Loss: 0.00002930
Iteration 195/1000 | Loss: 0.00002890
Iteration 196/1000 | Loss: 0.00002595
Iteration 197/1000 | Loss: 0.00002635
Iteration 198/1000 | Loss: 0.00002272
Iteration 199/1000 | Loss: 0.00002263
Iteration 200/1000 | Loss: 0.00002262
Iteration 201/1000 | Loss: 0.00002262
Iteration 202/1000 | Loss: 0.00002262
Iteration 203/1000 | Loss: 0.00004062
Iteration 204/1000 | Loss: 0.00002252
Iteration 205/1000 | Loss: 0.00002246
Iteration 206/1000 | Loss: 0.00002246
Iteration 207/1000 | Loss: 0.00002246
Iteration 208/1000 | Loss: 0.00002246
Iteration 209/1000 | Loss: 0.00002245
Iteration 210/1000 | Loss: 0.00002245
Iteration 211/1000 | Loss: 0.00002245
Iteration 212/1000 | Loss: 0.00002245
Iteration 213/1000 | Loss: 0.00002245
Iteration 214/1000 | Loss: 0.00002245
Iteration 215/1000 | Loss: 0.00002244
Iteration 216/1000 | Loss: 0.00002243
Iteration 217/1000 | Loss: 0.00002303
Iteration 218/1000 | Loss: 0.00002242
Iteration 219/1000 | Loss: 0.00002240
Iteration 220/1000 | Loss: 0.00002240
Iteration 221/1000 | Loss: 0.00003703
Iteration 222/1000 | Loss: 0.00006784
Iteration 223/1000 | Loss: 0.00007025
Iteration 224/1000 | Loss: 0.00002301
Iteration 225/1000 | Loss: 0.00002235
Iteration 226/1000 | Loss: 0.00002235
Iteration 227/1000 | Loss: 0.00002234
Iteration 228/1000 | Loss: 0.00002234
Iteration 229/1000 | Loss: 0.00002234
Iteration 230/1000 | Loss: 0.00002234
Iteration 231/1000 | Loss: 0.00002234
Iteration 232/1000 | Loss: 0.00002233
Iteration 233/1000 | Loss: 0.00002233
Iteration 234/1000 | Loss: 0.00002233
Iteration 235/1000 | Loss: 0.00002233
Iteration 236/1000 | Loss: 0.00002233
Iteration 237/1000 | Loss: 0.00002233
Iteration 238/1000 | Loss: 0.00002233
Iteration 239/1000 | Loss: 0.00002233
Iteration 240/1000 | Loss: 0.00002233
Iteration 241/1000 | Loss: 0.00002233
Iteration 242/1000 | Loss: 0.00002233
Iteration 243/1000 | Loss: 0.00002233
Iteration 244/1000 | Loss: 0.00002233
Iteration 245/1000 | Loss: 0.00004445
Iteration 246/1000 | Loss: 0.00004133
Iteration 247/1000 | Loss: 0.00003777
Iteration 248/1000 | Loss: 0.00004701
Iteration 249/1000 | Loss: 0.00002976
Iteration 250/1000 | Loss: 0.00003652
Iteration 251/1000 | Loss: 0.00002227
Iteration 252/1000 | Loss: 0.00002227
Iteration 253/1000 | Loss: 0.00002227
Iteration 254/1000 | Loss: 0.00002226
Iteration 255/1000 | Loss: 0.00002226
Iteration 256/1000 | Loss: 0.00002226
Iteration 257/1000 | Loss: 0.00002225
Iteration 258/1000 | Loss: 0.00002225
Iteration 259/1000 | Loss: 0.00002225
Iteration 260/1000 | Loss: 0.00002225
Iteration 261/1000 | Loss: 0.00002225
Iteration 262/1000 | Loss: 0.00002225
Iteration 263/1000 | Loss: 0.00002673
Iteration 264/1000 | Loss: 0.00002336
Iteration 265/1000 | Loss: 0.00002225
Iteration 266/1000 | Loss: 0.00002225
Iteration 267/1000 | Loss: 0.00002225
Iteration 268/1000 | Loss: 0.00002225
Iteration 269/1000 | Loss: 0.00002224
Iteration 270/1000 | Loss: 0.00002972
Iteration 271/1000 | Loss: 0.00003093
Iteration 272/1000 | Loss: 0.00002283
Iteration 273/1000 | Loss: 0.00002420
Iteration 274/1000 | Loss: 0.00002420
Iteration 275/1000 | Loss: 0.00006462
Iteration 276/1000 | Loss: 0.00002533
Iteration 277/1000 | Loss: 0.00002228
Iteration 278/1000 | Loss: 0.00002227
Iteration 279/1000 | Loss: 0.00002227
Iteration 280/1000 | Loss: 0.00002226
Iteration 281/1000 | Loss: 0.00002226
Iteration 282/1000 | Loss: 0.00002225
Iteration 283/1000 | Loss: 0.00002376
Iteration 284/1000 | Loss: 0.00002226
Iteration 285/1000 | Loss: 0.00002218
Iteration 286/1000 | Loss: 0.00002218
Iteration 287/1000 | Loss: 0.00002218
Iteration 288/1000 | Loss: 0.00002218
Iteration 289/1000 | Loss: 0.00002217
Iteration 290/1000 | Loss: 0.00002217
Iteration 291/1000 | Loss: 0.00002217
Iteration 292/1000 | Loss: 0.00002217
Iteration 293/1000 | Loss: 0.00002217
Iteration 294/1000 | Loss: 0.00002217
Iteration 295/1000 | Loss: 0.00002217
Iteration 296/1000 | Loss: 0.00002217
Iteration 297/1000 | Loss: 0.00002217
Iteration 298/1000 | Loss: 0.00002225
Iteration 299/1000 | Loss: 0.00002907
Iteration 300/1000 | Loss: 0.00002404
Iteration 301/1000 | Loss: 0.00002360
Iteration 302/1000 | Loss: 0.00002294
Iteration 303/1000 | Loss: 0.00002219
Iteration 304/1000 | Loss: 0.00002219
Iteration 305/1000 | Loss: 0.00002218
Iteration 306/1000 | Loss: 0.00002218
Iteration 307/1000 | Loss: 0.00002246
Iteration 308/1000 | Loss: 0.00002250
Iteration 309/1000 | Loss: 0.00002216
Iteration 310/1000 | Loss: 0.00002216
Iteration 311/1000 | Loss: 0.00002215
Iteration 312/1000 | Loss: 0.00002215
Iteration 313/1000 | Loss: 0.00002215
Iteration 314/1000 | Loss: 0.00002215
Iteration 315/1000 | Loss: 0.00002215
Iteration 316/1000 | Loss: 0.00002215
Iteration 317/1000 | Loss: 0.00002215
Iteration 318/1000 | Loss: 0.00002215
Iteration 319/1000 | Loss: 0.00002215
Iteration 320/1000 | Loss: 0.00002215
Iteration 321/1000 | Loss: 0.00002215
Iteration 322/1000 | Loss: 0.00002215
Iteration 323/1000 | Loss: 0.00002215
Iteration 324/1000 | Loss: 0.00002215
Iteration 325/1000 | Loss: 0.00002215
Iteration 326/1000 | Loss: 0.00002215
Iteration 327/1000 | Loss: 0.00002215
Iteration 328/1000 | Loss: 0.00002215
Iteration 329/1000 | Loss: 0.00002215
Iteration 330/1000 | Loss: 0.00002215
Iteration 331/1000 | Loss: 0.00002215
Iteration 332/1000 | Loss: 0.00002215
Iteration 333/1000 | Loss: 0.00002215
Iteration 334/1000 | Loss: 0.00002215
Iteration 335/1000 | Loss: 0.00002215
Iteration 336/1000 | Loss: 0.00002215
Iteration 337/1000 | Loss: 0.00002215
Iteration 338/1000 | Loss: 0.00002215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.214652340626344e-05, 2.214652340626344e-05, 2.214652340626344e-05, 2.214652340626344e-05, 2.214652340626344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.214652340626344e-05

Optimization complete. Final v2v error: 3.7052838802337646 mm

Highest mean error: 10.41508960723877 mm for frame 90

Lowest mean error: 2.9773740768432617 mm for frame 2

Saving results

Total time: 410.1382637023926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443228
Iteration 2/25 | Loss: 0.00131507
Iteration 3/25 | Loss: 0.00124120
Iteration 4/25 | Loss: 0.00122261
Iteration 5/25 | Loss: 0.00121610
Iteration 6/25 | Loss: 0.00121503
Iteration 7/25 | Loss: 0.00121503
Iteration 8/25 | Loss: 0.00121503
Iteration 9/25 | Loss: 0.00121503
Iteration 10/25 | Loss: 0.00121502
Iteration 11/25 | Loss: 0.00121502
Iteration 12/25 | Loss: 0.00121502
Iteration 13/25 | Loss: 0.00121502
Iteration 14/25 | Loss: 0.00121502
Iteration 15/25 | Loss: 0.00121502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012150168186053634, 0.0012150168186053634, 0.0012150168186053634, 0.0012150168186053634, 0.0012150168186053634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012150168186053634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51498854
Iteration 2/25 | Loss: 0.00076651
Iteration 3/25 | Loss: 0.00076651
Iteration 4/25 | Loss: 0.00076651
Iteration 5/25 | Loss: 0.00076651
Iteration 6/25 | Loss: 0.00076651
Iteration 7/25 | Loss: 0.00076651
Iteration 8/25 | Loss: 0.00076651
Iteration 9/25 | Loss: 0.00076651
Iteration 10/25 | Loss: 0.00076651
Iteration 11/25 | Loss: 0.00076651
Iteration 12/25 | Loss: 0.00076651
Iteration 13/25 | Loss: 0.00076651
Iteration 14/25 | Loss: 0.00076651
Iteration 15/25 | Loss: 0.00076651
Iteration 16/25 | Loss: 0.00076651
Iteration 17/25 | Loss: 0.00076651
Iteration 18/25 | Loss: 0.00076651
Iteration 19/25 | Loss: 0.00076651
Iteration 20/25 | Loss: 0.00076651
Iteration 21/25 | Loss: 0.00076651
Iteration 22/25 | Loss: 0.00076651
Iteration 23/25 | Loss: 0.00076651
Iteration 24/25 | Loss: 0.00076651
Iteration 25/25 | Loss: 0.00076651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000766508630476892, 0.000766508630476892, 0.000766508630476892, 0.000766508630476892, 0.000766508630476892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000766508630476892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076651
Iteration 2/1000 | Loss: 0.00002972
Iteration 3/1000 | Loss: 0.00002069
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001733
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00001670
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001586
Iteration 15/1000 | Loss: 0.00001583
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001575
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001571
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001569
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001558
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001557
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001537
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001533
Iteration 49/1000 | Loss: 0.00001533
Iteration 50/1000 | Loss: 0.00001532
Iteration 51/1000 | Loss: 0.00001532
Iteration 52/1000 | Loss: 0.00001532
Iteration 53/1000 | Loss: 0.00001532
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001529
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001523
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001519
Iteration 85/1000 | Loss: 0.00001519
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001511
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001503
Iteration 135/1000 | Loss: 0.00001503
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001501
Iteration 151/1000 | Loss: 0.00001501
Iteration 152/1000 | Loss: 0.00001501
Iteration 153/1000 | Loss: 0.00001501
Iteration 154/1000 | Loss: 0.00001500
Iteration 155/1000 | Loss: 0.00001500
Iteration 156/1000 | Loss: 0.00001500
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001500
Iteration 163/1000 | Loss: 0.00001500
Iteration 164/1000 | Loss: 0.00001500
Iteration 165/1000 | Loss: 0.00001500
Iteration 166/1000 | Loss: 0.00001499
Iteration 167/1000 | Loss: 0.00001499
Iteration 168/1000 | Loss: 0.00001499
Iteration 169/1000 | Loss: 0.00001499
Iteration 170/1000 | Loss: 0.00001499
Iteration 171/1000 | Loss: 0.00001499
Iteration 172/1000 | Loss: 0.00001499
Iteration 173/1000 | Loss: 0.00001499
Iteration 174/1000 | Loss: 0.00001499
Iteration 175/1000 | Loss: 0.00001499
Iteration 176/1000 | Loss: 0.00001499
Iteration 177/1000 | Loss: 0.00001499
Iteration 178/1000 | Loss: 0.00001499
Iteration 179/1000 | Loss: 0.00001499
Iteration 180/1000 | Loss: 0.00001498
Iteration 181/1000 | Loss: 0.00001498
Iteration 182/1000 | Loss: 0.00001498
Iteration 183/1000 | Loss: 0.00001498
Iteration 184/1000 | Loss: 0.00001498
Iteration 185/1000 | Loss: 0.00001498
Iteration 186/1000 | Loss: 0.00001498
Iteration 187/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.4984572771936655e-05, 1.4984572771936655e-05, 1.4984572771936655e-05, 1.4984572771936655e-05, 1.4984572771936655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4984572771936655e-05

Optimization complete. Final v2v error: 3.2828917503356934 mm

Highest mean error: 3.8173797130584717 mm for frame 77

Lowest mean error: 3.135148048400879 mm for frame 30

Saving results

Total time: 39.51998996734619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408637
Iteration 2/25 | Loss: 0.00124795
Iteration 3/25 | Loss: 0.00119830
Iteration 4/25 | Loss: 0.00118938
Iteration 5/25 | Loss: 0.00118642
Iteration 6/25 | Loss: 0.00118607
Iteration 7/25 | Loss: 0.00118607
Iteration 8/25 | Loss: 0.00118607
Iteration 9/25 | Loss: 0.00118607
Iteration 10/25 | Loss: 0.00118607
Iteration 11/25 | Loss: 0.00118607
Iteration 12/25 | Loss: 0.00118607
Iteration 13/25 | Loss: 0.00118607
Iteration 14/25 | Loss: 0.00118607
Iteration 15/25 | Loss: 0.00118607
Iteration 16/25 | Loss: 0.00118607
Iteration 17/25 | Loss: 0.00118607
Iteration 18/25 | Loss: 0.00118607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011860744562000036, 0.0011860744562000036, 0.0011860744562000036, 0.0011860744562000036, 0.0011860744562000036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011860744562000036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.91391039
Iteration 2/25 | Loss: 0.00076381
Iteration 3/25 | Loss: 0.00076380
Iteration 4/25 | Loss: 0.00076380
Iteration 5/25 | Loss: 0.00076380
Iteration 6/25 | Loss: 0.00076380
Iteration 7/25 | Loss: 0.00076380
Iteration 8/25 | Loss: 0.00076380
Iteration 9/25 | Loss: 0.00076380
Iteration 10/25 | Loss: 0.00076380
Iteration 11/25 | Loss: 0.00076380
Iteration 12/25 | Loss: 0.00076380
Iteration 13/25 | Loss: 0.00076380
Iteration 14/25 | Loss: 0.00076380
Iteration 15/25 | Loss: 0.00076380
Iteration 16/25 | Loss: 0.00076380
Iteration 17/25 | Loss: 0.00076380
Iteration 18/25 | Loss: 0.00076380
Iteration 19/25 | Loss: 0.00076380
Iteration 20/25 | Loss: 0.00076380
Iteration 21/25 | Loss: 0.00076380
Iteration 22/25 | Loss: 0.00076380
Iteration 23/25 | Loss: 0.00076380
Iteration 24/25 | Loss: 0.00076380
Iteration 25/25 | Loss: 0.00076380

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076380
Iteration 2/1000 | Loss: 0.00002388
Iteration 3/1000 | Loss: 0.00001790
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001315
Iteration 12/1000 | Loss: 0.00001305
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001274
Iteration 21/1000 | Loss: 0.00001273
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001257
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001252
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001242
Iteration 55/1000 | Loss: 0.00001242
Iteration 56/1000 | Loss: 0.00001242
Iteration 57/1000 | Loss: 0.00001242
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001225
Iteration 113/1000 | Loss: 0.00001225
Iteration 114/1000 | Loss: 0.00001225
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001223
Iteration 119/1000 | Loss: 0.00001223
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001222
Iteration 123/1000 | Loss: 0.00001222
Iteration 124/1000 | Loss: 0.00001221
Iteration 125/1000 | Loss: 0.00001221
Iteration 126/1000 | Loss: 0.00001221
Iteration 127/1000 | Loss: 0.00001221
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001220
Iteration 131/1000 | Loss: 0.00001220
Iteration 132/1000 | Loss: 0.00001220
Iteration 133/1000 | Loss: 0.00001220
Iteration 134/1000 | Loss: 0.00001220
Iteration 135/1000 | Loss: 0.00001220
Iteration 136/1000 | Loss: 0.00001220
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001219
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001217
Iteration 144/1000 | Loss: 0.00001217
Iteration 145/1000 | Loss: 0.00001217
Iteration 146/1000 | Loss: 0.00001217
Iteration 147/1000 | Loss: 0.00001217
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001216
Iteration 155/1000 | Loss: 0.00001216
Iteration 156/1000 | Loss: 0.00001216
Iteration 157/1000 | Loss: 0.00001216
Iteration 158/1000 | Loss: 0.00001216
Iteration 159/1000 | Loss: 0.00001216
Iteration 160/1000 | Loss: 0.00001216
Iteration 161/1000 | Loss: 0.00001216
Iteration 162/1000 | Loss: 0.00001216
Iteration 163/1000 | Loss: 0.00001216
Iteration 164/1000 | Loss: 0.00001215
Iteration 165/1000 | Loss: 0.00001215
Iteration 166/1000 | Loss: 0.00001215
Iteration 167/1000 | Loss: 0.00001215
Iteration 168/1000 | Loss: 0.00001215
Iteration 169/1000 | Loss: 0.00001215
Iteration 170/1000 | Loss: 0.00001215
Iteration 171/1000 | Loss: 0.00001215
Iteration 172/1000 | Loss: 0.00001215
Iteration 173/1000 | Loss: 0.00001215
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001214
Iteration 176/1000 | Loss: 0.00001214
Iteration 177/1000 | Loss: 0.00001214
Iteration 178/1000 | Loss: 0.00001214
Iteration 179/1000 | Loss: 0.00001214
Iteration 180/1000 | Loss: 0.00001214
Iteration 181/1000 | Loss: 0.00001214
Iteration 182/1000 | Loss: 0.00001214
Iteration 183/1000 | Loss: 0.00001214
Iteration 184/1000 | Loss: 0.00001214
Iteration 185/1000 | Loss: 0.00001214
Iteration 186/1000 | Loss: 0.00001214
Iteration 187/1000 | Loss: 0.00001214
Iteration 188/1000 | Loss: 0.00001214
Iteration 189/1000 | Loss: 0.00001213
Iteration 190/1000 | Loss: 0.00001213
Iteration 191/1000 | Loss: 0.00001213
Iteration 192/1000 | Loss: 0.00001213
Iteration 193/1000 | Loss: 0.00001213
Iteration 194/1000 | Loss: 0.00001213
Iteration 195/1000 | Loss: 0.00001213
Iteration 196/1000 | Loss: 0.00001213
Iteration 197/1000 | Loss: 0.00001213
Iteration 198/1000 | Loss: 0.00001213
Iteration 199/1000 | Loss: 0.00001213
Iteration 200/1000 | Loss: 0.00001213
Iteration 201/1000 | Loss: 0.00001213
Iteration 202/1000 | Loss: 0.00001212
Iteration 203/1000 | Loss: 0.00001212
Iteration 204/1000 | Loss: 0.00001212
Iteration 205/1000 | Loss: 0.00001212
Iteration 206/1000 | Loss: 0.00001212
Iteration 207/1000 | Loss: 0.00001212
Iteration 208/1000 | Loss: 0.00001212
Iteration 209/1000 | Loss: 0.00001212
Iteration 210/1000 | Loss: 0.00001212
Iteration 211/1000 | Loss: 0.00001212
Iteration 212/1000 | Loss: 0.00001212
Iteration 213/1000 | Loss: 0.00001212
Iteration 214/1000 | Loss: 0.00001212
Iteration 215/1000 | Loss: 0.00001212
Iteration 216/1000 | Loss: 0.00001212
Iteration 217/1000 | Loss: 0.00001212
Iteration 218/1000 | Loss: 0.00001212
Iteration 219/1000 | Loss: 0.00001212
Iteration 220/1000 | Loss: 0.00001212
Iteration 221/1000 | Loss: 0.00001211
Iteration 222/1000 | Loss: 0.00001211
Iteration 223/1000 | Loss: 0.00001211
Iteration 224/1000 | Loss: 0.00001211
Iteration 225/1000 | Loss: 0.00001211
Iteration 226/1000 | Loss: 0.00001211
Iteration 227/1000 | Loss: 0.00001211
Iteration 228/1000 | Loss: 0.00001211
Iteration 229/1000 | Loss: 0.00001211
Iteration 230/1000 | Loss: 0.00001211
Iteration 231/1000 | Loss: 0.00001211
Iteration 232/1000 | Loss: 0.00001211
Iteration 233/1000 | Loss: 0.00001211
Iteration 234/1000 | Loss: 0.00001211
Iteration 235/1000 | Loss: 0.00001211
Iteration 236/1000 | Loss: 0.00001211
Iteration 237/1000 | Loss: 0.00001211
Iteration 238/1000 | Loss: 0.00001211
Iteration 239/1000 | Loss: 0.00001211
Iteration 240/1000 | Loss: 0.00001211
Iteration 241/1000 | Loss: 0.00001210
Iteration 242/1000 | Loss: 0.00001210
Iteration 243/1000 | Loss: 0.00001210
Iteration 244/1000 | Loss: 0.00001210
Iteration 245/1000 | Loss: 0.00001210
Iteration 246/1000 | Loss: 0.00001210
Iteration 247/1000 | Loss: 0.00001210
Iteration 248/1000 | Loss: 0.00001210
Iteration 249/1000 | Loss: 0.00001210
Iteration 250/1000 | Loss: 0.00001210
Iteration 251/1000 | Loss: 0.00001210
Iteration 252/1000 | Loss: 0.00001210
Iteration 253/1000 | Loss: 0.00001210
Iteration 254/1000 | Loss: 0.00001210
Iteration 255/1000 | Loss: 0.00001210
Iteration 256/1000 | Loss: 0.00001210
Iteration 257/1000 | Loss: 0.00001210
Iteration 258/1000 | Loss: 0.00001210
Iteration 259/1000 | Loss: 0.00001210
Iteration 260/1000 | Loss: 0.00001210
Iteration 261/1000 | Loss: 0.00001210
Iteration 262/1000 | Loss: 0.00001210
Iteration 263/1000 | Loss: 0.00001210
Iteration 264/1000 | Loss: 0.00001210
Iteration 265/1000 | Loss: 0.00001210
Iteration 266/1000 | Loss: 0.00001210
Iteration 267/1000 | Loss: 0.00001210
Iteration 268/1000 | Loss: 0.00001210
Iteration 269/1000 | Loss: 0.00001210
Iteration 270/1000 | Loss: 0.00001210
Iteration 271/1000 | Loss: 0.00001210
Iteration 272/1000 | Loss: 0.00001210
Iteration 273/1000 | Loss: 0.00001210
Iteration 274/1000 | Loss: 0.00001210
Iteration 275/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.2097482795070391e-05, 1.2097482795070391e-05, 1.2097482795070391e-05, 1.2097482795070391e-05, 1.2097482795070391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2097482795070391e-05

Optimization complete. Final v2v error: 2.9733364582061768 mm

Highest mean error: 3.351454019546509 mm for frame 81

Lowest mean error: 2.7576451301574707 mm for frame 27

Saving results

Total time: 42.58898210525513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738511
Iteration 2/25 | Loss: 0.00155543
Iteration 3/25 | Loss: 0.00135387
Iteration 4/25 | Loss: 0.00133812
Iteration 5/25 | Loss: 0.00133586
Iteration 6/25 | Loss: 0.00133586
Iteration 7/25 | Loss: 0.00133586
Iteration 8/25 | Loss: 0.00133586
Iteration 9/25 | Loss: 0.00133586
Iteration 10/25 | Loss: 0.00133586
Iteration 11/25 | Loss: 0.00133586
Iteration 12/25 | Loss: 0.00133586
Iteration 13/25 | Loss: 0.00133586
Iteration 14/25 | Loss: 0.00133586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013358630239963531, 0.0013358630239963531, 0.0013358630239963531, 0.0013358630239963531, 0.0013358630239963531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013358630239963531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65862465
Iteration 2/25 | Loss: 0.00086324
Iteration 3/25 | Loss: 0.00086322
Iteration 4/25 | Loss: 0.00086322
Iteration 5/25 | Loss: 0.00086322
Iteration 6/25 | Loss: 0.00086322
Iteration 7/25 | Loss: 0.00086322
Iteration 8/25 | Loss: 0.00086322
Iteration 9/25 | Loss: 0.00086322
Iteration 10/25 | Loss: 0.00086322
Iteration 11/25 | Loss: 0.00086322
Iteration 12/25 | Loss: 0.00086322
Iteration 13/25 | Loss: 0.00086322
Iteration 14/25 | Loss: 0.00086322
Iteration 15/25 | Loss: 0.00086322
Iteration 16/25 | Loss: 0.00086322
Iteration 17/25 | Loss: 0.00086322
Iteration 18/25 | Loss: 0.00086322
Iteration 19/25 | Loss: 0.00086322
Iteration 20/25 | Loss: 0.00086322
Iteration 21/25 | Loss: 0.00086322
Iteration 22/25 | Loss: 0.00086322
Iteration 23/25 | Loss: 0.00086322
Iteration 24/25 | Loss: 0.00086322
Iteration 25/25 | Loss: 0.00086322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086322
Iteration 2/1000 | Loss: 0.00003590
Iteration 3/1000 | Loss: 0.00002740
Iteration 4/1000 | Loss: 0.00002471
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00002292
Iteration 7/1000 | Loss: 0.00002248
Iteration 8/1000 | Loss: 0.00002220
Iteration 9/1000 | Loss: 0.00002174
Iteration 10/1000 | Loss: 0.00002148
Iteration 11/1000 | Loss: 0.00002145
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002104
Iteration 15/1000 | Loss: 0.00002103
Iteration 16/1000 | Loss: 0.00002095
Iteration 17/1000 | Loss: 0.00002089
Iteration 18/1000 | Loss: 0.00002088
Iteration 19/1000 | Loss: 0.00002086
Iteration 20/1000 | Loss: 0.00002081
Iteration 21/1000 | Loss: 0.00002080
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002078
Iteration 24/1000 | Loss: 0.00002073
Iteration 25/1000 | Loss: 0.00002071
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002066
Iteration 28/1000 | Loss: 0.00002065
Iteration 29/1000 | Loss: 0.00002062
Iteration 30/1000 | Loss: 0.00002061
Iteration 31/1000 | Loss: 0.00002061
Iteration 32/1000 | Loss: 0.00002060
Iteration 33/1000 | Loss: 0.00002060
Iteration 34/1000 | Loss: 0.00002059
Iteration 35/1000 | Loss: 0.00002059
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002055
Iteration 40/1000 | Loss: 0.00002054
Iteration 41/1000 | Loss: 0.00002054
Iteration 42/1000 | Loss: 0.00002053
Iteration 43/1000 | Loss: 0.00002053
Iteration 44/1000 | Loss: 0.00002053
Iteration 45/1000 | Loss: 0.00002053
Iteration 46/1000 | Loss: 0.00002053
Iteration 47/1000 | Loss: 0.00002051
Iteration 48/1000 | Loss: 0.00002050
Iteration 49/1000 | Loss: 0.00002050
Iteration 50/1000 | Loss: 0.00002049
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002044
Iteration 55/1000 | Loss: 0.00002044
Iteration 56/1000 | Loss: 0.00002044
Iteration 57/1000 | Loss: 0.00002043
Iteration 58/1000 | Loss: 0.00002043
Iteration 59/1000 | Loss: 0.00002043
Iteration 60/1000 | Loss: 0.00002042
Iteration 61/1000 | Loss: 0.00002042
Iteration 62/1000 | Loss: 0.00002041
Iteration 63/1000 | Loss: 0.00002040
Iteration 64/1000 | Loss: 0.00002040
Iteration 65/1000 | Loss: 0.00002040
Iteration 66/1000 | Loss: 0.00002040
Iteration 67/1000 | Loss: 0.00002039
Iteration 68/1000 | Loss: 0.00002039
Iteration 69/1000 | Loss: 0.00002039
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002039
Iteration 72/1000 | Loss: 0.00002039
Iteration 73/1000 | Loss: 0.00002039
Iteration 74/1000 | Loss: 0.00002039
Iteration 75/1000 | Loss: 0.00002039
Iteration 76/1000 | Loss: 0.00002039
Iteration 77/1000 | Loss: 0.00002038
Iteration 78/1000 | Loss: 0.00002038
Iteration 79/1000 | Loss: 0.00002038
Iteration 80/1000 | Loss: 0.00002038
Iteration 81/1000 | Loss: 0.00002038
Iteration 82/1000 | Loss: 0.00002038
Iteration 83/1000 | Loss: 0.00002038
Iteration 84/1000 | Loss: 0.00002036
Iteration 85/1000 | Loss: 0.00002036
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002034
Iteration 90/1000 | Loss: 0.00002034
Iteration 91/1000 | Loss: 0.00002034
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002033
Iteration 94/1000 | Loss: 0.00002033
Iteration 95/1000 | Loss: 0.00002032
Iteration 96/1000 | Loss: 0.00002032
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002030
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002030
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002029
Iteration 109/1000 | Loss: 0.00002029
Iteration 110/1000 | Loss: 0.00002029
Iteration 111/1000 | Loss: 0.00002029
Iteration 112/1000 | Loss: 0.00002029
Iteration 113/1000 | Loss: 0.00002029
Iteration 114/1000 | Loss: 0.00002029
Iteration 115/1000 | Loss: 0.00002029
Iteration 116/1000 | Loss: 0.00002028
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002027
Iteration 119/1000 | Loss: 0.00002027
Iteration 120/1000 | Loss: 0.00002027
Iteration 121/1000 | Loss: 0.00002027
Iteration 122/1000 | Loss: 0.00002026
Iteration 123/1000 | Loss: 0.00002026
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002023
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002023
Iteration 132/1000 | Loss: 0.00002023
Iteration 133/1000 | Loss: 0.00002022
Iteration 134/1000 | Loss: 0.00002022
Iteration 135/1000 | Loss: 0.00002022
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002020
Iteration 138/1000 | Loss: 0.00002020
Iteration 139/1000 | Loss: 0.00002020
Iteration 140/1000 | Loss: 0.00002020
Iteration 141/1000 | Loss: 0.00002020
Iteration 142/1000 | Loss: 0.00002020
Iteration 143/1000 | Loss: 0.00002020
Iteration 144/1000 | Loss: 0.00002020
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00002019
Iteration 148/1000 | Loss: 0.00002019
Iteration 149/1000 | Loss: 0.00002019
Iteration 150/1000 | Loss: 0.00002019
Iteration 151/1000 | Loss: 0.00002019
Iteration 152/1000 | Loss: 0.00002018
Iteration 153/1000 | Loss: 0.00002018
Iteration 154/1000 | Loss: 0.00002017
Iteration 155/1000 | Loss: 0.00002017
Iteration 156/1000 | Loss: 0.00002017
Iteration 157/1000 | Loss: 0.00002017
Iteration 158/1000 | Loss: 0.00002016
Iteration 159/1000 | Loss: 0.00002016
Iteration 160/1000 | Loss: 0.00002016
Iteration 161/1000 | Loss: 0.00002016
Iteration 162/1000 | Loss: 0.00002016
Iteration 163/1000 | Loss: 0.00002016
Iteration 164/1000 | Loss: 0.00002016
Iteration 165/1000 | Loss: 0.00002016
Iteration 166/1000 | Loss: 0.00002016
Iteration 167/1000 | Loss: 0.00002016
Iteration 168/1000 | Loss: 0.00002015
Iteration 169/1000 | Loss: 0.00002015
Iteration 170/1000 | Loss: 0.00002015
Iteration 171/1000 | Loss: 0.00002015
Iteration 172/1000 | Loss: 0.00002015
Iteration 173/1000 | Loss: 0.00002015
Iteration 174/1000 | Loss: 0.00002015
Iteration 175/1000 | Loss: 0.00002015
Iteration 176/1000 | Loss: 0.00002014
Iteration 177/1000 | Loss: 0.00002014
Iteration 178/1000 | Loss: 0.00002014
Iteration 179/1000 | Loss: 0.00002014
Iteration 180/1000 | Loss: 0.00002013
Iteration 181/1000 | Loss: 0.00002013
Iteration 182/1000 | Loss: 0.00002013
Iteration 183/1000 | Loss: 0.00002013
Iteration 184/1000 | Loss: 0.00002013
Iteration 185/1000 | Loss: 0.00002013
Iteration 186/1000 | Loss: 0.00002013
Iteration 187/1000 | Loss: 0.00002013
Iteration 188/1000 | Loss: 0.00002012
Iteration 189/1000 | Loss: 0.00002012
Iteration 190/1000 | Loss: 0.00002012
Iteration 191/1000 | Loss: 0.00002012
Iteration 192/1000 | Loss: 0.00002012
Iteration 193/1000 | Loss: 0.00002011
Iteration 194/1000 | Loss: 0.00002011
Iteration 195/1000 | Loss: 0.00002011
Iteration 196/1000 | Loss: 0.00002011
Iteration 197/1000 | Loss: 0.00002011
Iteration 198/1000 | Loss: 0.00002011
Iteration 199/1000 | Loss: 0.00002011
Iteration 200/1000 | Loss: 0.00002011
Iteration 201/1000 | Loss: 0.00002011
Iteration 202/1000 | Loss: 0.00002011
Iteration 203/1000 | Loss: 0.00002011
Iteration 204/1000 | Loss: 0.00002011
Iteration 205/1000 | Loss: 0.00002011
Iteration 206/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.010632488236297e-05, 2.010632488236297e-05, 2.010632488236297e-05, 2.010632488236297e-05, 2.010632488236297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.010632488236297e-05

Optimization complete. Final v2v error: 3.670820474624634 mm

Highest mean error: 4.64609956741333 mm for frame 188

Lowest mean error: 3.0792076587677 mm for frame 16

Saving results

Total time: 50.277697801589966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987738
Iteration 2/25 | Loss: 0.00145365
Iteration 3/25 | Loss: 0.00131288
Iteration 4/25 | Loss: 0.00128997
Iteration 5/25 | Loss: 0.00127715
Iteration 6/25 | Loss: 0.00127694
Iteration 7/25 | Loss: 0.00126806
Iteration 8/25 | Loss: 0.00126972
Iteration 9/25 | Loss: 0.00126727
Iteration 10/25 | Loss: 0.00126600
Iteration 11/25 | Loss: 0.00126571
Iteration 12/25 | Loss: 0.00126564
Iteration 13/25 | Loss: 0.00126564
Iteration 14/25 | Loss: 0.00126564
Iteration 15/25 | Loss: 0.00126564
Iteration 16/25 | Loss: 0.00126564
Iteration 17/25 | Loss: 0.00126563
Iteration 18/25 | Loss: 0.00126563
Iteration 19/25 | Loss: 0.00126563
Iteration 20/25 | Loss: 0.00126563
Iteration 21/25 | Loss: 0.00126563
Iteration 22/25 | Loss: 0.00126563
Iteration 23/25 | Loss: 0.00126563
Iteration 24/25 | Loss: 0.00126563
Iteration 25/25 | Loss: 0.00126563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17835903
Iteration 2/25 | Loss: 0.00096021
Iteration 3/25 | Loss: 0.00093546
Iteration 4/25 | Loss: 0.00093545
Iteration 5/25 | Loss: 0.00093545
Iteration 6/25 | Loss: 0.00093545
Iteration 7/25 | Loss: 0.00093545
Iteration 8/25 | Loss: 0.00093545
Iteration 9/25 | Loss: 0.00093545
Iteration 10/25 | Loss: 0.00093545
Iteration 11/25 | Loss: 0.00093545
Iteration 12/25 | Loss: 0.00093545
Iteration 13/25 | Loss: 0.00093545
Iteration 14/25 | Loss: 0.00093545
Iteration 15/25 | Loss: 0.00093545
Iteration 16/25 | Loss: 0.00093545
Iteration 17/25 | Loss: 0.00093545
Iteration 18/25 | Loss: 0.00093545
Iteration 19/25 | Loss: 0.00093545
Iteration 20/25 | Loss: 0.00093545
Iteration 21/25 | Loss: 0.00093545
Iteration 22/25 | Loss: 0.00093545
Iteration 23/25 | Loss: 0.00093545
Iteration 24/25 | Loss: 0.00093545
Iteration 25/25 | Loss: 0.00093545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093545
Iteration 2/1000 | Loss: 0.00005968
Iteration 3/1000 | Loss: 0.00004991
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001722
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00003582
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00004572
Iteration 12/1000 | Loss: 0.00009394
Iteration 13/1000 | Loss: 0.00001543
Iteration 14/1000 | Loss: 0.00004953
Iteration 15/1000 | Loss: 0.00008972
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001505
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00005873
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001495
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001473
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001458
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001458
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001454
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001454
Iteration 106/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.4536251910612918e-05, 1.4536251910612918e-05, 1.4536251910612918e-05, 1.4536251910612918e-05, 1.4536251910612918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4536251910612918e-05

Optimization complete. Final v2v error: 3.207620143890381 mm

Highest mean error: 4.043898105621338 mm for frame 172

Lowest mean error: 2.9101850986480713 mm for frame 139

Saving results

Total time: 60.32516360282898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989221
Iteration 2/25 | Loss: 0.00328372
Iteration 3/25 | Loss: 0.00248467
Iteration 4/25 | Loss: 0.00217868
Iteration 5/25 | Loss: 0.00221533
Iteration 6/25 | Loss: 0.00200980
Iteration 7/25 | Loss: 0.00190097
Iteration 8/25 | Loss: 0.00178919
Iteration 9/25 | Loss: 0.00174928
Iteration 10/25 | Loss: 0.00172474
Iteration 11/25 | Loss: 0.00170514
Iteration 12/25 | Loss: 0.00170294
Iteration 13/25 | Loss: 0.00171387
Iteration 14/25 | Loss: 0.00169545
Iteration 15/25 | Loss: 0.00169205
Iteration 16/25 | Loss: 0.00169677
Iteration 17/25 | Loss: 0.00168868
Iteration 18/25 | Loss: 0.00169476
Iteration 19/25 | Loss: 0.00168072
Iteration 20/25 | Loss: 0.00167614
Iteration 21/25 | Loss: 0.00167436
Iteration 22/25 | Loss: 0.00167310
Iteration 23/25 | Loss: 0.00166753
Iteration 24/25 | Loss: 0.00166318
Iteration 25/25 | Loss: 0.00166202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45729733
Iteration 2/25 | Loss: 0.00592784
Iteration 3/25 | Loss: 0.00461595
Iteration 4/25 | Loss: 0.00461440
Iteration 5/25 | Loss: 0.00461440
Iteration 6/25 | Loss: 0.00461440
Iteration 7/25 | Loss: 0.00461440
Iteration 8/25 | Loss: 0.00461440
Iteration 9/25 | Loss: 0.00461440
Iteration 10/25 | Loss: 0.00461440
Iteration 11/25 | Loss: 0.00461440
Iteration 12/25 | Loss: 0.00461440
Iteration 13/25 | Loss: 0.00461440
Iteration 14/25 | Loss: 0.00461440
Iteration 15/25 | Loss: 0.00461440
Iteration 16/25 | Loss: 0.00461440
Iteration 17/25 | Loss: 0.00461440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004614397883415222, 0.004614397883415222, 0.004614397883415222, 0.004614397883415222, 0.004614397883415222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004614397883415222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00461440
Iteration 2/1000 | Loss: 0.00085513
Iteration 3/1000 | Loss: 0.00062057
Iteration 4/1000 | Loss: 0.00083793
Iteration 5/1000 | Loss: 0.00160877
Iteration 6/1000 | Loss: 0.00128534
Iteration 7/1000 | Loss: 0.00069711
Iteration 8/1000 | Loss: 0.00141314
Iteration 9/1000 | Loss: 0.00201327
Iteration 10/1000 | Loss: 0.00028196
Iteration 11/1000 | Loss: 0.00163255
Iteration 12/1000 | Loss: 0.00023716
Iteration 13/1000 | Loss: 0.00442011
Iteration 14/1000 | Loss: 0.00983647
Iteration 15/1000 | Loss: 0.00054359
Iteration 16/1000 | Loss: 0.00030295
Iteration 17/1000 | Loss: 0.00019051
Iteration 18/1000 | Loss: 0.00014946
Iteration 19/1000 | Loss: 0.00063122
Iteration 20/1000 | Loss: 0.00014018
Iteration 21/1000 | Loss: 0.00012417
Iteration 22/1000 | Loss: 0.00079945
Iteration 23/1000 | Loss: 0.00109812
Iteration 24/1000 | Loss: 0.00011814
Iteration 25/1000 | Loss: 0.00024113
Iteration 26/1000 | Loss: 0.00010324
Iteration 27/1000 | Loss: 0.00009610
Iteration 28/1000 | Loss: 0.00055078
Iteration 29/1000 | Loss: 0.00008776
Iteration 30/1000 | Loss: 0.00032555
Iteration 31/1000 | Loss: 0.00008358
Iteration 32/1000 | Loss: 0.00049912
Iteration 33/1000 | Loss: 0.00008173
Iteration 34/1000 | Loss: 0.00058829
Iteration 35/1000 | Loss: 0.00007851
Iteration 36/1000 | Loss: 0.00059094
Iteration 37/1000 | Loss: 0.00021275
Iteration 38/1000 | Loss: 0.00007146
Iteration 39/1000 | Loss: 0.00052495
Iteration 40/1000 | Loss: 0.00007527
Iteration 41/1000 | Loss: 0.00112879
Iteration 42/1000 | Loss: 0.00083754
Iteration 43/1000 | Loss: 0.00101198
Iteration 44/1000 | Loss: 0.00038960
Iteration 45/1000 | Loss: 0.00066884
Iteration 46/1000 | Loss: 0.00030992
Iteration 47/1000 | Loss: 0.00106206
Iteration 48/1000 | Loss: 0.00051600
Iteration 49/1000 | Loss: 0.00192041
Iteration 50/1000 | Loss: 0.00078366
Iteration 51/1000 | Loss: 0.00153331
Iteration 52/1000 | Loss: 0.00063201
Iteration 53/1000 | Loss: 0.00008147
Iteration 54/1000 | Loss: 0.00033056
Iteration 55/1000 | Loss: 0.00010224
Iteration 56/1000 | Loss: 0.00006403
Iteration 57/1000 | Loss: 0.00037092
Iteration 58/1000 | Loss: 0.00005968
Iteration 59/1000 | Loss: 0.00005740
Iteration 60/1000 | Loss: 0.00005629
Iteration 61/1000 | Loss: 0.00005523
Iteration 62/1000 | Loss: 0.00005432
Iteration 63/1000 | Loss: 0.00005365
Iteration 64/1000 | Loss: 0.00005308
Iteration 65/1000 | Loss: 0.00019239
Iteration 66/1000 | Loss: 0.00017407
Iteration 67/1000 | Loss: 0.00018632
Iteration 68/1000 | Loss: 0.00006123
Iteration 69/1000 | Loss: 0.00020999
Iteration 70/1000 | Loss: 0.00019453
Iteration 71/1000 | Loss: 0.00021491
Iteration 72/1000 | Loss: 0.00018371
Iteration 73/1000 | Loss: 0.00020908
Iteration 74/1000 | Loss: 0.00017045
Iteration 75/1000 | Loss: 0.00017392
Iteration 76/1000 | Loss: 0.00006676
Iteration 77/1000 | Loss: 0.00005743
Iteration 78/1000 | Loss: 0.00005310
Iteration 79/1000 | Loss: 0.00017938
Iteration 80/1000 | Loss: 0.00093546
Iteration 81/1000 | Loss: 0.00131964
Iteration 82/1000 | Loss: 0.00130039
Iteration 83/1000 | Loss: 0.00069000
Iteration 84/1000 | Loss: 0.00005974
Iteration 85/1000 | Loss: 0.00005248
Iteration 86/1000 | Loss: 0.00004948
Iteration 87/1000 | Loss: 0.00004772
Iteration 88/1000 | Loss: 0.00004582
Iteration 89/1000 | Loss: 0.00004511
Iteration 90/1000 | Loss: 0.00004463
Iteration 91/1000 | Loss: 0.00004425
Iteration 92/1000 | Loss: 0.00004397
Iteration 93/1000 | Loss: 0.00033830
Iteration 94/1000 | Loss: 0.00004608
Iteration 95/1000 | Loss: 0.00004365
Iteration 96/1000 | Loss: 0.00004265
Iteration 97/1000 | Loss: 0.00004218
Iteration 98/1000 | Loss: 0.00004177
Iteration 99/1000 | Loss: 0.00004154
Iteration 100/1000 | Loss: 0.00004148
Iteration 101/1000 | Loss: 0.00004148
Iteration 102/1000 | Loss: 0.00004147
Iteration 103/1000 | Loss: 0.00004146
Iteration 104/1000 | Loss: 0.00004142
Iteration 105/1000 | Loss: 0.00004141
Iteration 106/1000 | Loss: 0.00004140
Iteration 107/1000 | Loss: 0.00004137
Iteration 108/1000 | Loss: 0.00004137
Iteration 109/1000 | Loss: 0.00004137
Iteration 110/1000 | Loss: 0.00004131
Iteration 111/1000 | Loss: 0.00004130
Iteration 112/1000 | Loss: 0.00004129
Iteration 113/1000 | Loss: 0.00004127
Iteration 114/1000 | Loss: 0.00004122
Iteration 115/1000 | Loss: 0.00004119
Iteration 116/1000 | Loss: 0.00004110
Iteration 117/1000 | Loss: 0.00004103
Iteration 118/1000 | Loss: 0.00004103
Iteration 119/1000 | Loss: 0.00004100
Iteration 120/1000 | Loss: 0.00004100
Iteration 121/1000 | Loss: 0.00004098
Iteration 122/1000 | Loss: 0.00004098
Iteration 123/1000 | Loss: 0.00004098
Iteration 124/1000 | Loss: 0.00004098
Iteration 125/1000 | Loss: 0.00004097
Iteration 126/1000 | Loss: 0.00004097
Iteration 127/1000 | Loss: 0.00004097
Iteration 128/1000 | Loss: 0.00004097
Iteration 129/1000 | Loss: 0.00004097
Iteration 130/1000 | Loss: 0.00004097
Iteration 131/1000 | Loss: 0.00004097
Iteration 132/1000 | Loss: 0.00004096
Iteration 133/1000 | Loss: 0.00004096
Iteration 134/1000 | Loss: 0.00004096
Iteration 135/1000 | Loss: 0.00004095
Iteration 136/1000 | Loss: 0.00004095
Iteration 137/1000 | Loss: 0.00004095
Iteration 138/1000 | Loss: 0.00004094
Iteration 139/1000 | Loss: 0.00004094
Iteration 140/1000 | Loss: 0.00004094
Iteration 141/1000 | Loss: 0.00004093
Iteration 142/1000 | Loss: 0.00004093
Iteration 143/1000 | Loss: 0.00004093
Iteration 144/1000 | Loss: 0.00004093
Iteration 145/1000 | Loss: 0.00004093
Iteration 146/1000 | Loss: 0.00004093
Iteration 147/1000 | Loss: 0.00004093
Iteration 148/1000 | Loss: 0.00004093
Iteration 149/1000 | Loss: 0.00004093
Iteration 150/1000 | Loss: 0.00004092
Iteration 151/1000 | Loss: 0.00004092
Iteration 152/1000 | Loss: 0.00004092
Iteration 153/1000 | Loss: 0.00004092
Iteration 154/1000 | Loss: 0.00004091
Iteration 155/1000 | Loss: 0.00004091
Iteration 156/1000 | Loss: 0.00004090
Iteration 157/1000 | Loss: 0.00004090
Iteration 158/1000 | Loss: 0.00004090
Iteration 159/1000 | Loss: 0.00004090
Iteration 160/1000 | Loss: 0.00004090
Iteration 161/1000 | Loss: 0.00004090
Iteration 162/1000 | Loss: 0.00004090
Iteration 163/1000 | Loss: 0.00004090
Iteration 164/1000 | Loss: 0.00004089
Iteration 165/1000 | Loss: 0.00004089
Iteration 166/1000 | Loss: 0.00004089
Iteration 167/1000 | Loss: 0.00004088
Iteration 168/1000 | Loss: 0.00004088
Iteration 169/1000 | Loss: 0.00004088
Iteration 170/1000 | Loss: 0.00004088
Iteration 171/1000 | Loss: 0.00004087
Iteration 172/1000 | Loss: 0.00004087
Iteration 173/1000 | Loss: 0.00004087
Iteration 174/1000 | Loss: 0.00004087
Iteration 175/1000 | Loss: 0.00004087
Iteration 176/1000 | Loss: 0.00004087
Iteration 177/1000 | Loss: 0.00004087
Iteration 178/1000 | Loss: 0.00004087
Iteration 179/1000 | Loss: 0.00004087
Iteration 180/1000 | Loss: 0.00004086
Iteration 181/1000 | Loss: 0.00004086
Iteration 182/1000 | Loss: 0.00004086
Iteration 183/1000 | Loss: 0.00004086
Iteration 184/1000 | Loss: 0.00004086
Iteration 185/1000 | Loss: 0.00004085
Iteration 186/1000 | Loss: 0.00004085
Iteration 187/1000 | Loss: 0.00004085
Iteration 188/1000 | Loss: 0.00004085
Iteration 189/1000 | Loss: 0.00004084
Iteration 190/1000 | Loss: 0.00004084
Iteration 191/1000 | Loss: 0.00004084
Iteration 192/1000 | Loss: 0.00004084
Iteration 193/1000 | Loss: 0.00004083
Iteration 194/1000 | Loss: 0.00004083
Iteration 195/1000 | Loss: 0.00004083
Iteration 196/1000 | Loss: 0.00004083
Iteration 197/1000 | Loss: 0.00004082
Iteration 198/1000 | Loss: 0.00004082
Iteration 199/1000 | Loss: 0.00004082
Iteration 200/1000 | Loss: 0.00004081
Iteration 201/1000 | Loss: 0.00004081
Iteration 202/1000 | Loss: 0.00004081
Iteration 203/1000 | Loss: 0.00004081
Iteration 204/1000 | Loss: 0.00004081
Iteration 205/1000 | Loss: 0.00004081
Iteration 206/1000 | Loss: 0.00004080
Iteration 207/1000 | Loss: 0.00004080
Iteration 208/1000 | Loss: 0.00050698
Iteration 209/1000 | Loss: 0.00180406
Iteration 210/1000 | Loss: 0.00093008
Iteration 211/1000 | Loss: 0.00109580
Iteration 212/1000 | Loss: 0.00083781
Iteration 213/1000 | Loss: 0.00101671
Iteration 214/1000 | Loss: 0.00063767
Iteration 215/1000 | Loss: 0.00004073
Iteration 216/1000 | Loss: 0.00003734
Iteration 217/1000 | Loss: 0.00003407
Iteration 218/1000 | Loss: 0.00003138
Iteration 219/1000 | Loss: 0.00002952
Iteration 220/1000 | Loss: 0.00002853
Iteration 221/1000 | Loss: 0.00002792
Iteration 222/1000 | Loss: 0.00002749
Iteration 223/1000 | Loss: 0.00002717
Iteration 224/1000 | Loss: 0.00002695
Iteration 225/1000 | Loss: 0.00002688
Iteration 226/1000 | Loss: 0.00002681
Iteration 227/1000 | Loss: 0.00002679
Iteration 228/1000 | Loss: 0.00002678
Iteration 229/1000 | Loss: 0.00002677
Iteration 230/1000 | Loss: 0.00002671
Iteration 231/1000 | Loss: 0.00002668
Iteration 232/1000 | Loss: 0.00002667
Iteration 233/1000 | Loss: 0.00002663
Iteration 234/1000 | Loss: 0.00002648
Iteration 235/1000 | Loss: 0.00002648
Iteration 236/1000 | Loss: 0.00002646
Iteration 237/1000 | Loss: 0.00002645
Iteration 238/1000 | Loss: 0.00002644
Iteration 239/1000 | Loss: 0.00002644
Iteration 240/1000 | Loss: 0.00002642
Iteration 241/1000 | Loss: 0.00002642
Iteration 242/1000 | Loss: 0.00002642
Iteration 243/1000 | Loss: 0.00002641
Iteration 244/1000 | Loss: 0.00002637
Iteration 245/1000 | Loss: 0.00002637
Iteration 246/1000 | Loss: 0.00002636
Iteration 247/1000 | Loss: 0.00002636
Iteration 248/1000 | Loss: 0.00002630
Iteration 249/1000 | Loss: 0.00002627
Iteration 250/1000 | Loss: 0.00002626
Iteration 251/1000 | Loss: 0.00002626
Iteration 252/1000 | Loss: 0.00002625
Iteration 253/1000 | Loss: 0.00002625
Iteration 254/1000 | Loss: 0.00002624
Iteration 255/1000 | Loss: 0.00002623
Iteration 256/1000 | Loss: 0.00002622
Iteration 257/1000 | Loss: 0.00002622
Iteration 258/1000 | Loss: 0.00002620
Iteration 259/1000 | Loss: 0.00002620
Iteration 260/1000 | Loss: 0.00002619
Iteration 261/1000 | Loss: 0.00002619
Iteration 262/1000 | Loss: 0.00002618
Iteration 263/1000 | Loss: 0.00002613
Iteration 264/1000 | Loss: 0.00002612
Iteration 265/1000 | Loss: 0.00002611
Iteration 266/1000 | Loss: 0.00002611
Iteration 267/1000 | Loss: 0.00002611
Iteration 268/1000 | Loss: 0.00002610
Iteration 269/1000 | Loss: 0.00002609
Iteration 270/1000 | Loss: 0.00002607
Iteration 271/1000 | Loss: 0.00002607
Iteration 272/1000 | Loss: 0.00002607
Iteration 273/1000 | Loss: 0.00002607
Iteration 274/1000 | Loss: 0.00002607
Iteration 275/1000 | Loss: 0.00002607
Iteration 276/1000 | Loss: 0.00002607
Iteration 277/1000 | Loss: 0.00002606
Iteration 278/1000 | Loss: 0.00002606
Iteration 279/1000 | Loss: 0.00002605
Iteration 280/1000 | Loss: 0.00002604
Iteration 281/1000 | Loss: 0.00002603
Iteration 282/1000 | Loss: 0.00002603
Iteration 283/1000 | Loss: 0.00002602
Iteration 284/1000 | Loss: 0.00002602
Iteration 285/1000 | Loss: 0.00002601
Iteration 286/1000 | Loss: 0.00002601
Iteration 287/1000 | Loss: 0.00002600
Iteration 288/1000 | Loss: 0.00002600
Iteration 289/1000 | Loss: 0.00002599
Iteration 290/1000 | Loss: 0.00002599
Iteration 291/1000 | Loss: 0.00002599
Iteration 292/1000 | Loss: 0.00002598
Iteration 293/1000 | Loss: 0.00002598
Iteration 294/1000 | Loss: 0.00002597
Iteration 295/1000 | Loss: 0.00002597
Iteration 296/1000 | Loss: 0.00002597
Iteration 297/1000 | Loss: 0.00002597
Iteration 298/1000 | Loss: 0.00002597
Iteration 299/1000 | Loss: 0.00002597
Iteration 300/1000 | Loss: 0.00002597
Iteration 301/1000 | Loss: 0.00002596
Iteration 302/1000 | Loss: 0.00002596
Iteration 303/1000 | Loss: 0.00002595
Iteration 304/1000 | Loss: 0.00002595
Iteration 305/1000 | Loss: 0.00002594
Iteration 306/1000 | Loss: 0.00002594
Iteration 307/1000 | Loss: 0.00002594
Iteration 308/1000 | Loss: 0.00002593
Iteration 309/1000 | Loss: 0.00002593
Iteration 310/1000 | Loss: 0.00002592
Iteration 311/1000 | Loss: 0.00002592
Iteration 312/1000 | Loss: 0.00002590
Iteration 313/1000 | Loss: 0.00002590
Iteration 314/1000 | Loss: 0.00002589
Iteration 315/1000 | Loss: 0.00002588
Iteration 316/1000 | Loss: 0.00002588
Iteration 317/1000 | Loss: 0.00002588
Iteration 318/1000 | Loss: 0.00002588
Iteration 319/1000 | Loss: 0.00002587
Iteration 320/1000 | Loss: 0.00002587
Iteration 321/1000 | Loss: 0.00002586
Iteration 322/1000 | Loss: 0.00002586
Iteration 323/1000 | Loss: 0.00002586
Iteration 324/1000 | Loss: 0.00002586
Iteration 325/1000 | Loss: 0.00002586
Iteration 326/1000 | Loss: 0.00002586
Iteration 327/1000 | Loss: 0.00002586
Iteration 328/1000 | Loss: 0.00002586
Iteration 329/1000 | Loss: 0.00002585
Iteration 330/1000 | Loss: 0.00002585
Iteration 331/1000 | Loss: 0.00002585
Iteration 332/1000 | Loss: 0.00002585
Iteration 333/1000 | Loss: 0.00002585
Iteration 334/1000 | Loss: 0.00002584
Iteration 335/1000 | Loss: 0.00002584
Iteration 336/1000 | Loss: 0.00002584
Iteration 337/1000 | Loss: 0.00002583
Iteration 338/1000 | Loss: 0.00002583
Iteration 339/1000 | Loss: 0.00002583
Iteration 340/1000 | Loss: 0.00002583
Iteration 341/1000 | Loss: 0.00002583
Iteration 342/1000 | Loss: 0.00002582
Iteration 343/1000 | Loss: 0.00002582
Iteration 344/1000 | Loss: 0.00002582
Iteration 345/1000 | Loss: 0.00002582
Iteration 346/1000 | Loss: 0.00002582
Iteration 347/1000 | Loss: 0.00002582
Iteration 348/1000 | Loss: 0.00002582
Iteration 349/1000 | Loss: 0.00002581
Iteration 350/1000 | Loss: 0.00002581
Iteration 351/1000 | Loss: 0.00002581
Iteration 352/1000 | Loss: 0.00002581
Iteration 353/1000 | Loss: 0.00002581
Iteration 354/1000 | Loss: 0.00002581
Iteration 355/1000 | Loss: 0.00002581
Iteration 356/1000 | Loss: 0.00002581
Iteration 357/1000 | Loss: 0.00002581
Iteration 358/1000 | Loss: 0.00002580
Iteration 359/1000 | Loss: 0.00002580
Iteration 360/1000 | Loss: 0.00002580
Iteration 361/1000 | Loss: 0.00002580
Iteration 362/1000 | Loss: 0.00002579
Iteration 363/1000 | Loss: 0.00002579
Iteration 364/1000 | Loss: 0.00002579
Iteration 365/1000 | Loss: 0.00002579
Iteration 366/1000 | Loss: 0.00002579
Iteration 367/1000 | Loss: 0.00002579
Iteration 368/1000 | Loss: 0.00002579
Iteration 369/1000 | Loss: 0.00002579
Iteration 370/1000 | Loss: 0.00002579
Iteration 371/1000 | Loss: 0.00002579
Iteration 372/1000 | Loss: 0.00002578
Iteration 373/1000 | Loss: 0.00002578
Iteration 374/1000 | Loss: 0.00002578
Iteration 375/1000 | Loss: 0.00002578
Iteration 376/1000 | Loss: 0.00002578
Iteration 377/1000 | Loss: 0.00002578
Iteration 378/1000 | Loss: 0.00002578
Iteration 379/1000 | Loss: 0.00002578
Iteration 380/1000 | Loss: 0.00002578
Iteration 381/1000 | Loss: 0.00002578
Iteration 382/1000 | Loss: 0.00002578
Iteration 383/1000 | Loss: 0.00002578
Iteration 384/1000 | Loss: 0.00002578
Iteration 385/1000 | Loss: 0.00002578
Iteration 386/1000 | Loss: 0.00002578
Iteration 387/1000 | Loss: 0.00002577
Iteration 388/1000 | Loss: 0.00002577
Iteration 389/1000 | Loss: 0.00002577
Iteration 390/1000 | Loss: 0.00002577
Iteration 391/1000 | Loss: 0.00002577
Iteration 392/1000 | Loss: 0.00002577
Iteration 393/1000 | Loss: 0.00002577
Iteration 394/1000 | Loss: 0.00002577
Iteration 395/1000 | Loss: 0.00002577
Iteration 396/1000 | Loss: 0.00002577
Iteration 397/1000 | Loss: 0.00002577
Iteration 398/1000 | Loss: 0.00002577
Iteration 399/1000 | Loss: 0.00002576
Iteration 400/1000 | Loss: 0.00002576
Iteration 401/1000 | Loss: 0.00002576
Iteration 402/1000 | Loss: 0.00002576
Iteration 403/1000 | Loss: 0.00002576
Iteration 404/1000 | Loss: 0.00002576
Iteration 405/1000 | Loss: 0.00002576
Iteration 406/1000 | Loss: 0.00002576
Iteration 407/1000 | Loss: 0.00002576
Iteration 408/1000 | Loss: 0.00002576
Iteration 409/1000 | Loss: 0.00002576
Iteration 410/1000 | Loss: 0.00002575
Iteration 411/1000 | Loss: 0.00002575
Iteration 412/1000 | Loss: 0.00002575
Iteration 413/1000 | Loss: 0.00002575
Iteration 414/1000 | Loss: 0.00002575
Iteration 415/1000 | Loss: 0.00002575
Iteration 416/1000 | Loss: 0.00002575
Iteration 417/1000 | Loss: 0.00002575
Iteration 418/1000 | Loss: 0.00002575
Iteration 419/1000 | Loss: 0.00002575
Iteration 420/1000 | Loss: 0.00002575
Iteration 421/1000 | Loss: 0.00002575
Iteration 422/1000 | Loss: 0.00002575
Iteration 423/1000 | Loss: 0.00002575
Iteration 424/1000 | Loss: 0.00002575
Iteration 425/1000 | Loss: 0.00002575
Iteration 426/1000 | Loss: 0.00002575
Iteration 427/1000 | Loss: 0.00002575
Iteration 428/1000 | Loss: 0.00002575
Iteration 429/1000 | Loss: 0.00002575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 429. Stopping optimization.
Last 5 losses: [2.5750665372470394e-05, 2.5750665372470394e-05, 2.5750665372470394e-05, 2.5750665372470394e-05, 2.5750665372470394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5750665372470394e-05

Optimization complete. Final v2v error: 3.861496686935425 mm

Highest mean error: 11.396223068237305 mm for frame 72

Lowest mean error: 3.019605875015259 mm for frame 200

Saving results

Total time: 267.1060333251953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998666
Iteration 2/25 | Loss: 0.00178891
Iteration 3/25 | Loss: 0.00141041
Iteration 4/25 | Loss: 0.00133787
Iteration 5/25 | Loss: 0.00132282
Iteration 6/25 | Loss: 0.00132113
Iteration 7/25 | Loss: 0.00130171
Iteration 8/25 | Loss: 0.00129598
Iteration 9/25 | Loss: 0.00129467
Iteration 10/25 | Loss: 0.00129451
Iteration 11/25 | Loss: 0.00129449
Iteration 12/25 | Loss: 0.00129449
Iteration 13/25 | Loss: 0.00129449
Iteration 14/25 | Loss: 0.00129448
Iteration 15/25 | Loss: 0.00129448
Iteration 16/25 | Loss: 0.00129448
Iteration 17/25 | Loss: 0.00129448
Iteration 18/25 | Loss: 0.00129448
Iteration 19/25 | Loss: 0.00129448
Iteration 20/25 | Loss: 0.00129448
Iteration 21/25 | Loss: 0.00129448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001294483314268291, 0.001294483314268291, 0.001294483314268291, 0.001294483314268291, 0.001294483314268291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001294483314268291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44598329
Iteration 2/25 | Loss: 0.00078854
Iteration 3/25 | Loss: 0.00078854
Iteration 4/25 | Loss: 0.00078854
Iteration 5/25 | Loss: 0.00078854
Iteration 6/25 | Loss: 0.00078854
Iteration 7/25 | Loss: 0.00078854
Iteration 8/25 | Loss: 0.00078854
Iteration 9/25 | Loss: 0.00078854
Iteration 10/25 | Loss: 0.00078854
Iteration 11/25 | Loss: 0.00078854
Iteration 12/25 | Loss: 0.00078854
Iteration 13/25 | Loss: 0.00078854
Iteration 14/25 | Loss: 0.00078854
Iteration 15/25 | Loss: 0.00078854
Iteration 16/25 | Loss: 0.00078854
Iteration 17/25 | Loss: 0.00078854
Iteration 18/25 | Loss: 0.00078854
Iteration 19/25 | Loss: 0.00078854
Iteration 20/25 | Loss: 0.00078854
Iteration 21/25 | Loss: 0.00078854
Iteration 22/25 | Loss: 0.00078854
Iteration 23/25 | Loss: 0.00078854
Iteration 24/25 | Loss: 0.00078854
Iteration 25/25 | Loss: 0.00078854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078854
Iteration 2/1000 | Loss: 0.00003330
Iteration 3/1000 | Loss: 0.00002268
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002067
Iteration 6/1000 | Loss: 0.00002020
Iteration 7/1000 | Loss: 0.00001977
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001923
Iteration 10/1000 | Loss: 0.00001914
Iteration 11/1000 | Loss: 0.00001907
Iteration 12/1000 | Loss: 0.00001901
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001877
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001868
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001868
Iteration 23/1000 | Loss: 0.00001868
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001860
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001856
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001854
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001846
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001845
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001842
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001837
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8369082681601867e-05, 1.8369082681601867e-05, 1.8369082681601867e-05, 1.8369082681601867e-05, 1.8369082681601867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8369082681601867e-05

Optimization complete. Final v2v error: 3.6904048919677734 mm

Highest mean error: 3.762645721435547 mm for frame 55

Lowest mean error: 3.6014277935028076 mm for frame 0

Saving results

Total time: 42.83669853210449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457050
Iteration 2/25 | Loss: 0.00134446
Iteration 3/25 | Loss: 0.00125469
Iteration 4/25 | Loss: 0.00123626
Iteration 5/25 | Loss: 0.00123008
Iteration 6/25 | Loss: 0.00122963
Iteration 7/25 | Loss: 0.00122963
Iteration 8/25 | Loss: 0.00122963
Iteration 9/25 | Loss: 0.00122963
Iteration 10/25 | Loss: 0.00122963
Iteration 11/25 | Loss: 0.00122963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012296335771679878, 0.0012296335771679878, 0.0012296335771679878, 0.0012296335771679878, 0.0012296335771679878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012296335771679878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43044496
Iteration 2/25 | Loss: 0.00073570
Iteration 3/25 | Loss: 0.00073569
Iteration 4/25 | Loss: 0.00073569
Iteration 5/25 | Loss: 0.00073569
Iteration 6/25 | Loss: 0.00073569
Iteration 7/25 | Loss: 0.00073569
Iteration 8/25 | Loss: 0.00073569
Iteration 9/25 | Loss: 0.00073569
Iteration 10/25 | Loss: 0.00073569
Iteration 11/25 | Loss: 0.00073569
Iteration 12/25 | Loss: 0.00073569
Iteration 13/25 | Loss: 0.00073569
Iteration 14/25 | Loss: 0.00073569
Iteration 15/25 | Loss: 0.00073569
Iteration 16/25 | Loss: 0.00073569
Iteration 17/25 | Loss: 0.00073569
Iteration 18/25 | Loss: 0.00073569
Iteration 19/25 | Loss: 0.00073569
Iteration 20/25 | Loss: 0.00073569
Iteration 21/25 | Loss: 0.00073569
Iteration 22/25 | Loss: 0.00073569
Iteration 23/25 | Loss: 0.00073569
Iteration 24/25 | Loss: 0.00073569
Iteration 25/25 | Loss: 0.00073569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073569
Iteration 2/1000 | Loss: 0.00002956
Iteration 3/1000 | Loss: 0.00002291
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002059
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001938
Iteration 8/1000 | Loss: 0.00001904
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001843
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001806
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001804
Iteration 22/1000 | Loss: 0.00001803
Iteration 23/1000 | Loss: 0.00001803
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001800
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001799
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001793
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001790
Iteration 36/1000 | Loss: 0.00001790
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001778
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001770
Iteration 50/1000 | Loss: 0.00001770
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001769
Iteration 53/1000 | Loss: 0.00001769
Iteration 54/1000 | Loss: 0.00001769
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001768
Iteration 57/1000 | Loss: 0.00001768
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001763
Iteration 63/1000 | Loss: 0.00001763
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001761
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001759
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001753
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001753
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001747
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001747
Iteration 138/1000 | Loss: 0.00001747
Iteration 139/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.746747875586152e-05, 1.746747875586152e-05, 1.746747875586152e-05, 1.746747875586152e-05, 1.746747875586152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.746747875586152e-05

Optimization complete. Final v2v error: 3.5516104698181152 mm

Highest mean error: 3.672698497772217 mm for frame 115

Lowest mean error: 3.392944574356079 mm for frame 53

Saving results

Total time: 38.342281341552734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643908
Iteration 2/25 | Loss: 0.00138131
Iteration 3/25 | Loss: 0.00128290
Iteration 4/25 | Loss: 0.00127389
Iteration 5/25 | Loss: 0.00127086
Iteration 6/25 | Loss: 0.00127074
Iteration 7/25 | Loss: 0.00127074
Iteration 8/25 | Loss: 0.00127074
Iteration 9/25 | Loss: 0.00127074
Iteration 10/25 | Loss: 0.00127074
Iteration 11/25 | Loss: 0.00127074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012707404093816876, 0.0012707404093816876, 0.0012707404093816876, 0.0012707404093816876, 0.0012707404093816876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012707404093816876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15023518
Iteration 2/25 | Loss: 0.00083489
Iteration 3/25 | Loss: 0.00083483
Iteration 4/25 | Loss: 0.00083483
Iteration 5/25 | Loss: 0.00083483
Iteration 6/25 | Loss: 0.00083483
Iteration 7/25 | Loss: 0.00083483
Iteration 8/25 | Loss: 0.00083482
Iteration 9/25 | Loss: 0.00083482
Iteration 10/25 | Loss: 0.00083482
Iteration 11/25 | Loss: 0.00083482
Iteration 12/25 | Loss: 0.00083482
Iteration 13/25 | Loss: 0.00083482
Iteration 14/25 | Loss: 0.00083482
Iteration 15/25 | Loss: 0.00083482
Iteration 16/25 | Loss: 0.00083482
Iteration 17/25 | Loss: 0.00083482
Iteration 18/25 | Loss: 0.00083482
Iteration 19/25 | Loss: 0.00083482
Iteration 20/25 | Loss: 0.00083482
Iteration 21/25 | Loss: 0.00083482
Iteration 22/25 | Loss: 0.00083482
Iteration 23/25 | Loss: 0.00083482
Iteration 24/25 | Loss: 0.00083482
Iteration 25/25 | Loss: 0.00083482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083482
Iteration 2/1000 | Loss: 0.00002710
Iteration 3/1000 | Loss: 0.00002094
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001860
Iteration 6/1000 | Loss: 0.00001803
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001720
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001673
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001659
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001654
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001628
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001603
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001602
Iteration 49/1000 | Loss: 0.00001602
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001600
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001599
Iteration 56/1000 | Loss: 0.00001599
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001598
Iteration 60/1000 | Loss: 0.00001597
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001596
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001594
Iteration 70/1000 | Loss: 0.00001594
Iteration 71/1000 | Loss: 0.00001594
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001593
Iteration 74/1000 | Loss: 0.00001592
Iteration 75/1000 | Loss: 0.00001592
Iteration 76/1000 | Loss: 0.00001592
Iteration 77/1000 | Loss: 0.00001591
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001591
Iteration 81/1000 | Loss: 0.00001591
Iteration 82/1000 | Loss: 0.00001591
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00001591
Iteration 86/1000 | Loss: 0.00001591
Iteration 87/1000 | Loss: 0.00001591
Iteration 88/1000 | Loss: 0.00001590
Iteration 89/1000 | Loss: 0.00001590
Iteration 90/1000 | Loss: 0.00001590
Iteration 91/1000 | Loss: 0.00001589
Iteration 92/1000 | Loss: 0.00001589
Iteration 93/1000 | Loss: 0.00001589
Iteration 94/1000 | Loss: 0.00001589
Iteration 95/1000 | Loss: 0.00001588
Iteration 96/1000 | Loss: 0.00001588
Iteration 97/1000 | Loss: 0.00001588
Iteration 98/1000 | Loss: 0.00001588
Iteration 99/1000 | Loss: 0.00001588
Iteration 100/1000 | Loss: 0.00001588
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001587
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001587
Iteration 105/1000 | Loss: 0.00001587
Iteration 106/1000 | Loss: 0.00001587
Iteration 107/1000 | Loss: 0.00001587
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001586
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001586
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001586
Iteration 122/1000 | Loss: 0.00001586
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001586
Iteration 125/1000 | Loss: 0.00001586
Iteration 126/1000 | Loss: 0.00001586
Iteration 127/1000 | Loss: 0.00001586
Iteration 128/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5856174286454916e-05, 1.5856174286454916e-05, 1.5856174286454916e-05, 1.5856174286454916e-05, 1.5856174286454916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5856174286454916e-05

Optimization complete. Final v2v error: 3.347555637359619 mm

Highest mean error: 3.954406261444092 mm for frame 140

Lowest mean error: 3.112326145172119 mm for frame 18

Saving results

Total time: 38.293256998062134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470281
Iteration 2/25 | Loss: 0.00152619
Iteration 3/25 | Loss: 0.00133255
Iteration 4/25 | Loss: 0.00131073
Iteration 5/25 | Loss: 0.00130574
Iteration 6/25 | Loss: 0.00130532
Iteration 7/25 | Loss: 0.00130532
Iteration 8/25 | Loss: 0.00130532
Iteration 9/25 | Loss: 0.00130532
Iteration 10/25 | Loss: 0.00130532
Iteration 11/25 | Loss: 0.00130532
Iteration 12/25 | Loss: 0.00130532
Iteration 13/25 | Loss: 0.00130532
Iteration 14/25 | Loss: 0.00130532
Iteration 15/25 | Loss: 0.00130532
Iteration 16/25 | Loss: 0.00130532
Iteration 17/25 | Loss: 0.00130532
Iteration 18/25 | Loss: 0.00130532
Iteration 19/25 | Loss: 0.00130532
Iteration 20/25 | Loss: 0.00130532
Iteration 21/25 | Loss: 0.00130532
Iteration 22/25 | Loss: 0.00130532
Iteration 23/25 | Loss: 0.00130532
Iteration 24/25 | Loss: 0.00130532
Iteration 25/25 | Loss: 0.00130532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42749405
Iteration 2/25 | Loss: 0.00075935
Iteration 3/25 | Loss: 0.00075935
Iteration 4/25 | Loss: 0.00075935
Iteration 5/25 | Loss: 0.00075935
Iteration 6/25 | Loss: 0.00075935
Iteration 7/25 | Loss: 0.00075935
Iteration 8/25 | Loss: 0.00075935
Iteration 9/25 | Loss: 0.00075935
Iteration 10/25 | Loss: 0.00075935
Iteration 11/25 | Loss: 0.00075935
Iteration 12/25 | Loss: 0.00075935
Iteration 13/25 | Loss: 0.00075935
Iteration 14/25 | Loss: 0.00075935
Iteration 15/25 | Loss: 0.00075935
Iteration 16/25 | Loss: 0.00075935
Iteration 17/25 | Loss: 0.00075935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007593458285555243, 0.0007593458285555243, 0.0007593458285555243, 0.0007593458285555243, 0.0007593458285555243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007593458285555243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075935
Iteration 2/1000 | Loss: 0.00005512
Iteration 3/1000 | Loss: 0.00004094
Iteration 4/1000 | Loss: 0.00003822
Iteration 5/1000 | Loss: 0.00003588
Iteration 6/1000 | Loss: 0.00003403
Iteration 7/1000 | Loss: 0.00003275
Iteration 8/1000 | Loss: 0.00003183
Iteration 9/1000 | Loss: 0.00003119
Iteration 10/1000 | Loss: 0.00003074
Iteration 11/1000 | Loss: 0.00003047
Iteration 12/1000 | Loss: 0.00003032
Iteration 13/1000 | Loss: 0.00003017
Iteration 14/1000 | Loss: 0.00003014
Iteration 15/1000 | Loss: 0.00002996
Iteration 16/1000 | Loss: 0.00002983
Iteration 17/1000 | Loss: 0.00002978
Iteration 18/1000 | Loss: 0.00002969
Iteration 19/1000 | Loss: 0.00002956
Iteration 20/1000 | Loss: 0.00002951
Iteration 21/1000 | Loss: 0.00002946
Iteration 22/1000 | Loss: 0.00002944
Iteration 23/1000 | Loss: 0.00002943
Iteration 24/1000 | Loss: 0.00002940
Iteration 25/1000 | Loss: 0.00002940
Iteration 26/1000 | Loss: 0.00002939
Iteration 27/1000 | Loss: 0.00002939
Iteration 28/1000 | Loss: 0.00002938
Iteration 29/1000 | Loss: 0.00002937
Iteration 30/1000 | Loss: 0.00002936
Iteration 31/1000 | Loss: 0.00002936
Iteration 32/1000 | Loss: 0.00002936
Iteration 33/1000 | Loss: 0.00002936
Iteration 34/1000 | Loss: 0.00002936
Iteration 35/1000 | Loss: 0.00002936
Iteration 36/1000 | Loss: 0.00002936
Iteration 37/1000 | Loss: 0.00002936
Iteration 38/1000 | Loss: 0.00002936
Iteration 39/1000 | Loss: 0.00002936
Iteration 40/1000 | Loss: 0.00002936
Iteration 41/1000 | Loss: 0.00002935
Iteration 42/1000 | Loss: 0.00002935
Iteration 43/1000 | Loss: 0.00002935
Iteration 44/1000 | Loss: 0.00002935
Iteration 45/1000 | Loss: 0.00002934
Iteration 46/1000 | Loss: 0.00002933
Iteration 47/1000 | Loss: 0.00002933
Iteration 48/1000 | Loss: 0.00002933
Iteration 49/1000 | Loss: 0.00002932
Iteration 50/1000 | Loss: 0.00002932
Iteration 51/1000 | Loss: 0.00002931
Iteration 52/1000 | Loss: 0.00002931
Iteration 53/1000 | Loss: 0.00002931
Iteration 54/1000 | Loss: 0.00002931
Iteration 55/1000 | Loss: 0.00002929
Iteration 56/1000 | Loss: 0.00002929
Iteration 57/1000 | Loss: 0.00002929
Iteration 58/1000 | Loss: 0.00002929
Iteration 59/1000 | Loss: 0.00002929
Iteration 60/1000 | Loss: 0.00002929
Iteration 61/1000 | Loss: 0.00002929
Iteration 62/1000 | Loss: 0.00002928
Iteration 63/1000 | Loss: 0.00002928
Iteration 64/1000 | Loss: 0.00002928
Iteration 65/1000 | Loss: 0.00002927
Iteration 66/1000 | Loss: 0.00002927
Iteration 67/1000 | Loss: 0.00002927
Iteration 68/1000 | Loss: 0.00002927
Iteration 69/1000 | Loss: 0.00002927
Iteration 70/1000 | Loss: 0.00002927
Iteration 71/1000 | Loss: 0.00002927
Iteration 72/1000 | Loss: 0.00002927
Iteration 73/1000 | Loss: 0.00002927
Iteration 74/1000 | Loss: 0.00002927
Iteration 75/1000 | Loss: 0.00002927
Iteration 76/1000 | Loss: 0.00002926
Iteration 77/1000 | Loss: 0.00002926
Iteration 78/1000 | Loss: 0.00002926
Iteration 79/1000 | Loss: 0.00002926
Iteration 80/1000 | Loss: 0.00002926
Iteration 81/1000 | Loss: 0.00002926
Iteration 82/1000 | Loss: 0.00002926
Iteration 83/1000 | Loss: 0.00002926
Iteration 84/1000 | Loss: 0.00002925
Iteration 85/1000 | Loss: 0.00002925
Iteration 86/1000 | Loss: 0.00002925
Iteration 87/1000 | Loss: 0.00002925
Iteration 88/1000 | Loss: 0.00002925
Iteration 89/1000 | Loss: 0.00002925
Iteration 90/1000 | Loss: 0.00002925
Iteration 91/1000 | Loss: 0.00002925
Iteration 92/1000 | Loss: 0.00002925
Iteration 93/1000 | Loss: 0.00002925
Iteration 94/1000 | Loss: 0.00002925
Iteration 95/1000 | Loss: 0.00002925
Iteration 96/1000 | Loss: 0.00002925
Iteration 97/1000 | Loss: 0.00002925
Iteration 98/1000 | Loss: 0.00002924
Iteration 99/1000 | Loss: 0.00002924
Iteration 100/1000 | Loss: 0.00002924
Iteration 101/1000 | Loss: 0.00002924
Iteration 102/1000 | Loss: 0.00002924
Iteration 103/1000 | Loss: 0.00002923
Iteration 104/1000 | Loss: 0.00002923
Iteration 105/1000 | Loss: 0.00002923
Iteration 106/1000 | Loss: 0.00002923
Iteration 107/1000 | Loss: 0.00002923
Iteration 108/1000 | Loss: 0.00002923
Iteration 109/1000 | Loss: 0.00002923
Iteration 110/1000 | Loss: 0.00002923
Iteration 111/1000 | Loss: 0.00002923
Iteration 112/1000 | Loss: 0.00002923
Iteration 113/1000 | Loss: 0.00002923
Iteration 114/1000 | Loss: 0.00002923
Iteration 115/1000 | Loss: 0.00002923
Iteration 116/1000 | Loss: 0.00002923
Iteration 117/1000 | Loss: 0.00002923
Iteration 118/1000 | Loss: 0.00002923
Iteration 119/1000 | Loss: 0.00002923
Iteration 120/1000 | Loss: 0.00002923
Iteration 121/1000 | Loss: 0.00002923
Iteration 122/1000 | Loss: 0.00002923
Iteration 123/1000 | Loss: 0.00002923
Iteration 124/1000 | Loss: 0.00002923
Iteration 125/1000 | Loss: 0.00002923
Iteration 126/1000 | Loss: 0.00002923
Iteration 127/1000 | Loss: 0.00002923
Iteration 128/1000 | Loss: 0.00002923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.923199463111814e-05, 2.923199463111814e-05, 2.923199463111814e-05, 2.923199463111814e-05, 2.923199463111814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.923199463111814e-05

Optimization complete. Final v2v error: 4.3853325843811035 mm

Highest mean error: 4.766098976135254 mm for frame 160

Lowest mean error: 3.681307315826416 mm for frame 119

Saving results

Total time: 47.55229139328003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00361338
Iteration 2/25 | Loss: 0.00135586
Iteration 3/25 | Loss: 0.00123605
Iteration 4/25 | Loss: 0.00121939
Iteration 5/25 | Loss: 0.00121571
Iteration 6/25 | Loss: 0.00121498
Iteration 7/25 | Loss: 0.00121498
Iteration 8/25 | Loss: 0.00121498
Iteration 9/25 | Loss: 0.00121498
Iteration 10/25 | Loss: 0.00121498
Iteration 11/25 | Loss: 0.00121498
Iteration 12/25 | Loss: 0.00121498
Iteration 13/25 | Loss: 0.00121498
Iteration 14/25 | Loss: 0.00121498
Iteration 15/25 | Loss: 0.00121498
Iteration 16/25 | Loss: 0.00121498
Iteration 17/25 | Loss: 0.00121498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012149849208071828, 0.0012149849208071828, 0.0012149849208071828, 0.0012149849208071828, 0.0012149849208071828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012149849208071828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47183228
Iteration 2/25 | Loss: 0.00079260
Iteration 3/25 | Loss: 0.00079260
Iteration 4/25 | Loss: 0.00079260
Iteration 5/25 | Loss: 0.00079260
Iteration 6/25 | Loss: 0.00079260
Iteration 7/25 | Loss: 0.00079260
Iteration 8/25 | Loss: 0.00079260
Iteration 9/25 | Loss: 0.00079260
Iteration 10/25 | Loss: 0.00079260
Iteration 11/25 | Loss: 0.00079260
Iteration 12/25 | Loss: 0.00079260
Iteration 13/25 | Loss: 0.00079260
Iteration 14/25 | Loss: 0.00079260
Iteration 15/25 | Loss: 0.00079260
Iteration 16/25 | Loss: 0.00079260
Iteration 17/25 | Loss: 0.00079260
Iteration 18/25 | Loss: 0.00079260
Iteration 19/25 | Loss: 0.00079260
Iteration 20/25 | Loss: 0.00079260
Iteration 21/25 | Loss: 0.00079260
Iteration 22/25 | Loss: 0.00079260
Iteration 23/25 | Loss: 0.00079260
Iteration 24/25 | Loss: 0.00079260
Iteration 25/25 | Loss: 0.00079260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079260
Iteration 2/1000 | Loss: 0.00003264
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001669
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001496
Iteration 7/1000 | Loss: 0.00001444
Iteration 8/1000 | Loss: 0.00001414
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001369
Iteration 16/1000 | Loss: 0.00001369
Iteration 17/1000 | Loss: 0.00001368
Iteration 18/1000 | Loss: 0.00001359
Iteration 19/1000 | Loss: 0.00001359
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001349
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001347
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001338
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001338
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001337
Iteration 74/1000 | Loss: 0.00001337
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001337
Iteration 79/1000 | Loss: 0.00001337
Iteration 80/1000 | Loss: 0.00001337
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001334
Iteration 93/1000 | Loss: 0.00001334
Iteration 94/1000 | Loss: 0.00001334
Iteration 95/1000 | Loss: 0.00001334
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001333
Iteration 99/1000 | Loss: 0.00001333
Iteration 100/1000 | Loss: 0.00001333
Iteration 101/1000 | Loss: 0.00001333
Iteration 102/1000 | Loss: 0.00001333
Iteration 103/1000 | Loss: 0.00001333
Iteration 104/1000 | Loss: 0.00001333
Iteration 105/1000 | Loss: 0.00001333
Iteration 106/1000 | Loss: 0.00001333
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001332
Iteration 112/1000 | Loss: 0.00001332
Iteration 113/1000 | Loss: 0.00001332
Iteration 114/1000 | Loss: 0.00001332
Iteration 115/1000 | Loss: 0.00001332
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001332
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001330
Iteration 141/1000 | Loss: 0.00001330
Iteration 142/1000 | Loss: 0.00001330
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001330
Iteration 146/1000 | Loss: 0.00001330
Iteration 147/1000 | Loss: 0.00001330
Iteration 148/1000 | Loss: 0.00001330
Iteration 149/1000 | Loss: 0.00001330
Iteration 150/1000 | Loss: 0.00001330
Iteration 151/1000 | Loss: 0.00001330
Iteration 152/1000 | Loss: 0.00001330
Iteration 153/1000 | Loss: 0.00001330
Iteration 154/1000 | Loss: 0.00001330
Iteration 155/1000 | Loss: 0.00001330
Iteration 156/1000 | Loss: 0.00001330
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001329
Iteration 159/1000 | Loss: 0.00001329
Iteration 160/1000 | Loss: 0.00001329
Iteration 161/1000 | Loss: 0.00001329
Iteration 162/1000 | Loss: 0.00001329
Iteration 163/1000 | Loss: 0.00001329
Iteration 164/1000 | Loss: 0.00001329
Iteration 165/1000 | Loss: 0.00001329
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001329
Iteration 171/1000 | Loss: 0.00001329
Iteration 172/1000 | Loss: 0.00001329
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Iteration 179/1000 | Loss: 0.00001329
Iteration 180/1000 | Loss: 0.00001329
Iteration 181/1000 | Loss: 0.00001329
Iteration 182/1000 | Loss: 0.00001329
Iteration 183/1000 | Loss: 0.00001329
Iteration 184/1000 | Loss: 0.00001329
Iteration 185/1000 | Loss: 0.00001329
Iteration 186/1000 | Loss: 0.00001329
Iteration 187/1000 | Loss: 0.00001329
Iteration 188/1000 | Loss: 0.00001329
Iteration 189/1000 | Loss: 0.00001329
Iteration 190/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.3293856682139449e-05, 1.3293856682139449e-05, 1.3293856682139449e-05, 1.3293856682139449e-05, 1.3293856682139449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3293856682139449e-05

Optimization complete. Final v2v error: 3.1028995513916016 mm

Highest mean error: 3.448659658432007 mm for frame 13

Lowest mean error: 2.9057657718658447 mm for frame 60

Saving results

Total time: 37.05699968338013
