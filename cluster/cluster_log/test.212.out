Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=212, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11872-11927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997076
Iteration 2/25 | Loss: 0.00171409
Iteration 3/25 | Loss: 0.00133966
Iteration 4/25 | Loss: 0.00132354
Iteration 5/25 | Loss: 0.00132029
Iteration 6/25 | Loss: 0.00131966
Iteration 7/25 | Loss: 0.00131966
Iteration 8/25 | Loss: 0.00131966
Iteration 9/25 | Loss: 0.00131966
Iteration 10/25 | Loss: 0.00131966
Iteration 11/25 | Loss: 0.00131966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013196589425206184, 0.0013196589425206184, 0.0013196589425206184, 0.0013196589425206184, 0.0013196589425206184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013196589425206184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.46548897
Iteration 2/25 | Loss: 0.00121998
Iteration 3/25 | Loss: 0.00121998
Iteration 4/25 | Loss: 0.00121998
Iteration 5/25 | Loss: 0.00121998
Iteration 6/25 | Loss: 0.00121998
Iteration 7/25 | Loss: 0.00121998
Iteration 8/25 | Loss: 0.00121998
Iteration 9/25 | Loss: 0.00121998
Iteration 10/25 | Loss: 0.00121998
Iteration 11/25 | Loss: 0.00121998
Iteration 12/25 | Loss: 0.00121998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012199818156659603, 0.0012199818156659603, 0.0012199818156659603, 0.0012199818156659603, 0.0012199818156659603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012199818156659603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121998
Iteration 2/1000 | Loss: 0.00004179
Iteration 3/1000 | Loss: 0.00002733
Iteration 4/1000 | Loss: 0.00002468
Iteration 5/1000 | Loss: 0.00002265
Iteration 6/1000 | Loss: 0.00002193
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002098
Iteration 9/1000 | Loss: 0.00002065
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00002019
Iteration 12/1000 | Loss: 0.00002005
Iteration 13/1000 | Loss: 0.00001999
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001997
Iteration 17/1000 | Loss: 0.00001996
Iteration 18/1000 | Loss: 0.00001996
Iteration 19/1000 | Loss: 0.00001995
Iteration 20/1000 | Loss: 0.00001994
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001977
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001968
Iteration 37/1000 | Loss: 0.00001968
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001967
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001966
Iteration 44/1000 | Loss: 0.00001966
Iteration 45/1000 | Loss: 0.00001965
Iteration 46/1000 | Loss: 0.00001964
Iteration 47/1000 | Loss: 0.00001964
Iteration 48/1000 | Loss: 0.00001964
Iteration 49/1000 | Loss: 0.00001964
Iteration 50/1000 | Loss: 0.00001964
Iteration 51/1000 | Loss: 0.00001963
Iteration 52/1000 | Loss: 0.00001963
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001961
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001960
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001958
Iteration 78/1000 | Loss: 0.00001958
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001957
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001956
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001956
Iteration 91/1000 | Loss: 0.00001955
Iteration 92/1000 | Loss: 0.00001955
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001955
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001953
Iteration 108/1000 | Loss: 0.00001953
Iteration 109/1000 | Loss: 0.00001953
Iteration 110/1000 | Loss: 0.00001952
Iteration 111/1000 | Loss: 0.00001952
Iteration 112/1000 | Loss: 0.00001952
Iteration 113/1000 | Loss: 0.00001952
Iteration 114/1000 | Loss: 0.00001952
Iteration 115/1000 | Loss: 0.00001951
Iteration 116/1000 | Loss: 0.00001951
Iteration 117/1000 | Loss: 0.00001951
Iteration 118/1000 | Loss: 0.00001950
Iteration 119/1000 | Loss: 0.00001950
Iteration 120/1000 | Loss: 0.00001950
Iteration 121/1000 | Loss: 0.00001950
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001948
Iteration 126/1000 | Loss: 0.00001948
Iteration 127/1000 | Loss: 0.00001948
Iteration 128/1000 | Loss: 0.00001948
Iteration 129/1000 | Loss: 0.00001947
Iteration 130/1000 | Loss: 0.00001947
Iteration 131/1000 | Loss: 0.00001946
Iteration 132/1000 | Loss: 0.00001946
Iteration 133/1000 | Loss: 0.00001946
Iteration 134/1000 | Loss: 0.00001945
Iteration 135/1000 | Loss: 0.00001945
Iteration 136/1000 | Loss: 0.00001945
Iteration 137/1000 | Loss: 0.00001945
Iteration 138/1000 | Loss: 0.00001944
Iteration 139/1000 | Loss: 0.00001944
Iteration 140/1000 | Loss: 0.00001944
Iteration 141/1000 | Loss: 0.00001944
Iteration 142/1000 | Loss: 0.00001943
Iteration 143/1000 | Loss: 0.00001943
Iteration 144/1000 | Loss: 0.00001943
Iteration 145/1000 | Loss: 0.00001943
Iteration 146/1000 | Loss: 0.00001943
Iteration 147/1000 | Loss: 0.00001943
Iteration 148/1000 | Loss: 0.00001942
Iteration 149/1000 | Loss: 0.00001942
Iteration 150/1000 | Loss: 0.00001942
Iteration 151/1000 | Loss: 0.00001942
Iteration 152/1000 | Loss: 0.00001942
Iteration 153/1000 | Loss: 0.00001941
Iteration 154/1000 | Loss: 0.00001941
Iteration 155/1000 | Loss: 0.00001941
Iteration 156/1000 | Loss: 0.00001941
Iteration 157/1000 | Loss: 0.00001941
Iteration 158/1000 | Loss: 0.00001941
Iteration 159/1000 | Loss: 0.00001940
Iteration 160/1000 | Loss: 0.00001940
Iteration 161/1000 | Loss: 0.00001940
Iteration 162/1000 | Loss: 0.00001940
Iteration 163/1000 | Loss: 0.00001940
Iteration 164/1000 | Loss: 0.00001940
Iteration 165/1000 | Loss: 0.00001939
Iteration 166/1000 | Loss: 0.00001939
Iteration 167/1000 | Loss: 0.00001939
Iteration 168/1000 | Loss: 0.00001939
Iteration 169/1000 | Loss: 0.00001939
Iteration 170/1000 | Loss: 0.00001939
Iteration 171/1000 | Loss: 0.00001939
Iteration 172/1000 | Loss: 0.00001939
Iteration 173/1000 | Loss: 0.00001939
Iteration 174/1000 | Loss: 0.00001939
Iteration 175/1000 | Loss: 0.00001939
Iteration 176/1000 | Loss: 0.00001939
Iteration 177/1000 | Loss: 0.00001939
Iteration 178/1000 | Loss: 0.00001939
Iteration 179/1000 | Loss: 0.00001939
Iteration 180/1000 | Loss: 0.00001939
Iteration 181/1000 | Loss: 0.00001939
Iteration 182/1000 | Loss: 0.00001939
Iteration 183/1000 | Loss: 0.00001939
Iteration 184/1000 | Loss: 0.00001939
Iteration 185/1000 | Loss: 0.00001939
Iteration 186/1000 | Loss: 0.00001939
Iteration 187/1000 | Loss: 0.00001939
Iteration 188/1000 | Loss: 0.00001939
Iteration 189/1000 | Loss: 0.00001939
Iteration 190/1000 | Loss: 0.00001939
Iteration 191/1000 | Loss: 0.00001939
Iteration 192/1000 | Loss: 0.00001939
Iteration 193/1000 | Loss: 0.00001939
Iteration 194/1000 | Loss: 0.00001939
Iteration 195/1000 | Loss: 0.00001939
Iteration 196/1000 | Loss: 0.00001939
Iteration 197/1000 | Loss: 0.00001939
Iteration 198/1000 | Loss: 0.00001939
Iteration 199/1000 | Loss: 0.00001939
Iteration 200/1000 | Loss: 0.00001939
Iteration 201/1000 | Loss: 0.00001939
Iteration 202/1000 | Loss: 0.00001939
Iteration 203/1000 | Loss: 0.00001939
Iteration 204/1000 | Loss: 0.00001939
Iteration 205/1000 | Loss: 0.00001939
Iteration 206/1000 | Loss: 0.00001939
Iteration 207/1000 | Loss: 0.00001939
Iteration 208/1000 | Loss: 0.00001939
Iteration 209/1000 | Loss: 0.00001939
Iteration 210/1000 | Loss: 0.00001939
Iteration 211/1000 | Loss: 0.00001939
Iteration 212/1000 | Loss: 0.00001939
Iteration 213/1000 | Loss: 0.00001939
Iteration 214/1000 | Loss: 0.00001939
Iteration 215/1000 | Loss: 0.00001939
Iteration 216/1000 | Loss: 0.00001939
Iteration 217/1000 | Loss: 0.00001939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.939010871865321e-05, 1.939010871865321e-05, 1.939010871865321e-05, 1.939010871865321e-05, 1.939010871865321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.939010871865321e-05

Optimization complete. Final v2v error: 3.6216726303100586 mm

Highest mean error: 4.492985248565674 mm for frame 30

Lowest mean error: 3.0419797897338867 mm for frame 0

Saving results

Total time: 46.75549244880676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811418
Iteration 2/25 | Loss: 0.00159625
Iteration 3/25 | Loss: 0.00125203
Iteration 4/25 | Loss: 0.00122793
Iteration 5/25 | Loss: 0.00122793
Iteration 6/25 | Loss: 0.00122793
Iteration 7/25 | Loss: 0.00122793
Iteration 8/25 | Loss: 0.00122793
Iteration 9/25 | Loss: 0.00122793
Iteration 10/25 | Loss: 0.00122793
Iteration 11/25 | Loss: 0.00122793
Iteration 12/25 | Loss: 0.00122793
Iteration 13/25 | Loss: 0.00122793
Iteration 14/25 | Loss: 0.00122793
Iteration 15/25 | Loss: 0.00122793
Iteration 16/25 | Loss: 0.00122793
Iteration 17/25 | Loss: 0.00122793
Iteration 18/25 | Loss: 0.00122793
Iteration 19/25 | Loss: 0.00122793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012279343791306019, 0.0012279343791306019, 0.0012279343791306019, 0.0012279343791306019, 0.0012279343791306019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012279343791306019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26299059
Iteration 2/25 | Loss: 0.00097180
Iteration 3/25 | Loss: 0.00097180
Iteration 4/25 | Loss: 0.00097180
Iteration 5/25 | Loss: 0.00097180
Iteration 6/25 | Loss: 0.00097179
Iteration 7/25 | Loss: 0.00097179
Iteration 8/25 | Loss: 0.00097179
Iteration 9/25 | Loss: 0.00097179
Iteration 10/25 | Loss: 0.00097179
Iteration 11/25 | Loss: 0.00097179
Iteration 12/25 | Loss: 0.00097179
Iteration 13/25 | Loss: 0.00097179
Iteration 14/25 | Loss: 0.00097179
Iteration 15/25 | Loss: 0.00097179
Iteration 16/25 | Loss: 0.00097179
Iteration 17/25 | Loss: 0.00097179
Iteration 18/25 | Loss: 0.00097179
Iteration 19/25 | Loss: 0.00097179
Iteration 20/25 | Loss: 0.00097179
Iteration 21/25 | Loss: 0.00097179
Iteration 22/25 | Loss: 0.00097179
Iteration 23/25 | Loss: 0.00097179
Iteration 24/25 | Loss: 0.00097179
Iteration 25/25 | Loss: 0.00097179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097179
Iteration 2/1000 | Loss: 0.00002763
Iteration 3/1000 | Loss: 0.00002006
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001691
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001465
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001367
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001347
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001342
Iteration 22/1000 | Loss: 0.00001342
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001324
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001313
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001309
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001306
Iteration 51/1000 | Loss: 0.00001306
Iteration 52/1000 | Loss: 0.00001306
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001304
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001299
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001297
Iteration 119/1000 | Loss: 0.00001297
Iteration 120/1000 | Loss: 0.00001297
Iteration 121/1000 | Loss: 0.00001297
Iteration 122/1000 | Loss: 0.00001297
Iteration 123/1000 | Loss: 0.00001297
Iteration 124/1000 | Loss: 0.00001297
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001296
Iteration 127/1000 | Loss: 0.00001296
Iteration 128/1000 | Loss: 0.00001296
Iteration 129/1000 | Loss: 0.00001296
Iteration 130/1000 | Loss: 0.00001296
Iteration 131/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.2964893358002882e-05, 1.2964893358002882e-05, 1.2964893358002882e-05, 1.2964893358002882e-05, 1.2964893358002882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2964893358002882e-05

Optimization complete. Final v2v error: 3.029266595840454 mm

Highest mean error: 3.293027639389038 mm for frame 177

Lowest mean error: 2.8783228397369385 mm for frame 142

Saving results

Total time: 41.40508818626404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437157
Iteration 2/25 | Loss: 0.00125657
Iteration 3/25 | Loss: 0.00119814
Iteration 4/25 | Loss: 0.00118500
Iteration 5/25 | Loss: 0.00118085
Iteration 6/25 | Loss: 0.00118006
Iteration 7/25 | Loss: 0.00118006
Iteration 8/25 | Loss: 0.00118006
Iteration 9/25 | Loss: 0.00118006
Iteration 10/25 | Loss: 0.00118006
Iteration 11/25 | Loss: 0.00118006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011800561333075166, 0.0011800561333075166, 0.0011800561333075166, 0.0011800561333075166, 0.0011800561333075166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011800561333075166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45875752
Iteration 2/25 | Loss: 0.00134830
Iteration 3/25 | Loss: 0.00134830
Iteration 4/25 | Loss: 0.00134830
Iteration 5/25 | Loss: 0.00134830
Iteration 6/25 | Loss: 0.00134830
Iteration 7/25 | Loss: 0.00134830
Iteration 8/25 | Loss: 0.00134830
Iteration 9/25 | Loss: 0.00134830
Iteration 10/25 | Loss: 0.00134830
Iteration 11/25 | Loss: 0.00134830
Iteration 12/25 | Loss: 0.00134830
Iteration 13/25 | Loss: 0.00134830
Iteration 14/25 | Loss: 0.00134830
Iteration 15/25 | Loss: 0.00134830
Iteration 16/25 | Loss: 0.00134830
Iteration 17/25 | Loss: 0.00134830
Iteration 18/25 | Loss: 0.00134830
Iteration 19/25 | Loss: 0.00134830
Iteration 20/25 | Loss: 0.00134830
Iteration 21/25 | Loss: 0.00134830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013482985086739063, 0.0013482985086739063, 0.0013482985086739063, 0.0013482985086739063, 0.0013482985086739063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013482985086739063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134830
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001614
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001455
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001223
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001217
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001193
Iteration 144/1000 | Loss: 0.00001193
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001192
Iteration 149/1000 | Loss: 0.00001192
Iteration 150/1000 | Loss: 0.00001192
Iteration 151/1000 | Loss: 0.00001192
Iteration 152/1000 | Loss: 0.00001192
Iteration 153/1000 | Loss: 0.00001192
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.1908392480108887e-05, 1.1908392480108887e-05, 1.1908392480108887e-05, 1.1908392480108887e-05, 1.1908392480108887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1908392480108887e-05

Optimization complete. Final v2v error: 2.978691816329956 mm

Highest mean error: 3.358382225036621 mm for frame 113

Lowest mean error: 2.877912759780884 mm for frame 89

Saving results

Total time: 42.14580583572388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735215
Iteration 2/25 | Loss: 0.00151711
Iteration 3/25 | Loss: 0.00130873
Iteration 4/25 | Loss: 0.00128948
Iteration 5/25 | Loss: 0.00128708
Iteration 6/25 | Loss: 0.00128706
Iteration 7/25 | Loss: 0.00128706
Iteration 8/25 | Loss: 0.00128706
Iteration 9/25 | Loss: 0.00128706
Iteration 10/25 | Loss: 0.00128706
Iteration 11/25 | Loss: 0.00128706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012870605569332838, 0.0012870605569332838, 0.0012870605569332838, 0.0012870605569332838, 0.0012870605569332838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012870605569332838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51429105
Iteration 2/25 | Loss: 0.00136418
Iteration 3/25 | Loss: 0.00136417
Iteration 4/25 | Loss: 0.00136417
Iteration 5/25 | Loss: 0.00136416
Iteration 6/25 | Loss: 0.00136416
Iteration 7/25 | Loss: 0.00136416
Iteration 8/25 | Loss: 0.00136416
Iteration 9/25 | Loss: 0.00136416
Iteration 10/25 | Loss: 0.00136416
Iteration 11/25 | Loss: 0.00136416
Iteration 12/25 | Loss: 0.00136416
Iteration 13/25 | Loss: 0.00136416
Iteration 14/25 | Loss: 0.00136416
Iteration 15/25 | Loss: 0.00136416
Iteration 16/25 | Loss: 0.00136416
Iteration 17/25 | Loss: 0.00136416
Iteration 18/25 | Loss: 0.00136416
Iteration 19/25 | Loss: 0.00136416
Iteration 20/25 | Loss: 0.00136416
Iteration 21/25 | Loss: 0.00136416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013641624245792627, 0.0013641624245792627, 0.0013641624245792627, 0.0013641624245792627, 0.0013641624245792627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013641624245792627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136416
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00002379
Iteration 4/1000 | Loss: 0.00002119
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001942
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001869
Iteration 9/1000 | Loss: 0.00001826
Iteration 10/1000 | Loss: 0.00001794
Iteration 11/1000 | Loss: 0.00001772
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001718
Iteration 22/1000 | Loss: 0.00001715
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001700
Iteration 35/1000 | Loss: 0.00001700
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001699
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00001698
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001694
Iteration 43/1000 | Loss: 0.00001694
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001689
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001686
Iteration 55/1000 | Loss: 0.00001686
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001684
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001682
Iteration 61/1000 | Loss: 0.00001682
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001680
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001679
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001672
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001671
Iteration 87/1000 | Loss: 0.00001671
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001669
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001668
Iteration 95/1000 | Loss: 0.00001668
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001667
Iteration 98/1000 | Loss: 0.00001667
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001667
Iteration 106/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.667104152147658e-05, 1.667104152147658e-05, 1.667104152147658e-05, 1.667104152147658e-05, 1.667104152147658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.667104152147658e-05

Optimization complete. Final v2v error: 3.3607165813446045 mm

Highest mean error: 4.360823154449463 mm for frame 188

Lowest mean error: 2.7722158432006836 mm for frame 16

Saving results

Total time: 44.24998927116394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724086
Iteration 2/25 | Loss: 0.00127386
Iteration 3/25 | Loss: 0.00119633
Iteration 4/25 | Loss: 0.00118656
Iteration 5/25 | Loss: 0.00118433
Iteration 6/25 | Loss: 0.00118392
Iteration 7/25 | Loss: 0.00118392
Iteration 8/25 | Loss: 0.00118392
Iteration 9/25 | Loss: 0.00118392
Iteration 10/25 | Loss: 0.00118392
Iteration 11/25 | Loss: 0.00118392
Iteration 12/25 | Loss: 0.00118392
Iteration 13/25 | Loss: 0.00118392
Iteration 14/25 | Loss: 0.00118392
Iteration 15/25 | Loss: 0.00118392
Iteration 16/25 | Loss: 0.00118392
Iteration 17/25 | Loss: 0.00118392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011839159997180104, 0.0011839159997180104, 0.0011839159997180104, 0.0011839159997180104, 0.0011839159997180104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011839159997180104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32058275
Iteration 2/25 | Loss: 0.00135903
Iteration 3/25 | Loss: 0.00135903
Iteration 4/25 | Loss: 0.00135903
Iteration 5/25 | Loss: 0.00135903
Iteration 6/25 | Loss: 0.00135903
Iteration 7/25 | Loss: 0.00135903
Iteration 8/25 | Loss: 0.00135903
Iteration 9/25 | Loss: 0.00135903
Iteration 10/25 | Loss: 0.00135903
Iteration 11/25 | Loss: 0.00135903
Iteration 12/25 | Loss: 0.00135903
Iteration 13/25 | Loss: 0.00135903
Iteration 14/25 | Loss: 0.00135903
Iteration 15/25 | Loss: 0.00135903
Iteration 16/25 | Loss: 0.00135903
Iteration 17/25 | Loss: 0.00135903
Iteration 18/25 | Loss: 0.00135903
Iteration 19/25 | Loss: 0.00135903
Iteration 20/25 | Loss: 0.00135903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013590253656730056, 0.0013590253656730056, 0.0013590253656730056, 0.0013590253656730056, 0.0013590253656730056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013590253656730056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135903
Iteration 2/1000 | Loss: 0.00001987
Iteration 3/1000 | Loss: 0.00001578
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001403
Iteration 6/1000 | Loss: 0.00001363
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001241
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001224
Iteration 17/1000 | Loss: 0.00001214
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001192
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.1712658306350932e-05, 1.1712658306350932e-05, 1.1712658306350932e-05, 1.1712658306350932e-05, 1.1712658306350932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1712658306350932e-05

Optimization complete. Final v2v error: 2.9372758865356445 mm

Highest mean error: 3.2755885124206543 mm for frame 115

Lowest mean error: 2.8834598064422607 mm for frame 106

Saving results

Total time: 34.91020655632019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847470
Iteration 2/25 | Loss: 0.00130282
Iteration 3/25 | Loss: 0.00123219
Iteration 4/25 | Loss: 0.00121862
Iteration 5/25 | Loss: 0.00121538
Iteration 6/25 | Loss: 0.00121521
Iteration 7/25 | Loss: 0.00121521
Iteration 8/25 | Loss: 0.00121521
Iteration 9/25 | Loss: 0.00121521
Iteration 10/25 | Loss: 0.00121521
Iteration 11/25 | Loss: 0.00121521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012152092531323433, 0.0012152092531323433, 0.0012152092531323433, 0.0012152092531323433, 0.0012152092531323433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012152092531323433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23597741
Iteration 2/25 | Loss: 0.00119958
Iteration 3/25 | Loss: 0.00119949
Iteration 4/25 | Loss: 0.00119949
Iteration 5/25 | Loss: 0.00119949
Iteration 6/25 | Loss: 0.00119949
Iteration 7/25 | Loss: 0.00119949
Iteration 8/25 | Loss: 0.00119949
Iteration 9/25 | Loss: 0.00119949
Iteration 10/25 | Loss: 0.00119949
Iteration 11/25 | Loss: 0.00119949
Iteration 12/25 | Loss: 0.00119949
Iteration 13/25 | Loss: 0.00119949
Iteration 14/25 | Loss: 0.00119949
Iteration 15/25 | Loss: 0.00119949
Iteration 16/25 | Loss: 0.00119949
Iteration 17/25 | Loss: 0.00119949
Iteration 18/25 | Loss: 0.00119949
Iteration 19/25 | Loss: 0.00119949
Iteration 20/25 | Loss: 0.00119949
Iteration 21/25 | Loss: 0.00119949
Iteration 22/25 | Loss: 0.00119949
Iteration 23/25 | Loss: 0.00119949
Iteration 24/25 | Loss: 0.00119949
Iteration 25/25 | Loss: 0.00119949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119949
Iteration 2/1000 | Loss: 0.00002888
Iteration 3/1000 | Loss: 0.00001792
Iteration 4/1000 | Loss: 0.00001586
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001205
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001187
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001165
Iteration 113/1000 | Loss: 0.00001165
Iteration 114/1000 | Loss: 0.00001165
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001164
Iteration 121/1000 | Loss: 0.00001164
Iteration 122/1000 | Loss: 0.00001164
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.1640579941740725e-05, 1.1640579941740725e-05, 1.1640579941740725e-05, 1.1640579941740725e-05, 1.1640579941740725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1640579941740725e-05

Optimization complete. Final v2v error: 2.9445762634277344 mm

Highest mean error: 3.1685805320739746 mm for frame 69

Lowest mean error: 2.8073651790618896 mm for frame 160

Saving results

Total time: 41.219048261642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753669
Iteration 2/25 | Loss: 0.00149494
Iteration 3/25 | Loss: 0.00124303
Iteration 4/25 | Loss: 0.00122801
Iteration 5/25 | Loss: 0.00122657
Iteration 6/25 | Loss: 0.00122657
Iteration 7/25 | Loss: 0.00122657
Iteration 8/25 | Loss: 0.00122657
Iteration 9/25 | Loss: 0.00122657
Iteration 10/25 | Loss: 0.00122657
Iteration 11/25 | Loss: 0.00122657
Iteration 12/25 | Loss: 0.00122657
Iteration 13/25 | Loss: 0.00122657
Iteration 14/25 | Loss: 0.00122657
Iteration 15/25 | Loss: 0.00122657
Iteration 16/25 | Loss: 0.00122657
Iteration 17/25 | Loss: 0.00122657
Iteration 18/25 | Loss: 0.00122657
Iteration 19/25 | Loss: 0.00122657
Iteration 20/25 | Loss: 0.00122657
Iteration 21/25 | Loss: 0.00122657
Iteration 22/25 | Loss: 0.00122657
Iteration 23/25 | Loss: 0.00122657
Iteration 24/25 | Loss: 0.00122657
Iteration 25/25 | Loss: 0.00122657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012265732511878014, 0.0012265732511878014, 0.0012265732511878014, 0.0012265732511878014, 0.0012265732511878014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012265732511878014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25370610
Iteration 2/25 | Loss: 0.00101411
Iteration 3/25 | Loss: 0.00101410
Iteration 4/25 | Loss: 0.00101410
Iteration 5/25 | Loss: 0.00101410
Iteration 6/25 | Loss: 0.00101410
Iteration 7/25 | Loss: 0.00101410
Iteration 8/25 | Loss: 0.00101410
Iteration 9/25 | Loss: 0.00101410
Iteration 10/25 | Loss: 0.00101409
Iteration 11/25 | Loss: 0.00101409
Iteration 12/25 | Loss: 0.00101409
Iteration 13/25 | Loss: 0.00101409
Iteration 14/25 | Loss: 0.00101409
Iteration 15/25 | Loss: 0.00101409
Iteration 16/25 | Loss: 0.00101409
Iteration 17/25 | Loss: 0.00101409
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010140945669263601, 0.0010140945669263601, 0.0010140945669263601, 0.0010140945669263601, 0.0010140945669263601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010140945669263601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101409
Iteration 2/1000 | Loss: 0.00002509
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001472
Iteration 5/1000 | Loss: 0.00001398
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001314
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001164
Iteration 20/1000 | Loss: 0.00001163
Iteration 21/1000 | Loss: 0.00001163
Iteration 22/1000 | Loss: 0.00001163
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001160
Iteration 33/1000 | Loss: 0.00001160
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001150
Iteration 40/1000 | Loss: 0.00001150
Iteration 41/1000 | Loss: 0.00001150
Iteration 42/1000 | Loss: 0.00001150
Iteration 43/1000 | Loss: 0.00001150
Iteration 44/1000 | Loss: 0.00001150
Iteration 45/1000 | Loss: 0.00001149
Iteration 46/1000 | Loss: 0.00001149
Iteration 47/1000 | Loss: 0.00001149
Iteration 48/1000 | Loss: 0.00001149
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001149
Iteration 51/1000 | Loss: 0.00001147
Iteration 52/1000 | Loss: 0.00001147
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001144
Iteration 58/1000 | Loss: 0.00001144
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001144
Iteration 61/1000 | Loss: 0.00001143
Iteration 62/1000 | Loss: 0.00001143
Iteration 63/1000 | Loss: 0.00001142
Iteration 64/1000 | Loss: 0.00001142
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001133
Iteration 80/1000 | Loss: 0.00001133
Iteration 81/1000 | Loss: 0.00001133
Iteration 82/1000 | Loss: 0.00001133
Iteration 83/1000 | Loss: 0.00001132
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001131
Iteration 87/1000 | Loss: 0.00001131
Iteration 88/1000 | Loss: 0.00001131
Iteration 89/1000 | Loss: 0.00001131
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001130
Iteration 93/1000 | Loss: 0.00001130
Iteration 94/1000 | Loss: 0.00001130
Iteration 95/1000 | Loss: 0.00001130
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001129
Iteration 100/1000 | Loss: 0.00001129
Iteration 101/1000 | Loss: 0.00001129
Iteration 102/1000 | Loss: 0.00001129
Iteration 103/1000 | Loss: 0.00001128
Iteration 104/1000 | Loss: 0.00001128
Iteration 105/1000 | Loss: 0.00001128
Iteration 106/1000 | Loss: 0.00001128
Iteration 107/1000 | Loss: 0.00001128
Iteration 108/1000 | Loss: 0.00001127
Iteration 109/1000 | Loss: 0.00001127
Iteration 110/1000 | Loss: 0.00001127
Iteration 111/1000 | Loss: 0.00001127
Iteration 112/1000 | Loss: 0.00001127
Iteration 113/1000 | Loss: 0.00001126
Iteration 114/1000 | Loss: 0.00001126
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001124
Iteration 129/1000 | Loss: 0.00001124
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001124
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001123
Iteration 144/1000 | Loss: 0.00001123
Iteration 145/1000 | Loss: 0.00001123
Iteration 146/1000 | Loss: 0.00001123
Iteration 147/1000 | Loss: 0.00001123
Iteration 148/1000 | Loss: 0.00001123
Iteration 149/1000 | Loss: 0.00001123
Iteration 150/1000 | Loss: 0.00001123
Iteration 151/1000 | Loss: 0.00001123
Iteration 152/1000 | Loss: 0.00001123
Iteration 153/1000 | Loss: 0.00001123
Iteration 154/1000 | Loss: 0.00001123
Iteration 155/1000 | Loss: 0.00001123
Iteration 156/1000 | Loss: 0.00001123
Iteration 157/1000 | Loss: 0.00001123
Iteration 158/1000 | Loss: 0.00001123
Iteration 159/1000 | Loss: 0.00001123
Iteration 160/1000 | Loss: 0.00001123
Iteration 161/1000 | Loss: 0.00001123
Iteration 162/1000 | Loss: 0.00001123
Iteration 163/1000 | Loss: 0.00001123
Iteration 164/1000 | Loss: 0.00001123
Iteration 165/1000 | Loss: 0.00001123
Iteration 166/1000 | Loss: 0.00001123
Iteration 167/1000 | Loss: 0.00001123
Iteration 168/1000 | Loss: 0.00001123
Iteration 169/1000 | Loss: 0.00001123
Iteration 170/1000 | Loss: 0.00001123
Iteration 171/1000 | Loss: 0.00001123
Iteration 172/1000 | Loss: 0.00001123
Iteration 173/1000 | Loss: 0.00001123
Iteration 174/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1228670700802468e-05, 1.1228670700802468e-05, 1.1228670700802468e-05, 1.1228670700802468e-05, 1.1228670700802468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1228670700802468e-05

Optimization complete. Final v2v error: 2.851844072341919 mm

Highest mean error: 3.046569347381592 mm for frame 138

Lowest mean error: 2.706584930419922 mm for frame 176

Saving results

Total time: 43.70853042602539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531693
Iteration 2/25 | Loss: 0.00125613
Iteration 3/25 | Loss: 0.00117780
Iteration 4/25 | Loss: 0.00116454
Iteration 5/25 | Loss: 0.00116026
Iteration 6/25 | Loss: 0.00115972
Iteration 7/25 | Loss: 0.00115972
Iteration 8/25 | Loss: 0.00115972
Iteration 9/25 | Loss: 0.00115972
Iteration 10/25 | Loss: 0.00115972
Iteration 11/25 | Loss: 0.00115972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011597168631851673, 0.0011597168631851673, 0.0011597168631851673, 0.0011597168631851673, 0.0011597168631851673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011597168631851673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56635928
Iteration 2/25 | Loss: 0.00133127
Iteration 3/25 | Loss: 0.00133127
Iteration 4/25 | Loss: 0.00133127
Iteration 5/25 | Loss: 0.00133127
Iteration 6/25 | Loss: 0.00133127
Iteration 7/25 | Loss: 0.00133127
Iteration 8/25 | Loss: 0.00133127
Iteration 9/25 | Loss: 0.00133127
Iteration 10/25 | Loss: 0.00133127
Iteration 11/25 | Loss: 0.00133127
Iteration 12/25 | Loss: 0.00133127
Iteration 13/25 | Loss: 0.00133127
Iteration 14/25 | Loss: 0.00133127
Iteration 15/25 | Loss: 0.00133127
Iteration 16/25 | Loss: 0.00133127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001331267412751913, 0.001331267412751913, 0.001331267412751913, 0.001331267412751913, 0.001331267412751913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001331267412751913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133127
Iteration 2/1000 | Loss: 0.00001916
Iteration 3/1000 | Loss: 0.00001484
Iteration 4/1000 | Loss: 0.00001369
Iteration 5/1000 | Loss: 0.00001305
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001196
Iteration 8/1000 | Loss: 0.00001159
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001105
Iteration 11/1000 | Loss: 0.00001086
Iteration 12/1000 | Loss: 0.00001083
Iteration 13/1000 | Loss: 0.00001071
Iteration 14/1000 | Loss: 0.00001067
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001054
Iteration 19/1000 | Loss: 0.00001054
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001052
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001032
Iteration 27/1000 | Loss: 0.00001030
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001022
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001021
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001021
Iteration 36/1000 | Loss: 0.00001021
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001020
Iteration 39/1000 | Loss: 0.00001020
Iteration 40/1000 | Loss: 0.00001019
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00001017
Iteration 43/1000 | Loss: 0.00001017
Iteration 44/1000 | Loss: 0.00001017
Iteration 45/1000 | Loss: 0.00001017
Iteration 46/1000 | Loss: 0.00001017
Iteration 47/1000 | Loss: 0.00001016
Iteration 48/1000 | Loss: 0.00001016
Iteration 49/1000 | Loss: 0.00001016
Iteration 50/1000 | Loss: 0.00001016
Iteration 51/1000 | Loss: 0.00001016
Iteration 52/1000 | Loss: 0.00001016
Iteration 53/1000 | Loss: 0.00001016
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001015
Iteration 57/1000 | Loss: 0.00001015
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001014
Iteration 60/1000 | Loss: 0.00001014
Iteration 61/1000 | Loss: 0.00001013
Iteration 62/1000 | Loss: 0.00001013
Iteration 63/1000 | Loss: 0.00001013
Iteration 64/1000 | Loss: 0.00001013
Iteration 65/1000 | Loss: 0.00001013
Iteration 66/1000 | Loss: 0.00001012
Iteration 67/1000 | Loss: 0.00001012
Iteration 68/1000 | Loss: 0.00001012
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001011
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001010
Iteration 76/1000 | Loss: 0.00001010
Iteration 77/1000 | Loss: 0.00001009
Iteration 78/1000 | Loss: 0.00001009
Iteration 79/1000 | Loss: 0.00001008
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001008
Iteration 83/1000 | Loss: 0.00001008
Iteration 84/1000 | Loss: 0.00001007
Iteration 85/1000 | Loss: 0.00001007
Iteration 86/1000 | Loss: 0.00001007
Iteration 87/1000 | Loss: 0.00001007
Iteration 88/1000 | Loss: 0.00001007
Iteration 89/1000 | Loss: 0.00001006
Iteration 90/1000 | Loss: 0.00001006
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001006
Iteration 93/1000 | Loss: 0.00001005
Iteration 94/1000 | Loss: 0.00001005
Iteration 95/1000 | Loss: 0.00001005
Iteration 96/1000 | Loss: 0.00001004
Iteration 97/1000 | Loss: 0.00001004
Iteration 98/1000 | Loss: 0.00001004
Iteration 99/1000 | Loss: 0.00001004
Iteration 100/1000 | Loss: 0.00001004
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001004
Iteration 103/1000 | Loss: 0.00001004
Iteration 104/1000 | Loss: 0.00001004
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001002
Iteration 112/1000 | Loss: 0.00001002
Iteration 113/1000 | Loss: 0.00001002
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001001
Iteration 116/1000 | Loss: 0.00001001
Iteration 117/1000 | Loss: 0.00001001
Iteration 118/1000 | Loss: 0.00001001
Iteration 119/1000 | Loss: 0.00001001
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001000
Iteration 124/1000 | Loss: 0.00001000
Iteration 125/1000 | Loss: 0.00001000
Iteration 126/1000 | Loss: 0.00001000
Iteration 127/1000 | Loss: 0.00000999
Iteration 128/1000 | Loss: 0.00000998
Iteration 129/1000 | Loss: 0.00000998
Iteration 130/1000 | Loss: 0.00000997
Iteration 131/1000 | Loss: 0.00000997
Iteration 132/1000 | Loss: 0.00000997
Iteration 133/1000 | Loss: 0.00000997
Iteration 134/1000 | Loss: 0.00000997
Iteration 135/1000 | Loss: 0.00000997
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000996
Iteration 139/1000 | Loss: 0.00000996
Iteration 140/1000 | Loss: 0.00000996
Iteration 141/1000 | Loss: 0.00000996
Iteration 142/1000 | Loss: 0.00000996
Iteration 143/1000 | Loss: 0.00000996
Iteration 144/1000 | Loss: 0.00000996
Iteration 145/1000 | Loss: 0.00000996
Iteration 146/1000 | Loss: 0.00000996
Iteration 147/1000 | Loss: 0.00000996
Iteration 148/1000 | Loss: 0.00000995
Iteration 149/1000 | Loss: 0.00000995
Iteration 150/1000 | Loss: 0.00000995
Iteration 151/1000 | Loss: 0.00000995
Iteration 152/1000 | Loss: 0.00000995
Iteration 153/1000 | Loss: 0.00000995
Iteration 154/1000 | Loss: 0.00000995
Iteration 155/1000 | Loss: 0.00000995
Iteration 156/1000 | Loss: 0.00000995
Iteration 157/1000 | Loss: 0.00000995
Iteration 158/1000 | Loss: 0.00000995
Iteration 159/1000 | Loss: 0.00000995
Iteration 160/1000 | Loss: 0.00000995
Iteration 161/1000 | Loss: 0.00000995
Iteration 162/1000 | Loss: 0.00000994
Iteration 163/1000 | Loss: 0.00000994
Iteration 164/1000 | Loss: 0.00000994
Iteration 165/1000 | Loss: 0.00000994
Iteration 166/1000 | Loss: 0.00000994
Iteration 167/1000 | Loss: 0.00000994
Iteration 168/1000 | Loss: 0.00000994
Iteration 169/1000 | Loss: 0.00000994
Iteration 170/1000 | Loss: 0.00000994
Iteration 171/1000 | Loss: 0.00000994
Iteration 172/1000 | Loss: 0.00000994
Iteration 173/1000 | Loss: 0.00000994
Iteration 174/1000 | Loss: 0.00000994
Iteration 175/1000 | Loss: 0.00000994
Iteration 176/1000 | Loss: 0.00000994
Iteration 177/1000 | Loss: 0.00000994
Iteration 178/1000 | Loss: 0.00000994
Iteration 179/1000 | Loss: 0.00000994
Iteration 180/1000 | Loss: 0.00000993
Iteration 181/1000 | Loss: 0.00000993
Iteration 182/1000 | Loss: 0.00000993
Iteration 183/1000 | Loss: 0.00000993
Iteration 184/1000 | Loss: 0.00000993
Iteration 185/1000 | Loss: 0.00000993
Iteration 186/1000 | Loss: 0.00000993
Iteration 187/1000 | Loss: 0.00000993
Iteration 188/1000 | Loss: 0.00000993
Iteration 189/1000 | Loss: 0.00000993
Iteration 190/1000 | Loss: 0.00000993
Iteration 191/1000 | Loss: 0.00000993
Iteration 192/1000 | Loss: 0.00000993
Iteration 193/1000 | Loss: 0.00000993
Iteration 194/1000 | Loss: 0.00000993
Iteration 195/1000 | Loss: 0.00000993
Iteration 196/1000 | Loss: 0.00000993
Iteration 197/1000 | Loss: 0.00000993
Iteration 198/1000 | Loss: 0.00000993
Iteration 199/1000 | Loss: 0.00000993
Iteration 200/1000 | Loss: 0.00000993
Iteration 201/1000 | Loss: 0.00000993
Iteration 202/1000 | Loss: 0.00000993
Iteration 203/1000 | Loss: 0.00000993
Iteration 204/1000 | Loss: 0.00000993
Iteration 205/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [9.93071444099769e-06, 9.93071444099769e-06, 9.93071444099769e-06, 9.93071444099769e-06, 9.93071444099769e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.93071444099769e-06

Optimization complete. Final v2v error: 2.7367212772369385 mm

Highest mean error: 3.1027605533599854 mm for frame 151

Lowest mean error: 2.5256400108337402 mm for frame 35

Saving results

Total time: 44.93962383270264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795710
Iteration 2/25 | Loss: 0.00128896
Iteration 3/25 | Loss: 0.00118262
Iteration 4/25 | Loss: 0.00116474
Iteration 5/25 | Loss: 0.00116126
Iteration 6/25 | Loss: 0.00116126
Iteration 7/25 | Loss: 0.00116126
Iteration 8/25 | Loss: 0.00116126
Iteration 9/25 | Loss: 0.00116126
Iteration 10/25 | Loss: 0.00116126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011612619273364544, 0.0011612619273364544, 0.0011612619273364544, 0.0011612619273364544, 0.0011612619273364544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011612619273364544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28783870
Iteration 2/25 | Loss: 0.00142124
Iteration 3/25 | Loss: 0.00142124
Iteration 4/25 | Loss: 0.00142124
Iteration 5/25 | Loss: 0.00142124
Iteration 6/25 | Loss: 0.00142124
Iteration 7/25 | Loss: 0.00142124
Iteration 8/25 | Loss: 0.00142124
Iteration 9/25 | Loss: 0.00142124
Iteration 10/25 | Loss: 0.00142124
Iteration 11/25 | Loss: 0.00142124
Iteration 12/25 | Loss: 0.00142124
Iteration 13/25 | Loss: 0.00142124
Iteration 14/25 | Loss: 0.00142124
Iteration 15/25 | Loss: 0.00142124
Iteration 16/25 | Loss: 0.00142124
Iteration 17/25 | Loss: 0.00142124
Iteration 18/25 | Loss: 0.00142124
Iteration 19/25 | Loss: 0.00142124
Iteration 20/25 | Loss: 0.00142124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014212386449798942, 0.0014212386449798942, 0.0014212386449798942, 0.0014212386449798942, 0.0014212386449798942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014212386449798942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142124
Iteration 2/1000 | Loss: 0.00002229
Iteration 3/1000 | Loss: 0.00001531
Iteration 4/1000 | Loss: 0.00001344
Iteration 5/1000 | Loss: 0.00001277
Iteration 6/1000 | Loss: 0.00001209
Iteration 7/1000 | Loss: 0.00001158
Iteration 8/1000 | Loss: 0.00001124
Iteration 9/1000 | Loss: 0.00001092
Iteration 10/1000 | Loss: 0.00001071
Iteration 11/1000 | Loss: 0.00001057
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001035
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001022
Iteration 17/1000 | Loss: 0.00001020
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001008
Iteration 20/1000 | Loss: 0.00001007
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001004
Iteration 29/1000 | Loss: 0.00001004
Iteration 30/1000 | Loss: 0.00001003
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001002
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001001
Iteration 35/1000 | Loss: 0.00001001
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00001000
Iteration 39/1000 | Loss: 0.00000997
Iteration 40/1000 | Loss: 0.00000997
Iteration 41/1000 | Loss: 0.00000997
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000997
Iteration 48/1000 | Loss: 0.00000996
Iteration 49/1000 | Loss: 0.00000996
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000993
Iteration 54/1000 | Loss: 0.00000993
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000989
Iteration 66/1000 | Loss: 0.00000989
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000987
Iteration 70/1000 | Loss: 0.00000987
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000986
Iteration 75/1000 | Loss: 0.00000986
Iteration 76/1000 | Loss: 0.00000986
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000985
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000984
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000983
Iteration 89/1000 | Loss: 0.00000983
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000982
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000982
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000981
Iteration 99/1000 | Loss: 0.00000981
Iteration 100/1000 | Loss: 0.00000981
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000980
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000979
Iteration 112/1000 | Loss: 0.00000979
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000978
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000977
Iteration 119/1000 | Loss: 0.00000977
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000975
Iteration 124/1000 | Loss: 0.00000975
Iteration 125/1000 | Loss: 0.00000975
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000974
Iteration 128/1000 | Loss: 0.00000974
Iteration 129/1000 | Loss: 0.00000974
Iteration 130/1000 | Loss: 0.00000974
Iteration 131/1000 | Loss: 0.00000974
Iteration 132/1000 | Loss: 0.00000973
Iteration 133/1000 | Loss: 0.00000973
Iteration 134/1000 | Loss: 0.00000973
Iteration 135/1000 | Loss: 0.00000973
Iteration 136/1000 | Loss: 0.00000973
Iteration 137/1000 | Loss: 0.00000973
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000972
Iteration 140/1000 | Loss: 0.00000972
Iteration 141/1000 | Loss: 0.00000972
Iteration 142/1000 | Loss: 0.00000972
Iteration 143/1000 | Loss: 0.00000972
Iteration 144/1000 | Loss: 0.00000972
Iteration 145/1000 | Loss: 0.00000971
Iteration 146/1000 | Loss: 0.00000971
Iteration 147/1000 | Loss: 0.00000971
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000970
Iteration 156/1000 | Loss: 0.00000970
Iteration 157/1000 | Loss: 0.00000970
Iteration 158/1000 | Loss: 0.00000970
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000969
Iteration 161/1000 | Loss: 0.00000969
Iteration 162/1000 | Loss: 0.00000969
Iteration 163/1000 | Loss: 0.00000969
Iteration 164/1000 | Loss: 0.00000969
Iteration 165/1000 | Loss: 0.00000969
Iteration 166/1000 | Loss: 0.00000969
Iteration 167/1000 | Loss: 0.00000969
Iteration 168/1000 | Loss: 0.00000969
Iteration 169/1000 | Loss: 0.00000969
Iteration 170/1000 | Loss: 0.00000969
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000969
Iteration 177/1000 | Loss: 0.00000969
Iteration 178/1000 | Loss: 0.00000969
Iteration 179/1000 | Loss: 0.00000969
Iteration 180/1000 | Loss: 0.00000969
Iteration 181/1000 | Loss: 0.00000969
Iteration 182/1000 | Loss: 0.00000969
Iteration 183/1000 | Loss: 0.00000969
Iteration 184/1000 | Loss: 0.00000969
Iteration 185/1000 | Loss: 0.00000969
Iteration 186/1000 | Loss: 0.00000969
Iteration 187/1000 | Loss: 0.00000969
Iteration 188/1000 | Loss: 0.00000969
Iteration 189/1000 | Loss: 0.00000969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [9.685743862064555e-06, 9.685743862064555e-06, 9.685743862064555e-06, 9.685743862064555e-06, 9.685743862064555e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.685743862064555e-06

Optimization complete. Final v2v error: 2.648679494857788 mm

Highest mean error: 3.041238784790039 mm for frame 101

Lowest mean error: 2.3733255863189697 mm for frame 232

Saving results

Total time: 48.972089767456055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832179
Iteration 2/25 | Loss: 0.00192798
Iteration 3/25 | Loss: 0.00144803
Iteration 4/25 | Loss: 0.00131900
Iteration 5/25 | Loss: 0.00130147
Iteration 6/25 | Loss: 0.00129895
Iteration 7/25 | Loss: 0.00129895
Iteration 8/25 | Loss: 0.00129895
Iteration 9/25 | Loss: 0.00129895
Iteration 10/25 | Loss: 0.00129895
Iteration 11/25 | Loss: 0.00129895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012989522656425834, 0.0012989522656425834, 0.0012989522656425834, 0.0012989522656425834, 0.0012989522656425834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012989522656425834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27535152
Iteration 2/25 | Loss: 0.00126863
Iteration 3/25 | Loss: 0.00126862
Iteration 4/25 | Loss: 0.00126862
Iteration 5/25 | Loss: 0.00126862
Iteration 6/25 | Loss: 0.00126862
Iteration 7/25 | Loss: 0.00126862
Iteration 8/25 | Loss: 0.00126862
Iteration 9/25 | Loss: 0.00126862
Iteration 10/25 | Loss: 0.00126862
Iteration 11/25 | Loss: 0.00126862
Iteration 12/25 | Loss: 0.00126862
Iteration 13/25 | Loss: 0.00126862
Iteration 14/25 | Loss: 0.00126862
Iteration 15/25 | Loss: 0.00126862
Iteration 16/25 | Loss: 0.00126862
Iteration 17/25 | Loss: 0.00126862
Iteration 18/25 | Loss: 0.00126862
Iteration 19/25 | Loss: 0.00126862
Iteration 20/25 | Loss: 0.00126862
Iteration 21/25 | Loss: 0.00126862
Iteration 22/25 | Loss: 0.00126862
Iteration 23/25 | Loss: 0.00126862
Iteration 24/25 | Loss: 0.00126862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012686229310929775, 0.0012686229310929775, 0.0012686229310929775, 0.0012686229310929775, 0.0012686229310929775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012686229310929775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126862
Iteration 2/1000 | Loss: 0.00006282
Iteration 3/1000 | Loss: 0.00004259
Iteration 4/1000 | Loss: 0.00003325
Iteration 5/1000 | Loss: 0.00003050
Iteration 6/1000 | Loss: 0.00002939
Iteration 7/1000 | Loss: 0.00002849
Iteration 8/1000 | Loss: 0.00002805
Iteration 9/1000 | Loss: 0.00002755
Iteration 10/1000 | Loss: 0.00002716
Iteration 11/1000 | Loss: 0.00002695
Iteration 12/1000 | Loss: 0.00002670
Iteration 13/1000 | Loss: 0.00002658
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00002628
Iteration 16/1000 | Loss: 0.00002613
Iteration 17/1000 | Loss: 0.00002612
Iteration 18/1000 | Loss: 0.00002609
Iteration 19/1000 | Loss: 0.00002603
Iteration 20/1000 | Loss: 0.00002599
Iteration 21/1000 | Loss: 0.00002598
Iteration 22/1000 | Loss: 0.00002598
Iteration 23/1000 | Loss: 0.00002598
Iteration 24/1000 | Loss: 0.00002598
Iteration 25/1000 | Loss: 0.00002598
Iteration 26/1000 | Loss: 0.00002598
Iteration 27/1000 | Loss: 0.00002598
Iteration 28/1000 | Loss: 0.00002598
Iteration 29/1000 | Loss: 0.00002597
Iteration 30/1000 | Loss: 0.00002597
Iteration 31/1000 | Loss: 0.00002597
Iteration 32/1000 | Loss: 0.00002597
Iteration 33/1000 | Loss: 0.00002597
Iteration 34/1000 | Loss: 0.00002597
Iteration 35/1000 | Loss: 0.00002597
Iteration 36/1000 | Loss: 0.00002595
Iteration 37/1000 | Loss: 0.00002595
Iteration 38/1000 | Loss: 0.00002595
Iteration 39/1000 | Loss: 0.00002595
Iteration 40/1000 | Loss: 0.00002594
Iteration 41/1000 | Loss: 0.00002594
Iteration 42/1000 | Loss: 0.00002594
Iteration 43/1000 | Loss: 0.00002594
Iteration 44/1000 | Loss: 0.00002594
Iteration 45/1000 | Loss: 0.00002593
Iteration 46/1000 | Loss: 0.00002593
Iteration 47/1000 | Loss: 0.00002593
Iteration 48/1000 | Loss: 0.00002592
Iteration 49/1000 | Loss: 0.00002592
Iteration 50/1000 | Loss: 0.00002591
Iteration 51/1000 | Loss: 0.00002591
Iteration 52/1000 | Loss: 0.00002591
Iteration 53/1000 | Loss: 0.00002591
Iteration 54/1000 | Loss: 0.00002591
Iteration 55/1000 | Loss: 0.00002591
Iteration 56/1000 | Loss: 0.00002591
Iteration 57/1000 | Loss: 0.00002591
Iteration 58/1000 | Loss: 0.00002591
Iteration 59/1000 | Loss: 0.00002590
Iteration 60/1000 | Loss: 0.00002590
Iteration 61/1000 | Loss: 0.00002590
Iteration 62/1000 | Loss: 0.00002589
Iteration 63/1000 | Loss: 0.00002589
Iteration 64/1000 | Loss: 0.00002589
Iteration 65/1000 | Loss: 0.00002589
Iteration 66/1000 | Loss: 0.00002589
Iteration 67/1000 | Loss: 0.00002589
Iteration 68/1000 | Loss: 0.00002588
Iteration 69/1000 | Loss: 0.00002588
Iteration 70/1000 | Loss: 0.00002588
Iteration 71/1000 | Loss: 0.00002588
Iteration 72/1000 | Loss: 0.00002587
Iteration 73/1000 | Loss: 0.00002587
Iteration 74/1000 | Loss: 0.00002587
Iteration 75/1000 | Loss: 0.00002587
Iteration 76/1000 | Loss: 0.00002587
Iteration 77/1000 | Loss: 0.00002587
Iteration 78/1000 | Loss: 0.00002587
Iteration 79/1000 | Loss: 0.00002587
Iteration 80/1000 | Loss: 0.00002587
Iteration 81/1000 | Loss: 0.00002587
Iteration 82/1000 | Loss: 0.00002586
Iteration 83/1000 | Loss: 0.00002586
Iteration 84/1000 | Loss: 0.00002586
Iteration 85/1000 | Loss: 0.00002585
Iteration 86/1000 | Loss: 0.00002585
Iteration 87/1000 | Loss: 0.00002585
Iteration 88/1000 | Loss: 0.00002585
Iteration 89/1000 | Loss: 0.00002585
Iteration 90/1000 | Loss: 0.00002585
Iteration 91/1000 | Loss: 0.00002585
Iteration 92/1000 | Loss: 0.00002585
Iteration 93/1000 | Loss: 0.00002584
Iteration 94/1000 | Loss: 0.00002584
Iteration 95/1000 | Loss: 0.00002584
Iteration 96/1000 | Loss: 0.00002584
Iteration 97/1000 | Loss: 0.00002584
Iteration 98/1000 | Loss: 0.00002584
Iteration 99/1000 | Loss: 0.00002584
Iteration 100/1000 | Loss: 0.00002584
Iteration 101/1000 | Loss: 0.00002583
Iteration 102/1000 | Loss: 0.00002583
Iteration 103/1000 | Loss: 0.00002583
Iteration 104/1000 | Loss: 0.00002583
Iteration 105/1000 | Loss: 0.00002583
Iteration 106/1000 | Loss: 0.00002583
Iteration 107/1000 | Loss: 0.00002583
Iteration 108/1000 | Loss: 0.00002583
Iteration 109/1000 | Loss: 0.00002583
Iteration 110/1000 | Loss: 0.00002583
Iteration 111/1000 | Loss: 0.00002583
Iteration 112/1000 | Loss: 0.00002582
Iteration 113/1000 | Loss: 0.00002582
Iteration 114/1000 | Loss: 0.00002582
Iteration 115/1000 | Loss: 0.00002582
Iteration 116/1000 | Loss: 0.00002581
Iteration 117/1000 | Loss: 0.00002581
Iteration 118/1000 | Loss: 0.00002581
Iteration 119/1000 | Loss: 0.00002581
Iteration 120/1000 | Loss: 0.00002581
Iteration 121/1000 | Loss: 0.00002580
Iteration 122/1000 | Loss: 0.00002580
Iteration 123/1000 | Loss: 0.00002580
Iteration 124/1000 | Loss: 0.00002580
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002580
Iteration 127/1000 | Loss: 0.00002580
Iteration 128/1000 | Loss: 0.00002580
Iteration 129/1000 | Loss: 0.00002580
Iteration 130/1000 | Loss: 0.00002579
Iteration 131/1000 | Loss: 0.00002579
Iteration 132/1000 | Loss: 0.00002579
Iteration 133/1000 | Loss: 0.00002579
Iteration 134/1000 | Loss: 0.00002579
Iteration 135/1000 | Loss: 0.00002579
Iteration 136/1000 | Loss: 0.00002578
Iteration 137/1000 | Loss: 0.00002578
Iteration 138/1000 | Loss: 0.00002578
Iteration 139/1000 | Loss: 0.00002578
Iteration 140/1000 | Loss: 0.00002578
Iteration 141/1000 | Loss: 0.00002578
Iteration 142/1000 | Loss: 0.00002578
Iteration 143/1000 | Loss: 0.00002578
Iteration 144/1000 | Loss: 0.00002578
Iteration 145/1000 | Loss: 0.00002578
Iteration 146/1000 | Loss: 0.00002578
Iteration 147/1000 | Loss: 0.00002578
Iteration 148/1000 | Loss: 0.00002578
Iteration 149/1000 | Loss: 0.00002578
Iteration 150/1000 | Loss: 0.00002578
Iteration 151/1000 | Loss: 0.00002578
Iteration 152/1000 | Loss: 0.00002577
Iteration 153/1000 | Loss: 0.00002577
Iteration 154/1000 | Loss: 0.00002577
Iteration 155/1000 | Loss: 0.00002577
Iteration 156/1000 | Loss: 0.00002577
Iteration 157/1000 | Loss: 0.00002577
Iteration 158/1000 | Loss: 0.00002577
Iteration 159/1000 | Loss: 0.00002577
Iteration 160/1000 | Loss: 0.00002577
Iteration 161/1000 | Loss: 0.00002577
Iteration 162/1000 | Loss: 0.00002577
Iteration 163/1000 | Loss: 0.00002577
Iteration 164/1000 | Loss: 0.00002577
Iteration 165/1000 | Loss: 0.00002577
Iteration 166/1000 | Loss: 0.00002577
Iteration 167/1000 | Loss: 0.00002577
Iteration 168/1000 | Loss: 0.00002577
Iteration 169/1000 | Loss: 0.00002577
Iteration 170/1000 | Loss: 0.00002577
Iteration 171/1000 | Loss: 0.00002577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.577187478891574e-05, 2.577187478891574e-05, 2.577187478891574e-05, 2.577187478891574e-05, 2.577187478891574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.577187478891574e-05

Optimization complete. Final v2v error: 4.300039768218994 mm

Highest mean error: 4.880494594573975 mm for frame 29

Lowest mean error: 3.5344903469085693 mm for frame 127

Saving results

Total time: 42.19879102706909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948885
Iteration 2/25 | Loss: 0.00262150
Iteration 3/25 | Loss: 0.00202220
Iteration 4/25 | Loss: 0.00211313
Iteration 5/25 | Loss: 0.00177780
Iteration 6/25 | Loss: 0.00155996
Iteration 7/25 | Loss: 0.00149606
Iteration 8/25 | Loss: 0.00146739
Iteration 9/25 | Loss: 0.00146129
Iteration 10/25 | Loss: 0.00144958
Iteration 11/25 | Loss: 0.00144025
Iteration 12/25 | Loss: 0.00143660
Iteration 13/25 | Loss: 0.00143120
Iteration 14/25 | Loss: 0.00142905
Iteration 15/25 | Loss: 0.00142927
Iteration 16/25 | Loss: 0.00142874
Iteration 17/25 | Loss: 0.00142814
Iteration 18/25 | Loss: 0.00142734
Iteration 19/25 | Loss: 0.00142769
Iteration 20/25 | Loss: 0.00142811
Iteration 21/25 | Loss: 0.00142636
Iteration 22/25 | Loss: 0.00142586
Iteration 23/25 | Loss: 0.00142552
Iteration 24/25 | Loss: 0.00142535
Iteration 25/25 | Loss: 0.00142438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22188044
Iteration 2/25 | Loss: 0.00259425
Iteration 3/25 | Loss: 0.00259425
Iteration 4/25 | Loss: 0.00259425
Iteration 5/25 | Loss: 0.00259425
Iteration 6/25 | Loss: 0.00259425
Iteration 7/25 | Loss: 0.00259425
Iteration 8/25 | Loss: 0.00259425
Iteration 9/25 | Loss: 0.00259425
Iteration 10/25 | Loss: 0.00259425
Iteration 11/25 | Loss: 0.00259425
Iteration 12/25 | Loss: 0.00259425
Iteration 13/25 | Loss: 0.00259425
Iteration 14/25 | Loss: 0.00259425
Iteration 15/25 | Loss: 0.00259425
Iteration 16/25 | Loss: 0.00259425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002594252349808812, 0.002594252349808812, 0.002594252349808812, 0.002594252349808812, 0.002594252349808812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002594252349808812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259425
Iteration 2/1000 | Loss: 0.00021455
Iteration 3/1000 | Loss: 0.00016527
Iteration 4/1000 | Loss: 0.00029320
Iteration 5/1000 | Loss: 0.00012793
Iteration 6/1000 | Loss: 0.00011247
Iteration 7/1000 | Loss: 0.00013507
Iteration 8/1000 | Loss: 0.00010655
Iteration 9/1000 | Loss: 0.00010078
Iteration 10/1000 | Loss: 0.00010897
Iteration 11/1000 | Loss: 0.00009672
Iteration 12/1000 | Loss: 0.00009350
Iteration 13/1000 | Loss: 0.00009147
Iteration 14/1000 | Loss: 0.00009031
Iteration 15/1000 | Loss: 0.00010455
Iteration 16/1000 | Loss: 0.00009737
Iteration 17/1000 | Loss: 0.00010368
Iteration 18/1000 | Loss: 0.00011377
Iteration 19/1000 | Loss: 0.00010602
Iteration 20/1000 | Loss: 0.00010519
Iteration 21/1000 | Loss: 0.00009972
Iteration 22/1000 | Loss: 0.00008876
Iteration 23/1000 | Loss: 0.00010115
Iteration 24/1000 | Loss: 0.00009695
Iteration 25/1000 | Loss: 0.00011367
Iteration 26/1000 | Loss: 0.00011134
Iteration 27/1000 | Loss: 0.00012410
Iteration 28/1000 | Loss: 0.00011267
Iteration 29/1000 | Loss: 0.00012490
Iteration 30/1000 | Loss: 0.00010635
Iteration 31/1000 | Loss: 0.00010800
Iteration 32/1000 | Loss: 0.00009188
Iteration 33/1000 | Loss: 0.00010099
Iteration 34/1000 | Loss: 0.00009099
Iteration 35/1000 | Loss: 0.00009065
Iteration 36/1000 | Loss: 0.00011288
Iteration 37/1000 | Loss: 0.00010463
Iteration 38/1000 | Loss: 0.00034104
Iteration 39/1000 | Loss: 0.00011524
Iteration 40/1000 | Loss: 0.00010910
Iteration 41/1000 | Loss: 0.00011894
Iteration 42/1000 | Loss: 0.00011105
Iteration 43/1000 | Loss: 0.00010377
Iteration 44/1000 | Loss: 0.00010409
Iteration 45/1000 | Loss: 0.00010088
Iteration 46/1000 | Loss: 0.00010352
Iteration 47/1000 | Loss: 0.00011975
Iteration 48/1000 | Loss: 0.00011544
Iteration 49/1000 | Loss: 0.00010938
Iteration 50/1000 | Loss: 0.00009672
Iteration 51/1000 | Loss: 0.00010698
Iteration 52/1000 | Loss: 0.00009946
Iteration 53/1000 | Loss: 0.00011582
Iteration 54/1000 | Loss: 0.00011127
Iteration 55/1000 | Loss: 0.00009868
Iteration 56/1000 | Loss: 0.00011332
Iteration 57/1000 | Loss: 0.00009842
Iteration 58/1000 | Loss: 0.00009895
Iteration 59/1000 | Loss: 0.00010257
Iteration 60/1000 | Loss: 0.00010178
Iteration 61/1000 | Loss: 0.00009818
Iteration 62/1000 | Loss: 0.00009433
Iteration 63/1000 | Loss: 0.00010061
Iteration 64/1000 | Loss: 0.00010095
Iteration 65/1000 | Loss: 0.00010101
Iteration 66/1000 | Loss: 0.00009945
Iteration 67/1000 | Loss: 0.00010453
Iteration 68/1000 | Loss: 0.00010218
Iteration 69/1000 | Loss: 0.00008903
Iteration 70/1000 | Loss: 0.00008820
Iteration 71/1000 | Loss: 0.00008755
Iteration 72/1000 | Loss: 0.00008706
Iteration 73/1000 | Loss: 0.00008621
Iteration 74/1000 | Loss: 0.00008522
Iteration 75/1000 | Loss: 0.00008497
Iteration 76/1000 | Loss: 0.00008497
Iteration 77/1000 | Loss: 0.00008479
Iteration 78/1000 | Loss: 0.00008463
Iteration 79/1000 | Loss: 0.00008458
Iteration 80/1000 | Loss: 0.00008451
Iteration 81/1000 | Loss: 0.00008450
Iteration 82/1000 | Loss: 0.00008437
Iteration 83/1000 | Loss: 0.00008424
Iteration 84/1000 | Loss: 0.00008402
Iteration 85/1000 | Loss: 0.00008394
Iteration 86/1000 | Loss: 0.00008394
Iteration 87/1000 | Loss: 0.00008389
Iteration 88/1000 | Loss: 0.00008381
Iteration 89/1000 | Loss: 0.00008374
Iteration 90/1000 | Loss: 0.00008366
Iteration 91/1000 | Loss: 0.00008358
Iteration 92/1000 | Loss: 0.00008357
Iteration 93/1000 | Loss: 0.00008357
Iteration 94/1000 | Loss: 0.00008357
Iteration 95/1000 | Loss: 0.00008356
Iteration 96/1000 | Loss: 0.00008356
Iteration 97/1000 | Loss: 0.00008356
Iteration 98/1000 | Loss: 0.00008354
Iteration 99/1000 | Loss: 0.00008353
Iteration 100/1000 | Loss: 0.00008353
Iteration 101/1000 | Loss: 0.00008353
Iteration 102/1000 | Loss: 0.00008353
Iteration 103/1000 | Loss: 0.00008353
Iteration 104/1000 | Loss: 0.00008352
Iteration 105/1000 | Loss: 0.00008352
Iteration 106/1000 | Loss: 0.00008351
Iteration 107/1000 | Loss: 0.00008351
Iteration 108/1000 | Loss: 0.00008350
Iteration 109/1000 | Loss: 0.00008350
Iteration 110/1000 | Loss: 0.00008350
Iteration 111/1000 | Loss: 0.00008349
Iteration 112/1000 | Loss: 0.00008349
Iteration 113/1000 | Loss: 0.00008348
Iteration 114/1000 | Loss: 0.00008348
Iteration 115/1000 | Loss: 0.00008348
Iteration 116/1000 | Loss: 0.00008348
Iteration 117/1000 | Loss: 0.00008348
Iteration 118/1000 | Loss: 0.00008348
Iteration 119/1000 | Loss: 0.00008347
Iteration 120/1000 | Loss: 0.00008347
Iteration 121/1000 | Loss: 0.00008346
Iteration 122/1000 | Loss: 0.00008346
Iteration 123/1000 | Loss: 0.00008345
Iteration 124/1000 | Loss: 0.00008345
Iteration 125/1000 | Loss: 0.00008345
Iteration 126/1000 | Loss: 0.00008344
Iteration 127/1000 | Loss: 0.00008344
Iteration 128/1000 | Loss: 0.00008344
Iteration 129/1000 | Loss: 0.00008344
Iteration 130/1000 | Loss: 0.00008343
Iteration 131/1000 | Loss: 0.00008343
Iteration 132/1000 | Loss: 0.00008343
Iteration 133/1000 | Loss: 0.00008343
Iteration 134/1000 | Loss: 0.00008343
Iteration 135/1000 | Loss: 0.00008343
Iteration 136/1000 | Loss: 0.00008343
Iteration 137/1000 | Loss: 0.00008343
Iteration 138/1000 | Loss: 0.00008343
Iteration 139/1000 | Loss: 0.00008343
Iteration 140/1000 | Loss: 0.00008343
Iteration 141/1000 | Loss: 0.00008343
Iteration 142/1000 | Loss: 0.00008343
Iteration 143/1000 | Loss: 0.00008343
Iteration 144/1000 | Loss: 0.00008343
Iteration 145/1000 | Loss: 0.00008343
Iteration 146/1000 | Loss: 0.00008343
Iteration 147/1000 | Loss: 0.00008343
Iteration 148/1000 | Loss: 0.00008343
Iteration 149/1000 | Loss: 0.00008343
Iteration 150/1000 | Loss: 0.00008343
Iteration 151/1000 | Loss: 0.00008343
Iteration 152/1000 | Loss: 0.00008343
Iteration 153/1000 | Loss: 0.00008343
Iteration 154/1000 | Loss: 0.00008343
Iteration 155/1000 | Loss: 0.00008343
Iteration 156/1000 | Loss: 0.00008343
Iteration 157/1000 | Loss: 0.00008343
Iteration 158/1000 | Loss: 0.00008343
Iteration 159/1000 | Loss: 0.00008343
Iteration 160/1000 | Loss: 0.00008343
Iteration 161/1000 | Loss: 0.00008343
Iteration 162/1000 | Loss: 0.00008343
Iteration 163/1000 | Loss: 0.00008343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.342795626958832e-05, 8.342795626958832e-05, 8.342795626958832e-05, 8.342795626958832e-05, 8.342795626958832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.342795626958832e-05

Optimization complete. Final v2v error: 5.140855312347412 mm

Highest mean error: 11.502606391906738 mm for frame 142

Lowest mean error: 3.5855464935302734 mm for frame 152

Saving results

Total time: 174.73341751098633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039443
Iteration 2/25 | Loss: 0.00225487
Iteration 3/25 | Loss: 0.00165560
Iteration 4/25 | Loss: 0.00154887
Iteration 5/25 | Loss: 0.00150373
Iteration 6/25 | Loss: 0.00148072
Iteration 7/25 | Loss: 0.00138084
Iteration 8/25 | Loss: 0.00125649
Iteration 9/25 | Loss: 0.00125635
Iteration 10/25 | Loss: 0.00123896
Iteration 11/25 | Loss: 0.00124339
Iteration 12/25 | Loss: 0.00124685
Iteration 13/25 | Loss: 0.00124509
Iteration 14/25 | Loss: 0.00123450
Iteration 15/25 | Loss: 0.00123448
Iteration 16/25 | Loss: 0.00123394
Iteration 17/25 | Loss: 0.00123430
Iteration 18/25 | Loss: 0.00123414
Iteration 19/25 | Loss: 0.00123415
Iteration 20/25 | Loss: 0.00123481
Iteration 21/25 | Loss: 0.00123405
Iteration 22/25 | Loss: 0.00123397
Iteration 23/25 | Loss: 0.00123428
Iteration 24/25 | Loss: 0.00123341
Iteration 25/25 | Loss: 0.00123272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27480197
Iteration 2/25 | Loss: 0.00375644
Iteration 3/25 | Loss: 0.00140203
Iteration 4/25 | Loss: 0.00140203
Iteration 5/25 | Loss: 0.00140203
Iteration 6/25 | Loss: 0.00140203
Iteration 7/25 | Loss: 0.00140203
Iteration 8/25 | Loss: 0.00140203
Iteration 9/25 | Loss: 0.00140203
Iteration 10/25 | Loss: 0.00140203
Iteration 11/25 | Loss: 0.00140203
Iteration 12/25 | Loss: 0.00140203
Iteration 13/25 | Loss: 0.00140203
Iteration 14/25 | Loss: 0.00140203
Iteration 15/25 | Loss: 0.00140203
Iteration 16/25 | Loss: 0.00140203
Iteration 17/25 | Loss: 0.00140203
Iteration 18/25 | Loss: 0.00140203
Iteration 19/25 | Loss: 0.00140203
Iteration 20/25 | Loss: 0.00140203
Iteration 21/25 | Loss: 0.00140203
Iteration 22/25 | Loss: 0.00140203
Iteration 23/25 | Loss: 0.00140203
Iteration 24/25 | Loss: 0.00140203
Iteration 25/25 | Loss: 0.00140203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140203
Iteration 2/1000 | Loss: 0.00002769
Iteration 3/1000 | Loss: 0.00002187
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001852
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001750
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001740
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001733
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001720
Iteration 59/1000 | Loss: 0.00001720
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001716
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001715
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001714
Iteration 104/1000 | Loss: 0.00001714
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.7140675481641665e-05, 1.7140675481641665e-05, 1.7140675481641665e-05, 1.7140675481641665e-05, 1.7140675481641665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7140675481641665e-05

Optimization complete. Final v2v error: 3.5459542274475098 mm

Highest mean error: 3.7837841510772705 mm for frame 239

Lowest mean error: 3.426727533340454 mm for frame 174

Saving results

Total time: 82.59740900993347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967767
Iteration 2/25 | Loss: 0.00405722
Iteration 3/25 | Loss: 0.00249914
Iteration 4/25 | Loss: 0.00233267
Iteration 5/25 | Loss: 0.00222002
Iteration 6/25 | Loss: 0.00215041
Iteration 7/25 | Loss: 0.00208103
Iteration 8/25 | Loss: 0.00206328
Iteration 9/25 | Loss: 0.00198356
Iteration 10/25 | Loss: 0.00195347
Iteration 11/25 | Loss: 0.00194478
Iteration 12/25 | Loss: 0.00192241
Iteration 13/25 | Loss: 0.00190312
Iteration 14/25 | Loss: 0.00191665
Iteration 15/25 | Loss: 0.00189660
Iteration 16/25 | Loss: 0.00189600
Iteration 17/25 | Loss: 0.00189075
Iteration 18/25 | Loss: 0.00188338
Iteration 19/25 | Loss: 0.00188219
Iteration 20/25 | Loss: 0.00188191
Iteration 21/25 | Loss: 0.00188188
Iteration 22/25 | Loss: 0.00188183
Iteration 23/25 | Loss: 0.00188183
Iteration 24/25 | Loss: 0.00188183
Iteration 25/25 | Loss: 0.00188183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26220691
Iteration 2/25 | Loss: 0.00425503
Iteration 3/25 | Loss: 0.00425502
Iteration 4/25 | Loss: 0.00423295
Iteration 5/25 | Loss: 0.00423295
Iteration 6/25 | Loss: 0.00423295
Iteration 7/25 | Loss: 0.00423295
Iteration 8/25 | Loss: 0.00423295
Iteration 9/25 | Loss: 0.00423295
Iteration 10/25 | Loss: 0.00423295
Iteration 11/25 | Loss: 0.00423295
Iteration 12/25 | Loss: 0.00423295
Iteration 13/25 | Loss: 0.00423295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004232947714626789, 0.004232947714626789, 0.004232947714626789, 0.004232947714626789, 0.004232947714626789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004232947714626789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00423295
Iteration 2/1000 | Loss: 0.00082199
Iteration 3/1000 | Loss: 0.00054215
Iteration 4/1000 | Loss: 0.00042044
Iteration 5/1000 | Loss: 0.00045250
Iteration 6/1000 | Loss: 0.00035993
Iteration 7/1000 | Loss: 0.00039351
Iteration 8/1000 | Loss: 0.00038663
Iteration 9/1000 | Loss: 0.00031057
Iteration 10/1000 | Loss: 0.00080844
Iteration 11/1000 | Loss: 0.00265794
Iteration 12/1000 | Loss: 0.00629122
Iteration 13/1000 | Loss: 0.00064897
Iteration 14/1000 | Loss: 0.00033040
Iteration 15/1000 | Loss: 0.00068112
Iteration 16/1000 | Loss: 0.00022460
Iteration 17/1000 | Loss: 0.00021867
Iteration 18/1000 | Loss: 0.00009064
Iteration 19/1000 | Loss: 0.00007194
Iteration 20/1000 | Loss: 0.00005790
Iteration 21/1000 | Loss: 0.00004545
Iteration 22/1000 | Loss: 0.00003498
Iteration 23/1000 | Loss: 0.00002915
Iteration 24/1000 | Loss: 0.00002451
Iteration 25/1000 | Loss: 0.00002153
Iteration 26/1000 | Loss: 0.00001959
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001534
Iteration 35/1000 | Loss: 0.00001529
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001508
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001505
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001502
Iteration 68/1000 | Loss: 0.00001502
Iteration 69/1000 | Loss: 0.00001501
Iteration 70/1000 | Loss: 0.00001501
Iteration 71/1000 | Loss: 0.00001501
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001500
Iteration 76/1000 | Loss: 0.00001500
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001498
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001497
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001495
Iteration 97/1000 | Loss: 0.00001495
Iteration 98/1000 | Loss: 0.00001495
Iteration 99/1000 | Loss: 0.00001495
Iteration 100/1000 | Loss: 0.00001495
Iteration 101/1000 | Loss: 0.00001495
Iteration 102/1000 | Loss: 0.00001495
Iteration 103/1000 | Loss: 0.00001495
Iteration 104/1000 | Loss: 0.00001494
Iteration 105/1000 | Loss: 0.00001494
Iteration 106/1000 | Loss: 0.00001494
Iteration 107/1000 | Loss: 0.00001494
Iteration 108/1000 | Loss: 0.00001494
Iteration 109/1000 | Loss: 0.00001494
Iteration 110/1000 | Loss: 0.00001494
Iteration 111/1000 | Loss: 0.00001494
Iteration 112/1000 | Loss: 0.00001493
Iteration 113/1000 | Loss: 0.00001493
Iteration 114/1000 | Loss: 0.00001492
Iteration 115/1000 | Loss: 0.00001492
Iteration 116/1000 | Loss: 0.00001492
Iteration 117/1000 | Loss: 0.00001491
Iteration 118/1000 | Loss: 0.00001491
Iteration 119/1000 | Loss: 0.00001491
Iteration 120/1000 | Loss: 0.00001491
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001489
Iteration 128/1000 | Loss: 0.00001489
Iteration 129/1000 | Loss: 0.00001489
Iteration 130/1000 | Loss: 0.00001489
Iteration 131/1000 | Loss: 0.00001489
Iteration 132/1000 | Loss: 0.00001489
Iteration 133/1000 | Loss: 0.00001489
Iteration 134/1000 | Loss: 0.00001489
Iteration 135/1000 | Loss: 0.00001489
Iteration 136/1000 | Loss: 0.00001488
Iteration 137/1000 | Loss: 0.00001488
Iteration 138/1000 | Loss: 0.00001488
Iteration 139/1000 | Loss: 0.00001488
Iteration 140/1000 | Loss: 0.00001488
Iteration 141/1000 | Loss: 0.00001488
Iteration 142/1000 | Loss: 0.00001488
Iteration 143/1000 | Loss: 0.00001488
Iteration 144/1000 | Loss: 0.00001488
Iteration 145/1000 | Loss: 0.00001487
Iteration 146/1000 | Loss: 0.00001487
Iteration 147/1000 | Loss: 0.00001487
Iteration 148/1000 | Loss: 0.00001487
Iteration 149/1000 | Loss: 0.00001487
Iteration 150/1000 | Loss: 0.00001487
Iteration 151/1000 | Loss: 0.00001486
Iteration 152/1000 | Loss: 0.00001486
Iteration 153/1000 | Loss: 0.00001486
Iteration 154/1000 | Loss: 0.00001486
Iteration 155/1000 | Loss: 0.00001486
Iteration 156/1000 | Loss: 0.00001486
Iteration 157/1000 | Loss: 0.00001486
Iteration 158/1000 | Loss: 0.00001486
Iteration 159/1000 | Loss: 0.00001485
Iteration 160/1000 | Loss: 0.00001485
Iteration 161/1000 | Loss: 0.00001485
Iteration 162/1000 | Loss: 0.00001485
Iteration 163/1000 | Loss: 0.00001485
Iteration 164/1000 | Loss: 0.00001485
Iteration 165/1000 | Loss: 0.00001485
Iteration 166/1000 | Loss: 0.00001485
Iteration 167/1000 | Loss: 0.00001485
Iteration 168/1000 | Loss: 0.00001485
Iteration 169/1000 | Loss: 0.00001485
Iteration 170/1000 | Loss: 0.00001485
Iteration 171/1000 | Loss: 0.00001485
Iteration 172/1000 | Loss: 0.00001485
Iteration 173/1000 | Loss: 0.00001485
Iteration 174/1000 | Loss: 0.00001485
Iteration 175/1000 | Loss: 0.00001485
Iteration 176/1000 | Loss: 0.00001485
Iteration 177/1000 | Loss: 0.00001485
Iteration 178/1000 | Loss: 0.00001485
Iteration 179/1000 | Loss: 0.00001485
Iteration 180/1000 | Loss: 0.00001485
Iteration 181/1000 | Loss: 0.00001485
Iteration 182/1000 | Loss: 0.00001485
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001485
Iteration 185/1000 | Loss: 0.00001485
Iteration 186/1000 | Loss: 0.00001485
Iteration 187/1000 | Loss: 0.00001485
Iteration 188/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.4854046639811713e-05, 1.4854046639811713e-05, 1.4854046639811713e-05, 1.4854046639811713e-05, 1.4854046639811713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4854046639811713e-05

Optimization complete. Final v2v error: 3.327951431274414 mm

Highest mean error: 3.579484701156616 mm for frame 97

Lowest mean error: 3.1272876262664795 mm for frame 2

Saving results

Total time: 112.62331008911133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807030
Iteration 2/25 | Loss: 0.00147677
Iteration 3/25 | Loss: 0.00122678
Iteration 4/25 | Loss: 0.00121446
Iteration 5/25 | Loss: 0.00121307
Iteration 6/25 | Loss: 0.00121307
Iteration 7/25 | Loss: 0.00121307
Iteration 8/25 | Loss: 0.00121307
Iteration 9/25 | Loss: 0.00121307
Iteration 10/25 | Loss: 0.00121307
Iteration 11/25 | Loss: 0.00121307
Iteration 12/25 | Loss: 0.00121307
Iteration 13/25 | Loss: 0.00121307
Iteration 14/25 | Loss: 0.00121307
Iteration 15/25 | Loss: 0.00121307
Iteration 16/25 | Loss: 0.00121307
Iteration 17/25 | Loss: 0.00121307
Iteration 18/25 | Loss: 0.00121307
Iteration 19/25 | Loss: 0.00121307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012130697723478079, 0.0012130697723478079, 0.0012130697723478079, 0.0012130697723478079, 0.0012130697723478079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012130697723478079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26102638
Iteration 2/25 | Loss: 0.00110467
Iteration 3/25 | Loss: 0.00110466
Iteration 4/25 | Loss: 0.00110466
Iteration 5/25 | Loss: 0.00110466
Iteration 6/25 | Loss: 0.00110466
Iteration 7/25 | Loss: 0.00110466
Iteration 8/25 | Loss: 0.00110466
Iteration 9/25 | Loss: 0.00110466
Iteration 10/25 | Loss: 0.00110466
Iteration 11/25 | Loss: 0.00110466
Iteration 12/25 | Loss: 0.00110466
Iteration 13/25 | Loss: 0.00110466
Iteration 14/25 | Loss: 0.00110466
Iteration 15/25 | Loss: 0.00110466
Iteration 16/25 | Loss: 0.00110466
Iteration 17/25 | Loss: 0.00110466
Iteration 18/25 | Loss: 0.00110466
Iteration 19/25 | Loss: 0.00110466
Iteration 20/25 | Loss: 0.00110466
Iteration 21/25 | Loss: 0.00110466
Iteration 22/25 | Loss: 0.00110466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001104661962017417, 0.001104661962017417, 0.001104661962017417, 0.001104661962017417, 0.001104661962017417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001104661962017417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110466
Iteration 2/1000 | Loss: 0.00002844
Iteration 3/1000 | Loss: 0.00001994
Iteration 4/1000 | Loss: 0.00001784
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001595
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001509
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001362
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001343
Iteration 22/1000 | Loss: 0.00001342
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001319
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001319
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001316
Iteration 43/1000 | Loss: 0.00001316
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001316
Iteration 48/1000 | Loss: 0.00001316
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001316
Iteration 80/1000 | Loss: 0.00001316
Iteration 81/1000 | Loss: 0.00001316
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.3160729395167436e-05, 1.3160729395167436e-05, 1.3160729395167436e-05, 1.3160729395167436e-05, 1.3160729395167436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3160729395167436e-05

Optimization complete. Final v2v error: 3.04382586479187 mm

Highest mean error: 3.1682660579681396 mm for frame 33

Lowest mean error: 2.9691381454467773 mm for frame 106

Saving results

Total time: 32.89404296875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867784
Iteration 2/25 | Loss: 0.00137075
Iteration 3/25 | Loss: 0.00126059
Iteration 4/25 | Loss: 0.00123924
Iteration 5/25 | Loss: 0.00123241
Iteration 6/25 | Loss: 0.00123043
Iteration 7/25 | Loss: 0.00123032
Iteration 8/25 | Loss: 0.00123032
Iteration 9/25 | Loss: 0.00123032
Iteration 10/25 | Loss: 0.00123032
Iteration 11/25 | Loss: 0.00123032
Iteration 12/25 | Loss: 0.00123032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012303185649216175, 0.0012303185649216175, 0.0012303185649216175, 0.0012303185649216175, 0.0012303185649216175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012303185649216175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32183778
Iteration 2/25 | Loss: 0.00130322
Iteration 3/25 | Loss: 0.00130319
Iteration 4/25 | Loss: 0.00130319
Iteration 5/25 | Loss: 0.00130319
Iteration 6/25 | Loss: 0.00130319
Iteration 7/25 | Loss: 0.00130319
Iteration 8/25 | Loss: 0.00130319
Iteration 9/25 | Loss: 0.00130319
Iteration 10/25 | Loss: 0.00130319
Iteration 11/25 | Loss: 0.00130319
Iteration 12/25 | Loss: 0.00130319
Iteration 13/25 | Loss: 0.00130319
Iteration 14/25 | Loss: 0.00130319
Iteration 15/25 | Loss: 0.00130319
Iteration 16/25 | Loss: 0.00130319
Iteration 17/25 | Loss: 0.00130319
Iteration 18/25 | Loss: 0.00130319
Iteration 19/25 | Loss: 0.00130319
Iteration 20/25 | Loss: 0.00130319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00130318698938936, 0.00130318698938936, 0.00130318698938936, 0.00130318698938936, 0.00130318698938936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00130318698938936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130319
Iteration 2/1000 | Loss: 0.00004246
Iteration 3/1000 | Loss: 0.00002937
Iteration 4/1000 | Loss: 0.00002489
Iteration 5/1000 | Loss: 0.00002376
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002213
Iteration 8/1000 | Loss: 0.00002157
Iteration 9/1000 | Loss: 0.00002109
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002059
Iteration 14/1000 | Loss: 0.00002056
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002047
Iteration 19/1000 | Loss: 0.00002039
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002032
Iteration 22/1000 | Loss: 0.00002028
Iteration 23/1000 | Loss: 0.00002025
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002020
Iteration 27/1000 | Loss: 0.00002014
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002014
Iteration 30/1000 | Loss: 0.00002013
Iteration 31/1000 | Loss: 0.00002013
Iteration 32/1000 | Loss: 0.00002012
Iteration 33/1000 | Loss: 0.00002012
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002011
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002010
Iteration 41/1000 | Loss: 0.00002010
Iteration 42/1000 | Loss: 0.00002009
Iteration 43/1000 | Loss: 0.00002009
Iteration 44/1000 | Loss: 0.00002009
Iteration 45/1000 | Loss: 0.00002008
Iteration 46/1000 | Loss: 0.00002008
Iteration 47/1000 | Loss: 0.00002008
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002008
Iteration 51/1000 | Loss: 0.00002007
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00002007
Iteration 55/1000 | Loss: 0.00002007
Iteration 56/1000 | Loss: 0.00002007
Iteration 57/1000 | Loss: 0.00002007
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002007
Iteration 60/1000 | Loss: 0.00002007
Iteration 61/1000 | Loss: 0.00002007
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002006
Iteration 64/1000 | Loss: 0.00002006
Iteration 65/1000 | Loss: 0.00002006
Iteration 66/1000 | Loss: 0.00002005
Iteration 67/1000 | Loss: 0.00002005
Iteration 68/1000 | Loss: 0.00002005
Iteration 69/1000 | Loss: 0.00002005
Iteration 70/1000 | Loss: 0.00002005
Iteration 71/1000 | Loss: 0.00002005
Iteration 72/1000 | Loss: 0.00002004
Iteration 73/1000 | Loss: 0.00002004
Iteration 74/1000 | Loss: 0.00002004
Iteration 75/1000 | Loss: 0.00002004
Iteration 76/1000 | Loss: 0.00002004
Iteration 77/1000 | Loss: 0.00002004
Iteration 78/1000 | Loss: 0.00002004
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002003
Iteration 82/1000 | Loss: 0.00002003
Iteration 83/1000 | Loss: 0.00002002
Iteration 84/1000 | Loss: 0.00002002
Iteration 85/1000 | Loss: 0.00002002
Iteration 86/1000 | Loss: 0.00002002
Iteration 87/1000 | Loss: 0.00002002
Iteration 88/1000 | Loss: 0.00002002
Iteration 89/1000 | Loss: 0.00002002
Iteration 90/1000 | Loss: 0.00002001
Iteration 91/1000 | Loss: 0.00002001
Iteration 92/1000 | Loss: 0.00002001
Iteration 93/1000 | Loss: 0.00002001
Iteration 94/1000 | Loss: 0.00002001
Iteration 95/1000 | Loss: 0.00002000
Iteration 96/1000 | Loss: 0.00002000
Iteration 97/1000 | Loss: 0.00002000
Iteration 98/1000 | Loss: 0.00002000
Iteration 99/1000 | Loss: 0.00001999
Iteration 100/1000 | Loss: 0.00001998
Iteration 101/1000 | Loss: 0.00001998
Iteration 102/1000 | Loss: 0.00001998
Iteration 103/1000 | Loss: 0.00001998
Iteration 104/1000 | Loss: 0.00001998
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001997
Iteration 110/1000 | Loss: 0.00001997
Iteration 111/1000 | Loss: 0.00001997
Iteration 112/1000 | Loss: 0.00001997
Iteration 113/1000 | Loss: 0.00001996
Iteration 114/1000 | Loss: 0.00001996
Iteration 115/1000 | Loss: 0.00001996
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001996
Iteration 118/1000 | Loss: 0.00001995
Iteration 119/1000 | Loss: 0.00001995
Iteration 120/1000 | Loss: 0.00001995
Iteration 121/1000 | Loss: 0.00001995
Iteration 122/1000 | Loss: 0.00001995
Iteration 123/1000 | Loss: 0.00001995
Iteration 124/1000 | Loss: 0.00001995
Iteration 125/1000 | Loss: 0.00001995
Iteration 126/1000 | Loss: 0.00001995
Iteration 127/1000 | Loss: 0.00001994
Iteration 128/1000 | Loss: 0.00001994
Iteration 129/1000 | Loss: 0.00001994
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001993
Iteration 133/1000 | Loss: 0.00001993
Iteration 134/1000 | Loss: 0.00001993
Iteration 135/1000 | Loss: 0.00001993
Iteration 136/1000 | Loss: 0.00001993
Iteration 137/1000 | Loss: 0.00001993
Iteration 138/1000 | Loss: 0.00001993
Iteration 139/1000 | Loss: 0.00001993
Iteration 140/1000 | Loss: 0.00001993
Iteration 141/1000 | Loss: 0.00001993
Iteration 142/1000 | Loss: 0.00001993
Iteration 143/1000 | Loss: 0.00001993
Iteration 144/1000 | Loss: 0.00001993
Iteration 145/1000 | Loss: 0.00001993
Iteration 146/1000 | Loss: 0.00001993
Iteration 147/1000 | Loss: 0.00001993
Iteration 148/1000 | Loss: 0.00001992
Iteration 149/1000 | Loss: 0.00001992
Iteration 150/1000 | Loss: 0.00001992
Iteration 151/1000 | Loss: 0.00001992
Iteration 152/1000 | Loss: 0.00001992
Iteration 153/1000 | Loss: 0.00001992
Iteration 154/1000 | Loss: 0.00001992
Iteration 155/1000 | Loss: 0.00001992
Iteration 156/1000 | Loss: 0.00001992
Iteration 157/1000 | Loss: 0.00001992
Iteration 158/1000 | Loss: 0.00001992
Iteration 159/1000 | Loss: 0.00001992
Iteration 160/1000 | Loss: 0.00001992
Iteration 161/1000 | Loss: 0.00001992
Iteration 162/1000 | Loss: 0.00001992
Iteration 163/1000 | Loss: 0.00001992
Iteration 164/1000 | Loss: 0.00001992
Iteration 165/1000 | Loss: 0.00001992
Iteration 166/1000 | Loss: 0.00001991
Iteration 167/1000 | Loss: 0.00001991
Iteration 168/1000 | Loss: 0.00001991
Iteration 169/1000 | Loss: 0.00001991
Iteration 170/1000 | Loss: 0.00001991
Iteration 171/1000 | Loss: 0.00001991
Iteration 172/1000 | Loss: 0.00001991
Iteration 173/1000 | Loss: 0.00001991
Iteration 174/1000 | Loss: 0.00001991
Iteration 175/1000 | Loss: 0.00001991
Iteration 176/1000 | Loss: 0.00001991
Iteration 177/1000 | Loss: 0.00001991
Iteration 178/1000 | Loss: 0.00001991
Iteration 179/1000 | Loss: 0.00001991
Iteration 180/1000 | Loss: 0.00001991
Iteration 181/1000 | Loss: 0.00001991
Iteration 182/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.990925557038281e-05, 1.990925557038281e-05, 1.990925557038281e-05, 1.990925557038281e-05, 1.990925557038281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.990925557038281e-05

Optimization complete. Final v2v error: 3.7446751594543457 mm

Highest mean error: 5.435030937194824 mm for frame 67

Lowest mean error: 3.1597342491149902 mm for frame 99

Saving results

Total time: 40.49909830093384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795257
Iteration 2/25 | Loss: 0.00140360
Iteration 3/25 | Loss: 0.00124424
Iteration 4/25 | Loss: 0.00123286
Iteration 5/25 | Loss: 0.00123025
Iteration 6/25 | Loss: 0.00122996
Iteration 7/25 | Loss: 0.00122996
Iteration 8/25 | Loss: 0.00122996
Iteration 9/25 | Loss: 0.00122996
Iteration 10/25 | Loss: 0.00122996
Iteration 11/25 | Loss: 0.00122996
Iteration 12/25 | Loss: 0.00122996
Iteration 13/25 | Loss: 0.00122996
Iteration 14/25 | Loss: 0.00122996
Iteration 15/25 | Loss: 0.00122996
Iteration 16/25 | Loss: 0.00122996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012299638474360108, 0.0012299638474360108, 0.0012299638474360108, 0.0012299638474360108, 0.0012299638474360108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012299638474360108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24896276
Iteration 2/25 | Loss: 0.00120897
Iteration 3/25 | Loss: 0.00120893
Iteration 4/25 | Loss: 0.00120893
Iteration 5/25 | Loss: 0.00120893
Iteration 6/25 | Loss: 0.00120893
Iteration 7/25 | Loss: 0.00120893
Iteration 8/25 | Loss: 0.00120893
Iteration 9/25 | Loss: 0.00120893
Iteration 10/25 | Loss: 0.00120893
Iteration 11/25 | Loss: 0.00120893
Iteration 12/25 | Loss: 0.00120893
Iteration 13/25 | Loss: 0.00120893
Iteration 14/25 | Loss: 0.00120893
Iteration 15/25 | Loss: 0.00120893
Iteration 16/25 | Loss: 0.00120893
Iteration 17/25 | Loss: 0.00120893
Iteration 18/25 | Loss: 0.00120893
Iteration 19/25 | Loss: 0.00120893
Iteration 20/25 | Loss: 0.00120893
Iteration 21/25 | Loss: 0.00120893
Iteration 22/25 | Loss: 0.00120893
Iteration 23/25 | Loss: 0.00120893
Iteration 24/25 | Loss: 0.00120893
Iteration 25/25 | Loss: 0.00120893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120893
Iteration 2/1000 | Loss: 0.00003852
Iteration 3/1000 | Loss: 0.00002581
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001811
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001683
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001549
Iteration 14/1000 | Loss: 0.00001548
Iteration 15/1000 | Loss: 0.00001534
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001523
Iteration 18/1000 | Loss: 0.00001516
Iteration 19/1000 | Loss: 0.00001516
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001510
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001502
Iteration 37/1000 | Loss: 0.00001497
Iteration 38/1000 | Loss: 0.00001497
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001494
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001493
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001492
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001490
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001488
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001486
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001481
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001480
Iteration 95/1000 | Loss: 0.00001480
Iteration 96/1000 | Loss: 0.00001480
Iteration 97/1000 | Loss: 0.00001480
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001479
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001477
Iteration 114/1000 | Loss: 0.00001477
Iteration 115/1000 | Loss: 0.00001477
Iteration 116/1000 | Loss: 0.00001477
Iteration 117/1000 | Loss: 0.00001477
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001477
Iteration 120/1000 | Loss: 0.00001476
Iteration 121/1000 | Loss: 0.00001476
Iteration 122/1000 | Loss: 0.00001476
Iteration 123/1000 | Loss: 0.00001476
Iteration 124/1000 | Loss: 0.00001476
Iteration 125/1000 | Loss: 0.00001476
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001472
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001471
Iteration 150/1000 | Loss: 0.00001471
Iteration 151/1000 | Loss: 0.00001471
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001470
Iteration 155/1000 | Loss: 0.00001470
Iteration 156/1000 | Loss: 0.00001470
Iteration 157/1000 | Loss: 0.00001470
Iteration 158/1000 | Loss: 0.00001470
Iteration 159/1000 | Loss: 0.00001470
Iteration 160/1000 | Loss: 0.00001470
Iteration 161/1000 | Loss: 0.00001470
Iteration 162/1000 | Loss: 0.00001470
Iteration 163/1000 | Loss: 0.00001470
Iteration 164/1000 | Loss: 0.00001469
Iteration 165/1000 | Loss: 0.00001469
Iteration 166/1000 | Loss: 0.00001469
Iteration 167/1000 | Loss: 0.00001469
Iteration 168/1000 | Loss: 0.00001469
Iteration 169/1000 | Loss: 0.00001469
Iteration 170/1000 | Loss: 0.00001469
Iteration 171/1000 | Loss: 0.00001469
Iteration 172/1000 | Loss: 0.00001469
Iteration 173/1000 | Loss: 0.00001469
Iteration 174/1000 | Loss: 0.00001469
Iteration 175/1000 | Loss: 0.00001469
Iteration 176/1000 | Loss: 0.00001469
Iteration 177/1000 | Loss: 0.00001469
Iteration 178/1000 | Loss: 0.00001468
Iteration 179/1000 | Loss: 0.00001468
Iteration 180/1000 | Loss: 0.00001468
Iteration 181/1000 | Loss: 0.00001468
Iteration 182/1000 | Loss: 0.00001468
Iteration 183/1000 | Loss: 0.00001468
Iteration 184/1000 | Loss: 0.00001468
Iteration 185/1000 | Loss: 0.00001468
Iteration 186/1000 | Loss: 0.00001468
Iteration 187/1000 | Loss: 0.00001468
Iteration 188/1000 | Loss: 0.00001468
Iteration 189/1000 | Loss: 0.00001468
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001468
Iteration 192/1000 | Loss: 0.00001468
Iteration 193/1000 | Loss: 0.00001468
Iteration 194/1000 | Loss: 0.00001467
Iteration 195/1000 | Loss: 0.00001467
Iteration 196/1000 | Loss: 0.00001467
Iteration 197/1000 | Loss: 0.00001467
Iteration 198/1000 | Loss: 0.00001467
Iteration 199/1000 | Loss: 0.00001467
Iteration 200/1000 | Loss: 0.00001467
Iteration 201/1000 | Loss: 0.00001467
Iteration 202/1000 | Loss: 0.00001467
Iteration 203/1000 | Loss: 0.00001467
Iteration 204/1000 | Loss: 0.00001467
Iteration 205/1000 | Loss: 0.00001467
Iteration 206/1000 | Loss: 0.00001467
Iteration 207/1000 | Loss: 0.00001467
Iteration 208/1000 | Loss: 0.00001467
Iteration 209/1000 | Loss: 0.00001467
Iteration 210/1000 | Loss: 0.00001467
Iteration 211/1000 | Loss: 0.00001467
Iteration 212/1000 | Loss: 0.00001467
Iteration 213/1000 | Loss: 0.00001467
Iteration 214/1000 | Loss: 0.00001467
Iteration 215/1000 | Loss: 0.00001467
Iteration 216/1000 | Loss: 0.00001467
Iteration 217/1000 | Loss: 0.00001467
Iteration 218/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.467009224143112e-05, 1.467009224143112e-05, 1.467009224143112e-05, 1.467009224143112e-05, 1.467009224143112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.467009224143112e-05

Optimization complete. Final v2v error: 3.191987991333008 mm

Highest mean error: 4.515626907348633 mm for frame 23

Lowest mean error: 2.6261754035949707 mm for frame 206

Saving results

Total time: 52.84216117858887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799461
Iteration 2/25 | Loss: 0.00131944
Iteration 3/25 | Loss: 0.00121758
Iteration 4/25 | Loss: 0.00119069
Iteration 5/25 | Loss: 0.00118069
Iteration 6/25 | Loss: 0.00117839
Iteration 7/25 | Loss: 0.00117752
Iteration 8/25 | Loss: 0.00117752
Iteration 9/25 | Loss: 0.00117752
Iteration 10/25 | Loss: 0.00117752
Iteration 11/25 | Loss: 0.00117752
Iteration 12/25 | Loss: 0.00117752
Iteration 13/25 | Loss: 0.00117752
Iteration 14/25 | Loss: 0.00117752
Iteration 15/25 | Loss: 0.00117752
Iteration 16/25 | Loss: 0.00117752
Iteration 17/25 | Loss: 0.00117752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011775202583521605, 0.0011775202583521605, 0.0011775202583521605, 0.0011775202583521605, 0.0011775202583521605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011775202583521605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41217875
Iteration 2/25 | Loss: 0.00175019
Iteration 3/25 | Loss: 0.00175019
Iteration 4/25 | Loss: 0.00175019
Iteration 5/25 | Loss: 0.00175019
Iteration 6/25 | Loss: 0.00175019
Iteration 7/25 | Loss: 0.00175019
Iteration 8/25 | Loss: 0.00175019
Iteration 9/25 | Loss: 0.00175019
Iteration 10/25 | Loss: 0.00175019
Iteration 11/25 | Loss: 0.00175019
Iteration 12/25 | Loss: 0.00175019
Iteration 13/25 | Loss: 0.00175019
Iteration 14/25 | Loss: 0.00175019
Iteration 15/25 | Loss: 0.00175019
Iteration 16/25 | Loss: 0.00175019
Iteration 17/25 | Loss: 0.00175019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017501896945759654, 0.0017501896945759654, 0.0017501896945759654, 0.0017501896945759654, 0.0017501896945759654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017501896945759654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175019
Iteration 2/1000 | Loss: 0.00005249
Iteration 3/1000 | Loss: 0.00003879
Iteration 4/1000 | Loss: 0.00002853
Iteration 5/1000 | Loss: 0.00002588
Iteration 6/1000 | Loss: 0.00002454
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002208
Iteration 10/1000 | Loss: 0.00002172
Iteration 11/1000 | Loss: 0.00002146
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00002118
Iteration 14/1000 | Loss: 0.00002100
Iteration 15/1000 | Loss: 0.00002083
Iteration 16/1000 | Loss: 0.00002079
Iteration 17/1000 | Loss: 0.00002075
Iteration 18/1000 | Loss: 0.00002074
Iteration 19/1000 | Loss: 0.00002073
Iteration 20/1000 | Loss: 0.00002073
Iteration 21/1000 | Loss: 0.00002072
Iteration 22/1000 | Loss: 0.00002069
Iteration 23/1000 | Loss: 0.00002066
Iteration 24/1000 | Loss: 0.00002065
Iteration 25/1000 | Loss: 0.00002065
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00002056
Iteration 30/1000 | Loss: 0.00002055
Iteration 31/1000 | Loss: 0.00002054
Iteration 32/1000 | Loss: 0.00002054
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002048
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002047
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002042
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002041
Iteration 49/1000 | Loss: 0.00002040
Iteration 50/1000 | Loss: 0.00002040
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002038
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002036
Iteration 59/1000 | Loss: 0.00002036
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002035
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002035
Iteration 64/1000 | Loss: 0.00002033
Iteration 65/1000 | Loss: 0.00002033
Iteration 66/1000 | Loss: 0.00002033
Iteration 67/1000 | Loss: 0.00002032
Iteration 68/1000 | Loss: 0.00002032
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002030
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002029
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002028
Iteration 80/1000 | Loss: 0.00002028
Iteration 81/1000 | Loss: 0.00002028
Iteration 82/1000 | Loss: 0.00002027
Iteration 83/1000 | Loss: 0.00002027
Iteration 84/1000 | Loss: 0.00002027
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002026
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002026
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002026
Iteration 92/1000 | Loss: 0.00002026
Iteration 93/1000 | Loss: 0.00002025
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002025
Iteration 96/1000 | Loss: 0.00002025
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002024
Iteration 100/1000 | Loss: 0.00002023
Iteration 101/1000 | Loss: 0.00002023
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002023
Iteration 107/1000 | Loss: 0.00002023
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002022
Iteration 111/1000 | Loss: 0.00002022
Iteration 112/1000 | Loss: 0.00002022
Iteration 113/1000 | Loss: 0.00002022
Iteration 114/1000 | Loss: 0.00002022
Iteration 115/1000 | Loss: 0.00002022
Iteration 116/1000 | Loss: 0.00002022
Iteration 117/1000 | Loss: 0.00002022
Iteration 118/1000 | Loss: 0.00002022
Iteration 119/1000 | Loss: 0.00002022
Iteration 120/1000 | Loss: 0.00002022
Iteration 121/1000 | Loss: 0.00002022
Iteration 122/1000 | Loss: 0.00002022
Iteration 123/1000 | Loss: 0.00002022
Iteration 124/1000 | Loss: 0.00002022
Iteration 125/1000 | Loss: 0.00002022
Iteration 126/1000 | Loss: 0.00002022
Iteration 127/1000 | Loss: 0.00002022
Iteration 128/1000 | Loss: 0.00002022
Iteration 129/1000 | Loss: 0.00002022
Iteration 130/1000 | Loss: 0.00002022
Iteration 131/1000 | Loss: 0.00002022
Iteration 132/1000 | Loss: 0.00002022
Iteration 133/1000 | Loss: 0.00002022
Iteration 134/1000 | Loss: 0.00002022
Iteration 135/1000 | Loss: 0.00002022
Iteration 136/1000 | Loss: 0.00002022
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002022
Iteration 139/1000 | Loss: 0.00002022
Iteration 140/1000 | Loss: 0.00002022
Iteration 141/1000 | Loss: 0.00002022
Iteration 142/1000 | Loss: 0.00002022
Iteration 143/1000 | Loss: 0.00002022
Iteration 144/1000 | Loss: 0.00002022
Iteration 145/1000 | Loss: 0.00002022
Iteration 146/1000 | Loss: 0.00002022
Iteration 147/1000 | Loss: 0.00002022
Iteration 148/1000 | Loss: 0.00002022
Iteration 149/1000 | Loss: 0.00002022
Iteration 150/1000 | Loss: 0.00002022
Iteration 151/1000 | Loss: 0.00002022
Iteration 152/1000 | Loss: 0.00002022
Iteration 153/1000 | Loss: 0.00002022
Iteration 154/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.021594082179945e-05, 2.021594082179945e-05, 2.021594082179945e-05, 2.021594082179945e-05, 2.021594082179945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.021594082179945e-05

Optimization complete. Final v2v error: 3.7186594009399414 mm

Highest mean error: 5.492622375488281 mm for frame 128

Lowest mean error: 2.6990067958831787 mm for frame 44

Saving results

Total time: 42.442853689193726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534165
Iteration 2/25 | Loss: 0.00137092
Iteration 3/25 | Loss: 0.00126133
Iteration 4/25 | Loss: 0.00125218
Iteration 5/25 | Loss: 0.00125866
Iteration 6/25 | Loss: 0.00125060
Iteration 7/25 | Loss: 0.00124375
Iteration 8/25 | Loss: 0.00124157
Iteration 9/25 | Loss: 0.00124103
Iteration 10/25 | Loss: 0.00124089
Iteration 11/25 | Loss: 0.00124085
Iteration 12/25 | Loss: 0.00124085
Iteration 13/25 | Loss: 0.00124085
Iteration 14/25 | Loss: 0.00124085
Iteration 15/25 | Loss: 0.00124085
Iteration 16/25 | Loss: 0.00124085
Iteration 17/25 | Loss: 0.00124085
Iteration 18/25 | Loss: 0.00124085
Iteration 19/25 | Loss: 0.00124085
Iteration 20/25 | Loss: 0.00124085
Iteration 21/25 | Loss: 0.00124084
Iteration 22/25 | Loss: 0.00124084
Iteration 23/25 | Loss: 0.00124084
Iteration 24/25 | Loss: 0.00124084
Iteration 25/25 | Loss: 0.00124084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31846488
Iteration 2/25 | Loss: 0.00153811
Iteration 3/25 | Loss: 0.00153810
Iteration 4/25 | Loss: 0.00153810
Iteration 5/25 | Loss: 0.00153810
Iteration 6/25 | Loss: 0.00153810
Iteration 7/25 | Loss: 0.00153810
Iteration 8/25 | Loss: 0.00153810
Iteration 9/25 | Loss: 0.00153810
Iteration 10/25 | Loss: 0.00153810
Iteration 11/25 | Loss: 0.00153810
Iteration 12/25 | Loss: 0.00153810
Iteration 13/25 | Loss: 0.00153810
Iteration 14/25 | Loss: 0.00153810
Iteration 15/25 | Loss: 0.00153810
Iteration 16/25 | Loss: 0.00153810
Iteration 17/25 | Loss: 0.00153810
Iteration 18/25 | Loss: 0.00153810
Iteration 19/25 | Loss: 0.00153810
Iteration 20/25 | Loss: 0.00153810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015380994882434607, 0.0015380994882434607, 0.0015380994882434607, 0.0015380994882434607, 0.0015380994882434607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015380994882434607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153810
Iteration 2/1000 | Loss: 0.00004176
Iteration 3/1000 | Loss: 0.00002935
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002244
Iteration 6/1000 | Loss: 0.00002139
Iteration 7/1000 | Loss: 0.00002065
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001963
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001839
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001838
Iteration 18/1000 | Loss: 0.00001837
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001827
Iteration 22/1000 | Loss: 0.00001827
Iteration 23/1000 | Loss: 0.00001826
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001823
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00001822
Iteration 28/1000 | Loss: 0.00001820
Iteration 29/1000 | Loss: 0.00001816
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001812
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001812
Iteration 42/1000 | Loss: 0.00001812
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001804
Iteration 79/1000 | Loss: 0.00001804
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001803
Iteration 83/1000 | Loss: 0.00001803
Iteration 84/1000 | Loss: 0.00001803
Iteration 85/1000 | Loss: 0.00001803
Iteration 86/1000 | Loss: 0.00001803
Iteration 87/1000 | Loss: 0.00001803
Iteration 88/1000 | Loss: 0.00001803
Iteration 89/1000 | Loss: 0.00001803
Iteration 90/1000 | Loss: 0.00001803
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001800
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001800
Iteration 99/1000 | Loss: 0.00001799
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00001799
Iteration 102/1000 | Loss: 0.00001799
Iteration 103/1000 | Loss: 0.00001799
Iteration 104/1000 | Loss: 0.00001798
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001796
Iteration 111/1000 | Loss: 0.00001796
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001796
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001795
Iteration 118/1000 | Loss: 0.00001795
Iteration 119/1000 | Loss: 0.00001795
Iteration 120/1000 | Loss: 0.00001795
Iteration 121/1000 | Loss: 0.00001795
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001794
Iteration 124/1000 | Loss: 0.00001794
Iteration 125/1000 | Loss: 0.00001794
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001793
Iteration 129/1000 | Loss: 0.00001792
Iteration 130/1000 | Loss: 0.00001792
Iteration 131/1000 | Loss: 0.00001792
Iteration 132/1000 | Loss: 0.00001792
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001791
Iteration 136/1000 | Loss: 0.00001791
Iteration 137/1000 | Loss: 0.00001791
Iteration 138/1000 | Loss: 0.00001791
Iteration 139/1000 | Loss: 0.00001791
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001790
Iteration 143/1000 | Loss: 0.00001790
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001790
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001789
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001789
Iteration 152/1000 | Loss: 0.00001789
Iteration 153/1000 | Loss: 0.00001789
Iteration 154/1000 | Loss: 0.00001789
Iteration 155/1000 | Loss: 0.00001789
Iteration 156/1000 | Loss: 0.00001789
Iteration 157/1000 | Loss: 0.00001789
Iteration 158/1000 | Loss: 0.00001789
Iteration 159/1000 | Loss: 0.00001789
Iteration 160/1000 | Loss: 0.00001789
Iteration 161/1000 | Loss: 0.00001789
Iteration 162/1000 | Loss: 0.00001789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.7886697605717927e-05, 1.7886697605717927e-05, 1.7886697605717927e-05, 1.7886697605717927e-05, 1.7886697605717927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7886697605717927e-05

Optimization complete. Final v2v error: 3.474836587905884 mm

Highest mean error: 4.443559646606445 mm for frame 31

Lowest mean error: 2.735421895980835 mm for frame 213

Saving results

Total time: 54.40329933166504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039935
Iteration 2/25 | Loss: 0.00214514
Iteration 3/25 | Loss: 0.00164690
Iteration 4/25 | Loss: 0.00158947
Iteration 5/25 | Loss: 0.00155031
Iteration 6/25 | Loss: 0.00152378
Iteration 7/25 | Loss: 0.00149619
Iteration 8/25 | Loss: 0.00150649
Iteration 9/25 | Loss: 0.00143771
Iteration 10/25 | Loss: 0.00143154
Iteration 11/25 | Loss: 0.00143352
Iteration 12/25 | Loss: 0.00143208
Iteration 13/25 | Loss: 0.00141889
Iteration 14/25 | Loss: 0.00141076
Iteration 15/25 | Loss: 0.00140433
Iteration 16/25 | Loss: 0.00140574
Iteration 17/25 | Loss: 0.00139911
Iteration 18/25 | Loss: 0.00139637
Iteration 19/25 | Loss: 0.00139563
Iteration 20/25 | Loss: 0.00139290
Iteration 21/25 | Loss: 0.00138504
Iteration 22/25 | Loss: 0.00138417
Iteration 23/25 | Loss: 0.00138388
Iteration 24/25 | Loss: 0.00138365
Iteration 25/25 | Loss: 0.00138648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16627049
Iteration 2/25 | Loss: 0.00190812
Iteration 3/25 | Loss: 0.00190808
Iteration 4/25 | Loss: 0.00190808
Iteration 5/25 | Loss: 0.00190808
Iteration 6/25 | Loss: 0.00190808
Iteration 7/25 | Loss: 0.00190808
Iteration 8/25 | Loss: 0.00190808
Iteration 9/25 | Loss: 0.00190808
Iteration 10/25 | Loss: 0.00190808
Iteration 11/25 | Loss: 0.00190808
Iteration 12/25 | Loss: 0.00190808
Iteration 13/25 | Loss: 0.00190808
Iteration 14/25 | Loss: 0.00190808
Iteration 15/25 | Loss: 0.00190808
Iteration 16/25 | Loss: 0.00190808
Iteration 17/25 | Loss: 0.00190808
Iteration 18/25 | Loss: 0.00190808
Iteration 19/25 | Loss: 0.00190808
Iteration 20/25 | Loss: 0.00190808
Iteration 21/25 | Loss: 0.00190808
Iteration 22/25 | Loss: 0.00190808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019080754136666656, 0.0019080754136666656, 0.0019080754136666656, 0.0019080754136666656, 0.0019080754136666656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019080754136666656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190808
Iteration 2/1000 | Loss: 0.00036861
Iteration 3/1000 | Loss: 0.00026290
Iteration 4/1000 | Loss: 0.00009077
Iteration 5/1000 | Loss: 0.00007311
Iteration 6/1000 | Loss: 0.00006074
Iteration 7/1000 | Loss: 0.00005542
Iteration 8/1000 | Loss: 0.00029662
Iteration 9/1000 | Loss: 0.00027940
Iteration 10/1000 | Loss: 0.00032089
Iteration 11/1000 | Loss: 0.00024135
Iteration 12/1000 | Loss: 0.00098197
Iteration 13/1000 | Loss: 0.00006210
Iteration 14/1000 | Loss: 0.00004772
Iteration 15/1000 | Loss: 0.00004468
Iteration 16/1000 | Loss: 0.00036508
Iteration 17/1000 | Loss: 0.00004419
Iteration 18/1000 | Loss: 0.00003960
Iteration 19/1000 | Loss: 0.00033993
Iteration 20/1000 | Loss: 0.00004124
Iteration 21/1000 | Loss: 0.00032792
Iteration 22/1000 | Loss: 0.00003935
Iteration 23/1000 | Loss: 0.00003545
Iteration 24/1000 | Loss: 0.00003426
Iteration 25/1000 | Loss: 0.00003273
Iteration 26/1000 | Loss: 0.00003194
Iteration 27/1000 | Loss: 0.00031282
Iteration 28/1000 | Loss: 0.00003360
Iteration 29/1000 | Loss: 0.00003083
Iteration 30/1000 | Loss: 0.00002998
Iteration 31/1000 | Loss: 0.00026038
Iteration 32/1000 | Loss: 0.00028761
Iteration 33/1000 | Loss: 0.00003040
Iteration 34/1000 | Loss: 0.00002763
Iteration 35/1000 | Loss: 0.00002686
Iteration 36/1000 | Loss: 0.00002561
Iteration 37/1000 | Loss: 0.00002494
Iteration 38/1000 | Loss: 0.00002451
Iteration 39/1000 | Loss: 0.00002430
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002396
Iteration 42/1000 | Loss: 0.00002395
Iteration 43/1000 | Loss: 0.00002382
Iteration 44/1000 | Loss: 0.00002375
Iteration 45/1000 | Loss: 0.00002372
Iteration 46/1000 | Loss: 0.00002372
Iteration 47/1000 | Loss: 0.00002369
Iteration 48/1000 | Loss: 0.00002369
Iteration 49/1000 | Loss: 0.00002369
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002369
Iteration 52/1000 | Loss: 0.00002369
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00002368
Iteration 55/1000 | Loss: 0.00002368
Iteration 56/1000 | Loss: 0.00002368
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002367
Iteration 59/1000 | Loss: 0.00002367
Iteration 60/1000 | Loss: 0.00002367
Iteration 61/1000 | Loss: 0.00002367
Iteration 62/1000 | Loss: 0.00002367
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002367
Iteration 65/1000 | Loss: 0.00002367
Iteration 66/1000 | Loss: 0.00002367
Iteration 67/1000 | Loss: 0.00002367
Iteration 68/1000 | Loss: 0.00002366
Iteration 69/1000 | Loss: 0.00002366
Iteration 70/1000 | Loss: 0.00002366
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002366
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002364
Iteration 85/1000 | Loss: 0.00002363
Iteration 86/1000 | Loss: 0.00002363
Iteration 87/1000 | Loss: 0.00002363
Iteration 88/1000 | Loss: 0.00002363
Iteration 89/1000 | Loss: 0.00002363
Iteration 90/1000 | Loss: 0.00002363
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002362
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002362
Iteration 96/1000 | Loss: 0.00002362
Iteration 97/1000 | Loss: 0.00002361
Iteration 98/1000 | Loss: 0.00002361
Iteration 99/1000 | Loss: 0.00002361
Iteration 100/1000 | Loss: 0.00002361
Iteration 101/1000 | Loss: 0.00002361
Iteration 102/1000 | Loss: 0.00002361
Iteration 103/1000 | Loss: 0.00002361
Iteration 104/1000 | Loss: 0.00002361
Iteration 105/1000 | Loss: 0.00002361
Iteration 106/1000 | Loss: 0.00002361
Iteration 107/1000 | Loss: 0.00002361
Iteration 108/1000 | Loss: 0.00002361
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002361
Iteration 111/1000 | Loss: 0.00002360
Iteration 112/1000 | Loss: 0.00002360
Iteration 113/1000 | Loss: 0.00002360
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002360
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002359
Iteration 121/1000 | Loss: 0.00002359
Iteration 122/1000 | Loss: 0.00002359
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002358
Iteration 125/1000 | Loss: 0.00002358
Iteration 126/1000 | Loss: 0.00002358
Iteration 127/1000 | Loss: 0.00002358
Iteration 128/1000 | Loss: 0.00002358
Iteration 129/1000 | Loss: 0.00002358
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002356
Iteration 136/1000 | Loss: 0.00002356
Iteration 137/1000 | Loss: 0.00002356
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002356
Iteration 140/1000 | Loss: 0.00002356
Iteration 141/1000 | Loss: 0.00002355
Iteration 142/1000 | Loss: 0.00002355
Iteration 143/1000 | Loss: 0.00002355
Iteration 144/1000 | Loss: 0.00002355
Iteration 145/1000 | Loss: 0.00002355
Iteration 146/1000 | Loss: 0.00002355
Iteration 147/1000 | Loss: 0.00002355
Iteration 148/1000 | Loss: 0.00002355
Iteration 149/1000 | Loss: 0.00002355
Iteration 150/1000 | Loss: 0.00002355
Iteration 151/1000 | Loss: 0.00002355
Iteration 152/1000 | Loss: 0.00002355
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002354
Iteration 157/1000 | Loss: 0.00002354
Iteration 158/1000 | Loss: 0.00002354
Iteration 159/1000 | Loss: 0.00002354
Iteration 160/1000 | Loss: 0.00002354
Iteration 161/1000 | Loss: 0.00002354
Iteration 162/1000 | Loss: 0.00002354
Iteration 163/1000 | Loss: 0.00002354
Iteration 164/1000 | Loss: 0.00002353
Iteration 165/1000 | Loss: 0.00002353
Iteration 166/1000 | Loss: 0.00002353
Iteration 167/1000 | Loss: 0.00002353
Iteration 168/1000 | Loss: 0.00002353
Iteration 169/1000 | Loss: 0.00002353
Iteration 170/1000 | Loss: 0.00002353
Iteration 171/1000 | Loss: 0.00002353
Iteration 172/1000 | Loss: 0.00002352
Iteration 173/1000 | Loss: 0.00002352
Iteration 174/1000 | Loss: 0.00002352
Iteration 175/1000 | Loss: 0.00002352
Iteration 176/1000 | Loss: 0.00002351
Iteration 177/1000 | Loss: 0.00002351
Iteration 178/1000 | Loss: 0.00002351
Iteration 179/1000 | Loss: 0.00002351
Iteration 180/1000 | Loss: 0.00002351
Iteration 181/1000 | Loss: 0.00002351
Iteration 182/1000 | Loss: 0.00002351
Iteration 183/1000 | Loss: 0.00002351
Iteration 184/1000 | Loss: 0.00002350
Iteration 185/1000 | Loss: 0.00002350
Iteration 186/1000 | Loss: 0.00002350
Iteration 187/1000 | Loss: 0.00002350
Iteration 188/1000 | Loss: 0.00002350
Iteration 189/1000 | Loss: 0.00002350
Iteration 190/1000 | Loss: 0.00002350
Iteration 191/1000 | Loss: 0.00002350
Iteration 192/1000 | Loss: 0.00002350
Iteration 193/1000 | Loss: 0.00002350
Iteration 194/1000 | Loss: 0.00002350
Iteration 195/1000 | Loss: 0.00002350
Iteration 196/1000 | Loss: 0.00002349
Iteration 197/1000 | Loss: 0.00002349
Iteration 198/1000 | Loss: 0.00002349
Iteration 199/1000 | Loss: 0.00002349
Iteration 200/1000 | Loss: 0.00002349
Iteration 201/1000 | Loss: 0.00002349
Iteration 202/1000 | Loss: 0.00002349
Iteration 203/1000 | Loss: 0.00002349
Iteration 204/1000 | Loss: 0.00002349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.3494272682000883e-05, 2.3494272682000883e-05, 2.3494272682000883e-05, 2.3494272682000883e-05, 2.3494272682000883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3494272682000883e-05

Optimization complete. Final v2v error: 3.931814432144165 mm

Highest mean error: 5.320159912109375 mm for frame 230

Lowest mean error: 3.42435359954834 mm for frame 0

Saving results

Total time: 131.7228353023529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00289975
Iteration 2/25 | Loss: 0.00132014
Iteration 3/25 | Loss: 0.00117651
Iteration 4/25 | Loss: 0.00115715
Iteration 5/25 | Loss: 0.00115146
Iteration 6/25 | Loss: 0.00114960
Iteration 7/25 | Loss: 0.00114960
Iteration 8/25 | Loss: 0.00114960
Iteration 9/25 | Loss: 0.00114960
Iteration 10/25 | Loss: 0.00114960
Iteration 11/25 | Loss: 0.00114960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001149601535871625, 0.001149601535871625, 0.001149601535871625, 0.001149601535871625, 0.001149601535871625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001149601535871625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27509248
Iteration 2/25 | Loss: 0.00181742
Iteration 3/25 | Loss: 0.00181742
Iteration 4/25 | Loss: 0.00181742
Iteration 5/25 | Loss: 0.00181742
Iteration 6/25 | Loss: 0.00181742
Iteration 7/25 | Loss: 0.00181742
Iteration 8/25 | Loss: 0.00181742
Iteration 9/25 | Loss: 0.00181742
Iteration 10/25 | Loss: 0.00181742
Iteration 11/25 | Loss: 0.00181742
Iteration 12/25 | Loss: 0.00181741
Iteration 13/25 | Loss: 0.00181741
Iteration 14/25 | Loss: 0.00181741
Iteration 15/25 | Loss: 0.00181741
Iteration 16/25 | Loss: 0.00181741
Iteration 17/25 | Loss: 0.00181741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018174148863181472, 0.0018174148863181472, 0.0018174148863181472, 0.0018174148863181472, 0.0018174148863181472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018174148863181472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181741
Iteration 2/1000 | Loss: 0.00003916
Iteration 3/1000 | Loss: 0.00002574
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001676
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001504
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001487
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001483
Iteration 16/1000 | Loss: 0.00001473
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001463
Iteration 19/1000 | Loss: 0.00001462
Iteration 20/1000 | Loss: 0.00001462
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001457
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001449
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001444
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001440
Iteration 57/1000 | Loss: 0.00001440
Iteration 58/1000 | Loss: 0.00001440
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001439
Iteration 62/1000 | Loss: 0.00001439
Iteration 63/1000 | Loss: 0.00001438
Iteration 64/1000 | Loss: 0.00001438
Iteration 65/1000 | Loss: 0.00001438
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001437
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001436
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001434
Iteration 83/1000 | Loss: 0.00001434
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.4340569578052964e-05, 1.4340569578052964e-05, 1.4340569578052964e-05, 1.4340569578052964e-05, 1.4340569578052964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4340569578052964e-05

Optimization complete. Final v2v error: 3.208836317062378 mm

Highest mean error: 3.565087080001831 mm for frame 210

Lowest mean error: 2.797711133956909 mm for frame 5

Saving results

Total time: 40.45554518699646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771965
Iteration 2/25 | Loss: 0.00164210
Iteration 3/25 | Loss: 0.00138540
Iteration 4/25 | Loss: 0.00126806
Iteration 5/25 | Loss: 0.00124999
Iteration 6/25 | Loss: 0.00124560
Iteration 7/25 | Loss: 0.00123270
Iteration 8/25 | Loss: 0.00121446
Iteration 9/25 | Loss: 0.00120597
Iteration 10/25 | Loss: 0.00120472
Iteration 11/25 | Loss: 0.00120075
Iteration 12/25 | Loss: 0.00119990
Iteration 13/25 | Loss: 0.00119957
Iteration 14/25 | Loss: 0.00119946
Iteration 15/25 | Loss: 0.00119940
Iteration 16/25 | Loss: 0.00119940
Iteration 17/25 | Loss: 0.00119940
Iteration 18/25 | Loss: 0.00119939
Iteration 19/25 | Loss: 0.00119939
Iteration 20/25 | Loss: 0.00119939
Iteration 21/25 | Loss: 0.00119939
Iteration 22/25 | Loss: 0.00119938
Iteration 23/25 | Loss: 0.00119938
Iteration 24/25 | Loss: 0.00119938
Iteration 25/25 | Loss: 0.00119938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72818851
Iteration 2/25 | Loss: 0.00158863
Iteration 3/25 | Loss: 0.00158862
Iteration 4/25 | Loss: 0.00146346
Iteration 5/25 | Loss: 0.00146315
Iteration 6/25 | Loss: 0.00146315
Iteration 7/25 | Loss: 0.00146315
Iteration 8/25 | Loss: 0.00146315
Iteration 9/25 | Loss: 0.00146315
Iteration 10/25 | Loss: 0.00146315
Iteration 11/25 | Loss: 0.00146315
Iteration 12/25 | Loss: 0.00146315
Iteration 13/25 | Loss: 0.00146315
Iteration 14/25 | Loss: 0.00146315
Iteration 15/25 | Loss: 0.00146315
Iteration 16/25 | Loss: 0.00146315
Iteration 17/25 | Loss: 0.00146315
Iteration 18/25 | Loss: 0.00146315
Iteration 19/25 | Loss: 0.00146315
Iteration 20/25 | Loss: 0.00146315
Iteration 21/25 | Loss: 0.00146315
Iteration 22/25 | Loss: 0.00146315
Iteration 23/25 | Loss: 0.00146315
Iteration 24/25 | Loss: 0.00146315
Iteration 25/25 | Loss: 0.00146315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146315
Iteration 2/1000 | Loss: 0.00005263
Iteration 3/1000 | Loss: 0.00003480
Iteration 4/1000 | Loss: 0.00007977
Iteration 5/1000 | Loss: 0.00006030
Iteration 6/1000 | Loss: 0.00002245
Iteration 7/1000 | Loss: 0.00014278
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001854
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001700
Iteration 13/1000 | Loss: 0.00010296
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00035682
Iteration 16/1000 | Loss: 0.00078387
Iteration 17/1000 | Loss: 0.00074886
Iteration 18/1000 | Loss: 0.00078158
Iteration 19/1000 | Loss: 0.00042937
Iteration 20/1000 | Loss: 0.00078867
Iteration 21/1000 | Loss: 0.00012284
Iteration 22/1000 | Loss: 0.00002344
Iteration 23/1000 | Loss: 0.00009330
Iteration 24/1000 | Loss: 0.00001609
Iteration 25/1000 | Loss: 0.00001388
Iteration 26/1000 | Loss: 0.00007609
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00004713
Iteration 32/1000 | Loss: 0.00006110
Iteration 33/1000 | Loss: 0.00002153
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001058
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001046
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001036
Iteration 41/1000 | Loss: 0.00001035
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001032
Iteration 47/1000 | Loss: 0.00001032
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001028
Iteration 56/1000 | Loss: 0.00001027
Iteration 57/1000 | Loss: 0.00001027
Iteration 58/1000 | Loss: 0.00001027
Iteration 59/1000 | Loss: 0.00001026
Iteration 60/1000 | Loss: 0.00001026
Iteration 61/1000 | Loss: 0.00001025
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001023
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001019
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001018
Iteration 95/1000 | Loss: 0.00001018
Iteration 96/1000 | Loss: 0.00001018
Iteration 97/1000 | Loss: 0.00001018
Iteration 98/1000 | Loss: 0.00001017
Iteration 99/1000 | Loss: 0.00001017
Iteration 100/1000 | Loss: 0.00001017
Iteration 101/1000 | Loss: 0.00001016
Iteration 102/1000 | Loss: 0.00001016
Iteration 103/1000 | Loss: 0.00001016
Iteration 104/1000 | Loss: 0.00001016
Iteration 105/1000 | Loss: 0.00001016
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001016
Iteration 108/1000 | Loss: 0.00001015
Iteration 109/1000 | Loss: 0.00001015
Iteration 110/1000 | Loss: 0.00001014
Iteration 111/1000 | Loss: 0.00001014
Iteration 112/1000 | Loss: 0.00001014
Iteration 113/1000 | Loss: 0.00001014
Iteration 114/1000 | Loss: 0.00001014
Iteration 115/1000 | Loss: 0.00001013
Iteration 116/1000 | Loss: 0.00001013
Iteration 117/1000 | Loss: 0.00001012
Iteration 118/1000 | Loss: 0.00001012
Iteration 119/1000 | Loss: 0.00001012
Iteration 120/1000 | Loss: 0.00001012
Iteration 121/1000 | Loss: 0.00001012
Iteration 122/1000 | Loss: 0.00001011
Iteration 123/1000 | Loss: 0.00001011
Iteration 124/1000 | Loss: 0.00001011
Iteration 125/1000 | Loss: 0.00001010
Iteration 126/1000 | Loss: 0.00001010
Iteration 127/1000 | Loss: 0.00001010
Iteration 128/1000 | Loss: 0.00001010
Iteration 129/1000 | Loss: 0.00001010
Iteration 130/1000 | Loss: 0.00001010
Iteration 131/1000 | Loss: 0.00001010
Iteration 132/1000 | Loss: 0.00001010
Iteration 133/1000 | Loss: 0.00001010
Iteration 134/1000 | Loss: 0.00001010
Iteration 135/1000 | Loss: 0.00001010
Iteration 136/1000 | Loss: 0.00001010
Iteration 137/1000 | Loss: 0.00001010
Iteration 138/1000 | Loss: 0.00001010
Iteration 139/1000 | Loss: 0.00001010
Iteration 140/1000 | Loss: 0.00001010
Iteration 141/1000 | Loss: 0.00001010
Iteration 142/1000 | Loss: 0.00001010
Iteration 143/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.009963216347387e-05, 1.009963216347387e-05, 1.009963216347387e-05, 1.009963216347387e-05, 1.009963216347387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.009963216347387e-05

Optimization complete. Final v2v error: 2.7307729721069336 mm

Highest mean error: 3.3775930404663086 mm for frame 49

Lowest mean error: 2.4588496685028076 mm for frame 37

Saving results

Total time: 97.79066038131714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869664
Iteration 2/25 | Loss: 0.00147510
Iteration 3/25 | Loss: 0.00124913
Iteration 4/25 | Loss: 0.00123050
Iteration 5/25 | Loss: 0.00122696
Iteration 6/25 | Loss: 0.00122666
Iteration 7/25 | Loss: 0.00122666
Iteration 8/25 | Loss: 0.00122666
Iteration 9/25 | Loss: 0.00122666
Iteration 10/25 | Loss: 0.00122666
Iteration 11/25 | Loss: 0.00122666
Iteration 12/25 | Loss: 0.00122666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012266550911590457, 0.0012266550911590457, 0.0012266550911590457, 0.0012266550911590457, 0.0012266550911590457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012266550911590457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86622614
Iteration 2/25 | Loss: 0.00079990
Iteration 3/25 | Loss: 0.00079989
Iteration 4/25 | Loss: 0.00079989
Iteration 5/25 | Loss: 0.00079989
Iteration 6/25 | Loss: 0.00079989
Iteration 7/25 | Loss: 0.00079989
Iteration 8/25 | Loss: 0.00079989
Iteration 9/25 | Loss: 0.00079989
Iteration 10/25 | Loss: 0.00079989
Iteration 11/25 | Loss: 0.00079989
Iteration 12/25 | Loss: 0.00079989
Iteration 13/25 | Loss: 0.00079989
Iteration 14/25 | Loss: 0.00079989
Iteration 15/25 | Loss: 0.00079989
Iteration 16/25 | Loss: 0.00079989
Iteration 17/25 | Loss: 0.00079989
Iteration 18/25 | Loss: 0.00079989
Iteration 19/25 | Loss: 0.00079989
Iteration 20/25 | Loss: 0.00079989
Iteration 21/25 | Loss: 0.00079989
Iteration 22/25 | Loss: 0.00079989
Iteration 23/25 | Loss: 0.00079989
Iteration 24/25 | Loss: 0.00079989
Iteration 25/25 | Loss: 0.00079989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079989
Iteration 2/1000 | Loss: 0.00003781
Iteration 3/1000 | Loss: 0.00003117
Iteration 4/1000 | Loss: 0.00002924
Iteration 5/1000 | Loss: 0.00002816
Iteration 6/1000 | Loss: 0.00002752
Iteration 7/1000 | Loss: 0.00002699
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002648
Iteration 10/1000 | Loss: 0.00002629
Iteration 11/1000 | Loss: 0.00002611
Iteration 12/1000 | Loss: 0.00002602
Iteration 13/1000 | Loss: 0.00002583
Iteration 14/1000 | Loss: 0.00002575
Iteration 15/1000 | Loss: 0.00002575
Iteration 16/1000 | Loss: 0.00002573
Iteration 17/1000 | Loss: 0.00002573
Iteration 18/1000 | Loss: 0.00002573
Iteration 19/1000 | Loss: 0.00002573
Iteration 20/1000 | Loss: 0.00002573
Iteration 21/1000 | Loss: 0.00002573
Iteration 22/1000 | Loss: 0.00002573
Iteration 23/1000 | Loss: 0.00002573
Iteration 24/1000 | Loss: 0.00002573
Iteration 25/1000 | Loss: 0.00002573
Iteration 26/1000 | Loss: 0.00002573
Iteration 27/1000 | Loss: 0.00002573
Iteration 28/1000 | Loss: 0.00002572
Iteration 29/1000 | Loss: 0.00002571
Iteration 30/1000 | Loss: 0.00002571
Iteration 31/1000 | Loss: 0.00002571
Iteration 32/1000 | Loss: 0.00002571
Iteration 33/1000 | Loss: 0.00002571
Iteration 34/1000 | Loss: 0.00002571
Iteration 35/1000 | Loss: 0.00002571
Iteration 36/1000 | Loss: 0.00002571
Iteration 37/1000 | Loss: 0.00002571
Iteration 38/1000 | Loss: 0.00002571
Iteration 39/1000 | Loss: 0.00002571
Iteration 40/1000 | Loss: 0.00002571
Iteration 41/1000 | Loss: 0.00002571
Iteration 42/1000 | Loss: 0.00002571
Iteration 43/1000 | Loss: 0.00002571
Iteration 44/1000 | Loss: 0.00002571
Iteration 45/1000 | Loss: 0.00002571
Iteration 46/1000 | Loss: 0.00002571
Iteration 47/1000 | Loss: 0.00002571
Iteration 48/1000 | Loss: 0.00002571
Iteration 49/1000 | Loss: 0.00002571
Iteration 50/1000 | Loss: 0.00002571
Iteration 51/1000 | Loss: 0.00002571
Iteration 52/1000 | Loss: 0.00002571
Iteration 53/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [2.571140976215247e-05, 2.571140976215247e-05, 2.571140976215247e-05, 2.571140976215247e-05, 2.571140976215247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.571140976215247e-05

Optimization complete. Final v2v error: 4.341198921203613 mm

Highest mean error: 4.61592435836792 mm for frame 112

Lowest mean error: 4.0237650871276855 mm for frame 81

Saving results

Total time: 28.469773769378662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390344
Iteration 2/25 | Loss: 0.00139788
Iteration 3/25 | Loss: 0.00121421
Iteration 4/25 | Loss: 0.00119154
Iteration 5/25 | Loss: 0.00118870
Iteration 6/25 | Loss: 0.00118823
Iteration 7/25 | Loss: 0.00118823
Iteration 8/25 | Loss: 0.00118823
Iteration 9/25 | Loss: 0.00118823
Iteration 10/25 | Loss: 0.00118823
Iteration 11/25 | Loss: 0.00118823
Iteration 12/25 | Loss: 0.00118823
Iteration 13/25 | Loss: 0.00118823
Iteration 14/25 | Loss: 0.00118823
Iteration 15/25 | Loss: 0.00118823
Iteration 16/25 | Loss: 0.00118823
Iteration 17/25 | Loss: 0.00118823
Iteration 18/25 | Loss: 0.00118823
Iteration 19/25 | Loss: 0.00118823
Iteration 20/25 | Loss: 0.00118823
Iteration 21/25 | Loss: 0.00118823
Iteration 22/25 | Loss: 0.00118823
Iteration 23/25 | Loss: 0.00118823
Iteration 24/25 | Loss: 0.00118823
Iteration 25/25 | Loss: 0.00118823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29134405
Iteration 2/25 | Loss: 0.00143321
Iteration 3/25 | Loss: 0.00143321
Iteration 4/25 | Loss: 0.00143321
Iteration 5/25 | Loss: 0.00143321
Iteration 6/25 | Loss: 0.00143321
Iteration 7/25 | Loss: 0.00143321
Iteration 8/25 | Loss: 0.00143321
Iteration 9/25 | Loss: 0.00143321
Iteration 10/25 | Loss: 0.00143321
Iteration 11/25 | Loss: 0.00143321
Iteration 12/25 | Loss: 0.00143321
Iteration 13/25 | Loss: 0.00143321
Iteration 14/25 | Loss: 0.00143321
Iteration 15/25 | Loss: 0.00143321
Iteration 16/25 | Loss: 0.00143321
Iteration 17/25 | Loss: 0.00143321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014332099817693233, 0.0014332099817693233, 0.0014332099817693233, 0.0014332099817693233, 0.0014332099817693233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014332099817693233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143321
Iteration 2/1000 | Loss: 0.00002670
Iteration 3/1000 | Loss: 0.00001698
Iteration 4/1000 | Loss: 0.00001510
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001126
Iteration 11/1000 | Loss: 0.00001101
Iteration 12/1000 | Loss: 0.00001088
Iteration 13/1000 | Loss: 0.00001080
Iteration 14/1000 | Loss: 0.00001070
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001059
Iteration 19/1000 | Loss: 0.00001058
Iteration 20/1000 | Loss: 0.00001057
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001055
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001051
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001039
Iteration 36/1000 | Loss: 0.00001039
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001032
Iteration 42/1000 | Loss: 0.00001032
Iteration 43/1000 | Loss: 0.00001031
Iteration 44/1000 | Loss: 0.00001031
Iteration 45/1000 | Loss: 0.00001030
Iteration 46/1000 | Loss: 0.00001028
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001026
Iteration 52/1000 | Loss: 0.00001026
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001025
Iteration 56/1000 | Loss: 0.00001024
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001024
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001023
Iteration 62/1000 | Loss: 0.00001023
Iteration 63/1000 | Loss: 0.00001023
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001023
Iteration 66/1000 | Loss: 0.00001023
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001021
Iteration 73/1000 | Loss: 0.00001020
Iteration 74/1000 | Loss: 0.00001020
Iteration 75/1000 | Loss: 0.00001020
Iteration 76/1000 | Loss: 0.00001020
Iteration 77/1000 | Loss: 0.00001020
Iteration 78/1000 | Loss: 0.00001020
Iteration 79/1000 | Loss: 0.00001019
Iteration 80/1000 | Loss: 0.00001019
Iteration 81/1000 | Loss: 0.00001018
Iteration 82/1000 | Loss: 0.00001018
Iteration 83/1000 | Loss: 0.00001018
Iteration 84/1000 | Loss: 0.00001018
Iteration 85/1000 | Loss: 0.00001018
Iteration 86/1000 | Loss: 0.00001017
Iteration 87/1000 | Loss: 0.00001017
Iteration 88/1000 | Loss: 0.00001017
Iteration 89/1000 | Loss: 0.00001017
Iteration 90/1000 | Loss: 0.00001017
Iteration 91/1000 | Loss: 0.00001017
Iteration 92/1000 | Loss: 0.00001017
Iteration 93/1000 | Loss: 0.00001017
Iteration 94/1000 | Loss: 0.00001016
Iteration 95/1000 | Loss: 0.00001016
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001016
Iteration 100/1000 | Loss: 0.00001016
Iteration 101/1000 | Loss: 0.00001016
Iteration 102/1000 | Loss: 0.00001015
Iteration 103/1000 | Loss: 0.00001015
Iteration 104/1000 | Loss: 0.00001015
Iteration 105/1000 | Loss: 0.00001015
Iteration 106/1000 | Loss: 0.00001015
Iteration 107/1000 | Loss: 0.00001015
Iteration 108/1000 | Loss: 0.00001014
Iteration 109/1000 | Loss: 0.00001014
Iteration 110/1000 | Loss: 0.00001014
Iteration 111/1000 | Loss: 0.00001013
Iteration 112/1000 | Loss: 0.00001013
Iteration 113/1000 | Loss: 0.00001013
Iteration 114/1000 | Loss: 0.00001012
Iteration 115/1000 | Loss: 0.00001012
Iteration 116/1000 | Loss: 0.00001012
Iteration 117/1000 | Loss: 0.00001012
Iteration 118/1000 | Loss: 0.00001011
Iteration 119/1000 | Loss: 0.00001011
Iteration 120/1000 | Loss: 0.00001011
Iteration 121/1000 | Loss: 0.00001011
Iteration 122/1000 | Loss: 0.00001010
Iteration 123/1000 | Loss: 0.00001010
Iteration 124/1000 | Loss: 0.00001010
Iteration 125/1000 | Loss: 0.00001010
Iteration 126/1000 | Loss: 0.00001010
Iteration 127/1000 | Loss: 0.00001010
Iteration 128/1000 | Loss: 0.00001010
Iteration 129/1000 | Loss: 0.00001009
Iteration 130/1000 | Loss: 0.00001009
Iteration 131/1000 | Loss: 0.00001009
Iteration 132/1000 | Loss: 0.00001009
Iteration 133/1000 | Loss: 0.00001009
Iteration 134/1000 | Loss: 0.00001008
Iteration 135/1000 | Loss: 0.00001008
Iteration 136/1000 | Loss: 0.00001007
Iteration 137/1000 | Loss: 0.00001007
Iteration 138/1000 | Loss: 0.00001007
Iteration 139/1000 | Loss: 0.00001007
Iteration 140/1000 | Loss: 0.00001006
Iteration 141/1000 | Loss: 0.00001006
Iteration 142/1000 | Loss: 0.00001006
Iteration 143/1000 | Loss: 0.00001006
Iteration 144/1000 | Loss: 0.00001006
Iteration 145/1000 | Loss: 0.00001006
Iteration 146/1000 | Loss: 0.00001006
Iteration 147/1000 | Loss: 0.00001006
Iteration 148/1000 | Loss: 0.00001006
Iteration 149/1000 | Loss: 0.00001005
Iteration 150/1000 | Loss: 0.00001005
Iteration 151/1000 | Loss: 0.00001005
Iteration 152/1000 | Loss: 0.00001005
Iteration 153/1000 | Loss: 0.00001004
Iteration 154/1000 | Loss: 0.00001004
Iteration 155/1000 | Loss: 0.00001004
Iteration 156/1000 | Loss: 0.00001004
Iteration 157/1000 | Loss: 0.00001004
Iteration 158/1000 | Loss: 0.00001004
Iteration 159/1000 | Loss: 0.00001004
Iteration 160/1000 | Loss: 0.00001004
Iteration 161/1000 | Loss: 0.00001004
Iteration 162/1000 | Loss: 0.00001004
Iteration 163/1000 | Loss: 0.00001003
Iteration 164/1000 | Loss: 0.00001003
Iteration 165/1000 | Loss: 0.00001003
Iteration 166/1000 | Loss: 0.00001003
Iteration 167/1000 | Loss: 0.00001003
Iteration 168/1000 | Loss: 0.00001003
Iteration 169/1000 | Loss: 0.00001003
Iteration 170/1000 | Loss: 0.00001003
Iteration 171/1000 | Loss: 0.00001003
Iteration 172/1000 | Loss: 0.00001003
Iteration 173/1000 | Loss: 0.00001003
Iteration 174/1000 | Loss: 0.00001003
Iteration 175/1000 | Loss: 0.00001003
Iteration 176/1000 | Loss: 0.00001003
Iteration 177/1000 | Loss: 0.00001002
Iteration 178/1000 | Loss: 0.00001002
Iteration 179/1000 | Loss: 0.00001002
Iteration 180/1000 | Loss: 0.00001002
Iteration 181/1000 | Loss: 0.00001002
Iteration 182/1000 | Loss: 0.00001002
Iteration 183/1000 | Loss: 0.00001002
Iteration 184/1000 | Loss: 0.00001002
Iteration 185/1000 | Loss: 0.00001002
Iteration 186/1000 | Loss: 0.00001002
Iteration 187/1000 | Loss: 0.00001002
Iteration 188/1000 | Loss: 0.00001002
Iteration 189/1000 | Loss: 0.00001002
Iteration 190/1000 | Loss: 0.00001002
Iteration 191/1000 | Loss: 0.00001002
Iteration 192/1000 | Loss: 0.00001001
Iteration 193/1000 | Loss: 0.00001001
Iteration 194/1000 | Loss: 0.00001001
Iteration 195/1000 | Loss: 0.00001001
Iteration 196/1000 | Loss: 0.00001001
Iteration 197/1000 | Loss: 0.00001001
Iteration 198/1000 | Loss: 0.00001001
Iteration 199/1000 | Loss: 0.00001001
Iteration 200/1000 | Loss: 0.00001001
Iteration 201/1000 | Loss: 0.00001001
Iteration 202/1000 | Loss: 0.00001001
Iteration 203/1000 | Loss: 0.00001001
Iteration 204/1000 | Loss: 0.00001001
Iteration 205/1000 | Loss: 0.00001000
Iteration 206/1000 | Loss: 0.00001000
Iteration 207/1000 | Loss: 0.00001000
Iteration 208/1000 | Loss: 0.00001000
Iteration 209/1000 | Loss: 0.00001000
Iteration 210/1000 | Loss: 0.00001000
Iteration 211/1000 | Loss: 0.00001000
Iteration 212/1000 | Loss: 0.00001000
Iteration 213/1000 | Loss: 0.00001000
Iteration 214/1000 | Loss: 0.00001000
Iteration 215/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [9.999139365390874e-06, 9.999139365390874e-06, 9.999139365390874e-06, 9.999139365390874e-06, 9.999139365390874e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.999139365390874e-06

Optimization complete. Final v2v error: 2.7110865116119385 mm

Highest mean error: 3.333296775817871 mm for frame 80

Lowest mean error: 2.4771130084991455 mm for frame 210

Saving results

Total time: 49.46075177192688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935542
Iteration 2/25 | Loss: 0.00204238
Iteration 3/25 | Loss: 0.00153940
Iteration 4/25 | Loss: 0.00142471
Iteration 5/25 | Loss: 0.00140029
Iteration 6/25 | Loss: 0.00140011
Iteration 7/25 | Loss: 0.00135594
Iteration 8/25 | Loss: 0.00133578
Iteration 9/25 | Loss: 0.00131754
Iteration 10/25 | Loss: 0.00130979
Iteration 11/25 | Loss: 0.00130384
Iteration 12/25 | Loss: 0.00129922
Iteration 13/25 | Loss: 0.00129621
Iteration 14/25 | Loss: 0.00129485
Iteration 15/25 | Loss: 0.00129425
Iteration 16/25 | Loss: 0.00129412
Iteration 17/25 | Loss: 0.00129410
Iteration 18/25 | Loss: 0.00129409
Iteration 19/25 | Loss: 0.00129409
Iteration 20/25 | Loss: 0.00129409
Iteration 21/25 | Loss: 0.00129409
Iteration 22/25 | Loss: 0.00129409
Iteration 23/25 | Loss: 0.00129408
Iteration 24/25 | Loss: 0.00129408
Iteration 25/25 | Loss: 0.00129408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33576226
Iteration 2/25 | Loss: 0.00127548
Iteration 3/25 | Loss: 0.00127548
Iteration 4/25 | Loss: 0.00127548
Iteration 5/25 | Loss: 0.00127548
Iteration 6/25 | Loss: 0.00127548
Iteration 7/25 | Loss: 0.00127548
Iteration 8/25 | Loss: 0.00127548
Iteration 9/25 | Loss: 0.00127548
Iteration 10/25 | Loss: 0.00127548
Iteration 11/25 | Loss: 0.00127548
Iteration 12/25 | Loss: 0.00127547
Iteration 13/25 | Loss: 0.00127547
Iteration 14/25 | Loss: 0.00127547
Iteration 15/25 | Loss: 0.00127547
Iteration 16/25 | Loss: 0.00127547
Iteration 17/25 | Loss: 0.00127547
Iteration 18/25 | Loss: 0.00127547
Iteration 19/25 | Loss: 0.00127547
Iteration 20/25 | Loss: 0.00127547
Iteration 21/25 | Loss: 0.00127547
Iteration 22/25 | Loss: 0.00127547
Iteration 23/25 | Loss: 0.00127547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012754740891978145, 0.0012754740891978145, 0.0012754740891978145, 0.0012754740891978145, 0.0012754740891978145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012754740891978145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127547
Iteration 2/1000 | Loss: 0.00006648
Iteration 3/1000 | Loss: 0.00005056
Iteration 4/1000 | Loss: 0.00004479
Iteration 5/1000 | Loss: 0.00030376
Iteration 6/1000 | Loss: 0.00018007
Iteration 7/1000 | Loss: 0.00035288
Iteration 8/1000 | Loss: 0.00046330
Iteration 9/1000 | Loss: 0.00026194
Iteration 10/1000 | Loss: 0.00005399
Iteration 11/1000 | Loss: 0.00018368
Iteration 12/1000 | Loss: 0.00033633
Iteration 13/1000 | Loss: 0.00034051
Iteration 14/1000 | Loss: 0.00006716
Iteration 15/1000 | Loss: 0.00021922
Iteration 16/1000 | Loss: 0.00023442
Iteration 17/1000 | Loss: 0.00010921
Iteration 18/1000 | Loss: 0.00010977
Iteration 19/1000 | Loss: 0.00017396
Iteration 20/1000 | Loss: 0.00017508
Iteration 21/1000 | Loss: 0.00013003
Iteration 22/1000 | Loss: 0.00017455
Iteration 23/1000 | Loss: 0.00012563
Iteration 24/1000 | Loss: 0.00028131
Iteration 25/1000 | Loss: 0.00017532
Iteration 26/1000 | Loss: 0.00007695
Iteration 27/1000 | Loss: 0.00004580
Iteration 28/1000 | Loss: 0.00004113
Iteration 29/1000 | Loss: 0.00003909
Iteration 30/1000 | Loss: 0.00028492
Iteration 31/1000 | Loss: 0.00092895
Iteration 32/1000 | Loss: 0.00041166
Iteration 33/1000 | Loss: 0.00043072
Iteration 34/1000 | Loss: 0.00015211
Iteration 35/1000 | Loss: 0.00013004
Iteration 36/1000 | Loss: 0.00004069
Iteration 37/1000 | Loss: 0.00046230
Iteration 38/1000 | Loss: 0.00036116
Iteration 39/1000 | Loss: 0.00043580
Iteration 40/1000 | Loss: 0.00021158
Iteration 41/1000 | Loss: 0.00022574
Iteration 42/1000 | Loss: 0.00013723
Iteration 43/1000 | Loss: 0.00015906
Iteration 44/1000 | Loss: 0.00018720
Iteration 45/1000 | Loss: 0.00014386
Iteration 46/1000 | Loss: 0.00017632
Iteration 47/1000 | Loss: 0.00013956
Iteration 48/1000 | Loss: 0.00015116
Iteration 49/1000 | Loss: 0.00053721
Iteration 50/1000 | Loss: 0.00043057
Iteration 51/1000 | Loss: 0.00061543
Iteration 52/1000 | Loss: 0.00018244
Iteration 53/1000 | Loss: 0.00023727
Iteration 54/1000 | Loss: 0.00036025
Iteration 55/1000 | Loss: 0.00018660
Iteration 56/1000 | Loss: 0.00013317
Iteration 57/1000 | Loss: 0.00010563
Iteration 58/1000 | Loss: 0.00004570
Iteration 59/1000 | Loss: 0.00039755
Iteration 60/1000 | Loss: 0.00053685
Iteration 61/1000 | Loss: 0.00012651
Iteration 62/1000 | Loss: 0.00054159
Iteration 63/1000 | Loss: 0.00051014
Iteration 64/1000 | Loss: 0.00073931
Iteration 65/1000 | Loss: 0.00035522
Iteration 66/1000 | Loss: 0.00065111
Iteration 67/1000 | Loss: 0.00060975
Iteration 68/1000 | Loss: 0.00022934
Iteration 69/1000 | Loss: 0.00013856
Iteration 70/1000 | Loss: 0.00036011
Iteration 71/1000 | Loss: 0.00064934
Iteration 72/1000 | Loss: 0.00046088
Iteration 73/1000 | Loss: 0.00060095
Iteration 74/1000 | Loss: 0.00029432
Iteration 75/1000 | Loss: 0.00023804
Iteration 76/1000 | Loss: 0.00037531
Iteration 77/1000 | Loss: 0.00009962
Iteration 78/1000 | Loss: 0.00006602
Iteration 79/1000 | Loss: 0.00038073
Iteration 80/1000 | Loss: 0.00014134
Iteration 81/1000 | Loss: 0.00012654
Iteration 82/1000 | Loss: 0.00004126
Iteration 83/1000 | Loss: 0.00003681
Iteration 84/1000 | Loss: 0.00003375
Iteration 85/1000 | Loss: 0.00033337
Iteration 86/1000 | Loss: 0.00018874
Iteration 87/1000 | Loss: 0.00025088
Iteration 88/1000 | Loss: 0.00003656
Iteration 89/1000 | Loss: 0.00016260
Iteration 90/1000 | Loss: 0.00003205
Iteration 91/1000 | Loss: 0.00015701
Iteration 92/1000 | Loss: 0.00004159
Iteration 93/1000 | Loss: 0.00032993
Iteration 94/1000 | Loss: 0.00015267
Iteration 95/1000 | Loss: 0.00043945
Iteration 96/1000 | Loss: 0.00004477
Iteration 97/1000 | Loss: 0.00021662
Iteration 98/1000 | Loss: 0.00050730
Iteration 99/1000 | Loss: 0.00069895
Iteration 100/1000 | Loss: 0.00058275
Iteration 101/1000 | Loss: 0.00030048
Iteration 102/1000 | Loss: 0.00005495
Iteration 103/1000 | Loss: 0.00003844
Iteration 104/1000 | Loss: 0.00015607
Iteration 105/1000 | Loss: 0.00035915
Iteration 106/1000 | Loss: 0.00006967
Iteration 107/1000 | Loss: 0.00005345
Iteration 108/1000 | Loss: 0.00004440
Iteration 109/1000 | Loss: 0.00004694
Iteration 110/1000 | Loss: 0.00004176
Iteration 111/1000 | Loss: 0.00033177
Iteration 112/1000 | Loss: 0.00004287
Iteration 113/1000 | Loss: 0.00017047
Iteration 114/1000 | Loss: 0.00002816
Iteration 115/1000 | Loss: 0.00002708
Iteration 116/1000 | Loss: 0.00002303
Iteration 117/1000 | Loss: 0.00002136
Iteration 118/1000 | Loss: 0.00002049
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001941
Iteration 121/1000 | Loss: 0.00017122
Iteration 122/1000 | Loss: 0.00002364
Iteration 123/1000 | Loss: 0.00002088
Iteration 124/1000 | Loss: 0.00001997
Iteration 125/1000 | Loss: 0.00001949
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001902
Iteration 128/1000 | Loss: 0.00001898
Iteration 129/1000 | Loss: 0.00015754
Iteration 130/1000 | Loss: 0.00005959
Iteration 131/1000 | Loss: 0.00006284
Iteration 132/1000 | Loss: 0.00015553
Iteration 133/1000 | Loss: 0.00005230
Iteration 134/1000 | Loss: 0.00006352
Iteration 135/1000 | Loss: 0.00011799
Iteration 136/1000 | Loss: 0.00005132
Iteration 137/1000 | Loss: 0.00005168
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00013821
Iteration 140/1000 | Loss: 0.00022068
Iteration 141/1000 | Loss: 0.00020794
Iteration 142/1000 | Loss: 0.00005182
Iteration 143/1000 | Loss: 0.00008889
Iteration 144/1000 | Loss: 0.00004157
Iteration 145/1000 | Loss: 0.00012491
Iteration 146/1000 | Loss: 0.00025065
Iteration 147/1000 | Loss: 0.00011686
Iteration 148/1000 | Loss: 0.00020221
Iteration 149/1000 | Loss: 0.00011316
Iteration 150/1000 | Loss: 0.00014970
Iteration 151/1000 | Loss: 0.00012609
Iteration 152/1000 | Loss: 0.00009903
Iteration 153/1000 | Loss: 0.00022548
Iteration 154/1000 | Loss: 0.00015004
Iteration 155/1000 | Loss: 0.00002318
Iteration 156/1000 | Loss: 0.00002182
Iteration 157/1000 | Loss: 0.00016062
Iteration 158/1000 | Loss: 0.00019607
Iteration 159/1000 | Loss: 0.00017179
Iteration 160/1000 | Loss: 0.00003840
Iteration 161/1000 | Loss: 0.00002216
Iteration 162/1000 | Loss: 0.00001998
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002250
Iteration 165/1000 | Loss: 0.00005788
Iteration 166/1000 | Loss: 0.00004790
Iteration 167/1000 | Loss: 0.00002154
Iteration 168/1000 | Loss: 0.00001915
Iteration 169/1000 | Loss: 0.00001876
Iteration 170/1000 | Loss: 0.00001850
Iteration 171/1000 | Loss: 0.00001817
Iteration 172/1000 | Loss: 0.00001795
Iteration 173/1000 | Loss: 0.00001794
Iteration 174/1000 | Loss: 0.00001793
Iteration 175/1000 | Loss: 0.00001792
Iteration 176/1000 | Loss: 0.00001792
Iteration 177/1000 | Loss: 0.00001791
Iteration 178/1000 | Loss: 0.00001791
Iteration 179/1000 | Loss: 0.00001790
Iteration 180/1000 | Loss: 0.00001838
Iteration 181/1000 | Loss: 0.00001820
Iteration 182/1000 | Loss: 0.00001820
Iteration 183/1000 | Loss: 0.00001820
Iteration 184/1000 | Loss: 0.00001820
Iteration 185/1000 | Loss: 0.00001820
Iteration 186/1000 | Loss: 0.00001820
Iteration 187/1000 | Loss: 0.00001820
Iteration 188/1000 | Loss: 0.00001820
Iteration 189/1000 | Loss: 0.00001819
Iteration 190/1000 | Loss: 0.00001819
Iteration 191/1000 | Loss: 0.00001801
Iteration 192/1000 | Loss: 0.00001877
Iteration 193/1000 | Loss: 0.00001777
Iteration 194/1000 | Loss: 0.00001832
Iteration 195/1000 | Loss: 0.00001771
Iteration 196/1000 | Loss: 0.00001763
Iteration 197/1000 | Loss: 0.00001763
Iteration 198/1000 | Loss: 0.00001761
Iteration 199/1000 | Loss: 0.00001760
Iteration 200/1000 | Loss: 0.00001759
Iteration 201/1000 | Loss: 0.00001759
Iteration 202/1000 | Loss: 0.00001759
Iteration 203/1000 | Loss: 0.00001758
Iteration 204/1000 | Loss: 0.00018092
Iteration 205/1000 | Loss: 0.00011687
Iteration 206/1000 | Loss: 0.00003400
Iteration 207/1000 | Loss: 0.00002115
Iteration 208/1000 | Loss: 0.00002339
Iteration 209/1000 | Loss: 0.00001923
Iteration 210/1000 | Loss: 0.00001846
Iteration 211/1000 | Loss: 0.00002051
Iteration 212/1000 | Loss: 0.00001777
Iteration 213/1000 | Loss: 0.00001729
Iteration 214/1000 | Loss: 0.00001703
Iteration 215/1000 | Loss: 0.00001691
Iteration 216/1000 | Loss: 0.00001679
Iteration 217/1000 | Loss: 0.00001677
Iteration 218/1000 | Loss: 0.00001677
Iteration 219/1000 | Loss: 0.00001676
Iteration 220/1000 | Loss: 0.00001675
Iteration 221/1000 | Loss: 0.00001675
Iteration 222/1000 | Loss: 0.00001674
Iteration 223/1000 | Loss: 0.00001674
Iteration 224/1000 | Loss: 0.00001672
Iteration 225/1000 | Loss: 0.00001671
Iteration 226/1000 | Loss: 0.00001670
Iteration 227/1000 | Loss: 0.00001670
Iteration 228/1000 | Loss: 0.00001670
Iteration 229/1000 | Loss: 0.00001669
Iteration 230/1000 | Loss: 0.00001669
Iteration 231/1000 | Loss: 0.00001668
Iteration 232/1000 | Loss: 0.00001668
Iteration 233/1000 | Loss: 0.00001667
Iteration 234/1000 | Loss: 0.00001667
Iteration 235/1000 | Loss: 0.00001667
Iteration 236/1000 | Loss: 0.00001666
Iteration 237/1000 | Loss: 0.00001666
Iteration 238/1000 | Loss: 0.00001666
Iteration 239/1000 | Loss: 0.00001666
Iteration 240/1000 | Loss: 0.00001666
Iteration 241/1000 | Loss: 0.00001665
Iteration 242/1000 | Loss: 0.00001665
Iteration 243/1000 | Loss: 0.00001665
Iteration 244/1000 | Loss: 0.00001665
Iteration 245/1000 | Loss: 0.00001665
Iteration 246/1000 | Loss: 0.00001664
Iteration 247/1000 | Loss: 0.00001664
Iteration 248/1000 | Loss: 0.00001664
Iteration 249/1000 | Loss: 0.00001663
Iteration 250/1000 | Loss: 0.00001663
Iteration 251/1000 | Loss: 0.00001663
Iteration 252/1000 | Loss: 0.00001663
Iteration 253/1000 | Loss: 0.00001663
Iteration 254/1000 | Loss: 0.00001662
Iteration 255/1000 | Loss: 0.00001662
Iteration 256/1000 | Loss: 0.00001662
Iteration 257/1000 | Loss: 0.00001661
Iteration 258/1000 | Loss: 0.00001661
Iteration 259/1000 | Loss: 0.00001660
Iteration 260/1000 | Loss: 0.00001660
Iteration 261/1000 | Loss: 0.00001660
Iteration 262/1000 | Loss: 0.00001660
Iteration 263/1000 | Loss: 0.00001660
Iteration 264/1000 | Loss: 0.00001660
Iteration 265/1000 | Loss: 0.00001660
Iteration 266/1000 | Loss: 0.00001660
Iteration 267/1000 | Loss: 0.00001660
Iteration 268/1000 | Loss: 0.00001660
Iteration 269/1000 | Loss: 0.00001659
Iteration 270/1000 | Loss: 0.00001659
Iteration 271/1000 | Loss: 0.00001659
Iteration 272/1000 | Loss: 0.00001659
Iteration 273/1000 | Loss: 0.00001659
Iteration 274/1000 | Loss: 0.00001659
Iteration 275/1000 | Loss: 0.00001659
Iteration 276/1000 | Loss: 0.00001659
Iteration 277/1000 | Loss: 0.00001659
Iteration 278/1000 | Loss: 0.00001659
Iteration 279/1000 | Loss: 0.00001659
Iteration 280/1000 | Loss: 0.00001658
Iteration 281/1000 | Loss: 0.00001658
Iteration 282/1000 | Loss: 0.00001658
Iteration 283/1000 | Loss: 0.00001658
Iteration 284/1000 | Loss: 0.00001658
Iteration 285/1000 | Loss: 0.00001658
Iteration 286/1000 | Loss: 0.00001658
Iteration 287/1000 | Loss: 0.00001658
Iteration 288/1000 | Loss: 0.00001658
Iteration 289/1000 | Loss: 0.00001658
Iteration 290/1000 | Loss: 0.00001658
Iteration 291/1000 | Loss: 0.00001658
Iteration 292/1000 | Loss: 0.00001658
Iteration 293/1000 | Loss: 0.00001658
Iteration 294/1000 | Loss: 0.00001658
Iteration 295/1000 | Loss: 0.00001658
Iteration 296/1000 | Loss: 0.00001658
Iteration 297/1000 | Loss: 0.00001658
Iteration 298/1000 | Loss: 0.00001658
Iteration 299/1000 | Loss: 0.00001658
Iteration 300/1000 | Loss: 0.00001657
Iteration 301/1000 | Loss: 0.00001657
Iteration 302/1000 | Loss: 0.00001657
Iteration 303/1000 | Loss: 0.00001657
Iteration 304/1000 | Loss: 0.00001657
Iteration 305/1000 | Loss: 0.00001657
Iteration 306/1000 | Loss: 0.00001657
Iteration 307/1000 | Loss: 0.00001657
Iteration 308/1000 | Loss: 0.00001657
Iteration 309/1000 | Loss: 0.00001657
Iteration 310/1000 | Loss: 0.00001657
Iteration 311/1000 | Loss: 0.00001657
Iteration 312/1000 | Loss: 0.00001657
Iteration 313/1000 | Loss: 0.00001657
Iteration 314/1000 | Loss: 0.00001657
Iteration 315/1000 | Loss: 0.00001657
Iteration 316/1000 | Loss: 0.00001657
Iteration 317/1000 | Loss: 0.00001657
Iteration 318/1000 | Loss: 0.00001657
Iteration 319/1000 | Loss: 0.00001656
Iteration 320/1000 | Loss: 0.00001656
Iteration 321/1000 | Loss: 0.00001656
Iteration 322/1000 | Loss: 0.00001656
Iteration 323/1000 | Loss: 0.00001656
Iteration 324/1000 | Loss: 0.00001656
Iteration 325/1000 | Loss: 0.00001656
Iteration 326/1000 | Loss: 0.00001655
Iteration 327/1000 | Loss: 0.00001655
Iteration 328/1000 | Loss: 0.00001655
Iteration 329/1000 | Loss: 0.00001655
Iteration 330/1000 | Loss: 0.00001655
Iteration 331/1000 | Loss: 0.00001655
Iteration 332/1000 | Loss: 0.00001655
Iteration 333/1000 | Loss: 0.00001655
Iteration 334/1000 | Loss: 0.00001655
Iteration 335/1000 | Loss: 0.00001655
Iteration 336/1000 | Loss: 0.00001655
Iteration 337/1000 | Loss: 0.00001655
Iteration 338/1000 | Loss: 0.00001655
Iteration 339/1000 | Loss: 0.00001655
Iteration 340/1000 | Loss: 0.00001655
Iteration 341/1000 | Loss: 0.00001655
Iteration 342/1000 | Loss: 0.00001655
Iteration 343/1000 | Loss: 0.00001655
Iteration 344/1000 | Loss: 0.00001655
Iteration 345/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [1.6545793187106028e-05, 1.6545793187106028e-05, 1.6545793187106028e-05, 1.6545793187106028e-05, 1.6545793187106028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6545793187106028e-05

Optimization complete. Final v2v error: 3.2351529598236084 mm

Highest mean error: 5.106298923492432 mm for frame 121

Lowest mean error: 2.641310453414917 mm for frame 40

Saving results

Total time: 345.47172808647156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063108
Iteration 2/25 | Loss: 0.00183276
Iteration 3/25 | Loss: 0.00146905
Iteration 4/25 | Loss: 0.00142266
Iteration 5/25 | Loss: 0.00139705
Iteration 6/25 | Loss: 0.00131962
Iteration 7/25 | Loss: 0.00124209
Iteration 8/25 | Loss: 0.00121800
Iteration 9/25 | Loss: 0.00120657
Iteration 10/25 | Loss: 0.00120228
Iteration 11/25 | Loss: 0.00120160
Iteration 12/25 | Loss: 0.00120147
Iteration 13/25 | Loss: 0.00120147
Iteration 14/25 | Loss: 0.00120145
Iteration 15/25 | Loss: 0.00120145
Iteration 16/25 | Loss: 0.00120145
Iteration 17/25 | Loss: 0.00120144
Iteration 18/25 | Loss: 0.00120144
Iteration 19/25 | Loss: 0.00120144
Iteration 20/25 | Loss: 0.00120144
Iteration 21/25 | Loss: 0.00120144
Iteration 22/25 | Loss: 0.00120144
Iteration 23/25 | Loss: 0.00120144
Iteration 24/25 | Loss: 0.00120144
Iteration 25/25 | Loss: 0.00120144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46009028
Iteration 2/25 | Loss: 0.00148509
Iteration 3/25 | Loss: 0.00148509
Iteration 4/25 | Loss: 0.00148508
Iteration 5/25 | Loss: 0.00148508
Iteration 6/25 | Loss: 0.00148508
Iteration 7/25 | Loss: 0.00148508
Iteration 8/25 | Loss: 0.00148508
Iteration 9/25 | Loss: 0.00148508
Iteration 10/25 | Loss: 0.00148508
Iteration 11/25 | Loss: 0.00148508
Iteration 12/25 | Loss: 0.00148508
Iteration 13/25 | Loss: 0.00148508
Iteration 14/25 | Loss: 0.00148508
Iteration 15/25 | Loss: 0.00148508
Iteration 16/25 | Loss: 0.00148508
Iteration 17/25 | Loss: 0.00148508
Iteration 18/25 | Loss: 0.00148508
Iteration 19/25 | Loss: 0.00148508
Iteration 20/25 | Loss: 0.00148508
Iteration 21/25 | Loss: 0.00148508
Iteration 22/25 | Loss: 0.00148508
Iteration 23/25 | Loss: 0.00148508
Iteration 24/25 | Loss: 0.00148508
Iteration 25/25 | Loss: 0.00148508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148508
Iteration 2/1000 | Loss: 0.00003412
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002047
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001833
Iteration 7/1000 | Loss: 0.00004265
Iteration 8/1000 | Loss: 0.00003416
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00004463
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00004967
Iteration 15/1000 | Loss: 0.00004157
Iteration 16/1000 | Loss: 0.00009336
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00004350
Iteration 19/1000 | Loss: 0.00001632
Iteration 20/1000 | Loss: 0.00001624
Iteration 21/1000 | Loss: 0.00001624
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00001603
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001592
Iteration 29/1000 | Loss: 0.00001591
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001575
Iteration 36/1000 | Loss: 0.00001575
Iteration 37/1000 | Loss: 0.00001569
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001560
Iteration 63/1000 | Loss: 0.00001560
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001559
Iteration 74/1000 | Loss: 0.00001559
Iteration 75/1000 | Loss: 0.00001559
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001559
Iteration 79/1000 | Loss: 0.00001559
Iteration 80/1000 | Loss: 0.00001559
Iteration 81/1000 | Loss: 0.00001559
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001559
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001558
Iteration 87/1000 | Loss: 0.00001558
Iteration 88/1000 | Loss: 0.00001558
Iteration 89/1000 | Loss: 0.00001558
Iteration 90/1000 | Loss: 0.00001558
Iteration 91/1000 | Loss: 0.00001557
Iteration 92/1000 | Loss: 0.00001557
Iteration 93/1000 | Loss: 0.00001557
Iteration 94/1000 | Loss: 0.00001557
Iteration 95/1000 | Loss: 0.00001557
Iteration 96/1000 | Loss: 0.00001557
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001556
Iteration 99/1000 | Loss: 0.00001556
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001556
Iteration 102/1000 | Loss: 0.00001556
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001555
Iteration 108/1000 | Loss: 0.00001555
Iteration 109/1000 | Loss: 0.00001555
Iteration 110/1000 | Loss: 0.00001555
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001555
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001553
Iteration 122/1000 | Loss: 0.00001553
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.5534620615653694e-05, 1.5534620615653694e-05, 1.5534620615653694e-05, 1.5534620615653694e-05, 1.5534620615653694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5534620615653694e-05

Optimization complete. Final v2v error: 3.304471015930176 mm

Highest mean error: 4.681304454803467 mm for frame 80

Lowest mean error: 2.7146618366241455 mm for frame 45

Saving results

Total time: 61.86934423446655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032009
Iteration 2/25 | Loss: 0.01032009
Iteration 3/25 | Loss: 0.01032008
Iteration 4/25 | Loss: 0.01032008
Iteration 5/25 | Loss: 0.01032008
Iteration 6/25 | Loss: 0.01032008
Iteration 7/25 | Loss: 0.01032008
Iteration 8/25 | Loss: 0.01032008
Iteration 9/25 | Loss: 0.01032008
Iteration 10/25 | Loss: 0.01032008
Iteration 11/25 | Loss: 0.01032008
Iteration 12/25 | Loss: 0.01032007
Iteration 13/25 | Loss: 0.01032007
Iteration 14/25 | Loss: 0.01032007
Iteration 15/25 | Loss: 0.01032007
Iteration 16/25 | Loss: 0.01032007
Iteration 17/25 | Loss: 0.01032007
Iteration 18/25 | Loss: 0.01032007
Iteration 19/25 | Loss: 0.01032007
Iteration 20/25 | Loss: 0.01032007
Iteration 21/25 | Loss: 0.01032007
Iteration 22/25 | Loss: 0.01032007
Iteration 23/25 | Loss: 0.01032007
Iteration 24/25 | Loss: 0.01032006
Iteration 25/25 | Loss: 0.01032006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76270282
Iteration 2/25 | Loss: 0.06416868
Iteration 3/25 | Loss: 0.06415087
Iteration 4/25 | Loss: 0.06415086
Iteration 5/25 | Loss: 0.06415085
Iteration 6/25 | Loss: 0.06415085
Iteration 7/25 | Loss: 0.06415085
Iteration 8/25 | Loss: 0.06415085
Iteration 9/25 | Loss: 0.06415085
Iteration 10/25 | Loss: 0.06415085
Iteration 11/25 | Loss: 0.06415085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0641508474946022, 0.0641508474946022, 0.0641508474946022, 0.0641508474946022, 0.0641508474946022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0641508474946022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06415085
Iteration 2/1000 | Loss: 0.00344145
Iteration 3/1000 | Loss: 0.00064112
Iteration 4/1000 | Loss: 0.00025978
Iteration 5/1000 | Loss: 0.00013972
Iteration 6/1000 | Loss: 0.00009415
Iteration 7/1000 | Loss: 0.00007319
Iteration 8/1000 | Loss: 0.00006111
Iteration 9/1000 | Loss: 0.00005093
Iteration 10/1000 | Loss: 0.00004490
Iteration 11/1000 | Loss: 0.00003918
Iteration 12/1000 | Loss: 0.00003639
Iteration 13/1000 | Loss: 0.00003288
Iteration 14/1000 | Loss: 0.00003016
Iteration 15/1000 | Loss: 0.00002827
Iteration 16/1000 | Loss: 0.00002660
Iteration 17/1000 | Loss: 0.00002499
Iteration 18/1000 | Loss: 0.00002364
Iteration 19/1000 | Loss: 0.00002264
Iteration 20/1000 | Loss: 0.00002177
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001756
Iteration 33/1000 | Loss: 0.00001753
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001736
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001731
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001729
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001726
Iteration 49/1000 | Loss: 0.00001725
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001715
Iteration 70/1000 | Loss: 0.00001715
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001704
Iteration 105/1000 | Loss: 0.00001704
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001703
Iteration 109/1000 | Loss: 0.00001703
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001703
Iteration 112/1000 | Loss: 0.00001703
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00001703
Iteration 115/1000 | Loss: 0.00001703
Iteration 116/1000 | Loss: 0.00001703
Iteration 117/1000 | Loss: 0.00001703
Iteration 118/1000 | Loss: 0.00001703
Iteration 119/1000 | Loss: 0.00001703
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001703
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.703059569990728e-05, 1.703059569990728e-05, 1.703059569990728e-05, 1.703059569990728e-05, 1.703059569990728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.703059569990728e-05

Optimization complete. Final v2v error: 3.3240745067596436 mm

Highest mean error: 5.195486068725586 mm for frame 51

Lowest mean error: 2.683199167251587 mm for frame 185

Saving results

Total time: 68.03354048728943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791257
Iteration 2/25 | Loss: 0.00136553
Iteration 3/25 | Loss: 0.00122265
Iteration 4/25 | Loss: 0.00120884
Iteration 5/25 | Loss: 0.00120355
Iteration 6/25 | Loss: 0.00120190
Iteration 7/25 | Loss: 0.00120190
Iteration 8/25 | Loss: 0.00120190
Iteration 9/25 | Loss: 0.00120190
Iteration 10/25 | Loss: 0.00120190
Iteration 11/25 | Loss: 0.00120190
Iteration 12/25 | Loss: 0.00120190
Iteration 13/25 | Loss: 0.00120190
Iteration 14/25 | Loss: 0.00120190
Iteration 15/25 | Loss: 0.00120190
Iteration 16/25 | Loss: 0.00120190
Iteration 17/25 | Loss: 0.00120190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012019005371257663, 0.0012019005371257663, 0.0012019005371257663, 0.0012019005371257663, 0.0012019005371257663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012019005371257663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44811141
Iteration 2/25 | Loss: 0.00168742
Iteration 3/25 | Loss: 0.00168734
Iteration 4/25 | Loss: 0.00168734
Iteration 5/25 | Loss: 0.00168734
Iteration 6/25 | Loss: 0.00168733
Iteration 7/25 | Loss: 0.00168733
Iteration 8/25 | Loss: 0.00168733
Iteration 9/25 | Loss: 0.00168733
Iteration 10/25 | Loss: 0.00168733
Iteration 11/25 | Loss: 0.00168733
Iteration 12/25 | Loss: 0.00168733
Iteration 13/25 | Loss: 0.00168733
Iteration 14/25 | Loss: 0.00168733
Iteration 15/25 | Loss: 0.00168733
Iteration 16/25 | Loss: 0.00168733
Iteration 17/25 | Loss: 0.00168733
Iteration 18/25 | Loss: 0.00168733
Iteration 19/25 | Loss: 0.00168733
Iteration 20/25 | Loss: 0.00168733
Iteration 21/25 | Loss: 0.00168733
Iteration 22/25 | Loss: 0.00168733
Iteration 23/25 | Loss: 0.00168733
Iteration 24/25 | Loss: 0.00168733
Iteration 25/25 | Loss: 0.00168733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168733
Iteration 2/1000 | Loss: 0.00004627
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00002479
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002096
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001844
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001805
Iteration 13/1000 | Loss: 0.00001794
Iteration 14/1000 | Loss: 0.00001790
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001765
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001759
Iteration 26/1000 | Loss: 0.00001757
Iteration 27/1000 | Loss: 0.00001756
Iteration 28/1000 | Loss: 0.00001756
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001753
Iteration 35/1000 | Loss: 0.00001753
Iteration 36/1000 | Loss: 0.00001752
Iteration 37/1000 | Loss: 0.00001752
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001748
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001742
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001741
Iteration 51/1000 | Loss: 0.00001741
Iteration 52/1000 | Loss: 0.00001741
Iteration 53/1000 | Loss: 0.00001740
Iteration 54/1000 | Loss: 0.00001739
Iteration 55/1000 | Loss: 0.00001739
Iteration 56/1000 | Loss: 0.00001739
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001738
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001733
Iteration 85/1000 | Loss: 0.00001733
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001731
Iteration 93/1000 | Loss: 0.00001731
Iteration 94/1000 | Loss: 0.00001731
Iteration 95/1000 | Loss: 0.00001731
Iteration 96/1000 | Loss: 0.00001731
Iteration 97/1000 | Loss: 0.00001731
Iteration 98/1000 | Loss: 0.00001731
Iteration 99/1000 | Loss: 0.00001731
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001730
Iteration 105/1000 | Loss: 0.00001730
Iteration 106/1000 | Loss: 0.00001730
Iteration 107/1000 | Loss: 0.00001730
Iteration 108/1000 | Loss: 0.00001730
Iteration 109/1000 | Loss: 0.00001729
Iteration 110/1000 | Loss: 0.00001729
Iteration 111/1000 | Loss: 0.00001729
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001729
Iteration 115/1000 | Loss: 0.00001729
Iteration 116/1000 | Loss: 0.00001729
Iteration 117/1000 | Loss: 0.00001729
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Iteration 123/1000 | Loss: 0.00001728
Iteration 124/1000 | Loss: 0.00001728
Iteration 125/1000 | Loss: 0.00001728
Iteration 126/1000 | Loss: 0.00001728
Iteration 127/1000 | Loss: 0.00001728
Iteration 128/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.728385905153118e-05, 1.728385905153118e-05, 1.728385905153118e-05, 1.728385905153118e-05, 1.728385905153118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728385905153118e-05

Optimization complete. Final v2v error: 3.6024956703186035 mm

Highest mean error: 3.877037286758423 mm for frame 125

Lowest mean error: 3.2176592350006104 mm for frame 183

Saving results

Total time: 40.1757116317749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00650028
Iteration 2/25 | Loss: 0.00144534
Iteration 3/25 | Loss: 0.00126796
Iteration 4/25 | Loss: 0.00124854
Iteration 5/25 | Loss: 0.00124652
Iteration 6/25 | Loss: 0.00124638
Iteration 7/25 | Loss: 0.00124638
Iteration 8/25 | Loss: 0.00124638
Iteration 9/25 | Loss: 0.00124638
Iteration 10/25 | Loss: 0.00124638
Iteration 11/25 | Loss: 0.00124638
Iteration 12/25 | Loss: 0.00124638
Iteration 13/25 | Loss: 0.00124638
Iteration 14/25 | Loss: 0.00124638
Iteration 15/25 | Loss: 0.00124638
Iteration 16/25 | Loss: 0.00124638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012463778257369995, 0.0012463778257369995, 0.0012463778257369995, 0.0012463778257369995, 0.0012463778257369995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012463778257369995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23321104
Iteration 2/25 | Loss: 0.00156315
Iteration 3/25 | Loss: 0.00156315
Iteration 4/25 | Loss: 0.00156315
Iteration 5/25 | Loss: 0.00156315
Iteration 6/25 | Loss: 0.00156315
Iteration 7/25 | Loss: 0.00156315
Iteration 8/25 | Loss: 0.00156315
Iteration 9/25 | Loss: 0.00156315
Iteration 10/25 | Loss: 0.00156315
Iteration 11/25 | Loss: 0.00156315
Iteration 12/25 | Loss: 0.00156315
Iteration 13/25 | Loss: 0.00156315
Iteration 14/25 | Loss: 0.00156315
Iteration 15/25 | Loss: 0.00156315
Iteration 16/25 | Loss: 0.00156315
Iteration 17/25 | Loss: 0.00156315
Iteration 18/25 | Loss: 0.00156315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015631491551175714, 0.0015631491551175714, 0.0015631491551175714, 0.0015631491551175714, 0.0015631491551175714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015631491551175714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156315
Iteration 2/1000 | Loss: 0.00003895
Iteration 3/1000 | Loss: 0.00002666
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001495
Iteration 11/1000 | Loss: 0.00001491
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001461
Iteration 14/1000 | Loss: 0.00001458
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001435
Iteration 17/1000 | Loss: 0.00001430
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001407
Iteration 21/1000 | Loss: 0.00001405
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001394
Iteration 24/1000 | Loss: 0.00001393
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001389
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001387
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001373
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001372
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001369
Iteration 59/1000 | Loss: 0.00001369
Iteration 60/1000 | Loss: 0.00001369
Iteration 61/1000 | Loss: 0.00001368
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001364
Iteration 86/1000 | Loss: 0.00001364
Iteration 87/1000 | Loss: 0.00001364
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001363
Iteration 94/1000 | Loss: 0.00001363
Iteration 95/1000 | Loss: 0.00001363
Iteration 96/1000 | Loss: 0.00001363
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001360
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001359
Iteration 134/1000 | Loss: 0.00001359
Iteration 135/1000 | Loss: 0.00001359
Iteration 136/1000 | Loss: 0.00001359
Iteration 137/1000 | Loss: 0.00001359
Iteration 138/1000 | Loss: 0.00001359
Iteration 139/1000 | Loss: 0.00001359
Iteration 140/1000 | Loss: 0.00001358
Iteration 141/1000 | Loss: 0.00001358
Iteration 142/1000 | Loss: 0.00001358
Iteration 143/1000 | Loss: 0.00001358
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001356
Iteration 150/1000 | Loss: 0.00001356
Iteration 151/1000 | Loss: 0.00001356
Iteration 152/1000 | Loss: 0.00001356
Iteration 153/1000 | Loss: 0.00001356
Iteration 154/1000 | Loss: 0.00001356
Iteration 155/1000 | Loss: 0.00001356
Iteration 156/1000 | Loss: 0.00001356
Iteration 157/1000 | Loss: 0.00001355
Iteration 158/1000 | Loss: 0.00001355
Iteration 159/1000 | Loss: 0.00001355
Iteration 160/1000 | Loss: 0.00001355
Iteration 161/1000 | Loss: 0.00001355
Iteration 162/1000 | Loss: 0.00001355
Iteration 163/1000 | Loss: 0.00001355
Iteration 164/1000 | Loss: 0.00001355
Iteration 165/1000 | Loss: 0.00001355
Iteration 166/1000 | Loss: 0.00001355
Iteration 167/1000 | Loss: 0.00001355
Iteration 168/1000 | Loss: 0.00001355
Iteration 169/1000 | Loss: 0.00001355
Iteration 170/1000 | Loss: 0.00001355
Iteration 171/1000 | Loss: 0.00001355
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001354
Iteration 184/1000 | Loss: 0.00001354
Iteration 185/1000 | Loss: 0.00001354
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001354
Iteration 190/1000 | Loss: 0.00001354
Iteration 191/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.3535360267269425e-05, 1.3535360267269425e-05, 1.3535360267269425e-05, 1.3535360267269425e-05, 1.3535360267269425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3535360267269425e-05

Optimization complete. Final v2v error: 3.107213020324707 mm

Highest mean error: 3.470203399658203 mm for frame 108

Lowest mean error: 2.5989527702331543 mm for frame 92

Saving results

Total time: 42.86392307281494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415240
Iteration 2/25 | Loss: 0.00121121
Iteration 3/25 | Loss: 0.00115770
Iteration 4/25 | Loss: 0.00115097
Iteration 5/25 | Loss: 0.00114887
Iteration 6/25 | Loss: 0.00114846
Iteration 7/25 | Loss: 0.00114846
Iteration 8/25 | Loss: 0.00114846
Iteration 9/25 | Loss: 0.00114846
Iteration 10/25 | Loss: 0.00114846
Iteration 11/25 | Loss: 0.00114846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011484627611935139, 0.0011484627611935139, 0.0011484627611935139, 0.0011484627611935139, 0.0011484627611935139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011484627611935139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70411158
Iteration 2/25 | Loss: 0.00137550
Iteration 3/25 | Loss: 0.00137550
Iteration 4/25 | Loss: 0.00137550
Iteration 5/25 | Loss: 0.00137550
Iteration 6/25 | Loss: 0.00137550
Iteration 7/25 | Loss: 0.00137549
Iteration 8/25 | Loss: 0.00137549
Iteration 9/25 | Loss: 0.00137549
Iteration 10/25 | Loss: 0.00137549
Iteration 11/25 | Loss: 0.00137549
Iteration 12/25 | Loss: 0.00137549
Iteration 13/25 | Loss: 0.00137549
Iteration 14/25 | Loss: 0.00137549
Iteration 15/25 | Loss: 0.00137549
Iteration 16/25 | Loss: 0.00137549
Iteration 17/25 | Loss: 0.00137549
Iteration 18/25 | Loss: 0.00137549
Iteration 19/25 | Loss: 0.00137549
Iteration 20/25 | Loss: 0.00137549
Iteration 21/25 | Loss: 0.00137549
Iteration 22/25 | Loss: 0.00137549
Iteration 23/25 | Loss: 0.00137549
Iteration 24/25 | Loss: 0.00137549
Iteration 25/25 | Loss: 0.00137549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137549
Iteration 2/1000 | Loss: 0.00001956
Iteration 3/1000 | Loss: 0.00001442
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001212
Iteration 6/1000 | Loss: 0.00001145
Iteration 7/1000 | Loss: 0.00001102
Iteration 8/1000 | Loss: 0.00001077
Iteration 9/1000 | Loss: 0.00001047
Iteration 10/1000 | Loss: 0.00001036
Iteration 11/1000 | Loss: 0.00001034
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001023
Iteration 14/1000 | Loss: 0.00001011
Iteration 15/1000 | Loss: 0.00001008
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001006
Iteration 18/1000 | Loss: 0.00001004
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000996
Iteration 22/1000 | Loss: 0.00000996
Iteration 23/1000 | Loss: 0.00000995
Iteration 24/1000 | Loss: 0.00000994
Iteration 25/1000 | Loss: 0.00000994
Iteration 26/1000 | Loss: 0.00000994
Iteration 27/1000 | Loss: 0.00000993
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000989
Iteration 30/1000 | Loss: 0.00000989
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000988
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000980
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000973
Iteration 42/1000 | Loss: 0.00000971
Iteration 43/1000 | Loss: 0.00000970
Iteration 44/1000 | Loss: 0.00000969
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000964
Iteration 47/1000 | Loss: 0.00000963
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000961
Iteration 53/1000 | Loss: 0.00000961
Iteration 54/1000 | Loss: 0.00000961
Iteration 55/1000 | Loss: 0.00000961
Iteration 56/1000 | Loss: 0.00000961
Iteration 57/1000 | Loss: 0.00000960
Iteration 58/1000 | Loss: 0.00000960
Iteration 59/1000 | Loss: 0.00000960
Iteration 60/1000 | Loss: 0.00000960
Iteration 61/1000 | Loss: 0.00000960
Iteration 62/1000 | Loss: 0.00000960
Iteration 63/1000 | Loss: 0.00000960
Iteration 64/1000 | Loss: 0.00000960
Iteration 65/1000 | Loss: 0.00000960
Iteration 66/1000 | Loss: 0.00000960
Iteration 67/1000 | Loss: 0.00000960
Iteration 68/1000 | Loss: 0.00000960
Iteration 69/1000 | Loss: 0.00000960
Iteration 70/1000 | Loss: 0.00000960
Iteration 71/1000 | Loss: 0.00000960
Iteration 72/1000 | Loss: 0.00000960
Iteration 73/1000 | Loss: 0.00000960
Iteration 74/1000 | Loss: 0.00000960
Iteration 75/1000 | Loss: 0.00000960
Iteration 76/1000 | Loss: 0.00000960
Iteration 77/1000 | Loss: 0.00000960
Iteration 78/1000 | Loss: 0.00000960
Iteration 79/1000 | Loss: 0.00000960
Iteration 80/1000 | Loss: 0.00000960
Iteration 81/1000 | Loss: 0.00000960
Iteration 82/1000 | Loss: 0.00000960
Iteration 83/1000 | Loss: 0.00000960
Iteration 84/1000 | Loss: 0.00000960
Iteration 85/1000 | Loss: 0.00000960
Iteration 86/1000 | Loss: 0.00000960
Iteration 87/1000 | Loss: 0.00000960
Iteration 88/1000 | Loss: 0.00000960
Iteration 89/1000 | Loss: 0.00000960
Iteration 90/1000 | Loss: 0.00000960
Iteration 91/1000 | Loss: 0.00000960
Iteration 92/1000 | Loss: 0.00000960
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000960
Iteration 95/1000 | Loss: 0.00000960
Iteration 96/1000 | Loss: 0.00000960
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [9.595754818292335e-06, 9.595754818292335e-06, 9.595754818292335e-06, 9.595754818292335e-06, 9.595754818292335e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.595754818292335e-06

Optimization complete. Final v2v error: 2.716799736022949 mm

Highest mean error: 2.9226925373077393 mm for frame 96

Lowest mean error: 2.5459063053131104 mm for frame 162

Saving results

Total time: 32.75217938423157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055818
Iteration 2/25 | Loss: 0.01055818
Iteration 3/25 | Loss: 0.00319144
Iteration 4/25 | Loss: 0.00232440
Iteration 5/25 | Loss: 0.00178008
Iteration 6/25 | Loss: 0.00159141
Iteration 7/25 | Loss: 0.00146357
Iteration 8/25 | Loss: 0.00141915
Iteration 9/25 | Loss: 0.00132044
Iteration 10/25 | Loss: 0.00123477
Iteration 11/25 | Loss: 0.00121750
Iteration 12/25 | Loss: 0.00120279
Iteration 13/25 | Loss: 0.00118878
Iteration 14/25 | Loss: 0.00118668
Iteration 15/25 | Loss: 0.00118632
Iteration 16/25 | Loss: 0.00118621
Iteration 17/25 | Loss: 0.00118619
Iteration 18/25 | Loss: 0.00118618
Iteration 19/25 | Loss: 0.00118618
Iteration 20/25 | Loss: 0.00118617
Iteration 21/25 | Loss: 0.00118617
Iteration 22/25 | Loss: 0.00118617
Iteration 23/25 | Loss: 0.00118614
Iteration 24/25 | Loss: 0.00118614
Iteration 25/25 | Loss: 0.00118614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26395321
Iteration 2/25 | Loss: 0.00146727
Iteration 3/25 | Loss: 0.00146727
Iteration 4/25 | Loss: 0.00146726
Iteration 5/25 | Loss: 0.00146726
Iteration 6/25 | Loss: 0.00146726
Iteration 7/25 | Loss: 0.00146726
Iteration 8/25 | Loss: 0.00146726
Iteration 9/25 | Loss: 0.00146726
Iteration 10/25 | Loss: 0.00146726
Iteration 11/25 | Loss: 0.00146726
Iteration 12/25 | Loss: 0.00146726
Iteration 13/25 | Loss: 0.00146726
Iteration 14/25 | Loss: 0.00146726
Iteration 15/25 | Loss: 0.00146726
Iteration 16/25 | Loss: 0.00146726
Iteration 17/25 | Loss: 0.00146726
Iteration 18/25 | Loss: 0.00146726
Iteration 19/25 | Loss: 0.00146726
Iteration 20/25 | Loss: 0.00146726
Iteration 21/25 | Loss: 0.00146726
Iteration 22/25 | Loss: 0.00146726
Iteration 23/25 | Loss: 0.00146726
Iteration 24/25 | Loss: 0.00146726
Iteration 25/25 | Loss: 0.00146726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146726
Iteration 2/1000 | Loss: 0.00003760
Iteration 3/1000 | Loss: 0.00022604
Iteration 4/1000 | Loss: 0.00005194
Iteration 5/1000 | Loss: 0.00002825
Iteration 6/1000 | Loss: 0.00002186
Iteration 7/1000 | Loss: 0.00002051
Iteration 8/1000 | Loss: 0.00001968
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001827
Iteration 11/1000 | Loss: 0.00022846
Iteration 12/1000 | Loss: 0.00001835
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00049532
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00032257
Iteration 18/1000 | Loss: 0.00003343
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00006071
Iteration 21/1000 | Loss: 0.00005323
Iteration 22/1000 | Loss: 0.00002634
Iteration 23/1000 | Loss: 0.00002730
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001431
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001419
Iteration 38/1000 | Loss: 0.00001417
Iteration 39/1000 | Loss: 0.00001417
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001401
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001387
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001380
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001380
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001379
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001378
Iteration 107/1000 | Loss: 0.00001378
Iteration 108/1000 | Loss: 0.00001378
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001377
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001377
Iteration 120/1000 | Loss: 0.00001377
Iteration 121/1000 | Loss: 0.00001376
Iteration 122/1000 | Loss: 0.00001376
Iteration 123/1000 | Loss: 0.00001376
Iteration 124/1000 | Loss: 0.00001376
Iteration 125/1000 | Loss: 0.00001376
Iteration 126/1000 | Loss: 0.00001376
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001375
Iteration 130/1000 | Loss: 0.00001375
Iteration 131/1000 | Loss: 0.00001375
Iteration 132/1000 | Loss: 0.00001375
Iteration 133/1000 | Loss: 0.00001375
Iteration 134/1000 | Loss: 0.00001375
Iteration 135/1000 | Loss: 0.00001375
Iteration 136/1000 | Loss: 0.00001375
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Iteration 141/1000 | Loss: 0.00001374
Iteration 142/1000 | Loss: 0.00001374
Iteration 143/1000 | Loss: 0.00001374
Iteration 144/1000 | Loss: 0.00001374
Iteration 145/1000 | Loss: 0.00001374
Iteration 146/1000 | Loss: 0.00001374
Iteration 147/1000 | Loss: 0.00001374
Iteration 148/1000 | Loss: 0.00001374
Iteration 149/1000 | Loss: 0.00001374
Iteration 150/1000 | Loss: 0.00001374
Iteration 151/1000 | Loss: 0.00001374
Iteration 152/1000 | Loss: 0.00001374
Iteration 153/1000 | Loss: 0.00001374
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001373
Iteration 156/1000 | Loss: 0.00001373
Iteration 157/1000 | Loss: 0.00001373
Iteration 158/1000 | Loss: 0.00001373
Iteration 159/1000 | Loss: 0.00001373
Iteration 160/1000 | Loss: 0.00001373
Iteration 161/1000 | Loss: 0.00001373
Iteration 162/1000 | Loss: 0.00001373
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001372
Iteration 167/1000 | Loss: 0.00001372
Iteration 168/1000 | Loss: 0.00001372
Iteration 169/1000 | Loss: 0.00001372
Iteration 170/1000 | Loss: 0.00001372
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001372
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001371
Iteration 178/1000 | Loss: 0.00001371
Iteration 179/1000 | Loss: 0.00001371
Iteration 180/1000 | Loss: 0.00001371
Iteration 181/1000 | Loss: 0.00001371
Iteration 182/1000 | Loss: 0.00001371
Iteration 183/1000 | Loss: 0.00001371
Iteration 184/1000 | Loss: 0.00001371
Iteration 185/1000 | Loss: 0.00001371
Iteration 186/1000 | Loss: 0.00001371
Iteration 187/1000 | Loss: 0.00001371
Iteration 188/1000 | Loss: 0.00001371
Iteration 189/1000 | Loss: 0.00001371
Iteration 190/1000 | Loss: 0.00001371
Iteration 191/1000 | Loss: 0.00001371
Iteration 192/1000 | Loss: 0.00001371
Iteration 193/1000 | Loss: 0.00001371
Iteration 194/1000 | Loss: 0.00001371
Iteration 195/1000 | Loss: 0.00001371
Iteration 196/1000 | Loss: 0.00001371
Iteration 197/1000 | Loss: 0.00001370
Iteration 198/1000 | Loss: 0.00001370
Iteration 199/1000 | Loss: 0.00001370
Iteration 200/1000 | Loss: 0.00001370
Iteration 201/1000 | Loss: 0.00001370
Iteration 202/1000 | Loss: 0.00001370
Iteration 203/1000 | Loss: 0.00001370
Iteration 204/1000 | Loss: 0.00001370
Iteration 205/1000 | Loss: 0.00001370
Iteration 206/1000 | Loss: 0.00001370
Iteration 207/1000 | Loss: 0.00001370
Iteration 208/1000 | Loss: 0.00001370
Iteration 209/1000 | Loss: 0.00001370
Iteration 210/1000 | Loss: 0.00001370
Iteration 211/1000 | Loss: 0.00001370
Iteration 212/1000 | Loss: 0.00001370
Iteration 213/1000 | Loss: 0.00001370
Iteration 214/1000 | Loss: 0.00001370
Iteration 215/1000 | Loss: 0.00001370
Iteration 216/1000 | Loss: 0.00001370
Iteration 217/1000 | Loss: 0.00001369
Iteration 218/1000 | Loss: 0.00001369
Iteration 219/1000 | Loss: 0.00001369
Iteration 220/1000 | Loss: 0.00001369
Iteration 221/1000 | Loss: 0.00001369
Iteration 222/1000 | Loss: 0.00001369
Iteration 223/1000 | Loss: 0.00001369
Iteration 224/1000 | Loss: 0.00001369
Iteration 225/1000 | Loss: 0.00001369
Iteration 226/1000 | Loss: 0.00001369
Iteration 227/1000 | Loss: 0.00001369
Iteration 228/1000 | Loss: 0.00001369
Iteration 229/1000 | Loss: 0.00001369
Iteration 230/1000 | Loss: 0.00001369
Iteration 231/1000 | Loss: 0.00001369
Iteration 232/1000 | Loss: 0.00001369
Iteration 233/1000 | Loss: 0.00001369
Iteration 234/1000 | Loss: 0.00001369
Iteration 235/1000 | Loss: 0.00001369
Iteration 236/1000 | Loss: 0.00001369
Iteration 237/1000 | Loss: 0.00001369
Iteration 238/1000 | Loss: 0.00001369
Iteration 239/1000 | Loss: 0.00001369
Iteration 240/1000 | Loss: 0.00001369
Iteration 241/1000 | Loss: 0.00001369
Iteration 242/1000 | Loss: 0.00001369
Iteration 243/1000 | Loss: 0.00001369
Iteration 244/1000 | Loss: 0.00001369
Iteration 245/1000 | Loss: 0.00001369
Iteration 246/1000 | Loss: 0.00001369
Iteration 247/1000 | Loss: 0.00001369
Iteration 248/1000 | Loss: 0.00001369
Iteration 249/1000 | Loss: 0.00001369
Iteration 250/1000 | Loss: 0.00001369
Iteration 251/1000 | Loss: 0.00001369
Iteration 252/1000 | Loss: 0.00001369
Iteration 253/1000 | Loss: 0.00001369
Iteration 254/1000 | Loss: 0.00001369
Iteration 255/1000 | Loss: 0.00001369
Iteration 256/1000 | Loss: 0.00001369
Iteration 257/1000 | Loss: 0.00001369
Iteration 258/1000 | Loss: 0.00001369
Iteration 259/1000 | Loss: 0.00001369
Iteration 260/1000 | Loss: 0.00001369
Iteration 261/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.3690543710254133e-05, 1.3690543710254133e-05, 1.3690543710254133e-05, 1.3690543710254133e-05, 1.3690543710254133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3690543710254133e-05

Optimization complete. Final v2v error: 3.1398355960845947 mm

Highest mean error: 3.68472957611084 mm for frame 76

Lowest mean error: 2.921436309814453 mm for frame 155

Saving results

Total time: 83.38346886634827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893608
Iteration 2/25 | Loss: 0.00126317
Iteration 3/25 | Loss: 0.00119684
Iteration 4/25 | Loss: 0.00118483
Iteration 5/25 | Loss: 0.00118263
Iteration 6/25 | Loss: 0.00118263
Iteration 7/25 | Loss: 0.00118263
Iteration 8/25 | Loss: 0.00118263
Iteration 9/25 | Loss: 0.00118263
Iteration 10/25 | Loss: 0.00118263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001182634150609374, 0.001182634150609374, 0.001182634150609374, 0.001182634150609374, 0.001182634150609374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001182634150609374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37684619
Iteration 2/25 | Loss: 0.00123920
Iteration 3/25 | Loss: 0.00123920
Iteration 4/25 | Loss: 0.00123920
Iteration 5/25 | Loss: 0.00123920
Iteration 6/25 | Loss: 0.00123920
Iteration 7/25 | Loss: 0.00123920
Iteration 8/25 | Loss: 0.00123920
Iteration 9/25 | Loss: 0.00123920
Iteration 10/25 | Loss: 0.00123920
Iteration 11/25 | Loss: 0.00123920
Iteration 12/25 | Loss: 0.00123920
Iteration 13/25 | Loss: 0.00123920
Iteration 14/25 | Loss: 0.00123920
Iteration 15/25 | Loss: 0.00123920
Iteration 16/25 | Loss: 0.00123920
Iteration 17/25 | Loss: 0.00123920
Iteration 18/25 | Loss: 0.00123920
Iteration 19/25 | Loss: 0.00123920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012391973286867142, 0.0012391973286867142, 0.0012391973286867142, 0.0012391973286867142, 0.0012391973286867142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012391973286867142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123920
Iteration 2/1000 | Loss: 0.00001922
Iteration 3/1000 | Loss: 0.00001601
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001407
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001187
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001182
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001180
Iteration 24/1000 | Loss: 0.00001180
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001153
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001152
Iteration 41/1000 | Loss: 0.00001152
Iteration 42/1000 | Loss: 0.00001152
Iteration 43/1000 | Loss: 0.00001152
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001150
Iteration 48/1000 | Loss: 0.00001150
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001143
Iteration 57/1000 | Loss: 0.00001143
Iteration 58/1000 | Loss: 0.00001143
Iteration 59/1000 | Loss: 0.00001143
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001139
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001138
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001129
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001129
Iteration 159/1000 | Loss: 0.00001129
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.1286102562735323e-05, 1.1286102562735323e-05, 1.1286102562735323e-05, 1.1286102562735323e-05, 1.1286102562735323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1286102562735323e-05

Optimization complete. Final v2v error: 2.8891172409057617 mm

Highest mean error: 3.3445420265197754 mm for frame 146

Lowest mean error: 2.808206796646118 mm for frame 212

Saving results

Total time: 44.50026726722717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370584
Iteration 2/25 | Loss: 0.00121991
Iteration 3/25 | Loss: 0.00116113
Iteration 4/25 | Loss: 0.00115380
Iteration 5/25 | Loss: 0.00115156
Iteration 6/25 | Loss: 0.00115098
Iteration 7/25 | Loss: 0.00115098
Iteration 8/25 | Loss: 0.00115098
Iteration 9/25 | Loss: 0.00115098
Iteration 10/25 | Loss: 0.00115098
Iteration 11/25 | Loss: 0.00115098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011509759351611137, 0.0011509759351611137, 0.0011509759351611137, 0.0011509759351611137, 0.0011509759351611137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011509759351611137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69874942
Iteration 2/25 | Loss: 0.00143586
Iteration 3/25 | Loss: 0.00143586
Iteration 4/25 | Loss: 0.00143586
Iteration 5/25 | Loss: 0.00143585
Iteration 6/25 | Loss: 0.00143585
Iteration 7/25 | Loss: 0.00143585
Iteration 8/25 | Loss: 0.00143585
Iteration 9/25 | Loss: 0.00143585
Iteration 10/25 | Loss: 0.00143585
Iteration 11/25 | Loss: 0.00143585
Iteration 12/25 | Loss: 0.00143585
Iteration 13/25 | Loss: 0.00143585
Iteration 14/25 | Loss: 0.00143585
Iteration 15/25 | Loss: 0.00143585
Iteration 16/25 | Loss: 0.00143585
Iteration 17/25 | Loss: 0.00143585
Iteration 18/25 | Loss: 0.00143585
Iteration 19/25 | Loss: 0.00143585
Iteration 20/25 | Loss: 0.00143585
Iteration 21/25 | Loss: 0.00143585
Iteration 22/25 | Loss: 0.00143585
Iteration 23/25 | Loss: 0.00143585
Iteration 24/25 | Loss: 0.00143585
Iteration 25/25 | Loss: 0.00143585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143585
Iteration 2/1000 | Loss: 0.00002493
Iteration 3/1000 | Loss: 0.00001552
Iteration 4/1000 | Loss: 0.00001239
Iteration 5/1000 | Loss: 0.00001132
Iteration 6/1000 | Loss: 0.00001037
Iteration 7/1000 | Loss: 0.00000989
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000925
Iteration 10/1000 | Loss: 0.00000903
Iteration 11/1000 | Loss: 0.00000895
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000873
Iteration 14/1000 | Loss: 0.00000871
Iteration 15/1000 | Loss: 0.00000871
Iteration 16/1000 | Loss: 0.00000870
Iteration 17/1000 | Loss: 0.00000867
Iteration 18/1000 | Loss: 0.00000856
Iteration 19/1000 | Loss: 0.00000854
Iteration 20/1000 | Loss: 0.00000854
Iteration 21/1000 | Loss: 0.00000853
Iteration 22/1000 | Loss: 0.00000853
Iteration 23/1000 | Loss: 0.00000852
Iteration 24/1000 | Loss: 0.00000851
Iteration 25/1000 | Loss: 0.00000849
Iteration 26/1000 | Loss: 0.00000848
Iteration 27/1000 | Loss: 0.00000848
Iteration 28/1000 | Loss: 0.00000847
Iteration 29/1000 | Loss: 0.00000846
Iteration 30/1000 | Loss: 0.00000845
Iteration 31/1000 | Loss: 0.00000844
Iteration 32/1000 | Loss: 0.00000844
Iteration 33/1000 | Loss: 0.00000843
Iteration 34/1000 | Loss: 0.00000842
Iteration 35/1000 | Loss: 0.00000842
Iteration 36/1000 | Loss: 0.00000842
Iteration 37/1000 | Loss: 0.00000842
Iteration 38/1000 | Loss: 0.00000842
Iteration 39/1000 | Loss: 0.00000841
Iteration 40/1000 | Loss: 0.00000841
Iteration 41/1000 | Loss: 0.00000840
Iteration 42/1000 | Loss: 0.00000839
Iteration 43/1000 | Loss: 0.00000839
Iteration 44/1000 | Loss: 0.00000839
Iteration 45/1000 | Loss: 0.00000838
Iteration 46/1000 | Loss: 0.00000838
Iteration 47/1000 | Loss: 0.00000837
Iteration 48/1000 | Loss: 0.00000836
Iteration 49/1000 | Loss: 0.00000836
Iteration 50/1000 | Loss: 0.00000834
Iteration 51/1000 | Loss: 0.00000833
Iteration 52/1000 | Loss: 0.00000832
Iteration 53/1000 | Loss: 0.00000832
Iteration 54/1000 | Loss: 0.00000832
Iteration 55/1000 | Loss: 0.00000831
Iteration 56/1000 | Loss: 0.00000831
Iteration 57/1000 | Loss: 0.00000830
Iteration 58/1000 | Loss: 0.00000830
Iteration 59/1000 | Loss: 0.00000829
Iteration 60/1000 | Loss: 0.00000829
Iteration 61/1000 | Loss: 0.00000829
Iteration 62/1000 | Loss: 0.00000828
Iteration 63/1000 | Loss: 0.00000828
Iteration 64/1000 | Loss: 0.00000828
Iteration 65/1000 | Loss: 0.00000828
Iteration 66/1000 | Loss: 0.00000827
Iteration 67/1000 | Loss: 0.00000827
Iteration 68/1000 | Loss: 0.00000826
Iteration 69/1000 | Loss: 0.00000826
Iteration 70/1000 | Loss: 0.00000825
Iteration 71/1000 | Loss: 0.00000825
Iteration 72/1000 | Loss: 0.00000825
Iteration 73/1000 | Loss: 0.00000824
Iteration 74/1000 | Loss: 0.00000823
Iteration 75/1000 | Loss: 0.00000822
Iteration 76/1000 | Loss: 0.00000822
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000821
Iteration 79/1000 | Loss: 0.00000819
Iteration 80/1000 | Loss: 0.00000819
Iteration 81/1000 | Loss: 0.00000819
Iteration 82/1000 | Loss: 0.00000819
Iteration 83/1000 | Loss: 0.00000819
Iteration 84/1000 | Loss: 0.00000819
Iteration 85/1000 | Loss: 0.00000819
Iteration 86/1000 | Loss: 0.00000818
Iteration 87/1000 | Loss: 0.00000818
Iteration 88/1000 | Loss: 0.00000818
Iteration 89/1000 | Loss: 0.00000818
Iteration 90/1000 | Loss: 0.00000817
Iteration 91/1000 | Loss: 0.00000817
Iteration 92/1000 | Loss: 0.00000817
Iteration 93/1000 | Loss: 0.00000816
Iteration 94/1000 | Loss: 0.00000816
Iteration 95/1000 | Loss: 0.00000816
Iteration 96/1000 | Loss: 0.00000816
Iteration 97/1000 | Loss: 0.00000816
Iteration 98/1000 | Loss: 0.00000816
Iteration 99/1000 | Loss: 0.00000815
Iteration 100/1000 | Loss: 0.00000815
Iteration 101/1000 | Loss: 0.00000815
Iteration 102/1000 | Loss: 0.00000814
Iteration 103/1000 | Loss: 0.00000814
Iteration 104/1000 | Loss: 0.00000814
Iteration 105/1000 | Loss: 0.00000814
Iteration 106/1000 | Loss: 0.00000813
Iteration 107/1000 | Loss: 0.00000813
Iteration 108/1000 | Loss: 0.00000813
Iteration 109/1000 | Loss: 0.00000813
Iteration 110/1000 | Loss: 0.00000813
Iteration 111/1000 | Loss: 0.00000812
Iteration 112/1000 | Loss: 0.00000812
Iteration 113/1000 | Loss: 0.00000812
Iteration 114/1000 | Loss: 0.00000811
Iteration 115/1000 | Loss: 0.00000811
Iteration 116/1000 | Loss: 0.00000811
Iteration 117/1000 | Loss: 0.00000810
Iteration 118/1000 | Loss: 0.00000810
Iteration 119/1000 | Loss: 0.00000810
Iteration 120/1000 | Loss: 0.00000810
Iteration 121/1000 | Loss: 0.00000809
Iteration 122/1000 | Loss: 0.00000809
Iteration 123/1000 | Loss: 0.00000809
Iteration 124/1000 | Loss: 0.00000809
Iteration 125/1000 | Loss: 0.00000809
Iteration 126/1000 | Loss: 0.00000808
Iteration 127/1000 | Loss: 0.00000808
Iteration 128/1000 | Loss: 0.00000808
Iteration 129/1000 | Loss: 0.00000808
Iteration 130/1000 | Loss: 0.00000808
Iteration 131/1000 | Loss: 0.00000808
Iteration 132/1000 | Loss: 0.00000808
Iteration 133/1000 | Loss: 0.00000808
Iteration 134/1000 | Loss: 0.00000808
Iteration 135/1000 | Loss: 0.00000808
Iteration 136/1000 | Loss: 0.00000808
Iteration 137/1000 | Loss: 0.00000807
Iteration 138/1000 | Loss: 0.00000807
Iteration 139/1000 | Loss: 0.00000807
Iteration 140/1000 | Loss: 0.00000807
Iteration 141/1000 | Loss: 0.00000807
Iteration 142/1000 | Loss: 0.00000807
Iteration 143/1000 | Loss: 0.00000807
Iteration 144/1000 | Loss: 0.00000807
Iteration 145/1000 | Loss: 0.00000807
Iteration 146/1000 | Loss: 0.00000806
Iteration 147/1000 | Loss: 0.00000806
Iteration 148/1000 | Loss: 0.00000806
Iteration 149/1000 | Loss: 0.00000806
Iteration 150/1000 | Loss: 0.00000806
Iteration 151/1000 | Loss: 0.00000805
Iteration 152/1000 | Loss: 0.00000805
Iteration 153/1000 | Loss: 0.00000805
Iteration 154/1000 | Loss: 0.00000805
Iteration 155/1000 | Loss: 0.00000805
Iteration 156/1000 | Loss: 0.00000804
Iteration 157/1000 | Loss: 0.00000804
Iteration 158/1000 | Loss: 0.00000804
Iteration 159/1000 | Loss: 0.00000804
Iteration 160/1000 | Loss: 0.00000804
Iteration 161/1000 | Loss: 0.00000804
Iteration 162/1000 | Loss: 0.00000804
Iteration 163/1000 | Loss: 0.00000804
Iteration 164/1000 | Loss: 0.00000804
Iteration 165/1000 | Loss: 0.00000803
Iteration 166/1000 | Loss: 0.00000803
Iteration 167/1000 | Loss: 0.00000803
Iteration 168/1000 | Loss: 0.00000803
Iteration 169/1000 | Loss: 0.00000803
Iteration 170/1000 | Loss: 0.00000803
Iteration 171/1000 | Loss: 0.00000803
Iteration 172/1000 | Loss: 0.00000803
Iteration 173/1000 | Loss: 0.00000802
Iteration 174/1000 | Loss: 0.00000802
Iteration 175/1000 | Loss: 0.00000802
Iteration 176/1000 | Loss: 0.00000802
Iteration 177/1000 | Loss: 0.00000802
Iteration 178/1000 | Loss: 0.00000802
Iteration 179/1000 | Loss: 0.00000802
Iteration 180/1000 | Loss: 0.00000802
Iteration 181/1000 | Loss: 0.00000802
Iteration 182/1000 | Loss: 0.00000802
Iteration 183/1000 | Loss: 0.00000802
Iteration 184/1000 | Loss: 0.00000802
Iteration 185/1000 | Loss: 0.00000802
Iteration 186/1000 | Loss: 0.00000802
Iteration 187/1000 | Loss: 0.00000802
Iteration 188/1000 | Loss: 0.00000802
Iteration 189/1000 | Loss: 0.00000802
Iteration 190/1000 | Loss: 0.00000802
Iteration 191/1000 | Loss: 0.00000802
Iteration 192/1000 | Loss: 0.00000802
Iteration 193/1000 | Loss: 0.00000802
Iteration 194/1000 | Loss: 0.00000802
Iteration 195/1000 | Loss: 0.00000802
Iteration 196/1000 | Loss: 0.00000802
Iteration 197/1000 | Loss: 0.00000802
Iteration 198/1000 | Loss: 0.00000802
Iteration 199/1000 | Loss: 0.00000802
Iteration 200/1000 | Loss: 0.00000802
Iteration 201/1000 | Loss: 0.00000802
Iteration 202/1000 | Loss: 0.00000802
Iteration 203/1000 | Loss: 0.00000802
Iteration 204/1000 | Loss: 0.00000802
Iteration 205/1000 | Loss: 0.00000802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [8.020895620575175e-06, 8.020895620575175e-06, 8.020895620575175e-06, 8.020895620575175e-06, 8.020895620575175e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.020895620575175e-06

Optimization complete. Final v2v error: 2.445388078689575 mm

Highest mean error: 2.8819327354431152 mm for frame 76

Lowest mean error: 2.3384902477264404 mm for frame 112

Saving results

Total time: 42.198123931884766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965036
Iteration 2/25 | Loss: 0.00965036
Iteration 3/25 | Loss: 0.00262000
Iteration 4/25 | Loss: 0.00207769
Iteration 5/25 | Loss: 0.00213587
Iteration 6/25 | Loss: 0.00210844
Iteration 7/25 | Loss: 0.00177522
Iteration 8/25 | Loss: 0.00157964
Iteration 9/25 | Loss: 0.00148368
Iteration 10/25 | Loss: 0.00145769
Iteration 11/25 | Loss: 0.00144465
Iteration 12/25 | Loss: 0.00140484
Iteration 13/25 | Loss: 0.00135432
Iteration 14/25 | Loss: 0.00133783
Iteration 15/25 | Loss: 0.00132387
Iteration 16/25 | Loss: 0.00131383
Iteration 17/25 | Loss: 0.00130641
Iteration 18/25 | Loss: 0.00130322
Iteration 19/25 | Loss: 0.00130789
Iteration 20/25 | Loss: 0.00130746
Iteration 21/25 | Loss: 0.00130943
Iteration 22/25 | Loss: 0.00130217
Iteration 23/25 | Loss: 0.00129262
Iteration 24/25 | Loss: 0.00128916
Iteration 25/25 | Loss: 0.00128829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26220489
Iteration 2/25 | Loss: 0.00167194
Iteration 3/25 | Loss: 0.00167194
Iteration 4/25 | Loss: 0.00167194
Iteration 5/25 | Loss: 0.00167194
Iteration 6/25 | Loss: 0.00167194
Iteration 7/25 | Loss: 0.00167194
Iteration 8/25 | Loss: 0.00167194
Iteration 9/25 | Loss: 0.00167194
Iteration 10/25 | Loss: 0.00167194
Iteration 11/25 | Loss: 0.00167194
Iteration 12/25 | Loss: 0.00167194
Iteration 13/25 | Loss: 0.00167194
Iteration 14/25 | Loss: 0.00167194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016719386912882328, 0.0016719386912882328, 0.0016719386912882328, 0.0016719386912882328, 0.0016719386912882328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016719386912882328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167194
Iteration 2/1000 | Loss: 0.00070931
Iteration 3/1000 | Loss: 0.00026740
Iteration 4/1000 | Loss: 0.00009079
Iteration 5/1000 | Loss: 0.00007013
Iteration 6/1000 | Loss: 0.00004768
Iteration 7/1000 | Loss: 0.00014243
Iteration 8/1000 | Loss: 0.00003803
Iteration 9/1000 | Loss: 0.00011771
Iteration 10/1000 | Loss: 0.00020546
Iteration 11/1000 | Loss: 0.00004001
Iteration 12/1000 | Loss: 0.00003619
Iteration 13/1000 | Loss: 0.00003193
Iteration 14/1000 | Loss: 0.00002859
Iteration 15/1000 | Loss: 0.00002685
Iteration 16/1000 | Loss: 0.00002491
Iteration 17/1000 | Loss: 0.00028871
Iteration 18/1000 | Loss: 0.00002814
Iteration 19/1000 | Loss: 0.00002407
Iteration 20/1000 | Loss: 0.00002196
Iteration 21/1000 | Loss: 0.00002016
Iteration 22/1000 | Loss: 0.00001903
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001606
Iteration 29/1000 | Loss: 0.00001604
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001584
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001510
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001509
Iteration 74/1000 | Loss: 0.00001509
Iteration 75/1000 | Loss: 0.00001509
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001505
Iteration 79/1000 | Loss: 0.00001504
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001502
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001502
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001501
Iteration 92/1000 | Loss: 0.00001501
Iteration 93/1000 | Loss: 0.00001501
Iteration 94/1000 | Loss: 0.00001501
Iteration 95/1000 | Loss: 0.00001501
Iteration 96/1000 | Loss: 0.00001501
Iteration 97/1000 | Loss: 0.00001501
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001500
Iteration 100/1000 | Loss: 0.00001500
Iteration 101/1000 | Loss: 0.00001500
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001499
Iteration 104/1000 | Loss: 0.00001499
Iteration 105/1000 | Loss: 0.00001499
Iteration 106/1000 | Loss: 0.00001499
Iteration 107/1000 | Loss: 0.00001499
Iteration 108/1000 | Loss: 0.00001499
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001498
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Iteration 131/1000 | Loss: 0.00001497
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001496
Iteration 138/1000 | Loss: 0.00001496
Iteration 139/1000 | Loss: 0.00001496
Iteration 140/1000 | Loss: 0.00001496
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001495
Iteration 144/1000 | Loss: 0.00001495
Iteration 145/1000 | Loss: 0.00001495
Iteration 146/1000 | Loss: 0.00001495
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001494
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001494
Iteration 153/1000 | Loss: 0.00001494
Iteration 154/1000 | Loss: 0.00001494
Iteration 155/1000 | Loss: 0.00001494
Iteration 156/1000 | Loss: 0.00001494
Iteration 157/1000 | Loss: 0.00001494
Iteration 158/1000 | Loss: 0.00001494
Iteration 159/1000 | Loss: 0.00001494
Iteration 160/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4942680536478292e-05, 1.4942680536478292e-05, 1.4942680536478292e-05, 1.4942680536478292e-05, 1.4942680536478292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4942680536478292e-05

Optimization complete. Final v2v error: 3.266716957092285 mm

Highest mean error: 3.9548981189727783 mm for frame 16

Lowest mean error: 2.7864110469818115 mm for frame 216

Saving results

Total time: 115.23318243026733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469822
Iteration 2/25 | Loss: 0.00135289
Iteration 3/25 | Loss: 0.00127967
Iteration 4/25 | Loss: 0.00127413
Iteration 5/25 | Loss: 0.00127315
Iteration 6/25 | Loss: 0.00127315
Iteration 7/25 | Loss: 0.00127315
Iteration 8/25 | Loss: 0.00127315
Iteration 9/25 | Loss: 0.00127315
Iteration 10/25 | Loss: 0.00127315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012731504393741488, 0.0012731504393741488, 0.0012731504393741488, 0.0012731504393741488, 0.0012731504393741488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012731504393741488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27336645
Iteration 2/25 | Loss: 0.00157937
Iteration 3/25 | Loss: 0.00157935
Iteration 4/25 | Loss: 0.00157935
Iteration 5/25 | Loss: 0.00157935
Iteration 6/25 | Loss: 0.00157935
Iteration 7/25 | Loss: 0.00157935
Iteration 8/25 | Loss: 0.00157935
Iteration 9/25 | Loss: 0.00157935
Iteration 10/25 | Loss: 0.00157935
Iteration 11/25 | Loss: 0.00157935
Iteration 12/25 | Loss: 0.00157935
Iteration 13/25 | Loss: 0.00157935
Iteration 14/25 | Loss: 0.00157935
Iteration 15/25 | Loss: 0.00157935
Iteration 16/25 | Loss: 0.00157935
Iteration 17/25 | Loss: 0.00157935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015793470665812492, 0.0015793470665812492, 0.0015793470665812492, 0.0015793470665812492, 0.0015793470665812492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015793470665812492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157935
Iteration 2/1000 | Loss: 0.00004010
Iteration 3/1000 | Loss: 0.00002648
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001768
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001555
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001553
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001534
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001521
Iteration 36/1000 | Loss: 0.00001521
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001511
Iteration 42/1000 | Loss: 0.00001508
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001506
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001504
Iteration 48/1000 | Loss: 0.00001503
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001476
Iteration 74/1000 | Loss: 0.00001476
Iteration 75/1000 | Loss: 0.00001476
Iteration 76/1000 | Loss: 0.00001476
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001473
Iteration 89/1000 | Loss: 0.00001473
Iteration 90/1000 | Loss: 0.00001473
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001467
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001466
Iteration 119/1000 | Loss: 0.00001466
Iteration 120/1000 | Loss: 0.00001465
Iteration 121/1000 | Loss: 0.00001465
Iteration 122/1000 | Loss: 0.00001465
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001465
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001465
Iteration 148/1000 | Loss: 0.00001465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.4647659554611892e-05, 1.4647659554611892e-05, 1.4647659554611892e-05, 1.4647659554611892e-05, 1.4647659554611892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4647659554611892e-05

Optimization complete. Final v2v error: 3.2441599369049072 mm

Highest mean error: 3.68697452545166 mm for frame 72

Lowest mean error: 2.8439481258392334 mm for frame 189

Saving results

Total time: 47.28188943862915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777830
Iteration 2/25 | Loss: 0.00121501
Iteration 3/25 | Loss: 0.00115459
Iteration 4/25 | Loss: 0.00114479
Iteration 5/25 | Loss: 0.00114198
Iteration 6/25 | Loss: 0.00114160
Iteration 7/25 | Loss: 0.00114160
Iteration 8/25 | Loss: 0.00114160
Iteration 9/25 | Loss: 0.00114160
Iteration 10/25 | Loss: 0.00114160
Iteration 11/25 | Loss: 0.00114160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011416026391088963, 0.0011416026391088963, 0.0011416026391088963, 0.0011416026391088963, 0.0011416026391088963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011416026391088963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34182310
Iteration 2/25 | Loss: 0.00140095
Iteration 3/25 | Loss: 0.00140095
Iteration 4/25 | Loss: 0.00140095
Iteration 5/25 | Loss: 0.00140095
Iteration 6/25 | Loss: 0.00140095
Iteration 7/25 | Loss: 0.00140095
Iteration 8/25 | Loss: 0.00140095
Iteration 9/25 | Loss: 0.00140095
Iteration 10/25 | Loss: 0.00140095
Iteration 11/25 | Loss: 0.00140095
Iteration 12/25 | Loss: 0.00140095
Iteration 13/25 | Loss: 0.00140095
Iteration 14/25 | Loss: 0.00140095
Iteration 15/25 | Loss: 0.00140095
Iteration 16/25 | Loss: 0.00140095
Iteration 17/25 | Loss: 0.00140095
Iteration 18/25 | Loss: 0.00140095
Iteration 19/25 | Loss: 0.00140095
Iteration 20/25 | Loss: 0.00140095
Iteration 21/25 | Loss: 0.00140095
Iteration 22/25 | Loss: 0.00140095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014009482692927122, 0.0014009482692927122, 0.0014009482692927122, 0.0014009482692927122, 0.0014009482692927122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014009482692927122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140095
Iteration 2/1000 | Loss: 0.00001913
Iteration 3/1000 | Loss: 0.00001448
Iteration 4/1000 | Loss: 0.00001277
Iteration 5/1000 | Loss: 0.00001196
Iteration 6/1000 | Loss: 0.00001120
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001054
Iteration 9/1000 | Loss: 0.00001027
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000978
Iteration 13/1000 | Loss: 0.00000976
Iteration 14/1000 | Loss: 0.00000965
Iteration 15/1000 | Loss: 0.00000961
Iteration 16/1000 | Loss: 0.00000961
Iteration 17/1000 | Loss: 0.00000958
Iteration 18/1000 | Loss: 0.00000954
Iteration 19/1000 | Loss: 0.00000954
Iteration 20/1000 | Loss: 0.00000953
Iteration 21/1000 | Loss: 0.00000953
Iteration 22/1000 | Loss: 0.00000952
Iteration 23/1000 | Loss: 0.00000952
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000949
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000948
Iteration 32/1000 | Loss: 0.00000948
Iteration 33/1000 | Loss: 0.00000948
Iteration 34/1000 | Loss: 0.00000948
Iteration 35/1000 | Loss: 0.00000948
Iteration 36/1000 | Loss: 0.00000948
Iteration 37/1000 | Loss: 0.00000947
Iteration 38/1000 | Loss: 0.00000947
Iteration 39/1000 | Loss: 0.00000947
Iteration 40/1000 | Loss: 0.00000947
Iteration 41/1000 | Loss: 0.00000946
Iteration 42/1000 | Loss: 0.00000946
Iteration 43/1000 | Loss: 0.00000945
Iteration 44/1000 | Loss: 0.00000944
Iteration 45/1000 | Loss: 0.00000944
Iteration 46/1000 | Loss: 0.00000943
Iteration 47/1000 | Loss: 0.00000943
Iteration 48/1000 | Loss: 0.00000942
Iteration 49/1000 | Loss: 0.00000939
Iteration 50/1000 | Loss: 0.00000938
Iteration 51/1000 | Loss: 0.00000936
Iteration 52/1000 | Loss: 0.00000936
Iteration 53/1000 | Loss: 0.00000936
Iteration 54/1000 | Loss: 0.00000936
Iteration 55/1000 | Loss: 0.00000935
Iteration 56/1000 | Loss: 0.00000935
Iteration 57/1000 | Loss: 0.00000935
Iteration 58/1000 | Loss: 0.00000935
Iteration 59/1000 | Loss: 0.00000934
Iteration 60/1000 | Loss: 0.00000934
Iteration 61/1000 | Loss: 0.00000933
Iteration 62/1000 | Loss: 0.00000933
Iteration 63/1000 | Loss: 0.00000933
Iteration 64/1000 | Loss: 0.00000933
Iteration 65/1000 | Loss: 0.00000932
Iteration 66/1000 | Loss: 0.00000932
Iteration 67/1000 | Loss: 0.00000932
Iteration 68/1000 | Loss: 0.00000932
Iteration 69/1000 | Loss: 0.00000931
Iteration 70/1000 | Loss: 0.00000931
Iteration 71/1000 | Loss: 0.00000931
Iteration 72/1000 | Loss: 0.00000931
Iteration 73/1000 | Loss: 0.00000931
Iteration 74/1000 | Loss: 0.00000931
Iteration 75/1000 | Loss: 0.00000930
Iteration 76/1000 | Loss: 0.00000930
Iteration 77/1000 | Loss: 0.00000929
Iteration 78/1000 | Loss: 0.00000929
Iteration 79/1000 | Loss: 0.00000929
Iteration 80/1000 | Loss: 0.00000928
Iteration 81/1000 | Loss: 0.00000927
Iteration 82/1000 | Loss: 0.00000927
Iteration 83/1000 | Loss: 0.00000927
Iteration 84/1000 | Loss: 0.00000927
Iteration 85/1000 | Loss: 0.00000927
Iteration 86/1000 | Loss: 0.00000926
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000925
Iteration 91/1000 | Loss: 0.00000925
Iteration 92/1000 | Loss: 0.00000924
Iteration 93/1000 | Loss: 0.00000924
Iteration 94/1000 | Loss: 0.00000924
Iteration 95/1000 | Loss: 0.00000924
Iteration 96/1000 | Loss: 0.00000923
Iteration 97/1000 | Loss: 0.00000923
Iteration 98/1000 | Loss: 0.00000923
Iteration 99/1000 | Loss: 0.00000923
Iteration 100/1000 | Loss: 0.00000923
Iteration 101/1000 | Loss: 0.00000922
Iteration 102/1000 | Loss: 0.00000922
Iteration 103/1000 | Loss: 0.00000922
Iteration 104/1000 | Loss: 0.00000922
Iteration 105/1000 | Loss: 0.00000922
Iteration 106/1000 | Loss: 0.00000922
Iteration 107/1000 | Loss: 0.00000922
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000920
Iteration 111/1000 | Loss: 0.00000920
Iteration 112/1000 | Loss: 0.00000920
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000919
Iteration 117/1000 | Loss: 0.00000918
Iteration 118/1000 | Loss: 0.00000918
Iteration 119/1000 | Loss: 0.00000918
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000917
Iteration 122/1000 | Loss: 0.00000917
Iteration 123/1000 | Loss: 0.00000917
Iteration 124/1000 | Loss: 0.00000917
Iteration 125/1000 | Loss: 0.00000917
Iteration 126/1000 | Loss: 0.00000916
Iteration 127/1000 | Loss: 0.00000916
Iteration 128/1000 | Loss: 0.00000916
Iteration 129/1000 | Loss: 0.00000916
Iteration 130/1000 | Loss: 0.00000916
Iteration 131/1000 | Loss: 0.00000916
Iteration 132/1000 | Loss: 0.00000916
Iteration 133/1000 | Loss: 0.00000916
Iteration 134/1000 | Loss: 0.00000916
Iteration 135/1000 | Loss: 0.00000916
Iteration 136/1000 | Loss: 0.00000915
Iteration 137/1000 | Loss: 0.00000915
Iteration 138/1000 | Loss: 0.00000915
Iteration 139/1000 | Loss: 0.00000915
Iteration 140/1000 | Loss: 0.00000915
Iteration 141/1000 | Loss: 0.00000915
Iteration 142/1000 | Loss: 0.00000915
Iteration 143/1000 | Loss: 0.00000915
Iteration 144/1000 | Loss: 0.00000915
Iteration 145/1000 | Loss: 0.00000915
Iteration 146/1000 | Loss: 0.00000914
Iteration 147/1000 | Loss: 0.00000914
Iteration 148/1000 | Loss: 0.00000914
Iteration 149/1000 | Loss: 0.00000914
Iteration 150/1000 | Loss: 0.00000914
Iteration 151/1000 | Loss: 0.00000914
Iteration 152/1000 | Loss: 0.00000914
Iteration 153/1000 | Loss: 0.00000914
Iteration 154/1000 | Loss: 0.00000914
Iteration 155/1000 | Loss: 0.00000914
Iteration 156/1000 | Loss: 0.00000914
Iteration 157/1000 | Loss: 0.00000914
Iteration 158/1000 | Loss: 0.00000914
Iteration 159/1000 | Loss: 0.00000913
Iteration 160/1000 | Loss: 0.00000913
Iteration 161/1000 | Loss: 0.00000913
Iteration 162/1000 | Loss: 0.00000913
Iteration 163/1000 | Loss: 0.00000913
Iteration 164/1000 | Loss: 0.00000913
Iteration 165/1000 | Loss: 0.00000913
Iteration 166/1000 | Loss: 0.00000913
Iteration 167/1000 | Loss: 0.00000913
Iteration 168/1000 | Loss: 0.00000913
Iteration 169/1000 | Loss: 0.00000913
Iteration 170/1000 | Loss: 0.00000913
Iteration 171/1000 | Loss: 0.00000913
Iteration 172/1000 | Loss: 0.00000913
Iteration 173/1000 | Loss: 0.00000913
Iteration 174/1000 | Loss: 0.00000913
Iteration 175/1000 | Loss: 0.00000913
Iteration 176/1000 | Loss: 0.00000913
Iteration 177/1000 | Loss: 0.00000913
Iteration 178/1000 | Loss: 0.00000913
Iteration 179/1000 | Loss: 0.00000913
Iteration 180/1000 | Loss: 0.00000913
Iteration 181/1000 | Loss: 0.00000913
Iteration 182/1000 | Loss: 0.00000913
Iteration 183/1000 | Loss: 0.00000913
Iteration 184/1000 | Loss: 0.00000913
Iteration 185/1000 | Loss: 0.00000913
Iteration 186/1000 | Loss: 0.00000913
Iteration 187/1000 | Loss: 0.00000913
Iteration 188/1000 | Loss: 0.00000913
Iteration 189/1000 | Loss: 0.00000913
Iteration 190/1000 | Loss: 0.00000913
Iteration 191/1000 | Loss: 0.00000913
Iteration 192/1000 | Loss: 0.00000913
Iteration 193/1000 | Loss: 0.00000913
Iteration 194/1000 | Loss: 0.00000913
Iteration 195/1000 | Loss: 0.00000913
Iteration 196/1000 | Loss: 0.00000913
Iteration 197/1000 | Loss: 0.00000913
Iteration 198/1000 | Loss: 0.00000913
Iteration 199/1000 | Loss: 0.00000913
Iteration 200/1000 | Loss: 0.00000913
Iteration 201/1000 | Loss: 0.00000913
Iteration 202/1000 | Loss: 0.00000913
Iteration 203/1000 | Loss: 0.00000913
Iteration 204/1000 | Loss: 0.00000913
Iteration 205/1000 | Loss: 0.00000913
Iteration 206/1000 | Loss: 0.00000913
Iteration 207/1000 | Loss: 0.00000913
Iteration 208/1000 | Loss: 0.00000913
Iteration 209/1000 | Loss: 0.00000913
Iteration 210/1000 | Loss: 0.00000913
Iteration 211/1000 | Loss: 0.00000913
Iteration 212/1000 | Loss: 0.00000913
Iteration 213/1000 | Loss: 0.00000913
Iteration 214/1000 | Loss: 0.00000913
Iteration 215/1000 | Loss: 0.00000913
Iteration 216/1000 | Loss: 0.00000913
Iteration 217/1000 | Loss: 0.00000913
Iteration 218/1000 | Loss: 0.00000913
Iteration 219/1000 | Loss: 0.00000913
Iteration 220/1000 | Loss: 0.00000913
Iteration 221/1000 | Loss: 0.00000913
Iteration 222/1000 | Loss: 0.00000913
Iteration 223/1000 | Loss: 0.00000913
Iteration 224/1000 | Loss: 0.00000913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [9.13479470909806e-06, 9.13479470909806e-06, 9.13479470909806e-06, 9.13479470909806e-06, 9.13479470909806e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.13479470909806e-06

Optimization complete. Final v2v error: 2.641242265701294 mm

Highest mean error: 2.8336102962493896 mm for frame 82

Lowest mean error: 2.518559455871582 mm for frame 39

Saving results

Total time: 37.117318630218506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431751
Iteration 2/25 | Loss: 0.00133379
Iteration 3/25 | Loss: 0.00124637
Iteration 4/25 | Loss: 0.00123089
Iteration 5/25 | Loss: 0.00122451
Iteration 6/25 | Loss: 0.00122300
Iteration 7/25 | Loss: 0.00122298
Iteration 8/25 | Loss: 0.00122298
Iteration 9/25 | Loss: 0.00122298
Iteration 10/25 | Loss: 0.00122298
Iteration 11/25 | Loss: 0.00122298
Iteration 12/25 | Loss: 0.00122298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001222978695295751, 0.001222978695295751, 0.001222978695295751, 0.001222978695295751, 0.001222978695295751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001222978695295751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70835531
Iteration 2/25 | Loss: 0.00144896
Iteration 3/25 | Loss: 0.00144896
Iteration 4/25 | Loss: 0.00144896
Iteration 5/25 | Loss: 0.00144896
Iteration 6/25 | Loss: 0.00144896
Iteration 7/25 | Loss: 0.00144896
Iteration 8/25 | Loss: 0.00144896
Iteration 9/25 | Loss: 0.00144896
Iteration 10/25 | Loss: 0.00144896
Iteration 11/25 | Loss: 0.00144896
Iteration 12/25 | Loss: 0.00144895
Iteration 13/25 | Loss: 0.00144895
Iteration 14/25 | Loss: 0.00144895
Iteration 15/25 | Loss: 0.00144895
Iteration 16/25 | Loss: 0.00144895
Iteration 17/25 | Loss: 0.00144895
Iteration 18/25 | Loss: 0.00144895
Iteration 19/25 | Loss: 0.00144895
Iteration 20/25 | Loss: 0.00144895
Iteration 21/25 | Loss: 0.00144895
Iteration 22/25 | Loss: 0.00144895
Iteration 23/25 | Loss: 0.00144895
Iteration 24/25 | Loss: 0.00144895
Iteration 25/25 | Loss: 0.00144895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144895
Iteration 2/1000 | Loss: 0.00002566
Iteration 3/1000 | Loss: 0.00001971
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001766
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001670
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001611
Iteration 12/1000 | Loss: 0.00001607
Iteration 13/1000 | Loss: 0.00001603
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001581
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001580
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001578
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001577
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001576
Iteration 41/1000 | Loss: 0.00001575
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001569
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001566
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001563
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001562
Iteration 81/1000 | Loss: 0.00001562
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001558
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001557
Iteration 105/1000 | Loss: 0.00001557
Iteration 106/1000 | Loss: 0.00001557
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001556
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Iteration 114/1000 | Loss: 0.00001555
Iteration 115/1000 | Loss: 0.00001555
Iteration 116/1000 | Loss: 0.00001555
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001554
Iteration 132/1000 | Loss: 0.00001554
Iteration 133/1000 | Loss: 0.00001554
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Iteration 141/1000 | Loss: 0.00001552
Iteration 142/1000 | Loss: 0.00001552
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001550
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001550
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Iteration 159/1000 | Loss: 0.00001549
Iteration 160/1000 | Loss: 0.00001549
Iteration 161/1000 | Loss: 0.00001549
Iteration 162/1000 | Loss: 0.00001549
Iteration 163/1000 | Loss: 0.00001549
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001549
Iteration 168/1000 | Loss: 0.00001549
Iteration 169/1000 | Loss: 0.00001548
Iteration 170/1000 | Loss: 0.00001548
Iteration 171/1000 | Loss: 0.00001548
Iteration 172/1000 | Loss: 0.00001548
Iteration 173/1000 | Loss: 0.00001548
Iteration 174/1000 | Loss: 0.00001548
Iteration 175/1000 | Loss: 0.00001548
Iteration 176/1000 | Loss: 0.00001548
Iteration 177/1000 | Loss: 0.00001548
Iteration 178/1000 | Loss: 0.00001548
Iteration 179/1000 | Loss: 0.00001548
Iteration 180/1000 | Loss: 0.00001548
Iteration 181/1000 | Loss: 0.00001548
Iteration 182/1000 | Loss: 0.00001548
Iteration 183/1000 | Loss: 0.00001548
Iteration 184/1000 | Loss: 0.00001548
Iteration 185/1000 | Loss: 0.00001548
Iteration 186/1000 | Loss: 0.00001548
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001547
Iteration 195/1000 | Loss: 0.00001547
Iteration 196/1000 | Loss: 0.00001547
Iteration 197/1000 | Loss: 0.00001547
Iteration 198/1000 | Loss: 0.00001547
Iteration 199/1000 | Loss: 0.00001547
Iteration 200/1000 | Loss: 0.00001547
Iteration 201/1000 | Loss: 0.00001547
Iteration 202/1000 | Loss: 0.00001547
Iteration 203/1000 | Loss: 0.00001547
Iteration 204/1000 | Loss: 0.00001547
Iteration 205/1000 | Loss: 0.00001547
Iteration 206/1000 | Loss: 0.00001547
Iteration 207/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.5468931451323442e-05, 1.5468931451323442e-05, 1.5468931451323442e-05, 1.5468931451323442e-05, 1.5468931451323442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5468931451323442e-05

Optimization complete. Final v2v error: 3.3389251232147217 mm

Highest mean error: 3.8688597679138184 mm for frame 158

Lowest mean error: 3.2521438598632812 mm for frame 68

Saving results

Total time: 46.40769672393799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842887
Iteration 2/25 | Loss: 0.00261330
Iteration 3/25 | Loss: 0.00203887
Iteration 4/25 | Loss: 0.00174145
Iteration 5/25 | Loss: 0.00151930
Iteration 6/25 | Loss: 0.00140063
Iteration 7/25 | Loss: 0.00137555
Iteration 8/25 | Loss: 0.00136861
Iteration 9/25 | Loss: 0.00136422
Iteration 10/25 | Loss: 0.00136262
Iteration 11/25 | Loss: 0.00136236
Iteration 12/25 | Loss: 0.00136222
Iteration 13/25 | Loss: 0.00136222
Iteration 14/25 | Loss: 0.00136222
Iteration 15/25 | Loss: 0.00136222
Iteration 16/25 | Loss: 0.00136222
Iteration 17/25 | Loss: 0.00136222
Iteration 18/25 | Loss: 0.00136222
Iteration 19/25 | Loss: 0.00136222
Iteration 20/25 | Loss: 0.00136222
Iteration 21/25 | Loss: 0.00136222
Iteration 22/25 | Loss: 0.00136222
Iteration 23/25 | Loss: 0.00136222
Iteration 24/25 | Loss: 0.00136222
Iteration 25/25 | Loss: 0.00136222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21992970
Iteration 2/25 | Loss: 0.00109496
Iteration 3/25 | Loss: 0.00109496
Iteration 4/25 | Loss: 0.00109496
Iteration 5/25 | Loss: 0.00109496
Iteration 6/25 | Loss: 0.00109496
Iteration 7/25 | Loss: 0.00109496
Iteration 8/25 | Loss: 0.00109496
Iteration 9/25 | Loss: 0.00109496
Iteration 10/25 | Loss: 0.00109496
Iteration 11/25 | Loss: 0.00109496
Iteration 12/25 | Loss: 0.00109496
Iteration 13/25 | Loss: 0.00109496
Iteration 14/25 | Loss: 0.00109496
Iteration 15/25 | Loss: 0.00109496
Iteration 16/25 | Loss: 0.00109496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010949581628665328, 0.0010949581628665328, 0.0010949581628665328, 0.0010949581628665328, 0.0010949581628665328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010949581628665328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109496
Iteration 2/1000 | Loss: 0.00004307
Iteration 3/1000 | Loss: 0.00002625
Iteration 4/1000 | Loss: 0.00002369
Iteration 5/1000 | Loss: 0.00002295
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002202
Iteration 8/1000 | Loss: 0.00002176
Iteration 9/1000 | Loss: 0.00002153
Iteration 10/1000 | Loss: 0.00002137
Iteration 11/1000 | Loss: 0.00008889
Iteration 12/1000 | Loss: 0.00002325
Iteration 13/1000 | Loss: 0.00002211
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002085
Iteration 16/1000 | Loss: 0.00002084
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002078
Iteration 21/1000 | Loss: 0.00002076
Iteration 22/1000 | Loss: 0.00002061
Iteration 23/1000 | Loss: 0.00002049
Iteration 24/1000 | Loss: 0.00002049
Iteration 25/1000 | Loss: 0.00002048
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002046
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002046
Iteration 31/1000 | Loss: 0.00002046
Iteration 32/1000 | Loss: 0.00002046
Iteration 33/1000 | Loss: 0.00002046
Iteration 34/1000 | Loss: 0.00002045
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002044
Iteration 37/1000 | Loss: 0.00002044
Iteration 38/1000 | Loss: 0.00002044
Iteration 39/1000 | Loss: 0.00002044
Iteration 40/1000 | Loss: 0.00002044
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002044
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002042
Iteration 47/1000 | Loss: 0.00002042
Iteration 48/1000 | Loss: 0.00002041
Iteration 49/1000 | Loss: 0.00002040
Iteration 50/1000 | Loss: 0.00002040
Iteration 51/1000 | Loss: 0.00002040
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002038
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002035
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002035
Iteration 70/1000 | Loss: 0.00002035
Iteration 71/1000 | Loss: 0.00002035
Iteration 72/1000 | Loss: 0.00002035
Iteration 73/1000 | Loss: 0.00002035
Iteration 74/1000 | Loss: 0.00002035
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00002035
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002035
Iteration 90/1000 | Loss: 0.00002035
Iteration 91/1000 | Loss: 0.00002035
Iteration 92/1000 | Loss: 0.00002035
Iteration 93/1000 | Loss: 0.00002035
Iteration 94/1000 | Loss: 0.00002035
Iteration 95/1000 | Loss: 0.00002035
Iteration 96/1000 | Loss: 0.00002035
Iteration 97/1000 | Loss: 0.00002035
Iteration 98/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.0347324607428163e-05, 2.0347324607428163e-05, 2.0347324607428163e-05, 2.0347324607428163e-05, 2.0347324607428163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0347324607428163e-05

Optimization complete. Final v2v error: 3.8023722171783447 mm

Highest mean error: 5.127864837646484 mm for frame 103

Lowest mean error: 3.5163590908050537 mm for frame 0

Saving results

Total time: 54.8861882686615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008436
Iteration 2/25 | Loss: 0.00149428
Iteration 3/25 | Loss: 0.00122919
Iteration 4/25 | Loss: 0.00119704
Iteration 5/25 | Loss: 0.00119165
Iteration 6/25 | Loss: 0.00119023
Iteration 7/25 | Loss: 0.00119023
Iteration 8/25 | Loss: 0.00119023
Iteration 9/25 | Loss: 0.00119023
Iteration 10/25 | Loss: 0.00119023
Iteration 11/25 | Loss: 0.00119023
Iteration 12/25 | Loss: 0.00119023
Iteration 13/25 | Loss: 0.00119023
Iteration 14/25 | Loss: 0.00119023
Iteration 15/25 | Loss: 0.00119023
Iteration 16/25 | Loss: 0.00119023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011902303667739034, 0.0011902303667739034, 0.0011902303667739034, 0.0011902303667739034, 0.0011902303667739034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011902303667739034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58435345
Iteration 2/25 | Loss: 0.00139040
Iteration 3/25 | Loss: 0.00139039
Iteration 4/25 | Loss: 0.00139039
Iteration 5/25 | Loss: 0.00139039
Iteration 6/25 | Loss: 0.00139039
Iteration 7/25 | Loss: 0.00139039
Iteration 8/25 | Loss: 0.00139039
Iteration 9/25 | Loss: 0.00139039
Iteration 10/25 | Loss: 0.00139039
Iteration 11/25 | Loss: 0.00139039
Iteration 12/25 | Loss: 0.00139039
Iteration 13/25 | Loss: 0.00139039
Iteration 14/25 | Loss: 0.00139039
Iteration 15/25 | Loss: 0.00139039
Iteration 16/25 | Loss: 0.00139039
Iteration 17/25 | Loss: 0.00139039
Iteration 18/25 | Loss: 0.00139039
Iteration 19/25 | Loss: 0.00139039
Iteration 20/25 | Loss: 0.00139039
Iteration 21/25 | Loss: 0.00139039
Iteration 22/25 | Loss: 0.00139039
Iteration 23/25 | Loss: 0.00139039
Iteration 24/25 | Loss: 0.00139039
Iteration 25/25 | Loss: 0.00139039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139039
Iteration 2/1000 | Loss: 0.00002132
Iteration 3/1000 | Loss: 0.00001626
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001333
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001259
Iteration 9/1000 | Loss: 0.00001217
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001164
Iteration 15/1000 | Loss: 0.00001159
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001147
Iteration 22/1000 | Loss: 0.00001147
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001131
Iteration 34/1000 | Loss: 0.00001131
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001130
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001129
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001125
Iteration 61/1000 | Loss: 0.00001125
Iteration 62/1000 | Loss: 0.00001125
Iteration 63/1000 | Loss: 0.00001125
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001124
Iteration 67/1000 | Loss: 0.00001124
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001123
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001122
Iteration 93/1000 | Loss: 0.00001122
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001122
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001121
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001121
Iteration 105/1000 | Loss: 0.00001121
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001121
Iteration 108/1000 | Loss: 0.00001121
Iteration 109/1000 | Loss: 0.00001121
Iteration 110/1000 | Loss: 0.00001121
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001121
Iteration 121/1000 | Loss: 0.00001121
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001121
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001121
Iteration 128/1000 | Loss: 0.00001121
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001121
Iteration 132/1000 | Loss: 0.00001121
Iteration 133/1000 | Loss: 0.00001121
Iteration 134/1000 | Loss: 0.00001121
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001121
Iteration 138/1000 | Loss: 0.00001121
Iteration 139/1000 | Loss: 0.00001121
Iteration 140/1000 | Loss: 0.00001121
Iteration 141/1000 | Loss: 0.00001121
Iteration 142/1000 | Loss: 0.00001121
Iteration 143/1000 | Loss: 0.00001121
Iteration 144/1000 | Loss: 0.00001121
Iteration 145/1000 | Loss: 0.00001121
Iteration 146/1000 | Loss: 0.00001121
Iteration 147/1000 | Loss: 0.00001121
Iteration 148/1000 | Loss: 0.00001121
Iteration 149/1000 | Loss: 0.00001121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1212991012143902e-05, 1.1212991012143902e-05, 1.1212991012143902e-05, 1.1212991012143902e-05, 1.1212991012143902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1212991012143902e-05

Optimization complete. Final v2v error: 2.877471446990967 mm

Highest mean error: 3.2955546379089355 mm for frame 95

Lowest mean error: 2.6653597354888916 mm for frame 28

Saving results

Total time: 41.97283458709717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965600
Iteration 2/25 | Loss: 0.00160795
Iteration 3/25 | Loss: 0.00132640
Iteration 4/25 | Loss: 0.00125528
Iteration 5/25 | Loss: 0.00124912
Iteration 6/25 | Loss: 0.00124684
Iteration 7/25 | Loss: 0.00123739
Iteration 8/25 | Loss: 0.00123358
Iteration 9/25 | Loss: 0.00123257
Iteration 10/25 | Loss: 0.00123205
Iteration 11/25 | Loss: 0.00123476
Iteration 12/25 | Loss: 0.00123409
Iteration 13/25 | Loss: 0.00123394
Iteration 14/25 | Loss: 0.00123371
Iteration 15/25 | Loss: 0.00123128
Iteration 16/25 | Loss: 0.00122973
Iteration 17/25 | Loss: 0.00122943
Iteration 18/25 | Loss: 0.00122933
Iteration 19/25 | Loss: 0.00122931
Iteration 20/25 | Loss: 0.00122931
Iteration 21/25 | Loss: 0.00122931
Iteration 22/25 | Loss: 0.00122931
Iteration 23/25 | Loss: 0.00122931
Iteration 24/25 | Loss: 0.00122931
Iteration 25/25 | Loss: 0.00122931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39006317
Iteration 2/25 | Loss: 0.00177548
Iteration 3/25 | Loss: 0.00177547
Iteration 4/25 | Loss: 0.00177547
Iteration 5/25 | Loss: 0.00177547
Iteration 6/25 | Loss: 0.00177547
Iteration 7/25 | Loss: 0.00177547
Iteration 8/25 | Loss: 0.00177547
Iteration 9/25 | Loss: 0.00177547
Iteration 10/25 | Loss: 0.00177547
Iteration 11/25 | Loss: 0.00177547
Iteration 12/25 | Loss: 0.00177547
Iteration 13/25 | Loss: 0.00177547
Iteration 14/25 | Loss: 0.00177547
Iteration 15/25 | Loss: 0.00177547
Iteration 16/25 | Loss: 0.00177547
Iteration 17/25 | Loss: 0.00177547
Iteration 18/25 | Loss: 0.00177547
Iteration 19/25 | Loss: 0.00177547
Iteration 20/25 | Loss: 0.00177547
Iteration 21/25 | Loss: 0.00177547
Iteration 22/25 | Loss: 0.00177547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017754671862348914, 0.0017754671862348914, 0.0017754671862348914, 0.0017754671862348914, 0.0017754671862348914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017754671862348914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177547
Iteration 2/1000 | Loss: 0.00002800
Iteration 3/1000 | Loss: 0.00001872
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001496
Iteration 9/1000 | Loss: 0.00001464
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00014418
Iteration 13/1000 | Loss: 0.00011010
Iteration 14/1000 | Loss: 0.00013895
Iteration 15/1000 | Loss: 0.00001750
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00018598
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001666
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001448
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001386
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00002140
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001396
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001387
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001383
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001382
Iteration 52/1000 | Loss: 0.00001382
Iteration 53/1000 | Loss: 0.00001382
Iteration 54/1000 | Loss: 0.00001382
Iteration 55/1000 | Loss: 0.00001382
Iteration 56/1000 | Loss: 0.00001381
Iteration 57/1000 | Loss: 0.00001381
Iteration 58/1000 | Loss: 0.00001381
Iteration 59/1000 | Loss: 0.00001381
Iteration 60/1000 | Loss: 0.00001381
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001379
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001378
Iteration 70/1000 | Loss: 0.00001378
Iteration 71/1000 | Loss: 0.00001378
Iteration 72/1000 | Loss: 0.00001378
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001371
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001359
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001313
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001295
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001287
Iteration 119/1000 | Loss: 0.00001286
Iteration 120/1000 | Loss: 0.00001286
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001283
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001282
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001281
Iteration 141/1000 | Loss: 0.00001280
Iteration 142/1000 | Loss: 0.00001280
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001279
Iteration 149/1000 | Loss: 0.00001279
Iteration 150/1000 | Loss: 0.00001279
Iteration 151/1000 | Loss: 0.00001279
Iteration 152/1000 | Loss: 0.00001279
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001278
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001276
Iteration 158/1000 | Loss: 0.00001276
Iteration 159/1000 | Loss: 0.00001276
Iteration 160/1000 | Loss: 0.00001275
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001274
Iteration 168/1000 | Loss: 0.00001274
Iteration 169/1000 | Loss: 0.00001274
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001272
Iteration 176/1000 | Loss: 0.00001272
Iteration 177/1000 | Loss: 0.00001272
Iteration 178/1000 | Loss: 0.00001272
Iteration 179/1000 | Loss: 0.00001272
Iteration 180/1000 | Loss: 0.00001272
Iteration 181/1000 | Loss: 0.00001272
Iteration 182/1000 | Loss: 0.00001272
Iteration 183/1000 | Loss: 0.00001272
Iteration 184/1000 | Loss: 0.00001272
Iteration 185/1000 | Loss: 0.00001272
Iteration 186/1000 | Loss: 0.00001272
Iteration 187/1000 | Loss: 0.00001272
Iteration 188/1000 | Loss: 0.00001272
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001271
Iteration 205/1000 | Loss: 0.00001271
Iteration 206/1000 | Loss: 0.00001271
Iteration 207/1000 | Loss: 0.00001271
Iteration 208/1000 | Loss: 0.00001271
Iteration 209/1000 | Loss: 0.00001271
Iteration 210/1000 | Loss: 0.00001271
Iteration 211/1000 | Loss: 0.00001271
Iteration 212/1000 | Loss: 0.00001271
Iteration 213/1000 | Loss: 0.00001271
Iteration 214/1000 | Loss: 0.00001271
Iteration 215/1000 | Loss: 0.00001271
Iteration 216/1000 | Loss: 0.00001271
Iteration 217/1000 | Loss: 0.00001271
Iteration 218/1000 | Loss: 0.00001271
Iteration 219/1000 | Loss: 0.00001271
Iteration 220/1000 | Loss: 0.00001271
Iteration 221/1000 | Loss: 0.00001271
Iteration 222/1000 | Loss: 0.00001271
Iteration 223/1000 | Loss: 0.00001271
Iteration 224/1000 | Loss: 0.00001271
Iteration 225/1000 | Loss: 0.00001271
Iteration 226/1000 | Loss: 0.00001271
Iteration 227/1000 | Loss: 0.00001271
Iteration 228/1000 | Loss: 0.00001271
Iteration 229/1000 | Loss: 0.00001271
Iteration 230/1000 | Loss: 0.00001271
Iteration 231/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.2709891962003894e-05, 1.2709891962003894e-05, 1.2709891962003894e-05, 1.2709891962003894e-05, 1.2709891962003894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2709891962003894e-05

Optimization complete. Final v2v error: 3.042262077331543 mm

Highest mean error: 4.858023643493652 mm for frame 181

Lowest mean error: 2.7471072673797607 mm for frame 96

Saving results

Total time: 108.41296315193176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902708
Iteration 2/25 | Loss: 0.00164044
Iteration 3/25 | Loss: 0.00140437
Iteration 4/25 | Loss: 0.00138084
Iteration 5/25 | Loss: 0.00137692
Iteration 6/25 | Loss: 0.00137573
Iteration 7/25 | Loss: 0.00137565
Iteration 8/25 | Loss: 0.00137565
Iteration 9/25 | Loss: 0.00137565
Iteration 10/25 | Loss: 0.00137565
Iteration 11/25 | Loss: 0.00137565
Iteration 12/25 | Loss: 0.00137565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013756498228758574, 0.0013756498228758574, 0.0013756498228758574, 0.0013756498228758574, 0.0013756498228758574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013756498228758574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53188384
Iteration 2/25 | Loss: 0.00138884
Iteration 3/25 | Loss: 0.00138884
Iteration 4/25 | Loss: 0.00138884
Iteration 5/25 | Loss: 0.00138884
Iteration 6/25 | Loss: 0.00138884
Iteration 7/25 | Loss: 0.00138884
Iteration 8/25 | Loss: 0.00138884
Iteration 9/25 | Loss: 0.00138884
Iteration 10/25 | Loss: 0.00138884
Iteration 11/25 | Loss: 0.00138884
Iteration 12/25 | Loss: 0.00138884
Iteration 13/25 | Loss: 0.00138884
Iteration 14/25 | Loss: 0.00138884
Iteration 15/25 | Loss: 0.00138884
Iteration 16/25 | Loss: 0.00138884
Iteration 17/25 | Loss: 0.00138884
Iteration 18/25 | Loss: 0.00138883
Iteration 19/25 | Loss: 0.00138883
Iteration 20/25 | Loss: 0.00138883
Iteration 21/25 | Loss: 0.00138883
Iteration 22/25 | Loss: 0.00138883
Iteration 23/25 | Loss: 0.00138883
Iteration 24/25 | Loss: 0.00138883
Iteration 25/25 | Loss: 0.00138883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138883
Iteration 2/1000 | Loss: 0.00005253
Iteration 3/1000 | Loss: 0.00003478
Iteration 4/1000 | Loss: 0.00002902
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00002594
Iteration 7/1000 | Loss: 0.00002520
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002425
Iteration 10/1000 | Loss: 0.00002399
Iteration 11/1000 | Loss: 0.00002379
Iteration 12/1000 | Loss: 0.00002373
Iteration 13/1000 | Loss: 0.00002359
Iteration 14/1000 | Loss: 0.00002350
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002344
Iteration 17/1000 | Loss: 0.00002333
Iteration 18/1000 | Loss: 0.00002331
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002329
Iteration 21/1000 | Loss: 0.00002328
Iteration 22/1000 | Loss: 0.00002319
Iteration 23/1000 | Loss: 0.00002313
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002312
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002310
Iteration 29/1000 | Loss: 0.00002310
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00002309
Iteration 32/1000 | Loss: 0.00002309
Iteration 33/1000 | Loss: 0.00002309
Iteration 34/1000 | Loss: 0.00002309
Iteration 35/1000 | Loss: 0.00002309
Iteration 36/1000 | Loss: 0.00002308
Iteration 37/1000 | Loss: 0.00002308
Iteration 38/1000 | Loss: 0.00002308
Iteration 39/1000 | Loss: 0.00002307
Iteration 40/1000 | Loss: 0.00002307
Iteration 41/1000 | Loss: 0.00002307
Iteration 42/1000 | Loss: 0.00002307
Iteration 43/1000 | Loss: 0.00002306
Iteration 44/1000 | Loss: 0.00002305
Iteration 45/1000 | Loss: 0.00002305
Iteration 46/1000 | Loss: 0.00002305
Iteration 47/1000 | Loss: 0.00002305
Iteration 48/1000 | Loss: 0.00002305
Iteration 49/1000 | Loss: 0.00002305
Iteration 50/1000 | Loss: 0.00002305
Iteration 51/1000 | Loss: 0.00002305
Iteration 52/1000 | Loss: 0.00002305
Iteration 53/1000 | Loss: 0.00002305
Iteration 54/1000 | Loss: 0.00002305
Iteration 55/1000 | Loss: 0.00002304
Iteration 56/1000 | Loss: 0.00002304
Iteration 57/1000 | Loss: 0.00002304
Iteration 58/1000 | Loss: 0.00002304
Iteration 59/1000 | Loss: 0.00002304
Iteration 60/1000 | Loss: 0.00002304
Iteration 61/1000 | Loss: 0.00002304
Iteration 62/1000 | Loss: 0.00002303
Iteration 63/1000 | Loss: 0.00002303
Iteration 64/1000 | Loss: 0.00002303
Iteration 65/1000 | Loss: 0.00002303
Iteration 66/1000 | Loss: 0.00002303
Iteration 67/1000 | Loss: 0.00002303
Iteration 68/1000 | Loss: 0.00002303
Iteration 69/1000 | Loss: 0.00002303
Iteration 70/1000 | Loss: 0.00002302
Iteration 71/1000 | Loss: 0.00002302
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002302
Iteration 76/1000 | Loss: 0.00002302
Iteration 77/1000 | Loss: 0.00002301
Iteration 78/1000 | Loss: 0.00002301
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002301
Iteration 81/1000 | Loss: 0.00002300
Iteration 82/1000 | Loss: 0.00002300
Iteration 83/1000 | Loss: 0.00002300
Iteration 84/1000 | Loss: 0.00002300
Iteration 85/1000 | Loss: 0.00002300
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002299
Iteration 88/1000 | Loss: 0.00002299
Iteration 89/1000 | Loss: 0.00002299
Iteration 90/1000 | Loss: 0.00002299
Iteration 91/1000 | Loss: 0.00002299
Iteration 92/1000 | Loss: 0.00002299
Iteration 93/1000 | Loss: 0.00002299
Iteration 94/1000 | Loss: 0.00002299
Iteration 95/1000 | Loss: 0.00002299
Iteration 96/1000 | Loss: 0.00002299
Iteration 97/1000 | Loss: 0.00002298
Iteration 98/1000 | Loss: 0.00002298
Iteration 99/1000 | Loss: 0.00002298
Iteration 100/1000 | Loss: 0.00002298
Iteration 101/1000 | Loss: 0.00002298
Iteration 102/1000 | Loss: 0.00002298
Iteration 103/1000 | Loss: 0.00002298
Iteration 104/1000 | Loss: 0.00002298
Iteration 105/1000 | Loss: 0.00002298
Iteration 106/1000 | Loss: 0.00002298
Iteration 107/1000 | Loss: 0.00002298
Iteration 108/1000 | Loss: 0.00002297
Iteration 109/1000 | Loss: 0.00002297
Iteration 110/1000 | Loss: 0.00002297
Iteration 111/1000 | Loss: 0.00002297
Iteration 112/1000 | Loss: 0.00002297
Iteration 113/1000 | Loss: 0.00002296
Iteration 114/1000 | Loss: 0.00002296
Iteration 115/1000 | Loss: 0.00002296
Iteration 116/1000 | Loss: 0.00002296
Iteration 117/1000 | Loss: 0.00002296
Iteration 118/1000 | Loss: 0.00002295
Iteration 119/1000 | Loss: 0.00002295
Iteration 120/1000 | Loss: 0.00002295
Iteration 121/1000 | Loss: 0.00002295
Iteration 122/1000 | Loss: 0.00002295
Iteration 123/1000 | Loss: 0.00002295
Iteration 124/1000 | Loss: 0.00002295
Iteration 125/1000 | Loss: 0.00002295
Iteration 126/1000 | Loss: 0.00002294
Iteration 127/1000 | Loss: 0.00002294
Iteration 128/1000 | Loss: 0.00002294
Iteration 129/1000 | Loss: 0.00002294
Iteration 130/1000 | Loss: 0.00002294
Iteration 131/1000 | Loss: 0.00002294
Iteration 132/1000 | Loss: 0.00002294
Iteration 133/1000 | Loss: 0.00002294
Iteration 134/1000 | Loss: 0.00002294
Iteration 135/1000 | Loss: 0.00002294
Iteration 136/1000 | Loss: 0.00002294
Iteration 137/1000 | Loss: 0.00002294
Iteration 138/1000 | Loss: 0.00002294
Iteration 139/1000 | Loss: 0.00002294
Iteration 140/1000 | Loss: 0.00002293
Iteration 141/1000 | Loss: 0.00002293
Iteration 142/1000 | Loss: 0.00002293
Iteration 143/1000 | Loss: 0.00002293
Iteration 144/1000 | Loss: 0.00002293
Iteration 145/1000 | Loss: 0.00002293
Iteration 146/1000 | Loss: 0.00002293
Iteration 147/1000 | Loss: 0.00002293
Iteration 148/1000 | Loss: 0.00002293
Iteration 149/1000 | Loss: 0.00002293
Iteration 150/1000 | Loss: 0.00002293
Iteration 151/1000 | Loss: 0.00002293
Iteration 152/1000 | Loss: 0.00002293
Iteration 153/1000 | Loss: 0.00002293
Iteration 154/1000 | Loss: 0.00002293
Iteration 155/1000 | Loss: 0.00002293
Iteration 156/1000 | Loss: 0.00002292
Iteration 157/1000 | Loss: 0.00002292
Iteration 158/1000 | Loss: 0.00002292
Iteration 159/1000 | Loss: 0.00002292
Iteration 160/1000 | Loss: 0.00002292
Iteration 161/1000 | Loss: 0.00002292
Iteration 162/1000 | Loss: 0.00002292
Iteration 163/1000 | Loss: 0.00002292
Iteration 164/1000 | Loss: 0.00002292
Iteration 165/1000 | Loss: 0.00002292
Iteration 166/1000 | Loss: 0.00002292
Iteration 167/1000 | Loss: 0.00002292
Iteration 168/1000 | Loss: 0.00002292
Iteration 169/1000 | Loss: 0.00002292
Iteration 170/1000 | Loss: 0.00002291
Iteration 171/1000 | Loss: 0.00002291
Iteration 172/1000 | Loss: 0.00002291
Iteration 173/1000 | Loss: 0.00002291
Iteration 174/1000 | Loss: 0.00002291
Iteration 175/1000 | Loss: 0.00002291
Iteration 176/1000 | Loss: 0.00002291
Iteration 177/1000 | Loss: 0.00002291
Iteration 178/1000 | Loss: 0.00002291
Iteration 179/1000 | Loss: 0.00002291
Iteration 180/1000 | Loss: 0.00002291
Iteration 181/1000 | Loss: 0.00002291
Iteration 182/1000 | Loss: 0.00002291
Iteration 183/1000 | Loss: 0.00002290
Iteration 184/1000 | Loss: 0.00002290
Iteration 185/1000 | Loss: 0.00002290
Iteration 186/1000 | Loss: 0.00002290
Iteration 187/1000 | Loss: 0.00002290
Iteration 188/1000 | Loss: 0.00002290
Iteration 189/1000 | Loss: 0.00002290
Iteration 190/1000 | Loss: 0.00002290
Iteration 191/1000 | Loss: 0.00002289
Iteration 192/1000 | Loss: 0.00002289
Iteration 193/1000 | Loss: 0.00002289
Iteration 194/1000 | Loss: 0.00002289
Iteration 195/1000 | Loss: 0.00002289
Iteration 196/1000 | Loss: 0.00002289
Iteration 197/1000 | Loss: 0.00002289
Iteration 198/1000 | Loss: 0.00002289
Iteration 199/1000 | Loss: 0.00002289
Iteration 200/1000 | Loss: 0.00002289
Iteration 201/1000 | Loss: 0.00002289
Iteration 202/1000 | Loss: 0.00002289
Iteration 203/1000 | Loss: 0.00002289
Iteration 204/1000 | Loss: 0.00002289
Iteration 205/1000 | Loss: 0.00002289
Iteration 206/1000 | Loss: 0.00002289
Iteration 207/1000 | Loss: 0.00002289
Iteration 208/1000 | Loss: 0.00002289
Iteration 209/1000 | Loss: 0.00002289
Iteration 210/1000 | Loss: 0.00002289
Iteration 211/1000 | Loss: 0.00002289
Iteration 212/1000 | Loss: 0.00002289
Iteration 213/1000 | Loss: 0.00002289
Iteration 214/1000 | Loss: 0.00002288
Iteration 215/1000 | Loss: 0.00002288
Iteration 216/1000 | Loss: 0.00002288
Iteration 217/1000 | Loss: 0.00002288
Iteration 218/1000 | Loss: 0.00002288
Iteration 219/1000 | Loss: 0.00002288
Iteration 220/1000 | Loss: 0.00002288
Iteration 221/1000 | Loss: 0.00002288
Iteration 222/1000 | Loss: 0.00002288
Iteration 223/1000 | Loss: 0.00002288
Iteration 224/1000 | Loss: 0.00002288
Iteration 225/1000 | Loss: 0.00002288
Iteration 226/1000 | Loss: 0.00002288
Iteration 227/1000 | Loss: 0.00002288
Iteration 228/1000 | Loss: 0.00002288
Iteration 229/1000 | Loss: 0.00002288
Iteration 230/1000 | Loss: 0.00002288
Iteration 231/1000 | Loss: 0.00002288
Iteration 232/1000 | Loss: 0.00002288
Iteration 233/1000 | Loss: 0.00002288
Iteration 234/1000 | Loss: 0.00002288
Iteration 235/1000 | Loss: 0.00002288
Iteration 236/1000 | Loss: 0.00002288
Iteration 237/1000 | Loss: 0.00002288
Iteration 238/1000 | Loss: 0.00002288
Iteration 239/1000 | Loss: 0.00002288
Iteration 240/1000 | Loss: 0.00002288
Iteration 241/1000 | Loss: 0.00002288
Iteration 242/1000 | Loss: 0.00002288
Iteration 243/1000 | Loss: 0.00002288
Iteration 244/1000 | Loss: 0.00002288
Iteration 245/1000 | Loss: 0.00002288
Iteration 246/1000 | Loss: 0.00002288
Iteration 247/1000 | Loss: 0.00002288
Iteration 248/1000 | Loss: 0.00002288
Iteration 249/1000 | Loss: 0.00002288
Iteration 250/1000 | Loss: 0.00002288
Iteration 251/1000 | Loss: 0.00002288
Iteration 252/1000 | Loss: 0.00002288
Iteration 253/1000 | Loss: 0.00002288
Iteration 254/1000 | Loss: 0.00002288
Iteration 255/1000 | Loss: 0.00002288
Iteration 256/1000 | Loss: 0.00002288
Iteration 257/1000 | Loss: 0.00002288
Iteration 258/1000 | Loss: 0.00002288
Iteration 259/1000 | Loss: 0.00002288
Iteration 260/1000 | Loss: 0.00002288
Iteration 261/1000 | Loss: 0.00002288
Iteration 262/1000 | Loss: 0.00002288
Iteration 263/1000 | Loss: 0.00002288
Iteration 264/1000 | Loss: 0.00002288
Iteration 265/1000 | Loss: 0.00002288
Iteration 266/1000 | Loss: 0.00002288
Iteration 267/1000 | Loss: 0.00002288
Iteration 268/1000 | Loss: 0.00002288
Iteration 269/1000 | Loss: 0.00002288
Iteration 270/1000 | Loss: 0.00002288
Iteration 271/1000 | Loss: 0.00002288
Iteration 272/1000 | Loss: 0.00002288
Iteration 273/1000 | Loss: 0.00002288
Iteration 274/1000 | Loss: 0.00002288
Iteration 275/1000 | Loss: 0.00002288
Iteration 276/1000 | Loss: 0.00002288
Iteration 277/1000 | Loss: 0.00002288
Iteration 278/1000 | Loss: 0.00002288
Iteration 279/1000 | Loss: 0.00002288
Iteration 280/1000 | Loss: 0.00002288
Iteration 281/1000 | Loss: 0.00002288
Iteration 282/1000 | Loss: 0.00002288
Iteration 283/1000 | Loss: 0.00002288
Iteration 284/1000 | Loss: 0.00002288
Iteration 285/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.2882752091391012e-05, 2.2882752091391012e-05, 2.2882752091391012e-05, 2.2882752091391012e-05, 2.2882752091391012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2882752091391012e-05

Optimization complete. Final v2v error: 4.042217254638672 mm

Highest mean error: 4.54453182220459 mm for frame 20

Lowest mean error: 3.615464687347412 mm for frame 42

Saving results

Total time: 44.22363257408142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750643
Iteration 2/25 | Loss: 0.00167384
Iteration 3/25 | Loss: 0.00148109
Iteration 4/25 | Loss: 0.00122772
Iteration 5/25 | Loss: 0.00120411
Iteration 6/25 | Loss: 0.00118810
Iteration 7/25 | Loss: 0.00120209
Iteration 8/25 | Loss: 0.00118396
Iteration 9/25 | Loss: 0.00118754
Iteration 10/25 | Loss: 0.00117930
Iteration 11/25 | Loss: 0.00117872
Iteration 12/25 | Loss: 0.00117837
Iteration 13/25 | Loss: 0.00117825
Iteration 14/25 | Loss: 0.00117824
Iteration 15/25 | Loss: 0.00117823
Iteration 16/25 | Loss: 0.00117823
Iteration 17/25 | Loss: 0.00117823
Iteration 18/25 | Loss: 0.00117823
Iteration 19/25 | Loss: 0.00117823
Iteration 20/25 | Loss: 0.00117823
Iteration 21/25 | Loss: 0.00117822
Iteration 22/25 | Loss: 0.00117822
Iteration 23/25 | Loss: 0.00117822
Iteration 24/25 | Loss: 0.00117822
Iteration 25/25 | Loss: 0.00117822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79893839
Iteration 2/25 | Loss: 0.00152653
Iteration 3/25 | Loss: 0.00152653
Iteration 4/25 | Loss: 0.00152653
Iteration 5/25 | Loss: 0.00152653
Iteration 6/25 | Loss: 0.00152653
Iteration 7/25 | Loss: 0.00152653
Iteration 8/25 | Loss: 0.00152653
Iteration 9/25 | Loss: 0.00152653
Iteration 10/25 | Loss: 0.00152652
Iteration 11/25 | Loss: 0.00152652
Iteration 12/25 | Loss: 0.00152652
Iteration 13/25 | Loss: 0.00152652
Iteration 14/25 | Loss: 0.00152652
Iteration 15/25 | Loss: 0.00152652
Iteration 16/25 | Loss: 0.00152652
Iteration 17/25 | Loss: 0.00152652
Iteration 18/25 | Loss: 0.00152652
Iteration 19/25 | Loss: 0.00152652
Iteration 20/25 | Loss: 0.00152652
Iteration 21/25 | Loss: 0.00152652
Iteration 22/25 | Loss: 0.00152652
Iteration 23/25 | Loss: 0.00152652
Iteration 24/25 | Loss: 0.00152652
Iteration 25/25 | Loss: 0.00152652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152652
Iteration 2/1000 | Loss: 0.00001928
Iteration 3/1000 | Loss: 0.00001426
Iteration 4/1000 | Loss: 0.00011851
Iteration 5/1000 | Loss: 0.00009909
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00010322
Iteration 8/1000 | Loss: 0.00018964
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001151
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001111
Iteration 17/1000 | Loss: 0.00001108
Iteration 18/1000 | Loss: 0.00001108
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001096
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001093
Iteration 44/1000 | Loss: 0.00001093
Iteration 45/1000 | Loss: 0.00001093
Iteration 46/1000 | Loss: 0.00001093
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001087
Iteration 71/1000 | Loss: 0.00001087
Iteration 72/1000 | Loss: 0.00001086
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001086
Iteration 77/1000 | Loss: 0.00001086
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001084
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001084
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001083
Iteration 94/1000 | Loss: 0.00001083
Iteration 95/1000 | Loss: 0.00001083
Iteration 96/1000 | Loss: 0.00001083
Iteration 97/1000 | Loss: 0.00001083
Iteration 98/1000 | Loss: 0.00001083
Iteration 99/1000 | Loss: 0.00001083
Iteration 100/1000 | Loss: 0.00001083
Iteration 101/1000 | Loss: 0.00001083
Iteration 102/1000 | Loss: 0.00001083
Iteration 103/1000 | Loss: 0.00001083
Iteration 104/1000 | Loss: 0.00001083
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001082
Iteration 109/1000 | Loss: 0.00001082
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001081
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001081
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001079
Iteration 125/1000 | Loss: 0.00001079
Iteration 126/1000 | Loss: 0.00001079
Iteration 127/1000 | Loss: 0.00001079
Iteration 128/1000 | Loss: 0.00001079
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001077
Iteration 137/1000 | Loss: 0.00001077
Iteration 138/1000 | Loss: 0.00001077
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001077
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001077
Iteration 146/1000 | Loss: 0.00001077
Iteration 147/1000 | Loss: 0.00001077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.0774323527584784e-05, 1.0774323527584784e-05, 1.0774323527584784e-05, 1.0774323527584784e-05, 1.0774323527584784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0774323527584784e-05

Optimization complete. Final v2v error: 2.812678098678589 mm

Highest mean error: 3.248258113861084 mm for frame 64

Lowest mean error: 2.5092263221740723 mm for frame 24

Saving results

Total time: 56.41579532623291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807488
Iteration 2/25 | Loss: 0.00142349
Iteration 3/25 | Loss: 0.00129919
Iteration 4/25 | Loss: 0.00128304
Iteration 5/25 | Loss: 0.00127851
Iteration 6/25 | Loss: 0.00127660
Iteration 7/25 | Loss: 0.00127629
Iteration 8/25 | Loss: 0.00127629
Iteration 9/25 | Loss: 0.00127629
Iteration 10/25 | Loss: 0.00127629
Iteration 11/25 | Loss: 0.00127629
Iteration 12/25 | Loss: 0.00127629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012762933038175106, 0.0012762933038175106, 0.0012762933038175106, 0.0012762933038175106, 0.0012762933038175106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012762933038175106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17715716
Iteration 2/25 | Loss: 0.00247590
Iteration 3/25 | Loss: 0.00247590
Iteration 4/25 | Loss: 0.00247590
Iteration 5/25 | Loss: 0.00247590
Iteration 6/25 | Loss: 0.00247590
Iteration 7/25 | Loss: 0.00247590
Iteration 8/25 | Loss: 0.00247590
Iteration 9/25 | Loss: 0.00247590
Iteration 10/25 | Loss: 0.00247590
Iteration 11/25 | Loss: 0.00247590
Iteration 12/25 | Loss: 0.00247590
Iteration 13/25 | Loss: 0.00247590
Iteration 14/25 | Loss: 0.00247590
Iteration 15/25 | Loss: 0.00247590
Iteration 16/25 | Loss: 0.00247590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024759003426879644, 0.0024759003426879644, 0.0024759003426879644, 0.0024759003426879644, 0.0024759003426879644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024759003426879644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247590
Iteration 2/1000 | Loss: 0.00006620
Iteration 3/1000 | Loss: 0.00004259
Iteration 4/1000 | Loss: 0.00003800
Iteration 5/1000 | Loss: 0.00003633
Iteration 6/1000 | Loss: 0.00003538
Iteration 7/1000 | Loss: 0.00003447
Iteration 8/1000 | Loss: 0.00003352
Iteration 9/1000 | Loss: 0.00003265
Iteration 10/1000 | Loss: 0.00003227
Iteration 11/1000 | Loss: 0.00003184
Iteration 12/1000 | Loss: 0.00003153
Iteration 13/1000 | Loss: 0.00003132
Iteration 14/1000 | Loss: 0.00003113
Iteration 15/1000 | Loss: 0.00003110
Iteration 16/1000 | Loss: 0.00003102
Iteration 17/1000 | Loss: 0.00003098
Iteration 18/1000 | Loss: 0.00003086
Iteration 19/1000 | Loss: 0.00003078
Iteration 20/1000 | Loss: 0.00003078
Iteration 21/1000 | Loss: 0.00003077
Iteration 22/1000 | Loss: 0.00003077
Iteration 23/1000 | Loss: 0.00003076
Iteration 24/1000 | Loss: 0.00003076
Iteration 25/1000 | Loss: 0.00003075
Iteration 26/1000 | Loss: 0.00003074
Iteration 27/1000 | Loss: 0.00003073
Iteration 28/1000 | Loss: 0.00003073
Iteration 29/1000 | Loss: 0.00003073
Iteration 30/1000 | Loss: 0.00003071
Iteration 31/1000 | Loss: 0.00003071
Iteration 32/1000 | Loss: 0.00003071
Iteration 33/1000 | Loss: 0.00003071
Iteration 34/1000 | Loss: 0.00003071
Iteration 35/1000 | Loss: 0.00003071
Iteration 36/1000 | Loss: 0.00003071
Iteration 37/1000 | Loss: 0.00003071
Iteration 38/1000 | Loss: 0.00003070
Iteration 39/1000 | Loss: 0.00003070
Iteration 40/1000 | Loss: 0.00003070
Iteration 41/1000 | Loss: 0.00003070
Iteration 42/1000 | Loss: 0.00003067
Iteration 43/1000 | Loss: 0.00003067
Iteration 44/1000 | Loss: 0.00003067
Iteration 45/1000 | Loss: 0.00003067
Iteration 46/1000 | Loss: 0.00003067
Iteration 47/1000 | Loss: 0.00003066
Iteration 48/1000 | Loss: 0.00003066
Iteration 49/1000 | Loss: 0.00003066
Iteration 50/1000 | Loss: 0.00003066
Iteration 51/1000 | Loss: 0.00003066
Iteration 52/1000 | Loss: 0.00003066
Iteration 53/1000 | Loss: 0.00003066
Iteration 54/1000 | Loss: 0.00003066
Iteration 55/1000 | Loss: 0.00003066
Iteration 56/1000 | Loss: 0.00003066
Iteration 57/1000 | Loss: 0.00003065
Iteration 58/1000 | Loss: 0.00003065
Iteration 59/1000 | Loss: 0.00003064
Iteration 60/1000 | Loss: 0.00003064
Iteration 61/1000 | Loss: 0.00003064
Iteration 62/1000 | Loss: 0.00003064
Iteration 63/1000 | Loss: 0.00003064
Iteration 64/1000 | Loss: 0.00003064
Iteration 65/1000 | Loss: 0.00003064
Iteration 66/1000 | Loss: 0.00003064
Iteration 67/1000 | Loss: 0.00003063
Iteration 68/1000 | Loss: 0.00003061
Iteration 69/1000 | Loss: 0.00003061
Iteration 70/1000 | Loss: 0.00003061
Iteration 71/1000 | Loss: 0.00003061
Iteration 72/1000 | Loss: 0.00003060
Iteration 73/1000 | Loss: 0.00003059
Iteration 74/1000 | Loss: 0.00003059
Iteration 75/1000 | Loss: 0.00003059
Iteration 76/1000 | Loss: 0.00003059
Iteration 77/1000 | Loss: 0.00003058
Iteration 78/1000 | Loss: 0.00003058
Iteration 79/1000 | Loss: 0.00003058
Iteration 80/1000 | Loss: 0.00003058
Iteration 81/1000 | Loss: 0.00003058
Iteration 82/1000 | Loss: 0.00003058
Iteration 83/1000 | Loss: 0.00003058
Iteration 84/1000 | Loss: 0.00003058
Iteration 85/1000 | Loss: 0.00003058
Iteration 86/1000 | Loss: 0.00003058
Iteration 87/1000 | Loss: 0.00003058
Iteration 88/1000 | Loss: 0.00003058
Iteration 89/1000 | Loss: 0.00003057
Iteration 90/1000 | Loss: 0.00003057
Iteration 91/1000 | Loss: 0.00003057
Iteration 92/1000 | Loss: 0.00003057
Iteration 93/1000 | Loss: 0.00003057
Iteration 94/1000 | Loss: 0.00003057
Iteration 95/1000 | Loss: 0.00003057
Iteration 96/1000 | Loss: 0.00003057
Iteration 97/1000 | Loss: 0.00003057
Iteration 98/1000 | Loss: 0.00003057
Iteration 99/1000 | Loss: 0.00003057
Iteration 100/1000 | Loss: 0.00003057
Iteration 101/1000 | Loss: 0.00003056
Iteration 102/1000 | Loss: 0.00003056
Iteration 103/1000 | Loss: 0.00003056
Iteration 104/1000 | Loss: 0.00003056
Iteration 105/1000 | Loss: 0.00003056
Iteration 106/1000 | Loss: 0.00003056
Iteration 107/1000 | Loss: 0.00003056
Iteration 108/1000 | Loss: 0.00003056
Iteration 109/1000 | Loss: 0.00003056
Iteration 110/1000 | Loss: 0.00003056
Iteration 111/1000 | Loss: 0.00003055
Iteration 112/1000 | Loss: 0.00003055
Iteration 113/1000 | Loss: 0.00003055
Iteration 114/1000 | Loss: 0.00003055
Iteration 115/1000 | Loss: 0.00003055
Iteration 116/1000 | Loss: 0.00003055
Iteration 117/1000 | Loss: 0.00003055
Iteration 118/1000 | Loss: 0.00003055
Iteration 119/1000 | Loss: 0.00003055
Iteration 120/1000 | Loss: 0.00003055
Iteration 121/1000 | Loss: 0.00003055
Iteration 122/1000 | Loss: 0.00003055
Iteration 123/1000 | Loss: 0.00003055
Iteration 124/1000 | Loss: 0.00003055
Iteration 125/1000 | Loss: 0.00003054
Iteration 126/1000 | Loss: 0.00003053
Iteration 127/1000 | Loss: 0.00003053
Iteration 128/1000 | Loss: 0.00003053
Iteration 129/1000 | Loss: 0.00003053
Iteration 130/1000 | Loss: 0.00003053
Iteration 131/1000 | Loss: 0.00003053
Iteration 132/1000 | Loss: 0.00003053
Iteration 133/1000 | Loss: 0.00003053
Iteration 134/1000 | Loss: 0.00003052
Iteration 135/1000 | Loss: 0.00003052
Iteration 136/1000 | Loss: 0.00003052
Iteration 137/1000 | Loss: 0.00003052
Iteration 138/1000 | Loss: 0.00003052
Iteration 139/1000 | Loss: 0.00003052
Iteration 140/1000 | Loss: 0.00003052
Iteration 141/1000 | Loss: 0.00003052
Iteration 142/1000 | Loss: 0.00003052
Iteration 143/1000 | Loss: 0.00003052
Iteration 144/1000 | Loss: 0.00003052
Iteration 145/1000 | Loss: 0.00003052
Iteration 146/1000 | Loss: 0.00003052
Iteration 147/1000 | Loss: 0.00003051
Iteration 148/1000 | Loss: 0.00003051
Iteration 149/1000 | Loss: 0.00003051
Iteration 150/1000 | Loss: 0.00003051
Iteration 151/1000 | Loss: 0.00003051
Iteration 152/1000 | Loss: 0.00003051
Iteration 153/1000 | Loss: 0.00003051
Iteration 154/1000 | Loss: 0.00003051
Iteration 155/1000 | Loss: 0.00003051
Iteration 156/1000 | Loss: 0.00003051
Iteration 157/1000 | Loss: 0.00003051
Iteration 158/1000 | Loss: 0.00003051
Iteration 159/1000 | Loss: 0.00003051
Iteration 160/1000 | Loss: 0.00003051
Iteration 161/1000 | Loss: 0.00003051
Iteration 162/1000 | Loss: 0.00003051
Iteration 163/1000 | Loss: 0.00003051
Iteration 164/1000 | Loss: 0.00003051
Iteration 165/1000 | Loss: 0.00003051
Iteration 166/1000 | Loss: 0.00003051
Iteration 167/1000 | Loss: 0.00003051
Iteration 168/1000 | Loss: 0.00003051
Iteration 169/1000 | Loss: 0.00003051
Iteration 170/1000 | Loss: 0.00003051
Iteration 171/1000 | Loss: 0.00003051
Iteration 172/1000 | Loss: 0.00003051
Iteration 173/1000 | Loss: 0.00003051
Iteration 174/1000 | Loss: 0.00003051
Iteration 175/1000 | Loss: 0.00003051
Iteration 176/1000 | Loss: 0.00003051
Iteration 177/1000 | Loss: 0.00003051
Iteration 178/1000 | Loss: 0.00003051
Iteration 179/1000 | Loss: 0.00003051
Iteration 180/1000 | Loss: 0.00003051
Iteration 181/1000 | Loss: 0.00003051
Iteration 182/1000 | Loss: 0.00003051
Iteration 183/1000 | Loss: 0.00003051
Iteration 184/1000 | Loss: 0.00003051
Iteration 185/1000 | Loss: 0.00003051
Iteration 186/1000 | Loss: 0.00003051
Iteration 187/1000 | Loss: 0.00003051
Iteration 188/1000 | Loss: 0.00003051
Iteration 189/1000 | Loss: 0.00003051
Iteration 190/1000 | Loss: 0.00003051
Iteration 191/1000 | Loss: 0.00003051
Iteration 192/1000 | Loss: 0.00003051
Iteration 193/1000 | Loss: 0.00003051
Iteration 194/1000 | Loss: 0.00003051
Iteration 195/1000 | Loss: 0.00003051
Iteration 196/1000 | Loss: 0.00003051
Iteration 197/1000 | Loss: 0.00003051
Iteration 198/1000 | Loss: 0.00003051
Iteration 199/1000 | Loss: 0.00003051
Iteration 200/1000 | Loss: 0.00003051
Iteration 201/1000 | Loss: 0.00003051
Iteration 202/1000 | Loss: 0.00003051
Iteration 203/1000 | Loss: 0.00003051
Iteration 204/1000 | Loss: 0.00003051
Iteration 205/1000 | Loss: 0.00003051
Iteration 206/1000 | Loss: 0.00003051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.0508977943100035e-05, 3.0508977943100035e-05, 3.0508977943100035e-05, 3.0508977943100035e-05, 3.0508977943100035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0508977943100035e-05

Optimization complete. Final v2v error: 4.450131416320801 mm

Highest mean error: 4.527074337005615 mm for frame 29

Lowest mean error: 4.353984832763672 mm for frame 61

Saving results

Total time: 42.63530707359314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049810
Iteration 2/25 | Loss: 0.01049810
Iteration 3/25 | Loss: 0.01049810
Iteration 4/25 | Loss: 0.00260845
Iteration 5/25 | Loss: 0.00170395
Iteration 6/25 | Loss: 0.00160796
Iteration 7/25 | Loss: 0.00156501
Iteration 8/25 | Loss: 0.00154540
Iteration 9/25 | Loss: 0.00150671
Iteration 10/25 | Loss: 0.00145527
Iteration 11/25 | Loss: 0.00143097
Iteration 12/25 | Loss: 0.00140563
Iteration 13/25 | Loss: 0.00138570
Iteration 14/25 | Loss: 0.00137725
Iteration 15/25 | Loss: 0.00136588
Iteration 16/25 | Loss: 0.00134589
Iteration 17/25 | Loss: 0.00133225
Iteration 18/25 | Loss: 0.00132674
Iteration 19/25 | Loss: 0.00132479
Iteration 20/25 | Loss: 0.00132368
Iteration 21/25 | Loss: 0.00131801
Iteration 22/25 | Loss: 0.00131603
Iteration 23/25 | Loss: 0.00131556
Iteration 24/25 | Loss: 0.00131539
Iteration 25/25 | Loss: 0.00131531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20930660
Iteration 2/25 | Loss: 0.00354354
Iteration 3/25 | Loss: 0.00222332
Iteration 4/25 | Loss: 0.00222332
Iteration 5/25 | Loss: 0.00222332
Iteration 6/25 | Loss: 0.00222332
Iteration 7/25 | Loss: 0.00222332
Iteration 8/25 | Loss: 0.00222332
Iteration 9/25 | Loss: 0.00222332
Iteration 10/25 | Loss: 0.00222332
Iteration 11/25 | Loss: 0.00222332
Iteration 12/25 | Loss: 0.00222332
Iteration 13/25 | Loss: 0.00222332
Iteration 14/25 | Loss: 0.00222332
Iteration 15/25 | Loss: 0.00222332
Iteration 16/25 | Loss: 0.00222332
Iteration 17/25 | Loss: 0.00222332
Iteration 18/25 | Loss: 0.00222332
Iteration 19/25 | Loss: 0.00222332
Iteration 20/25 | Loss: 0.00222332
Iteration 21/25 | Loss: 0.00222332
Iteration 22/25 | Loss: 0.00222332
Iteration 23/25 | Loss: 0.00222332
Iteration 24/25 | Loss: 0.00222332
Iteration 25/25 | Loss: 0.00222332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222332
Iteration 2/1000 | Loss: 0.00082609
Iteration 3/1000 | Loss: 0.00147363
Iteration 4/1000 | Loss: 0.00165280
Iteration 5/1000 | Loss: 0.00017116
Iteration 6/1000 | Loss: 0.00018531
Iteration 7/1000 | Loss: 0.00021023
Iteration 8/1000 | Loss: 0.00028807
Iteration 9/1000 | Loss: 0.00018559
Iteration 10/1000 | Loss: 0.00016437
Iteration 11/1000 | Loss: 0.00047013
Iteration 12/1000 | Loss: 0.00026486
Iteration 13/1000 | Loss: 0.00447076
Iteration 14/1000 | Loss: 0.00559824
Iteration 15/1000 | Loss: 0.00767527
Iteration 16/1000 | Loss: 0.01035180
Iteration 17/1000 | Loss: 0.00323320
Iteration 18/1000 | Loss: 0.00312079
Iteration 19/1000 | Loss: 0.00253588
Iteration 20/1000 | Loss: 0.00155131
Iteration 21/1000 | Loss: 0.00033481
Iteration 22/1000 | Loss: 0.00111112
Iteration 23/1000 | Loss: 0.00126020
Iteration 24/1000 | Loss: 0.00110667
Iteration 25/1000 | Loss: 0.00205200
Iteration 26/1000 | Loss: 0.00153628
Iteration 27/1000 | Loss: 0.00151963
Iteration 28/1000 | Loss: 0.00137947
Iteration 29/1000 | Loss: 0.00493494
Iteration 30/1000 | Loss: 0.00117195
Iteration 31/1000 | Loss: 0.00179457
Iteration 32/1000 | Loss: 0.00234654
Iteration 33/1000 | Loss: 0.00147793
Iteration 34/1000 | Loss: 0.00281655
Iteration 35/1000 | Loss: 0.00226238
Iteration 36/1000 | Loss: 0.00352810
Iteration 37/1000 | Loss: 0.00346789
Iteration 38/1000 | Loss: 0.00110687
Iteration 39/1000 | Loss: 0.00079210
Iteration 40/1000 | Loss: 0.00073870
Iteration 41/1000 | Loss: 0.00442285
Iteration 42/1000 | Loss: 0.00167007
Iteration 43/1000 | Loss: 0.00094446
Iteration 44/1000 | Loss: 0.00045754
Iteration 45/1000 | Loss: 0.00093029
Iteration 46/1000 | Loss: 0.00074289
Iteration 47/1000 | Loss: 0.00107236
Iteration 48/1000 | Loss: 0.00436720
Iteration 49/1000 | Loss: 0.00172512
Iteration 50/1000 | Loss: 0.00131647
Iteration 51/1000 | Loss: 0.00139505
Iteration 52/1000 | Loss: 0.00133910
Iteration 53/1000 | Loss: 0.00141461
Iteration 54/1000 | Loss: 0.00090273
Iteration 55/1000 | Loss: 0.00274180
Iteration 56/1000 | Loss: 0.00116605
Iteration 57/1000 | Loss: 0.00104041
Iteration 58/1000 | Loss: 0.00143550
Iteration 59/1000 | Loss: 0.00125847
Iteration 60/1000 | Loss: 0.00116988
Iteration 61/1000 | Loss: 0.00170652
Iteration 62/1000 | Loss: 0.00092220
Iteration 63/1000 | Loss: 0.00101605
Iteration 64/1000 | Loss: 0.00100346
Iteration 65/1000 | Loss: 0.00094104
Iteration 66/1000 | Loss: 0.00084472
Iteration 67/1000 | Loss: 0.00077848
Iteration 68/1000 | Loss: 0.00146787
Iteration 69/1000 | Loss: 0.00150646
Iteration 70/1000 | Loss: 0.00082736
Iteration 71/1000 | Loss: 0.00304899
Iteration 72/1000 | Loss: 0.00135466
Iteration 73/1000 | Loss: 0.00197354
Iteration 74/1000 | Loss: 0.00017208
Iteration 75/1000 | Loss: 0.00116214
Iteration 76/1000 | Loss: 0.00115805
Iteration 77/1000 | Loss: 0.00039214
Iteration 78/1000 | Loss: 0.00030820
Iteration 79/1000 | Loss: 0.00044994
Iteration 80/1000 | Loss: 0.00115172
Iteration 81/1000 | Loss: 0.00009468
Iteration 82/1000 | Loss: 0.00004839
Iteration 83/1000 | Loss: 0.00006102
Iteration 84/1000 | Loss: 0.00003575
Iteration 85/1000 | Loss: 0.00012229
Iteration 86/1000 | Loss: 0.00003032
Iteration 87/1000 | Loss: 0.00012884
Iteration 88/1000 | Loss: 0.00034098
Iteration 89/1000 | Loss: 0.00025839
Iteration 90/1000 | Loss: 0.00002673
Iteration 91/1000 | Loss: 0.00029816
Iteration 92/1000 | Loss: 0.00018987
Iteration 93/1000 | Loss: 0.00039176
Iteration 94/1000 | Loss: 0.00054345
Iteration 95/1000 | Loss: 0.00003053
Iteration 96/1000 | Loss: 0.00016453
Iteration 97/1000 | Loss: 0.00012073
Iteration 98/1000 | Loss: 0.00006182
Iteration 99/1000 | Loss: 0.00030893
Iteration 100/1000 | Loss: 0.00004611
Iteration 101/1000 | Loss: 0.00027249
Iteration 102/1000 | Loss: 0.00018437
Iteration 103/1000 | Loss: 0.00017170
Iteration 104/1000 | Loss: 0.00024322
Iteration 105/1000 | Loss: 0.00004953
Iteration 106/1000 | Loss: 0.00024481
Iteration 107/1000 | Loss: 0.00081621
Iteration 108/1000 | Loss: 0.00064687
Iteration 109/1000 | Loss: 0.00026860
Iteration 110/1000 | Loss: 0.00003308
Iteration 111/1000 | Loss: 0.00019453
Iteration 112/1000 | Loss: 0.00030350
Iteration 113/1000 | Loss: 0.00026011
Iteration 114/1000 | Loss: 0.00022574
Iteration 115/1000 | Loss: 0.00004016
Iteration 116/1000 | Loss: 0.00031464
Iteration 117/1000 | Loss: 0.00020267
Iteration 118/1000 | Loss: 0.00031579
Iteration 119/1000 | Loss: 0.00011569
Iteration 120/1000 | Loss: 0.00009364
Iteration 121/1000 | Loss: 0.00033522
Iteration 122/1000 | Loss: 0.00026559
Iteration 123/1000 | Loss: 0.00022464
Iteration 124/1000 | Loss: 0.00030546
Iteration 125/1000 | Loss: 0.00045364
Iteration 126/1000 | Loss: 0.00003345
Iteration 127/1000 | Loss: 0.00041574
Iteration 128/1000 | Loss: 0.00047959
Iteration 129/1000 | Loss: 0.00002544
Iteration 130/1000 | Loss: 0.00002269
Iteration 131/1000 | Loss: 0.00002214
Iteration 132/1000 | Loss: 0.00023613
Iteration 133/1000 | Loss: 0.00008845
Iteration 134/1000 | Loss: 0.00025683
Iteration 135/1000 | Loss: 0.00015149
Iteration 136/1000 | Loss: 0.00002876
Iteration 137/1000 | Loss: 0.00032641
Iteration 138/1000 | Loss: 0.00028876
Iteration 139/1000 | Loss: 0.00010439
Iteration 140/1000 | Loss: 0.00019638
Iteration 141/1000 | Loss: 0.00055597
Iteration 142/1000 | Loss: 0.00019145
Iteration 143/1000 | Loss: 0.00016525
Iteration 144/1000 | Loss: 0.00022670
Iteration 145/1000 | Loss: 0.00024842
Iteration 146/1000 | Loss: 0.00023200
Iteration 147/1000 | Loss: 0.00007486
Iteration 148/1000 | Loss: 0.00034628
Iteration 149/1000 | Loss: 0.00019267
Iteration 150/1000 | Loss: 0.00027593
Iteration 151/1000 | Loss: 0.00024585
Iteration 152/1000 | Loss: 0.00028312
Iteration 153/1000 | Loss: 0.00037993
Iteration 154/1000 | Loss: 0.00021625
Iteration 155/1000 | Loss: 0.00033001
Iteration 156/1000 | Loss: 0.00057277
Iteration 157/1000 | Loss: 0.00047934
Iteration 158/1000 | Loss: 0.00019604
Iteration 159/1000 | Loss: 0.00026857
Iteration 160/1000 | Loss: 0.00060290
Iteration 161/1000 | Loss: 0.00041218
Iteration 162/1000 | Loss: 0.00008353
Iteration 163/1000 | Loss: 0.00005919
Iteration 164/1000 | Loss: 0.00020748
Iteration 165/1000 | Loss: 0.00027911
Iteration 166/1000 | Loss: 0.00028781
Iteration 167/1000 | Loss: 0.00025175
Iteration 168/1000 | Loss: 0.00040044
Iteration 169/1000 | Loss: 0.00027544
Iteration 170/1000 | Loss: 0.00051392
Iteration 171/1000 | Loss: 0.00034673
Iteration 172/1000 | Loss: 0.00041891
Iteration 173/1000 | Loss: 0.00060105
Iteration 174/1000 | Loss: 0.00018452
Iteration 175/1000 | Loss: 0.00003781
Iteration 176/1000 | Loss: 0.00008468
Iteration 177/1000 | Loss: 0.00002429
Iteration 178/1000 | Loss: 0.00018774
Iteration 179/1000 | Loss: 0.00011686
Iteration 180/1000 | Loss: 0.00035417
Iteration 181/1000 | Loss: 0.00006092
Iteration 182/1000 | Loss: 0.00029826
Iteration 183/1000 | Loss: 0.00019415
Iteration 184/1000 | Loss: 0.00062730
Iteration 185/1000 | Loss: 0.00033909
Iteration 186/1000 | Loss: 0.00031434
Iteration 187/1000 | Loss: 0.00052088
Iteration 188/1000 | Loss: 0.00047270
Iteration 189/1000 | Loss: 0.00045837
Iteration 190/1000 | Loss: 0.00033053
Iteration 191/1000 | Loss: 0.00003464
Iteration 192/1000 | Loss: 0.00022017
Iteration 193/1000 | Loss: 0.00008111
Iteration 194/1000 | Loss: 0.00005426
Iteration 195/1000 | Loss: 0.00003649
Iteration 196/1000 | Loss: 0.00048543
Iteration 197/1000 | Loss: 0.00031591
Iteration 198/1000 | Loss: 0.00024434
Iteration 199/1000 | Loss: 0.00002210
Iteration 200/1000 | Loss: 0.00021695
Iteration 201/1000 | Loss: 0.00045824
Iteration 202/1000 | Loss: 0.00216672
Iteration 203/1000 | Loss: 0.00022465
Iteration 204/1000 | Loss: 0.00004079
Iteration 205/1000 | Loss: 0.00044493
Iteration 206/1000 | Loss: 0.00034431
Iteration 207/1000 | Loss: 0.00252474
Iteration 208/1000 | Loss: 0.00060217
Iteration 209/1000 | Loss: 0.00154411
Iteration 210/1000 | Loss: 0.00031905
Iteration 211/1000 | Loss: 0.00011365
Iteration 212/1000 | Loss: 0.00051384
Iteration 213/1000 | Loss: 0.00049619
Iteration 214/1000 | Loss: 0.00020696
Iteration 215/1000 | Loss: 0.00053603
Iteration 216/1000 | Loss: 0.00036269
Iteration 217/1000 | Loss: 0.00033244
Iteration 218/1000 | Loss: 0.00021196
Iteration 219/1000 | Loss: 0.00018502
Iteration 220/1000 | Loss: 0.00038525
Iteration 221/1000 | Loss: 0.00058779
Iteration 222/1000 | Loss: 0.00043145
Iteration 223/1000 | Loss: 0.00035040
Iteration 224/1000 | Loss: 0.00173523
Iteration 225/1000 | Loss: 0.00028756
Iteration 226/1000 | Loss: 0.00044785
Iteration 227/1000 | Loss: 0.00035222
Iteration 228/1000 | Loss: 0.00045050
Iteration 229/1000 | Loss: 0.00015008
Iteration 230/1000 | Loss: 0.00024938
Iteration 231/1000 | Loss: 0.00043240
Iteration 232/1000 | Loss: 0.00194316
Iteration 233/1000 | Loss: 0.00012327
Iteration 234/1000 | Loss: 0.00027671
Iteration 235/1000 | Loss: 0.00003834
Iteration 236/1000 | Loss: 0.00002640
Iteration 237/1000 | Loss: 0.00002345
Iteration 238/1000 | Loss: 0.00002203
Iteration 239/1000 | Loss: 0.00002074
Iteration 240/1000 | Loss: 0.00001964
Iteration 241/1000 | Loss: 0.00001880
Iteration 242/1000 | Loss: 0.00001778
Iteration 243/1000 | Loss: 0.00001682
Iteration 244/1000 | Loss: 0.00001614
Iteration 245/1000 | Loss: 0.00001563
Iteration 246/1000 | Loss: 0.00001525
Iteration 247/1000 | Loss: 0.00001511
Iteration 248/1000 | Loss: 0.00001506
Iteration 249/1000 | Loss: 0.00001506
Iteration 250/1000 | Loss: 0.00002406
Iteration 251/1000 | Loss: 0.00001569
Iteration 252/1000 | Loss: 0.00001533
Iteration 253/1000 | Loss: 0.00001493
Iteration 254/1000 | Loss: 0.00001485
Iteration 255/1000 | Loss: 0.00001483
Iteration 256/1000 | Loss: 0.00001481
Iteration 257/1000 | Loss: 0.00001479
Iteration 258/1000 | Loss: 0.00001475
Iteration 259/1000 | Loss: 0.00001473
Iteration 260/1000 | Loss: 0.00001473
Iteration 261/1000 | Loss: 0.00001472
Iteration 262/1000 | Loss: 0.00001472
Iteration 263/1000 | Loss: 0.00001472
Iteration 264/1000 | Loss: 0.00001470
Iteration 265/1000 | Loss: 0.00001470
Iteration 266/1000 | Loss: 0.00001469
Iteration 267/1000 | Loss: 0.00001469
Iteration 268/1000 | Loss: 0.00001469
Iteration 269/1000 | Loss: 0.00001468
Iteration 270/1000 | Loss: 0.00001468
Iteration 271/1000 | Loss: 0.00001467
Iteration 272/1000 | Loss: 0.00001467
Iteration 273/1000 | Loss: 0.00001466
Iteration 274/1000 | Loss: 0.00001466
Iteration 275/1000 | Loss: 0.00001466
Iteration 276/1000 | Loss: 0.00001465
Iteration 277/1000 | Loss: 0.00001465
Iteration 278/1000 | Loss: 0.00001464
Iteration 279/1000 | Loss: 0.00001464
Iteration 280/1000 | Loss: 0.00001464
Iteration 281/1000 | Loss: 0.00001464
Iteration 282/1000 | Loss: 0.00001464
Iteration 283/1000 | Loss: 0.00001464
Iteration 284/1000 | Loss: 0.00001464
Iteration 285/1000 | Loss: 0.00001464
Iteration 286/1000 | Loss: 0.00001464
Iteration 287/1000 | Loss: 0.00001463
Iteration 288/1000 | Loss: 0.00001463
Iteration 289/1000 | Loss: 0.00001463
Iteration 290/1000 | Loss: 0.00001463
Iteration 291/1000 | Loss: 0.00001463
Iteration 292/1000 | Loss: 0.00001460
Iteration 293/1000 | Loss: 0.00001460
Iteration 294/1000 | Loss: 0.00001459
Iteration 295/1000 | Loss: 0.00001458
Iteration 296/1000 | Loss: 0.00001457
Iteration 297/1000 | Loss: 0.00001457
Iteration 298/1000 | Loss: 0.00001456
Iteration 299/1000 | Loss: 0.00001455
Iteration 300/1000 | Loss: 0.00001455
Iteration 301/1000 | Loss: 0.00001454
Iteration 302/1000 | Loss: 0.00001454
Iteration 303/1000 | Loss: 0.00001453
Iteration 304/1000 | Loss: 0.00001453
Iteration 305/1000 | Loss: 0.00001453
Iteration 306/1000 | Loss: 0.00001453
Iteration 307/1000 | Loss: 0.00001452
Iteration 308/1000 | Loss: 0.00001450
Iteration 309/1000 | Loss: 0.00006883
Iteration 310/1000 | Loss: 0.00002231
Iteration 311/1000 | Loss: 0.00001649
Iteration 312/1000 | Loss: 0.00001466
Iteration 313/1000 | Loss: 0.00001447
Iteration 314/1000 | Loss: 0.00001446
Iteration 315/1000 | Loss: 0.00001446
Iteration 316/1000 | Loss: 0.00001446
Iteration 317/1000 | Loss: 0.00001446
Iteration 318/1000 | Loss: 0.00001446
Iteration 319/1000 | Loss: 0.00001446
Iteration 320/1000 | Loss: 0.00001446
Iteration 321/1000 | Loss: 0.00001446
Iteration 322/1000 | Loss: 0.00001446
Iteration 323/1000 | Loss: 0.00001446
Iteration 324/1000 | Loss: 0.00001446
Iteration 325/1000 | Loss: 0.00001446
Iteration 326/1000 | Loss: 0.00001445
Iteration 327/1000 | Loss: 0.00001445
Iteration 328/1000 | Loss: 0.00001445
Iteration 329/1000 | Loss: 0.00001445
Iteration 330/1000 | Loss: 0.00001445
Iteration 331/1000 | Loss: 0.00001444
Iteration 332/1000 | Loss: 0.00001444
Iteration 333/1000 | Loss: 0.00001443
Iteration 334/1000 | Loss: 0.00001443
Iteration 335/1000 | Loss: 0.00001443
Iteration 336/1000 | Loss: 0.00001443
Iteration 337/1000 | Loss: 0.00001443
Iteration 338/1000 | Loss: 0.00001443
Iteration 339/1000 | Loss: 0.00001443
Iteration 340/1000 | Loss: 0.00001443
Iteration 341/1000 | Loss: 0.00001443
Iteration 342/1000 | Loss: 0.00001443
Iteration 343/1000 | Loss: 0.00001442
Iteration 344/1000 | Loss: 0.00001442
Iteration 345/1000 | Loss: 0.00001442
Iteration 346/1000 | Loss: 0.00001441
Iteration 347/1000 | Loss: 0.00001441
Iteration 348/1000 | Loss: 0.00001441
Iteration 349/1000 | Loss: 0.00001441
Iteration 350/1000 | Loss: 0.00001440
Iteration 351/1000 | Loss: 0.00001440
Iteration 352/1000 | Loss: 0.00001440
Iteration 353/1000 | Loss: 0.00001440
Iteration 354/1000 | Loss: 0.00001440
Iteration 355/1000 | Loss: 0.00001440
Iteration 356/1000 | Loss: 0.00001440
Iteration 357/1000 | Loss: 0.00001440
Iteration 358/1000 | Loss: 0.00001440
Iteration 359/1000 | Loss: 0.00001440
Iteration 360/1000 | Loss: 0.00001440
Iteration 361/1000 | Loss: 0.00001440
Iteration 362/1000 | Loss: 0.00001440
Iteration 363/1000 | Loss: 0.00001439
Iteration 364/1000 | Loss: 0.00001737
Iteration 365/1000 | Loss: 0.00010736
Iteration 366/1000 | Loss: 0.00001803
Iteration 367/1000 | Loss: 0.00001591
Iteration 368/1000 | Loss: 0.00001555
Iteration 369/1000 | Loss: 0.00001442
Iteration 370/1000 | Loss: 0.00001438
Iteration 371/1000 | Loss: 0.00001450
Iteration 372/1000 | Loss: 0.00001436
Iteration 373/1000 | Loss: 0.00001436
Iteration 374/1000 | Loss: 0.00001436
Iteration 375/1000 | Loss: 0.00001436
Iteration 376/1000 | Loss: 0.00001436
Iteration 377/1000 | Loss: 0.00001436
Iteration 378/1000 | Loss: 0.00001436
Iteration 379/1000 | Loss: 0.00001436
Iteration 380/1000 | Loss: 0.00001436
Iteration 381/1000 | Loss: 0.00001435
Iteration 382/1000 | Loss: 0.00001435
Iteration 383/1000 | Loss: 0.00001435
Iteration 384/1000 | Loss: 0.00001435
Iteration 385/1000 | Loss: 0.00001435
Iteration 386/1000 | Loss: 0.00001435
Iteration 387/1000 | Loss: 0.00001435
Iteration 388/1000 | Loss: 0.00001435
Iteration 389/1000 | Loss: 0.00001435
Iteration 390/1000 | Loss: 0.00001435
Iteration 391/1000 | Loss: 0.00001435
Iteration 392/1000 | Loss: 0.00001435
Iteration 393/1000 | Loss: 0.00001435
Iteration 394/1000 | Loss: 0.00001435
Iteration 395/1000 | Loss: 0.00001435
Iteration 396/1000 | Loss: 0.00001435
Iteration 397/1000 | Loss: 0.00001435
Iteration 398/1000 | Loss: 0.00001435
Iteration 399/1000 | Loss: 0.00001435
Iteration 400/1000 | Loss: 0.00001435
Iteration 401/1000 | Loss: 0.00001435
Iteration 402/1000 | Loss: 0.00001435
Iteration 403/1000 | Loss: 0.00001435
Iteration 404/1000 | Loss: 0.00001435
Iteration 405/1000 | Loss: 0.00001435
Iteration 406/1000 | Loss: 0.00001435
Iteration 407/1000 | Loss: 0.00001435
Iteration 408/1000 | Loss: 0.00001435
Iteration 409/1000 | Loss: 0.00001435
Iteration 410/1000 | Loss: 0.00001435
Iteration 411/1000 | Loss: 0.00001435
Iteration 412/1000 | Loss: 0.00001435
Iteration 413/1000 | Loss: 0.00001435
Iteration 414/1000 | Loss: 0.00001435
Iteration 415/1000 | Loss: 0.00001435
Iteration 416/1000 | Loss: 0.00001435
Iteration 417/1000 | Loss: 0.00001435
Iteration 418/1000 | Loss: 0.00001435
Iteration 419/1000 | Loss: 0.00001435
Iteration 420/1000 | Loss: 0.00001435
Iteration 421/1000 | Loss: 0.00001435
Iteration 422/1000 | Loss: 0.00001435
Iteration 423/1000 | Loss: 0.00001435
Iteration 424/1000 | Loss: 0.00001435
Iteration 425/1000 | Loss: 0.00001435
Iteration 426/1000 | Loss: 0.00001435
Iteration 427/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 427. Stopping optimization.
Last 5 losses: [1.4348637705552392e-05, 1.4348637705552392e-05, 1.4348637705552392e-05, 1.4348637705552392e-05, 1.4348637705552392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4348637705552392e-05

Optimization complete. Final v2v error: 3.2119107246398926 mm

Highest mean error: 4.70717191696167 mm for frame 168

Lowest mean error: 2.727236032485962 mm for frame 229

Saving results

Total time: 478.1579432487488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00561000
Iteration 2/25 | Loss: 0.00126092
Iteration 3/25 | Loss: 0.00119272
Iteration 4/25 | Loss: 0.00118172
Iteration 5/25 | Loss: 0.00117858
Iteration 6/25 | Loss: 0.00117858
Iteration 7/25 | Loss: 0.00117858
Iteration 8/25 | Loss: 0.00117858
Iteration 9/25 | Loss: 0.00117858
Iteration 10/25 | Loss: 0.00117858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011785790557041764, 0.0011785790557041764, 0.0011785790557041764, 0.0011785790557041764, 0.0011785790557041764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011785790557041764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94785678
Iteration 2/25 | Loss: 0.00086820
Iteration 3/25 | Loss: 0.00086820
Iteration 4/25 | Loss: 0.00086820
Iteration 5/25 | Loss: 0.00086820
Iteration 6/25 | Loss: 0.00086820
Iteration 7/25 | Loss: 0.00086820
Iteration 8/25 | Loss: 0.00086820
Iteration 9/25 | Loss: 0.00086820
Iteration 10/25 | Loss: 0.00086820
Iteration 11/25 | Loss: 0.00086820
Iteration 12/25 | Loss: 0.00086820
Iteration 13/25 | Loss: 0.00086820
Iteration 14/25 | Loss: 0.00086820
Iteration 15/25 | Loss: 0.00086820
Iteration 16/25 | Loss: 0.00086820
Iteration 17/25 | Loss: 0.00086820
Iteration 18/25 | Loss: 0.00086820
Iteration 19/25 | Loss: 0.00086820
Iteration 20/25 | Loss: 0.00086820
Iteration 21/25 | Loss: 0.00086820
Iteration 22/25 | Loss: 0.00086820
Iteration 23/25 | Loss: 0.00086820
Iteration 24/25 | Loss: 0.00086820
Iteration 25/25 | Loss: 0.00086820

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086820
Iteration 2/1000 | Loss: 0.00003611
Iteration 3/1000 | Loss: 0.00003156
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002771
Iteration 6/1000 | Loss: 0.00002678
Iteration 7/1000 | Loss: 0.00002617
Iteration 8/1000 | Loss: 0.00002571
Iteration 9/1000 | Loss: 0.00002540
Iteration 10/1000 | Loss: 0.00002523
Iteration 11/1000 | Loss: 0.00002495
Iteration 12/1000 | Loss: 0.00002484
Iteration 13/1000 | Loss: 0.00002464
Iteration 14/1000 | Loss: 0.00002450
Iteration 15/1000 | Loss: 0.00002450
Iteration 16/1000 | Loss: 0.00002438
Iteration 17/1000 | Loss: 0.00002438
Iteration 18/1000 | Loss: 0.00002438
Iteration 19/1000 | Loss: 0.00002437
Iteration 20/1000 | Loss: 0.00002437
Iteration 21/1000 | Loss: 0.00002437
Iteration 22/1000 | Loss: 0.00002437
Iteration 23/1000 | Loss: 0.00002437
Iteration 24/1000 | Loss: 0.00002437
Iteration 25/1000 | Loss: 0.00002437
Iteration 26/1000 | Loss: 0.00002437
Iteration 27/1000 | Loss: 0.00002437
Iteration 28/1000 | Loss: 0.00002437
Iteration 29/1000 | Loss: 0.00002437
Iteration 30/1000 | Loss: 0.00002437
Iteration 31/1000 | Loss: 0.00002437
Iteration 32/1000 | Loss: 0.00002436
Iteration 33/1000 | Loss: 0.00002436
Iteration 34/1000 | Loss: 0.00002436
Iteration 35/1000 | Loss: 0.00002436
Iteration 36/1000 | Loss: 0.00002436
Iteration 37/1000 | Loss: 0.00002434
Iteration 38/1000 | Loss: 0.00002434
Iteration 39/1000 | Loss: 0.00002433
Iteration 40/1000 | Loss: 0.00002433
Iteration 41/1000 | Loss: 0.00002433
Iteration 42/1000 | Loss: 0.00002433
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002432
Iteration 45/1000 | Loss: 0.00002432
Iteration 46/1000 | Loss: 0.00002432
Iteration 47/1000 | Loss: 0.00002432
Iteration 48/1000 | Loss: 0.00002431
Iteration 49/1000 | Loss: 0.00002431
Iteration 50/1000 | Loss: 0.00002431
Iteration 51/1000 | Loss: 0.00002431
Iteration 52/1000 | Loss: 0.00002430
Iteration 53/1000 | Loss: 0.00002430
Iteration 54/1000 | Loss: 0.00002430
Iteration 55/1000 | Loss: 0.00002430
Iteration 56/1000 | Loss: 0.00002430
Iteration 57/1000 | Loss: 0.00002430
Iteration 58/1000 | Loss: 0.00002430
Iteration 59/1000 | Loss: 0.00002430
Iteration 60/1000 | Loss: 0.00002430
Iteration 61/1000 | Loss: 0.00002430
Iteration 62/1000 | Loss: 0.00002429
Iteration 63/1000 | Loss: 0.00002429
Iteration 64/1000 | Loss: 0.00002428
Iteration 65/1000 | Loss: 0.00002428
Iteration 66/1000 | Loss: 0.00002428
Iteration 67/1000 | Loss: 0.00002427
Iteration 68/1000 | Loss: 0.00002427
Iteration 69/1000 | Loss: 0.00002427
Iteration 70/1000 | Loss: 0.00002426
Iteration 71/1000 | Loss: 0.00002426
Iteration 72/1000 | Loss: 0.00002426
Iteration 73/1000 | Loss: 0.00002426
Iteration 74/1000 | Loss: 0.00002425
Iteration 75/1000 | Loss: 0.00002425
Iteration 76/1000 | Loss: 0.00002425
Iteration 77/1000 | Loss: 0.00002425
Iteration 78/1000 | Loss: 0.00002425
Iteration 79/1000 | Loss: 0.00002425
Iteration 80/1000 | Loss: 0.00002425
Iteration 81/1000 | Loss: 0.00002425
Iteration 82/1000 | Loss: 0.00002424
Iteration 83/1000 | Loss: 0.00002424
Iteration 84/1000 | Loss: 0.00002424
Iteration 85/1000 | Loss: 0.00002419
Iteration 86/1000 | Loss: 0.00002419
Iteration 87/1000 | Loss: 0.00002418
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002415
Iteration 90/1000 | Loss: 0.00002415
Iteration 91/1000 | Loss: 0.00002411
Iteration 92/1000 | Loss: 0.00002410
Iteration 93/1000 | Loss: 0.00002409
Iteration 94/1000 | Loss: 0.00002408
Iteration 95/1000 | Loss: 0.00002408
Iteration 96/1000 | Loss: 0.00002408
Iteration 97/1000 | Loss: 0.00002408
Iteration 98/1000 | Loss: 0.00002407
Iteration 99/1000 | Loss: 0.00002405
Iteration 100/1000 | Loss: 0.00002405
Iteration 101/1000 | Loss: 0.00002405
Iteration 102/1000 | Loss: 0.00002405
Iteration 103/1000 | Loss: 0.00002404
Iteration 104/1000 | Loss: 0.00002404
Iteration 105/1000 | Loss: 0.00002404
Iteration 106/1000 | Loss: 0.00002404
Iteration 107/1000 | Loss: 0.00002404
Iteration 108/1000 | Loss: 0.00002404
Iteration 109/1000 | Loss: 0.00002404
Iteration 110/1000 | Loss: 0.00002403
Iteration 111/1000 | Loss: 0.00002402
Iteration 112/1000 | Loss: 0.00002402
Iteration 113/1000 | Loss: 0.00002402
Iteration 114/1000 | Loss: 0.00002402
Iteration 115/1000 | Loss: 0.00002401
Iteration 116/1000 | Loss: 0.00002401
Iteration 117/1000 | Loss: 0.00002401
Iteration 118/1000 | Loss: 0.00002401
Iteration 119/1000 | Loss: 0.00002401
Iteration 120/1000 | Loss: 0.00002401
Iteration 121/1000 | Loss: 0.00002401
Iteration 122/1000 | Loss: 0.00002401
Iteration 123/1000 | Loss: 0.00002401
Iteration 124/1000 | Loss: 0.00002400
Iteration 125/1000 | Loss: 0.00002400
Iteration 126/1000 | Loss: 0.00002400
Iteration 127/1000 | Loss: 0.00002400
Iteration 128/1000 | Loss: 0.00002400
Iteration 129/1000 | Loss: 0.00002400
Iteration 130/1000 | Loss: 0.00002400
Iteration 131/1000 | Loss: 0.00002400
Iteration 132/1000 | Loss: 0.00002400
Iteration 133/1000 | Loss: 0.00002400
Iteration 134/1000 | Loss: 0.00002399
Iteration 135/1000 | Loss: 0.00002399
Iteration 136/1000 | Loss: 0.00002399
Iteration 137/1000 | Loss: 0.00002399
Iteration 138/1000 | Loss: 0.00002399
Iteration 139/1000 | Loss: 0.00002399
Iteration 140/1000 | Loss: 0.00002399
Iteration 141/1000 | Loss: 0.00002399
Iteration 142/1000 | Loss: 0.00002399
Iteration 143/1000 | Loss: 0.00002399
Iteration 144/1000 | Loss: 0.00002399
Iteration 145/1000 | Loss: 0.00002399
Iteration 146/1000 | Loss: 0.00002399
Iteration 147/1000 | Loss: 0.00002399
Iteration 148/1000 | Loss: 0.00002398
Iteration 149/1000 | Loss: 0.00002398
Iteration 150/1000 | Loss: 0.00002398
Iteration 151/1000 | Loss: 0.00002398
Iteration 152/1000 | Loss: 0.00002398
Iteration 153/1000 | Loss: 0.00002398
Iteration 154/1000 | Loss: 0.00002398
Iteration 155/1000 | Loss: 0.00002398
Iteration 156/1000 | Loss: 0.00002398
Iteration 157/1000 | Loss: 0.00002398
Iteration 158/1000 | Loss: 0.00002398
Iteration 159/1000 | Loss: 0.00002398
Iteration 160/1000 | Loss: 0.00002398
Iteration 161/1000 | Loss: 0.00002398
Iteration 162/1000 | Loss: 0.00002398
Iteration 163/1000 | Loss: 0.00002398
Iteration 164/1000 | Loss: 0.00002398
Iteration 165/1000 | Loss: 0.00002398
Iteration 166/1000 | Loss: 0.00002398
Iteration 167/1000 | Loss: 0.00002398
Iteration 168/1000 | Loss: 0.00002398
Iteration 169/1000 | Loss: 0.00002398
Iteration 170/1000 | Loss: 0.00002398
Iteration 171/1000 | Loss: 0.00002398
Iteration 172/1000 | Loss: 0.00002398
Iteration 173/1000 | Loss: 0.00002398
Iteration 174/1000 | Loss: 0.00002398
Iteration 175/1000 | Loss: 0.00002398
Iteration 176/1000 | Loss: 0.00002398
Iteration 177/1000 | Loss: 0.00002398
Iteration 178/1000 | Loss: 0.00002398
Iteration 179/1000 | Loss: 0.00002398
Iteration 180/1000 | Loss: 0.00002398
Iteration 181/1000 | Loss: 0.00002398
Iteration 182/1000 | Loss: 0.00002398
Iteration 183/1000 | Loss: 0.00002398
Iteration 184/1000 | Loss: 0.00002398
Iteration 185/1000 | Loss: 0.00002398
Iteration 186/1000 | Loss: 0.00002398
Iteration 187/1000 | Loss: 0.00002398
Iteration 188/1000 | Loss: 0.00002398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.3976852389751002e-05, 2.3976852389751002e-05, 2.3976852389751002e-05, 2.3976852389751002e-05, 2.3976852389751002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3976852389751002e-05

Optimization complete. Final v2v error: 4.240467071533203 mm

Highest mean error: 4.354487895965576 mm for frame 13

Lowest mean error: 4.080326080322266 mm for frame 143

Saving results

Total time: 37.6566116809845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903944
Iteration 2/25 | Loss: 0.00134090
Iteration 3/25 | Loss: 0.00124776
Iteration 4/25 | Loss: 0.00122818
Iteration 5/25 | Loss: 0.00122300
Iteration 6/25 | Loss: 0.00122225
Iteration 7/25 | Loss: 0.00122225
Iteration 8/25 | Loss: 0.00122225
Iteration 9/25 | Loss: 0.00122225
Iteration 10/25 | Loss: 0.00122225
Iteration 11/25 | Loss: 0.00122225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012222465593367815, 0.0012222465593367815, 0.0012222465593367815, 0.0012222465593367815, 0.0012222465593367815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012222465593367815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.32076597
Iteration 2/25 | Loss: 0.00191376
Iteration 3/25 | Loss: 0.00191373
Iteration 4/25 | Loss: 0.00191373
Iteration 5/25 | Loss: 0.00191373
Iteration 6/25 | Loss: 0.00191373
Iteration 7/25 | Loss: 0.00191373
Iteration 8/25 | Loss: 0.00191373
Iteration 9/25 | Loss: 0.00191373
Iteration 10/25 | Loss: 0.00191373
Iteration 11/25 | Loss: 0.00191373
Iteration 12/25 | Loss: 0.00191373
Iteration 13/25 | Loss: 0.00191373
Iteration 14/25 | Loss: 0.00191373
Iteration 15/25 | Loss: 0.00191373
Iteration 16/25 | Loss: 0.00191373
Iteration 17/25 | Loss: 0.00191373
Iteration 18/25 | Loss: 0.00191373
Iteration 19/25 | Loss: 0.00191373
Iteration 20/25 | Loss: 0.00191373
Iteration 21/25 | Loss: 0.00191373
Iteration 22/25 | Loss: 0.00191373
Iteration 23/25 | Loss: 0.00191373
Iteration 24/25 | Loss: 0.00191373
Iteration 25/25 | Loss: 0.00191373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191373
Iteration 2/1000 | Loss: 0.00003096
Iteration 3/1000 | Loss: 0.00002046
Iteration 4/1000 | Loss: 0.00001900
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001673
Iteration 9/1000 | Loss: 0.00001638
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001573
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001545
Iteration 15/1000 | Loss: 0.00001538
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001537
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001534
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001530
Iteration 33/1000 | Loss: 0.00001530
Iteration 34/1000 | Loss: 0.00001529
Iteration 35/1000 | Loss: 0.00001529
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001528
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001524
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001520
Iteration 78/1000 | Loss: 0.00001520
Iteration 79/1000 | Loss: 0.00001520
Iteration 80/1000 | Loss: 0.00001520
Iteration 81/1000 | Loss: 0.00001520
Iteration 82/1000 | Loss: 0.00001520
Iteration 83/1000 | Loss: 0.00001520
Iteration 84/1000 | Loss: 0.00001520
Iteration 85/1000 | Loss: 0.00001520
Iteration 86/1000 | Loss: 0.00001520
Iteration 87/1000 | Loss: 0.00001520
Iteration 88/1000 | Loss: 0.00001520
Iteration 89/1000 | Loss: 0.00001520
Iteration 90/1000 | Loss: 0.00001520
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.519837405794533e-05, 1.519837405794533e-05, 1.519837405794533e-05, 1.519837405794533e-05, 1.519837405794533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.519837405794533e-05

Optimization complete. Final v2v error: 3.35514235496521 mm

Highest mean error: 3.835669994354248 mm for frame 65

Lowest mean error: 2.980140447616577 mm for frame 77

Saving results

Total time: 32.552924156188965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846239
Iteration 2/25 | Loss: 0.00148257
Iteration 3/25 | Loss: 0.00127845
Iteration 4/25 | Loss: 0.00125481
Iteration 5/25 | Loss: 0.00124874
Iteration 6/25 | Loss: 0.00126221
Iteration 7/25 | Loss: 0.00125708
Iteration 8/25 | Loss: 0.00124826
Iteration 9/25 | Loss: 0.00123762
Iteration 10/25 | Loss: 0.00122726
Iteration 11/25 | Loss: 0.00122425
Iteration 12/25 | Loss: 0.00123635
Iteration 13/25 | Loss: 0.00124146
Iteration 14/25 | Loss: 0.00122700
Iteration 15/25 | Loss: 0.00121237
Iteration 16/25 | Loss: 0.00120539
Iteration 17/25 | Loss: 0.00120397
Iteration 18/25 | Loss: 0.00120240
Iteration 19/25 | Loss: 0.00120202
Iteration 20/25 | Loss: 0.00120177
Iteration 21/25 | Loss: 0.00120163
Iteration 22/25 | Loss: 0.00120163
Iteration 23/25 | Loss: 0.00120162
Iteration 24/25 | Loss: 0.00120162
Iteration 25/25 | Loss: 0.00120161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10078168
Iteration 2/25 | Loss: 0.00125449
Iteration 3/25 | Loss: 0.00125444
Iteration 4/25 | Loss: 0.00125444
Iteration 5/25 | Loss: 0.00125444
Iteration 6/25 | Loss: 0.00125444
Iteration 7/25 | Loss: 0.00125444
Iteration 8/25 | Loss: 0.00125444
Iteration 9/25 | Loss: 0.00125444
Iteration 10/25 | Loss: 0.00125444
Iteration 11/25 | Loss: 0.00125444
Iteration 12/25 | Loss: 0.00125444
Iteration 13/25 | Loss: 0.00125444
Iteration 14/25 | Loss: 0.00125444
Iteration 15/25 | Loss: 0.00125444
Iteration 16/25 | Loss: 0.00125444
Iteration 17/25 | Loss: 0.00125444
Iteration 18/25 | Loss: 0.00125444
Iteration 19/25 | Loss: 0.00125444
Iteration 20/25 | Loss: 0.00125444
Iteration 21/25 | Loss: 0.00125444
Iteration 22/25 | Loss: 0.00125444
Iteration 23/25 | Loss: 0.00125444
Iteration 24/25 | Loss: 0.00125444
Iteration 25/25 | Loss: 0.00125444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125444
Iteration 2/1000 | Loss: 0.00002342
Iteration 3/1000 | Loss: 0.00001746
Iteration 4/1000 | Loss: 0.00001441
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001292
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001178
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001176
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001169
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001163
Iteration 28/1000 | Loss: 0.00001163
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001162
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001156
Iteration 34/1000 | Loss: 0.00001155
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001151
Iteration 39/1000 | Loss: 0.00001150
Iteration 40/1000 | Loss: 0.00001150
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001144
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001141
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001137
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001137
Iteration 85/1000 | Loss: 0.00001137
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001137
Iteration 98/1000 | Loss: 0.00001137
Iteration 99/1000 | Loss: 0.00001137
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001137
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001137
Iteration 105/1000 | Loss: 0.00001137
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.1368060768290889e-05, 1.1368060768290889e-05, 1.1368060768290889e-05, 1.1368060768290889e-05, 1.1368060768290889e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1368060768290889e-05

Optimization complete. Final v2v error: 2.8950576782226562 mm

Highest mean error: 3.355903148651123 mm for frame 116

Lowest mean error: 2.5519492626190186 mm for frame 25

Saving results

Total time: 59.937947034835815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00706648
Iteration 2/25 | Loss: 0.00143089
Iteration 3/25 | Loss: 0.00132502
Iteration 4/25 | Loss: 0.00131770
Iteration 5/25 | Loss: 0.00131603
Iteration 6/25 | Loss: 0.00131603
Iteration 7/25 | Loss: 0.00131603
Iteration 8/25 | Loss: 0.00131603
Iteration 9/25 | Loss: 0.00131603
Iteration 10/25 | Loss: 0.00131603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001316034235060215, 0.001316034235060215, 0.001316034235060215, 0.001316034235060215, 0.001316034235060215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001316034235060215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43808901
Iteration 2/25 | Loss: 0.00108377
Iteration 3/25 | Loss: 0.00108375
Iteration 4/25 | Loss: 0.00108375
Iteration 5/25 | Loss: 0.00108375
Iteration 6/25 | Loss: 0.00108375
Iteration 7/25 | Loss: 0.00108375
Iteration 8/25 | Loss: 0.00108375
Iteration 9/25 | Loss: 0.00108375
Iteration 10/25 | Loss: 0.00108375
Iteration 11/25 | Loss: 0.00108375
Iteration 12/25 | Loss: 0.00108375
Iteration 13/25 | Loss: 0.00108375
Iteration 14/25 | Loss: 0.00108375
Iteration 15/25 | Loss: 0.00108375
Iteration 16/25 | Loss: 0.00108375
Iteration 17/25 | Loss: 0.00108375
Iteration 18/25 | Loss: 0.00108375
Iteration 19/25 | Loss: 0.00108375
Iteration 20/25 | Loss: 0.00108375
Iteration 21/25 | Loss: 0.00108375
Iteration 22/25 | Loss: 0.00108375
Iteration 23/25 | Loss: 0.00108375
Iteration 24/25 | Loss: 0.00108375
Iteration 25/25 | Loss: 0.00108375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108375
Iteration 2/1000 | Loss: 0.00003811
Iteration 3/1000 | Loss: 0.00002693
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002445
Iteration 6/1000 | Loss: 0.00002400
Iteration 7/1000 | Loss: 0.00002370
Iteration 8/1000 | Loss: 0.00002333
Iteration 9/1000 | Loss: 0.00002312
Iteration 10/1000 | Loss: 0.00002297
Iteration 11/1000 | Loss: 0.00002278
Iteration 12/1000 | Loss: 0.00002265
Iteration 13/1000 | Loss: 0.00002263
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002257
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002257
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002257
Iteration 20/1000 | Loss: 0.00002255
Iteration 21/1000 | Loss: 0.00002252
Iteration 22/1000 | Loss: 0.00002244
Iteration 23/1000 | Loss: 0.00002243
Iteration 24/1000 | Loss: 0.00002243
Iteration 25/1000 | Loss: 0.00002234
Iteration 26/1000 | Loss: 0.00002223
Iteration 27/1000 | Loss: 0.00002220
Iteration 28/1000 | Loss: 0.00002217
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002202
Iteration 31/1000 | Loss: 0.00002202
Iteration 32/1000 | Loss: 0.00002201
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002197
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002185
Iteration 37/1000 | Loss: 0.00002185
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002183
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002182
Iteration 52/1000 | Loss: 0.00002181
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002181
Iteration 56/1000 | Loss: 0.00002181
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002180
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002178
Iteration 70/1000 | Loss: 0.00002178
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002177
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002177
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002176
Iteration 85/1000 | Loss: 0.00002176
Iteration 86/1000 | Loss: 0.00002176
Iteration 87/1000 | Loss: 0.00002176
Iteration 88/1000 | Loss: 0.00002176
Iteration 89/1000 | Loss: 0.00002176
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002176
Iteration 92/1000 | Loss: 0.00002176
Iteration 93/1000 | Loss: 0.00002176
Iteration 94/1000 | Loss: 0.00002176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.175501322199125e-05, 2.175501322199125e-05, 2.175501322199125e-05, 2.175501322199125e-05, 2.175501322199125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.175501322199125e-05

Optimization complete. Final v2v error: 3.6448781490325928 mm

Highest mean error: 4.075122833251953 mm for frame 98

Lowest mean error: 3.218280553817749 mm for frame 159

Saving results

Total time: 42.10470628738403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930168
Iteration 2/25 | Loss: 0.00155829
Iteration 3/25 | Loss: 0.00135400
Iteration 4/25 | Loss: 0.00131714
Iteration 5/25 | Loss: 0.00131608
Iteration 6/25 | Loss: 0.00129919
Iteration 7/25 | Loss: 0.00129599
Iteration 8/25 | Loss: 0.00129499
Iteration 9/25 | Loss: 0.00129450
Iteration 10/25 | Loss: 0.00129428
Iteration 11/25 | Loss: 0.00129418
Iteration 12/25 | Loss: 0.00129413
Iteration 13/25 | Loss: 0.00129410
Iteration 14/25 | Loss: 0.00129410
Iteration 15/25 | Loss: 0.00129410
Iteration 16/25 | Loss: 0.00129410
Iteration 17/25 | Loss: 0.00129410
Iteration 18/25 | Loss: 0.00129409
Iteration 19/25 | Loss: 0.00129409
Iteration 20/25 | Loss: 0.00129409
Iteration 21/25 | Loss: 0.00129409
Iteration 22/25 | Loss: 0.00129409
Iteration 23/25 | Loss: 0.00129409
Iteration 24/25 | Loss: 0.00129409
Iteration 25/25 | Loss: 0.00129409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03377080
Iteration 2/25 | Loss: 0.00134996
Iteration 3/25 | Loss: 0.00134992
Iteration 4/25 | Loss: 0.00134992
Iteration 5/25 | Loss: 0.00134992
Iteration 6/25 | Loss: 0.00134992
Iteration 7/25 | Loss: 0.00134992
Iteration 8/25 | Loss: 0.00134992
Iteration 9/25 | Loss: 0.00134992
Iteration 10/25 | Loss: 0.00134992
Iteration 11/25 | Loss: 0.00134992
Iteration 12/25 | Loss: 0.00134992
Iteration 13/25 | Loss: 0.00134992
Iteration 14/25 | Loss: 0.00134992
Iteration 15/25 | Loss: 0.00134992
Iteration 16/25 | Loss: 0.00134992
Iteration 17/25 | Loss: 0.00134992
Iteration 18/25 | Loss: 0.00134992
Iteration 19/25 | Loss: 0.00134992
Iteration 20/25 | Loss: 0.00134992
Iteration 21/25 | Loss: 0.00134992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013499166816473007, 0.0013499166816473007, 0.0013499166816473007, 0.0013499166816473007, 0.0013499166816473007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013499166816473007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134992
Iteration 2/1000 | Loss: 0.00004556
Iteration 3/1000 | Loss: 0.00002999
Iteration 4/1000 | Loss: 0.00002698
Iteration 5/1000 | Loss: 0.00002545
Iteration 6/1000 | Loss: 0.00002495
Iteration 7/1000 | Loss: 0.00002446
Iteration 8/1000 | Loss: 0.00002417
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002361
Iteration 11/1000 | Loss: 0.00002354
Iteration 12/1000 | Loss: 0.00002336
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002323
Iteration 15/1000 | Loss: 0.00002313
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002312
Iteration 18/1000 | Loss: 0.00002311
Iteration 19/1000 | Loss: 0.00002310
Iteration 20/1000 | Loss: 0.00002310
Iteration 21/1000 | Loss: 0.00002310
Iteration 22/1000 | Loss: 0.00002309
Iteration 23/1000 | Loss: 0.00002309
Iteration 24/1000 | Loss: 0.00002309
Iteration 25/1000 | Loss: 0.00002308
Iteration 26/1000 | Loss: 0.00002308
Iteration 27/1000 | Loss: 0.00002306
Iteration 28/1000 | Loss: 0.00002306
Iteration 29/1000 | Loss: 0.00002306
Iteration 30/1000 | Loss: 0.00002305
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002305
Iteration 33/1000 | Loss: 0.00002305
Iteration 34/1000 | Loss: 0.00002305
Iteration 35/1000 | Loss: 0.00002305
Iteration 36/1000 | Loss: 0.00002305
Iteration 37/1000 | Loss: 0.00002303
Iteration 38/1000 | Loss: 0.00002303
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002302
Iteration 41/1000 | Loss: 0.00002302
Iteration 42/1000 | Loss: 0.00002302
Iteration 43/1000 | Loss: 0.00002301
Iteration 44/1000 | Loss: 0.00002300
Iteration 45/1000 | Loss: 0.00002300
Iteration 46/1000 | Loss: 0.00002299
Iteration 47/1000 | Loss: 0.00002299
Iteration 48/1000 | Loss: 0.00002299
Iteration 49/1000 | Loss: 0.00002299
Iteration 50/1000 | Loss: 0.00002298
Iteration 51/1000 | Loss: 0.00002298
Iteration 52/1000 | Loss: 0.00002298
Iteration 53/1000 | Loss: 0.00002297
Iteration 54/1000 | Loss: 0.00002297
Iteration 55/1000 | Loss: 0.00002297
Iteration 56/1000 | Loss: 0.00002296
Iteration 57/1000 | Loss: 0.00002296
Iteration 58/1000 | Loss: 0.00002296
Iteration 59/1000 | Loss: 0.00002295
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002295
Iteration 62/1000 | Loss: 0.00002294
Iteration 63/1000 | Loss: 0.00002294
Iteration 64/1000 | Loss: 0.00002294
Iteration 65/1000 | Loss: 0.00002294
Iteration 66/1000 | Loss: 0.00002294
Iteration 67/1000 | Loss: 0.00002293
Iteration 68/1000 | Loss: 0.00002293
Iteration 69/1000 | Loss: 0.00002292
Iteration 70/1000 | Loss: 0.00002292
Iteration 71/1000 | Loss: 0.00002292
Iteration 72/1000 | Loss: 0.00002292
Iteration 73/1000 | Loss: 0.00002292
Iteration 74/1000 | Loss: 0.00002292
Iteration 75/1000 | Loss: 0.00002292
Iteration 76/1000 | Loss: 0.00002292
Iteration 77/1000 | Loss: 0.00002291
Iteration 78/1000 | Loss: 0.00002291
Iteration 79/1000 | Loss: 0.00002291
Iteration 80/1000 | Loss: 0.00002291
Iteration 81/1000 | Loss: 0.00002291
Iteration 82/1000 | Loss: 0.00002290
Iteration 83/1000 | Loss: 0.00002289
Iteration 84/1000 | Loss: 0.00002289
Iteration 85/1000 | Loss: 0.00002289
Iteration 86/1000 | Loss: 0.00002289
Iteration 87/1000 | Loss: 0.00002289
Iteration 88/1000 | Loss: 0.00002289
Iteration 89/1000 | Loss: 0.00002289
Iteration 90/1000 | Loss: 0.00002289
Iteration 91/1000 | Loss: 0.00002288
Iteration 92/1000 | Loss: 0.00002288
Iteration 93/1000 | Loss: 0.00002288
Iteration 94/1000 | Loss: 0.00002288
Iteration 95/1000 | Loss: 0.00002288
Iteration 96/1000 | Loss: 0.00002288
Iteration 97/1000 | Loss: 0.00002288
Iteration 98/1000 | Loss: 0.00002288
Iteration 99/1000 | Loss: 0.00002288
Iteration 100/1000 | Loss: 0.00002287
Iteration 101/1000 | Loss: 0.00002287
Iteration 102/1000 | Loss: 0.00002287
Iteration 103/1000 | Loss: 0.00002287
Iteration 104/1000 | Loss: 0.00002287
Iteration 105/1000 | Loss: 0.00002287
Iteration 106/1000 | Loss: 0.00002287
Iteration 107/1000 | Loss: 0.00002287
Iteration 108/1000 | Loss: 0.00002286
Iteration 109/1000 | Loss: 0.00002286
Iteration 110/1000 | Loss: 0.00002286
Iteration 111/1000 | Loss: 0.00002286
Iteration 112/1000 | Loss: 0.00002286
Iteration 113/1000 | Loss: 0.00002286
Iteration 114/1000 | Loss: 0.00002286
Iteration 115/1000 | Loss: 0.00002286
Iteration 116/1000 | Loss: 0.00002286
Iteration 117/1000 | Loss: 0.00002285
Iteration 118/1000 | Loss: 0.00002285
Iteration 119/1000 | Loss: 0.00002285
Iteration 120/1000 | Loss: 0.00002285
Iteration 121/1000 | Loss: 0.00002285
Iteration 122/1000 | Loss: 0.00002285
Iteration 123/1000 | Loss: 0.00002285
Iteration 124/1000 | Loss: 0.00002284
Iteration 125/1000 | Loss: 0.00002284
Iteration 126/1000 | Loss: 0.00002284
Iteration 127/1000 | Loss: 0.00002284
Iteration 128/1000 | Loss: 0.00002284
Iteration 129/1000 | Loss: 0.00002283
Iteration 130/1000 | Loss: 0.00002283
Iteration 131/1000 | Loss: 0.00002283
Iteration 132/1000 | Loss: 0.00002283
Iteration 133/1000 | Loss: 0.00002283
Iteration 134/1000 | Loss: 0.00002283
Iteration 135/1000 | Loss: 0.00002283
Iteration 136/1000 | Loss: 0.00002283
Iteration 137/1000 | Loss: 0.00002282
Iteration 138/1000 | Loss: 0.00002282
Iteration 139/1000 | Loss: 0.00002282
Iteration 140/1000 | Loss: 0.00002282
Iteration 141/1000 | Loss: 0.00002282
Iteration 142/1000 | Loss: 0.00002281
Iteration 143/1000 | Loss: 0.00002281
Iteration 144/1000 | Loss: 0.00002281
Iteration 145/1000 | Loss: 0.00002281
Iteration 146/1000 | Loss: 0.00002281
Iteration 147/1000 | Loss: 0.00002281
Iteration 148/1000 | Loss: 0.00002281
Iteration 149/1000 | Loss: 0.00002281
Iteration 150/1000 | Loss: 0.00002281
Iteration 151/1000 | Loss: 0.00002281
Iteration 152/1000 | Loss: 0.00002281
Iteration 153/1000 | Loss: 0.00002281
Iteration 154/1000 | Loss: 0.00002280
Iteration 155/1000 | Loss: 0.00002280
Iteration 156/1000 | Loss: 0.00002280
Iteration 157/1000 | Loss: 0.00002280
Iteration 158/1000 | Loss: 0.00002280
Iteration 159/1000 | Loss: 0.00002280
Iteration 160/1000 | Loss: 0.00002280
Iteration 161/1000 | Loss: 0.00002280
Iteration 162/1000 | Loss: 0.00002279
Iteration 163/1000 | Loss: 0.00002279
Iteration 164/1000 | Loss: 0.00002279
Iteration 165/1000 | Loss: 0.00002279
Iteration 166/1000 | Loss: 0.00002279
Iteration 167/1000 | Loss: 0.00002279
Iteration 168/1000 | Loss: 0.00002279
Iteration 169/1000 | Loss: 0.00002279
Iteration 170/1000 | Loss: 0.00002279
Iteration 171/1000 | Loss: 0.00002279
Iteration 172/1000 | Loss: 0.00002279
Iteration 173/1000 | Loss: 0.00002279
Iteration 174/1000 | Loss: 0.00002279
Iteration 175/1000 | Loss: 0.00002279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.2790753064327873e-05, 2.2790753064327873e-05, 2.2790753064327873e-05, 2.2790753064327873e-05, 2.2790753064327873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2790753064327873e-05

Optimization complete. Final v2v error: 3.734884262084961 mm

Highest mean error: 4.424536228179932 mm for frame 104

Lowest mean error: 3.2360575199127197 mm for frame 145

Saving results

Total time: 59.16529560089111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954607
Iteration 2/25 | Loss: 0.00544147
Iteration 3/25 | Loss: 0.00267751
Iteration 4/25 | Loss: 0.00237673
Iteration 5/25 | Loss: 0.00232225
Iteration 6/25 | Loss: 0.00236689
Iteration 7/25 | Loss: 0.00218751
Iteration 8/25 | Loss: 0.00203972
Iteration 9/25 | Loss: 0.00199928
Iteration 10/25 | Loss: 0.00194846
Iteration 11/25 | Loss: 0.00188139
Iteration 12/25 | Loss: 0.00185855
Iteration 13/25 | Loss: 0.00185476
Iteration 14/25 | Loss: 0.00184592
Iteration 15/25 | Loss: 0.00183786
Iteration 16/25 | Loss: 0.00184128
Iteration 17/25 | Loss: 0.00184292
Iteration 18/25 | Loss: 0.00183438
Iteration 19/25 | Loss: 0.00183262
Iteration 20/25 | Loss: 0.00183229
Iteration 21/25 | Loss: 0.00183221
Iteration 22/25 | Loss: 0.00183221
Iteration 23/25 | Loss: 0.00183221
Iteration 24/25 | Loss: 0.00183220
Iteration 25/25 | Loss: 0.00183220

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23755789
Iteration 2/25 | Loss: 0.00568340
Iteration 3/25 | Loss: 0.00416259
Iteration 4/25 | Loss: 0.00416259
Iteration 5/25 | Loss: 0.00416259
Iteration 6/25 | Loss: 0.00416259
Iteration 7/25 | Loss: 0.00416259
Iteration 8/25 | Loss: 0.00416259
Iteration 9/25 | Loss: 0.00416259
Iteration 10/25 | Loss: 0.00416259
Iteration 11/25 | Loss: 0.00416259
Iteration 12/25 | Loss: 0.00416259
Iteration 13/25 | Loss: 0.00416259
Iteration 14/25 | Loss: 0.00416259
Iteration 15/25 | Loss: 0.00416259
Iteration 16/25 | Loss: 0.00416259
Iteration 17/25 | Loss: 0.00416259
Iteration 18/25 | Loss: 0.00416259
Iteration 19/25 | Loss: 0.00416259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004162585362792015, 0.004162585362792015, 0.004162585362792015, 0.004162585362792015, 0.004162585362792015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004162585362792015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00416259
Iteration 2/1000 | Loss: 0.00150525
Iteration 3/1000 | Loss: 0.00282817
Iteration 4/1000 | Loss: 0.00111169
Iteration 5/1000 | Loss: 0.00058080
Iteration 6/1000 | Loss: 0.00274669
Iteration 7/1000 | Loss: 0.00034165
Iteration 8/1000 | Loss: 0.00046727
Iteration 9/1000 | Loss: 0.00038255
Iteration 10/1000 | Loss: 0.00029230
Iteration 11/1000 | Loss: 0.00028034
Iteration 12/1000 | Loss: 0.00074544
Iteration 13/1000 | Loss: 0.00607001
Iteration 14/1000 | Loss: 0.00569221
Iteration 15/1000 | Loss: 0.00064758
Iteration 16/1000 | Loss: 0.00070455
Iteration 17/1000 | Loss: 0.00124634
Iteration 18/1000 | Loss: 0.00733483
Iteration 19/1000 | Loss: 0.00397505
Iteration 20/1000 | Loss: 0.00566977
Iteration 21/1000 | Loss: 0.00017134
Iteration 22/1000 | Loss: 0.00051952
Iteration 23/1000 | Loss: 0.00072749
Iteration 24/1000 | Loss: 0.00097704
Iteration 25/1000 | Loss: 0.00167919
Iteration 26/1000 | Loss: 0.00074535
Iteration 27/1000 | Loss: 0.00051983
Iteration 28/1000 | Loss: 0.00022131
Iteration 29/1000 | Loss: 0.00005694
Iteration 30/1000 | Loss: 0.00005282
Iteration 31/1000 | Loss: 0.00019310
Iteration 32/1000 | Loss: 0.00303606
Iteration 33/1000 | Loss: 0.00015054
Iteration 34/1000 | Loss: 0.00010066
Iteration 35/1000 | Loss: 0.00048109
Iteration 36/1000 | Loss: 0.00026122
Iteration 37/1000 | Loss: 0.00040375
Iteration 38/1000 | Loss: 0.00086656
Iteration 39/1000 | Loss: 0.00035771
Iteration 40/1000 | Loss: 0.00013951
Iteration 41/1000 | Loss: 0.00039232
Iteration 42/1000 | Loss: 0.00039018
Iteration 43/1000 | Loss: 0.00002415
Iteration 44/1000 | Loss: 0.00003669
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00004683
Iteration 47/1000 | Loss: 0.00005779
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00009173
Iteration 50/1000 | Loss: 0.00007294
Iteration 51/1000 | Loss: 0.00002222
Iteration 52/1000 | Loss: 0.00002778
Iteration 53/1000 | Loss: 0.00006506
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00009926
Iteration 56/1000 | Loss: 0.00010814
Iteration 57/1000 | Loss: 0.00005289
Iteration 58/1000 | Loss: 0.00017534
Iteration 59/1000 | Loss: 0.00002769
Iteration 60/1000 | Loss: 0.00004363
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00005654
Iteration 66/1000 | Loss: 0.00004775
Iteration 67/1000 | Loss: 0.00002076
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00002140
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001251
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00002500
Iteration 82/1000 | Loss: 0.00005709
Iteration 83/1000 | Loss: 0.00010342
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00004470
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00006239
Iteration 108/1000 | Loss: 0.00006239
Iteration 109/1000 | Loss: 0.00037920
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001224
Iteration 113/1000 | Loss: 0.00001223
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001217
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001215
Iteration 125/1000 | Loss: 0.00001215
Iteration 126/1000 | Loss: 0.00001215
Iteration 127/1000 | Loss: 0.00001215
Iteration 128/1000 | Loss: 0.00001215
Iteration 129/1000 | Loss: 0.00001215
Iteration 130/1000 | Loss: 0.00001215
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001213
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001213
Iteration 139/1000 | Loss: 0.00001213
Iteration 140/1000 | Loss: 0.00001213
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00004323
Iteration 146/1000 | Loss: 0.00004612
Iteration 147/1000 | Loss: 0.00006740
Iteration 148/1000 | Loss: 0.00004817
Iteration 149/1000 | Loss: 0.00007947
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001237
Iteration 152/1000 | Loss: 0.00002088
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001212
Iteration 156/1000 | Loss: 0.00001211
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001211
Iteration 159/1000 | Loss: 0.00001211
Iteration 160/1000 | Loss: 0.00001211
Iteration 161/1000 | Loss: 0.00001211
Iteration 162/1000 | Loss: 0.00001211
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001209
Iteration 167/1000 | Loss: 0.00001208
Iteration 168/1000 | Loss: 0.00001207
Iteration 169/1000 | Loss: 0.00001207
Iteration 170/1000 | Loss: 0.00001207
Iteration 171/1000 | Loss: 0.00001207
Iteration 172/1000 | Loss: 0.00001207
Iteration 173/1000 | Loss: 0.00001207
Iteration 174/1000 | Loss: 0.00001206
Iteration 175/1000 | Loss: 0.00001206
Iteration 176/1000 | Loss: 0.00001206
Iteration 177/1000 | Loss: 0.00001205
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001205
Iteration 180/1000 | Loss: 0.00001205
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001204
Iteration 184/1000 | Loss: 0.00001204
Iteration 185/1000 | Loss: 0.00001204
Iteration 186/1000 | Loss: 0.00001204
Iteration 187/1000 | Loss: 0.00001204
Iteration 188/1000 | Loss: 0.00001204
Iteration 189/1000 | Loss: 0.00001204
Iteration 190/1000 | Loss: 0.00001204
Iteration 191/1000 | Loss: 0.00001204
Iteration 192/1000 | Loss: 0.00001204
Iteration 193/1000 | Loss: 0.00001204
Iteration 194/1000 | Loss: 0.00001204
Iteration 195/1000 | Loss: 0.00001204
Iteration 196/1000 | Loss: 0.00001204
Iteration 197/1000 | Loss: 0.00001204
Iteration 198/1000 | Loss: 0.00001204
Iteration 199/1000 | Loss: 0.00001204
Iteration 200/1000 | Loss: 0.00001204
Iteration 201/1000 | Loss: 0.00001204
Iteration 202/1000 | Loss: 0.00001203
Iteration 203/1000 | Loss: 0.00001203
Iteration 204/1000 | Loss: 0.00001203
Iteration 205/1000 | Loss: 0.00001203
Iteration 206/1000 | Loss: 0.00001203
Iteration 207/1000 | Loss: 0.00001203
Iteration 208/1000 | Loss: 0.00001203
Iteration 209/1000 | Loss: 0.00001203
Iteration 210/1000 | Loss: 0.00001203
Iteration 211/1000 | Loss: 0.00001203
Iteration 212/1000 | Loss: 0.00001203
Iteration 213/1000 | Loss: 0.00001203
Iteration 214/1000 | Loss: 0.00001203
Iteration 215/1000 | Loss: 0.00001203
Iteration 216/1000 | Loss: 0.00001203
Iteration 217/1000 | Loss: 0.00001203
Iteration 218/1000 | Loss: 0.00001203
Iteration 219/1000 | Loss: 0.00001203
Iteration 220/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.203109059133567e-05, 1.203109059133567e-05, 1.203109059133567e-05, 1.203109059133567e-05, 1.203109059133567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.203109059133567e-05

Optimization complete. Final v2v error: 2.996062994003296 mm

Highest mean error: 3.6155412197113037 mm for frame 71

Lowest mean error: 2.751495122909546 mm for frame 46

Saving results

Total time: 169.5174744129181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003312
Iteration 2/25 | Loss: 0.00232222
Iteration 3/25 | Loss: 0.00161089
Iteration 4/25 | Loss: 0.00158064
Iteration 5/25 | Loss: 0.00144056
Iteration 6/25 | Loss: 0.00135075
Iteration 7/25 | Loss: 0.00132699
Iteration 8/25 | Loss: 0.00132858
Iteration 9/25 | Loss: 0.00133905
Iteration 10/25 | Loss: 0.00133091
Iteration 11/25 | Loss: 0.00131415
Iteration 12/25 | Loss: 0.00131060
Iteration 13/25 | Loss: 0.00130799
Iteration 14/25 | Loss: 0.00130726
Iteration 15/25 | Loss: 0.00130695
Iteration 16/25 | Loss: 0.00130681
Iteration 17/25 | Loss: 0.00130680
Iteration 18/25 | Loss: 0.00130680
Iteration 19/25 | Loss: 0.00130680
Iteration 20/25 | Loss: 0.00130680
Iteration 21/25 | Loss: 0.00130680
Iteration 22/25 | Loss: 0.00130680
Iteration 23/25 | Loss: 0.00130680
Iteration 24/25 | Loss: 0.00130680
Iteration 25/25 | Loss: 0.00130680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24348223
Iteration 2/25 | Loss: 0.00131577
Iteration 3/25 | Loss: 0.00131577
Iteration 4/25 | Loss: 0.00131577
Iteration 5/25 | Loss: 0.00131577
Iteration 6/25 | Loss: 0.00131577
Iteration 7/25 | Loss: 0.00131577
Iteration 8/25 | Loss: 0.00131577
Iteration 9/25 | Loss: 0.00131577
Iteration 10/25 | Loss: 0.00131577
Iteration 11/25 | Loss: 0.00131577
Iteration 12/25 | Loss: 0.00131577
Iteration 13/25 | Loss: 0.00131577
Iteration 14/25 | Loss: 0.00131577
Iteration 15/25 | Loss: 0.00131577
Iteration 16/25 | Loss: 0.00131577
Iteration 17/25 | Loss: 0.00131577
Iteration 18/25 | Loss: 0.00131577
Iteration 19/25 | Loss: 0.00131577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013157667126506567, 0.0013157667126506567, 0.0013157667126506567, 0.0013157667126506567, 0.0013157667126506567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013157667126506567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131577
Iteration 2/1000 | Loss: 0.00004630
Iteration 3/1000 | Loss: 0.00003048
Iteration 4/1000 | Loss: 0.00002803
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00018104
Iteration 7/1000 | Loss: 0.00020385
Iteration 8/1000 | Loss: 0.00002760
Iteration 9/1000 | Loss: 0.00018402
Iteration 10/1000 | Loss: 0.00002364
Iteration 11/1000 | Loss: 0.00002275
Iteration 12/1000 | Loss: 0.00002201
Iteration 13/1000 | Loss: 0.00002147
Iteration 14/1000 | Loss: 0.00002109
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00003795
Iteration 19/1000 | Loss: 0.00007159
Iteration 20/1000 | Loss: 0.00002926
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001905
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001887
Iteration 27/1000 | Loss: 0.00001887
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001880
Iteration 34/1000 | Loss: 0.00001880
Iteration 35/1000 | Loss: 0.00001880
Iteration 36/1000 | Loss: 0.00001880
Iteration 37/1000 | Loss: 0.00001880
Iteration 38/1000 | Loss: 0.00001880
Iteration 39/1000 | Loss: 0.00001880
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001880
Iteration 44/1000 | Loss: 0.00001880
Iteration 45/1000 | Loss: 0.00001880
Iteration 46/1000 | Loss: 0.00001879
Iteration 47/1000 | Loss: 0.00001879
Iteration 48/1000 | Loss: 0.00001879
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001878
Iteration 53/1000 | Loss: 0.00001878
Iteration 54/1000 | Loss: 0.00001878
Iteration 55/1000 | Loss: 0.00001877
Iteration 56/1000 | Loss: 0.00001877
Iteration 57/1000 | Loss: 0.00001876
Iteration 58/1000 | Loss: 0.00001876
Iteration 59/1000 | Loss: 0.00001875
Iteration 60/1000 | Loss: 0.00001875
Iteration 61/1000 | Loss: 0.00001875
Iteration 62/1000 | Loss: 0.00001875
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001873
Iteration 70/1000 | Loss: 0.00001873
Iteration 71/1000 | Loss: 0.00001873
Iteration 72/1000 | Loss: 0.00001873
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001873
Iteration 75/1000 | Loss: 0.00001873
Iteration 76/1000 | Loss: 0.00001873
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.8726552298176102e-05, 1.8726552298176102e-05, 1.8726552298176102e-05, 1.8726552298176102e-05, 1.8726552298176102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8726552298176102e-05

Optimization complete. Final v2v error: 3.6808085441589355 mm

Highest mean error: 10.541707038879395 mm for frame 234

Lowest mean error: 3.4673643112182617 mm for frame 11

Saving results

Total time: 73.25533819198608
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827067
Iteration 2/25 | Loss: 0.00129039
Iteration 3/25 | Loss: 0.00118994
Iteration 4/25 | Loss: 0.00117702
Iteration 5/25 | Loss: 0.00117301
Iteration 6/25 | Loss: 0.00117230
Iteration 7/25 | Loss: 0.00117230
Iteration 8/25 | Loss: 0.00117230
Iteration 9/25 | Loss: 0.00117230
Iteration 10/25 | Loss: 0.00117230
Iteration 11/25 | Loss: 0.00117230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011723036877810955, 0.0011723036877810955, 0.0011723036877810955, 0.0011723036877810955, 0.0011723036877810955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011723036877810955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96969736
Iteration 2/25 | Loss: 0.00140825
Iteration 3/25 | Loss: 0.00140824
Iteration 4/25 | Loss: 0.00140824
Iteration 5/25 | Loss: 0.00140824
Iteration 6/25 | Loss: 0.00140824
Iteration 7/25 | Loss: 0.00140824
Iteration 8/25 | Loss: 0.00140824
Iteration 9/25 | Loss: 0.00140824
Iteration 10/25 | Loss: 0.00140824
Iteration 11/25 | Loss: 0.00140824
Iteration 12/25 | Loss: 0.00140824
Iteration 13/25 | Loss: 0.00140824
Iteration 14/25 | Loss: 0.00140824
Iteration 15/25 | Loss: 0.00140824
Iteration 16/25 | Loss: 0.00140824
Iteration 17/25 | Loss: 0.00140824
Iteration 18/25 | Loss: 0.00140824
Iteration 19/25 | Loss: 0.00140824
Iteration 20/25 | Loss: 0.00140824
Iteration 21/25 | Loss: 0.00140824
Iteration 22/25 | Loss: 0.00140824
Iteration 23/25 | Loss: 0.00140824
Iteration 24/25 | Loss: 0.00140824
Iteration 25/25 | Loss: 0.00140824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140824
Iteration 2/1000 | Loss: 0.00002077
Iteration 3/1000 | Loss: 0.00001417
Iteration 4/1000 | Loss: 0.00001285
Iteration 5/1000 | Loss: 0.00001218
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001118
Iteration 9/1000 | Loss: 0.00001093
Iteration 10/1000 | Loss: 0.00001069
Iteration 11/1000 | Loss: 0.00001058
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001044
Iteration 14/1000 | Loss: 0.00001043
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001042
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001036
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001025
Iteration 22/1000 | Loss: 0.00001022
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001015
Iteration 27/1000 | Loss: 0.00001014
Iteration 28/1000 | Loss: 0.00001014
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001007
Iteration 31/1000 | Loss: 0.00001007
Iteration 32/1000 | Loss: 0.00001006
Iteration 33/1000 | Loss: 0.00001006
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001003
Iteration 36/1000 | Loss: 0.00001002
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000998
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000998
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000995
Iteration 52/1000 | Loss: 0.00000995
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000992
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000991
Iteration 64/1000 | Loss: 0.00000991
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000991
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000991
Iteration 75/1000 | Loss: 0.00000991
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000991
Iteration 81/1000 | Loss: 0.00000991
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000991
Iteration 86/1000 | Loss: 0.00000991
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000991
Iteration 89/1000 | Loss: 0.00000991
Iteration 90/1000 | Loss: 0.00000991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [9.913624126056675e-06, 9.913624126056675e-06, 9.913624126056675e-06, 9.913624126056675e-06, 9.913624126056675e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.913624126056675e-06

Optimization complete. Final v2v error: 2.665797710418701 mm

Highest mean error: 3.561267375946045 mm for frame 91

Lowest mean error: 2.4113917350769043 mm for frame 146

Saving results

Total time: 35.53262734413147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016170
Iteration 2/25 | Loss: 0.00278906
Iteration 3/25 | Loss: 0.00215029
Iteration 4/25 | Loss: 0.00212271
Iteration 5/25 | Loss: 0.00186950
Iteration 6/25 | Loss: 0.00167883
Iteration 7/25 | Loss: 0.00163317
Iteration 8/25 | Loss: 0.00156149
Iteration 9/25 | Loss: 0.00152892
Iteration 10/25 | Loss: 0.00149985
Iteration 11/25 | Loss: 0.00148315
Iteration 12/25 | Loss: 0.00147164
Iteration 13/25 | Loss: 0.00147149
Iteration 14/25 | Loss: 0.00148281
Iteration 15/25 | Loss: 0.00148035
Iteration 16/25 | Loss: 0.00146791
Iteration 17/25 | Loss: 0.00146138
Iteration 18/25 | Loss: 0.00145813
Iteration 19/25 | Loss: 0.00145588
Iteration 20/25 | Loss: 0.00145501
Iteration 21/25 | Loss: 0.00145468
Iteration 22/25 | Loss: 0.00145457
Iteration 23/25 | Loss: 0.00145686
Iteration 24/25 | Loss: 0.00145480
Iteration 25/25 | Loss: 0.00145336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40597713
Iteration 2/25 | Loss: 0.00347990
Iteration 3/25 | Loss: 0.00313921
Iteration 4/25 | Loss: 0.00313921
Iteration 5/25 | Loss: 0.00313921
Iteration 6/25 | Loss: 0.00313921
Iteration 7/25 | Loss: 0.00313921
Iteration 8/25 | Loss: 0.00313921
Iteration 9/25 | Loss: 0.00313921
Iteration 10/25 | Loss: 0.00313921
Iteration 11/25 | Loss: 0.00313921
Iteration 12/25 | Loss: 0.00313921
Iteration 13/25 | Loss: 0.00313921
Iteration 14/25 | Loss: 0.00313921
Iteration 15/25 | Loss: 0.00313921
Iteration 16/25 | Loss: 0.00313921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0031392069067806005, 0.0031392069067806005, 0.0031392069067806005, 0.0031392069067806005, 0.0031392069067806005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031392069067806005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313921
Iteration 2/1000 | Loss: 0.00166046
Iteration 3/1000 | Loss: 0.00236340
Iteration 4/1000 | Loss: 0.00043869
Iteration 5/1000 | Loss: 0.00093726
Iteration 6/1000 | Loss: 0.00229344
Iteration 7/1000 | Loss: 0.00017287
Iteration 8/1000 | Loss: 0.00017712
Iteration 9/1000 | Loss: 0.00066166
Iteration 10/1000 | Loss: 0.00036659
Iteration 11/1000 | Loss: 0.00015429
Iteration 12/1000 | Loss: 0.00013748
Iteration 13/1000 | Loss: 0.00118472
Iteration 14/1000 | Loss: 0.00015741
Iteration 15/1000 | Loss: 0.00013486
Iteration 16/1000 | Loss: 0.00012377
Iteration 17/1000 | Loss: 0.00100416
Iteration 18/1000 | Loss: 0.00028897
Iteration 19/1000 | Loss: 0.00013081
Iteration 20/1000 | Loss: 0.00019702
Iteration 21/1000 | Loss: 0.00011613
Iteration 22/1000 | Loss: 0.00034820
Iteration 23/1000 | Loss: 0.00013465
Iteration 24/1000 | Loss: 0.00011404
Iteration 25/1000 | Loss: 0.00023175
Iteration 26/1000 | Loss: 0.00011102
Iteration 27/1000 | Loss: 0.00010931
Iteration 28/1000 | Loss: 0.00010758
Iteration 29/1000 | Loss: 0.00010631
Iteration 30/1000 | Loss: 0.00010554
Iteration 31/1000 | Loss: 0.00036901
Iteration 32/1000 | Loss: 0.00179422
Iteration 33/1000 | Loss: 0.00283149
Iteration 34/1000 | Loss: 0.00053352
Iteration 35/1000 | Loss: 0.00062001
Iteration 36/1000 | Loss: 0.00014653
Iteration 37/1000 | Loss: 0.00021401
Iteration 38/1000 | Loss: 0.00028838
Iteration 39/1000 | Loss: 0.00088441
Iteration 40/1000 | Loss: 0.00128461
Iteration 41/1000 | Loss: 0.00074765
Iteration 42/1000 | Loss: 0.00019997
Iteration 43/1000 | Loss: 0.00009769
Iteration 44/1000 | Loss: 0.00009527
Iteration 45/1000 | Loss: 0.00051757
Iteration 46/1000 | Loss: 0.00009623
Iteration 47/1000 | Loss: 0.00009296
Iteration 48/1000 | Loss: 0.00009190
Iteration 49/1000 | Loss: 0.00009105
Iteration 50/1000 | Loss: 0.00009041
Iteration 51/1000 | Loss: 0.00008962
Iteration 52/1000 | Loss: 0.00008901
Iteration 53/1000 | Loss: 0.00008833
Iteration 54/1000 | Loss: 0.00008775
Iteration 55/1000 | Loss: 0.00027941
Iteration 56/1000 | Loss: 0.00012009
Iteration 57/1000 | Loss: 0.00059026
Iteration 58/1000 | Loss: 0.00034122
Iteration 59/1000 | Loss: 0.00011707
Iteration 60/1000 | Loss: 0.00009828
Iteration 61/1000 | Loss: 0.00009286
Iteration 62/1000 | Loss: 0.00009021
Iteration 63/1000 | Loss: 0.00008839
Iteration 64/1000 | Loss: 0.00008738
Iteration 65/1000 | Loss: 0.00008671
Iteration 66/1000 | Loss: 0.00008579
Iteration 67/1000 | Loss: 0.00008511
Iteration 68/1000 | Loss: 0.00008459
Iteration 69/1000 | Loss: 0.00008423
Iteration 70/1000 | Loss: 0.00008377
Iteration 71/1000 | Loss: 0.00008343
Iteration 72/1000 | Loss: 0.00008284
Iteration 73/1000 | Loss: 0.00024783
Iteration 74/1000 | Loss: 0.00063194
Iteration 75/1000 | Loss: 0.00018539
Iteration 76/1000 | Loss: 0.00020302
Iteration 77/1000 | Loss: 0.00012032
Iteration 78/1000 | Loss: 0.00009399
Iteration 79/1000 | Loss: 0.00008377
Iteration 80/1000 | Loss: 0.00008121
Iteration 81/1000 | Loss: 0.00007897
Iteration 82/1000 | Loss: 0.00007792
Iteration 83/1000 | Loss: 0.00007726
Iteration 84/1000 | Loss: 0.00007681
Iteration 85/1000 | Loss: 0.00007647
Iteration 86/1000 | Loss: 0.00007620
Iteration 87/1000 | Loss: 0.00007603
Iteration 88/1000 | Loss: 0.00007603
Iteration 89/1000 | Loss: 0.00007582
Iteration 90/1000 | Loss: 0.00007569
Iteration 91/1000 | Loss: 0.00007562
Iteration 92/1000 | Loss: 0.00007562
Iteration 93/1000 | Loss: 0.00007558
Iteration 94/1000 | Loss: 0.00007558
Iteration 95/1000 | Loss: 0.00007557
Iteration 96/1000 | Loss: 0.00007557
Iteration 97/1000 | Loss: 0.00007557
Iteration 98/1000 | Loss: 0.00007556
Iteration 99/1000 | Loss: 0.00007556
Iteration 100/1000 | Loss: 0.00007556
Iteration 101/1000 | Loss: 0.00007556
Iteration 102/1000 | Loss: 0.00007555
Iteration 103/1000 | Loss: 0.00007555
Iteration 104/1000 | Loss: 0.00007555
Iteration 105/1000 | Loss: 0.00007554
Iteration 106/1000 | Loss: 0.00007554
Iteration 107/1000 | Loss: 0.00007554
Iteration 108/1000 | Loss: 0.00007553
Iteration 109/1000 | Loss: 0.00007553
Iteration 110/1000 | Loss: 0.00007553
Iteration 111/1000 | Loss: 0.00007552
Iteration 112/1000 | Loss: 0.00007552
Iteration 113/1000 | Loss: 0.00007551
Iteration 114/1000 | Loss: 0.00007551
Iteration 115/1000 | Loss: 0.00033969
Iteration 116/1000 | Loss: 0.00040258
Iteration 117/1000 | Loss: 0.00026634
Iteration 118/1000 | Loss: 0.00008794
Iteration 119/1000 | Loss: 0.00007907
Iteration 120/1000 | Loss: 0.00007706
Iteration 121/1000 | Loss: 0.00031062
Iteration 122/1000 | Loss: 0.00008198
Iteration 123/1000 | Loss: 0.00007680
Iteration 124/1000 | Loss: 0.00007577
Iteration 125/1000 | Loss: 0.00007497
Iteration 126/1000 | Loss: 0.00007436
Iteration 127/1000 | Loss: 0.00007398
Iteration 128/1000 | Loss: 0.00007389
Iteration 129/1000 | Loss: 0.00007378
Iteration 130/1000 | Loss: 0.00007367
Iteration 131/1000 | Loss: 0.00007362
Iteration 132/1000 | Loss: 0.00007356
Iteration 133/1000 | Loss: 0.00007352
Iteration 134/1000 | Loss: 0.00007352
Iteration 135/1000 | Loss: 0.00007351
Iteration 136/1000 | Loss: 0.00007350
Iteration 137/1000 | Loss: 0.00007350
Iteration 138/1000 | Loss: 0.00007348
Iteration 139/1000 | Loss: 0.00007347
Iteration 140/1000 | Loss: 0.00007344
Iteration 141/1000 | Loss: 0.00007342
Iteration 142/1000 | Loss: 0.00007342
Iteration 143/1000 | Loss: 0.00007341
Iteration 144/1000 | Loss: 0.00007341
Iteration 145/1000 | Loss: 0.00007340
Iteration 146/1000 | Loss: 0.00007340
Iteration 147/1000 | Loss: 0.00007339
Iteration 148/1000 | Loss: 0.00007333
Iteration 149/1000 | Loss: 0.00007325
Iteration 150/1000 | Loss: 0.00007325
Iteration 151/1000 | Loss: 0.00007325
Iteration 152/1000 | Loss: 0.00007324
Iteration 153/1000 | Loss: 0.00007324
Iteration 154/1000 | Loss: 0.00007324
Iteration 155/1000 | Loss: 0.00007323
Iteration 156/1000 | Loss: 0.00007323
Iteration 157/1000 | Loss: 0.00007322
Iteration 158/1000 | Loss: 0.00007322
Iteration 159/1000 | Loss: 0.00007322
Iteration 160/1000 | Loss: 0.00007320
Iteration 161/1000 | Loss: 0.00007320
Iteration 162/1000 | Loss: 0.00007320
Iteration 163/1000 | Loss: 0.00007320
Iteration 164/1000 | Loss: 0.00007320
Iteration 165/1000 | Loss: 0.00007320
Iteration 166/1000 | Loss: 0.00007320
Iteration 167/1000 | Loss: 0.00007320
Iteration 168/1000 | Loss: 0.00007320
Iteration 169/1000 | Loss: 0.00007319
Iteration 170/1000 | Loss: 0.00007319
Iteration 171/1000 | Loss: 0.00007319
Iteration 172/1000 | Loss: 0.00007319
Iteration 173/1000 | Loss: 0.00007319
Iteration 174/1000 | Loss: 0.00007319
Iteration 175/1000 | Loss: 0.00007319
Iteration 176/1000 | Loss: 0.00007319
Iteration 177/1000 | Loss: 0.00007318
Iteration 178/1000 | Loss: 0.00007317
Iteration 179/1000 | Loss: 0.00007317
Iteration 180/1000 | Loss: 0.00007316
Iteration 181/1000 | Loss: 0.00007316
Iteration 182/1000 | Loss: 0.00007316
Iteration 183/1000 | Loss: 0.00007316
Iteration 184/1000 | Loss: 0.00007316
Iteration 185/1000 | Loss: 0.00007316
Iteration 186/1000 | Loss: 0.00007316
Iteration 187/1000 | Loss: 0.00007316
Iteration 188/1000 | Loss: 0.00007315
Iteration 189/1000 | Loss: 0.00007315
Iteration 190/1000 | Loss: 0.00007315
Iteration 191/1000 | Loss: 0.00007314
Iteration 192/1000 | Loss: 0.00007314
Iteration 193/1000 | Loss: 0.00007314
Iteration 194/1000 | Loss: 0.00007313
Iteration 195/1000 | Loss: 0.00007313
Iteration 196/1000 | Loss: 0.00007313
Iteration 197/1000 | Loss: 0.00007313
Iteration 198/1000 | Loss: 0.00007313
Iteration 199/1000 | Loss: 0.00007313
Iteration 200/1000 | Loss: 0.00007313
Iteration 201/1000 | Loss: 0.00007313
Iteration 202/1000 | Loss: 0.00007312
Iteration 203/1000 | Loss: 0.00007312
Iteration 204/1000 | Loss: 0.00007312
Iteration 205/1000 | Loss: 0.00007312
Iteration 206/1000 | Loss: 0.00007312
Iteration 207/1000 | Loss: 0.00007312
Iteration 208/1000 | Loss: 0.00007312
Iteration 209/1000 | Loss: 0.00007311
Iteration 210/1000 | Loss: 0.00007311
Iteration 211/1000 | Loss: 0.00007311
Iteration 212/1000 | Loss: 0.00007311
Iteration 213/1000 | Loss: 0.00007311
Iteration 214/1000 | Loss: 0.00007311
Iteration 215/1000 | Loss: 0.00007311
Iteration 216/1000 | Loss: 0.00007310
Iteration 217/1000 | Loss: 0.00007310
Iteration 218/1000 | Loss: 0.00007310
Iteration 219/1000 | Loss: 0.00007310
Iteration 220/1000 | Loss: 0.00007310
Iteration 221/1000 | Loss: 0.00007310
Iteration 222/1000 | Loss: 0.00007310
Iteration 223/1000 | Loss: 0.00007310
Iteration 224/1000 | Loss: 0.00007310
Iteration 225/1000 | Loss: 0.00007310
Iteration 226/1000 | Loss: 0.00007310
Iteration 227/1000 | Loss: 0.00007310
Iteration 228/1000 | Loss: 0.00007310
Iteration 229/1000 | Loss: 0.00007310
Iteration 230/1000 | Loss: 0.00007310
Iteration 231/1000 | Loss: 0.00007310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [7.310062937904149e-05, 7.310062937904149e-05, 7.310062937904149e-05, 7.310062937904149e-05, 7.310062937904149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.310062937904149e-05

Optimization complete. Final v2v error: 4.830904006958008 mm

Highest mean error: 12.036128044128418 mm for frame 50

Lowest mean error: 3.2498462200164795 mm for frame 18

Saving results

Total time: 212.6783492565155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779861
Iteration 2/25 | Loss: 0.00156083
Iteration 3/25 | Loss: 0.00139472
Iteration 4/25 | Loss: 0.00138345
Iteration 5/25 | Loss: 0.00138198
Iteration 6/25 | Loss: 0.00134599
Iteration 7/25 | Loss: 0.00130808
Iteration 8/25 | Loss: 0.00131393
Iteration 9/25 | Loss: 0.00127224
Iteration 10/25 | Loss: 0.00128341
Iteration 11/25 | Loss: 0.00125398
Iteration 12/25 | Loss: 0.00124946
Iteration 13/25 | Loss: 0.00124775
Iteration 14/25 | Loss: 0.00124330
Iteration 15/25 | Loss: 0.00124260
Iteration 16/25 | Loss: 0.00124186
Iteration 17/25 | Loss: 0.00124902
Iteration 18/25 | Loss: 0.00124590
Iteration 19/25 | Loss: 0.00123789
Iteration 20/25 | Loss: 0.00123352
Iteration 21/25 | Loss: 0.00123297
Iteration 22/25 | Loss: 0.00123452
Iteration 23/25 | Loss: 0.00123362
Iteration 24/25 | Loss: 0.00123142
Iteration 25/25 | Loss: 0.00122898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14489961
Iteration 2/25 | Loss: 0.00171957
Iteration 3/25 | Loss: 0.00171955
Iteration 4/25 | Loss: 0.00171955
Iteration 5/25 | Loss: 0.00171955
Iteration 6/25 | Loss: 0.00171955
Iteration 7/25 | Loss: 0.00171955
Iteration 8/25 | Loss: 0.00171955
Iteration 9/25 | Loss: 0.00171955
Iteration 10/25 | Loss: 0.00171955
Iteration 11/25 | Loss: 0.00171955
Iteration 12/25 | Loss: 0.00171955
Iteration 13/25 | Loss: 0.00171955
Iteration 14/25 | Loss: 0.00171955
Iteration 15/25 | Loss: 0.00171955
Iteration 16/25 | Loss: 0.00171955
Iteration 17/25 | Loss: 0.00171955
Iteration 18/25 | Loss: 0.00171955
Iteration 19/25 | Loss: 0.00171955
Iteration 20/25 | Loss: 0.00171955
Iteration 21/25 | Loss: 0.00171955
Iteration 22/25 | Loss: 0.00171955
Iteration 23/25 | Loss: 0.00171955
Iteration 24/25 | Loss: 0.00171955
Iteration 25/25 | Loss: 0.00171955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171955
Iteration 2/1000 | Loss: 0.00007306
Iteration 3/1000 | Loss: 0.00007218
Iteration 4/1000 | Loss: 0.00006403
Iteration 5/1000 | Loss: 0.00015862
Iteration 6/1000 | Loss: 0.00014390
Iteration 7/1000 | Loss: 0.00005232
Iteration 8/1000 | Loss: 0.00018544
Iteration 9/1000 | Loss: 0.00004381
Iteration 10/1000 | Loss: 0.00009540
Iteration 11/1000 | Loss: 0.00044176
Iteration 12/1000 | Loss: 0.00043221
Iteration 13/1000 | Loss: 0.00054285
Iteration 14/1000 | Loss: 0.00011540
Iteration 15/1000 | Loss: 0.00016439
Iteration 16/1000 | Loss: 0.00004842
Iteration 17/1000 | Loss: 0.00003905
Iteration 18/1000 | Loss: 0.00002856
Iteration 19/1000 | Loss: 0.00002446
Iteration 20/1000 | Loss: 0.00002304
Iteration 21/1000 | Loss: 0.00002139
Iteration 22/1000 | Loss: 0.00002027
Iteration 23/1000 | Loss: 0.00001983
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001872
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00010007
Iteration 30/1000 | Loss: 0.00011791
Iteration 31/1000 | Loss: 0.00009966
Iteration 32/1000 | Loss: 0.00054929
Iteration 33/1000 | Loss: 0.00016537
Iteration 34/1000 | Loss: 0.00039541
Iteration 35/1000 | Loss: 0.00007708
Iteration 36/1000 | Loss: 0.00038373
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001896
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001840
Iteration 45/1000 | Loss: 0.00001830
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001809
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001789
Iteration 54/1000 | Loss: 0.00001788
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001763
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001753
Iteration 81/1000 | Loss: 0.00001753
Iteration 82/1000 | Loss: 0.00001753
Iteration 83/1000 | Loss: 0.00001753
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001749
Iteration 91/1000 | Loss: 0.00001748
Iteration 92/1000 | Loss: 0.00001748
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001743
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001738
Iteration 109/1000 | Loss: 0.00001737
Iteration 110/1000 | Loss: 0.00001737
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001735
Iteration 113/1000 | Loss: 0.00001734
Iteration 114/1000 | Loss: 0.00001734
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001733
Iteration 118/1000 | Loss: 0.00001733
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001731
Iteration 122/1000 | Loss: 0.00001731
Iteration 123/1000 | Loss: 0.00001731
Iteration 124/1000 | Loss: 0.00001730
Iteration 125/1000 | Loss: 0.00001730
Iteration 126/1000 | Loss: 0.00001729
Iteration 127/1000 | Loss: 0.00001729
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001729
Iteration 130/1000 | Loss: 0.00001729
Iteration 131/1000 | Loss: 0.00001729
Iteration 132/1000 | Loss: 0.00001728
Iteration 133/1000 | Loss: 0.00001728
Iteration 134/1000 | Loss: 0.00001728
Iteration 135/1000 | Loss: 0.00001728
Iteration 136/1000 | Loss: 0.00001728
Iteration 137/1000 | Loss: 0.00001728
Iteration 138/1000 | Loss: 0.00001728
Iteration 139/1000 | Loss: 0.00001728
Iteration 140/1000 | Loss: 0.00001727
Iteration 141/1000 | Loss: 0.00001727
Iteration 142/1000 | Loss: 0.00001727
Iteration 143/1000 | Loss: 0.00001726
Iteration 144/1000 | Loss: 0.00001726
Iteration 145/1000 | Loss: 0.00001726
Iteration 146/1000 | Loss: 0.00001726
Iteration 147/1000 | Loss: 0.00001725
Iteration 148/1000 | Loss: 0.00001725
Iteration 149/1000 | Loss: 0.00001725
Iteration 150/1000 | Loss: 0.00001724
Iteration 151/1000 | Loss: 0.00001724
Iteration 152/1000 | Loss: 0.00001724
Iteration 153/1000 | Loss: 0.00001724
Iteration 154/1000 | Loss: 0.00001723
Iteration 155/1000 | Loss: 0.00001723
Iteration 156/1000 | Loss: 0.00001723
Iteration 157/1000 | Loss: 0.00001723
Iteration 158/1000 | Loss: 0.00001723
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Iteration 165/1000 | Loss: 0.00001722
Iteration 166/1000 | Loss: 0.00001722
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001721
Iteration 173/1000 | Loss: 0.00001721
Iteration 174/1000 | Loss: 0.00001721
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001720
Iteration 187/1000 | Loss: 0.00001720
Iteration 188/1000 | Loss: 0.00001720
Iteration 189/1000 | Loss: 0.00001720
Iteration 190/1000 | Loss: 0.00001720
Iteration 191/1000 | Loss: 0.00001720
Iteration 192/1000 | Loss: 0.00001720
Iteration 193/1000 | Loss: 0.00001719
Iteration 194/1000 | Loss: 0.00001719
Iteration 195/1000 | Loss: 0.00001719
Iteration 196/1000 | Loss: 0.00001719
Iteration 197/1000 | Loss: 0.00001719
Iteration 198/1000 | Loss: 0.00001719
Iteration 199/1000 | Loss: 0.00001719
Iteration 200/1000 | Loss: 0.00001719
Iteration 201/1000 | Loss: 0.00001719
Iteration 202/1000 | Loss: 0.00001719
Iteration 203/1000 | Loss: 0.00001719
Iteration 204/1000 | Loss: 0.00001719
Iteration 205/1000 | Loss: 0.00001719
Iteration 206/1000 | Loss: 0.00001719
Iteration 207/1000 | Loss: 0.00001719
Iteration 208/1000 | Loss: 0.00001719
Iteration 209/1000 | Loss: 0.00001719
Iteration 210/1000 | Loss: 0.00001719
Iteration 211/1000 | Loss: 0.00001719
Iteration 212/1000 | Loss: 0.00001719
Iteration 213/1000 | Loss: 0.00001719
Iteration 214/1000 | Loss: 0.00001719
Iteration 215/1000 | Loss: 0.00001719
Iteration 216/1000 | Loss: 0.00001719
Iteration 217/1000 | Loss: 0.00001718
Iteration 218/1000 | Loss: 0.00001718
Iteration 219/1000 | Loss: 0.00001718
Iteration 220/1000 | Loss: 0.00001718
Iteration 221/1000 | Loss: 0.00001718
Iteration 222/1000 | Loss: 0.00001718
Iteration 223/1000 | Loss: 0.00001718
Iteration 224/1000 | Loss: 0.00001718
Iteration 225/1000 | Loss: 0.00001718
Iteration 226/1000 | Loss: 0.00001718
Iteration 227/1000 | Loss: 0.00001718
Iteration 228/1000 | Loss: 0.00001718
Iteration 229/1000 | Loss: 0.00001718
Iteration 230/1000 | Loss: 0.00001718
Iteration 231/1000 | Loss: 0.00001718
Iteration 232/1000 | Loss: 0.00001718
Iteration 233/1000 | Loss: 0.00001718
Iteration 234/1000 | Loss: 0.00001718
Iteration 235/1000 | Loss: 0.00001718
Iteration 236/1000 | Loss: 0.00001718
Iteration 237/1000 | Loss: 0.00001718
Iteration 238/1000 | Loss: 0.00001718
Iteration 239/1000 | Loss: 0.00001718
Iteration 240/1000 | Loss: 0.00001718
Iteration 241/1000 | Loss: 0.00001718
Iteration 242/1000 | Loss: 0.00001718
Iteration 243/1000 | Loss: 0.00001718
Iteration 244/1000 | Loss: 0.00001718
Iteration 245/1000 | Loss: 0.00001718
Iteration 246/1000 | Loss: 0.00001718
Iteration 247/1000 | Loss: 0.00001718
Iteration 248/1000 | Loss: 0.00001718
Iteration 249/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.7182201190735213e-05, 1.7182201190735213e-05, 1.7182201190735213e-05, 1.7182201190735213e-05, 1.7182201190735213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7182201190735213e-05

Optimization complete. Final v2v error: 3.2490766048431396 mm

Highest mean error: 12.570588111877441 mm for frame 236

Lowest mean error: 2.407785654067993 mm for frame 177

Saving results

Total time: 148.07857465744019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533836
Iteration 2/25 | Loss: 0.00136485
Iteration 3/25 | Loss: 0.00113912
Iteration 4/25 | Loss: 0.00111143
Iteration 5/25 | Loss: 0.00110761
Iteration 6/25 | Loss: 0.00110737
Iteration 7/25 | Loss: 0.00110737
Iteration 8/25 | Loss: 0.00110737
Iteration 9/25 | Loss: 0.00110737
Iteration 10/25 | Loss: 0.00110737
Iteration 11/25 | Loss: 0.00110737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011073682690039277, 0.0011073682690039277, 0.0011073682690039277, 0.0011073682690039277, 0.0011073682690039277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011073682690039277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.09698296
Iteration 2/25 | Loss: 0.00058961
Iteration 3/25 | Loss: 0.00058961
Iteration 4/25 | Loss: 0.00058960
Iteration 5/25 | Loss: 0.00058960
Iteration 6/25 | Loss: 0.00058960
Iteration 7/25 | Loss: 0.00058960
Iteration 8/25 | Loss: 0.00058960
Iteration 9/25 | Loss: 0.00058960
Iteration 10/25 | Loss: 0.00058960
Iteration 11/25 | Loss: 0.00058960
Iteration 12/25 | Loss: 0.00058960
Iteration 13/25 | Loss: 0.00058960
Iteration 14/25 | Loss: 0.00058960
Iteration 15/25 | Loss: 0.00058960
Iteration 16/25 | Loss: 0.00058960
Iteration 17/25 | Loss: 0.00058960
Iteration 18/25 | Loss: 0.00058960
Iteration 19/25 | Loss: 0.00058960
Iteration 20/25 | Loss: 0.00058960
Iteration 21/25 | Loss: 0.00058960
Iteration 22/25 | Loss: 0.00058960
Iteration 23/25 | Loss: 0.00058960
Iteration 24/25 | Loss: 0.00058960
Iteration 25/25 | Loss: 0.00058960

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058960
Iteration 2/1000 | Loss: 0.00006990
Iteration 3/1000 | Loss: 0.00003839
Iteration 4/1000 | Loss: 0.00003105
Iteration 5/1000 | Loss: 0.00002843
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002627
Iteration 8/1000 | Loss: 0.00002571
Iteration 9/1000 | Loss: 0.00002542
Iteration 10/1000 | Loss: 0.00002505
Iteration 11/1000 | Loss: 0.00002478
Iteration 12/1000 | Loss: 0.00002457
Iteration 13/1000 | Loss: 0.00002447
Iteration 14/1000 | Loss: 0.00002446
Iteration 15/1000 | Loss: 0.00002445
Iteration 16/1000 | Loss: 0.00002444
Iteration 17/1000 | Loss: 0.00002444
Iteration 18/1000 | Loss: 0.00002444
Iteration 19/1000 | Loss: 0.00002444
Iteration 20/1000 | Loss: 0.00002444
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002444
Iteration 23/1000 | Loss: 0.00002444
Iteration 24/1000 | Loss: 0.00002443
Iteration 25/1000 | Loss: 0.00002443
Iteration 26/1000 | Loss: 0.00002443
Iteration 27/1000 | Loss: 0.00002443
Iteration 28/1000 | Loss: 0.00002443
Iteration 29/1000 | Loss: 0.00002443
Iteration 30/1000 | Loss: 0.00002443
Iteration 31/1000 | Loss: 0.00002443
Iteration 32/1000 | Loss: 0.00002443
Iteration 33/1000 | Loss: 0.00002442
Iteration 34/1000 | Loss: 0.00002442
Iteration 35/1000 | Loss: 0.00002441
Iteration 36/1000 | Loss: 0.00002441
Iteration 37/1000 | Loss: 0.00002441
Iteration 38/1000 | Loss: 0.00002441
Iteration 39/1000 | Loss: 0.00002440
Iteration 40/1000 | Loss: 0.00002440
Iteration 41/1000 | Loss: 0.00002439
Iteration 42/1000 | Loss: 0.00002439
Iteration 43/1000 | Loss: 0.00002439
Iteration 44/1000 | Loss: 0.00002439
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002439
Iteration 47/1000 | Loss: 0.00002439
Iteration 48/1000 | Loss: 0.00002439
Iteration 49/1000 | Loss: 0.00002438
Iteration 50/1000 | Loss: 0.00002437
Iteration 51/1000 | Loss: 0.00002437
Iteration 52/1000 | Loss: 0.00002436
Iteration 53/1000 | Loss: 0.00002436
Iteration 54/1000 | Loss: 0.00002436
Iteration 55/1000 | Loss: 0.00002436
Iteration 56/1000 | Loss: 0.00002436
Iteration 57/1000 | Loss: 0.00002436
Iteration 58/1000 | Loss: 0.00002436
Iteration 59/1000 | Loss: 0.00002436
Iteration 60/1000 | Loss: 0.00002435
Iteration 61/1000 | Loss: 0.00002434
Iteration 62/1000 | Loss: 0.00002434
Iteration 63/1000 | Loss: 0.00002434
Iteration 64/1000 | Loss: 0.00002434
Iteration 65/1000 | Loss: 0.00002433
Iteration 66/1000 | Loss: 0.00002433
Iteration 67/1000 | Loss: 0.00002433
Iteration 68/1000 | Loss: 0.00002433
Iteration 69/1000 | Loss: 0.00002433
Iteration 70/1000 | Loss: 0.00002433
Iteration 71/1000 | Loss: 0.00002433
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002431
Iteration 75/1000 | Loss: 0.00002430
Iteration 76/1000 | Loss: 0.00002430
Iteration 77/1000 | Loss: 0.00002430
Iteration 78/1000 | Loss: 0.00002429
Iteration 79/1000 | Loss: 0.00002428
Iteration 80/1000 | Loss: 0.00002428
Iteration 81/1000 | Loss: 0.00002427
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002426
Iteration 85/1000 | Loss: 0.00002426
Iteration 86/1000 | Loss: 0.00002426
Iteration 87/1000 | Loss: 0.00002426
Iteration 88/1000 | Loss: 0.00002426
Iteration 89/1000 | Loss: 0.00002426
Iteration 90/1000 | Loss: 0.00002425
Iteration 91/1000 | Loss: 0.00002425
Iteration 92/1000 | Loss: 0.00002425
Iteration 93/1000 | Loss: 0.00002425
Iteration 94/1000 | Loss: 0.00002425
Iteration 95/1000 | Loss: 0.00002424
Iteration 96/1000 | Loss: 0.00002423
Iteration 97/1000 | Loss: 0.00002423
Iteration 98/1000 | Loss: 0.00002423
Iteration 99/1000 | Loss: 0.00002423
Iteration 100/1000 | Loss: 0.00002422
Iteration 101/1000 | Loss: 0.00002422
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002421
Iteration 104/1000 | Loss: 0.00002421
Iteration 105/1000 | Loss: 0.00002421
Iteration 106/1000 | Loss: 0.00002421
Iteration 107/1000 | Loss: 0.00002421
Iteration 108/1000 | Loss: 0.00002421
Iteration 109/1000 | Loss: 0.00002421
Iteration 110/1000 | Loss: 0.00002420
Iteration 111/1000 | Loss: 0.00002420
Iteration 112/1000 | Loss: 0.00002420
Iteration 113/1000 | Loss: 0.00002420
Iteration 114/1000 | Loss: 0.00002419
Iteration 115/1000 | Loss: 0.00002419
Iteration 116/1000 | Loss: 0.00002419
Iteration 117/1000 | Loss: 0.00002418
Iteration 118/1000 | Loss: 0.00002418
Iteration 119/1000 | Loss: 0.00002417
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00002417
Iteration 122/1000 | Loss: 0.00002417
Iteration 123/1000 | Loss: 0.00002417
Iteration 124/1000 | Loss: 0.00002417
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002417
Iteration 130/1000 | Loss: 0.00002416
Iteration 131/1000 | Loss: 0.00002416
Iteration 132/1000 | Loss: 0.00002416
Iteration 133/1000 | Loss: 0.00002416
Iteration 134/1000 | Loss: 0.00002416
Iteration 135/1000 | Loss: 0.00002416
Iteration 136/1000 | Loss: 0.00002415
Iteration 137/1000 | Loss: 0.00002415
Iteration 138/1000 | Loss: 0.00002415
Iteration 139/1000 | Loss: 0.00002415
Iteration 140/1000 | Loss: 0.00002415
Iteration 141/1000 | Loss: 0.00002415
Iteration 142/1000 | Loss: 0.00002415
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002414
Iteration 145/1000 | Loss: 0.00002414
Iteration 146/1000 | Loss: 0.00002414
Iteration 147/1000 | Loss: 0.00002414
Iteration 148/1000 | Loss: 0.00002414
Iteration 149/1000 | Loss: 0.00002414
Iteration 150/1000 | Loss: 0.00002414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.414476330159232e-05, 2.414476330159232e-05, 2.414476330159232e-05, 2.414476330159232e-05, 2.414476330159232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.414476330159232e-05

Optimization complete. Final v2v error: 4.158140659332275 mm

Highest mean error: 4.903982162475586 mm for frame 59

Lowest mean error: 3.6254870891571045 mm for frame 29

Saving results

Total time: 35.526708126068115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360388
Iteration 2/25 | Loss: 0.00109628
Iteration 3/25 | Loss: 0.00101723
Iteration 4/25 | Loss: 0.00100728
Iteration 5/25 | Loss: 0.00100347
Iteration 6/25 | Loss: 0.00100280
Iteration 7/25 | Loss: 0.00100280
Iteration 8/25 | Loss: 0.00100280
Iteration 9/25 | Loss: 0.00100280
Iteration 10/25 | Loss: 0.00100280
Iteration 11/25 | Loss: 0.00100280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010027963435277343, 0.0010027963435277343, 0.0010027963435277343, 0.0010027963435277343, 0.0010027963435277343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010027963435277343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38272977
Iteration 2/25 | Loss: 0.00058053
Iteration 3/25 | Loss: 0.00058053
Iteration 4/25 | Loss: 0.00058053
Iteration 5/25 | Loss: 0.00058053
Iteration 6/25 | Loss: 0.00058053
Iteration 7/25 | Loss: 0.00058053
Iteration 8/25 | Loss: 0.00058053
Iteration 9/25 | Loss: 0.00058053
Iteration 10/25 | Loss: 0.00058053
Iteration 11/25 | Loss: 0.00058053
Iteration 12/25 | Loss: 0.00058053
Iteration 13/25 | Loss: 0.00058053
Iteration 14/25 | Loss: 0.00058053
Iteration 15/25 | Loss: 0.00058053
Iteration 16/25 | Loss: 0.00058053
Iteration 17/25 | Loss: 0.00058053
Iteration 18/25 | Loss: 0.00058053
Iteration 19/25 | Loss: 0.00058053
Iteration 20/25 | Loss: 0.00058053
Iteration 21/25 | Loss: 0.00058053
Iteration 22/25 | Loss: 0.00058053
Iteration 23/25 | Loss: 0.00058053
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005805311957374215, 0.0005805311957374215, 0.0005805311957374215, 0.0005805311957374215, 0.0005805311957374215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005805311957374215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058053
Iteration 2/1000 | Loss: 0.00004285
Iteration 3/1000 | Loss: 0.00001712
Iteration 4/1000 | Loss: 0.00001386
Iteration 5/1000 | Loss: 0.00001282
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001206
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001179
Iteration 22/1000 | Loss: 0.00001178
Iteration 23/1000 | Loss: 0.00001177
Iteration 24/1000 | Loss: 0.00001176
Iteration 25/1000 | Loss: 0.00001176
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001171
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.1670846106426325e-05, 1.1670846106426325e-05, 1.1670846106426325e-05, 1.1670846106426325e-05, 1.1670846106426325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1670846106426325e-05

Optimization complete. Final v2v error: 3.027087688446045 mm

Highest mean error: 3.1896040439605713 mm for frame 129

Lowest mean error: 2.7793190479278564 mm for frame 22

Saving results

Total time: 25.57903289794922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079896
Iteration 2/25 | Loss: 0.01079896
Iteration 3/25 | Loss: 0.01079895
Iteration 4/25 | Loss: 0.01079895
Iteration 5/25 | Loss: 0.01079895
Iteration 6/25 | Loss: 0.01079895
Iteration 7/25 | Loss: 0.01079895
Iteration 8/25 | Loss: 0.01079895
Iteration 9/25 | Loss: 0.01079895
Iteration 10/25 | Loss: 0.01079895
Iteration 11/25 | Loss: 0.01079895
Iteration 12/25 | Loss: 0.01079895
Iteration 13/25 | Loss: 0.01079895
Iteration 14/25 | Loss: 0.01079894
Iteration 15/25 | Loss: 0.01079894
Iteration 16/25 | Loss: 0.01079894
Iteration 17/25 | Loss: 0.01079894
Iteration 18/25 | Loss: 0.01079894
Iteration 19/25 | Loss: 0.01079894
Iteration 20/25 | Loss: 0.01079894
Iteration 21/25 | Loss: 0.01079894
Iteration 22/25 | Loss: 0.01079894
Iteration 23/25 | Loss: 0.01079894
Iteration 24/25 | Loss: 0.01079894
Iteration 25/25 | Loss: 0.01079894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63470304
Iteration 2/25 | Loss: 0.10943282
Iteration 3/25 | Loss: 0.10784931
Iteration 4/25 | Loss: 0.10668329
Iteration 5/25 | Loss: 0.10668328
Iteration 6/25 | Loss: 0.10668327
Iteration 7/25 | Loss: 0.10668328
Iteration 8/25 | Loss: 0.10668328
Iteration 9/25 | Loss: 0.10668325
Iteration 10/25 | Loss: 0.10668325
Iteration 11/25 | Loss: 0.10668325
Iteration 12/25 | Loss: 0.10668325
Iteration 13/25 | Loss: 0.10668325
Iteration 14/25 | Loss: 0.10668325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.10668325424194336, 0.10668325424194336, 0.10668325424194336, 0.10668325424194336, 0.10668325424194336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10668325424194336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10668325
Iteration 2/1000 | Loss: 0.00360607
Iteration 3/1000 | Loss: 0.00128849
Iteration 4/1000 | Loss: 0.00062993
Iteration 5/1000 | Loss: 0.00040099
Iteration 6/1000 | Loss: 0.00121402
Iteration 7/1000 | Loss: 0.00041509
Iteration 8/1000 | Loss: 0.00080902
Iteration 9/1000 | Loss: 0.00022101
Iteration 10/1000 | Loss: 0.00059707
Iteration 11/1000 | Loss: 0.00005319
Iteration 12/1000 | Loss: 0.00010781
Iteration 13/1000 | Loss: 0.00017386
Iteration 14/1000 | Loss: 0.00216503
Iteration 15/1000 | Loss: 0.00021269
Iteration 16/1000 | Loss: 0.00005614
Iteration 17/1000 | Loss: 0.00022780
Iteration 18/1000 | Loss: 0.00002618
Iteration 19/1000 | Loss: 0.00007344
Iteration 20/1000 | Loss: 0.00010015
Iteration 21/1000 | Loss: 0.00004667
Iteration 22/1000 | Loss: 0.00014804
Iteration 23/1000 | Loss: 0.00008444
Iteration 24/1000 | Loss: 0.00005327
Iteration 25/1000 | Loss: 0.00007885
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00008186
Iteration 28/1000 | Loss: 0.00006961
Iteration 29/1000 | Loss: 0.00008834
Iteration 30/1000 | Loss: 0.00002142
Iteration 31/1000 | Loss: 0.00002704
Iteration 32/1000 | Loss: 0.00002977
Iteration 33/1000 | Loss: 0.00022748
Iteration 34/1000 | Loss: 0.00008406
Iteration 35/1000 | Loss: 0.00005203
Iteration 36/1000 | Loss: 0.00003909
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001974
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001576
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00002594
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00002361
Iteration 49/1000 | Loss: 0.00003294
Iteration 50/1000 | Loss: 0.00005585
Iteration 51/1000 | Loss: 0.00014724
Iteration 52/1000 | Loss: 0.00020273
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00007827
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00003557
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001862
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001686
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001527
Iteration 90/1000 | Loss: 0.00001527
Iteration 91/1000 | Loss: 0.00001527
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001527
Iteration 109/1000 | Loss: 0.00001527
Iteration 110/1000 | Loss: 0.00001527
Iteration 111/1000 | Loss: 0.00001527
Iteration 112/1000 | Loss: 0.00001527
Iteration 113/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.5265542970155366e-05, 1.5265542970155366e-05, 1.5265542970155366e-05, 1.5265542970155366e-05, 1.5265542970155366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5265542970155366e-05

Optimization complete. Final v2v error: 3.3445229530334473 mm

Highest mean error: 3.913797378540039 mm for frame 227

Lowest mean error: 3.137176990509033 mm for frame 150

Saving results

Total time: 96.98795413970947
