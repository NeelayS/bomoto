Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=89, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4984-5039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889361
Iteration 2/25 | Loss: 0.00168901
Iteration 3/25 | Loss: 0.00151609
Iteration 4/25 | Loss: 0.00148351
Iteration 5/25 | Loss: 0.00146099
Iteration 6/25 | Loss: 0.00143920
Iteration 7/25 | Loss: 0.00142186
Iteration 8/25 | Loss: 0.00141256
Iteration 9/25 | Loss: 0.00141186
Iteration 10/25 | Loss: 0.00141111
Iteration 11/25 | Loss: 0.00142778
Iteration 12/25 | Loss: 0.00145896
Iteration 13/25 | Loss: 0.00143820
Iteration 14/25 | Loss: 0.00141656
Iteration 15/25 | Loss: 0.00139759
Iteration 16/25 | Loss: 0.00139376
Iteration 17/25 | Loss: 0.00139574
Iteration 18/25 | Loss: 0.00139884
Iteration 19/25 | Loss: 0.00139845
Iteration 20/25 | Loss: 0.00139494
Iteration 21/25 | Loss: 0.00138902
Iteration 22/25 | Loss: 0.00138452
Iteration 23/25 | Loss: 0.00138155
Iteration 24/25 | Loss: 0.00138018
Iteration 25/25 | Loss: 0.00137905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40407228
Iteration 2/25 | Loss: 0.00104886
Iteration 3/25 | Loss: 0.00104882
Iteration 4/25 | Loss: 0.00104882
Iteration 5/25 | Loss: 0.00104882
Iteration 6/25 | Loss: 0.00104882
Iteration 7/25 | Loss: 0.00104882
Iteration 8/25 | Loss: 0.00104882
Iteration 9/25 | Loss: 0.00104882
Iteration 10/25 | Loss: 0.00104882
Iteration 11/25 | Loss: 0.00104882
Iteration 12/25 | Loss: 0.00104882
Iteration 13/25 | Loss: 0.00104882
Iteration 14/25 | Loss: 0.00104882
Iteration 15/25 | Loss: 0.00104882
Iteration 16/25 | Loss: 0.00104882
Iteration 17/25 | Loss: 0.00104882
Iteration 18/25 | Loss: 0.00104882
Iteration 19/25 | Loss: 0.00104882
Iteration 20/25 | Loss: 0.00104882
Iteration 21/25 | Loss: 0.00104882
Iteration 22/25 | Loss: 0.00104882
Iteration 23/25 | Loss: 0.00104882
Iteration 24/25 | Loss: 0.00104882
Iteration 25/25 | Loss: 0.00104882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104882
Iteration 2/1000 | Loss: 0.00068738
Iteration 3/1000 | Loss: 0.00034787
Iteration 4/1000 | Loss: 0.00004229
Iteration 5/1000 | Loss: 0.00003430
Iteration 6/1000 | Loss: 0.00003155
Iteration 7/1000 | Loss: 0.00003006
Iteration 8/1000 | Loss: 0.00002914
Iteration 9/1000 | Loss: 0.00010439
Iteration 10/1000 | Loss: 0.00009313
Iteration 11/1000 | Loss: 0.00002931
Iteration 12/1000 | Loss: 0.00010592
Iteration 13/1000 | Loss: 0.00008619
Iteration 14/1000 | Loss: 0.00002797
Iteration 15/1000 | Loss: 0.00010263
Iteration 16/1000 | Loss: 0.00019696
Iteration 17/1000 | Loss: 0.00017017
Iteration 18/1000 | Loss: 0.00007660
Iteration 19/1000 | Loss: 0.00002746
Iteration 20/1000 | Loss: 0.00002714
Iteration 21/1000 | Loss: 0.00002692
Iteration 22/1000 | Loss: 0.00010072
Iteration 23/1000 | Loss: 0.00006825
Iteration 24/1000 | Loss: 0.00003111
Iteration 25/1000 | Loss: 0.00002795
Iteration 26/1000 | Loss: 0.00002667
Iteration 27/1000 | Loss: 0.00002650
Iteration 28/1000 | Loss: 0.00009216
Iteration 29/1000 | Loss: 0.00005093
Iteration 30/1000 | Loss: 0.00014949
Iteration 31/1000 | Loss: 0.00034763
Iteration 32/1000 | Loss: 0.00023845
Iteration 33/1000 | Loss: 0.00007889
Iteration 34/1000 | Loss: 0.00018973
Iteration 35/1000 | Loss: 0.00006067
Iteration 36/1000 | Loss: 0.00003055
Iteration 37/1000 | Loss: 0.00005605
Iteration 38/1000 | Loss: 0.00005930
Iteration 39/1000 | Loss: 0.00021606
Iteration 40/1000 | Loss: 0.00016081
Iteration 41/1000 | Loss: 0.00010375
Iteration 42/1000 | Loss: 0.00010298
Iteration 43/1000 | Loss: 0.00009519
Iteration 44/1000 | Loss: 0.00009400
Iteration 45/1000 | Loss: 0.00013784
Iteration 46/1000 | Loss: 0.00009726
Iteration 47/1000 | Loss: 0.00007938
Iteration 48/1000 | Loss: 0.00003131
Iteration 49/1000 | Loss: 0.00018543
Iteration 50/1000 | Loss: 0.00003767
Iteration 51/1000 | Loss: 0.00019531
Iteration 52/1000 | Loss: 0.00020002
Iteration 53/1000 | Loss: 0.00016841
Iteration 54/1000 | Loss: 0.00015456
Iteration 55/1000 | Loss: 0.00015191
Iteration 56/1000 | Loss: 0.00002916
Iteration 57/1000 | Loss: 0.00009921
Iteration 58/1000 | Loss: 0.00014077
Iteration 59/1000 | Loss: 0.00009426
Iteration 60/1000 | Loss: 0.00019210
Iteration 61/1000 | Loss: 0.00003467
Iteration 62/1000 | Loss: 0.00003125
Iteration 63/1000 | Loss: 0.00002966
Iteration 64/1000 | Loss: 0.00002857
Iteration 65/1000 | Loss: 0.00002774
Iteration 66/1000 | Loss: 0.00002709
Iteration 67/1000 | Loss: 0.00002671
Iteration 68/1000 | Loss: 0.00002630
Iteration 69/1000 | Loss: 0.00010247
Iteration 70/1000 | Loss: 0.00007023
Iteration 71/1000 | Loss: 0.00009556
Iteration 72/1000 | Loss: 0.00007817
Iteration 73/1000 | Loss: 0.00002603
Iteration 74/1000 | Loss: 0.00002576
Iteration 75/1000 | Loss: 0.00002560
Iteration 76/1000 | Loss: 0.00009258
Iteration 77/1000 | Loss: 0.00017881
Iteration 78/1000 | Loss: 0.00006599
Iteration 79/1000 | Loss: 0.00009037
Iteration 80/1000 | Loss: 0.00008179
Iteration 81/1000 | Loss: 0.00008377
Iteration 82/1000 | Loss: 0.00007817
Iteration 83/1000 | Loss: 0.00007905
Iteration 84/1000 | Loss: 0.00006298
Iteration 85/1000 | Loss: 0.00002537
Iteration 86/1000 | Loss: 0.00002531
Iteration 87/1000 | Loss: 0.00002529
Iteration 88/1000 | Loss: 0.00002528
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002523
Iteration 92/1000 | Loss: 0.00002521
Iteration 93/1000 | Loss: 0.00002521
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002520
Iteration 96/1000 | Loss: 0.00009634
Iteration 97/1000 | Loss: 0.00006932
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00009126
Iteration 100/1000 | Loss: 0.00012493
Iteration 101/1000 | Loss: 0.00006658
Iteration 102/1000 | Loss: 0.00009474
Iteration 103/1000 | Loss: 0.00007395
Iteration 104/1000 | Loss: 0.00008606
Iteration 105/1000 | Loss: 0.00009911
Iteration 106/1000 | Loss: 0.00005569
Iteration 107/1000 | Loss: 0.00005066
Iteration 108/1000 | Loss: 0.00004753
Iteration 109/1000 | Loss: 0.00003671
Iteration 110/1000 | Loss: 0.00002684
Iteration 111/1000 | Loss: 0.00002589
Iteration 112/1000 | Loss: 0.00009358
Iteration 113/1000 | Loss: 0.00010823
Iteration 114/1000 | Loss: 0.00005178
Iteration 115/1000 | Loss: 0.00002558
Iteration 116/1000 | Loss: 0.00002537
Iteration 117/1000 | Loss: 0.00002534
Iteration 118/1000 | Loss: 0.00002534
Iteration 119/1000 | Loss: 0.00002531
Iteration 120/1000 | Loss: 0.00002530
Iteration 121/1000 | Loss: 0.00002529
Iteration 122/1000 | Loss: 0.00002529
Iteration 123/1000 | Loss: 0.00002529
Iteration 124/1000 | Loss: 0.00002528
Iteration 125/1000 | Loss: 0.00002528
Iteration 126/1000 | Loss: 0.00002528
Iteration 127/1000 | Loss: 0.00002528
Iteration 128/1000 | Loss: 0.00002527
Iteration 129/1000 | Loss: 0.00002527
Iteration 130/1000 | Loss: 0.00002527
Iteration 131/1000 | Loss: 0.00002524
Iteration 132/1000 | Loss: 0.00002521
Iteration 133/1000 | Loss: 0.00002521
Iteration 134/1000 | Loss: 0.00009255
Iteration 135/1000 | Loss: 0.00006436
Iteration 136/1000 | Loss: 0.00002518
Iteration 137/1000 | Loss: 0.00002516
Iteration 138/1000 | Loss: 0.00002516
Iteration 139/1000 | Loss: 0.00008687
Iteration 140/1000 | Loss: 0.00007040
Iteration 141/1000 | Loss: 0.00002517
Iteration 142/1000 | Loss: 0.00002516
Iteration 143/1000 | Loss: 0.00002513
Iteration 144/1000 | Loss: 0.00002513
Iteration 145/1000 | Loss: 0.00002512
Iteration 146/1000 | Loss: 0.00002512
Iteration 147/1000 | Loss: 0.00002512
Iteration 148/1000 | Loss: 0.00002512
Iteration 149/1000 | Loss: 0.00002512
Iteration 150/1000 | Loss: 0.00002512
Iteration 151/1000 | Loss: 0.00002512
Iteration 152/1000 | Loss: 0.00002512
Iteration 153/1000 | Loss: 0.00002512
Iteration 154/1000 | Loss: 0.00002512
Iteration 155/1000 | Loss: 0.00002512
Iteration 156/1000 | Loss: 0.00002512
Iteration 157/1000 | Loss: 0.00002512
Iteration 158/1000 | Loss: 0.00002512
Iteration 159/1000 | Loss: 0.00002512
Iteration 160/1000 | Loss: 0.00002511
Iteration 161/1000 | Loss: 0.00002511
Iteration 162/1000 | Loss: 0.00002511
Iteration 163/1000 | Loss: 0.00002511
Iteration 164/1000 | Loss: 0.00002511
Iteration 165/1000 | Loss: 0.00002510
Iteration 166/1000 | Loss: 0.00002510
Iteration 167/1000 | Loss: 0.00002510
Iteration 168/1000 | Loss: 0.00002510
Iteration 169/1000 | Loss: 0.00002510
Iteration 170/1000 | Loss: 0.00002510
Iteration 171/1000 | Loss: 0.00002510
Iteration 172/1000 | Loss: 0.00002510
Iteration 173/1000 | Loss: 0.00002510
Iteration 174/1000 | Loss: 0.00002510
Iteration 175/1000 | Loss: 0.00002510
Iteration 176/1000 | Loss: 0.00002510
Iteration 177/1000 | Loss: 0.00002510
Iteration 178/1000 | Loss: 0.00002510
Iteration 179/1000 | Loss: 0.00002510
Iteration 180/1000 | Loss: 0.00002510
Iteration 181/1000 | Loss: 0.00002510
Iteration 182/1000 | Loss: 0.00002509
Iteration 183/1000 | Loss: 0.00002509
Iteration 184/1000 | Loss: 0.00002509
Iteration 185/1000 | Loss: 0.00002509
Iteration 186/1000 | Loss: 0.00002509
Iteration 187/1000 | Loss: 0.00002508
Iteration 188/1000 | Loss: 0.00002508
Iteration 189/1000 | Loss: 0.00002508
Iteration 190/1000 | Loss: 0.00002508
Iteration 191/1000 | Loss: 0.00002508
Iteration 192/1000 | Loss: 0.00002508
Iteration 193/1000 | Loss: 0.00002508
Iteration 194/1000 | Loss: 0.00002508
Iteration 195/1000 | Loss: 0.00002508
Iteration 196/1000 | Loss: 0.00002508
Iteration 197/1000 | Loss: 0.00002507
Iteration 198/1000 | Loss: 0.00002507
Iteration 199/1000 | Loss: 0.00002507
Iteration 200/1000 | Loss: 0.00002507
Iteration 201/1000 | Loss: 0.00002507
Iteration 202/1000 | Loss: 0.00002507
Iteration 203/1000 | Loss: 0.00002506
Iteration 204/1000 | Loss: 0.00002506
Iteration 205/1000 | Loss: 0.00002506
Iteration 206/1000 | Loss: 0.00002506
Iteration 207/1000 | Loss: 0.00002506
Iteration 208/1000 | Loss: 0.00002506
Iteration 209/1000 | Loss: 0.00002506
Iteration 210/1000 | Loss: 0.00002506
Iteration 211/1000 | Loss: 0.00002506
Iteration 212/1000 | Loss: 0.00002506
Iteration 213/1000 | Loss: 0.00002505
Iteration 214/1000 | Loss: 0.00002505
Iteration 215/1000 | Loss: 0.00002505
Iteration 216/1000 | Loss: 0.00002505
Iteration 217/1000 | Loss: 0.00002505
Iteration 218/1000 | Loss: 0.00002505
Iteration 219/1000 | Loss: 0.00002504
Iteration 220/1000 | Loss: 0.00002504
Iteration 221/1000 | Loss: 0.00002504
Iteration 222/1000 | Loss: 0.00002504
Iteration 223/1000 | Loss: 0.00002504
Iteration 224/1000 | Loss: 0.00002504
Iteration 225/1000 | Loss: 0.00009836
Iteration 226/1000 | Loss: 0.00005105
Iteration 227/1000 | Loss: 0.00002528
Iteration 228/1000 | Loss: 0.00009115
Iteration 229/1000 | Loss: 0.00007580
Iteration 230/1000 | Loss: 0.00002528
Iteration 231/1000 | Loss: 0.00002504
Iteration 232/1000 | Loss: 0.00002504
Iteration 233/1000 | Loss: 0.00002504
Iteration 234/1000 | Loss: 0.00002504
Iteration 235/1000 | Loss: 0.00002504
Iteration 236/1000 | Loss: 0.00002504
Iteration 237/1000 | Loss: 0.00002503
Iteration 238/1000 | Loss: 0.00002503
Iteration 239/1000 | Loss: 0.00002503
Iteration 240/1000 | Loss: 0.00002503
Iteration 241/1000 | Loss: 0.00002503
Iteration 242/1000 | Loss: 0.00002503
Iteration 243/1000 | Loss: 0.00002503
Iteration 244/1000 | Loss: 0.00002503
Iteration 245/1000 | Loss: 0.00002503
Iteration 246/1000 | Loss: 0.00002503
Iteration 247/1000 | Loss: 0.00002503
Iteration 248/1000 | Loss: 0.00002503
Iteration 249/1000 | Loss: 0.00002503
Iteration 250/1000 | Loss: 0.00002503
Iteration 251/1000 | Loss: 0.00002503
Iteration 252/1000 | Loss: 0.00002503
Iteration 253/1000 | Loss: 0.00002503
Iteration 254/1000 | Loss: 0.00002503
Iteration 255/1000 | Loss: 0.00002503
Iteration 256/1000 | Loss: 0.00002503
Iteration 257/1000 | Loss: 0.00002503
Iteration 258/1000 | Loss: 0.00002503
Iteration 259/1000 | Loss: 0.00002503
Iteration 260/1000 | Loss: 0.00002503
Iteration 261/1000 | Loss: 0.00002503
Iteration 262/1000 | Loss: 0.00002503
Iteration 263/1000 | Loss: 0.00002503
Iteration 264/1000 | Loss: 0.00002503
Iteration 265/1000 | Loss: 0.00002503
Iteration 266/1000 | Loss: 0.00002503
Iteration 267/1000 | Loss: 0.00002503
Iteration 268/1000 | Loss: 0.00002503
Iteration 269/1000 | Loss: 0.00002503
Iteration 270/1000 | Loss: 0.00002503
Iteration 271/1000 | Loss: 0.00002503
Iteration 272/1000 | Loss: 0.00002503
Iteration 273/1000 | Loss: 0.00002503
Iteration 274/1000 | Loss: 0.00002503
Iteration 275/1000 | Loss: 0.00002503
Iteration 276/1000 | Loss: 0.00002503
Iteration 277/1000 | Loss: 0.00002503
Iteration 278/1000 | Loss: 0.00002503
Iteration 279/1000 | Loss: 0.00002503
Iteration 280/1000 | Loss: 0.00002503
Iteration 281/1000 | Loss: 0.00002503
Iteration 282/1000 | Loss: 0.00002503
Iteration 283/1000 | Loss: 0.00002503
Iteration 284/1000 | Loss: 0.00002503
Iteration 285/1000 | Loss: 0.00002503
Iteration 286/1000 | Loss: 0.00002503
Iteration 287/1000 | Loss: 0.00002503
Iteration 288/1000 | Loss: 0.00002503
Iteration 289/1000 | Loss: 0.00002503
Iteration 290/1000 | Loss: 0.00002503
Iteration 291/1000 | Loss: 0.00002503
Iteration 292/1000 | Loss: 0.00002503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.5028790332726203e-05, 2.5028790332726203e-05, 2.5028790332726203e-05, 2.5028790332726203e-05, 2.5028790332726203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5028790332726203e-05

Optimization complete. Final v2v error: 4.0689544677734375 mm

Highest mean error: 8.334912300109863 mm for frame 213

Lowest mean error: 3.375030755996704 mm for frame 22

Saving results

Total time: 247.75130033493042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444680
Iteration 2/25 | Loss: 0.00138313
Iteration 3/25 | Loss: 0.00131701
Iteration 4/25 | Loss: 0.00130077
Iteration 5/25 | Loss: 0.00129532
Iteration 6/25 | Loss: 0.00129467
Iteration 7/25 | Loss: 0.00129467
Iteration 8/25 | Loss: 0.00129467
Iteration 9/25 | Loss: 0.00129467
Iteration 10/25 | Loss: 0.00129467
Iteration 11/25 | Loss: 0.00129467
Iteration 12/25 | Loss: 0.00129467
Iteration 13/25 | Loss: 0.00129467
Iteration 14/25 | Loss: 0.00129467
Iteration 15/25 | Loss: 0.00129467
Iteration 16/25 | Loss: 0.00129467
Iteration 17/25 | Loss: 0.00129467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012946674833074212, 0.0012946674833074212, 0.0012946674833074212, 0.0012946674833074212, 0.0012946674833074212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012946674833074212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47551751
Iteration 2/25 | Loss: 0.00086480
Iteration 3/25 | Loss: 0.00086480
Iteration 4/25 | Loss: 0.00086480
Iteration 5/25 | Loss: 0.00086480
Iteration 6/25 | Loss: 0.00086480
Iteration 7/25 | Loss: 0.00086480
Iteration 8/25 | Loss: 0.00086480
Iteration 9/25 | Loss: 0.00086480
Iteration 10/25 | Loss: 0.00086480
Iteration 11/25 | Loss: 0.00086480
Iteration 12/25 | Loss: 0.00086480
Iteration 13/25 | Loss: 0.00086480
Iteration 14/25 | Loss: 0.00086480
Iteration 15/25 | Loss: 0.00086480
Iteration 16/25 | Loss: 0.00086480
Iteration 17/25 | Loss: 0.00086480
Iteration 18/25 | Loss: 0.00086480
Iteration 19/25 | Loss: 0.00086480
Iteration 20/25 | Loss: 0.00086480
Iteration 21/25 | Loss: 0.00086480
Iteration 22/25 | Loss: 0.00086480
Iteration 23/25 | Loss: 0.00086480
Iteration 24/25 | Loss: 0.00086480
Iteration 25/25 | Loss: 0.00086480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008647993672639132, 0.0008647993672639132, 0.0008647993672639132, 0.0008647993672639132, 0.0008647993672639132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008647993672639132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086480
Iteration 2/1000 | Loss: 0.00003254
Iteration 3/1000 | Loss: 0.00002270
Iteration 4/1000 | Loss: 0.00002016
Iteration 5/1000 | Loss: 0.00001910
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001745
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001710
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001674
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001642
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001631
Iteration 27/1000 | Loss: 0.00001631
Iteration 28/1000 | Loss: 0.00001631
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001629
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001627
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001627
Iteration 43/1000 | Loss: 0.00001627
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001617
Iteration 57/1000 | Loss: 0.00001616
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001614
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001612
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001604
Iteration 73/1000 | Loss: 0.00001604
Iteration 74/1000 | Loss: 0.00001604
Iteration 75/1000 | Loss: 0.00001604
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001602
Iteration 79/1000 | Loss: 0.00001602
Iteration 80/1000 | Loss: 0.00001601
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001600
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001597
Iteration 90/1000 | Loss: 0.00001597
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001596
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001594
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001592
Iteration 106/1000 | Loss: 0.00001592
Iteration 107/1000 | Loss: 0.00001592
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001592
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001591
Iteration 113/1000 | Loss: 0.00001591
Iteration 114/1000 | Loss: 0.00001591
Iteration 115/1000 | Loss: 0.00001591
Iteration 116/1000 | Loss: 0.00001591
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001590
Iteration 119/1000 | Loss: 0.00001590
Iteration 120/1000 | Loss: 0.00001590
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.590498141013086e-05, 1.590498141013086e-05, 1.590498141013086e-05, 1.590498141013086e-05, 1.590498141013086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.590498141013086e-05

Optimization complete. Final v2v error: 3.388343095779419 mm

Highest mean error: 3.8828089237213135 mm for frame 78

Lowest mean error: 3.24556040763855 mm for frame 28

Saving results

Total time: 36.10554528236389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011054
Iteration 2/25 | Loss: 0.01011054
Iteration 3/25 | Loss: 0.01011054
Iteration 4/25 | Loss: 0.01011053
Iteration 5/25 | Loss: 0.01011053
Iteration 6/25 | Loss: 0.01011053
Iteration 7/25 | Loss: 0.01011053
Iteration 8/25 | Loss: 0.01011053
Iteration 9/25 | Loss: 0.01011053
Iteration 10/25 | Loss: 0.01011052
Iteration 11/25 | Loss: 0.01011052
Iteration 12/25 | Loss: 0.01011052
Iteration 13/25 | Loss: 0.01011052
Iteration 14/25 | Loss: 0.01011051
Iteration 15/25 | Loss: 0.01011051
Iteration 16/25 | Loss: 0.01011051
Iteration 17/25 | Loss: 0.01011051
Iteration 18/25 | Loss: 0.01011050
Iteration 19/25 | Loss: 0.01011050
Iteration 20/25 | Loss: 0.01011050
Iteration 21/25 | Loss: 0.01011050
Iteration 22/25 | Loss: 0.01011049
Iteration 23/25 | Loss: 0.01011049
Iteration 24/25 | Loss: 0.01011049
Iteration 25/25 | Loss: 0.01011048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62434268
Iteration 2/25 | Loss: 0.10351449
Iteration 3/25 | Loss: 0.10513311
Iteration 4/25 | Loss: 0.09947235
Iteration 5/25 | Loss: 0.09947236
Iteration 6/25 | Loss: 0.09947236
Iteration 7/25 | Loss: 0.09947236
Iteration 8/25 | Loss: 0.09947234
Iteration 9/25 | Loss: 0.09947234
Iteration 10/25 | Loss: 0.09947234
Iteration 11/25 | Loss: 0.09947234
Iteration 12/25 | Loss: 0.09947234
Iteration 13/25 | Loss: 0.09947235
Iteration 14/25 | Loss: 0.09947234
Iteration 15/25 | Loss: 0.09947234
Iteration 16/25 | Loss: 0.09947234
Iteration 17/25 | Loss: 0.09947234
Iteration 18/25 | Loss: 0.09947234
Iteration 19/25 | Loss: 0.09947234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.09947234392166138, 0.09947234392166138, 0.09947234392166138, 0.09947234392166138, 0.09947234392166138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09947234392166138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09947234
Iteration 2/1000 | Loss: 0.00644675
Iteration 3/1000 | Loss: 0.00410475
Iteration 4/1000 | Loss: 0.00207074
Iteration 5/1000 | Loss: 0.00219515
Iteration 6/1000 | Loss: 0.00038003
Iteration 7/1000 | Loss: 0.00037897
Iteration 8/1000 | Loss: 0.00023867
Iteration 9/1000 | Loss: 0.00112585
Iteration 10/1000 | Loss: 0.00039220
Iteration 11/1000 | Loss: 0.00065906
Iteration 12/1000 | Loss: 0.00015467
Iteration 13/1000 | Loss: 0.00080532
Iteration 14/1000 | Loss: 0.00009926
Iteration 15/1000 | Loss: 0.00027049
Iteration 16/1000 | Loss: 0.00049365
Iteration 17/1000 | Loss: 0.00027812
Iteration 18/1000 | Loss: 0.00017532
Iteration 19/1000 | Loss: 0.00079854
Iteration 20/1000 | Loss: 0.00012988
Iteration 21/1000 | Loss: 0.00005920
Iteration 22/1000 | Loss: 0.00006062
Iteration 23/1000 | Loss: 0.00028332
Iteration 24/1000 | Loss: 0.00012728
Iteration 25/1000 | Loss: 0.00008433
Iteration 26/1000 | Loss: 0.00004424
Iteration 27/1000 | Loss: 0.00016278
Iteration 28/1000 | Loss: 0.00009178
Iteration 29/1000 | Loss: 0.00009173
Iteration 30/1000 | Loss: 0.00004770
Iteration 31/1000 | Loss: 0.00010460
Iteration 32/1000 | Loss: 0.00045249
Iteration 33/1000 | Loss: 0.00013068
Iteration 34/1000 | Loss: 0.00009246
Iteration 35/1000 | Loss: 0.00003246
Iteration 36/1000 | Loss: 0.00007153
Iteration 37/1000 | Loss: 0.00005567
Iteration 38/1000 | Loss: 0.00008393
Iteration 39/1000 | Loss: 0.00019967
Iteration 40/1000 | Loss: 0.00005723
Iteration 41/1000 | Loss: 0.00033369
Iteration 42/1000 | Loss: 0.00037437
Iteration 43/1000 | Loss: 0.00003512
Iteration 44/1000 | Loss: 0.00004201
Iteration 45/1000 | Loss: 0.00004264
Iteration 46/1000 | Loss: 0.00007052
Iteration 47/1000 | Loss: 0.00004970
Iteration 48/1000 | Loss: 0.00002840
Iteration 49/1000 | Loss: 0.00048184
Iteration 50/1000 | Loss: 0.00010245
Iteration 51/1000 | Loss: 0.00036603
Iteration 52/1000 | Loss: 0.00012596
Iteration 53/1000 | Loss: 0.00015574
Iteration 54/1000 | Loss: 0.00023314
Iteration 55/1000 | Loss: 0.00002893
Iteration 56/1000 | Loss: 0.00002783
Iteration 57/1000 | Loss: 0.00002750
Iteration 58/1000 | Loss: 0.00004133
Iteration 59/1000 | Loss: 0.00002716
Iteration 60/1000 | Loss: 0.00003041
Iteration 61/1000 | Loss: 0.00035312
Iteration 62/1000 | Loss: 0.00009829
Iteration 63/1000 | Loss: 0.00039920
Iteration 64/1000 | Loss: 0.00063804
Iteration 65/1000 | Loss: 0.00040952
Iteration 66/1000 | Loss: 0.00003123
Iteration 67/1000 | Loss: 0.00005489
Iteration 68/1000 | Loss: 0.00003175
Iteration 69/1000 | Loss: 0.00005393
Iteration 70/1000 | Loss: 0.00002723
Iteration 71/1000 | Loss: 0.00002705
Iteration 72/1000 | Loss: 0.00002696
Iteration 73/1000 | Loss: 0.00006038
Iteration 74/1000 | Loss: 0.00002694
Iteration 75/1000 | Loss: 0.00002679
Iteration 76/1000 | Loss: 0.00002678
Iteration 77/1000 | Loss: 0.00002678
Iteration 78/1000 | Loss: 0.00002678
Iteration 79/1000 | Loss: 0.00002678
Iteration 80/1000 | Loss: 0.00002677
Iteration 81/1000 | Loss: 0.00002677
Iteration 82/1000 | Loss: 0.00002676
Iteration 83/1000 | Loss: 0.00042437
Iteration 84/1000 | Loss: 0.00031230
Iteration 85/1000 | Loss: 0.00004877
Iteration 86/1000 | Loss: 0.00021908
Iteration 87/1000 | Loss: 0.00003375
Iteration 88/1000 | Loss: 0.00003798
Iteration 89/1000 | Loss: 0.00002669
Iteration 90/1000 | Loss: 0.00048848
Iteration 91/1000 | Loss: 0.00009884
Iteration 92/1000 | Loss: 0.00027001
Iteration 93/1000 | Loss: 0.00003913
Iteration 94/1000 | Loss: 0.00002896
Iteration 95/1000 | Loss: 0.00002857
Iteration 96/1000 | Loss: 0.00002641
Iteration 97/1000 | Loss: 0.00033079
Iteration 98/1000 | Loss: 0.00003802
Iteration 99/1000 | Loss: 0.00003135
Iteration 100/1000 | Loss: 0.00029905
Iteration 101/1000 | Loss: 0.00003763
Iteration 102/1000 | Loss: 0.00003778
Iteration 103/1000 | Loss: 0.00003271
Iteration 104/1000 | Loss: 0.00003800
Iteration 105/1000 | Loss: 0.00002751
Iteration 106/1000 | Loss: 0.00005847
Iteration 107/1000 | Loss: 0.00003692
Iteration 108/1000 | Loss: 0.00008873
Iteration 109/1000 | Loss: 0.00003033
Iteration 110/1000 | Loss: 0.00005189
Iteration 111/1000 | Loss: 0.00002640
Iteration 112/1000 | Loss: 0.00002605
Iteration 113/1000 | Loss: 0.00002599
Iteration 114/1000 | Loss: 0.00002588
Iteration 115/1000 | Loss: 0.00004009
Iteration 116/1000 | Loss: 0.00002583
Iteration 117/1000 | Loss: 0.00002582
Iteration 118/1000 | Loss: 0.00002582
Iteration 119/1000 | Loss: 0.00002580
Iteration 120/1000 | Loss: 0.00002579
Iteration 121/1000 | Loss: 0.00002578
Iteration 122/1000 | Loss: 0.00002578
Iteration 123/1000 | Loss: 0.00002577
Iteration 124/1000 | Loss: 0.00002577
Iteration 125/1000 | Loss: 0.00002577
Iteration 126/1000 | Loss: 0.00002577
Iteration 127/1000 | Loss: 0.00002576
Iteration 128/1000 | Loss: 0.00002576
Iteration 129/1000 | Loss: 0.00002576
Iteration 130/1000 | Loss: 0.00002576
Iteration 131/1000 | Loss: 0.00002576
Iteration 132/1000 | Loss: 0.00002575
Iteration 133/1000 | Loss: 0.00002575
Iteration 134/1000 | Loss: 0.00002575
Iteration 135/1000 | Loss: 0.00002574
Iteration 136/1000 | Loss: 0.00004027
Iteration 137/1000 | Loss: 0.00003006
Iteration 138/1000 | Loss: 0.00002572
Iteration 139/1000 | Loss: 0.00003550
Iteration 140/1000 | Loss: 0.00004314
Iteration 141/1000 | Loss: 0.00006756
Iteration 142/1000 | Loss: 0.00003239
Iteration 143/1000 | Loss: 0.00002564
Iteration 144/1000 | Loss: 0.00002564
Iteration 145/1000 | Loss: 0.00002562
Iteration 146/1000 | Loss: 0.00003029
Iteration 147/1000 | Loss: 0.00038615
Iteration 148/1000 | Loss: 0.00002673
Iteration 149/1000 | Loss: 0.00009518
Iteration 150/1000 | Loss: 0.00002994
Iteration 151/1000 | Loss: 0.00005621
Iteration 152/1000 | Loss: 0.00020764
Iteration 153/1000 | Loss: 0.00022582
Iteration 154/1000 | Loss: 0.00002929
Iteration 155/1000 | Loss: 0.00002635
Iteration 156/1000 | Loss: 0.00002567
Iteration 157/1000 | Loss: 0.00002564
Iteration 158/1000 | Loss: 0.00002558
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002558
Iteration 162/1000 | Loss: 0.00002558
Iteration 163/1000 | Loss: 0.00002558
Iteration 164/1000 | Loss: 0.00002558
Iteration 165/1000 | Loss: 0.00002558
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002557
Iteration 168/1000 | Loss: 0.00002557
Iteration 169/1000 | Loss: 0.00002557
Iteration 170/1000 | Loss: 0.00002556
Iteration 171/1000 | Loss: 0.00002556
Iteration 172/1000 | Loss: 0.00002556
Iteration 173/1000 | Loss: 0.00002555
Iteration 174/1000 | Loss: 0.00002555
Iteration 175/1000 | Loss: 0.00002555
Iteration 176/1000 | Loss: 0.00002555
Iteration 177/1000 | Loss: 0.00002555
Iteration 178/1000 | Loss: 0.00002555
Iteration 179/1000 | Loss: 0.00002555
Iteration 180/1000 | Loss: 0.00002555
Iteration 181/1000 | Loss: 0.00002555
Iteration 182/1000 | Loss: 0.00002555
Iteration 183/1000 | Loss: 0.00002554
Iteration 184/1000 | Loss: 0.00002554
Iteration 185/1000 | Loss: 0.00002554
Iteration 186/1000 | Loss: 0.00002554
Iteration 187/1000 | Loss: 0.00002554
Iteration 188/1000 | Loss: 0.00002554
Iteration 189/1000 | Loss: 0.00002553
Iteration 190/1000 | Loss: 0.00002553
Iteration 191/1000 | Loss: 0.00002553
Iteration 192/1000 | Loss: 0.00002552
Iteration 193/1000 | Loss: 0.00002552
Iteration 194/1000 | Loss: 0.00002552
Iteration 195/1000 | Loss: 0.00002552
Iteration 196/1000 | Loss: 0.00002552
Iteration 197/1000 | Loss: 0.00002552
Iteration 198/1000 | Loss: 0.00002551
Iteration 199/1000 | Loss: 0.00002551
Iteration 200/1000 | Loss: 0.00002551
Iteration 201/1000 | Loss: 0.00005228
Iteration 202/1000 | Loss: 0.00003360
Iteration 203/1000 | Loss: 0.00030986
Iteration 204/1000 | Loss: 0.00005672
Iteration 205/1000 | Loss: 0.00003809
Iteration 206/1000 | Loss: 0.00002629
Iteration 207/1000 | Loss: 0.00010006
Iteration 208/1000 | Loss: 0.00003146
Iteration 209/1000 | Loss: 0.00004659
Iteration 210/1000 | Loss: 0.00002575
Iteration 211/1000 | Loss: 0.00002570
Iteration 212/1000 | Loss: 0.00006040
Iteration 213/1000 | Loss: 0.00002563
Iteration 214/1000 | Loss: 0.00002557
Iteration 215/1000 | Loss: 0.00002556
Iteration 216/1000 | Loss: 0.00002554
Iteration 217/1000 | Loss: 0.00002554
Iteration 218/1000 | Loss: 0.00002554
Iteration 219/1000 | Loss: 0.00002553
Iteration 220/1000 | Loss: 0.00002553
Iteration 221/1000 | Loss: 0.00002552
Iteration 222/1000 | Loss: 0.00002552
Iteration 223/1000 | Loss: 0.00002551
Iteration 224/1000 | Loss: 0.00002551
Iteration 225/1000 | Loss: 0.00002551
Iteration 226/1000 | Loss: 0.00002551
Iteration 227/1000 | Loss: 0.00002551
Iteration 228/1000 | Loss: 0.00002551
Iteration 229/1000 | Loss: 0.00002551
Iteration 230/1000 | Loss: 0.00002551
Iteration 231/1000 | Loss: 0.00002551
Iteration 232/1000 | Loss: 0.00002551
Iteration 233/1000 | Loss: 0.00002550
Iteration 234/1000 | Loss: 0.00002550
Iteration 235/1000 | Loss: 0.00002550
Iteration 236/1000 | Loss: 0.00002550
Iteration 237/1000 | Loss: 0.00002549
Iteration 238/1000 | Loss: 0.00002549
Iteration 239/1000 | Loss: 0.00002549
Iteration 240/1000 | Loss: 0.00002549
Iteration 241/1000 | Loss: 0.00002549
Iteration 242/1000 | Loss: 0.00002548
Iteration 243/1000 | Loss: 0.00002548
Iteration 244/1000 | Loss: 0.00002548
Iteration 245/1000 | Loss: 0.00002548
Iteration 246/1000 | Loss: 0.00002548
Iteration 247/1000 | Loss: 0.00002548
Iteration 248/1000 | Loss: 0.00002548
Iteration 249/1000 | Loss: 0.00002548
Iteration 250/1000 | Loss: 0.00002548
Iteration 251/1000 | Loss: 0.00002548
Iteration 252/1000 | Loss: 0.00002548
Iteration 253/1000 | Loss: 0.00002548
Iteration 254/1000 | Loss: 0.00002548
Iteration 255/1000 | Loss: 0.00002548
Iteration 256/1000 | Loss: 0.00002547
Iteration 257/1000 | Loss: 0.00002547
Iteration 258/1000 | Loss: 0.00002547
Iteration 259/1000 | Loss: 0.00002547
Iteration 260/1000 | Loss: 0.00002547
Iteration 261/1000 | Loss: 0.00002547
Iteration 262/1000 | Loss: 0.00002547
Iteration 263/1000 | Loss: 0.00002547
Iteration 264/1000 | Loss: 0.00002547
Iteration 265/1000 | Loss: 0.00002547
Iteration 266/1000 | Loss: 0.00002547
Iteration 267/1000 | Loss: 0.00002547
Iteration 268/1000 | Loss: 0.00002547
Iteration 269/1000 | Loss: 0.00002547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [2.5471948902122676e-05, 2.5471948902122676e-05, 2.5471948902122676e-05, 2.5471948902122676e-05, 2.5471948902122676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5471948902122676e-05

Optimization complete. Final v2v error: 4.337301254272461 mm

Highest mean error: 5.5632500648498535 mm for frame 6

Lowest mean error: 3.8624229431152344 mm for frame 129

Saving results

Total time: 232.4622004032135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042325
Iteration 2/25 | Loss: 0.01042325
Iteration 3/25 | Loss: 0.01042324
Iteration 4/25 | Loss: 0.01042324
Iteration 5/25 | Loss: 0.01042324
Iteration 6/25 | Loss: 0.00252614
Iteration 7/25 | Loss: 0.00164993
Iteration 8/25 | Loss: 0.00156597
Iteration 9/25 | Loss: 0.00156744
Iteration 10/25 | Loss: 0.00148626
Iteration 11/25 | Loss: 0.00144539
Iteration 12/25 | Loss: 0.00143556
Iteration 13/25 | Loss: 0.00139608
Iteration 14/25 | Loss: 0.00138433
Iteration 15/25 | Loss: 0.00139149
Iteration 16/25 | Loss: 0.00138068
Iteration 17/25 | Loss: 0.00136842
Iteration 18/25 | Loss: 0.00136347
Iteration 19/25 | Loss: 0.00135986
Iteration 20/25 | Loss: 0.00135128
Iteration 21/25 | Loss: 0.00134882
Iteration 22/25 | Loss: 0.00134780
Iteration 23/25 | Loss: 0.00134747
Iteration 24/25 | Loss: 0.00134734
Iteration 25/25 | Loss: 0.00134734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54753160
Iteration 2/25 | Loss: 0.00147138
Iteration 3/25 | Loss: 0.00147123
Iteration 4/25 | Loss: 0.00147012
Iteration 5/25 | Loss: 0.00147012
Iteration 6/25 | Loss: 0.00147012
Iteration 7/25 | Loss: 0.00147012
Iteration 8/25 | Loss: 0.00147131
Iteration 9/25 | Loss: 0.00147128
Iteration 10/25 | Loss: 0.00147144
Iteration 11/25 | Loss: 0.00147125
Iteration 12/25 | Loss: 0.00147130
Iteration 13/25 | Loss: 0.00147129
Iteration 14/25 | Loss: 0.00147132
Iteration 15/25 | Loss: 0.00147130
Iteration 16/25 | Loss: 0.00147138
Iteration 17/25 | Loss: 0.00147128
Iteration 18/25 | Loss: 0.00147142
Iteration 19/25 | Loss: 0.00147124
Iteration 20/25 | Loss: 0.00147135
Iteration 21/25 | Loss: 0.00147125
Iteration 22/25 | Loss: 0.00147141
Iteration 23/25 | Loss: 0.00147122
Iteration 24/25 | Loss: 0.00147012
Iteration 25/25 | Loss: 0.00147012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147012
Iteration 2/1000 | Loss: 0.00034667
Iteration 3/1000 | Loss: 0.00037922
Iteration 4/1000 | Loss: 0.00025593
Iteration 5/1000 | Loss: 0.00026715
Iteration 6/1000 | Loss: 0.00024001
Iteration 7/1000 | Loss: 0.00021678
Iteration 8/1000 | Loss: 0.00031644
Iteration 9/1000 | Loss: 0.00013606
Iteration 10/1000 | Loss: 0.00017199
Iteration 11/1000 | Loss: 0.00017710
Iteration 12/1000 | Loss: 0.00011234
Iteration 13/1000 | Loss: 0.00023694
Iteration 14/1000 | Loss: 0.00019760
Iteration 15/1000 | Loss: 0.00020906
Iteration 16/1000 | Loss: 0.00012592
Iteration 17/1000 | Loss: 0.00006600
Iteration 18/1000 | Loss: 0.00011375
Iteration 19/1000 | Loss: 0.00009884
Iteration 20/1000 | Loss: 0.00003301
Iteration 21/1000 | Loss: 0.00003093
Iteration 22/1000 | Loss: 0.00015565
Iteration 23/1000 | Loss: 0.00018706
Iteration 24/1000 | Loss: 0.00015850
Iteration 25/1000 | Loss: 0.00011705
Iteration 26/1000 | Loss: 0.00003273
Iteration 27/1000 | Loss: 0.00002869
Iteration 28/1000 | Loss: 0.00002635
Iteration 29/1000 | Loss: 0.00018787
Iteration 30/1000 | Loss: 0.00017135
Iteration 31/1000 | Loss: 0.00018648
Iteration 32/1000 | Loss: 0.00014354
Iteration 33/1000 | Loss: 0.00018742
Iteration 34/1000 | Loss: 0.00003662
Iteration 35/1000 | Loss: 0.00002965
Iteration 36/1000 | Loss: 0.00002624
Iteration 37/1000 | Loss: 0.00025806
Iteration 38/1000 | Loss: 0.00022653
Iteration 39/1000 | Loss: 0.00021903
Iteration 40/1000 | Loss: 0.00025485
Iteration 41/1000 | Loss: 0.00007326
Iteration 42/1000 | Loss: 0.00010970
Iteration 43/1000 | Loss: 0.00018387
Iteration 44/1000 | Loss: 0.00006417
Iteration 45/1000 | Loss: 0.00018130
Iteration 46/1000 | Loss: 0.00008146
Iteration 47/1000 | Loss: 0.00017964
Iteration 48/1000 | Loss: 0.00011678
Iteration 49/1000 | Loss: 0.00003525
Iteration 50/1000 | Loss: 0.00003137
Iteration 51/1000 | Loss: 0.00002710
Iteration 52/1000 | Loss: 0.00002543
Iteration 53/1000 | Loss: 0.00002449
Iteration 54/1000 | Loss: 0.00002341
Iteration 55/1000 | Loss: 0.00004134
Iteration 56/1000 | Loss: 0.00018173
Iteration 57/1000 | Loss: 0.00002879
Iteration 58/1000 | Loss: 0.00002769
Iteration 59/1000 | Loss: 0.00002305
Iteration 60/1000 | Loss: 0.00006241
Iteration 61/1000 | Loss: 0.00004936
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002138
Iteration 64/1000 | Loss: 0.00002093
Iteration 65/1000 | Loss: 0.00007749
Iteration 66/1000 | Loss: 0.00003671
Iteration 67/1000 | Loss: 0.00002051
Iteration 68/1000 | Loss: 0.00002010
Iteration 69/1000 | Loss: 0.00001980
Iteration 70/1000 | Loss: 0.00001976
Iteration 71/1000 | Loss: 0.00001971
Iteration 72/1000 | Loss: 0.00001966
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00013170
Iteration 75/1000 | Loss: 0.00030581
Iteration 76/1000 | Loss: 0.00005933
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001943
Iteration 79/1000 | Loss: 0.00001943
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001940
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001939
Iteration 85/1000 | Loss: 0.00001939
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001939
Iteration 90/1000 | Loss: 0.00001939
Iteration 91/1000 | Loss: 0.00001939
Iteration 92/1000 | Loss: 0.00001938
Iteration 93/1000 | Loss: 0.00001938
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00013582
Iteration 100/1000 | Loss: 0.00020013
Iteration 101/1000 | Loss: 0.00014541
Iteration 102/1000 | Loss: 0.00026528
Iteration 103/1000 | Loss: 0.00011551
Iteration 104/1000 | Loss: 0.00013871
Iteration 105/1000 | Loss: 0.00018175
Iteration 106/1000 | Loss: 0.00044956
Iteration 107/1000 | Loss: 0.00013522
Iteration 108/1000 | Loss: 0.00016974
Iteration 109/1000 | Loss: 0.00016297
Iteration 110/1000 | Loss: 0.00023847
Iteration 111/1000 | Loss: 0.00008671
Iteration 112/1000 | Loss: 0.00017748
Iteration 113/1000 | Loss: 0.00021981
Iteration 114/1000 | Loss: 0.00016132
Iteration 115/1000 | Loss: 0.00015998
Iteration 116/1000 | Loss: 0.00023510
Iteration 117/1000 | Loss: 0.00018320
Iteration 118/1000 | Loss: 0.00016850
Iteration 119/1000 | Loss: 0.00006603
Iteration 120/1000 | Loss: 0.00018407
Iteration 121/1000 | Loss: 0.00015321
Iteration 122/1000 | Loss: 0.00013515
Iteration 123/1000 | Loss: 0.00018692
Iteration 124/1000 | Loss: 0.00016862
Iteration 125/1000 | Loss: 0.00007208
Iteration 126/1000 | Loss: 0.00013513
Iteration 127/1000 | Loss: 0.00013959
Iteration 128/1000 | Loss: 0.00013058
Iteration 129/1000 | Loss: 0.00016075
Iteration 130/1000 | Loss: 0.00013791
Iteration 131/1000 | Loss: 0.00015272
Iteration 132/1000 | Loss: 0.00010405
Iteration 133/1000 | Loss: 0.00014560
Iteration 134/1000 | Loss: 0.00011092
Iteration 135/1000 | Loss: 0.00015974
Iteration 136/1000 | Loss: 0.00010319
Iteration 137/1000 | Loss: 0.00014720
Iteration 138/1000 | Loss: 0.00011250
Iteration 139/1000 | Loss: 0.00014497
Iteration 140/1000 | Loss: 0.00013928
Iteration 141/1000 | Loss: 0.00017810
Iteration 142/1000 | Loss: 0.00016273
Iteration 143/1000 | Loss: 0.00018904
Iteration 144/1000 | Loss: 0.00019987
Iteration 145/1000 | Loss: 0.00044027
Iteration 146/1000 | Loss: 0.00013422
Iteration 147/1000 | Loss: 0.00003836
Iteration 148/1000 | Loss: 0.00003259
Iteration 149/1000 | Loss: 0.00003814
Iteration 150/1000 | Loss: 0.00003471
Iteration 151/1000 | Loss: 0.00003036
Iteration 152/1000 | Loss: 0.00002501
Iteration 153/1000 | Loss: 0.00002832
Iteration 154/1000 | Loss: 0.00055816
Iteration 155/1000 | Loss: 0.00016175
Iteration 156/1000 | Loss: 0.00003495
Iteration 157/1000 | Loss: 0.00002790
Iteration 158/1000 | Loss: 0.00002529
Iteration 159/1000 | Loss: 0.00002393
Iteration 160/1000 | Loss: 0.00002334
Iteration 161/1000 | Loss: 0.00002242
Iteration 162/1000 | Loss: 0.00026345
Iteration 163/1000 | Loss: 0.00002349
Iteration 164/1000 | Loss: 0.00002209
Iteration 165/1000 | Loss: 0.00002134
Iteration 166/1000 | Loss: 0.00002049
Iteration 167/1000 | Loss: 0.00001974
Iteration 168/1000 | Loss: 0.00001926
Iteration 169/1000 | Loss: 0.00001888
Iteration 170/1000 | Loss: 0.00001857
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001834
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001822
Iteration 175/1000 | Loss: 0.00001814
Iteration 176/1000 | Loss: 0.00001807
Iteration 177/1000 | Loss: 0.00001805
Iteration 178/1000 | Loss: 0.00001804
Iteration 179/1000 | Loss: 0.00001799
Iteration 180/1000 | Loss: 0.00001796
Iteration 181/1000 | Loss: 0.00001796
Iteration 182/1000 | Loss: 0.00001794
Iteration 183/1000 | Loss: 0.00001794
Iteration 184/1000 | Loss: 0.00001794
Iteration 185/1000 | Loss: 0.00001794
Iteration 186/1000 | Loss: 0.00001794
Iteration 187/1000 | Loss: 0.00001794
Iteration 188/1000 | Loss: 0.00013474
Iteration 189/1000 | Loss: 0.00006771
Iteration 190/1000 | Loss: 0.00001810
Iteration 191/1000 | Loss: 0.00001792
Iteration 192/1000 | Loss: 0.00001792
Iteration 193/1000 | Loss: 0.00001791
Iteration 194/1000 | Loss: 0.00001791
Iteration 195/1000 | Loss: 0.00001791
Iteration 196/1000 | Loss: 0.00001791
Iteration 197/1000 | Loss: 0.00001791
Iteration 198/1000 | Loss: 0.00001791
Iteration 199/1000 | Loss: 0.00001790
Iteration 200/1000 | Loss: 0.00001790
Iteration 201/1000 | Loss: 0.00001790
Iteration 202/1000 | Loss: 0.00001790
Iteration 203/1000 | Loss: 0.00001790
Iteration 204/1000 | Loss: 0.00001790
Iteration 205/1000 | Loss: 0.00001789
Iteration 206/1000 | Loss: 0.00001789
Iteration 207/1000 | Loss: 0.00001788
Iteration 208/1000 | Loss: 0.00001788
Iteration 209/1000 | Loss: 0.00001788
Iteration 210/1000 | Loss: 0.00001788
Iteration 211/1000 | Loss: 0.00001788
Iteration 212/1000 | Loss: 0.00001788
Iteration 213/1000 | Loss: 0.00001787
Iteration 214/1000 | Loss: 0.00001787
Iteration 215/1000 | Loss: 0.00001787
Iteration 216/1000 | Loss: 0.00001787
Iteration 217/1000 | Loss: 0.00001787
Iteration 218/1000 | Loss: 0.00001787
Iteration 219/1000 | Loss: 0.00001787
Iteration 220/1000 | Loss: 0.00001787
Iteration 221/1000 | Loss: 0.00001786
Iteration 222/1000 | Loss: 0.00001786
Iteration 223/1000 | Loss: 0.00001786
Iteration 224/1000 | Loss: 0.00001786
Iteration 225/1000 | Loss: 0.00001786
Iteration 226/1000 | Loss: 0.00001785
Iteration 227/1000 | Loss: 0.00001785
Iteration 228/1000 | Loss: 0.00001784
Iteration 229/1000 | Loss: 0.00001784
Iteration 230/1000 | Loss: 0.00001783
Iteration 231/1000 | Loss: 0.00001783
Iteration 232/1000 | Loss: 0.00001783
Iteration 233/1000 | Loss: 0.00001783
Iteration 234/1000 | Loss: 0.00001783
Iteration 235/1000 | Loss: 0.00001782
Iteration 236/1000 | Loss: 0.00001782
Iteration 237/1000 | Loss: 0.00001782
Iteration 238/1000 | Loss: 0.00001782
Iteration 239/1000 | Loss: 0.00001782
Iteration 240/1000 | Loss: 0.00001782
Iteration 241/1000 | Loss: 0.00001782
Iteration 242/1000 | Loss: 0.00001782
Iteration 243/1000 | Loss: 0.00001782
Iteration 244/1000 | Loss: 0.00001782
Iteration 245/1000 | Loss: 0.00001782
Iteration 246/1000 | Loss: 0.00001781
Iteration 247/1000 | Loss: 0.00001781
Iteration 248/1000 | Loss: 0.00001781
Iteration 249/1000 | Loss: 0.00001781
Iteration 250/1000 | Loss: 0.00001781
Iteration 251/1000 | Loss: 0.00001781
Iteration 252/1000 | Loss: 0.00001781
Iteration 253/1000 | Loss: 0.00001781
Iteration 254/1000 | Loss: 0.00001781
Iteration 255/1000 | Loss: 0.00001781
Iteration 256/1000 | Loss: 0.00001780
Iteration 257/1000 | Loss: 0.00001780
Iteration 258/1000 | Loss: 0.00001780
Iteration 259/1000 | Loss: 0.00001780
Iteration 260/1000 | Loss: 0.00001780
Iteration 261/1000 | Loss: 0.00001780
Iteration 262/1000 | Loss: 0.00001780
Iteration 263/1000 | Loss: 0.00001780
Iteration 264/1000 | Loss: 0.00001780
Iteration 265/1000 | Loss: 0.00001780
Iteration 266/1000 | Loss: 0.00001780
Iteration 267/1000 | Loss: 0.00001780
Iteration 268/1000 | Loss: 0.00001780
Iteration 269/1000 | Loss: 0.00001780
Iteration 270/1000 | Loss: 0.00001779
Iteration 271/1000 | Loss: 0.00001779
Iteration 272/1000 | Loss: 0.00001779
Iteration 273/1000 | Loss: 0.00001779
Iteration 274/1000 | Loss: 0.00001779
Iteration 275/1000 | Loss: 0.00001779
Iteration 276/1000 | Loss: 0.00001779
Iteration 277/1000 | Loss: 0.00001779
Iteration 278/1000 | Loss: 0.00001779
Iteration 279/1000 | Loss: 0.00001779
Iteration 280/1000 | Loss: 0.00001778
Iteration 281/1000 | Loss: 0.00001778
Iteration 282/1000 | Loss: 0.00001778
Iteration 283/1000 | Loss: 0.00001778
Iteration 284/1000 | Loss: 0.00001778
Iteration 285/1000 | Loss: 0.00001778
Iteration 286/1000 | Loss: 0.00001778
Iteration 287/1000 | Loss: 0.00001778
Iteration 288/1000 | Loss: 0.00001778
Iteration 289/1000 | Loss: 0.00001778
Iteration 290/1000 | Loss: 0.00001778
Iteration 291/1000 | Loss: 0.00001778
Iteration 292/1000 | Loss: 0.00001778
Iteration 293/1000 | Loss: 0.00001778
Iteration 294/1000 | Loss: 0.00001778
Iteration 295/1000 | Loss: 0.00001778
Iteration 296/1000 | Loss: 0.00001777
Iteration 297/1000 | Loss: 0.00001777
Iteration 298/1000 | Loss: 0.00001777
Iteration 299/1000 | Loss: 0.00001777
Iteration 300/1000 | Loss: 0.00001777
Iteration 301/1000 | Loss: 0.00001777
Iteration 302/1000 | Loss: 0.00001777
Iteration 303/1000 | Loss: 0.00001777
Iteration 304/1000 | Loss: 0.00013704
Iteration 305/1000 | Loss: 0.00005789
Iteration 306/1000 | Loss: 0.00002100
Iteration 307/1000 | Loss: 0.00001870
Iteration 308/1000 | Loss: 0.00011169
Iteration 309/1000 | Loss: 0.00004721
Iteration 310/1000 | Loss: 0.00001866
Iteration 311/1000 | Loss: 0.00001789
Iteration 312/1000 | Loss: 0.00001782
Iteration 313/1000 | Loss: 0.00001782
Iteration 314/1000 | Loss: 0.00001782
Iteration 315/1000 | Loss: 0.00001782
Iteration 316/1000 | Loss: 0.00001782
Iteration 317/1000 | Loss: 0.00001782
Iteration 318/1000 | Loss: 0.00001782
Iteration 319/1000 | Loss: 0.00001782
Iteration 320/1000 | Loss: 0.00001782
Iteration 321/1000 | Loss: 0.00001782
Iteration 322/1000 | Loss: 0.00001782
Iteration 323/1000 | Loss: 0.00001781
Iteration 324/1000 | Loss: 0.00001781
Iteration 325/1000 | Loss: 0.00001781
Iteration 326/1000 | Loss: 0.00001781
Iteration 327/1000 | Loss: 0.00001781
Iteration 328/1000 | Loss: 0.00001781
Iteration 329/1000 | Loss: 0.00001781
Iteration 330/1000 | Loss: 0.00001780
Iteration 331/1000 | Loss: 0.00001780
Iteration 332/1000 | Loss: 0.00001780
Iteration 333/1000 | Loss: 0.00001780
Iteration 334/1000 | Loss: 0.00001780
Iteration 335/1000 | Loss: 0.00001779
Iteration 336/1000 | Loss: 0.00001779
Iteration 337/1000 | Loss: 0.00001779
Iteration 338/1000 | Loss: 0.00001779
Iteration 339/1000 | Loss: 0.00001778
Iteration 340/1000 | Loss: 0.00001778
Iteration 341/1000 | Loss: 0.00001778
Iteration 342/1000 | Loss: 0.00001778
Iteration 343/1000 | Loss: 0.00001778
Iteration 344/1000 | Loss: 0.00001778
Iteration 345/1000 | Loss: 0.00001778
Iteration 346/1000 | Loss: 0.00001778
Iteration 347/1000 | Loss: 0.00001777
Iteration 348/1000 | Loss: 0.00001777
Iteration 349/1000 | Loss: 0.00001777
Iteration 350/1000 | Loss: 0.00001777
Iteration 351/1000 | Loss: 0.00001777
Iteration 352/1000 | Loss: 0.00001777
Iteration 353/1000 | Loss: 0.00001777
Iteration 354/1000 | Loss: 0.00001777
Iteration 355/1000 | Loss: 0.00001777
Iteration 356/1000 | Loss: 0.00001777
Iteration 357/1000 | Loss: 0.00001776
Iteration 358/1000 | Loss: 0.00001776
Iteration 359/1000 | Loss: 0.00001776
Iteration 360/1000 | Loss: 0.00001776
Iteration 361/1000 | Loss: 0.00001776
Iteration 362/1000 | Loss: 0.00001776
Iteration 363/1000 | Loss: 0.00001776
Iteration 364/1000 | Loss: 0.00001776
Iteration 365/1000 | Loss: 0.00001776
Iteration 366/1000 | Loss: 0.00001776
Iteration 367/1000 | Loss: 0.00001776
Iteration 368/1000 | Loss: 0.00001776
Iteration 369/1000 | Loss: 0.00001776
Iteration 370/1000 | Loss: 0.00001776
Iteration 371/1000 | Loss: 0.00001776
Iteration 372/1000 | Loss: 0.00001775
Iteration 373/1000 | Loss: 0.00001775
Iteration 374/1000 | Loss: 0.00001775
Iteration 375/1000 | Loss: 0.00001775
Iteration 376/1000 | Loss: 0.00001775
Iteration 377/1000 | Loss: 0.00001775
Iteration 378/1000 | Loss: 0.00001775
Iteration 379/1000 | Loss: 0.00001775
Iteration 380/1000 | Loss: 0.00001775
Iteration 381/1000 | Loss: 0.00001775
Iteration 382/1000 | Loss: 0.00001775
Iteration 383/1000 | Loss: 0.00001775
Iteration 384/1000 | Loss: 0.00001775
Iteration 385/1000 | Loss: 0.00001775
Iteration 386/1000 | Loss: 0.00001775
Iteration 387/1000 | Loss: 0.00001775
Iteration 388/1000 | Loss: 0.00001775
Iteration 389/1000 | Loss: 0.00001775
Iteration 390/1000 | Loss: 0.00001775
Iteration 391/1000 | Loss: 0.00001775
Iteration 392/1000 | Loss: 0.00001775
Iteration 393/1000 | Loss: 0.00001775
Iteration 394/1000 | Loss: 0.00001774
Iteration 395/1000 | Loss: 0.00001774
Iteration 396/1000 | Loss: 0.00001774
Iteration 397/1000 | Loss: 0.00001774
Iteration 398/1000 | Loss: 0.00001774
Iteration 399/1000 | Loss: 0.00001774
Iteration 400/1000 | Loss: 0.00001774
Iteration 401/1000 | Loss: 0.00001774
Iteration 402/1000 | Loss: 0.00001774
Iteration 403/1000 | Loss: 0.00001774
Iteration 404/1000 | Loss: 0.00001774
Iteration 405/1000 | Loss: 0.00001774
Iteration 406/1000 | Loss: 0.00001774
Iteration 407/1000 | Loss: 0.00001774
Iteration 408/1000 | Loss: 0.00001774
Iteration 409/1000 | Loss: 0.00001774
Iteration 410/1000 | Loss: 0.00001774
Iteration 411/1000 | Loss: 0.00001774
Iteration 412/1000 | Loss: 0.00001774
Iteration 413/1000 | Loss: 0.00001774
Iteration 414/1000 | Loss: 0.00001774
Iteration 415/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 415. Stopping optimization.
Last 5 losses: [1.7738897440722212e-05, 1.7738897440722212e-05, 1.7738897440722212e-05, 1.7738897440722212e-05, 1.7738897440722212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7738897440722212e-05

Optimization complete. Final v2v error: 3.328050136566162 mm

Highest mean error: 6.349626064300537 mm for frame 12

Lowest mean error: 2.845384120941162 mm for frame 162

Saving results

Total time: 319.9263594150543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981344
Iteration 2/25 | Loss: 0.00266023
Iteration 3/25 | Loss: 0.00196054
Iteration 4/25 | Loss: 0.00191410
Iteration 5/25 | Loss: 0.00184405
Iteration 6/25 | Loss: 0.00167882
Iteration 7/25 | Loss: 0.00166469
Iteration 8/25 | Loss: 0.00160899
Iteration 9/25 | Loss: 0.00159783
Iteration 10/25 | Loss: 0.00157650
Iteration 11/25 | Loss: 0.00154293
Iteration 12/25 | Loss: 0.00154225
Iteration 13/25 | Loss: 0.00153915
Iteration 14/25 | Loss: 0.00154378
Iteration 15/25 | Loss: 0.00152911
Iteration 16/25 | Loss: 0.00153749
Iteration 17/25 | Loss: 0.00151564
Iteration 18/25 | Loss: 0.00151474
Iteration 19/25 | Loss: 0.00149114
Iteration 20/25 | Loss: 0.00147640
Iteration 21/25 | Loss: 0.00147421
Iteration 22/25 | Loss: 0.00146045
Iteration 23/25 | Loss: 0.00145240
Iteration 24/25 | Loss: 0.00145058
Iteration 25/25 | Loss: 0.00144963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39278316
Iteration 2/25 | Loss: 0.00205019
Iteration 3/25 | Loss: 0.00205019
Iteration 4/25 | Loss: 0.00205019
Iteration 5/25 | Loss: 0.00203405
Iteration 6/25 | Loss: 0.00203405
Iteration 7/25 | Loss: 0.00203404
Iteration 8/25 | Loss: 0.00203404
Iteration 9/25 | Loss: 0.00203404
Iteration 10/25 | Loss: 0.00203404
Iteration 11/25 | Loss: 0.00203404
Iteration 12/25 | Loss: 0.00203404
Iteration 13/25 | Loss: 0.00203404
Iteration 14/25 | Loss: 0.00203404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0020340438932180405, 0.0020340438932180405, 0.0020340438932180405, 0.0020340438932180405, 0.0020340438932180405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020340438932180405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203404
Iteration 2/1000 | Loss: 0.00025864
Iteration 3/1000 | Loss: 0.00026178
Iteration 4/1000 | Loss: 0.00037289
Iteration 5/1000 | Loss: 0.00076771
Iteration 6/1000 | Loss: 0.00018485
Iteration 7/1000 | Loss: 0.00027303
Iteration 8/1000 | Loss: 0.00047595
Iteration 9/1000 | Loss: 0.00033733
Iteration 10/1000 | Loss: 0.00013090
Iteration 11/1000 | Loss: 0.00018316
Iteration 12/1000 | Loss: 0.00023978
Iteration 13/1000 | Loss: 0.00034537
Iteration 14/1000 | Loss: 0.00075833
Iteration 15/1000 | Loss: 0.00135911
Iteration 16/1000 | Loss: 0.00043723
Iteration 17/1000 | Loss: 0.00078972
Iteration 18/1000 | Loss: 0.00066310
Iteration 19/1000 | Loss: 0.00151859
Iteration 20/1000 | Loss: 0.00059473
Iteration 21/1000 | Loss: 0.00019920
Iteration 22/1000 | Loss: 0.00050602
Iteration 23/1000 | Loss: 0.00046462
Iteration 24/1000 | Loss: 0.00033565
Iteration 25/1000 | Loss: 0.00070462
Iteration 26/1000 | Loss: 0.00019649
Iteration 27/1000 | Loss: 0.00020520
Iteration 28/1000 | Loss: 0.00013937
Iteration 29/1000 | Loss: 0.00009890
Iteration 30/1000 | Loss: 0.00021210
Iteration 31/1000 | Loss: 0.00032061
Iteration 32/1000 | Loss: 0.00017274
Iteration 33/1000 | Loss: 0.00014745
Iteration 34/1000 | Loss: 0.00018325
Iteration 35/1000 | Loss: 0.00015672
Iteration 36/1000 | Loss: 0.00012831
Iteration 37/1000 | Loss: 0.00009799
Iteration 38/1000 | Loss: 0.00006891
Iteration 39/1000 | Loss: 0.00033794
Iteration 40/1000 | Loss: 0.00034204
Iteration 41/1000 | Loss: 0.00009069
Iteration 42/1000 | Loss: 0.00028652
Iteration 43/1000 | Loss: 0.00005759
Iteration 44/1000 | Loss: 0.00017259
Iteration 45/1000 | Loss: 0.00031016
Iteration 46/1000 | Loss: 0.00007897
Iteration 47/1000 | Loss: 0.00010284
Iteration 48/1000 | Loss: 0.00017014
Iteration 49/1000 | Loss: 0.00005102
Iteration 50/1000 | Loss: 0.00009598
Iteration 51/1000 | Loss: 0.00004593
Iteration 52/1000 | Loss: 0.00007423
Iteration 53/1000 | Loss: 0.00012799
Iteration 54/1000 | Loss: 0.00005470
Iteration 55/1000 | Loss: 0.00024552
Iteration 56/1000 | Loss: 0.00009535
Iteration 57/1000 | Loss: 0.00004672
Iteration 58/1000 | Loss: 0.00003862
Iteration 59/1000 | Loss: 0.00005312
Iteration 60/1000 | Loss: 0.00006820
Iteration 61/1000 | Loss: 0.00003579
Iteration 62/1000 | Loss: 0.00005449
Iteration 63/1000 | Loss: 0.00003520
Iteration 64/1000 | Loss: 0.00006450
Iteration 65/1000 | Loss: 0.00003736
Iteration 66/1000 | Loss: 0.00007200
Iteration 67/1000 | Loss: 0.00003435
Iteration 68/1000 | Loss: 0.00006129
Iteration 69/1000 | Loss: 0.00003411
Iteration 70/1000 | Loss: 0.00003379
Iteration 71/1000 | Loss: 0.00006766
Iteration 72/1000 | Loss: 0.00006178
Iteration 73/1000 | Loss: 0.00006310
Iteration 74/1000 | Loss: 0.00007795
Iteration 75/1000 | Loss: 0.00003336
Iteration 76/1000 | Loss: 0.00003316
Iteration 77/1000 | Loss: 0.00003314
Iteration 78/1000 | Loss: 0.00005362
Iteration 79/1000 | Loss: 0.00003298
Iteration 80/1000 | Loss: 0.00005275
Iteration 81/1000 | Loss: 0.00003482
Iteration 82/1000 | Loss: 0.00003293
Iteration 83/1000 | Loss: 0.00003291
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003289
Iteration 86/1000 | Loss: 0.00003289
Iteration 87/1000 | Loss: 0.00003287
Iteration 88/1000 | Loss: 0.00003343
Iteration 89/1000 | Loss: 0.00003282
Iteration 90/1000 | Loss: 0.00003282
Iteration 91/1000 | Loss: 0.00003282
Iteration 92/1000 | Loss: 0.00003282
Iteration 93/1000 | Loss: 0.00003282
Iteration 94/1000 | Loss: 0.00003282
Iteration 95/1000 | Loss: 0.00003282
Iteration 96/1000 | Loss: 0.00003282
Iteration 97/1000 | Loss: 0.00003282
Iteration 98/1000 | Loss: 0.00003282
Iteration 99/1000 | Loss: 0.00003282
Iteration 100/1000 | Loss: 0.00003282
Iteration 101/1000 | Loss: 0.00003282
Iteration 102/1000 | Loss: 0.00003281
Iteration 103/1000 | Loss: 0.00003281
Iteration 104/1000 | Loss: 0.00003281
Iteration 105/1000 | Loss: 0.00003281
Iteration 106/1000 | Loss: 0.00003281
Iteration 107/1000 | Loss: 0.00003280
Iteration 108/1000 | Loss: 0.00003280
Iteration 109/1000 | Loss: 0.00003280
Iteration 110/1000 | Loss: 0.00003476
Iteration 111/1000 | Loss: 0.00004992
Iteration 112/1000 | Loss: 0.00003273
Iteration 113/1000 | Loss: 0.00003273
Iteration 114/1000 | Loss: 0.00003273
Iteration 115/1000 | Loss: 0.00003273
Iteration 116/1000 | Loss: 0.00003272
Iteration 117/1000 | Loss: 0.00003272
Iteration 118/1000 | Loss: 0.00003272
Iteration 119/1000 | Loss: 0.00003272
Iteration 120/1000 | Loss: 0.00003272
Iteration 121/1000 | Loss: 0.00003272
Iteration 122/1000 | Loss: 0.00003272
Iteration 123/1000 | Loss: 0.00003272
Iteration 124/1000 | Loss: 0.00003272
Iteration 125/1000 | Loss: 0.00003272
Iteration 126/1000 | Loss: 0.00003272
Iteration 127/1000 | Loss: 0.00003272
Iteration 128/1000 | Loss: 0.00003272
Iteration 129/1000 | Loss: 0.00003272
Iteration 130/1000 | Loss: 0.00003272
Iteration 131/1000 | Loss: 0.00003272
Iteration 132/1000 | Loss: 0.00003271
Iteration 133/1000 | Loss: 0.00003271
Iteration 134/1000 | Loss: 0.00003271
Iteration 135/1000 | Loss: 0.00003271
Iteration 136/1000 | Loss: 0.00003271
Iteration 137/1000 | Loss: 0.00003271
Iteration 138/1000 | Loss: 0.00003271
Iteration 139/1000 | Loss: 0.00003271
Iteration 140/1000 | Loss: 0.00003270
Iteration 141/1000 | Loss: 0.00003270
Iteration 142/1000 | Loss: 0.00003270
Iteration 143/1000 | Loss: 0.00003270
Iteration 144/1000 | Loss: 0.00003270
Iteration 145/1000 | Loss: 0.00004259
Iteration 146/1000 | Loss: 0.00003269
Iteration 147/1000 | Loss: 0.00003269
Iteration 148/1000 | Loss: 0.00003269
Iteration 149/1000 | Loss: 0.00003269
Iteration 150/1000 | Loss: 0.00005040
Iteration 151/1000 | Loss: 0.00005040
Iteration 152/1000 | Loss: 0.00005040
Iteration 153/1000 | Loss: 0.00005039
Iteration 154/1000 | Loss: 0.00005039
Iteration 155/1000 | Loss: 0.00023120
Iteration 156/1000 | Loss: 0.00080999
Iteration 157/1000 | Loss: 0.00012719
Iteration 158/1000 | Loss: 0.00006052
Iteration 159/1000 | Loss: 0.00005656
Iteration 160/1000 | Loss: 0.00004906
Iteration 161/1000 | Loss: 0.00003520
Iteration 162/1000 | Loss: 0.00012634
Iteration 163/1000 | Loss: 0.00005555
Iteration 164/1000 | Loss: 0.00005457
Iteration 165/1000 | Loss: 0.00003364
Iteration 166/1000 | Loss: 0.00003317
Iteration 167/1000 | Loss: 0.00004249
Iteration 168/1000 | Loss: 0.00010042
Iteration 169/1000 | Loss: 0.00003664
Iteration 170/1000 | Loss: 0.00003373
Iteration 171/1000 | Loss: 0.00003241
Iteration 172/1000 | Loss: 0.00003240
Iteration 173/1000 | Loss: 0.00003240
Iteration 174/1000 | Loss: 0.00003240
Iteration 175/1000 | Loss: 0.00003240
Iteration 176/1000 | Loss: 0.00003240
Iteration 177/1000 | Loss: 0.00003240
Iteration 178/1000 | Loss: 0.00003240
Iteration 179/1000 | Loss: 0.00003240
Iteration 180/1000 | Loss: 0.00003240
Iteration 181/1000 | Loss: 0.00003239
Iteration 182/1000 | Loss: 0.00003239
Iteration 183/1000 | Loss: 0.00003238
Iteration 184/1000 | Loss: 0.00003238
Iteration 185/1000 | Loss: 0.00003238
Iteration 186/1000 | Loss: 0.00003238
Iteration 187/1000 | Loss: 0.00003238
Iteration 188/1000 | Loss: 0.00003237
Iteration 189/1000 | Loss: 0.00003680
Iteration 190/1000 | Loss: 0.00004972
Iteration 191/1000 | Loss: 0.00003324
Iteration 192/1000 | Loss: 0.00003233
Iteration 193/1000 | Loss: 0.00003233
Iteration 194/1000 | Loss: 0.00003510
Iteration 195/1000 | Loss: 0.00003771
Iteration 196/1000 | Loss: 0.00003259
Iteration 197/1000 | Loss: 0.00003225
Iteration 198/1000 | Loss: 0.00004912
Iteration 199/1000 | Loss: 0.00003324
Iteration 200/1000 | Loss: 0.00003225
Iteration 201/1000 | Loss: 0.00003225
Iteration 202/1000 | Loss: 0.00003225
Iteration 203/1000 | Loss: 0.00003225
Iteration 204/1000 | Loss: 0.00003225
Iteration 205/1000 | Loss: 0.00003224
Iteration 206/1000 | Loss: 0.00003224
Iteration 207/1000 | Loss: 0.00003224
Iteration 208/1000 | Loss: 0.00003224
Iteration 209/1000 | Loss: 0.00003224
Iteration 210/1000 | Loss: 0.00003223
Iteration 211/1000 | Loss: 0.00003679
Iteration 212/1000 | Loss: 0.00003222
Iteration 213/1000 | Loss: 0.00003221
Iteration 214/1000 | Loss: 0.00003221
Iteration 215/1000 | Loss: 0.00003221
Iteration 216/1000 | Loss: 0.00003221
Iteration 217/1000 | Loss: 0.00003221
Iteration 218/1000 | Loss: 0.00003221
Iteration 219/1000 | Loss: 0.00003221
Iteration 220/1000 | Loss: 0.00003221
Iteration 221/1000 | Loss: 0.00003221
Iteration 222/1000 | Loss: 0.00003221
Iteration 223/1000 | Loss: 0.00003221
Iteration 224/1000 | Loss: 0.00003221
Iteration 225/1000 | Loss: 0.00003221
Iteration 226/1000 | Loss: 0.00003221
Iteration 227/1000 | Loss: 0.00003221
Iteration 228/1000 | Loss: 0.00003221
Iteration 229/1000 | Loss: 0.00003221
Iteration 230/1000 | Loss: 0.00003221
Iteration 231/1000 | Loss: 0.00003221
Iteration 232/1000 | Loss: 0.00003221
Iteration 233/1000 | Loss: 0.00003221
Iteration 234/1000 | Loss: 0.00003221
Iteration 235/1000 | Loss: 0.00003221
Iteration 236/1000 | Loss: 0.00003221
Iteration 237/1000 | Loss: 0.00003221
Iteration 238/1000 | Loss: 0.00003221
Iteration 239/1000 | Loss: 0.00003221
Iteration 240/1000 | Loss: 0.00003221
Iteration 241/1000 | Loss: 0.00003221
Iteration 242/1000 | Loss: 0.00003221
Iteration 243/1000 | Loss: 0.00003221
Iteration 244/1000 | Loss: 0.00003221
Iteration 245/1000 | Loss: 0.00003221
Iteration 246/1000 | Loss: 0.00003221
Iteration 247/1000 | Loss: 0.00003221
Iteration 248/1000 | Loss: 0.00003221
Iteration 249/1000 | Loss: 0.00003221
Iteration 250/1000 | Loss: 0.00003221
Iteration 251/1000 | Loss: 0.00003221
Iteration 252/1000 | Loss: 0.00003221
Iteration 253/1000 | Loss: 0.00003221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [3.221196311642416e-05, 3.221196311642416e-05, 3.221196311642416e-05, 3.221196311642416e-05, 3.221196311642416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.221196311642416e-05

Optimization complete. Final v2v error: 4.06241512298584 mm

Highest mean error: 10.70631217956543 mm for frame 159

Lowest mean error: 3.758075475692749 mm for frame 88

Saving results

Total time: 231.6940052509308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066810
Iteration 2/25 | Loss: 0.00266190
Iteration 3/25 | Loss: 0.00191689
Iteration 4/25 | Loss: 0.00176317
Iteration 5/25 | Loss: 0.00169842
Iteration 6/25 | Loss: 0.00172388
Iteration 7/25 | Loss: 0.00167879
Iteration 8/25 | Loss: 0.00162665
Iteration 9/25 | Loss: 0.00157252
Iteration 10/25 | Loss: 0.00154920
Iteration 11/25 | Loss: 0.00154120
Iteration 12/25 | Loss: 0.00154625
Iteration 13/25 | Loss: 0.00155510
Iteration 14/25 | Loss: 0.00150158
Iteration 15/25 | Loss: 0.00150311
Iteration 16/25 | Loss: 0.00148738
Iteration 17/25 | Loss: 0.00146942
Iteration 18/25 | Loss: 0.00145058
Iteration 19/25 | Loss: 0.00143806
Iteration 20/25 | Loss: 0.00142925
Iteration 21/25 | Loss: 0.00143626
Iteration 22/25 | Loss: 0.00143234
Iteration 23/25 | Loss: 0.00142904
Iteration 24/25 | Loss: 0.00142790
Iteration 25/25 | Loss: 0.00142610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12297440
Iteration 2/25 | Loss: 0.00127512
Iteration 3/25 | Loss: 0.00115434
Iteration 4/25 | Loss: 0.00115434
Iteration 5/25 | Loss: 0.00115434
Iteration 6/25 | Loss: 0.00115434
Iteration 7/25 | Loss: 0.00115434
Iteration 8/25 | Loss: 0.00115434
Iteration 9/25 | Loss: 0.00115434
Iteration 10/25 | Loss: 0.00115434
Iteration 11/25 | Loss: 0.00115434
Iteration 12/25 | Loss: 0.00115434
Iteration 13/25 | Loss: 0.00115434
Iteration 14/25 | Loss: 0.00115434
Iteration 15/25 | Loss: 0.00115434
Iteration 16/25 | Loss: 0.00115434
Iteration 17/25 | Loss: 0.00115434
Iteration 18/25 | Loss: 0.00115434
Iteration 19/25 | Loss: 0.00115434
Iteration 20/25 | Loss: 0.00115434
Iteration 21/25 | Loss: 0.00115434
Iteration 22/25 | Loss: 0.00115434
Iteration 23/25 | Loss: 0.00115434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011543401051312685, 0.0011543401051312685, 0.0011543401051312685, 0.0011543401051312685, 0.0011543401051312685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011543401051312685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115434
Iteration 2/1000 | Loss: 0.00033482
Iteration 3/1000 | Loss: 0.00009747
Iteration 4/1000 | Loss: 0.00009071
Iteration 5/1000 | Loss: 0.00006249
Iteration 6/1000 | Loss: 0.00006394
Iteration 7/1000 | Loss: 0.00005400
Iteration 8/1000 | Loss: 0.00006366
Iteration 9/1000 | Loss: 0.00006782
Iteration 10/1000 | Loss: 0.00005352
Iteration 11/1000 | Loss: 0.00006046
Iteration 12/1000 | Loss: 0.00005544
Iteration 13/1000 | Loss: 0.00015258
Iteration 14/1000 | Loss: 0.00004955
Iteration 15/1000 | Loss: 0.00011754
Iteration 16/1000 | Loss: 0.00006930
Iteration 17/1000 | Loss: 0.00007037
Iteration 18/1000 | Loss: 0.00006033
Iteration 19/1000 | Loss: 0.00007316
Iteration 20/1000 | Loss: 0.00005806
Iteration 21/1000 | Loss: 0.00006766
Iteration 22/1000 | Loss: 0.00006353
Iteration 23/1000 | Loss: 0.00007001
Iteration 24/1000 | Loss: 0.00006900
Iteration 25/1000 | Loss: 0.00007730
Iteration 26/1000 | Loss: 0.00006770
Iteration 27/1000 | Loss: 0.00005575
Iteration 28/1000 | Loss: 0.00005242
Iteration 29/1000 | Loss: 0.00006039
Iteration 30/1000 | Loss: 0.00005884
Iteration 31/1000 | Loss: 0.00088045
Iteration 32/1000 | Loss: 0.00048921
Iteration 33/1000 | Loss: 0.00022273
Iteration 34/1000 | Loss: 0.00005747
Iteration 35/1000 | Loss: 0.00004636
Iteration 36/1000 | Loss: 0.00009083
Iteration 37/1000 | Loss: 0.00007531
Iteration 38/1000 | Loss: 0.00009683
Iteration 39/1000 | Loss: 0.00003089
Iteration 40/1000 | Loss: 0.00002814
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002513
Iteration 43/1000 | Loss: 0.00002433
Iteration 44/1000 | Loss: 0.00002373
Iteration 45/1000 | Loss: 0.00002327
Iteration 46/1000 | Loss: 0.00002287
Iteration 47/1000 | Loss: 0.00012015
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00006769
Iteration 50/1000 | Loss: 0.00002272
Iteration 51/1000 | Loss: 0.00002233
Iteration 52/1000 | Loss: 0.00002220
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002204
Iteration 58/1000 | Loss: 0.00002203
Iteration 59/1000 | Loss: 0.00002203
Iteration 60/1000 | Loss: 0.00002202
Iteration 61/1000 | Loss: 0.00002202
Iteration 62/1000 | Loss: 0.00002201
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002201
Iteration 65/1000 | Loss: 0.00002199
Iteration 66/1000 | Loss: 0.00002199
Iteration 67/1000 | Loss: 0.00002198
Iteration 68/1000 | Loss: 0.00002196
Iteration 69/1000 | Loss: 0.00002195
Iteration 70/1000 | Loss: 0.00002195
Iteration 71/1000 | Loss: 0.00002195
Iteration 72/1000 | Loss: 0.00002194
Iteration 73/1000 | Loss: 0.00002194
Iteration 74/1000 | Loss: 0.00002194
Iteration 75/1000 | Loss: 0.00002193
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002192
Iteration 79/1000 | Loss: 0.00002192
Iteration 80/1000 | Loss: 0.00002192
Iteration 81/1000 | Loss: 0.00002192
Iteration 82/1000 | Loss: 0.00002191
Iteration 83/1000 | Loss: 0.00002191
Iteration 84/1000 | Loss: 0.00002191
Iteration 85/1000 | Loss: 0.00002191
Iteration 86/1000 | Loss: 0.00002190
Iteration 87/1000 | Loss: 0.00002190
Iteration 88/1000 | Loss: 0.00002189
Iteration 89/1000 | Loss: 0.00002189
Iteration 90/1000 | Loss: 0.00002189
Iteration 91/1000 | Loss: 0.00002188
Iteration 92/1000 | Loss: 0.00002188
Iteration 93/1000 | Loss: 0.00002188
Iteration 94/1000 | Loss: 0.00002187
Iteration 95/1000 | Loss: 0.00002187
Iteration 96/1000 | Loss: 0.00002187
Iteration 97/1000 | Loss: 0.00002186
Iteration 98/1000 | Loss: 0.00002186
Iteration 99/1000 | Loss: 0.00002186
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00002184
Iteration 111/1000 | Loss: 0.00002184
Iteration 112/1000 | Loss: 0.00002184
Iteration 113/1000 | Loss: 0.00002183
Iteration 114/1000 | Loss: 0.00002183
Iteration 115/1000 | Loss: 0.00002182
Iteration 116/1000 | Loss: 0.00002182
Iteration 117/1000 | Loss: 0.00002182
Iteration 118/1000 | Loss: 0.00002181
Iteration 119/1000 | Loss: 0.00002181
Iteration 120/1000 | Loss: 0.00002181
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Iteration 124/1000 | Loss: 0.00002180
Iteration 125/1000 | Loss: 0.00002179
Iteration 126/1000 | Loss: 0.00002179
Iteration 127/1000 | Loss: 0.00002179
Iteration 128/1000 | Loss: 0.00002179
Iteration 129/1000 | Loss: 0.00002179
Iteration 130/1000 | Loss: 0.00002179
Iteration 131/1000 | Loss: 0.00002179
Iteration 132/1000 | Loss: 0.00002178
Iteration 133/1000 | Loss: 0.00002178
Iteration 134/1000 | Loss: 0.00002178
Iteration 135/1000 | Loss: 0.00002178
Iteration 136/1000 | Loss: 0.00002178
Iteration 137/1000 | Loss: 0.00002178
Iteration 138/1000 | Loss: 0.00002178
Iteration 139/1000 | Loss: 0.00002178
Iteration 140/1000 | Loss: 0.00002178
Iteration 141/1000 | Loss: 0.00002178
Iteration 142/1000 | Loss: 0.00002178
Iteration 143/1000 | Loss: 0.00002177
Iteration 144/1000 | Loss: 0.00002177
Iteration 145/1000 | Loss: 0.00002177
Iteration 146/1000 | Loss: 0.00002177
Iteration 147/1000 | Loss: 0.00002177
Iteration 148/1000 | Loss: 0.00002176
Iteration 149/1000 | Loss: 0.00002176
Iteration 150/1000 | Loss: 0.00002176
Iteration 151/1000 | Loss: 0.00002176
Iteration 152/1000 | Loss: 0.00002176
Iteration 153/1000 | Loss: 0.00002176
Iteration 154/1000 | Loss: 0.00032426
Iteration 155/1000 | Loss: 0.00039177
Iteration 156/1000 | Loss: 0.00014447
Iteration 157/1000 | Loss: 0.00002190
Iteration 158/1000 | Loss: 0.00002176
Iteration 159/1000 | Loss: 0.00002176
Iteration 160/1000 | Loss: 0.00002176
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002175
Iteration 165/1000 | Loss: 0.00002175
Iteration 166/1000 | Loss: 0.00002175
Iteration 167/1000 | Loss: 0.00002175
Iteration 168/1000 | Loss: 0.00002175
Iteration 169/1000 | Loss: 0.00002175
Iteration 170/1000 | Loss: 0.00002175
Iteration 171/1000 | Loss: 0.00002175
Iteration 172/1000 | Loss: 0.00002175
Iteration 173/1000 | Loss: 0.00002175
Iteration 174/1000 | Loss: 0.00002175
Iteration 175/1000 | Loss: 0.00002175
Iteration 176/1000 | Loss: 0.00002174
Iteration 177/1000 | Loss: 0.00002174
Iteration 178/1000 | Loss: 0.00002174
Iteration 179/1000 | Loss: 0.00002174
Iteration 180/1000 | Loss: 0.00002174
Iteration 181/1000 | Loss: 0.00002174
Iteration 182/1000 | Loss: 0.00002174
Iteration 183/1000 | Loss: 0.00002174
Iteration 184/1000 | Loss: 0.00002174
Iteration 185/1000 | Loss: 0.00002174
Iteration 186/1000 | Loss: 0.00002174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.174051223846618e-05, 2.174051223846618e-05, 2.174051223846618e-05, 2.174051223846618e-05, 2.174051223846618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.174051223846618e-05

Optimization complete. Final v2v error: 3.9115402698516846 mm

Highest mean error: 7.115591049194336 mm for frame 108

Lowest mean error: 3.723700761795044 mm for frame 0

Saving results

Total time: 133.0935561656952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763805
Iteration 2/25 | Loss: 0.00180686
Iteration 3/25 | Loss: 0.00147673
Iteration 4/25 | Loss: 0.00145495
Iteration 5/25 | Loss: 0.00145301
Iteration 6/25 | Loss: 0.00145301
Iteration 7/25 | Loss: 0.00145301
Iteration 8/25 | Loss: 0.00145301
Iteration 9/25 | Loss: 0.00145301
Iteration 10/25 | Loss: 0.00145301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001453005475923419, 0.001453005475923419, 0.001453005475923419, 0.001453005475923419, 0.001453005475923419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001453005475923419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39709342
Iteration 2/25 | Loss: 0.00093613
Iteration 3/25 | Loss: 0.00093611
Iteration 4/25 | Loss: 0.00093611
Iteration 5/25 | Loss: 0.00093611
Iteration 6/25 | Loss: 0.00093611
Iteration 7/25 | Loss: 0.00093611
Iteration 8/25 | Loss: 0.00093611
Iteration 9/25 | Loss: 0.00093610
Iteration 10/25 | Loss: 0.00093610
Iteration 11/25 | Loss: 0.00093610
Iteration 12/25 | Loss: 0.00093610
Iteration 13/25 | Loss: 0.00093610
Iteration 14/25 | Loss: 0.00093610
Iteration 15/25 | Loss: 0.00093610
Iteration 16/25 | Loss: 0.00093610
Iteration 17/25 | Loss: 0.00093610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009361043339595199, 0.0009361043339595199, 0.0009361043339595199, 0.0009361043339595199, 0.0009361043339595199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009361043339595199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093610
Iteration 2/1000 | Loss: 0.00005123
Iteration 3/1000 | Loss: 0.00003573
Iteration 4/1000 | Loss: 0.00003213
Iteration 5/1000 | Loss: 0.00003046
Iteration 6/1000 | Loss: 0.00002963
Iteration 7/1000 | Loss: 0.00002914
Iteration 8/1000 | Loss: 0.00002873
Iteration 9/1000 | Loss: 0.00002842
Iteration 10/1000 | Loss: 0.00002817
Iteration 11/1000 | Loss: 0.00002802
Iteration 12/1000 | Loss: 0.00002784
Iteration 13/1000 | Loss: 0.00002781
Iteration 14/1000 | Loss: 0.00002780
Iteration 15/1000 | Loss: 0.00002776
Iteration 16/1000 | Loss: 0.00002771
Iteration 17/1000 | Loss: 0.00002755
Iteration 18/1000 | Loss: 0.00002754
Iteration 19/1000 | Loss: 0.00002753
Iteration 20/1000 | Loss: 0.00002752
Iteration 21/1000 | Loss: 0.00002752
Iteration 22/1000 | Loss: 0.00002751
Iteration 23/1000 | Loss: 0.00002751
Iteration 24/1000 | Loss: 0.00002744
Iteration 25/1000 | Loss: 0.00002743
Iteration 26/1000 | Loss: 0.00002742
Iteration 27/1000 | Loss: 0.00002742
Iteration 28/1000 | Loss: 0.00002741
Iteration 29/1000 | Loss: 0.00002741
Iteration 30/1000 | Loss: 0.00002741
Iteration 31/1000 | Loss: 0.00002741
Iteration 32/1000 | Loss: 0.00002741
Iteration 33/1000 | Loss: 0.00002741
Iteration 34/1000 | Loss: 0.00002741
Iteration 35/1000 | Loss: 0.00002740
Iteration 36/1000 | Loss: 0.00002740
Iteration 37/1000 | Loss: 0.00002740
Iteration 38/1000 | Loss: 0.00002740
Iteration 39/1000 | Loss: 0.00002740
Iteration 40/1000 | Loss: 0.00002740
Iteration 41/1000 | Loss: 0.00002740
Iteration 42/1000 | Loss: 0.00002740
Iteration 43/1000 | Loss: 0.00002739
Iteration 44/1000 | Loss: 0.00002739
Iteration 45/1000 | Loss: 0.00002739
Iteration 46/1000 | Loss: 0.00002739
Iteration 47/1000 | Loss: 0.00002739
Iteration 48/1000 | Loss: 0.00002739
Iteration 49/1000 | Loss: 0.00002739
Iteration 50/1000 | Loss: 0.00002738
Iteration 51/1000 | Loss: 0.00002738
Iteration 52/1000 | Loss: 0.00002738
Iteration 53/1000 | Loss: 0.00002738
Iteration 54/1000 | Loss: 0.00002738
Iteration 55/1000 | Loss: 0.00002738
Iteration 56/1000 | Loss: 0.00002738
Iteration 57/1000 | Loss: 0.00002738
Iteration 58/1000 | Loss: 0.00002738
Iteration 59/1000 | Loss: 0.00002738
Iteration 60/1000 | Loss: 0.00002738
Iteration 61/1000 | Loss: 0.00002737
Iteration 62/1000 | Loss: 0.00002737
Iteration 63/1000 | Loss: 0.00002737
Iteration 64/1000 | Loss: 0.00002736
Iteration 65/1000 | Loss: 0.00002736
Iteration 66/1000 | Loss: 0.00002736
Iteration 67/1000 | Loss: 0.00002736
Iteration 68/1000 | Loss: 0.00002736
Iteration 69/1000 | Loss: 0.00002736
Iteration 70/1000 | Loss: 0.00002736
Iteration 71/1000 | Loss: 0.00002736
Iteration 72/1000 | Loss: 0.00002735
Iteration 73/1000 | Loss: 0.00002735
Iteration 74/1000 | Loss: 0.00002735
Iteration 75/1000 | Loss: 0.00002735
Iteration 76/1000 | Loss: 0.00002735
Iteration 77/1000 | Loss: 0.00002735
Iteration 78/1000 | Loss: 0.00002734
Iteration 79/1000 | Loss: 0.00002734
Iteration 80/1000 | Loss: 0.00002734
Iteration 81/1000 | Loss: 0.00002733
Iteration 82/1000 | Loss: 0.00002733
Iteration 83/1000 | Loss: 0.00002733
Iteration 84/1000 | Loss: 0.00002733
Iteration 85/1000 | Loss: 0.00002733
Iteration 86/1000 | Loss: 0.00002733
Iteration 87/1000 | Loss: 0.00002733
Iteration 88/1000 | Loss: 0.00002733
Iteration 89/1000 | Loss: 0.00002733
Iteration 90/1000 | Loss: 0.00002733
Iteration 91/1000 | Loss: 0.00002733
Iteration 92/1000 | Loss: 0.00002733
Iteration 93/1000 | Loss: 0.00002733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.732548091444187e-05, 2.732548091444187e-05, 2.732548091444187e-05, 2.732548091444187e-05, 2.732548091444187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.732548091444187e-05

Optimization complete. Final v2v error: 4.383693218231201 mm

Highest mean error: 4.577588081359863 mm for frame 54

Lowest mean error: 4.220600128173828 mm for frame 109

Saving results

Total time: 32.475496768951416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598152
Iteration 2/25 | Loss: 0.00133801
Iteration 3/25 | Loss: 0.00127285
Iteration 4/25 | Loss: 0.00126134
Iteration 5/25 | Loss: 0.00125756
Iteration 6/25 | Loss: 0.00125687
Iteration 7/25 | Loss: 0.00125687
Iteration 8/25 | Loss: 0.00125687
Iteration 9/25 | Loss: 0.00125687
Iteration 10/25 | Loss: 0.00125687
Iteration 11/25 | Loss: 0.00125687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012568737147375941, 0.0012568737147375941, 0.0012568737147375941, 0.0012568737147375941, 0.0012568737147375941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012568737147375941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95223832
Iteration 2/25 | Loss: 0.00086652
Iteration 3/25 | Loss: 0.00086652
Iteration 4/25 | Loss: 0.00086652
Iteration 5/25 | Loss: 0.00086652
Iteration 6/25 | Loss: 0.00086652
Iteration 7/25 | Loss: 0.00086652
Iteration 8/25 | Loss: 0.00086651
Iteration 9/25 | Loss: 0.00086651
Iteration 10/25 | Loss: 0.00086651
Iteration 11/25 | Loss: 0.00086651
Iteration 12/25 | Loss: 0.00086651
Iteration 13/25 | Loss: 0.00086651
Iteration 14/25 | Loss: 0.00086651
Iteration 15/25 | Loss: 0.00086651
Iteration 16/25 | Loss: 0.00086651
Iteration 17/25 | Loss: 0.00086651
Iteration 18/25 | Loss: 0.00086651
Iteration 19/25 | Loss: 0.00086651
Iteration 20/25 | Loss: 0.00086651
Iteration 21/25 | Loss: 0.00086651
Iteration 22/25 | Loss: 0.00086651
Iteration 23/25 | Loss: 0.00086651
Iteration 24/25 | Loss: 0.00086651
Iteration 25/25 | Loss: 0.00086651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086651
Iteration 2/1000 | Loss: 0.00002861
Iteration 3/1000 | Loss: 0.00001958
Iteration 4/1000 | Loss: 0.00001653
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001437
Iteration 8/1000 | Loss: 0.00001420
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001346
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001326
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001280
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001273
Iteration 46/1000 | Loss: 0.00001273
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001270
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001270
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001267
Iteration 64/1000 | Loss: 0.00001267
Iteration 65/1000 | Loss: 0.00001267
Iteration 66/1000 | Loss: 0.00001267
Iteration 67/1000 | Loss: 0.00001267
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001267
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001260
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001260
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001259
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001257
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001256
Iteration 131/1000 | Loss: 0.00001256
Iteration 132/1000 | Loss: 0.00001256
Iteration 133/1000 | Loss: 0.00001256
Iteration 134/1000 | Loss: 0.00001256
Iteration 135/1000 | Loss: 0.00001256
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001255
Iteration 145/1000 | Loss: 0.00001255
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001254
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001252
Iteration 159/1000 | Loss: 0.00001252
Iteration 160/1000 | Loss: 0.00001252
Iteration 161/1000 | Loss: 0.00001252
Iteration 162/1000 | Loss: 0.00001252
Iteration 163/1000 | Loss: 0.00001252
Iteration 164/1000 | Loss: 0.00001252
Iteration 165/1000 | Loss: 0.00001252
Iteration 166/1000 | Loss: 0.00001252
Iteration 167/1000 | Loss: 0.00001251
Iteration 168/1000 | Loss: 0.00001251
Iteration 169/1000 | Loss: 0.00001251
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001250
Iteration 174/1000 | Loss: 0.00001250
Iteration 175/1000 | Loss: 0.00001250
Iteration 176/1000 | Loss: 0.00001250
Iteration 177/1000 | Loss: 0.00001250
Iteration 178/1000 | Loss: 0.00001250
Iteration 179/1000 | Loss: 0.00001250
Iteration 180/1000 | Loss: 0.00001250
Iteration 181/1000 | Loss: 0.00001250
Iteration 182/1000 | Loss: 0.00001250
Iteration 183/1000 | Loss: 0.00001250
Iteration 184/1000 | Loss: 0.00001250
Iteration 185/1000 | Loss: 0.00001250
Iteration 186/1000 | Loss: 0.00001250
Iteration 187/1000 | Loss: 0.00001250
Iteration 188/1000 | Loss: 0.00001249
Iteration 189/1000 | Loss: 0.00001249
Iteration 190/1000 | Loss: 0.00001249
Iteration 191/1000 | Loss: 0.00001249
Iteration 192/1000 | Loss: 0.00001249
Iteration 193/1000 | Loss: 0.00001249
Iteration 194/1000 | Loss: 0.00001249
Iteration 195/1000 | Loss: 0.00001249
Iteration 196/1000 | Loss: 0.00001249
Iteration 197/1000 | Loss: 0.00001249
Iteration 198/1000 | Loss: 0.00001249
Iteration 199/1000 | Loss: 0.00001249
Iteration 200/1000 | Loss: 0.00001249
Iteration 201/1000 | Loss: 0.00001249
Iteration 202/1000 | Loss: 0.00001248
Iteration 203/1000 | Loss: 0.00001248
Iteration 204/1000 | Loss: 0.00001248
Iteration 205/1000 | Loss: 0.00001248
Iteration 206/1000 | Loss: 0.00001248
Iteration 207/1000 | Loss: 0.00001248
Iteration 208/1000 | Loss: 0.00001248
Iteration 209/1000 | Loss: 0.00001248
Iteration 210/1000 | Loss: 0.00001248
Iteration 211/1000 | Loss: 0.00001248
Iteration 212/1000 | Loss: 0.00001248
Iteration 213/1000 | Loss: 0.00001248
Iteration 214/1000 | Loss: 0.00001248
Iteration 215/1000 | Loss: 0.00001248
Iteration 216/1000 | Loss: 0.00001248
Iteration 217/1000 | Loss: 0.00001248
Iteration 218/1000 | Loss: 0.00001248
Iteration 219/1000 | Loss: 0.00001248
Iteration 220/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.2478861208364833e-05, 1.2478861208364833e-05, 1.2478861208364833e-05, 1.2478861208364833e-05, 1.2478861208364833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2478861208364833e-05

Optimization complete. Final v2v error: 3.0417280197143555 mm

Highest mean error: 3.3047802448272705 mm for frame 143

Lowest mean error: 2.8685412406921387 mm for frame 169

Saving results

Total time: 44.23032999038696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956630
Iteration 2/25 | Loss: 0.00381493
Iteration 3/25 | Loss: 0.00299073
Iteration 4/25 | Loss: 0.00244615
Iteration 5/25 | Loss: 0.00208225
Iteration 6/25 | Loss: 0.00202043
Iteration 7/25 | Loss: 0.00199956
Iteration 8/25 | Loss: 0.00199487
Iteration 9/25 | Loss: 0.00198940
Iteration 10/25 | Loss: 0.00198586
Iteration 11/25 | Loss: 0.00197955
Iteration 12/25 | Loss: 0.00197620
Iteration 13/25 | Loss: 0.00197444
Iteration 14/25 | Loss: 0.00197441
Iteration 15/25 | Loss: 0.00197426
Iteration 16/25 | Loss: 0.00197426
Iteration 17/25 | Loss: 0.00197406
Iteration 18/25 | Loss: 0.00197406
Iteration 19/25 | Loss: 0.00197406
Iteration 20/25 | Loss: 0.00197405
Iteration 21/25 | Loss: 0.00197405
Iteration 22/25 | Loss: 0.00197405
Iteration 23/25 | Loss: 0.00197405
Iteration 24/25 | Loss: 0.00197405
Iteration 25/25 | Loss: 0.00197405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34432018
Iteration 2/25 | Loss: 0.00443029
Iteration 3/25 | Loss: 0.00441727
Iteration 4/25 | Loss: 0.00441729
Iteration 5/25 | Loss: 0.00442348
Iteration 6/25 | Loss: 0.00440455
Iteration 7/25 | Loss: 0.00440455
Iteration 8/25 | Loss: 0.00440455
Iteration 9/25 | Loss: 0.00440455
Iteration 10/25 | Loss: 0.00440455
Iteration 11/25 | Loss: 0.00440455
Iteration 12/25 | Loss: 0.00440455
Iteration 13/25 | Loss: 0.00440455
Iteration 14/25 | Loss: 0.00440455
Iteration 15/25 | Loss: 0.00440455
Iteration 16/25 | Loss: 0.00440455
Iteration 17/25 | Loss: 0.00440455
Iteration 18/25 | Loss: 0.00440455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0044045462273061275, 0.0044045462273061275, 0.0044045462273061275, 0.0044045462273061275, 0.0044045462273061275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0044045462273061275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00440455
Iteration 2/1000 | Loss: 0.00061467
Iteration 3/1000 | Loss: 0.00050830
Iteration 4/1000 | Loss: 0.00042560
Iteration 5/1000 | Loss: 0.00169554
Iteration 6/1000 | Loss: 0.00041072
Iteration 7/1000 | Loss: 0.00036353
Iteration 8/1000 | Loss: 0.00034454
Iteration 9/1000 | Loss: 0.00033151
Iteration 10/1000 | Loss: 0.00031966
Iteration 11/1000 | Loss: 0.00241010
Iteration 12/1000 | Loss: 0.00030383
Iteration 13/1000 | Loss: 0.00047599
Iteration 14/1000 | Loss: 0.00038423
Iteration 15/1000 | Loss: 0.00525547
Iteration 16/1000 | Loss: 0.01586456
Iteration 17/1000 | Loss: 0.00101303
Iteration 18/1000 | Loss: 0.00047157
Iteration 19/1000 | Loss: 0.00034410
Iteration 20/1000 | Loss: 0.00024896
Iteration 21/1000 | Loss: 0.00021848
Iteration 22/1000 | Loss: 0.00018245
Iteration 23/1000 | Loss: 0.00010118
Iteration 24/1000 | Loss: 0.00017779
Iteration 25/1000 | Loss: 0.00013401
Iteration 26/1000 | Loss: 0.00005716
Iteration 27/1000 | Loss: 0.00004991
Iteration 28/1000 | Loss: 0.00007064
Iteration 29/1000 | Loss: 0.00008939
Iteration 30/1000 | Loss: 0.00003989
Iteration 31/1000 | Loss: 0.00008626
Iteration 32/1000 | Loss: 0.00005940
Iteration 33/1000 | Loss: 0.00007943
Iteration 34/1000 | Loss: 0.00023968
Iteration 35/1000 | Loss: 0.00004679
Iteration 36/1000 | Loss: 0.00009022
Iteration 37/1000 | Loss: 0.00009934
Iteration 38/1000 | Loss: 0.00010458
Iteration 39/1000 | Loss: 0.00003306
Iteration 40/1000 | Loss: 0.00003555
Iteration 41/1000 | Loss: 0.00002207
Iteration 42/1000 | Loss: 0.00002201
Iteration 43/1000 | Loss: 0.00002810
Iteration 44/1000 | Loss: 0.00002863
Iteration 45/1000 | Loss: 0.00005960
Iteration 46/1000 | Loss: 0.00002105
Iteration 47/1000 | Loss: 0.00002899
Iteration 48/1000 | Loss: 0.00002704
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001990
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001989
Iteration 53/1000 | Loss: 0.00001989
Iteration 54/1000 | Loss: 0.00002100
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001982
Iteration 58/1000 | Loss: 0.00001982
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00001982
Iteration 61/1000 | Loss: 0.00001981
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001981
Iteration 65/1000 | Loss: 0.00001981
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00001980
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001994
Iteration 71/1000 | Loss: 0.00002620
Iteration 72/1000 | Loss: 0.00001980
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001961
Iteration 76/1000 | Loss: 0.00001961
Iteration 77/1000 | Loss: 0.00001961
Iteration 78/1000 | Loss: 0.00001961
Iteration 79/1000 | Loss: 0.00001961
Iteration 80/1000 | Loss: 0.00001961
Iteration 81/1000 | Loss: 0.00001961
Iteration 82/1000 | Loss: 0.00001961
Iteration 83/1000 | Loss: 0.00001961
Iteration 84/1000 | Loss: 0.00001961
Iteration 85/1000 | Loss: 0.00001961
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001960
Iteration 88/1000 | Loss: 0.00001960
Iteration 89/1000 | Loss: 0.00001960
Iteration 90/1000 | Loss: 0.00001960
Iteration 91/1000 | Loss: 0.00001960
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001960
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.96046876226319e-05, 1.96046876226319e-05, 1.96046876226319e-05, 1.96046876226319e-05, 1.96046876226319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.96046876226319e-05

Optimization complete. Final v2v error: 3.768399953842163 mm

Highest mean error: 3.9139537811279297 mm for frame 27

Lowest mean error: 3.323033332824707 mm for frame 130

Saving results

Total time: 105.56606149673462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041047
Iteration 2/25 | Loss: 0.00315729
Iteration 3/25 | Loss: 0.00250257
Iteration 4/25 | Loss: 0.00210291
Iteration 5/25 | Loss: 0.00174398
Iteration 6/25 | Loss: 0.00165669
Iteration 7/25 | Loss: 0.00150911
Iteration 8/25 | Loss: 0.00151135
Iteration 9/25 | Loss: 0.00146418
Iteration 10/25 | Loss: 0.00141482
Iteration 11/25 | Loss: 0.00137672
Iteration 12/25 | Loss: 0.00136683
Iteration 13/25 | Loss: 0.00136292
Iteration 14/25 | Loss: 0.00136114
Iteration 15/25 | Loss: 0.00135573
Iteration 16/25 | Loss: 0.00135079
Iteration 17/25 | Loss: 0.00134907
Iteration 18/25 | Loss: 0.00134864
Iteration 19/25 | Loss: 0.00134859
Iteration 20/25 | Loss: 0.00134859
Iteration 21/25 | Loss: 0.00134859
Iteration 22/25 | Loss: 0.00134859
Iteration 23/25 | Loss: 0.00134859
Iteration 24/25 | Loss: 0.00134858
Iteration 25/25 | Loss: 0.00134858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44600546
Iteration 2/25 | Loss: 0.00137004
Iteration 3/25 | Loss: 0.00092159
Iteration 4/25 | Loss: 0.00092159
Iteration 5/25 | Loss: 0.00092159
Iteration 6/25 | Loss: 0.00092159
Iteration 7/25 | Loss: 0.00092159
Iteration 8/25 | Loss: 0.00092159
Iteration 9/25 | Loss: 0.00092159
Iteration 10/25 | Loss: 0.00092159
Iteration 11/25 | Loss: 0.00092159
Iteration 12/25 | Loss: 0.00092159
Iteration 13/25 | Loss: 0.00092159
Iteration 14/25 | Loss: 0.00092159
Iteration 15/25 | Loss: 0.00092159
Iteration 16/25 | Loss: 0.00092159
Iteration 17/25 | Loss: 0.00092159
Iteration 18/25 | Loss: 0.00092159
Iteration 19/25 | Loss: 0.00092159
Iteration 20/25 | Loss: 0.00092159
Iteration 21/25 | Loss: 0.00092159
Iteration 22/25 | Loss: 0.00092159
Iteration 23/25 | Loss: 0.00092159
Iteration 24/25 | Loss: 0.00092159
Iteration 25/25 | Loss: 0.00092159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092159
Iteration 2/1000 | Loss: 0.00239796
Iteration 3/1000 | Loss: 0.00042327
Iteration 4/1000 | Loss: 0.00135311
Iteration 5/1000 | Loss: 0.00247534
Iteration 6/1000 | Loss: 0.00118795
Iteration 7/1000 | Loss: 0.00004088
Iteration 8/1000 | Loss: 0.00002884
Iteration 9/1000 | Loss: 0.00002577
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002322
Iteration 12/1000 | Loss: 0.00002249
Iteration 13/1000 | Loss: 0.00002196
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002136
Iteration 16/1000 | Loss: 0.00053480
Iteration 17/1000 | Loss: 0.00002464
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001862
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001836
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001809
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001788
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001783
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001782
Iteration 66/1000 | Loss: 0.00001782
Iteration 67/1000 | Loss: 0.00001782
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001782
Iteration 70/1000 | Loss: 0.00001782
Iteration 71/1000 | Loss: 0.00001782
Iteration 72/1000 | Loss: 0.00001782
Iteration 73/1000 | Loss: 0.00001782
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001782
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001782
Iteration 96/1000 | Loss: 0.00001782
Iteration 97/1000 | Loss: 0.00001782
Iteration 98/1000 | Loss: 0.00001782
Iteration 99/1000 | Loss: 0.00001782
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001782
Iteration 103/1000 | Loss: 0.00001782
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001782
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001782
Iteration 124/1000 | Loss: 0.00001782
Iteration 125/1000 | Loss: 0.00001782
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001782
Iteration 128/1000 | Loss: 0.00001782
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.7822803783928975e-05, 1.7822803783928975e-05, 1.7822803783928975e-05, 1.7822803783928975e-05, 1.7822803783928975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7822803783928975e-05

Optimization complete. Final v2v error: 3.580406665802002 mm

Highest mean error: 4.600044250488281 mm for frame 12

Lowest mean error: 3.1176393032073975 mm for frame 122

Saving results

Total time: 74.49898552894592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441453
Iteration 2/25 | Loss: 0.00136260
Iteration 3/25 | Loss: 0.00129672
Iteration 4/25 | Loss: 0.00128269
Iteration 5/25 | Loss: 0.00127812
Iteration 6/25 | Loss: 0.00127789
Iteration 7/25 | Loss: 0.00127789
Iteration 8/25 | Loss: 0.00127789
Iteration 9/25 | Loss: 0.00127789
Iteration 10/25 | Loss: 0.00127789
Iteration 11/25 | Loss: 0.00127789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012778900563716888, 0.0012778900563716888, 0.0012778900563716888, 0.0012778900563716888, 0.0012778900563716888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012778900563716888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.26833725
Iteration 2/25 | Loss: 0.00082101
Iteration 3/25 | Loss: 0.00082101
Iteration 4/25 | Loss: 0.00082101
Iteration 5/25 | Loss: 0.00082101
Iteration 6/25 | Loss: 0.00082101
Iteration 7/25 | Loss: 0.00082101
Iteration 8/25 | Loss: 0.00082101
Iteration 9/25 | Loss: 0.00082101
Iteration 10/25 | Loss: 0.00082101
Iteration 11/25 | Loss: 0.00082101
Iteration 12/25 | Loss: 0.00082101
Iteration 13/25 | Loss: 0.00082101
Iteration 14/25 | Loss: 0.00082101
Iteration 15/25 | Loss: 0.00082101
Iteration 16/25 | Loss: 0.00082101
Iteration 17/25 | Loss: 0.00082101
Iteration 18/25 | Loss: 0.00082101
Iteration 19/25 | Loss: 0.00082101
Iteration 20/25 | Loss: 0.00082101
Iteration 21/25 | Loss: 0.00082101
Iteration 22/25 | Loss: 0.00082101
Iteration 23/25 | Loss: 0.00082101
Iteration 24/25 | Loss: 0.00082101
Iteration 25/25 | Loss: 0.00082101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082101
Iteration 2/1000 | Loss: 0.00002473
Iteration 3/1000 | Loss: 0.00002038
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001826
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001673
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001625
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001618
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001564
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001563
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001556
Iteration 30/1000 | Loss: 0.00001556
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001533
Iteration 45/1000 | Loss: 0.00001528
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001520
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001511
Iteration 74/1000 | Loss: 0.00001511
Iteration 75/1000 | Loss: 0.00001511
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001511
Iteration 78/1000 | Loss: 0.00001511
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001510
Iteration 83/1000 | Loss: 0.00001510
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001510
Iteration 94/1000 | Loss: 0.00001510
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001509
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001507
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001507
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001507
Iteration 143/1000 | Loss: 0.00001507
Iteration 144/1000 | Loss: 0.00001506
Iteration 145/1000 | Loss: 0.00001506
Iteration 146/1000 | Loss: 0.00001506
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001506
Iteration 149/1000 | Loss: 0.00001506
Iteration 150/1000 | Loss: 0.00001506
Iteration 151/1000 | Loss: 0.00001506
Iteration 152/1000 | Loss: 0.00001506
Iteration 153/1000 | Loss: 0.00001506
Iteration 154/1000 | Loss: 0.00001506
Iteration 155/1000 | Loss: 0.00001506
Iteration 156/1000 | Loss: 0.00001506
Iteration 157/1000 | Loss: 0.00001506
Iteration 158/1000 | Loss: 0.00001506
Iteration 159/1000 | Loss: 0.00001506
Iteration 160/1000 | Loss: 0.00001506
Iteration 161/1000 | Loss: 0.00001506
Iteration 162/1000 | Loss: 0.00001506
Iteration 163/1000 | Loss: 0.00001506
Iteration 164/1000 | Loss: 0.00001506
Iteration 165/1000 | Loss: 0.00001506
Iteration 166/1000 | Loss: 0.00001506
Iteration 167/1000 | Loss: 0.00001506
Iteration 168/1000 | Loss: 0.00001506
Iteration 169/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.5058461940498091e-05, 1.5058461940498091e-05, 1.5058461940498091e-05, 1.5058461940498091e-05, 1.5058461940498091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5058461940498091e-05

Optimization complete. Final v2v error: 3.2887518405914307 mm

Highest mean error: 3.6683695316314697 mm for frame 173

Lowest mean error: 2.993199110031128 mm for frame 79

Saving results

Total time: 46.680264711380005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800584
Iteration 2/25 | Loss: 0.00154974
Iteration 3/25 | Loss: 0.00132578
Iteration 4/25 | Loss: 0.00130298
Iteration 5/25 | Loss: 0.00130005
Iteration 6/25 | Loss: 0.00129934
Iteration 7/25 | Loss: 0.00129913
Iteration 8/25 | Loss: 0.00129913
Iteration 9/25 | Loss: 0.00129913
Iteration 10/25 | Loss: 0.00129913
Iteration 11/25 | Loss: 0.00129913
Iteration 12/25 | Loss: 0.00129913
Iteration 13/25 | Loss: 0.00129913
Iteration 14/25 | Loss: 0.00129913
Iteration 15/25 | Loss: 0.00129913
Iteration 16/25 | Loss: 0.00129913
Iteration 17/25 | Loss: 0.00129913
Iteration 18/25 | Loss: 0.00129913
Iteration 19/25 | Loss: 0.00129913
Iteration 20/25 | Loss: 0.00129913
Iteration 21/25 | Loss: 0.00129913
Iteration 22/25 | Loss: 0.00129913
Iteration 23/25 | Loss: 0.00129913
Iteration 24/25 | Loss: 0.00129913
Iteration 25/25 | Loss: 0.00129913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40079761
Iteration 2/25 | Loss: 0.00088810
Iteration 3/25 | Loss: 0.00088810
Iteration 4/25 | Loss: 0.00088810
Iteration 5/25 | Loss: 0.00088810
Iteration 6/25 | Loss: 0.00088810
Iteration 7/25 | Loss: 0.00088810
Iteration 8/25 | Loss: 0.00088810
Iteration 9/25 | Loss: 0.00088810
Iteration 10/25 | Loss: 0.00088810
Iteration 11/25 | Loss: 0.00088810
Iteration 12/25 | Loss: 0.00088810
Iteration 13/25 | Loss: 0.00088810
Iteration 14/25 | Loss: 0.00088810
Iteration 15/25 | Loss: 0.00088810
Iteration 16/25 | Loss: 0.00088810
Iteration 17/25 | Loss: 0.00088810
Iteration 18/25 | Loss: 0.00088810
Iteration 19/25 | Loss: 0.00088810
Iteration 20/25 | Loss: 0.00088810
Iteration 21/25 | Loss: 0.00088810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008880977402441204, 0.0008880977402441204, 0.0008880977402441204, 0.0008880977402441204, 0.0008880977402441204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008880977402441204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088810
Iteration 2/1000 | Loss: 0.00002734
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001666
Iteration 5/1000 | Loss: 0.00001569
Iteration 6/1000 | Loss: 0.00001490
Iteration 7/1000 | Loss: 0.00001436
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001350
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001344
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001331
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001330
Iteration 20/1000 | Loss: 0.00001325
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001313
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001310
Iteration 44/1000 | Loss: 0.00001309
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001305
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001305
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001304
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001302
Iteration 84/1000 | Loss: 0.00001302
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001293
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001293
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001292
Iteration 122/1000 | Loss: 0.00001292
Iteration 123/1000 | Loss: 0.00001292
Iteration 124/1000 | Loss: 0.00001292
Iteration 125/1000 | Loss: 0.00001292
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001290
Iteration 130/1000 | Loss: 0.00001290
Iteration 131/1000 | Loss: 0.00001290
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001287
Iteration 141/1000 | Loss: 0.00001287
Iteration 142/1000 | Loss: 0.00001286
Iteration 143/1000 | Loss: 0.00001286
Iteration 144/1000 | Loss: 0.00001284
Iteration 145/1000 | Loss: 0.00001284
Iteration 146/1000 | Loss: 0.00001284
Iteration 147/1000 | Loss: 0.00001284
Iteration 148/1000 | Loss: 0.00001284
Iteration 149/1000 | Loss: 0.00001284
Iteration 150/1000 | Loss: 0.00001284
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001283
Iteration 155/1000 | Loss: 0.00001283
Iteration 156/1000 | Loss: 0.00001283
Iteration 157/1000 | Loss: 0.00001283
Iteration 158/1000 | Loss: 0.00001283
Iteration 159/1000 | Loss: 0.00001282
Iteration 160/1000 | Loss: 0.00001282
Iteration 161/1000 | Loss: 0.00001282
Iteration 162/1000 | Loss: 0.00001282
Iteration 163/1000 | Loss: 0.00001282
Iteration 164/1000 | Loss: 0.00001281
Iteration 165/1000 | Loss: 0.00001281
Iteration 166/1000 | Loss: 0.00001281
Iteration 167/1000 | Loss: 0.00001281
Iteration 168/1000 | Loss: 0.00001281
Iteration 169/1000 | Loss: 0.00001281
Iteration 170/1000 | Loss: 0.00001281
Iteration 171/1000 | Loss: 0.00001281
Iteration 172/1000 | Loss: 0.00001281
Iteration 173/1000 | Loss: 0.00001280
Iteration 174/1000 | Loss: 0.00001280
Iteration 175/1000 | Loss: 0.00001280
Iteration 176/1000 | Loss: 0.00001280
Iteration 177/1000 | Loss: 0.00001280
Iteration 178/1000 | Loss: 0.00001280
Iteration 179/1000 | Loss: 0.00001280
Iteration 180/1000 | Loss: 0.00001280
Iteration 181/1000 | Loss: 0.00001280
Iteration 182/1000 | Loss: 0.00001280
Iteration 183/1000 | Loss: 0.00001280
Iteration 184/1000 | Loss: 0.00001279
Iteration 185/1000 | Loss: 0.00001279
Iteration 186/1000 | Loss: 0.00001279
Iteration 187/1000 | Loss: 0.00001279
Iteration 188/1000 | Loss: 0.00001279
Iteration 189/1000 | Loss: 0.00001279
Iteration 190/1000 | Loss: 0.00001279
Iteration 191/1000 | Loss: 0.00001279
Iteration 192/1000 | Loss: 0.00001279
Iteration 193/1000 | Loss: 0.00001279
Iteration 194/1000 | Loss: 0.00001279
Iteration 195/1000 | Loss: 0.00001279
Iteration 196/1000 | Loss: 0.00001279
Iteration 197/1000 | Loss: 0.00001279
Iteration 198/1000 | Loss: 0.00001279
Iteration 199/1000 | Loss: 0.00001279
Iteration 200/1000 | Loss: 0.00001278
Iteration 201/1000 | Loss: 0.00001278
Iteration 202/1000 | Loss: 0.00001278
Iteration 203/1000 | Loss: 0.00001278
Iteration 204/1000 | Loss: 0.00001278
Iteration 205/1000 | Loss: 0.00001278
Iteration 206/1000 | Loss: 0.00001278
Iteration 207/1000 | Loss: 0.00001278
Iteration 208/1000 | Loss: 0.00001278
Iteration 209/1000 | Loss: 0.00001278
Iteration 210/1000 | Loss: 0.00001278
Iteration 211/1000 | Loss: 0.00001278
Iteration 212/1000 | Loss: 0.00001278
Iteration 213/1000 | Loss: 0.00001278
Iteration 214/1000 | Loss: 0.00001277
Iteration 215/1000 | Loss: 0.00001277
Iteration 216/1000 | Loss: 0.00001277
Iteration 217/1000 | Loss: 0.00001277
Iteration 218/1000 | Loss: 0.00001277
Iteration 219/1000 | Loss: 0.00001277
Iteration 220/1000 | Loss: 0.00001277
Iteration 221/1000 | Loss: 0.00001277
Iteration 222/1000 | Loss: 0.00001277
Iteration 223/1000 | Loss: 0.00001277
Iteration 224/1000 | Loss: 0.00001277
Iteration 225/1000 | Loss: 0.00001277
Iteration 226/1000 | Loss: 0.00001277
Iteration 227/1000 | Loss: 0.00001276
Iteration 228/1000 | Loss: 0.00001276
Iteration 229/1000 | Loss: 0.00001276
Iteration 230/1000 | Loss: 0.00001276
Iteration 231/1000 | Loss: 0.00001276
Iteration 232/1000 | Loss: 0.00001276
Iteration 233/1000 | Loss: 0.00001276
Iteration 234/1000 | Loss: 0.00001276
Iteration 235/1000 | Loss: 0.00001276
Iteration 236/1000 | Loss: 0.00001275
Iteration 237/1000 | Loss: 0.00001275
Iteration 238/1000 | Loss: 0.00001275
Iteration 239/1000 | Loss: 0.00001275
Iteration 240/1000 | Loss: 0.00001275
Iteration 241/1000 | Loss: 0.00001275
Iteration 242/1000 | Loss: 0.00001275
Iteration 243/1000 | Loss: 0.00001275
Iteration 244/1000 | Loss: 0.00001275
Iteration 245/1000 | Loss: 0.00001275
Iteration 246/1000 | Loss: 0.00001275
Iteration 247/1000 | Loss: 0.00001274
Iteration 248/1000 | Loss: 0.00001274
Iteration 249/1000 | Loss: 0.00001274
Iteration 250/1000 | Loss: 0.00001274
Iteration 251/1000 | Loss: 0.00001274
Iteration 252/1000 | Loss: 0.00001274
Iteration 253/1000 | Loss: 0.00001274
Iteration 254/1000 | Loss: 0.00001274
Iteration 255/1000 | Loss: 0.00001274
Iteration 256/1000 | Loss: 0.00001274
Iteration 257/1000 | Loss: 0.00001274
Iteration 258/1000 | Loss: 0.00001274
Iteration 259/1000 | Loss: 0.00001274
Iteration 260/1000 | Loss: 0.00001274
Iteration 261/1000 | Loss: 0.00001274
Iteration 262/1000 | Loss: 0.00001274
Iteration 263/1000 | Loss: 0.00001273
Iteration 264/1000 | Loss: 0.00001273
Iteration 265/1000 | Loss: 0.00001273
Iteration 266/1000 | Loss: 0.00001273
Iteration 267/1000 | Loss: 0.00001273
Iteration 268/1000 | Loss: 0.00001273
Iteration 269/1000 | Loss: 0.00001273
Iteration 270/1000 | Loss: 0.00001273
Iteration 271/1000 | Loss: 0.00001273
Iteration 272/1000 | Loss: 0.00001273
Iteration 273/1000 | Loss: 0.00001273
Iteration 274/1000 | Loss: 0.00001273
Iteration 275/1000 | Loss: 0.00001273
Iteration 276/1000 | Loss: 0.00001273
Iteration 277/1000 | Loss: 0.00001273
Iteration 278/1000 | Loss: 0.00001273
Iteration 279/1000 | Loss: 0.00001273
Iteration 280/1000 | Loss: 0.00001273
Iteration 281/1000 | Loss: 0.00001272
Iteration 282/1000 | Loss: 0.00001272
Iteration 283/1000 | Loss: 0.00001272
Iteration 284/1000 | Loss: 0.00001272
Iteration 285/1000 | Loss: 0.00001272
Iteration 286/1000 | Loss: 0.00001272
Iteration 287/1000 | Loss: 0.00001272
Iteration 288/1000 | Loss: 0.00001272
Iteration 289/1000 | Loss: 0.00001272
Iteration 290/1000 | Loss: 0.00001272
Iteration 291/1000 | Loss: 0.00001272
Iteration 292/1000 | Loss: 0.00001272
Iteration 293/1000 | Loss: 0.00001272
Iteration 294/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.2722465726255905e-05, 1.2722465726255905e-05, 1.2722465726255905e-05, 1.2722465726255905e-05, 1.2722465726255905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2722465726255905e-05

Optimization complete. Final v2v error: 3.0552022457122803 mm

Highest mean error: 3.3313329219818115 mm for frame 86

Lowest mean error: 2.919227123260498 mm for frame 3

Saving results

Total time: 47.81370544433594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821565
Iteration 2/25 | Loss: 0.00153440
Iteration 3/25 | Loss: 0.00132531
Iteration 4/25 | Loss: 0.00129902
Iteration 5/25 | Loss: 0.00129464
Iteration 6/25 | Loss: 0.00129384
Iteration 7/25 | Loss: 0.00129384
Iteration 8/25 | Loss: 0.00129384
Iteration 9/25 | Loss: 0.00129384
Iteration 10/25 | Loss: 0.00129384
Iteration 11/25 | Loss: 0.00129384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012938391882926226, 0.0012938391882926226, 0.0012938391882926226, 0.0012938391882926226, 0.0012938391882926226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012938391882926226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40067863
Iteration 2/25 | Loss: 0.00084200
Iteration 3/25 | Loss: 0.00084200
Iteration 4/25 | Loss: 0.00084200
Iteration 5/25 | Loss: 0.00084200
Iteration 6/25 | Loss: 0.00084200
Iteration 7/25 | Loss: 0.00084200
Iteration 8/25 | Loss: 0.00084200
Iteration 9/25 | Loss: 0.00084200
Iteration 10/25 | Loss: 0.00084200
Iteration 11/25 | Loss: 0.00084200
Iteration 12/25 | Loss: 0.00084200
Iteration 13/25 | Loss: 0.00084200
Iteration 14/25 | Loss: 0.00084200
Iteration 15/25 | Loss: 0.00084200
Iteration 16/25 | Loss: 0.00084200
Iteration 17/25 | Loss: 0.00084200
Iteration 18/25 | Loss: 0.00084200
Iteration 19/25 | Loss: 0.00084200
Iteration 20/25 | Loss: 0.00084200
Iteration 21/25 | Loss: 0.00084200
Iteration 22/25 | Loss: 0.00084200
Iteration 23/25 | Loss: 0.00084200
Iteration 24/25 | Loss: 0.00084200
Iteration 25/25 | Loss: 0.00084200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008419981459155679, 0.0008419981459155679, 0.0008419981459155679, 0.0008419981459155679, 0.0008419981459155679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008419981459155679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084200
Iteration 2/1000 | Loss: 0.00003600
Iteration 3/1000 | Loss: 0.00002245
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001811
Iteration 6/1000 | Loss: 0.00001712
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001551
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001479
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001463
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001453
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001449
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001430
Iteration 129/1000 | Loss: 0.00001430
Iteration 130/1000 | Loss: 0.00001430
Iteration 131/1000 | Loss: 0.00001430
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001426
Iteration 155/1000 | Loss: 0.00001426
Iteration 156/1000 | Loss: 0.00001426
Iteration 157/1000 | Loss: 0.00001426
Iteration 158/1000 | Loss: 0.00001426
Iteration 159/1000 | Loss: 0.00001425
Iteration 160/1000 | Loss: 0.00001425
Iteration 161/1000 | Loss: 0.00001425
Iteration 162/1000 | Loss: 0.00001425
Iteration 163/1000 | Loss: 0.00001425
Iteration 164/1000 | Loss: 0.00001425
Iteration 165/1000 | Loss: 0.00001425
Iteration 166/1000 | Loss: 0.00001425
Iteration 167/1000 | Loss: 0.00001425
Iteration 168/1000 | Loss: 0.00001425
Iteration 169/1000 | Loss: 0.00001424
Iteration 170/1000 | Loss: 0.00001424
Iteration 171/1000 | Loss: 0.00001424
Iteration 172/1000 | Loss: 0.00001423
Iteration 173/1000 | Loss: 0.00001423
Iteration 174/1000 | Loss: 0.00001423
Iteration 175/1000 | Loss: 0.00001423
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001423
Iteration 184/1000 | Loss: 0.00001423
Iteration 185/1000 | Loss: 0.00001423
Iteration 186/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.4231585737434216e-05, 1.4231585737434216e-05, 1.4231585737434216e-05, 1.4231585737434216e-05, 1.4231585737434216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4231585737434216e-05

Optimization complete. Final v2v error: 3.2287943363189697 mm

Highest mean error: 3.950099468231201 mm for frame 114

Lowest mean error: 2.848267078399658 mm for frame 30

Saving results

Total time: 43.512404680252075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867508
Iteration 2/25 | Loss: 0.00143969
Iteration 3/25 | Loss: 0.00132207
Iteration 4/25 | Loss: 0.00130833
Iteration 5/25 | Loss: 0.00130383
Iteration 6/25 | Loss: 0.00130383
Iteration 7/25 | Loss: 0.00130383
Iteration 8/25 | Loss: 0.00130383
Iteration 9/25 | Loss: 0.00130383
Iteration 10/25 | Loss: 0.00130383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013038258766755462, 0.0013038258766755462, 0.0013038258766755462, 0.0013038258766755462, 0.0013038258766755462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013038258766755462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42909586
Iteration 2/25 | Loss: 0.00095155
Iteration 3/25 | Loss: 0.00095155
Iteration 4/25 | Loss: 0.00095155
Iteration 5/25 | Loss: 0.00095155
Iteration 6/25 | Loss: 0.00095155
Iteration 7/25 | Loss: 0.00095155
Iteration 8/25 | Loss: 0.00095155
Iteration 9/25 | Loss: 0.00095155
Iteration 10/25 | Loss: 0.00095154
Iteration 11/25 | Loss: 0.00095154
Iteration 12/25 | Loss: 0.00095154
Iteration 13/25 | Loss: 0.00095154
Iteration 14/25 | Loss: 0.00095154
Iteration 15/25 | Loss: 0.00095154
Iteration 16/25 | Loss: 0.00095154
Iteration 17/25 | Loss: 0.00095154
Iteration 18/25 | Loss: 0.00095154
Iteration 19/25 | Loss: 0.00095154
Iteration 20/25 | Loss: 0.00095154
Iteration 21/25 | Loss: 0.00095154
Iteration 22/25 | Loss: 0.00095154
Iteration 23/25 | Loss: 0.00095154
Iteration 24/25 | Loss: 0.00095154
Iteration 25/25 | Loss: 0.00095154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095154
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00002271
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00002040
Iteration 6/1000 | Loss: 0.00001981
Iteration 7/1000 | Loss: 0.00001943
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001874
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001836
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001827
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001819
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001816
Iteration 19/1000 | Loss: 0.00001814
Iteration 20/1000 | Loss: 0.00001812
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001811
Iteration 23/1000 | Loss: 0.00001810
Iteration 24/1000 | Loss: 0.00001810
Iteration 25/1000 | Loss: 0.00001809
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001803
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001799
Iteration 34/1000 | Loss: 0.00001799
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001798
Iteration 37/1000 | Loss: 0.00001797
Iteration 38/1000 | Loss: 0.00001797
Iteration 39/1000 | Loss: 0.00001797
Iteration 40/1000 | Loss: 0.00001796
Iteration 41/1000 | Loss: 0.00001795
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001793
Iteration 46/1000 | Loss: 0.00001793
Iteration 47/1000 | Loss: 0.00001792
Iteration 48/1000 | Loss: 0.00001792
Iteration 49/1000 | Loss: 0.00001792
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001790
Iteration 54/1000 | Loss: 0.00001790
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001789
Iteration 57/1000 | Loss: 0.00001788
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001784
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001770
Iteration 80/1000 | Loss: 0.00001770
Iteration 81/1000 | Loss: 0.00001770
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001769
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001769
Iteration 89/1000 | Loss: 0.00001769
Iteration 90/1000 | Loss: 0.00001769
Iteration 91/1000 | Loss: 0.00001769
Iteration 92/1000 | Loss: 0.00001769
Iteration 93/1000 | Loss: 0.00001769
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001769
Iteration 98/1000 | Loss: 0.00001768
Iteration 99/1000 | Loss: 0.00001768
Iteration 100/1000 | Loss: 0.00001768
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001767
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001766
Iteration 115/1000 | Loss: 0.00001766
Iteration 116/1000 | Loss: 0.00001766
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.7663649487076327e-05, 1.7663649487076327e-05, 1.7663649487076327e-05, 1.7663649487076327e-05, 1.7663649487076327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7663649487076327e-05

Optimization complete. Final v2v error: 3.448824167251587 mm

Highest mean error: 4.03718900680542 mm for frame 184

Lowest mean error: 2.889676570892334 mm for frame 230

Saving results

Total time: 40.46892285346985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790194
Iteration 2/25 | Loss: 0.00140736
Iteration 3/25 | Loss: 0.00132922
Iteration 4/25 | Loss: 0.00131786
Iteration 5/25 | Loss: 0.00131484
Iteration 6/25 | Loss: 0.00131463
Iteration 7/25 | Loss: 0.00131463
Iteration 8/25 | Loss: 0.00131463
Iteration 9/25 | Loss: 0.00131463
Iteration 10/25 | Loss: 0.00131463
Iteration 11/25 | Loss: 0.00131463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013146308483555913, 0.0013146308483555913, 0.0013146308483555913, 0.0013146308483555913, 0.0013146308483555913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013146308483555913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44105613
Iteration 2/25 | Loss: 0.00083959
Iteration 3/25 | Loss: 0.00083959
Iteration 4/25 | Loss: 0.00083959
Iteration 5/25 | Loss: 0.00083959
Iteration 6/25 | Loss: 0.00083959
Iteration 7/25 | Loss: 0.00083959
Iteration 8/25 | Loss: 0.00083959
Iteration 9/25 | Loss: 0.00083959
Iteration 10/25 | Loss: 0.00083959
Iteration 11/25 | Loss: 0.00083959
Iteration 12/25 | Loss: 0.00083959
Iteration 13/25 | Loss: 0.00083959
Iteration 14/25 | Loss: 0.00083959
Iteration 15/25 | Loss: 0.00083959
Iteration 16/25 | Loss: 0.00083959
Iteration 17/25 | Loss: 0.00083959
Iteration 18/25 | Loss: 0.00083959
Iteration 19/25 | Loss: 0.00083959
Iteration 20/25 | Loss: 0.00083959
Iteration 21/25 | Loss: 0.00083959
Iteration 22/25 | Loss: 0.00083959
Iteration 23/25 | Loss: 0.00083959
Iteration 24/25 | Loss: 0.00083959
Iteration 25/25 | Loss: 0.00083959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083959
Iteration 2/1000 | Loss: 0.00003292
Iteration 3/1000 | Loss: 0.00002434
Iteration 4/1000 | Loss: 0.00002086
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001932
Iteration 7/1000 | Loss: 0.00001878
Iteration 8/1000 | Loss: 0.00001839
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001777
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001739
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001707
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001699
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001688
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001687
Iteration 30/1000 | Loss: 0.00001687
Iteration 31/1000 | Loss: 0.00001686
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001684
Iteration 37/1000 | Loss: 0.00001684
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001681
Iteration 45/1000 | Loss: 0.00001681
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001673
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001670
Iteration 70/1000 | Loss: 0.00001670
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001669
Iteration 74/1000 | Loss: 0.00001668
Iteration 75/1000 | Loss: 0.00001668
Iteration 76/1000 | Loss: 0.00001668
Iteration 77/1000 | Loss: 0.00001667
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001663
Iteration 90/1000 | Loss: 0.00001663
Iteration 91/1000 | Loss: 0.00001663
Iteration 92/1000 | Loss: 0.00001662
Iteration 93/1000 | Loss: 0.00001662
Iteration 94/1000 | Loss: 0.00001662
Iteration 95/1000 | Loss: 0.00001662
Iteration 96/1000 | Loss: 0.00001662
Iteration 97/1000 | Loss: 0.00001662
Iteration 98/1000 | Loss: 0.00001662
Iteration 99/1000 | Loss: 0.00001662
Iteration 100/1000 | Loss: 0.00001661
Iteration 101/1000 | Loss: 0.00001661
Iteration 102/1000 | Loss: 0.00001661
Iteration 103/1000 | Loss: 0.00001661
Iteration 104/1000 | Loss: 0.00001661
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001661
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001661
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001660
Iteration 123/1000 | Loss: 0.00001660
Iteration 124/1000 | Loss: 0.00001660
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001659
Iteration 130/1000 | Loss: 0.00001659
Iteration 131/1000 | Loss: 0.00001659
Iteration 132/1000 | Loss: 0.00001659
Iteration 133/1000 | Loss: 0.00001659
Iteration 134/1000 | Loss: 0.00001659
Iteration 135/1000 | Loss: 0.00001659
Iteration 136/1000 | Loss: 0.00001659
Iteration 137/1000 | Loss: 0.00001659
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001659
Iteration 143/1000 | Loss: 0.00001659
Iteration 144/1000 | Loss: 0.00001659
Iteration 145/1000 | Loss: 0.00001659
Iteration 146/1000 | Loss: 0.00001659
Iteration 147/1000 | Loss: 0.00001659
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001659
Iteration 163/1000 | Loss: 0.00001659
Iteration 164/1000 | Loss: 0.00001659
Iteration 165/1000 | Loss: 0.00001659
Iteration 166/1000 | Loss: 0.00001659
Iteration 167/1000 | Loss: 0.00001659
Iteration 168/1000 | Loss: 0.00001659
Iteration 169/1000 | Loss: 0.00001659
Iteration 170/1000 | Loss: 0.00001659
Iteration 171/1000 | Loss: 0.00001659
Iteration 172/1000 | Loss: 0.00001659
Iteration 173/1000 | Loss: 0.00001659
Iteration 174/1000 | Loss: 0.00001659
Iteration 175/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.6588513972237706e-05, 1.6588513972237706e-05, 1.6588513972237706e-05, 1.6588513972237706e-05, 1.6588513972237706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6588513972237706e-05

Optimization complete. Final v2v error: 3.479154109954834 mm

Highest mean error: 4.161116600036621 mm for frame 86

Lowest mean error: 3.2939791679382324 mm for frame 134

Saving results

Total time: 39.184136390686035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769057
Iteration 2/25 | Loss: 0.00179564
Iteration 3/25 | Loss: 0.00153932
Iteration 4/25 | Loss: 0.00149846
Iteration 5/25 | Loss: 0.00147957
Iteration 6/25 | Loss: 0.00148267
Iteration 7/25 | Loss: 0.00145758
Iteration 8/25 | Loss: 0.00144517
Iteration 9/25 | Loss: 0.00144362
Iteration 10/25 | Loss: 0.00144322
Iteration 11/25 | Loss: 0.00144387
Iteration 12/25 | Loss: 0.00144187
Iteration 13/25 | Loss: 0.00144128
Iteration 14/25 | Loss: 0.00144115
Iteration 15/25 | Loss: 0.00144115
Iteration 16/25 | Loss: 0.00144115
Iteration 17/25 | Loss: 0.00144114
Iteration 18/25 | Loss: 0.00144114
Iteration 19/25 | Loss: 0.00144114
Iteration 20/25 | Loss: 0.00144114
Iteration 21/25 | Loss: 0.00144114
Iteration 22/25 | Loss: 0.00144114
Iteration 23/25 | Loss: 0.00144114
Iteration 24/25 | Loss: 0.00144114
Iteration 25/25 | Loss: 0.00144113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60942245
Iteration 2/25 | Loss: 0.00091391
Iteration 3/25 | Loss: 0.00091390
Iteration 4/25 | Loss: 0.00091389
Iteration 5/25 | Loss: 0.00091389
Iteration 6/25 | Loss: 0.00091389
Iteration 7/25 | Loss: 0.00091389
Iteration 8/25 | Loss: 0.00091389
Iteration 9/25 | Loss: 0.00091389
Iteration 10/25 | Loss: 0.00091389
Iteration 11/25 | Loss: 0.00091389
Iteration 12/25 | Loss: 0.00091389
Iteration 13/25 | Loss: 0.00091389
Iteration 14/25 | Loss: 0.00091389
Iteration 15/25 | Loss: 0.00091389
Iteration 16/25 | Loss: 0.00091389
Iteration 17/25 | Loss: 0.00091389
Iteration 18/25 | Loss: 0.00091389
Iteration 19/25 | Loss: 0.00091389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009138921741396189, 0.0009138921741396189, 0.0009138921741396189, 0.0009138921741396189, 0.0009138921741396189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009138921741396189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091389
Iteration 2/1000 | Loss: 0.00005012
Iteration 3/1000 | Loss: 0.00003620
Iteration 4/1000 | Loss: 0.00003315
Iteration 5/1000 | Loss: 0.00003171
Iteration 6/1000 | Loss: 0.00003056
Iteration 7/1000 | Loss: 0.00002979
Iteration 8/1000 | Loss: 0.00002937
Iteration 9/1000 | Loss: 0.00002895
Iteration 10/1000 | Loss: 0.00002865
Iteration 11/1000 | Loss: 0.00002846
Iteration 12/1000 | Loss: 0.00002840
Iteration 13/1000 | Loss: 0.00002836
Iteration 14/1000 | Loss: 0.00002832
Iteration 15/1000 | Loss: 0.00002829
Iteration 16/1000 | Loss: 0.00002819
Iteration 17/1000 | Loss: 0.00002815
Iteration 18/1000 | Loss: 0.00002811
Iteration 19/1000 | Loss: 0.00002807
Iteration 20/1000 | Loss: 0.00002806
Iteration 21/1000 | Loss: 0.00002806
Iteration 22/1000 | Loss: 0.00002804
Iteration 23/1000 | Loss: 0.00002798
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002794
Iteration 27/1000 | Loss: 0.00002794
Iteration 28/1000 | Loss: 0.00002794
Iteration 29/1000 | Loss: 0.00002792
Iteration 30/1000 | Loss: 0.00002792
Iteration 31/1000 | Loss: 0.00002792
Iteration 32/1000 | Loss: 0.00002791
Iteration 33/1000 | Loss: 0.00002791
Iteration 34/1000 | Loss: 0.00002790
Iteration 35/1000 | Loss: 0.00002789
Iteration 36/1000 | Loss: 0.00002789
Iteration 37/1000 | Loss: 0.00002789
Iteration 38/1000 | Loss: 0.00002789
Iteration 39/1000 | Loss: 0.00002789
Iteration 40/1000 | Loss: 0.00002788
Iteration 41/1000 | Loss: 0.00002788
Iteration 42/1000 | Loss: 0.00002788
Iteration 43/1000 | Loss: 0.00002788
Iteration 44/1000 | Loss: 0.00002788
Iteration 45/1000 | Loss: 0.00002788
Iteration 46/1000 | Loss: 0.00002787
Iteration 47/1000 | Loss: 0.00002787
Iteration 48/1000 | Loss: 0.00002787
Iteration 49/1000 | Loss: 0.00002787
Iteration 50/1000 | Loss: 0.00002786
Iteration 51/1000 | Loss: 0.00002786
Iteration 52/1000 | Loss: 0.00002785
Iteration 53/1000 | Loss: 0.00002785
Iteration 54/1000 | Loss: 0.00002785
Iteration 55/1000 | Loss: 0.00002784
Iteration 56/1000 | Loss: 0.00002784
Iteration 57/1000 | Loss: 0.00002784
Iteration 58/1000 | Loss: 0.00002783
Iteration 59/1000 | Loss: 0.00002783
Iteration 60/1000 | Loss: 0.00002783
Iteration 61/1000 | Loss: 0.00002783
Iteration 62/1000 | Loss: 0.00002782
Iteration 63/1000 | Loss: 0.00002782
Iteration 64/1000 | Loss: 0.00002782
Iteration 65/1000 | Loss: 0.00002782
Iteration 66/1000 | Loss: 0.00002781
Iteration 67/1000 | Loss: 0.00002781
Iteration 68/1000 | Loss: 0.00002781
Iteration 69/1000 | Loss: 0.00002781
Iteration 70/1000 | Loss: 0.00002781
Iteration 71/1000 | Loss: 0.00002781
Iteration 72/1000 | Loss: 0.00002780
Iteration 73/1000 | Loss: 0.00002780
Iteration 74/1000 | Loss: 0.00002780
Iteration 75/1000 | Loss: 0.00002779
Iteration 76/1000 | Loss: 0.00002779
Iteration 77/1000 | Loss: 0.00002778
Iteration 78/1000 | Loss: 0.00002778
Iteration 79/1000 | Loss: 0.00002778
Iteration 80/1000 | Loss: 0.00002778
Iteration 81/1000 | Loss: 0.00002778
Iteration 82/1000 | Loss: 0.00002778
Iteration 83/1000 | Loss: 0.00002778
Iteration 84/1000 | Loss: 0.00002778
Iteration 85/1000 | Loss: 0.00002777
Iteration 86/1000 | Loss: 0.00002777
Iteration 87/1000 | Loss: 0.00002777
Iteration 88/1000 | Loss: 0.00002777
Iteration 89/1000 | Loss: 0.00002777
Iteration 90/1000 | Loss: 0.00002777
Iteration 91/1000 | Loss: 0.00002777
Iteration 92/1000 | Loss: 0.00002777
Iteration 93/1000 | Loss: 0.00002777
Iteration 94/1000 | Loss: 0.00002776
Iteration 95/1000 | Loss: 0.00002776
Iteration 96/1000 | Loss: 0.00002776
Iteration 97/1000 | Loss: 0.00002775
Iteration 98/1000 | Loss: 0.00002775
Iteration 99/1000 | Loss: 0.00002775
Iteration 100/1000 | Loss: 0.00002774
Iteration 101/1000 | Loss: 0.00002774
Iteration 102/1000 | Loss: 0.00002773
Iteration 103/1000 | Loss: 0.00002773
Iteration 104/1000 | Loss: 0.00002773
Iteration 105/1000 | Loss: 0.00002773
Iteration 106/1000 | Loss: 0.00002773
Iteration 107/1000 | Loss: 0.00002773
Iteration 108/1000 | Loss: 0.00002772
Iteration 109/1000 | Loss: 0.00002772
Iteration 110/1000 | Loss: 0.00002771
Iteration 111/1000 | Loss: 0.00002771
Iteration 112/1000 | Loss: 0.00002770
Iteration 113/1000 | Loss: 0.00002770
Iteration 114/1000 | Loss: 0.00002770
Iteration 115/1000 | Loss: 0.00002770
Iteration 116/1000 | Loss: 0.00002769
Iteration 117/1000 | Loss: 0.00002769
Iteration 118/1000 | Loss: 0.00002769
Iteration 119/1000 | Loss: 0.00002769
Iteration 120/1000 | Loss: 0.00002769
Iteration 121/1000 | Loss: 0.00002769
Iteration 122/1000 | Loss: 0.00002769
Iteration 123/1000 | Loss: 0.00002769
Iteration 124/1000 | Loss: 0.00002768
Iteration 125/1000 | Loss: 0.00002768
Iteration 126/1000 | Loss: 0.00002768
Iteration 127/1000 | Loss: 0.00002768
Iteration 128/1000 | Loss: 0.00002768
Iteration 129/1000 | Loss: 0.00002768
Iteration 130/1000 | Loss: 0.00002767
Iteration 131/1000 | Loss: 0.00002767
Iteration 132/1000 | Loss: 0.00002767
Iteration 133/1000 | Loss: 0.00002767
Iteration 134/1000 | Loss: 0.00002767
Iteration 135/1000 | Loss: 0.00002767
Iteration 136/1000 | Loss: 0.00002766
Iteration 137/1000 | Loss: 0.00002766
Iteration 138/1000 | Loss: 0.00002766
Iteration 139/1000 | Loss: 0.00002766
Iteration 140/1000 | Loss: 0.00002766
Iteration 141/1000 | Loss: 0.00002766
Iteration 142/1000 | Loss: 0.00002766
Iteration 143/1000 | Loss: 0.00002766
Iteration 144/1000 | Loss: 0.00002766
Iteration 145/1000 | Loss: 0.00002766
Iteration 146/1000 | Loss: 0.00002766
Iteration 147/1000 | Loss: 0.00002766
Iteration 148/1000 | Loss: 0.00002766
Iteration 149/1000 | Loss: 0.00002766
Iteration 150/1000 | Loss: 0.00002766
Iteration 151/1000 | Loss: 0.00002766
Iteration 152/1000 | Loss: 0.00002766
Iteration 153/1000 | Loss: 0.00002766
Iteration 154/1000 | Loss: 0.00002766
Iteration 155/1000 | Loss: 0.00002766
Iteration 156/1000 | Loss: 0.00002766
Iteration 157/1000 | Loss: 0.00002766
Iteration 158/1000 | Loss: 0.00002766
Iteration 159/1000 | Loss: 0.00002766
Iteration 160/1000 | Loss: 0.00002766
Iteration 161/1000 | Loss: 0.00002766
Iteration 162/1000 | Loss: 0.00002766
Iteration 163/1000 | Loss: 0.00002766
Iteration 164/1000 | Loss: 0.00002766
Iteration 165/1000 | Loss: 0.00002766
Iteration 166/1000 | Loss: 0.00002766
Iteration 167/1000 | Loss: 0.00002766
Iteration 168/1000 | Loss: 0.00002766
Iteration 169/1000 | Loss: 0.00002766
Iteration 170/1000 | Loss: 0.00002766
Iteration 171/1000 | Loss: 0.00002766
Iteration 172/1000 | Loss: 0.00002766
Iteration 173/1000 | Loss: 0.00002766
Iteration 174/1000 | Loss: 0.00002766
Iteration 175/1000 | Loss: 0.00002766
Iteration 176/1000 | Loss: 0.00002766
Iteration 177/1000 | Loss: 0.00002766
Iteration 178/1000 | Loss: 0.00002766
Iteration 179/1000 | Loss: 0.00002766
Iteration 180/1000 | Loss: 0.00002766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.766231045825407e-05, 2.766231045825407e-05, 2.766231045825407e-05, 2.766231045825407e-05, 2.766231045825407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.766231045825407e-05

Optimization complete. Final v2v error: 4.279448986053467 mm

Highest mean error: 6.157138347625732 mm for frame 40

Lowest mean error: 3.3896751403808594 mm for frame 146

Saving results

Total time: 62.09943985939026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799673
Iteration 2/25 | Loss: 0.00156910
Iteration 3/25 | Loss: 0.00138216
Iteration 4/25 | Loss: 0.00136627
Iteration 5/25 | Loss: 0.00136352
Iteration 6/25 | Loss: 0.00136352
Iteration 7/25 | Loss: 0.00136352
Iteration 8/25 | Loss: 0.00136352
Iteration 9/25 | Loss: 0.00136352
Iteration 10/25 | Loss: 0.00136352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013635249342769384, 0.0013635249342769384, 0.0013635249342769384, 0.0013635249342769384, 0.0013635249342769384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013635249342769384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30364573
Iteration 2/25 | Loss: 0.00084518
Iteration 3/25 | Loss: 0.00084517
Iteration 4/25 | Loss: 0.00084517
Iteration 5/25 | Loss: 0.00084517
Iteration 6/25 | Loss: 0.00084517
Iteration 7/25 | Loss: 0.00084517
Iteration 8/25 | Loss: 0.00084517
Iteration 9/25 | Loss: 0.00084517
Iteration 10/25 | Loss: 0.00084517
Iteration 11/25 | Loss: 0.00084517
Iteration 12/25 | Loss: 0.00084517
Iteration 13/25 | Loss: 0.00084517
Iteration 14/25 | Loss: 0.00084517
Iteration 15/25 | Loss: 0.00084517
Iteration 16/25 | Loss: 0.00084517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008451688918285072, 0.0008451688918285072, 0.0008451688918285072, 0.0008451688918285072, 0.0008451688918285072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008451688918285072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084517
Iteration 2/1000 | Loss: 0.00003360
Iteration 3/1000 | Loss: 0.00002564
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002193
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002103
Iteration 10/1000 | Loss: 0.00002078
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002043
Iteration 13/1000 | Loss: 0.00002031
Iteration 14/1000 | Loss: 0.00002030
Iteration 15/1000 | Loss: 0.00002029
Iteration 16/1000 | Loss: 0.00002025
Iteration 17/1000 | Loss: 0.00002024
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002013
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002007
Iteration 23/1000 | Loss: 0.00002006
Iteration 24/1000 | Loss: 0.00002006
Iteration 25/1000 | Loss: 0.00002006
Iteration 26/1000 | Loss: 0.00002005
Iteration 27/1000 | Loss: 0.00002005
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002002
Iteration 32/1000 | Loss: 0.00002002
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002000
Iteration 37/1000 | Loss: 0.00001998
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001997
Iteration 41/1000 | Loss: 0.00001997
Iteration 42/1000 | Loss: 0.00001996
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001996
Iteration 48/1000 | Loss: 0.00001996
Iteration 49/1000 | Loss: 0.00001996
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001991
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001989
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001988
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001986
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001985
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001985
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001983
Iteration 108/1000 | Loss: 0.00001983
Iteration 109/1000 | Loss: 0.00001983
Iteration 110/1000 | Loss: 0.00001983
Iteration 111/1000 | Loss: 0.00001983
Iteration 112/1000 | Loss: 0.00001983
Iteration 113/1000 | Loss: 0.00001983
Iteration 114/1000 | Loss: 0.00001982
Iteration 115/1000 | Loss: 0.00001982
Iteration 116/1000 | Loss: 0.00001982
Iteration 117/1000 | Loss: 0.00001982
Iteration 118/1000 | Loss: 0.00001982
Iteration 119/1000 | Loss: 0.00001982
Iteration 120/1000 | Loss: 0.00001982
Iteration 121/1000 | Loss: 0.00001982
Iteration 122/1000 | Loss: 0.00001982
Iteration 123/1000 | Loss: 0.00001982
Iteration 124/1000 | Loss: 0.00001982
Iteration 125/1000 | Loss: 0.00001982
Iteration 126/1000 | Loss: 0.00001982
Iteration 127/1000 | Loss: 0.00001982
Iteration 128/1000 | Loss: 0.00001982
Iteration 129/1000 | Loss: 0.00001981
Iteration 130/1000 | Loss: 0.00001981
Iteration 131/1000 | Loss: 0.00001981
Iteration 132/1000 | Loss: 0.00001981
Iteration 133/1000 | Loss: 0.00001981
Iteration 134/1000 | Loss: 0.00001981
Iteration 135/1000 | Loss: 0.00001981
Iteration 136/1000 | Loss: 0.00001981
Iteration 137/1000 | Loss: 0.00001981
Iteration 138/1000 | Loss: 0.00001981
Iteration 139/1000 | Loss: 0.00001981
Iteration 140/1000 | Loss: 0.00001981
Iteration 141/1000 | Loss: 0.00001981
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.9808676370303147e-05, 1.9808676370303147e-05, 1.9808676370303147e-05, 1.9808676370303147e-05, 1.9808676370303147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9808676370303147e-05

Optimization complete. Final v2v error: 3.762242078781128 mm

Highest mean error: 3.917003631591797 mm for frame 180

Lowest mean error: 3.644782543182373 mm for frame 68

Saving results

Total time: 40.943110942840576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824181
Iteration 2/25 | Loss: 0.00140301
Iteration 3/25 | Loss: 0.00132468
Iteration 4/25 | Loss: 0.00131697
Iteration 5/25 | Loss: 0.00131417
Iteration 6/25 | Loss: 0.00131397
Iteration 7/25 | Loss: 0.00131397
Iteration 8/25 | Loss: 0.00131397
Iteration 9/25 | Loss: 0.00131397
Iteration 10/25 | Loss: 0.00131397
Iteration 11/25 | Loss: 0.00131397
Iteration 12/25 | Loss: 0.00131397
Iteration 13/25 | Loss: 0.00131397
Iteration 14/25 | Loss: 0.00131397
Iteration 15/25 | Loss: 0.00131397
Iteration 16/25 | Loss: 0.00131397
Iteration 17/25 | Loss: 0.00131397
Iteration 18/25 | Loss: 0.00131397
Iteration 19/25 | Loss: 0.00131397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013139721704646945, 0.0013139721704646945, 0.0013139721704646945, 0.0013139721704646945, 0.0013139721704646945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013139721704646945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.25069857
Iteration 2/25 | Loss: 0.00096767
Iteration 3/25 | Loss: 0.00096766
Iteration 4/25 | Loss: 0.00096766
Iteration 5/25 | Loss: 0.00096766
Iteration 6/25 | Loss: 0.00096766
Iteration 7/25 | Loss: 0.00096766
Iteration 8/25 | Loss: 0.00096766
Iteration 9/25 | Loss: 0.00096766
Iteration 10/25 | Loss: 0.00096766
Iteration 11/25 | Loss: 0.00096766
Iteration 12/25 | Loss: 0.00096766
Iteration 13/25 | Loss: 0.00096766
Iteration 14/25 | Loss: 0.00096766
Iteration 15/25 | Loss: 0.00096766
Iteration 16/25 | Loss: 0.00096766
Iteration 17/25 | Loss: 0.00096766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009676610352471471, 0.0009676610352471471, 0.0009676610352471471, 0.0009676610352471471, 0.0009676610352471471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009676610352471471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096766
Iteration 2/1000 | Loss: 0.00002962
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00001943
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001717
Iteration 9/1000 | Loss: 0.00001705
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001555
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001552
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001551
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001551
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001547
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001544
Iteration 93/1000 | Loss: 0.00001544
Iteration 94/1000 | Loss: 0.00001544
Iteration 95/1000 | Loss: 0.00001544
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00001543
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00001543
Iteration 104/1000 | Loss: 0.00001543
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001542
Iteration 107/1000 | Loss: 0.00001542
Iteration 108/1000 | Loss: 0.00001542
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001541
Iteration 114/1000 | Loss: 0.00001541
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001538
Iteration 143/1000 | Loss: 0.00001538
Iteration 144/1000 | Loss: 0.00001538
Iteration 145/1000 | Loss: 0.00001538
Iteration 146/1000 | Loss: 0.00001538
Iteration 147/1000 | Loss: 0.00001538
Iteration 148/1000 | Loss: 0.00001538
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Iteration 164/1000 | Loss: 0.00001537
Iteration 165/1000 | Loss: 0.00001537
Iteration 166/1000 | Loss: 0.00001537
Iteration 167/1000 | Loss: 0.00001537
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001537
Iteration 172/1000 | Loss: 0.00001537
Iteration 173/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.5367329979198985e-05, 1.5367329979198985e-05, 1.5367329979198985e-05, 1.5367329979198985e-05, 1.5367329979198985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5367329979198985e-05

Optimization complete. Final v2v error: 3.3019659519195557 mm

Highest mean error: 4.016557216644287 mm for frame 90

Lowest mean error: 3.022641658782959 mm for frame 13

Saving results

Total time: 41.75322699546814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807310
Iteration 2/25 | Loss: 0.00171421
Iteration 3/25 | Loss: 0.00144782
Iteration 4/25 | Loss: 0.00142405
Iteration 5/25 | Loss: 0.00141961
Iteration 6/25 | Loss: 0.00141852
Iteration 7/25 | Loss: 0.00141852
Iteration 8/25 | Loss: 0.00141852
Iteration 9/25 | Loss: 0.00141852
Iteration 10/25 | Loss: 0.00141852
Iteration 11/25 | Loss: 0.00141852
Iteration 12/25 | Loss: 0.00141852
Iteration 13/25 | Loss: 0.00141852
Iteration 14/25 | Loss: 0.00141852
Iteration 15/25 | Loss: 0.00141852
Iteration 16/25 | Loss: 0.00141852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014185246545821428, 0.0014185246545821428, 0.0014185246545821428, 0.0014185246545821428, 0.0014185246545821428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014185246545821428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43222463
Iteration 2/25 | Loss: 0.00092202
Iteration 3/25 | Loss: 0.00092202
Iteration 4/25 | Loss: 0.00092202
Iteration 5/25 | Loss: 0.00092202
Iteration 6/25 | Loss: 0.00092202
Iteration 7/25 | Loss: 0.00092202
Iteration 8/25 | Loss: 0.00092202
Iteration 9/25 | Loss: 0.00092202
Iteration 10/25 | Loss: 0.00092202
Iteration 11/25 | Loss: 0.00092202
Iteration 12/25 | Loss: 0.00092202
Iteration 13/25 | Loss: 0.00092202
Iteration 14/25 | Loss: 0.00092202
Iteration 15/25 | Loss: 0.00092202
Iteration 16/25 | Loss: 0.00092202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009220193023793399, 0.0009220193023793399, 0.0009220193023793399, 0.0009220193023793399, 0.0009220193023793399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009220193023793399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092202
Iteration 2/1000 | Loss: 0.00006046
Iteration 3/1000 | Loss: 0.00003692
Iteration 4/1000 | Loss: 0.00003052
Iteration 5/1000 | Loss: 0.00002848
Iteration 6/1000 | Loss: 0.00002768
Iteration 7/1000 | Loss: 0.00002675
Iteration 8/1000 | Loss: 0.00002605
Iteration 9/1000 | Loss: 0.00002561
Iteration 10/1000 | Loss: 0.00002525
Iteration 11/1000 | Loss: 0.00002493
Iteration 12/1000 | Loss: 0.00002469
Iteration 13/1000 | Loss: 0.00002445
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002422
Iteration 16/1000 | Loss: 0.00002417
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002397
Iteration 19/1000 | Loss: 0.00002395
Iteration 20/1000 | Loss: 0.00002395
Iteration 21/1000 | Loss: 0.00002391
Iteration 22/1000 | Loss: 0.00002389
Iteration 23/1000 | Loss: 0.00002389
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002388
Iteration 26/1000 | Loss: 0.00002387
Iteration 27/1000 | Loss: 0.00002387
Iteration 28/1000 | Loss: 0.00002387
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002386
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002385
Iteration 34/1000 | Loss: 0.00002385
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002384
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00002384
Iteration 40/1000 | Loss: 0.00002383
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002382
Iteration 43/1000 | Loss: 0.00002382
Iteration 44/1000 | Loss: 0.00002382
Iteration 45/1000 | Loss: 0.00002382
Iteration 46/1000 | Loss: 0.00002382
Iteration 47/1000 | Loss: 0.00002382
Iteration 48/1000 | Loss: 0.00002382
Iteration 49/1000 | Loss: 0.00002382
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002381
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002380
Iteration 56/1000 | Loss: 0.00002378
Iteration 57/1000 | Loss: 0.00002378
Iteration 58/1000 | Loss: 0.00002378
Iteration 59/1000 | Loss: 0.00002378
Iteration 60/1000 | Loss: 0.00002378
Iteration 61/1000 | Loss: 0.00002378
Iteration 62/1000 | Loss: 0.00002378
Iteration 63/1000 | Loss: 0.00002378
Iteration 64/1000 | Loss: 0.00002378
Iteration 65/1000 | Loss: 0.00002378
Iteration 66/1000 | Loss: 0.00002378
Iteration 67/1000 | Loss: 0.00002377
Iteration 68/1000 | Loss: 0.00002377
Iteration 69/1000 | Loss: 0.00002377
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002375
Iteration 72/1000 | Loss: 0.00002375
Iteration 73/1000 | Loss: 0.00002375
Iteration 74/1000 | Loss: 0.00002375
Iteration 75/1000 | Loss: 0.00002375
Iteration 76/1000 | Loss: 0.00002374
Iteration 77/1000 | Loss: 0.00002374
Iteration 78/1000 | Loss: 0.00002374
Iteration 79/1000 | Loss: 0.00002374
Iteration 80/1000 | Loss: 0.00002373
Iteration 81/1000 | Loss: 0.00002372
Iteration 82/1000 | Loss: 0.00002372
Iteration 83/1000 | Loss: 0.00002372
Iteration 84/1000 | Loss: 0.00002371
Iteration 85/1000 | Loss: 0.00002371
Iteration 86/1000 | Loss: 0.00002371
Iteration 87/1000 | Loss: 0.00002371
Iteration 88/1000 | Loss: 0.00002371
Iteration 89/1000 | Loss: 0.00002371
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00002370
Iteration 92/1000 | Loss: 0.00002370
Iteration 93/1000 | Loss: 0.00002370
Iteration 94/1000 | Loss: 0.00002370
Iteration 95/1000 | Loss: 0.00002370
Iteration 96/1000 | Loss: 0.00002370
Iteration 97/1000 | Loss: 0.00002370
Iteration 98/1000 | Loss: 0.00002369
Iteration 99/1000 | Loss: 0.00002369
Iteration 100/1000 | Loss: 0.00002369
Iteration 101/1000 | Loss: 0.00002369
Iteration 102/1000 | Loss: 0.00002369
Iteration 103/1000 | Loss: 0.00002369
Iteration 104/1000 | Loss: 0.00002369
Iteration 105/1000 | Loss: 0.00002369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.369217691011727e-05, 2.369217691011727e-05, 2.369217691011727e-05, 2.369217691011727e-05, 2.369217691011727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.369217691011727e-05

Optimization complete. Final v2v error: 4.0005903244018555 mm

Highest mean error: 5.305708885192871 mm for frame 150

Lowest mean error: 3.1994214057922363 mm for frame 8

Saving results

Total time: 38.80243134498596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475332
Iteration 2/25 | Loss: 0.00146298
Iteration 3/25 | Loss: 0.00132155
Iteration 4/25 | Loss: 0.00131212
Iteration 5/25 | Loss: 0.00131002
Iteration 6/25 | Loss: 0.00130996
Iteration 7/25 | Loss: 0.00130996
Iteration 8/25 | Loss: 0.00130996
Iteration 9/25 | Loss: 0.00130996
Iteration 10/25 | Loss: 0.00130996
Iteration 11/25 | Loss: 0.00130996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013099592179059982, 0.0013099592179059982, 0.0013099592179059982, 0.0013099592179059982, 0.0013099592179059982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013099592179059982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40706384
Iteration 2/25 | Loss: 0.00082456
Iteration 3/25 | Loss: 0.00082456
Iteration 4/25 | Loss: 0.00082456
Iteration 5/25 | Loss: 0.00082456
Iteration 6/25 | Loss: 0.00082456
Iteration 7/25 | Loss: 0.00082456
Iteration 8/25 | Loss: 0.00082456
Iteration 9/25 | Loss: 0.00082456
Iteration 10/25 | Loss: 0.00082456
Iteration 11/25 | Loss: 0.00082456
Iteration 12/25 | Loss: 0.00082455
Iteration 13/25 | Loss: 0.00082455
Iteration 14/25 | Loss: 0.00082455
Iteration 15/25 | Loss: 0.00082455
Iteration 16/25 | Loss: 0.00082455
Iteration 17/25 | Loss: 0.00082455
Iteration 18/25 | Loss: 0.00082455
Iteration 19/25 | Loss: 0.00082455
Iteration 20/25 | Loss: 0.00082455
Iteration 21/25 | Loss: 0.00082455
Iteration 22/25 | Loss: 0.00082455
Iteration 23/25 | Loss: 0.00082455
Iteration 24/25 | Loss: 0.00082455
Iteration 25/25 | Loss: 0.00082455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082455
Iteration 2/1000 | Loss: 0.00003544
Iteration 3/1000 | Loss: 0.00002350
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001440
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001433
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001431
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001430
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001425
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001420
Iteration 57/1000 | Loss: 0.00001419
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001417
Iteration 66/1000 | Loss: 0.00001417
Iteration 67/1000 | Loss: 0.00001416
Iteration 68/1000 | Loss: 0.00001416
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001410
Iteration 74/1000 | Loss: 0.00001410
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001407
Iteration 85/1000 | Loss: 0.00001407
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001402
Iteration 97/1000 | Loss: 0.00001402
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001400
Iteration 102/1000 | Loss: 0.00001400
Iteration 103/1000 | Loss: 0.00001400
Iteration 104/1000 | Loss: 0.00001399
Iteration 105/1000 | Loss: 0.00001399
Iteration 106/1000 | Loss: 0.00001399
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001396
Iteration 112/1000 | Loss: 0.00001395
Iteration 113/1000 | Loss: 0.00001395
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001394
Iteration 117/1000 | Loss: 0.00001394
Iteration 118/1000 | Loss: 0.00001394
Iteration 119/1000 | Loss: 0.00001394
Iteration 120/1000 | Loss: 0.00001394
Iteration 121/1000 | Loss: 0.00001394
Iteration 122/1000 | Loss: 0.00001394
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001393
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001391
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001390
Iteration 132/1000 | Loss: 0.00001390
Iteration 133/1000 | Loss: 0.00001390
Iteration 134/1000 | Loss: 0.00001390
Iteration 135/1000 | Loss: 0.00001389
Iteration 136/1000 | Loss: 0.00001389
Iteration 137/1000 | Loss: 0.00001389
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001387
Iteration 150/1000 | Loss: 0.00001386
Iteration 151/1000 | Loss: 0.00001386
Iteration 152/1000 | Loss: 0.00001386
Iteration 153/1000 | Loss: 0.00001386
Iteration 154/1000 | Loss: 0.00001386
Iteration 155/1000 | Loss: 0.00001385
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001384
Iteration 159/1000 | Loss: 0.00001384
Iteration 160/1000 | Loss: 0.00001384
Iteration 161/1000 | Loss: 0.00001384
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001382
Iteration 170/1000 | Loss: 0.00001382
Iteration 171/1000 | Loss: 0.00001382
Iteration 172/1000 | Loss: 0.00001382
Iteration 173/1000 | Loss: 0.00001382
Iteration 174/1000 | Loss: 0.00001381
Iteration 175/1000 | Loss: 0.00001381
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001380
Iteration 179/1000 | Loss: 0.00001380
Iteration 180/1000 | Loss: 0.00001380
Iteration 181/1000 | Loss: 0.00001380
Iteration 182/1000 | Loss: 0.00001380
Iteration 183/1000 | Loss: 0.00001379
Iteration 184/1000 | Loss: 0.00001379
Iteration 185/1000 | Loss: 0.00001379
Iteration 186/1000 | Loss: 0.00001379
Iteration 187/1000 | Loss: 0.00001379
Iteration 188/1000 | Loss: 0.00001379
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001378
Iteration 192/1000 | Loss: 0.00001378
Iteration 193/1000 | Loss: 0.00001378
Iteration 194/1000 | Loss: 0.00001378
Iteration 195/1000 | Loss: 0.00001378
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001376
Iteration 200/1000 | Loss: 0.00001376
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001376
Iteration 208/1000 | Loss: 0.00001376
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Iteration 214/1000 | Loss: 0.00001374
Iteration 215/1000 | Loss: 0.00001374
Iteration 216/1000 | Loss: 0.00001374
Iteration 217/1000 | Loss: 0.00001374
Iteration 218/1000 | Loss: 0.00001374
Iteration 219/1000 | Loss: 0.00001374
Iteration 220/1000 | Loss: 0.00001374
Iteration 221/1000 | Loss: 0.00001374
Iteration 222/1000 | Loss: 0.00001374
Iteration 223/1000 | Loss: 0.00001374
Iteration 224/1000 | Loss: 0.00001374
Iteration 225/1000 | Loss: 0.00001374
Iteration 226/1000 | Loss: 0.00001373
Iteration 227/1000 | Loss: 0.00001373
Iteration 228/1000 | Loss: 0.00001373
Iteration 229/1000 | Loss: 0.00001373
Iteration 230/1000 | Loss: 0.00001373
Iteration 231/1000 | Loss: 0.00001373
Iteration 232/1000 | Loss: 0.00001373
Iteration 233/1000 | Loss: 0.00001373
Iteration 234/1000 | Loss: 0.00001373
Iteration 235/1000 | Loss: 0.00001373
Iteration 236/1000 | Loss: 0.00001373
Iteration 237/1000 | Loss: 0.00001373
Iteration 238/1000 | Loss: 0.00001373
Iteration 239/1000 | Loss: 0.00001373
Iteration 240/1000 | Loss: 0.00001373
Iteration 241/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.3731769286096096e-05, 1.3731769286096096e-05, 1.3731769286096096e-05, 1.3731769286096096e-05, 1.3731769286096096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3731769286096096e-05

Optimization complete. Final v2v error: 3.121098279953003 mm

Highest mean error: 3.7697153091430664 mm for frame 76

Lowest mean error: 2.849027156829834 mm for frame 38

Saving results

Total time: 46.37552452087402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807744
Iteration 2/25 | Loss: 0.00162863
Iteration 3/25 | Loss: 0.00141687
Iteration 4/25 | Loss: 0.00140268
Iteration 5/25 | Loss: 0.00139758
Iteration 6/25 | Loss: 0.00140990
Iteration 7/25 | Loss: 0.00139601
Iteration 8/25 | Loss: 0.00139111
Iteration 9/25 | Loss: 0.00139059
Iteration 10/25 | Loss: 0.00139048
Iteration 11/25 | Loss: 0.00139048
Iteration 12/25 | Loss: 0.00139048
Iteration 13/25 | Loss: 0.00139047
Iteration 14/25 | Loss: 0.00139047
Iteration 15/25 | Loss: 0.00139047
Iteration 16/25 | Loss: 0.00139047
Iteration 17/25 | Loss: 0.00139047
Iteration 18/25 | Loss: 0.00139047
Iteration 19/25 | Loss: 0.00139047
Iteration 20/25 | Loss: 0.00139047
Iteration 21/25 | Loss: 0.00139047
Iteration 22/25 | Loss: 0.00139047
Iteration 23/25 | Loss: 0.00139047
Iteration 24/25 | Loss: 0.00139047
Iteration 25/25 | Loss: 0.00139047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39913630
Iteration 2/25 | Loss: 0.00092767
Iteration 3/25 | Loss: 0.00092762
Iteration 4/25 | Loss: 0.00092762
Iteration 5/25 | Loss: 0.00092762
Iteration 6/25 | Loss: 0.00092762
Iteration 7/25 | Loss: 0.00092762
Iteration 8/25 | Loss: 0.00092762
Iteration 9/25 | Loss: 0.00092762
Iteration 10/25 | Loss: 0.00092762
Iteration 11/25 | Loss: 0.00092762
Iteration 12/25 | Loss: 0.00092762
Iteration 13/25 | Loss: 0.00092762
Iteration 14/25 | Loss: 0.00092762
Iteration 15/25 | Loss: 0.00092762
Iteration 16/25 | Loss: 0.00092762
Iteration 17/25 | Loss: 0.00092762
Iteration 18/25 | Loss: 0.00092762
Iteration 19/25 | Loss: 0.00092762
Iteration 20/25 | Loss: 0.00092762
Iteration 21/25 | Loss: 0.00092762
Iteration 22/25 | Loss: 0.00092762
Iteration 23/25 | Loss: 0.00092762
Iteration 24/25 | Loss: 0.00092762
Iteration 25/25 | Loss: 0.00092762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092762
Iteration 2/1000 | Loss: 0.00005031
Iteration 3/1000 | Loss: 0.00002954
Iteration 4/1000 | Loss: 0.00002547
Iteration 5/1000 | Loss: 0.00002368
Iteration 6/1000 | Loss: 0.00002280
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002127
Iteration 10/1000 | Loss: 0.00002104
Iteration 11/1000 | Loss: 0.00002087
Iteration 12/1000 | Loss: 0.00002070
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00002059
Iteration 15/1000 | Loss: 0.00002056
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002046
Iteration 19/1000 | Loss: 0.00002045
Iteration 20/1000 | Loss: 0.00002045
Iteration 21/1000 | Loss: 0.00002044
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00002042
Iteration 24/1000 | Loss: 0.00002041
Iteration 25/1000 | Loss: 0.00002041
Iteration 26/1000 | Loss: 0.00002040
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002038
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002037
Iteration 36/1000 | Loss: 0.00002037
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002036
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002034
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002032
Iteration 54/1000 | Loss: 0.00002032
Iteration 55/1000 | Loss: 0.00002032
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002030
Iteration 59/1000 | Loss: 0.00002030
Iteration 60/1000 | Loss: 0.00002030
Iteration 61/1000 | Loss: 0.00002030
Iteration 62/1000 | Loss: 0.00002030
Iteration 63/1000 | Loss: 0.00002029
Iteration 64/1000 | Loss: 0.00002029
Iteration 65/1000 | Loss: 0.00002029
Iteration 66/1000 | Loss: 0.00002028
Iteration 67/1000 | Loss: 0.00002028
Iteration 68/1000 | Loss: 0.00002028
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002026
Iteration 74/1000 | Loss: 0.00002026
Iteration 75/1000 | Loss: 0.00002026
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002025
Iteration 79/1000 | Loss: 0.00002025
Iteration 80/1000 | Loss: 0.00002025
Iteration 81/1000 | Loss: 0.00002024
Iteration 82/1000 | Loss: 0.00002024
Iteration 83/1000 | Loss: 0.00002024
Iteration 84/1000 | Loss: 0.00002024
Iteration 85/1000 | Loss: 0.00002024
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002023
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002023
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002023
Iteration 95/1000 | Loss: 0.00002023
Iteration 96/1000 | Loss: 0.00002023
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002022
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002021
Iteration 101/1000 | Loss: 0.00002021
Iteration 102/1000 | Loss: 0.00002021
Iteration 103/1000 | Loss: 0.00002021
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002021
Iteration 106/1000 | Loss: 0.00002021
Iteration 107/1000 | Loss: 0.00002021
Iteration 108/1000 | Loss: 0.00002021
Iteration 109/1000 | Loss: 0.00002021
Iteration 110/1000 | Loss: 0.00002021
Iteration 111/1000 | Loss: 0.00002021
Iteration 112/1000 | Loss: 0.00002020
Iteration 113/1000 | Loss: 0.00002020
Iteration 114/1000 | Loss: 0.00002020
Iteration 115/1000 | Loss: 0.00002020
Iteration 116/1000 | Loss: 0.00002020
Iteration 117/1000 | Loss: 0.00002020
Iteration 118/1000 | Loss: 0.00002020
Iteration 119/1000 | Loss: 0.00002019
Iteration 120/1000 | Loss: 0.00002019
Iteration 121/1000 | Loss: 0.00002019
Iteration 122/1000 | Loss: 0.00002019
Iteration 123/1000 | Loss: 0.00002019
Iteration 124/1000 | Loss: 0.00002019
Iteration 125/1000 | Loss: 0.00002018
Iteration 126/1000 | Loss: 0.00002018
Iteration 127/1000 | Loss: 0.00002018
Iteration 128/1000 | Loss: 0.00002018
Iteration 129/1000 | Loss: 0.00002018
Iteration 130/1000 | Loss: 0.00002018
Iteration 131/1000 | Loss: 0.00002018
Iteration 132/1000 | Loss: 0.00002018
Iteration 133/1000 | Loss: 0.00002018
Iteration 134/1000 | Loss: 0.00002017
Iteration 135/1000 | Loss: 0.00002017
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002017
Iteration 138/1000 | Loss: 0.00002017
Iteration 139/1000 | Loss: 0.00002016
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002016
Iteration 145/1000 | Loss: 0.00002015
Iteration 146/1000 | Loss: 0.00002015
Iteration 147/1000 | Loss: 0.00002015
Iteration 148/1000 | Loss: 0.00002015
Iteration 149/1000 | Loss: 0.00002015
Iteration 150/1000 | Loss: 0.00002015
Iteration 151/1000 | Loss: 0.00002014
Iteration 152/1000 | Loss: 0.00002014
Iteration 153/1000 | Loss: 0.00002014
Iteration 154/1000 | Loss: 0.00002014
Iteration 155/1000 | Loss: 0.00002014
Iteration 156/1000 | Loss: 0.00002014
Iteration 157/1000 | Loss: 0.00002014
Iteration 158/1000 | Loss: 0.00002014
Iteration 159/1000 | Loss: 0.00002014
Iteration 160/1000 | Loss: 0.00002014
Iteration 161/1000 | Loss: 0.00002014
Iteration 162/1000 | Loss: 0.00002014
Iteration 163/1000 | Loss: 0.00002014
Iteration 164/1000 | Loss: 0.00002014
Iteration 165/1000 | Loss: 0.00002014
Iteration 166/1000 | Loss: 0.00002014
Iteration 167/1000 | Loss: 0.00002014
Iteration 168/1000 | Loss: 0.00002014
Iteration 169/1000 | Loss: 0.00002014
Iteration 170/1000 | Loss: 0.00002014
Iteration 171/1000 | Loss: 0.00002014
Iteration 172/1000 | Loss: 0.00002014
Iteration 173/1000 | Loss: 0.00002014
Iteration 174/1000 | Loss: 0.00002014
Iteration 175/1000 | Loss: 0.00002014
Iteration 176/1000 | Loss: 0.00002014
Iteration 177/1000 | Loss: 0.00002014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.0138606487307698e-05, 2.0138606487307698e-05, 2.0138606487307698e-05, 2.0138606487307698e-05, 2.0138606487307698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0138606487307698e-05

Optimization complete. Final v2v error: 3.699085235595703 mm

Highest mean error: 4.344876766204834 mm for frame 96

Lowest mean error: 3.2706055641174316 mm for frame 47

Saving results

Total time: 48.14549016952515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062480
Iteration 2/25 | Loss: 0.01062480
Iteration 3/25 | Loss: 0.01062479
Iteration 4/25 | Loss: 0.01062479
Iteration 5/25 | Loss: 0.01062479
Iteration 6/25 | Loss: 0.01062478
Iteration 7/25 | Loss: 0.01062478
Iteration 8/25 | Loss: 0.01062478
Iteration 9/25 | Loss: 0.01062478
Iteration 10/25 | Loss: 0.01062478
Iteration 11/25 | Loss: 0.01062477
Iteration 12/25 | Loss: 0.01062477
Iteration 13/25 | Loss: 0.01062477
Iteration 14/25 | Loss: 0.01062477
Iteration 15/25 | Loss: 0.01062476
Iteration 16/25 | Loss: 0.01062476
Iteration 17/25 | Loss: 0.01062476
Iteration 18/25 | Loss: 0.01062475
Iteration 19/25 | Loss: 0.01062475
Iteration 20/25 | Loss: 0.01062474
Iteration 21/25 | Loss: 0.01062474
Iteration 22/25 | Loss: 0.01062474
Iteration 23/25 | Loss: 0.01062474
Iteration 24/25 | Loss: 0.01062474
Iteration 25/25 | Loss: 0.01062474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99718094
Iteration 2/25 | Loss: 0.09310745
Iteration 3/25 | Loss: 0.09306487
Iteration 4/25 | Loss: 0.09303169
Iteration 5/25 | Loss: 0.09303167
Iteration 6/25 | Loss: 0.09303165
Iteration 7/25 | Loss: 0.09303165
Iteration 8/25 | Loss: 0.09303165
Iteration 9/25 | Loss: 0.09303165
Iteration 10/25 | Loss: 0.09303165
Iteration 11/25 | Loss: 0.09303165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.09303165227174759, 0.09303165227174759, 0.09303165227174759, 0.09303165227174759, 0.09303165227174759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09303165227174759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09303165
Iteration 2/1000 | Loss: 0.00514912
Iteration 3/1000 | Loss: 0.00221370
Iteration 4/1000 | Loss: 0.00071518
Iteration 5/1000 | Loss: 0.00250934
Iteration 6/1000 | Loss: 0.00161987
Iteration 7/1000 | Loss: 0.00012515
Iteration 8/1000 | Loss: 0.00008250
Iteration 9/1000 | Loss: 0.00045594
Iteration 10/1000 | Loss: 0.00006149
Iteration 11/1000 | Loss: 0.00005348
Iteration 12/1000 | Loss: 0.00004606
Iteration 13/1000 | Loss: 0.00004105
Iteration 14/1000 | Loss: 0.00003681
Iteration 15/1000 | Loss: 0.00003442
Iteration 16/1000 | Loss: 0.00003248
Iteration 17/1000 | Loss: 0.00003108
Iteration 18/1000 | Loss: 0.00003028
Iteration 19/1000 | Loss: 0.00002948
Iteration 20/1000 | Loss: 0.00002871
Iteration 21/1000 | Loss: 0.00002809
Iteration 22/1000 | Loss: 0.00002761
Iteration 23/1000 | Loss: 0.00002712
Iteration 24/1000 | Loss: 0.00002671
Iteration 25/1000 | Loss: 0.00002631
Iteration 26/1000 | Loss: 0.00004880
Iteration 27/1000 | Loss: 0.00004879
Iteration 28/1000 | Loss: 0.00003650
Iteration 29/1000 | Loss: 0.00002564
Iteration 30/1000 | Loss: 0.00003910
Iteration 31/1000 | Loss: 0.00002572
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002534
Iteration 35/1000 | Loss: 0.00002522
Iteration 36/1000 | Loss: 0.00002518
Iteration 37/1000 | Loss: 0.00002516
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002501
Iteration 40/1000 | Loss: 0.00002501
Iteration 41/1000 | Loss: 0.00002500
Iteration 42/1000 | Loss: 0.00002499
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002498
Iteration 45/1000 | Loss: 0.00002498
Iteration 46/1000 | Loss: 0.00002490
Iteration 47/1000 | Loss: 0.00002488
Iteration 48/1000 | Loss: 0.00002488
Iteration 49/1000 | Loss: 0.00002487
Iteration 50/1000 | Loss: 0.00002487
Iteration 51/1000 | Loss: 0.00002487
Iteration 52/1000 | Loss: 0.00002487
Iteration 53/1000 | Loss: 0.00002487
Iteration 54/1000 | Loss: 0.00002487
Iteration 55/1000 | Loss: 0.00002487
Iteration 56/1000 | Loss: 0.00002487
Iteration 57/1000 | Loss: 0.00002487
Iteration 58/1000 | Loss: 0.00002487
Iteration 59/1000 | Loss: 0.00002487
Iteration 60/1000 | Loss: 0.00002486
Iteration 61/1000 | Loss: 0.00002486
Iteration 62/1000 | Loss: 0.00002480
Iteration 63/1000 | Loss: 0.00002479
Iteration 64/1000 | Loss: 0.00002478
Iteration 65/1000 | Loss: 0.00002477
Iteration 66/1000 | Loss: 0.00002477
Iteration 67/1000 | Loss: 0.00002477
Iteration 68/1000 | Loss: 0.00002477
Iteration 69/1000 | Loss: 0.00002476
Iteration 70/1000 | Loss: 0.00002476
Iteration 71/1000 | Loss: 0.00002475
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002474
Iteration 74/1000 | Loss: 0.00002473
Iteration 75/1000 | Loss: 0.00002473
Iteration 76/1000 | Loss: 0.00002473
Iteration 77/1000 | Loss: 0.00002473
Iteration 78/1000 | Loss: 0.00002473
Iteration 79/1000 | Loss: 0.00002473
Iteration 80/1000 | Loss: 0.00002472
Iteration 81/1000 | Loss: 0.00002472
Iteration 82/1000 | Loss: 0.00002472
Iteration 83/1000 | Loss: 0.00002472
Iteration 84/1000 | Loss: 0.00002472
Iteration 85/1000 | Loss: 0.00002472
Iteration 86/1000 | Loss: 0.00002472
Iteration 87/1000 | Loss: 0.00002472
Iteration 88/1000 | Loss: 0.00002472
Iteration 89/1000 | Loss: 0.00002471
Iteration 90/1000 | Loss: 0.00002471
Iteration 91/1000 | Loss: 0.00002471
Iteration 92/1000 | Loss: 0.00002471
Iteration 93/1000 | Loss: 0.00002471
Iteration 94/1000 | Loss: 0.00002471
Iteration 95/1000 | Loss: 0.00002471
Iteration 96/1000 | Loss: 0.00002470
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002466
Iteration 100/1000 | Loss: 0.00002466
Iteration 101/1000 | Loss: 0.00002466
Iteration 102/1000 | Loss: 0.00002465
Iteration 103/1000 | Loss: 0.00002465
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002465
Iteration 107/1000 | Loss: 0.00002464
Iteration 108/1000 | Loss: 0.00002464
Iteration 109/1000 | Loss: 0.00002464
Iteration 110/1000 | Loss: 0.00002464
Iteration 111/1000 | Loss: 0.00002463
Iteration 112/1000 | Loss: 0.00002463
Iteration 113/1000 | Loss: 0.00002463
Iteration 114/1000 | Loss: 0.00002463
Iteration 115/1000 | Loss: 0.00002463
Iteration 116/1000 | Loss: 0.00002463
Iteration 117/1000 | Loss: 0.00002463
Iteration 118/1000 | Loss: 0.00002463
Iteration 119/1000 | Loss: 0.00002462
Iteration 120/1000 | Loss: 0.00002462
Iteration 121/1000 | Loss: 0.00002462
Iteration 122/1000 | Loss: 0.00002462
Iteration 123/1000 | Loss: 0.00002461
Iteration 124/1000 | Loss: 0.00002460
Iteration 125/1000 | Loss: 0.00002460
Iteration 126/1000 | Loss: 0.00002460
Iteration 127/1000 | Loss: 0.00002459
Iteration 128/1000 | Loss: 0.00002459
Iteration 129/1000 | Loss: 0.00002459
Iteration 130/1000 | Loss: 0.00002459
Iteration 131/1000 | Loss: 0.00002458
Iteration 132/1000 | Loss: 0.00002458
Iteration 133/1000 | Loss: 0.00002458
Iteration 134/1000 | Loss: 0.00002458
Iteration 135/1000 | Loss: 0.00002457
Iteration 136/1000 | Loss: 0.00002457
Iteration 137/1000 | Loss: 0.00002457
Iteration 138/1000 | Loss: 0.00002457
Iteration 139/1000 | Loss: 0.00002457
Iteration 140/1000 | Loss: 0.00002456
Iteration 141/1000 | Loss: 0.00002456
Iteration 142/1000 | Loss: 0.00002456
Iteration 143/1000 | Loss: 0.00002456
Iteration 144/1000 | Loss: 0.00002455
Iteration 145/1000 | Loss: 0.00002455
Iteration 146/1000 | Loss: 0.00002455
Iteration 147/1000 | Loss: 0.00002454
Iteration 148/1000 | Loss: 0.00002454
Iteration 149/1000 | Loss: 0.00002454
Iteration 150/1000 | Loss: 0.00002454
Iteration 151/1000 | Loss: 0.00002453
Iteration 152/1000 | Loss: 0.00002453
Iteration 153/1000 | Loss: 0.00002453
Iteration 154/1000 | Loss: 0.00002452
Iteration 155/1000 | Loss: 0.00002452
Iteration 156/1000 | Loss: 0.00002452
Iteration 157/1000 | Loss: 0.00002452
Iteration 158/1000 | Loss: 0.00002452
Iteration 159/1000 | Loss: 0.00002451
Iteration 160/1000 | Loss: 0.00002451
Iteration 161/1000 | Loss: 0.00002451
Iteration 162/1000 | Loss: 0.00002451
Iteration 163/1000 | Loss: 0.00002450
Iteration 164/1000 | Loss: 0.00002450
Iteration 165/1000 | Loss: 0.00002450
Iteration 166/1000 | Loss: 0.00002450
Iteration 167/1000 | Loss: 0.00002449
Iteration 168/1000 | Loss: 0.00002449
Iteration 169/1000 | Loss: 0.00002449
Iteration 170/1000 | Loss: 0.00002449
Iteration 171/1000 | Loss: 0.00002449
Iteration 172/1000 | Loss: 0.00002449
Iteration 173/1000 | Loss: 0.00002449
Iteration 174/1000 | Loss: 0.00002448
Iteration 175/1000 | Loss: 0.00002448
Iteration 176/1000 | Loss: 0.00002448
Iteration 177/1000 | Loss: 0.00002448
Iteration 178/1000 | Loss: 0.00002448
Iteration 179/1000 | Loss: 0.00002447
Iteration 180/1000 | Loss: 0.00002447
Iteration 181/1000 | Loss: 0.00002447
Iteration 182/1000 | Loss: 0.00002447
Iteration 183/1000 | Loss: 0.00002447
Iteration 184/1000 | Loss: 0.00004807
Iteration 185/1000 | Loss: 0.00003065
Iteration 186/1000 | Loss: 0.00002609
Iteration 187/1000 | Loss: 0.00002443
Iteration 188/1000 | Loss: 0.00002442
Iteration 189/1000 | Loss: 0.00002442
Iteration 190/1000 | Loss: 0.00002441
Iteration 191/1000 | Loss: 0.00002441
Iteration 192/1000 | Loss: 0.00002441
Iteration 193/1000 | Loss: 0.00002441
Iteration 194/1000 | Loss: 0.00002441
Iteration 195/1000 | Loss: 0.00002441
Iteration 196/1000 | Loss: 0.00002441
Iteration 197/1000 | Loss: 0.00002441
Iteration 198/1000 | Loss: 0.00002441
Iteration 199/1000 | Loss: 0.00002440
Iteration 200/1000 | Loss: 0.00002440
Iteration 201/1000 | Loss: 0.00002440
Iteration 202/1000 | Loss: 0.00002440
Iteration 203/1000 | Loss: 0.00002440
Iteration 204/1000 | Loss: 0.00004091
Iteration 205/1000 | Loss: 0.00002723
Iteration 206/1000 | Loss: 0.00002443
Iteration 207/1000 | Loss: 0.00002443
Iteration 208/1000 | Loss: 0.00002443
Iteration 209/1000 | Loss: 0.00002443
Iteration 210/1000 | Loss: 0.00002442
Iteration 211/1000 | Loss: 0.00002442
Iteration 212/1000 | Loss: 0.00002441
Iteration 213/1000 | Loss: 0.00003217
Iteration 214/1000 | Loss: 0.00002444
Iteration 215/1000 | Loss: 0.00002740
Iteration 216/1000 | Loss: 0.00002440
Iteration 217/1000 | Loss: 0.00002440
Iteration 218/1000 | Loss: 0.00002440
Iteration 219/1000 | Loss: 0.00002440
Iteration 220/1000 | Loss: 0.00002440
Iteration 221/1000 | Loss: 0.00002440
Iteration 222/1000 | Loss: 0.00002440
Iteration 223/1000 | Loss: 0.00002440
Iteration 224/1000 | Loss: 0.00002440
Iteration 225/1000 | Loss: 0.00002440
Iteration 226/1000 | Loss: 0.00002440
Iteration 227/1000 | Loss: 0.00002440
Iteration 228/1000 | Loss: 0.00002440
Iteration 229/1000 | Loss: 0.00002440
Iteration 230/1000 | Loss: 0.00002439
Iteration 231/1000 | Loss: 0.00002439
Iteration 232/1000 | Loss: 0.00002439
Iteration 233/1000 | Loss: 0.00002439
Iteration 234/1000 | Loss: 0.00002439
Iteration 235/1000 | Loss: 0.00002439
Iteration 236/1000 | Loss: 0.00002439
Iteration 237/1000 | Loss: 0.00002439
Iteration 238/1000 | Loss: 0.00002439
Iteration 239/1000 | Loss: 0.00002439
Iteration 240/1000 | Loss: 0.00002439
Iteration 241/1000 | Loss: 0.00002439
Iteration 242/1000 | Loss: 0.00002439
Iteration 243/1000 | Loss: 0.00002439
Iteration 244/1000 | Loss: 0.00002439
Iteration 245/1000 | Loss: 0.00002439
Iteration 246/1000 | Loss: 0.00002439
Iteration 247/1000 | Loss: 0.00002439
Iteration 248/1000 | Loss: 0.00002439
Iteration 249/1000 | Loss: 0.00002439
Iteration 250/1000 | Loss: 0.00002439
Iteration 251/1000 | Loss: 0.00002439
Iteration 252/1000 | Loss: 0.00002439
Iteration 253/1000 | Loss: 0.00002439
Iteration 254/1000 | Loss: 0.00002439
Iteration 255/1000 | Loss: 0.00002439
Iteration 256/1000 | Loss: 0.00002439
Iteration 257/1000 | Loss: 0.00002439
Iteration 258/1000 | Loss: 0.00002439
Iteration 259/1000 | Loss: 0.00002439
Iteration 260/1000 | Loss: 0.00002439
Iteration 261/1000 | Loss: 0.00002439
Iteration 262/1000 | Loss: 0.00002439
Iteration 263/1000 | Loss: 0.00002439
Iteration 264/1000 | Loss: 0.00002439
Iteration 265/1000 | Loss: 0.00002439
Iteration 266/1000 | Loss: 0.00002439
Iteration 267/1000 | Loss: 0.00002439
Iteration 268/1000 | Loss: 0.00002439
Iteration 269/1000 | Loss: 0.00002439
Iteration 270/1000 | Loss: 0.00002439
Iteration 271/1000 | Loss: 0.00002439
Iteration 272/1000 | Loss: 0.00002439
Iteration 273/1000 | Loss: 0.00002439
Iteration 274/1000 | Loss: 0.00002439
Iteration 275/1000 | Loss: 0.00002439
Iteration 276/1000 | Loss: 0.00002439
Iteration 277/1000 | Loss: 0.00002439
Iteration 278/1000 | Loss: 0.00002439
Iteration 279/1000 | Loss: 0.00002439
Iteration 280/1000 | Loss: 0.00002439
Iteration 281/1000 | Loss: 0.00002439
Iteration 282/1000 | Loss: 0.00002439
Iteration 283/1000 | Loss: 0.00002439
Iteration 284/1000 | Loss: 0.00002439
Iteration 285/1000 | Loss: 0.00002439
Iteration 286/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [2.4391099941567518e-05, 2.4391099941567518e-05, 2.4391099941567518e-05, 2.4391099941567518e-05, 2.4391099941567518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4391099941567518e-05

Optimization complete. Final v2v error: 3.9994819164276123 mm

Highest mean error: 5.119636535644531 mm for frame 181

Lowest mean error: 3.310300588607788 mm for frame 2

Saving results

Total time: 97.00144386291504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970898
Iteration 2/25 | Loss: 0.00193959
Iteration 3/25 | Loss: 0.00159763
Iteration 4/25 | Loss: 0.00153018
Iteration 5/25 | Loss: 0.00150210
Iteration 6/25 | Loss: 0.00150576
Iteration 7/25 | Loss: 0.00146951
Iteration 8/25 | Loss: 0.00145516
Iteration 9/25 | Loss: 0.00141879
Iteration 10/25 | Loss: 0.00140862
Iteration 11/25 | Loss: 0.00140281
Iteration 12/25 | Loss: 0.00140001
Iteration 13/25 | Loss: 0.00140894
Iteration 14/25 | Loss: 0.00140842
Iteration 15/25 | Loss: 0.00139546
Iteration 16/25 | Loss: 0.00140258
Iteration 17/25 | Loss: 0.00139565
Iteration 18/25 | Loss: 0.00139887
Iteration 19/25 | Loss: 0.00139382
Iteration 20/25 | Loss: 0.00138936
Iteration 21/25 | Loss: 0.00138609
Iteration 22/25 | Loss: 0.00138650
Iteration 23/25 | Loss: 0.00139282
Iteration 24/25 | Loss: 0.00138487
Iteration 25/25 | Loss: 0.00138214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38011718
Iteration 2/25 | Loss: 0.00089330
Iteration 3/25 | Loss: 0.00089316
Iteration 4/25 | Loss: 0.00089316
Iteration 5/25 | Loss: 0.00089316
Iteration 6/25 | Loss: 0.00089316
Iteration 7/25 | Loss: 0.00089316
Iteration 8/25 | Loss: 0.00089316
Iteration 9/25 | Loss: 0.00089316
Iteration 10/25 | Loss: 0.00089316
Iteration 11/25 | Loss: 0.00089316
Iteration 12/25 | Loss: 0.00089316
Iteration 13/25 | Loss: 0.00089316
Iteration 14/25 | Loss: 0.00089316
Iteration 15/25 | Loss: 0.00089316
Iteration 16/25 | Loss: 0.00089316
Iteration 17/25 | Loss: 0.00089316
Iteration 18/25 | Loss: 0.00089316
Iteration 19/25 | Loss: 0.00089316
Iteration 20/25 | Loss: 0.00089316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000893157091923058, 0.000893157091923058, 0.000893157091923058, 0.000893157091923058, 0.000893157091923058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000893157091923058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089316
Iteration 2/1000 | Loss: 0.00008273
Iteration 3/1000 | Loss: 0.00022109
Iteration 4/1000 | Loss: 0.00020938
Iteration 5/1000 | Loss: 0.00004864
Iteration 6/1000 | Loss: 0.00004283
Iteration 7/1000 | Loss: 0.00003592
Iteration 8/1000 | Loss: 0.00007517
Iteration 9/1000 | Loss: 0.00003432
Iteration 10/1000 | Loss: 0.00003239
Iteration 11/1000 | Loss: 0.00013098
Iteration 12/1000 | Loss: 0.00003141
Iteration 13/1000 | Loss: 0.00003094
Iteration 14/1000 | Loss: 0.00022766
Iteration 15/1000 | Loss: 0.00022616
Iteration 16/1000 | Loss: 0.00011487
Iteration 17/1000 | Loss: 0.00003294
Iteration 18/1000 | Loss: 0.00003088
Iteration 19/1000 | Loss: 0.00002926
Iteration 20/1000 | Loss: 0.00006667
Iteration 21/1000 | Loss: 0.00003229
Iteration 22/1000 | Loss: 0.00002765
Iteration 23/1000 | Loss: 0.00003577
Iteration 24/1000 | Loss: 0.00002686
Iteration 25/1000 | Loss: 0.00003770
Iteration 26/1000 | Loss: 0.00002657
Iteration 27/1000 | Loss: 0.00002644
Iteration 28/1000 | Loss: 0.00002643
Iteration 29/1000 | Loss: 0.00002643
Iteration 30/1000 | Loss: 0.00002642
Iteration 31/1000 | Loss: 0.00002640
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002638
Iteration 34/1000 | Loss: 0.00002630
Iteration 35/1000 | Loss: 0.00002622
Iteration 36/1000 | Loss: 0.00002622
Iteration 37/1000 | Loss: 0.00002621
Iteration 38/1000 | Loss: 0.00002618
Iteration 39/1000 | Loss: 0.00002617
Iteration 40/1000 | Loss: 0.00002616
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002612
Iteration 43/1000 | Loss: 0.00002611
Iteration 44/1000 | Loss: 0.00002611
Iteration 45/1000 | Loss: 0.00002610
Iteration 46/1000 | Loss: 0.00002610
Iteration 47/1000 | Loss: 0.00002609
Iteration 48/1000 | Loss: 0.00002609
Iteration 49/1000 | Loss: 0.00002608
Iteration 50/1000 | Loss: 0.00002608
Iteration 51/1000 | Loss: 0.00002608
Iteration 52/1000 | Loss: 0.00002606
Iteration 53/1000 | Loss: 0.00002605
Iteration 54/1000 | Loss: 0.00002605
Iteration 55/1000 | Loss: 0.00002604
Iteration 56/1000 | Loss: 0.00002603
Iteration 57/1000 | Loss: 0.00002603
Iteration 58/1000 | Loss: 0.00002602
Iteration 59/1000 | Loss: 0.00002602
Iteration 60/1000 | Loss: 0.00002602
Iteration 61/1000 | Loss: 0.00002601
Iteration 62/1000 | Loss: 0.00002601
Iteration 63/1000 | Loss: 0.00002600
Iteration 64/1000 | Loss: 0.00002600
Iteration 65/1000 | Loss: 0.00002599
Iteration 66/1000 | Loss: 0.00002599
Iteration 67/1000 | Loss: 0.00002599
Iteration 68/1000 | Loss: 0.00002598
Iteration 69/1000 | Loss: 0.00002598
Iteration 70/1000 | Loss: 0.00002598
Iteration 71/1000 | Loss: 0.00002598
Iteration 72/1000 | Loss: 0.00002598
Iteration 73/1000 | Loss: 0.00002598
Iteration 74/1000 | Loss: 0.00002598
Iteration 75/1000 | Loss: 0.00002598
Iteration 76/1000 | Loss: 0.00002598
Iteration 77/1000 | Loss: 0.00002597
Iteration 78/1000 | Loss: 0.00002597
Iteration 79/1000 | Loss: 0.00002597
Iteration 80/1000 | Loss: 0.00002596
Iteration 81/1000 | Loss: 0.00002596
Iteration 82/1000 | Loss: 0.00002596
Iteration 83/1000 | Loss: 0.00002596
Iteration 84/1000 | Loss: 0.00002595
Iteration 85/1000 | Loss: 0.00002595
Iteration 86/1000 | Loss: 0.00002595
Iteration 87/1000 | Loss: 0.00002595
Iteration 88/1000 | Loss: 0.00002595
Iteration 89/1000 | Loss: 0.00002594
Iteration 90/1000 | Loss: 0.00002594
Iteration 91/1000 | Loss: 0.00002594
Iteration 92/1000 | Loss: 0.00002594
Iteration 93/1000 | Loss: 0.00002594
Iteration 94/1000 | Loss: 0.00002594
Iteration 95/1000 | Loss: 0.00002593
Iteration 96/1000 | Loss: 0.00002593
Iteration 97/1000 | Loss: 0.00002593
Iteration 98/1000 | Loss: 0.00002593
Iteration 99/1000 | Loss: 0.00002593
Iteration 100/1000 | Loss: 0.00002593
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002593
Iteration 103/1000 | Loss: 0.00002592
Iteration 104/1000 | Loss: 0.00002592
Iteration 105/1000 | Loss: 0.00002592
Iteration 106/1000 | Loss: 0.00002592
Iteration 107/1000 | Loss: 0.00002592
Iteration 108/1000 | Loss: 0.00002592
Iteration 109/1000 | Loss: 0.00002592
Iteration 110/1000 | Loss: 0.00002592
Iteration 111/1000 | Loss: 0.00002592
Iteration 112/1000 | Loss: 0.00002592
Iteration 113/1000 | Loss: 0.00002591
Iteration 114/1000 | Loss: 0.00002591
Iteration 115/1000 | Loss: 0.00002591
Iteration 116/1000 | Loss: 0.00002591
Iteration 117/1000 | Loss: 0.00002591
Iteration 118/1000 | Loss: 0.00002591
Iteration 119/1000 | Loss: 0.00002591
Iteration 120/1000 | Loss: 0.00002591
Iteration 121/1000 | Loss: 0.00002591
Iteration 122/1000 | Loss: 0.00002591
Iteration 123/1000 | Loss: 0.00002591
Iteration 124/1000 | Loss: 0.00002591
Iteration 125/1000 | Loss: 0.00002591
Iteration 126/1000 | Loss: 0.00002591
Iteration 127/1000 | Loss: 0.00002590
Iteration 128/1000 | Loss: 0.00002590
Iteration 129/1000 | Loss: 0.00002590
Iteration 130/1000 | Loss: 0.00002590
Iteration 131/1000 | Loss: 0.00002590
Iteration 132/1000 | Loss: 0.00002590
Iteration 133/1000 | Loss: 0.00002589
Iteration 134/1000 | Loss: 0.00002589
Iteration 135/1000 | Loss: 0.00002589
Iteration 136/1000 | Loss: 0.00002589
Iteration 137/1000 | Loss: 0.00002589
Iteration 138/1000 | Loss: 0.00002589
Iteration 139/1000 | Loss: 0.00002589
Iteration 140/1000 | Loss: 0.00002589
Iteration 141/1000 | Loss: 0.00002589
Iteration 142/1000 | Loss: 0.00002589
Iteration 143/1000 | Loss: 0.00002589
Iteration 144/1000 | Loss: 0.00002589
Iteration 145/1000 | Loss: 0.00002588
Iteration 146/1000 | Loss: 0.00002588
Iteration 147/1000 | Loss: 0.00002588
Iteration 148/1000 | Loss: 0.00002588
Iteration 149/1000 | Loss: 0.00002588
Iteration 150/1000 | Loss: 0.00002588
Iteration 151/1000 | Loss: 0.00002588
Iteration 152/1000 | Loss: 0.00002588
Iteration 153/1000 | Loss: 0.00002587
Iteration 154/1000 | Loss: 0.00002587
Iteration 155/1000 | Loss: 0.00002587
Iteration 156/1000 | Loss: 0.00002587
Iteration 157/1000 | Loss: 0.00002586
Iteration 158/1000 | Loss: 0.00002586
Iteration 159/1000 | Loss: 0.00002586
Iteration 160/1000 | Loss: 0.00002586
Iteration 161/1000 | Loss: 0.00002586
Iteration 162/1000 | Loss: 0.00002586
Iteration 163/1000 | Loss: 0.00002586
Iteration 164/1000 | Loss: 0.00002585
Iteration 165/1000 | Loss: 0.00002585
Iteration 166/1000 | Loss: 0.00002585
Iteration 167/1000 | Loss: 0.00002585
Iteration 168/1000 | Loss: 0.00002585
Iteration 169/1000 | Loss: 0.00002585
Iteration 170/1000 | Loss: 0.00002585
Iteration 171/1000 | Loss: 0.00002585
Iteration 172/1000 | Loss: 0.00002584
Iteration 173/1000 | Loss: 0.00006115
Iteration 174/1000 | Loss: 0.00002588
Iteration 175/1000 | Loss: 0.00002587
Iteration 176/1000 | Loss: 0.00002584
Iteration 177/1000 | Loss: 0.00002584
Iteration 178/1000 | Loss: 0.00002584
Iteration 179/1000 | Loss: 0.00002584
Iteration 180/1000 | Loss: 0.00002583
Iteration 181/1000 | Loss: 0.00002583
Iteration 182/1000 | Loss: 0.00002583
Iteration 183/1000 | Loss: 0.00002582
Iteration 184/1000 | Loss: 0.00002582
Iteration 185/1000 | Loss: 0.00002582
Iteration 186/1000 | Loss: 0.00002582
Iteration 187/1000 | Loss: 0.00002582
Iteration 188/1000 | Loss: 0.00002582
Iteration 189/1000 | Loss: 0.00002582
Iteration 190/1000 | Loss: 0.00002582
Iteration 191/1000 | Loss: 0.00002582
Iteration 192/1000 | Loss: 0.00002582
Iteration 193/1000 | Loss: 0.00002582
Iteration 194/1000 | Loss: 0.00002582
Iteration 195/1000 | Loss: 0.00002582
Iteration 196/1000 | Loss: 0.00002582
Iteration 197/1000 | Loss: 0.00002582
Iteration 198/1000 | Loss: 0.00002582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.5816107154241763e-05, 2.5816107154241763e-05, 2.5816107154241763e-05, 2.5816107154241763e-05, 2.5816107154241763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5816107154241763e-05

Optimization complete. Final v2v error: 4.2884135246276855 mm

Highest mean error: 5.292532920837402 mm for frame 90

Lowest mean error: 3.644336223602295 mm for frame 28

Saving results

Total time: 115.11703038215637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410007
Iteration 2/25 | Loss: 0.00134016
Iteration 3/25 | Loss: 0.00128469
Iteration 4/25 | Loss: 0.00127398
Iteration 5/25 | Loss: 0.00127032
Iteration 6/25 | Loss: 0.00126996
Iteration 7/25 | Loss: 0.00126996
Iteration 8/25 | Loss: 0.00126996
Iteration 9/25 | Loss: 0.00126996
Iteration 10/25 | Loss: 0.00126996
Iteration 11/25 | Loss: 0.00126996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012699640356004238, 0.0012699640356004238, 0.0012699640356004238, 0.0012699640356004238, 0.0012699640356004238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012699640356004238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83497965
Iteration 2/25 | Loss: 0.00081838
Iteration 3/25 | Loss: 0.00081838
Iteration 4/25 | Loss: 0.00081837
Iteration 5/25 | Loss: 0.00081837
Iteration 6/25 | Loss: 0.00081837
Iteration 7/25 | Loss: 0.00081837
Iteration 8/25 | Loss: 0.00081837
Iteration 9/25 | Loss: 0.00081837
Iteration 10/25 | Loss: 0.00081837
Iteration 11/25 | Loss: 0.00081837
Iteration 12/25 | Loss: 0.00081837
Iteration 13/25 | Loss: 0.00081837
Iteration 14/25 | Loss: 0.00081837
Iteration 15/25 | Loss: 0.00081837
Iteration 16/25 | Loss: 0.00081837
Iteration 17/25 | Loss: 0.00081837
Iteration 18/25 | Loss: 0.00081837
Iteration 19/25 | Loss: 0.00081837
Iteration 20/25 | Loss: 0.00081837
Iteration 21/25 | Loss: 0.00081837
Iteration 22/25 | Loss: 0.00081837
Iteration 23/25 | Loss: 0.00081837
Iteration 24/25 | Loss: 0.00081837
Iteration 25/25 | Loss: 0.00081837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081837
Iteration 2/1000 | Loss: 0.00003365
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001686
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001590
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00001542
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001488
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001466
Iteration 32/1000 | Loss: 0.00001466
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001461
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001455
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001450
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001443
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001438
Iteration 108/1000 | Loss: 0.00001438
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001438
Iteration 111/1000 | Loss: 0.00001437
Iteration 112/1000 | Loss: 0.00001437
Iteration 113/1000 | Loss: 0.00001437
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001437
Iteration 117/1000 | Loss: 0.00001436
Iteration 118/1000 | Loss: 0.00001436
Iteration 119/1000 | Loss: 0.00001435
Iteration 120/1000 | Loss: 0.00001435
Iteration 121/1000 | Loss: 0.00001435
Iteration 122/1000 | Loss: 0.00001435
Iteration 123/1000 | Loss: 0.00001435
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001434
Iteration 127/1000 | Loss: 0.00001434
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001434
Iteration 132/1000 | Loss: 0.00001434
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001434
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001432
Iteration 140/1000 | Loss: 0.00001432
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001432
Iteration 143/1000 | Loss: 0.00001432
Iteration 144/1000 | Loss: 0.00001431
Iteration 145/1000 | Loss: 0.00001431
Iteration 146/1000 | Loss: 0.00001431
Iteration 147/1000 | Loss: 0.00001431
Iteration 148/1000 | Loss: 0.00001431
Iteration 149/1000 | Loss: 0.00001431
Iteration 150/1000 | Loss: 0.00001431
Iteration 151/1000 | Loss: 0.00001431
Iteration 152/1000 | Loss: 0.00001431
Iteration 153/1000 | Loss: 0.00001431
Iteration 154/1000 | Loss: 0.00001431
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001430
Iteration 163/1000 | Loss: 0.00001430
Iteration 164/1000 | Loss: 0.00001430
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001429
Iteration 173/1000 | Loss: 0.00001429
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001429
Iteration 176/1000 | Loss: 0.00001429
Iteration 177/1000 | Loss: 0.00001429
Iteration 178/1000 | Loss: 0.00001429
Iteration 179/1000 | Loss: 0.00001429
Iteration 180/1000 | Loss: 0.00001429
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001429
Iteration 184/1000 | Loss: 0.00001429
Iteration 185/1000 | Loss: 0.00001429
Iteration 186/1000 | Loss: 0.00001429
Iteration 187/1000 | Loss: 0.00001429
Iteration 188/1000 | Loss: 0.00001429
Iteration 189/1000 | Loss: 0.00001429
Iteration 190/1000 | Loss: 0.00001429
Iteration 191/1000 | Loss: 0.00001429
Iteration 192/1000 | Loss: 0.00001429
Iteration 193/1000 | Loss: 0.00001429
Iteration 194/1000 | Loss: 0.00001429
Iteration 195/1000 | Loss: 0.00001429
Iteration 196/1000 | Loss: 0.00001429
Iteration 197/1000 | Loss: 0.00001429
Iteration 198/1000 | Loss: 0.00001429
Iteration 199/1000 | Loss: 0.00001429
Iteration 200/1000 | Loss: 0.00001429
Iteration 201/1000 | Loss: 0.00001429
Iteration 202/1000 | Loss: 0.00001429
Iteration 203/1000 | Loss: 0.00001429
Iteration 204/1000 | Loss: 0.00001429
Iteration 205/1000 | Loss: 0.00001429
Iteration 206/1000 | Loss: 0.00001429
Iteration 207/1000 | Loss: 0.00001429
Iteration 208/1000 | Loss: 0.00001429
Iteration 209/1000 | Loss: 0.00001429
Iteration 210/1000 | Loss: 0.00001429
Iteration 211/1000 | Loss: 0.00001429
Iteration 212/1000 | Loss: 0.00001429
Iteration 213/1000 | Loss: 0.00001429
Iteration 214/1000 | Loss: 0.00001429
Iteration 215/1000 | Loss: 0.00001429
Iteration 216/1000 | Loss: 0.00001429
Iteration 217/1000 | Loss: 0.00001429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.4289274076872971e-05, 1.4289274076872971e-05, 1.4289274076872971e-05, 1.4289274076872971e-05, 1.4289274076872971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4289274076872971e-05

Optimization complete. Final v2v error: 3.2327520847320557 mm

Highest mean error: 3.7876758575439453 mm for frame 63

Lowest mean error: 3.0323195457458496 mm for frame 80

Saving results

Total time: 41.86694407463074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062028
Iteration 2/25 | Loss: 0.01062027
Iteration 3/25 | Loss: 0.01062027
Iteration 4/25 | Loss: 0.01062027
Iteration 5/25 | Loss: 0.01062027
Iteration 6/25 | Loss: 0.01062027
Iteration 7/25 | Loss: 0.01062027
Iteration 8/25 | Loss: 0.01062026
Iteration 9/25 | Loss: 0.01062026
Iteration 10/25 | Loss: 0.00606116
Iteration 11/25 | Loss: 0.00397074
Iteration 12/25 | Loss: 0.00327839
Iteration 13/25 | Loss: 0.00324675
Iteration 14/25 | Loss: 0.00237549
Iteration 15/25 | Loss: 0.00198515
Iteration 16/25 | Loss: 0.00184226
Iteration 17/25 | Loss: 0.00171769
Iteration 18/25 | Loss: 0.00154606
Iteration 19/25 | Loss: 0.00151205
Iteration 20/25 | Loss: 0.00151462
Iteration 21/25 | Loss: 0.00149116
Iteration 22/25 | Loss: 0.00147499
Iteration 23/25 | Loss: 0.00147033
Iteration 24/25 | Loss: 0.00146955
Iteration 25/25 | Loss: 0.00146847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56971067
Iteration 2/25 | Loss: 0.00107986
Iteration 3/25 | Loss: 0.00100174
Iteration 4/25 | Loss: 0.00100174
Iteration 5/25 | Loss: 0.00100174
Iteration 6/25 | Loss: 0.00100174
Iteration 7/25 | Loss: 0.00100174
Iteration 8/25 | Loss: 0.00100174
Iteration 9/25 | Loss: 0.00100174
Iteration 10/25 | Loss: 0.00100174
Iteration 11/25 | Loss: 0.00100174
Iteration 12/25 | Loss: 0.00100174
Iteration 13/25 | Loss: 0.00100174
Iteration 14/25 | Loss: 0.00100174
Iteration 15/25 | Loss: 0.00100174
Iteration 16/25 | Loss: 0.00100174
Iteration 17/25 | Loss: 0.00100174
Iteration 18/25 | Loss: 0.00100174
Iteration 19/25 | Loss: 0.00100174
Iteration 20/25 | Loss: 0.00100174
Iteration 21/25 | Loss: 0.00100174
Iteration 22/25 | Loss: 0.00100174
Iteration 23/25 | Loss: 0.00100174
Iteration 24/25 | Loss: 0.00100174
Iteration 25/25 | Loss: 0.00100174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100174
Iteration 2/1000 | Loss: 0.00023080
Iteration 3/1000 | Loss: 0.00005692
Iteration 4/1000 | Loss: 0.00007734
Iteration 5/1000 | Loss: 0.00006385
Iteration 6/1000 | Loss: 0.00004812
Iteration 7/1000 | Loss: 0.00003406
Iteration 8/1000 | Loss: 0.00003323
Iteration 9/1000 | Loss: 0.00003249
Iteration 10/1000 | Loss: 0.00003452
Iteration 11/1000 | Loss: 0.00003723
Iteration 12/1000 | Loss: 0.00003097
Iteration 13/1000 | Loss: 0.00003066
Iteration 14/1000 | Loss: 0.00003021
Iteration 15/1000 | Loss: 0.00143134
Iteration 16/1000 | Loss: 0.00077265
Iteration 17/1000 | Loss: 0.00027589
Iteration 18/1000 | Loss: 0.00004933
Iteration 19/1000 | Loss: 0.00003153
Iteration 20/1000 | Loss: 0.00006300
Iteration 21/1000 | Loss: 0.00004059
Iteration 22/1000 | Loss: 0.00002646
Iteration 23/1000 | Loss: 0.00002500
Iteration 24/1000 | Loss: 0.00002980
Iteration 25/1000 | Loss: 0.00005087
Iteration 26/1000 | Loss: 0.00002656
Iteration 27/1000 | Loss: 0.00003032
Iteration 28/1000 | Loss: 0.00002318
Iteration 29/1000 | Loss: 0.00002279
Iteration 30/1000 | Loss: 0.00002768
Iteration 31/1000 | Loss: 0.00002352
Iteration 32/1000 | Loss: 0.00002221
Iteration 33/1000 | Loss: 0.00002201
Iteration 34/1000 | Loss: 0.00002192
Iteration 35/1000 | Loss: 0.00002191
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002184
Iteration 39/1000 | Loss: 0.00002181
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002180
Iteration 44/1000 | Loss: 0.00002180
Iteration 45/1000 | Loss: 0.00002180
Iteration 46/1000 | Loss: 0.00002180
Iteration 47/1000 | Loss: 0.00002179
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002179
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002177
Iteration 55/1000 | Loss: 0.00002177
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002176
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002174
Iteration 64/1000 | Loss: 0.00002174
Iteration 65/1000 | Loss: 0.00002174
Iteration 66/1000 | Loss: 0.00002173
Iteration 67/1000 | Loss: 0.00002173
Iteration 68/1000 | Loss: 0.00002173
Iteration 69/1000 | Loss: 0.00002173
Iteration 70/1000 | Loss: 0.00002172
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002172
Iteration 73/1000 | Loss: 0.00002172
Iteration 74/1000 | Loss: 0.00002172
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002171
Iteration 78/1000 | Loss: 0.00002171
Iteration 79/1000 | Loss: 0.00002171
Iteration 80/1000 | Loss: 0.00002171
Iteration 81/1000 | Loss: 0.00002171
Iteration 82/1000 | Loss: 0.00002171
Iteration 83/1000 | Loss: 0.00002171
Iteration 84/1000 | Loss: 0.00002171
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002170
Iteration 87/1000 | Loss: 0.00002170
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002169
Iteration 91/1000 | Loss: 0.00002169
Iteration 92/1000 | Loss: 0.00002169
Iteration 93/1000 | Loss: 0.00002169
Iteration 94/1000 | Loss: 0.00002168
Iteration 95/1000 | Loss: 0.00002168
Iteration 96/1000 | Loss: 0.00002168
Iteration 97/1000 | Loss: 0.00002168
Iteration 98/1000 | Loss: 0.00002168
Iteration 99/1000 | Loss: 0.00002168
Iteration 100/1000 | Loss: 0.00002168
Iteration 101/1000 | Loss: 0.00002168
Iteration 102/1000 | Loss: 0.00002168
Iteration 103/1000 | Loss: 0.00002168
Iteration 104/1000 | Loss: 0.00002168
Iteration 105/1000 | Loss: 0.00002168
Iteration 106/1000 | Loss: 0.00002167
Iteration 107/1000 | Loss: 0.00002167
Iteration 108/1000 | Loss: 0.00002167
Iteration 109/1000 | Loss: 0.00002167
Iteration 110/1000 | Loss: 0.00002167
Iteration 111/1000 | Loss: 0.00002167
Iteration 112/1000 | Loss: 0.00002166
Iteration 113/1000 | Loss: 0.00002166
Iteration 114/1000 | Loss: 0.00002166
Iteration 115/1000 | Loss: 0.00002166
Iteration 116/1000 | Loss: 0.00002166
Iteration 117/1000 | Loss: 0.00002166
Iteration 118/1000 | Loss: 0.00002166
Iteration 119/1000 | Loss: 0.00002166
Iteration 120/1000 | Loss: 0.00002166
Iteration 121/1000 | Loss: 0.00002166
Iteration 122/1000 | Loss: 0.00002166
Iteration 123/1000 | Loss: 0.00002166
Iteration 124/1000 | Loss: 0.00002166
Iteration 125/1000 | Loss: 0.00002166
Iteration 126/1000 | Loss: 0.00002166
Iteration 127/1000 | Loss: 0.00002166
Iteration 128/1000 | Loss: 0.00002166
Iteration 129/1000 | Loss: 0.00002166
Iteration 130/1000 | Loss: 0.00002166
Iteration 131/1000 | Loss: 0.00002166
Iteration 132/1000 | Loss: 0.00002166
Iteration 133/1000 | Loss: 0.00002166
Iteration 134/1000 | Loss: 0.00002166
Iteration 135/1000 | Loss: 0.00002166
Iteration 136/1000 | Loss: 0.00002166
Iteration 137/1000 | Loss: 0.00002166
Iteration 138/1000 | Loss: 0.00002166
Iteration 139/1000 | Loss: 0.00002166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.1655168893630616e-05, 2.1655168893630616e-05, 2.1655168893630616e-05, 2.1655168893630616e-05, 2.1655168893630616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1655168893630616e-05

Optimization complete. Final v2v error: 3.991940975189209 mm

Highest mean error: 5.8171162605285645 mm for frame 73

Lowest mean error: 3.87076735496521 mm for frame 35

Saving results

Total time: 101.00553369522095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974897
Iteration 2/25 | Loss: 0.00974897
Iteration 3/25 | Loss: 0.00974897
Iteration 4/25 | Loss: 0.00974897
Iteration 5/25 | Loss: 0.00974896
Iteration 6/25 | Loss: 0.00974896
Iteration 7/25 | Loss: 0.00974896
Iteration 8/25 | Loss: 0.00974896
Iteration 9/25 | Loss: 0.00974896
Iteration 10/25 | Loss: 0.00974896
Iteration 11/25 | Loss: 0.00974896
Iteration 12/25 | Loss: 0.00974896
Iteration 13/25 | Loss: 0.00974896
Iteration 14/25 | Loss: 0.00974895
Iteration 15/25 | Loss: 0.00974895
Iteration 16/25 | Loss: 0.00974895
Iteration 17/25 | Loss: 0.00974895
Iteration 18/25 | Loss: 0.00974895
Iteration 19/25 | Loss: 0.00974895
Iteration 20/25 | Loss: 0.00974895
Iteration 21/25 | Loss: 0.00974894
Iteration 22/25 | Loss: 0.00974894
Iteration 23/25 | Loss: 0.00974894
Iteration 24/25 | Loss: 0.00974894
Iteration 25/25 | Loss: 0.00974894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48130679
Iteration 2/25 | Loss: 0.18754672
Iteration 3/25 | Loss: 0.18751304
Iteration 4/25 | Loss: 0.18751299
Iteration 5/25 | Loss: 0.18751302
Iteration 6/25 | Loss: 0.18751299
Iteration 7/25 | Loss: 0.18751299
Iteration 8/25 | Loss: 0.18751299
Iteration 9/25 | Loss: 0.18751298
Iteration 10/25 | Loss: 0.18751298
Iteration 11/25 | Loss: 0.18751298
Iteration 12/25 | Loss: 0.18751298
Iteration 13/25 | Loss: 0.18751298
Iteration 14/25 | Loss: 0.18751298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.18751297891139984, 0.18751297891139984, 0.18751297891139984, 0.18751297891139984, 0.18751297891139984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18751297891139984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18751298
Iteration 2/1000 | Loss: 0.00571779
Iteration 3/1000 | Loss: 0.00245662
Iteration 4/1000 | Loss: 0.00149392
Iteration 5/1000 | Loss: 0.00201886
Iteration 6/1000 | Loss: 0.00267610
Iteration 7/1000 | Loss: 0.00043000
Iteration 8/1000 | Loss: 0.00111479
Iteration 9/1000 | Loss: 0.00071915
Iteration 10/1000 | Loss: 0.00051806
Iteration 11/1000 | Loss: 0.00018312
Iteration 12/1000 | Loss: 0.00015248
Iteration 13/1000 | Loss: 0.00088723
Iteration 14/1000 | Loss: 0.00038744
Iteration 15/1000 | Loss: 0.00035698
Iteration 16/1000 | Loss: 0.00016867
Iteration 17/1000 | Loss: 0.00006414
Iteration 18/1000 | Loss: 0.00010197
Iteration 19/1000 | Loss: 0.00005872
Iteration 20/1000 | Loss: 0.00018846
Iteration 21/1000 | Loss: 0.00018547
Iteration 22/1000 | Loss: 0.00026274
Iteration 23/1000 | Loss: 0.00020933
Iteration 24/1000 | Loss: 0.00023778
Iteration 25/1000 | Loss: 0.00003325
Iteration 26/1000 | Loss: 0.00009418
Iteration 27/1000 | Loss: 0.00016389
Iteration 28/1000 | Loss: 0.00003317
Iteration 29/1000 | Loss: 0.00009721
Iteration 30/1000 | Loss: 0.00017916
Iteration 31/1000 | Loss: 0.00017850
Iteration 32/1000 | Loss: 0.00007037
Iteration 33/1000 | Loss: 0.00003625
Iteration 34/1000 | Loss: 0.00005980
Iteration 35/1000 | Loss: 0.00002615
Iteration 36/1000 | Loss: 0.00002557
Iteration 37/1000 | Loss: 0.00005954
Iteration 38/1000 | Loss: 0.00002458
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00004405
Iteration 41/1000 | Loss: 0.00009129
Iteration 42/1000 | Loss: 0.00002357
Iteration 43/1000 | Loss: 0.00002352
Iteration 44/1000 | Loss: 0.00009072
Iteration 45/1000 | Loss: 0.00009607
Iteration 46/1000 | Loss: 0.00008377
Iteration 47/1000 | Loss: 0.00002367
Iteration 48/1000 | Loss: 0.00005787
Iteration 49/1000 | Loss: 0.00002336
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002322
Iteration 52/1000 | Loss: 0.00002320
Iteration 53/1000 | Loss: 0.00002319
Iteration 54/1000 | Loss: 0.00002317
Iteration 55/1000 | Loss: 0.00007976
Iteration 56/1000 | Loss: 0.00007976
Iteration 57/1000 | Loss: 0.00007141
Iteration 58/1000 | Loss: 0.00004722
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00004899
Iteration 61/1000 | Loss: 0.00013783
Iteration 62/1000 | Loss: 0.00002588
Iteration 63/1000 | Loss: 0.00002594
Iteration 64/1000 | Loss: 0.00002303
Iteration 65/1000 | Loss: 0.00002303
Iteration 66/1000 | Loss: 0.00002303
Iteration 67/1000 | Loss: 0.00002303
Iteration 68/1000 | Loss: 0.00002303
Iteration 69/1000 | Loss: 0.00002303
Iteration 70/1000 | Loss: 0.00002303
Iteration 71/1000 | Loss: 0.00002303
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002302
Iteration 76/1000 | Loss: 0.00002302
Iteration 77/1000 | Loss: 0.00002302
Iteration 78/1000 | Loss: 0.00002301
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002300
Iteration 81/1000 | Loss: 0.00002300
Iteration 82/1000 | Loss: 0.00002300
Iteration 83/1000 | Loss: 0.00002300
Iteration 84/1000 | Loss: 0.00002300
Iteration 85/1000 | Loss: 0.00002300
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002300
Iteration 88/1000 | Loss: 0.00002300
Iteration 89/1000 | Loss: 0.00002299
Iteration 90/1000 | Loss: 0.00002299
Iteration 91/1000 | Loss: 0.00002299
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002297
Iteration 94/1000 | Loss: 0.00002297
Iteration 95/1000 | Loss: 0.00002296
Iteration 96/1000 | Loss: 0.00002296
Iteration 97/1000 | Loss: 0.00002296
Iteration 98/1000 | Loss: 0.00002296
Iteration 99/1000 | Loss: 0.00002296
Iteration 100/1000 | Loss: 0.00002295
Iteration 101/1000 | Loss: 0.00002295
Iteration 102/1000 | Loss: 0.00002295
Iteration 103/1000 | Loss: 0.00002294
Iteration 104/1000 | Loss: 0.00004588
Iteration 105/1000 | Loss: 0.00002646
Iteration 106/1000 | Loss: 0.00002701
Iteration 107/1000 | Loss: 0.00002385
Iteration 108/1000 | Loss: 0.00002296
Iteration 109/1000 | Loss: 0.00002296
Iteration 110/1000 | Loss: 0.00002295
Iteration 111/1000 | Loss: 0.00002295
Iteration 112/1000 | Loss: 0.00002295
Iteration 113/1000 | Loss: 0.00002294
Iteration 114/1000 | Loss: 0.00002294
Iteration 115/1000 | Loss: 0.00002294
Iteration 116/1000 | Loss: 0.00002294
Iteration 117/1000 | Loss: 0.00002294
Iteration 118/1000 | Loss: 0.00002294
Iteration 119/1000 | Loss: 0.00002294
Iteration 120/1000 | Loss: 0.00002294
Iteration 121/1000 | Loss: 0.00002294
Iteration 122/1000 | Loss: 0.00002294
Iteration 123/1000 | Loss: 0.00002362
Iteration 124/1000 | Loss: 0.00002514
Iteration 125/1000 | Loss: 0.00002293
Iteration 126/1000 | Loss: 0.00002291
Iteration 127/1000 | Loss: 0.00002291
Iteration 128/1000 | Loss: 0.00002291
Iteration 129/1000 | Loss: 0.00002291
Iteration 130/1000 | Loss: 0.00002291
Iteration 131/1000 | Loss: 0.00002291
Iteration 132/1000 | Loss: 0.00002291
Iteration 133/1000 | Loss: 0.00002290
Iteration 134/1000 | Loss: 0.00002290
Iteration 135/1000 | Loss: 0.00002290
Iteration 136/1000 | Loss: 0.00002290
Iteration 137/1000 | Loss: 0.00002290
Iteration 138/1000 | Loss: 0.00002290
Iteration 139/1000 | Loss: 0.00002290
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002290
Iteration 143/1000 | Loss: 0.00002290
Iteration 144/1000 | Loss: 0.00002290
Iteration 145/1000 | Loss: 0.00002290
Iteration 146/1000 | Loss: 0.00002290
Iteration 147/1000 | Loss: 0.00002290
Iteration 148/1000 | Loss: 0.00002290
Iteration 149/1000 | Loss: 0.00002290
Iteration 150/1000 | Loss: 0.00002290
Iteration 151/1000 | Loss: 0.00002290
Iteration 152/1000 | Loss: 0.00002290
Iteration 153/1000 | Loss: 0.00002290
Iteration 154/1000 | Loss: 0.00002290
Iteration 155/1000 | Loss: 0.00002290
Iteration 156/1000 | Loss: 0.00002290
Iteration 157/1000 | Loss: 0.00002290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.2903459466760978e-05, 2.2903459466760978e-05, 2.2903459466760978e-05, 2.2903459466760978e-05, 2.2903459466760978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2903459466760978e-05

Optimization complete. Final v2v error: 4.079431056976318 mm

Highest mean error: 4.525125503540039 mm for frame 30

Lowest mean error: 3.797123432159424 mm for frame 139

Saving results

Total time: 112.52386784553528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407921
Iteration 2/25 | Loss: 0.00140767
Iteration 3/25 | Loss: 0.00132682
Iteration 4/25 | Loss: 0.00131674
Iteration 5/25 | Loss: 0.00131429
Iteration 6/25 | Loss: 0.00131403
Iteration 7/25 | Loss: 0.00131403
Iteration 8/25 | Loss: 0.00131403
Iteration 9/25 | Loss: 0.00131403
Iteration 10/25 | Loss: 0.00131403
Iteration 11/25 | Loss: 0.00131403
Iteration 12/25 | Loss: 0.00131403
Iteration 13/25 | Loss: 0.00131403
Iteration 14/25 | Loss: 0.00131403
Iteration 15/25 | Loss: 0.00131403
Iteration 16/25 | Loss: 0.00131403
Iteration 17/25 | Loss: 0.00131403
Iteration 18/25 | Loss: 0.00131403
Iteration 19/25 | Loss: 0.00131403
Iteration 20/25 | Loss: 0.00131403
Iteration 21/25 | Loss: 0.00131403
Iteration 22/25 | Loss: 0.00131403
Iteration 23/25 | Loss: 0.00131403
Iteration 24/25 | Loss: 0.00131403
Iteration 25/25 | Loss: 0.00131403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40539610
Iteration 2/25 | Loss: 0.00089128
Iteration 3/25 | Loss: 0.00089127
Iteration 4/25 | Loss: 0.00089127
Iteration 5/25 | Loss: 0.00089127
Iteration 6/25 | Loss: 0.00089127
Iteration 7/25 | Loss: 0.00089127
Iteration 8/25 | Loss: 0.00089127
Iteration 9/25 | Loss: 0.00089127
Iteration 10/25 | Loss: 0.00089127
Iteration 11/25 | Loss: 0.00089127
Iteration 12/25 | Loss: 0.00089127
Iteration 13/25 | Loss: 0.00089127
Iteration 14/25 | Loss: 0.00089127
Iteration 15/25 | Loss: 0.00089127
Iteration 16/25 | Loss: 0.00089127
Iteration 17/25 | Loss: 0.00089127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000891266914550215, 0.000891266914550215, 0.000891266914550215, 0.000891266914550215, 0.000891266914550215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000891266914550215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089127
Iteration 2/1000 | Loss: 0.00003873
Iteration 3/1000 | Loss: 0.00002708
Iteration 4/1000 | Loss: 0.00002157
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001910
Iteration 8/1000 | Loss: 0.00001866
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001798
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001739
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001738
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001721
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001713
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001706
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001702
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00001699
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001698
Iteration 126/1000 | Loss: 0.00001698
Iteration 127/1000 | Loss: 0.00001698
Iteration 128/1000 | Loss: 0.00001698
Iteration 129/1000 | Loss: 0.00001698
Iteration 130/1000 | Loss: 0.00001698
Iteration 131/1000 | Loss: 0.00001698
Iteration 132/1000 | Loss: 0.00001698
Iteration 133/1000 | Loss: 0.00001698
Iteration 134/1000 | Loss: 0.00001698
Iteration 135/1000 | Loss: 0.00001698
Iteration 136/1000 | Loss: 0.00001698
Iteration 137/1000 | Loss: 0.00001698
Iteration 138/1000 | Loss: 0.00001698
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001697
Iteration 142/1000 | Loss: 0.00001697
Iteration 143/1000 | Loss: 0.00001697
Iteration 144/1000 | Loss: 0.00001697
Iteration 145/1000 | Loss: 0.00001697
Iteration 146/1000 | Loss: 0.00001697
Iteration 147/1000 | Loss: 0.00001697
Iteration 148/1000 | Loss: 0.00001697
Iteration 149/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.6974083337117918e-05, 1.6974083337117918e-05, 1.6974083337117918e-05, 1.6974083337117918e-05, 1.6974083337117918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6974083337117918e-05

Optimization complete. Final v2v error: 3.4342007637023926 mm

Highest mean error: 3.6491081714630127 mm for frame 21

Lowest mean error: 3.093061923980713 mm for frame 65

Saving results

Total time: 39.2334098815918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019104
Iteration 2/25 | Loss: 0.00226041
Iteration 3/25 | Loss: 0.00207701
Iteration 4/25 | Loss: 0.00155031
Iteration 5/25 | Loss: 0.00156245
Iteration 6/25 | Loss: 0.00152129
Iteration 7/25 | Loss: 0.00144749
Iteration 8/25 | Loss: 0.00139000
Iteration 9/25 | Loss: 0.00135219
Iteration 10/25 | Loss: 0.00133833
Iteration 11/25 | Loss: 0.00134649
Iteration 12/25 | Loss: 0.00132458
Iteration 13/25 | Loss: 0.00133178
Iteration 14/25 | Loss: 0.00132501
Iteration 15/25 | Loss: 0.00132179
Iteration 16/25 | Loss: 0.00131713
Iteration 17/25 | Loss: 0.00132146
Iteration 18/25 | Loss: 0.00131527
Iteration 19/25 | Loss: 0.00131216
Iteration 20/25 | Loss: 0.00131189
Iteration 21/25 | Loss: 0.00131225
Iteration 22/25 | Loss: 0.00131024
Iteration 23/25 | Loss: 0.00131021
Iteration 24/25 | Loss: 0.00131021
Iteration 25/25 | Loss: 0.00131020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43471348
Iteration 2/25 | Loss: 0.00104517
Iteration 3/25 | Loss: 0.00090013
Iteration 4/25 | Loss: 0.00090013
Iteration 5/25 | Loss: 0.00090013
Iteration 6/25 | Loss: 0.00090013
Iteration 7/25 | Loss: 0.00090013
Iteration 8/25 | Loss: 0.00090013
Iteration 9/25 | Loss: 0.00090013
Iteration 10/25 | Loss: 0.00090013
Iteration 11/25 | Loss: 0.00090013
Iteration 12/25 | Loss: 0.00090013
Iteration 13/25 | Loss: 0.00090013
Iteration 14/25 | Loss: 0.00090013
Iteration 15/25 | Loss: 0.00090013
Iteration 16/25 | Loss: 0.00090013
Iteration 17/25 | Loss: 0.00090013
Iteration 18/25 | Loss: 0.00090013
Iteration 19/25 | Loss: 0.00090013
Iteration 20/25 | Loss: 0.00090013
Iteration 21/25 | Loss: 0.00090013
Iteration 22/25 | Loss: 0.00090013
Iteration 23/25 | Loss: 0.00090013
Iteration 24/25 | Loss: 0.00090013
Iteration 25/25 | Loss: 0.00090013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090013
Iteration 2/1000 | Loss: 0.00064807
Iteration 3/1000 | Loss: 0.00006854
Iteration 4/1000 | Loss: 0.00004511
Iteration 5/1000 | Loss: 0.00075498
Iteration 6/1000 | Loss: 0.00157803
Iteration 7/1000 | Loss: 0.00042046
Iteration 8/1000 | Loss: 0.00026242
Iteration 9/1000 | Loss: 0.00019577
Iteration 10/1000 | Loss: 0.00017592
Iteration 11/1000 | Loss: 0.00004598
Iteration 12/1000 | Loss: 0.00024613
Iteration 13/1000 | Loss: 0.00024342
Iteration 14/1000 | Loss: 0.00026605
Iteration 15/1000 | Loss: 0.00051278
Iteration 16/1000 | Loss: 0.00003322
Iteration 17/1000 | Loss: 0.00002634
Iteration 18/1000 | Loss: 0.00072051
Iteration 19/1000 | Loss: 0.00017751
Iteration 20/1000 | Loss: 0.00004824
Iteration 21/1000 | Loss: 0.00045295
Iteration 22/1000 | Loss: 0.00010554
Iteration 23/1000 | Loss: 0.00010403
Iteration 24/1000 | Loss: 0.00002366
Iteration 25/1000 | Loss: 0.00010512
Iteration 26/1000 | Loss: 0.00003065
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001906
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00004040
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002619
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001664
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001652
Iteration 44/1000 | Loss: 0.00001640
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001634
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001633
Iteration 51/1000 | Loss: 0.00001633
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001625
Iteration 57/1000 | Loss: 0.00001625
Iteration 58/1000 | Loss: 0.00001625
Iteration 59/1000 | Loss: 0.00001624
Iteration 60/1000 | Loss: 0.00001624
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001623
Iteration 64/1000 | Loss: 0.00001623
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001619
Iteration 78/1000 | Loss: 0.00001619
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001614
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001607
Iteration 108/1000 | Loss: 0.00001606
Iteration 109/1000 | Loss: 0.00001606
Iteration 110/1000 | Loss: 0.00001606
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001604
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001603
Iteration 118/1000 | Loss: 0.00001603
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001601
Iteration 124/1000 | Loss: 0.00001601
Iteration 125/1000 | Loss: 0.00001601
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001601
Iteration 128/1000 | Loss: 0.00001600
Iteration 129/1000 | Loss: 0.00001600
Iteration 130/1000 | Loss: 0.00001599
Iteration 131/1000 | Loss: 0.00001599
Iteration 132/1000 | Loss: 0.00001599
Iteration 133/1000 | Loss: 0.00001599
Iteration 134/1000 | Loss: 0.00001599
Iteration 135/1000 | Loss: 0.00001599
Iteration 136/1000 | Loss: 0.00001598
Iteration 137/1000 | Loss: 0.00001598
Iteration 138/1000 | Loss: 0.00001598
Iteration 139/1000 | Loss: 0.00001598
Iteration 140/1000 | Loss: 0.00001598
Iteration 141/1000 | Loss: 0.00001598
Iteration 142/1000 | Loss: 0.00001598
Iteration 143/1000 | Loss: 0.00001598
Iteration 144/1000 | Loss: 0.00001598
Iteration 145/1000 | Loss: 0.00001597
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001597
Iteration 150/1000 | Loss: 0.00001597
Iteration 151/1000 | Loss: 0.00001597
Iteration 152/1000 | Loss: 0.00001597
Iteration 153/1000 | Loss: 0.00001597
Iteration 154/1000 | Loss: 0.00001597
Iteration 155/1000 | Loss: 0.00001597
Iteration 156/1000 | Loss: 0.00001597
Iteration 157/1000 | Loss: 0.00001597
Iteration 158/1000 | Loss: 0.00001596
Iteration 159/1000 | Loss: 0.00001596
Iteration 160/1000 | Loss: 0.00001596
Iteration 161/1000 | Loss: 0.00001596
Iteration 162/1000 | Loss: 0.00001596
Iteration 163/1000 | Loss: 0.00001596
Iteration 164/1000 | Loss: 0.00001596
Iteration 165/1000 | Loss: 0.00001596
Iteration 166/1000 | Loss: 0.00001596
Iteration 167/1000 | Loss: 0.00001596
Iteration 168/1000 | Loss: 0.00001595
Iteration 169/1000 | Loss: 0.00001595
Iteration 170/1000 | Loss: 0.00001595
Iteration 171/1000 | Loss: 0.00001595
Iteration 172/1000 | Loss: 0.00001595
Iteration 173/1000 | Loss: 0.00001595
Iteration 174/1000 | Loss: 0.00001595
Iteration 175/1000 | Loss: 0.00001595
Iteration 176/1000 | Loss: 0.00001595
Iteration 177/1000 | Loss: 0.00001595
Iteration 178/1000 | Loss: 0.00001595
Iteration 179/1000 | Loss: 0.00001595
Iteration 180/1000 | Loss: 0.00001595
Iteration 181/1000 | Loss: 0.00001594
Iteration 182/1000 | Loss: 0.00001594
Iteration 183/1000 | Loss: 0.00001594
Iteration 184/1000 | Loss: 0.00001594
Iteration 185/1000 | Loss: 0.00001594
Iteration 186/1000 | Loss: 0.00001594
Iteration 187/1000 | Loss: 0.00001594
Iteration 188/1000 | Loss: 0.00001594
Iteration 189/1000 | Loss: 0.00001594
Iteration 190/1000 | Loss: 0.00001594
Iteration 191/1000 | Loss: 0.00001594
Iteration 192/1000 | Loss: 0.00001594
Iteration 193/1000 | Loss: 0.00001594
Iteration 194/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.594364584889263e-05, 1.594364584889263e-05, 1.594364584889263e-05, 1.594364584889263e-05, 1.594364584889263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.594364584889263e-05

Optimization complete. Final v2v error: 3.341132640838623 mm

Highest mean error: 5.947795867919922 mm for frame 88

Lowest mean error: 2.9249327182769775 mm for frame 149

Saving results

Total time: 111.03680610656738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968555
Iteration 2/25 | Loss: 0.00181033
Iteration 3/25 | Loss: 0.00151927
Iteration 4/25 | Loss: 0.00150060
Iteration 5/25 | Loss: 0.00149602
Iteration 6/25 | Loss: 0.00149602
Iteration 7/25 | Loss: 0.00149602
Iteration 8/25 | Loss: 0.00149602
Iteration 9/25 | Loss: 0.00149602
Iteration 10/25 | Loss: 0.00149602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001496016513556242, 0.001496016513556242, 0.001496016513556242, 0.001496016513556242, 0.001496016513556242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001496016513556242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53748661
Iteration 2/25 | Loss: 0.00104310
Iteration 3/25 | Loss: 0.00104310
Iteration 4/25 | Loss: 0.00104310
Iteration 5/25 | Loss: 0.00104310
Iteration 6/25 | Loss: 0.00104310
Iteration 7/25 | Loss: 0.00104310
Iteration 8/25 | Loss: 0.00104310
Iteration 9/25 | Loss: 0.00104310
Iteration 10/25 | Loss: 0.00104310
Iteration 11/25 | Loss: 0.00104310
Iteration 12/25 | Loss: 0.00104309
Iteration 13/25 | Loss: 0.00104309
Iteration 14/25 | Loss: 0.00104309
Iteration 15/25 | Loss: 0.00104309
Iteration 16/25 | Loss: 0.00104309
Iteration 17/25 | Loss: 0.00104309
Iteration 18/25 | Loss: 0.00104309
Iteration 19/25 | Loss: 0.00104309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001043094671331346, 0.001043094671331346, 0.001043094671331346, 0.001043094671331346, 0.001043094671331346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043094671331346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104309
Iteration 2/1000 | Loss: 0.00008157
Iteration 3/1000 | Loss: 0.00005428
Iteration 4/1000 | Loss: 0.00004357
Iteration 5/1000 | Loss: 0.00004077
Iteration 6/1000 | Loss: 0.00003969
Iteration 7/1000 | Loss: 0.00003884
Iteration 8/1000 | Loss: 0.00003793
Iteration 9/1000 | Loss: 0.00003709
Iteration 10/1000 | Loss: 0.00003661
Iteration 11/1000 | Loss: 0.00003622
Iteration 12/1000 | Loss: 0.00003570
Iteration 13/1000 | Loss: 0.00003521
Iteration 14/1000 | Loss: 0.00003482
Iteration 15/1000 | Loss: 0.00003442
Iteration 16/1000 | Loss: 0.00003398
Iteration 17/1000 | Loss: 0.00003365
Iteration 18/1000 | Loss: 0.00003339
Iteration 19/1000 | Loss: 0.00003317
Iteration 20/1000 | Loss: 0.00003297
Iteration 21/1000 | Loss: 0.00003294
Iteration 22/1000 | Loss: 0.00003279
Iteration 23/1000 | Loss: 0.00003265
Iteration 24/1000 | Loss: 0.00003263
Iteration 25/1000 | Loss: 0.00003251
Iteration 26/1000 | Loss: 0.00003249
Iteration 27/1000 | Loss: 0.00003248
Iteration 28/1000 | Loss: 0.00003248
Iteration 29/1000 | Loss: 0.00003247
Iteration 30/1000 | Loss: 0.00003247
Iteration 31/1000 | Loss: 0.00003244
Iteration 32/1000 | Loss: 0.00003242
Iteration 33/1000 | Loss: 0.00003242
Iteration 34/1000 | Loss: 0.00003242
Iteration 35/1000 | Loss: 0.00003242
Iteration 36/1000 | Loss: 0.00003239
Iteration 37/1000 | Loss: 0.00003239
Iteration 38/1000 | Loss: 0.00003237
Iteration 39/1000 | Loss: 0.00003237
Iteration 40/1000 | Loss: 0.00003235
Iteration 41/1000 | Loss: 0.00003235
Iteration 42/1000 | Loss: 0.00003234
Iteration 43/1000 | Loss: 0.00003234
Iteration 44/1000 | Loss: 0.00003234
Iteration 45/1000 | Loss: 0.00003233
Iteration 46/1000 | Loss: 0.00003232
Iteration 47/1000 | Loss: 0.00003232
Iteration 48/1000 | Loss: 0.00003232
Iteration 49/1000 | Loss: 0.00003232
Iteration 50/1000 | Loss: 0.00003232
Iteration 51/1000 | Loss: 0.00003232
Iteration 52/1000 | Loss: 0.00003232
Iteration 53/1000 | Loss: 0.00003231
Iteration 54/1000 | Loss: 0.00003230
Iteration 55/1000 | Loss: 0.00003229
Iteration 56/1000 | Loss: 0.00003228
Iteration 57/1000 | Loss: 0.00003227
Iteration 58/1000 | Loss: 0.00003227
Iteration 59/1000 | Loss: 0.00003227
Iteration 60/1000 | Loss: 0.00003226
Iteration 61/1000 | Loss: 0.00003226
Iteration 62/1000 | Loss: 0.00003226
Iteration 63/1000 | Loss: 0.00003225
Iteration 64/1000 | Loss: 0.00003225
Iteration 65/1000 | Loss: 0.00003225
Iteration 66/1000 | Loss: 0.00003225
Iteration 67/1000 | Loss: 0.00003225
Iteration 68/1000 | Loss: 0.00003225
Iteration 69/1000 | Loss: 0.00003225
Iteration 70/1000 | Loss: 0.00003224
Iteration 71/1000 | Loss: 0.00003224
Iteration 72/1000 | Loss: 0.00003224
Iteration 73/1000 | Loss: 0.00003224
Iteration 74/1000 | Loss: 0.00003224
Iteration 75/1000 | Loss: 0.00003224
Iteration 76/1000 | Loss: 0.00003224
Iteration 77/1000 | Loss: 0.00003224
Iteration 78/1000 | Loss: 0.00003224
Iteration 79/1000 | Loss: 0.00003224
Iteration 80/1000 | Loss: 0.00003223
Iteration 81/1000 | Loss: 0.00003223
Iteration 82/1000 | Loss: 0.00003223
Iteration 83/1000 | Loss: 0.00003223
Iteration 84/1000 | Loss: 0.00003223
Iteration 85/1000 | Loss: 0.00003223
Iteration 86/1000 | Loss: 0.00003222
Iteration 87/1000 | Loss: 0.00003222
Iteration 88/1000 | Loss: 0.00003222
Iteration 89/1000 | Loss: 0.00003222
Iteration 90/1000 | Loss: 0.00003222
Iteration 91/1000 | Loss: 0.00003222
Iteration 92/1000 | Loss: 0.00003221
Iteration 93/1000 | Loss: 0.00003221
Iteration 94/1000 | Loss: 0.00003221
Iteration 95/1000 | Loss: 0.00003221
Iteration 96/1000 | Loss: 0.00003221
Iteration 97/1000 | Loss: 0.00003221
Iteration 98/1000 | Loss: 0.00003220
Iteration 99/1000 | Loss: 0.00003220
Iteration 100/1000 | Loss: 0.00003220
Iteration 101/1000 | Loss: 0.00003220
Iteration 102/1000 | Loss: 0.00003220
Iteration 103/1000 | Loss: 0.00003220
Iteration 104/1000 | Loss: 0.00003219
Iteration 105/1000 | Loss: 0.00003219
Iteration 106/1000 | Loss: 0.00003219
Iteration 107/1000 | Loss: 0.00003219
Iteration 108/1000 | Loss: 0.00003219
Iteration 109/1000 | Loss: 0.00003219
Iteration 110/1000 | Loss: 0.00003219
Iteration 111/1000 | Loss: 0.00003219
Iteration 112/1000 | Loss: 0.00003218
Iteration 113/1000 | Loss: 0.00003218
Iteration 114/1000 | Loss: 0.00003218
Iteration 115/1000 | Loss: 0.00003218
Iteration 116/1000 | Loss: 0.00003218
Iteration 117/1000 | Loss: 0.00003217
Iteration 118/1000 | Loss: 0.00003217
Iteration 119/1000 | Loss: 0.00003217
Iteration 120/1000 | Loss: 0.00003217
Iteration 121/1000 | Loss: 0.00003217
Iteration 122/1000 | Loss: 0.00003217
Iteration 123/1000 | Loss: 0.00003217
Iteration 124/1000 | Loss: 0.00003217
Iteration 125/1000 | Loss: 0.00003216
Iteration 126/1000 | Loss: 0.00003216
Iteration 127/1000 | Loss: 0.00003216
Iteration 128/1000 | Loss: 0.00003216
Iteration 129/1000 | Loss: 0.00003216
Iteration 130/1000 | Loss: 0.00003216
Iteration 131/1000 | Loss: 0.00003216
Iteration 132/1000 | Loss: 0.00003216
Iteration 133/1000 | Loss: 0.00003216
Iteration 134/1000 | Loss: 0.00003215
Iteration 135/1000 | Loss: 0.00003215
Iteration 136/1000 | Loss: 0.00003215
Iteration 137/1000 | Loss: 0.00003215
Iteration 138/1000 | Loss: 0.00003215
Iteration 139/1000 | Loss: 0.00003215
Iteration 140/1000 | Loss: 0.00003215
Iteration 141/1000 | Loss: 0.00003215
Iteration 142/1000 | Loss: 0.00003215
Iteration 143/1000 | Loss: 0.00003215
Iteration 144/1000 | Loss: 0.00003215
Iteration 145/1000 | Loss: 0.00003215
Iteration 146/1000 | Loss: 0.00003215
Iteration 147/1000 | Loss: 0.00003215
Iteration 148/1000 | Loss: 0.00003214
Iteration 149/1000 | Loss: 0.00003214
Iteration 150/1000 | Loss: 0.00003214
Iteration 151/1000 | Loss: 0.00003214
Iteration 152/1000 | Loss: 0.00003214
Iteration 153/1000 | Loss: 0.00003214
Iteration 154/1000 | Loss: 0.00003214
Iteration 155/1000 | Loss: 0.00003214
Iteration 156/1000 | Loss: 0.00003214
Iteration 157/1000 | Loss: 0.00003213
Iteration 158/1000 | Loss: 0.00003213
Iteration 159/1000 | Loss: 0.00003213
Iteration 160/1000 | Loss: 0.00003213
Iteration 161/1000 | Loss: 0.00003213
Iteration 162/1000 | Loss: 0.00003213
Iteration 163/1000 | Loss: 0.00003213
Iteration 164/1000 | Loss: 0.00003213
Iteration 165/1000 | Loss: 0.00003213
Iteration 166/1000 | Loss: 0.00003213
Iteration 167/1000 | Loss: 0.00003213
Iteration 168/1000 | Loss: 0.00003212
Iteration 169/1000 | Loss: 0.00003212
Iteration 170/1000 | Loss: 0.00003212
Iteration 171/1000 | Loss: 0.00003212
Iteration 172/1000 | Loss: 0.00003212
Iteration 173/1000 | Loss: 0.00003212
Iteration 174/1000 | Loss: 0.00003212
Iteration 175/1000 | Loss: 0.00003212
Iteration 176/1000 | Loss: 0.00003212
Iteration 177/1000 | Loss: 0.00003212
Iteration 178/1000 | Loss: 0.00003212
Iteration 179/1000 | Loss: 0.00003212
Iteration 180/1000 | Loss: 0.00003212
Iteration 181/1000 | Loss: 0.00003211
Iteration 182/1000 | Loss: 0.00003211
Iteration 183/1000 | Loss: 0.00003211
Iteration 184/1000 | Loss: 0.00003211
Iteration 185/1000 | Loss: 0.00003211
Iteration 186/1000 | Loss: 0.00003211
Iteration 187/1000 | Loss: 0.00003211
Iteration 188/1000 | Loss: 0.00003210
Iteration 189/1000 | Loss: 0.00003210
Iteration 190/1000 | Loss: 0.00003210
Iteration 191/1000 | Loss: 0.00003210
Iteration 192/1000 | Loss: 0.00003210
Iteration 193/1000 | Loss: 0.00003210
Iteration 194/1000 | Loss: 0.00003210
Iteration 195/1000 | Loss: 0.00003210
Iteration 196/1000 | Loss: 0.00003210
Iteration 197/1000 | Loss: 0.00003210
Iteration 198/1000 | Loss: 0.00003210
Iteration 199/1000 | Loss: 0.00003209
Iteration 200/1000 | Loss: 0.00003209
Iteration 201/1000 | Loss: 0.00003209
Iteration 202/1000 | Loss: 0.00003209
Iteration 203/1000 | Loss: 0.00003209
Iteration 204/1000 | Loss: 0.00003209
Iteration 205/1000 | Loss: 0.00003208
Iteration 206/1000 | Loss: 0.00003208
Iteration 207/1000 | Loss: 0.00003208
Iteration 208/1000 | Loss: 0.00003208
Iteration 209/1000 | Loss: 0.00003208
Iteration 210/1000 | Loss: 0.00003208
Iteration 211/1000 | Loss: 0.00003208
Iteration 212/1000 | Loss: 0.00003208
Iteration 213/1000 | Loss: 0.00003208
Iteration 214/1000 | Loss: 0.00003208
Iteration 215/1000 | Loss: 0.00003208
Iteration 216/1000 | Loss: 0.00003207
Iteration 217/1000 | Loss: 0.00003207
Iteration 218/1000 | Loss: 0.00003207
Iteration 219/1000 | Loss: 0.00003207
Iteration 220/1000 | Loss: 0.00003206
Iteration 221/1000 | Loss: 0.00003206
Iteration 222/1000 | Loss: 0.00003206
Iteration 223/1000 | Loss: 0.00003206
Iteration 224/1000 | Loss: 0.00003206
Iteration 225/1000 | Loss: 0.00003206
Iteration 226/1000 | Loss: 0.00003206
Iteration 227/1000 | Loss: 0.00003206
Iteration 228/1000 | Loss: 0.00003206
Iteration 229/1000 | Loss: 0.00003206
Iteration 230/1000 | Loss: 0.00003205
Iteration 231/1000 | Loss: 0.00003205
Iteration 232/1000 | Loss: 0.00003205
Iteration 233/1000 | Loss: 0.00003205
Iteration 234/1000 | Loss: 0.00003205
Iteration 235/1000 | Loss: 0.00003205
Iteration 236/1000 | Loss: 0.00003205
Iteration 237/1000 | Loss: 0.00003205
Iteration 238/1000 | Loss: 0.00003205
Iteration 239/1000 | Loss: 0.00003205
Iteration 240/1000 | Loss: 0.00003205
Iteration 241/1000 | Loss: 0.00003205
Iteration 242/1000 | Loss: 0.00003204
Iteration 243/1000 | Loss: 0.00003204
Iteration 244/1000 | Loss: 0.00003204
Iteration 245/1000 | Loss: 0.00003204
Iteration 246/1000 | Loss: 0.00003204
Iteration 247/1000 | Loss: 0.00003204
Iteration 248/1000 | Loss: 0.00003204
Iteration 249/1000 | Loss: 0.00003204
Iteration 250/1000 | Loss: 0.00003204
Iteration 251/1000 | Loss: 0.00003204
Iteration 252/1000 | Loss: 0.00003204
Iteration 253/1000 | Loss: 0.00003204
Iteration 254/1000 | Loss: 0.00003204
Iteration 255/1000 | Loss: 0.00003204
Iteration 256/1000 | Loss: 0.00003204
Iteration 257/1000 | Loss: 0.00003204
Iteration 258/1000 | Loss: 0.00003203
Iteration 259/1000 | Loss: 0.00003203
Iteration 260/1000 | Loss: 0.00003203
Iteration 261/1000 | Loss: 0.00003203
Iteration 262/1000 | Loss: 0.00003203
Iteration 263/1000 | Loss: 0.00003203
Iteration 264/1000 | Loss: 0.00003203
Iteration 265/1000 | Loss: 0.00003203
Iteration 266/1000 | Loss: 0.00003203
Iteration 267/1000 | Loss: 0.00003203
Iteration 268/1000 | Loss: 0.00003203
Iteration 269/1000 | Loss: 0.00003203
Iteration 270/1000 | Loss: 0.00003203
Iteration 271/1000 | Loss: 0.00003203
Iteration 272/1000 | Loss: 0.00003203
Iteration 273/1000 | Loss: 0.00003203
Iteration 274/1000 | Loss: 0.00003203
Iteration 275/1000 | Loss: 0.00003203
Iteration 276/1000 | Loss: 0.00003203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [3.202839070581831e-05, 3.202839070581831e-05, 3.202839070581831e-05, 3.202839070581831e-05, 3.202839070581831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.202839070581831e-05

Optimization complete. Final v2v error: 4.8231048583984375 mm

Highest mean error: 5.0398783683776855 mm for frame 100

Lowest mean error: 4.326423168182373 mm for frame 41

Saving results

Total time: 59.60257387161255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858647
Iteration 2/25 | Loss: 0.00219565
Iteration 3/25 | Loss: 0.00203004
Iteration 4/25 | Loss: 0.00164497
Iteration 5/25 | Loss: 0.00166155
Iteration 6/25 | Loss: 0.00165253
Iteration 7/25 | Loss: 0.00161768
Iteration 8/25 | Loss: 0.00157577
Iteration 9/25 | Loss: 0.00156539
Iteration 10/25 | Loss: 0.00156911
Iteration 11/25 | Loss: 0.00157418
Iteration 12/25 | Loss: 0.00156059
Iteration 13/25 | Loss: 0.00155766
Iteration 14/25 | Loss: 0.00154774
Iteration 15/25 | Loss: 0.00154081
Iteration 16/25 | Loss: 0.00154157
Iteration 17/25 | Loss: 0.00154057
Iteration 18/25 | Loss: 0.00153828
Iteration 19/25 | Loss: 0.00154178
Iteration 20/25 | Loss: 0.00153899
Iteration 21/25 | Loss: 0.00153771
Iteration 22/25 | Loss: 0.00153596
Iteration 23/25 | Loss: 0.00154103
Iteration 24/25 | Loss: 0.00154009
Iteration 25/25 | Loss: 0.00153880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.82348490
Iteration 2/25 | Loss: 0.00318469
Iteration 3/25 | Loss: 0.00318469
Iteration 4/25 | Loss: 0.00318469
Iteration 5/25 | Loss: 0.00318469
Iteration 6/25 | Loss: 0.00318469
Iteration 7/25 | Loss: 0.00318469
Iteration 8/25 | Loss: 0.00318469
Iteration 9/25 | Loss: 0.00318469
Iteration 10/25 | Loss: 0.00318469
Iteration 11/25 | Loss: 0.00318469
Iteration 12/25 | Loss: 0.00318469
Iteration 13/25 | Loss: 0.00318469
Iteration 14/25 | Loss: 0.00318469
Iteration 15/25 | Loss: 0.00318469
Iteration 16/25 | Loss: 0.00318469
Iteration 17/25 | Loss: 0.00318469
Iteration 18/25 | Loss: 0.00318469
Iteration 19/25 | Loss: 0.00318469
Iteration 20/25 | Loss: 0.00318469
Iteration 21/25 | Loss: 0.00318469
Iteration 22/25 | Loss: 0.00318469
Iteration 23/25 | Loss: 0.00318469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0031846887432038784, 0.0031846887432038784, 0.0031846887432038784, 0.0031846887432038784, 0.0031846887432038784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031846887432038784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00318469
Iteration 2/1000 | Loss: 0.00044479
Iteration 3/1000 | Loss: 0.00025317
Iteration 4/1000 | Loss: 0.00430440
Iteration 5/1000 | Loss: 0.00231200
Iteration 6/1000 | Loss: 0.00430007
Iteration 7/1000 | Loss: 0.00057061
Iteration 8/1000 | Loss: 0.00207155
Iteration 9/1000 | Loss: 0.00150359
Iteration 10/1000 | Loss: 0.00011341
Iteration 11/1000 | Loss: 0.00093094
Iteration 12/1000 | Loss: 0.00087585
Iteration 13/1000 | Loss: 0.00018337
Iteration 14/1000 | Loss: 0.00047097
Iteration 15/1000 | Loss: 0.00013682
Iteration 16/1000 | Loss: 0.00024910
Iteration 17/1000 | Loss: 0.00004565
Iteration 18/1000 | Loss: 0.00003612
Iteration 19/1000 | Loss: 0.00067472
Iteration 20/1000 | Loss: 0.00003297
Iteration 21/1000 | Loss: 0.00002962
Iteration 22/1000 | Loss: 0.00016319
Iteration 23/1000 | Loss: 0.00003532
Iteration 24/1000 | Loss: 0.00002821
Iteration 25/1000 | Loss: 0.00002576
Iteration 26/1000 | Loss: 0.00002419
Iteration 27/1000 | Loss: 0.00002333
Iteration 28/1000 | Loss: 0.00002235
Iteration 29/1000 | Loss: 0.00002147
Iteration 30/1000 | Loss: 0.00014340
Iteration 31/1000 | Loss: 0.00002549
Iteration 32/1000 | Loss: 0.00002297
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002105
Iteration 35/1000 | Loss: 0.00002064
Iteration 36/1000 | Loss: 0.00002025
Iteration 37/1000 | Loss: 0.00024788
Iteration 38/1000 | Loss: 0.00026346
Iteration 39/1000 | Loss: 0.00002934
Iteration 40/1000 | Loss: 0.00002158
Iteration 41/1000 | Loss: 0.00002000
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001826
Iteration 47/1000 | Loss: 0.00001798
Iteration 48/1000 | Loss: 0.00001774
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001745
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001740
Iteration 57/1000 | Loss: 0.00001740
Iteration 58/1000 | Loss: 0.00001739
Iteration 59/1000 | Loss: 0.00001739
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001728
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001723
Iteration 124/1000 | Loss: 0.00001723
Iteration 125/1000 | Loss: 0.00001723
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001723
Iteration 130/1000 | Loss: 0.00001723
Iteration 131/1000 | Loss: 0.00001722
Iteration 132/1000 | Loss: 0.00001722
Iteration 133/1000 | Loss: 0.00001722
Iteration 134/1000 | Loss: 0.00001722
Iteration 135/1000 | Loss: 0.00001722
Iteration 136/1000 | Loss: 0.00001722
Iteration 137/1000 | Loss: 0.00001721
Iteration 138/1000 | Loss: 0.00001721
Iteration 139/1000 | Loss: 0.00001721
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001719
Iteration 144/1000 | Loss: 0.00001719
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001719
Iteration 147/1000 | Loss: 0.00001719
Iteration 148/1000 | Loss: 0.00001719
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001718
Iteration 151/1000 | Loss: 0.00001718
Iteration 152/1000 | Loss: 0.00001718
Iteration 153/1000 | Loss: 0.00001717
Iteration 154/1000 | Loss: 0.00001717
Iteration 155/1000 | Loss: 0.00001717
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00001714
Iteration 167/1000 | Loss: 0.00001714
Iteration 168/1000 | Loss: 0.00001714
Iteration 169/1000 | Loss: 0.00001714
Iteration 170/1000 | Loss: 0.00001714
Iteration 171/1000 | Loss: 0.00001714
Iteration 172/1000 | Loss: 0.00001714
Iteration 173/1000 | Loss: 0.00001714
Iteration 174/1000 | Loss: 0.00001714
Iteration 175/1000 | Loss: 0.00001713
Iteration 176/1000 | Loss: 0.00001713
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001712
Iteration 181/1000 | Loss: 0.00001712
Iteration 182/1000 | Loss: 0.00001712
Iteration 183/1000 | Loss: 0.00001712
Iteration 184/1000 | Loss: 0.00001712
Iteration 185/1000 | Loss: 0.00001712
Iteration 186/1000 | Loss: 0.00001712
Iteration 187/1000 | Loss: 0.00001711
Iteration 188/1000 | Loss: 0.00001711
Iteration 189/1000 | Loss: 0.00001711
Iteration 190/1000 | Loss: 0.00001711
Iteration 191/1000 | Loss: 0.00001711
Iteration 192/1000 | Loss: 0.00001711
Iteration 193/1000 | Loss: 0.00001711
Iteration 194/1000 | Loss: 0.00001711
Iteration 195/1000 | Loss: 0.00001711
Iteration 196/1000 | Loss: 0.00001710
Iteration 197/1000 | Loss: 0.00001710
Iteration 198/1000 | Loss: 0.00001710
Iteration 199/1000 | Loss: 0.00001710
Iteration 200/1000 | Loss: 0.00001710
Iteration 201/1000 | Loss: 0.00001710
Iteration 202/1000 | Loss: 0.00001710
Iteration 203/1000 | Loss: 0.00001710
Iteration 204/1000 | Loss: 0.00001710
Iteration 205/1000 | Loss: 0.00001709
Iteration 206/1000 | Loss: 0.00001709
Iteration 207/1000 | Loss: 0.00001709
Iteration 208/1000 | Loss: 0.00001709
Iteration 209/1000 | Loss: 0.00001709
Iteration 210/1000 | Loss: 0.00001709
Iteration 211/1000 | Loss: 0.00001709
Iteration 212/1000 | Loss: 0.00001708
Iteration 213/1000 | Loss: 0.00001708
Iteration 214/1000 | Loss: 0.00001708
Iteration 215/1000 | Loss: 0.00001708
Iteration 216/1000 | Loss: 0.00001708
Iteration 217/1000 | Loss: 0.00001707
Iteration 218/1000 | Loss: 0.00001707
Iteration 219/1000 | Loss: 0.00001707
Iteration 220/1000 | Loss: 0.00001707
Iteration 221/1000 | Loss: 0.00001707
Iteration 222/1000 | Loss: 0.00001707
Iteration 223/1000 | Loss: 0.00001707
Iteration 224/1000 | Loss: 0.00001707
Iteration 225/1000 | Loss: 0.00001707
Iteration 226/1000 | Loss: 0.00001707
Iteration 227/1000 | Loss: 0.00001706
Iteration 228/1000 | Loss: 0.00001706
Iteration 229/1000 | Loss: 0.00001706
Iteration 230/1000 | Loss: 0.00001706
Iteration 231/1000 | Loss: 0.00001706
Iteration 232/1000 | Loss: 0.00001706
Iteration 233/1000 | Loss: 0.00001706
Iteration 234/1000 | Loss: 0.00001705
Iteration 235/1000 | Loss: 0.00001705
Iteration 236/1000 | Loss: 0.00001705
Iteration 237/1000 | Loss: 0.00001705
Iteration 238/1000 | Loss: 0.00001705
Iteration 239/1000 | Loss: 0.00001705
Iteration 240/1000 | Loss: 0.00001705
Iteration 241/1000 | Loss: 0.00001705
Iteration 242/1000 | Loss: 0.00001705
Iteration 243/1000 | Loss: 0.00001705
Iteration 244/1000 | Loss: 0.00001705
Iteration 245/1000 | Loss: 0.00001705
Iteration 246/1000 | Loss: 0.00001705
Iteration 247/1000 | Loss: 0.00001705
Iteration 248/1000 | Loss: 0.00001705
Iteration 249/1000 | Loss: 0.00001705
Iteration 250/1000 | Loss: 0.00001705
Iteration 251/1000 | Loss: 0.00001705
Iteration 252/1000 | Loss: 0.00001705
Iteration 253/1000 | Loss: 0.00001705
Iteration 254/1000 | Loss: 0.00001705
Iteration 255/1000 | Loss: 0.00001704
Iteration 256/1000 | Loss: 0.00001704
Iteration 257/1000 | Loss: 0.00001704
Iteration 258/1000 | Loss: 0.00001704
Iteration 259/1000 | Loss: 0.00001704
Iteration 260/1000 | Loss: 0.00001704
Iteration 261/1000 | Loss: 0.00001704
Iteration 262/1000 | Loss: 0.00001704
Iteration 263/1000 | Loss: 0.00001704
Iteration 264/1000 | Loss: 0.00001704
Iteration 265/1000 | Loss: 0.00001704
Iteration 266/1000 | Loss: 0.00001704
Iteration 267/1000 | Loss: 0.00001704
Iteration 268/1000 | Loss: 0.00001704
Iteration 269/1000 | Loss: 0.00001704
Iteration 270/1000 | Loss: 0.00001704
Iteration 271/1000 | Loss: 0.00001704
Iteration 272/1000 | Loss: 0.00001704
Iteration 273/1000 | Loss: 0.00001704
Iteration 274/1000 | Loss: 0.00001704
Iteration 275/1000 | Loss: 0.00001704
Iteration 276/1000 | Loss: 0.00001704
Iteration 277/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.7041182218235917e-05, 1.7041182218235917e-05, 1.7041182218235917e-05, 1.7041182218235917e-05, 1.7041182218235917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7041182218235917e-05

Optimization complete. Final v2v error: 3.439997434616089 mm

Highest mean error: 5.234684467315674 mm for frame 63

Lowest mean error: 2.832397937774658 mm for frame 114

Saving results

Total time: 135.8547887802124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398534
Iteration 2/25 | Loss: 0.00133880
Iteration 3/25 | Loss: 0.00127635
Iteration 4/25 | Loss: 0.00126773
Iteration 5/25 | Loss: 0.00126521
Iteration 6/25 | Loss: 0.00126504
Iteration 7/25 | Loss: 0.00126504
Iteration 8/25 | Loss: 0.00126504
Iteration 9/25 | Loss: 0.00126504
Iteration 10/25 | Loss: 0.00126504
Iteration 11/25 | Loss: 0.00126504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012650417629629374, 0.0012650417629629374, 0.0012650417629629374, 0.0012650417629629374, 0.0012650417629629374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012650417629629374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41567540
Iteration 2/25 | Loss: 0.00090034
Iteration 3/25 | Loss: 0.00090034
Iteration 4/25 | Loss: 0.00090034
Iteration 5/25 | Loss: 0.00090034
Iteration 6/25 | Loss: 0.00090034
Iteration 7/25 | Loss: 0.00090033
Iteration 8/25 | Loss: 0.00090033
Iteration 9/25 | Loss: 0.00090033
Iteration 10/25 | Loss: 0.00090033
Iteration 11/25 | Loss: 0.00090033
Iteration 12/25 | Loss: 0.00090033
Iteration 13/25 | Loss: 0.00090033
Iteration 14/25 | Loss: 0.00090033
Iteration 15/25 | Loss: 0.00090033
Iteration 16/25 | Loss: 0.00090033
Iteration 17/25 | Loss: 0.00090033
Iteration 18/25 | Loss: 0.00090033
Iteration 19/25 | Loss: 0.00090033
Iteration 20/25 | Loss: 0.00090033
Iteration 21/25 | Loss: 0.00090033
Iteration 22/25 | Loss: 0.00090033
Iteration 23/25 | Loss: 0.00090033
Iteration 24/25 | Loss: 0.00090033
Iteration 25/25 | Loss: 0.00090033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090033
Iteration 2/1000 | Loss: 0.00002163
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001435
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001297
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001243
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001220
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001181
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001179
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001173
Iteration 99/1000 | Loss: 0.00001173
Iteration 100/1000 | Loss: 0.00001173
Iteration 101/1000 | Loss: 0.00001173
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001171
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001167
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001167
Iteration 148/1000 | Loss: 0.00001167
Iteration 149/1000 | Loss: 0.00001167
Iteration 150/1000 | Loss: 0.00001167
Iteration 151/1000 | Loss: 0.00001167
Iteration 152/1000 | Loss: 0.00001167
Iteration 153/1000 | Loss: 0.00001167
Iteration 154/1000 | Loss: 0.00001167
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001165
Iteration 163/1000 | Loss: 0.00001165
Iteration 164/1000 | Loss: 0.00001165
Iteration 165/1000 | Loss: 0.00001165
Iteration 166/1000 | Loss: 0.00001164
Iteration 167/1000 | Loss: 0.00001164
Iteration 168/1000 | Loss: 0.00001164
Iteration 169/1000 | Loss: 0.00001164
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001162
Iteration 180/1000 | Loss: 0.00001162
Iteration 181/1000 | Loss: 0.00001162
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001162
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001161
Iteration 191/1000 | Loss: 0.00001161
Iteration 192/1000 | Loss: 0.00001161
Iteration 193/1000 | Loss: 0.00001161
Iteration 194/1000 | Loss: 0.00001161
Iteration 195/1000 | Loss: 0.00001161
Iteration 196/1000 | Loss: 0.00001161
Iteration 197/1000 | Loss: 0.00001161
Iteration 198/1000 | Loss: 0.00001161
Iteration 199/1000 | Loss: 0.00001161
Iteration 200/1000 | Loss: 0.00001161
Iteration 201/1000 | Loss: 0.00001161
Iteration 202/1000 | Loss: 0.00001161
Iteration 203/1000 | Loss: 0.00001161
Iteration 204/1000 | Loss: 0.00001161
Iteration 205/1000 | Loss: 0.00001161
Iteration 206/1000 | Loss: 0.00001161
Iteration 207/1000 | Loss: 0.00001161
Iteration 208/1000 | Loss: 0.00001161
Iteration 209/1000 | Loss: 0.00001161
Iteration 210/1000 | Loss: 0.00001161
Iteration 211/1000 | Loss: 0.00001161
Iteration 212/1000 | Loss: 0.00001161
Iteration 213/1000 | Loss: 0.00001161
Iteration 214/1000 | Loss: 0.00001161
Iteration 215/1000 | Loss: 0.00001161
Iteration 216/1000 | Loss: 0.00001161
Iteration 217/1000 | Loss: 0.00001161
Iteration 218/1000 | Loss: 0.00001161
Iteration 219/1000 | Loss: 0.00001161
Iteration 220/1000 | Loss: 0.00001161
Iteration 221/1000 | Loss: 0.00001161
Iteration 222/1000 | Loss: 0.00001161
Iteration 223/1000 | Loss: 0.00001161
Iteration 224/1000 | Loss: 0.00001161
Iteration 225/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1605869076447561e-05, 1.1605869076447561e-05, 1.1605869076447561e-05, 1.1605869076447561e-05, 1.1605869076447561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1605869076447561e-05

Optimization complete. Final v2v error: 2.923032283782959 mm

Highest mean error: 3.1385509967803955 mm for frame 104

Lowest mean error: 2.7779295444488525 mm for frame 72

Saving results

Total time: 41.86419177055359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948458
Iteration 2/25 | Loss: 0.00948458
Iteration 3/25 | Loss: 0.00369330
Iteration 4/25 | Loss: 0.00246122
Iteration 5/25 | Loss: 0.00239200
Iteration 6/25 | Loss: 0.00231035
Iteration 7/25 | Loss: 0.00225539
Iteration 8/25 | Loss: 0.00217261
Iteration 9/25 | Loss: 0.00211246
Iteration 10/25 | Loss: 0.00210749
Iteration 11/25 | Loss: 0.00210287
Iteration 12/25 | Loss: 0.00209759
Iteration 13/25 | Loss: 0.00208873
Iteration 14/25 | Loss: 0.00209138
Iteration 15/25 | Loss: 0.00207697
Iteration 16/25 | Loss: 0.00206894
Iteration 17/25 | Loss: 0.00207437
Iteration 18/25 | Loss: 0.00206626
Iteration 19/25 | Loss: 0.00206243
Iteration 20/25 | Loss: 0.00206137
Iteration 21/25 | Loss: 0.00206110
Iteration 22/25 | Loss: 0.00206097
Iteration 23/25 | Loss: 0.00206090
Iteration 24/25 | Loss: 0.00206089
Iteration 25/25 | Loss: 0.00206089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37066257
Iteration 2/25 | Loss: 0.00527693
Iteration 3/25 | Loss: 0.00527693
Iteration 4/25 | Loss: 0.00527693
Iteration 5/25 | Loss: 0.00527693
Iteration 6/25 | Loss: 0.00527693
Iteration 7/25 | Loss: 0.00527693
Iteration 8/25 | Loss: 0.00527693
Iteration 9/25 | Loss: 0.00527693
Iteration 10/25 | Loss: 0.00527693
Iteration 11/25 | Loss: 0.00527693
Iteration 12/25 | Loss: 0.00527693
Iteration 13/25 | Loss: 0.00527693
Iteration 14/25 | Loss: 0.00527693
Iteration 15/25 | Loss: 0.00527693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00527692586183548, 0.00527692586183548, 0.00527692586183548, 0.00527692586183548, 0.00527692586183548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00527692586183548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00527693
Iteration 2/1000 | Loss: 0.00062000
Iteration 3/1000 | Loss: 0.00050645
Iteration 4/1000 | Loss: 0.00045062
Iteration 5/1000 | Loss: 0.00040918
Iteration 6/1000 | Loss: 0.00037976
Iteration 7/1000 | Loss: 0.00035709
Iteration 8/1000 | Loss: 0.00034024
Iteration 9/1000 | Loss: 0.00033031
Iteration 10/1000 | Loss: 0.00032371
Iteration 11/1000 | Loss: 0.01536603
Iteration 12/1000 | Loss: 0.00646395
Iteration 13/1000 | Loss: 0.00112369
Iteration 14/1000 | Loss: 0.00038562
Iteration 15/1000 | Loss: 0.00024859
Iteration 16/1000 | Loss: 0.00020334
Iteration 17/1000 | Loss: 0.00014303
Iteration 18/1000 | Loss: 0.00010077
Iteration 19/1000 | Loss: 0.00007621
Iteration 20/1000 | Loss: 0.00005763
Iteration 21/1000 | Loss: 0.00004619
Iteration 22/1000 | Loss: 0.00004057
Iteration 23/1000 | Loss: 0.00003547
Iteration 24/1000 | Loss: 0.00003220
Iteration 25/1000 | Loss: 0.00002871
Iteration 26/1000 | Loss: 0.00002629
Iteration 27/1000 | Loss: 0.00002460
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002091
Iteration 31/1000 | Loss: 0.00002015
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001923
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001903
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001882
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001879
Iteration 42/1000 | Loss: 0.00001879
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001878
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001872
Iteration 53/1000 | Loss: 0.00001872
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001869
Iteration 65/1000 | Loss: 0.00001869
Iteration 66/1000 | Loss: 0.00001869
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001864
Iteration 81/1000 | Loss: 0.00001864
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001863
Iteration 84/1000 | Loss: 0.00001863
Iteration 85/1000 | Loss: 0.00001863
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001863
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001860
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001860
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001859
Iteration 108/1000 | Loss: 0.00001859
Iteration 109/1000 | Loss: 0.00001859
Iteration 110/1000 | Loss: 0.00001859
Iteration 111/1000 | Loss: 0.00001859
Iteration 112/1000 | Loss: 0.00001858
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001858
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001856
Iteration 121/1000 | Loss: 0.00001856
Iteration 122/1000 | Loss: 0.00001856
Iteration 123/1000 | Loss: 0.00001856
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001856
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001856
Iteration 133/1000 | Loss: 0.00001856
Iteration 134/1000 | Loss: 0.00001856
Iteration 135/1000 | Loss: 0.00001856
Iteration 136/1000 | Loss: 0.00001856
Iteration 137/1000 | Loss: 0.00001856
Iteration 138/1000 | Loss: 0.00001856
Iteration 139/1000 | Loss: 0.00001856
Iteration 140/1000 | Loss: 0.00001856
Iteration 141/1000 | Loss: 0.00001856
Iteration 142/1000 | Loss: 0.00001856
Iteration 143/1000 | Loss: 0.00001856
Iteration 144/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.8556524082669057e-05, 1.8556524082669057e-05, 1.8556524082669057e-05, 1.8556524082669057e-05, 1.8556524082669057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8556524082669057e-05

Optimization complete. Final v2v error: 3.695117473602295 mm

Highest mean error: 3.905726432800293 mm for frame 75

Lowest mean error: 3.599433660507202 mm for frame 207

Saving results

Total time: 109.9878261089325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469532
Iteration 2/25 | Loss: 0.00140287
Iteration 3/25 | Loss: 0.00131583
Iteration 4/25 | Loss: 0.00130504
Iteration 5/25 | Loss: 0.00130272
Iteration 6/25 | Loss: 0.00130267
Iteration 7/25 | Loss: 0.00130267
Iteration 8/25 | Loss: 0.00130267
Iteration 9/25 | Loss: 0.00130267
Iteration 10/25 | Loss: 0.00130267
Iteration 11/25 | Loss: 0.00130267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00130266894120723, 0.00130266894120723, 0.00130266894120723, 0.00130266894120723, 0.00130266894120723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00130266894120723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39514124
Iteration 2/25 | Loss: 0.00095794
Iteration 3/25 | Loss: 0.00095793
Iteration 4/25 | Loss: 0.00095793
Iteration 5/25 | Loss: 0.00095793
Iteration 6/25 | Loss: 0.00095793
Iteration 7/25 | Loss: 0.00095793
Iteration 8/25 | Loss: 0.00095793
Iteration 9/25 | Loss: 0.00095793
Iteration 10/25 | Loss: 0.00095793
Iteration 11/25 | Loss: 0.00095793
Iteration 12/25 | Loss: 0.00095793
Iteration 13/25 | Loss: 0.00095793
Iteration 14/25 | Loss: 0.00095793
Iteration 15/25 | Loss: 0.00095793
Iteration 16/25 | Loss: 0.00095793
Iteration 17/25 | Loss: 0.00095793
Iteration 18/25 | Loss: 0.00095793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009579261532053351, 0.0009579261532053351, 0.0009579261532053351, 0.0009579261532053351, 0.0009579261532053351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009579261532053351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095793
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00001842
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001315
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001279
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001254
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001228
Iteration 77/1000 | Loss: 0.00001228
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001219
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001210
Iteration 134/1000 | Loss: 0.00001210
Iteration 135/1000 | Loss: 0.00001210
Iteration 136/1000 | Loss: 0.00001210
Iteration 137/1000 | Loss: 0.00001209
Iteration 138/1000 | Loss: 0.00001209
Iteration 139/1000 | Loss: 0.00001209
Iteration 140/1000 | Loss: 0.00001208
Iteration 141/1000 | Loss: 0.00001208
Iteration 142/1000 | Loss: 0.00001208
Iteration 143/1000 | Loss: 0.00001208
Iteration 144/1000 | Loss: 0.00001208
Iteration 145/1000 | Loss: 0.00001208
Iteration 146/1000 | Loss: 0.00001208
Iteration 147/1000 | Loss: 0.00001208
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001207
Iteration 151/1000 | Loss: 0.00001207
Iteration 152/1000 | Loss: 0.00001207
Iteration 153/1000 | Loss: 0.00001207
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001205
Iteration 164/1000 | Loss: 0.00001205
Iteration 165/1000 | Loss: 0.00001205
Iteration 166/1000 | Loss: 0.00001205
Iteration 167/1000 | Loss: 0.00001205
Iteration 168/1000 | Loss: 0.00001205
Iteration 169/1000 | Loss: 0.00001205
Iteration 170/1000 | Loss: 0.00001204
Iteration 171/1000 | Loss: 0.00001204
Iteration 172/1000 | Loss: 0.00001204
Iteration 173/1000 | Loss: 0.00001204
Iteration 174/1000 | Loss: 0.00001204
Iteration 175/1000 | Loss: 0.00001204
Iteration 176/1000 | Loss: 0.00001204
Iteration 177/1000 | Loss: 0.00001204
Iteration 178/1000 | Loss: 0.00001204
Iteration 179/1000 | Loss: 0.00001203
Iteration 180/1000 | Loss: 0.00001203
Iteration 181/1000 | Loss: 0.00001203
Iteration 182/1000 | Loss: 0.00001203
Iteration 183/1000 | Loss: 0.00001203
Iteration 184/1000 | Loss: 0.00001203
Iteration 185/1000 | Loss: 0.00001203
Iteration 186/1000 | Loss: 0.00001203
Iteration 187/1000 | Loss: 0.00001203
Iteration 188/1000 | Loss: 0.00001203
Iteration 189/1000 | Loss: 0.00001203
Iteration 190/1000 | Loss: 0.00001202
Iteration 191/1000 | Loss: 0.00001202
Iteration 192/1000 | Loss: 0.00001202
Iteration 193/1000 | Loss: 0.00001202
Iteration 194/1000 | Loss: 0.00001202
Iteration 195/1000 | Loss: 0.00001202
Iteration 196/1000 | Loss: 0.00001202
Iteration 197/1000 | Loss: 0.00001202
Iteration 198/1000 | Loss: 0.00001202
Iteration 199/1000 | Loss: 0.00001202
Iteration 200/1000 | Loss: 0.00001202
Iteration 201/1000 | Loss: 0.00001202
Iteration 202/1000 | Loss: 0.00001202
Iteration 203/1000 | Loss: 0.00001201
Iteration 204/1000 | Loss: 0.00001201
Iteration 205/1000 | Loss: 0.00001201
Iteration 206/1000 | Loss: 0.00001201
Iteration 207/1000 | Loss: 0.00001201
Iteration 208/1000 | Loss: 0.00001201
Iteration 209/1000 | Loss: 0.00001201
Iteration 210/1000 | Loss: 0.00001201
Iteration 211/1000 | Loss: 0.00001200
Iteration 212/1000 | Loss: 0.00001200
Iteration 213/1000 | Loss: 0.00001200
Iteration 214/1000 | Loss: 0.00001200
Iteration 215/1000 | Loss: 0.00001200
Iteration 216/1000 | Loss: 0.00001200
Iteration 217/1000 | Loss: 0.00001200
Iteration 218/1000 | Loss: 0.00001200
Iteration 219/1000 | Loss: 0.00001199
Iteration 220/1000 | Loss: 0.00001199
Iteration 221/1000 | Loss: 0.00001199
Iteration 222/1000 | Loss: 0.00001199
Iteration 223/1000 | Loss: 0.00001199
Iteration 224/1000 | Loss: 0.00001199
Iteration 225/1000 | Loss: 0.00001199
Iteration 226/1000 | Loss: 0.00001199
Iteration 227/1000 | Loss: 0.00001198
Iteration 228/1000 | Loss: 0.00001198
Iteration 229/1000 | Loss: 0.00001198
Iteration 230/1000 | Loss: 0.00001198
Iteration 231/1000 | Loss: 0.00001198
Iteration 232/1000 | Loss: 0.00001198
Iteration 233/1000 | Loss: 0.00001197
Iteration 234/1000 | Loss: 0.00001197
Iteration 235/1000 | Loss: 0.00001197
Iteration 236/1000 | Loss: 0.00001197
Iteration 237/1000 | Loss: 0.00001197
Iteration 238/1000 | Loss: 0.00001197
Iteration 239/1000 | Loss: 0.00001197
Iteration 240/1000 | Loss: 0.00001196
Iteration 241/1000 | Loss: 0.00001196
Iteration 242/1000 | Loss: 0.00001196
Iteration 243/1000 | Loss: 0.00001196
Iteration 244/1000 | Loss: 0.00001195
Iteration 245/1000 | Loss: 0.00001195
Iteration 246/1000 | Loss: 0.00001195
Iteration 247/1000 | Loss: 0.00001195
Iteration 248/1000 | Loss: 0.00001195
Iteration 249/1000 | Loss: 0.00001195
Iteration 250/1000 | Loss: 0.00001195
Iteration 251/1000 | Loss: 0.00001195
Iteration 252/1000 | Loss: 0.00001195
Iteration 253/1000 | Loss: 0.00001195
Iteration 254/1000 | Loss: 0.00001195
Iteration 255/1000 | Loss: 0.00001194
Iteration 256/1000 | Loss: 0.00001194
Iteration 257/1000 | Loss: 0.00001194
Iteration 258/1000 | Loss: 0.00001194
Iteration 259/1000 | Loss: 0.00001194
Iteration 260/1000 | Loss: 0.00001194
Iteration 261/1000 | Loss: 0.00001194
Iteration 262/1000 | Loss: 0.00001194
Iteration 263/1000 | Loss: 0.00001194
Iteration 264/1000 | Loss: 0.00001193
Iteration 265/1000 | Loss: 0.00001193
Iteration 266/1000 | Loss: 0.00001193
Iteration 267/1000 | Loss: 0.00001193
Iteration 268/1000 | Loss: 0.00001193
Iteration 269/1000 | Loss: 0.00001193
Iteration 270/1000 | Loss: 0.00001193
Iteration 271/1000 | Loss: 0.00001193
Iteration 272/1000 | Loss: 0.00001193
Iteration 273/1000 | Loss: 0.00001193
Iteration 274/1000 | Loss: 0.00001193
Iteration 275/1000 | Loss: 0.00001193
Iteration 276/1000 | Loss: 0.00001193
Iteration 277/1000 | Loss: 0.00001193
Iteration 278/1000 | Loss: 0.00001193
Iteration 279/1000 | Loss: 0.00001193
Iteration 280/1000 | Loss: 0.00001193
Iteration 281/1000 | Loss: 0.00001193
Iteration 282/1000 | Loss: 0.00001193
Iteration 283/1000 | Loss: 0.00001193
Iteration 284/1000 | Loss: 0.00001193
Iteration 285/1000 | Loss: 0.00001193
Iteration 286/1000 | Loss: 0.00001193
Iteration 287/1000 | Loss: 0.00001193
Iteration 288/1000 | Loss: 0.00001193
Iteration 289/1000 | Loss: 0.00001193
Iteration 290/1000 | Loss: 0.00001193
Iteration 291/1000 | Loss: 0.00001193
Iteration 292/1000 | Loss: 0.00001193
Iteration 293/1000 | Loss: 0.00001193
Iteration 294/1000 | Loss: 0.00001193
Iteration 295/1000 | Loss: 0.00001193
Iteration 296/1000 | Loss: 0.00001193
Iteration 297/1000 | Loss: 0.00001193
Iteration 298/1000 | Loss: 0.00001193
Iteration 299/1000 | Loss: 0.00001193
Iteration 300/1000 | Loss: 0.00001193
Iteration 301/1000 | Loss: 0.00001193
Iteration 302/1000 | Loss: 0.00001193
Iteration 303/1000 | Loss: 0.00001193
Iteration 304/1000 | Loss: 0.00001193
Iteration 305/1000 | Loss: 0.00001193
Iteration 306/1000 | Loss: 0.00001193
Iteration 307/1000 | Loss: 0.00001193
Iteration 308/1000 | Loss: 0.00001193
Iteration 309/1000 | Loss: 0.00001193
Iteration 310/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [1.1930567779927514e-05, 1.1930567779927514e-05, 1.1930567779927514e-05, 1.1930567779927514e-05, 1.1930567779927514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1930567779927514e-05

Optimization complete. Final v2v error: 2.9117538928985596 mm

Highest mean error: 3.2005207538604736 mm for frame 61

Lowest mean error: 2.793508768081665 mm for frame 96

Saving results

Total time: 45.88587522506714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00350607
Iteration 2/25 | Loss: 0.00157276
Iteration 3/25 | Loss: 0.00138041
Iteration 4/25 | Loss: 0.00131004
Iteration 5/25 | Loss: 0.00129876
Iteration 6/25 | Loss: 0.00129372
Iteration 7/25 | Loss: 0.00129558
Iteration 8/25 | Loss: 0.00129687
Iteration 9/25 | Loss: 0.00129198
Iteration 10/25 | Loss: 0.00129096
Iteration 11/25 | Loss: 0.00128856
Iteration 12/25 | Loss: 0.00128627
Iteration 13/25 | Loss: 0.00128600
Iteration 14/25 | Loss: 0.00128597
Iteration 15/25 | Loss: 0.00128597
Iteration 16/25 | Loss: 0.00128597
Iteration 17/25 | Loss: 0.00128596
Iteration 18/25 | Loss: 0.00128596
Iteration 19/25 | Loss: 0.00128596
Iteration 20/25 | Loss: 0.00128596
Iteration 21/25 | Loss: 0.00128596
Iteration 22/25 | Loss: 0.00128596
Iteration 23/25 | Loss: 0.00128596
Iteration 24/25 | Loss: 0.00128596
Iteration 25/25 | Loss: 0.00128596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40134609
Iteration 2/25 | Loss: 0.00097984
Iteration 3/25 | Loss: 0.00097984
Iteration 4/25 | Loss: 0.00097984
Iteration 5/25 | Loss: 0.00097984
Iteration 6/25 | Loss: 0.00097984
Iteration 7/25 | Loss: 0.00097984
Iteration 8/25 | Loss: 0.00097984
Iteration 9/25 | Loss: 0.00097984
Iteration 10/25 | Loss: 0.00097984
Iteration 11/25 | Loss: 0.00097984
Iteration 12/25 | Loss: 0.00097984
Iteration 13/25 | Loss: 0.00097984
Iteration 14/25 | Loss: 0.00097984
Iteration 15/25 | Loss: 0.00097984
Iteration 16/25 | Loss: 0.00097984
Iteration 17/25 | Loss: 0.00097984
Iteration 18/25 | Loss: 0.00097984
Iteration 19/25 | Loss: 0.00097984
Iteration 20/25 | Loss: 0.00097984
Iteration 21/25 | Loss: 0.00097984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009798378450796008, 0.0009798378450796008, 0.0009798378450796008, 0.0009798378450796008, 0.0009798378450796008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009798378450796008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097984
Iteration 2/1000 | Loss: 0.00005169
Iteration 3/1000 | Loss: 0.00003129
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002321
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00001999
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001863
Iteration 12/1000 | Loss: 0.00001839
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001812
Iteration 16/1000 | Loss: 0.00001806
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001778
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00001778
Iteration 56/1000 | Loss: 0.00001778
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001777
Iteration 59/1000 | Loss: 0.00001777
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001776
Iteration 62/1000 | Loss: 0.00001776
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001772
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001771
Iteration 80/1000 | Loss: 0.00001771
Iteration 81/1000 | Loss: 0.00001771
Iteration 82/1000 | Loss: 0.00001771
Iteration 83/1000 | Loss: 0.00001770
Iteration 84/1000 | Loss: 0.00001770
Iteration 85/1000 | Loss: 0.00001770
Iteration 86/1000 | Loss: 0.00001770
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001769
Iteration 89/1000 | Loss: 0.00001769
Iteration 90/1000 | Loss: 0.00001769
Iteration 91/1000 | Loss: 0.00001768
Iteration 92/1000 | Loss: 0.00001768
Iteration 93/1000 | Loss: 0.00001768
Iteration 94/1000 | Loss: 0.00001768
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001766
Iteration 104/1000 | Loss: 0.00001766
Iteration 105/1000 | Loss: 0.00001766
Iteration 106/1000 | Loss: 0.00001766
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001765
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001764
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001763
Iteration 131/1000 | Loss: 0.00001763
Iteration 132/1000 | Loss: 0.00001763
Iteration 133/1000 | Loss: 0.00001763
Iteration 134/1000 | Loss: 0.00001763
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001763
Iteration 138/1000 | Loss: 0.00001763
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001762
Iteration 142/1000 | Loss: 0.00001762
Iteration 143/1000 | Loss: 0.00001762
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001762
Iteration 146/1000 | Loss: 0.00001762
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001761
Iteration 149/1000 | Loss: 0.00001761
Iteration 150/1000 | Loss: 0.00001761
Iteration 151/1000 | Loss: 0.00001761
Iteration 152/1000 | Loss: 0.00001761
Iteration 153/1000 | Loss: 0.00001761
Iteration 154/1000 | Loss: 0.00001761
Iteration 155/1000 | Loss: 0.00001761
Iteration 156/1000 | Loss: 0.00001760
Iteration 157/1000 | Loss: 0.00001760
Iteration 158/1000 | Loss: 0.00001760
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001760
Iteration 167/1000 | Loss: 0.00001760
Iteration 168/1000 | Loss: 0.00001759
Iteration 169/1000 | Loss: 0.00001759
Iteration 170/1000 | Loss: 0.00001759
Iteration 171/1000 | Loss: 0.00001759
Iteration 172/1000 | Loss: 0.00001759
Iteration 173/1000 | Loss: 0.00001759
Iteration 174/1000 | Loss: 0.00001759
Iteration 175/1000 | Loss: 0.00001759
Iteration 176/1000 | Loss: 0.00001759
Iteration 177/1000 | Loss: 0.00001759
Iteration 178/1000 | Loss: 0.00001759
Iteration 179/1000 | Loss: 0.00001759
Iteration 180/1000 | Loss: 0.00001759
Iteration 181/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.7585680325282738e-05, 1.7585680325282738e-05, 1.7585680325282738e-05, 1.7585680325282738e-05, 1.7585680325282738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7585680325282738e-05

Optimization complete. Final v2v error: 3.55145525932312 mm

Highest mean error: 4.298765182495117 mm for frame 70

Lowest mean error: 2.927980899810791 mm for frame 1

Saving results

Total time: 55.693763732910156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955269
Iteration 2/25 | Loss: 0.00173415
Iteration 3/25 | Loss: 0.00136025
Iteration 4/25 | Loss: 0.00132053
Iteration 5/25 | Loss: 0.00131285
Iteration 6/25 | Loss: 0.00130045
Iteration 7/25 | Loss: 0.00129933
Iteration 8/25 | Loss: 0.00129728
Iteration 9/25 | Loss: 0.00129657
Iteration 10/25 | Loss: 0.00129624
Iteration 11/25 | Loss: 0.00129700
Iteration 12/25 | Loss: 0.00129777
Iteration 13/25 | Loss: 0.00129625
Iteration 14/25 | Loss: 0.00129531
Iteration 15/25 | Loss: 0.00129502
Iteration 16/25 | Loss: 0.00129494
Iteration 17/25 | Loss: 0.00129493
Iteration 18/25 | Loss: 0.00129493
Iteration 19/25 | Loss: 0.00129493
Iteration 20/25 | Loss: 0.00129493
Iteration 21/25 | Loss: 0.00129493
Iteration 22/25 | Loss: 0.00129492
Iteration 23/25 | Loss: 0.00129492
Iteration 24/25 | Loss: 0.00129492
Iteration 25/25 | Loss: 0.00129492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89228773
Iteration 2/25 | Loss: 0.00092065
Iteration 3/25 | Loss: 0.00092056
Iteration 4/25 | Loss: 0.00092056
Iteration 5/25 | Loss: 0.00092056
Iteration 6/25 | Loss: 0.00092055
Iteration 7/25 | Loss: 0.00092055
Iteration 8/25 | Loss: 0.00092055
Iteration 9/25 | Loss: 0.00092055
Iteration 10/25 | Loss: 0.00092055
Iteration 11/25 | Loss: 0.00092055
Iteration 12/25 | Loss: 0.00092055
Iteration 13/25 | Loss: 0.00092055
Iteration 14/25 | Loss: 0.00092055
Iteration 15/25 | Loss: 0.00092055
Iteration 16/25 | Loss: 0.00092055
Iteration 17/25 | Loss: 0.00092055
Iteration 18/25 | Loss: 0.00092055
Iteration 19/25 | Loss: 0.00092055
Iteration 20/25 | Loss: 0.00092055
Iteration 21/25 | Loss: 0.00092055
Iteration 22/25 | Loss: 0.00092055
Iteration 23/25 | Loss: 0.00092055
Iteration 24/25 | Loss: 0.00092055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009205532260239124, 0.0009205532260239124, 0.0009205532260239124, 0.0009205532260239124, 0.0009205532260239124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009205532260239124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092055
Iteration 2/1000 | Loss: 0.00004278
Iteration 3/1000 | Loss: 0.00005229
Iteration 4/1000 | Loss: 0.00002428
Iteration 5/1000 | Loss: 0.00003712
Iteration 6/1000 | Loss: 0.00002208
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00002361
Iteration 9/1000 | Loss: 0.00002083
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00001962
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001874
Iteration 17/1000 | Loss: 0.00001870
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001864
Iteration 20/1000 | Loss: 0.00001861
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00001859
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001854
Iteration 27/1000 | Loss: 0.00004684
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00001841
Iteration 31/1000 | Loss: 0.00001839
Iteration 32/1000 | Loss: 0.00001839
Iteration 33/1000 | Loss: 0.00001838
Iteration 34/1000 | Loss: 0.00001838
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001832
Iteration 59/1000 | Loss: 0.00001832
Iteration 60/1000 | Loss: 0.00001831
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001830
Iteration 64/1000 | Loss: 0.00001830
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00004862
Iteration 73/1000 | Loss: 0.00001995
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001833
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001819
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001819
Iteration 119/1000 | Loss: 0.00001819
Iteration 120/1000 | Loss: 0.00001819
Iteration 121/1000 | Loss: 0.00001819
Iteration 122/1000 | Loss: 0.00001819
Iteration 123/1000 | Loss: 0.00001819
Iteration 124/1000 | Loss: 0.00001819
Iteration 125/1000 | Loss: 0.00001819
Iteration 126/1000 | Loss: 0.00001819
Iteration 127/1000 | Loss: 0.00001819
Iteration 128/1000 | Loss: 0.00001819
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001818
Iteration 131/1000 | Loss: 0.00001818
Iteration 132/1000 | Loss: 0.00001818
Iteration 133/1000 | Loss: 0.00001818
Iteration 134/1000 | Loss: 0.00001818
Iteration 135/1000 | Loss: 0.00001818
Iteration 136/1000 | Loss: 0.00001818
Iteration 137/1000 | Loss: 0.00001818
Iteration 138/1000 | Loss: 0.00001818
Iteration 139/1000 | Loss: 0.00001818
Iteration 140/1000 | Loss: 0.00001818
Iteration 141/1000 | Loss: 0.00001818
Iteration 142/1000 | Loss: 0.00001818
Iteration 143/1000 | Loss: 0.00001818
Iteration 144/1000 | Loss: 0.00001818
Iteration 145/1000 | Loss: 0.00001818
Iteration 146/1000 | Loss: 0.00001818
Iteration 147/1000 | Loss: 0.00001818
Iteration 148/1000 | Loss: 0.00001818
Iteration 149/1000 | Loss: 0.00001818
Iteration 150/1000 | Loss: 0.00001818
Iteration 151/1000 | Loss: 0.00001818
Iteration 152/1000 | Loss: 0.00001818
Iteration 153/1000 | Loss: 0.00001818
Iteration 154/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.81757532118354e-05, 1.81757532118354e-05, 1.81757532118354e-05, 1.81757532118354e-05, 1.81757532118354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.81757532118354e-05

Optimization complete. Final v2v error: 3.580556631088257 mm

Highest mean error: 4.476093292236328 mm for frame 239

Lowest mean error: 3.006772756576538 mm for frame 135

Saving results

Total time: 77.01301765441895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473399
Iteration 2/25 | Loss: 0.00151381
Iteration 3/25 | Loss: 0.00133084
Iteration 4/25 | Loss: 0.00131137
Iteration 5/25 | Loss: 0.00130856
Iteration 6/25 | Loss: 0.00130813
Iteration 7/25 | Loss: 0.00130813
Iteration 8/25 | Loss: 0.00130813
Iteration 9/25 | Loss: 0.00130813
Iteration 10/25 | Loss: 0.00130813
Iteration 11/25 | Loss: 0.00130813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013081268407404423, 0.0013081268407404423, 0.0013081268407404423, 0.0013081268407404423, 0.0013081268407404423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013081268407404423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40145016
Iteration 2/25 | Loss: 0.00080306
Iteration 3/25 | Loss: 0.00080306
Iteration 4/25 | Loss: 0.00080306
Iteration 5/25 | Loss: 0.00080305
Iteration 6/25 | Loss: 0.00080305
Iteration 7/25 | Loss: 0.00080305
Iteration 8/25 | Loss: 0.00080305
Iteration 9/25 | Loss: 0.00080305
Iteration 10/25 | Loss: 0.00080305
Iteration 11/25 | Loss: 0.00080305
Iteration 12/25 | Loss: 0.00080305
Iteration 13/25 | Loss: 0.00080305
Iteration 14/25 | Loss: 0.00080305
Iteration 15/25 | Loss: 0.00080305
Iteration 16/25 | Loss: 0.00080305
Iteration 17/25 | Loss: 0.00080305
Iteration 18/25 | Loss: 0.00080305
Iteration 19/25 | Loss: 0.00080305
Iteration 20/25 | Loss: 0.00080305
Iteration 21/25 | Loss: 0.00080305
Iteration 22/25 | Loss: 0.00080305
Iteration 23/25 | Loss: 0.00080305
Iteration 24/25 | Loss: 0.00080305
Iteration 25/25 | Loss: 0.00080305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080305
Iteration 2/1000 | Loss: 0.00002929
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001712
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001468
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001345
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001334
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001333
Iteration 110/1000 | Loss: 0.00001333
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001332
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001328
Iteration 143/1000 | Loss: 0.00001328
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001327
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001326
Iteration 160/1000 | Loss: 0.00001326
Iteration 161/1000 | Loss: 0.00001326
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001325
Iteration 167/1000 | Loss: 0.00001325
Iteration 168/1000 | Loss: 0.00001325
Iteration 169/1000 | Loss: 0.00001325
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00001324
Iteration 174/1000 | Loss: 0.00001324
Iteration 175/1000 | Loss: 0.00001324
Iteration 176/1000 | Loss: 0.00001324
Iteration 177/1000 | Loss: 0.00001324
Iteration 178/1000 | Loss: 0.00001324
Iteration 179/1000 | Loss: 0.00001324
Iteration 180/1000 | Loss: 0.00001324
Iteration 181/1000 | Loss: 0.00001323
Iteration 182/1000 | Loss: 0.00001323
Iteration 183/1000 | Loss: 0.00001323
Iteration 184/1000 | Loss: 0.00001323
Iteration 185/1000 | Loss: 0.00001323
Iteration 186/1000 | Loss: 0.00001322
Iteration 187/1000 | Loss: 0.00001322
Iteration 188/1000 | Loss: 0.00001322
Iteration 189/1000 | Loss: 0.00001322
Iteration 190/1000 | Loss: 0.00001322
Iteration 191/1000 | Loss: 0.00001322
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001322
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001321
Iteration 196/1000 | Loss: 0.00001321
Iteration 197/1000 | Loss: 0.00001321
Iteration 198/1000 | Loss: 0.00001320
Iteration 199/1000 | Loss: 0.00001320
Iteration 200/1000 | Loss: 0.00001320
Iteration 201/1000 | Loss: 0.00001320
Iteration 202/1000 | Loss: 0.00001320
Iteration 203/1000 | Loss: 0.00001320
Iteration 204/1000 | Loss: 0.00001319
Iteration 205/1000 | Loss: 0.00001319
Iteration 206/1000 | Loss: 0.00001319
Iteration 207/1000 | Loss: 0.00001319
Iteration 208/1000 | Loss: 0.00001319
Iteration 209/1000 | Loss: 0.00001319
Iteration 210/1000 | Loss: 0.00001319
Iteration 211/1000 | Loss: 0.00001319
Iteration 212/1000 | Loss: 0.00001319
Iteration 213/1000 | Loss: 0.00001318
Iteration 214/1000 | Loss: 0.00001318
Iteration 215/1000 | Loss: 0.00001318
Iteration 216/1000 | Loss: 0.00001318
Iteration 217/1000 | Loss: 0.00001318
Iteration 218/1000 | Loss: 0.00001318
Iteration 219/1000 | Loss: 0.00001318
Iteration 220/1000 | Loss: 0.00001318
Iteration 221/1000 | Loss: 0.00001318
Iteration 222/1000 | Loss: 0.00001317
Iteration 223/1000 | Loss: 0.00001317
Iteration 224/1000 | Loss: 0.00001317
Iteration 225/1000 | Loss: 0.00001317
Iteration 226/1000 | Loss: 0.00001317
Iteration 227/1000 | Loss: 0.00001317
Iteration 228/1000 | Loss: 0.00001317
Iteration 229/1000 | Loss: 0.00001317
Iteration 230/1000 | Loss: 0.00001317
Iteration 231/1000 | Loss: 0.00001317
Iteration 232/1000 | Loss: 0.00001317
Iteration 233/1000 | Loss: 0.00001317
Iteration 234/1000 | Loss: 0.00001317
Iteration 235/1000 | Loss: 0.00001317
Iteration 236/1000 | Loss: 0.00001317
Iteration 237/1000 | Loss: 0.00001317
Iteration 238/1000 | Loss: 0.00001316
Iteration 239/1000 | Loss: 0.00001316
Iteration 240/1000 | Loss: 0.00001316
Iteration 241/1000 | Loss: 0.00001316
Iteration 242/1000 | Loss: 0.00001316
Iteration 243/1000 | Loss: 0.00001316
Iteration 244/1000 | Loss: 0.00001316
Iteration 245/1000 | Loss: 0.00001316
Iteration 246/1000 | Loss: 0.00001316
Iteration 247/1000 | Loss: 0.00001316
Iteration 248/1000 | Loss: 0.00001316
Iteration 249/1000 | Loss: 0.00001316
Iteration 250/1000 | Loss: 0.00001316
Iteration 251/1000 | Loss: 0.00001316
Iteration 252/1000 | Loss: 0.00001316
Iteration 253/1000 | Loss: 0.00001316
Iteration 254/1000 | Loss: 0.00001316
Iteration 255/1000 | Loss: 0.00001316
Iteration 256/1000 | Loss: 0.00001316
Iteration 257/1000 | Loss: 0.00001316
Iteration 258/1000 | Loss: 0.00001316
Iteration 259/1000 | Loss: 0.00001316
Iteration 260/1000 | Loss: 0.00001316
Iteration 261/1000 | Loss: 0.00001316
Iteration 262/1000 | Loss: 0.00001316
Iteration 263/1000 | Loss: 0.00001316
Iteration 264/1000 | Loss: 0.00001316
Iteration 265/1000 | Loss: 0.00001316
Iteration 266/1000 | Loss: 0.00001316
Iteration 267/1000 | Loss: 0.00001316
Iteration 268/1000 | Loss: 0.00001316
Iteration 269/1000 | Loss: 0.00001316
Iteration 270/1000 | Loss: 0.00001316
Iteration 271/1000 | Loss: 0.00001316
Iteration 272/1000 | Loss: 0.00001316
Iteration 273/1000 | Loss: 0.00001316
Iteration 274/1000 | Loss: 0.00001316
Iteration 275/1000 | Loss: 0.00001316
Iteration 276/1000 | Loss: 0.00001316
Iteration 277/1000 | Loss: 0.00001316
Iteration 278/1000 | Loss: 0.00001316
Iteration 279/1000 | Loss: 0.00001316
Iteration 280/1000 | Loss: 0.00001316
Iteration 281/1000 | Loss: 0.00001316
Iteration 282/1000 | Loss: 0.00001316
Iteration 283/1000 | Loss: 0.00001316
Iteration 284/1000 | Loss: 0.00001316
Iteration 285/1000 | Loss: 0.00001316
Iteration 286/1000 | Loss: 0.00001316
Iteration 287/1000 | Loss: 0.00001316
Iteration 288/1000 | Loss: 0.00001316
Iteration 289/1000 | Loss: 0.00001316
Iteration 290/1000 | Loss: 0.00001316
Iteration 291/1000 | Loss: 0.00001316
Iteration 292/1000 | Loss: 0.00001316
Iteration 293/1000 | Loss: 0.00001316
Iteration 294/1000 | Loss: 0.00001316
Iteration 295/1000 | Loss: 0.00001316
Iteration 296/1000 | Loss: 0.00001316
Iteration 297/1000 | Loss: 0.00001316
Iteration 298/1000 | Loss: 0.00001316
Iteration 299/1000 | Loss: 0.00001316
Iteration 300/1000 | Loss: 0.00001316
Iteration 301/1000 | Loss: 0.00001316
Iteration 302/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.3164935808163136e-05, 1.3164935808163136e-05, 1.3164935808163136e-05, 1.3164935808163136e-05, 1.3164935808163136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3164935808163136e-05

Optimization complete. Final v2v error: 3.0680222511291504 mm

Highest mean error: 3.6231701374053955 mm for frame 82

Lowest mean error: 2.795789957046509 mm for frame 153

Saving results

Total time: 47.34793734550476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436571
Iteration 2/25 | Loss: 0.00137744
Iteration 3/25 | Loss: 0.00130307
Iteration 4/25 | Loss: 0.00129006
Iteration 5/25 | Loss: 0.00128532
Iteration 6/25 | Loss: 0.00128406
Iteration 7/25 | Loss: 0.00128387
Iteration 8/25 | Loss: 0.00128387
Iteration 9/25 | Loss: 0.00128387
Iteration 10/25 | Loss: 0.00128387
Iteration 11/25 | Loss: 0.00128387
Iteration 12/25 | Loss: 0.00128387
Iteration 13/25 | Loss: 0.00128387
Iteration 14/25 | Loss: 0.00128387
Iteration 15/25 | Loss: 0.00128387
Iteration 16/25 | Loss: 0.00128387
Iteration 17/25 | Loss: 0.00128387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012838693801313639, 0.0012838693801313639, 0.0012838693801313639, 0.0012838693801313639, 0.0012838693801313639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012838693801313639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88744354
Iteration 2/25 | Loss: 0.00090672
Iteration 3/25 | Loss: 0.00090670
Iteration 4/25 | Loss: 0.00090670
Iteration 5/25 | Loss: 0.00090670
Iteration 6/25 | Loss: 0.00090669
Iteration 7/25 | Loss: 0.00090669
Iteration 8/25 | Loss: 0.00090669
Iteration 9/25 | Loss: 0.00090669
Iteration 10/25 | Loss: 0.00090669
Iteration 11/25 | Loss: 0.00090669
Iteration 12/25 | Loss: 0.00090669
Iteration 13/25 | Loss: 0.00090669
Iteration 14/25 | Loss: 0.00090669
Iteration 15/25 | Loss: 0.00090669
Iteration 16/25 | Loss: 0.00090669
Iteration 17/25 | Loss: 0.00090669
Iteration 18/25 | Loss: 0.00090669
Iteration 19/25 | Loss: 0.00090669
Iteration 20/25 | Loss: 0.00090669
Iteration 21/25 | Loss: 0.00090669
Iteration 22/25 | Loss: 0.00090669
Iteration 23/25 | Loss: 0.00090669
Iteration 24/25 | Loss: 0.00090669
Iteration 25/25 | Loss: 0.00090669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090669
Iteration 2/1000 | Loss: 0.00003532
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001860
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001704
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001560
Iteration 12/1000 | Loss: 0.00001533
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001500
Iteration 18/1000 | Loss: 0.00001498
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001495
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001492
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001490
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001488
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001487
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001472
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001466
Iteration 74/1000 | Loss: 0.00001466
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001465
Iteration 78/1000 | Loss: 0.00001465
Iteration 79/1000 | Loss: 0.00001464
Iteration 80/1000 | Loss: 0.00001464
Iteration 81/1000 | Loss: 0.00001464
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001460
Iteration 92/1000 | Loss: 0.00001460
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00001460
Iteration 95/1000 | Loss: 0.00001459
Iteration 96/1000 | Loss: 0.00001459
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001459
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001456
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001455
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001450
Iteration 122/1000 | Loss: 0.00001450
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001449
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001448
Iteration 146/1000 | Loss: 0.00001448
Iteration 147/1000 | Loss: 0.00001448
Iteration 148/1000 | Loss: 0.00001448
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001448
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001448
Iteration 161/1000 | Loss: 0.00001448
Iteration 162/1000 | Loss: 0.00001448
Iteration 163/1000 | Loss: 0.00001448
Iteration 164/1000 | Loss: 0.00001448
Iteration 165/1000 | Loss: 0.00001448
Iteration 166/1000 | Loss: 0.00001448
Iteration 167/1000 | Loss: 0.00001448
Iteration 168/1000 | Loss: 0.00001448
Iteration 169/1000 | Loss: 0.00001448
Iteration 170/1000 | Loss: 0.00001448
Iteration 171/1000 | Loss: 0.00001448
Iteration 172/1000 | Loss: 0.00001448
Iteration 173/1000 | Loss: 0.00001448
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001448
Iteration 176/1000 | Loss: 0.00001448
Iteration 177/1000 | Loss: 0.00001448
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001448
Iteration 180/1000 | Loss: 0.00001448
Iteration 181/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.4477378499577753e-05, 1.4477378499577753e-05, 1.4477378499577753e-05, 1.4477378499577753e-05, 1.4477378499577753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4477378499577753e-05

Optimization complete. Final v2v error: 3.243760347366333 mm

Highest mean error: 3.64319109916687 mm for frame 62

Lowest mean error: 2.8516993522644043 mm for frame 130

Saving results

Total time: 42.9769344329834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827382
Iteration 2/25 | Loss: 0.00144616
Iteration 3/25 | Loss: 0.00136245
Iteration 4/25 | Loss: 0.00135268
Iteration 5/25 | Loss: 0.00134974
Iteration 6/25 | Loss: 0.00134961
Iteration 7/25 | Loss: 0.00134961
Iteration 8/25 | Loss: 0.00134961
Iteration 9/25 | Loss: 0.00134961
Iteration 10/25 | Loss: 0.00134961
Iteration 11/25 | Loss: 0.00134961
Iteration 12/25 | Loss: 0.00134961
Iteration 13/25 | Loss: 0.00134961
Iteration 14/25 | Loss: 0.00134961
Iteration 15/25 | Loss: 0.00134961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001349613768979907, 0.001349613768979907, 0.001349613768979907, 0.001349613768979907, 0.001349613768979907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001349613768979907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36536646
Iteration 2/25 | Loss: 0.00083886
Iteration 3/25 | Loss: 0.00083885
Iteration 4/25 | Loss: 0.00083885
Iteration 5/25 | Loss: 0.00083885
Iteration 6/25 | Loss: 0.00083885
Iteration 7/25 | Loss: 0.00083885
Iteration 8/25 | Loss: 0.00083885
Iteration 9/25 | Loss: 0.00083885
Iteration 10/25 | Loss: 0.00083885
Iteration 11/25 | Loss: 0.00083885
Iteration 12/25 | Loss: 0.00083885
Iteration 13/25 | Loss: 0.00083885
Iteration 14/25 | Loss: 0.00083885
Iteration 15/25 | Loss: 0.00083885
Iteration 16/25 | Loss: 0.00083885
Iteration 17/25 | Loss: 0.00083885
Iteration 18/25 | Loss: 0.00083885
Iteration 19/25 | Loss: 0.00083885
Iteration 20/25 | Loss: 0.00083885
Iteration 21/25 | Loss: 0.00083885
Iteration 22/25 | Loss: 0.00083885
Iteration 23/25 | Loss: 0.00083885
Iteration 24/25 | Loss: 0.00083885
Iteration 25/25 | Loss: 0.00083885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083885
Iteration 2/1000 | Loss: 0.00004669
Iteration 3/1000 | Loss: 0.00003084
Iteration 4/1000 | Loss: 0.00002740
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002455
Iteration 7/1000 | Loss: 0.00002377
Iteration 8/1000 | Loss: 0.00002329
Iteration 9/1000 | Loss: 0.00002302
Iteration 10/1000 | Loss: 0.00002270
Iteration 11/1000 | Loss: 0.00002245
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002242
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002232
Iteration 16/1000 | Loss: 0.00002220
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002211
Iteration 19/1000 | Loss: 0.00002207
Iteration 20/1000 | Loss: 0.00002205
Iteration 21/1000 | Loss: 0.00002205
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002201
Iteration 25/1000 | Loss: 0.00002201
Iteration 26/1000 | Loss: 0.00002198
Iteration 27/1000 | Loss: 0.00002197
Iteration 28/1000 | Loss: 0.00002196
Iteration 29/1000 | Loss: 0.00002195
Iteration 30/1000 | Loss: 0.00002194
Iteration 31/1000 | Loss: 0.00002188
Iteration 32/1000 | Loss: 0.00002187
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002183
Iteration 35/1000 | Loss: 0.00002179
Iteration 36/1000 | Loss: 0.00002177
Iteration 37/1000 | Loss: 0.00002175
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002174
Iteration 40/1000 | Loss: 0.00002173
Iteration 41/1000 | Loss: 0.00002173
Iteration 42/1000 | Loss: 0.00002172
Iteration 43/1000 | Loss: 0.00002172
Iteration 44/1000 | Loss: 0.00002172
Iteration 45/1000 | Loss: 0.00002171
Iteration 46/1000 | Loss: 0.00002171
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002168
Iteration 51/1000 | Loss: 0.00002168
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002167
Iteration 54/1000 | Loss: 0.00002167
Iteration 55/1000 | Loss: 0.00002167
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002166
Iteration 58/1000 | Loss: 0.00002164
Iteration 59/1000 | Loss: 0.00002164
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00002164
Iteration 62/1000 | Loss: 0.00002164
Iteration 63/1000 | Loss: 0.00002164
Iteration 64/1000 | Loss: 0.00002164
Iteration 65/1000 | Loss: 0.00002163
Iteration 66/1000 | Loss: 0.00002163
Iteration 67/1000 | Loss: 0.00002162
Iteration 68/1000 | Loss: 0.00002162
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002162
Iteration 71/1000 | Loss: 0.00002162
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002161
Iteration 74/1000 | Loss: 0.00002161
Iteration 75/1000 | Loss: 0.00002161
Iteration 76/1000 | Loss: 0.00002161
Iteration 77/1000 | Loss: 0.00002160
Iteration 78/1000 | Loss: 0.00002160
Iteration 79/1000 | Loss: 0.00002160
Iteration 80/1000 | Loss: 0.00002159
Iteration 81/1000 | Loss: 0.00002159
Iteration 82/1000 | Loss: 0.00002159
Iteration 83/1000 | Loss: 0.00002158
Iteration 84/1000 | Loss: 0.00002158
Iteration 85/1000 | Loss: 0.00002158
Iteration 86/1000 | Loss: 0.00002157
Iteration 87/1000 | Loss: 0.00002157
Iteration 88/1000 | Loss: 0.00002157
Iteration 89/1000 | Loss: 0.00002156
Iteration 90/1000 | Loss: 0.00002156
Iteration 91/1000 | Loss: 0.00002156
Iteration 92/1000 | Loss: 0.00002155
Iteration 93/1000 | Loss: 0.00002155
Iteration 94/1000 | Loss: 0.00002155
Iteration 95/1000 | Loss: 0.00002155
Iteration 96/1000 | Loss: 0.00002155
Iteration 97/1000 | Loss: 0.00002155
Iteration 98/1000 | Loss: 0.00002155
Iteration 99/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.1550902602029964e-05, 2.1550902602029964e-05, 2.1550902602029964e-05, 2.1550902602029964e-05, 2.1550902602029964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1550902602029964e-05

Optimization complete. Final v2v error: 3.844679117202759 mm

Highest mean error: 4.779170989990234 mm for frame 173

Lowest mean error: 3.104692220687866 mm for frame 9

Saving results

Total time: 41.96726679801941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925346
Iteration 2/25 | Loss: 0.00206640
Iteration 3/25 | Loss: 0.00165060
Iteration 4/25 | Loss: 0.00161845
Iteration 5/25 | Loss: 0.00160421
Iteration 6/25 | Loss: 0.00158637
Iteration 7/25 | Loss: 0.00150224
Iteration 8/25 | Loss: 0.00146726
Iteration 9/25 | Loss: 0.00140881
Iteration 10/25 | Loss: 0.00138846
Iteration 11/25 | Loss: 0.00137699
Iteration 12/25 | Loss: 0.00137868
Iteration 13/25 | Loss: 0.00137455
Iteration 14/25 | Loss: 0.00137374
Iteration 15/25 | Loss: 0.00138141
Iteration 16/25 | Loss: 0.00139310
Iteration 17/25 | Loss: 0.00139017
Iteration 18/25 | Loss: 0.00138974
Iteration 19/25 | Loss: 0.00138271
Iteration 20/25 | Loss: 0.00138259
Iteration 21/25 | Loss: 0.00138322
Iteration 22/25 | Loss: 0.00138078
Iteration 23/25 | Loss: 0.00138144
Iteration 24/25 | Loss: 0.00137415
Iteration 25/25 | Loss: 0.00138666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47394502
Iteration 2/25 | Loss: 0.00144664
Iteration 3/25 | Loss: 0.00119412
Iteration 4/25 | Loss: 0.00119412
Iteration 5/25 | Loss: 0.00119412
Iteration 6/25 | Loss: 0.00119412
Iteration 7/25 | Loss: 0.00119412
Iteration 8/25 | Loss: 0.00119412
Iteration 9/25 | Loss: 0.00119412
Iteration 10/25 | Loss: 0.00119411
Iteration 11/25 | Loss: 0.00119411
Iteration 12/25 | Loss: 0.00119411
Iteration 13/25 | Loss: 0.00119411
Iteration 14/25 | Loss: 0.00119411
Iteration 15/25 | Loss: 0.00119411
Iteration 16/25 | Loss: 0.00119411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011941147968173027, 0.0011941147968173027, 0.0011941147968173027, 0.0011941147968173027, 0.0011941147968173027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011941147968173027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119411
Iteration 2/1000 | Loss: 0.00024860
Iteration 3/1000 | Loss: 0.00041470
Iteration 4/1000 | Loss: 0.00015482
Iteration 5/1000 | Loss: 0.00007426
Iteration 6/1000 | Loss: 0.00037164
Iteration 7/1000 | Loss: 0.00034104
Iteration 8/1000 | Loss: 0.00017806
Iteration 9/1000 | Loss: 0.00021188
Iteration 10/1000 | Loss: 0.00021275
Iteration 11/1000 | Loss: 0.00035521
Iteration 12/1000 | Loss: 0.00034874
Iteration 13/1000 | Loss: 0.00051756
Iteration 14/1000 | Loss: 0.00016328
Iteration 15/1000 | Loss: 0.00025468
Iteration 16/1000 | Loss: 0.00027276
Iteration 17/1000 | Loss: 0.00043219
Iteration 18/1000 | Loss: 0.00157031
Iteration 19/1000 | Loss: 0.00069444
Iteration 20/1000 | Loss: 0.00052662
Iteration 21/1000 | Loss: 0.00013427
Iteration 22/1000 | Loss: 0.00021527
Iteration 23/1000 | Loss: 0.00018015
Iteration 24/1000 | Loss: 0.00014319
Iteration 25/1000 | Loss: 0.00014129
Iteration 26/1000 | Loss: 0.00032803
Iteration 27/1000 | Loss: 0.00022296
Iteration 28/1000 | Loss: 0.00027398
Iteration 29/1000 | Loss: 0.00014028
Iteration 30/1000 | Loss: 0.00012514
Iteration 31/1000 | Loss: 0.00013764
Iteration 32/1000 | Loss: 0.00018393
Iteration 33/1000 | Loss: 0.00013415
Iteration 34/1000 | Loss: 0.00012625
Iteration 35/1000 | Loss: 0.00015079
Iteration 36/1000 | Loss: 0.00035408
Iteration 37/1000 | Loss: 0.00021753
Iteration 38/1000 | Loss: 0.00031250
Iteration 39/1000 | Loss: 0.00032234
Iteration 40/1000 | Loss: 0.00015646
Iteration 41/1000 | Loss: 0.00021413
Iteration 42/1000 | Loss: 0.00009617
Iteration 43/1000 | Loss: 0.00013318
Iteration 44/1000 | Loss: 0.00020802
Iteration 45/1000 | Loss: 0.00013547
Iteration 46/1000 | Loss: 0.00012413
Iteration 47/1000 | Loss: 0.00025624
Iteration 48/1000 | Loss: 0.00013056
Iteration 49/1000 | Loss: 0.00012652
Iteration 50/1000 | Loss: 0.00025311
Iteration 51/1000 | Loss: 0.00010362
Iteration 52/1000 | Loss: 0.00006972
Iteration 53/1000 | Loss: 0.00011738
Iteration 54/1000 | Loss: 0.00024919
Iteration 55/1000 | Loss: 0.00012086
Iteration 56/1000 | Loss: 0.00014952
Iteration 57/1000 | Loss: 0.00012380
Iteration 58/1000 | Loss: 0.00027845
Iteration 59/1000 | Loss: 0.00011629
Iteration 60/1000 | Loss: 0.00012228
Iteration 61/1000 | Loss: 0.00013472
Iteration 62/1000 | Loss: 0.00027718
Iteration 63/1000 | Loss: 0.00023666
Iteration 64/1000 | Loss: 0.00013878
Iteration 65/1000 | Loss: 0.00014099
Iteration 66/1000 | Loss: 0.00017161
Iteration 67/1000 | Loss: 0.00011283
Iteration 68/1000 | Loss: 0.00013351
Iteration 69/1000 | Loss: 0.00012912
Iteration 70/1000 | Loss: 0.00014072
Iteration 71/1000 | Loss: 0.00012808
Iteration 72/1000 | Loss: 0.00019193
Iteration 73/1000 | Loss: 0.00020598
Iteration 74/1000 | Loss: 0.00021641
Iteration 75/1000 | Loss: 0.00018834
Iteration 76/1000 | Loss: 0.00013839
Iteration 77/1000 | Loss: 0.00012364
Iteration 78/1000 | Loss: 0.00012765
Iteration 79/1000 | Loss: 0.00012199
Iteration 80/1000 | Loss: 0.00021273
Iteration 81/1000 | Loss: 0.00012524
Iteration 82/1000 | Loss: 0.00008869
Iteration 83/1000 | Loss: 0.00015970
Iteration 84/1000 | Loss: 0.00010587
Iteration 85/1000 | Loss: 0.00011201
Iteration 86/1000 | Loss: 0.00011845
Iteration 87/1000 | Loss: 0.00011755
Iteration 88/1000 | Loss: 0.00014369
Iteration 89/1000 | Loss: 0.00012079
Iteration 90/1000 | Loss: 0.00012443
Iteration 91/1000 | Loss: 0.00013884
Iteration 92/1000 | Loss: 0.00010247
Iteration 93/1000 | Loss: 0.00010820
Iteration 94/1000 | Loss: 0.00010645
Iteration 95/1000 | Loss: 0.00014397
Iteration 96/1000 | Loss: 0.00011811
Iteration 97/1000 | Loss: 0.00013427
Iteration 98/1000 | Loss: 0.00013539
Iteration 99/1000 | Loss: 0.00012555
Iteration 100/1000 | Loss: 0.00014300
Iteration 101/1000 | Loss: 0.00013939
Iteration 102/1000 | Loss: 0.00017752
Iteration 103/1000 | Loss: 0.00012730
Iteration 104/1000 | Loss: 0.00014985
Iteration 105/1000 | Loss: 0.00013170
Iteration 106/1000 | Loss: 0.00012096
Iteration 107/1000 | Loss: 0.00013055
Iteration 108/1000 | Loss: 0.00014785
Iteration 109/1000 | Loss: 0.00017324
Iteration 110/1000 | Loss: 0.00014411
Iteration 111/1000 | Loss: 0.00012667
Iteration 112/1000 | Loss: 0.00018810
Iteration 113/1000 | Loss: 0.00036270
Iteration 114/1000 | Loss: 0.00017023
Iteration 115/1000 | Loss: 0.00014875
Iteration 116/1000 | Loss: 0.00012932
Iteration 117/1000 | Loss: 0.00012869
Iteration 118/1000 | Loss: 0.00013526
Iteration 119/1000 | Loss: 0.00014062
Iteration 120/1000 | Loss: 0.00017992
Iteration 121/1000 | Loss: 0.00013572
Iteration 122/1000 | Loss: 0.00012604
Iteration 123/1000 | Loss: 0.00020298
Iteration 124/1000 | Loss: 0.00012374
Iteration 125/1000 | Loss: 0.00008581
Iteration 126/1000 | Loss: 0.00015811
Iteration 127/1000 | Loss: 0.00008556
Iteration 128/1000 | Loss: 0.00007155
Iteration 129/1000 | Loss: 0.00006914
Iteration 130/1000 | Loss: 0.00005836
Iteration 131/1000 | Loss: 0.00005060
Iteration 132/1000 | Loss: 0.00007050
Iteration 133/1000 | Loss: 0.00007244
Iteration 134/1000 | Loss: 0.00022977
Iteration 135/1000 | Loss: 0.00009174
Iteration 136/1000 | Loss: 0.00016098
Iteration 137/1000 | Loss: 0.00005758
Iteration 138/1000 | Loss: 0.00008545
Iteration 139/1000 | Loss: 0.00009369
Iteration 140/1000 | Loss: 0.00008360
Iteration 141/1000 | Loss: 0.00008122
Iteration 142/1000 | Loss: 0.00008974
Iteration 143/1000 | Loss: 0.00008979
Iteration 144/1000 | Loss: 0.00008336
Iteration 145/1000 | Loss: 0.00007735
Iteration 146/1000 | Loss: 0.00008458
Iteration 147/1000 | Loss: 0.00014294
Iteration 148/1000 | Loss: 0.00009267
Iteration 149/1000 | Loss: 0.00009338
Iteration 150/1000 | Loss: 0.00012611
Iteration 151/1000 | Loss: 0.00051830
Iteration 152/1000 | Loss: 0.00009022
Iteration 153/1000 | Loss: 0.00010557
Iteration 154/1000 | Loss: 0.00009237
Iteration 155/1000 | Loss: 0.00009215
Iteration 156/1000 | Loss: 0.00009036
Iteration 157/1000 | Loss: 0.00009165
Iteration 158/1000 | Loss: 0.00009495
Iteration 159/1000 | Loss: 0.00009232
Iteration 160/1000 | Loss: 0.00009365
Iteration 161/1000 | Loss: 0.00008762
Iteration 162/1000 | Loss: 0.00008247
Iteration 163/1000 | Loss: 0.00008981
Iteration 164/1000 | Loss: 0.00009249
Iteration 165/1000 | Loss: 0.00008934
Iteration 166/1000 | Loss: 0.00011895
Iteration 167/1000 | Loss: 0.00008403
Iteration 168/1000 | Loss: 0.00007848
Iteration 169/1000 | Loss: 0.00006026
Iteration 170/1000 | Loss: 0.00016859
Iteration 171/1000 | Loss: 0.00005808
Iteration 172/1000 | Loss: 0.00005138
Iteration 173/1000 | Loss: 0.00005914
Iteration 174/1000 | Loss: 0.00010368
Iteration 175/1000 | Loss: 0.00016829
Iteration 176/1000 | Loss: 0.00096351
Iteration 177/1000 | Loss: 0.00035050
Iteration 178/1000 | Loss: 0.00008190
Iteration 179/1000 | Loss: 0.00005991
Iteration 180/1000 | Loss: 0.00004167
Iteration 181/1000 | Loss: 0.00017621
Iteration 182/1000 | Loss: 0.00004086
Iteration 183/1000 | Loss: 0.00004897
Iteration 184/1000 | Loss: 0.00007104
Iteration 185/1000 | Loss: 0.00004270
Iteration 186/1000 | Loss: 0.00002755
Iteration 187/1000 | Loss: 0.00006909
Iteration 188/1000 | Loss: 0.00005977
Iteration 189/1000 | Loss: 0.00010781
Iteration 190/1000 | Loss: 0.00019755
Iteration 191/1000 | Loss: 0.00005453
Iteration 192/1000 | Loss: 0.00004964
Iteration 193/1000 | Loss: 0.00004192
Iteration 194/1000 | Loss: 0.00004923
Iteration 195/1000 | Loss: 0.00005308
Iteration 196/1000 | Loss: 0.00003929
Iteration 197/1000 | Loss: 0.00003694
Iteration 198/1000 | Loss: 0.00005093
Iteration 199/1000 | Loss: 0.00011390
Iteration 200/1000 | Loss: 0.00017770
Iteration 201/1000 | Loss: 0.00031607
Iteration 202/1000 | Loss: 0.00017646
Iteration 203/1000 | Loss: 0.00005738
Iteration 204/1000 | Loss: 0.00008141
Iteration 205/1000 | Loss: 0.00014999
Iteration 206/1000 | Loss: 0.00005957
Iteration 207/1000 | Loss: 0.00004140
Iteration 208/1000 | Loss: 0.00006128
Iteration 209/1000 | Loss: 0.00003349
Iteration 210/1000 | Loss: 0.00004244
Iteration 211/1000 | Loss: 0.00004688
Iteration 212/1000 | Loss: 0.00003466
Iteration 213/1000 | Loss: 0.00004993
Iteration 214/1000 | Loss: 0.00002704
Iteration 215/1000 | Loss: 0.00004285
Iteration 216/1000 | Loss: 0.00021178
Iteration 217/1000 | Loss: 0.00004788
Iteration 218/1000 | Loss: 0.00002431
Iteration 219/1000 | Loss: 0.00005063
Iteration 220/1000 | Loss: 0.00004697
Iteration 221/1000 | Loss: 0.00002442
Iteration 222/1000 | Loss: 0.00002217
Iteration 223/1000 | Loss: 0.00003358
Iteration 224/1000 | Loss: 0.00007612
Iteration 225/1000 | Loss: 0.00017709
Iteration 226/1000 | Loss: 0.00008301
Iteration 227/1000 | Loss: 0.00006978
Iteration 228/1000 | Loss: 0.00002720
Iteration 229/1000 | Loss: 0.00002433
Iteration 230/1000 | Loss: 0.00002338
Iteration 231/1000 | Loss: 0.00015663
Iteration 232/1000 | Loss: 0.00002294
Iteration 233/1000 | Loss: 0.00002012
Iteration 234/1000 | Loss: 0.00001945
Iteration 235/1000 | Loss: 0.00001899
Iteration 236/1000 | Loss: 0.00001853
Iteration 237/1000 | Loss: 0.00001823
Iteration 238/1000 | Loss: 0.00001789
Iteration 239/1000 | Loss: 0.00001757
Iteration 240/1000 | Loss: 0.00001724
Iteration 241/1000 | Loss: 0.00001703
Iteration 242/1000 | Loss: 0.00001675
Iteration 243/1000 | Loss: 0.00001666
Iteration 244/1000 | Loss: 0.00001646
Iteration 245/1000 | Loss: 0.00001644
Iteration 246/1000 | Loss: 0.00001644
Iteration 247/1000 | Loss: 0.00001631
Iteration 248/1000 | Loss: 0.00001618
Iteration 249/1000 | Loss: 0.00001616
Iteration 250/1000 | Loss: 0.00001615
Iteration 251/1000 | Loss: 0.00001613
Iteration 252/1000 | Loss: 0.00001610
Iteration 253/1000 | Loss: 0.00001609
Iteration 254/1000 | Loss: 0.00001609
Iteration 255/1000 | Loss: 0.00001609
Iteration 256/1000 | Loss: 0.00001609
Iteration 257/1000 | Loss: 0.00001609
Iteration 258/1000 | Loss: 0.00001608
Iteration 259/1000 | Loss: 0.00001608
Iteration 260/1000 | Loss: 0.00001601
Iteration 261/1000 | Loss: 0.00001599
Iteration 262/1000 | Loss: 0.00001599
Iteration 263/1000 | Loss: 0.00001599
Iteration 264/1000 | Loss: 0.00001596
Iteration 265/1000 | Loss: 0.00001596
Iteration 266/1000 | Loss: 0.00001595
Iteration 267/1000 | Loss: 0.00001595
Iteration 268/1000 | Loss: 0.00029161
Iteration 269/1000 | Loss: 0.00002208
Iteration 270/1000 | Loss: 0.00001966
Iteration 271/1000 | Loss: 0.00001831
Iteration 272/1000 | Loss: 0.00001745
Iteration 273/1000 | Loss: 0.00001677
Iteration 274/1000 | Loss: 0.00001598
Iteration 275/1000 | Loss: 0.00001558
Iteration 276/1000 | Loss: 0.00001557
Iteration 277/1000 | Loss: 0.00001544
Iteration 278/1000 | Loss: 0.00001543
Iteration 279/1000 | Loss: 0.00001543
Iteration 280/1000 | Loss: 0.00001541
Iteration 281/1000 | Loss: 0.00001541
Iteration 282/1000 | Loss: 0.00001541
Iteration 283/1000 | Loss: 0.00001540
Iteration 284/1000 | Loss: 0.00001540
Iteration 285/1000 | Loss: 0.00001540
Iteration 286/1000 | Loss: 0.00001540
Iteration 287/1000 | Loss: 0.00001540
Iteration 288/1000 | Loss: 0.00001539
Iteration 289/1000 | Loss: 0.00001539
Iteration 290/1000 | Loss: 0.00001539
Iteration 291/1000 | Loss: 0.00001539
Iteration 292/1000 | Loss: 0.00001538
Iteration 293/1000 | Loss: 0.00001538
Iteration 294/1000 | Loss: 0.00001537
Iteration 295/1000 | Loss: 0.00001536
Iteration 296/1000 | Loss: 0.00001536
Iteration 297/1000 | Loss: 0.00001536
Iteration 298/1000 | Loss: 0.00001536
Iteration 299/1000 | Loss: 0.00001535
Iteration 300/1000 | Loss: 0.00001535
Iteration 301/1000 | Loss: 0.00001535
Iteration 302/1000 | Loss: 0.00001535
Iteration 303/1000 | Loss: 0.00001535
Iteration 304/1000 | Loss: 0.00001534
Iteration 305/1000 | Loss: 0.00001534
Iteration 306/1000 | Loss: 0.00001534
Iteration 307/1000 | Loss: 0.00001534
Iteration 308/1000 | Loss: 0.00001534
Iteration 309/1000 | Loss: 0.00001534
Iteration 310/1000 | Loss: 0.00001534
Iteration 311/1000 | Loss: 0.00001533
Iteration 312/1000 | Loss: 0.00001533
Iteration 313/1000 | Loss: 0.00001533
Iteration 314/1000 | Loss: 0.00001533
Iteration 315/1000 | Loss: 0.00001533
Iteration 316/1000 | Loss: 0.00001533
Iteration 317/1000 | Loss: 0.00001533
Iteration 318/1000 | Loss: 0.00001533
Iteration 319/1000 | Loss: 0.00001533
Iteration 320/1000 | Loss: 0.00001533
Iteration 321/1000 | Loss: 0.00001533
Iteration 322/1000 | Loss: 0.00001533
Iteration 323/1000 | Loss: 0.00001533
Iteration 324/1000 | Loss: 0.00001532
Iteration 325/1000 | Loss: 0.00001532
Iteration 326/1000 | Loss: 0.00001532
Iteration 327/1000 | Loss: 0.00001532
Iteration 328/1000 | Loss: 0.00001532
Iteration 329/1000 | Loss: 0.00001532
Iteration 330/1000 | Loss: 0.00001532
Iteration 331/1000 | Loss: 0.00001532
Iteration 332/1000 | Loss: 0.00001532
Iteration 333/1000 | Loss: 0.00001532
Iteration 334/1000 | Loss: 0.00001531
Iteration 335/1000 | Loss: 0.00001531
Iteration 336/1000 | Loss: 0.00001531
Iteration 337/1000 | Loss: 0.00001531
Iteration 338/1000 | Loss: 0.00001531
Iteration 339/1000 | Loss: 0.00001531
Iteration 340/1000 | Loss: 0.00001531
Iteration 341/1000 | Loss: 0.00001531
Iteration 342/1000 | Loss: 0.00001531
Iteration 343/1000 | Loss: 0.00001531
Iteration 344/1000 | Loss: 0.00001530
Iteration 345/1000 | Loss: 0.00001530
Iteration 346/1000 | Loss: 0.00001530
Iteration 347/1000 | Loss: 0.00001530
Iteration 348/1000 | Loss: 0.00001530
Iteration 349/1000 | Loss: 0.00001530
Iteration 350/1000 | Loss: 0.00001530
Iteration 351/1000 | Loss: 0.00001530
Iteration 352/1000 | Loss: 0.00001529
Iteration 353/1000 | Loss: 0.00001529
Iteration 354/1000 | Loss: 0.00001529
Iteration 355/1000 | Loss: 0.00001529
Iteration 356/1000 | Loss: 0.00001529
Iteration 357/1000 | Loss: 0.00001529
Iteration 358/1000 | Loss: 0.00001529
Iteration 359/1000 | Loss: 0.00001529
Iteration 360/1000 | Loss: 0.00001529
Iteration 361/1000 | Loss: 0.00001529
Iteration 362/1000 | Loss: 0.00001529
Iteration 363/1000 | Loss: 0.00001529
Iteration 364/1000 | Loss: 0.00001529
Iteration 365/1000 | Loss: 0.00001529
Iteration 366/1000 | Loss: 0.00001529
Iteration 367/1000 | Loss: 0.00001529
Iteration 368/1000 | Loss: 0.00001529
Iteration 369/1000 | Loss: 0.00001529
Iteration 370/1000 | Loss: 0.00001529
Iteration 371/1000 | Loss: 0.00001529
Iteration 372/1000 | Loss: 0.00001529
Iteration 373/1000 | Loss: 0.00001529
Iteration 374/1000 | Loss: 0.00001529
Iteration 375/1000 | Loss: 0.00001529
Iteration 376/1000 | Loss: 0.00001529
Iteration 377/1000 | Loss: 0.00001529
Iteration 378/1000 | Loss: 0.00001529
Iteration 379/1000 | Loss: 0.00001529
Iteration 380/1000 | Loss: 0.00001529
Iteration 381/1000 | Loss: 0.00001529
Iteration 382/1000 | Loss: 0.00001529
Iteration 383/1000 | Loss: 0.00001529
Iteration 384/1000 | Loss: 0.00001529
Iteration 385/1000 | Loss: 0.00001529
Iteration 386/1000 | Loss: 0.00001529
Iteration 387/1000 | Loss: 0.00001529
Iteration 388/1000 | Loss: 0.00001529
Iteration 389/1000 | Loss: 0.00001529
Iteration 390/1000 | Loss: 0.00001529
Iteration 391/1000 | Loss: 0.00001529
Iteration 392/1000 | Loss: 0.00001529
Iteration 393/1000 | Loss: 0.00001529
Iteration 394/1000 | Loss: 0.00001529
Iteration 395/1000 | Loss: 0.00001529
Iteration 396/1000 | Loss: 0.00001529
Iteration 397/1000 | Loss: 0.00001529
Iteration 398/1000 | Loss: 0.00001529
Iteration 399/1000 | Loss: 0.00001529
Iteration 400/1000 | Loss: 0.00001529
Iteration 401/1000 | Loss: 0.00001529
Iteration 402/1000 | Loss: 0.00001529
Iteration 403/1000 | Loss: 0.00001529
Iteration 404/1000 | Loss: 0.00001529
Iteration 405/1000 | Loss: 0.00001529
Iteration 406/1000 | Loss: 0.00001529
Iteration 407/1000 | Loss: 0.00001529
Iteration 408/1000 | Loss: 0.00001529
Iteration 409/1000 | Loss: 0.00001529
Iteration 410/1000 | Loss: 0.00001529
Iteration 411/1000 | Loss: 0.00001529
Iteration 412/1000 | Loss: 0.00001529
Iteration 413/1000 | Loss: 0.00001529
Iteration 414/1000 | Loss: 0.00001529
Iteration 415/1000 | Loss: 0.00001529
Iteration 416/1000 | Loss: 0.00001529
Iteration 417/1000 | Loss: 0.00001529
Iteration 418/1000 | Loss: 0.00001529
Iteration 419/1000 | Loss: 0.00001529
Iteration 420/1000 | Loss: 0.00001529
Iteration 421/1000 | Loss: 0.00001529
Iteration 422/1000 | Loss: 0.00001529
Iteration 423/1000 | Loss: 0.00001529
Iteration 424/1000 | Loss: 0.00001529
Iteration 425/1000 | Loss: 0.00001529
Iteration 426/1000 | Loss: 0.00001529
Iteration 427/1000 | Loss: 0.00001529
Iteration 428/1000 | Loss: 0.00001529
Iteration 429/1000 | Loss: 0.00001529
Iteration 430/1000 | Loss: 0.00001529
Iteration 431/1000 | Loss: 0.00001529
Iteration 432/1000 | Loss: 0.00001529
Iteration 433/1000 | Loss: 0.00001529
Iteration 434/1000 | Loss: 0.00001529
Iteration 435/1000 | Loss: 0.00001529
Iteration 436/1000 | Loss: 0.00001529
Iteration 437/1000 | Loss: 0.00001529
Iteration 438/1000 | Loss: 0.00001529
Iteration 439/1000 | Loss: 0.00001529
Iteration 440/1000 | Loss: 0.00001529
Iteration 441/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 441. Stopping optimization.
Last 5 losses: [1.5289442671928555e-05, 1.5289442671928555e-05, 1.5289442671928555e-05, 1.5289442671928555e-05, 1.5289442671928555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5289442671928555e-05

Optimization complete. Final v2v error: 3.331256151199341 mm

Highest mean error: 4.892288684844971 mm for frame 16

Lowest mean error: 2.989360809326172 mm for frame 58

Saving results

Total time: 421.81992769241333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042769
Iteration 2/25 | Loss: 0.01042769
Iteration 3/25 | Loss: 0.00247837
Iteration 4/25 | Loss: 0.00181350
Iteration 5/25 | Loss: 0.00171706
Iteration 6/25 | Loss: 0.00170374
Iteration 7/25 | Loss: 0.00166003
Iteration 8/25 | Loss: 0.00163885
Iteration 9/25 | Loss: 0.00160089
Iteration 10/25 | Loss: 0.00158009
Iteration 11/25 | Loss: 0.00155530
Iteration 12/25 | Loss: 0.00152812
Iteration 13/25 | Loss: 0.00151336
Iteration 14/25 | Loss: 0.00150540
Iteration 15/25 | Loss: 0.00150040
Iteration 16/25 | Loss: 0.00148770
Iteration 17/25 | Loss: 0.00147909
Iteration 18/25 | Loss: 0.00147630
Iteration 19/25 | Loss: 0.00147638
Iteration 20/25 | Loss: 0.00147103
Iteration 21/25 | Loss: 0.00146971
Iteration 22/25 | Loss: 0.00146927
Iteration 23/25 | Loss: 0.00146913
Iteration 24/25 | Loss: 0.00146909
Iteration 25/25 | Loss: 0.00146909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30182028
Iteration 2/25 | Loss: 0.00327887
Iteration 3/25 | Loss: 0.00191837
Iteration 4/25 | Loss: 0.00191837
Iteration 5/25 | Loss: 0.00191837
Iteration 6/25 | Loss: 0.00191837
Iteration 7/25 | Loss: 0.00191837
Iteration 8/25 | Loss: 0.00191837
Iteration 9/25 | Loss: 0.00191837
Iteration 10/25 | Loss: 0.00191837
Iteration 11/25 | Loss: 0.00191837
Iteration 12/25 | Loss: 0.00191837
Iteration 13/25 | Loss: 0.00191837
Iteration 14/25 | Loss: 0.00191837
Iteration 15/25 | Loss: 0.00191837
Iteration 16/25 | Loss: 0.00191837
Iteration 17/25 | Loss: 0.00191837
Iteration 18/25 | Loss: 0.00191837
Iteration 19/25 | Loss: 0.00191837
Iteration 20/25 | Loss: 0.00191837
Iteration 21/25 | Loss: 0.00191837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019183703698217869, 0.0019183703698217869, 0.0019183703698217869, 0.0019183703698217869, 0.0019183703698217869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019183703698217869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191837
Iteration 2/1000 | Loss: 0.00043469
Iteration 3/1000 | Loss: 0.00066318
Iteration 4/1000 | Loss: 0.00113307
Iteration 5/1000 | Loss: 0.00274620
Iteration 6/1000 | Loss: 0.00100080
Iteration 7/1000 | Loss: 0.00302358
Iteration 8/1000 | Loss: 0.00016501
Iteration 9/1000 | Loss: 0.00018329
Iteration 10/1000 | Loss: 0.00041811
Iteration 11/1000 | Loss: 0.00011147
Iteration 12/1000 | Loss: 0.00010376
Iteration 13/1000 | Loss: 0.00009376
Iteration 14/1000 | Loss: 0.00014748
Iteration 15/1000 | Loss: 0.00352847
Iteration 16/1000 | Loss: 0.00157472
Iteration 17/1000 | Loss: 0.00054880
Iteration 18/1000 | Loss: 0.00379425
Iteration 19/1000 | Loss: 0.00517918
Iteration 20/1000 | Loss: 0.00557909
Iteration 21/1000 | Loss: 0.00298963
Iteration 22/1000 | Loss: 0.00203316
Iteration 23/1000 | Loss: 0.00156977
Iteration 24/1000 | Loss: 0.00182124
Iteration 25/1000 | Loss: 0.00281396
Iteration 26/1000 | Loss: 0.00218856
Iteration 27/1000 | Loss: 0.00125761
Iteration 28/1000 | Loss: 0.00107406
Iteration 29/1000 | Loss: 0.00110575
Iteration 30/1000 | Loss: 0.00208974
Iteration 31/1000 | Loss: 0.00109124
Iteration 32/1000 | Loss: 0.00111513
Iteration 33/1000 | Loss: 0.00108994
Iteration 34/1000 | Loss: 0.00133837
Iteration 35/1000 | Loss: 0.00117744
Iteration 36/1000 | Loss: 0.00294153
Iteration 37/1000 | Loss: 0.00191783
Iteration 38/1000 | Loss: 0.00110247
Iteration 39/1000 | Loss: 0.00229224
Iteration 40/1000 | Loss: 0.00080958
Iteration 41/1000 | Loss: 0.00152044
Iteration 42/1000 | Loss: 0.00187539
Iteration 43/1000 | Loss: 0.00132695
Iteration 44/1000 | Loss: 0.00082434
Iteration 45/1000 | Loss: 0.00120623
Iteration 46/1000 | Loss: 0.00148469
Iteration 47/1000 | Loss: 0.00084708
Iteration 48/1000 | Loss: 0.00078938
Iteration 49/1000 | Loss: 0.00080229
Iteration 50/1000 | Loss: 0.00094687
Iteration 51/1000 | Loss: 0.00111161
Iteration 52/1000 | Loss: 0.00080405
Iteration 53/1000 | Loss: 0.00149583
Iteration 54/1000 | Loss: 0.00011631
Iteration 55/1000 | Loss: 0.00010049
Iteration 56/1000 | Loss: 0.00011777
Iteration 57/1000 | Loss: 0.00060587
Iteration 58/1000 | Loss: 0.00035810
Iteration 59/1000 | Loss: 0.00012286
Iteration 60/1000 | Loss: 0.00011907
Iteration 61/1000 | Loss: 0.00016242
Iteration 62/1000 | Loss: 0.00046914
Iteration 63/1000 | Loss: 0.00051036
Iteration 64/1000 | Loss: 0.00041314
Iteration 65/1000 | Loss: 0.00031817
Iteration 66/1000 | Loss: 0.00032998
Iteration 67/1000 | Loss: 0.00014659
Iteration 68/1000 | Loss: 0.00022870
Iteration 69/1000 | Loss: 0.00123796
Iteration 70/1000 | Loss: 0.00114568
Iteration 71/1000 | Loss: 0.00138603
Iteration 72/1000 | Loss: 0.00078483
Iteration 73/1000 | Loss: 0.00107789
Iteration 74/1000 | Loss: 0.00055678
Iteration 75/1000 | Loss: 0.00073073
Iteration 76/1000 | Loss: 0.00051940
Iteration 77/1000 | Loss: 0.00154295
Iteration 78/1000 | Loss: 0.00107218
Iteration 79/1000 | Loss: 0.00124348
Iteration 80/1000 | Loss: 0.00045712
Iteration 81/1000 | Loss: 0.00071054
Iteration 82/1000 | Loss: 0.00101538
Iteration 83/1000 | Loss: 0.00134217
Iteration 84/1000 | Loss: 0.00039210
Iteration 85/1000 | Loss: 0.00157804
Iteration 86/1000 | Loss: 0.00079790
Iteration 87/1000 | Loss: 0.00063361
Iteration 88/1000 | Loss: 0.00056915
Iteration 89/1000 | Loss: 0.00079781
Iteration 90/1000 | Loss: 0.00050195
Iteration 91/1000 | Loss: 0.00065134
Iteration 92/1000 | Loss: 0.00071493
Iteration 93/1000 | Loss: 0.00103380
Iteration 94/1000 | Loss: 0.00185460
Iteration 95/1000 | Loss: 0.00104903
Iteration 96/1000 | Loss: 0.00085161
Iteration 97/1000 | Loss: 0.00080936
Iteration 98/1000 | Loss: 0.00098816
Iteration 99/1000 | Loss: 0.00405542
Iteration 100/1000 | Loss: 0.00048086
Iteration 101/1000 | Loss: 0.00027589
Iteration 102/1000 | Loss: 0.00020214
Iteration 103/1000 | Loss: 0.00025231
Iteration 104/1000 | Loss: 0.00024340
Iteration 105/1000 | Loss: 0.00025914
Iteration 106/1000 | Loss: 0.00164775
Iteration 107/1000 | Loss: 0.00015541
Iteration 108/1000 | Loss: 0.00023638
Iteration 109/1000 | Loss: 0.00066459
Iteration 110/1000 | Loss: 0.00015420
Iteration 111/1000 | Loss: 0.00017168
Iteration 112/1000 | Loss: 0.00014886
Iteration 113/1000 | Loss: 0.00055181
Iteration 114/1000 | Loss: 0.00006913
Iteration 115/1000 | Loss: 0.00005429
Iteration 116/1000 | Loss: 0.00005108
Iteration 117/1000 | Loss: 0.00004280
Iteration 118/1000 | Loss: 0.00003815
Iteration 119/1000 | Loss: 0.00004468
Iteration 120/1000 | Loss: 0.00003523
Iteration 121/1000 | Loss: 0.00004236
Iteration 122/1000 | Loss: 0.00004273
Iteration 123/1000 | Loss: 0.00004693
Iteration 124/1000 | Loss: 0.00004215
Iteration 125/1000 | Loss: 0.00004428
Iteration 126/1000 | Loss: 0.00026214
Iteration 127/1000 | Loss: 0.00014302
Iteration 128/1000 | Loss: 0.00021397
Iteration 129/1000 | Loss: 0.00022348
Iteration 130/1000 | Loss: 0.00021645
Iteration 131/1000 | Loss: 0.00038661
Iteration 132/1000 | Loss: 0.00022579
Iteration 133/1000 | Loss: 0.00052445
Iteration 134/1000 | Loss: 0.00033240
Iteration 135/1000 | Loss: 0.00039176
Iteration 136/1000 | Loss: 0.00004480
Iteration 137/1000 | Loss: 0.00004627
Iteration 138/1000 | Loss: 0.00003939
Iteration 139/1000 | Loss: 0.00026545
Iteration 140/1000 | Loss: 0.00005520
Iteration 141/1000 | Loss: 0.00004434
Iteration 142/1000 | Loss: 0.00003604
Iteration 143/1000 | Loss: 0.00003100
Iteration 144/1000 | Loss: 0.00003088
Iteration 145/1000 | Loss: 0.00004200
Iteration 146/1000 | Loss: 0.00004236
Iteration 147/1000 | Loss: 0.00003802
Iteration 148/1000 | Loss: 0.00004749
Iteration 149/1000 | Loss: 0.00003952
Iteration 150/1000 | Loss: 0.00004553
Iteration 151/1000 | Loss: 0.00003226
Iteration 152/1000 | Loss: 0.00025947
Iteration 153/1000 | Loss: 0.00042651
Iteration 154/1000 | Loss: 0.00003061
Iteration 155/1000 | Loss: 0.00070764
Iteration 156/1000 | Loss: 0.00048602
Iteration 157/1000 | Loss: 0.00134665
Iteration 158/1000 | Loss: 0.00022538
Iteration 159/1000 | Loss: 0.00031813
Iteration 160/1000 | Loss: 0.00089513
Iteration 161/1000 | Loss: 0.00020711
Iteration 162/1000 | Loss: 0.00006357
Iteration 163/1000 | Loss: 0.00004636
Iteration 164/1000 | Loss: 0.00003882
Iteration 165/1000 | Loss: 0.00003607
Iteration 166/1000 | Loss: 0.00031932
Iteration 167/1000 | Loss: 0.00003806
Iteration 168/1000 | Loss: 0.00033408
Iteration 169/1000 | Loss: 0.00019523
Iteration 170/1000 | Loss: 0.00002955
Iteration 171/1000 | Loss: 0.00002733
Iteration 172/1000 | Loss: 0.00030163
Iteration 173/1000 | Loss: 0.00016622
Iteration 174/1000 | Loss: 0.00002674
Iteration 175/1000 | Loss: 0.00002973
Iteration 176/1000 | Loss: 0.00032306
Iteration 177/1000 | Loss: 0.00014131
Iteration 178/1000 | Loss: 0.00003054
Iteration 179/1000 | Loss: 0.00002582
Iteration 180/1000 | Loss: 0.00035697
Iteration 181/1000 | Loss: 0.00012449
Iteration 182/1000 | Loss: 0.00002587
Iteration 183/1000 | Loss: 0.00002649
Iteration 184/1000 | Loss: 0.00096389
Iteration 185/1000 | Loss: 0.00049208
Iteration 186/1000 | Loss: 0.00006101
Iteration 187/1000 | Loss: 0.00060553
Iteration 188/1000 | Loss: 0.00025409
Iteration 189/1000 | Loss: 0.00017274
Iteration 190/1000 | Loss: 0.00023866
Iteration 191/1000 | Loss: 0.00015927
Iteration 192/1000 | Loss: 0.00052115
Iteration 193/1000 | Loss: 0.00015755
Iteration 194/1000 | Loss: 0.00027226
Iteration 195/1000 | Loss: 0.00065663
Iteration 196/1000 | Loss: 0.00051193
Iteration 197/1000 | Loss: 0.00006346
Iteration 198/1000 | Loss: 0.00010447
Iteration 199/1000 | Loss: 0.00002643
Iteration 200/1000 | Loss: 0.00004658
Iteration 201/1000 | Loss: 0.00002289
Iteration 202/1000 | Loss: 0.00003938
Iteration 203/1000 | Loss: 0.00002221
Iteration 204/1000 | Loss: 0.00002191
Iteration 205/1000 | Loss: 0.00051437
Iteration 206/1000 | Loss: 0.00002415
Iteration 207/1000 | Loss: 0.00002255
Iteration 208/1000 | Loss: 0.00002041
Iteration 209/1000 | Loss: 0.00002213
Iteration 210/1000 | Loss: 0.00001981
Iteration 211/1000 | Loss: 0.00001955
Iteration 212/1000 | Loss: 0.00001938
Iteration 213/1000 | Loss: 0.00001928
Iteration 214/1000 | Loss: 0.00001928
Iteration 215/1000 | Loss: 0.00001927
Iteration 216/1000 | Loss: 0.00001926
Iteration 217/1000 | Loss: 0.00001924
Iteration 218/1000 | Loss: 0.00001919
Iteration 219/1000 | Loss: 0.00001915
Iteration 220/1000 | Loss: 0.00001915
Iteration 221/1000 | Loss: 0.00001914
Iteration 222/1000 | Loss: 0.00001912
Iteration 223/1000 | Loss: 0.00001912
Iteration 224/1000 | Loss: 0.00001970
Iteration 225/1000 | Loss: 0.00001989
Iteration 226/1000 | Loss: 0.00002585
Iteration 227/1000 | Loss: 0.00001907
Iteration 228/1000 | Loss: 0.00001907
Iteration 229/1000 | Loss: 0.00001907
Iteration 230/1000 | Loss: 0.00001907
Iteration 231/1000 | Loss: 0.00001907
Iteration 232/1000 | Loss: 0.00001907
Iteration 233/1000 | Loss: 0.00001907
Iteration 234/1000 | Loss: 0.00001907
Iteration 235/1000 | Loss: 0.00001907
Iteration 236/1000 | Loss: 0.00001971
Iteration 237/1000 | Loss: 0.00001904
Iteration 238/1000 | Loss: 0.00001904
Iteration 239/1000 | Loss: 0.00001903
Iteration 240/1000 | Loss: 0.00001902
Iteration 241/1000 | Loss: 0.00001902
Iteration 242/1000 | Loss: 0.00001901
Iteration 243/1000 | Loss: 0.00001901
Iteration 244/1000 | Loss: 0.00001901
Iteration 245/1000 | Loss: 0.00001900
Iteration 246/1000 | Loss: 0.00001900
Iteration 247/1000 | Loss: 0.00001900
Iteration 248/1000 | Loss: 0.00001900
Iteration 249/1000 | Loss: 0.00001900
Iteration 250/1000 | Loss: 0.00001900
Iteration 251/1000 | Loss: 0.00001900
Iteration 252/1000 | Loss: 0.00001900
Iteration 253/1000 | Loss: 0.00001900
Iteration 254/1000 | Loss: 0.00001900
Iteration 255/1000 | Loss: 0.00001900
Iteration 256/1000 | Loss: 0.00001900
Iteration 257/1000 | Loss: 0.00001900
Iteration 258/1000 | Loss: 0.00001900
Iteration 259/1000 | Loss: 0.00001900
Iteration 260/1000 | Loss: 0.00001900
Iteration 261/1000 | Loss: 0.00001900
Iteration 262/1000 | Loss: 0.00001900
Iteration 263/1000 | Loss: 0.00001900
Iteration 264/1000 | Loss: 0.00001900
Iteration 265/1000 | Loss: 0.00001900
Iteration 266/1000 | Loss: 0.00001900
Iteration 267/1000 | Loss: 0.00001900
Iteration 268/1000 | Loss: 0.00001900
Iteration 269/1000 | Loss: 0.00001900
Iteration 270/1000 | Loss: 0.00001900
Iteration 271/1000 | Loss: 0.00001900
Iteration 272/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.899762537505012e-05, 1.899762537505012e-05, 1.899762537505012e-05, 1.899762537505012e-05, 1.899762537505012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.899762537505012e-05

Optimization complete. Final v2v error: 3.682440996170044 mm

Highest mean error: 6.09730339050293 mm for frame 154

Lowest mean error: 3.2037694454193115 mm for frame 229

Saving results

Total time: 390.09767413139343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816286
Iteration 2/25 | Loss: 0.00228790
Iteration 3/25 | Loss: 0.00172005
Iteration 4/25 | Loss: 0.00160882
Iteration 5/25 | Loss: 0.00161774
Iteration 6/25 | Loss: 0.00154418
Iteration 7/25 | Loss: 0.00152684
Iteration 8/25 | Loss: 0.00152189
Iteration 9/25 | Loss: 0.00152036
Iteration 10/25 | Loss: 0.00151940
Iteration 11/25 | Loss: 0.00151830
Iteration 12/25 | Loss: 0.00151867
Iteration 13/25 | Loss: 0.00151638
Iteration 14/25 | Loss: 0.00151611
Iteration 15/25 | Loss: 0.00151598
Iteration 16/25 | Loss: 0.00151614
Iteration 17/25 | Loss: 0.00151624
Iteration 18/25 | Loss: 0.00151597
Iteration 19/25 | Loss: 0.00151584
Iteration 20/25 | Loss: 0.00151570
Iteration 21/25 | Loss: 0.00151952
Iteration 22/25 | Loss: 0.00151515
Iteration 23/25 | Loss: 0.00151371
Iteration 24/25 | Loss: 0.00151360
Iteration 25/25 | Loss: 0.00151288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24172831
Iteration 2/25 | Loss: 0.00102532
Iteration 3/25 | Loss: 0.00102528
Iteration 4/25 | Loss: 0.00102528
Iteration 5/25 | Loss: 0.00102528
Iteration 6/25 | Loss: 0.00102528
Iteration 7/25 | Loss: 0.00102528
Iteration 8/25 | Loss: 0.00102528
Iteration 9/25 | Loss: 0.00102528
Iteration 10/25 | Loss: 0.00102528
Iteration 11/25 | Loss: 0.00102528
Iteration 12/25 | Loss: 0.00102528
Iteration 13/25 | Loss: 0.00102528
Iteration 14/25 | Loss: 0.00102528
Iteration 15/25 | Loss: 0.00102528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010252832435071468, 0.0010252832435071468, 0.0010252832435071468, 0.0010252832435071468, 0.0010252832435071468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010252832435071468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102528
Iteration 2/1000 | Loss: 0.00005754
Iteration 3/1000 | Loss: 0.00004635
Iteration 4/1000 | Loss: 0.00003913
Iteration 5/1000 | Loss: 0.00003958
Iteration 6/1000 | Loss: 0.00003389
Iteration 7/1000 | Loss: 0.00004254
Iteration 8/1000 | Loss: 0.00004508
Iteration 9/1000 | Loss: 0.00003655
Iteration 10/1000 | Loss: 0.00003931
Iteration 11/1000 | Loss: 0.00003623
Iteration 12/1000 | Loss: 0.00005152
Iteration 13/1000 | Loss: 0.00003389
Iteration 14/1000 | Loss: 0.00003232
Iteration 15/1000 | Loss: 0.00003178
Iteration 16/1000 | Loss: 0.00003143
Iteration 17/1000 | Loss: 0.00003098
Iteration 18/1000 | Loss: 0.00003074
Iteration 19/1000 | Loss: 0.00003062
Iteration 20/1000 | Loss: 0.00003061
Iteration 21/1000 | Loss: 0.00003061
Iteration 22/1000 | Loss: 0.00003061
Iteration 23/1000 | Loss: 0.00003061
Iteration 24/1000 | Loss: 0.00003060
Iteration 25/1000 | Loss: 0.00003060
Iteration 26/1000 | Loss: 0.00003059
Iteration 27/1000 | Loss: 0.00003059
Iteration 28/1000 | Loss: 0.00003058
Iteration 29/1000 | Loss: 0.00003058
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003058
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003057
Iteration 34/1000 | Loss: 0.00003057
Iteration 35/1000 | Loss: 0.00003057
Iteration 36/1000 | Loss: 0.00003056
Iteration 37/1000 | Loss: 0.00003053
Iteration 38/1000 | Loss: 0.00003053
Iteration 39/1000 | Loss: 0.00003053
Iteration 40/1000 | Loss: 0.00003053
Iteration 41/1000 | Loss: 0.00003052
Iteration 42/1000 | Loss: 0.00003050
Iteration 43/1000 | Loss: 0.00003049
Iteration 44/1000 | Loss: 0.00003046
Iteration 45/1000 | Loss: 0.00003045
Iteration 46/1000 | Loss: 0.00003043
Iteration 47/1000 | Loss: 0.00003043
Iteration 48/1000 | Loss: 0.00003042
Iteration 49/1000 | Loss: 0.00003042
Iteration 50/1000 | Loss: 0.00003042
Iteration 51/1000 | Loss: 0.00003042
Iteration 52/1000 | Loss: 0.00003041
Iteration 53/1000 | Loss: 0.00003041
Iteration 54/1000 | Loss: 0.00003041
Iteration 55/1000 | Loss: 0.00003041
Iteration 56/1000 | Loss: 0.00003040
Iteration 57/1000 | Loss: 0.00003040
Iteration 58/1000 | Loss: 0.00003040
Iteration 59/1000 | Loss: 0.00003037
Iteration 60/1000 | Loss: 0.00003037
Iteration 61/1000 | Loss: 0.00003037
Iteration 62/1000 | Loss: 0.00003036
Iteration 63/1000 | Loss: 0.00003036
Iteration 64/1000 | Loss: 0.00003035
Iteration 65/1000 | Loss: 0.00003034
Iteration 66/1000 | Loss: 0.00003034
Iteration 67/1000 | Loss: 0.00003034
Iteration 68/1000 | Loss: 0.00003033
Iteration 69/1000 | Loss: 0.00003033
Iteration 70/1000 | Loss: 0.00003032
Iteration 71/1000 | Loss: 0.00003032
Iteration 72/1000 | Loss: 0.00003032
Iteration 73/1000 | Loss: 0.00003032
Iteration 74/1000 | Loss: 0.00003031
Iteration 75/1000 | Loss: 0.00003031
Iteration 76/1000 | Loss: 0.00003031
Iteration 77/1000 | Loss: 0.00003031
Iteration 78/1000 | Loss: 0.00003031
Iteration 79/1000 | Loss: 0.00003031
Iteration 80/1000 | Loss: 0.00003031
Iteration 81/1000 | Loss: 0.00003030
Iteration 82/1000 | Loss: 0.00003030
Iteration 83/1000 | Loss: 0.00003030
Iteration 84/1000 | Loss: 0.00003030
Iteration 85/1000 | Loss: 0.00003030
Iteration 86/1000 | Loss: 0.00003030
Iteration 87/1000 | Loss: 0.00003030
Iteration 88/1000 | Loss: 0.00003030
Iteration 89/1000 | Loss: 0.00003030
Iteration 90/1000 | Loss: 0.00003030
Iteration 91/1000 | Loss: 0.00003030
Iteration 92/1000 | Loss: 0.00003030
Iteration 93/1000 | Loss: 0.00003030
Iteration 94/1000 | Loss: 0.00003030
Iteration 95/1000 | Loss: 0.00003030
Iteration 96/1000 | Loss: 0.00003029
Iteration 97/1000 | Loss: 0.00003029
Iteration 98/1000 | Loss: 0.00003029
Iteration 99/1000 | Loss: 0.00003029
Iteration 100/1000 | Loss: 0.00003029
Iteration 101/1000 | Loss: 0.00003029
Iteration 102/1000 | Loss: 0.00003029
Iteration 103/1000 | Loss: 0.00003029
Iteration 104/1000 | Loss: 0.00003029
Iteration 105/1000 | Loss: 0.00003029
Iteration 106/1000 | Loss: 0.00003029
Iteration 107/1000 | Loss: 0.00003029
Iteration 108/1000 | Loss: 0.00003029
Iteration 109/1000 | Loss: 0.00003029
Iteration 110/1000 | Loss: 0.00003029
Iteration 111/1000 | Loss: 0.00003029
Iteration 112/1000 | Loss: 0.00003029
Iteration 113/1000 | Loss: 0.00003029
Iteration 114/1000 | Loss: 0.00003029
Iteration 115/1000 | Loss: 0.00003029
Iteration 116/1000 | Loss: 0.00003029
Iteration 117/1000 | Loss: 0.00003029
Iteration 118/1000 | Loss: 0.00003029
Iteration 119/1000 | Loss: 0.00003029
Iteration 120/1000 | Loss: 0.00003029
Iteration 121/1000 | Loss: 0.00003029
Iteration 122/1000 | Loss: 0.00003029
Iteration 123/1000 | Loss: 0.00003029
Iteration 124/1000 | Loss: 0.00003029
Iteration 125/1000 | Loss: 0.00003029
Iteration 126/1000 | Loss: 0.00003029
Iteration 127/1000 | Loss: 0.00003029
Iteration 128/1000 | Loss: 0.00003029
Iteration 129/1000 | Loss: 0.00003029
Iteration 130/1000 | Loss: 0.00003029
Iteration 131/1000 | Loss: 0.00003029
Iteration 132/1000 | Loss: 0.00003029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.0294619136839174e-05, 3.0294619136839174e-05, 3.0294619136839174e-05, 3.0294619136839174e-05, 3.0294619136839174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0294619136839174e-05

Optimization complete. Final v2v error: 4.481242656707764 mm

Highest mean error: 5.929280757904053 mm for frame 155

Lowest mean error: 4.323218822479248 mm for frame 30

Saving results

Total time: 89.90521001815796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457667
Iteration 2/25 | Loss: 0.00152897
Iteration 3/25 | Loss: 0.00128906
Iteration 4/25 | Loss: 0.00125953
Iteration 5/25 | Loss: 0.00125436
Iteration 6/25 | Loss: 0.00125309
Iteration 7/25 | Loss: 0.00125309
Iteration 8/25 | Loss: 0.00125309
Iteration 9/25 | Loss: 0.00125309
Iteration 10/25 | Loss: 0.00125309
Iteration 11/25 | Loss: 0.00125309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012530907988548279, 0.0012530907988548279, 0.0012530907988548279, 0.0012530907988548279, 0.0012530907988548279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012530907988548279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58213413
Iteration 2/25 | Loss: 0.00079310
Iteration 3/25 | Loss: 0.00079310
Iteration 4/25 | Loss: 0.00079310
Iteration 5/25 | Loss: 0.00079310
Iteration 6/25 | Loss: 0.00079310
Iteration 7/25 | Loss: 0.00079310
Iteration 8/25 | Loss: 0.00079310
Iteration 9/25 | Loss: 0.00079310
Iteration 10/25 | Loss: 0.00079310
Iteration 11/25 | Loss: 0.00079310
Iteration 12/25 | Loss: 0.00079310
Iteration 13/25 | Loss: 0.00079310
Iteration 14/25 | Loss: 0.00079310
Iteration 15/25 | Loss: 0.00079310
Iteration 16/25 | Loss: 0.00079310
Iteration 17/25 | Loss: 0.00079310
Iteration 18/25 | Loss: 0.00079310
Iteration 19/25 | Loss: 0.00079310
Iteration 20/25 | Loss: 0.00079310
Iteration 21/25 | Loss: 0.00079310
Iteration 22/25 | Loss: 0.00079310
Iteration 23/25 | Loss: 0.00079310
Iteration 24/25 | Loss: 0.00079310
Iteration 25/25 | Loss: 0.00079310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007930980646051466, 0.0007930980646051466, 0.0007930980646051466, 0.0007930980646051466, 0.0007930980646051466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007930980646051466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079310
Iteration 2/1000 | Loss: 0.00003398
Iteration 3/1000 | Loss: 0.00002236
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001804
Iteration 7/1000 | Loss: 0.00001748
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001685
Iteration 10/1000 | Loss: 0.00001658
Iteration 11/1000 | Loss: 0.00001643
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001612
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001611
Iteration 26/1000 | Loss: 0.00001611
Iteration 27/1000 | Loss: 0.00001610
Iteration 28/1000 | Loss: 0.00001610
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001610
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001603
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001598
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001597
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001595
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001594
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001593
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001592
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001591
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001591
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001590
Iteration 78/1000 | Loss: 0.00001590
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001583
Iteration 120/1000 | Loss: 0.00001583
Iteration 121/1000 | Loss: 0.00001583
Iteration 122/1000 | Loss: 0.00001582
Iteration 123/1000 | Loss: 0.00001582
Iteration 124/1000 | Loss: 0.00001582
Iteration 125/1000 | Loss: 0.00001582
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001581
Iteration 129/1000 | Loss: 0.00001581
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001578
Iteration 140/1000 | Loss: 0.00001578
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001576
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001576
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001575
Iteration 154/1000 | Loss: 0.00001575
Iteration 155/1000 | Loss: 0.00001575
Iteration 156/1000 | Loss: 0.00001575
Iteration 157/1000 | Loss: 0.00001574
Iteration 158/1000 | Loss: 0.00001574
Iteration 159/1000 | Loss: 0.00001574
Iteration 160/1000 | Loss: 0.00001574
Iteration 161/1000 | Loss: 0.00001574
Iteration 162/1000 | Loss: 0.00001574
Iteration 163/1000 | Loss: 0.00001574
Iteration 164/1000 | Loss: 0.00001573
Iteration 165/1000 | Loss: 0.00001573
Iteration 166/1000 | Loss: 0.00001573
Iteration 167/1000 | Loss: 0.00001573
Iteration 168/1000 | Loss: 0.00001573
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00001573
Iteration 171/1000 | Loss: 0.00001573
Iteration 172/1000 | Loss: 0.00001573
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001572
Iteration 175/1000 | Loss: 0.00001572
Iteration 176/1000 | Loss: 0.00001572
Iteration 177/1000 | Loss: 0.00001572
Iteration 178/1000 | Loss: 0.00001572
Iteration 179/1000 | Loss: 0.00001572
Iteration 180/1000 | Loss: 0.00001572
Iteration 181/1000 | Loss: 0.00001572
Iteration 182/1000 | Loss: 0.00001572
Iteration 183/1000 | Loss: 0.00001572
Iteration 184/1000 | Loss: 0.00001571
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001571
Iteration 189/1000 | Loss: 0.00001571
Iteration 190/1000 | Loss: 0.00001571
Iteration 191/1000 | Loss: 0.00001571
Iteration 192/1000 | Loss: 0.00001570
Iteration 193/1000 | Loss: 0.00001570
Iteration 194/1000 | Loss: 0.00001570
Iteration 195/1000 | Loss: 0.00001570
Iteration 196/1000 | Loss: 0.00001570
Iteration 197/1000 | Loss: 0.00001570
Iteration 198/1000 | Loss: 0.00001570
Iteration 199/1000 | Loss: 0.00001570
Iteration 200/1000 | Loss: 0.00001570
Iteration 201/1000 | Loss: 0.00001569
Iteration 202/1000 | Loss: 0.00001569
Iteration 203/1000 | Loss: 0.00001569
Iteration 204/1000 | Loss: 0.00001569
Iteration 205/1000 | Loss: 0.00001569
Iteration 206/1000 | Loss: 0.00001569
Iteration 207/1000 | Loss: 0.00001569
Iteration 208/1000 | Loss: 0.00001569
Iteration 209/1000 | Loss: 0.00001569
Iteration 210/1000 | Loss: 0.00001569
Iteration 211/1000 | Loss: 0.00001568
Iteration 212/1000 | Loss: 0.00001568
Iteration 213/1000 | Loss: 0.00001568
Iteration 214/1000 | Loss: 0.00001568
Iteration 215/1000 | Loss: 0.00001568
Iteration 216/1000 | Loss: 0.00001568
Iteration 217/1000 | Loss: 0.00001568
Iteration 218/1000 | Loss: 0.00001568
Iteration 219/1000 | Loss: 0.00001568
Iteration 220/1000 | Loss: 0.00001568
Iteration 221/1000 | Loss: 0.00001568
Iteration 222/1000 | Loss: 0.00001568
Iteration 223/1000 | Loss: 0.00001568
Iteration 224/1000 | Loss: 0.00001568
Iteration 225/1000 | Loss: 0.00001568
Iteration 226/1000 | Loss: 0.00001568
Iteration 227/1000 | Loss: 0.00001568
Iteration 228/1000 | Loss: 0.00001568
Iteration 229/1000 | Loss: 0.00001568
Iteration 230/1000 | Loss: 0.00001568
Iteration 231/1000 | Loss: 0.00001568
Iteration 232/1000 | Loss: 0.00001568
Iteration 233/1000 | Loss: 0.00001568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.5681291188229807e-05, 1.5681291188229807e-05, 1.5681291188229807e-05, 1.5681291188229807e-05, 1.5681291188229807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5681291188229807e-05

Optimization complete. Final v2v error: 3.3117856979370117 mm

Highest mean error: 4.31754207611084 mm for frame 105

Lowest mean error: 2.9794232845306396 mm for frame 159

Saving results

Total time: 48.42870473861694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371885
Iteration 2/25 | Loss: 0.00128184
Iteration 3/25 | Loss: 0.00121470
Iteration 4/25 | Loss: 0.00120520
Iteration 5/25 | Loss: 0.00120382
Iteration 6/25 | Loss: 0.00120382
Iteration 7/25 | Loss: 0.00120382
Iteration 8/25 | Loss: 0.00120382
Iteration 9/25 | Loss: 0.00120382
Iteration 10/25 | Loss: 0.00120382
Iteration 11/25 | Loss: 0.00120382
Iteration 12/25 | Loss: 0.00120382
Iteration 13/25 | Loss: 0.00120382
Iteration 14/25 | Loss: 0.00120382
Iteration 15/25 | Loss: 0.00120382
Iteration 16/25 | Loss: 0.00120382
Iteration 17/25 | Loss: 0.00120382
Iteration 18/25 | Loss: 0.00120382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012038182467222214, 0.0012038182467222214, 0.0012038182467222214, 0.0012038182467222214, 0.0012038182467222214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012038182467222214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70056534
Iteration 2/25 | Loss: 0.00075951
Iteration 3/25 | Loss: 0.00075951
Iteration 4/25 | Loss: 0.00075951
Iteration 5/25 | Loss: 0.00075951
Iteration 6/25 | Loss: 0.00075951
Iteration 7/25 | Loss: 0.00075951
Iteration 8/25 | Loss: 0.00075951
Iteration 9/25 | Loss: 0.00075951
Iteration 10/25 | Loss: 0.00075951
Iteration 11/25 | Loss: 0.00075951
Iteration 12/25 | Loss: 0.00075950
Iteration 13/25 | Loss: 0.00075950
Iteration 14/25 | Loss: 0.00075950
Iteration 15/25 | Loss: 0.00075950
Iteration 16/25 | Loss: 0.00075950
Iteration 17/25 | Loss: 0.00075950
Iteration 18/25 | Loss: 0.00075950
Iteration 19/25 | Loss: 0.00075950
Iteration 20/25 | Loss: 0.00075950
Iteration 21/25 | Loss: 0.00075950
Iteration 22/25 | Loss: 0.00075950
Iteration 23/25 | Loss: 0.00075950
Iteration 24/25 | Loss: 0.00075950
Iteration 25/25 | Loss: 0.00075950

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075950
Iteration 2/1000 | Loss: 0.00002251
Iteration 3/1000 | Loss: 0.00001538
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001278
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001135
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001128
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001117
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001116
Iteration 47/1000 | Loss: 0.00001116
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001114
Iteration 53/1000 | Loss: 0.00001114
Iteration 54/1000 | Loss: 0.00001114
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001112
Iteration 58/1000 | Loss: 0.00001112
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001111
Iteration 63/1000 | Loss: 0.00001111
Iteration 64/1000 | Loss: 0.00001110
Iteration 65/1000 | Loss: 0.00001110
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001109
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001108
Iteration 71/1000 | Loss: 0.00001108
Iteration 72/1000 | Loss: 0.00001108
Iteration 73/1000 | Loss: 0.00001108
Iteration 74/1000 | Loss: 0.00001108
Iteration 75/1000 | Loss: 0.00001107
Iteration 76/1000 | Loss: 0.00001107
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001104
Iteration 82/1000 | Loss: 0.00001104
Iteration 83/1000 | Loss: 0.00001104
Iteration 84/1000 | Loss: 0.00001104
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001098
Iteration 96/1000 | Loss: 0.00001098
Iteration 97/1000 | Loss: 0.00001098
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001097
Iteration 100/1000 | Loss: 0.00001097
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001096
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001095
Iteration 108/1000 | Loss: 0.00001095
Iteration 109/1000 | Loss: 0.00001095
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001094
Iteration 112/1000 | Loss: 0.00001094
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001093
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001092
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001092
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001091
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001090
Iteration 126/1000 | Loss: 0.00001090
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001089
Iteration 130/1000 | Loss: 0.00001089
Iteration 131/1000 | Loss: 0.00001089
Iteration 132/1000 | Loss: 0.00001089
Iteration 133/1000 | Loss: 0.00001089
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Iteration 136/1000 | Loss: 0.00001088
Iteration 137/1000 | Loss: 0.00001088
Iteration 138/1000 | Loss: 0.00001088
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001086
Iteration 148/1000 | Loss: 0.00001086
Iteration 149/1000 | Loss: 0.00001086
Iteration 150/1000 | Loss: 0.00001086
Iteration 151/1000 | Loss: 0.00001086
Iteration 152/1000 | Loss: 0.00001086
Iteration 153/1000 | Loss: 0.00001086
Iteration 154/1000 | Loss: 0.00001085
Iteration 155/1000 | Loss: 0.00001085
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001085
Iteration 158/1000 | Loss: 0.00001085
Iteration 159/1000 | Loss: 0.00001085
Iteration 160/1000 | Loss: 0.00001085
Iteration 161/1000 | Loss: 0.00001085
Iteration 162/1000 | Loss: 0.00001084
Iteration 163/1000 | Loss: 0.00001084
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001083
Iteration 166/1000 | Loss: 0.00001083
Iteration 167/1000 | Loss: 0.00001083
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001082
Iteration 170/1000 | Loss: 0.00001082
Iteration 171/1000 | Loss: 0.00001082
Iteration 172/1000 | Loss: 0.00001082
Iteration 173/1000 | Loss: 0.00001082
Iteration 174/1000 | Loss: 0.00001082
Iteration 175/1000 | Loss: 0.00001082
Iteration 176/1000 | Loss: 0.00001081
Iteration 177/1000 | Loss: 0.00001081
Iteration 178/1000 | Loss: 0.00001080
Iteration 179/1000 | Loss: 0.00001080
Iteration 180/1000 | Loss: 0.00001080
Iteration 181/1000 | Loss: 0.00001080
Iteration 182/1000 | Loss: 0.00001080
Iteration 183/1000 | Loss: 0.00001080
Iteration 184/1000 | Loss: 0.00001080
Iteration 185/1000 | Loss: 0.00001080
Iteration 186/1000 | Loss: 0.00001079
Iteration 187/1000 | Loss: 0.00001079
Iteration 188/1000 | Loss: 0.00001079
Iteration 189/1000 | Loss: 0.00001079
Iteration 190/1000 | Loss: 0.00001079
Iteration 191/1000 | Loss: 0.00001079
Iteration 192/1000 | Loss: 0.00001079
Iteration 193/1000 | Loss: 0.00001079
Iteration 194/1000 | Loss: 0.00001079
Iteration 195/1000 | Loss: 0.00001079
Iteration 196/1000 | Loss: 0.00001079
Iteration 197/1000 | Loss: 0.00001079
Iteration 198/1000 | Loss: 0.00001078
Iteration 199/1000 | Loss: 0.00001078
Iteration 200/1000 | Loss: 0.00001078
Iteration 201/1000 | Loss: 0.00001078
Iteration 202/1000 | Loss: 0.00001078
Iteration 203/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.0782679055409972e-05, 1.0782679055409972e-05, 1.0782679055409972e-05, 1.0782679055409972e-05, 1.0782679055409972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0782679055409972e-05

Optimization complete. Final v2v error: 2.812039613723755 mm

Highest mean error: 3.2429254055023193 mm for frame 138

Lowest mean error: 2.7311184406280518 mm for frame 120

Saving results

Total time: 43.32691168785095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835064
Iteration 2/25 | Loss: 0.00158874
Iteration 3/25 | Loss: 0.00130569
Iteration 4/25 | Loss: 0.00126314
Iteration 5/25 | Loss: 0.00125625
Iteration 6/25 | Loss: 0.00125453
Iteration 7/25 | Loss: 0.00125428
Iteration 8/25 | Loss: 0.00125428
Iteration 9/25 | Loss: 0.00125428
Iteration 10/25 | Loss: 0.00125428
Iteration 11/25 | Loss: 0.00125428
Iteration 12/25 | Loss: 0.00125428
Iteration 13/25 | Loss: 0.00125428
Iteration 14/25 | Loss: 0.00125428
Iteration 15/25 | Loss: 0.00125428
Iteration 16/25 | Loss: 0.00125428
Iteration 17/25 | Loss: 0.00125428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001254281378351152, 0.001254281378351152, 0.001254281378351152, 0.001254281378351152, 0.001254281378351152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001254281378351152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47910500
Iteration 2/25 | Loss: 0.00073918
Iteration 3/25 | Loss: 0.00073918
Iteration 4/25 | Loss: 0.00073918
Iteration 5/25 | Loss: 0.00073918
Iteration 6/25 | Loss: 0.00073918
Iteration 7/25 | Loss: 0.00073918
Iteration 8/25 | Loss: 0.00073918
Iteration 9/25 | Loss: 0.00073918
Iteration 10/25 | Loss: 0.00073918
Iteration 11/25 | Loss: 0.00073918
Iteration 12/25 | Loss: 0.00073918
Iteration 13/25 | Loss: 0.00073918
Iteration 14/25 | Loss: 0.00073918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007391784456558526, 0.0007391784456558526, 0.0007391784456558526, 0.0007391784456558526, 0.0007391784456558526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007391784456558526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073918
Iteration 2/1000 | Loss: 0.00003470
Iteration 3/1000 | Loss: 0.00002159
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001578
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001523
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001512
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001502
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001500
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001500
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001497
Iteration 47/1000 | Loss: 0.00001497
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001494
Iteration 54/1000 | Loss: 0.00001494
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001486
Iteration 119/1000 | Loss: 0.00001486
Iteration 120/1000 | Loss: 0.00001486
Iteration 121/1000 | Loss: 0.00001486
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001485
Iteration 127/1000 | Loss: 0.00001485
Iteration 128/1000 | Loss: 0.00001485
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001484
Iteration 132/1000 | Loss: 0.00001484
Iteration 133/1000 | Loss: 0.00001484
Iteration 134/1000 | Loss: 0.00001484
Iteration 135/1000 | Loss: 0.00001484
Iteration 136/1000 | Loss: 0.00001484
Iteration 137/1000 | Loss: 0.00001484
Iteration 138/1000 | Loss: 0.00001484
Iteration 139/1000 | Loss: 0.00001484
Iteration 140/1000 | Loss: 0.00001484
Iteration 141/1000 | Loss: 0.00001483
Iteration 142/1000 | Loss: 0.00001483
Iteration 143/1000 | Loss: 0.00001483
Iteration 144/1000 | Loss: 0.00001483
Iteration 145/1000 | Loss: 0.00001483
Iteration 146/1000 | Loss: 0.00001482
Iteration 147/1000 | Loss: 0.00001482
Iteration 148/1000 | Loss: 0.00001482
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001482
Iteration 156/1000 | Loss: 0.00001482
Iteration 157/1000 | Loss: 0.00001482
Iteration 158/1000 | Loss: 0.00001482
Iteration 159/1000 | Loss: 0.00001481
Iteration 160/1000 | Loss: 0.00001481
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001481
Iteration 164/1000 | Loss: 0.00001481
Iteration 165/1000 | Loss: 0.00001481
Iteration 166/1000 | Loss: 0.00001481
Iteration 167/1000 | Loss: 0.00001481
Iteration 168/1000 | Loss: 0.00001481
Iteration 169/1000 | Loss: 0.00001480
Iteration 170/1000 | Loss: 0.00001480
Iteration 171/1000 | Loss: 0.00001480
Iteration 172/1000 | Loss: 0.00001480
Iteration 173/1000 | Loss: 0.00001480
Iteration 174/1000 | Loss: 0.00001480
Iteration 175/1000 | Loss: 0.00001480
Iteration 176/1000 | Loss: 0.00001480
Iteration 177/1000 | Loss: 0.00001480
Iteration 178/1000 | Loss: 0.00001479
Iteration 179/1000 | Loss: 0.00001479
Iteration 180/1000 | Loss: 0.00001479
Iteration 181/1000 | Loss: 0.00001479
Iteration 182/1000 | Loss: 0.00001479
Iteration 183/1000 | Loss: 0.00001479
Iteration 184/1000 | Loss: 0.00001479
Iteration 185/1000 | Loss: 0.00001479
Iteration 186/1000 | Loss: 0.00001479
Iteration 187/1000 | Loss: 0.00001479
Iteration 188/1000 | Loss: 0.00001479
Iteration 189/1000 | Loss: 0.00001479
Iteration 190/1000 | Loss: 0.00001479
Iteration 191/1000 | Loss: 0.00001479
Iteration 192/1000 | Loss: 0.00001479
Iteration 193/1000 | Loss: 0.00001479
Iteration 194/1000 | Loss: 0.00001479
Iteration 195/1000 | Loss: 0.00001479
Iteration 196/1000 | Loss: 0.00001479
Iteration 197/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.4785230632696766e-05, 1.4785230632696766e-05, 1.4785230632696766e-05, 1.4785230632696766e-05, 1.4785230632696766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4785230632696766e-05

Optimization complete. Final v2v error: 3.2648940086364746 mm

Highest mean error: 3.9700920581817627 mm for frame 97

Lowest mean error: 2.929976224899292 mm for frame 32

Saving results

Total time: 43.01117539405823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849896
Iteration 2/25 | Loss: 0.00134324
Iteration 3/25 | Loss: 0.00126281
Iteration 4/25 | Loss: 0.00125158
Iteration 5/25 | Loss: 0.00124831
Iteration 6/25 | Loss: 0.00124817
Iteration 7/25 | Loss: 0.00124817
Iteration 8/25 | Loss: 0.00124817
Iteration 9/25 | Loss: 0.00124817
Iteration 10/25 | Loss: 0.00124817
Iteration 11/25 | Loss: 0.00124817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012481671292334795, 0.0012481671292334795, 0.0012481671292334795, 0.0012481671292334795, 0.0012481671292334795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012481671292334795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54987049
Iteration 2/25 | Loss: 0.00083242
Iteration 3/25 | Loss: 0.00083241
Iteration 4/25 | Loss: 0.00083241
Iteration 5/25 | Loss: 0.00083241
Iteration 6/25 | Loss: 0.00083241
Iteration 7/25 | Loss: 0.00083241
Iteration 8/25 | Loss: 0.00083241
Iteration 9/25 | Loss: 0.00083241
Iteration 10/25 | Loss: 0.00083241
Iteration 11/25 | Loss: 0.00083241
Iteration 12/25 | Loss: 0.00083241
Iteration 13/25 | Loss: 0.00083241
Iteration 14/25 | Loss: 0.00083241
Iteration 15/25 | Loss: 0.00083241
Iteration 16/25 | Loss: 0.00083241
Iteration 17/25 | Loss: 0.00083241
Iteration 18/25 | Loss: 0.00083241
Iteration 19/25 | Loss: 0.00083241
Iteration 20/25 | Loss: 0.00083241
Iteration 21/25 | Loss: 0.00083241
Iteration 22/25 | Loss: 0.00083241
Iteration 23/25 | Loss: 0.00083241
Iteration 24/25 | Loss: 0.00083241
Iteration 25/25 | Loss: 0.00083241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083241
Iteration 2/1000 | Loss: 0.00003649
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001598
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00001446
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001441
Iteration 19/1000 | Loss: 0.00001440
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001439
Iteration 22/1000 | Loss: 0.00001438
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001437
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001429
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001426
Iteration 32/1000 | Loss: 0.00001426
Iteration 33/1000 | Loss: 0.00001425
Iteration 34/1000 | Loss: 0.00001425
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001422
Iteration 43/1000 | Loss: 0.00001421
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001421
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001420
Iteration 57/1000 | Loss: 0.00001420
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001419
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001416
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001413
Iteration 78/1000 | Loss: 0.00001413
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001407
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001404
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001403
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001403
Iteration 118/1000 | Loss: 0.00001403
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001400
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001400
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001398
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001397
Iteration 150/1000 | Loss: 0.00001397
Iteration 151/1000 | Loss: 0.00001396
Iteration 152/1000 | Loss: 0.00001396
Iteration 153/1000 | Loss: 0.00001396
Iteration 154/1000 | Loss: 0.00001396
Iteration 155/1000 | Loss: 0.00001396
Iteration 156/1000 | Loss: 0.00001396
Iteration 157/1000 | Loss: 0.00001396
Iteration 158/1000 | Loss: 0.00001396
Iteration 159/1000 | Loss: 0.00001396
Iteration 160/1000 | Loss: 0.00001396
Iteration 161/1000 | Loss: 0.00001395
Iteration 162/1000 | Loss: 0.00001395
Iteration 163/1000 | Loss: 0.00001395
Iteration 164/1000 | Loss: 0.00001395
Iteration 165/1000 | Loss: 0.00001395
Iteration 166/1000 | Loss: 0.00001395
Iteration 167/1000 | Loss: 0.00001395
Iteration 168/1000 | Loss: 0.00001395
Iteration 169/1000 | Loss: 0.00001395
Iteration 170/1000 | Loss: 0.00001394
Iteration 171/1000 | Loss: 0.00001394
Iteration 172/1000 | Loss: 0.00001394
Iteration 173/1000 | Loss: 0.00001394
Iteration 174/1000 | Loss: 0.00001394
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001393
Iteration 180/1000 | Loss: 0.00001393
Iteration 181/1000 | Loss: 0.00001393
Iteration 182/1000 | Loss: 0.00001393
Iteration 183/1000 | Loss: 0.00001393
Iteration 184/1000 | Loss: 0.00001393
Iteration 185/1000 | Loss: 0.00001393
Iteration 186/1000 | Loss: 0.00001393
Iteration 187/1000 | Loss: 0.00001393
Iteration 188/1000 | Loss: 0.00001393
Iteration 189/1000 | Loss: 0.00001392
Iteration 190/1000 | Loss: 0.00001392
Iteration 191/1000 | Loss: 0.00001392
Iteration 192/1000 | Loss: 0.00001392
Iteration 193/1000 | Loss: 0.00001392
Iteration 194/1000 | Loss: 0.00001392
Iteration 195/1000 | Loss: 0.00001392
Iteration 196/1000 | Loss: 0.00001392
Iteration 197/1000 | Loss: 0.00001392
Iteration 198/1000 | Loss: 0.00001392
Iteration 199/1000 | Loss: 0.00001392
Iteration 200/1000 | Loss: 0.00001392
Iteration 201/1000 | Loss: 0.00001392
Iteration 202/1000 | Loss: 0.00001392
Iteration 203/1000 | Loss: 0.00001392
Iteration 204/1000 | Loss: 0.00001392
Iteration 205/1000 | Loss: 0.00001392
Iteration 206/1000 | Loss: 0.00001392
Iteration 207/1000 | Loss: 0.00001392
Iteration 208/1000 | Loss: 0.00001392
Iteration 209/1000 | Loss: 0.00001392
Iteration 210/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.392446029058192e-05, 1.392446029058192e-05, 1.392446029058192e-05, 1.392446029058192e-05, 1.392446029058192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.392446029058192e-05

Optimization complete. Final v2v error: 3.109936237335205 mm

Highest mean error: 3.7176482677459717 mm for frame 47

Lowest mean error: 2.9071338176727295 mm for frame 14

Saving results

Total time: 38.60404872894287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412365
Iteration 2/25 | Loss: 0.00136265
Iteration 3/25 | Loss: 0.00126305
Iteration 4/25 | Loss: 0.00125138
Iteration 5/25 | Loss: 0.00124767
Iteration 6/25 | Loss: 0.00124670
Iteration 7/25 | Loss: 0.00124670
Iteration 8/25 | Loss: 0.00124670
Iteration 9/25 | Loss: 0.00124670
Iteration 10/25 | Loss: 0.00124670
Iteration 11/25 | Loss: 0.00124670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012466982007026672, 0.0012466982007026672, 0.0012466982007026672, 0.0012466982007026672, 0.0012466982007026672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012466982007026672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40299416
Iteration 2/25 | Loss: 0.00082949
Iteration 3/25 | Loss: 0.00082947
Iteration 4/25 | Loss: 0.00082947
Iteration 5/25 | Loss: 0.00082947
Iteration 6/25 | Loss: 0.00082947
Iteration 7/25 | Loss: 0.00082947
Iteration 8/25 | Loss: 0.00082947
Iteration 9/25 | Loss: 0.00082947
Iteration 10/25 | Loss: 0.00082947
Iteration 11/25 | Loss: 0.00082947
Iteration 12/25 | Loss: 0.00082947
Iteration 13/25 | Loss: 0.00082947
Iteration 14/25 | Loss: 0.00082947
Iteration 15/25 | Loss: 0.00082947
Iteration 16/25 | Loss: 0.00082947
Iteration 17/25 | Loss: 0.00082947
Iteration 18/25 | Loss: 0.00082947
Iteration 19/25 | Loss: 0.00082947
Iteration 20/25 | Loss: 0.00082947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008294675499200821, 0.0008294675499200821, 0.0008294675499200821, 0.0008294675499200821, 0.0008294675499200821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008294675499200821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082947
Iteration 2/1000 | Loss: 0.00005669
Iteration 3/1000 | Loss: 0.00003528
Iteration 4/1000 | Loss: 0.00002623
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002084
Iteration 8/1000 | Loss: 0.00002001
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001897
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00001844
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001816
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001807
Iteration 23/1000 | Loss: 0.00001807
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001806
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001806
Iteration 31/1000 | Loss: 0.00001805
Iteration 32/1000 | Loss: 0.00001805
Iteration 33/1000 | Loss: 0.00001805
Iteration 34/1000 | Loss: 0.00001805
Iteration 35/1000 | Loss: 0.00001804
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001803
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001803
Iteration 41/1000 | Loss: 0.00001802
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001802
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001802
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001793
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001789
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001788
Iteration 60/1000 | Loss: 0.00001788
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001783
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001781
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001780
Iteration 98/1000 | Loss: 0.00001780
Iteration 99/1000 | Loss: 0.00001780
Iteration 100/1000 | Loss: 0.00001780
Iteration 101/1000 | Loss: 0.00001780
Iteration 102/1000 | Loss: 0.00001780
Iteration 103/1000 | Loss: 0.00001780
Iteration 104/1000 | Loss: 0.00001779
Iteration 105/1000 | Loss: 0.00001779
Iteration 106/1000 | Loss: 0.00001779
Iteration 107/1000 | Loss: 0.00001779
Iteration 108/1000 | Loss: 0.00001779
Iteration 109/1000 | Loss: 0.00001779
Iteration 110/1000 | Loss: 0.00001779
Iteration 111/1000 | Loss: 0.00001779
Iteration 112/1000 | Loss: 0.00001779
Iteration 113/1000 | Loss: 0.00001779
Iteration 114/1000 | Loss: 0.00001778
Iteration 115/1000 | Loss: 0.00001778
Iteration 116/1000 | Loss: 0.00001778
Iteration 117/1000 | Loss: 0.00001778
Iteration 118/1000 | Loss: 0.00001778
Iteration 119/1000 | Loss: 0.00001778
Iteration 120/1000 | Loss: 0.00001778
Iteration 121/1000 | Loss: 0.00001778
Iteration 122/1000 | Loss: 0.00001778
Iteration 123/1000 | Loss: 0.00001778
Iteration 124/1000 | Loss: 0.00001778
Iteration 125/1000 | Loss: 0.00001778
Iteration 126/1000 | Loss: 0.00001777
Iteration 127/1000 | Loss: 0.00001777
Iteration 128/1000 | Loss: 0.00001777
Iteration 129/1000 | Loss: 0.00001777
Iteration 130/1000 | Loss: 0.00001777
Iteration 131/1000 | Loss: 0.00001777
Iteration 132/1000 | Loss: 0.00001777
Iteration 133/1000 | Loss: 0.00001777
Iteration 134/1000 | Loss: 0.00001776
Iteration 135/1000 | Loss: 0.00001776
Iteration 136/1000 | Loss: 0.00001776
Iteration 137/1000 | Loss: 0.00001776
Iteration 138/1000 | Loss: 0.00001776
Iteration 139/1000 | Loss: 0.00001776
Iteration 140/1000 | Loss: 0.00001776
Iteration 141/1000 | Loss: 0.00001776
Iteration 142/1000 | Loss: 0.00001775
Iteration 143/1000 | Loss: 0.00001775
Iteration 144/1000 | Loss: 0.00001775
Iteration 145/1000 | Loss: 0.00001775
Iteration 146/1000 | Loss: 0.00001775
Iteration 147/1000 | Loss: 0.00001775
Iteration 148/1000 | Loss: 0.00001775
Iteration 149/1000 | Loss: 0.00001775
Iteration 150/1000 | Loss: 0.00001775
Iteration 151/1000 | Loss: 0.00001775
Iteration 152/1000 | Loss: 0.00001775
Iteration 153/1000 | Loss: 0.00001775
Iteration 154/1000 | Loss: 0.00001775
Iteration 155/1000 | Loss: 0.00001775
Iteration 156/1000 | Loss: 0.00001775
Iteration 157/1000 | Loss: 0.00001775
Iteration 158/1000 | Loss: 0.00001774
Iteration 159/1000 | Loss: 0.00001774
Iteration 160/1000 | Loss: 0.00001774
Iteration 161/1000 | Loss: 0.00001774
Iteration 162/1000 | Loss: 0.00001774
Iteration 163/1000 | Loss: 0.00001774
Iteration 164/1000 | Loss: 0.00001774
Iteration 165/1000 | Loss: 0.00001773
Iteration 166/1000 | Loss: 0.00001773
Iteration 167/1000 | Loss: 0.00001773
Iteration 168/1000 | Loss: 0.00001773
Iteration 169/1000 | Loss: 0.00001773
Iteration 170/1000 | Loss: 0.00001773
Iteration 171/1000 | Loss: 0.00001773
Iteration 172/1000 | Loss: 0.00001773
Iteration 173/1000 | Loss: 0.00001773
Iteration 174/1000 | Loss: 0.00001772
Iteration 175/1000 | Loss: 0.00001772
Iteration 176/1000 | Loss: 0.00001772
Iteration 177/1000 | Loss: 0.00001772
Iteration 178/1000 | Loss: 0.00001772
Iteration 179/1000 | Loss: 0.00001772
Iteration 180/1000 | Loss: 0.00001772
Iteration 181/1000 | Loss: 0.00001772
Iteration 182/1000 | Loss: 0.00001772
Iteration 183/1000 | Loss: 0.00001772
Iteration 184/1000 | Loss: 0.00001772
Iteration 185/1000 | Loss: 0.00001772
Iteration 186/1000 | Loss: 0.00001772
Iteration 187/1000 | Loss: 0.00001772
Iteration 188/1000 | Loss: 0.00001771
Iteration 189/1000 | Loss: 0.00001771
Iteration 190/1000 | Loss: 0.00001771
Iteration 191/1000 | Loss: 0.00001771
Iteration 192/1000 | Loss: 0.00001771
Iteration 193/1000 | Loss: 0.00001771
Iteration 194/1000 | Loss: 0.00001771
Iteration 195/1000 | Loss: 0.00001771
Iteration 196/1000 | Loss: 0.00001771
Iteration 197/1000 | Loss: 0.00001771
Iteration 198/1000 | Loss: 0.00001771
Iteration 199/1000 | Loss: 0.00001771
Iteration 200/1000 | Loss: 0.00001771
Iteration 201/1000 | Loss: 0.00001770
Iteration 202/1000 | Loss: 0.00001770
Iteration 203/1000 | Loss: 0.00001770
Iteration 204/1000 | Loss: 0.00001770
Iteration 205/1000 | Loss: 0.00001770
Iteration 206/1000 | Loss: 0.00001770
Iteration 207/1000 | Loss: 0.00001770
Iteration 208/1000 | Loss: 0.00001770
Iteration 209/1000 | Loss: 0.00001770
Iteration 210/1000 | Loss: 0.00001769
Iteration 211/1000 | Loss: 0.00001769
Iteration 212/1000 | Loss: 0.00001769
Iteration 213/1000 | Loss: 0.00001769
Iteration 214/1000 | Loss: 0.00001769
Iteration 215/1000 | Loss: 0.00001769
Iteration 216/1000 | Loss: 0.00001769
Iteration 217/1000 | Loss: 0.00001769
Iteration 218/1000 | Loss: 0.00001769
Iteration 219/1000 | Loss: 0.00001769
Iteration 220/1000 | Loss: 0.00001769
Iteration 221/1000 | Loss: 0.00001769
Iteration 222/1000 | Loss: 0.00001769
Iteration 223/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.7694663256406784e-05, 1.7694663256406784e-05, 1.7694663256406784e-05, 1.7694663256406784e-05, 1.7694663256406784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7694663256406784e-05

Optimization complete. Final v2v error: 3.426365613937378 mm

Highest mean error: 5.3916730880737305 mm for frame 88

Lowest mean error: 2.8563263416290283 mm for frame 17

Saving results

Total time: 43.42414903640747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794039
Iteration 2/25 | Loss: 0.00163348
Iteration 3/25 | Loss: 0.00139612
Iteration 4/25 | Loss: 0.00136443
Iteration 5/25 | Loss: 0.00135843
Iteration 6/25 | Loss: 0.00135781
Iteration 7/25 | Loss: 0.00135781
Iteration 8/25 | Loss: 0.00135781
Iteration 9/25 | Loss: 0.00135781
Iteration 10/25 | Loss: 0.00135781
Iteration 11/25 | Loss: 0.00135781
Iteration 12/25 | Loss: 0.00135781
Iteration 13/25 | Loss: 0.00135781
Iteration 14/25 | Loss: 0.00135781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013578084763139486, 0.0013578084763139486, 0.0013578084763139486, 0.0013578084763139486, 0.0013578084763139486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013578084763139486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46399009
Iteration 2/25 | Loss: 0.00104937
Iteration 3/25 | Loss: 0.00104931
Iteration 4/25 | Loss: 0.00104931
Iteration 5/25 | Loss: 0.00104931
Iteration 6/25 | Loss: 0.00104931
Iteration 7/25 | Loss: 0.00104931
Iteration 8/25 | Loss: 0.00104931
Iteration 9/25 | Loss: 0.00104931
Iteration 10/25 | Loss: 0.00104931
Iteration 11/25 | Loss: 0.00104931
Iteration 12/25 | Loss: 0.00104931
Iteration 13/25 | Loss: 0.00104931
Iteration 14/25 | Loss: 0.00104931
Iteration 15/25 | Loss: 0.00104931
Iteration 16/25 | Loss: 0.00104931
Iteration 17/25 | Loss: 0.00104931
Iteration 18/25 | Loss: 0.00104931
Iteration 19/25 | Loss: 0.00104931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010493064764887094, 0.0010493064764887094, 0.0010493064764887094, 0.0010493064764887094, 0.0010493064764887094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010493064764887094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104931
Iteration 2/1000 | Loss: 0.00005278
Iteration 3/1000 | Loss: 0.00003730
Iteration 4/1000 | Loss: 0.00003003
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002624
Iteration 7/1000 | Loss: 0.00002530
Iteration 8/1000 | Loss: 0.00002457
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002379
Iteration 11/1000 | Loss: 0.00002358
Iteration 12/1000 | Loss: 0.00002352
Iteration 13/1000 | Loss: 0.00002351
Iteration 14/1000 | Loss: 0.00002343
Iteration 15/1000 | Loss: 0.00002318
Iteration 16/1000 | Loss: 0.00002310
Iteration 17/1000 | Loss: 0.00002303
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002297
Iteration 20/1000 | Loss: 0.00002291
Iteration 21/1000 | Loss: 0.00002290
Iteration 22/1000 | Loss: 0.00002288
Iteration 23/1000 | Loss: 0.00002287
Iteration 24/1000 | Loss: 0.00002286
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00002285
Iteration 28/1000 | Loss: 0.00002285
Iteration 29/1000 | Loss: 0.00002284
Iteration 30/1000 | Loss: 0.00002284
Iteration 31/1000 | Loss: 0.00002284
Iteration 32/1000 | Loss: 0.00002284
Iteration 33/1000 | Loss: 0.00002283
Iteration 34/1000 | Loss: 0.00002283
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002282
Iteration 39/1000 | Loss: 0.00002282
Iteration 40/1000 | Loss: 0.00002281
Iteration 41/1000 | Loss: 0.00002281
Iteration 42/1000 | Loss: 0.00002281
Iteration 43/1000 | Loss: 0.00002281
Iteration 44/1000 | Loss: 0.00002281
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002279
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002278
Iteration 50/1000 | Loss: 0.00002278
Iteration 51/1000 | Loss: 0.00002278
Iteration 52/1000 | Loss: 0.00002278
Iteration 53/1000 | Loss: 0.00002278
Iteration 54/1000 | Loss: 0.00002278
Iteration 55/1000 | Loss: 0.00002278
Iteration 56/1000 | Loss: 0.00002278
Iteration 57/1000 | Loss: 0.00002278
Iteration 58/1000 | Loss: 0.00002278
Iteration 59/1000 | Loss: 0.00002278
Iteration 60/1000 | Loss: 0.00002278
Iteration 61/1000 | Loss: 0.00002278
Iteration 62/1000 | Loss: 0.00002277
Iteration 63/1000 | Loss: 0.00002277
Iteration 64/1000 | Loss: 0.00002277
Iteration 65/1000 | Loss: 0.00002277
Iteration 66/1000 | Loss: 0.00002276
Iteration 67/1000 | Loss: 0.00002276
Iteration 68/1000 | Loss: 0.00002276
Iteration 69/1000 | Loss: 0.00002276
Iteration 70/1000 | Loss: 0.00002275
Iteration 71/1000 | Loss: 0.00002275
Iteration 72/1000 | Loss: 0.00002275
Iteration 73/1000 | Loss: 0.00002275
Iteration 74/1000 | Loss: 0.00002275
Iteration 75/1000 | Loss: 0.00002275
Iteration 76/1000 | Loss: 0.00002275
Iteration 77/1000 | Loss: 0.00002275
Iteration 78/1000 | Loss: 0.00002275
Iteration 79/1000 | Loss: 0.00002275
Iteration 80/1000 | Loss: 0.00002274
Iteration 81/1000 | Loss: 0.00002274
Iteration 82/1000 | Loss: 0.00002274
Iteration 83/1000 | Loss: 0.00002274
Iteration 84/1000 | Loss: 0.00002274
Iteration 85/1000 | Loss: 0.00002274
Iteration 86/1000 | Loss: 0.00002274
Iteration 87/1000 | Loss: 0.00002273
Iteration 88/1000 | Loss: 0.00002273
Iteration 89/1000 | Loss: 0.00002273
Iteration 90/1000 | Loss: 0.00002273
Iteration 91/1000 | Loss: 0.00002272
Iteration 92/1000 | Loss: 0.00002272
Iteration 93/1000 | Loss: 0.00002272
Iteration 94/1000 | Loss: 0.00002272
Iteration 95/1000 | Loss: 0.00002272
Iteration 96/1000 | Loss: 0.00002272
Iteration 97/1000 | Loss: 0.00002272
Iteration 98/1000 | Loss: 0.00002272
Iteration 99/1000 | Loss: 0.00002272
Iteration 100/1000 | Loss: 0.00002272
Iteration 101/1000 | Loss: 0.00002272
Iteration 102/1000 | Loss: 0.00002272
Iteration 103/1000 | Loss: 0.00002271
Iteration 104/1000 | Loss: 0.00002271
Iteration 105/1000 | Loss: 0.00002271
Iteration 106/1000 | Loss: 0.00002271
Iteration 107/1000 | Loss: 0.00002271
Iteration 108/1000 | Loss: 0.00002271
Iteration 109/1000 | Loss: 0.00002270
Iteration 110/1000 | Loss: 0.00002270
Iteration 111/1000 | Loss: 0.00002270
Iteration 112/1000 | Loss: 0.00002270
Iteration 113/1000 | Loss: 0.00002270
Iteration 114/1000 | Loss: 0.00002270
Iteration 115/1000 | Loss: 0.00002270
Iteration 116/1000 | Loss: 0.00002270
Iteration 117/1000 | Loss: 0.00002270
Iteration 118/1000 | Loss: 0.00002270
Iteration 119/1000 | Loss: 0.00002270
Iteration 120/1000 | Loss: 0.00002270
Iteration 121/1000 | Loss: 0.00002270
Iteration 122/1000 | Loss: 0.00002270
Iteration 123/1000 | Loss: 0.00002270
Iteration 124/1000 | Loss: 0.00002270
Iteration 125/1000 | Loss: 0.00002270
Iteration 126/1000 | Loss: 0.00002270
Iteration 127/1000 | Loss: 0.00002270
Iteration 128/1000 | Loss: 0.00002270
Iteration 129/1000 | Loss: 0.00002270
Iteration 130/1000 | Loss: 0.00002270
Iteration 131/1000 | Loss: 0.00002270
Iteration 132/1000 | Loss: 0.00002270
Iteration 133/1000 | Loss: 0.00002270
Iteration 134/1000 | Loss: 0.00002270
Iteration 135/1000 | Loss: 0.00002270
Iteration 136/1000 | Loss: 0.00002270
Iteration 137/1000 | Loss: 0.00002270
Iteration 138/1000 | Loss: 0.00002270
Iteration 139/1000 | Loss: 0.00002270
Iteration 140/1000 | Loss: 0.00002270
Iteration 141/1000 | Loss: 0.00002270
Iteration 142/1000 | Loss: 0.00002270
Iteration 143/1000 | Loss: 0.00002270
Iteration 144/1000 | Loss: 0.00002270
Iteration 145/1000 | Loss: 0.00002270
Iteration 146/1000 | Loss: 0.00002270
Iteration 147/1000 | Loss: 0.00002270
Iteration 148/1000 | Loss: 0.00002270
Iteration 149/1000 | Loss: 0.00002270
Iteration 150/1000 | Loss: 0.00002270
Iteration 151/1000 | Loss: 0.00002270
Iteration 152/1000 | Loss: 0.00002270
Iteration 153/1000 | Loss: 0.00002270
Iteration 154/1000 | Loss: 0.00002270
Iteration 155/1000 | Loss: 0.00002270
Iteration 156/1000 | Loss: 0.00002270
Iteration 157/1000 | Loss: 0.00002270
Iteration 158/1000 | Loss: 0.00002270
Iteration 159/1000 | Loss: 0.00002270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.2699934561387636e-05, 2.2699934561387636e-05, 2.2699934561387636e-05, 2.2699934561387636e-05, 2.2699934561387636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2699934561387636e-05

Optimization complete. Final v2v error: 3.980435371398926 mm

Highest mean error: 4.30682897567749 mm for frame 29

Lowest mean error: 3.7889654636383057 mm for frame 12

Saving results

Total time: 36.1751754283905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00933270
Iteration 2/25 | Loss: 0.00340845
Iteration 3/25 | Loss: 0.00252667
Iteration 4/25 | Loss: 0.00240109
Iteration 5/25 | Loss: 0.00211416
Iteration 6/25 | Loss: 0.00204071
Iteration 7/25 | Loss: 0.00193328
Iteration 8/25 | Loss: 0.00164438
Iteration 9/25 | Loss: 0.00157413
Iteration 10/25 | Loss: 0.00155438
Iteration 11/25 | Loss: 0.00154610
Iteration 12/25 | Loss: 0.00154198
Iteration 13/25 | Loss: 0.00153880
Iteration 14/25 | Loss: 0.00153569
Iteration 15/25 | Loss: 0.00153431
Iteration 16/25 | Loss: 0.00153366
Iteration 17/25 | Loss: 0.00153675
Iteration 18/25 | Loss: 0.00153147
Iteration 19/25 | Loss: 0.00153015
Iteration 20/25 | Loss: 0.00152992
Iteration 21/25 | Loss: 0.00152990
Iteration 22/25 | Loss: 0.00152990
Iteration 23/25 | Loss: 0.00152989
Iteration 24/25 | Loss: 0.00152989
Iteration 25/25 | Loss: 0.00152989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46208930
Iteration 2/25 | Loss: 0.00251980
Iteration 3/25 | Loss: 0.00251980
Iteration 4/25 | Loss: 0.00251980
Iteration 5/25 | Loss: 0.00251979
Iteration 6/25 | Loss: 0.00251979
Iteration 7/25 | Loss: 0.00251979
Iteration 8/25 | Loss: 0.00251979
Iteration 9/25 | Loss: 0.00251979
Iteration 10/25 | Loss: 0.00251979
Iteration 11/25 | Loss: 0.00251979
Iteration 12/25 | Loss: 0.00251979
Iteration 13/25 | Loss: 0.00251979
Iteration 14/25 | Loss: 0.00251979
Iteration 15/25 | Loss: 0.00251979
Iteration 16/25 | Loss: 0.00251979
Iteration 17/25 | Loss: 0.00251979
Iteration 18/25 | Loss: 0.00251979
Iteration 19/25 | Loss: 0.00251979
Iteration 20/25 | Loss: 0.00251979
Iteration 21/25 | Loss: 0.00251979
Iteration 22/25 | Loss: 0.00251979
Iteration 23/25 | Loss: 0.00251979
Iteration 24/25 | Loss: 0.00251979
Iteration 25/25 | Loss: 0.00251979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251979
Iteration 2/1000 | Loss: 0.00091361
Iteration 3/1000 | Loss: 0.00020963
Iteration 4/1000 | Loss: 0.00013767
Iteration 5/1000 | Loss: 0.00009383
Iteration 6/1000 | Loss: 0.00007151
Iteration 7/1000 | Loss: 0.00005811
Iteration 8/1000 | Loss: 0.00005032
Iteration 9/1000 | Loss: 0.00004287
Iteration 10/1000 | Loss: 0.00070510
Iteration 11/1000 | Loss: 0.00037904
Iteration 12/1000 | Loss: 0.00006583
Iteration 13/1000 | Loss: 0.00004341
Iteration 14/1000 | Loss: 0.00003753
Iteration 15/1000 | Loss: 0.00003356
Iteration 16/1000 | Loss: 0.00002990
Iteration 17/1000 | Loss: 0.00002782
Iteration 18/1000 | Loss: 0.00002642
Iteration 19/1000 | Loss: 0.00002531
Iteration 20/1000 | Loss: 0.00002460
Iteration 21/1000 | Loss: 0.00002435
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002383
Iteration 24/1000 | Loss: 0.00002364
Iteration 25/1000 | Loss: 0.00002361
Iteration 26/1000 | Loss: 0.00002356
Iteration 27/1000 | Loss: 0.00002343
Iteration 28/1000 | Loss: 0.00002339
Iteration 29/1000 | Loss: 0.00002336
Iteration 30/1000 | Loss: 0.00002336
Iteration 31/1000 | Loss: 0.00002335
Iteration 32/1000 | Loss: 0.00002335
Iteration 33/1000 | Loss: 0.00002335
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002334
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00002333
Iteration 38/1000 | Loss: 0.00002333
Iteration 39/1000 | Loss: 0.00002333
Iteration 40/1000 | Loss: 0.00002333
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002332
Iteration 43/1000 | Loss: 0.00002332
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002332
Iteration 46/1000 | Loss: 0.00002332
Iteration 47/1000 | Loss: 0.00002332
Iteration 48/1000 | Loss: 0.00002332
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00002331
Iteration 52/1000 | Loss: 0.00002331
Iteration 53/1000 | Loss: 0.00002331
Iteration 54/1000 | Loss: 0.00002331
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002328
Iteration 60/1000 | Loss: 0.00002328
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002328
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002327
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002327
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002326
Iteration 82/1000 | Loss: 0.00002326
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002325
Iteration 87/1000 | Loss: 0.00002325
Iteration 88/1000 | Loss: 0.00002325
Iteration 89/1000 | Loss: 0.00002325
Iteration 90/1000 | Loss: 0.00002325
Iteration 91/1000 | Loss: 0.00002325
Iteration 92/1000 | Loss: 0.00002325
Iteration 93/1000 | Loss: 0.00002325
Iteration 94/1000 | Loss: 0.00002325
Iteration 95/1000 | Loss: 0.00002325
Iteration 96/1000 | Loss: 0.00002325
Iteration 97/1000 | Loss: 0.00002325
Iteration 98/1000 | Loss: 0.00002325
Iteration 99/1000 | Loss: 0.00002325
Iteration 100/1000 | Loss: 0.00002325
Iteration 101/1000 | Loss: 0.00002325
Iteration 102/1000 | Loss: 0.00002325
Iteration 103/1000 | Loss: 0.00002325
Iteration 104/1000 | Loss: 0.00002325
Iteration 105/1000 | Loss: 0.00002325
Iteration 106/1000 | Loss: 0.00002325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.3246833734447137e-05, 2.3246833734447137e-05, 2.3246833734447137e-05, 2.3246833734447137e-05, 2.3246833734447137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3246833734447137e-05

Optimization complete. Final v2v error: 4.110538005828857 mm

Highest mean error: 4.643024444580078 mm for frame 58

Lowest mean error: 3.5870730876922607 mm for frame 154

Saving results

Total time: 79.60699915885925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539786
Iteration 2/25 | Loss: 0.00132333
Iteration 3/25 | Loss: 0.00125795
Iteration 4/25 | Loss: 0.00124772
Iteration 5/25 | Loss: 0.00124433
Iteration 6/25 | Loss: 0.00124400
Iteration 7/25 | Loss: 0.00124400
Iteration 8/25 | Loss: 0.00124400
Iteration 9/25 | Loss: 0.00124400
Iteration 10/25 | Loss: 0.00124400
Iteration 11/25 | Loss: 0.00124400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001244000974111259, 0.001244000974111259, 0.001244000974111259, 0.001244000974111259, 0.001244000974111259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001244000974111259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.23179150
Iteration 2/25 | Loss: 0.00076306
Iteration 3/25 | Loss: 0.00076305
Iteration 4/25 | Loss: 0.00076305
Iteration 5/25 | Loss: 0.00076305
Iteration 6/25 | Loss: 0.00076305
Iteration 7/25 | Loss: 0.00076305
Iteration 8/25 | Loss: 0.00076305
Iteration 9/25 | Loss: 0.00076305
Iteration 10/25 | Loss: 0.00076305
Iteration 11/25 | Loss: 0.00076305
Iteration 12/25 | Loss: 0.00076305
Iteration 13/25 | Loss: 0.00076305
Iteration 14/25 | Loss: 0.00076305
Iteration 15/25 | Loss: 0.00076305
Iteration 16/25 | Loss: 0.00076305
Iteration 17/25 | Loss: 0.00076305
Iteration 18/25 | Loss: 0.00076305
Iteration 19/25 | Loss: 0.00076305
Iteration 20/25 | Loss: 0.00076305
Iteration 21/25 | Loss: 0.00076305
Iteration 22/25 | Loss: 0.00076305
Iteration 23/25 | Loss: 0.00076305
Iteration 24/25 | Loss: 0.00076305
Iteration 25/25 | Loss: 0.00076305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076305
Iteration 2/1000 | Loss: 0.00003031
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001716
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001547
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001511
Iteration 27/1000 | Loss: 0.00001511
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.4785721759835724e-05, 1.4785721759835724e-05, 1.4785721759835724e-05, 1.4785721759835724e-05, 1.4785721759835724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4785721759835724e-05

Optimization complete. Final v2v error: 3.2657063007354736 mm

Highest mean error: 3.814128875732422 mm for frame 59

Lowest mean error: 2.9512135982513428 mm for frame 37

Saving results

Total time: 32.21697425842285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074466
Iteration 2/25 | Loss: 0.00215990
Iteration 3/25 | Loss: 0.00151988
Iteration 4/25 | Loss: 0.00147866
Iteration 5/25 | Loss: 0.00145123
Iteration 6/25 | Loss: 0.00144698
Iteration 7/25 | Loss: 0.00142585
Iteration 8/25 | Loss: 0.00142469
Iteration 9/25 | Loss: 0.00142154
Iteration 10/25 | Loss: 0.00141119
Iteration 11/25 | Loss: 0.00141401
Iteration 12/25 | Loss: 0.00140687
Iteration 13/25 | Loss: 0.00139796
Iteration 14/25 | Loss: 0.00139564
Iteration 15/25 | Loss: 0.00139478
Iteration 16/25 | Loss: 0.00139618
Iteration 17/25 | Loss: 0.00139499
Iteration 18/25 | Loss: 0.00139324
Iteration 19/25 | Loss: 0.00139473
Iteration 20/25 | Loss: 0.00139355
Iteration 21/25 | Loss: 0.00139784
Iteration 22/25 | Loss: 0.00139573
Iteration 23/25 | Loss: 0.00139334
Iteration 24/25 | Loss: 0.00139200
Iteration 25/25 | Loss: 0.00139160

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81739557
Iteration 2/25 | Loss: 0.00106538
Iteration 3/25 | Loss: 0.00106537
Iteration 4/25 | Loss: 0.00106537
Iteration 5/25 | Loss: 0.00106537
Iteration 6/25 | Loss: 0.00106537
Iteration 7/25 | Loss: 0.00106537
Iteration 8/25 | Loss: 0.00106537
Iteration 9/25 | Loss: 0.00106537
Iteration 10/25 | Loss: 0.00106537
Iteration 11/25 | Loss: 0.00106537
Iteration 12/25 | Loss: 0.00106537
Iteration 13/25 | Loss: 0.00106537
Iteration 14/25 | Loss: 0.00106537
Iteration 15/25 | Loss: 0.00106537
Iteration 16/25 | Loss: 0.00106537
Iteration 17/25 | Loss: 0.00106537
Iteration 18/25 | Loss: 0.00106537
Iteration 19/25 | Loss: 0.00106537
Iteration 20/25 | Loss: 0.00106537
Iteration 21/25 | Loss: 0.00106537
Iteration 22/25 | Loss: 0.00106537
Iteration 23/25 | Loss: 0.00106537
Iteration 24/25 | Loss: 0.00106537
Iteration 25/25 | Loss: 0.00106537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106537
Iteration 2/1000 | Loss: 0.00013212
Iteration 3/1000 | Loss: 0.00014196
Iteration 4/1000 | Loss: 0.00003570
Iteration 5/1000 | Loss: 0.00021581
Iteration 6/1000 | Loss: 0.00019476
Iteration 7/1000 | Loss: 0.00012825
Iteration 8/1000 | Loss: 0.00011790
Iteration 9/1000 | Loss: 0.00003797
Iteration 10/1000 | Loss: 0.00003502
Iteration 11/1000 | Loss: 0.00003357
Iteration 12/1000 | Loss: 0.00003194
Iteration 13/1000 | Loss: 0.00012261
Iteration 14/1000 | Loss: 0.00003245
Iteration 15/1000 | Loss: 0.00003029
Iteration 16/1000 | Loss: 0.00006734
Iteration 17/1000 | Loss: 0.00003066
Iteration 18/1000 | Loss: 0.00002920
Iteration 19/1000 | Loss: 0.00002858
Iteration 20/1000 | Loss: 0.00002809
Iteration 21/1000 | Loss: 0.00002757
Iteration 22/1000 | Loss: 0.00002729
Iteration 23/1000 | Loss: 0.00002692
Iteration 24/1000 | Loss: 0.00005353
Iteration 25/1000 | Loss: 0.00002638
Iteration 26/1000 | Loss: 0.00002619
Iteration 27/1000 | Loss: 0.00005704
Iteration 28/1000 | Loss: 0.00002632
Iteration 29/1000 | Loss: 0.00004194
Iteration 30/1000 | Loss: 0.00002578
Iteration 31/1000 | Loss: 0.00002561
Iteration 32/1000 | Loss: 0.00002557
Iteration 33/1000 | Loss: 0.00002557
Iteration 34/1000 | Loss: 0.00002556
Iteration 35/1000 | Loss: 0.00002555
Iteration 36/1000 | Loss: 0.00002555
Iteration 37/1000 | Loss: 0.00002555
Iteration 38/1000 | Loss: 0.00002554
Iteration 39/1000 | Loss: 0.00002553
Iteration 40/1000 | Loss: 0.00002553
Iteration 41/1000 | Loss: 0.00002553
Iteration 42/1000 | Loss: 0.00002552
Iteration 43/1000 | Loss: 0.00002552
Iteration 44/1000 | Loss: 0.00002552
Iteration 45/1000 | Loss: 0.00002552
Iteration 46/1000 | Loss: 0.00002551
Iteration 47/1000 | Loss: 0.00002551
Iteration 48/1000 | Loss: 0.00002550
Iteration 49/1000 | Loss: 0.00002550
Iteration 50/1000 | Loss: 0.00002550
Iteration 51/1000 | Loss: 0.00002549
Iteration 52/1000 | Loss: 0.00002549
Iteration 53/1000 | Loss: 0.00002549
Iteration 54/1000 | Loss: 0.00002548
Iteration 55/1000 | Loss: 0.00002547
Iteration 56/1000 | Loss: 0.00002546
Iteration 57/1000 | Loss: 0.00002546
Iteration 58/1000 | Loss: 0.00002546
Iteration 59/1000 | Loss: 0.00002546
Iteration 60/1000 | Loss: 0.00002546
Iteration 61/1000 | Loss: 0.00002545
Iteration 62/1000 | Loss: 0.00002545
Iteration 63/1000 | Loss: 0.00002545
Iteration 64/1000 | Loss: 0.00002545
Iteration 65/1000 | Loss: 0.00002545
Iteration 66/1000 | Loss: 0.00002545
Iteration 67/1000 | Loss: 0.00002545
Iteration 68/1000 | Loss: 0.00002545
Iteration 69/1000 | Loss: 0.00002545
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002544
Iteration 72/1000 | Loss: 0.00002544
Iteration 73/1000 | Loss: 0.00002544
Iteration 74/1000 | Loss: 0.00002544
Iteration 75/1000 | Loss: 0.00002544
Iteration 76/1000 | Loss: 0.00002543
Iteration 77/1000 | Loss: 0.00002543
Iteration 78/1000 | Loss: 0.00002543
Iteration 79/1000 | Loss: 0.00002543
Iteration 80/1000 | Loss: 0.00002543
Iteration 81/1000 | Loss: 0.00002543
Iteration 82/1000 | Loss: 0.00002543
Iteration 83/1000 | Loss: 0.00002543
Iteration 84/1000 | Loss: 0.00002543
Iteration 85/1000 | Loss: 0.00002542
Iteration 86/1000 | Loss: 0.00002542
Iteration 87/1000 | Loss: 0.00002542
Iteration 88/1000 | Loss: 0.00002542
Iteration 89/1000 | Loss: 0.00002542
Iteration 90/1000 | Loss: 0.00002542
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00002542
Iteration 93/1000 | Loss: 0.00002542
Iteration 94/1000 | Loss: 0.00002542
Iteration 95/1000 | Loss: 0.00002542
Iteration 96/1000 | Loss: 0.00002542
Iteration 97/1000 | Loss: 0.00002542
Iteration 98/1000 | Loss: 0.00002542
Iteration 99/1000 | Loss: 0.00002542
Iteration 100/1000 | Loss: 0.00002541
Iteration 101/1000 | Loss: 0.00002541
Iteration 102/1000 | Loss: 0.00002541
Iteration 103/1000 | Loss: 0.00002541
Iteration 104/1000 | Loss: 0.00002541
Iteration 105/1000 | Loss: 0.00002541
Iteration 106/1000 | Loss: 0.00002541
Iteration 107/1000 | Loss: 0.00002541
Iteration 108/1000 | Loss: 0.00002541
Iteration 109/1000 | Loss: 0.00002541
Iteration 110/1000 | Loss: 0.00002540
Iteration 111/1000 | Loss: 0.00002540
Iteration 112/1000 | Loss: 0.00002540
Iteration 113/1000 | Loss: 0.00002540
Iteration 114/1000 | Loss: 0.00002539
Iteration 115/1000 | Loss: 0.00002539
Iteration 116/1000 | Loss: 0.00002539
Iteration 117/1000 | Loss: 0.00002539
Iteration 118/1000 | Loss: 0.00002539
Iteration 119/1000 | Loss: 0.00002539
Iteration 120/1000 | Loss: 0.00002539
Iteration 121/1000 | Loss: 0.00002539
Iteration 122/1000 | Loss: 0.00002539
Iteration 123/1000 | Loss: 0.00002539
Iteration 124/1000 | Loss: 0.00002538
Iteration 125/1000 | Loss: 0.00002538
Iteration 126/1000 | Loss: 0.00002538
Iteration 127/1000 | Loss: 0.00002538
Iteration 128/1000 | Loss: 0.00002537
Iteration 129/1000 | Loss: 0.00002537
Iteration 130/1000 | Loss: 0.00002537
Iteration 131/1000 | Loss: 0.00002537
Iteration 132/1000 | Loss: 0.00002537
Iteration 133/1000 | Loss: 0.00002537
Iteration 134/1000 | Loss: 0.00002536
Iteration 135/1000 | Loss: 0.00002536
Iteration 136/1000 | Loss: 0.00002536
Iteration 137/1000 | Loss: 0.00002536
Iteration 138/1000 | Loss: 0.00002536
Iteration 139/1000 | Loss: 0.00002536
Iteration 140/1000 | Loss: 0.00002535
Iteration 141/1000 | Loss: 0.00002535
Iteration 142/1000 | Loss: 0.00002535
Iteration 143/1000 | Loss: 0.00002535
Iteration 144/1000 | Loss: 0.00002535
Iteration 145/1000 | Loss: 0.00002535
Iteration 146/1000 | Loss: 0.00002535
Iteration 147/1000 | Loss: 0.00002535
Iteration 148/1000 | Loss: 0.00002535
Iteration 149/1000 | Loss: 0.00002535
Iteration 150/1000 | Loss: 0.00002535
Iteration 151/1000 | Loss: 0.00002535
Iteration 152/1000 | Loss: 0.00002535
Iteration 153/1000 | Loss: 0.00002535
Iteration 154/1000 | Loss: 0.00002535
Iteration 155/1000 | Loss: 0.00002535
Iteration 156/1000 | Loss: 0.00002535
Iteration 157/1000 | Loss: 0.00002534
Iteration 158/1000 | Loss: 0.00002534
Iteration 159/1000 | Loss: 0.00002534
Iteration 160/1000 | Loss: 0.00002534
Iteration 161/1000 | Loss: 0.00002534
Iteration 162/1000 | Loss: 0.00002534
Iteration 163/1000 | Loss: 0.00002534
Iteration 164/1000 | Loss: 0.00002534
Iteration 165/1000 | Loss: 0.00002534
Iteration 166/1000 | Loss: 0.00002534
Iteration 167/1000 | Loss: 0.00002534
Iteration 168/1000 | Loss: 0.00002534
Iteration 169/1000 | Loss: 0.00002534
Iteration 170/1000 | Loss: 0.00002534
Iteration 171/1000 | Loss: 0.00002533
Iteration 172/1000 | Loss: 0.00002533
Iteration 173/1000 | Loss: 0.00002533
Iteration 174/1000 | Loss: 0.00002533
Iteration 175/1000 | Loss: 0.00002533
Iteration 176/1000 | Loss: 0.00002533
Iteration 177/1000 | Loss: 0.00002533
Iteration 178/1000 | Loss: 0.00002533
Iteration 179/1000 | Loss: 0.00002533
Iteration 180/1000 | Loss: 0.00002533
Iteration 181/1000 | Loss: 0.00002533
Iteration 182/1000 | Loss: 0.00002533
Iteration 183/1000 | Loss: 0.00002533
Iteration 184/1000 | Loss: 0.00002533
Iteration 185/1000 | Loss: 0.00002533
Iteration 186/1000 | Loss: 0.00002533
Iteration 187/1000 | Loss: 0.00002532
Iteration 188/1000 | Loss: 0.00002532
Iteration 189/1000 | Loss: 0.00002532
Iteration 190/1000 | Loss: 0.00002532
Iteration 191/1000 | Loss: 0.00002532
Iteration 192/1000 | Loss: 0.00002532
Iteration 193/1000 | Loss: 0.00002532
Iteration 194/1000 | Loss: 0.00002532
Iteration 195/1000 | Loss: 0.00002532
Iteration 196/1000 | Loss: 0.00002532
Iteration 197/1000 | Loss: 0.00002532
Iteration 198/1000 | Loss: 0.00002532
Iteration 199/1000 | Loss: 0.00002532
Iteration 200/1000 | Loss: 0.00002532
Iteration 201/1000 | Loss: 0.00002532
Iteration 202/1000 | Loss: 0.00002532
Iteration 203/1000 | Loss: 0.00002532
Iteration 204/1000 | Loss: 0.00002531
Iteration 205/1000 | Loss: 0.00002531
Iteration 206/1000 | Loss: 0.00002531
Iteration 207/1000 | Loss: 0.00002531
Iteration 208/1000 | Loss: 0.00002531
Iteration 209/1000 | Loss: 0.00002531
Iteration 210/1000 | Loss: 0.00002531
Iteration 211/1000 | Loss: 0.00002531
Iteration 212/1000 | Loss: 0.00002531
Iteration 213/1000 | Loss: 0.00002531
Iteration 214/1000 | Loss: 0.00002531
Iteration 215/1000 | Loss: 0.00002531
Iteration 216/1000 | Loss: 0.00002531
Iteration 217/1000 | Loss: 0.00002531
Iteration 218/1000 | Loss: 0.00002531
Iteration 219/1000 | Loss: 0.00002531
Iteration 220/1000 | Loss: 0.00002531
Iteration 221/1000 | Loss: 0.00002531
Iteration 222/1000 | Loss: 0.00002531
Iteration 223/1000 | Loss: 0.00002531
Iteration 224/1000 | Loss: 0.00002530
Iteration 225/1000 | Loss: 0.00002530
Iteration 226/1000 | Loss: 0.00002530
Iteration 227/1000 | Loss: 0.00002530
Iteration 228/1000 | Loss: 0.00002530
Iteration 229/1000 | Loss: 0.00002530
Iteration 230/1000 | Loss: 0.00002530
Iteration 231/1000 | Loss: 0.00002530
Iteration 232/1000 | Loss: 0.00002530
Iteration 233/1000 | Loss: 0.00002530
Iteration 234/1000 | Loss: 0.00002530
Iteration 235/1000 | Loss: 0.00002530
Iteration 236/1000 | Loss: 0.00002530
Iteration 237/1000 | Loss: 0.00002530
Iteration 238/1000 | Loss: 0.00002530
Iteration 239/1000 | Loss: 0.00002530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.530165875214152e-05, 2.530165875214152e-05, 2.530165875214152e-05, 2.530165875214152e-05, 2.530165875214152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.530165875214152e-05

Optimization complete. Final v2v error: 4.038763999938965 mm

Highest mean error: 5.208897590637207 mm for frame 183

Lowest mean error: 3.330366849899292 mm for frame 2

Saving results

Total time: 115.52617812156677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788391
Iteration 2/25 | Loss: 0.00145231
Iteration 3/25 | Loss: 0.00127922
Iteration 4/25 | Loss: 0.00124720
Iteration 5/25 | Loss: 0.00123774
Iteration 6/25 | Loss: 0.00123587
Iteration 7/25 | Loss: 0.00123551
Iteration 8/25 | Loss: 0.00123551
Iteration 9/25 | Loss: 0.00123551
Iteration 10/25 | Loss: 0.00123551
Iteration 11/25 | Loss: 0.00123551
Iteration 12/25 | Loss: 0.00123551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001235507195815444, 0.001235507195815444, 0.001235507195815444, 0.001235507195815444, 0.001235507195815444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001235507195815444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43112564
Iteration 2/25 | Loss: 0.00058005
Iteration 3/25 | Loss: 0.00058005
Iteration 4/25 | Loss: 0.00058005
Iteration 5/25 | Loss: 0.00058004
Iteration 6/25 | Loss: 0.00058004
Iteration 7/25 | Loss: 0.00058004
Iteration 8/25 | Loss: 0.00058004
Iteration 9/25 | Loss: 0.00058004
Iteration 10/25 | Loss: 0.00058004
Iteration 11/25 | Loss: 0.00058004
Iteration 12/25 | Loss: 0.00058004
Iteration 13/25 | Loss: 0.00058004
Iteration 14/25 | Loss: 0.00058004
Iteration 15/25 | Loss: 0.00058004
Iteration 16/25 | Loss: 0.00058004
Iteration 17/25 | Loss: 0.00058004
Iteration 18/25 | Loss: 0.00058004
Iteration 19/25 | Loss: 0.00058004
Iteration 20/25 | Loss: 0.00058004
Iteration 21/25 | Loss: 0.00058004
Iteration 22/25 | Loss: 0.00058004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005800430662930012, 0.0005800430662930012, 0.0005800430662930012, 0.0005800430662930012, 0.0005800430662930012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005800430662930012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058004
Iteration 2/1000 | Loss: 0.00005391
Iteration 3/1000 | Loss: 0.00003610
Iteration 4/1000 | Loss: 0.00002961
Iteration 5/1000 | Loss: 0.00002758
Iteration 6/1000 | Loss: 0.00002651
Iteration 7/1000 | Loss: 0.00002526
Iteration 8/1000 | Loss: 0.00002456
Iteration 9/1000 | Loss: 0.00002411
Iteration 10/1000 | Loss: 0.00002369
Iteration 11/1000 | Loss: 0.00002343
Iteration 12/1000 | Loss: 0.00002316
Iteration 13/1000 | Loss: 0.00002298
Iteration 14/1000 | Loss: 0.00002291
Iteration 15/1000 | Loss: 0.00002274
Iteration 16/1000 | Loss: 0.00002268
Iteration 17/1000 | Loss: 0.00002263
Iteration 18/1000 | Loss: 0.00002262
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002261
Iteration 21/1000 | Loss: 0.00002261
Iteration 22/1000 | Loss: 0.00002259
Iteration 23/1000 | Loss: 0.00002259
Iteration 24/1000 | Loss: 0.00002255
Iteration 25/1000 | Loss: 0.00002255
Iteration 26/1000 | Loss: 0.00002254
Iteration 27/1000 | Loss: 0.00002254
Iteration 28/1000 | Loss: 0.00002253
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002246
Iteration 31/1000 | Loss: 0.00002244
Iteration 32/1000 | Loss: 0.00002240
Iteration 33/1000 | Loss: 0.00002240
Iteration 34/1000 | Loss: 0.00002238
Iteration 35/1000 | Loss: 0.00002238
Iteration 36/1000 | Loss: 0.00002237
Iteration 37/1000 | Loss: 0.00002237
Iteration 38/1000 | Loss: 0.00002236
Iteration 39/1000 | Loss: 0.00002236
Iteration 40/1000 | Loss: 0.00002236
Iteration 41/1000 | Loss: 0.00002235
Iteration 42/1000 | Loss: 0.00002235
Iteration 43/1000 | Loss: 0.00002235
Iteration 44/1000 | Loss: 0.00002234
Iteration 45/1000 | Loss: 0.00002234
Iteration 46/1000 | Loss: 0.00002233
Iteration 47/1000 | Loss: 0.00002232
Iteration 48/1000 | Loss: 0.00002232
Iteration 49/1000 | Loss: 0.00002231
Iteration 50/1000 | Loss: 0.00002231
Iteration 51/1000 | Loss: 0.00002231
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002229
Iteration 61/1000 | Loss: 0.00002229
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002228
Iteration 64/1000 | Loss: 0.00002228
Iteration 65/1000 | Loss: 0.00002228
Iteration 66/1000 | Loss: 0.00002228
Iteration 67/1000 | Loss: 0.00002228
Iteration 68/1000 | Loss: 0.00002228
Iteration 69/1000 | Loss: 0.00002227
Iteration 70/1000 | Loss: 0.00002227
Iteration 71/1000 | Loss: 0.00002227
Iteration 72/1000 | Loss: 0.00002227
Iteration 73/1000 | Loss: 0.00002226
Iteration 74/1000 | Loss: 0.00002226
Iteration 75/1000 | Loss: 0.00002226
Iteration 76/1000 | Loss: 0.00002226
Iteration 77/1000 | Loss: 0.00002225
Iteration 78/1000 | Loss: 0.00002225
Iteration 79/1000 | Loss: 0.00002225
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00002225
Iteration 82/1000 | Loss: 0.00002225
Iteration 83/1000 | Loss: 0.00002225
Iteration 84/1000 | Loss: 0.00002225
Iteration 85/1000 | Loss: 0.00002225
Iteration 86/1000 | Loss: 0.00002224
Iteration 87/1000 | Loss: 0.00002224
Iteration 88/1000 | Loss: 0.00002224
Iteration 89/1000 | Loss: 0.00002224
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002223
Iteration 92/1000 | Loss: 0.00002223
Iteration 93/1000 | Loss: 0.00002223
Iteration 94/1000 | Loss: 0.00002223
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002222
Iteration 97/1000 | Loss: 0.00002222
Iteration 98/1000 | Loss: 0.00002222
Iteration 99/1000 | Loss: 0.00002222
Iteration 100/1000 | Loss: 0.00002222
Iteration 101/1000 | Loss: 0.00002222
Iteration 102/1000 | Loss: 0.00002222
Iteration 103/1000 | Loss: 0.00002221
Iteration 104/1000 | Loss: 0.00002221
Iteration 105/1000 | Loss: 0.00002221
Iteration 106/1000 | Loss: 0.00002221
Iteration 107/1000 | Loss: 0.00002221
Iteration 108/1000 | Loss: 0.00002221
Iteration 109/1000 | Loss: 0.00002221
Iteration 110/1000 | Loss: 0.00002221
Iteration 111/1000 | Loss: 0.00002221
Iteration 112/1000 | Loss: 0.00002221
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002221
Iteration 115/1000 | Loss: 0.00002221
Iteration 116/1000 | Loss: 0.00002221
Iteration 117/1000 | Loss: 0.00002221
Iteration 118/1000 | Loss: 0.00002221
Iteration 119/1000 | Loss: 0.00002221
Iteration 120/1000 | Loss: 0.00002221
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002220
Iteration 125/1000 | Loss: 0.00002220
Iteration 126/1000 | Loss: 0.00002220
Iteration 127/1000 | Loss: 0.00002220
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002219
Iteration 131/1000 | Loss: 0.00002219
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002219
Iteration 135/1000 | Loss: 0.00002219
Iteration 136/1000 | Loss: 0.00002219
Iteration 137/1000 | Loss: 0.00002219
Iteration 138/1000 | Loss: 0.00002219
Iteration 139/1000 | Loss: 0.00002219
Iteration 140/1000 | Loss: 0.00002219
Iteration 141/1000 | Loss: 0.00002219
Iteration 142/1000 | Loss: 0.00002218
Iteration 143/1000 | Loss: 0.00002218
Iteration 144/1000 | Loss: 0.00002218
Iteration 145/1000 | Loss: 0.00002218
Iteration 146/1000 | Loss: 0.00002218
Iteration 147/1000 | Loss: 0.00002218
Iteration 148/1000 | Loss: 0.00002218
Iteration 149/1000 | Loss: 0.00002218
Iteration 150/1000 | Loss: 0.00002218
Iteration 151/1000 | Loss: 0.00002218
Iteration 152/1000 | Loss: 0.00002218
Iteration 153/1000 | Loss: 0.00002218
Iteration 154/1000 | Loss: 0.00002218
Iteration 155/1000 | Loss: 0.00002218
Iteration 156/1000 | Loss: 0.00002218
Iteration 157/1000 | Loss: 0.00002218
Iteration 158/1000 | Loss: 0.00002217
Iteration 159/1000 | Loss: 0.00002217
Iteration 160/1000 | Loss: 0.00002217
Iteration 161/1000 | Loss: 0.00002217
Iteration 162/1000 | Loss: 0.00002217
Iteration 163/1000 | Loss: 0.00002217
Iteration 164/1000 | Loss: 0.00002217
Iteration 165/1000 | Loss: 0.00002217
Iteration 166/1000 | Loss: 0.00002217
Iteration 167/1000 | Loss: 0.00002217
Iteration 168/1000 | Loss: 0.00002217
Iteration 169/1000 | Loss: 0.00002217
Iteration 170/1000 | Loss: 0.00002217
Iteration 171/1000 | Loss: 0.00002217
Iteration 172/1000 | Loss: 0.00002217
Iteration 173/1000 | Loss: 0.00002217
Iteration 174/1000 | Loss: 0.00002217
Iteration 175/1000 | Loss: 0.00002217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.2168893337948248e-05, 2.2168893337948248e-05, 2.2168893337948248e-05, 2.2168893337948248e-05, 2.2168893337948248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2168893337948248e-05

Optimization complete. Final v2v error: 3.9287898540496826 mm

Highest mean error: 4.414224147796631 mm for frame 91

Lowest mean error: 3.2731075286865234 mm for frame 131

Saving results

Total time: 42.11099815368652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062481
Iteration 2/25 | Loss: 0.00188033
Iteration 3/25 | Loss: 0.00148417
Iteration 4/25 | Loss: 0.00143110
Iteration 5/25 | Loss: 0.00142135
Iteration 6/25 | Loss: 0.00138807
Iteration 7/25 | Loss: 0.00137360
Iteration 8/25 | Loss: 0.00135896
Iteration 9/25 | Loss: 0.00137169
Iteration 10/25 | Loss: 0.00136024
Iteration 11/25 | Loss: 0.00136852
Iteration 12/25 | Loss: 0.00135310
Iteration 13/25 | Loss: 0.00135342
Iteration 14/25 | Loss: 0.00136115
Iteration 15/25 | Loss: 0.00135654
Iteration 16/25 | Loss: 0.00134842
Iteration 17/25 | Loss: 0.00134817
Iteration 18/25 | Loss: 0.00134807
Iteration 19/25 | Loss: 0.00134807
Iteration 20/25 | Loss: 0.00134806
Iteration 21/25 | Loss: 0.00134806
Iteration 22/25 | Loss: 0.00134806
Iteration 23/25 | Loss: 0.00134806
Iteration 24/25 | Loss: 0.00134806
Iteration 25/25 | Loss: 0.00134806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.16022110
Iteration 2/25 | Loss: 0.00094954
Iteration 3/25 | Loss: 0.00094954
Iteration 4/25 | Loss: 0.00094954
Iteration 5/25 | Loss: 0.00094954
Iteration 6/25 | Loss: 0.00094954
Iteration 7/25 | Loss: 0.00094954
Iteration 8/25 | Loss: 0.00094954
Iteration 9/25 | Loss: 0.00094954
Iteration 10/25 | Loss: 0.00094954
Iteration 11/25 | Loss: 0.00094954
Iteration 12/25 | Loss: 0.00094954
Iteration 13/25 | Loss: 0.00094953
Iteration 14/25 | Loss: 0.00094953
Iteration 15/25 | Loss: 0.00094953
Iteration 16/25 | Loss: 0.00094953
Iteration 17/25 | Loss: 0.00094953
Iteration 18/25 | Loss: 0.00094953
Iteration 19/25 | Loss: 0.00094953
Iteration 20/25 | Loss: 0.00094953
Iteration 21/25 | Loss: 0.00094953
Iteration 22/25 | Loss: 0.00094953
Iteration 23/25 | Loss: 0.00094953
Iteration 24/25 | Loss: 0.00094953
Iteration 25/25 | Loss: 0.00094953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094953
Iteration 2/1000 | Loss: 0.00009290
Iteration 3/1000 | Loss: 0.00006411
Iteration 4/1000 | Loss: 0.00005263
Iteration 5/1000 | Loss: 0.00004865
Iteration 6/1000 | Loss: 0.00004664
Iteration 7/1000 | Loss: 0.00004510
Iteration 8/1000 | Loss: 0.00004398
Iteration 9/1000 | Loss: 0.00004305
Iteration 10/1000 | Loss: 0.00004237
Iteration 11/1000 | Loss: 0.00004186
Iteration 12/1000 | Loss: 0.00004152
Iteration 13/1000 | Loss: 0.00004122
Iteration 14/1000 | Loss: 0.00004093
Iteration 15/1000 | Loss: 0.00004068
Iteration 16/1000 | Loss: 0.00004048
Iteration 17/1000 | Loss: 0.00004030
Iteration 18/1000 | Loss: 0.00004026
Iteration 19/1000 | Loss: 0.00004020
Iteration 20/1000 | Loss: 0.00004014
Iteration 21/1000 | Loss: 0.00004009
Iteration 22/1000 | Loss: 0.00004009
Iteration 23/1000 | Loss: 0.00004008
Iteration 24/1000 | Loss: 0.00004004
Iteration 25/1000 | Loss: 0.00004003
Iteration 26/1000 | Loss: 0.00004001
Iteration 27/1000 | Loss: 0.00003997
Iteration 28/1000 | Loss: 0.00003996
Iteration 29/1000 | Loss: 0.00003996
Iteration 30/1000 | Loss: 0.00003995
Iteration 31/1000 | Loss: 0.00003995
Iteration 32/1000 | Loss: 0.00003993
Iteration 33/1000 | Loss: 0.00003993
Iteration 34/1000 | Loss: 0.00003990
Iteration 35/1000 | Loss: 0.00003986
Iteration 36/1000 | Loss: 0.00003985
Iteration 37/1000 | Loss: 0.00003981
Iteration 38/1000 | Loss: 0.00003981
Iteration 39/1000 | Loss: 0.00003980
Iteration 40/1000 | Loss: 0.00003977
Iteration 41/1000 | Loss: 0.00003976
Iteration 42/1000 | Loss: 0.00003976
Iteration 43/1000 | Loss: 0.00003976
Iteration 44/1000 | Loss: 0.00003974
Iteration 45/1000 | Loss: 0.00003974
Iteration 46/1000 | Loss: 0.00003974
Iteration 47/1000 | Loss: 0.00003973
Iteration 48/1000 | Loss: 0.00003973
Iteration 49/1000 | Loss: 0.00003973
Iteration 50/1000 | Loss: 0.00003973
Iteration 51/1000 | Loss: 0.00003973
Iteration 52/1000 | Loss: 0.00003973
Iteration 53/1000 | Loss: 0.00003970
Iteration 54/1000 | Loss: 0.00003970
Iteration 55/1000 | Loss: 0.00003969
Iteration 56/1000 | Loss: 0.00003968
Iteration 57/1000 | Loss: 0.00003968
Iteration 58/1000 | Loss: 0.00003967
Iteration 59/1000 | Loss: 0.00003967
Iteration 60/1000 | Loss: 0.00003966
Iteration 61/1000 | Loss: 0.00003966
Iteration 62/1000 | Loss: 0.00003966
Iteration 63/1000 | Loss: 0.00003966
Iteration 64/1000 | Loss: 0.00003966
Iteration 65/1000 | Loss: 0.00003965
Iteration 66/1000 | Loss: 0.00003965
Iteration 67/1000 | Loss: 0.00003965
Iteration 68/1000 | Loss: 0.00003964
Iteration 69/1000 | Loss: 0.00003963
Iteration 70/1000 | Loss: 0.00003962
Iteration 71/1000 | Loss: 0.00003962
Iteration 72/1000 | Loss: 0.00003962
Iteration 73/1000 | Loss: 0.00003961
Iteration 74/1000 | Loss: 0.00003961
Iteration 75/1000 | Loss: 0.00003961
Iteration 76/1000 | Loss: 0.00003960
Iteration 77/1000 | Loss: 0.00003960
Iteration 78/1000 | Loss: 0.00003960
Iteration 79/1000 | Loss: 0.00003958
Iteration 80/1000 | Loss: 0.00003958
Iteration 81/1000 | Loss: 0.00003958
Iteration 82/1000 | Loss: 0.00003958
Iteration 83/1000 | Loss: 0.00003958
Iteration 84/1000 | Loss: 0.00003958
Iteration 85/1000 | Loss: 0.00003958
Iteration 86/1000 | Loss: 0.00003958
Iteration 87/1000 | Loss: 0.00003957
Iteration 88/1000 | Loss: 0.00003957
Iteration 89/1000 | Loss: 0.00003957
Iteration 90/1000 | Loss: 0.00003957
Iteration 91/1000 | Loss: 0.00003957
Iteration 92/1000 | Loss: 0.00003957
Iteration 93/1000 | Loss: 0.00003957
Iteration 94/1000 | Loss: 0.00003957
Iteration 95/1000 | Loss: 0.00003957
Iteration 96/1000 | Loss: 0.00003956
Iteration 97/1000 | Loss: 0.00003955
Iteration 98/1000 | Loss: 0.00003955
Iteration 99/1000 | Loss: 0.00003954
Iteration 100/1000 | Loss: 0.00003954
Iteration 101/1000 | Loss: 0.00003954
Iteration 102/1000 | Loss: 0.00003953
Iteration 103/1000 | Loss: 0.00003953
Iteration 104/1000 | Loss: 0.00003953
Iteration 105/1000 | Loss: 0.00003953
Iteration 106/1000 | Loss: 0.00003952
Iteration 107/1000 | Loss: 0.00003952
Iteration 108/1000 | Loss: 0.00003952
Iteration 109/1000 | Loss: 0.00003952
Iteration 110/1000 | Loss: 0.00003951
Iteration 111/1000 | Loss: 0.00003951
Iteration 112/1000 | Loss: 0.00003951
Iteration 113/1000 | Loss: 0.00003950
Iteration 114/1000 | Loss: 0.00003950
Iteration 115/1000 | Loss: 0.00003950
Iteration 116/1000 | Loss: 0.00003950
Iteration 117/1000 | Loss: 0.00003950
Iteration 118/1000 | Loss: 0.00003949
Iteration 119/1000 | Loss: 0.00003949
Iteration 120/1000 | Loss: 0.00003949
Iteration 121/1000 | Loss: 0.00003949
Iteration 122/1000 | Loss: 0.00003948
Iteration 123/1000 | Loss: 0.00003948
Iteration 124/1000 | Loss: 0.00003948
Iteration 125/1000 | Loss: 0.00003948
Iteration 126/1000 | Loss: 0.00003948
Iteration 127/1000 | Loss: 0.00003948
Iteration 128/1000 | Loss: 0.00003947
Iteration 129/1000 | Loss: 0.00003947
Iteration 130/1000 | Loss: 0.00003947
Iteration 131/1000 | Loss: 0.00003947
Iteration 132/1000 | Loss: 0.00003947
Iteration 133/1000 | Loss: 0.00003947
Iteration 134/1000 | Loss: 0.00003947
Iteration 135/1000 | Loss: 0.00003946
Iteration 136/1000 | Loss: 0.00003946
Iteration 137/1000 | Loss: 0.00003946
Iteration 138/1000 | Loss: 0.00003946
Iteration 139/1000 | Loss: 0.00003946
Iteration 140/1000 | Loss: 0.00003946
Iteration 141/1000 | Loss: 0.00003945
Iteration 142/1000 | Loss: 0.00003945
Iteration 143/1000 | Loss: 0.00003945
Iteration 144/1000 | Loss: 0.00003945
Iteration 145/1000 | Loss: 0.00003945
Iteration 146/1000 | Loss: 0.00003944
Iteration 147/1000 | Loss: 0.00003944
Iteration 148/1000 | Loss: 0.00003944
Iteration 149/1000 | Loss: 0.00003943
Iteration 150/1000 | Loss: 0.00003943
Iteration 151/1000 | Loss: 0.00003943
Iteration 152/1000 | Loss: 0.00003943
Iteration 153/1000 | Loss: 0.00003942
Iteration 154/1000 | Loss: 0.00003942
Iteration 155/1000 | Loss: 0.00003942
Iteration 156/1000 | Loss: 0.00003942
Iteration 157/1000 | Loss: 0.00003942
Iteration 158/1000 | Loss: 0.00003942
Iteration 159/1000 | Loss: 0.00003942
Iteration 160/1000 | Loss: 0.00003941
Iteration 161/1000 | Loss: 0.00003941
Iteration 162/1000 | Loss: 0.00003941
Iteration 163/1000 | Loss: 0.00003941
Iteration 164/1000 | Loss: 0.00003941
Iteration 165/1000 | Loss: 0.00003941
Iteration 166/1000 | Loss: 0.00003940
Iteration 167/1000 | Loss: 0.00003940
Iteration 168/1000 | Loss: 0.00003940
Iteration 169/1000 | Loss: 0.00003940
Iteration 170/1000 | Loss: 0.00003940
Iteration 171/1000 | Loss: 0.00003939
Iteration 172/1000 | Loss: 0.00003939
Iteration 173/1000 | Loss: 0.00003939
Iteration 174/1000 | Loss: 0.00003938
Iteration 175/1000 | Loss: 0.00003938
Iteration 176/1000 | Loss: 0.00003938
Iteration 177/1000 | Loss: 0.00003938
Iteration 178/1000 | Loss: 0.00003938
Iteration 179/1000 | Loss: 0.00003937
Iteration 180/1000 | Loss: 0.00003937
Iteration 181/1000 | Loss: 0.00003937
Iteration 182/1000 | Loss: 0.00003937
Iteration 183/1000 | Loss: 0.00003936
Iteration 184/1000 | Loss: 0.00003936
Iteration 185/1000 | Loss: 0.00003936
Iteration 186/1000 | Loss: 0.00003936
Iteration 187/1000 | Loss: 0.00003936
Iteration 188/1000 | Loss: 0.00003936
Iteration 189/1000 | Loss: 0.00003935
Iteration 190/1000 | Loss: 0.00003935
Iteration 191/1000 | Loss: 0.00003935
Iteration 192/1000 | Loss: 0.00003935
Iteration 193/1000 | Loss: 0.00003935
Iteration 194/1000 | Loss: 0.00003935
Iteration 195/1000 | Loss: 0.00003935
Iteration 196/1000 | Loss: 0.00003935
Iteration 197/1000 | Loss: 0.00003935
Iteration 198/1000 | Loss: 0.00003935
Iteration 199/1000 | Loss: 0.00003935
Iteration 200/1000 | Loss: 0.00003934
Iteration 201/1000 | Loss: 0.00003934
Iteration 202/1000 | Loss: 0.00003934
Iteration 203/1000 | Loss: 0.00003934
Iteration 204/1000 | Loss: 0.00003934
Iteration 205/1000 | Loss: 0.00003934
Iteration 206/1000 | Loss: 0.00003934
Iteration 207/1000 | Loss: 0.00003934
Iteration 208/1000 | Loss: 0.00003934
Iteration 209/1000 | Loss: 0.00003933
Iteration 210/1000 | Loss: 0.00003933
Iteration 211/1000 | Loss: 0.00003933
Iteration 212/1000 | Loss: 0.00003933
Iteration 213/1000 | Loss: 0.00003933
Iteration 214/1000 | Loss: 0.00003933
Iteration 215/1000 | Loss: 0.00003933
Iteration 216/1000 | Loss: 0.00003933
Iteration 217/1000 | Loss: 0.00003933
Iteration 218/1000 | Loss: 0.00003933
Iteration 219/1000 | Loss: 0.00003933
Iteration 220/1000 | Loss: 0.00003933
Iteration 221/1000 | Loss: 0.00003933
Iteration 222/1000 | Loss: 0.00003933
Iteration 223/1000 | Loss: 0.00003933
Iteration 224/1000 | Loss: 0.00003933
Iteration 225/1000 | Loss: 0.00003933
Iteration 226/1000 | Loss: 0.00003932
Iteration 227/1000 | Loss: 0.00003932
Iteration 228/1000 | Loss: 0.00003932
Iteration 229/1000 | Loss: 0.00003932
Iteration 230/1000 | Loss: 0.00003932
Iteration 231/1000 | Loss: 0.00003932
Iteration 232/1000 | Loss: 0.00003932
Iteration 233/1000 | Loss: 0.00003932
Iteration 234/1000 | Loss: 0.00003932
Iteration 235/1000 | Loss: 0.00003932
Iteration 236/1000 | Loss: 0.00003932
Iteration 237/1000 | Loss: 0.00003932
Iteration 238/1000 | Loss: 0.00003932
Iteration 239/1000 | Loss: 0.00003932
Iteration 240/1000 | Loss: 0.00003932
Iteration 241/1000 | Loss: 0.00003932
Iteration 242/1000 | Loss: 0.00003932
Iteration 243/1000 | Loss: 0.00003932
Iteration 244/1000 | Loss: 0.00003932
Iteration 245/1000 | Loss: 0.00003932
Iteration 246/1000 | Loss: 0.00003932
Iteration 247/1000 | Loss: 0.00003932
Iteration 248/1000 | Loss: 0.00003932
Iteration 249/1000 | Loss: 0.00003932
Iteration 250/1000 | Loss: 0.00003932
Iteration 251/1000 | Loss: 0.00003932
Iteration 252/1000 | Loss: 0.00003932
Iteration 253/1000 | Loss: 0.00003932
Iteration 254/1000 | Loss: 0.00003932
Iteration 255/1000 | Loss: 0.00003932
Iteration 256/1000 | Loss: 0.00003932
Iteration 257/1000 | Loss: 0.00003932
Iteration 258/1000 | Loss: 0.00003932
Iteration 259/1000 | Loss: 0.00003932
Iteration 260/1000 | Loss: 0.00003932
Iteration 261/1000 | Loss: 0.00003932
Iteration 262/1000 | Loss: 0.00003932
Iteration 263/1000 | Loss: 0.00003932
Iteration 264/1000 | Loss: 0.00003932
Iteration 265/1000 | Loss: 0.00003932
Iteration 266/1000 | Loss: 0.00003932
Iteration 267/1000 | Loss: 0.00003932
Iteration 268/1000 | Loss: 0.00003932
Iteration 269/1000 | Loss: 0.00003932
Iteration 270/1000 | Loss: 0.00003932
Iteration 271/1000 | Loss: 0.00003932
Iteration 272/1000 | Loss: 0.00003932
Iteration 273/1000 | Loss: 0.00003932
Iteration 274/1000 | Loss: 0.00003932
Iteration 275/1000 | Loss: 0.00003932
Iteration 276/1000 | Loss: 0.00003932
Iteration 277/1000 | Loss: 0.00003932
Iteration 278/1000 | Loss: 0.00003932
Iteration 279/1000 | Loss: 0.00003932
Iteration 280/1000 | Loss: 0.00003932
Iteration 281/1000 | Loss: 0.00003932
Iteration 282/1000 | Loss: 0.00003932
Iteration 283/1000 | Loss: 0.00003932
Iteration 284/1000 | Loss: 0.00003932
Iteration 285/1000 | Loss: 0.00003932
Iteration 286/1000 | Loss: 0.00003932
Iteration 287/1000 | Loss: 0.00003932
Iteration 288/1000 | Loss: 0.00003932
Iteration 289/1000 | Loss: 0.00003932
Iteration 290/1000 | Loss: 0.00003932
Iteration 291/1000 | Loss: 0.00003932
Iteration 292/1000 | Loss: 0.00003932
Iteration 293/1000 | Loss: 0.00003932
Iteration 294/1000 | Loss: 0.00003932
Iteration 295/1000 | Loss: 0.00003932
Iteration 296/1000 | Loss: 0.00003932
Iteration 297/1000 | Loss: 0.00003932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [3.932223989977501e-05, 3.932223989977501e-05, 3.932223989977501e-05, 3.932223989977501e-05, 3.932223989977501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.932223989977501e-05

Optimization complete. Final v2v error: 5.025503158569336 mm

Highest mean error: 7.541873455047607 mm for frame 99

Lowest mean error: 3.606321096420288 mm for frame 139

Saving results

Total time: 76.16717886924744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796615
Iteration 2/25 | Loss: 0.00131202
Iteration 3/25 | Loss: 0.00122959
Iteration 4/25 | Loss: 0.00121895
Iteration 5/25 | Loss: 0.00121763
Iteration 6/25 | Loss: 0.00121763
Iteration 7/25 | Loss: 0.00121763
Iteration 8/25 | Loss: 0.00121763
Iteration 9/25 | Loss: 0.00121763
Iteration 10/25 | Loss: 0.00121763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012176326708868146, 0.0012176326708868146, 0.0012176326708868146, 0.0012176326708868146, 0.0012176326708868146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012176326708868146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47200394
Iteration 2/25 | Loss: 0.00071101
Iteration 3/25 | Loss: 0.00071100
Iteration 4/25 | Loss: 0.00071100
Iteration 5/25 | Loss: 0.00071100
Iteration 6/25 | Loss: 0.00071100
Iteration 7/25 | Loss: 0.00071100
Iteration 8/25 | Loss: 0.00071100
Iteration 9/25 | Loss: 0.00071100
Iteration 10/25 | Loss: 0.00071100
Iteration 11/25 | Loss: 0.00071100
Iteration 12/25 | Loss: 0.00071100
Iteration 13/25 | Loss: 0.00071100
Iteration 14/25 | Loss: 0.00071100
Iteration 15/25 | Loss: 0.00071100
Iteration 16/25 | Loss: 0.00071100
Iteration 17/25 | Loss: 0.00071100
Iteration 18/25 | Loss: 0.00071100
Iteration 19/25 | Loss: 0.00071100
Iteration 20/25 | Loss: 0.00071100
Iteration 21/25 | Loss: 0.00071100
Iteration 22/25 | Loss: 0.00071100
Iteration 23/25 | Loss: 0.00071100
Iteration 24/25 | Loss: 0.00071100
Iteration 25/25 | Loss: 0.00071100

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071100
Iteration 2/1000 | Loss: 0.00002595
Iteration 3/1000 | Loss: 0.00001885
Iteration 4/1000 | Loss: 0.00001616
Iteration 5/1000 | Loss: 0.00001482
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001320
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001275
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001266
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001237
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001212
Iteration 121/1000 | Loss: 0.00001212
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001210
Iteration 131/1000 | Loss: 0.00001210
Iteration 132/1000 | Loss: 0.00001209
Iteration 133/1000 | Loss: 0.00001209
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001206
Iteration 147/1000 | Loss: 0.00001206
Iteration 148/1000 | Loss: 0.00001206
Iteration 149/1000 | Loss: 0.00001206
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001205
Iteration 160/1000 | Loss: 0.00001205
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001204
Iteration 169/1000 | Loss: 0.00001204
Iteration 170/1000 | Loss: 0.00001203
Iteration 171/1000 | Loss: 0.00001203
Iteration 172/1000 | Loss: 0.00001203
Iteration 173/1000 | Loss: 0.00001203
Iteration 174/1000 | Loss: 0.00001203
Iteration 175/1000 | Loss: 0.00001203
Iteration 176/1000 | Loss: 0.00001203
Iteration 177/1000 | Loss: 0.00001202
Iteration 178/1000 | Loss: 0.00001202
Iteration 179/1000 | Loss: 0.00001202
Iteration 180/1000 | Loss: 0.00001202
Iteration 181/1000 | Loss: 0.00001202
Iteration 182/1000 | Loss: 0.00001202
Iteration 183/1000 | Loss: 0.00001202
Iteration 184/1000 | Loss: 0.00001202
Iteration 185/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2022623195662163e-05, 1.2022623195662163e-05, 1.2022623195662163e-05, 1.2022623195662163e-05, 1.2022623195662163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2022623195662163e-05

Optimization complete. Final v2v error: 2.950011730194092 mm

Highest mean error: 3.1419456005096436 mm for frame 42

Lowest mean error: 2.8128254413604736 mm for frame 14

Saving results

Total time: 36.64286398887634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053079
Iteration 2/25 | Loss: 0.00294216
Iteration 3/25 | Loss: 0.00198467
Iteration 4/25 | Loss: 0.00183257
Iteration 5/25 | Loss: 0.00181865
Iteration 6/25 | Loss: 0.00174359
Iteration 7/25 | Loss: 0.00171366
Iteration 8/25 | Loss: 0.00171104
Iteration 9/25 | Loss: 0.00169180
Iteration 10/25 | Loss: 0.00169320
Iteration 11/25 | Loss: 0.00169136
Iteration 12/25 | Loss: 0.00170146
Iteration 13/25 | Loss: 0.00168046
Iteration 14/25 | Loss: 0.00166422
Iteration 15/25 | Loss: 0.00166047
Iteration 16/25 | Loss: 0.00165950
Iteration 17/25 | Loss: 0.00165906
Iteration 18/25 | Loss: 0.00165879
Iteration 19/25 | Loss: 0.00165863
Iteration 20/25 | Loss: 0.00165847
Iteration 21/25 | Loss: 0.00165821
Iteration 22/25 | Loss: 0.00165794
Iteration 23/25 | Loss: 0.00165771
Iteration 24/25 | Loss: 0.00165754
Iteration 25/25 | Loss: 0.00165737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23052025
Iteration 2/25 | Loss: 0.00422115
Iteration 3/25 | Loss: 0.00422115
Iteration 4/25 | Loss: 0.00422115
Iteration 5/25 | Loss: 0.00422114
Iteration 6/25 | Loss: 0.00422114
Iteration 7/25 | Loss: 0.00422114
Iteration 8/25 | Loss: 0.00422114
Iteration 9/25 | Loss: 0.00422114
Iteration 10/25 | Loss: 0.00422114
Iteration 11/25 | Loss: 0.00422114
Iteration 12/25 | Loss: 0.00422114
Iteration 13/25 | Loss: 0.00422114
Iteration 14/25 | Loss: 0.00422114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004221142269670963, 0.004221142269670963, 0.004221142269670963, 0.004221142269670963, 0.004221142269670963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004221142269670963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00422114
Iteration 2/1000 | Loss: 0.00293814
Iteration 3/1000 | Loss: 0.00339495
Iteration 4/1000 | Loss: 0.00222832
Iteration 5/1000 | Loss: 0.00087987
Iteration 6/1000 | Loss: 0.00030436
Iteration 7/1000 | Loss: 0.00300700
Iteration 8/1000 | Loss: 0.00158097
Iteration 9/1000 | Loss: 0.00095172
Iteration 10/1000 | Loss: 0.00085333
Iteration 11/1000 | Loss: 0.00041382
Iteration 12/1000 | Loss: 0.00177632
Iteration 13/1000 | Loss: 0.00025205
Iteration 14/1000 | Loss: 0.00164558
Iteration 15/1000 | Loss: 0.00184184
Iteration 16/1000 | Loss: 0.00055866
Iteration 17/1000 | Loss: 0.00067193
Iteration 18/1000 | Loss: 0.00056411
Iteration 19/1000 | Loss: 0.00038973
Iteration 20/1000 | Loss: 0.00038868
Iteration 21/1000 | Loss: 0.00028987
Iteration 22/1000 | Loss: 0.00212220
Iteration 23/1000 | Loss: 0.00297330
Iteration 24/1000 | Loss: 0.00043360
Iteration 25/1000 | Loss: 0.00012926
Iteration 26/1000 | Loss: 0.00028425
Iteration 27/1000 | Loss: 0.00011463
Iteration 28/1000 | Loss: 0.00153690
Iteration 29/1000 | Loss: 0.00069346
Iteration 30/1000 | Loss: 0.00011179
Iteration 31/1000 | Loss: 0.00010416
Iteration 32/1000 | Loss: 0.00216824
Iteration 33/1000 | Loss: 0.00028252
Iteration 34/1000 | Loss: 0.00048586
Iteration 35/1000 | Loss: 0.00031722
Iteration 36/1000 | Loss: 0.00031372
Iteration 37/1000 | Loss: 0.00105651
Iteration 38/1000 | Loss: 0.00319919
Iteration 39/1000 | Loss: 0.00113995
Iteration 40/1000 | Loss: 0.00270895
Iteration 41/1000 | Loss: 0.00132349
Iteration 42/1000 | Loss: 0.00518156
Iteration 43/1000 | Loss: 0.00019499
Iteration 44/1000 | Loss: 0.00046608
Iteration 45/1000 | Loss: 0.00090698
Iteration 46/1000 | Loss: 0.00013842
Iteration 47/1000 | Loss: 0.00010933
Iteration 48/1000 | Loss: 0.00008957
Iteration 49/1000 | Loss: 0.00007542
Iteration 50/1000 | Loss: 0.00006559
Iteration 51/1000 | Loss: 0.00091380
Iteration 52/1000 | Loss: 0.00040281
Iteration 53/1000 | Loss: 0.00078752
Iteration 54/1000 | Loss: 0.00018553
Iteration 55/1000 | Loss: 0.00005442
Iteration 56/1000 | Loss: 0.00020975
Iteration 57/1000 | Loss: 0.00005745
Iteration 58/1000 | Loss: 0.00045143
Iteration 59/1000 | Loss: 0.00009250
Iteration 60/1000 | Loss: 0.00076342
Iteration 61/1000 | Loss: 0.00048194
Iteration 62/1000 | Loss: 0.00005067
Iteration 63/1000 | Loss: 0.00004361
Iteration 64/1000 | Loss: 0.00090795
Iteration 65/1000 | Loss: 0.00046940
Iteration 66/1000 | Loss: 0.00112734
Iteration 67/1000 | Loss: 0.00042646
Iteration 68/1000 | Loss: 0.00176235
Iteration 69/1000 | Loss: 0.00139326
Iteration 70/1000 | Loss: 0.00018759
Iteration 71/1000 | Loss: 0.00004573
Iteration 72/1000 | Loss: 0.00107128
Iteration 73/1000 | Loss: 0.00031595
Iteration 74/1000 | Loss: 0.00087617
Iteration 75/1000 | Loss: 0.00022368
Iteration 76/1000 | Loss: 0.00067024
Iteration 77/1000 | Loss: 0.00020280
Iteration 78/1000 | Loss: 0.00025291
Iteration 79/1000 | Loss: 0.00004796
Iteration 80/1000 | Loss: 0.00004171
Iteration 81/1000 | Loss: 0.00083713
Iteration 82/1000 | Loss: 0.00006623
Iteration 83/1000 | Loss: 0.00005224
Iteration 84/1000 | Loss: 0.00004746
Iteration 85/1000 | Loss: 0.00102752
Iteration 86/1000 | Loss: 0.00097053
Iteration 87/1000 | Loss: 0.00033764
Iteration 88/1000 | Loss: 0.00048033
Iteration 89/1000 | Loss: 0.00018371
Iteration 90/1000 | Loss: 0.00009957
Iteration 91/1000 | Loss: 0.00094842
Iteration 92/1000 | Loss: 0.00036193
Iteration 93/1000 | Loss: 0.00009488
Iteration 94/1000 | Loss: 0.00027837
Iteration 95/1000 | Loss: 0.00027811
Iteration 96/1000 | Loss: 0.00026694
Iteration 97/1000 | Loss: 0.00143638
Iteration 98/1000 | Loss: 0.00005971
Iteration 99/1000 | Loss: 0.00004262
Iteration 100/1000 | Loss: 0.00003985
Iteration 101/1000 | Loss: 0.00003830
Iteration 102/1000 | Loss: 0.00003743
Iteration 103/1000 | Loss: 0.00003658
Iteration 104/1000 | Loss: 0.00111657
Iteration 105/1000 | Loss: 0.00004964
Iteration 106/1000 | Loss: 0.00003783
Iteration 107/1000 | Loss: 0.00003467
Iteration 108/1000 | Loss: 0.00003339
Iteration 109/1000 | Loss: 0.00003283
Iteration 110/1000 | Loss: 0.00003229
Iteration 111/1000 | Loss: 0.00003181
Iteration 112/1000 | Loss: 0.00003135
Iteration 113/1000 | Loss: 0.00003113
Iteration 114/1000 | Loss: 0.00003105
Iteration 115/1000 | Loss: 0.00019067
Iteration 116/1000 | Loss: 0.00011018
Iteration 117/1000 | Loss: 0.00015004
Iteration 118/1000 | Loss: 0.00084337
Iteration 119/1000 | Loss: 0.00021179
Iteration 120/1000 | Loss: 0.00003262
Iteration 121/1000 | Loss: 0.00003116
Iteration 122/1000 | Loss: 0.00003014
Iteration 123/1000 | Loss: 0.00002948
Iteration 124/1000 | Loss: 0.00002884
Iteration 125/1000 | Loss: 0.00002845
Iteration 126/1000 | Loss: 0.00002816
Iteration 127/1000 | Loss: 0.00002806
Iteration 128/1000 | Loss: 0.00002805
Iteration 129/1000 | Loss: 0.00002801
Iteration 130/1000 | Loss: 0.00002799
Iteration 131/1000 | Loss: 0.00002799
Iteration 132/1000 | Loss: 0.00002799
Iteration 133/1000 | Loss: 0.00002798
Iteration 134/1000 | Loss: 0.00002798
Iteration 135/1000 | Loss: 0.00002798
Iteration 136/1000 | Loss: 0.00002797
Iteration 137/1000 | Loss: 0.00002797
Iteration 138/1000 | Loss: 0.00002797
Iteration 139/1000 | Loss: 0.00002797
Iteration 140/1000 | Loss: 0.00002797
Iteration 141/1000 | Loss: 0.00002797
Iteration 142/1000 | Loss: 0.00002797
Iteration 143/1000 | Loss: 0.00002797
Iteration 144/1000 | Loss: 0.00002795
Iteration 145/1000 | Loss: 0.00002795
Iteration 146/1000 | Loss: 0.00002794
Iteration 147/1000 | Loss: 0.00002794
Iteration 148/1000 | Loss: 0.00002794
Iteration 149/1000 | Loss: 0.00002794
Iteration 150/1000 | Loss: 0.00002794
Iteration 151/1000 | Loss: 0.00002794
Iteration 152/1000 | Loss: 0.00002793
Iteration 153/1000 | Loss: 0.00002793
Iteration 154/1000 | Loss: 0.00002793
Iteration 155/1000 | Loss: 0.00002793
Iteration 156/1000 | Loss: 0.00002792
Iteration 157/1000 | Loss: 0.00002792
Iteration 158/1000 | Loss: 0.00002792
Iteration 159/1000 | Loss: 0.00002792
Iteration 160/1000 | Loss: 0.00002792
Iteration 161/1000 | Loss: 0.00002792
Iteration 162/1000 | Loss: 0.00002791
Iteration 163/1000 | Loss: 0.00002791
Iteration 164/1000 | Loss: 0.00002791
Iteration 165/1000 | Loss: 0.00002791
Iteration 166/1000 | Loss: 0.00002791
Iteration 167/1000 | Loss: 0.00002790
Iteration 168/1000 | Loss: 0.00002790
Iteration 169/1000 | Loss: 0.00002790
Iteration 170/1000 | Loss: 0.00002789
Iteration 171/1000 | Loss: 0.00002789
Iteration 172/1000 | Loss: 0.00002789
Iteration 173/1000 | Loss: 0.00002789
Iteration 174/1000 | Loss: 0.00002789
Iteration 175/1000 | Loss: 0.00002789
Iteration 176/1000 | Loss: 0.00002789
Iteration 177/1000 | Loss: 0.00002788
Iteration 178/1000 | Loss: 0.00002788
Iteration 179/1000 | Loss: 0.00002788
Iteration 180/1000 | Loss: 0.00002788
Iteration 181/1000 | Loss: 0.00002788
Iteration 182/1000 | Loss: 0.00002788
Iteration 183/1000 | Loss: 0.00002788
Iteration 184/1000 | Loss: 0.00002788
Iteration 185/1000 | Loss: 0.00002788
Iteration 186/1000 | Loss: 0.00002788
Iteration 187/1000 | Loss: 0.00002787
Iteration 188/1000 | Loss: 0.00002787
Iteration 189/1000 | Loss: 0.00002787
Iteration 190/1000 | Loss: 0.00002787
Iteration 191/1000 | Loss: 0.00002787
Iteration 192/1000 | Loss: 0.00002787
Iteration 193/1000 | Loss: 0.00002787
Iteration 194/1000 | Loss: 0.00002787
Iteration 195/1000 | Loss: 0.00002787
Iteration 196/1000 | Loss: 0.00002787
Iteration 197/1000 | Loss: 0.00002787
Iteration 198/1000 | Loss: 0.00002787
Iteration 199/1000 | Loss: 0.00002787
Iteration 200/1000 | Loss: 0.00002787
Iteration 201/1000 | Loss: 0.00002787
Iteration 202/1000 | Loss: 0.00002787
Iteration 203/1000 | Loss: 0.00002787
Iteration 204/1000 | Loss: 0.00002787
Iteration 205/1000 | Loss: 0.00002787
Iteration 206/1000 | Loss: 0.00002787
Iteration 207/1000 | Loss: 0.00002787
Iteration 208/1000 | Loss: 0.00002787
Iteration 209/1000 | Loss: 0.00002787
Iteration 210/1000 | Loss: 0.00002786
Iteration 211/1000 | Loss: 0.00002786
Iteration 212/1000 | Loss: 0.00002786
Iteration 213/1000 | Loss: 0.00002786
Iteration 214/1000 | Loss: 0.00002786
Iteration 215/1000 | Loss: 0.00002786
Iteration 216/1000 | Loss: 0.00002786
Iteration 217/1000 | Loss: 0.00002786
Iteration 218/1000 | Loss: 0.00002786
Iteration 219/1000 | Loss: 0.00002786
Iteration 220/1000 | Loss: 0.00002786
Iteration 221/1000 | Loss: 0.00002786
Iteration 222/1000 | Loss: 0.00002786
Iteration 223/1000 | Loss: 0.00002786
Iteration 224/1000 | Loss: 0.00002786
Iteration 225/1000 | Loss: 0.00002786
Iteration 226/1000 | Loss: 0.00002786
Iteration 227/1000 | Loss: 0.00002786
Iteration 228/1000 | Loss: 0.00002786
Iteration 229/1000 | Loss: 0.00002786
Iteration 230/1000 | Loss: 0.00002786
Iteration 231/1000 | Loss: 0.00002786
Iteration 232/1000 | Loss: 0.00002786
Iteration 233/1000 | Loss: 0.00002786
Iteration 234/1000 | Loss: 0.00002786
Iteration 235/1000 | Loss: 0.00002786
Iteration 236/1000 | Loss: 0.00002786
Iteration 237/1000 | Loss: 0.00002786
Iteration 238/1000 | Loss: 0.00002786
Iteration 239/1000 | Loss: 0.00002786
Iteration 240/1000 | Loss: 0.00002786
Iteration 241/1000 | Loss: 0.00002786
Iteration 242/1000 | Loss: 0.00002786
Iteration 243/1000 | Loss: 0.00002786
Iteration 244/1000 | Loss: 0.00002786
Iteration 245/1000 | Loss: 0.00002786
Iteration 246/1000 | Loss: 0.00002786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.7863263312610798e-05, 2.7863263312610798e-05, 2.7863263312610798e-05, 2.7863263312610798e-05, 2.7863263312610798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7863263312610798e-05

Optimization complete. Final v2v error: 4.031272888183594 mm

Highest mean error: 12.193702697753906 mm for frame 65

Lowest mean error: 3.1317617893218994 mm for frame 139

Saving results

Total time: 222.5686695575714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713887
Iteration 2/25 | Loss: 0.00168689
Iteration 3/25 | Loss: 0.00142942
Iteration 4/25 | Loss: 0.00138976
Iteration 5/25 | Loss: 0.00138134
Iteration 6/25 | Loss: 0.00137625
Iteration 7/25 | Loss: 0.00137343
Iteration 8/25 | Loss: 0.00137261
Iteration 9/25 | Loss: 0.00137209
Iteration 10/25 | Loss: 0.00137171
Iteration 11/25 | Loss: 0.00137147
Iteration 12/25 | Loss: 0.00137131
Iteration 13/25 | Loss: 0.00137131
Iteration 14/25 | Loss: 0.00137131
Iteration 15/25 | Loss: 0.00137131
Iteration 16/25 | Loss: 0.00137131
Iteration 17/25 | Loss: 0.00137131
Iteration 18/25 | Loss: 0.00137131
Iteration 19/25 | Loss: 0.00137131
Iteration 20/25 | Loss: 0.00137131
Iteration 21/25 | Loss: 0.00137131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001371312653645873, 0.001371312653645873, 0.001371312653645873, 0.001371312653645873, 0.001371312653645873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001371312653645873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36433387
Iteration 2/25 | Loss: 0.00126590
Iteration 3/25 | Loss: 0.00126587
Iteration 4/25 | Loss: 0.00126587
Iteration 5/25 | Loss: 0.00126587
Iteration 6/25 | Loss: 0.00126586
Iteration 7/25 | Loss: 0.00126586
Iteration 8/25 | Loss: 0.00126586
Iteration 9/25 | Loss: 0.00126586
Iteration 10/25 | Loss: 0.00126586
Iteration 11/25 | Loss: 0.00126586
Iteration 12/25 | Loss: 0.00126586
Iteration 13/25 | Loss: 0.00126586
Iteration 14/25 | Loss: 0.00126586
Iteration 15/25 | Loss: 0.00126586
Iteration 16/25 | Loss: 0.00126586
Iteration 17/25 | Loss: 0.00126586
Iteration 18/25 | Loss: 0.00126586
Iteration 19/25 | Loss: 0.00126586
Iteration 20/25 | Loss: 0.00126586
Iteration 21/25 | Loss: 0.00126586
Iteration 22/25 | Loss: 0.00126586
Iteration 23/25 | Loss: 0.00126586
Iteration 24/25 | Loss: 0.00126586
Iteration 25/25 | Loss: 0.00126586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126586
Iteration 2/1000 | Loss: 0.00008826
Iteration 3/1000 | Loss: 0.00006461
Iteration 4/1000 | Loss: 0.00005815
Iteration 5/1000 | Loss: 0.00005503
Iteration 6/1000 | Loss: 0.00005321
Iteration 7/1000 | Loss: 0.00005236
Iteration 8/1000 | Loss: 0.00005140
Iteration 9/1000 | Loss: 0.00005073
Iteration 10/1000 | Loss: 0.00005011
Iteration 11/1000 | Loss: 0.00004972
Iteration 12/1000 | Loss: 0.00004936
Iteration 13/1000 | Loss: 0.00004894
Iteration 14/1000 | Loss: 0.00004862
Iteration 15/1000 | Loss: 0.00004832
Iteration 16/1000 | Loss: 0.00004813
Iteration 17/1000 | Loss: 0.00004794
Iteration 18/1000 | Loss: 0.00004786
Iteration 19/1000 | Loss: 0.00004782
Iteration 20/1000 | Loss: 0.00004781
Iteration 21/1000 | Loss: 0.00004775
Iteration 22/1000 | Loss: 0.00004771
Iteration 23/1000 | Loss: 0.00004767
Iteration 24/1000 | Loss: 0.00004760
Iteration 25/1000 | Loss: 0.00004759
Iteration 26/1000 | Loss: 0.00004753
Iteration 27/1000 | Loss: 0.00004752
Iteration 28/1000 | Loss: 0.00004751
Iteration 29/1000 | Loss: 0.00004750
Iteration 30/1000 | Loss: 0.00004748
Iteration 31/1000 | Loss: 0.00004747
Iteration 32/1000 | Loss: 0.00004747
Iteration 33/1000 | Loss: 0.00004746
Iteration 34/1000 | Loss: 0.00004746
Iteration 35/1000 | Loss: 0.00004744
Iteration 36/1000 | Loss: 0.00004743
Iteration 37/1000 | Loss: 0.00004743
Iteration 38/1000 | Loss: 0.00004740
Iteration 39/1000 | Loss: 0.00004735
Iteration 40/1000 | Loss: 0.00004735
Iteration 41/1000 | Loss: 0.00004735
Iteration 42/1000 | Loss: 0.00004735
Iteration 43/1000 | Loss: 0.00004735
Iteration 44/1000 | Loss: 0.00004734
Iteration 45/1000 | Loss: 0.00004733
Iteration 46/1000 | Loss: 0.00004733
Iteration 47/1000 | Loss: 0.00004732
Iteration 48/1000 | Loss: 0.00004732
Iteration 49/1000 | Loss: 0.00004731
Iteration 50/1000 | Loss: 0.00004731
Iteration 51/1000 | Loss: 0.00004731
Iteration 52/1000 | Loss: 0.00004731
Iteration 53/1000 | Loss: 0.00004731
Iteration 54/1000 | Loss: 0.00004730
Iteration 55/1000 | Loss: 0.00004730
Iteration 56/1000 | Loss: 0.00004730
Iteration 57/1000 | Loss: 0.00004729
Iteration 58/1000 | Loss: 0.00004729
Iteration 59/1000 | Loss: 0.00004729
Iteration 60/1000 | Loss: 0.00004728
Iteration 61/1000 | Loss: 0.00004728
Iteration 62/1000 | Loss: 0.00004728
Iteration 63/1000 | Loss: 0.00004727
Iteration 64/1000 | Loss: 0.00004727
Iteration 65/1000 | Loss: 0.00004727
Iteration 66/1000 | Loss: 0.00004726
Iteration 67/1000 | Loss: 0.00004726
Iteration 68/1000 | Loss: 0.00004725
Iteration 69/1000 | Loss: 0.00004725
Iteration 70/1000 | Loss: 0.00004725
Iteration 71/1000 | Loss: 0.00004725
Iteration 72/1000 | Loss: 0.00004724
Iteration 73/1000 | Loss: 0.00004724
Iteration 74/1000 | Loss: 0.00004724
Iteration 75/1000 | Loss: 0.00004723
Iteration 76/1000 | Loss: 0.00004723
Iteration 77/1000 | Loss: 0.00004723
Iteration 78/1000 | Loss: 0.00004722
Iteration 79/1000 | Loss: 0.00004721
Iteration 80/1000 | Loss: 0.00004721
Iteration 81/1000 | Loss: 0.00004721
Iteration 82/1000 | Loss: 0.00004720
Iteration 83/1000 | Loss: 0.00004720
Iteration 84/1000 | Loss: 0.00004719
Iteration 85/1000 | Loss: 0.00004719
Iteration 86/1000 | Loss: 0.00004718
Iteration 87/1000 | Loss: 0.00004718
Iteration 88/1000 | Loss: 0.00004718
Iteration 89/1000 | Loss: 0.00004718
Iteration 90/1000 | Loss: 0.00004717
Iteration 91/1000 | Loss: 0.00004716
Iteration 92/1000 | Loss: 0.00004716
Iteration 93/1000 | Loss: 0.00004716
Iteration 94/1000 | Loss: 0.00004716
Iteration 95/1000 | Loss: 0.00004716
Iteration 96/1000 | Loss: 0.00004716
Iteration 97/1000 | Loss: 0.00004716
Iteration 98/1000 | Loss: 0.00004716
Iteration 99/1000 | Loss: 0.00004716
Iteration 100/1000 | Loss: 0.00004716
Iteration 101/1000 | Loss: 0.00004716
Iteration 102/1000 | Loss: 0.00004716
Iteration 103/1000 | Loss: 0.00004716
Iteration 104/1000 | Loss: 0.00004716
Iteration 105/1000 | Loss: 0.00004716
Iteration 106/1000 | Loss: 0.00004716
Iteration 107/1000 | Loss: 0.00004716
Iteration 108/1000 | Loss: 0.00004716
Iteration 109/1000 | Loss: 0.00004716
Iteration 110/1000 | Loss: 0.00004716
Iteration 111/1000 | Loss: 0.00004716
Iteration 112/1000 | Loss: 0.00004716
Iteration 113/1000 | Loss: 0.00004716
Iteration 114/1000 | Loss: 0.00004716
Iteration 115/1000 | Loss: 0.00004716
Iteration 116/1000 | Loss: 0.00004716
Iteration 117/1000 | Loss: 0.00004716
Iteration 118/1000 | Loss: 0.00004716
Iteration 119/1000 | Loss: 0.00004716
Iteration 120/1000 | Loss: 0.00004716
Iteration 121/1000 | Loss: 0.00004716
Iteration 122/1000 | Loss: 0.00004716
Iteration 123/1000 | Loss: 0.00004716
Iteration 124/1000 | Loss: 0.00004716
Iteration 125/1000 | Loss: 0.00004716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [4.716453258879483e-05, 4.716453258879483e-05, 4.716453258879483e-05, 4.716453258879483e-05, 4.716453258879483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.716453258879483e-05

Optimization complete. Final v2v error: 4.350677967071533 mm

Highest mean error: 12.332067489624023 mm for frame 20

Lowest mean error: 3.2326462268829346 mm for frame 236

Saving results

Total time: 63.567012310028076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396956
Iteration 2/25 | Loss: 0.00130327
Iteration 3/25 | Loss: 0.00123811
Iteration 4/25 | Loss: 0.00122846
Iteration 5/25 | Loss: 0.00122559
Iteration 6/25 | Loss: 0.00122559
Iteration 7/25 | Loss: 0.00122559
Iteration 8/25 | Loss: 0.00122559
Iteration 9/25 | Loss: 0.00122559
Iteration 10/25 | Loss: 0.00122559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012255923356860876, 0.0012255923356860876, 0.0012255923356860876, 0.0012255923356860876, 0.0012255923356860876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012255923356860876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48082018
Iteration 2/25 | Loss: 0.00074781
Iteration 3/25 | Loss: 0.00074781
Iteration 4/25 | Loss: 0.00074780
Iteration 5/25 | Loss: 0.00074780
Iteration 6/25 | Loss: 0.00074780
Iteration 7/25 | Loss: 0.00074780
Iteration 8/25 | Loss: 0.00074780
Iteration 9/25 | Loss: 0.00074780
Iteration 10/25 | Loss: 0.00074780
Iteration 11/25 | Loss: 0.00074780
Iteration 12/25 | Loss: 0.00074780
Iteration 13/25 | Loss: 0.00074780
Iteration 14/25 | Loss: 0.00074780
Iteration 15/25 | Loss: 0.00074780
Iteration 16/25 | Loss: 0.00074780
Iteration 17/25 | Loss: 0.00074780
Iteration 18/25 | Loss: 0.00074780
Iteration 19/25 | Loss: 0.00074780
Iteration 20/25 | Loss: 0.00074780
Iteration 21/25 | Loss: 0.00074780
Iteration 22/25 | Loss: 0.00074780
Iteration 23/25 | Loss: 0.00074780
Iteration 24/25 | Loss: 0.00074780
Iteration 25/25 | Loss: 0.00074780

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074780
Iteration 2/1000 | Loss: 0.00002623
Iteration 3/1000 | Loss: 0.00001947
Iteration 4/1000 | Loss: 0.00001773
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001592
Iteration 7/1000 | Loss: 0.00001555
Iteration 8/1000 | Loss: 0.00001550
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001505
Iteration 11/1000 | Loss: 0.00001493
Iteration 12/1000 | Loss: 0.00001489
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001482
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001459
Iteration 23/1000 | Loss: 0.00001456
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001452
Iteration 31/1000 | Loss: 0.00001451
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001440
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001436
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001436
Iteration 54/1000 | Loss: 0.00001434
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001428
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001427
Iteration 69/1000 | Loss: 0.00001427
Iteration 70/1000 | Loss: 0.00001427
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001425
Iteration 78/1000 | Loss: 0.00001425
Iteration 79/1000 | Loss: 0.00001425
Iteration 80/1000 | Loss: 0.00001425
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001418
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001416
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.4161761100695003e-05, 1.4161761100695003e-05, 1.4161761100695003e-05, 1.4161761100695003e-05, 1.4161761100695003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4161761100695003e-05

Optimization complete. Final v2v error: 3.187138795852661 mm

Highest mean error: 3.318467140197754 mm for frame 182

Lowest mean error: 3.1249420642852783 mm for frame 203

Saving results

Total time: 38.71921181678772
