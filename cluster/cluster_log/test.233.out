Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=233, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13048-13103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509877
Iteration 2/25 | Loss: 0.00144009
Iteration 3/25 | Loss: 0.00129322
Iteration 4/25 | Loss: 0.00127464
Iteration 5/25 | Loss: 0.00126867
Iteration 6/25 | Loss: 0.00126860
Iteration 7/25 | Loss: 0.00126860
Iteration 8/25 | Loss: 0.00126860
Iteration 9/25 | Loss: 0.00126860
Iteration 10/25 | Loss: 0.00126860
Iteration 11/25 | Loss: 0.00126860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012686047703027725, 0.0012686047703027725, 0.0012686047703027725, 0.0012686047703027725, 0.0012686047703027725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012686047703027725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87547749
Iteration 2/25 | Loss: 0.00074242
Iteration 3/25 | Loss: 0.00074242
Iteration 4/25 | Loss: 0.00074242
Iteration 5/25 | Loss: 0.00074242
Iteration 6/25 | Loss: 0.00074242
Iteration 7/25 | Loss: 0.00074242
Iteration 8/25 | Loss: 0.00074242
Iteration 9/25 | Loss: 0.00074242
Iteration 10/25 | Loss: 0.00074242
Iteration 11/25 | Loss: 0.00074242
Iteration 12/25 | Loss: 0.00074242
Iteration 13/25 | Loss: 0.00074242
Iteration 14/25 | Loss: 0.00074242
Iteration 15/25 | Loss: 0.00074242
Iteration 16/25 | Loss: 0.00074242
Iteration 17/25 | Loss: 0.00074242
Iteration 18/25 | Loss: 0.00074242
Iteration 19/25 | Loss: 0.00074242
Iteration 20/25 | Loss: 0.00074242
Iteration 21/25 | Loss: 0.00074242
Iteration 22/25 | Loss: 0.00074242
Iteration 23/25 | Loss: 0.00074242
Iteration 24/25 | Loss: 0.00074242
Iteration 25/25 | Loss: 0.00074242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074242
Iteration 2/1000 | Loss: 0.00004213
Iteration 3/1000 | Loss: 0.00002614
Iteration 4/1000 | Loss: 0.00002356
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002095
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002021
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001963
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001875
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001855
Iteration 18/1000 | Loss: 0.00001845
Iteration 19/1000 | Loss: 0.00001845
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001831
Iteration 22/1000 | Loss: 0.00001831
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001823
Iteration 25/1000 | Loss: 0.00001820
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001817
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001817
Iteration 31/1000 | Loss: 0.00001817
Iteration 32/1000 | Loss: 0.00001817
Iteration 33/1000 | Loss: 0.00001817
Iteration 34/1000 | Loss: 0.00001817
Iteration 35/1000 | Loss: 0.00001817
Iteration 36/1000 | Loss: 0.00001817
Iteration 37/1000 | Loss: 0.00001817
Iteration 38/1000 | Loss: 0.00001816
Iteration 39/1000 | Loss: 0.00001816
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001814
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001812
Iteration 52/1000 | Loss: 0.00001812
Iteration 53/1000 | Loss: 0.00001812
Iteration 54/1000 | Loss: 0.00001812
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001812
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001812
Iteration 60/1000 | Loss: 0.00001811
Iteration 61/1000 | Loss: 0.00001811
Iteration 62/1000 | Loss: 0.00001811
Iteration 63/1000 | Loss: 0.00001811
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00001810
Iteration 67/1000 | Loss: 0.00001809
Iteration 68/1000 | Loss: 0.00001809
Iteration 69/1000 | Loss: 0.00001809
Iteration 70/1000 | Loss: 0.00001809
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001808
Iteration 74/1000 | Loss: 0.00001808
Iteration 75/1000 | Loss: 0.00001808
Iteration 76/1000 | Loss: 0.00001808
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001807
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001803
Iteration 99/1000 | Loss: 0.00001803
Iteration 100/1000 | Loss: 0.00001802
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001801
Iteration 107/1000 | Loss: 0.00001801
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001800
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001800
Iteration 112/1000 | Loss: 0.00001800
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001800
Iteration 115/1000 | Loss: 0.00001800
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001800
Iteration 118/1000 | Loss: 0.00001799
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001799
Iteration 121/1000 | Loss: 0.00001799
Iteration 122/1000 | Loss: 0.00001799
Iteration 123/1000 | Loss: 0.00001799
Iteration 124/1000 | Loss: 0.00001799
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001798
Iteration 127/1000 | Loss: 0.00001798
Iteration 128/1000 | Loss: 0.00001798
Iteration 129/1000 | Loss: 0.00001797
Iteration 130/1000 | Loss: 0.00001797
Iteration 131/1000 | Loss: 0.00001797
Iteration 132/1000 | Loss: 0.00001797
Iteration 133/1000 | Loss: 0.00001796
Iteration 134/1000 | Loss: 0.00001796
Iteration 135/1000 | Loss: 0.00001796
Iteration 136/1000 | Loss: 0.00001796
Iteration 137/1000 | Loss: 0.00001795
Iteration 138/1000 | Loss: 0.00001795
Iteration 139/1000 | Loss: 0.00001795
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001795
Iteration 142/1000 | Loss: 0.00001795
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00001795
Iteration 145/1000 | Loss: 0.00001795
Iteration 146/1000 | Loss: 0.00001795
Iteration 147/1000 | Loss: 0.00001794
Iteration 148/1000 | Loss: 0.00001794
Iteration 149/1000 | Loss: 0.00001794
Iteration 150/1000 | Loss: 0.00001794
Iteration 151/1000 | Loss: 0.00001793
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001793
Iteration 155/1000 | Loss: 0.00001793
Iteration 156/1000 | Loss: 0.00001793
Iteration 157/1000 | Loss: 0.00001792
Iteration 158/1000 | Loss: 0.00001792
Iteration 159/1000 | Loss: 0.00001792
Iteration 160/1000 | Loss: 0.00001792
Iteration 161/1000 | Loss: 0.00001792
Iteration 162/1000 | Loss: 0.00001791
Iteration 163/1000 | Loss: 0.00001791
Iteration 164/1000 | Loss: 0.00001791
Iteration 165/1000 | Loss: 0.00001791
Iteration 166/1000 | Loss: 0.00001791
Iteration 167/1000 | Loss: 0.00001791
Iteration 168/1000 | Loss: 0.00001791
Iteration 169/1000 | Loss: 0.00001790
Iteration 170/1000 | Loss: 0.00001790
Iteration 171/1000 | Loss: 0.00001790
Iteration 172/1000 | Loss: 0.00001790
Iteration 173/1000 | Loss: 0.00001789
Iteration 174/1000 | Loss: 0.00001789
Iteration 175/1000 | Loss: 0.00001789
Iteration 176/1000 | Loss: 0.00001789
Iteration 177/1000 | Loss: 0.00001789
Iteration 178/1000 | Loss: 0.00001789
Iteration 179/1000 | Loss: 0.00001789
Iteration 180/1000 | Loss: 0.00001789
Iteration 181/1000 | Loss: 0.00001789
Iteration 182/1000 | Loss: 0.00001789
Iteration 183/1000 | Loss: 0.00001789
Iteration 184/1000 | Loss: 0.00001789
Iteration 185/1000 | Loss: 0.00001789
Iteration 186/1000 | Loss: 0.00001789
Iteration 187/1000 | Loss: 0.00001789
Iteration 188/1000 | Loss: 0.00001789
Iteration 189/1000 | Loss: 0.00001789
Iteration 190/1000 | Loss: 0.00001789
Iteration 191/1000 | Loss: 0.00001789
Iteration 192/1000 | Loss: 0.00001789
Iteration 193/1000 | Loss: 0.00001789
Iteration 194/1000 | Loss: 0.00001789
Iteration 195/1000 | Loss: 0.00001789
Iteration 196/1000 | Loss: 0.00001789
Iteration 197/1000 | Loss: 0.00001789
Iteration 198/1000 | Loss: 0.00001789
Iteration 199/1000 | Loss: 0.00001789
Iteration 200/1000 | Loss: 0.00001789
Iteration 201/1000 | Loss: 0.00001789
Iteration 202/1000 | Loss: 0.00001789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.7888949514599517e-05, 1.7888949514599517e-05, 1.7888949514599517e-05, 1.7888949514599517e-05, 1.7888949514599517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7888949514599517e-05

Optimization complete. Final v2v error: 3.532907485961914 mm

Highest mean error: 3.867316484451294 mm for frame 0

Lowest mean error: 3.494838237762451 mm for frame 142

Saving results

Total time: 57.04003071784973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960371
Iteration 2/25 | Loss: 0.00330499
Iteration 3/25 | Loss: 0.00228438
Iteration 4/25 | Loss: 0.00211548
Iteration 5/25 | Loss: 0.00193903
Iteration 6/25 | Loss: 0.00178874
Iteration 7/25 | Loss: 0.00191797
Iteration 8/25 | Loss: 0.00188661
Iteration 9/25 | Loss: 0.00183106
Iteration 10/25 | Loss: 0.00173771
Iteration 11/25 | Loss: 0.00165409
Iteration 12/25 | Loss: 0.00162587
Iteration 13/25 | Loss: 0.00159482
Iteration 14/25 | Loss: 0.00156813
Iteration 15/25 | Loss: 0.00155956
Iteration 16/25 | Loss: 0.00152798
Iteration 17/25 | Loss: 0.00151536
Iteration 18/25 | Loss: 0.00157014
Iteration 19/25 | Loss: 0.00157281
Iteration 20/25 | Loss: 0.00155366
Iteration 21/25 | Loss: 0.00155116
Iteration 22/25 | Loss: 0.00153591
Iteration 23/25 | Loss: 0.00152703
Iteration 24/25 | Loss: 0.00152588
Iteration 25/25 | Loss: 0.00151643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41258979
Iteration 2/25 | Loss: 0.00235382
Iteration 3/25 | Loss: 0.00225204
Iteration 4/25 | Loss: 0.00225204
Iteration 5/25 | Loss: 0.00225204
Iteration 6/25 | Loss: 0.00225204
Iteration 7/25 | Loss: 0.00225204
Iteration 8/25 | Loss: 0.00225204
Iteration 9/25 | Loss: 0.00225204
Iteration 10/25 | Loss: 0.00225204
Iteration 11/25 | Loss: 0.00225204
Iteration 12/25 | Loss: 0.00225204
Iteration 13/25 | Loss: 0.00225204
Iteration 14/25 | Loss: 0.00225204
Iteration 15/25 | Loss: 0.00225204
Iteration 16/25 | Loss: 0.00225204
Iteration 17/25 | Loss: 0.00225204
Iteration 18/25 | Loss: 0.00225204
Iteration 19/25 | Loss: 0.00225204
Iteration 20/25 | Loss: 0.00225204
Iteration 21/25 | Loss: 0.00225204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022520357742905617, 0.0022520357742905617, 0.0022520357742905617, 0.0022520357742905617, 0.0022520357742905617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022520357742905617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225204
Iteration 2/1000 | Loss: 0.00047253
Iteration 3/1000 | Loss: 0.00090239
Iteration 4/1000 | Loss: 0.00099056
Iteration 5/1000 | Loss: 0.00050382
Iteration 6/1000 | Loss: 0.00023705
Iteration 7/1000 | Loss: 0.00025119
Iteration 8/1000 | Loss: 0.00026987
Iteration 9/1000 | Loss: 0.00019875
Iteration 10/1000 | Loss: 0.00047576
Iteration 11/1000 | Loss: 0.00033146
Iteration 12/1000 | Loss: 0.00034714
Iteration 13/1000 | Loss: 0.00021451
Iteration 14/1000 | Loss: 0.00016746
Iteration 15/1000 | Loss: 0.00021960
Iteration 16/1000 | Loss: 0.00029252
Iteration 17/1000 | Loss: 0.00016870
Iteration 18/1000 | Loss: 0.00025833
Iteration 19/1000 | Loss: 0.00015512
Iteration 20/1000 | Loss: 0.00018982
Iteration 21/1000 | Loss: 0.00021358
Iteration 22/1000 | Loss: 0.00018033
Iteration 23/1000 | Loss: 0.00017319
Iteration 24/1000 | Loss: 0.00016394
Iteration 25/1000 | Loss: 0.00016190
Iteration 26/1000 | Loss: 0.00017016
Iteration 27/1000 | Loss: 0.00032816
Iteration 28/1000 | Loss: 0.00079135
Iteration 29/1000 | Loss: 0.00026261
Iteration 30/1000 | Loss: 0.00018691
Iteration 31/1000 | Loss: 0.00021354
Iteration 32/1000 | Loss: 0.00019593
Iteration 33/1000 | Loss: 0.00018208
Iteration 34/1000 | Loss: 0.00020641
Iteration 35/1000 | Loss: 0.00016034
Iteration 36/1000 | Loss: 0.00017566
Iteration 37/1000 | Loss: 0.00016723
Iteration 38/1000 | Loss: 0.00018377
Iteration 39/1000 | Loss: 0.00016826
Iteration 40/1000 | Loss: 0.00016668
Iteration 41/1000 | Loss: 0.00047966
Iteration 42/1000 | Loss: 0.00809612
Iteration 43/1000 | Loss: 0.00157129
Iteration 44/1000 | Loss: 0.00089933
Iteration 45/1000 | Loss: 0.00073396
Iteration 46/1000 | Loss: 0.00042664
Iteration 47/1000 | Loss: 0.00081860
Iteration 48/1000 | Loss: 0.00048370
Iteration 49/1000 | Loss: 0.00022556
Iteration 50/1000 | Loss: 0.00013951
Iteration 51/1000 | Loss: 0.00054309
Iteration 52/1000 | Loss: 0.00018990
Iteration 53/1000 | Loss: 0.00090206
Iteration 54/1000 | Loss: 0.00060581
Iteration 55/1000 | Loss: 0.00044306
Iteration 56/1000 | Loss: 0.00043910
Iteration 57/1000 | Loss: 0.00036341
Iteration 58/1000 | Loss: 0.00011476
Iteration 59/1000 | Loss: 0.00011707
Iteration 60/1000 | Loss: 0.00033416
Iteration 61/1000 | Loss: 0.00074916
Iteration 62/1000 | Loss: 0.00055463
Iteration 63/1000 | Loss: 0.00010669
Iteration 64/1000 | Loss: 0.00006640
Iteration 65/1000 | Loss: 0.00007067
Iteration 66/1000 | Loss: 0.00005969
Iteration 67/1000 | Loss: 0.00005829
Iteration 68/1000 | Loss: 0.00005480
Iteration 69/1000 | Loss: 0.00005317
Iteration 70/1000 | Loss: 0.00016619
Iteration 71/1000 | Loss: 0.00028592
Iteration 72/1000 | Loss: 0.00028953
Iteration 73/1000 | Loss: 0.00025139
Iteration 74/1000 | Loss: 0.00064354
Iteration 75/1000 | Loss: 0.00041283
Iteration 76/1000 | Loss: 0.00033710
Iteration 77/1000 | Loss: 0.00042536
Iteration 78/1000 | Loss: 0.00047689
Iteration 79/1000 | Loss: 0.00006846
Iteration 80/1000 | Loss: 0.00008874
Iteration 81/1000 | Loss: 0.00006364
Iteration 82/1000 | Loss: 0.00005278
Iteration 83/1000 | Loss: 0.00005084
Iteration 84/1000 | Loss: 0.00007137
Iteration 85/1000 | Loss: 0.00005941
Iteration 86/1000 | Loss: 0.00004883
Iteration 87/1000 | Loss: 0.00006722
Iteration 88/1000 | Loss: 0.00004790
Iteration 89/1000 | Loss: 0.00007394
Iteration 90/1000 | Loss: 0.00004724
Iteration 91/1000 | Loss: 0.00004693
Iteration 92/1000 | Loss: 0.00004666
Iteration 93/1000 | Loss: 0.00004642
Iteration 94/1000 | Loss: 0.00004616
Iteration 95/1000 | Loss: 0.00012235
Iteration 96/1000 | Loss: 0.00005209
Iteration 97/1000 | Loss: 0.00004597
Iteration 98/1000 | Loss: 0.00005822
Iteration 99/1000 | Loss: 0.00004585
Iteration 100/1000 | Loss: 0.00004580
Iteration 101/1000 | Loss: 0.00004571
Iteration 102/1000 | Loss: 0.00004568
Iteration 103/1000 | Loss: 0.00004567
Iteration 104/1000 | Loss: 0.00004558
Iteration 105/1000 | Loss: 0.00004555
Iteration 106/1000 | Loss: 0.00004548
Iteration 107/1000 | Loss: 0.00015313
Iteration 108/1000 | Loss: 0.00004734
Iteration 109/1000 | Loss: 0.00004579
Iteration 110/1000 | Loss: 0.00004530
Iteration 111/1000 | Loss: 0.00004527
Iteration 112/1000 | Loss: 0.00004527
Iteration 113/1000 | Loss: 0.00004526
Iteration 114/1000 | Loss: 0.00004526
Iteration 115/1000 | Loss: 0.00004525
Iteration 116/1000 | Loss: 0.00004525
Iteration 117/1000 | Loss: 0.00004524
Iteration 118/1000 | Loss: 0.00004524
Iteration 119/1000 | Loss: 0.00004524
Iteration 120/1000 | Loss: 0.00004523
Iteration 121/1000 | Loss: 0.00004520
Iteration 122/1000 | Loss: 0.00012785
Iteration 123/1000 | Loss: 0.00004568
Iteration 124/1000 | Loss: 0.00004516
Iteration 125/1000 | Loss: 0.00004507
Iteration 126/1000 | Loss: 0.00004507
Iteration 127/1000 | Loss: 0.00004504
Iteration 128/1000 | Loss: 0.00004500
Iteration 129/1000 | Loss: 0.00004500
Iteration 130/1000 | Loss: 0.00004499
Iteration 131/1000 | Loss: 0.00004498
Iteration 132/1000 | Loss: 0.00004497
Iteration 133/1000 | Loss: 0.00004497
Iteration 134/1000 | Loss: 0.00004496
Iteration 135/1000 | Loss: 0.00004495
Iteration 136/1000 | Loss: 0.00004495
Iteration 137/1000 | Loss: 0.00004495
Iteration 138/1000 | Loss: 0.00004495
Iteration 139/1000 | Loss: 0.00004495
Iteration 140/1000 | Loss: 0.00004495
Iteration 141/1000 | Loss: 0.00004495
Iteration 142/1000 | Loss: 0.00004495
Iteration 143/1000 | Loss: 0.00004495
Iteration 144/1000 | Loss: 0.00004495
Iteration 145/1000 | Loss: 0.00004494
Iteration 146/1000 | Loss: 0.00004494
Iteration 147/1000 | Loss: 0.00004493
Iteration 148/1000 | Loss: 0.00004493
Iteration 149/1000 | Loss: 0.00004492
Iteration 150/1000 | Loss: 0.00004492
Iteration 151/1000 | Loss: 0.00004491
Iteration 152/1000 | Loss: 0.00004491
Iteration 153/1000 | Loss: 0.00004491
Iteration 154/1000 | Loss: 0.00004490
Iteration 155/1000 | Loss: 0.00004490
Iteration 156/1000 | Loss: 0.00004489
Iteration 157/1000 | Loss: 0.00004489
Iteration 158/1000 | Loss: 0.00004489
Iteration 159/1000 | Loss: 0.00004488
Iteration 160/1000 | Loss: 0.00004488
Iteration 161/1000 | Loss: 0.00004487
Iteration 162/1000 | Loss: 0.00004487
Iteration 163/1000 | Loss: 0.00004486
Iteration 164/1000 | Loss: 0.00004486
Iteration 165/1000 | Loss: 0.00004486
Iteration 166/1000 | Loss: 0.00004485
Iteration 167/1000 | Loss: 0.00004485
Iteration 168/1000 | Loss: 0.00004485
Iteration 169/1000 | Loss: 0.00004484
Iteration 170/1000 | Loss: 0.00004484
Iteration 171/1000 | Loss: 0.00004484
Iteration 172/1000 | Loss: 0.00004484
Iteration 173/1000 | Loss: 0.00004483
Iteration 174/1000 | Loss: 0.00004483
Iteration 175/1000 | Loss: 0.00004483
Iteration 176/1000 | Loss: 0.00004483
Iteration 177/1000 | Loss: 0.00004483
Iteration 178/1000 | Loss: 0.00004483
Iteration 179/1000 | Loss: 0.00004482
Iteration 180/1000 | Loss: 0.00004482
Iteration 181/1000 | Loss: 0.00004482
Iteration 182/1000 | Loss: 0.00004482
Iteration 183/1000 | Loss: 0.00004482
Iteration 184/1000 | Loss: 0.00004482
Iteration 185/1000 | Loss: 0.00004481
Iteration 186/1000 | Loss: 0.00004481
Iteration 187/1000 | Loss: 0.00004481
Iteration 188/1000 | Loss: 0.00004480
Iteration 189/1000 | Loss: 0.00004480
Iteration 190/1000 | Loss: 0.00004480
Iteration 191/1000 | Loss: 0.00004479
Iteration 192/1000 | Loss: 0.00004479
Iteration 193/1000 | Loss: 0.00004478
Iteration 194/1000 | Loss: 0.00004478
Iteration 195/1000 | Loss: 0.00004477
Iteration 196/1000 | Loss: 0.00004476
Iteration 197/1000 | Loss: 0.00004476
Iteration 198/1000 | Loss: 0.00004475
Iteration 199/1000 | Loss: 0.00004473
Iteration 200/1000 | Loss: 0.00004473
Iteration 201/1000 | Loss: 0.00004473
Iteration 202/1000 | Loss: 0.00004473
Iteration 203/1000 | Loss: 0.00004473
Iteration 204/1000 | Loss: 0.00004472
Iteration 205/1000 | Loss: 0.00004472
Iteration 206/1000 | Loss: 0.00004466
Iteration 207/1000 | Loss: 0.00039054
Iteration 208/1000 | Loss: 0.00043505
Iteration 209/1000 | Loss: 0.00063835
Iteration 210/1000 | Loss: 0.00036936
Iteration 211/1000 | Loss: 0.00020692
Iteration 212/1000 | Loss: 0.00021754
Iteration 213/1000 | Loss: 0.00015088
Iteration 214/1000 | Loss: 0.00005878
Iteration 215/1000 | Loss: 0.00005114
Iteration 216/1000 | Loss: 0.00004885
Iteration 217/1000 | Loss: 0.00004655
Iteration 218/1000 | Loss: 0.00033679
Iteration 219/1000 | Loss: 0.00034910
Iteration 220/1000 | Loss: 0.00028360
Iteration 221/1000 | Loss: 0.00009469
Iteration 222/1000 | Loss: 0.00027045
Iteration 223/1000 | Loss: 0.00027659
Iteration 224/1000 | Loss: 0.00015325
Iteration 225/1000 | Loss: 0.00031517
Iteration 226/1000 | Loss: 0.00016024
Iteration 227/1000 | Loss: 0.00019024
Iteration 228/1000 | Loss: 0.00014129
Iteration 229/1000 | Loss: 0.00004658
Iteration 230/1000 | Loss: 0.00030058
Iteration 231/1000 | Loss: 0.00055996
Iteration 232/1000 | Loss: 0.00024629
Iteration 233/1000 | Loss: 0.00066970
Iteration 234/1000 | Loss: 0.00162269
Iteration 235/1000 | Loss: 0.00006246
Iteration 236/1000 | Loss: 0.00033843
Iteration 237/1000 | Loss: 0.00026414
Iteration 238/1000 | Loss: 0.00008109
Iteration 239/1000 | Loss: 0.00005457
Iteration 240/1000 | Loss: 0.00004424
Iteration 241/1000 | Loss: 0.00005680
Iteration 242/1000 | Loss: 0.00005306
Iteration 243/1000 | Loss: 0.00004897
Iteration 244/1000 | Loss: 0.00004329
Iteration 245/1000 | Loss: 0.00003813
Iteration 246/1000 | Loss: 0.00037903
Iteration 247/1000 | Loss: 0.00156948
Iteration 248/1000 | Loss: 0.00104488
Iteration 249/1000 | Loss: 0.00015408
Iteration 250/1000 | Loss: 0.00017699
Iteration 251/1000 | Loss: 0.00008863
Iteration 252/1000 | Loss: 0.00005558
Iteration 253/1000 | Loss: 0.00008398
Iteration 254/1000 | Loss: 0.00014125
Iteration 255/1000 | Loss: 0.00003950
Iteration 256/1000 | Loss: 0.00036601
Iteration 257/1000 | Loss: 0.00003577
Iteration 258/1000 | Loss: 0.00013299
Iteration 259/1000 | Loss: 0.00010117
Iteration 260/1000 | Loss: 0.00004004
Iteration 261/1000 | Loss: 0.00003049
Iteration 262/1000 | Loss: 0.00007324
Iteration 263/1000 | Loss: 0.00020798
Iteration 264/1000 | Loss: 0.00060195
Iteration 265/1000 | Loss: 0.00013677
Iteration 266/1000 | Loss: 0.00004986
Iteration 267/1000 | Loss: 0.00005009
Iteration 268/1000 | Loss: 0.00002914
Iteration 269/1000 | Loss: 0.00013194
Iteration 270/1000 | Loss: 0.00002883
Iteration 271/1000 | Loss: 0.00002846
Iteration 272/1000 | Loss: 0.00009673
Iteration 273/1000 | Loss: 0.00002807
Iteration 274/1000 | Loss: 0.00002773
Iteration 275/1000 | Loss: 0.00012418
Iteration 276/1000 | Loss: 0.00037089
Iteration 277/1000 | Loss: 0.00005006
Iteration 278/1000 | Loss: 0.00004718
Iteration 279/1000 | Loss: 0.00002709
Iteration 280/1000 | Loss: 0.00002695
Iteration 281/1000 | Loss: 0.00002668
Iteration 282/1000 | Loss: 0.00010019
Iteration 283/1000 | Loss: 0.00005111
Iteration 284/1000 | Loss: 0.00002607
Iteration 285/1000 | Loss: 0.00005846
Iteration 286/1000 | Loss: 0.00002623
Iteration 287/1000 | Loss: 0.00002562
Iteration 288/1000 | Loss: 0.00002554
Iteration 289/1000 | Loss: 0.00002554
Iteration 290/1000 | Loss: 0.00002554
Iteration 291/1000 | Loss: 0.00002553
Iteration 292/1000 | Loss: 0.00002553
Iteration 293/1000 | Loss: 0.00002553
Iteration 294/1000 | Loss: 0.00002553
Iteration 295/1000 | Loss: 0.00002553
Iteration 296/1000 | Loss: 0.00002553
Iteration 297/1000 | Loss: 0.00002553
Iteration 298/1000 | Loss: 0.00002552
Iteration 299/1000 | Loss: 0.00002552
Iteration 300/1000 | Loss: 0.00002552
Iteration 301/1000 | Loss: 0.00002551
Iteration 302/1000 | Loss: 0.00002551
Iteration 303/1000 | Loss: 0.00002551
Iteration 304/1000 | Loss: 0.00002551
Iteration 305/1000 | Loss: 0.00002550
Iteration 306/1000 | Loss: 0.00002550
Iteration 307/1000 | Loss: 0.00002550
Iteration 308/1000 | Loss: 0.00002549
Iteration 309/1000 | Loss: 0.00002549
Iteration 310/1000 | Loss: 0.00002549
Iteration 311/1000 | Loss: 0.00002548
Iteration 312/1000 | Loss: 0.00002548
Iteration 313/1000 | Loss: 0.00002548
Iteration 314/1000 | Loss: 0.00002548
Iteration 315/1000 | Loss: 0.00002547
Iteration 316/1000 | Loss: 0.00002547
Iteration 317/1000 | Loss: 0.00002546
Iteration 318/1000 | Loss: 0.00002546
Iteration 319/1000 | Loss: 0.00002546
Iteration 320/1000 | Loss: 0.00002546
Iteration 321/1000 | Loss: 0.00002546
Iteration 322/1000 | Loss: 0.00002545
Iteration 323/1000 | Loss: 0.00002545
Iteration 324/1000 | Loss: 0.00002545
Iteration 325/1000 | Loss: 0.00002545
Iteration 326/1000 | Loss: 0.00002544
Iteration 327/1000 | Loss: 0.00002544
Iteration 328/1000 | Loss: 0.00002544
Iteration 329/1000 | Loss: 0.00002544
Iteration 330/1000 | Loss: 0.00002543
Iteration 331/1000 | Loss: 0.00002543
Iteration 332/1000 | Loss: 0.00002543
Iteration 333/1000 | Loss: 0.00002543
Iteration 334/1000 | Loss: 0.00002543
Iteration 335/1000 | Loss: 0.00002543
Iteration 336/1000 | Loss: 0.00002543
Iteration 337/1000 | Loss: 0.00002542
Iteration 338/1000 | Loss: 0.00002542
Iteration 339/1000 | Loss: 0.00002542
Iteration 340/1000 | Loss: 0.00002542
Iteration 341/1000 | Loss: 0.00002542
Iteration 342/1000 | Loss: 0.00002542
Iteration 343/1000 | Loss: 0.00002542
Iteration 344/1000 | Loss: 0.00002541
Iteration 345/1000 | Loss: 0.00002541
Iteration 346/1000 | Loss: 0.00002541
Iteration 347/1000 | Loss: 0.00002541
Iteration 348/1000 | Loss: 0.00002541
Iteration 349/1000 | Loss: 0.00002541
Iteration 350/1000 | Loss: 0.00002541
Iteration 351/1000 | Loss: 0.00002541
Iteration 352/1000 | Loss: 0.00002541
Iteration 353/1000 | Loss: 0.00002541
Iteration 354/1000 | Loss: 0.00002541
Iteration 355/1000 | Loss: 0.00002541
Iteration 356/1000 | Loss: 0.00002541
Iteration 357/1000 | Loss: 0.00002541
Iteration 358/1000 | Loss: 0.00002541
Iteration 359/1000 | Loss: 0.00002541
Iteration 360/1000 | Loss: 0.00002541
Iteration 361/1000 | Loss: 0.00002541
Iteration 362/1000 | Loss: 0.00002541
Iteration 363/1000 | Loss: 0.00002541
Iteration 364/1000 | Loss: 0.00002540
Iteration 365/1000 | Loss: 0.00002540
Iteration 366/1000 | Loss: 0.00002540
Iteration 367/1000 | Loss: 0.00002540
Iteration 368/1000 | Loss: 0.00002540
Iteration 369/1000 | Loss: 0.00002540
Iteration 370/1000 | Loss: 0.00002539
Iteration 371/1000 | Loss: 0.00002539
Iteration 372/1000 | Loss: 0.00002539
Iteration 373/1000 | Loss: 0.00002539
Iteration 374/1000 | Loss: 0.00002539
Iteration 375/1000 | Loss: 0.00002539
Iteration 376/1000 | Loss: 0.00002539
Iteration 377/1000 | Loss: 0.00002539
Iteration 378/1000 | Loss: 0.00002539
Iteration 379/1000 | Loss: 0.00002539
Iteration 380/1000 | Loss: 0.00002539
Iteration 381/1000 | Loss: 0.00002539
Iteration 382/1000 | Loss: 0.00002539
Iteration 383/1000 | Loss: 0.00002539
Iteration 384/1000 | Loss: 0.00002539
Iteration 385/1000 | Loss: 0.00002539
Iteration 386/1000 | Loss: 0.00002539
Iteration 387/1000 | Loss: 0.00002539
Iteration 388/1000 | Loss: 0.00002539
Iteration 389/1000 | Loss: 0.00002539
Iteration 390/1000 | Loss: 0.00002539
Iteration 391/1000 | Loss: 0.00002539
Iteration 392/1000 | Loss: 0.00002539
Iteration 393/1000 | Loss: 0.00002539
Iteration 394/1000 | Loss: 0.00002539
Iteration 395/1000 | Loss: 0.00002539
Iteration 396/1000 | Loss: 0.00002539
Iteration 397/1000 | Loss: 0.00002539
Iteration 398/1000 | Loss: 0.00002539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [2.5391189410584047e-05, 2.5391189410584047e-05, 2.5391189410584047e-05, 2.5391189410584047e-05, 2.5391189410584047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5391189410584047e-05

Optimization complete. Final v2v error: 3.7899930477142334 mm

Highest mean error: 5.670810222625732 mm for frame 107

Lowest mean error: 3.251971483230591 mm for frame 1

Saving results

Total time: 336.3726043701172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384272
Iteration 2/25 | Loss: 0.00117976
Iteration 3/25 | Loss: 0.00111041
Iteration 4/25 | Loss: 0.00110392
Iteration 5/25 | Loss: 0.00110182
Iteration 6/25 | Loss: 0.00110120
Iteration 7/25 | Loss: 0.00110120
Iteration 8/25 | Loss: 0.00110120
Iteration 9/25 | Loss: 0.00110120
Iteration 10/25 | Loss: 0.00110120
Iteration 11/25 | Loss: 0.00110120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011012026807293296, 0.0011012026807293296, 0.0011012026807293296, 0.0011012026807293296, 0.0011012026807293296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011012026807293296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81800294
Iteration 2/25 | Loss: 0.00343586
Iteration 3/25 | Loss: 0.00343586
Iteration 4/25 | Loss: 0.00343585
Iteration 5/25 | Loss: 0.00343585
Iteration 6/25 | Loss: 0.00343585
Iteration 7/25 | Loss: 0.00343585
Iteration 8/25 | Loss: 0.00343585
Iteration 9/25 | Loss: 0.00343585
Iteration 10/25 | Loss: 0.00343585
Iteration 11/25 | Loss: 0.00343585
Iteration 12/25 | Loss: 0.00343585
Iteration 13/25 | Loss: 0.00343585
Iteration 14/25 | Loss: 0.00343585
Iteration 15/25 | Loss: 0.00343585
Iteration 16/25 | Loss: 0.00343585
Iteration 17/25 | Loss: 0.00343585
Iteration 18/25 | Loss: 0.00343585
Iteration 19/25 | Loss: 0.00343585
Iteration 20/25 | Loss: 0.00343585
Iteration 21/25 | Loss: 0.00343585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003435851074755192, 0.003435851074755192, 0.003435851074755192, 0.003435851074755192, 0.003435851074755192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003435851074755192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00343585
Iteration 2/1000 | Loss: 0.00002390
Iteration 3/1000 | Loss: 0.00001427
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001087
Iteration 6/1000 | Loss: 0.00001014
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000939
Iteration 9/1000 | Loss: 0.00000917
Iteration 10/1000 | Loss: 0.00000914
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000907
Iteration 16/1000 | Loss: 0.00000905
Iteration 17/1000 | Loss: 0.00000905
Iteration 18/1000 | Loss: 0.00000905
Iteration 19/1000 | Loss: 0.00000903
Iteration 20/1000 | Loss: 0.00000896
Iteration 21/1000 | Loss: 0.00000893
Iteration 22/1000 | Loss: 0.00000892
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000888
Iteration 26/1000 | Loss: 0.00000888
Iteration 27/1000 | Loss: 0.00000887
Iteration 28/1000 | Loss: 0.00000887
Iteration 29/1000 | Loss: 0.00000887
Iteration 30/1000 | Loss: 0.00000887
Iteration 31/1000 | Loss: 0.00000887
Iteration 32/1000 | Loss: 0.00000886
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000884
Iteration 35/1000 | Loss: 0.00000882
Iteration 36/1000 | Loss: 0.00000881
Iteration 37/1000 | Loss: 0.00000881
Iteration 38/1000 | Loss: 0.00000881
Iteration 39/1000 | Loss: 0.00000881
Iteration 40/1000 | Loss: 0.00000881
Iteration 41/1000 | Loss: 0.00000881
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000881
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000881
Iteration 48/1000 | Loss: 0.00000880
Iteration 49/1000 | Loss: 0.00000880
Iteration 50/1000 | Loss: 0.00000880
Iteration 51/1000 | Loss: 0.00000880
Iteration 52/1000 | Loss: 0.00000880
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000879
Iteration 55/1000 | Loss: 0.00000879
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000876
Iteration 60/1000 | Loss: 0.00000876
Iteration 61/1000 | Loss: 0.00000876
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000875
Iteration 64/1000 | Loss: 0.00000875
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000872
Iteration 67/1000 | Loss: 0.00000872
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000872
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000871
Iteration 74/1000 | Loss: 0.00000870
Iteration 75/1000 | Loss: 0.00000870
Iteration 76/1000 | Loss: 0.00000870
Iteration 77/1000 | Loss: 0.00000870
Iteration 78/1000 | Loss: 0.00000869
Iteration 79/1000 | Loss: 0.00000869
Iteration 80/1000 | Loss: 0.00000869
Iteration 81/1000 | Loss: 0.00000869
Iteration 82/1000 | Loss: 0.00000869
Iteration 83/1000 | Loss: 0.00000869
Iteration 84/1000 | Loss: 0.00000868
Iteration 85/1000 | Loss: 0.00000868
Iteration 86/1000 | Loss: 0.00000868
Iteration 87/1000 | Loss: 0.00000868
Iteration 88/1000 | Loss: 0.00000868
Iteration 89/1000 | Loss: 0.00000868
Iteration 90/1000 | Loss: 0.00000868
Iteration 91/1000 | Loss: 0.00000868
Iteration 92/1000 | Loss: 0.00000867
Iteration 93/1000 | Loss: 0.00000867
Iteration 94/1000 | Loss: 0.00000867
Iteration 95/1000 | Loss: 0.00000867
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000866
Iteration 102/1000 | Loss: 0.00000866
Iteration 103/1000 | Loss: 0.00000866
Iteration 104/1000 | Loss: 0.00000866
Iteration 105/1000 | Loss: 0.00000866
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000866
Iteration 108/1000 | Loss: 0.00000866
Iteration 109/1000 | Loss: 0.00000866
Iteration 110/1000 | Loss: 0.00000866
Iteration 111/1000 | Loss: 0.00000866
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000866
Iteration 115/1000 | Loss: 0.00000866
Iteration 116/1000 | Loss: 0.00000866
Iteration 117/1000 | Loss: 0.00000866
Iteration 118/1000 | Loss: 0.00000866
Iteration 119/1000 | Loss: 0.00000866
Iteration 120/1000 | Loss: 0.00000866
Iteration 121/1000 | Loss: 0.00000865
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000865
Iteration 124/1000 | Loss: 0.00000865
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000865
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000865
Iteration 130/1000 | Loss: 0.00000865
Iteration 131/1000 | Loss: 0.00000865
Iteration 132/1000 | Loss: 0.00000865
Iteration 133/1000 | Loss: 0.00000865
Iteration 134/1000 | Loss: 0.00000865
Iteration 135/1000 | Loss: 0.00000865
Iteration 136/1000 | Loss: 0.00000865
Iteration 137/1000 | Loss: 0.00000865
Iteration 138/1000 | Loss: 0.00000865
Iteration 139/1000 | Loss: 0.00000865
Iteration 140/1000 | Loss: 0.00000865
Iteration 141/1000 | Loss: 0.00000865
Iteration 142/1000 | Loss: 0.00000865
Iteration 143/1000 | Loss: 0.00000865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [8.651934876979794e-06, 8.651934876979794e-06, 8.651934876979794e-06, 8.651934876979794e-06, 8.651934876979794e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.651934876979794e-06

Optimization complete. Final v2v error: 2.4463889598846436 mm

Highest mean error: 2.613454580307007 mm for frame 112

Lowest mean error: 2.234334707260132 mm for frame 6

Saving results

Total time: 33.320284605026245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773569
Iteration 2/25 | Loss: 0.00153531
Iteration 3/25 | Loss: 0.00120855
Iteration 4/25 | Loss: 0.00117917
Iteration 5/25 | Loss: 0.00117906
Iteration 6/25 | Loss: 0.00117538
Iteration 7/25 | Loss: 0.00115858
Iteration 8/25 | Loss: 0.00114907
Iteration 9/25 | Loss: 0.00114692
Iteration 10/25 | Loss: 0.00114648
Iteration 11/25 | Loss: 0.00114637
Iteration 12/25 | Loss: 0.00114636
Iteration 13/25 | Loss: 0.00114636
Iteration 14/25 | Loss: 0.00114636
Iteration 15/25 | Loss: 0.00114636
Iteration 16/25 | Loss: 0.00114636
Iteration 17/25 | Loss: 0.00114636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011463647242635489, 0.0011463647242635489, 0.0011463647242635489, 0.0011463647242635489, 0.0011463647242635489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011463647242635489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.25371742
Iteration 2/25 | Loss: 0.00285765
Iteration 3/25 | Loss: 0.00285753
Iteration 4/25 | Loss: 0.00285753
Iteration 5/25 | Loss: 0.00285753
Iteration 6/25 | Loss: 0.00285753
Iteration 7/25 | Loss: 0.00285753
Iteration 8/25 | Loss: 0.00285752
Iteration 9/25 | Loss: 0.00285752
Iteration 10/25 | Loss: 0.00285752
Iteration 11/25 | Loss: 0.00285752
Iteration 12/25 | Loss: 0.00285752
Iteration 13/25 | Loss: 0.00285752
Iteration 14/25 | Loss: 0.00285752
Iteration 15/25 | Loss: 0.00285752
Iteration 16/25 | Loss: 0.00285752
Iteration 17/25 | Loss: 0.00285752
Iteration 18/25 | Loss: 0.00285752
Iteration 19/25 | Loss: 0.00285752
Iteration 20/25 | Loss: 0.00285752
Iteration 21/25 | Loss: 0.00285752
Iteration 22/25 | Loss: 0.00285752
Iteration 23/25 | Loss: 0.00285752
Iteration 24/25 | Loss: 0.00285752
Iteration 25/25 | Loss: 0.00285752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285752
Iteration 2/1000 | Loss: 0.00004867
Iteration 3/1000 | Loss: 0.00002301
Iteration 4/1000 | Loss: 0.00001854
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001515
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001363
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001361
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001356
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001355
Iteration 42/1000 | Loss: 0.00001355
Iteration 43/1000 | Loss: 0.00001355
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001354
Iteration 46/1000 | Loss: 0.00001354
Iteration 47/1000 | Loss: 0.00001354
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001353
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001352
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001351
Iteration 61/1000 | Loss: 0.00001351
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001349
Iteration 71/1000 | Loss: 0.00001349
Iteration 72/1000 | Loss: 0.00001349
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.3492968719219789e-05, 1.3492968719219789e-05, 1.3492968719219789e-05, 1.3492968719219789e-05, 1.3492968719219789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3492968719219789e-05

Optimization complete. Final v2v error: 3.0204784870147705 mm

Highest mean error: 3.7387149333953857 mm for frame 25

Lowest mean error: 2.647284746170044 mm for frame 41

Saving results

Total time: 47.491883993148804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096767
Iteration 2/25 | Loss: 0.00170649
Iteration 3/25 | Loss: 0.00172575
Iteration 4/25 | Loss: 0.00123107
Iteration 5/25 | Loss: 0.00116906
Iteration 6/25 | Loss: 0.00114765
Iteration 7/25 | Loss: 0.00113314
Iteration 8/25 | Loss: 0.00112910
Iteration 9/25 | Loss: 0.00112565
Iteration 10/25 | Loss: 0.00112430
Iteration 11/25 | Loss: 0.00112397
Iteration 12/25 | Loss: 0.00112381
Iteration 13/25 | Loss: 0.00112378
Iteration 14/25 | Loss: 0.00112378
Iteration 15/25 | Loss: 0.00112378
Iteration 16/25 | Loss: 0.00112377
Iteration 17/25 | Loss: 0.00112377
Iteration 18/25 | Loss: 0.00112377
Iteration 19/25 | Loss: 0.00112377
Iteration 20/25 | Loss: 0.00112377
Iteration 21/25 | Loss: 0.00112377
Iteration 22/25 | Loss: 0.00112377
Iteration 23/25 | Loss: 0.00112377
Iteration 24/25 | Loss: 0.00112377
Iteration 25/25 | Loss: 0.00112376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.67475510
Iteration 2/25 | Loss: 0.00331992
Iteration 3/25 | Loss: 0.00323243
Iteration 4/25 | Loss: 0.00323243
Iteration 5/25 | Loss: 0.00323243
Iteration 6/25 | Loss: 0.00323243
Iteration 7/25 | Loss: 0.00323242
Iteration 8/25 | Loss: 0.00323242
Iteration 9/25 | Loss: 0.00323242
Iteration 10/25 | Loss: 0.00323242
Iteration 11/25 | Loss: 0.00323242
Iteration 12/25 | Loss: 0.00323242
Iteration 13/25 | Loss: 0.00323242
Iteration 14/25 | Loss: 0.00323242
Iteration 15/25 | Loss: 0.00323242
Iteration 16/25 | Loss: 0.00323242
Iteration 17/25 | Loss: 0.00323242
Iteration 18/25 | Loss: 0.00323242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003232423448935151, 0.003232423448935151, 0.003232423448935151, 0.003232423448935151, 0.003232423448935151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003232423448935151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00323242
Iteration 2/1000 | Loss: 0.00004266
Iteration 3/1000 | Loss: 0.00002498
Iteration 4/1000 | Loss: 0.00006335
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00008430
Iteration 7/1000 | Loss: 0.00004601
Iteration 8/1000 | Loss: 0.00001690
Iteration 9/1000 | Loss: 0.00004753
Iteration 10/1000 | Loss: 0.00008461
Iteration 11/1000 | Loss: 0.00002904
Iteration 12/1000 | Loss: 0.00002743
Iteration 13/1000 | Loss: 0.00005807
Iteration 14/1000 | Loss: 0.00002053
Iteration 15/1000 | Loss: 0.00003238
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001533
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00002004
Iteration 22/1000 | Loss: 0.00001590
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001464
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00002437
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00002008
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001458
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00004569
Iteration 49/1000 | Loss: 0.00001503
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001454
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001695
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001451
Iteration 62/1000 | Loss: 0.00003584
Iteration 63/1000 | Loss: 0.00003584
Iteration 64/1000 | Loss: 0.00013595
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001447
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00003070
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001445
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.4449363334279042e-05, 1.4449363334279042e-05, 1.4449363334279042e-05, 1.4449363334279042e-05, 1.4449363334279042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4449363334279042e-05

Optimization complete. Final v2v error: 3.2002322673797607 mm

Highest mean error: 8.64159107208252 mm for frame 62

Lowest mean error: 2.6729838848114014 mm for frame 109

Saving results

Total time: 70.29167652130127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926313
Iteration 2/25 | Loss: 0.00130488
Iteration 3/25 | Loss: 0.00116082
Iteration 4/25 | Loss: 0.00114670
Iteration 5/25 | Loss: 0.00114356
Iteration 6/25 | Loss: 0.00114346
Iteration 7/25 | Loss: 0.00114346
Iteration 8/25 | Loss: 0.00114346
Iteration 9/25 | Loss: 0.00114309
Iteration 10/25 | Loss: 0.00114309
Iteration 11/25 | Loss: 0.00114309
Iteration 12/25 | Loss: 0.00114309
Iteration 13/25 | Loss: 0.00114309
Iteration 14/25 | Loss: 0.00114309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011430935701355338, 0.0011430935701355338, 0.0011430935701355338, 0.0011430935701355338, 0.0011430935701355338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011430935701355338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.52197289
Iteration 2/25 | Loss: 0.00326025
Iteration 3/25 | Loss: 0.00326025
Iteration 4/25 | Loss: 0.00326024
Iteration 5/25 | Loss: 0.00326024
Iteration 6/25 | Loss: 0.00326024
Iteration 7/25 | Loss: 0.00326024
Iteration 8/25 | Loss: 0.00326024
Iteration 9/25 | Loss: 0.00326024
Iteration 10/25 | Loss: 0.00326024
Iteration 11/25 | Loss: 0.00326024
Iteration 12/25 | Loss: 0.00326024
Iteration 13/25 | Loss: 0.00326024
Iteration 14/25 | Loss: 0.00326024
Iteration 15/25 | Loss: 0.00326024
Iteration 16/25 | Loss: 0.00326024
Iteration 17/25 | Loss: 0.00326024
Iteration 18/25 | Loss: 0.00326024
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0032602420542389154, 0.0032602420542389154, 0.0032602420542389154, 0.0032602420542389154, 0.0032602420542389154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032602420542389154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326024
Iteration 2/1000 | Loss: 0.00003123
Iteration 3/1000 | Loss: 0.00001839
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001307
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001260
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001190
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001181
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001170
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001153
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001149
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1489289136079606e-05, 1.1489289136079606e-05, 1.1489289136079606e-05, 1.1489289136079606e-05, 1.1489289136079606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1489289136079606e-05

Optimization complete. Final v2v error: 2.8130619525909424 mm

Highest mean error: 3.1821517944335938 mm for frame 209

Lowest mean error: 2.3941400051116943 mm for frame 0

Saving results

Total time: 34.77505326271057
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003564
Iteration 2/25 | Loss: 0.00251770
Iteration 3/25 | Loss: 0.00177940
Iteration 4/25 | Loss: 0.00183216
Iteration 5/25 | Loss: 0.00157152
Iteration 6/25 | Loss: 0.00138981
Iteration 7/25 | Loss: 0.00132372
Iteration 8/25 | Loss: 0.00128265
Iteration 9/25 | Loss: 0.00127649
Iteration 10/25 | Loss: 0.00127272
Iteration 11/25 | Loss: 0.00127829
Iteration 12/25 | Loss: 0.00124847
Iteration 13/25 | Loss: 0.00122857
Iteration 14/25 | Loss: 0.00122665
Iteration 15/25 | Loss: 0.00122659
Iteration 16/25 | Loss: 0.00122626
Iteration 17/25 | Loss: 0.00122432
Iteration 18/25 | Loss: 0.00122375
Iteration 19/25 | Loss: 0.00122585
Iteration 20/25 | Loss: 0.00122633
Iteration 21/25 | Loss: 0.00122637
Iteration 22/25 | Loss: 0.00122603
Iteration 23/25 | Loss: 0.00122291
Iteration 24/25 | Loss: 0.00122186
Iteration 25/25 | Loss: 0.00122183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03557599
Iteration 2/25 | Loss: 0.00378272
Iteration 3/25 | Loss: 0.00378272
Iteration 4/25 | Loss: 0.00378272
Iteration 5/25 | Loss: 0.00378272
Iteration 6/25 | Loss: 0.00378272
Iteration 7/25 | Loss: 0.00378272
Iteration 8/25 | Loss: 0.00378272
Iteration 9/25 | Loss: 0.00378272
Iteration 10/25 | Loss: 0.00378272
Iteration 11/25 | Loss: 0.00378272
Iteration 12/25 | Loss: 0.00378272
Iteration 13/25 | Loss: 0.00378272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003782720072194934, 0.003782720072194934, 0.003782720072194934, 0.003782720072194934, 0.003782720072194934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003782720072194934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00378272
Iteration 2/1000 | Loss: 0.00004396
Iteration 3/1000 | Loss: 0.00002477
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001852
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001769
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001732
Iteration 12/1000 | Loss: 0.00001723
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001718
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001715
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001715
Iteration 25/1000 | Loss: 0.00001715
Iteration 26/1000 | Loss: 0.00001715
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001706
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001704
Iteration 57/1000 | Loss: 0.00001704
Iteration 58/1000 | Loss: 0.00001704
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001703
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001701
Iteration 68/1000 | Loss: 0.00001701
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001701
Iteration 72/1000 | Loss: 0.00001701
Iteration 73/1000 | Loss: 0.00001701
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001700
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001699
Iteration 89/1000 | Loss: 0.00001699
Iteration 90/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.6994446923490614e-05, 1.6994446923490614e-05, 1.6994446923490614e-05, 1.6994446923490614e-05, 1.6994446923490614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6994446923490614e-05

Optimization complete. Final v2v error: 3.40539288520813 mm

Highest mean error: 3.6592960357666016 mm for frame 9

Lowest mean error: 3.0873448848724365 mm for frame 153

Saving results

Total time: 66.37527132034302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610599
Iteration 2/25 | Loss: 0.00134608
Iteration 3/25 | Loss: 0.00116122
Iteration 4/25 | Loss: 0.00113970
Iteration 5/25 | Loss: 0.00113543
Iteration 6/25 | Loss: 0.00113456
Iteration 7/25 | Loss: 0.00113456
Iteration 8/25 | Loss: 0.00113456
Iteration 9/25 | Loss: 0.00113456
Iteration 10/25 | Loss: 0.00113456
Iteration 11/25 | Loss: 0.00113456
Iteration 12/25 | Loss: 0.00113456
Iteration 13/25 | Loss: 0.00113456
Iteration 14/25 | Loss: 0.00113456
Iteration 15/25 | Loss: 0.00113456
Iteration 16/25 | Loss: 0.00113456
Iteration 17/25 | Loss: 0.00113456
Iteration 18/25 | Loss: 0.00113456
Iteration 19/25 | Loss: 0.00113456
Iteration 20/25 | Loss: 0.00113456
Iteration 21/25 | Loss: 0.00113456
Iteration 22/25 | Loss: 0.00113456
Iteration 23/25 | Loss: 0.00113456
Iteration 24/25 | Loss: 0.00113456
Iteration 25/25 | Loss: 0.00113456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19930911
Iteration 2/25 | Loss: 0.00286517
Iteration 3/25 | Loss: 0.00286515
Iteration 4/25 | Loss: 0.00286515
Iteration 5/25 | Loss: 0.00286515
Iteration 6/25 | Loss: 0.00286515
Iteration 7/25 | Loss: 0.00286515
Iteration 8/25 | Loss: 0.00286515
Iteration 9/25 | Loss: 0.00286515
Iteration 10/25 | Loss: 0.00286515
Iteration 11/25 | Loss: 0.00286515
Iteration 12/25 | Loss: 0.00286515
Iteration 13/25 | Loss: 0.00286515
Iteration 14/25 | Loss: 0.00286515
Iteration 15/25 | Loss: 0.00286515
Iteration 16/25 | Loss: 0.00286515
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002865150338038802, 0.002865150338038802, 0.002865150338038802, 0.002865150338038802, 0.002865150338038802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002865150338038802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286515
Iteration 2/1000 | Loss: 0.00003204
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001561
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001305
Iteration 13/1000 | Loss: 0.00001301
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001290
Iteration 17/1000 | Loss: 0.00001289
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001283
Iteration 22/1000 | Loss: 0.00001283
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001272
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001272
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001271
Iteration 38/1000 | Loss: 0.00001271
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001270
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001260
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001258
Iteration 121/1000 | Loss: 0.00001258
Iteration 122/1000 | Loss: 0.00001258
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001258
Iteration 125/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.2582270755956415e-05, 1.2582270755956415e-05, 1.2582270755956415e-05, 1.2582270755956415e-05, 1.2582270755956415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2582270755956415e-05

Optimization complete. Final v2v error: 2.9264795780181885 mm

Highest mean error: 3.6928822994232178 mm for frame 68

Lowest mean error: 2.5742275714874268 mm for frame 31

Saving results

Total time: 38.455949783325195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566693
Iteration 2/25 | Loss: 0.00138659
Iteration 3/25 | Loss: 0.00122050
Iteration 4/25 | Loss: 0.00120464
Iteration 5/25 | Loss: 0.00119909
Iteration 6/25 | Loss: 0.00119842
Iteration 7/25 | Loss: 0.00119842
Iteration 8/25 | Loss: 0.00119842
Iteration 9/25 | Loss: 0.00119842
Iteration 10/25 | Loss: 0.00119842
Iteration 11/25 | Loss: 0.00119842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011984177399426699, 0.0011984177399426699, 0.0011984177399426699, 0.0011984177399426699, 0.0011984177399426699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011984177399426699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50540632
Iteration 2/25 | Loss: 0.00200546
Iteration 3/25 | Loss: 0.00200545
Iteration 4/25 | Loss: 0.00200545
Iteration 5/25 | Loss: 0.00200545
Iteration 6/25 | Loss: 0.00200545
Iteration 7/25 | Loss: 0.00200545
Iteration 8/25 | Loss: 0.00200545
Iteration 9/25 | Loss: 0.00200545
Iteration 10/25 | Loss: 0.00200545
Iteration 11/25 | Loss: 0.00200545
Iteration 12/25 | Loss: 0.00200545
Iteration 13/25 | Loss: 0.00200545
Iteration 14/25 | Loss: 0.00200545
Iteration 15/25 | Loss: 0.00200545
Iteration 16/25 | Loss: 0.00200545
Iteration 17/25 | Loss: 0.00200545
Iteration 18/25 | Loss: 0.00200545
Iteration 19/25 | Loss: 0.00200545
Iteration 20/25 | Loss: 0.00200545
Iteration 21/25 | Loss: 0.00200545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0020054453052580357, 0.0020054453052580357, 0.0020054453052580357, 0.0020054453052580357, 0.0020054453052580357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020054453052580357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200545
Iteration 2/1000 | Loss: 0.00004029
Iteration 3/1000 | Loss: 0.00003062
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002608
Iteration 7/1000 | Loss: 0.00002572
Iteration 8/1000 | Loss: 0.00002539
Iteration 9/1000 | Loss: 0.00002513
Iteration 10/1000 | Loss: 0.00002494
Iteration 11/1000 | Loss: 0.00002471
Iteration 12/1000 | Loss: 0.00002444
Iteration 13/1000 | Loss: 0.00002419
Iteration 14/1000 | Loss: 0.00002401
Iteration 15/1000 | Loss: 0.00002401
Iteration 16/1000 | Loss: 0.00002401
Iteration 17/1000 | Loss: 0.00002389
Iteration 18/1000 | Loss: 0.00002374
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002343
Iteration 21/1000 | Loss: 0.00002326
Iteration 22/1000 | Loss: 0.00002322
Iteration 23/1000 | Loss: 0.00002319
Iteration 24/1000 | Loss: 0.00002319
Iteration 25/1000 | Loss: 0.00002318
Iteration 26/1000 | Loss: 0.00002316
Iteration 27/1000 | Loss: 0.00002314
Iteration 28/1000 | Loss: 0.00002314
Iteration 29/1000 | Loss: 0.00002314
Iteration 30/1000 | Loss: 0.00002314
Iteration 31/1000 | Loss: 0.00002314
Iteration 32/1000 | Loss: 0.00002314
Iteration 33/1000 | Loss: 0.00002314
Iteration 34/1000 | Loss: 0.00002314
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002313
Iteration 38/1000 | Loss: 0.00002311
Iteration 39/1000 | Loss: 0.00002310
Iteration 40/1000 | Loss: 0.00002310
Iteration 41/1000 | Loss: 0.00002310
Iteration 42/1000 | Loss: 0.00002310
Iteration 43/1000 | Loss: 0.00002310
Iteration 44/1000 | Loss: 0.00002310
Iteration 45/1000 | Loss: 0.00002309
Iteration 46/1000 | Loss: 0.00002309
Iteration 47/1000 | Loss: 0.00002308
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002307
Iteration 51/1000 | Loss: 0.00002307
Iteration 52/1000 | Loss: 0.00002307
Iteration 53/1000 | Loss: 0.00002307
Iteration 54/1000 | Loss: 0.00002307
Iteration 55/1000 | Loss: 0.00002306
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002304
Iteration 59/1000 | Loss: 0.00002303
Iteration 60/1000 | Loss: 0.00002301
Iteration 61/1000 | Loss: 0.00002300
Iteration 62/1000 | Loss: 0.00002300
Iteration 63/1000 | Loss: 0.00002300
Iteration 64/1000 | Loss: 0.00002300
Iteration 65/1000 | Loss: 0.00002300
Iteration 66/1000 | Loss: 0.00002300
Iteration 67/1000 | Loss: 0.00002300
Iteration 68/1000 | Loss: 0.00002300
Iteration 69/1000 | Loss: 0.00002299
Iteration 70/1000 | Loss: 0.00002299
Iteration 71/1000 | Loss: 0.00002299
Iteration 72/1000 | Loss: 0.00002299
Iteration 73/1000 | Loss: 0.00002299
Iteration 74/1000 | Loss: 0.00002299
Iteration 75/1000 | Loss: 0.00002299
Iteration 76/1000 | Loss: 0.00002299
Iteration 77/1000 | Loss: 0.00002299
Iteration 78/1000 | Loss: 0.00002298
Iteration 79/1000 | Loss: 0.00002297
Iteration 80/1000 | Loss: 0.00002297
Iteration 81/1000 | Loss: 0.00002297
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002296
Iteration 84/1000 | Loss: 0.00002296
Iteration 85/1000 | Loss: 0.00002296
Iteration 86/1000 | Loss: 0.00002296
Iteration 87/1000 | Loss: 0.00002296
Iteration 88/1000 | Loss: 0.00002295
Iteration 89/1000 | Loss: 0.00002295
Iteration 90/1000 | Loss: 0.00002295
Iteration 91/1000 | Loss: 0.00002295
Iteration 92/1000 | Loss: 0.00002295
Iteration 93/1000 | Loss: 0.00002295
Iteration 94/1000 | Loss: 0.00002295
Iteration 95/1000 | Loss: 0.00002295
Iteration 96/1000 | Loss: 0.00002295
Iteration 97/1000 | Loss: 0.00002295
Iteration 98/1000 | Loss: 0.00002295
Iteration 99/1000 | Loss: 0.00002295
Iteration 100/1000 | Loss: 0.00002295
Iteration 101/1000 | Loss: 0.00002295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.2949505364522338e-05, 2.2949505364522338e-05, 2.2949505364522338e-05, 2.2949505364522338e-05, 2.2949505364522338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2949505364522338e-05

Optimization complete. Final v2v error: 3.8616273403167725 mm

Highest mean error: 4.038621425628662 mm for frame 150

Lowest mean error: 3.680431604385376 mm for frame 239

Saving results

Total time: 49.12804841995239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756385
Iteration 2/25 | Loss: 0.00127047
Iteration 3/25 | Loss: 0.00116389
Iteration 4/25 | Loss: 0.00115179
Iteration 5/25 | Loss: 0.00114901
Iteration 6/25 | Loss: 0.00114859
Iteration 7/25 | Loss: 0.00114859
Iteration 8/25 | Loss: 0.00114859
Iteration 9/25 | Loss: 0.00114859
Iteration 10/25 | Loss: 0.00114859
Iteration 11/25 | Loss: 0.00114859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011485856957733631, 0.0011485856957733631, 0.0011485856957733631, 0.0011485856957733631, 0.0011485856957733631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011485856957733631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24267113
Iteration 2/25 | Loss: 0.00324587
Iteration 3/25 | Loss: 0.00324587
Iteration 4/25 | Loss: 0.00324587
Iteration 5/25 | Loss: 0.00324587
Iteration 6/25 | Loss: 0.00324587
Iteration 7/25 | Loss: 0.00324587
Iteration 8/25 | Loss: 0.00324587
Iteration 9/25 | Loss: 0.00324587
Iteration 10/25 | Loss: 0.00324587
Iteration 11/25 | Loss: 0.00324587
Iteration 12/25 | Loss: 0.00324587
Iteration 13/25 | Loss: 0.00324587
Iteration 14/25 | Loss: 0.00324587
Iteration 15/25 | Loss: 0.00324587
Iteration 16/25 | Loss: 0.00324587
Iteration 17/25 | Loss: 0.00324587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0032458666246384382, 0.0032458666246384382, 0.0032458666246384382, 0.0032458666246384382, 0.0032458666246384382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032458666246384382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00324587
Iteration 2/1000 | Loss: 0.00003585
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001378
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001197
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001196
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001192
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001184
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001182
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001179
Iteration 58/1000 | Loss: 0.00001179
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001172
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001171
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00001171
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.169706229120493e-05, 1.169706229120493e-05, 1.169706229120493e-05, 1.169706229120493e-05, 1.169706229120493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.169706229120493e-05

Optimization complete. Final v2v error: 2.880450487136841 mm

Highest mean error: 3.1814780235290527 mm for frame 74

Lowest mean error: 2.468740224838257 mm for frame 36

Saving results

Total time: 37.58993411064148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879764
Iteration 2/25 | Loss: 0.00126972
Iteration 3/25 | Loss: 0.00114415
Iteration 4/25 | Loss: 0.00113176
Iteration 5/25 | Loss: 0.00112771
Iteration 6/25 | Loss: 0.00112641
Iteration 7/25 | Loss: 0.00112641
Iteration 8/25 | Loss: 0.00112641
Iteration 9/25 | Loss: 0.00112641
Iteration 10/25 | Loss: 0.00112641
Iteration 11/25 | Loss: 0.00112641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011264068307355046, 0.0011264068307355046, 0.0011264068307355046, 0.0011264068307355046, 0.0011264068307355046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011264068307355046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83643734
Iteration 2/25 | Loss: 0.00306513
Iteration 3/25 | Loss: 0.00306513
Iteration 4/25 | Loss: 0.00306513
Iteration 5/25 | Loss: 0.00306513
Iteration 6/25 | Loss: 0.00306513
Iteration 7/25 | Loss: 0.00306513
Iteration 8/25 | Loss: 0.00306513
Iteration 9/25 | Loss: 0.00306512
Iteration 10/25 | Loss: 0.00306512
Iteration 11/25 | Loss: 0.00306512
Iteration 12/25 | Loss: 0.00306512
Iteration 13/25 | Loss: 0.00306512
Iteration 14/25 | Loss: 0.00306512
Iteration 15/25 | Loss: 0.00306512
Iteration 16/25 | Loss: 0.00306512
Iteration 17/25 | Loss: 0.00306512
Iteration 18/25 | Loss: 0.00306512
Iteration 19/25 | Loss: 0.00306512
Iteration 20/25 | Loss: 0.00306512
Iteration 21/25 | Loss: 0.00306512
Iteration 22/25 | Loss: 0.00306512
Iteration 23/25 | Loss: 0.00306512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0030651239212602377, 0.0030651239212602377, 0.0030651239212602377, 0.0030651239212602377, 0.0030651239212602377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030651239212602377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306512
Iteration 2/1000 | Loss: 0.00003183
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001623
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001301
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001187
Iteration 29/1000 | Loss: 0.00001187
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001187
Iteration 36/1000 | Loss: 0.00001187
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001186
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001179
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001179
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001174
Iteration 92/1000 | Loss: 0.00001174
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.17365989353857e-05, 1.17365989353857e-05, 1.17365989353857e-05, 1.17365989353857e-05, 1.17365989353857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.17365989353857e-05

Optimization complete. Final v2v error: 2.8626818656921387 mm

Highest mean error: 3.8533411026000977 mm for frame 91

Lowest mean error: 2.472116470336914 mm for frame 2

Saving results

Total time: 34.982869386672974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887208
Iteration 2/25 | Loss: 0.00163622
Iteration 3/25 | Loss: 0.00130314
Iteration 4/25 | Loss: 0.00126677
Iteration 5/25 | Loss: 0.00121906
Iteration 6/25 | Loss: 0.00116642
Iteration 7/25 | Loss: 0.00116427
Iteration 8/25 | Loss: 0.00116367
Iteration 9/25 | Loss: 0.00116354
Iteration 10/25 | Loss: 0.00116351
Iteration 11/25 | Loss: 0.00116351
Iteration 12/25 | Loss: 0.00116351
Iteration 13/25 | Loss: 0.00116351
Iteration 14/25 | Loss: 0.00116351
Iteration 15/25 | Loss: 0.00116351
Iteration 16/25 | Loss: 0.00116351
Iteration 17/25 | Loss: 0.00116351
Iteration 18/25 | Loss: 0.00116351
Iteration 19/25 | Loss: 0.00116350
Iteration 20/25 | Loss: 0.00116350
Iteration 21/25 | Loss: 0.00116350
Iteration 22/25 | Loss: 0.00116350
Iteration 23/25 | Loss: 0.00116350
Iteration 24/25 | Loss: 0.00116350
Iteration 25/25 | Loss: 0.00116350

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74149954
Iteration 2/25 | Loss: 0.00211709
Iteration 3/25 | Loss: 0.00211708
Iteration 4/25 | Loss: 0.00211708
Iteration 5/25 | Loss: 0.00211708
Iteration 6/25 | Loss: 0.00211708
Iteration 7/25 | Loss: 0.00211708
Iteration 8/25 | Loss: 0.00211708
Iteration 9/25 | Loss: 0.00211708
Iteration 10/25 | Loss: 0.00211708
Iteration 11/25 | Loss: 0.00211708
Iteration 12/25 | Loss: 0.00211708
Iteration 13/25 | Loss: 0.00211708
Iteration 14/25 | Loss: 0.00211708
Iteration 15/25 | Loss: 0.00211708
Iteration 16/25 | Loss: 0.00211708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021170785184949636, 0.0021170785184949636, 0.0021170785184949636, 0.0021170785184949636, 0.0021170785184949636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021170785184949636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211708
Iteration 2/1000 | Loss: 0.00003817
Iteration 3/1000 | Loss: 0.00002856
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002217
Iteration 6/1000 | Loss: 0.00002116
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001973
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001955
Iteration 17/1000 | Loss: 0.00001955
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001954
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001952
Iteration 29/1000 | Loss: 0.00001952
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001940
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001938
Iteration 38/1000 | Loss: 0.00001938
Iteration 39/1000 | Loss: 0.00001938
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001937
Iteration 43/1000 | Loss: 0.00001937
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001936
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001936
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001935
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00001935
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001934
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001933
Iteration 69/1000 | Loss: 0.00001933
Iteration 70/1000 | Loss: 0.00001933
Iteration 71/1000 | Loss: 0.00001933
Iteration 72/1000 | Loss: 0.00001933
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001932
Iteration 76/1000 | Loss: 0.00001932
Iteration 77/1000 | Loss: 0.00001932
Iteration 78/1000 | Loss: 0.00001932
Iteration 79/1000 | Loss: 0.00001932
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001931
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00001931
Iteration 86/1000 | Loss: 0.00001931
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00001931
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001931
Iteration 97/1000 | Loss: 0.00001931
Iteration 98/1000 | Loss: 0.00001931
Iteration 99/1000 | Loss: 0.00001931
Iteration 100/1000 | Loss: 0.00001931
Iteration 101/1000 | Loss: 0.00001931
Iteration 102/1000 | Loss: 0.00001931
Iteration 103/1000 | Loss: 0.00001931
Iteration 104/1000 | Loss: 0.00001931
Iteration 105/1000 | Loss: 0.00001931
Iteration 106/1000 | Loss: 0.00001931
Iteration 107/1000 | Loss: 0.00001931
Iteration 108/1000 | Loss: 0.00001931
Iteration 109/1000 | Loss: 0.00001931
Iteration 110/1000 | Loss: 0.00001931
Iteration 111/1000 | Loss: 0.00001931
Iteration 112/1000 | Loss: 0.00001931
Iteration 113/1000 | Loss: 0.00001931
Iteration 114/1000 | Loss: 0.00001931
Iteration 115/1000 | Loss: 0.00001931
Iteration 116/1000 | Loss: 0.00001931
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001931
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001931
Iteration 125/1000 | Loss: 0.00001931
Iteration 126/1000 | Loss: 0.00001931
Iteration 127/1000 | Loss: 0.00001931
Iteration 128/1000 | Loss: 0.00001931
Iteration 129/1000 | Loss: 0.00001931
Iteration 130/1000 | Loss: 0.00001931
Iteration 131/1000 | Loss: 0.00001931
Iteration 132/1000 | Loss: 0.00001931
Iteration 133/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.9309058188810013e-05, 1.9309058188810013e-05, 1.9309058188810013e-05, 1.9309058188810013e-05, 1.9309058188810013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9309058188810013e-05

Optimization complete. Final v2v error: 3.669650077819824 mm

Highest mean error: 3.81840443611145 mm for frame 22

Lowest mean error: 3.56060528755188 mm for frame 97

Saving results

Total time: 40.2615008354187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508324
Iteration 2/25 | Loss: 0.00150440
Iteration 3/25 | Loss: 0.00119487
Iteration 4/25 | Loss: 0.00116886
Iteration 5/25 | Loss: 0.00116527
Iteration 6/25 | Loss: 0.00116455
Iteration 7/25 | Loss: 0.00116455
Iteration 8/25 | Loss: 0.00116455
Iteration 9/25 | Loss: 0.00116455
Iteration 10/25 | Loss: 0.00116455
Iteration 11/25 | Loss: 0.00116455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011645533377304673, 0.0011645533377304673, 0.0011645533377304673, 0.0011645533377304673, 0.0011645533377304673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011645533377304673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15296650
Iteration 2/25 | Loss: 0.00310164
Iteration 3/25 | Loss: 0.00310163
Iteration 4/25 | Loss: 0.00310163
Iteration 5/25 | Loss: 0.00310163
Iteration 6/25 | Loss: 0.00310163
Iteration 7/25 | Loss: 0.00310163
Iteration 8/25 | Loss: 0.00310163
Iteration 9/25 | Loss: 0.00310163
Iteration 10/25 | Loss: 0.00310163
Iteration 11/25 | Loss: 0.00310163
Iteration 12/25 | Loss: 0.00310163
Iteration 13/25 | Loss: 0.00310163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0031016303692013025, 0.0031016303692013025, 0.0031016303692013025, 0.0031016303692013025, 0.0031016303692013025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031016303692013025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310163
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002602
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001735
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001533
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001427
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001404
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00001397
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001396
Iteration 41/1000 | Loss: 0.00001396
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001395
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001391
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001384
Iteration 68/1000 | Loss: 0.00001384
Iteration 69/1000 | Loss: 0.00001384
Iteration 70/1000 | Loss: 0.00001384
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001384
Iteration 73/1000 | Loss: 0.00001384
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001383
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001381
Iteration 89/1000 | Loss: 0.00001381
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001380
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001380
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001379
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001378
Iteration 123/1000 | Loss: 0.00001378
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001377
Iteration 128/1000 | Loss: 0.00001377
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.3768395547231194e-05, 1.3768395547231194e-05, 1.3768395547231194e-05, 1.3768395547231194e-05, 1.3768395547231194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3768395547231194e-05

Optimization complete. Final v2v error: 3.017700672149658 mm

Highest mean error: 4.584785461425781 mm for frame 91

Lowest mean error: 2.5413103103637695 mm for frame 11

Saving results

Total time: 39.71446514129639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839818
Iteration 2/25 | Loss: 0.00120258
Iteration 3/25 | Loss: 0.00111616
Iteration 4/25 | Loss: 0.00110281
Iteration 5/25 | Loss: 0.00109969
Iteration 6/25 | Loss: 0.00109915
Iteration 7/25 | Loss: 0.00109915
Iteration 8/25 | Loss: 0.00109915
Iteration 9/25 | Loss: 0.00109915
Iteration 10/25 | Loss: 0.00109915
Iteration 11/25 | Loss: 0.00109915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010991491144523025, 0.0010991491144523025, 0.0010991491144523025, 0.0010991491144523025, 0.0010991491144523025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010991491144523025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14795232
Iteration 2/25 | Loss: 0.00347035
Iteration 3/25 | Loss: 0.00347035
Iteration 4/25 | Loss: 0.00347035
Iteration 5/25 | Loss: 0.00347035
Iteration 6/25 | Loss: 0.00347034
Iteration 7/25 | Loss: 0.00347034
Iteration 8/25 | Loss: 0.00347034
Iteration 9/25 | Loss: 0.00347034
Iteration 10/25 | Loss: 0.00347034
Iteration 11/25 | Loss: 0.00347034
Iteration 12/25 | Loss: 0.00347034
Iteration 13/25 | Loss: 0.00347034
Iteration 14/25 | Loss: 0.00347034
Iteration 15/25 | Loss: 0.00347034
Iteration 16/25 | Loss: 0.00347034
Iteration 17/25 | Loss: 0.00347034
Iteration 18/25 | Loss: 0.00347034
Iteration 19/25 | Loss: 0.00347034
Iteration 20/25 | Loss: 0.00347034
Iteration 21/25 | Loss: 0.00347034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0034703435376286507, 0.0034703435376286507, 0.0034703435376286507, 0.0034703435376286507, 0.0034703435376286507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034703435376286507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00347034
Iteration 2/1000 | Loss: 0.00004010
Iteration 3/1000 | Loss: 0.00002274
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001498
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001289
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001177
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001177
Iteration 46/1000 | Loss: 0.00001177
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001174
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001174
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001170
Iteration 157/1000 | Loss: 0.00001170
Iteration 158/1000 | Loss: 0.00001170
Iteration 159/1000 | Loss: 0.00001170
Iteration 160/1000 | Loss: 0.00001170
Iteration 161/1000 | Loss: 0.00001170
Iteration 162/1000 | Loss: 0.00001170
Iteration 163/1000 | Loss: 0.00001170
Iteration 164/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1704534699674696e-05, 1.1704534699674696e-05, 1.1704534699674696e-05, 1.1704534699674696e-05, 1.1704534699674696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1704534699674696e-05

Optimization complete. Final v2v error: 2.7759411334991455 mm

Highest mean error: 2.989576816558838 mm for frame 58

Lowest mean error: 2.56558895111084 mm for frame 175

Saving results

Total time: 37.07134199142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995074
Iteration 2/25 | Loss: 0.00191834
Iteration 3/25 | Loss: 0.00149774
Iteration 4/25 | Loss: 0.00138272
Iteration 5/25 | Loss: 0.00134643
Iteration 6/25 | Loss: 0.00129627
Iteration 7/25 | Loss: 0.00126742
Iteration 8/25 | Loss: 0.00125240
Iteration 9/25 | Loss: 0.00122855
Iteration 10/25 | Loss: 0.00121547
Iteration 11/25 | Loss: 0.00121527
Iteration 12/25 | Loss: 0.00121427
Iteration 13/25 | Loss: 0.00121120
Iteration 14/25 | Loss: 0.00120781
Iteration 15/25 | Loss: 0.00120419
Iteration 16/25 | Loss: 0.00120029
Iteration 17/25 | Loss: 0.00120165
Iteration 18/25 | Loss: 0.00120133
Iteration 19/25 | Loss: 0.00120083
Iteration 20/25 | Loss: 0.00120127
Iteration 21/25 | Loss: 0.00120172
Iteration 22/25 | Loss: 0.00120130
Iteration 23/25 | Loss: 0.00120147
Iteration 24/25 | Loss: 0.00120194
Iteration 25/25 | Loss: 0.00120168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15731955
Iteration 2/25 | Loss: 0.00333668
Iteration 3/25 | Loss: 0.00333667
Iteration 4/25 | Loss: 0.00333667
Iteration 5/25 | Loss: 0.00333667
Iteration 6/25 | Loss: 0.00333667
Iteration 7/25 | Loss: 0.00333667
Iteration 8/25 | Loss: 0.00333667
Iteration 9/25 | Loss: 0.00333667
Iteration 10/25 | Loss: 0.00333667
Iteration 11/25 | Loss: 0.00333667
Iteration 12/25 | Loss: 0.00333667
Iteration 13/25 | Loss: 0.00333667
Iteration 14/25 | Loss: 0.00333667
Iteration 15/25 | Loss: 0.00333667
Iteration 16/25 | Loss: 0.00333667
Iteration 17/25 | Loss: 0.00333667
Iteration 18/25 | Loss: 0.00333667
Iteration 19/25 | Loss: 0.00333667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0033366724383085966, 0.0033366724383085966, 0.0033366724383085966, 0.0033366724383085966, 0.0033366724383085966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033366724383085966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333667
Iteration 2/1000 | Loss: 0.00163563
Iteration 3/1000 | Loss: 0.00014794
Iteration 4/1000 | Loss: 0.00010850
Iteration 5/1000 | Loss: 0.00010463
Iteration 6/1000 | Loss: 0.00023713
Iteration 7/1000 | Loss: 0.00007746
Iteration 8/1000 | Loss: 0.00007313
Iteration 9/1000 | Loss: 0.00014773
Iteration 10/1000 | Loss: 0.00026824
Iteration 11/1000 | Loss: 0.00018135
Iteration 12/1000 | Loss: 0.00030988
Iteration 13/1000 | Loss: 0.00036212
Iteration 14/1000 | Loss: 0.00048057
Iteration 15/1000 | Loss: 0.00008145
Iteration 16/1000 | Loss: 0.00013969
Iteration 17/1000 | Loss: 0.00012390
Iteration 18/1000 | Loss: 0.00016699
Iteration 19/1000 | Loss: 0.00017812
Iteration 20/1000 | Loss: 0.00017632
Iteration 21/1000 | Loss: 0.00005654
Iteration 22/1000 | Loss: 0.00005545
Iteration 23/1000 | Loss: 0.00036949
Iteration 24/1000 | Loss: 0.00037255
Iteration 25/1000 | Loss: 0.00036640
Iteration 26/1000 | Loss: 0.00067853
Iteration 27/1000 | Loss: 0.00187372
Iteration 28/1000 | Loss: 0.00142895
Iteration 29/1000 | Loss: 0.00065292
Iteration 30/1000 | Loss: 0.00020392
Iteration 31/1000 | Loss: 0.00022341
Iteration 32/1000 | Loss: 0.00006424
Iteration 33/1000 | Loss: 0.00007952
Iteration 34/1000 | Loss: 0.00005486
Iteration 35/1000 | Loss: 0.00003940
Iteration 36/1000 | Loss: 0.00004559
Iteration 37/1000 | Loss: 0.00003943
Iteration 38/1000 | Loss: 0.00005539
Iteration 39/1000 | Loss: 0.00003669
Iteration 40/1000 | Loss: 0.00003408
Iteration 41/1000 | Loss: 0.00003156
Iteration 42/1000 | Loss: 0.00004054
Iteration 43/1000 | Loss: 0.00003868
Iteration 44/1000 | Loss: 0.00003993
Iteration 45/1000 | Loss: 0.00004932
Iteration 46/1000 | Loss: 0.00004061
Iteration 47/1000 | Loss: 0.00004744
Iteration 48/1000 | Loss: 0.00004273
Iteration 49/1000 | Loss: 0.00004107
Iteration 50/1000 | Loss: 0.00004638
Iteration 51/1000 | Loss: 0.00004385
Iteration 52/1000 | Loss: 0.00042162
Iteration 53/1000 | Loss: 0.00018049
Iteration 54/1000 | Loss: 0.00017559
Iteration 55/1000 | Loss: 0.00018534
Iteration 56/1000 | Loss: 0.00015224
Iteration 57/1000 | Loss: 0.00014936
Iteration 58/1000 | Loss: 0.00010916
Iteration 59/1000 | Loss: 0.00010900
Iteration 60/1000 | Loss: 0.00005192
Iteration 61/1000 | Loss: 0.00004281
Iteration 62/1000 | Loss: 0.00004950
Iteration 63/1000 | Loss: 0.00004689
Iteration 64/1000 | Loss: 0.00004283
Iteration 65/1000 | Loss: 0.00004271
Iteration 66/1000 | Loss: 0.00003699
Iteration 67/1000 | Loss: 0.00004022
Iteration 68/1000 | Loss: 0.00002419
Iteration 69/1000 | Loss: 0.00002023
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001806
Iteration 72/1000 | Loss: 0.00001751
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001681
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001591
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001584
Iteration 85/1000 | Loss: 0.00001583
Iteration 86/1000 | Loss: 0.00001583
Iteration 87/1000 | Loss: 0.00001582
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001579
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001578
Iteration 100/1000 | Loss: 0.00001578
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001576
Iteration 106/1000 | Loss: 0.00001575
Iteration 107/1000 | Loss: 0.00001575
Iteration 108/1000 | Loss: 0.00001575
Iteration 109/1000 | Loss: 0.00001575
Iteration 110/1000 | Loss: 0.00001575
Iteration 111/1000 | Loss: 0.00001575
Iteration 112/1000 | Loss: 0.00001575
Iteration 113/1000 | Loss: 0.00001574
Iteration 114/1000 | Loss: 0.00001574
Iteration 115/1000 | Loss: 0.00001574
Iteration 116/1000 | Loss: 0.00001573
Iteration 117/1000 | Loss: 0.00001573
Iteration 118/1000 | Loss: 0.00001573
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001573
Iteration 121/1000 | Loss: 0.00001573
Iteration 122/1000 | Loss: 0.00001573
Iteration 123/1000 | Loss: 0.00001572
Iteration 124/1000 | Loss: 0.00001572
Iteration 125/1000 | Loss: 0.00001572
Iteration 126/1000 | Loss: 0.00001572
Iteration 127/1000 | Loss: 0.00001572
Iteration 128/1000 | Loss: 0.00001571
Iteration 129/1000 | Loss: 0.00001571
Iteration 130/1000 | Loss: 0.00001571
Iteration 131/1000 | Loss: 0.00001571
Iteration 132/1000 | Loss: 0.00001571
Iteration 133/1000 | Loss: 0.00001570
Iteration 134/1000 | Loss: 0.00001570
Iteration 135/1000 | Loss: 0.00001569
Iteration 136/1000 | Loss: 0.00001569
Iteration 137/1000 | Loss: 0.00001569
Iteration 138/1000 | Loss: 0.00001569
Iteration 139/1000 | Loss: 0.00001569
Iteration 140/1000 | Loss: 0.00001569
Iteration 141/1000 | Loss: 0.00001569
Iteration 142/1000 | Loss: 0.00001569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.569001869938802e-05, 1.569001869938802e-05, 1.569001869938802e-05, 1.569001869938802e-05, 1.569001869938802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.569001869938802e-05

Optimization complete. Final v2v error: 3.1322200298309326 mm

Highest mean error: 4.907956123352051 mm for frame 164

Lowest mean error: 2.586270332336426 mm for frame 35

Saving results

Total time: 187.65183329582214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522297
Iteration 2/25 | Loss: 0.00129375
Iteration 3/25 | Loss: 0.00115160
Iteration 4/25 | Loss: 0.00114101
Iteration 5/25 | Loss: 0.00113735
Iteration 6/25 | Loss: 0.00113650
Iteration 7/25 | Loss: 0.00113650
Iteration 8/25 | Loss: 0.00113650
Iteration 9/25 | Loss: 0.00113650
Iteration 10/25 | Loss: 0.00113650
Iteration 11/25 | Loss: 0.00113650
Iteration 12/25 | Loss: 0.00113650
Iteration 13/25 | Loss: 0.00113650
Iteration 14/25 | Loss: 0.00113650
Iteration 15/25 | Loss: 0.00113650
Iteration 16/25 | Loss: 0.00113650
Iteration 17/25 | Loss: 0.00113650
Iteration 18/25 | Loss: 0.00113650
Iteration 19/25 | Loss: 0.00113650
Iteration 20/25 | Loss: 0.00113650
Iteration 21/25 | Loss: 0.00113650
Iteration 22/25 | Loss: 0.00113650
Iteration 23/25 | Loss: 0.00113650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00113650131970644, 0.00113650131970644, 0.00113650131970644, 0.00113650131970644, 0.00113650131970644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00113650131970644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14106321
Iteration 2/25 | Loss: 0.00412267
Iteration 3/25 | Loss: 0.00412253
Iteration 4/25 | Loss: 0.00412253
Iteration 5/25 | Loss: 0.00412253
Iteration 6/25 | Loss: 0.00412253
Iteration 7/25 | Loss: 0.00412253
Iteration 8/25 | Loss: 0.00412253
Iteration 9/25 | Loss: 0.00412253
Iteration 10/25 | Loss: 0.00412253
Iteration 11/25 | Loss: 0.00412253
Iteration 12/25 | Loss: 0.00412253
Iteration 13/25 | Loss: 0.00412253
Iteration 14/25 | Loss: 0.00412253
Iteration 15/25 | Loss: 0.00412253
Iteration 16/25 | Loss: 0.00412253
Iteration 17/25 | Loss: 0.00412253
Iteration 18/25 | Loss: 0.00412253
Iteration 19/25 | Loss: 0.00412253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004122528713196516, 0.004122528713196516, 0.004122528713196516, 0.004122528713196516, 0.004122528713196516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004122528713196516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00412253
Iteration 2/1000 | Loss: 0.00006893
Iteration 3/1000 | Loss: 0.00002998
Iteration 4/1000 | Loss: 0.00001687
Iteration 5/1000 | Loss: 0.00001394
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001240
Iteration 8/1000 | Loss: 0.00001194
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001146
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001079
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001078
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001077
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001073
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001069
Iteration 37/1000 | Loss: 0.00001068
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001064
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001063
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001062
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001060
Iteration 54/1000 | Loss: 0.00001059
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001057
Iteration 62/1000 | Loss: 0.00001057
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001056
Iteration 65/1000 | Loss: 0.00001055
Iteration 66/1000 | Loss: 0.00001055
Iteration 67/1000 | Loss: 0.00001054
Iteration 68/1000 | Loss: 0.00001054
Iteration 69/1000 | Loss: 0.00001054
Iteration 70/1000 | Loss: 0.00001053
Iteration 71/1000 | Loss: 0.00001053
Iteration 72/1000 | Loss: 0.00001053
Iteration 73/1000 | Loss: 0.00001053
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001052
Iteration 77/1000 | Loss: 0.00001052
Iteration 78/1000 | Loss: 0.00001051
Iteration 79/1000 | Loss: 0.00001051
Iteration 80/1000 | Loss: 0.00001051
Iteration 81/1000 | Loss: 0.00001051
Iteration 82/1000 | Loss: 0.00001050
Iteration 83/1000 | Loss: 0.00001050
Iteration 84/1000 | Loss: 0.00001050
Iteration 85/1000 | Loss: 0.00001050
Iteration 86/1000 | Loss: 0.00001050
Iteration 87/1000 | Loss: 0.00001050
Iteration 88/1000 | Loss: 0.00001049
Iteration 89/1000 | Loss: 0.00001049
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001049
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001047
Iteration 104/1000 | Loss: 0.00001047
Iteration 105/1000 | Loss: 0.00001047
Iteration 106/1000 | Loss: 0.00001047
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001047
Iteration 110/1000 | Loss: 0.00001047
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001045
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001045
Iteration 120/1000 | Loss: 0.00001045
Iteration 121/1000 | Loss: 0.00001045
Iteration 122/1000 | Loss: 0.00001045
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001044
Iteration 126/1000 | Loss: 0.00001044
Iteration 127/1000 | Loss: 0.00001044
Iteration 128/1000 | Loss: 0.00001044
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001043
Iteration 133/1000 | Loss: 0.00001043
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001042
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001041
Iteration 142/1000 | Loss: 0.00001041
Iteration 143/1000 | Loss: 0.00001041
Iteration 144/1000 | Loss: 0.00001041
Iteration 145/1000 | Loss: 0.00001041
Iteration 146/1000 | Loss: 0.00001040
Iteration 147/1000 | Loss: 0.00001040
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001039
Iteration 152/1000 | Loss: 0.00001039
Iteration 153/1000 | Loss: 0.00001039
Iteration 154/1000 | Loss: 0.00001039
Iteration 155/1000 | Loss: 0.00001039
Iteration 156/1000 | Loss: 0.00001038
Iteration 157/1000 | Loss: 0.00001038
Iteration 158/1000 | Loss: 0.00001038
Iteration 159/1000 | Loss: 0.00001038
Iteration 160/1000 | Loss: 0.00001038
Iteration 161/1000 | Loss: 0.00001038
Iteration 162/1000 | Loss: 0.00001038
Iteration 163/1000 | Loss: 0.00001038
Iteration 164/1000 | Loss: 0.00001037
Iteration 165/1000 | Loss: 0.00001037
Iteration 166/1000 | Loss: 0.00001037
Iteration 167/1000 | Loss: 0.00001037
Iteration 168/1000 | Loss: 0.00001037
Iteration 169/1000 | Loss: 0.00001037
Iteration 170/1000 | Loss: 0.00001037
Iteration 171/1000 | Loss: 0.00001037
Iteration 172/1000 | Loss: 0.00001037
Iteration 173/1000 | Loss: 0.00001037
Iteration 174/1000 | Loss: 0.00001037
Iteration 175/1000 | Loss: 0.00001037
Iteration 176/1000 | Loss: 0.00001037
Iteration 177/1000 | Loss: 0.00001036
Iteration 178/1000 | Loss: 0.00001036
Iteration 179/1000 | Loss: 0.00001036
Iteration 180/1000 | Loss: 0.00001036
Iteration 181/1000 | Loss: 0.00001036
Iteration 182/1000 | Loss: 0.00001036
Iteration 183/1000 | Loss: 0.00001035
Iteration 184/1000 | Loss: 0.00001035
Iteration 185/1000 | Loss: 0.00001035
Iteration 186/1000 | Loss: 0.00001035
Iteration 187/1000 | Loss: 0.00001035
Iteration 188/1000 | Loss: 0.00001035
Iteration 189/1000 | Loss: 0.00001035
Iteration 190/1000 | Loss: 0.00001035
Iteration 191/1000 | Loss: 0.00001035
Iteration 192/1000 | Loss: 0.00001035
Iteration 193/1000 | Loss: 0.00001034
Iteration 194/1000 | Loss: 0.00001034
Iteration 195/1000 | Loss: 0.00001034
Iteration 196/1000 | Loss: 0.00001034
Iteration 197/1000 | Loss: 0.00001034
Iteration 198/1000 | Loss: 0.00001034
Iteration 199/1000 | Loss: 0.00001034
Iteration 200/1000 | Loss: 0.00001034
Iteration 201/1000 | Loss: 0.00001034
Iteration 202/1000 | Loss: 0.00001034
Iteration 203/1000 | Loss: 0.00001034
Iteration 204/1000 | Loss: 0.00001034
Iteration 205/1000 | Loss: 0.00001033
Iteration 206/1000 | Loss: 0.00001033
Iteration 207/1000 | Loss: 0.00001033
Iteration 208/1000 | Loss: 0.00001033
Iteration 209/1000 | Loss: 0.00001033
Iteration 210/1000 | Loss: 0.00001033
Iteration 211/1000 | Loss: 0.00001033
Iteration 212/1000 | Loss: 0.00001033
Iteration 213/1000 | Loss: 0.00001033
Iteration 214/1000 | Loss: 0.00001033
Iteration 215/1000 | Loss: 0.00001033
Iteration 216/1000 | Loss: 0.00001033
Iteration 217/1000 | Loss: 0.00001032
Iteration 218/1000 | Loss: 0.00001032
Iteration 219/1000 | Loss: 0.00001032
Iteration 220/1000 | Loss: 0.00001032
Iteration 221/1000 | Loss: 0.00001032
Iteration 222/1000 | Loss: 0.00001032
Iteration 223/1000 | Loss: 0.00001032
Iteration 224/1000 | Loss: 0.00001032
Iteration 225/1000 | Loss: 0.00001032
Iteration 226/1000 | Loss: 0.00001032
Iteration 227/1000 | Loss: 0.00001032
Iteration 228/1000 | Loss: 0.00001032
Iteration 229/1000 | Loss: 0.00001032
Iteration 230/1000 | Loss: 0.00001032
Iteration 231/1000 | Loss: 0.00001032
Iteration 232/1000 | Loss: 0.00001032
Iteration 233/1000 | Loss: 0.00001032
Iteration 234/1000 | Loss: 0.00001032
Iteration 235/1000 | Loss: 0.00001032
Iteration 236/1000 | Loss: 0.00001032
Iteration 237/1000 | Loss: 0.00001032
Iteration 238/1000 | Loss: 0.00001032
Iteration 239/1000 | Loss: 0.00001032
Iteration 240/1000 | Loss: 0.00001032
Iteration 241/1000 | Loss: 0.00001032
Iteration 242/1000 | Loss: 0.00001032
Iteration 243/1000 | Loss: 0.00001032
Iteration 244/1000 | Loss: 0.00001032
Iteration 245/1000 | Loss: 0.00001032
Iteration 246/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.0318215572624467e-05, 1.0318215572624467e-05, 1.0318215572624467e-05, 1.0318215572624467e-05, 1.0318215572624467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0318215572624467e-05

Optimization complete. Final v2v error: 2.7176411151885986 mm

Highest mean error: 3.1476452350616455 mm for frame 64

Lowest mean error: 2.46600079536438 mm for frame 148

Saving results

Total time: 45.94142723083496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786236
Iteration 2/25 | Loss: 0.00168525
Iteration 3/25 | Loss: 0.00135738
Iteration 4/25 | Loss: 0.00128439
Iteration 5/25 | Loss: 0.00127306
Iteration 6/25 | Loss: 0.00126968
Iteration 7/25 | Loss: 0.00126943
Iteration 8/25 | Loss: 0.00126943
Iteration 9/25 | Loss: 0.00126943
Iteration 10/25 | Loss: 0.00126943
Iteration 11/25 | Loss: 0.00126943
Iteration 12/25 | Loss: 0.00126943
Iteration 13/25 | Loss: 0.00126943
Iteration 14/25 | Loss: 0.00126943
Iteration 15/25 | Loss: 0.00126943
Iteration 16/25 | Loss: 0.00126943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012694308534264565, 0.0012694308534264565, 0.0012694308534264565, 0.0012694308534264565, 0.0012694308534264565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012694308534264565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02207792
Iteration 2/25 | Loss: 0.00320224
Iteration 3/25 | Loss: 0.00320223
Iteration 4/25 | Loss: 0.00320223
Iteration 5/25 | Loss: 0.00320223
Iteration 6/25 | Loss: 0.00320223
Iteration 7/25 | Loss: 0.00320223
Iteration 8/25 | Loss: 0.00320223
Iteration 9/25 | Loss: 0.00320223
Iteration 10/25 | Loss: 0.00320223
Iteration 11/25 | Loss: 0.00320223
Iteration 12/25 | Loss: 0.00320223
Iteration 13/25 | Loss: 0.00320223
Iteration 14/25 | Loss: 0.00320223
Iteration 15/25 | Loss: 0.00320223
Iteration 16/25 | Loss: 0.00320223
Iteration 17/25 | Loss: 0.00320223
Iteration 18/25 | Loss: 0.00320223
Iteration 19/25 | Loss: 0.00320223
Iteration 20/25 | Loss: 0.00320223
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003202226012945175, 0.003202226012945175, 0.003202226012945175, 0.003202226012945175, 0.003202226012945175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003202226012945175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320223
Iteration 2/1000 | Loss: 0.00010671
Iteration 3/1000 | Loss: 0.00005987
Iteration 4/1000 | Loss: 0.00004424
Iteration 5/1000 | Loss: 0.00003915
Iteration 6/1000 | Loss: 0.00003634
Iteration 7/1000 | Loss: 0.00003474
Iteration 8/1000 | Loss: 0.00003374
Iteration 9/1000 | Loss: 0.00003287
Iteration 10/1000 | Loss: 0.00003236
Iteration 11/1000 | Loss: 0.00003188
Iteration 12/1000 | Loss: 0.00003147
Iteration 13/1000 | Loss: 0.00003116
Iteration 14/1000 | Loss: 0.00003094
Iteration 15/1000 | Loss: 0.00003080
Iteration 16/1000 | Loss: 0.00003072
Iteration 17/1000 | Loss: 0.00003068
Iteration 18/1000 | Loss: 0.00003067
Iteration 19/1000 | Loss: 0.00003061
Iteration 20/1000 | Loss: 0.00003058
Iteration 21/1000 | Loss: 0.00003056
Iteration 22/1000 | Loss: 0.00003056
Iteration 23/1000 | Loss: 0.00003055
Iteration 24/1000 | Loss: 0.00003054
Iteration 25/1000 | Loss: 0.00003054
Iteration 26/1000 | Loss: 0.00003053
Iteration 27/1000 | Loss: 0.00003052
Iteration 28/1000 | Loss: 0.00003051
Iteration 29/1000 | Loss: 0.00003050
Iteration 30/1000 | Loss: 0.00003049
Iteration 31/1000 | Loss: 0.00003049
Iteration 32/1000 | Loss: 0.00003049
Iteration 33/1000 | Loss: 0.00003048
Iteration 34/1000 | Loss: 0.00003048
Iteration 35/1000 | Loss: 0.00003046
Iteration 36/1000 | Loss: 0.00003045
Iteration 37/1000 | Loss: 0.00003045
Iteration 38/1000 | Loss: 0.00003045
Iteration 39/1000 | Loss: 0.00003044
Iteration 40/1000 | Loss: 0.00003044
Iteration 41/1000 | Loss: 0.00003044
Iteration 42/1000 | Loss: 0.00003043
Iteration 43/1000 | Loss: 0.00003043
Iteration 44/1000 | Loss: 0.00003043
Iteration 45/1000 | Loss: 0.00003042
Iteration 46/1000 | Loss: 0.00003042
Iteration 47/1000 | Loss: 0.00003042
Iteration 48/1000 | Loss: 0.00003041
Iteration 49/1000 | Loss: 0.00003041
Iteration 50/1000 | Loss: 0.00003041
Iteration 51/1000 | Loss: 0.00003040
Iteration 52/1000 | Loss: 0.00003040
Iteration 53/1000 | Loss: 0.00003040
Iteration 54/1000 | Loss: 0.00003040
Iteration 55/1000 | Loss: 0.00003039
Iteration 56/1000 | Loss: 0.00003039
Iteration 57/1000 | Loss: 0.00003038
Iteration 58/1000 | Loss: 0.00003038
Iteration 59/1000 | Loss: 0.00003038
Iteration 60/1000 | Loss: 0.00003037
Iteration 61/1000 | Loss: 0.00003037
Iteration 62/1000 | Loss: 0.00003037
Iteration 63/1000 | Loss: 0.00003037
Iteration 64/1000 | Loss: 0.00003036
Iteration 65/1000 | Loss: 0.00003036
Iteration 66/1000 | Loss: 0.00003035
Iteration 67/1000 | Loss: 0.00003035
Iteration 68/1000 | Loss: 0.00003035
Iteration 69/1000 | Loss: 0.00003035
Iteration 70/1000 | Loss: 0.00003035
Iteration 71/1000 | Loss: 0.00003035
Iteration 72/1000 | Loss: 0.00003034
Iteration 73/1000 | Loss: 0.00003034
Iteration 74/1000 | Loss: 0.00003034
Iteration 75/1000 | Loss: 0.00003034
Iteration 76/1000 | Loss: 0.00003033
Iteration 77/1000 | Loss: 0.00003033
Iteration 78/1000 | Loss: 0.00003033
Iteration 79/1000 | Loss: 0.00003033
Iteration 80/1000 | Loss: 0.00003032
Iteration 81/1000 | Loss: 0.00003032
Iteration 82/1000 | Loss: 0.00003032
Iteration 83/1000 | Loss: 0.00003032
Iteration 84/1000 | Loss: 0.00003032
Iteration 85/1000 | Loss: 0.00003032
Iteration 86/1000 | Loss: 0.00003032
Iteration 87/1000 | Loss: 0.00003032
Iteration 88/1000 | Loss: 0.00003032
Iteration 89/1000 | Loss: 0.00003032
Iteration 90/1000 | Loss: 0.00003032
Iteration 91/1000 | Loss: 0.00003032
Iteration 92/1000 | Loss: 0.00003032
Iteration 93/1000 | Loss: 0.00003032
Iteration 94/1000 | Loss: 0.00003032
Iteration 95/1000 | Loss: 0.00003032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [3.0317767595988698e-05, 3.0317767595988698e-05, 3.0317767595988698e-05, 3.0317767595988698e-05, 3.0317767595988698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0317767595988698e-05

Optimization complete. Final v2v error: 4.435781002044678 mm

Highest mean error: 6.0121564865112305 mm for frame 60

Lowest mean error: 3.31058669090271 mm for frame 217

Saving results

Total time: 46.72000503540039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074649
Iteration 2/25 | Loss: 0.01074649
Iteration 3/25 | Loss: 0.01074649
Iteration 4/25 | Loss: 0.01074649
Iteration 5/25 | Loss: 0.01074649
Iteration 6/25 | Loss: 0.01074649
Iteration 7/25 | Loss: 0.01074649
Iteration 8/25 | Loss: 0.01074649
Iteration 9/25 | Loss: 0.01074649
Iteration 10/25 | Loss: 0.01074649
Iteration 11/25 | Loss: 0.01074649
Iteration 12/25 | Loss: 0.01074649
Iteration 13/25 | Loss: 0.01074648
Iteration 14/25 | Loss: 0.01074648
Iteration 15/25 | Loss: 0.01074648
Iteration 16/25 | Loss: 0.01074648
Iteration 17/25 | Loss: 0.01074648
Iteration 18/25 | Loss: 0.01074648
Iteration 19/25 | Loss: 0.01074648
Iteration 20/25 | Loss: 0.01074648
Iteration 21/25 | Loss: 0.01074648
Iteration 22/25 | Loss: 0.01074648
Iteration 23/25 | Loss: 0.01074648
Iteration 24/25 | Loss: 0.01074647
Iteration 25/25 | Loss: 0.01074647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72014701
Iteration 2/25 | Loss: 0.07591141
Iteration 3/25 | Loss: 0.07590640
Iteration 4/25 | Loss: 0.07590635
Iteration 5/25 | Loss: 0.07590635
Iteration 6/25 | Loss: 0.07590635
Iteration 7/25 | Loss: 0.07590634
Iteration 8/25 | Loss: 0.07590634
Iteration 9/25 | Loss: 0.07590634
Iteration 10/25 | Loss: 0.07590634
Iteration 11/25 | Loss: 0.07590634
Iteration 12/25 | Loss: 0.07590634
Iteration 13/25 | Loss: 0.07590634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.07590633630752563, 0.07590633630752563, 0.07590633630752563, 0.07590633630752563, 0.07590633630752563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07590633630752563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07590634
Iteration 2/1000 | Loss: 0.00188939
Iteration 3/1000 | Loss: 0.00111125
Iteration 4/1000 | Loss: 0.00102172
Iteration 5/1000 | Loss: 0.00083499
Iteration 6/1000 | Loss: 0.00118759
Iteration 7/1000 | Loss: 0.00035201
Iteration 8/1000 | Loss: 0.00023860
Iteration 9/1000 | Loss: 0.00013617
Iteration 10/1000 | Loss: 0.00012283
Iteration 11/1000 | Loss: 0.00019374
Iteration 12/1000 | Loss: 0.00021608
Iteration 13/1000 | Loss: 0.00037910
Iteration 14/1000 | Loss: 0.00016062
Iteration 15/1000 | Loss: 0.00005518
Iteration 16/1000 | Loss: 0.00005942
Iteration 17/1000 | Loss: 0.00004809
Iteration 18/1000 | Loss: 0.00004927
Iteration 19/1000 | Loss: 0.00005279
Iteration 20/1000 | Loss: 0.00020549
Iteration 21/1000 | Loss: 0.00009912
Iteration 22/1000 | Loss: 0.00003852
Iteration 23/1000 | Loss: 0.00003373
Iteration 24/1000 | Loss: 0.00002983
Iteration 25/1000 | Loss: 0.00005877
Iteration 26/1000 | Loss: 0.00003289
Iteration 27/1000 | Loss: 0.00011036
Iteration 28/1000 | Loss: 0.00002679
Iteration 29/1000 | Loss: 0.00004111
Iteration 30/1000 | Loss: 0.00002727
Iteration 31/1000 | Loss: 0.00005499
Iteration 32/1000 | Loss: 0.00013853
Iteration 33/1000 | Loss: 0.00007426
Iteration 34/1000 | Loss: 0.00003091
Iteration 35/1000 | Loss: 0.00002643
Iteration 36/1000 | Loss: 0.00002439
Iteration 37/1000 | Loss: 0.00021363
Iteration 38/1000 | Loss: 0.00010599
Iteration 39/1000 | Loss: 0.00004280
Iteration 40/1000 | Loss: 0.00002247
Iteration 41/1000 | Loss: 0.00003693
Iteration 42/1000 | Loss: 0.00002168
Iteration 43/1000 | Loss: 0.00002322
Iteration 44/1000 | Loss: 0.00002092
Iteration 45/1000 | Loss: 0.00002542
Iteration 46/1000 | Loss: 0.00003858
Iteration 47/1000 | Loss: 0.00001974
Iteration 48/1000 | Loss: 0.00003238
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00004572
Iteration 51/1000 | Loss: 0.00010406
Iteration 52/1000 | Loss: 0.00010300
Iteration 53/1000 | Loss: 0.00002580
Iteration 54/1000 | Loss: 0.00005534
Iteration 55/1000 | Loss: 0.00002090
Iteration 56/1000 | Loss: 0.00003694
Iteration 57/1000 | Loss: 0.00004068
Iteration 58/1000 | Loss: 0.00002597
Iteration 59/1000 | Loss: 0.00002454
Iteration 60/1000 | Loss: 0.00003150
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00002310
Iteration 63/1000 | Loss: 0.00001957
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00002083
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001679
Iteration 70/1000 | Loss: 0.00001679
Iteration 71/1000 | Loss: 0.00001679
Iteration 72/1000 | Loss: 0.00001679
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001679
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001679
Iteration 79/1000 | Loss: 0.00001679
Iteration 80/1000 | Loss: 0.00001678
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00010308
Iteration 88/1000 | Loss: 0.00003593
Iteration 89/1000 | Loss: 0.00010334
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00009221
Iteration 93/1000 | Loss: 0.00005878
Iteration 94/1000 | Loss: 0.00051447
Iteration 95/1000 | Loss: 0.00113614
Iteration 96/1000 | Loss: 0.00050946
Iteration 97/1000 | Loss: 0.00005930
Iteration 98/1000 | Loss: 0.00009835
Iteration 99/1000 | Loss: 0.00001874
Iteration 100/1000 | Loss: 0.00001639
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001562
Iteration 103/1000 | Loss: 0.00002188
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001814
Iteration 106/1000 | Loss: 0.00002103
Iteration 107/1000 | Loss: 0.00002051
Iteration 108/1000 | Loss: 0.00002481
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00002232
Iteration 113/1000 | Loss: 0.00002944
Iteration 114/1000 | Loss: 0.00001537
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001544
Iteration 125/1000 | Loss: 0.00001977
Iteration 126/1000 | Loss: 0.00001569
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001732
Iteration 130/1000 | Loss: 0.00001518
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001515
Iteration 146/1000 | Loss: 0.00001515
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001515
Iteration 150/1000 | Loss: 0.00001556
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001512
Iteration 153/1000 | Loss: 0.00001511
Iteration 154/1000 | Loss: 0.00001511
Iteration 155/1000 | Loss: 0.00001511
Iteration 156/1000 | Loss: 0.00001511
Iteration 157/1000 | Loss: 0.00001511
Iteration 158/1000 | Loss: 0.00001511
Iteration 159/1000 | Loss: 0.00001693
Iteration 160/1000 | Loss: 0.00001522
Iteration 161/1000 | Loss: 0.00001511
Iteration 162/1000 | Loss: 0.00001511
Iteration 163/1000 | Loss: 0.00001511
Iteration 164/1000 | Loss: 0.00001511
Iteration 165/1000 | Loss: 0.00001511
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001511
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.5105555576155894e-05, 1.5105555576155894e-05, 1.5105555576155894e-05, 1.5105555576155894e-05, 1.5105555576155894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5105555576155894e-05

Optimization complete. Final v2v error: 2.8998024463653564 mm

Highest mean error: 15.223062515258789 mm for frame 49

Lowest mean error: 2.2952611446380615 mm for frame 69

Saving results

Total time: 168.04091596603394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446612
Iteration 2/25 | Loss: 0.00125232
Iteration 3/25 | Loss: 0.00118201
Iteration 4/25 | Loss: 0.00117135
Iteration 5/25 | Loss: 0.00116783
Iteration 6/25 | Loss: 0.00116719
Iteration 7/25 | Loss: 0.00116719
Iteration 8/25 | Loss: 0.00116719
Iteration 9/25 | Loss: 0.00116719
Iteration 10/25 | Loss: 0.00116719
Iteration 11/25 | Loss: 0.00116719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011671942193061113, 0.0011671942193061113, 0.0011671942193061113, 0.0011671942193061113, 0.0011671942193061113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011671942193061113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12761545
Iteration 2/25 | Loss: 0.00305926
Iteration 3/25 | Loss: 0.00305925
Iteration 4/25 | Loss: 0.00305925
Iteration 5/25 | Loss: 0.00305925
Iteration 6/25 | Loss: 0.00305925
Iteration 7/25 | Loss: 0.00305925
Iteration 8/25 | Loss: 0.00305925
Iteration 9/25 | Loss: 0.00305925
Iteration 10/25 | Loss: 0.00305925
Iteration 11/25 | Loss: 0.00305925
Iteration 12/25 | Loss: 0.00305925
Iteration 13/25 | Loss: 0.00305925
Iteration 14/25 | Loss: 0.00305925
Iteration 15/25 | Loss: 0.00305925
Iteration 16/25 | Loss: 0.00305925
Iteration 17/25 | Loss: 0.00305925
Iteration 18/25 | Loss: 0.00305925
Iteration 19/25 | Loss: 0.00305925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003059251466766, 0.003059251466766, 0.003059251466766, 0.003059251466766, 0.003059251466766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003059251466766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305925
Iteration 2/1000 | Loss: 0.00003065
Iteration 3/1000 | Loss: 0.00001971
Iteration 4/1000 | Loss: 0.00001795
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001286
Iteration 21/1000 | Loss: 0.00001286
Iteration 22/1000 | Loss: 0.00001285
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001285
Iteration 26/1000 | Loss: 0.00001285
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001283
Iteration 29/1000 | Loss: 0.00001282
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001282
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001281
Iteration 36/1000 | Loss: 0.00001281
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001279
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001275
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001275
Iteration 70/1000 | Loss: 0.00001275
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001275
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001274
Iteration 81/1000 | Loss: 0.00001274
Iteration 82/1000 | Loss: 0.00001274
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001273
Iteration 85/1000 | Loss: 0.00001273
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001273
Iteration 88/1000 | Loss: 0.00001273
Iteration 89/1000 | Loss: 0.00001273
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001272
Iteration 98/1000 | Loss: 0.00001272
Iteration 99/1000 | Loss: 0.00001272
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001270
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001270
Iteration 116/1000 | Loss: 0.00001270
Iteration 117/1000 | Loss: 0.00001270
Iteration 118/1000 | Loss: 0.00001270
Iteration 119/1000 | Loss: 0.00001270
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001270
Iteration 122/1000 | Loss: 0.00001270
Iteration 123/1000 | Loss: 0.00001270
Iteration 124/1000 | Loss: 0.00001270
Iteration 125/1000 | Loss: 0.00001270
Iteration 126/1000 | Loss: 0.00001270
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001270
Iteration 132/1000 | Loss: 0.00001270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2699405488092452e-05, 1.2699405488092452e-05, 1.2699405488092452e-05, 1.2699405488092452e-05, 1.2699405488092452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2699405488092452e-05

Optimization complete. Final v2v error: 3.058756113052368 mm

Highest mean error: 3.4641060829162598 mm for frame 101

Lowest mean error: 2.5815231800079346 mm for frame 0

Saving results

Total time: 39.675902366638184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402578
Iteration 2/25 | Loss: 0.00128880
Iteration 3/25 | Loss: 0.00115698
Iteration 4/25 | Loss: 0.00115014
Iteration 5/25 | Loss: 0.00114877
Iteration 6/25 | Loss: 0.00114877
Iteration 7/25 | Loss: 0.00114877
Iteration 8/25 | Loss: 0.00114877
Iteration 9/25 | Loss: 0.00114877
Iteration 10/25 | Loss: 0.00114877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011487716110423207, 0.0011487716110423207, 0.0011487716110423207, 0.0011487716110423207, 0.0011487716110423207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011487716110423207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16722918
Iteration 2/25 | Loss: 0.00368008
Iteration 3/25 | Loss: 0.00368007
Iteration 4/25 | Loss: 0.00368007
Iteration 5/25 | Loss: 0.00368007
Iteration 6/25 | Loss: 0.00368007
Iteration 7/25 | Loss: 0.00368007
Iteration 8/25 | Loss: 0.00368007
Iteration 9/25 | Loss: 0.00368007
Iteration 10/25 | Loss: 0.00368007
Iteration 11/25 | Loss: 0.00368007
Iteration 12/25 | Loss: 0.00368007
Iteration 13/25 | Loss: 0.00368007
Iteration 14/25 | Loss: 0.00368007
Iteration 15/25 | Loss: 0.00368007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003680070396512747, 0.003680070396512747, 0.003680070396512747, 0.003680070396512747, 0.003680070396512747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003680070396512747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00368007
Iteration 2/1000 | Loss: 0.00005878
Iteration 3/1000 | Loss: 0.00003044
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001542
Iteration 6/1000 | Loss: 0.00001374
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001239
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001167
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001112
Iteration 13/1000 | Loss: 0.00001111
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001098
Iteration 20/1000 | Loss: 0.00001098
Iteration 21/1000 | Loss: 0.00001097
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001075
Iteration 27/1000 | Loss: 0.00001073
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001072
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001068
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001064
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001056
Iteration 45/1000 | Loss: 0.00001055
Iteration 46/1000 | Loss: 0.00001054
Iteration 47/1000 | Loss: 0.00001054
Iteration 48/1000 | Loss: 0.00001054
Iteration 49/1000 | Loss: 0.00001054
Iteration 50/1000 | Loss: 0.00001054
Iteration 51/1000 | Loss: 0.00001054
Iteration 52/1000 | Loss: 0.00001053
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001053
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001051
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001050
Iteration 61/1000 | Loss: 0.00001050
Iteration 62/1000 | Loss: 0.00001050
Iteration 63/1000 | Loss: 0.00001050
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001049
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001048
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001047
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001046
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001045
Iteration 89/1000 | Loss: 0.00001045
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001044
Iteration 92/1000 | Loss: 0.00001044
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001044
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001043
Iteration 104/1000 | Loss: 0.00001043
Iteration 105/1000 | Loss: 0.00001043
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001042
Iteration 110/1000 | Loss: 0.00001042
Iteration 111/1000 | Loss: 0.00001042
Iteration 112/1000 | Loss: 0.00001042
Iteration 113/1000 | Loss: 0.00001042
Iteration 114/1000 | Loss: 0.00001042
Iteration 115/1000 | Loss: 0.00001042
Iteration 116/1000 | Loss: 0.00001042
Iteration 117/1000 | Loss: 0.00001042
Iteration 118/1000 | Loss: 0.00001042
Iteration 119/1000 | Loss: 0.00001041
Iteration 120/1000 | Loss: 0.00001041
Iteration 121/1000 | Loss: 0.00001041
Iteration 122/1000 | Loss: 0.00001041
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001041
Iteration 127/1000 | Loss: 0.00001041
Iteration 128/1000 | Loss: 0.00001041
Iteration 129/1000 | Loss: 0.00001041
Iteration 130/1000 | Loss: 0.00001041
Iteration 131/1000 | Loss: 0.00001041
Iteration 132/1000 | Loss: 0.00001041
Iteration 133/1000 | Loss: 0.00001041
Iteration 134/1000 | Loss: 0.00001041
Iteration 135/1000 | Loss: 0.00001041
Iteration 136/1000 | Loss: 0.00001041
Iteration 137/1000 | Loss: 0.00001040
Iteration 138/1000 | Loss: 0.00001040
Iteration 139/1000 | Loss: 0.00001040
Iteration 140/1000 | Loss: 0.00001040
Iteration 141/1000 | Loss: 0.00001040
Iteration 142/1000 | Loss: 0.00001040
Iteration 143/1000 | Loss: 0.00001039
Iteration 144/1000 | Loss: 0.00001039
Iteration 145/1000 | Loss: 0.00001039
Iteration 146/1000 | Loss: 0.00001039
Iteration 147/1000 | Loss: 0.00001039
Iteration 148/1000 | Loss: 0.00001039
Iteration 149/1000 | Loss: 0.00001039
Iteration 150/1000 | Loss: 0.00001039
Iteration 151/1000 | Loss: 0.00001039
Iteration 152/1000 | Loss: 0.00001039
Iteration 153/1000 | Loss: 0.00001039
Iteration 154/1000 | Loss: 0.00001039
Iteration 155/1000 | Loss: 0.00001039
Iteration 156/1000 | Loss: 0.00001038
Iteration 157/1000 | Loss: 0.00001038
Iteration 158/1000 | Loss: 0.00001038
Iteration 159/1000 | Loss: 0.00001038
Iteration 160/1000 | Loss: 0.00001038
Iteration 161/1000 | Loss: 0.00001038
Iteration 162/1000 | Loss: 0.00001038
Iteration 163/1000 | Loss: 0.00001038
Iteration 164/1000 | Loss: 0.00001038
Iteration 165/1000 | Loss: 0.00001038
Iteration 166/1000 | Loss: 0.00001038
Iteration 167/1000 | Loss: 0.00001038
Iteration 168/1000 | Loss: 0.00001037
Iteration 169/1000 | Loss: 0.00001037
Iteration 170/1000 | Loss: 0.00001037
Iteration 171/1000 | Loss: 0.00001037
Iteration 172/1000 | Loss: 0.00001037
Iteration 173/1000 | Loss: 0.00001037
Iteration 174/1000 | Loss: 0.00001037
Iteration 175/1000 | Loss: 0.00001036
Iteration 176/1000 | Loss: 0.00001036
Iteration 177/1000 | Loss: 0.00001036
Iteration 178/1000 | Loss: 0.00001036
Iteration 179/1000 | Loss: 0.00001036
Iteration 180/1000 | Loss: 0.00001036
Iteration 181/1000 | Loss: 0.00001036
Iteration 182/1000 | Loss: 0.00001035
Iteration 183/1000 | Loss: 0.00001035
Iteration 184/1000 | Loss: 0.00001035
Iteration 185/1000 | Loss: 0.00001035
Iteration 186/1000 | Loss: 0.00001035
Iteration 187/1000 | Loss: 0.00001035
Iteration 188/1000 | Loss: 0.00001035
Iteration 189/1000 | Loss: 0.00001035
Iteration 190/1000 | Loss: 0.00001035
Iteration 191/1000 | Loss: 0.00001035
Iteration 192/1000 | Loss: 0.00001035
Iteration 193/1000 | Loss: 0.00001035
Iteration 194/1000 | Loss: 0.00001035
Iteration 195/1000 | Loss: 0.00001035
Iteration 196/1000 | Loss: 0.00001035
Iteration 197/1000 | Loss: 0.00001034
Iteration 198/1000 | Loss: 0.00001034
Iteration 199/1000 | Loss: 0.00001034
Iteration 200/1000 | Loss: 0.00001034
Iteration 201/1000 | Loss: 0.00001034
Iteration 202/1000 | Loss: 0.00001034
Iteration 203/1000 | Loss: 0.00001034
Iteration 204/1000 | Loss: 0.00001034
Iteration 205/1000 | Loss: 0.00001034
Iteration 206/1000 | Loss: 0.00001034
Iteration 207/1000 | Loss: 0.00001034
Iteration 208/1000 | Loss: 0.00001034
Iteration 209/1000 | Loss: 0.00001034
Iteration 210/1000 | Loss: 0.00001034
Iteration 211/1000 | Loss: 0.00001034
Iteration 212/1000 | Loss: 0.00001034
Iteration 213/1000 | Loss: 0.00001034
Iteration 214/1000 | Loss: 0.00001034
Iteration 215/1000 | Loss: 0.00001034
Iteration 216/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.0342628229409456e-05, 1.0342628229409456e-05, 1.0342628229409456e-05, 1.0342628229409456e-05, 1.0342628229409456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0342628229409456e-05

Optimization complete. Final v2v error: 2.6913514137268066 mm

Highest mean error: 3.255936622619629 mm for frame 107

Lowest mean error: 2.305647611618042 mm for frame 159

Saving results

Total time: 44.5873007774353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504804
Iteration 2/25 | Loss: 0.00139343
Iteration 3/25 | Loss: 0.00123446
Iteration 4/25 | Loss: 0.00121639
Iteration 5/25 | Loss: 0.00121015
Iteration 6/25 | Loss: 0.00120856
Iteration 7/25 | Loss: 0.00120856
Iteration 8/25 | Loss: 0.00120856
Iteration 9/25 | Loss: 0.00120856
Iteration 10/25 | Loss: 0.00120856
Iteration 11/25 | Loss: 0.00120856
Iteration 12/25 | Loss: 0.00120856
Iteration 13/25 | Loss: 0.00120856
Iteration 14/25 | Loss: 0.00120856
Iteration 15/25 | Loss: 0.00120856
Iteration 16/25 | Loss: 0.00120856
Iteration 17/25 | Loss: 0.00120856
Iteration 18/25 | Loss: 0.00120856
Iteration 19/25 | Loss: 0.00120856
Iteration 20/25 | Loss: 0.00120856
Iteration 21/25 | Loss: 0.00120856
Iteration 22/25 | Loss: 0.00120856
Iteration 23/25 | Loss: 0.00120856
Iteration 24/25 | Loss: 0.00120856
Iteration 25/25 | Loss: 0.00120856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50652611
Iteration 2/25 | Loss: 0.00225735
Iteration 3/25 | Loss: 0.00225735
Iteration 4/25 | Loss: 0.00225735
Iteration 5/25 | Loss: 0.00225735
Iteration 6/25 | Loss: 0.00225735
Iteration 7/25 | Loss: 0.00225735
Iteration 8/25 | Loss: 0.00225735
Iteration 9/25 | Loss: 0.00225735
Iteration 10/25 | Loss: 0.00225735
Iteration 11/25 | Loss: 0.00225735
Iteration 12/25 | Loss: 0.00225735
Iteration 13/25 | Loss: 0.00225735
Iteration 14/25 | Loss: 0.00225735
Iteration 15/25 | Loss: 0.00225735
Iteration 16/25 | Loss: 0.00225735
Iteration 17/25 | Loss: 0.00225735
Iteration 18/25 | Loss: 0.00225735
Iteration 19/25 | Loss: 0.00225735
Iteration 20/25 | Loss: 0.00225735
Iteration 21/25 | Loss: 0.00225735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022573501337319613, 0.0022573501337319613, 0.0022573501337319613, 0.0022573501337319613, 0.0022573501337319613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022573501337319613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225735
Iteration 2/1000 | Loss: 0.00005458
Iteration 3/1000 | Loss: 0.00003151
Iteration 4/1000 | Loss: 0.00002632
Iteration 5/1000 | Loss: 0.00002438
Iteration 6/1000 | Loss: 0.00002318
Iteration 7/1000 | Loss: 0.00002261
Iteration 8/1000 | Loss: 0.00002221
Iteration 9/1000 | Loss: 0.00002192
Iteration 10/1000 | Loss: 0.00002166
Iteration 11/1000 | Loss: 0.00002146
Iteration 12/1000 | Loss: 0.00002142
Iteration 13/1000 | Loss: 0.00002126
Iteration 14/1000 | Loss: 0.00002110
Iteration 15/1000 | Loss: 0.00002095
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002067
Iteration 19/1000 | Loss: 0.00002062
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00002056
Iteration 22/1000 | Loss: 0.00002046
Iteration 23/1000 | Loss: 0.00002046
Iteration 24/1000 | Loss: 0.00002033
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002007
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001994
Iteration 31/1000 | Loss: 0.00001990
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001987
Iteration 36/1000 | Loss: 0.00001987
Iteration 37/1000 | Loss: 0.00001983
Iteration 38/1000 | Loss: 0.00001982
Iteration 39/1000 | Loss: 0.00001982
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001981
Iteration 43/1000 | Loss: 0.00001981
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001980
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001980
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001980
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001979
Iteration 56/1000 | Loss: 0.00001979
Iteration 57/1000 | Loss: 0.00001979
Iteration 58/1000 | Loss: 0.00001979
Iteration 59/1000 | Loss: 0.00001979
Iteration 60/1000 | Loss: 0.00001979
Iteration 61/1000 | Loss: 0.00001979
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001978
Iteration 64/1000 | Loss: 0.00001978
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001977
Iteration 67/1000 | Loss: 0.00001977
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001976
Iteration 71/1000 | Loss: 0.00001976
Iteration 72/1000 | Loss: 0.00001976
Iteration 73/1000 | Loss: 0.00001976
Iteration 74/1000 | Loss: 0.00001976
Iteration 75/1000 | Loss: 0.00001976
Iteration 76/1000 | Loss: 0.00001976
Iteration 77/1000 | Loss: 0.00001976
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001975
Iteration 81/1000 | Loss: 0.00001975
Iteration 82/1000 | Loss: 0.00001975
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001974
Iteration 90/1000 | Loss: 0.00001974
Iteration 91/1000 | Loss: 0.00001974
Iteration 92/1000 | Loss: 0.00001974
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001971
Iteration 103/1000 | Loss: 0.00001971
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001969
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00001969
Iteration 111/1000 | Loss: 0.00001968
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001967
Iteration 119/1000 | Loss: 0.00001967
Iteration 120/1000 | Loss: 0.00001967
Iteration 121/1000 | Loss: 0.00001966
Iteration 122/1000 | Loss: 0.00001966
Iteration 123/1000 | Loss: 0.00001966
Iteration 124/1000 | Loss: 0.00001966
Iteration 125/1000 | Loss: 0.00001966
Iteration 126/1000 | Loss: 0.00001966
Iteration 127/1000 | Loss: 0.00001966
Iteration 128/1000 | Loss: 0.00001966
Iteration 129/1000 | Loss: 0.00001965
Iteration 130/1000 | Loss: 0.00001965
Iteration 131/1000 | Loss: 0.00001965
Iteration 132/1000 | Loss: 0.00001965
Iteration 133/1000 | Loss: 0.00001965
Iteration 134/1000 | Loss: 0.00001965
Iteration 135/1000 | Loss: 0.00001965
Iteration 136/1000 | Loss: 0.00001965
Iteration 137/1000 | Loss: 0.00001965
Iteration 138/1000 | Loss: 0.00001965
Iteration 139/1000 | Loss: 0.00001965
Iteration 140/1000 | Loss: 0.00001965
Iteration 141/1000 | Loss: 0.00001964
Iteration 142/1000 | Loss: 0.00001964
Iteration 143/1000 | Loss: 0.00001964
Iteration 144/1000 | Loss: 0.00001964
Iteration 145/1000 | Loss: 0.00001964
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001964
Iteration 149/1000 | Loss: 0.00001964
Iteration 150/1000 | Loss: 0.00001964
Iteration 151/1000 | Loss: 0.00001964
Iteration 152/1000 | Loss: 0.00001964
Iteration 153/1000 | Loss: 0.00001963
Iteration 154/1000 | Loss: 0.00001963
Iteration 155/1000 | Loss: 0.00001963
Iteration 156/1000 | Loss: 0.00001963
Iteration 157/1000 | Loss: 0.00001963
Iteration 158/1000 | Loss: 0.00001963
Iteration 159/1000 | Loss: 0.00001963
Iteration 160/1000 | Loss: 0.00001963
Iteration 161/1000 | Loss: 0.00001963
Iteration 162/1000 | Loss: 0.00001963
Iteration 163/1000 | Loss: 0.00001963
Iteration 164/1000 | Loss: 0.00001963
Iteration 165/1000 | Loss: 0.00001962
Iteration 166/1000 | Loss: 0.00001962
Iteration 167/1000 | Loss: 0.00001962
Iteration 168/1000 | Loss: 0.00001962
Iteration 169/1000 | Loss: 0.00001962
Iteration 170/1000 | Loss: 0.00001962
Iteration 171/1000 | Loss: 0.00001962
Iteration 172/1000 | Loss: 0.00001962
Iteration 173/1000 | Loss: 0.00001962
Iteration 174/1000 | Loss: 0.00001962
Iteration 175/1000 | Loss: 0.00001962
Iteration 176/1000 | Loss: 0.00001962
Iteration 177/1000 | Loss: 0.00001962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.9621827959781513e-05, 1.9621827959781513e-05, 1.9621827959781513e-05, 1.9621827959781513e-05, 1.9621827959781513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9621827959781513e-05

Optimization complete. Final v2v error: 3.5568008422851562 mm

Highest mean error: 3.8109376430511475 mm for frame 76

Lowest mean error: 3.387794017791748 mm for frame 223

Saving results

Total time: 60.58294105529785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048907
Iteration 2/25 | Loss: 0.01048907
Iteration 3/25 | Loss: 0.00498495
Iteration 4/25 | Loss: 0.00288617
Iteration 5/25 | Loss: 0.00250924
Iteration 6/25 | Loss: 0.00208467
Iteration 7/25 | Loss: 0.00203423
Iteration 8/25 | Loss: 0.00186997
Iteration 9/25 | Loss: 0.00171827
Iteration 10/25 | Loss: 0.00164781
Iteration 11/25 | Loss: 0.00160033
Iteration 12/25 | Loss: 0.00145626
Iteration 13/25 | Loss: 0.00144575
Iteration 14/25 | Loss: 0.00138838
Iteration 15/25 | Loss: 0.00134798
Iteration 16/25 | Loss: 0.00133679
Iteration 17/25 | Loss: 0.00134099
Iteration 18/25 | Loss: 0.00133121
Iteration 19/25 | Loss: 0.00134131
Iteration 20/25 | Loss: 0.00132697
Iteration 21/25 | Loss: 0.00132136
Iteration 22/25 | Loss: 0.00132058
Iteration 23/25 | Loss: 0.00132043
Iteration 24/25 | Loss: 0.00132043
Iteration 25/25 | Loss: 0.00132043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17916346
Iteration 2/25 | Loss: 0.00464628
Iteration 3/25 | Loss: 0.00443592
Iteration 4/25 | Loss: 0.00443591
Iteration 5/25 | Loss: 0.00443591
Iteration 6/25 | Loss: 0.00443591
Iteration 7/25 | Loss: 0.00443591
Iteration 8/25 | Loss: 0.00443591
Iteration 9/25 | Loss: 0.00443591
Iteration 10/25 | Loss: 0.00443591
Iteration 11/25 | Loss: 0.00443591
Iteration 12/25 | Loss: 0.00443591
Iteration 13/25 | Loss: 0.00443590
Iteration 14/25 | Loss: 0.00443590
Iteration 15/25 | Loss: 0.00443590
Iteration 16/25 | Loss: 0.00443590
Iteration 17/25 | Loss: 0.00443590
Iteration 18/25 | Loss: 0.00443590
Iteration 19/25 | Loss: 0.00443590
Iteration 20/25 | Loss: 0.00443590
Iteration 21/25 | Loss: 0.00443590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004435904324054718, 0.004435904324054718, 0.004435904324054718, 0.004435904324054718, 0.004435904324054718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004435904324054718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00443590
Iteration 2/1000 | Loss: 0.00050331
Iteration 3/1000 | Loss: 0.00132834
Iteration 4/1000 | Loss: 0.00036512
Iteration 5/1000 | Loss: 0.00018721
Iteration 6/1000 | Loss: 0.00080104
Iteration 7/1000 | Loss: 0.00019932
Iteration 8/1000 | Loss: 0.00022104
Iteration 9/1000 | Loss: 0.00012889
Iteration 10/1000 | Loss: 0.00047534
Iteration 11/1000 | Loss: 0.00055066
Iteration 12/1000 | Loss: 0.00035818
Iteration 13/1000 | Loss: 0.00026219
Iteration 14/1000 | Loss: 0.00014975
Iteration 15/1000 | Loss: 0.00015176
Iteration 16/1000 | Loss: 0.00016607
Iteration 17/1000 | Loss: 0.00011315
Iteration 18/1000 | Loss: 0.00010704
Iteration 19/1000 | Loss: 0.00105434
Iteration 20/1000 | Loss: 0.00099171
Iteration 21/1000 | Loss: 0.00136877
Iteration 22/1000 | Loss: 0.00089477
Iteration 23/1000 | Loss: 0.00108006
Iteration 24/1000 | Loss: 0.00062629
Iteration 25/1000 | Loss: 0.00060497
Iteration 26/1000 | Loss: 0.00015807
Iteration 27/1000 | Loss: 0.00021365
Iteration 28/1000 | Loss: 0.00043814
Iteration 29/1000 | Loss: 0.00033666
Iteration 30/1000 | Loss: 0.00060706
Iteration 31/1000 | Loss: 0.00024285
Iteration 32/1000 | Loss: 0.00014467
Iteration 33/1000 | Loss: 0.00014421
Iteration 34/1000 | Loss: 0.00011686
Iteration 35/1000 | Loss: 0.00017449
Iteration 36/1000 | Loss: 0.00008149
Iteration 37/1000 | Loss: 0.00052009
Iteration 38/1000 | Loss: 0.00013587
Iteration 39/1000 | Loss: 0.00032794
Iteration 40/1000 | Loss: 0.00008852
Iteration 41/1000 | Loss: 0.00044369
Iteration 42/1000 | Loss: 0.00015707
Iteration 43/1000 | Loss: 0.00007242
Iteration 44/1000 | Loss: 0.00009944
Iteration 45/1000 | Loss: 0.00021191
Iteration 46/1000 | Loss: 0.00047642
Iteration 47/1000 | Loss: 0.00007456
Iteration 48/1000 | Loss: 0.00006484
Iteration 49/1000 | Loss: 0.00010955
Iteration 50/1000 | Loss: 0.00040545
Iteration 51/1000 | Loss: 0.00041582
Iteration 52/1000 | Loss: 0.00009275
Iteration 53/1000 | Loss: 0.00024351
Iteration 54/1000 | Loss: 0.00006178
Iteration 55/1000 | Loss: 0.00022703
Iteration 56/1000 | Loss: 0.00006302
Iteration 57/1000 | Loss: 0.00017757
Iteration 58/1000 | Loss: 0.00011823
Iteration 59/1000 | Loss: 0.00009242
Iteration 60/1000 | Loss: 0.00013150
Iteration 61/1000 | Loss: 0.00032590
Iteration 62/1000 | Loss: 0.00006205
Iteration 63/1000 | Loss: 0.00019101
Iteration 64/1000 | Loss: 0.00013017
Iteration 65/1000 | Loss: 0.00031480
Iteration 66/1000 | Loss: 0.00012057
Iteration 67/1000 | Loss: 0.00005385
Iteration 68/1000 | Loss: 0.00010190
Iteration 69/1000 | Loss: 0.00004663
Iteration 70/1000 | Loss: 0.00004487
Iteration 71/1000 | Loss: 0.00018527
Iteration 72/1000 | Loss: 0.00005293
Iteration 73/1000 | Loss: 0.00005423
Iteration 74/1000 | Loss: 0.00004719
Iteration 75/1000 | Loss: 0.00004335
Iteration 76/1000 | Loss: 0.00029501
Iteration 77/1000 | Loss: 0.00005907
Iteration 78/1000 | Loss: 0.00005057
Iteration 79/1000 | Loss: 0.00020055
Iteration 80/1000 | Loss: 0.00018642
Iteration 81/1000 | Loss: 0.00029336
Iteration 82/1000 | Loss: 0.00020000
Iteration 83/1000 | Loss: 0.00032357
Iteration 84/1000 | Loss: 0.00013917
Iteration 85/1000 | Loss: 0.00021991
Iteration 86/1000 | Loss: 0.00005217
Iteration 87/1000 | Loss: 0.00017669
Iteration 88/1000 | Loss: 0.00005007
Iteration 89/1000 | Loss: 0.00007885
Iteration 90/1000 | Loss: 0.00029883
Iteration 91/1000 | Loss: 0.00005217
Iteration 92/1000 | Loss: 0.00007064
Iteration 93/1000 | Loss: 0.00004043
Iteration 94/1000 | Loss: 0.00003662
Iteration 95/1000 | Loss: 0.00003402
Iteration 96/1000 | Loss: 0.00003339
Iteration 97/1000 | Loss: 0.00003294
Iteration 98/1000 | Loss: 0.00015074
Iteration 99/1000 | Loss: 0.00003275
Iteration 100/1000 | Loss: 0.00003237
Iteration 101/1000 | Loss: 0.00003218
Iteration 102/1000 | Loss: 0.00003204
Iteration 103/1000 | Loss: 0.00011673
Iteration 104/1000 | Loss: 0.00007725
Iteration 105/1000 | Loss: 0.00005443
Iteration 106/1000 | Loss: 0.00003200
Iteration 107/1000 | Loss: 0.00010083
Iteration 108/1000 | Loss: 0.00010461
Iteration 109/1000 | Loss: 0.00009510
Iteration 110/1000 | Loss: 0.00009525
Iteration 111/1000 | Loss: 0.00003202
Iteration 112/1000 | Loss: 0.00003194
Iteration 113/1000 | Loss: 0.00010866
Iteration 114/1000 | Loss: 0.00003465
Iteration 115/1000 | Loss: 0.00003311
Iteration 116/1000 | Loss: 0.00003211
Iteration 117/1000 | Loss: 0.00008287
Iteration 118/1000 | Loss: 0.00003306
Iteration 119/1000 | Loss: 0.00003230
Iteration 120/1000 | Loss: 0.00008285
Iteration 121/1000 | Loss: 0.00008285
Iteration 122/1000 | Loss: 0.00004410
Iteration 123/1000 | Loss: 0.00006265
Iteration 124/1000 | Loss: 0.00004385
Iteration 125/1000 | Loss: 0.00005653
Iteration 126/1000 | Loss: 0.00003201
Iteration 127/1000 | Loss: 0.00003186
Iteration 128/1000 | Loss: 0.00003185
Iteration 129/1000 | Loss: 0.00003185
Iteration 130/1000 | Loss: 0.00003184
Iteration 131/1000 | Loss: 0.00003184
Iteration 132/1000 | Loss: 0.00003183
Iteration 133/1000 | Loss: 0.00003183
Iteration 134/1000 | Loss: 0.00003183
Iteration 135/1000 | Loss: 0.00003182
Iteration 136/1000 | Loss: 0.00003182
Iteration 137/1000 | Loss: 0.00003182
Iteration 138/1000 | Loss: 0.00003182
Iteration 139/1000 | Loss: 0.00003182
Iteration 140/1000 | Loss: 0.00003182
Iteration 141/1000 | Loss: 0.00003182
Iteration 142/1000 | Loss: 0.00003182
Iteration 143/1000 | Loss: 0.00003182
Iteration 144/1000 | Loss: 0.00003182
Iteration 145/1000 | Loss: 0.00003182
Iteration 146/1000 | Loss: 0.00003181
Iteration 147/1000 | Loss: 0.00003181
Iteration 148/1000 | Loss: 0.00003181
Iteration 149/1000 | Loss: 0.00003181
Iteration 150/1000 | Loss: 0.00003181
Iteration 151/1000 | Loss: 0.00003181
Iteration 152/1000 | Loss: 0.00003180
Iteration 153/1000 | Loss: 0.00003180
Iteration 154/1000 | Loss: 0.00003180
Iteration 155/1000 | Loss: 0.00003180
Iteration 156/1000 | Loss: 0.00003180
Iteration 157/1000 | Loss: 0.00003180
Iteration 158/1000 | Loss: 0.00003179
Iteration 159/1000 | Loss: 0.00003179
Iteration 160/1000 | Loss: 0.00003179
Iteration 161/1000 | Loss: 0.00003179
Iteration 162/1000 | Loss: 0.00003179
Iteration 163/1000 | Loss: 0.00003178
Iteration 164/1000 | Loss: 0.00003178
Iteration 165/1000 | Loss: 0.00003178
Iteration 166/1000 | Loss: 0.00003178
Iteration 167/1000 | Loss: 0.00003178
Iteration 168/1000 | Loss: 0.00003178
Iteration 169/1000 | Loss: 0.00003178
Iteration 170/1000 | Loss: 0.00003178
Iteration 171/1000 | Loss: 0.00003178
Iteration 172/1000 | Loss: 0.00003178
Iteration 173/1000 | Loss: 0.00003178
Iteration 174/1000 | Loss: 0.00003178
Iteration 175/1000 | Loss: 0.00003178
Iteration 176/1000 | Loss: 0.00003178
Iteration 177/1000 | Loss: 0.00003178
Iteration 178/1000 | Loss: 0.00003178
Iteration 179/1000 | Loss: 0.00003178
Iteration 180/1000 | Loss: 0.00003177
Iteration 181/1000 | Loss: 0.00003177
Iteration 182/1000 | Loss: 0.00003177
Iteration 183/1000 | Loss: 0.00003177
Iteration 184/1000 | Loss: 0.00003177
Iteration 185/1000 | Loss: 0.00003177
Iteration 186/1000 | Loss: 0.00003177
Iteration 187/1000 | Loss: 0.00003177
Iteration 188/1000 | Loss: 0.00003177
Iteration 189/1000 | Loss: 0.00003177
Iteration 190/1000 | Loss: 0.00003177
Iteration 191/1000 | Loss: 0.00003177
Iteration 192/1000 | Loss: 0.00003177
Iteration 193/1000 | Loss: 0.00003177
Iteration 194/1000 | Loss: 0.00003177
Iteration 195/1000 | Loss: 0.00003177
Iteration 196/1000 | Loss: 0.00003177
Iteration 197/1000 | Loss: 0.00003177
Iteration 198/1000 | Loss: 0.00003177
Iteration 199/1000 | Loss: 0.00003177
Iteration 200/1000 | Loss: 0.00003177
Iteration 201/1000 | Loss: 0.00003177
Iteration 202/1000 | Loss: 0.00003177
Iteration 203/1000 | Loss: 0.00003177
Iteration 204/1000 | Loss: 0.00003177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.1766419851919636e-05, 3.1766419851919636e-05, 3.1766419851919636e-05, 3.1766419851919636e-05, 3.1766419851919636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1766419851919636e-05

Optimization complete. Final v2v error: 3.660722494125366 mm

Highest mean error: 11.370904922485352 mm for frame 26

Lowest mean error: 2.823798656463623 mm for frame 1

Saving results

Total time: 251.98284363746643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866448
Iteration 2/25 | Loss: 0.00128770
Iteration 3/25 | Loss: 0.00116627
Iteration 4/25 | Loss: 0.00115148
Iteration 5/25 | Loss: 0.00114795
Iteration 6/25 | Loss: 0.00114745
Iteration 7/25 | Loss: 0.00114745
Iteration 8/25 | Loss: 0.00114745
Iteration 9/25 | Loss: 0.00114745
Iteration 10/25 | Loss: 0.00114745
Iteration 11/25 | Loss: 0.00114745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001147446339018643, 0.001147446339018643, 0.001147446339018643, 0.001147446339018643, 0.001147446339018643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001147446339018643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39240909
Iteration 2/25 | Loss: 0.00335859
Iteration 3/25 | Loss: 0.00335859
Iteration 4/25 | Loss: 0.00335859
Iteration 5/25 | Loss: 0.00335859
Iteration 6/25 | Loss: 0.00335859
Iteration 7/25 | Loss: 0.00335858
Iteration 8/25 | Loss: 0.00335858
Iteration 9/25 | Loss: 0.00335858
Iteration 10/25 | Loss: 0.00335858
Iteration 11/25 | Loss: 0.00335858
Iteration 12/25 | Loss: 0.00335858
Iteration 13/25 | Loss: 0.00335858
Iteration 14/25 | Loss: 0.00335858
Iteration 15/25 | Loss: 0.00335858
Iteration 16/25 | Loss: 0.00335858
Iteration 17/25 | Loss: 0.00335858
Iteration 18/25 | Loss: 0.00335858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003358582966029644, 0.003358582966029644, 0.003358582966029644, 0.003358582966029644, 0.003358582966029644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003358582966029644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00335858
Iteration 2/1000 | Loss: 0.00003782
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002052
Iteration 5/1000 | Loss: 0.00001816
Iteration 6/1000 | Loss: 0.00001674
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001536
Iteration 9/1000 | Loss: 0.00001489
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001414
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001394
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001386
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001380
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001376
Iteration 42/1000 | Loss: 0.00001376
Iteration 43/1000 | Loss: 0.00001376
Iteration 44/1000 | Loss: 0.00001375
Iteration 45/1000 | Loss: 0.00001375
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001367
Iteration 78/1000 | Loss: 0.00001367
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001367
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001365
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001364
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.363905357720796e-05, 1.363905357720796e-05, 1.363905357720796e-05, 1.363905357720796e-05, 1.363905357720796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.363905357720796e-05

Optimization complete. Final v2v error: 3.06965708732605 mm

Highest mean error: 3.9436583518981934 mm for frame 157

Lowest mean error: 2.398622512817383 mm for frame 129

Saving results

Total time: 34.78248119354248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032899
Iteration 2/25 | Loss: 0.00284985
Iteration 3/25 | Loss: 0.00175142
Iteration 4/25 | Loss: 0.00155450
Iteration 5/25 | Loss: 0.00169858
Iteration 6/25 | Loss: 0.00153913
Iteration 7/25 | Loss: 0.00134977
Iteration 8/25 | Loss: 0.00132625
Iteration 9/25 | Loss: 0.00124470
Iteration 10/25 | Loss: 0.00121803
Iteration 11/25 | Loss: 0.00120248
Iteration 12/25 | Loss: 0.00119686
Iteration 13/25 | Loss: 0.00119812
Iteration 14/25 | Loss: 0.00120035
Iteration 15/25 | Loss: 0.00120169
Iteration 16/25 | Loss: 0.00119737
Iteration 17/25 | Loss: 0.00119420
Iteration 18/25 | Loss: 0.00119980
Iteration 19/25 | Loss: 0.00119322
Iteration 20/25 | Loss: 0.00119266
Iteration 21/25 | Loss: 0.00119498
Iteration 22/25 | Loss: 0.00118748
Iteration 23/25 | Loss: 0.00118805
Iteration 24/25 | Loss: 0.00119290
Iteration 25/25 | Loss: 0.00119273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24260378
Iteration 2/25 | Loss: 0.00420781
Iteration 3/25 | Loss: 0.00376437
Iteration 4/25 | Loss: 0.00376437
Iteration 5/25 | Loss: 0.00376437
Iteration 6/25 | Loss: 0.00376437
Iteration 7/25 | Loss: 0.00376437
Iteration 8/25 | Loss: 0.00376437
Iteration 9/25 | Loss: 0.00376437
Iteration 10/25 | Loss: 0.00376437
Iteration 11/25 | Loss: 0.00376437
Iteration 12/25 | Loss: 0.00376437
Iteration 13/25 | Loss: 0.00376437
Iteration 14/25 | Loss: 0.00376437
Iteration 15/25 | Loss: 0.00376437
Iteration 16/25 | Loss: 0.00376437
Iteration 17/25 | Loss: 0.00376437
Iteration 18/25 | Loss: 0.00376437
Iteration 19/25 | Loss: 0.00376437
Iteration 20/25 | Loss: 0.00376437
Iteration 21/25 | Loss: 0.00376437
Iteration 22/25 | Loss: 0.00376437
Iteration 23/25 | Loss: 0.00376437
Iteration 24/25 | Loss: 0.00376437
Iteration 25/25 | Loss: 0.00376437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00376437
Iteration 2/1000 | Loss: 0.00042216
Iteration 3/1000 | Loss: 0.00018891
Iteration 4/1000 | Loss: 0.00036134
Iteration 5/1000 | Loss: 0.00021081
Iteration 6/1000 | Loss: 0.00014150
Iteration 7/1000 | Loss: 0.00049541
Iteration 8/1000 | Loss: 0.00037549
Iteration 9/1000 | Loss: 0.00042734
Iteration 10/1000 | Loss: 0.00019707
Iteration 11/1000 | Loss: 0.00049353
Iteration 12/1000 | Loss: 0.00019342
Iteration 13/1000 | Loss: 0.00029675
Iteration 14/1000 | Loss: 0.00028527
Iteration 15/1000 | Loss: 0.00023977
Iteration 16/1000 | Loss: 0.00033296
Iteration 17/1000 | Loss: 0.00020862
Iteration 18/1000 | Loss: 0.00016151
Iteration 19/1000 | Loss: 0.00049812
Iteration 20/1000 | Loss: 0.00030856
Iteration 21/1000 | Loss: 0.00028932
Iteration 22/1000 | Loss: 0.00028270
Iteration 23/1000 | Loss: 0.00034246
Iteration 24/1000 | Loss: 0.00041499
Iteration 25/1000 | Loss: 0.00017038
Iteration 26/1000 | Loss: 0.00036699
Iteration 27/1000 | Loss: 0.00028186
Iteration 28/1000 | Loss: 0.00026086
Iteration 29/1000 | Loss: 0.00043488
Iteration 30/1000 | Loss: 0.00044372
Iteration 31/1000 | Loss: 0.00031810
Iteration 32/1000 | Loss: 0.00033603
Iteration 33/1000 | Loss: 0.00033717
Iteration 34/1000 | Loss: 0.00031822
Iteration 35/1000 | Loss: 0.00015539
Iteration 36/1000 | Loss: 0.00014324
Iteration 37/1000 | Loss: 0.00033142
Iteration 38/1000 | Loss: 0.00017698
Iteration 39/1000 | Loss: 0.00019909
Iteration 40/1000 | Loss: 0.00009210
Iteration 41/1000 | Loss: 0.00017671
Iteration 42/1000 | Loss: 0.00015263
Iteration 43/1000 | Loss: 0.00021956
Iteration 44/1000 | Loss: 0.00012374
Iteration 45/1000 | Loss: 0.00020710
Iteration 46/1000 | Loss: 0.00020965
Iteration 47/1000 | Loss: 0.00075340
Iteration 48/1000 | Loss: 0.00085203
Iteration 49/1000 | Loss: 0.00067527
Iteration 50/1000 | Loss: 0.00040962
Iteration 51/1000 | Loss: 0.00042056
Iteration 52/1000 | Loss: 0.00032961
Iteration 53/1000 | Loss: 0.00020339
Iteration 54/1000 | Loss: 0.00014897
Iteration 55/1000 | Loss: 0.00035678
Iteration 56/1000 | Loss: 0.00070157
Iteration 57/1000 | Loss: 0.00040047
Iteration 58/1000 | Loss: 0.00032814
Iteration 59/1000 | Loss: 0.00013325
Iteration 60/1000 | Loss: 0.00023566
Iteration 61/1000 | Loss: 0.00019469
Iteration 62/1000 | Loss: 0.00033853
Iteration 63/1000 | Loss: 0.00018549
Iteration 64/1000 | Loss: 0.00022942
Iteration 65/1000 | Loss: 0.00036431
Iteration 66/1000 | Loss: 0.00052087
Iteration 67/1000 | Loss: 0.00029421
Iteration 68/1000 | Loss: 0.00004958
Iteration 69/1000 | Loss: 0.00003842
Iteration 70/1000 | Loss: 0.00009336
Iteration 71/1000 | Loss: 0.00018007
Iteration 72/1000 | Loss: 0.00030226
Iteration 73/1000 | Loss: 0.00017759
Iteration 74/1000 | Loss: 0.00016873
Iteration 75/1000 | Loss: 0.00029911
Iteration 76/1000 | Loss: 0.00032591
Iteration 77/1000 | Loss: 0.00034673
Iteration 78/1000 | Loss: 0.00030959
Iteration 79/1000 | Loss: 0.00034571
Iteration 80/1000 | Loss: 0.00031134
Iteration 81/1000 | Loss: 0.00024961
Iteration 82/1000 | Loss: 0.00021752
Iteration 83/1000 | Loss: 0.00007004
Iteration 84/1000 | Loss: 0.00016817
Iteration 85/1000 | Loss: 0.00014550
Iteration 86/1000 | Loss: 0.00024799
Iteration 87/1000 | Loss: 0.00016387
Iteration 88/1000 | Loss: 0.00025338
Iteration 89/1000 | Loss: 0.00038533
Iteration 90/1000 | Loss: 0.00021199
Iteration 91/1000 | Loss: 0.00004360
Iteration 92/1000 | Loss: 0.00003325
Iteration 93/1000 | Loss: 0.00015130
Iteration 94/1000 | Loss: 0.00010826
Iteration 95/1000 | Loss: 0.00012045
Iteration 96/1000 | Loss: 0.00008918
Iteration 97/1000 | Loss: 0.00025345
Iteration 98/1000 | Loss: 0.00005552
Iteration 99/1000 | Loss: 0.00021404
Iteration 100/1000 | Loss: 0.00017465
Iteration 101/1000 | Loss: 0.00015337
Iteration 102/1000 | Loss: 0.00014974
Iteration 103/1000 | Loss: 0.00025560
Iteration 104/1000 | Loss: 0.00013108
Iteration 105/1000 | Loss: 0.00028327
Iteration 106/1000 | Loss: 0.00017432
Iteration 107/1000 | Loss: 0.00031578
Iteration 108/1000 | Loss: 0.00015573
Iteration 109/1000 | Loss: 0.00020885
Iteration 110/1000 | Loss: 0.00019810
Iteration 111/1000 | Loss: 0.00007257
Iteration 112/1000 | Loss: 0.00006077
Iteration 113/1000 | Loss: 0.00010972
Iteration 114/1000 | Loss: 0.00021555
Iteration 115/1000 | Loss: 0.00033677
Iteration 116/1000 | Loss: 0.00044994
Iteration 117/1000 | Loss: 0.00019726
Iteration 118/1000 | Loss: 0.00014311
Iteration 119/1000 | Loss: 0.00011984
Iteration 120/1000 | Loss: 0.00014832
Iteration 121/1000 | Loss: 0.00020549
Iteration 122/1000 | Loss: 0.00017142
Iteration 123/1000 | Loss: 0.00029268
Iteration 124/1000 | Loss: 0.00006659
Iteration 125/1000 | Loss: 0.00003735
Iteration 126/1000 | Loss: 0.00003983
Iteration 127/1000 | Loss: 0.00003080
Iteration 128/1000 | Loss: 0.00002876
Iteration 129/1000 | Loss: 0.00081917
Iteration 130/1000 | Loss: 0.00004325
Iteration 131/1000 | Loss: 0.00002669
Iteration 132/1000 | Loss: 0.00002410
Iteration 133/1000 | Loss: 0.00007464
Iteration 134/1000 | Loss: 0.00002133
Iteration 135/1000 | Loss: 0.00002039
Iteration 136/1000 | Loss: 0.00001961
Iteration 137/1000 | Loss: 0.00001915
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001835
Iteration 140/1000 | Loss: 0.00001817
Iteration 141/1000 | Loss: 0.00001805
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00002586
Iteration 145/1000 | Loss: 0.00001786
Iteration 146/1000 | Loss: 0.00002044
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001778
Iteration 149/1000 | Loss: 0.00001778
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001776
Iteration 152/1000 | Loss: 0.00001776
Iteration 153/1000 | Loss: 0.00001776
Iteration 154/1000 | Loss: 0.00001775
Iteration 155/1000 | Loss: 0.00001775
Iteration 156/1000 | Loss: 0.00001774
Iteration 157/1000 | Loss: 0.00001774
Iteration 158/1000 | Loss: 0.00001774
Iteration 159/1000 | Loss: 0.00001773
Iteration 160/1000 | Loss: 0.00001773
Iteration 161/1000 | Loss: 0.00001773
Iteration 162/1000 | Loss: 0.00001773
Iteration 163/1000 | Loss: 0.00001773
Iteration 164/1000 | Loss: 0.00001772
Iteration 165/1000 | Loss: 0.00001772
Iteration 166/1000 | Loss: 0.00001771
Iteration 167/1000 | Loss: 0.00001771
Iteration 168/1000 | Loss: 0.00001771
Iteration 169/1000 | Loss: 0.00001770
Iteration 170/1000 | Loss: 0.00001770
Iteration 171/1000 | Loss: 0.00001770
Iteration 172/1000 | Loss: 0.00001770
Iteration 173/1000 | Loss: 0.00001769
Iteration 174/1000 | Loss: 0.00001769
Iteration 175/1000 | Loss: 0.00001769
Iteration 176/1000 | Loss: 0.00001769
Iteration 177/1000 | Loss: 0.00001769
Iteration 178/1000 | Loss: 0.00001769
Iteration 179/1000 | Loss: 0.00001769
Iteration 180/1000 | Loss: 0.00001769
Iteration 181/1000 | Loss: 0.00001769
Iteration 182/1000 | Loss: 0.00001769
Iteration 183/1000 | Loss: 0.00001768
Iteration 184/1000 | Loss: 0.00001768
Iteration 185/1000 | Loss: 0.00001768
Iteration 186/1000 | Loss: 0.00001767
Iteration 187/1000 | Loss: 0.00001766
Iteration 188/1000 | Loss: 0.00001766
Iteration 189/1000 | Loss: 0.00001765
Iteration 190/1000 | Loss: 0.00001765
Iteration 191/1000 | Loss: 0.00001765
Iteration 192/1000 | Loss: 0.00001765
Iteration 193/1000 | Loss: 0.00001765
Iteration 194/1000 | Loss: 0.00001765
Iteration 195/1000 | Loss: 0.00001765
Iteration 196/1000 | Loss: 0.00001764
Iteration 197/1000 | Loss: 0.00001764
Iteration 198/1000 | Loss: 0.00001764
Iteration 199/1000 | Loss: 0.00001763
Iteration 200/1000 | Loss: 0.00001763
Iteration 201/1000 | Loss: 0.00001763
Iteration 202/1000 | Loss: 0.00001762
Iteration 203/1000 | Loss: 0.00001762
Iteration 204/1000 | Loss: 0.00001762
Iteration 205/1000 | Loss: 0.00001762
Iteration 206/1000 | Loss: 0.00001761
Iteration 207/1000 | Loss: 0.00001761
Iteration 208/1000 | Loss: 0.00001761
Iteration 209/1000 | Loss: 0.00001761
Iteration 210/1000 | Loss: 0.00001761
Iteration 211/1000 | Loss: 0.00001761
Iteration 212/1000 | Loss: 0.00001761
Iteration 213/1000 | Loss: 0.00001761
Iteration 214/1000 | Loss: 0.00001761
Iteration 215/1000 | Loss: 0.00001760
Iteration 216/1000 | Loss: 0.00001760
Iteration 217/1000 | Loss: 0.00001760
Iteration 218/1000 | Loss: 0.00001760
Iteration 219/1000 | Loss: 0.00001760
Iteration 220/1000 | Loss: 0.00001760
Iteration 221/1000 | Loss: 0.00001760
Iteration 222/1000 | Loss: 0.00001760
Iteration 223/1000 | Loss: 0.00001759
Iteration 224/1000 | Loss: 0.00001759
Iteration 225/1000 | Loss: 0.00001759
Iteration 226/1000 | Loss: 0.00001759
Iteration 227/1000 | Loss: 0.00001759
Iteration 228/1000 | Loss: 0.00001759
Iteration 229/1000 | Loss: 0.00001758
Iteration 230/1000 | Loss: 0.00001758
Iteration 231/1000 | Loss: 0.00001758
Iteration 232/1000 | Loss: 0.00001758
Iteration 233/1000 | Loss: 0.00001757
Iteration 234/1000 | Loss: 0.00001757
Iteration 235/1000 | Loss: 0.00001757
Iteration 236/1000 | Loss: 0.00001756
Iteration 237/1000 | Loss: 0.00001756
Iteration 238/1000 | Loss: 0.00001756
Iteration 239/1000 | Loss: 0.00001756
Iteration 240/1000 | Loss: 0.00001756
Iteration 241/1000 | Loss: 0.00001755
Iteration 242/1000 | Loss: 0.00001755
Iteration 243/1000 | Loss: 0.00001755
Iteration 244/1000 | Loss: 0.00001755
Iteration 245/1000 | Loss: 0.00001755
Iteration 246/1000 | Loss: 0.00001755
Iteration 247/1000 | Loss: 0.00001755
Iteration 248/1000 | Loss: 0.00001755
Iteration 249/1000 | Loss: 0.00001755
Iteration 250/1000 | Loss: 0.00001754
Iteration 251/1000 | Loss: 0.00001754
Iteration 252/1000 | Loss: 0.00001754
Iteration 253/1000 | Loss: 0.00001754
Iteration 254/1000 | Loss: 0.00001754
Iteration 255/1000 | Loss: 0.00001754
Iteration 256/1000 | Loss: 0.00001754
Iteration 257/1000 | Loss: 0.00001754
Iteration 258/1000 | Loss: 0.00001754
Iteration 259/1000 | Loss: 0.00001754
Iteration 260/1000 | Loss: 0.00001754
Iteration 261/1000 | Loss: 0.00001754
Iteration 262/1000 | Loss: 0.00001754
Iteration 263/1000 | Loss: 0.00001754
Iteration 264/1000 | Loss: 0.00001754
Iteration 265/1000 | Loss: 0.00001754
Iteration 266/1000 | Loss: 0.00001754
Iteration 267/1000 | Loss: 0.00001754
Iteration 268/1000 | Loss: 0.00001754
Iteration 269/1000 | Loss: 0.00001754
Iteration 270/1000 | Loss: 0.00001754
Iteration 271/1000 | Loss: 0.00001754
Iteration 272/1000 | Loss: 0.00001754
Iteration 273/1000 | Loss: 0.00001754
Iteration 274/1000 | Loss: 0.00001753
Iteration 275/1000 | Loss: 0.00001753
Iteration 276/1000 | Loss: 0.00001753
Iteration 277/1000 | Loss: 0.00001753
Iteration 278/1000 | Loss: 0.00001753
Iteration 279/1000 | Loss: 0.00001753
Iteration 280/1000 | Loss: 0.00001753
Iteration 281/1000 | Loss: 0.00001753
Iteration 282/1000 | Loss: 0.00001753
Iteration 283/1000 | Loss: 0.00001753
Iteration 284/1000 | Loss: 0.00001753
Iteration 285/1000 | Loss: 0.00001752
Iteration 286/1000 | Loss: 0.00001752
Iteration 287/1000 | Loss: 0.00001752
Iteration 288/1000 | Loss: 0.00001752
Iteration 289/1000 | Loss: 0.00001752
Iteration 290/1000 | Loss: 0.00001752
Iteration 291/1000 | Loss: 0.00001752
Iteration 292/1000 | Loss: 0.00001752
Iteration 293/1000 | Loss: 0.00001752
Iteration 294/1000 | Loss: 0.00001752
Iteration 295/1000 | Loss: 0.00001752
Iteration 296/1000 | Loss: 0.00001752
Iteration 297/1000 | Loss: 0.00001752
Iteration 298/1000 | Loss: 0.00001752
Iteration 299/1000 | Loss: 0.00001752
Iteration 300/1000 | Loss: 0.00001752
Iteration 301/1000 | Loss: 0.00001751
Iteration 302/1000 | Loss: 0.00001751
Iteration 303/1000 | Loss: 0.00001751
Iteration 304/1000 | Loss: 0.00001751
Iteration 305/1000 | Loss: 0.00001751
Iteration 306/1000 | Loss: 0.00001751
Iteration 307/1000 | Loss: 0.00001751
Iteration 308/1000 | Loss: 0.00001751
Iteration 309/1000 | Loss: 0.00001751
Iteration 310/1000 | Loss: 0.00001751
Iteration 311/1000 | Loss: 0.00001751
Iteration 312/1000 | Loss: 0.00001751
Iteration 313/1000 | Loss: 0.00001751
Iteration 314/1000 | Loss: 0.00001751
Iteration 315/1000 | Loss: 0.00001751
Iteration 316/1000 | Loss: 0.00001751
Iteration 317/1000 | Loss: 0.00001750
Iteration 318/1000 | Loss: 0.00001750
Iteration 319/1000 | Loss: 0.00001750
Iteration 320/1000 | Loss: 0.00001750
Iteration 321/1000 | Loss: 0.00001750
Iteration 322/1000 | Loss: 0.00001750
Iteration 323/1000 | Loss: 0.00001750
Iteration 324/1000 | Loss: 0.00001750
Iteration 325/1000 | Loss: 0.00001750
Iteration 326/1000 | Loss: 0.00001750
Iteration 327/1000 | Loss: 0.00001750
Iteration 328/1000 | Loss: 0.00001750
Iteration 329/1000 | Loss: 0.00001750
Iteration 330/1000 | Loss: 0.00001750
Iteration 331/1000 | Loss: 0.00001750
Iteration 332/1000 | Loss: 0.00001750
Iteration 333/1000 | Loss: 0.00001750
Iteration 334/1000 | Loss: 0.00001750
Iteration 335/1000 | Loss: 0.00001750
Iteration 336/1000 | Loss: 0.00001750
Iteration 337/1000 | Loss: 0.00001750
Iteration 338/1000 | Loss: 0.00001750
Iteration 339/1000 | Loss: 0.00001750
Iteration 340/1000 | Loss: 0.00001750
Iteration 341/1000 | Loss: 0.00001750
Iteration 342/1000 | Loss: 0.00001750
Iteration 343/1000 | Loss: 0.00001750
Iteration 344/1000 | Loss: 0.00001750
Iteration 345/1000 | Loss: 0.00001750
Iteration 346/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 346. Stopping optimization.
Last 5 losses: [1.7495220163255e-05, 1.7495220163255e-05, 1.7495220163255e-05, 1.7495220163255e-05, 1.7495220163255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7495220163255e-05

Optimization complete. Final v2v error: 3.1286122798919678 mm

Highest mean error: 11.44730281829834 mm for frame 97

Lowest mean error: 2.528695821762085 mm for frame 54

Saving results

Total time: 263.98832178115845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808953
Iteration 2/25 | Loss: 0.00162784
Iteration 3/25 | Loss: 0.00134727
Iteration 4/25 | Loss: 0.00127471
Iteration 5/25 | Loss: 0.00125741
Iteration 6/25 | Loss: 0.00123063
Iteration 7/25 | Loss: 0.00122483
Iteration 8/25 | Loss: 0.00122423
Iteration 9/25 | Loss: 0.00122427
Iteration 10/25 | Loss: 0.00122078
Iteration 11/25 | Loss: 0.00121903
Iteration 12/25 | Loss: 0.00121893
Iteration 13/25 | Loss: 0.00121893
Iteration 14/25 | Loss: 0.00121892
Iteration 15/25 | Loss: 0.00121892
Iteration 16/25 | Loss: 0.00121892
Iteration 17/25 | Loss: 0.00121892
Iteration 18/25 | Loss: 0.00121892
Iteration 19/25 | Loss: 0.00121892
Iteration 20/25 | Loss: 0.00121892
Iteration 21/25 | Loss: 0.00121892
Iteration 22/25 | Loss: 0.00121892
Iteration 23/25 | Loss: 0.00121892
Iteration 24/25 | Loss: 0.00121892
Iteration 25/25 | Loss: 0.00121891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03339267
Iteration 2/25 | Loss: 0.00484257
Iteration 3/25 | Loss: 0.00484251
Iteration 4/25 | Loss: 0.00484251
Iteration 5/25 | Loss: 0.00484251
Iteration 6/25 | Loss: 0.00484251
Iteration 7/25 | Loss: 0.00484251
Iteration 8/25 | Loss: 0.00484251
Iteration 9/25 | Loss: 0.00484251
Iteration 10/25 | Loss: 0.00484251
Iteration 11/25 | Loss: 0.00484251
Iteration 12/25 | Loss: 0.00484251
Iteration 13/25 | Loss: 0.00484251
Iteration 14/25 | Loss: 0.00484251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004842506255954504, 0.004842506255954504, 0.004842506255954504, 0.004842506255954504, 0.004842506255954504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004842506255954504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00484251
Iteration 2/1000 | Loss: 0.00016295
Iteration 3/1000 | Loss: 0.00123869
Iteration 4/1000 | Loss: 0.00085724
Iteration 5/1000 | Loss: 0.00031418
Iteration 6/1000 | Loss: 0.00049627
Iteration 7/1000 | Loss: 0.00023029
Iteration 8/1000 | Loss: 0.00004668
Iteration 9/1000 | Loss: 0.00003751
Iteration 10/1000 | Loss: 0.00003200
Iteration 11/1000 | Loss: 0.00057562
Iteration 12/1000 | Loss: 0.00076536
Iteration 13/1000 | Loss: 0.00004904
Iteration 14/1000 | Loss: 0.00003650
Iteration 15/1000 | Loss: 0.00003170
Iteration 16/1000 | Loss: 0.00002929
Iteration 17/1000 | Loss: 0.00002797
Iteration 18/1000 | Loss: 0.00035114
Iteration 19/1000 | Loss: 0.00014205
Iteration 20/1000 | Loss: 0.00013372
Iteration 21/1000 | Loss: 0.00002603
Iteration 22/1000 | Loss: 0.00002400
Iteration 23/1000 | Loss: 0.00002275
Iteration 24/1000 | Loss: 0.00002199
Iteration 25/1000 | Loss: 0.00002144
Iteration 26/1000 | Loss: 0.00053727
Iteration 27/1000 | Loss: 0.00034027
Iteration 28/1000 | Loss: 0.00064076
Iteration 29/1000 | Loss: 0.00030025
Iteration 30/1000 | Loss: 0.00046301
Iteration 31/1000 | Loss: 0.00020863
Iteration 32/1000 | Loss: 0.00045093
Iteration 33/1000 | Loss: 0.00022585
Iteration 34/1000 | Loss: 0.00025403
Iteration 35/1000 | Loss: 0.00038748
Iteration 36/1000 | Loss: 0.00003031
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00001920
Iteration 39/1000 | Loss: 0.00001796
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001664
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001625
Iteration 58/1000 | Loss: 0.00001624
Iteration 59/1000 | Loss: 0.00001624
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001602
Iteration 81/1000 | Loss: 0.00001602
Iteration 82/1000 | Loss: 0.00001602
Iteration 83/1000 | Loss: 0.00001601
Iteration 84/1000 | Loss: 0.00001601
Iteration 85/1000 | Loss: 0.00001601
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001599
Iteration 89/1000 | Loss: 0.00001599
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001597
Iteration 93/1000 | Loss: 0.00001597
Iteration 94/1000 | Loss: 0.00001597
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001596
Iteration 97/1000 | Loss: 0.00001596
Iteration 98/1000 | Loss: 0.00001596
Iteration 99/1000 | Loss: 0.00001596
Iteration 100/1000 | Loss: 0.00001596
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001593
Iteration 113/1000 | Loss: 0.00001593
Iteration 114/1000 | Loss: 0.00001593
Iteration 115/1000 | Loss: 0.00001593
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001593
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001593
Iteration 124/1000 | Loss: 0.00001593
Iteration 125/1000 | Loss: 0.00001593
Iteration 126/1000 | Loss: 0.00001593
Iteration 127/1000 | Loss: 0.00001593
Iteration 128/1000 | Loss: 0.00001593
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001592
Iteration 131/1000 | Loss: 0.00001592
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001592
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001591
Iteration 139/1000 | Loss: 0.00001591
Iteration 140/1000 | Loss: 0.00001591
Iteration 141/1000 | Loss: 0.00001591
Iteration 142/1000 | Loss: 0.00001590
Iteration 143/1000 | Loss: 0.00001590
Iteration 144/1000 | Loss: 0.00001590
Iteration 145/1000 | Loss: 0.00001590
Iteration 146/1000 | Loss: 0.00001590
Iteration 147/1000 | Loss: 0.00001590
Iteration 148/1000 | Loss: 0.00001590
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001589
Iteration 151/1000 | Loss: 0.00001589
Iteration 152/1000 | Loss: 0.00001589
Iteration 153/1000 | Loss: 0.00001589
Iteration 154/1000 | Loss: 0.00001589
Iteration 155/1000 | Loss: 0.00001589
Iteration 156/1000 | Loss: 0.00001589
Iteration 157/1000 | Loss: 0.00001589
Iteration 158/1000 | Loss: 0.00001589
Iteration 159/1000 | Loss: 0.00001589
Iteration 160/1000 | Loss: 0.00001589
Iteration 161/1000 | Loss: 0.00001589
Iteration 162/1000 | Loss: 0.00001589
Iteration 163/1000 | Loss: 0.00001589
Iteration 164/1000 | Loss: 0.00001589
Iteration 165/1000 | Loss: 0.00001589
Iteration 166/1000 | Loss: 0.00001589
Iteration 167/1000 | Loss: 0.00001589
Iteration 168/1000 | Loss: 0.00001589
Iteration 169/1000 | Loss: 0.00001589
Iteration 170/1000 | Loss: 0.00001589
Iteration 171/1000 | Loss: 0.00001589
Iteration 172/1000 | Loss: 0.00001589
Iteration 173/1000 | Loss: 0.00001589
Iteration 174/1000 | Loss: 0.00001589
Iteration 175/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.588838676980231e-05, 1.588838676980231e-05, 1.588838676980231e-05, 1.588838676980231e-05, 1.588838676980231e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.588838676980231e-05

Optimization complete. Final v2v error: 3.4373714923858643 mm

Highest mean error: 4.1968536376953125 mm for frame 15

Lowest mean error: 3.1012439727783203 mm for frame 92

Saving results

Total time: 108.23986339569092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488620
Iteration 2/25 | Loss: 0.00130170
Iteration 3/25 | Loss: 0.00114162
Iteration 4/25 | Loss: 0.00113072
Iteration 5/25 | Loss: 0.00112740
Iteration 6/25 | Loss: 0.00112715
Iteration 7/25 | Loss: 0.00112715
Iteration 8/25 | Loss: 0.00112715
Iteration 9/25 | Loss: 0.00112715
Iteration 10/25 | Loss: 0.00112715
Iteration 11/25 | Loss: 0.00112715
Iteration 12/25 | Loss: 0.00112715
Iteration 13/25 | Loss: 0.00112715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011271466501057148, 0.0011271466501057148, 0.0011271466501057148, 0.0011271466501057148, 0.0011271466501057148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011271466501057148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.45761776
Iteration 2/25 | Loss: 0.00267859
Iteration 3/25 | Loss: 0.00267851
Iteration 4/25 | Loss: 0.00267851
Iteration 5/25 | Loss: 0.00267851
Iteration 6/25 | Loss: 0.00267851
Iteration 7/25 | Loss: 0.00267851
Iteration 8/25 | Loss: 0.00267851
Iteration 9/25 | Loss: 0.00267851
Iteration 10/25 | Loss: 0.00267851
Iteration 11/25 | Loss: 0.00267851
Iteration 12/25 | Loss: 0.00267851
Iteration 13/25 | Loss: 0.00267851
Iteration 14/25 | Loss: 0.00267851
Iteration 15/25 | Loss: 0.00267851
Iteration 16/25 | Loss: 0.00267851
Iteration 17/25 | Loss: 0.00267851
Iteration 18/25 | Loss: 0.00267851
Iteration 19/25 | Loss: 0.00267851
Iteration 20/25 | Loss: 0.00267851
Iteration 21/25 | Loss: 0.00267851
Iteration 22/25 | Loss: 0.00267851
Iteration 23/25 | Loss: 0.00267851
Iteration 24/25 | Loss: 0.00267851
Iteration 25/25 | Loss: 0.00267851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267851
Iteration 2/1000 | Loss: 0.00003897
Iteration 3/1000 | Loss: 0.00002227
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001869
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001619
Iteration 12/1000 | Loss: 0.00001609
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001592
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001591
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001591
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001588
Iteration 41/1000 | Loss: 0.00001588
Iteration 42/1000 | Loss: 0.00001588
Iteration 43/1000 | Loss: 0.00001588
Iteration 44/1000 | Loss: 0.00001588
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001587
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001586
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001585
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001585
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001585
Iteration 65/1000 | Loss: 0.00001585
Iteration 66/1000 | Loss: 0.00001585
Iteration 67/1000 | Loss: 0.00001585
Iteration 68/1000 | Loss: 0.00001585
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001585
Iteration 71/1000 | Loss: 0.00001585
Iteration 72/1000 | Loss: 0.00001585
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001584
Iteration 77/1000 | Loss: 0.00001584
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001584
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.5844589142943732e-05, 1.5844589142943732e-05, 1.5844589142943732e-05, 1.5844589142943732e-05, 1.5844589142943732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5844589142943732e-05

Optimization complete. Final v2v error: 3.2435247898101807 mm

Highest mean error: 3.8595917224884033 mm for frame 39

Lowest mean error: 2.660521984100342 mm for frame 0

Saving results

Total time: 31.223150491714478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_nl_5724/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_nl_5724/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819272
Iteration 2/25 | Loss: 0.00139318
Iteration 3/25 | Loss: 0.00120747
Iteration 4/25 | Loss: 0.00117594
Iteration 5/25 | Loss: 0.00116766
Iteration 6/25 | Loss: 0.00116452
Iteration 7/25 | Loss: 0.00116405
Iteration 8/25 | Loss: 0.00116405
Iteration 9/25 | Loss: 0.00116405
Iteration 10/25 | Loss: 0.00116405
Iteration 11/25 | Loss: 0.00116405
Iteration 12/25 | Loss: 0.00116405
Iteration 13/25 | Loss: 0.00116405
Iteration 14/25 | Loss: 0.00116405
Iteration 15/25 | Loss: 0.00116405
Iteration 16/25 | Loss: 0.00116405
Iteration 17/25 | Loss: 0.00116405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011640519369393587, 0.0011640519369393587, 0.0011640519369393587, 0.0011640519369393587, 0.0011640519369393587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011640519369393587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10814941
Iteration 2/25 | Loss: 0.00419007
Iteration 3/25 | Loss: 0.00419006
Iteration 4/25 | Loss: 0.00419006
Iteration 5/25 | Loss: 0.00419006
Iteration 6/25 | Loss: 0.00419006
Iteration 7/25 | Loss: 0.00419006
Iteration 8/25 | Loss: 0.00419006
Iteration 9/25 | Loss: 0.00419006
Iteration 10/25 | Loss: 0.00419006
Iteration 11/25 | Loss: 0.00419006
Iteration 12/25 | Loss: 0.00419006
Iteration 13/25 | Loss: 0.00419006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0041900621727108955, 0.0041900621727108955, 0.0041900621727108955, 0.0041900621727108955, 0.0041900621727108955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0041900621727108955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00419006
Iteration 2/1000 | Loss: 0.00006012
Iteration 3/1000 | Loss: 0.00003366
Iteration 4/1000 | Loss: 0.00002461
Iteration 5/1000 | Loss: 0.00002208
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00001981
Iteration 8/1000 | Loss: 0.00001922
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001827
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001809
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001802
Iteration 16/1000 | Loss: 0.00001802
Iteration 17/1000 | Loss: 0.00001801
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001800
Iteration 20/1000 | Loss: 0.00001800
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001798
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001795
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001794
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001794
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001791
Iteration 36/1000 | Loss: 0.00001791
Iteration 37/1000 | Loss: 0.00001790
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001786
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.7844860849436373e-05, 1.7844860849436373e-05, 1.7844860849436373e-05, 1.7844860849436373e-05, 1.7844860849436373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7844860849436373e-05

Optimization complete. Final v2v error: 3.4329941272735596 mm

Highest mean error: 3.984466075897217 mm for frame 154

Lowest mean error: 2.5555503368377686 mm for frame 1

Saving results

Total time: 32.48529529571533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108638
Iteration 2/25 | Loss: 0.00354318
Iteration 3/25 | Loss: 0.00224142
Iteration 4/25 | Loss: 0.00208454
Iteration 5/25 | Loss: 0.00198668
Iteration 6/25 | Loss: 0.00190637
Iteration 7/25 | Loss: 0.00173838
Iteration 8/25 | Loss: 0.00171279
Iteration 9/25 | Loss: 0.00173984
Iteration 10/25 | Loss: 0.00166806
Iteration 11/25 | Loss: 0.00168160
Iteration 12/25 | Loss: 0.00166459
Iteration 13/25 | Loss: 0.00160707
Iteration 14/25 | Loss: 0.00164363
Iteration 15/25 | Loss: 0.00152693
Iteration 16/25 | Loss: 0.00151736
Iteration 17/25 | Loss: 0.00151683
Iteration 18/25 | Loss: 0.00149454
Iteration 19/25 | Loss: 0.00147759
Iteration 20/25 | Loss: 0.00147149
Iteration 21/25 | Loss: 0.00146914
Iteration 22/25 | Loss: 0.00148743
Iteration 23/25 | Loss: 0.00146788
Iteration 24/25 | Loss: 0.00146545
Iteration 25/25 | Loss: 0.00150796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03805161
Iteration 2/25 | Loss: 0.00282623
Iteration 3/25 | Loss: 0.00282622
Iteration 4/25 | Loss: 0.00282622
Iteration 5/25 | Loss: 0.00282622
Iteration 6/25 | Loss: 0.00282622
Iteration 7/25 | Loss: 0.00282622
Iteration 8/25 | Loss: 0.00282622
Iteration 9/25 | Loss: 0.00282622
Iteration 10/25 | Loss: 0.00282622
Iteration 11/25 | Loss: 0.00282622
Iteration 12/25 | Loss: 0.00282622
Iteration 13/25 | Loss: 0.00282622
Iteration 14/25 | Loss: 0.00282622
Iteration 15/25 | Loss: 0.00282622
Iteration 16/25 | Loss: 0.00282622
Iteration 17/25 | Loss: 0.00282622
Iteration 18/25 | Loss: 0.00282622
Iteration 19/25 | Loss: 0.00282622
Iteration 20/25 | Loss: 0.00282622
Iteration 21/25 | Loss: 0.00282622
Iteration 22/25 | Loss: 0.00282622
Iteration 23/25 | Loss: 0.00282622
Iteration 24/25 | Loss: 0.00282622
Iteration 25/25 | Loss: 0.00282622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282622
Iteration 2/1000 | Loss: 0.00068704
Iteration 3/1000 | Loss: 0.00074691
Iteration 4/1000 | Loss: 0.00330369
Iteration 5/1000 | Loss: 0.00056947
Iteration 6/1000 | Loss: 0.00069131
Iteration 7/1000 | Loss: 0.00035839
Iteration 8/1000 | Loss: 0.00119002
Iteration 9/1000 | Loss: 0.00022808
Iteration 10/1000 | Loss: 0.00031694
Iteration 11/1000 | Loss: 0.00046275
Iteration 12/1000 | Loss: 0.00039041
Iteration 13/1000 | Loss: 0.00089681
Iteration 14/1000 | Loss: 0.00031226
Iteration 15/1000 | Loss: 0.00025010
Iteration 16/1000 | Loss: 0.00038094
Iteration 17/1000 | Loss: 0.00121423
Iteration 18/1000 | Loss: 0.00080862
Iteration 19/1000 | Loss: 0.00053789
Iteration 20/1000 | Loss: 0.00031442
Iteration 21/1000 | Loss: 0.00045359
Iteration 22/1000 | Loss: 0.00021855
Iteration 23/1000 | Loss: 0.00051279
Iteration 24/1000 | Loss: 0.00067059
Iteration 25/1000 | Loss: 0.00054110
Iteration 26/1000 | Loss: 0.00044874
Iteration 27/1000 | Loss: 0.00036924
Iteration 28/1000 | Loss: 0.00032043
Iteration 29/1000 | Loss: 0.00055853
Iteration 30/1000 | Loss: 0.00038817
Iteration 31/1000 | Loss: 0.00023249
Iteration 32/1000 | Loss: 0.00018120
Iteration 33/1000 | Loss: 0.00022787
Iteration 34/1000 | Loss: 0.00019792
Iteration 35/1000 | Loss: 0.00079198
Iteration 36/1000 | Loss: 0.00040490
Iteration 37/1000 | Loss: 0.00042319
Iteration 38/1000 | Loss: 0.00072508
Iteration 39/1000 | Loss: 0.00049438
Iteration 40/1000 | Loss: 0.00046523
Iteration 41/1000 | Loss: 0.00048351
Iteration 42/1000 | Loss: 0.00025001
Iteration 43/1000 | Loss: 0.00021859
Iteration 44/1000 | Loss: 0.00024281
Iteration 45/1000 | Loss: 0.00052529
Iteration 46/1000 | Loss: 0.00020182
Iteration 47/1000 | Loss: 0.00015722
Iteration 48/1000 | Loss: 0.00060058
Iteration 49/1000 | Loss: 0.00019114
Iteration 50/1000 | Loss: 0.00018261
Iteration 51/1000 | Loss: 0.00052867
Iteration 52/1000 | Loss: 0.00017678
Iteration 53/1000 | Loss: 0.00064071
Iteration 54/1000 | Loss: 0.00024591
Iteration 55/1000 | Loss: 0.00037717
Iteration 56/1000 | Loss: 0.00023835
Iteration 57/1000 | Loss: 0.00019753
Iteration 58/1000 | Loss: 0.00015101
Iteration 59/1000 | Loss: 0.00013707
Iteration 60/1000 | Loss: 0.00033406
Iteration 61/1000 | Loss: 0.00089132
Iteration 62/1000 | Loss: 0.00091603
Iteration 63/1000 | Loss: 0.00026199
Iteration 64/1000 | Loss: 0.00024654
Iteration 65/1000 | Loss: 0.00014631
Iteration 66/1000 | Loss: 0.00013825
Iteration 67/1000 | Loss: 0.00031883
Iteration 68/1000 | Loss: 0.00023001
Iteration 69/1000 | Loss: 0.00016847
Iteration 70/1000 | Loss: 0.00013157
Iteration 71/1000 | Loss: 0.00043146
Iteration 72/1000 | Loss: 0.00036146
Iteration 73/1000 | Loss: 0.00019878
Iteration 74/1000 | Loss: 0.00013286
Iteration 75/1000 | Loss: 0.00015773
Iteration 76/1000 | Loss: 0.00035144
Iteration 77/1000 | Loss: 0.00153959
Iteration 78/1000 | Loss: 0.00017674
Iteration 79/1000 | Loss: 0.00013360
Iteration 80/1000 | Loss: 0.00012413
Iteration 81/1000 | Loss: 0.00017507
Iteration 82/1000 | Loss: 0.00011681
Iteration 83/1000 | Loss: 0.00011956
Iteration 84/1000 | Loss: 0.00010219
Iteration 85/1000 | Loss: 0.00011986
Iteration 86/1000 | Loss: 0.00014119
Iteration 87/1000 | Loss: 0.00013903
Iteration 88/1000 | Loss: 0.00009914
Iteration 89/1000 | Loss: 0.00009804
Iteration 90/1000 | Loss: 0.00009738
Iteration 91/1000 | Loss: 0.00009695
Iteration 92/1000 | Loss: 0.00011916
Iteration 93/1000 | Loss: 0.00009719
Iteration 94/1000 | Loss: 0.00009641
Iteration 95/1000 | Loss: 0.00011967
Iteration 96/1000 | Loss: 0.00009861
Iteration 97/1000 | Loss: 0.00012463
Iteration 98/1000 | Loss: 0.00009964
Iteration 99/1000 | Loss: 0.00009576
Iteration 100/1000 | Loss: 0.00009573
Iteration 101/1000 | Loss: 0.00009573
Iteration 102/1000 | Loss: 0.00013155
Iteration 103/1000 | Loss: 0.00009683
Iteration 104/1000 | Loss: 0.00038777
Iteration 105/1000 | Loss: 0.00078086
Iteration 106/1000 | Loss: 0.00015054
Iteration 107/1000 | Loss: 0.00016962
Iteration 108/1000 | Loss: 0.00013980
Iteration 109/1000 | Loss: 0.00020018
Iteration 110/1000 | Loss: 0.00010351
Iteration 111/1000 | Loss: 0.00010445
Iteration 112/1000 | Loss: 0.00020061
Iteration 113/1000 | Loss: 0.00016872
Iteration 114/1000 | Loss: 0.00022772
Iteration 115/1000 | Loss: 0.00036246
Iteration 116/1000 | Loss: 0.00009643
Iteration 117/1000 | Loss: 0.00023118
Iteration 118/1000 | Loss: 0.00010414
Iteration 119/1000 | Loss: 0.00010623
Iteration 120/1000 | Loss: 0.00071970
Iteration 121/1000 | Loss: 0.00048690
Iteration 122/1000 | Loss: 0.00019223
Iteration 123/1000 | Loss: 0.00016505
Iteration 124/1000 | Loss: 0.00011888
Iteration 125/1000 | Loss: 0.00020681
Iteration 126/1000 | Loss: 0.00010044
Iteration 127/1000 | Loss: 0.00011412
Iteration 128/1000 | Loss: 0.00114217
Iteration 129/1000 | Loss: 0.00048973
Iteration 130/1000 | Loss: 0.00014452
Iteration 131/1000 | Loss: 0.00015677
Iteration 132/1000 | Loss: 0.00073455
Iteration 133/1000 | Loss: 0.00024869
Iteration 134/1000 | Loss: 0.00014292
Iteration 135/1000 | Loss: 0.00072986
Iteration 136/1000 | Loss: 0.00054297
Iteration 137/1000 | Loss: 0.00027092
Iteration 138/1000 | Loss: 0.00014055
Iteration 139/1000 | Loss: 0.00013423
Iteration 140/1000 | Loss: 0.00011996
Iteration 141/1000 | Loss: 0.00012398
Iteration 142/1000 | Loss: 0.00010612
Iteration 143/1000 | Loss: 0.00052941
Iteration 144/1000 | Loss: 0.00028840
Iteration 145/1000 | Loss: 0.00051727
Iteration 146/1000 | Loss: 0.00024388
Iteration 147/1000 | Loss: 0.00070892
Iteration 148/1000 | Loss: 0.00027884
Iteration 149/1000 | Loss: 0.00036015
Iteration 150/1000 | Loss: 0.00015431
Iteration 151/1000 | Loss: 0.00036874
Iteration 152/1000 | Loss: 0.00045045
Iteration 153/1000 | Loss: 0.00023173
Iteration 154/1000 | Loss: 0.00047027
Iteration 155/1000 | Loss: 0.00038791
Iteration 156/1000 | Loss: 0.00012277
Iteration 157/1000 | Loss: 0.00012024
Iteration 158/1000 | Loss: 0.00043790
Iteration 159/1000 | Loss: 0.00039219
Iteration 160/1000 | Loss: 0.00045311
Iteration 161/1000 | Loss: 0.00037993
Iteration 162/1000 | Loss: 0.00028445
Iteration 163/1000 | Loss: 0.00017520
Iteration 164/1000 | Loss: 0.00016258
Iteration 165/1000 | Loss: 0.00029225
Iteration 166/1000 | Loss: 0.00021887
Iteration 167/1000 | Loss: 0.00035496
Iteration 168/1000 | Loss: 0.00024922
Iteration 169/1000 | Loss: 0.00014155
Iteration 170/1000 | Loss: 0.00012877
Iteration 171/1000 | Loss: 0.00024379
Iteration 172/1000 | Loss: 0.00020239
Iteration 173/1000 | Loss: 0.00018284
Iteration 174/1000 | Loss: 0.00009956
Iteration 175/1000 | Loss: 0.00009375
Iteration 176/1000 | Loss: 0.00011643
Iteration 177/1000 | Loss: 0.00010572
Iteration 178/1000 | Loss: 0.00009012
Iteration 179/1000 | Loss: 0.00011000
Iteration 180/1000 | Loss: 0.00010718
Iteration 181/1000 | Loss: 0.00008794
Iteration 182/1000 | Loss: 0.00008752
Iteration 183/1000 | Loss: 0.00011931
Iteration 184/1000 | Loss: 0.00008901
Iteration 185/1000 | Loss: 0.00008715
Iteration 186/1000 | Loss: 0.00008714
Iteration 187/1000 | Loss: 0.00009001
Iteration 188/1000 | Loss: 0.00008706
Iteration 189/1000 | Loss: 0.00008704
Iteration 190/1000 | Loss: 0.00008704
Iteration 191/1000 | Loss: 0.00008704
Iteration 192/1000 | Loss: 0.00008704
Iteration 193/1000 | Loss: 0.00008859
Iteration 194/1000 | Loss: 0.00008904
Iteration 195/1000 | Loss: 0.00008702
Iteration 196/1000 | Loss: 0.00008702
Iteration 197/1000 | Loss: 0.00008702
Iteration 198/1000 | Loss: 0.00008702
Iteration 199/1000 | Loss: 0.00008702
Iteration 200/1000 | Loss: 0.00008702
Iteration 201/1000 | Loss: 0.00008702
Iteration 202/1000 | Loss: 0.00008702
Iteration 203/1000 | Loss: 0.00008702
Iteration 204/1000 | Loss: 0.00008702
Iteration 205/1000 | Loss: 0.00008702
Iteration 206/1000 | Loss: 0.00008695
Iteration 207/1000 | Loss: 0.00008694
Iteration 208/1000 | Loss: 0.00008694
Iteration 209/1000 | Loss: 0.00008694
Iteration 210/1000 | Loss: 0.00008694
Iteration 211/1000 | Loss: 0.00008694
Iteration 212/1000 | Loss: 0.00008694
Iteration 213/1000 | Loss: 0.00008694
Iteration 214/1000 | Loss: 0.00008693
Iteration 215/1000 | Loss: 0.00008693
Iteration 216/1000 | Loss: 0.00008692
Iteration 217/1000 | Loss: 0.00008692
Iteration 218/1000 | Loss: 0.00008692
Iteration 219/1000 | Loss: 0.00008692
Iteration 220/1000 | Loss: 0.00008692
Iteration 221/1000 | Loss: 0.00008691
Iteration 222/1000 | Loss: 0.00008691
Iteration 223/1000 | Loss: 0.00008691
Iteration 224/1000 | Loss: 0.00008691
Iteration 225/1000 | Loss: 0.00008691
Iteration 226/1000 | Loss: 0.00008691
Iteration 227/1000 | Loss: 0.00008691
Iteration 228/1000 | Loss: 0.00008691
Iteration 229/1000 | Loss: 0.00008691
Iteration 230/1000 | Loss: 0.00008690
Iteration 231/1000 | Loss: 0.00008690
Iteration 232/1000 | Loss: 0.00008690
Iteration 233/1000 | Loss: 0.00008690
Iteration 234/1000 | Loss: 0.00008690
Iteration 235/1000 | Loss: 0.00008690
Iteration 236/1000 | Loss: 0.00008690
Iteration 237/1000 | Loss: 0.00008690
Iteration 238/1000 | Loss: 0.00008690
Iteration 239/1000 | Loss: 0.00008690
Iteration 240/1000 | Loss: 0.00008690
Iteration 241/1000 | Loss: 0.00008690
Iteration 242/1000 | Loss: 0.00008690
Iteration 243/1000 | Loss: 0.00008690
Iteration 244/1000 | Loss: 0.00008689
Iteration 245/1000 | Loss: 0.00008689
Iteration 246/1000 | Loss: 0.00008689
Iteration 247/1000 | Loss: 0.00008688
Iteration 248/1000 | Loss: 0.00008688
Iteration 249/1000 | Loss: 0.00008688
Iteration 250/1000 | Loss: 0.00008688
Iteration 251/1000 | Loss: 0.00008688
Iteration 252/1000 | Loss: 0.00008688
Iteration 253/1000 | Loss: 0.00008688
Iteration 254/1000 | Loss: 0.00008688
Iteration 255/1000 | Loss: 0.00008688
Iteration 256/1000 | Loss: 0.00008687
Iteration 257/1000 | Loss: 0.00008687
Iteration 258/1000 | Loss: 0.00008687
Iteration 259/1000 | Loss: 0.00008687
Iteration 260/1000 | Loss: 0.00008687
Iteration 261/1000 | Loss: 0.00008687
Iteration 262/1000 | Loss: 0.00008687
Iteration 263/1000 | Loss: 0.00008687
Iteration 264/1000 | Loss: 0.00008687
Iteration 265/1000 | Loss: 0.00008687
Iteration 266/1000 | Loss: 0.00008687
Iteration 267/1000 | Loss: 0.00008687
Iteration 268/1000 | Loss: 0.00008687
Iteration 269/1000 | Loss: 0.00008687
Iteration 270/1000 | Loss: 0.00008686
Iteration 271/1000 | Loss: 0.00008686
Iteration 272/1000 | Loss: 0.00008686
Iteration 273/1000 | Loss: 0.00008686
Iteration 274/1000 | Loss: 0.00008686
Iteration 275/1000 | Loss: 0.00008686
Iteration 276/1000 | Loss: 0.00008686
Iteration 277/1000 | Loss: 0.00008686
Iteration 278/1000 | Loss: 0.00008686
Iteration 279/1000 | Loss: 0.00008686
Iteration 280/1000 | Loss: 0.00008686
Iteration 281/1000 | Loss: 0.00008686
Iteration 282/1000 | Loss: 0.00008686
Iteration 283/1000 | Loss: 0.00008686
Iteration 284/1000 | Loss: 0.00008686
Iteration 285/1000 | Loss: 0.00008686
Iteration 286/1000 | Loss: 0.00008686
Iteration 287/1000 | Loss: 0.00008686
Iteration 288/1000 | Loss: 0.00008685
Iteration 289/1000 | Loss: 0.00008685
Iteration 290/1000 | Loss: 0.00008685
Iteration 291/1000 | Loss: 0.00008685
Iteration 292/1000 | Loss: 0.00008685
Iteration 293/1000 | Loss: 0.00008685
Iteration 294/1000 | Loss: 0.00008685
Iteration 295/1000 | Loss: 0.00008685
Iteration 296/1000 | Loss: 0.00008685
Iteration 297/1000 | Loss: 0.00008685
Iteration 298/1000 | Loss: 0.00008685
Iteration 299/1000 | Loss: 0.00008685
Iteration 300/1000 | Loss: 0.00008685
Iteration 301/1000 | Loss: 0.00008685
Iteration 302/1000 | Loss: 0.00010523
Iteration 303/1000 | Loss: 0.00010475
Iteration 304/1000 | Loss: 0.00008911
Iteration 305/1000 | Loss: 0.00008688
Iteration 306/1000 | Loss: 0.00009366
Iteration 307/1000 | Loss: 0.00008688
Iteration 308/1000 | Loss: 0.00008685
Iteration 309/1000 | Loss: 0.00008685
Iteration 310/1000 | Loss: 0.00008685
Iteration 311/1000 | Loss: 0.00008685
Iteration 312/1000 | Loss: 0.00008685
Iteration 313/1000 | Loss: 0.00008685
Iteration 314/1000 | Loss: 0.00008685
Iteration 315/1000 | Loss: 0.00008685
Iteration 316/1000 | Loss: 0.00008685
Iteration 317/1000 | Loss: 0.00008685
Iteration 318/1000 | Loss: 0.00008685
Iteration 319/1000 | Loss: 0.00008684
Iteration 320/1000 | Loss: 0.00008684
Iteration 321/1000 | Loss: 0.00008684
Iteration 322/1000 | Loss: 0.00008684
Iteration 323/1000 | Loss: 0.00008684
Iteration 324/1000 | Loss: 0.00008684
Iteration 325/1000 | Loss: 0.00008683
Iteration 326/1000 | Loss: 0.00008683
Iteration 327/1000 | Loss: 0.00008683
Iteration 328/1000 | Loss: 0.00008683
Iteration 329/1000 | Loss: 0.00008683
Iteration 330/1000 | Loss: 0.00008683
Iteration 331/1000 | Loss: 0.00008682
Iteration 332/1000 | Loss: 0.00008682
Iteration 333/1000 | Loss: 0.00008682
Iteration 334/1000 | Loss: 0.00008682
Iteration 335/1000 | Loss: 0.00008682
Iteration 336/1000 | Loss: 0.00008682
Iteration 337/1000 | Loss: 0.00008681
Iteration 338/1000 | Loss: 0.00008681
Iteration 339/1000 | Loss: 0.00008681
Iteration 340/1000 | Loss: 0.00008681
Iteration 341/1000 | Loss: 0.00008681
Iteration 342/1000 | Loss: 0.00008681
Iteration 343/1000 | Loss: 0.00008681
Iteration 344/1000 | Loss: 0.00008681
Iteration 345/1000 | Loss: 0.00008681
Iteration 346/1000 | Loss: 0.00008681
Iteration 347/1000 | Loss: 0.00008681
Iteration 348/1000 | Loss: 0.00008680
Iteration 349/1000 | Loss: 0.00008680
Iteration 350/1000 | Loss: 0.00008680
Iteration 351/1000 | Loss: 0.00008680
Iteration 352/1000 | Loss: 0.00008680
Iteration 353/1000 | Loss: 0.00008680
Iteration 354/1000 | Loss: 0.00008680
Iteration 355/1000 | Loss: 0.00008680
Iteration 356/1000 | Loss: 0.00008680
Iteration 357/1000 | Loss: 0.00008680
Iteration 358/1000 | Loss: 0.00008680
Iteration 359/1000 | Loss: 0.00008680
Iteration 360/1000 | Loss: 0.00008680
Iteration 361/1000 | Loss: 0.00008680
Iteration 362/1000 | Loss: 0.00008680
Iteration 363/1000 | Loss: 0.00008680
Iteration 364/1000 | Loss: 0.00008680
Iteration 365/1000 | Loss: 0.00008680
Iteration 366/1000 | Loss: 0.00008680
Iteration 367/1000 | Loss: 0.00008680
Iteration 368/1000 | Loss: 0.00008680
Iteration 369/1000 | Loss: 0.00008680
Iteration 370/1000 | Loss: 0.00008680
Iteration 371/1000 | Loss: 0.00008680
Iteration 372/1000 | Loss: 0.00008680
Iteration 373/1000 | Loss: 0.00008680
Iteration 374/1000 | Loss: 0.00008680
Iteration 375/1000 | Loss: 0.00008680
Iteration 376/1000 | Loss: 0.00008680
Iteration 377/1000 | Loss: 0.00008680
Iteration 378/1000 | Loss: 0.00008680
Iteration 379/1000 | Loss: 0.00008680
Iteration 380/1000 | Loss: 0.00008680
Iteration 381/1000 | Loss: 0.00008680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [8.679696475155652e-05, 8.679696475155652e-05, 8.679696475155652e-05, 8.679696475155652e-05, 8.679696475155652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.679696475155652e-05

Optimization complete. Final v2v error: 5.4115753173828125 mm

Highest mean error: 12.647089004516602 mm for frame 110

Lowest mean error: 3.4825332164764404 mm for frame 9

Saving results

Total time: 319.0060341358185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416107
Iteration 2/25 | Loss: 0.00108014
Iteration 3/25 | Loss: 0.00100586
Iteration 4/25 | Loss: 0.00099524
Iteration 5/25 | Loss: 0.00099224
Iteration 6/25 | Loss: 0.00099186
Iteration 7/25 | Loss: 0.00099186
Iteration 8/25 | Loss: 0.00099186
Iteration 9/25 | Loss: 0.00099186
Iteration 10/25 | Loss: 0.00099186
Iteration 11/25 | Loss: 0.00099186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009918574942275882, 0.0009918574942275882, 0.0009918574942275882, 0.0009918574942275882, 0.0009918574942275882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009918574942275882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76977611
Iteration 2/25 | Loss: 0.00108209
Iteration 3/25 | Loss: 0.00108209
Iteration 4/25 | Loss: 0.00108209
Iteration 5/25 | Loss: 0.00108209
Iteration 6/25 | Loss: 0.00108209
Iteration 7/25 | Loss: 0.00108209
Iteration 8/25 | Loss: 0.00108209
Iteration 9/25 | Loss: 0.00108209
Iteration 10/25 | Loss: 0.00108208
Iteration 11/25 | Loss: 0.00108208
Iteration 12/25 | Loss: 0.00108208
Iteration 13/25 | Loss: 0.00108208
Iteration 14/25 | Loss: 0.00108208
Iteration 15/25 | Loss: 0.00108208
Iteration 16/25 | Loss: 0.00108208
Iteration 17/25 | Loss: 0.00108208
Iteration 18/25 | Loss: 0.00108208
Iteration 19/25 | Loss: 0.00108208
Iteration 20/25 | Loss: 0.00108208
Iteration 21/25 | Loss: 0.00108208
Iteration 22/25 | Loss: 0.00108208
Iteration 23/25 | Loss: 0.00108208
Iteration 24/25 | Loss: 0.00108208
Iteration 25/25 | Loss: 0.00108208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108208
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001602
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001321
Iteration 19/1000 | Loss: 0.00001310
Iteration 20/1000 | Loss: 0.00001308
Iteration 21/1000 | Loss: 0.00001308
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001307
Iteration 24/1000 | Loss: 0.00001306
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001303
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001302
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001298
Iteration 51/1000 | Loss: 0.00001298
Iteration 52/1000 | Loss: 0.00001298
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001297
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001295
Iteration 67/1000 | Loss: 0.00001295
Iteration 68/1000 | Loss: 0.00001294
Iteration 69/1000 | Loss: 0.00001294
Iteration 70/1000 | Loss: 0.00001293
Iteration 71/1000 | Loss: 0.00001293
Iteration 72/1000 | Loss: 0.00001293
Iteration 73/1000 | Loss: 0.00001293
Iteration 74/1000 | Loss: 0.00001293
Iteration 75/1000 | Loss: 0.00001292
Iteration 76/1000 | Loss: 0.00001292
Iteration 77/1000 | Loss: 0.00001291
Iteration 78/1000 | Loss: 0.00001291
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001289
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001287
Iteration 89/1000 | Loss: 0.00001287
Iteration 90/1000 | Loss: 0.00001287
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001277
Iteration 115/1000 | Loss: 0.00001277
Iteration 116/1000 | Loss: 0.00001276
Iteration 117/1000 | Loss: 0.00001276
Iteration 118/1000 | Loss: 0.00001276
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001274
Iteration 122/1000 | Loss: 0.00001274
Iteration 123/1000 | Loss: 0.00001274
Iteration 124/1000 | Loss: 0.00001274
Iteration 125/1000 | Loss: 0.00001274
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001273
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001272
Iteration 139/1000 | Loss: 0.00001272
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001272
Iteration 142/1000 | Loss: 0.00001272
Iteration 143/1000 | Loss: 0.00001272
Iteration 144/1000 | Loss: 0.00001272
Iteration 145/1000 | Loss: 0.00001272
Iteration 146/1000 | Loss: 0.00001272
Iteration 147/1000 | Loss: 0.00001272
Iteration 148/1000 | Loss: 0.00001272
Iteration 149/1000 | Loss: 0.00001271
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001271
Iteration 152/1000 | Loss: 0.00001271
Iteration 153/1000 | Loss: 0.00001271
Iteration 154/1000 | Loss: 0.00001271
Iteration 155/1000 | Loss: 0.00001270
Iteration 156/1000 | Loss: 0.00001270
Iteration 157/1000 | Loss: 0.00001270
Iteration 158/1000 | Loss: 0.00001270
Iteration 159/1000 | Loss: 0.00001270
Iteration 160/1000 | Loss: 0.00001270
Iteration 161/1000 | Loss: 0.00001270
Iteration 162/1000 | Loss: 0.00001270
Iteration 163/1000 | Loss: 0.00001270
Iteration 164/1000 | Loss: 0.00001270
Iteration 165/1000 | Loss: 0.00001270
Iteration 166/1000 | Loss: 0.00001270
Iteration 167/1000 | Loss: 0.00001269
Iteration 168/1000 | Loss: 0.00001269
Iteration 169/1000 | Loss: 0.00001269
Iteration 170/1000 | Loss: 0.00001269
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001269
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001269
Iteration 178/1000 | Loss: 0.00001269
Iteration 179/1000 | Loss: 0.00001269
Iteration 180/1000 | Loss: 0.00001269
Iteration 181/1000 | Loss: 0.00001269
Iteration 182/1000 | Loss: 0.00001269
Iteration 183/1000 | Loss: 0.00001269
Iteration 184/1000 | Loss: 0.00001269
Iteration 185/1000 | Loss: 0.00001268
Iteration 186/1000 | Loss: 0.00001268
Iteration 187/1000 | Loss: 0.00001268
Iteration 188/1000 | Loss: 0.00001268
Iteration 189/1000 | Loss: 0.00001268
Iteration 190/1000 | Loss: 0.00001268
Iteration 191/1000 | Loss: 0.00001268
Iteration 192/1000 | Loss: 0.00001268
Iteration 193/1000 | Loss: 0.00001268
Iteration 194/1000 | Loss: 0.00001268
Iteration 195/1000 | Loss: 0.00001268
Iteration 196/1000 | Loss: 0.00001268
Iteration 197/1000 | Loss: 0.00001268
Iteration 198/1000 | Loss: 0.00001268
Iteration 199/1000 | Loss: 0.00001268
Iteration 200/1000 | Loss: 0.00001268
Iteration 201/1000 | Loss: 0.00001268
Iteration 202/1000 | Loss: 0.00001268
Iteration 203/1000 | Loss: 0.00001267
Iteration 204/1000 | Loss: 0.00001267
Iteration 205/1000 | Loss: 0.00001267
Iteration 206/1000 | Loss: 0.00001267
Iteration 207/1000 | Loss: 0.00001267
Iteration 208/1000 | Loss: 0.00001267
Iteration 209/1000 | Loss: 0.00001267
Iteration 210/1000 | Loss: 0.00001267
Iteration 211/1000 | Loss: 0.00001267
Iteration 212/1000 | Loss: 0.00001267
Iteration 213/1000 | Loss: 0.00001267
Iteration 214/1000 | Loss: 0.00001267
Iteration 215/1000 | Loss: 0.00001267
Iteration 216/1000 | Loss: 0.00001267
Iteration 217/1000 | Loss: 0.00001267
Iteration 218/1000 | Loss: 0.00001267
Iteration 219/1000 | Loss: 0.00001267
Iteration 220/1000 | Loss: 0.00001267
Iteration 221/1000 | Loss: 0.00001267
Iteration 222/1000 | Loss: 0.00001267
Iteration 223/1000 | Loss: 0.00001266
Iteration 224/1000 | Loss: 0.00001266
Iteration 225/1000 | Loss: 0.00001266
Iteration 226/1000 | Loss: 0.00001266
Iteration 227/1000 | Loss: 0.00001266
Iteration 228/1000 | Loss: 0.00001266
Iteration 229/1000 | Loss: 0.00001266
Iteration 230/1000 | Loss: 0.00001266
Iteration 231/1000 | Loss: 0.00001266
Iteration 232/1000 | Loss: 0.00001266
Iteration 233/1000 | Loss: 0.00001266
Iteration 234/1000 | Loss: 0.00001266
Iteration 235/1000 | Loss: 0.00001266
Iteration 236/1000 | Loss: 0.00001266
Iteration 237/1000 | Loss: 0.00001266
Iteration 238/1000 | Loss: 0.00001266
Iteration 239/1000 | Loss: 0.00001266
Iteration 240/1000 | Loss: 0.00001266
Iteration 241/1000 | Loss: 0.00001266
Iteration 242/1000 | Loss: 0.00001266
Iteration 243/1000 | Loss: 0.00001266
Iteration 244/1000 | Loss: 0.00001266
Iteration 245/1000 | Loss: 0.00001266
Iteration 246/1000 | Loss: 0.00001266
Iteration 247/1000 | Loss: 0.00001266
Iteration 248/1000 | Loss: 0.00001266
Iteration 249/1000 | Loss: 0.00001266
Iteration 250/1000 | Loss: 0.00001266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.266438721359009e-05, 1.266438721359009e-05, 1.266438721359009e-05, 1.266438721359009e-05, 1.266438721359009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.266438721359009e-05

Optimization complete. Final v2v error: 2.97159481048584 mm

Highest mean error: 3.2838211059570312 mm for frame 90

Lowest mean error: 2.742494821548462 mm for frame 183

Saving results

Total time: 39.79923176765442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088444
Iteration 2/25 | Loss: 0.01088444
Iteration 3/25 | Loss: 0.00505218
Iteration 4/25 | Loss: 0.00341289
Iteration 5/25 | Loss: 0.00262169
Iteration 6/25 | Loss: 0.00233602
Iteration 7/25 | Loss: 0.00236843
Iteration 8/25 | Loss: 0.00221643
Iteration 9/25 | Loss: 0.00191641
Iteration 10/25 | Loss: 0.00168032
Iteration 11/25 | Loss: 0.00157216
Iteration 12/25 | Loss: 0.00154308
Iteration 13/25 | Loss: 0.00149945
Iteration 14/25 | Loss: 0.00145648
Iteration 15/25 | Loss: 0.00144517
Iteration 16/25 | Loss: 0.00142077
Iteration 17/25 | Loss: 0.00141007
Iteration 18/25 | Loss: 0.00136136
Iteration 19/25 | Loss: 0.00134360
Iteration 20/25 | Loss: 0.00133751
Iteration 21/25 | Loss: 0.00133506
Iteration 22/25 | Loss: 0.00131843
Iteration 23/25 | Loss: 0.00131771
Iteration 24/25 | Loss: 0.00131574
Iteration 25/25 | Loss: 0.00131555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55576771
Iteration 2/25 | Loss: 0.00118416
Iteration 3/25 | Loss: 0.00111989
Iteration 4/25 | Loss: 0.00111989
Iteration 5/25 | Loss: 0.00111989
Iteration 6/25 | Loss: 0.00111989
Iteration 7/25 | Loss: 0.00111989
Iteration 8/25 | Loss: 0.00111989
Iteration 9/25 | Loss: 0.00111989
Iteration 10/25 | Loss: 0.00111989
Iteration 11/25 | Loss: 0.00111989
Iteration 12/25 | Loss: 0.00111989
Iteration 13/25 | Loss: 0.00111989
Iteration 14/25 | Loss: 0.00111989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011198901338502765, 0.0011198901338502765, 0.0011198901338502765, 0.0011198901338502765, 0.0011198901338502765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011198901338502765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111989
Iteration 2/1000 | Loss: 0.00029868
Iteration 3/1000 | Loss: 0.00016471
Iteration 4/1000 | Loss: 0.00041417
Iteration 5/1000 | Loss: 0.00074804
Iteration 6/1000 | Loss: 0.00031539
Iteration 7/1000 | Loss: 0.00144366
Iteration 8/1000 | Loss: 0.00039430
Iteration 9/1000 | Loss: 0.00146328
Iteration 10/1000 | Loss: 0.00041000
Iteration 11/1000 | Loss: 0.00083846
Iteration 12/1000 | Loss: 0.00030308
Iteration 13/1000 | Loss: 0.00057575
Iteration 14/1000 | Loss: 0.00032955
Iteration 15/1000 | Loss: 0.00056653
Iteration 16/1000 | Loss: 0.00043704
Iteration 17/1000 | Loss: 0.00009023
Iteration 18/1000 | Loss: 0.00067840
Iteration 19/1000 | Loss: 0.00049465
Iteration 20/1000 | Loss: 0.00019595
Iteration 21/1000 | Loss: 0.00006534
Iteration 22/1000 | Loss: 0.00005460
Iteration 23/1000 | Loss: 0.00004934
Iteration 24/1000 | Loss: 0.00004573
Iteration 25/1000 | Loss: 0.00004280
Iteration 26/1000 | Loss: 0.00004143
Iteration 27/1000 | Loss: 0.00004049
Iteration 28/1000 | Loss: 0.00003989
Iteration 29/1000 | Loss: 0.00003914
Iteration 30/1000 | Loss: 0.00003877
Iteration 31/1000 | Loss: 0.00003828
Iteration 32/1000 | Loss: 0.00003798
Iteration 33/1000 | Loss: 0.00022541
Iteration 34/1000 | Loss: 0.00005017
Iteration 35/1000 | Loss: 0.00006797
Iteration 36/1000 | Loss: 0.00006105
Iteration 37/1000 | Loss: 0.00003711
Iteration 38/1000 | Loss: 0.00003533
Iteration 39/1000 | Loss: 0.00018890
Iteration 40/1000 | Loss: 0.00003742
Iteration 41/1000 | Loss: 0.00003398
Iteration 42/1000 | Loss: 0.00003308
Iteration 43/1000 | Loss: 0.00003264
Iteration 44/1000 | Loss: 0.00003295
Iteration 45/1000 | Loss: 0.00003265
Iteration 46/1000 | Loss: 0.00003240
Iteration 47/1000 | Loss: 0.00003226
Iteration 48/1000 | Loss: 0.00003226
Iteration 49/1000 | Loss: 0.00003225
Iteration 50/1000 | Loss: 0.00003426
Iteration 51/1000 | Loss: 0.00003425
Iteration 52/1000 | Loss: 0.00003425
Iteration 53/1000 | Loss: 0.00003425
Iteration 54/1000 | Loss: 0.00003337
Iteration 55/1000 | Loss: 0.00003289
Iteration 56/1000 | Loss: 0.00003384
Iteration 57/1000 | Loss: 0.00003256
Iteration 58/1000 | Loss: 0.00003212
Iteration 59/1000 | Loss: 0.00003325
Iteration 60/1000 | Loss: 0.00003277
Iteration 61/1000 | Loss: 0.00003213
Iteration 62/1000 | Loss: 0.00003213
Iteration 63/1000 | Loss: 0.00003213
Iteration 64/1000 | Loss: 0.00003213
Iteration 65/1000 | Loss: 0.00003328
Iteration 66/1000 | Loss: 0.00003309
Iteration 67/1000 | Loss: 0.00003212
Iteration 68/1000 | Loss: 0.00003313
Iteration 69/1000 | Loss: 0.00003344
Iteration 70/1000 | Loss: 0.00003325
Iteration 71/1000 | Loss: 0.00003308
Iteration 72/1000 | Loss: 0.00003342
Iteration 73/1000 | Loss: 0.00003327
Iteration 74/1000 | Loss: 0.00003312
Iteration 75/1000 | Loss: 0.00003340
Iteration 76/1000 | Loss: 0.00003320
Iteration 77/1000 | Loss: 0.00003325
Iteration 78/1000 | Loss: 0.00003314
Iteration 79/1000 | Loss: 0.00003354
Iteration 80/1000 | Loss: 0.00003317
Iteration 81/1000 | Loss: 0.00003336
Iteration 82/1000 | Loss: 0.00003354
Iteration 83/1000 | Loss: 0.00003300
Iteration 84/1000 | Loss: 0.00003315
Iteration 85/1000 | Loss: 0.00003315
Iteration 86/1000 | Loss: 0.00003276
Iteration 87/1000 | Loss: 0.00003248
Iteration 88/1000 | Loss: 0.00003253
Iteration 89/1000 | Loss: 0.00003249
Iteration 90/1000 | Loss: 0.00003289
Iteration 91/1000 | Loss: 0.00003297
Iteration 92/1000 | Loss: 0.00003291
Iteration 93/1000 | Loss: 0.00003289
Iteration 94/1000 | Loss: 0.00003299
Iteration 95/1000 | Loss: 0.00003329
Iteration 96/1000 | Loss: 0.00003292
Iteration 97/1000 | Loss: 0.00003335
Iteration 98/1000 | Loss: 0.00003313
Iteration 99/1000 | Loss: 0.00003310
Iteration 100/1000 | Loss: 0.00003284
Iteration 101/1000 | Loss: 0.00003309
Iteration 102/1000 | Loss: 0.00003325
Iteration 103/1000 | Loss: 0.00003325
Iteration 104/1000 | Loss: 0.00003301
Iteration 105/1000 | Loss: 0.00003337
Iteration 106/1000 | Loss: 0.00003315
Iteration 107/1000 | Loss: 0.00003344
Iteration 108/1000 | Loss: 0.00003328
Iteration 109/1000 | Loss: 0.00003327
Iteration 110/1000 | Loss: 0.00003325
Iteration 111/1000 | Loss: 0.00003302
Iteration 112/1000 | Loss: 0.00003363
Iteration 113/1000 | Loss: 0.00003283
Iteration 114/1000 | Loss: 0.00003309
Iteration 115/1000 | Loss: 0.00003323
Iteration 116/1000 | Loss: 0.00003311
Iteration 117/1000 | Loss: 0.00003311
Iteration 118/1000 | Loss: 0.00003300
Iteration 119/1000 | Loss: 0.00003289
Iteration 120/1000 | Loss: 0.00003310
Iteration 121/1000 | Loss: 0.00003291
Iteration 122/1000 | Loss: 0.00003336
Iteration 123/1000 | Loss: 0.00003291
Iteration 124/1000 | Loss: 0.00003336
Iteration 125/1000 | Loss: 0.00003320
Iteration 126/1000 | Loss: 0.00003319
Iteration 127/1000 | Loss: 0.00003319
Iteration 128/1000 | Loss: 0.00003331
Iteration 129/1000 | Loss: 0.00003288
Iteration 130/1000 | Loss: 0.00003333
Iteration 131/1000 | Loss: 0.00003316
Iteration 132/1000 | Loss: 0.00003322
Iteration 133/1000 | Loss: 0.00003340
Iteration 134/1000 | Loss: 0.00003317
Iteration 135/1000 | Loss: 0.00003309
Iteration 136/1000 | Loss: 0.00003355
Iteration 137/1000 | Loss: 0.00003355
Iteration 138/1000 | Loss: 0.00003355
Iteration 139/1000 | Loss: 0.00003323
Iteration 140/1000 | Loss: 0.00003305
Iteration 141/1000 | Loss: 0.00003347
Iteration 142/1000 | Loss: 0.00003313
Iteration 143/1000 | Loss: 0.00003319
Iteration 144/1000 | Loss: 0.00003338
Iteration 145/1000 | Loss: 0.00003329
Iteration 146/1000 | Loss: 0.00003310
Iteration 147/1000 | Loss: 0.00003319
Iteration 148/1000 | Loss: 0.00003303
Iteration 149/1000 | Loss: 0.00003335
Iteration 150/1000 | Loss: 0.00003324
Iteration 151/1000 | Loss: 0.00003311
Iteration 152/1000 | Loss: 0.00003336
Iteration 153/1000 | Loss: 0.00003302
Iteration 154/1000 | Loss: 0.00003338
Iteration 155/1000 | Loss: 0.00003328
Iteration 156/1000 | Loss: 0.00003339
Iteration 157/1000 | Loss: 0.00003325
Iteration 158/1000 | Loss: 0.00003317
Iteration 159/1000 | Loss: 0.00003309
Iteration 160/1000 | Loss: 0.00003345
Iteration 161/1000 | Loss: 0.00003322
Iteration 162/1000 | Loss: 0.00003337
Iteration 163/1000 | Loss: 0.00003306
Iteration 164/1000 | Loss: 0.00003315
Iteration 165/1000 | Loss: 0.00003285
Iteration 166/1000 | Loss: 0.00003315
Iteration 167/1000 | Loss: 0.00003324
Iteration 168/1000 | Loss: 0.00003317
Iteration 169/1000 | Loss: 0.00003294
Iteration 170/1000 | Loss: 0.00003342
Iteration 171/1000 | Loss: 0.00003301
Iteration 172/1000 | Loss: 0.00003346
Iteration 173/1000 | Loss: 0.00003296
Iteration 174/1000 | Loss: 0.00003354
Iteration 175/1000 | Loss: 0.00003293
Iteration 176/1000 | Loss: 0.00003322
Iteration 177/1000 | Loss: 0.00003312
Iteration 178/1000 | Loss: 0.00003315
Iteration 179/1000 | Loss: 0.00003312
Iteration 180/1000 | Loss: 0.00003316
Iteration 181/1000 | Loss: 0.00003304
Iteration 182/1000 | Loss: 0.00003327
Iteration 183/1000 | Loss: 0.00003305
Iteration 184/1000 | Loss: 0.00003312
Iteration 185/1000 | Loss: 0.00003295
Iteration 186/1000 | Loss: 0.00003325
Iteration 187/1000 | Loss: 0.00003312
Iteration 188/1000 | Loss: 0.00003318
Iteration 189/1000 | Loss: 0.00003302
Iteration 190/1000 | Loss: 0.00003322
Iteration 191/1000 | Loss: 0.00003309
Iteration 192/1000 | Loss: 0.00003310
Iteration 193/1000 | Loss: 0.00003304
Iteration 194/1000 | Loss: 0.00003304
Iteration 195/1000 | Loss: 0.00003265
Iteration 196/1000 | Loss: 0.00003351
Iteration 197/1000 | Loss: 0.00003294
Iteration 198/1000 | Loss: 0.00003325
Iteration 199/1000 | Loss: 0.00003303
Iteration 200/1000 | Loss: 0.00003328
Iteration 201/1000 | Loss: 0.00003284
Iteration 202/1000 | Loss: 0.00003340
Iteration 203/1000 | Loss: 0.00003294
Iteration 204/1000 | Loss: 0.00003319
Iteration 205/1000 | Loss: 0.00003311
Iteration 206/1000 | Loss: 0.00003315
Iteration 207/1000 | Loss: 0.00003309
Iteration 208/1000 | Loss: 0.00003309
Iteration 209/1000 | Loss: 0.00003288
Iteration 210/1000 | Loss: 0.00003287
Iteration 211/1000 | Loss: 0.00003230
Iteration 212/1000 | Loss: 0.00003203
Iteration 213/1000 | Loss: 0.00003202
Iteration 214/1000 | Loss: 0.00003333
Iteration 215/1000 | Loss: 0.00003300
Iteration 216/1000 | Loss: 0.00003304
Iteration 217/1000 | Loss: 0.00003314
Iteration 218/1000 | Loss: 0.00003297
Iteration 219/1000 | Loss: 0.00003286
Iteration 220/1000 | Loss: 0.00003323
Iteration 221/1000 | Loss: 0.00003281
Iteration 222/1000 | Loss: 0.00003329
Iteration 223/1000 | Loss: 0.00003314
Iteration 224/1000 | Loss: 0.00003318
Iteration 225/1000 | Loss: 0.00003291
Iteration 226/1000 | Loss: 0.00003334
Iteration 227/1000 | Loss: 0.00003298
Iteration 228/1000 | Loss: 0.00003298
Iteration 229/1000 | Loss: 0.00003312
Iteration 230/1000 | Loss: 0.00003263
Iteration 231/1000 | Loss: 0.00003201
Iteration 232/1000 | Loss: 0.00003309
Iteration 233/1000 | Loss: 0.00003277
Iteration 234/1000 | Loss: 0.00003321
Iteration 235/1000 | Loss: 0.00003320
Iteration 236/1000 | Loss: 0.00003313
Iteration 237/1000 | Loss: 0.00003199
Iteration 238/1000 | Loss: 0.00003318
Iteration 239/1000 | Loss: 0.00003287
Iteration 240/1000 | Loss: 0.00003307
Iteration 241/1000 | Loss: 0.00003299
Iteration 242/1000 | Loss: 0.00003333
Iteration 243/1000 | Loss: 0.00003292
Iteration 244/1000 | Loss: 0.00003296
Iteration 245/1000 | Loss: 0.00003334
Iteration 246/1000 | Loss: 0.00003298
Iteration 247/1000 | Loss: 0.00003298
Iteration 248/1000 | Loss: 0.00003333
Iteration 249/1000 | Loss: 0.00003333
Iteration 250/1000 | Loss: 0.00003333
Iteration 251/1000 | Loss: 0.00003333
Iteration 252/1000 | Loss: 0.00003333
Iteration 253/1000 | Loss: 0.00003332
Iteration 254/1000 | Loss: 0.00003332
Iteration 255/1000 | Loss: 0.00003332
Iteration 256/1000 | Loss: 0.00003332
Iteration 257/1000 | Loss: 0.00003332
Iteration 258/1000 | Loss: 0.00003332
Iteration 259/1000 | Loss: 0.00003332
Iteration 260/1000 | Loss: 0.00003332
Iteration 261/1000 | Loss: 0.00003283
Iteration 262/1000 | Loss: 0.00003241
Iteration 263/1000 | Loss: 0.00003309
Iteration 264/1000 | Loss: 0.00003339
Iteration 265/1000 | Loss: 0.00003322
Iteration 266/1000 | Loss: 0.00003329
Iteration 267/1000 | Loss: 0.00003299
Iteration 268/1000 | Loss: 0.00003367
Iteration 269/1000 | Loss: 0.00003302
Iteration 270/1000 | Loss: 0.00003350
Iteration 271/1000 | Loss: 0.00003322
Iteration 272/1000 | Loss: 0.00003336
Iteration 273/1000 | Loss: 0.00003329
Iteration 274/1000 | Loss: 0.00003300
Iteration 275/1000 | Loss: 0.00003328
Iteration 276/1000 | Loss: 0.00003328
Iteration 277/1000 | Loss: 0.00003337
Iteration 278/1000 | Loss: 0.00003260
Iteration 279/1000 | Loss: 0.00003310
Iteration 280/1000 | Loss: 0.00003293
Iteration 281/1000 | Loss: 0.00003307
Iteration 282/1000 | Loss: 0.00003304
Iteration 283/1000 | Loss: 0.00003300
Iteration 284/1000 | Loss: 0.00003304
Iteration 285/1000 | Loss: 0.00003334
Iteration 286/1000 | Loss: 0.00003297
Iteration 287/1000 | Loss: 0.00003336
Iteration 288/1000 | Loss: 0.00003318
Iteration 289/1000 | Loss: 0.00003329
Iteration 290/1000 | Loss: 0.00003293
Iteration 291/1000 | Loss: 0.00003363
Iteration 292/1000 | Loss: 0.00003303
Iteration 293/1000 | Loss: 0.00003325
Iteration 294/1000 | Loss: 0.00003323
Iteration 295/1000 | Loss: 0.00003337
Iteration 296/1000 | Loss: 0.00003330
Iteration 297/1000 | Loss: 0.00003333
Iteration 298/1000 | Loss: 0.00003327
Iteration 299/1000 | Loss: 0.00003325
Iteration 300/1000 | Loss: 0.00003290
Iteration 301/1000 | Loss: 0.00003377
Iteration 302/1000 | Loss: 0.00003309
Iteration 303/1000 | Loss: 0.00003337
Iteration 304/1000 | Loss: 0.00003346
Iteration 305/1000 | Loss: 0.00003335
Iteration 306/1000 | Loss: 0.00003347
Iteration 307/1000 | Loss: 0.00003321
Iteration 308/1000 | Loss: 0.00003308
Iteration 309/1000 | Loss: 0.00003367
Iteration 310/1000 | Loss: 0.00003314
Iteration 311/1000 | Loss: 0.00003320
Iteration 312/1000 | Loss: 0.00003332
Iteration 313/1000 | Loss: 0.00003337
Iteration 314/1000 | Loss: 0.00003317
Iteration 315/1000 | Loss: 0.00003364
Iteration 316/1000 | Loss: 0.00003314
Iteration 317/1000 | Loss: 0.00003342
Iteration 318/1000 | Loss: 0.00003319
Iteration 319/1000 | Loss: 0.00003319
Iteration 320/1000 | Loss: 0.00003324
Iteration 321/1000 | Loss: 0.00003297
Iteration 322/1000 | Loss: 0.00003339
Iteration 323/1000 | Loss: 0.00003291
Iteration 324/1000 | Loss: 0.00003320
Iteration 325/1000 | Loss: 0.00003317
Iteration 326/1000 | Loss: 0.00003302
Iteration 327/1000 | Loss: 0.00003340
Iteration 328/1000 | Loss: 0.00003313
Iteration 329/1000 | Loss: 0.00003308
Iteration 330/1000 | Loss: 0.00003355
Iteration 331/1000 | Loss: 0.00003307
Iteration 332/1000 | Loss: 0.00003330
Iteration 333/1000 | Loss: 0.00003310
Iteration 334/1000 | Loss: 0.00003363
Iteration 335/1000 | Loss: 0.00003304
Iteration 336/1000 | Loss: 0.00003338
Iteration 337/1000 | Loss: 0.00003360
Iteration 338/1000 | Loss: 0.00003317
Iteration 339/1000 | Loss: 0.00003312
Iteration 340/1000 | Loss: 0.00003345
Iteration 341/1000 | Loss: 0.00003320
Iteration 342/1000 | Loss: 0.00003342
Iteration 343/1000 | Loss: 0.00003303
Iteration 344/1000 | Loss: 0.00003318
Iteration 345/1000 | Loss: 0.00003315
Iteration 346/1000 | Loss: 0.00003315
Iteration 347/1000 | Loss: 0.00003316
Iteration 348/1000 | Loss: 0.00003316
Iteration 349/1000 | Loss: 0.00003238
Iteration 350/1000 | Loss: 0.00003280
Iteration 351/1000 | Loss: 0.00003277
Iteration 352/1000 | Loss: 0.00003308
Iteration 353/1000 | Loss: 0.00003258
Iteration 354/1000 | Loss: 0.00003277
Iteration 355/1000 | Loss: 0.00003259
Iteration 356/1000 | Loss: 0.00003289
Iteration 357/1000 | Loss: 0.00003297
Iteration 358/1000 | Loss: 0.00003324
Iteration 359/1000 | Loss: 0.00003287
Iteration 360/1000 | Loss: 0.00003322
Iteration 361/1000 | Loss: 0.00003278
Iteration 362/1000 | Loss: 0.00003318
Iteration 363/1000 | Loss: 0.00003294
Iteration 364/1000 | Loss: 0.00003344
Iteration 365/1000 | Loss: 0.00003274
Iteration 366/1000 | Loss: 0.00003322
Iteration 367/1000 | Loss: 0.00003292
Iteration 368/1000 | Loss: 0.00003307
Iteration 369/1000 | Loss: 0.00003296
Iteration 370/1000 | Loss: 0.00003325
Iteration 371/1000 | Loss: 0.00003291
Iteration 372/1000 | Loss: 0.00003314
Iteration 373/1000 | Loss: 0.00003307
Iteration 374/1000 | Loss: 0.00003307
Iteration 375/1000 | Loss: 0.00003303
Iteration 376/1000 | Loss: 0.00003285
Iteration 377/1000 | Loss: 0.00003310
Iteration 378/1000 | Loss: 0.00003300
Iteration 379/1000 | Loss: 0.00003309
Iteration 380/1000 | Loss: 0.00003285
Iteration 381/1000 | Loss: 0.00003311
Iteration 382/1000 | Loss: 0.00003314
Iteration 383/1000 | Loss: 0.00003313
Iteration 384/1000 | Loss: 0.00003319
Iteration 385/1000 | Loss: 0.00003253
Iteration 386/1000 | Loss: 0.00003310
Iteration 387/1000 | Loss: 0.00003277
Iteration 388/1000 | Loss: 0.00003292
Iteration 389/1000 | Loss: 0.00003301
Iteration 390/1000 | Loss: 0.00003233
Iteration 391/1000 | Loss: 0.00003255
Iteration 392/1000 | Loss: 0.00003265
Iteration 393/1000 | Loss: 0.00003290
Iteration 394/1000 | Loss: 0.00003298
Iteration 395/1000 | Loss: 0.00003296
Iteration 396/1000 | Loss: 0.00003291
Iteration 397/1000 | Loss: 0.00003301
Iteration 398/1000 | Loss: 0.00003285
Iteration 399/1000 | Loss: 0.00003299
Iteration 400/1000 | Loss: 0.00003285
Iteration 401/1000 | Loss: 0.00003276
Iteration 402/1000 | Loss: 0.00003318
Iteration 403/1000 | Loss: 0.00003260
Iteration 404/1000 | Loss: 0.00003271
Iteration 405/1000 | Loss: 0.00003287
Iteration 406/1000 | Loss: 0.00003275
Iteration 407/1000 | Loss: 0.00003303
Iteration 408/1000 | Loss: 0.00003295
Iteration 409/1000 | Loss: 0.00003294
Iteration 410/1000 | Loss: 0.00003269
Iteration 411/1000 | Loss: 0.00003269
Iteration 412/1000 | Loss: 0.00003257
Iteration 413/1000 | Loss: 0.00003300
Iteration 414/1000 | Loss: 0.00003265
Iteration 415/1000 | Loss: 0.00003264
Iteration 416/1000 | Loss: 0.00003276
Iteration 417/1000 | Loss: 0.00003296
Iteration 418/1000 | Loss: 0.00003268
Iteration 419/1000 | Loss: 0.00003277
Iteration 420/1000 | Loss: 0.00003300
Iteration 421/1000 | Loss: 0.00003296
Iteration 422/1000 | Loss: 0.00003281
Iteration 423/1000 | Loss: 0.00003283
Iteration 424/1000 | Loss: 0.00003270
Iteration 425/1000 | Loss: 0.00003302
Iteration 426/1000 | Loss: 0.00003296
Iteration 427/1000 | Loss: 0.00003292
Iteration 428/1000 | Loss: 0.00003282
Iteration 429/1000 | Loss: 0.00003279
Iteration 430/1000 | Loss: 0.00003307
Iteration 431/1000 | Loss: 0.00003296
Iteration 432/1000 | Loss: 0.00003287
Iteration 433/1000 | Loss: 0.00003287
Iteration 434/1000 | Loss: 0.00003247
Iteration 435/1000 | Loss: 0.00003274
Iteration 436/1000 | Loss: 0.00003281
Iteration 437/1000 | Loss: 0.00003286
Iteration 438/1000 | Loss: 0.00003276
Iteration 439/1000 | Loss: 0.00003271
Iteration 440/1000 | Loss: 0.00003271
Iteration 441/1000 | Loss: 0.00003295
Iteration 442/1000 | Loss: 0.00003279
Iteration 443/1000 | Loss: 0.00003274
Iteration 444/1000 | Loss: 0.00003286
Iteration 445/1000 | Loss: 0.00003294
Iteration 446/1000 | Loss: 0.00003280
Iteration 447/1000 | Loss: 0.00003268
Iteration 448/1000 | Loss: 0.00003289
Iteration 449/1000 | Loss: 0.00003272
Iteration 450/1000 | Loss: 0.00003284
Iteration 451/1000 | Loss: 0.00003321
Iteration 452/1000 | Loss: 0.00003282
Iteration 453/1000 | Loss: 0.00003286
Iteration 454/1000 | Loss: 0.00003304
Iteration 455/1000 | Loss: 0.00003283
Iteration 456/1000 | Loss: 0.00003304
Iteration 457/1000 | Loss: 0.00003272
Iteration 458/1000 | Loss: 0.00003279
Iteration 459/1000 | Loss: 0.00003287
Iteration 460/1000 | Loss: 0.00003274
Iteration 461/1000 | Loss: 0.00003278
Iteration 462/1000 | Loss: 0.00003298
Iteration 463/1000 | Loss: 0.00003298
Iteration 464/1000 | Loss: 0.00003268
Iteration 465/1000 | Loss: 0.00003213
Iteration 466/1000 | Loss: 0.00003218
Iteration 467/1000 | Loss: 0.00003247
Iteration 468/1000 | Loss: 0.00003255
Iteration 469/1000 | Loss: 0.00003251
Iteration 470/1000 | Loss: 0.00003261
Iteration 471/1000 | Loss: 0.00003250
Iteration 472/1000 | Loss: 0.00003246
Iteration 473/1000 | Loss: 0.00003281
Iteration 474/1000 | Loss: 0.00003254
Iteration 475/1000 | Loss: 0.00003255
Iteration 476/1000 | Loss: 0.00003277
Iteration 477/1000 | Loss: 0.00003271
Iteration 478/1000 | Loss: 0.00003260
Iteration 479/1000 | Loss: 0.00003221
Iteration 480/1000 | Loss: 0.00003249
Iteration 481/1000 | Loss: 0.00003220
Iteration 482/1000 | Loss: 0.00003274
Iteration 483/1000 | Loss: 0.00003285
Iteration 484/1000 | Loss: 0.00003275
Iteration 485/1000 | Loss: 0.00003291
Iteration 486/1000 | Loss: 0.00003293
Iteration 487/1000 | Loss: 0.00003295
Iteration 488/1000 | Loss: 0.00003297
Iteration 489/1000 | Loss: 0.00003315
Iteration 490/1000 | Loss: 0.00003272
Iteration 491/1000 | Loss: 0.00003282
Iteration 492/1000 | Loss: 0.00003306
Iteration 493/1000 | Loss: 0.00003306
Iteration 494/1000 | Loss: 0.00003303
Iteration 495/1000 | Loss: 0.00003271
Iteration 496/1000 | Loss: 0.00003297
Iteration 497/1000 | Loss: 0.00003272
Iteration 498/1000 | Loss: 0.00003284
Iteration 499/1000 | Loss: 0.00003307
Iteration 500/1000 | Loss: 0.00003270
Iteration 501/1000 | Loss: 0.00003281
Iteration 502/1000 | Loss: 0.00003293
Iteration 503/1000 | Loss: 0.00003278
Iteration 504/1000 | Loss: 0.00003274
Iteration 505/1000 | Loss: 0.00003284
Iteration 506/1000 | Loss: 0.00003314
Iteration 507/1000 | Loss: 0.00003278
Iteration 508/1000 | Loss: 0.00003282
Iteration 509/1000 | Loss: 0.00003292
Iteration 510/1000 | Loss: 0.00003266
Iteration 511/1000 | Loss: 0.00003298
Iteration 512/1000 | Loss: 0.00003297
Iteration 513/1000 | Loss: 0.00003297
Iteration 514/1000 | Loss: 0.00003297
Iteration 515/1000 | Loss: 0.00003297
Iteration 516/1000 | Loss: 0.00003297
Iteration 517/1000 | Loss: 0.00003297
Iteration 518/1000 | Loss: 0.00003297
Iteration 519/1000 | Loss: 0.00003297
Iteration 520/1000 | Loss: 0.00003297
Iteration 521/1000 | Loss: 0.00003297
Iteration 522/1000 | Loss: 0.00003297
Iteration 523/1000 | Loss: 0.00003297
Iteration 524/1000 | Loss: 0.00003297
Iteration 525/1000 | Loss: 0.00003296
Iteration 526/1000 | Loss: 0.00003242
Iteration 527/1000 | Loss: 0.00003207
Iteration 528/1000 | Loss: 0.00003210
Iteration 529/1000 | Loss: 0.00003258
Iteration 530/1000 | Loss: 0.00003243
Iteration 531/1000 | Loss: 0.00003273
Iteration 532/1000 | Loss: 0.00003280
Iteration 533/1000 | Loss: 0.00003259
Iteration 534/1000 | Loss: 0.00003256
Iteration 535/1000 | Loss: 0.00003279
Iteration 536/1000 | Loss: 0.00003267
Iteration 537/1000 | Loss: 0.00003268
Iteration 538/1000 | Loss: 0.00003262
Iteration 539/1000 | Loss: 0.00003264
Iteration 540/1000 | Loss: 0.00003250
Iteration 541/1000 | Loss: 0.00003279
Iteration 542/1000 | Loss: 0.00003289
Iteration 543/1000 | Loss: 0.00003253
Iteration 544/1000 | Loss: 0.00003239
Iteration 545/1000 | Loss: 0.00003242
Iteration 546/1000 | Loss: 0.00003230
Iteration 547/1000 | Loss: 0.00003252
Iteration 548/1000 | Loss: 0.00003248
Iteration 549/1000 | Loss: 0.00003288
Iteration 550/1000 | Loss: 0.00003243
Iteration 551/1000 | Loss: 0.00003270
Iteration 552/1000 | Loss: 0.00003232
Iteration 553/1000 | Loss: 0.00003235
Iteration 554/1000 | Loss: 0.00003258
Iteration 555/1000 | Loss: 0.00003250
Iteration 556/1000 | Loss: 0.00003282
Iteration 557/1000 | Loss: 0.00003233
Iteration 558/1000 | Loss: 0.00003254
Iteration 559/1000 | Loss: 0.00003278
Iteration 560/1000 | Loss: 0.00003256
Iteration 561/1000 | Loss: 0.00003236
Iteration 562/1000 | Loss: 0.00003274
Iteration 563/1000 | Loss: 0.00003241
Iteration 564/1000 | Loss: 0.00003298
Iteration 565/1000 | Loss: 0.00003237
Iteration 566/1000 | Loss: 0.00003255
Iteration 567/1000 | Loss: 0.00003267
Iteration 568/1000 | Loss: 0.00003251
Iteration 569/1000 | Loss: 0.00003240
Iteration 570/1000 | Loss: 0.00003262
Iteration 571/1000 | Loss: 0.00003234
Iteration 572/1000 | Loss: 0.00003285
Iteration 573/1000 | Loss: 0.00003226
Iteration 574/1000 | Loss: 0.00003260
Iteration 575/1000 | Loss: 0.00003256
Iteration 576/1000 | Loss: 0.00003255
Iteration 577/1000 | Loss: 0.00003251
Iteration 578/1000 | Loss: 0.00003260
Iteration 579/1000 | Loss: 0.00003242
Iteration 580/1000 | Loss: 0.00003230
Iteration 581/1000 | Loss: 0.00003255
Iteration 582/1000 | Loss: 0.00003267
Iteration 583/1000 | Loss: 0.00003242
Iteration 584/1000 | Loss: 0.00003252
Iteration 585/1000 | Loss: 0.00003258
Iteration 586/1000 | Loss: 0.00003235
Iteration 587/1000 | Loss: 0.00003273
Iteration 588/1000 | Loss: 0.00003230
Iteration 589/1000 | Loss: 0.00003269
Iteration 590/1000 | Loss: 0.00003252
Iteration 591/1000 | Loss: 0.00003252
Iteration 592/1000 | Loss: 0.00003252
Iteration 593/1000 | Loss: 0.00003252
Iteration 594/1000 | Loss: 0.00003252
Iteration 595/1000 | Loss: 0.00003252
Iteration 596/1000 | Loss: 0.00003252
Iteration 597/1000 | Loss: 0.00003252
Iteration 598/1000 | Loss: 0.00003252
Iteration 599/1000 | Loss: 0.00003252
Iteration 600/1000 | Loss: 0.00003252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 600. Stopping optimization.
Last 5 losses: [3.251953239669092e-05, 3.251953239669092e-05, 3.251953239669092e-05, 3.251953239669092e-05, 3.251953239669092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.251953239669092e-05

Optimization complete. Final v2v error: 4.482081413269043 mm

Highest mean error: 17.026596069335938 mm for frame 65

Lowest mean error: 3.8229238986968994 mm for frame 58

Saving results

Total time: 619.0702614784241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067475
Iteration 2/25 | Loss: 0.00160543
Iteration 3/25 | Loss: 0.00122551
Iteration 4/25 | Loss: 0.00119835
Iteration 5/25 | Loss: 0.00119053
Iteration 6/25 | Loss: 0.00118803
Iteration 7/25 | Loss: 0.00118793
Iteration 8/25 | Loss: 0.00118793
Iteration 9/25 | Loss: 0.00118793
Iteration 10/25 | Loss: 0.00118793
Iteration 11/25 | Loss: 0.00118793
Iteration 12/25 | Loss: 0.00118793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011879345402121544, 0.0011879345402121544, 0.0011879345402121544, 0.0011879345402121544, 0.0011879345402121544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011879345402121544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91041315
Iteration 2/25 | Loss: 0.00106672
Iteration 3/25 | Loss: 0.00106670
Iteration 4/25 | Loss: 0.00106670
Iteration 5/25 | Loss: 0.00106670
Iteration 6/25 | Loss: 0.00106670
Iteration 7/25 | Loss: 0.00106670
Iteration 8/25 | Loss: 0.00106670
Iteration 9/25 | Loss: 0.00106670
Iteration 10/25 | Loss: 0.00106670
Iteration 11/25 | Loss: 0.00106670
Iteration 12/25 | Loss: 0.00106670
Iteration 13/25 | Loss: 0.00106670
Iteration 14/25 | Loss: 0.00106670
Iteration 15/25 | Loss: 0.00106670
Iteration 16/25 | Loss: 0.00106670
Iteration 17/25 | Loss: 0.00106670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010666974121704698, 0.0010666974121704698, 0.0010666974121704698, 0.0010666974121704698, 0.0010666974121704698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010666974121704698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106670
Iteration 2/1000 | Loss: 0.00008923
Iteration 3/1000 | Loss: 0.00005699
Iteration 4/1000 | Loss: 0.00004313
Iteration 5/1000 | Loss: 0.00003941
Iteration 6/1000 | Loss: 0.00003704
Iteration 7/1000 | Loss: 0.00003581
Iteration 8/1000 | Loss: 0.00003510
Iteration 9/1000 | Loss: 0.00003446
Iteration 10/1000 | Loss: 0.00003399
Iteration 11/1000 | Loss: 0.00003368
Iteration 12/1000 | Loss: 0.00003345
Iteration 13/1000 | Loss: 0.00003325
Iteration 14/1000 | Loss: 0.00003302
Iteration 15/1000 | Loss: 0.00003284
Iteration 16/1000 | Loss: 0.00003269
Iteration 17/1000 | Loss: 0.00003266
Iteration 18/1000 | Loss: 0.00003248
Iteration 19/1000 | Loss: 0.00003232
Iteration 20/1000 | Loss: 0.00003230
Iteration 21/1000 | Loss: 0.00003226
Iteration 22/1000 | Loss: 0.00003212
Iteration 23/1000 | Loss: 0.00003203
Iteration 24/1000 | Loss: 0.00003202
Iteration 25/1000 | Loss: 0.00003200
Iteration 26/1000 | Loss: 0.00003194
Iteration 27/1000 | Loss: 0.00003191
Iteration 28/1000 | Loss: 0.00003191
Iteration 29/1000 | Loss: 0.00003189
Iteration 30/1000 | Loss: 0.00003189
Iteration 31/1000 | Loss: 0.00003188
Iteration 32/1000 | Loss: 0.00003185
Iteration 33/1000 | Loss: 0.00003180
Iteration 34/1000 | Loss: 0.00003180
Iteration 35/1000 | Loss: 0.00003174
Iteration 36/1000 | Loss: 0.00003174
Iteration 37/1000 | Loss: 0.00003172
Iteration 38/1000 | Loss: 0.00003172
Iteration 39/1000 | Loss: 0.00003171
Iteration 40/1000 | Loss: 0.00003171
Iteration 41/1000 | Loss: 0.00003171
Iteration 42/1000 | Loss: 0.00003171
Iteration 43/1000 | Loss: 0.00003171
Iteration 44/1000 | Loss: 0.00003170
Iteration 45/1000 | Loss: 0.00003170
Iteration 46/1000 | Loss: 0.00003170
Iteration 47/1000 | Loss: 0.00003169
Iteration 48/1000 | Loss: 0.00003169
Iteration 49/1000 | Loss: 0.00003168
Iteration 50/1000 | Loss: 0.00003168
Iteration 51/1000 | Loss: 0.00003168
Iteration 52/1000 | Loss: 0.00003167
Iteration 53/1000 | Loss: 0.00003167
Iteration 54/1000 | Loss: 0.00003167
Iteration 55/1000 | Loss: 0.00003167
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00003166
Iteration 58/1000 | Loss: 0.00003166
Iteration 59/1000 | Loss: 0.00003166
Iteration 60/1000 | Loss: 0.00003166
Iteration 61/1000 | Loss: 0.00003166
Iteration 62/1000 | Loss: 0.00003166
Iteration 63/1000 | Loss: 0.00003165
Iteration 64/1000 | Loss: 0.00003165
Iteration 65/1000 | Loss: 0.00003165
Iteration 66/1000 | Loss: 0.00003165
Iteration 67/1000 | Loss: 0.00003165
Iteration 68/1000 | Loss: 0.00003165
Iteration 69/1000 | Loss: 0.00003165
Iteration 70/1000 | Loss: 0.00003164
Iteration 71/1000 | Loss: 0.00003164
Iteration 72/1000 | Loss: 0.00003164
Iteration 73/1000 | Loss: 0.00003164
Iteration 74/1000 | Loss: 0.00003164
Iteration 75/1000 | Loss: 0.00003164
Iteration 76/1000 | Loss: 0.00003163
Iteration 77/1000 | Loss: 0.00003163
Iteration 78/1000 | Loss: 0.00003163
Iteration 79/1000 | Loss: 0.00003163
Iteration 80/1000 | Loss: 0.00003162
Iteration 81/1000 | Loss: 0.00003162
Iteration 82/1000 | Loss: 0.00003162
Iteration 83/1000 | Loss: 0.00003161
Iteration 84/1000 | Loss: 0.00003161
Iteration 85/1000 | Loss: 0.00003161
Iteration 86/1000 | Loss: 0.00003161
Iteration 87/1000 | Loss: 0.00003161
Iteration 88/1000 | Loss: 0.00003161
Iteration 89/1000 | Loss: 0.00003160
Iteration 90/1000 | Loss: 0.00003160
Iteration 91/1000 | Loss: 0.00003160
Iteration 92/1000 | Loss: 0.00003160
Iteration 93/1000 | Loss: 0.00003160
Iteration 94/1000 | Loss: 0.00003159
Iteration 95/1000 | Loss: 0.00003159
Iteration 96/1000 | Loss: 0.00003159
Iteration 97/1000 | Loss: 0.00003159
Iteration 98/1000 | Loss: 0.00003159
Iteration 99/1000 | Loss: 0.00003159
Iteration 100/1000 | Loss: 0.00003159
Iteration 101/1000 | Loss: 0.00003159
Iteration 102/1000 | Loss: 0.00003158
Iteration 103/1000 | Loss: 0.00003158
Iteration 104/1000 | Loss: 0.00003158
Iteration 105/1000 | Loss: 0.00003158
Iteration 106/1000 | Loss: 0.00003158
Iteration 107/1000 | Loss: 0.00003158
Iteration 108/1000 | Loss: 0.00003157
Iteration 109/1000 | Loss: 0.00003157
Iteration 110/1000 | Loss: 0.00003157
Iteration 111/1000 | Loss: 0.00003157
Iteration 112/1000 | Loss: 0.00003157
Iteration 113/1000 | Loss: 0.00003156
Iteration 114/1000 | Loss: 0.00003156
Iteration 115/1000 | Loss: 0.00003156
Iteration 116/1000 | Loss: 0.00003156
Iteration 117/1000 | Loss: 0.00003155
Iteration 118/1000 | Loss: 0.00003155
Iteration 119/1000 | Loss: 0.00003155
Iteration 120/1000 | Loss: 0.00003155
Iteration 121/1000 | Loss: 0.00003155
Iteration 122/1000 | Loss: 0.00003155
Iteration 123/1000 | Loss: 0.00003155
Iteration 124/1000 | Loss: 0.00003155
Iteration 125/1000 | Loss: 0.00003155
Iteration 126/1000 | Loss: 0.00003155
Iteration 127/1000 | Loss: 0.00003155
Iteration 128/1000 | Loss: 0.00003155
Iteration 129/1000 | Loss: 0.00003154
Iteration 130/1000 | Loss: 0.00003154
Iteration 131/1000 | Loss: 0.00003154
Iteration 132/1000 | Loss: 0.00003154
Iteration 133/1000 | Loss: 0.00003154
Iteration 134/1000 | Loss: 0.00003153
Iteration 135/1000 | Loss: 0.00003153
Iteration 136/1000 | Loss: 0.00003153
Iteration 137/1000 | Loss: 0.00003153
Iteration 138/1000 | Loss: 0.00003152
Iteration 139/1000 | Loss: 0.00003152
Iteration 140/1000 | Loss: 0.00003152
Iteration 141/1000 | Loss: 0.00003152
Iteration 142/1000 | Loss: 0.00003152
Iteration 143/1000 | Loss: 0.00003152
Iteration 144/1000 | Loss: 0.00003152
Iteration 145/1000 | Loss: 0.00003152
Iteration 146/1000 | Loss: 0.00003152
Iteration 147/1000 | Loss: 0.00003152
Iteration 148/1000 | Loss: 0.00003152
Iteration 149/1000 | Loss: 0.00003152
Iteration 150/1000 | Loss: 0.00003152
Iteration 151/1000 | Loss: 0.00003151
Iteration 152/1000 | Loss: 0.00003151
Iteration 153/1000 | Loss: 0.00003151
Iteration 154/1000 | Loss: 0.00003151
Iteration 155/1000 | Loss: 0.00003151
Iteration 156/1000 | Loss: 0.00003151
Iteration 157/1000 | Loss: 0.00003151
Iteration 158/1000 | Loss: 0.00003151
Iteration 159/1000 | Loss: 0.00003150
Iteration 160/1000 | Loss: 0.00003150
Iteration 161/1000 | Loss: 0.00003150
Iteration 162/1000 | Loss: 0.00003150
Iteration 163/1000 | Loss: 0.00003150
Iteration 164/1000 | Loss: 0.00003150
Iteration 165/1000 | Loss: 0.00003150
Iteration 166/1000 | Loss: 0.00003150
Iteration 167/1000 | Loss: 0.00003150
Iteration 168/1000 | Loss: 0.00003150
Iteration 169/1000 | Loss: 0.00003150
Iteration 170/1000 | Loss: 0.00003150
Iteration 171/1000 | Loss: 0.00003150
Iteration 172/1000 | Loss: 0.00003150
Iteration 173/1000 | Loss: 0.00003150
Iteration 174/1000 | Loss: 0.00003150
Iteration 175/1000 | Loss: 0.00003150
Iteration 176/1000 | Loss: 0.00003150
Iteration 177/1000 | Loss: 0.00003149
Iteration 178/1000 | Loss: 0.00003149
Iteration 179/1000 | Loss: 0.00003149
Iteration 180/1000 | Loss: 0.00003149
Iteration 181/1000 | Loss: 0.00003149
Iteration 182/1000 | Loss: 0.00003149
Iteration 183/1000 | Loss: 0.00003149
Iteration 184/1000 | Loss: 0.00003149
Iteration 185/1000 | Loss: 0.00003149
Iteration 186/1000 | Loss: 0.00003149
Iteration 187/1000 | Loss: 0.00003149
Iteration 188/1000 | Loss: 0.00003149
Iteration 189/1000 | Loss: 0.00003149
Iteration 190/1000 | Loss: 0.00003149
Iteration 191/1000 | Loss: 0.00003149
Iteration 192/1000 | Loss: 0.00003148
Iteration 193/1000 | Loss: 0.00003148
Iteration 194/1000 | Loss: 0.00003148
Iteration 195/1000 | Loss: 0.00003148
Iteration 196/1000 | Loss: 0.00003148
Iteration 197/1000 | Loss: 0.00003148
Iteration 198/1000 | Loss: 0.00003148
Iteration 199/1000 | Loss: 0.00003148
Iteration 200/1000 | Loss: 0.00003148
Iteration 201/1000 | Loss: 0.00003148
Iteration 202/1000 | Loss: 0.00003148
Iteration 203/1000 | Loss: 0.00003148
Iteration 204/1000 | Loss: 0.00003148
Iteration 205/1000 | Loss: 0.00003148
Iteration 206/1000 | Loss: 0.00003148
Iteration 207/1000 | Loss: 0.00003148
Iteration 208/1000 | Loss: 0.00003148
Iteration 209/1000 | Loss: 0.00003148
Iteration 210/1000 | Loss: 0.00003148
Iteration 211/1000 | Loss: 0.00003148
Iteration 212/1000 | Loss: 0.00003148
Iteration 213/1000 | Loss: 0.00003148
Iteration 214/1000 | Loss: 0.00003148
Iteration 215/1000 | Loss: 0.00003148
Iteration 216/1000 | Loss: 0.00003148
Iteration 217/1000 | Loss: 0.00003148
Iteration 218/1000 | Loss: 0.00003148
Iteration 219/1000 | Loss: 0.00003148
Iteration 220/1000 | Loss: 0.00003148
Iteration 221/1000 | Loss: 0.00003148
Iteration 222/1000 | Loss: 0.00003148
Iteration 223/1000 | Loss: 0.00003148
Iteration 224/1000 | Loss: 0.00003148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [3.1480514735449106e-05, 3.1480514735449106e-05, 3.1480514735449106e-05, 3.1480514735449106e-05, 3.1480514735449106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1480514735449106e-05

Optimization complete. Final v2v error: 4.480426788330078 mm

Highest mean error: 5.590864181518555 mm for frame 67

Lowest mean error: 3.713026762008667 mm for frame 0

Saving results

Total time: 59.267831325531006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806909
Iteration 2/25 | Loss: 0.00128079
Iteration 3/25 | Loss: 0.00112655
Iteration 4/25 | Loss: 0.00106229
Iteration 5/25 | Loss: 0.00105356
Iteration 6/25 | Loss: 0.00105075
Iteration 7/25 | Loss: 0.00104091
Iteration 8/25 | Loss: 0.00104574
Iteration 9/25 | Loss: 0.00103661
Iteration 10/25 | Loss: 0.00103656
Iteration 11/25 | Loss: 0.00103656
Iteration 12/25 | Loss: 0.00103655
Iteration 13/25 | Loss: 0.00103655
Iteration 14/25 | Loss: 0.00103655
Iteration 15/25 | Loss: 0.00103655
Iteration 16/25 | Loss: 0.00103654
Iteration 17/25 | Loss: 0.00103654
Iteration 18/25 | Loss: 0.00103654
Iteration 19/25 | Loss: 0.00103654
Iteration 20/25 | Loss: 0.00103654
Iteration 21/25 | Loss: 0.00103654
Iteration 22/25 | Loss: 0.00103654
Iteration 23/25 | Loss: 0.00103654
Iteration 24/25 | Loss: 0.00103654
Iteration 25/25 | Loss: 0.00103654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.33341813
Iteration 2/25 | Loss: 0.00148649
Iteration 3/25 | Loss: 0.00148646
Iteration 4/25 | Loss: 0.00148646
Iteration 5/25 | Loss: 0.00148646
Iteration 6/25 | Loss: 0.00148646
Iteration 7/25 | Loss: 0.00148646
Iteration 8/25 | Loss: 0.00148646
Iteration 9/25 | Loss: 0.00148646
Iteration 10/25 | Loss: 0.00148646
Iteration 11/25 | Loss: 0.00148646
Iteration 12/25 | Loss: 0.00148646
Iteration 13/25 | Loss: 0.00148646
Iteration 14/25 | Loss: 0.00148646
Iteration 15/25 | Loss: 0.00148646
Iteration 16/25 | Loss: 0.00148646
Iteration 17/25 | Loss: 0.00148646
Iteration 18/25 | Loss: 0.00148646
Iteration 19/25 | Loss: 0.00148646
Iteration 20/25 | Loss: 0.00148646
Iteration 21/25 | Loss: 0.00148646
Iteration 22/25 | Loss: 0.00148646
Iteration 23/25 | Loss: 0.00148646
Iteration 24/25 | Loss: 0.00148646
Iteration 25/25 | Loss: 0.00148646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148646
Iteration 2/1000 | Loss: 0.00004774
Iteration 3/1000 | Loss: 0.00030799
Iteration 4/1000 | Loss: 0.00002209
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001726
Iteration 8/1000 | Loss: 0.00001688
Iteration 9/1000 | Loss: 0.00001658
Iteration 10/1000 | Loss: 0.00001641
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001624
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001607
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001593
Iteration 22/1000 | Loss: 0.00001588
Iteration 23/1000 | Loss: 0.00001587
Iteration 24/1000 | Loss: 0.00001587
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001586
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001577
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001576
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001576
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001575
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001574
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001574
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001574
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001573
Iteration 60/1000 | Loss: 0.00001573
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001572
Iteration 66/1000 | Loss: 0.00001572
Iteration 67/1000 | Loss: 0.00001572
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00001571
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001569
Iteration 80/1000 | Loss: 0.00001569
Iteration 81/1000 | Loss: 0.00001569
Iteration 82/1000 | Loss: 0.00001569
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001568
Iteration 85/1000 | Loss: 0.00001568
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001568
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001567
Iteration 91/1000 | Loss: 0.00001567
Iteration 92/1000 | Loss: 0.00001567
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001566
Iteration 95/1000 | Loss: 0.00001566
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001563
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001562
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001552
Iteration 129/1000 | Loss: 0.00001552
Iteration 130/1000 | Loss: 0.00001552
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001551
Iteration 134/1000 | Loss: 0.00001551
Iteration 135/1000 | Loss: 0.00001551
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001550
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001549
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001548
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001545
Iteration 156/1000 | Loss: 0.00001545
Iteration 157/1000 | Loss: 0.00001545
Iteration 158/1000 | Loss: 0.00001545
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001543
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001543
Iteration 168/1000 | Loss: 0.00001543
Iteration 169/1000 | Loss: 0.00001543
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001543
Iteration 177/1000 | Loss: 0.00001542
Iteration 178/1000 | Loss: 0.00001542
Iteration 179/1000 | Loss: 0.00001542
Iteration 180/1000 | Loss: 0.00001541
Iteration 181/1000 | Loss: 0.00001541
Iteration 182/1000 | Loss: 0.00001541
Iteration 183/1000 | Loss: 0.00001540
Iteration 184/1000 | Loss: 0.00001540
Iteration 185/1000 | Loss: 0.00001540
Iteration 186/1000 | Loss: 0.00001540
Iteration 187/1000 | Loss: 0.00001540
Iteration 188/1000 | Loss: 0.00001540
Iteration 189/1000 | Loss: 0.00001540
Iteration 190/1000 | Loss: 0.00001539
Iteration 191/1000 | Loss: 0.00001539
Iteration 192/1000 | Loss: 0.00001538
Iteration 193/1000 | Loss: 0.00001538
Iteration 194/1000 | Loss: 0.00001537
Iteration 195/1000 | Loss: 0.00001537
Iteration 196/1000 | Loss: 0.00001537
Iteration 197/1000 | Loss: 0.00001537
Iteration 198/1000 | Loss: 0.00001537
Iteration 199/1000 | Loss: 0.00001536
Iteration 200/1000 | Loss: 0.00001536
Iteration 201/1000 | Loss: 0.00001536
Iteration 202/1000 | Loss: 0.00001536
Iteration 203/1000 | Loss: 0.00001536
Iteration 204/1000 | Loss: 0.00001536
Iteration 205/1000 | Loss: 0.00001536
Iteration 206/1000 | Loss: 0.00001535
Iteration 207/1000 | Loss: 0.00001535
Iteration 208/1000 | Loss: 0.00001535
Iteration 209/1000 | Loss: 0.00001535
Iteration 210/1000 | Loss: 0.00001535
Iteration 211/1000 | Loss: 0.00001535
Iteration 212/1000 | Loss: 0.00001534
Iteration 213/1000 | Loss: 0.00001534
Iteration 214/1000 | Loss: 0.00001534
Iteration 215/1000 | Loss: 0.00001534
Iteration 216/1000 | Loss: 0.00001534
Iteration 217/1000 | Loss: 0.00001534
Iteration 218/1000 | Loss: 0.00001534
Iteration 219/1000 | Loss: 0.00001534
Iteration 220/1000 | Loss: 0.00001534
Iteration 221/1000 | Loss: 0.00001534
Iteration 222/1000 | Loss: 0.00001534
Iteration 223/1000 | Loss: 0.00001534
Iteration 224/1000 | Loss: 0.00001534
Iteration 225/1000 | Loss: 0.00001534
Iteration 226/1000 | Loss: 0.00001534
Iteration 227/1000 | Loss: 0.00001533
Iteration 228/1000 | Loss: 0.00001533
Iteration 229/1000 | Loss: 0.00001533
Iteration 230/1000 | Loss: 0.00001533
Iteration 231/1000 | Loss: 0.00001533
Iteration 232/1000 | Loss: 0.00001533
Iteration 233/1000 | Loss: 0.00001533
Iteration 234/1000 | Loss: 0.00001533
Iteration 235/1000 | Loss: 0.00001532
Iteration 236/1000 | Loss: 0.00001532
Iteration 237/1000 | Loss: 0.00001532
Iteration 238/1000 | Loss: 0.00001532
Iteration 239/1000 | Loss: 0.00001532
Iteration 240/1000 | Loss: 0.00001532
Iteration 241/1000 | Loss: 0.00001532
Iteration 242/1000 | Loss: 0.00001532
Iteration 243/1000 | Loss: 0.00001532
Iteration 244/1000 | Loss: 0.00001532
Iteration 245/1000 | Loss: 0.00001532
Iteration 246/1000 | Loss: 0.00001532
Iteration 247/1000 | Loss: 0.00001532
Iteration 248/1000 | Loss: 0.00001532
Iteration 249/1000 | Loss: 0.00001531
Iteration 250/1000 | Loss: 0.00001531
Iteration 251/1000 | Loss: 0.00001531
Iteration 252/1000 | Loss: 0.00001531
Iteration 253/1000 | Loss: 0.00001531
Iteration 254/1000 | Loss: 0.00001531
Iteration 255/1000 | Loss: 0.00001531
Iteration 256/1000 | Loss: 0.00001531
Iteration 257/1000 | Loss: 0.00001530
Iteration 258/1000 | Loss: 0.00001530
Iteration 259/1000 | Loss: 0.00001530
Iteration 260/1000 | Loss: 0.00001530
Iteration 261/1000 | Loss: 0.00001530
Iteration 262/1000 | Loss: 0.00001530
Iteration 263/1000 | Loss: 0.00001530
Iteration 264/1000 | Loss: 0.00001530
Iteration 265/1000 | Loss: 0.00001530
Iteration 266/1000 | Loss: 0.00001530
Iteration 267/1000 | Loss: 0.00001530
Iteration 268/1000 | Loss: 0.00001530
Iteration 269/1000 | Loss: 0.00001530
Iteration 270/1000 | Loss: 0.00001530
Iteration 271/1000 | Loss: 0.00001530
Iteration 272/1000 | Loss: 0.00001530
Iteration 273/1000 | Loss: 0.00001529
Iteration 274/1000 | Loss: 0.00001529
Iteration 275/1000 | Loss: 0.00001529
Iteration 276/1000 | Loss: 0.00001529
Iteration 277/1000 | Loss: 0.00001529
Iteration 278/1000 | Loss: 0.00001529
Iteration 279/1000 | Loss: 0.00001529
Iteration 280/1000 | Loss: 0.00001529
Iteration 281/1000 | Loss: 0.00001529
Iteration 282/1000 | Loss: 0.00001529
Iteration 283/1000 | Loss: 0.00001529
Iteration 284/1000 | Loss: 0.00001529
Iteration 285/1000 | Loss: 0.00001528
Iteration 286/1000 | Loss: 0.00001528
Iteration 287/1000 | Loss: 0.00001528
Iteration 288/1000 | Loss: 0.00001528
Iteration 289/1000 | Loss: 0.00001528
Iteration 290/1000 | Loss: 0.00001528
Iteration 291/1000 | Loss: 0.00001527
Iteration 292/1000 | Loss: 0.00001527
Iteration 293/1000 | Loss: 0.00001527
Iteration 294/1000 | Loss: 0.00001527
Iteration 295/1000 | Loss: 0.00001527
Iteration 296/1000 | Loss: 0.00001527
Iteration 297/1000 | Loss: 0.00001527
Iteration 298/1000 | Loss: 0.00001527
Iteration 299/1000 | Loss: 0.00001527
Iteration 300/1000 | Loss: 0.00001526
Iteration 301/1000 | Loss: 0.00001526
Iteration 302/1000 | Loss: 0.00001526
Iteration 303/1000 | Loss: 0.00001526
Iteration 304/1000 | Loss: 0.00001526
Iteration 305/1000 | Loss: 0.00001525
Iteration 306/1000 | Loss: 0.00001525
Iteration 307/1000 | Loss: 0.00001525
Iteration 308/1000 | Loss: 0.00001525
Iteration 309/1000 | Loss: 0.00001525
Iteration 310/1000 | Loss: 0.00001525
Iteration 311/1000 | Loss: 0.00001525
Iteration 312/1000 | Loss: 0.00001525
Iteration 313/1000 | Loss: 0.00001525
Iteration 314/1000 | Loss: 0.00001525
Iteration 315/1000 | Loss: 0.00001524
Iteration 316/1000 | Loss: 0.00001524
Iteration 317/1000 | Loss: 0.00001524
Iteration 318/1000 | Loss: 0.00001524
Iteration 319/1000 | Loss: 0.00001524
Iteration 320/1000 | Loss: 0.00001524
Iteration 321/1000 | Loss: 0.00001524
Iteration 322/1000 | Loss: 0.00001524
Iteration 323/1000 | Loss: 0.00001524
Iteration 324/1000 | Loss: 0.00001524
Iteration 325/1000 | Loss: 0.00001524
Iteration 326/1000 | Loss: 0.00001524
Iteration 327/1000 | Loss: 0.00001524
Iteration 328/1000 | Loss: 0.00001524
Iteration 329/1000 | Loss: 0.00001524
Iteration 330/1000 | Loss: 0.00001524
Iteration 331/1000 | Loss: 0.00001524
Iteration 332/1000 | Loss: 0.00001524
Iteration 333/1000 | Loss: 0.00001524
Iteration 334/1000 | Loss: 0.00001524
Iteration 335/1000 | Loss: 0.00001524
Iteration 336/1000 | Loss: 0.00001524
Iteration 337/1000 | Loss: 0.00001524
Iteration 338/1000 | Loss: 0.00001524
Iteration 339/1000 | Loss: 0.00001524
Iteration 340/1000 | Loss: 0.00001524
Iteration 341/1000 | Loss: 0.00001523
Iteration 342/1000 | Loss: 0.00001523
Iteration 343/1000 | Loss: 0.00001523
Iteration 344/1000 | Loss: 0.00001523
Iteration 345/1000 | Loss: 0.00001523
Iteration 346/1000 | Loss: 0.00001523
Iteration 347/1000 | Loss: 0.00001523
Iteration 348/1000 | Loss: 0.00001523
Iteration 349/1000 | Loss: 0.00001523
Iteration 350/1000 | Loss: 0.00001523
Iteration 351/1000 | Loss: 0.00001523
Iteration 352/1000 | Loss: 0.00001523
Iteration 353/1000 | Loss: 0.00001523
Iteration 354/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 354. Stopping optimization.
Last 5 losses: [1.5233840713335667e-05, 1.5233840713335667e-05, 1.5233840713335667e-05, 1.5233840713335667e-05, 1.5233840713335667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5233840713335667e-05

Optimization complete. Final v2v error: 3.297119379043579 mm

Highest mean error: 3.8273391723632812 mm for frame 69

Lowest mean error: 3.068873405456543 mm for frame 85

Saving results

Total time: 63.076072692871094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735610
Iteration 2/25 | Loss: 0.00131125
Iteration 3/25 | Loss: 0.00113789
Iteration 4/25 | Loss: 0.00112045
Iteration 5/25 | Loss: 0.00111862
Iteration 6/25 | Loss: 0.00111862
Iteration 7/25 | Loss: 0.00111862
Iteration 8/25 | Loss: 0.00111862
Iteration 9/25 | Loss: 0.00111862
Iteration 10/25 | Loss: 0.00111862
Iteration 11/25 | Loss: 0.00111862
Iteration 12/25 | Loss: 0.00111862
Iteration 13/25 | Loss: 0.00111862
Iteration 14/25 | Loss: 0.00111862
Iteration 15/25 | Loss: 0.00111862
Iteration 16/25 | Loss: 0.00111862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011186173651367426, 0.0011186173651367426, 0.0011186173651367426, 0.0011186173651367426, 0.0011186173651367426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011186173651367426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26210093
Iteration 2/25 | Loss: 0.00078155
Iteration 3/25 | Loss: 0.00078154
Iteration 4/25 | Loss: 0.00078154
Iteration 5/25 | Loss: 0.00078154
Iteration 6/25 | Loss: 0.00078154
Iteration 7/25 | Loss: 0.00078154
Iteration 8/25 | Loss: 0.00078154
Iteration 9/25 | Loss: 0.00078154
Iteration 10/25 | Loss: 0.00078154
Iteration 11/25 | Loss: 0.00078154
Iteration 12/25 | Loss: 0.00078154
Iteration 13/25 | Loss: 0.00078154
Iteration 14/25 | Loss: 0.00078154
Iteration 15/25 | Loss: 0.00078154
Iteration 16/25 | Loss: 0.00078154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007815361022949219, 0.0007815361022949219, 0.0007815361022949219, 0.0007815361022949219, 0.0007815361022949219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007815361022949219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078154
Iteration 2/1000 | Loss: 0.00003819
Iteration 3/1000 | Loss: 0.00002402
Iteration 4/1000 | Loss: 0.00002078
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001919
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001869
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001826
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001782
Iteration 17/1000 | Loss: 0.00001779
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001776
Iteration 25/1000 | Loss: 0.00001775
Iteration 26/1000 | Loss: 0.00001775
Iteration 27/1000 | Loss: 0.00001774
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001768
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001767
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001766
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001766
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001766
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001766
Iteration 51/1000 | Loss: 0.00001766
Iteration 52/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.7656180716585368e-05, 1.7656180716585368e-05, 1.7656180716585368e-05, 1.7656180716585368e-05, 1.7656180716585368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7656180716585368e-05

Optimization complete. Final v2v error: 3.5094661712646484 mm

Highest mean error: 4.0693278312683105 mm for frame 41

Lowest mean error: 3.2503740787506104 mm for frame 144

Saving results

Total time: 31.24451470375061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869473
Iteration 2/25 | Loss: 0.00248316
Iteration 3/25 | Loss: 0.00186077
Iteration 4/25 | Loss: 0.00173894
Iteration 5/25 | Loss: 0.00152534
Iteration 6/25 | Loss: 0.00133736
Iteration 7/25 | Loss: 0.00130020
Iteration 8/25 | Loss: 0.00127797
Iteration 9/25 | Loss: 0.00126516
Iteration 10/25 | Loss: 0.00126805
Iteration 11/25 | Loss: 0.00126622
Iteration 12/25 | Loss: 0.00125567
Iteration 13/25 | Loss: 0.00125768
Iteration 14/25 | Loss: 0.00125911
Iteration 15/25 | Loss: 0.00125083
Iteration 16/25 | Loss: 0.00124836
Iteration 17/25 | Loss: 0.00124002
Iteration 18/25 | Loss: 0.00123382
Iteration 19/25 | Loss: 0.00123065
Iteration 20/25 | Loss: 0.00122915
Iteration 21/25 | Loss: 0.00122852
Iteration 22/25 | Loss: 0.00122839
Iteration 23/25 | Loss: 0.00122880
Iteration 24/25 | Loss: 0.00122872
Iteration 25/25 | Loss: 0.00122828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29060507
Iteration 2/25 | Loss: 0.00066851
Iteration 3/25 | Loss: 0.00066850
Iteration 4/25 | Loss: 0.00066850
Iteration 5/25 | Loss: 0.00066850
Iteration 6/25 | Loss: 0.00066850
Iteration 7/25 | Loss: 0.00066850
Iteration 8/25 | Loss: 0.00066850
Iteration 9/25 | Loss: 0.00066850
Iteration 10/25 | Loss: 0.00066850
Iteration 11/25 | Loss: 0.00066850
Iteration 12/25 | Loss: 0.00066850
Iteration 13/25 | Loss: 0.00066850
Iteration 14/25 | Loss: 0.00066850
Iteration 15/25 | Loss: 0.00066850
Iteration 16/25 | Loss: 0.00066850
Iteration 17/25 | Loss: 0.00066850
Iteration 18/25 | Loss: 0.00066850
Iteration 19/25 | Loss: 0.00066850
Iteration 20/25 | Loss: 0.00066850
Iteration 21/25 | Loss: 0.00066850
Iteration 22/25 | Loss: 0.00066850
Iteration 23/25 | Loss: 0.00066850
Iteration 24/25 | Loss: 0.00066850
Iteration 25/25 | Loss: 0.00066850

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066850
Iteration 2/1000 | Loss: 0.00006361
Iteration 3/1000 | Loss: 0.00004960
Iteration 4/1000 | Loss: 0.00004064
Iteration 5/1000 | Loss: 0.00003216
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00003914
Iteration 9/1000 | Loss: 0.00004456
Iteration 10/1000 | Loss: 0.00002407
Iteration 11/1000 | Loss: 0.00003250
Iteration 12/1000 | Loss: 0.00002941
Iteration 13/1000 | Loss: 0.00003839
Iteration 14/1000 | Loss: 0.00004356
Iteration 15/1000 | Loss: 0.00003495
Iteration 16/1000 | Loss: 0.00004819
Iteration 17/1000 | Loss: 0.00004856
Iteration 18/1000 | Loss: 0.00004657
Iteration 19/1000 | Loss: 0.00004617
Iteration 20/1000 | Loss: 0.00004429
Iteration 21/1000 | Loss: 0.00004411
Iteration 22/1000 | Loss: 0.00004200
Iteration 23/1000 | Loss: 0.00003985
Iteration 24/1000 | Loss: 0.00003865
Iteration 25/1000 | Loss: 0.00004017
Iteration 26/1000 | Loss: 0.00005476
Iteration 27/1000 | Loss: 0.00004015
Iteration 28/1000 | Loss: 0.00003383
Iteration 29/1000 | Loss: 0.00003454
Iteration 30/1000 | Loss: 0.00003500
Iteration 31/1000 | Loss: 0.00004266
Iteration 32/1000 | Loss: 0.00004097
Iteration 33/1000 | Loss: 0.00003495
Iteration 34/1000 | Loss: 0.00003407
Iteration 35/1000 | Loss: 0.00004464
Iteration 36/1000 | Loss: 0.00004473
Iteration 37/1000 | Loss: 0.00005690
Iteration 38/1000 | Loss: 0.00004331
Iteration 39/1000 | Loss: 0.00003642
Iteration 40/1000 | Loss: 0.00002476
Iteration 41/1000 | Loss: 0.00002671
Iteration 42/1000 | Loss: 0.00003407
Iteration 43/1000 | Loss: 0.00003207
Iteration 44/1000 | Loss: 0.00004448
Iteration 45/1000 | Loss: 0.00003430
Iteration 46/1000 | Loss: 0.00004632
Iteration 47/1000 | Loss: 0.00003468
Iteration 48/1000 | Loss: 0.00004255
Iteration 49/1000 | Loss: 0.00002397
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001750
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001749
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001749
Iteration 86/1000 | Loss: 0.00001749
Iteration 87/1000 | Loss: 0.00001749
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001745
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001744
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001739
Iteration 101/1000 | Loss: 0.00001739
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001722
Iteration 113/1000 | Loss: 0.00001722
Iteration 114/1000 | Loss: 0.00001721
Iteration 115/1000 | Loss: 0.00001721
Iteration 116/1000 | Loss: 0.00001721
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001720
Iteration 119/1000 | Loss: 0.00001720
Iteration 120/1000 | Loss: 0.00001720
Iteration 121/1000 | Loss: 0.00001720
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001720
Iteration 126/1000 | Loss: 0.00001720
Iteration 127/1000 | Loss: 0.00001720
Iteration 128/1000 | Loss: 0.00001720
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001719
Iteration 131/1000 | Loss: 0.00001719
Iteration 132/1000 | Loss: 0.00001719
Iteration 133/1000 | Loss: 0.00001719
Iteration 134/1000 | Loss: 0.00001719
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001717
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001714
Iteration 157/1000 | Loss: 0.00001714
Iteration 158/1000 | Loss: 0.00001714
Iteration 159/1000 | Loss: 0.00001714
Iteration 160/1000 | Loss: 0.00001714
Iteration 161/1000 | Loss: 0.00001713
Iteration 162/1000 | Loss: 0.00001713
Iteration 163/1000 | Loss: 0.00001713
Iteration 164/1000 | Loss: 0.00001713
Iteration 165/1000 | Loss: 0.00001713
Iteration 166/1000 | Loss: 0.00001713
Iteration 167/1000 | Loss: 0.00001713
Iteration 168/1000 | Loss: 0.00001713
Iteration 169/1000 | Loss: 0.00001713
Iteration 170/1000 | Loss: 0.00001713
Iteration 171/1000 | Loss: 0.00001713
Iteration 172/1000 | Loss: 0.00001713
Iteration 173/1000 | Loss: 0.00001712
Iteration 174/1000 | Loss: 0.00001712
Iteration 175/1000 | Loss: 0.00001712
Iteration 176/1000 | Loss: 0.00001712
Iteration 177/1000 | Loss: 0.00001712
Iteration 178/1000 | Loss: 0.00001712
Iteration 179/1000 | Loss: 0.00001712
Iteration 180/1000 | Loss: 0.00001712
Iteration 181/1000 | Loss: 0.00001712
Iteration 182/1000 | Loss: 0.00001712
Iteration 183/1000 | Loss: 0.00001712
Iteration 184/1000 | Loss: 0.00001712
Iteration 185/1000 | Loss: 0.00001712
Iteration 186/1000 | Loss: 0.00001712
Iteration 187/1000 | Loss: 0.00001712
Iteration 188/1000 | Loss: 0.00001712
Iteration 189/1000 | Loss: 0.00001711
Iteration 190/1000 | Loss: 0.00001711
Iteration 191/1000 | Loss: 0.00001711
Iteration 192/1000 | Loss: 0.00001711
Iteration 193/1000 | Loss: 0.00001711
Iteration 194/1000 | Loss: 0.00001711
Iteration 195/1000 | Loss: 0.00001711
Iteration 196/1000 | Loss: 0.00001711
Iteration 197/1000 | Loss: 0.00001711
Iteration 198/1000 | Loss: 0.00001710
Iteration 199/1000 | Loss: 0.00001710
Iteration 200/1000 | Loss: 0.00001710
Iteration 201/1000 | Loss: 0.00001710
Iteration 202/1000 | Loss: 0.00001710
Iteration 203/1000 | Loss: 0.00001710
Iteration 204/1000 | Loss: 0.00001709
Iteration 205/1000 | Loss: 0.00001709
Iteration 206/1000 | Loss: 0.00001709
Iteration 207/1000 | Loss: 0.00001709
Iteration 208/1000 | Loss: 0.00001709
Iteration 209/1000 | Loss: 0.00001709
Iteration 210/1000 | Loss: 0.00001709
Iteration 211/1000 | Loss: 0.00001709
Iteration 212/1000 | Loss: 0.00001709
Iteration 213/1000 | Loss: 0.00001709
Iteration 214/1000 | Loss: 0.00001709
Iteration 215/1000 | Loss: 0.00001709
Iteration 216/1000 | Loss: 0.00001708
Iteration 217/1000 | Loss: 0.00001708
Iteration 218/1000 | Loss: 0.00001708
Iteration 219/1000 | Loss: 0.00001708
Iteration 220/1000 | Loss: 0.00001708
Iteration 221/1000 | Loss: 0.00001708
Iteration 222/1000 | Loss: 0.00001708
Iteration 223/1000 | Loss: 0.00001707
Iteration 224/1000 | Loss: 0.00001707
Iteration 225/1000 | Loss: 0.00001707
Iteration 226/1000 | Loss: 0.00001707
Iteration 227/1000 | Loss: 0.00001707
Iteration 228/1000 | Loss: 0.00001707
Iteration 229/1000 | Loss: 0.00001707
Iteration 230/1000 | Loss: 0.00001707
Iteration 231/1000 | Loss: 0.00001707
Iteration 232/1000 | Loss: 0.00001707
Iteration 233/1000 | Loss: 0.00001707
Iteration 234/1000 | Loss: 0.00001706
Iteration 235/1000 | Loss: 0.00001706
Iteration 236/1000 | Loss: 0.00001706
Iteration 237/1000 | Loss: 0.00001706
Iteration 238/1000 | Loss: 0.00001706
Iteration 239/1000 | Loss: 0.00001706
Iteration 240/1000 | Loss: 0.00001706
Iteration 241/1000 | Loss: 0.00001706
Iteration 242/1000 | Loss: 0.00001706
Iteration 243/1000 | Loss: 0.00001706
Iteration 244/1000 | Loss: 0.00001706
Iteration 245/1000 | Loss: 0.00001706
Iteration 246/1000 | Loss: 0.00001706
Iteration 247/1000 | Loss: 0.00001706
Iteration 248/1000 | Loss: 0.00001706
Iteration 249/1000 | Loss: 0.00001706
Iteration 250/1000 | Loss: 0.00001705
Iteration 251/1000 | Loss: 0.00001705
Iteration 252/1000 | Loss: 0.00001705
Iteration 253/1000 | Loss: 0.00001705
Iteration 254/1000 | Loss: 0.00001705
Iteration 255/1000 | Loss: 0.00001705
Iteration 256/1000 | Loss: 0.00001705
Iteration 257/1000 | Loss: 0.00001705
Iteration 258/1000 | Loss: 0.00001705
Iteration 259/1000 | Loss: 0.00001705
Iteration 260/1000 | Loss: 0.00001705
Iteration 261/1000 | Loss: 0.00001705
Iteration 262/1000 | Loss: 0.00001705
Iteration 263/1000 | Loss: 0.00001705
Iteration 264/1000 | Loss: 0.00001705
Iteration 265/1000 | Loss: 0.00001705
Iteration 266/1000 | Loss: 0.00001705
Iteration 267/1000 | Loss: 0.00001705
Iteration 268/1000 | Loss: 0.00001705
Iteration 269/1000 | Loss: 0.00001705
Iteration 270/1000 | Loss: 0.00001705
Iteration 271/1000 | Loss: 0.00001705
Iteration 272/1000 | Loss: 0.00001705
Iteration 273/1000 | Loss: 0.00001705
Iteration 274/1000 | Loss: 0.00001705
Iteration 275/1000 | Loss: 0.00001705
Iteration 276/1000 | Loss: 0.00001705
Iteration 277/1000 | Loss: 0.00001705
Iteration 278/1000 | Loss: 0.00001705
Iteration 279/1000 | Loss: 0.00001705
Iteration 280/1000 | Loss: 0.00001705
Iteration 281/1000 | Loss: 0.00001705
Iteration 282/1000 | Loss: 0.00001705
Iteration 283/1000 | Loss: 0.00001705
Iteration 284/1000 | Loss: 0.00001705
Iteration 285/1000 | Loss: 0.00001705
Iteration 286/1000 | Loss: 0.00001705
Iteration 287/1000 | Loss: 0.00001705
Iteration 288/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.7045798813342117e-05, 1.7045798813342117e-05, 1.7045798813342117e-05, 1.7045798813342117e-05, 1.7045798813342117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7045798813342117e-05

Optimization complete. Final v2v error: 3.5945804119110107 mm

Highest mean error: 4.166033744812012 mm for frame 83

Lowest mean error: 3.288663148880005 mm for frame 57

Saving results

Total time: 157.6487979888916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865806
Iteration 2/25 | Loss: 0.00119248
Iteration 3/25 | Loss: 0.00107190
Iteration 4/25 | Loss: 0.00105867
Iteration 5/25 | Loss: 0.00105612
Iteration 6/25 | Loss: 0.00105607
Iteration 7/25 | Loss: 0.00105607
Iteration 8/25 | Loss: 0.00105607
Iteration 9/25 | Loss: 0.00105607
Iteration 10/25 | Loss: 0.00105607
Iteration 11/25 | Loss: 0.00105607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001056067063473165, 0.001056067063473165, 0.001056067063473165, 0.001056067063473165, 0.001056067063473165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001056067063473165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29286957
Iteration 2/25 | Loss: 0.00098407
Iteration 3/25 | Loss: 0.00098406
Iteration 4/25 | Loss: 0.00098406
Iteration 5/25 | Loss: 0.00098406
Iteration 6/25 | Loss: 0.00098406
Iteration 7/25 | Loss: 0.00098406
Iteration 8/25 | Loss: 0.00098406
Iteration 9/25 | Loss: 0.00098406
Iteration 10/25 | Loss: 0.00098406
Iteration 11/25 | Loss: 0.00098406
Iteration 12/25 | Loss: 0.00098406
Iteration 13/25 | Loss: 0.00098406
Iteration 14/25 | Loss: 0.00098406
Iteration 15/25 | Loss: 0.00098406
Iteration 16/25 | Loss: 0.00098406
Iteration 17/25 | Loss: 0.00098406
Iteration 18/25 | Loss: 0.00098406
Iteration 19/25 | Loss: 0.00098406
Iteration 20/25 | Loss: 0.00098406
Iteration 21/25 | Loss: 0.00098406
Iteration 22/25 | Loss: 0.00098406
Iteration 23/25 | Loss: 0.00098406
Iteration 24/25 | Loss: 0.00098406
Iteration 25/25 | Loss: 0.00098406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098406
Iteration 2/1000 | Loss: 0.00003654
Iteration 3/1000 | Loss: 0.00002179
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001612
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001448
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001393
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001381
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001380
Iteration 31/1000 | Loss: 0.00001380
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001374
Iteration 47/1000 | Loss: 0.00001374
Iteration 48/1000 | Loss: 0.00001374
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001371
Iteration 51/1000 | Loss: 0.00001371
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001369
Iteration 59/1000 | Loss: 0.00001368
Iteration 60/1000 | Loss: 0.00001367
Iteration 61/1000 | Loss: 0.00001367
Iteration 62/1000 | Loss: 0.00001367
Iteration 63/1000 | Loss: 0.00001366
Iteration 64/1000 | Loss: 0.00001366
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001355
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001355
Iteration 84/1000 | Loss: 0.00001355
Iteration 85/1000 | Loss: 0.00001355
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001354
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001354
Iteration 92/1000 | Loss: 0.00001354
Iteration 93/1000 | Loss: 0.00001354
Iteration 94/1000 | Loss: 0.00001354
Iteration 95/1000 | Loss: 0.00001354
Iteration 96/1000 | Loss: 0.00001354
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001353
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.3520268112188205e-05, 1.3520268112188205e-05, 1.3520268112188205e-05, 1.3520268112188205e-05, 1.3520268112188205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3520268112188205e-05

Optimization complete. Final v2v error: 3.170926570892334 mm

Highest mean error: 3.4781296253204346 mm for frame 32

Lowest mean error: 2.8151915073394775 mm for frame 146

Saving results

Total time: 38.86410093307495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107513
Iteration 2/25 | Loss: 0.00174476
Iteration 3/25 | Loss: 0.00129512
Iteration 4/25 | Loss: 0.00118654
Iteration 5/25 | Loss: 0.00117508
Iteration 6/25 | Loss: 0.00118374
Iteration 7/25 | Loss: 0.00117882
Iteration 8/25 | Loss: 0.00116281
Iteration 9/25 | Loss: 0.00114789
Iteration 10/25 | Loss: 0.00114416
Iteration 11/25 | Loss: 0.00114090
Iteration 12/25 | Loss: 0.00115610
Iteration 13/25 | Loss: 0.00116691
Iteration 14/25 | Loss: 0.00116334
Iteration 15/25 | Loss: 0.00115589
Iteration 16/25 | Loss: 0.00116183
Iteration 17/25 | Loss: 0.00115789
Iteration 18/25 | Loss: 0.00115406
Iteration 19/25 | Loss: 0.00115298
Iteration 20/25 | Loss: 0.00115045
Iteration 21/25 | Loss: 0.00114973
Iteration 22/25 | Loss: 0.00115183
Iteration 23/25 | Loss: 0.00114891
Iteration 24/25 | Loss: 0.00114639
Iteration 25/25 | Loss: 0.00114839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07436907
Iteration 2/25 | Loss: 0.00170728
Iteration 3/25 | Loss: 0.00170728
Iteration 4/25 | Loss: 0.00170728
Iteration 5/25 | Loss: 0.00170728
Iteration 6/25 | Loss: 0.00170728
Iteration 7/25 | Loss: 0.00170728
Iteration 8/25 | Loss: 0.00170728
Iteration 9/25 | Loss: 0.00170728
Iteration 10/25 | Loss: 0.00170728
Iteration 11/25 | Loss: 0.00170728
Iteration 12/25 | Loss: 0.00170728
Iteration 13/25 | Loss: 0.00170728
Iteration 14/25 | Loss: 0.00170728
Iteration 15/25 | Loss: 0.00170728
Iteration 16/25 | Loss: 0.00170728
Iteration 17/25 | Loss: 0.00170728
Iteration 18/25 | Loss: 0.00170728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017072788905352354, 0.0017072788905352354, 0.0017072788905352354, 0.0017072788905352354, 0.0017072788905352354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017072788905352354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170728
Iteration 2/1000 | Loss: 0.00030324
Iteration 3/1000 | Loss: 0.00028982
Iteration 4/1000 | Loss: 0.00080893
Iteration 5/1000 | Loss: 0.00016863
Iteration 6/1000 | Loss: 0.00054474
Iteration 7/1000 | Loss: 0.00027262
Iteration 8/1000 | Loss: 0.00036079
Iteration 9/1000 | Loss: 0.00033810
Iteration 10/1000 | Loss: 0.00028797
Iteration 11/1000 | Loss: 0.00075079
Iteration 12/1000 | Loss: 0.00055317
Iteration 13/1000 | Loss: 0.00021072
Iteration 14/1000 | Loss: 0.00006155
Iteration 15/1000 | Loss: 0.00111327
Iteration 16/1000 | Loss: 0.00062226
Iteration 17/1000 | Loss: 0.00054563
Iteration 18/1000 | Loss: 0.00044119
Iteration 19/1000 | Loss: 0.00049629
Iteration 20/1000 | Loss: 0.00038139
Iteration 21/1000 | Loss: 0.00051351
Iteration 22/1000 | Loss: 0.00067122
Iteration 23/1000 | Loss: 0.00100974
Iteration 24/1000 | Loss: 0.00086067
Iteration 25/1000 | Loss: 0.00035858
Iteration 26/1000 | Loss: 0.00023237
Iteration 27/1000 | Loss: 0.00036572
Iteration 28/1000 | Loss: 0.00036944
Iteration 29/1000 | Loss: 0.00038054
Iteration 30/1000 | Loss: 0.00052851
Iteration 31/1000 | Loss: 0.00064461
Iteration 32/1000 | Loss: 0.00029239
Iteration 33/1000 | Loss: 0.00038598
Iteration 34/1000 | Loss: 0.00104691
Iteration 35/1000 | Loss: 0.00071435
Iteration 36/1000 | Loss: 0.00061692
Iteration 37/1000 | Loss: 0.00050158
Iteration 38/1000 | Loss: 0.00011553
Iteration 39/1000 | Loss: 0.00020946
Iteration 40/1000 | Loss: 0.00037116
Iteration 41/1000 | Loss: 0.00005217
Iteration 42/1000 | Loss: 0.00027305
Iteration 43/1000 | Loss: 0.00019711
Iteration 44/1000 | Loss: 0.00004518
Iteration 45/1000 | Loss: 0.00021371
Iteration 46/1000 | Loss: 0.00013097
Iteration 47/1000 | Loss: 0.00019596
Iteration 48/1000 | Loss: 0.00014150
Iteration 49/1000 | Loss: 0.00014276
Iteration 50/1000 | Loss: 0.00019007
Iteration 51/1000 | Loss: 0.00014408
Iteration 52/1000 | Loss: 0.00009393
Iteration 53/1000 | Loss: 0.00005163
Iteration 54/1000 | Loss: 0.00012229
Iteration 55/1000 | Loss: 0.00005434
Iteration 56/1000 | Loss: 0.00032748
Iteration 57/1000 | Loss: 0.00004847
Iteration 58/1000 | Loss: 0.00007812
Iteration 59/1000 | Loss: 0.00049004
Iteration 60/1000 | Loss: 0.00057345
Iteration 61/1000 | Loss: 0.00022840
Iteration 62/1000 | Loss: 0.00032487
Iteration 63/1000 | Loss: 0.00033651
Iteration 64/1000 | Loss: 0.00017102
Iteration 65/1000 | Loss: 0.00021716
Iteration 66/1000 | Loss: 0.00022829
Iteration 67/1000 | Loss: 0.00005138
Iteration 68/1000 | Loss: 0.00004495
Iteration 69/1000 | Loss: 0.00004099
Iteration 70/1000 | Loss: 0.00003946
Iteration 71/1000 | Loss: 0.00003842
Iteration 72/1000 | Loss: 0.00070129
Iteration 73/1000 | Loss: 0.00067640
Iteration 74/1000 | Loss: 0.00020280
Iteration 75/1000 | Loss: 0.00052572
Iteration 76/1000 | Loss: 0.00063602
Iteration 77/1000 | Loss: 0.00016796
Iteration 78/1000 | Loss: 0.00029096
Iteration 79/1000 | Loss: 0.00028223
Iteration 80/1000 | Loss: 0.00064663
Iteration 81/1000 | Loss: 0.00021759
Iteration 82/1000 | Loss: 0.00031289
Iteration 83/1000 | Loss: 0.00026373
Iteration 84/1000 | Loss: 0.00026995
Iteration 85/1000 | Loss: 0.00027647
Iteration 86/1000 | Loss: 0.00016429
Iteration 87/1000 | Loss: 0.00009995
Iteration 88/1000 | Loss: 0.00005795
Iteration 89/1000 | Loss: 0.00014692
Iteration 90/1000 | Loss: 0.00011396
Iteration 91/1000 | Loss: 0.00004397
Iteration 92/1000 | Loss: 0.00063587
Iteration 93/1000 | Loss: 0.00037289
Iteration 94/1000 | Loss: 0.00012992
Iteration 95/1000 | Loss: 0.00004883
Iteration 96/1000 | Loss: 0.00037136
Iteration 97/1000 | Loss: 0.00044124
Iteration 98/1000 | Loss: 0.00020538
Iteration 99/1000 | Loss: 0.00004005
Iteration 100/1000 | Loss: 0.00019990
Iteration 101/1000 | Loss: 0.00018948
Iteration 102/1000 | Loss: 0.00032784
Iteration 103/1000 | Loss: 0.00046689
Iteration 104/1000 | Loss: 0.00092952
Iteration 105/1000 | Loss: 0.00033006
Iteration 106/1000 | Loss: 0.00008858
Iteration 107/1000 | Loss: 0.00019178
Iteration 108/1000 | Loss: 0.00006503
Iteration 109/1000 | Loss: 0.00019547
Iteration 110/1000 | Loss: 0.00020015
Iteration 111/1000 | Loss: 0.00042419
Iteration 112/1000 | Loss: 0.00018733
Iteration 113/1000 | Loss: 0.00019501
Iteration 114/1000 | Loss: 0.00009891
Iteration 115/1000 | Loss: 0.00021374
Iteration 116/1000 | Loss: 0.00022550
Iteration 117/1000 | Loss: 0.00014844
Iteration 118/1000 | Loss: 0.00027779
Iteration 119/1000 | Loss: 0.00023742
Iteration 120/1000 | Loss: 0.00028628
Iteration 121/1000 | Loss: 0.00034300
Iteration 122/1000 | Loss: 0.00012889
Iteration 123/1000 | Loss: 0.00004413
Iteration 124/1000 | Loss: 0.00006438
Iteration 125/1000 | Loss: 0.00006170
Iteration 126/1000 | Loss: 0.00045704
Iteration 127/1000 | Loss: 0.00022557
Iteration 128/1000 | Loss: 0.00043755
Iteration 129/1000 | Loss: 0.00005315
Iteration 130/1000 | Loss: 0.00004530
Iteration 131/1000 | Loss: 0.00052668
Iteration 132/1000 | Loss: 0.00004791
Iteration 133/1000 | Loss: 0.00003946
Iteration 134/1000 | Loss: 0.00003566
Iteration 135/1000 | Loss: 0.00003369
Iteration 136/1000 | Loss: 0.00017660
Iteration 137/1000 | Loss: 0.00004047
Iteration 138/1000 | Loss: 0.00003501
Iteration 139/1000 | Loss: 0.00003306
Iteration 140/1000 | Loss: 0.00019421
Iteration 141/1000 | Loss: 0.00010962
Iteration 142/1000 | Loss: 0.00013662
Iteration 143/1000 | Loss: 0.00021691
Iteration 144/1000 | Loss: 0.00003855
Iteration 145/1000 | Loss: 0.00027937
Iteration 146/1000 | Loss: 0.00024448
Iteration 147/1000 | Loss: 0.00027774
Iteration 148/1000 | Loss: 0.00023250
Iteration 149/1000 | Loss: 0.00027935
Iteration 150/1000 | Loss: 0.00020822
Iteration 151/1000 | Loss: 0.00024535
Iteration 152/1000 | Loss: 0.00018110
Iteration 153/1000 | Loss: 0.00003138
Iteration 154/1000 | Loss: 0.00003029
Iteration 155/1000 | Loss: 0.00002967
Iteration 156/1000 | Loss: 0.00002924
Iteration 157/1000 | Loss: 0.00002891
Iteration 158/1000 | Loss: 0.00020213
Iteration 159/1000 | Loss: 0.00024948
Iteration 160/1000 | Loss: 0.00021423
Iteration 161/1000 | Loss: 0.00013371
Iteration 162/1000 | Loss: 0.00003748
Iteration 163/1000 | Loss: 0.00003175
Iteration 164/1000 | Loss: 0.00017386
Iteration 165/1000 | Loss: 0.00004275
Iteration 166/1000 | Loss: 0.00006758
Iteration 167/1000 | Loss: 0.00007731
Iteration 168/1000 | Loss: 0.00011463
Iteration 169/1000 | Loss: 0.00012196
Iteration 170/1000 | Loss: 0.00005785
Iteration 171/1000 | Loss: 0.00002990
Iteration 172/1000 | Loss: 0.00002895
Iteration 173/1000 | Loss: 0.00002869
Iteration 174/1000 | Loss: 0.00002853
Iteration 175/1000 | Loss: 0.00002853
Iteration 176/1000 | Loss: 0.00002852
Iteration 177/1000 | Loss: 0.00002851
Iteration 178/1000 | Loss: 0.00002850
Iteration 179/1000 | Loss: 0.00002849
Iteration 180/1000 | Loss: 0.00002849
Iteration 181/1000 | Loss: 0.00002849
Iteration 182/1000 | Loss: 0.00002848
Iteration 183/1000 | Loss: 0.00002848
Iteration 184/1000 | Loss: 0.00002848
Iteration 185/1000 | Loss: 0.00002847
Iteration 186/1000 | Loss: 0.00002847
Iteration 187/1000 | Loss: 0.00002846
Iteration 188/1000 | Loss: 0.00002846
Iteration 189/1000 | Loss: 0.00002846
Iteration 190/1000 | Loss: 0.00002846
Iteration 191/1000 | Loss: 0.00002845
Iteration 192/1000 | Loss: 0.00002845
Iteration 193/1000 | Loss: 0.00002845
Iteration 194/1000 | Loss: 0.00002845
Iteration 195/1000 | Loss: 0.00002845
Iteration 196/1000 | Loss: 0.00002845
Iteration 197/1000 | Loss: 0.00002845
Iteration 198/1000 | Loss: 0.00002843
Iteration 199/1000 | Loss: 0.00002843
Iteration 200/1000 | Loss: 0.00002843
Iteration 201/1000 | Loss: 0.00002842
Iteration 202/1000 | Loss: 0.00002842
Iteration 203/1000 | Loss: 0.00017848
Iteration 204/1000 | Loss: 0.00011796
Iteration 205/1000 | Loss: 0.00003379
Iteration 206/1000 | Loss: 0.00003024
Iteration 207/1000 | Loss: 0.00002885
Iteration 208/1000 | Loss: 0.00018489
Iteration 209/1000 | Loss: 0.00004231
Iteration 210/1000 | Loss: 0.00003135
Iteration 211/1000 | Loss: 0.00003003
Iteration 212/1000 | Loss: 0.00002944
Iteration 213/1000 | Loss: 0.00002908
Iteration 214/1000 | Loss: 0.00002902
Iteration 215/1000 | Loss: 0.00002899
Iteration 216/1000 | Loss: 0.00002887
Iteration 217/1000 | Loss: 0.00002871
Iteration 218/1000 | Loss: 0.00002853
Iteration 219/1000 | Loss: 0.00018119
Iteration 220/1000 | Loss: 0.00023541
Iteration 221/1000 | Loss: 0.00003527
Iteration 222/1000 | Loss: 0.00003241
Iteration 223/1000 | Loss: 0.00003029
Iteration 224/1000 | Loss: 0.00002916
Iteration 225/1000 | Loss: 0.00002866
Iteration 226/1000 | Loss: 0.00002842
Iteration 227/1000 | Loss: 0.00002814
Iteration 228/1000 | Loss: 0.00002792
Iteration 229/1000 | Loss: 0.00002771
Iteration 230/1000 | Loss: 0.00002768
Iteration 231/1000 | Loss: 0.00002760
Iteration 232/1000 | Loss: 0.00002759
Iteration 233/1000 | Loss: 0.00002756
Iteration 234/1000 | Loss: 0.00002756
Iteration 235/1000 | Loss: 0.00002755
Iteration 236/1000 | Loss: 0.00002755
Iteration 237/1000 | Loss: 0.00002755
Iteration 238/1000 | Loss: 0.00002754
Iteration 239/1000 | Loss: 0.00002754
Iteration 240/1000 | Loss: 0.00002754
Iteration 241/1000 | Loss: 0.00002754
Iteration 242/1000 | Loss: 0.00002753
Iteration 243/1000 | Loss: 0.00002753
Iteration 244/1000 | Loss: 0.00002753
Iteration 245/1000 | Loss: 0.00002753
Iteration 246/1000 | Loss: 0.00002753
Iteration 247/1000 | Loss: 0.00002753
Iteration 248/1000 | Loss: 0.00002753
Iteration 249/1000 | Loss: 0.00002753
Iteration 250/1000 | Loss: 0.00002753
Iteration 251/1000 | Loss: 0.00002753
Iteration 252/1000 | Loss: 0.00002753
Iteration 253/1000 | Loss: 0.00002752
Iteration 254/1000 | Loss: 0.00002752
Iteration 255/1000 | Loss: 0.00002752
Iteration 256/1000 | Loss: 0.00002752
Iteration 257/1000 | Loss: 0.00002752
Iteration 258/1000 | Loss: 0.00002752
Iteration 259/1000 | Loss: 0.00002752
Iteration 260/1000 | Loss: 0.00002752
Iteration 261/1000 | Loss: 0.00002752
Iteration 262/1000 | Loss: 0.00002752
Iteration 263/1000 | Loss: 0.00002752
Iteration 264/1000 | Loss: 0.00002752
Iteration 265/1000 | Loss: 0.00002752
Iteration 266/1000 | Loss: 0.00002752
Iteration 267/1000 | Loss: 0.00002752
Iteration 268/1000 | Loss: 0.00002751
Iteration 269/1000 | Loss: 0.00002751
Iteration 270/1000 | Loss: 0.00002751
Iteration 271/1000 | Loss: 0.00002751
Iteration 272/1000 | Loss: 0.00002751
Iteration 273/1000 | Loss: 0.00002751
Iteration 274/1000 | Loss: 0.00002751
Iteration 275/1000 | Loss: 0.00002751
Iteration 276/1000 | Loss: 0.00002751
Iteration 277/1000 | Loss: 0.00002751
Iteration 278/1000 | Loss: 0.00002751
Iteration 279/1000 | Loss: 0.00002751
Iteration 280/1000 | Loss: 0.00002751
Iteration 281/1000 | Loss: 0.00002751
Iteration 282/1000 | Loss: 0.00002751
Iteration 283/1000 | Loss: 0.00002751
Iteration 284/1000 | Loss: 0.00002751
Iteration 285/1000 | Loss: 0.00002750
Iteration 286/1000 | Loss: 0.00002750
Iteration 287/1000 | Loss: 0.00002750
Iteration 288/1000 | Loss: 0.00002750
Iteration 289/1000 | Loss: 0.00002750
Iteration 290/1000 | Loss: 0.00002750
Iteration 291/1000 | Loss: 0.00002750
Iteration 292/1000 | Loss: 0.00002750
Iteration 293/1000 | Loss: 0.00002750
Iteration 294/1000 | Loss: 0.00002750
Iteration 295/1000 | Loss: 0.00002750
Iteration 296/1000 | Loss: 0.00002750
Iteration 297/1000 | Loss: 0.00002750
Iteration 298/1000 | Loss: 0.00002750
Iteration 299/1000 | Loss: 0.00002750
Iteration 300/1000 | Loss: 0.00002750
Iteration 301/1000 | Loss: 0.00002750
Iteration 302/1000 | Loss: 0.00002750
Iteration 303/1000 | Loss: 0.00002750
Iteration 304/1000 | Loss: 0.00002750
Iteration 305/1000 | Loss: 0.00002750
Iteration 306/1000 | Loss: 0.00002750
Iteration 307/1000 | Loss: 0.00002750
Iteration 308/1000 | Loss: 0.00002750
Iteration 309/1000 | Loss: 0.00002750
Iteration 310/1000 | Loss: 0.00002750
Iteration 311/1000 | Loss: 0.00002750
Iteration 312/1000 | Loss: 0.00002750
Iteration 313/1000 | Loss: 0.00002750
Iteration 314/1000 | Loss: 0.00002750
Iteration 315/1000 | Loss: 0.00002750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [2.7497690098243766e-05, 2.7497690098243766e-05, 2.7497690098243766e-05, 2.7497690098243766e-05, 2.7497690098243766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7497690098243766e-05

Optimization complete. Final v2v error: 4.121585845947266 mm

Highest mean error: 6.918175220489502 mm for frame 59

Lowest mean error: 3.589812755584717 mm for frame 100

Saving results

Total time: 361.45331168174744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045628
Iteration 2/25 | Loss: 0.00365732
Iteration 3/25 | Loss: 0.00241614
Iteration 4/25 | Loss: 0.00190754
Iteration 5/25 | Loss: 0.00206268
Iteration 6/25 | Loss: 0.00190243
Iteration 7/25 | Loss: 0.00178356
Iteration 8/25 | Loss: 0.00169634
Iteration 9/25 | Loss: 0.00163003
Iteration 10/25 | Loss: 0.00159176
Iteration 11/25 | Loss: 0.00161653
Iteration 12/25 | Loss: 0.00155760
Iteration 13/25 | Loss: 0.00155805
Iteration 14/25 | Loss: 0.00153806
Iteration 15/25 | Loss: 0.00153082
Iteration 16/25 | Loss: 0.00153208
Iteration 17/25 | Loss: 0.00152902
Iteration 18/25 | Loss: 0.00153013
Iteration 19/25 | Loss: 0.00152874
Iteration 20/25 | Loss: 0.00153000
Iteration 21/25 | Loss: 0.00153006
Iteration 22/25 | Loss: 0.00152729
Iteration 23/25 | Loss: 0.00152837
Iteration 24/25 | Loss: 0.00152823
Iteration 25/25 | Loss: 0.00154787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32416821
Iteration 2/25 | Loss: 0.00410129
Iteration 3/25 | Loss: 0.00410129
Iteration 4/25 | Loss: 0.00410129
Iteration 5/25 | Loss: 0.00410129
Iteration 6/25 | Loss: 0.00410128
Iteration 7/25 | Loss: 0.00410128
Iteration 8/25 | Loss: 0.00410128
Iteration 9/25 | Loss: 0.00410128
Iteration 10/25 | Loss: 0.00410128
Iteration 11/25 | Loss: 0.00410128
Iteration 12/25 | Loss: 0.00410128
Iteration 13/25 | Loss: 0.00410128
Iteration 14/25 | Loss: 0.00410128
Iteration 15/25 | Loss: 0.00410128
Iteration 16/25 | Loss: 0.00410128
Iteration 17/25 | Loss: 0.00410128
Iteration 18/25 | Loss: 0.00410128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004101283382624388, 0.004101283382624388, 0.004101283382624388, 0.004101283382624388, 0.004101283382624388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004101283382624388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00410128
Iteration 2/1000 | Loss: 0.00065528
Iteration 3/1000 | Loss: 0.00143618
Iteration 4/1000 | Loss: 0.00224960
Iteration 5/1000 | Loss: 0.00783580
Iteration 6/1000 | Loss: 0.00060143
Iteration 7/1000 | Loss: 0.00406619
Iteration 8/1000 | Loss: 0.00024503
Iteration 9/1000 | Loss: 0.00118309
Iteration 10/1000 | Loss: 0.00086818
Iteration 11/1000 | Loss: 0.00019064
Iteration 12/1000 | Loss: 0.00066615
Iteration 13/1000 | Loss: 0.00211340
Iteration 14/1000 | Loss: 0.00268772
Iteration 15/1000 | Loss: 0.00069357
Iteration 16/1000 | Loss: 0.00082198
Iteration 17/1000 | Loss: 0.00015693
Iteration 18/1000 | Loss: 0.00052747
Iteration 19/1000 | Loss: 0.00012812
Iteration 20/1000 | Loss: 0.00121326
Iteration 21/1000 | Loss: 0.00014754
Iteration 22/1000 | Loss: 0.00083250
Iteration 23/1000 | Loss: 0.00012544
Iteration 24/1000 | Loss: 0.00010781
Iteration 25/1000 | Loss: 0.00040807
Iteration 26/1000 | Loss: 0.00032742
Iteration 27/1000 | Loss: 0.00074104
Iteration 28/1000 | Loss: 0.00082789
Iteration 29/1000 | Loss: 0.00027987
Iteration 30/1000 | Loss: 0.00015046
Iteration 31/1000 | Loss: 0.00010321
Iteration 32/1000 | Loss: 0.00010827
Iteration 33/1000 | Loss: 0.00009041
Iteration 34/1000 | Loss: 0.00032985
Iteration 35/1000 | Loss: 0.00009169
Iteration 36/1000 | Loss: 0.00045510
Iteration 37/1000 | Loss: 0.00109478
Iteration 38/1000 | Loss: 0.00121961
Iteration 39/1000 | Loss: 0.00092021
Iteration 40/1000 | Loss: 0.00061953
Iteration 41/1000 | Loss: 0.00011791
Iteration 42/1000 | Loss: 0.00008737
Iteration 43/1000 | Loss: 0.00008243
Iteration 44/1000 | Loss: 0.00022508
Iteration 45/1000 | Loss: 0.00008559
Iteration 46/1000 | Loss: 0.00007633
Iteration 47/1000 | Loss: 0.00007356
Iteration 48/1000 | Loss: 0.00007139
Iteration 49/1000 | Loss: 0.00060667
Iteration 50/1000 | Loss: 0.00074543
Iteration 51/1000 | Loss: 0.00014186
Iteration 52/1000 | Loss: 0.00007000
Iteration 53/1000 | Loss: 0.00006905
Iteration 54/1000 | Loss: 0.00006840
Iteration 55/1000 | Loss: 0.00062860
Iteration 56/1000 | Loss: 0.00010961
Iteration 57/1000 | Loss: 0.00006766
Iteration 58/1000 | Loss: 0.00054122
Iteration 59/1000 | Loss: 0.00012239
Iteration 60/1000 | Loss: 0.00006650
Iteration 61/1000 | Loss: 0.00051038
Iteration 62/1000 | Loss: 0.00007908
Iteration 63/1000 | Loss: 0.00007270
Iteration 64/1000 | Loss: 0.00006863
Iteration 65/1000 | Loss: 0.00006461
Iteration 66/1000 | Loss: 0.00006247
Iteration 67/1000 | Loss: 0.00006136
Iteration 68/1000 | Loss: 0.00006030
Iteration 69/1000 | Loss: 0.00005965
Iteration 70/1000 | Loss: 0.00005921
Iteration 71/1000 | Loss: 0.00005894
Iteration 72/1000 | Loss: 0.00005870
Iteration 73/1000 | Loss: 0.00029763
Iteration 74/1000 | Loss: 0.00017290
Iteration 75/1000 | Loss: 0.00027336
Iteration 76/1000 | Loss: 0.00028354
Iteration 77/1000 | Loss: 0.00027973
Iteration 78/1000 | Loss: 0.00026657
Iteration 79/1000 | Loss: 0.00026223
Iteration 80/1000 | Loss: 0.00023225
Iteration 81/1000 | Loss: 0.00022706
Iteration 82/1000 | Loss: 0.00019640
Iteration 83/1000 | Loss: 0.00006146
Iteration 84/1000 | Loss: 0.00005950
Iteration 85/1000 | Loss: 0.00005866
Iteration 86/1000 | Loss: 0.00005837
Iteration 87/1000 | Loss: 0.00005829
Iteration 88/1000 | Loss: 0.00030433
Iteration 89/1000 | Loss: 0.00059675
Iteration 90/1000 | Loss: 0.00186374
Iteration 91/1000 | Loss: 0.00058635
Iteration 92/1000 | Loss: 0.00007868
Iteration 93/1000 | Loss: 0.00006147
Iteration 94/1000 | Loss: 0.00005882
Iteration 95/1000 | Loss: 0.00005808
Iteration 96/1000 | Loss: 0.00005771
Iteration 97/1000 | Loss: 0.00005747
Iteration 98/1000 | Loss: 0.00005725
Iteration 99/1000 | Loss: 0.00005724
Iteration 100/1000 | Loss: 0.00005718
Iteration 101/1000 | Loss: 0.00005713
Iteration 102/1000 | Loss: 0.00005713
Iteration 103/1000 | Loss: 0.00005712
Iteration 104/1000 | Loss: 0.00005711
Iteration 105/1000 | Loss: 0.00005710
Iteration 106/1000 | Loss: 0.00005709
Iteration 107/1000 | Loss: 0.00005709
Iteration 108/1000 | Loss: 0.00005709
Iteration 109/1000 | Loss: 0.00005709
Iteration 110/1000 | Loss: 0.00005708
Iteration 111/1000 | Loss: 0.00005708
Iteration 112/1000 | Loss: 0.00005708
Iteration 113/1000 | Loss: 0.00005708
Iteration 114/1000 | Loss: 0.00005708
Iteration 115/1000 | Loss: 0.00005708
Iteration 116/1000 | Loss: 0.00005705
Iteration 117/1000 | Loss: 0.00005704
Iteration 118/1000 | Loss: 0.00005704
Iteration 119/1000 | Loss: 0.00005704
Iteration 120/1000 | Loss: 0.00005704
Iteration 121/1000 | Loss: 0.00005704
Iteration 122/1000 | Loss: 0.00005703
Iteration 123/1000 | Loss: 0.00005703
Iteration 124/1000 | Loss: 0.00005703
Iteration 125/1000 | Loss: 0.00005703
Iteration 126/1000 | Loss: 0.00005703
Iteration 127/1000 | Loss: 0.00005703
Iteration 128/1000 | Loss: 0.00005703
Iteration 129/1000 | Loss: 0.00005703
Iteration 130/1000 | Loss: 0.00005703
Iteration 131/1000 | Loss: 0.00005703
Iteration 132/1000 | Loss: 0.00005703
Iteration 133/1000 | Loss: 0.00005703
Iteration 134/1000 | Loss: 0.00005702
Iteration 135/1000 | Loss: 0.00005702
Iteration 136/1000 | Loss: 0.00005702
Iteration 137/1000 | Loss: 0.00005702
Iteration 138/1000 | Loss: 0.00005701
Iteration 139/1000 | Loss: 0.00005701
Iteration 140/1000 | Loss: 0.00005701
Iteration 141/1000 | Loss: 0.00005700
Iteration 142/1000 | Loss: 0.00005700
Iteration 143/1000 | Loss: 0.00005699
Iteration 144/1000 | Loss: 0.00005699
Iteration 145/1000 | Loss: 0.00005699
Iteration 146/1000 | Loss: 0.00005699
Iteration 147/1000 | Loss: 0.00005699
Iteration 148/1000 | Loss: 0.00005699
Iteration 149/1000 | Loss: 0.00005699
Iteration 150/1000 | Loss: 0.00005699
Iteration 151/1000 | Loss: 0.00005699
Iteration 152/1000 | Loss: 0.00005699
Iteration 153/1000 | Loss: 0.00005699
Iteration 154/1000 | Loss: 0.00005698
Iteration 155/1000 | Loss: 0.00005698
Iteration 156/1000 | Loss: 0.00005697
Iteration 157/1000 | Loss: 0.00005696
Iteration 158/1000 | Loss: 0.00005696
Iteration 159/1000 | Loss: 0.00005696
Iteration 160/1000 | Loss: 0.00005695
Iteration 161/1000 | Loss: 0.00005695
Iteration 162/1000 | Loss: 0.00005695
Iteration 163/1000 | Loss: 0.00005695
Iteration 164/1000 | Loss: 0.00005695
Iteration 165/1000 | Loss: 0.00005695
Iteration 166/1000 | Loss: 0.00005695
Iteration 167/1000 | Loss: 0.00005694
Iteration 168/1000 | Loss: 0.00005694
Iteration 169/1000 | Loss: 0.00005694
Iteration 170/1000 | Loss: 0.00005694
Iteration 171/1000 | Loss: 0.00005694
Iteration 172/1000 | Loss: 0.00005694
Iteration 173/1000 | Loss: 0.00005694
Iteration 174/1000 | Loss: 0.00005693
Iteration 175/1000 | Loss: 0.00005693
Iteration 176/1000 | Loss: 0.00005693
Iteration 177/1000 | Loss: 0.00005692
Iteration 178/1000 | Loss: 0.00005692
Iteration 179/1000 | Loss: 0.00005692
Iteration 180/1000 | Loss: 0.00005692
Iteration 181/1000 | Loss: 0.00005692
Iteration 182/1000 | Loss: 0.00005692
Iteration 183/1000 | Loss: 0.00005692
Iteration 184/1000 | Loss: 0.00005692
Iteration 185/1000 | Loss: 0.00005692
Iteration 186/1000 | Loss: 0.00005691
Iteration 187/1000 | Loss: 0.00005691
Iteration 188/1000 | Loss: 0.00005691
Iteration 189/1000 | Loss: 0.00005691
Iteration 190/1000 | Loss: 0.00005691
Iteration 191/1000 | Loss: 0.00005691
Iteration 192/1000 | Loss: 0.00005691
Iteration 193/1000 | Loss: 0.00005691
Iteration 194/1000 | Loss: 0.00005691
Iteration 195/1000 | Loss: 0.00005691
Iteration 196/1000 | Loss: 0.00005690
Iteration 197/1000 | Loss: 0.00005690
Iteration 198/1000 | Loss: 0.00005690
Iteration 199/1000 | Loss: 0.00005689
Iteration 200/1000 | Loss: 0.00005689
Iteration 201/1000 | Loss: 0.00005689
Iteration 202/1000 | Loss: 0.00005689
Iteration 203/1000 | Loss: 0.00005689
Iteration 204/1000 | Loss: 0.00005688
Iteration 205/1000 | Loss: 0.00005688
Iteration 206/1000 | Loss: 0.00005688
Iteration 207/1000 | Loss: 0.00005688
Iteration 208/1000 | Loss: 0.00005688
Iteration 209/1000 | Loss: 0.00005688
Iteration 210/1000 | Loss: 0.00005688
Iteration 211/1000 | Loss: 0.00005688
Iteration 212/1000 | Loss: 0.00005688
Iteration 213/1000 | Loss: 0.00005687
Iteration 214/1000 | Loss: 0.00005687
Iteration 215/1000 | Loss: 0.00005687
Iteration 216/1000 | Loss: 0.00005687
Iteration 217/1000 | Loss: 0.00005687
Iteration 218/1000 | Loss: 0.00005687
Iteration 219/1000 | Loss: 0.00005687
Iteration 220/1000 | Loss: 0.00005687
Iteration 221/1000 | Loss: 0.00005687
Iteration 222/1000 | Loss: 0.00005687
Iteration 223/1000 | Loss: 0.00005686
Iteration 224/1000 | Loss: 0.00005686
Iteration 225/1000 | Loss: 0.00005686
Iteration 226/1000 | Loss: 0.00005686
Iteration 227/1000 | Loss: 0.00005685
Iteration 228/1000 | Loss: 0.00005685
Iteration 229/1000 | Loss: 0.00005685
Iteration 230/1000 | Loss: 0.00005685
Iteration 231/1000 | Loss: 0.00005685
Iteration 232/1000 | Loss: 0.00005685
Iteration 233/1000 | Loss: 0.00005685
Iteration 234/1000 | Loss: 0.00005684
Iteration 235/1000 | Loss: 0.00005684
Iteration 236/1000 | Loss: 0.00005684
Iteration 237/1000 | Loss: 0.00005683
Iteration 238/1000 | Loss: 0.00005683
Iteration 239/1000 | Loss: 0.00005682
Iteration 240/1000 | Loss: 0.00005682
Iteration 241/1000 | Loss: 0.00005681
Iteration 242/1000 | Loss: 0.00005681
Iteration 243/1000 | Loss: 0.00005681
Iteration 244/1000 | Loss: 0.00005680
Iteration 245/1000 | Loss: 0.00005680
Iteration 246/1000 | Loss: 0.00005680
Iteration 247/1000 | Loss: 0.00005679
Iteration 248/1000 | Loss: 0.00005679
Iteration 249/1000 | Loss: 0.00005679
Iteration 250/1000 | Loss: 0.00005678
Iteration 251/1000 | Loss: 0.00005678
Iteration 252/1000 | Loss: 0.00005678
Iteration 253/1000 | Loss: 0.00005678
Iteration 254/1000 | Loss: 0.00005678
Iteration 255/1000 | Loss: 0.00005678
Iteration 256/1000 | Loss: 0.00005678
Iteration 257/1000 | Loss: 0.00005678
Iteration 258/1000 | Loss: 0.00005678
Iteration 259/1000 | Loss: 0.00005677
Iteration 260/1000 | Loss: 0.00005677
Iteration 261/1000 | Loss: 0.00005677
Iteration 262/1000 | Loss: 0.00005677
Iteration 263/1000 | Loss: 0.00053748
Iteration 264/1000 | Loss: 0.00049163
Iteration 265/1000 | Loss: 0.00117351
Iteration 266/1000 | Loss: 0.00006647
Iteration 267/1000 | Loss: 0.00006120
Iteration 268/1000 | Loss: 0.00005750
Iteration 269/1000 | Loss: 0.00005705
Iteration 270/1000 | Loss: 0.00005688
Iteration 271/1000 | Loss: 0.00005687
Iteration 272/1000 | Loss: 0.00005687
Iteration 273/1000 | Loss: 0.00005684
Iteration 274/1000 | Loss: 0.00005681
Iteration 275/1000 | Loss: 0.00005677
Iteration 276/1000 | Loss: 0.00005675
Iteration 277/1000 | Loss: 0.00005673
Iteration 278/1000 | Loss: 0.00005673
Iteration 279/1000 | Loss: 0.00005673
Iteration 280/1000 | Loss: 0.00005673
Iteration 281/1000 | Loss: 0.00005672
Iteration 282/1000 | Loss: 0.00005672
Iteration 283/1000 | Loss: 0.00005672
Iteration 284/1000 | Loss: 0.00005671
Iteration 285/1000 | Loss: 0.00005671
Iteration 286/1000 | Loss: 0.00005671
Iteration 287/1000 | Loss: 0.00005671
Iteration 288/1000 | Loss: 0.00005670
Iteration 289/1000 | Loss: 0.00005670
Iteration 290/1000 | Loss: 0.00005670
Iteration 291/1000 | Loss: 0.00005670
Iteration 292/1000 | Loss: 0.00005670
Iteration 293/1000 | Loss: 0.00005670
Iteration 294/1000 | Loss: 0.00005670
Iteration 295/1000 | Loss: 0.00005670
Iteration 296/1000 | Loss: 0.00005670
Iteration 297/1000 | Loss: 0.00005670
Iteration 298/1000 | Loss: 0.00005670
Iteration 299/1000 | Loss: 0.00005670
Iteration 300/1000 | Loss: 0.00005670
Iteration 301/1000 | Loss: 0.00005669
Iteration 302/1000 | Loss: 0.00005669
Iteration 303/1000 | Loss: 0.00005669
Iteration 304/1000 | Loss: 0.00005669
Iteration 305/1000 | Loss: 0.00005669
Iteration 306/1000 | Loss: 0.00005669
Iteration 307/1000 | Loss: 0.00005669
Iteration 308/1000 | Loss: 0.00005669
Iteration 309/1000 | Loss: 0.00005669
Iteration 310/1000 | Loss: 0.00005669
Iteration 311/1000 | Loss: 0.00005669
Iteration 312/1000 | Loss: 0.00005669
Iteration 313/1000 | Loss: 0.00005669
Iteration 314/1000 | Loss: 0.00005669
Iteration 315/1000 | Loss: 0.00005668
Iteration 316/1000 | Loss: 0.00005668
Iteration 317/1000 | Loss: 0.00005668
Iteration 318/1000 | Loss: 0.00005668
Iteration 319/1000 | Loss: 0.00005668
Iteration 320/1000 | Loss: 0.00005668
Iteration 321/1000 | Loss: 0.00005668
Iteration 322/1000 | Loss: 0.00005668
Iteration 323/1000 | Loss: 0.00005668
Iteration 324/1000 | Loss: 0.00005668
Iteration 325/1000 | Loss: 0.00005668
Iteration 326/1000 | Loss: 0.00005668
Iteration 327/1000 | Loss: 0.00005668
Iteration 328/1000 | Loss: 0.00005668
Iteration 329/1000 | Loss: 0.00005668
Iteration 330/1000 | Loss: 0.00005668
Iteration 331/1000 | Loss: 0.00005668
Iteration 332/1000 | Loss: 0.00005668
Iteration 333/1000 | Loss: 0.00005667
Iteration 334/1000 | Loss: 0.00005667
Iteration 335/1000 | Loss: 0.00005667
Iteration 336/1000 | Loss: 0.00005667
Iteration 337/1000 | Loss: 0.00005667
Iteration 338/1000 | Loss: 0.00005667
Iteration 339/1000 | Loss: 0.00005667
Iteration 340/1000 | Loss: 0.00005667
Iteration 341/1000 | Loss: 0.00005667
Iteration 342/1000 | Loss: 0.00005667
Iteration 343/1000 | Loss: 0.00005667
Iteration 344/1000 | Loss: 0.00005667
Iteration 345/1000 | Loss: 0.00005667
Iteration 346/1000 | Loss: 0.00005667
Iteration 347/1000 | Loss: 0.00005667
Iteration 348/1000 | Loss: 0.00005667
Iteration 349/1000 | Loss: 0.00005666
Iteration 350/1000 | Loss: 0.00005666
Iteration 351/1000 | Loss: 0.00005666
Iteration 352/1000 | Loss: 0.00005666
Iteration 353/1000 | Loss: 0.00005666
Iteration 354/1000 | Loss: 0.00005666
Iteration 355/1000 | Loss: 0.00005666
Iteration 356/1000 | Loss: 0.00005666
Iteration 357/1000 | Loss: 0.00005666
Iteration 358/1000 | Loss: 0.00005666
Iteration 359/1000 | Loss: 0.00005666
Iteration 360/1000 | Loss: 0.00005666
Iteration 361/1000 | Loss: 0.00005666
Iteration 362/1000 | Loss: 0.00005666
Iteration 363/1000 | Loss: 0.00005665
Iteration 364/1000 | Loss: 0.00005665
Iteration 365/1000 | Loss: 0.00005665
Iteration 366/1000 | Loss: 0.00005665
Iteration 367/1000 | Loss: 0.00005665
Iteration 368/1000 | Loss: 0.00005665
Iteration 369/1000 | Loss: 0.00005665
Iteration 370/1000 | Loss: 0.00005665
Iteration 371/1000 | Loss: 0.00005665
Iteration 372/1000 | Loss: 0.00005665
Iteration 373/1000 | Loss: 0.00005665
Iteration 374/1000 | Loss: 0.00005665
Iteration 375/1000 | Loss: 0.00005665
Iteration 376/1000 | Loss: 0.00005665
Iteration 377/1000 | Loss: 0.00005665
Iteration 378/1000 | Loss: 0.00005665
Iteration 379/1000 | Loss: 0.00005665
Iteration 380/1000 | Loss: 0.00005665
Iteration 381/1000 | Loss: 0.00005665
Iteration 382/1000 | Loss: 0.00005665
Iteration 383/1000 | Loss: 0.00005665
Iteration 384/1000 | Loss: 0.00005665
Iteration 385/1000 | Loss: 0.00005665
Iteration 386/1000 | Loss: 0.00005665
Iteration 387/1000 | Loss: 0.00005665
Iteration 388/1000 | Loss: 0.00005665
Iteration 389/1000 | Loss: 0.00005665
Iteration 390/1000 | Loss: 0.00005665
Iteration 391/1000 | Loss: 0.00005665
Iteration 392/1000 | Loss: 0.00005665
Iteration 393/1000 | Loss: 0.00005665
Iteration 394/1000 | Loss: 0.00005665
Iteration 395/1000 | Loss: 0.00005665
Iteration 396/1000 | Loss: 0.00005665
Iteration 397/1000 | Loss: 0.00005665
Iteration 398/1000 | Loss: 0.00005665
Iteration 399/1000 | Loss: 0.00005665
Iteration 400/1000 | Loss: 0.00005665
Iteration 401/1000 | Loss: 0.00005665
Iteration 402/1000 | Loss: 0.00005665
Iteration 403/1000 | Loss: 0.00005665
Iteration 404/1000 | Loss: 0.00005665
Iteration 405/1000 | Loss: 0.00005665
Iteration 406/1000 | Loss: 0.00005665
Iteration 407/1000 | Loss: 0.00005665
Iteration 408/1000 | Loss: 0.00005665
Iteration 409/1000 | Loss: 0.00005665
Iteration 410/1000 | Loss: 0.00005665
Iteration 411/1000 | Loss: 0.00005665
Iteration 412/1000 | Loss: 0.00005665
Iteration 413/1000 | Loss: 0.00005665
Iteration 414/1000 | Loss: 0.00005665
Iteration 415/1000 | Loss: 0.00005665
Iteration 416/1000 | Loss: 0.00005665
Iteration 417/1000 | Loss: 0.00005665
Iteration 418/1000 | Loss: 0.00005665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 418. Stopping optimization.
Last 5 losses: [5.664691707352176e-05, 5.664691707352176e-05, 5.664691707352176e-05, 5.664691707352176e-05, 5.664691707352176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.664691707352176e-05

Optimization complete. Final v2v error: 4.616881847381592 mm

Highest mean error: 13.205509185791016 mm for frame 134

Lowest mean error: 3.2837162017822266 mm for frame 197

Saving results

Total time: 233.60244607925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864585
Iteration 2/25 | Loss: 0.00190017
Iteration 3/25 | Loss: 0.00133863
Iteration 4/25 | Loss: 0.00126639
Iteration 5/25 | Loss: 0.00131869
Iteration 6/25 | Loss: 0.00118523
Iteration 7/25 | Loss: 0.00113035
Iteration 8/25 | Loss: 0.00111693
Iteration 9/25 | Loss: 0.00111822
Iteration 10/25 | Loss: 0.00111682
Iteration 11/25 | Loss: 0.00111622
Iteration 12/25 | Loss: 0.00111666
Iteration 13/25 | Loss: 0.00111411
Iteration 14/25 | Loss: 0.00111260
Iteration 15/25 | Loss: 0.00111227
Iteration 16/25 | Loss: 0.00111211
Iteration 17/25 | Loss: 0.00111908
Iteration 18/25 | Loss: 0.00111546
Iteration 19/25 | Loss: 0.00111176
Iteration 20/25 | Loss: 0.00111113
Iteration 21/25 | Loss: 0.00111403
Iteration 22/25 | Loss: 0.00111034
Iteration 23/25 | Loss: 0.00110926
Iteration 24/25 | Loss: 0.00110876
Iteration 25/25 | Loss: 0.00110859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25566947
Iteration 2/25 | Loss: 0.00087968
Iteration 3/25 | Loss: 0.00087968
Iteration 4/25 | Loss: 0.00087968
Iteration 5/25 | Loss: 0.00087968
Iteration 6/25 | Loss: 0.00087968
Iteration 7/25 | Loss: 0.00087967
Iteration 8/25 | Loss: 0.00087967
Iteration 9/25 | Loss: 0.00087967
Iteration 10/25 | Loss: 0.00087967
Iteration 11/25 | Loss: 0.00087967
Iteration 12/25 | Loss: 0.00087967
Iteration 13/25 | Loss: 0.00087967
Iteration 14/25 | Loss: 0.00087967
Iteration 15/25 | Loss: 0.00087967
Iteration 16/25 | Loss: 0.00087967
Iteration 17/25 | Loss: 0.00087967
Iteration 18/25 | Loss: 0.00087967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008796736947260797, 0.0008796736947260797, 0.0008796736947260797, 0.0008796736947260797, 0.0008796736947260797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008796736947260797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087967
Iteration 2/1000 | Loss: 0.00065748
Iteration 3/1000 | Loss: 0.00019522
Iteration 4/1000 | Loss: 0.00004920
Iteration 5/1000 | Loss: 0.00003983
Iteration 6/1000 | Loss: 0.00003172
Iteration 7/1000 | Loss: 0.00002889
Iteration 8/1000 | Loss: 0.00002753
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00035456
Iteration 11/1000 | Loss: 0.00014897
Iteration 12/1000 | Loss: 0.00003270
Iteration 13/1000 | Loss: 0.00002593
Iteration 14/1000 | Loss: 0.00002325
Iteration 15/1000 | Loss: 0.00002246
Iteration 16/1000 | Loss: 0.00013801
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002018
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001963
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001946
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001942
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001939
Iteration 38/1000 | Loss: 0.00001935
Iteration 39/1000 | Loss: 0.00001934
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001934
Iteration 42/1000 | Loss: 0.00001933
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001931
Iteration 50/1000 | Loss: 0.00001931
Iteration 51/1000 | Loss: 0.00001931
Iteration 52/1000 | Loss: 0.00001931
Iteration 53/1000 | Loss: 0.00001931
Iteration 54/1000 | Loss: 0.00001931
Iteration 55/1000 | Loss: 0.00001931
Iteration 56/1000 | Loss: 0.00001930
Iteration 57/1000 | Loss: 0.00001930
Iteration 58/1000 | Loss: 0.00001930
Iteration 59/1000 | Loss: 0.00001930
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001928
Iteration 65/1000 | Loss: 0.00001928
Iteration 66/1000 | Loss: 0.00001928
Iteration 67/1000 | Loss: 0.00001928
Iteration 68/1000 | Loss: 0.00001928
Iteration 69/1000 | Loss: 0.00001928
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001927
Iteration 72/1000 | Loss: 0.00001927
Iteration 73/1000 | Loss: 0.00001926
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001926
Iteration 76/1000 | Loss: 0.00001926
Iteration 77/1000 | Loss: 0.00001925
Iteration 78/1000 | Loss: 0.00001925
Iteration 79/1000 | Loss: 0.00001925
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001924
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001924
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001921
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001921
Iteration 105/1000 | Loss: 0.00001921
Iteration 106/1000 | Loss: 0.00001921
Iteration 107/1000 | Loss: 0.00001921
Iteration 108/1000 | Loss: 0.00001921
Iteration 109/1000 | Loss: 0.00001921
Iteration 110/1000 | Loss: 0.00001921
Iteration 111/1000 | Loss: 0.00001921
Iteration 112/1000 | Loss: 0.00001921
Iteration 113/1000 | Loss: 0.00001921
Iteration 114/1000 | Loss: 0.00001921
Iteration 115/1000 | Loss: 0.00001921
Iteration 116/1000 | Loss: 0.00001921
Iteration 117/1000 | Loss: 0.00001920
Iteration 118/1000 | Loss: 0.00001920
Iteration 119/1000 | Loss: 0.00001920
Iteration 120/1000 | Loss: 0.00001920
Iteration 121/1000 | Loss: 0.00001920
Iteration 122/1000 | Loss: 0.00001920
Iteration 123/1000 | Loss: 0.00001920
Iteration 124/1000 | Loss: 0.00001920
Iteration 125/1000 | Loss: 0.00001920
Iteration 126/1000 | Loss: 0.00001920
Iteration 127/1000 | Loss: 0.00001920
Iteration 128/1000 | Loss: 0.00001920
Iteration 129/1000 | Loss: 0.00001920
Iteration 130/1000 | Loss: 0.00001920
Iteration 131/1000 | Loss: 0.00001920
Iteration 132/1000 | Loss: 0.00001920
Iteration 133/1000 | Loss: 0.00001920
Iteration 134/1000 | Loss: 0.00001920
Iteration 135/1000 | Loss: 0.00001920
Iteration 136/1000 | Loss: 0.00001920
Iteration 137/1000 | Loss: 0.00001920
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001920
Iteration 140/1000 | Loss: 0.00001920
Iteration 141/1000 | Loss: 0.00001920
Iteration 142/1000 | Loss: 0.00001920
Iteration 143/1000 | Loss: 0.00001920
Iteration 144/1000 | Loss: 0.00001920
Iteration 145/1000 | Loss: 0.00001920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.9200726455892436e-05, 1.9200726455892436e-05, 1.9200726455892436e-05, 1.9200726455892436e-05, 1.9200726455892436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9200726455892436e-05

Optimization complete. Final v2v error: 3.6645679473876953 mm

Highest mean error: 5.304357051849365 mm for frame 57

Lowest mean error: 3.1538963317871094 mm for frame 152

Saving results

Total time: 102.20275568962097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880721
Iteration 2/25 | Loss: 0.00163924
Iteration 3/25 | Loss: 0.00122372
Iteration 4/25 | Loss: 0.00115105
Iteration 5/25 | Loss: 0.00114187
Iteration 6/25 | Loss: 0.00113974
Iteration 7/25 | Loss: 0.00113947
Iteration 8/25 | Loss: 0.00113947
Iteration 9/25 | Loss: 0.00113947
Iteration 10/25 | Loss: 0.00113947
Iteration 11/25 | Loss: 0.00113947
Iteration 12/25 | Loss: 0.00113947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011394743341952562, 0.0011394743341952562, 0.0011394743341952562, 0.0011394743341952562, 0.0011394743341952562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011394743341952562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59854221
Iteration 2/25 | Loss: 0.00089783
Iteration 3/25 | Loss: 0.00089781
Iteration 4/25 | Loss: 0.00089781
Iteration 5/25 | Loss: 0.00089781
Iteration 6/25 | Loss: 0.00089781
Iteration 7/25 | Loss: 0.00089781
Iteration 8/25 | Loss: 0.00089781
Iteration 9/25 | Loss: 0.00089781
Iteration 10/25 | Loss: 0.00089781
Iteration 11/25 | Loss: 0.00089781
Iteration 12/25 | Loss: 0.00089781
Iteration 13/25 | Loss: 0.00089781
Iteration 14/25 | Loss: 0.00089781
Iteration 15/25 | Loss: 0.00089781
Iteration 16/25 | Loss: 0.00089781
Iteration 17/25 | Loss: 0.00089781
Iteration 18/25 | Loss: 0.00089781
Iteration 19/25 | Loss: 0.00089781
Iteration 20/25 | Loss: 0.00089781
Iteration 21/25 | Loss: 0.00089781
Iteration 22/25 | Loss: 0.00089781
Iteration 23/25 | Loss: 0.00089781
Iteration 24/25 | Loss: 0.00089781
Iteration 25/25 | Loss: 0.00089781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089781
Iteration 2/1000 | Loss: 0.00005935
Iteration 3/1000 | Loss: 0.00003842
Iteration 4/1000 | Loss: 0.00002849
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002426
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002284
Iteration 9/1000 | Loss: 0.00002256
Iteration 10/1000 | Loss: 0.00002233
Iteration 11/1000 | Loss: 0.00002213
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002199
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002188
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002183
Iteration 20/1000 | Loss: 0.00002182
Iteration 21/1000 | Loss: 0.00002182
Iteration 22/1000 | Loss: 0.00002180
Iteration 23/1000 | Loss: 0.00002180
Iteration 24/1000 | Loss: 0.00002180
Iteration 25/1000 | Loss: 0.00002180
Iteration 26/1000 | Loss: 0.00002179
Iteration 27/1000 | Loss: 0.00002178
Iteration 28/1000 | Loss: 0.00002177
Iteration 29/1000 | Loss: 0.00002177
Iteration 30/1000 | Loss: 0.00002177
Iteration 31/1000 | Loss: 0.00002176
Iteration 32/1000 | Loss: 0.00002176
Iteration 33/1000 | Loss: 0.00002176
Iteration 34/1000 | Loss: 0.00002175
Iteration 35/1000 | Loss: 0.00002175
Iteration 36/1000 | Loss: 0.00002175
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002174
Iteration 40/1000 | Loss: 0.00002173
Iteration 41/1000 | Loss: 0.00002173
Iteration 42/1000 | Loss: 0.00002173
Iteration 43/1000 | Loss: 0.00002173
Iteration 44/1000 | Loss: 0.00002172
Iteration 45/1000 | Loss: 0.00002172
Iteration 46/1000 | Loss: 0.00002172
Iteration 47/1000 | Loss: 0.00002172
Iteration 48/1000 | Loss: 0.00002171
Iteration 49/1000 | Loss: 0.00002171
Iteration 50/1000 | Loss: 0.00002170
Iteration 51/1000 | Loss: 0.00002170
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002169
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002167
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002166
Iteration 61/1000 | Loss: 0.00002166
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002165
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002164
Iteration 68/1000 | Loss: 0.00002163
Iteration 69/1000 | Loss: 0.00002163
Iteration 70/1000 | Loss: 0.00002163
Iteration 71/1000 | Loss: 0.00002163
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002162
Iteration 74/1000 | Loss: 0.00002162
Iteration 75/1000 | Loss: 0.00002162
Iteration 76/1000 | Loss: 0.00002162
Iteration 77/1000 | Loss: 0.00002162
Iteration 78/1000 | Loss: 0.00002162
Iteration 79/1000 | Loss: 0.00002162
Iteration 80/1000 | Loss: 0.00002162
Iteration 81/1000 | Loss: 0.00002161
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00002161
Iteration 84/1000 | Loss: 0.00002161
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Iteration 87/1000 | Loss: 0.00002160
Iteration 88/1000 | Loss: 0.00002160
Iteration 89/1000 | Loss: 0.00002160
Iteration 90/1000 | Loss: 0.00002159
Iteration 91/1000 | Loss: 0.00002159
Iteration 92/1000 | Loss: 0.00002159
Iteration 93/1000 | Loss: 0.00002159
Iteration 94/1000 | Loss: 0.00002159
Iteration 95/1000 | Loss: 0.00002159
Iteration 96/1000 | Loss: 0.00002159
Iteration 97/1000 | Loss: 0.00002159
Iteration 98/1000 | Loss: 0.00002159
Iteration 99/1000 | Loss: 0.00002159
Iteration 100/1000 | Loss: 0.00002158
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002157
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002156
Iteration 117/1000 | Loss: 0.00002156
Iteration 118/1000 | Loss: 0.00002156
Iteration 119/1000 | Loss: 0.00002156
Iteration 120/1000 | Loss: 0.00002156
Iteration 121/1000 | Loss: 0.00002156
Iteration 122/1000 | Loss: 0.00002156
Iteration 123/1000 | Loss: 0.00002156
Iteration 124/1000 | Loss: 0.00002156
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00002156
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00002156
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00002156
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002156
Iteration 135/1000 | Loss: 0.00002156
Iteration 136/1000 | Loss: 0.00002156
Iteration 137/1000 | Loss: 0.00002156
Iteration 138/1000 | Loss: 0.00002156
Iteration 139/1000 | Loss: 0.00002156
Iteration 140/1000 | Loss: 0.00002156
Iteration 141/1000 | Loss: 0.00002156
Iteration 142/1000 | Loss: 0.00002156
Iteration 143/1000 | Loss: 0.00002156
Iteration 144/1000 | Loss: 0.00002156
Iteration 145/1000 | Loss: 0.00002156
Iteration 146/1000 | Loss: 0.00002156
Iteration 147/1000 | Loss: 0.00002156
Iteration 148/1000 | Loss: 0.00002156
Iteration 149/1000 | Loss: 0.00002156
Iteration 150/1000 | Loss: 0.00002156
Iteration 151/1000 | Loss: 0.00002156
Iteration 152/1000 | Loss: 0.00002156
Iteration 153/1000 | Loss: 0.00002156
Iteration 154/1000 | Loss: 0.00002156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.155916990886908e-05, 2.155916990886908e-05, 2.155916990886908e-05, 2.155916990886908e-05, 2.155916990886908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.155916990886908e-05

Optimization complete. Final v2v error: 3.886198043823242 mm

Highest mean error: 4.25445556640625 mm for frame 75

Lowest mean error: 3.4560961723327637 mm for frame 5

Saving results

Total time: 38.438751459121704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069084
Iteration 2/25 | Loss: 0.00280363
Iteration 3/25 | Loss: 0.00173242
Iteration 4/25 | Loss: 0.00136206
Iteration 5/25 | Loss: 0.00132956
Iteration 6/25 | Loss: 0.00130558
Iteration 7/25 | Loss: 0.00123573
Iteration 8/25 | Loss: 0.00118130
Iteration 9/25 | Loss: 0.00116093
Iteration 10/25 | Loss: 0.00116037
Iteration 11/25 | Loss: 0.00115261
Iteration 12/25 | Loss: 0.00114194
Iteration 13/25 | Loss: 0.00113094
Iteration 14/25 | Loss: 0.00112651
Iteration 15/25 | Loss: 0.00112982
Iteration 16/25 | Loss: 0.00112857
Iteration 17/25 | Loss: 0.00113001
Iteration 18/25 | Loss: 0.00112800
Iteration 19/25 | Loss: 0.00112873
Iteration 20/25 | Loss: 0.00112698
Iteration 21/25 | Loss: 0.00112848
Iteration 22/25 | Loss: 0.00112393
Iteration 23/25 | Loss: 0.00112236
Iteration 24/25 | Loss: 0.00112141
Iteration 25/25 | Loss: 0.00111918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91679907
Iteration 2/25 | Loss: 0.00140019
Iteration 3/25 | Loss: 0.00140019
Iteration 4/25 | Loss: 0.00140019
Iteration 5/25 | Loss: 0.00140019
Iteration 6/25 | Loss: 0.00140019
Iteration 7/25 | Loss: 0.00140019
Iteration 8/25 | Loss: 0.00140019
Iteration 9/25 | Loss: 0.00140019
Iteration 10/25 | Loss: 0.00140019
Iteration 11/25 | Loss: 0.00140019
Iteration 12/25 | Loss: 0.00140019
Iteration 13/25 | Loss: 0.00140019
Iteration 14/25 | Loss: 0.00140019
Iteration 15/25 | Loss: 0.00140019
Iteration 16/25 | Loss: 0.00140019
Iteration 17/25 | Loss: 0.00140019
Iteration 18/25 | Loss: 0.00140019
Iteration 19/25 | Loss: 0.00140019
Iteration 20/25 | Loss: 0.00140019
Iteration 21/25 | Loss: 0.00140019
Iteration 22/25 | Loss: 0.00140019
Iteration 23/25 | Loss: 0.00140019
Iteration 24/25 | Loss: 0.00140019
Iteration 25/25 | Loss: 0.00140019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140019
Iteration 2/1000 | Loss: 0.00011860
Iteration 3/1000 | Loss: 0.00007953
Iteration 4/1000 | Loss: 0.00006588
Iteration 5/1000 | Loss: 0.00006072
Iteration 6/1000 | Loss: 0.00005780
Iteration 7/1000 | Loss: 0.00005546
Iteration 8/1000 | Loss: 0.00005393
Iteration 9/1000 | Loss: 0.00005257
Iteration 10/1000 | Loss: 0.00005175
Iteration 11/1000 | Loss: 0.00005107
Iteration 12/1000 | Loss: 0.00506325
Iteration 13/1000 | Loss: 0.00038774
Iteration 14/1000 | Loss: 0.00005939
Iteration 15/1000 | Loss: 0.00004911
Iteration 16/1000 | Loss: 0.00004134
Iteration 17/1000 | Loss: 0.00003502
Iteration 18/1000 | Loss: 0.00003092
Iteration 19/1000 | Loss: 0.00003501
Iteration 20/1000 | Loss: 0.00002637
Iteration 21/1000 | Loss: 0.00002452
Iteration 22/1000 | Loss: 0.00002290
Iteration 23/1000 | Loss: 0.00002179
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001753
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001741
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001718
Iteration 63/1000 | Loss: 0.00001718
Iteration 64/1000 | Loss: 0.00001718
Iteration 65/1000 | Loss: 0.00001718
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001715
Iteration 70/1000 | Loss: 0.00001715
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001713
Iteration 83/1000 | Loss: 0.00001713
Iteration 84/1000 | Loss: 0.00001713
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001713
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001713
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001710
Iteration 106/1000 | Loss: 0.00001710
Iteration 107/1000 | Loss: 0.00001710
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Iteration 110/1000 | Loss: 0.00001709
Iteration 111/1000 | Loss: 0.00001709
Iteration 112/1000 | Loss: 0.00001709
Iteration 113/1000 | Loss: 0.00001709
Iteration 114/1000 | Loss: 0.00001709
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001709
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001708
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001708
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001708
Iteration 144/1000 | Loss: 0.00001708
Iteration 145/1000 | Loss: 0.00001708
Iteration 146/1000 | Loss: 0.00001708
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.7075075447792187e-05, 1.7075075447792187e-05, 1.7075075447792187e-05, 1.7075075447792187e-05, 1.7075075447792187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7075075447792187e-05

Optimization complete. Final v2v error: 3.4593870639801025 mm

Highest mean error: 8.19162654876709 mm for frame 207

Lowest mean error: 3.027157783508301 mm for frame 130

Saving results

Total time: 117.68647980690002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724620
Iteration 2/25 | Loss: 0.00120444
Iteration 3/25 | Loss: 0.00107221
Iteration 4/25 | Loss: 0.00105366
Iteration 5/25 | Loss: 0.00105041
Iteration 6/25 | Loss: 0.00105026
Iteration 7/25 | Loss: 0.00105026
Iteration 8/25 | Loss: 0.00105026
Iteration 9/25 | Loss: 0.00105026
Iteration 10/25 | Loss: 0.00105026
Iteration 11/25 | Loss: 0.00105026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010502621298655868, 0.0010502621298655868, 0.0010502621298655868, 0.0010502621298655868, 0.0010502621298655868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010502621298655868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91571093
Iteration 2/25 | Loss: 0.00121638
Iteration 3/25 | Loss: 0.00121638
Iteration 4/25 | Loss: 0.00121638
Iteration 5/25 | Loss: 0.00121638
Iteration 6/25 | Loss: 0.00121638
Iteration 7/25 | Loss: 0.00121638
Iteration 8/25 | Loss: 0.00121638
Iteration 9/25 | Loss: 0.00121638
Iteration 10/25 | Loss: 0.00121638
Iteration 11/25 | Loss: 0.00121638
Iteration 12/25 | Loss: 0.00121638
Iteration 13/25 | Loss: 0.00121638
Iteration 14/25 | Loss: 0.00121638
Iteration 15/25 | Loss: 0.00121638
Iteration 16/25 | Loss: 0.00121638
Iteration 17/25 | Loss: 0.00121638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012163788778707385, 0.0012163788778707385, 0.0012163788778707385, 0.0012163788778707385, 0.0012163788778707385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012163788778707385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121638
Iteration 2/1000 | Loss: 0.00004458
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00001790
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001483
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001428
Iteration 10/1000 | Loss: 0.00001420
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001379
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001376
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001371
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001365
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001363
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001363
Iteration 61/1000 | Loss: 0.00001363
Iteration 62/1000 | Loss: 0.00001363
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001355
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001355
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001354
Iteration 97/1000 | Loss: 0.00001354
Iteration 98/1000 | Loss: 0.00001354
Iteration 99/1000 | Loss: 0.00001354
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001350
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001350
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001350
Iteration 115/1000 | Loss: 0.00001349
Iteration 116/1000 | Loss: 0.00001349
Iteration 117/1000 | Loss: 0.00001349
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001346
Iteration 124/1000 | Loss: 0.00001346
Iteration 125/1000 | Loss: 0.00001346
Iteration 126/1000 | Loss: 0.00001346
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00001346
Iteration 133/1000 | Loss: 0.00001346
Iteration 134/1000 | Loss: 0.00001346
Iteration 135/1000 | Loss: 0.00001346
Iteration 136/1000 | Loss: 0.00001346
Iteration 137/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3462637980410364e-05, 1.3462637980410364e-05, 1.3462637980410364e-05, 1.3462637980410364e-05, 1.3462637980410364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3462637980410364e-05

Optimization complete. Final v2v error: 3.1547155380249023 mm

Highest mean error: 3.4657790660858154 mm for frame 71

Lowest mean error: 2.8790791034698486 mm for frame 8

Saving results

Total time: 39.043010234832764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046448
Iteration 2/25 | Loss: 0.00208553
Iteration 3/25 | Loss: 0.00158388
Iteration 4/25 | Loss: 0.00142145
Iteration 5/25 | Loss: 0.00151079
Iteration 6/25 | Loss: 0.00138640
Iteration 7/25 | Loss: 0.00125458
Iteration 8/25 | Loss: 0.00117195
Iteration 9/25 | Loss: 0.00110264
Iteration 10/25 | Loss: 0.00107101
Iteration 11/25 | Loss: 0.00107371
Iteration 12/25 | Loss: 0.00105659
Iteration 13/25 | Loss: 0.00105159
Iteration 14/25 | Loss: 0.00104988
Iteration 15/25 | Loss: 0.00104931
Iteration 16/25 | Loss: 0.00105033
Iteration 17/25 | Loss: 0.00104766
Iteration 18/25 | Loss: 0.00104653
Iteration 19/25 | Loss: 0.00104524
Iteration 20/25 | Loss: 0.00104413
Iteration 21/25 | Loss: 0.00104371
Iteration 22/25 | Loss: 0.00104358
Iteration 23/25 | Loss: 0.00104358
Iteration 24/25 | Loss: 0.00104358
Iteration 25/25 | Loss: 0.00104358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39313495
Iteration 2/25 | Loss: 0.00097301
Iteration 3/25 | Loss: 0.00096761
Iteration 4/25 | Loss: 0.00096761
Iteration 5/25 | Loss: 0.00096761
Iteration 6/25 | Loss: 0.00096761
Iteration 7/25 | Loss: 0.00096761
Iteration 8/25 | Loss: 0.00096761
Iteration 9/25 | Loss: 0.00096761
Iteration 10/25 | Loss: 0.00096761
Iteration 11/25 | Loss: 0.00096761
Iteration 12/25 | Loss: 0.00096761
Iteration 13/25 | Loss: 0.00096761
Iteration 14/25 | Loss: 0.00096761
Iteration 15/25 | Loss: 0.00096761
Iteration 16/25 | Loss: 0.00096761
Iteration 17/25 | Loss: 0.00096761
Iteration 18/25 | Loss: 0.00096761
Iteration 19/25 | Loss: 0.00096761
Iteration 20/25 | Loss: 0.00096761
Iteration 21/25 | Loss: 0.00096761
Iteration 22/25 | Loss: 0.00096761
Iteration 23/25 | Loss: 0.00096761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000967609288636595, 0.000967609288636595, 0.000967609288636595, 0.000967609288636595, 0.000967609288636595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000967609288636595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096761
Iteration 2/1000 | Loss: 0.00007054
Iteration 3/1000 | Loss: 0.00004111
Iteration 4/1000 | Loss: 0.00002567
Iteration 5/1000 | Loss: 0.00002169
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00003818
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001792
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001740
Iteration 14/1000 | Loss: 0.00001737
Iteration 15/1000 | Loss: 0.00001732
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00083158
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00003313
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001438
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001342
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001338
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001322
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001318
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001318
Iteration 142/1000 | Loss: 0.00001318
Iteration 143/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3184132512833457e-05, 1.3184132512833457e-05, 1.3184132512833457e-05, 1.3184132512833457e-05, 1.3184132512833457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3184132512833457e-05

Optimization complete. Final v2v error: 3.0346782207489014 mm

Highest mean error: 4.342273235321045 mm for frame 80

Lowest mean error: 2.5228707790374756 mm for frame 134

Saving results

Total time: 79.5039930343628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873928
Iteration 2/25 | Loss: 0.00134543
Iteration 3/25 | Loss: 0.00105582
Iteration 4/25 | Loss: 0.00103455
Iteration 5/25 | Loss: 0.00103204
Iteration 6/25 | Loss: 0.00103175
Iteration 7/25 | Loss: 0.00103175
Iteration 8/25 | Loss: 0.00103175
Iteration 9/25 | Loss: 0.00103175
Iteration 10/25 | Loss: 0.00103175
Iteration 11/25 | Loss: 0.00103175
Iteration 12/25 | Loss: 0.00103175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010317516280338168, 0.0010317516280338168, 0.0010317516280338168, 0.0010317516280338168, 0.0010317516280338168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010317516280338168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29882669
Iteration 2/25 | Loss: 0.00075979
Iteration 3/25 | Loss: 0.00075977
Iteration 4/25 | Loss: 0.00075977
Iteration 5/25 | Loss: 0.00075977
Iteration 6/25 | Loss: 0.00075977
Iteration 7/25 | Loss: 0.00075977
Iteration 8/25 | Loss: 0.00075977
Iteration 9/25 | Loss: 0.00075977
Iteration 10/25 | Loss: 0.00075977
Iteration 11/25 | Loss: 0.00075977
Iteration 12/25 | Loss: 0.00075977
Iteration 13/25 | Loss: 0.00075977
Iteration 14/25 | Loss: 0.00075977
Iteration 15/25 | Loss: 0.00075977
Iteration 16/25 | Loss: 0.00075977
Iteration 17/25 | Loss: 0.00075977
Iteration 18/25 | Loss: 0.00075977
Iteration 19/25 | Loss: 0.00075977
Iteration 20/25 | Loss: 0.00075977
Iteration 21/25 | Loss: 0.00075977
Iteration 22/25 | Loss: 0.00075977
Iteration 23/25 | Loss: 0.00075977
Iteration 24/25 | Loss: 0.00075977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007597717340104282, 0.0007597717340104282, 0.0007597717340104282, 0.0007597717340104282, 0.0007597717340104282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007597717340104282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075977
Iteration 2/1000 | Loss: 0.00003629
Iteration 3/1000 | Loss: 0.00002138
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001282
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001243
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001239
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001231
Iteration 130/1000 | Loss: 0.00001231
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001231
Iteration 137/1000 | Loss: 0.00001231
Iteration 138/1000 | Loss: 0.00001231
Iteration 139/1000 | Loss: 0.00001231
Iteration 140/1000 | Loss: 0.00001231
Iteration 141/1000 | Loss: 0.00001231
Iteration 142/1000 | Loss: 0.00001231
Iteration 143/1000 | Loss: 0.00001231
Iteration 144/1000 | Loss: 0.00001231
Iteration 145/1000 | Loss: 0.00001231
Iteration 146/1000 | Loss: 0.00001231
Iteration 147/1000 | Loss: 0.00001231
Iteration 148/1000 | Loss: 0.00001231
Iteration 149/1000 | Loss: 0.00001231
Iteration 150/1000 | Loss: 0.00001231
Iteration 151/1000 | Loss: 0.00001231
Iteration 152/1000 | Loss: 0.00001231
Iteration 153/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.23069921755814e-05, 1.23069921755814e-05, 1.23069921755814e-05, 1.23069921755814e-05, 1.23069921755814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.23069921755814e-05

Optimization complete. Final v2v error: 3.000664472579956 mm

Highest mean error: 3.257734537124634 mm for frame 62

Lowest mean error: 2.7851216793060303 mm for frame 88

Saving results

Total time: 34.079912185668945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410320
Iteration 2/25 | Loss: 0.00114155
Iteration 3/25 | Loss: 0.00103288
Iteration 4/25 | Loss: 0.00102688
Iteration 5/25 | Loss: 0.00102534
Iteration 6/25 | Loss: 0.00102491
Iteration 7/25 | Loss: 0.00102491
Iteration 8/25 | Loss: 0.00102491
Iteration 9/25 | Loss: 0.00102491
Iteration 10/25 | Loss: 0.00102491
Iteration 11/25 | Loss: 0.00102491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001024905126541853, 0.001024905126541853, 0.001024905126541853, 0.001024905126541853, 0.001024905126541853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001024905126541853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31034732
Iteration 2/25 | Loss: 0.00108556
Iteration 3/25 | Loss: 0.00108556
Iteration 4/25 | Loss: 0.00108556
Iteration 5/25 | Loss: 0.00108556
Iteration 6/25 | Loss: 0.00108556
Iteration 7/25 | Loss: 0.00108556
Iteration 8/25 | Loss: 0.00108556
Iteration 9/25 | Loss: 0.00108556
Iteration 10/25 | Loss: 0.00108556
Iteration 11/25 | Loss: 0.00108556
Iteration 12/25 | Loss: 0.00108556
Iteration 13/25 | Loss: 0.00108556
Iteration 14/25 | Loss: 0.00108556
Iteration 15/25 | Loss: 0.00108556
Iteration 16/25 | Loss: 0.00108556
Iteration 17/25 | Loss: 0.00108556
Iteration 18/25 | Loss: 0.00108556
Iteration 19/25 | Loss: 0.00108556
Iteration 20/25 | Loss: 0.00108556
Iteration 21/25 | Loss: 0.00108556
Iteration 22/25 | Loss: 0.00108556
Iteration 23/25 | Loss: 0.00108556
Iteration 24/25 | Loss: 0.00108556
Iteration 25/25 | Loss: 0.00108556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108556
Iteration 2/1000 | Loss: 0.00004018
Iteration 3/1000 | Loss: 0.00002408
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001738
Iteration 6/1000 | Loss: 0.00001653
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001488
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001469
Iteration 14/1000 | Loss: 0.00001466
Iteration 15/1000 | Loss: 0.00001465
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001461
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001455
Iteration 22/1000 | Loss: 0.00001455
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001451
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001438
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001437
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001431
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001430
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001418
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001418
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001417
Iteration 112/1000 | Loss: 0.00001417
Iteration 113/1000 | Loss: 0.00001417
Iteration 114/1000 | Loss: 0.00001417
Iteration 115/1000 | Loss: 0.00001417
Iteration 116/1000 | Loss: 0.00001417
Iteration 117/1000 | Loss: 0.00001417
Iteration 118/1000 | Loss: 0.00001417
Iteration 119/1000 | Loss: 0.00001417
Iteration 120/1000 | Loss: 0.00001417
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001416
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001416
Iteration 129/1000 | Loss: 0.00001415
Iteration 130/1000 | Loss: 0.00001415
Iteration 131/1000 | Loss: 0.00001415
Iteration 132/1000 | Loss: 0.00001415
Iteration 133/1000 | Loss: 0.00001415
Iteration 134/1000 | Loss: 0.00001415
Iteration 135/1000 | Loss: 0.00001415
Iteration 136/1000 | Loss: 0.00001415
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001415
Iteration 141/1000 | Loss: 0.00001415
Iteration 142/1000 | Loss: 0.00001415
Iteration 143/1000 | Loss: 0.00001415
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001414
Iteration 147/1000 | Loss: 0.00001414
Iteration 148/1000 | Loss: 0.00001414
Iteration 149/1000 | Loss: 0.00001414
Iteration 150/1000 | Loss: 0.00001414
Iteration 151/1000 | Loss: 0.00001414
Iteration 152/1000 | Loss: 0.00001414
Iteration 153/1000 | Loss: 0.00001413
Iteration 154/1000 | Loss: 0.00001413
Iteration 155/1000 | Loss: 0.00001413
Iteration 156/1000 | Loss: 0.00001413
Iteration 157/1000 | Loss: 0.00001413
Iteration 158/1000 | Loss: 0.00001413
Iteration 159/1000 | Loss: 0.00001413
Iteration 160/1000 | Loss: 0.00001413
Iteration 161/1000 | Loss: 0.00001413
Iteration 162/1000 | Loss: 0.00001413
Iteration 163/1000 | Loss: 0.00001413
Iteration 164/1000 | Loss: 0.00001413
Iteration 165/1000 | Loss: 0.00001413
Iteration 166/1000 | Loss: 0.00001413
Iteration 167/1000 | Loss: 0.00001413
Iteration 168/1000 | Loss: 0.00001413
Iteration 169/1000 | Loss: 0.00001413
Iteration 170/1000 | Loss: 0.00001413
Iteration 171/1000 | Loss: 0.00001413
Iteration 172/1000 | Loss: 0.00001413
Iteration 173/1000 | Loss: 0.00001413
Iteration 174/1000 | Loss: 0.00001413
Iteration 175/1000 | Loss: 0.00001413
Iteration 176/1000 | Loss: 0.00001413
Iteration 177/1000 | Loss: 0.00001413
Iteration 178/1000 | Loss: 0.00001413
Iteration 179/1000 | Loss: 0.00001413
Iteration 180/1000 | Loss: 0.00001413
Iteration 181/1000 | Loss: 0.00001413
Iteration 182/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.4127016584097873e-05, 1.4127016584097873e-05, 1.4127016584097873e-05, 1.4127016584097873e-05, 1.4127016584097873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4127016584097873e-05

Optimization complete. Final v2v error: 3.1892852783203125 mm

Highest mean error: 3.9609317779541016 mm for frame 66

Lowest mean error: 2.902980089187622 mm for frame 49

Saving results

Total time: 40.6580376625061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466199
Iteration 2/25 | Loss: 0.00139237
Iteration 3/25 | Loss: 0.00115979
Iteration 4/25 | Loss: 0.00111802
Iteration 5/25 | Loss: 0.00111212
Iteration 6/25 | Loss: 0.00111094
Iteration 7/25 | Loss: 0.00111094
Iteration 8/25 | Loss: 0.00111094
Iteration 9/25 | Loss: 0.00111094
Iteration 10/25 | Loss: 0.00111094
Iteration 11/25 | Loss: 0.00111094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001110936515033245, 0.001110936515033245, 0.001110936515033245, 0.001110936515033245, 0.001110936515033245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001110936515033245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21678770
Iteration 2/25 | Loss: 0.00129099
Iteration 3/25 | Loss: 0.00129097
Iteration 4/25 | Loss: 0.00129097
Iteration 5/25 | Loss: 0.00129097
Iteration 6/25 | Loss: 0.00129097
Iteration 7/25 | Loss: 0.00129097
Iteration 8/25 | Loss: 0.00129097
Iteration 9/25 | Loss: 0.00129097
Iteration 10/25 | Loss: 0.00129097
Iteration 11/25 | Loss: 0.00129096
Iteration 12/25 | Loss: 0.00129096
Iteration 13/25 | Loss: 0.00129096
Iteration 14/25 | Loss: 0.00129096
Iteration 15/25 | Loss: 0.00129096
Iteration 16/25 | Loss: 0.00129096
Iteration 17/25 | Loss: 0.00129096
Iteration 18/25 | Loss: 0.00129096
Iteration 19/25 | Loss: 0.00129096
Iteration 20/25 | Loss: 0.00129096
Iteration 21/25 | Loss: 0.00129096
Iteration 22/25 | Loss: 0.00129096
Iteration 23/25 | Loss: 0.00129096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00129096454475075, 0.00129096454475075, 0.00129096454475075, 0.00129096454475075, 0.00129096454475075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00129096454475075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129096
Iteration 2/1000 | Loss: 0.00005684
Iteration 3/1000 | Loss: 0.00002916
Iteration 4/1000 | Loss: 0.00002266
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001960
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001867
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001811
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001775
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001738
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001722
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001716
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001707
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001700
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001700
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.6998314094962552e-05, 1.6998314094962552e-05, 1.6998314094962552e-05, 1.6998314094962552e-05, 1.6998314094962552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6998314094962552e-05

Optimization complete. Final v2v error: 3.5207886695861816 mm

Highest mean error: 4.035506248474121 mm for frame 120

Lowest mean error: 3.1196842193603516 mm for frame 168

Saving results

Total time: 42.95101714134216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397602
Iteration 2/25 | Loss: 0.00111182
Iteration 3/25 | Loss: 0.00104377
Iteration 4/25 | Loss: 0.00103187
Iteration 5/25 | Loss: 0.00102839
Iteration 6/25 | Loss: 0.00102788
Iteration 7/25 | Loss: 0.00102788
Iteration 8/25 | Loss: 0.00102788
Iteration 9/25 | Loss: 0.00102788
Iteration 10/25 | Loss: 0.00102788
Iteration 11/25 | Loss: 0.00102788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010278807021677494, 0.0010278807021677494, 0.0010278807021677494, 0.0010278807021677494, 0.0010278807021677494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010278807021677494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31173348
Iteration 2/25 | Loss: 0.00115665
Iteration 3/25 | Loss: 0.00115665
Iteration 4/25 | Loss: 0.00115665
Iteration 5/25 | Loss: 0.00115665
Iteration 6/25 | Loss: 0.00115665
Iteration 7/25 | Loss: 0.00115665
Iteration 8/25 | Loss: 0.00115665
Iteration 9/25 | Loss: 0.00115665
Iteration 10/25 | Loss: 0.00115665
Iteration 11/25 | Loss: 0.00115664
Iteration 12/25 | Loss: 0.00115664
Iteration 13/25 | Loss: 0.00115664
Iteration 14/25 | Loss: 0.00115664
Iteration 15/25 | Loss: 0.00115664
Iteration 16/25 | Loss: 0.00115664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011566448956727982, 0.0011566448956727982, 0.0011566448956727982, 0.0011566448956727982, 0.0011566448956727982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011566448956727982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115664
Iteration 2/1000 | Loss: 0.00005044
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001462
Iteration 9/1000 | Loss: 0.00001438
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001356
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001355
Iteration 60/1000 | Loss: 0.00001355
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001354
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001348
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001347
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001341
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001340
Iteration 88/1000 | Loss: 0.00001340
Iteration 89/1000 | Loss: 0.00001340
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001334
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001333
Iteration 110/1000 | Loss: 0.00001333
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001332
Iteration 113/1000 | Loss: 0.00001332
Iteration 114/1000 | Loss: 0.00001332
Iteration 115/1000 | Loss: 0.00001332
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001331
Iteration 118/1000 | Loss: 0.00001331
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001326
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001325
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001324
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00001324
Iteration 174/1000 | Loss: 0.00001324
Iteration 175/1000 | Loss: 0.00001324
Iteration 176/1000 | Loss: 0.00001324
Iteration 177/1000 | Loss: 0.00001324
Iteration 178/1000 | Loss: 0.00001324
Iteration 179/1000 | Loss: 0.00001324
Iteration 180/1000 | Loss: 0.00001324
Iteration 181/1000 | Loss: 0.00001324
Iteration 182/1000 | Loss: 0.00001324
Iteration 183/1000 | Loss: 0.00001324
Iteration 184/1000 | Loss: 0.00001324
Iteration 185/1000 | Loss: 0.00001324
Iteration 186/1000 | Loss: 0.00001324
Iteration 187/1000 | Loss: 0.00001324
Iteration 188/1000 | Loss: 0.00001323
Iteration 189/1000 | Loss: 0.00001323
Iteration 190/1000 | Loss: 0.00001323
Iteration 191/1000 | Loss: 0.00001323
Iteration 192/1000 | Loss: 0.00001323
Iteration 193/1000 | Loss: 0.00001323
Iteration 194/1000 | Loss: 0.00001323
Iteration 195/1000 | Loss: 0.00001323
Iteration 196/1000 | Loss: 0.00001323
Iteration 197/1000 | Loss: 0.00001323
Iteration 198/1000 | Loss: 0.00001323
Iteration 199/1000 | Loss: 0.00001323
Iteration 200/1000 | Loss: 0.00001323
Iteration 201/1000 | Loss: 0.00001323
Iteration 202/1000 | Loss: 0.00001323
Iteration 203/1000 | Loss: 0.00001323
Iteration 204/1000 | Loss: 0.00001323
Iteration 205/1000 | Loss: 0.00001323
Iteration 206/1000 | Loss: 0.00001323
Iteration 207/1000 | Loss: 0.00001323
Iteration 208/1000 | Loss: 0.00001323
Iteration 209/1000 | Loss: 0.00001323
Iteration 210/1000 | Loss: 0.00001323
Iteration 211/1000 | Loss: 0.00001323
Iteration 212/1000 | Loss: 0.00001323
Iteration 213/1000 | Loss: 0.00001323
Iteration 214/1000 | Loss: 0.00001323
Iteration 215/1000 | Loss: 0.00001323
Iteration 216/1000 | Loss: 0.00001323
Iteration 217/1000 | Loss: 0.00001323
Iteration 218/1000 | Loss: 0.00001323
Iteration 219/1000 | Loss: 0.00001323
Iteration 220/1000 | Loss: 0.00001323
Iteration 221/1000 | Loss: 0.00001323
Iteration 222/1000 | Loss: 0.00001323
Iteration 223/1000 | Loss: 0.00001323
Iteration 224/1000 | Loss: 0.00001323
Iteration 225/1000 | Loss: 0.00001323
Iteration 226/1000 | Loss: 0.00001323
Iteration 227/1000 | Loss: 0.00001323
Iteration 228/1000 | Loss: 0.00001323
Iteration 229/1000 | Loss: 0.00001323
Iteration 230/1000 | Loss: 0.00001323
Iteration 231/1000 | Loss: 0.00001323
Iteration 232/1000 | Loss: 0.00001323
Iteration 233/1000 | Loss: 0.00001323
Iteration 234/1000 | Loss: 0.00001323
Iteration 235/1000 | Loss: 0.00001323
Iteration 236/1000 | Loss: 0.00001323
Iteration 237/1000 | Loss: 0.00001323
Iteration 238/1000 | Loss: 0.00001323
Iteration 239/1000 | Loss: 0.00001323
Iteration 240/1000 | Loss: 0.00001323
Iteration 241/1000 | Loss: 0.00001323
Iteration 242/1000 | Loss: 0.00001323
Iteration 243/1000 | Loss: 0.00001323
Iteration 244/1000 | Loss: 0.00001323
Iteration 245/1000 | Loss: 0.00001323
Iteration 246/1000 | Loss: 0.00001323
Iteration 247/1000 | Loss: 0.00001323
Iteration 248/1000 | Loss: 0.00001323
Iteration 249/1000 | Loss: 0.00001323
Iteration 250/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.3231108823674731e-05, 1.3231108823674731e-05, 1.3231108823674731e-05, 1.3231108823674731e-05, 1.3231108823674731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3231108823674731e-05

Optimization complete. Final v2v error: 3.102475881576538 mm

Highest mean error: 3.373471736907959 mm for frame 51

Lowest mean error: 2.9040467739105225 mm for frame 137

Saving results

Total time: 39.03751587867737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915332
Iteration 2/25 | Loss: 0.00115023
Iteration 3/25 | Loss: 0.00105516
Iteration 4/25 | Loss: 0.00104480
Iteration 5/25 | Loss: 0.00104173
Iteration 6/25 | Loss: 0.00104101
Iteration 7/25 | Loss: 0.00104101
Iteration 8/25 | Loss: 0.00104101
Iteration 9/25 | Loss: 0.00104101
Iteration 10/25 | Loss: 0.00104101
Iteration 11/25 | Loss: 0.00104101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010410059476271272, 0.0010410059476271272, 0.0010410059476271272, 0.0010410059476271272, 0.0010410059476271272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010410059476271272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65679860
Iteration 2/25 | Loss: 0.00108008
Iteration 3/25 | Loss: 0.00108008
Iteration 4/25 | Loss: 0.00108008
Iteration 5/25 | Loss: 0.00108008
Iteration 6/25 | Loss: 0.00108007
Iteration 7/25 | Loss: 0.00108007
Iteration 8/25 | Loss: 0.00108007
Iteration 9/25 | Loss: 0.00108007
Iteration 10/25 | Loss: 0.00108007
Iteration 11/25 | Loss: 0.00108007
Iteration 12/25 | Loss: 0.00108007
Iteration 13/25 | Loss: 0.00108007
Iteration 14/25 | Loss: 0.00108007
Iteration 15/25 | Loss: 0.00108007
Iteration 16/25 | Loss: 0.00108007
Iteration 17/25 | Loss: 0.00108007
Iteration 18/25 | Loss: 0.00108007
Iteration 19/25 | Loss: 0.00108007
Iteration 20/25 | Loss: 0.00108007
Iteration 21/25 | Loss: 0.00108007
Iteration 22/25 | Loss: 0.00108007
Iteration 23/25 | Loss: 0.00108007
Iteration 24/25 | Loss: 0.00108007
Iteration 25/25 | Loss: 0.00108007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108007
Iteration 2/1000 | Loss: 0.00005543
Iteration 3/1000 | Loss: 0.00003303
Iteration 4/1000 | Loss: 0.00002391
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001643
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001613
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001598
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001595
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001594
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001589
Iteration 29/1000 | Loss: 0.00001589
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001587
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001586
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001584
Iteration 54/1000 | Loss: 0.00001584
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001584
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001582
Iteration 63/1000 | Loss: 0.00001582
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001582
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001580
Iteration 74/1000 | Loss: 0.00001580
Iteration 75/1000 | Loss: 0.00001579
Iteration 76/1000 | Loss: 0.00001579
Iteration 77/1000 | Loss: 0.00001577
Iteration 78/1000 | Loss: 0.00001577
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001576
Iteration 85/1000 | Loss: 0.00001576
Iteration 86/1000 | Loss: 0.00001576
Iteration 87/1000 | Loss: 0.00001575
Iteration 88/1000 | Loss: 0.00001573
Iteration 89/1000 | Loss: 0.00001573
Iteration 90/1000 | Loss: 0.00001573
Iteration 91/1000 | Loss: 0.00001573
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001570
Iteration 101/1000 | Loss: 0.00001570
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001567
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001566
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001566
Iteration 121/1000 | Loss: 0.00001566
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001565
Iteration 124/1000 | Loss: 0.00001565
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001565
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001564
Iteration 129/1000 | Loss: 0.00001564
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001563
Iteration 145/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5630123016308062e-05, 1.5630123016308062e-05, 1.5630123016308062e-05, 1.5630123016308062e-05, 1.5630123016308062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5630123016308062e-05

Optimization complete. Final v2v error: 3.2474520206451416 mm

Highest mean error: 4.268523216247559 mm for frame 95

Lowest mean error: 2.8023030757904053 mm for frame 157

Saving results

Total time: 36.32291650772095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846086
Iteration 2/25 | Loss: 0.00124123
Iteration 3/25 | Loss: 0.00103674
Iteration 4/25 | Loss: 0.00101592
Iteration 5/25 | Loss: 0.00101151
Iteration 6/25 | Loss: 0.00101093
Iteration 7/25 | Loss: 0.00101093
Iteration 8/25 | Loss: 0.00101093
Iteration 9/25 | Loss: 0.00101093
Iteration 10/25 | Loss: 0.00101093
Iteration 11/25 | Loss: 0.00101093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010109315626323223, 0.0010109315626323223, 0.0010109315626323223, 0.0010109315626323223, 0.0010109315626323223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010109315626323223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28426456
Iteration 2/25 | Loss: 0.00104531
Iteration 3/25 | Loss: 0.00104531
Iteration 4/25 | Loss: 0.00104531
Iteration 5/25 | Loss: 0.00104531
Iteration 6/25 | Loss: 0.00104531
Iteration 7/25 | Loss: 0.00104531
Iteration 8/25 | Loss: 0.00104531
Iteration 9/25 | Loss: 0.00104531
Iteration 10/25 | Loss: 0.00104531
Iteration 11/25 | Loss: 0.00104531
Iteration 12/25 | Loss: 0.00104531
Iteration 13/25 | Loss: 0.00104531
Iteration 14/25 | Loss: 0.00104531
Iteration 15/25 | Loss: 0.00104531
Iteration 16/25 | Loss: 0.00104531
Iteration 17/25 | Loss: 0.00104531
Iteration 18/25 | Loss: 0.00104531
Iteration 19/25 | Loss: 0.00104531
Iteration 20/25 | Loss: 0.00104531
Iteration 21/25 | Loss: 0.00104531
Iteration 22/25 | Loss: 0.00104531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010453102877363563, 0.0010453102877363563, 0.0010453102877363563, 0.0010453102877363563, 0.0010453102877363563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010453102877363563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104531
Iteration 2/1000 | Loss: 0.00004942
Iteration 3/1000 | Loss: 0.00002905
Iteration 4/1000 | Loss: 0.00002002
Iteration 5/1000 | Loss: 0.00001699
Iteration 6/1000 | Loss: 0.00001580
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001317
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001314
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001310
Iteration 36/1000 | Loss: 0.00001310
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001307
Iteration 46/1000 | Loss: 0.00001306
Iteration 47/1000 | Loss: 0.00001306
Iteration 48/1000 | Loss: 0.00001306
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001293
Iteration 80/1000 | Loss: 0.00001293
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001292
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Iteration 85/1000 | Loss: 0.00001290
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001290
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001289
Iteration 90/1000 | Loss: 0.00001289
Iteration 91/1000 | Loss: 0.00001289
Iteration 92/1000 | Loss: 0.00001288
Iteration 93/1000 | Loss: 0.00001288
Iteration 94/1000 | Loss: 0.00001288
Iteration 95/1000 | Loss: 0.00001288
Iteration 96/1000 | Loss: 0.00001288
Iteration 97/1000 | Loss: 0.00001287
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001287
Iteration 104/1000 | Loss: 0.00001287
Iteration 105/1000 | Loss: 0.00001287
Iteration 106/1000 | Loss: 0.00001287
Iteration 107/1000 | Loss: 0.00001287
Iteration 108/1000 | Loss: 0.00001287
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001286
Iteration 114/1000 | Loss: 0.00001286
Iteration 115/1000 | Loss: 0.00001286
Iteration 116/1000 | Loss: 0.00001286
Iteration 117/1000 | Loss: 0.00001286
Iteration 118/1000 | Loss: 0.00001285
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001285
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001285
Iteration 128/1000 | Loss: 0.00001284
Iteration 129/1000 | Loss: 0.00001284
Iteration 130/1000 | Loss: 0.00001284
Iteration 131/1000 | Loss: 0.00001284
Iteration 132/1000 | Loss: 0.00001284
Iteration 133/1000 | Loss: 0.00001284
Iteration 134/1000 | Loss: 0.00001284
Iteration 135/1000 | Loss: 0.00001284
Iteration 136/1000 | Loss: 0.00001284
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Iteration 139/1000 | Loss: 0.00001283
Iteration 140/1000 | Loss: 0.00001283
Iteration 141/1000 | Loss: 0.00001283
Iteration 142/1000 | Loss: 0.00001283
Iteration 143/1000 | Loss: 0.00001283
Iteration 144/1000 | Loss: 0.00001283
Iteration 145/1000 | Loss: 0.00001283
Iteration 146/1000 | Loss: 0.00001283
Iteration 147/1000 | Loss: 0.00001283
Iteration 148/1000 | Loss: 0.00001283
Iteration 149/1000 | Loss: 0.00001283
Iteration 150/1000 | Loss: 0.00001283
Iteration 151/1000 | Loss: 0.00001283
Iteration 152/1000 | Loss: 0.00001283
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001282
Iteration 155/1000 | Loss: 0.00001282
Iteration 156/1000 | Loss: 0.00001282
Iteration 157/1000 | Loss: 0.00001282
Iteration 158/1000 | Loss: 0.00001282
Iteration 159/1000 | Loss: 0.00001282
Iteration 160/1000 | Loss: 0.00001282
Iteration 161/1000 | Loss: 0.00001282
Iteration 162/1000 | Loss: 0.00001282
Iteration 163/1000 | Loss: 0.00001282
Iteration 164/1000 | Loss: 0.00001282
Iteration 165/1000 | Loss: 0.00001281
Iteration 166/1000 | Loss: 0.00001281
Iteration 167/1000 | Loss: 0.00001281
Iteration 168/1000 | Loss: 0.00001281
Iteration 169/1000 | Loss: 0.00001281
Iteration 170/1000 | Loss: 0.00001281
Iteration 171/1000 | Loss: 0.00001281
Iteration 172/1000 | Loss: 0.00001280
Iteration 173/1000 | Loss: 0.00001280
Iteration 174/1000 | Loss: 0.00001280
Iteration 175/1000 | Loss: 0.00001280
Iteration 176/1000 | Loss: 0.00001280
Iteration 177/1000 | Loss: 0.00001280
Iteration 178/1000 | Loss: 0.00001280
Iteration 179/1000 | Loss: 0.00001280
Iteration 180/1000 | Loss: 0.00001280
Iteration 181/1000 | Loss: 0.00001280
Iteration 182/1000 | Loss: 0.00001280
Iteration 183/1000 | Loss: 0.00001280
Iteration 184/1000 | Loss: 0.00001280
Iteration 185/1000 | Loss: 0.00001280
Iteration 186/1000 | Loss: 0.00001280
Iteration 187/1000 | Loss: 0.00001280
Iteration 188/1000 | Loss: 0.00001280
Iteration 189/1000 | Loss: 0.00001280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2802266610378865e-05, 1.2802266610378865e-05, 1.2802266610378865e-05, 1.2802266610378865e-05, 1.2802266610378865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2802266610378865e-05

Optimization complete. Final v2v error: 3.035353422164917 mm

Highest mean error: 3.355775833129883 mm for frame 67

Lowest mean error: 2.71058988571167 mm for frame 82

Saving results

Total time: 44.07211208343506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048313
Iteration 2/25 | Loss: 0.00215888
Iteration 3/25 | Loss: 0.00166363
Iteration 4/25 | Loss: 0.00161502
Iteration 5/25 | Loss: 0.00159677
Iteration 6/25 | Loss: 0.00145274
Iteration 7/25 | Loss: 0.00134796
Iteration 8/25 | Loss: 0.00128338
Iteration 9/25 | Loss: 0.00123718
Iteration 10/25 | Loss: 0.00119138
Iteration 11/25 | Loss: 0.00114311
Iteration 12/25 | Loss: 0.00113344
Iteration 13/25 | Loss: 0.00110597
Iteration 14/25 | Loss: 0.00110415
Iteration 15/25 | Loss: 0.00110467
Iteration 16/25 | Loss: 0.00109214
Iteration 17/25 | Loss: 0.00108280
Iteration 18/25 | Loss: 0.00108297
Iteration 19/25 | Loss: 0.00107512
Iteration 20/25 | Loss: 0.00107853
Iteration 21/25 | Loss: 0.00108345
Iteration 22/25 | Loss: 0.00106897
Iteration 23/25 | Loss: 0.00106437
Iteration 24/25 | Loss: 0.00106331
Iteration 25/25 | Loss: 0.00106303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66966558
Iteration 2/25 | Loss: 0.00169184
Iteration 3/25 | Loss: 0.00169184
Iteration 4/25 | Loss: 0.00169184
Iteration 5/25 | Loss: 0.00169184
Iteration 6/25 | Loss: 0.00169184
Iteration 7/25 | Loss: 0.00169184
Iteration 8/25 | Loss: 0.00169184
Iteration 9/25 | Loss: 0.00169184
Iteration 10/25 | Loss: 0.00169184
Iteration 11/25 | Loss: 0.00169184
Iteration 12/25 | Loss: 0.00169183
Iteration 13/25 | Loss: 0.00169183
Iteration 14/25 | Loss: 0.00169183
Iteration 15/25 | Loss: 0.00169183
Iteration 16/25 | Loss: 0.00169183
Iteration 17/25 | Loss: 0.00169183
Iteration 18/25 | Loss: 0.00169183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016918343026190996, 0.0016918343026190996, 0.0016918343026190996, 0.0016918343026190996, 0.0016918343026190996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016918343026190996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169183
Iteration 2/1000 | Loss: 0.00016278
Iteration 3/1000 | Loss: 0.00009657
Iteration 4/1000 | Loss: 0.00211641
Iteration 5/1000 | Loss: 0.00011634
Iteration 6/1000 | Loss: 0.00191248
Iteration 7/1000 | Loss: 0.00039385
Iteration 8/1000 | Loss: 0.00008929
Iteration 9/1000 | Loss: 0.00164329
Iteration 10/1000 | Loss: 0.00006776
Iteration 11/1000 | Loss: 0.00005056
Iteration 12/1000 | Loss: 0.00059900
Iteration 13/1000 | Loss: 0.00027975
Iteration 14/1000 | Loss: 0.00037773
Iteration 15/1000 | Loss: 0.00004004
Iteration 16/1000 | Loss: 0.00009858
Iteration 17/1000 | Loss: 0.00006516
Iteration 18/1000 | Loss: 0.00054408
Iteration 19/1000 | Loss: 0.00027099
Iteration 20/1000 | Loss: 0.00088458
Iteration 21/1000 | Loss: 0.00013555
Iteration 22/1000 | Loss: 0.00016326
Iteration 23/1000 | Loss: 0.00006552
Iteration 24/1000 | Loss: 0.00003283
Iteration 25/1000 | Loss: 0.00007914
Iteration 26/1000 | Loss: 0.00127718
Iteration 27/1000 | Loss: 0.00061746
Iteration 28/1000 | Loss: 0.00067445
Iteration 29/1000 | Loss: 0.00053968
Iteration 30/1000 | Loss: 0.00019776
Iteration 31/1000 | Loss: 0.00003087
Iteration 32/1000 | Loss: 0.00002770
Iteration 33/1000 | Loss: 0.00002568
Iteration 34/1000 | Loss: 0.00002410
Iteration 35/1000 | Loss: 0.00043640
Iteration 36/1000 | Loss: 0.00003531
Iteration 37/1000 | Loss: 0.00002530
Iteration 38/1000 | Loss: 0.00002305
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00033481
Iteration 41/1000 | Loss: 0.00047938
Iteration 42/1000 | Loss: 0.00046418
Iteration 43/1000 | Loss: 0.00058537
Iteration 44/1000 | Loss: 0.00019742
Iteration 45/1000 | Loss: 0.00020760
Iteration 46/1000 | Loss: 0.00014102
Iteration 47/1000 | Loss: 0.00013138
Iteration 48/1000 | Loss: 0.00011851
Iteration 49/1000 | Loss: 0.00036515
Iteration 50/1000 | Loss: 0.00053714
Iteration 51/1000 | Loss: 0.00048016
Iteration 52/1000 | Loss: 0.00042856
Iteration 53/1000 | Loss: 0.00040353
Iteration 54/1000 | Loss: 0.00005514
Iteration 55/1000 | Loss: 0.00038028
Iteration 56/1000 | Loss: 0.00051406
Iteration 57/1000 | Loss: 0.00020783
Iteration 58/1000 | Loss: 0.00074576
Iteration 59/1000 | Loss: 0.00031411
Iteration 60/1000 | Loss: 0.00014273
Iteration 61/1000 | Loss: 0.00062919
Iteration 62/1000 | Loss: 0.00047353
Iteration 63/1000 | Loss: 0.00002671
Iteration 64/1000 | Loss: 0.00002300
Iteration 65/1000 | Loss: 0.00012944
Iteration 66/1000 | Loss: 0.00002049
Iteration 67/1000 | Loss: 0.00001978
Iteration 68/1000 | Loss: 0.00048351
Iteration 69/1000 | Loss: 0.00006388
Iteration 70/1000 | Loss: 0.00054139
Iteration 71/1000 | Loss: 0.00025431
Iteration 72/1000 | Loss: 0.00047004
Iteration 73/1000 | Loss: 0.00090104
Iteration 74/1000 | Loss: 0.00041801
Iteration 75/1000 | Loss: 0.00003395
Iteration 76/1000 | Loss: 0.00003068
Iteration 77/1000 | Loss: 0.00002272
Iteration 78/1000 | Loss: 0.00002118
Iteration 79/1000 | Loss: 0.00002063
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002016
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00108084
Iteration 87/1000 | Loss: 0.00051316
Iteration 88/1000 | Loss: 0.00002662
Iteration 89/1000 | Loss: 0.00001934
Iteration 90/1000 | Loss: 0.00001831
Iteration 91/1000 | Loss: 0.00102576
Iteration 92/1000 | Loss: 0.00017707
Iteration 93/1000 | Loss: 0.00017490
Iteration 94/1000 | Loss: 0.00007057
Iteration 95/1000 | Loss: 0.00002208
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001768
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00108894
Iteration 101/1000 | Loss: 0.00002558
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001666
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001389
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001315
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001288
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001277
Iteration 117/1000 | Loss: 0.00001277
Iteration 118/1000 | Loss: 0.00001276
Iteration 119/1000 | Loss: 0.00001274
Iteration 120/1000 | Loss: 0.00001274
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001271
Iteration 125/1000 | Loss: 0.00001271
Iteration 126/1000 | Loss: 0.00001271
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001269
Iteration 132/1000 | Loss: 0.00001269
Iteration 133/1000 | Loss: 0.00001269
Iteration 134/1000 | Loss: 0.00001269
Iteration 135/1000 | Loss: 0.00001269
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001269
Iteration 139/1000 | Loss: 0.00001269
Iteration 140/1000 | Loss: 0.00001269
Iteration 141/1000 | Loss: 0.00001268
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001268
Iteration 144/1000 | Loss: 0.00001268
Iteration 145/1000 | Loss: 0.00001268
Iteration 146/1000 | Loss: 0.00001268
Iteration 147/1000 | Loss: 0.00001267
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001267
Iteration 150/1000 | Loss: 0.00001267
Iteration 151/1000 | Loss: 0.00001267
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001267
Iteration 155/1000 | Loss: 0.00001267
Iteration 156/1000 | Loss: 0.00001267
Iteration 157/1000 | Loss: 0.00001267
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001266
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001265
Iteration 171/1000 | Loss: 0.00001265
Iteration 172/1000 | Loss: 0.00001265
Iteration 173/1000 | Loss: 0.00001265
Iteration 174/1000 | Loss: 0.00001265
Iteration 175/1000 | Loss: 0.00001265
Iteration 176/1000 | Loss: 0.00001265
Iteration 177/1000 | Loss: 0.00001265
Iteration 178/1000 | Loss: 0.00001265
Iteration 179/1000 | Loss: 0.00001264
Iteration 180/1000 | Loss: 0.00001264
Iteration 181/1000 | Loss: 0.00001264
Iteration 182/1000 | Loss: 0.00001264
Iteration 183/1000 | Loss: 0.00001264
Iteration 184/1000 | Loss: 0.00001264
Iteration 185/1000 | Loss: 0.00001264
Iteration 186/1000 | Loss: 0.00001264
Iteration 187/1000 | Loss: 0.00001264
Iteration 188/1000 | Loss: 0.00001264
Iteration 189/1000 | Loss: 0.00001264
Iteration 190/1000 | Loss: 0.00001264
Iteration 191/1000 | Loss: 0.00001264
Iteration 192/1000 | Loss: 0.00001264
Iteration 193/1000 | Loss: 0.00001264
Iteration 194/1000 | Loss: 0.00001264
Iteration 195/1000 | Loss: 0.00001264
Iteration 196/1000 | Loss: 0.00001264
Iteration 197/1000 | Loss: 0.00001264
Iteration 198/1000 | Loss: 0.00001264
Iteration 199/1000 | Loss: 0.00001264
Iteration 200/1000 | Loss: 0.00001264
Iteration 201/1000 | Loss: 0.00001264
Iteration 202/1000 | Loss: 0.00001264
Iteration 203/1000 | Loss: 0.00001264
Iteration 204/1000 | Loss: 0.00001264
Iteration 205/1000 | Loss: 0.00001264
Iteration 206/1000 | Loss: 0.00001264
Iteration 207/1000 | Loss: 0.00001264
Iteration 208/1000 | Loss: 0.00001264
Iteration 209/1000 | Loss: 0.00001264
Iteration 210/1000 | Loss: 0.00001264
Iteration 211/1000 | Loss: 0.00001264
Iteration 212/1000 | Loss: 0.00001264
Iteration 213/1000 | Loss: 0.00001264
Iteration 214/1000 | Loss: 0.00001264
Iteration 215/1000 | Loss: 0.00001264
Iteration 216/1000 | Loss: 0.00001264
Iteration 217/1000 | Loss: 0.00001264
Iteration 218/1000 | Loss: 0.00001264
Iteration 219/1000 | Loss: 0.00001264
Iteration 220/1000 | Loss: 0.00001264
Iteration 221/1000 | Loss: 0.00001264
Iteration 222/1000 | Loss: 0.00001264
Iteration 223/1000 | Loss: 0.00001264
Iteration 224/1000 | Loss: 0.00001264
Iteration 225/1000 | Loss: 0.00001264
Iteration 226/1000 | Loss: 0.00001264
Iteration 227/1000 | Loss: 0.00001264
Iteration 228/1000 | Loss: 0.00001264
Iteration 229/1000 | Loss: 0.00001264
Iteration 230/1000 | Loss: 0.00001264
Iteration 231/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.26405384435202e-05, 1.26405384435202e-05, 1.26405384435202e-05, 1.26405384435202e-05, 1.26405384435202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.26405384435202e-05

Optimization complete. Final v2v error: 3.0121920108795166 mm

Highest mean error: 4.176607608795166 mm for frame 59

Lowest mean error: 2.6085445880889893 mm for frame 112

Saving results

Total time: 203.28035306930542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481243
Iteration 2/25 | Loss: 0.00126111
Iteration 3/25 | Loss: 0.00109667
Iteration 4/25 | Loss: 0.00107248
Iteration 5/25 | Loss: 0.00106498
Iteration 6/25 | Loss: 0.00106284
Iteration 7/25 | Loss: 0.00106280
Iteration 8/25 | Loss: 0.00106280
Iteration 9/25 | Loss: 0.00106280
Iteration 10/25 | Loss: 0.00106280
Iteration 11/25 | Loss: 0.00106280
Iteration 12/25 | Loss: 0.00106280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010628015734255314, 0.0010628015734255314, 0.0010628015734255314, 0.0010628015734255314, 0.0010628015734255314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010628015734255314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84664452
Iteration 2/25 | Loss: 0.00106019
Iteration 3/25 | Loss: 0.00106018
Iteration 4/25 | Loss: 0.00106018
Iteration 5/25 | Loss: 0.00106018
Iteration 6/25 | Loss: 0.00106018
Iteration 7/25 | Loss: 0.00106018
Iteration 8/25 | Loss: 0.00106018
Iteration 9/25 | Loss: 0.00106018
Iteration 10/25 | Loss: 0.00106018
Iteration 11/25 | Loss: 0.00106018
Iteration 12/25 | Loss: 0.00106018
Iteration 13/25 | Loss: 0.00106018
Iteration 14/25 | Loss: 0.00106018
Iteration 15/25 | Loss: 0.00106018
Iteration 16/25 | Loss: 0.00106018
Iteration 17/25 | Loss: 0.00106018
Iteration 18/25 | Loss: 0.00106018
Iteration 19/25 | Loss: 0.00106018
Iteration 20/25 | Loss: 0.00106018
Iteration 21/25 | Loss: 0.00106018
Iteration 22/25 | Loss: 0.00106018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010601781541481614, 0.0010601781541481614, 0.0010601781541481614, 0.0010601781541481614, 0.0010601781541481614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010601781541481614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106018
Iteration 2/1000 | Loss: 0.00005157
Iteration 3/1000 | Loss: 0.00003138
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001709
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001681
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001680
Iteration 23/1000 | Loss: 0.00001680
Iteration 24/1000 | Loss: 0.00001680
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001679
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001678
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001677
Iteration 33/1000 | Loss: 0.00001676
Iteration 34/1000 | Loss: 0.00001676
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001674
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001673
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001671
Iteration 47/1000 | Loss: 0.00001671
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00001669
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001668
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001662
Iteration 71/1000 | Loss: 0.00001662
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001661
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001660
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001659
Iteration 83/1000 | Loss: 0.00001659
Iteration 84/1000 | Loss: 0.00001659
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001655
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001655
Iteration 102/1000 | Loss: 0.00001655
Iteration 103/1000 | Loss: 0.00001655
Iteration 104/1000 | Loss: 0.00001655
Iteration 105/1000 | Loss: 0.00001655
Iteration 106/1000 | Loss: 0.00001655
Iteration 107/1000 | Loss: 0.00001655
Iteration 108/1000 | Loss: 0.00001654
Iteration 109/1000 | Loss: 0.00001654
Iteration 110/1000 | Loss: 0.00001654
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001654
Iteration 114/1000 | Loss: 0.00001653
Iteration 115/1000 | Loss: 0.00001653
Iteration 116/1000 | Loss: 0.00001653
Iteration 117/1000 | Loss: 0.00001653
Iteration 118/1000 | Loss: 0.00001653
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00001653
Iteration 123/1000 | Loss: 0.00001652
Iteration 124/1000 | Loss: 0.00001652
Iteration 125/1000 | Loss: 0.00001652
Iteration 126/1000 | Loss: 0.00001652
Iteration 127/1000 | Loss: 0.00001652
Iteration 128/1000 | Loss: 0.00001652
Iteration 129/1000 | Loss: 0.00001652
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.6522024452569894e-05, 1.6522024452569894e-05, 1.6522024452569894e-05, 1.6522024452569894e-05, 1.6522024452569894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6522024452569894e-05

Optimization complete. Final v2v error: 3.390291690826416 mm

Highest mean error: 4.4388203620910645 mm for frame 55

Lowest mean error: 2.7683794498443604 mm for frame 213

Saving results

Total time: 42.9453706741333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802456
Iteration 2/25 | Loss: 0.00181047
Iteration 3/25 | Loss: 0.00120980
Iteration 4/25 | Loss: 0.00114607
Iteration 5/25 | Loss: 0.00115061
Iteration 6/25 | Loss: 0.00112433
Iteration 7/25 | Loss: 0.00109088
Iteration 8/25 | Loss: 0.00107549
Iteration 9/25 | Loss: 0.00106472
Iteration 10/25 | Loss: 0.00106689
Iteration 11/25 | Loss: 0.00106294
Iteration 12/25 | Loss: 0.00106141
Iteration 13/25 | Loss: 0.00106108
Iteration 14/25 | Loss: 0.00106101
Iteration 15/25 | Loss: 0.00106101
Iteration 16/25 | Loss: 0.00106101
Iteration 17/25 | Loss: 0.00106100
Iteration 18/25 | Loss: 0.00106100
Iteration 19/25 | Loss: 0.00106100
Iteration 20/25 | Loss: 0.00106100
Iteration 21/25 | Loss: 0.00106100
Iteration 22/25 | Loss: 0.00106100
Iteration 23/25 | Loss: 0.00106100
Iteration 24/25 | Loss: 0.00106100
Iteration 25/25 | Loss: 0.00106100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.81225681
Iteration 2/25 | Loss: 0.00060381
Iteration 3/25 | Loss: 0.00060359
Iteration 4/25 | Loss: 0.00060359
Iteration 5/25 | Loss: 0.00060359
Iteration 6/25 | Loss: 0.00060359
Iteration 7/25 | Loss: 0.00060359
Iteration 8/25 | Loss: 0.00060359
Iteration 9/25 | Loss: 0.00060358
Iteration 10/25 | Loss: 0.00060358
Iteration 11/25 | Loss: 0.00060358
Iteration 12/25 | Loss: 0.00060358
Iteration 13/25 | Loss: 0.00060358
Iteration 14/25 | Loss: 0.00060358
Iteration 15/25 | Loss: 0.00060358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006035846308805048, 0.0006035846308805048, 0.0006035846308805048, 0.0006035846308805048, 0.0006035846308805048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006035846308805048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060358
Iteration 2/1000 | Loss: 0.00003425
Iteration 3/1000 | Loss: 0.00002334
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001879
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00012392
Iteration 8/1000 | Loss: 0.00001920
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001604
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001571
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001570
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001549
Iteration 39/1000 | Loss: 0.00001549
Iteration 40/1000 | Loss: 0.00001549
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001545
Iteration 57/1000 | Loss: 0.00001545
Iteration 58/1000 | Loss: 0.00001545
Iteration 59/1000 | Loss: 0.00001545
Iteration 60/1000 | Loss: 0.00001545
Iteration 61/1000 | Loss: 0.00001544
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001544
Iteration 64/1000 | Loss: 0.00001544
Iteration 65/1000 | Loss: 0.00001544
Iteration 66/1000 | Loss: 0.00001543
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001543
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001542
Iteration 107/1000 | Loss: 0.00001542
Iteration 108/1000 | Loss: 0.00001542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.5418745533679612e-05, 1.5418745533679612e-05, 1.5418745533679612e-05, 1.5418745533679612e-05, 1.5418745533679612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5418745533679612e-05

Optimization complete. Final v2v error: 3.296684980392456 mm

Highest mean error: 4.207518100738525 mm for frame 223

Lowest mean error: 3.086214303970337 mm for frame 73

Saving results

Total time: 60.20373272895813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2947/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2947/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496765
Iteration 2/25 | Loss: 0.00115447
Iteration 3/25 | Loss: 0.00105935
Iteration 4/25 | Loss: 0.00104414
Iteration 5/25 | Loss: 0.00103789
Iteration 6/25 | Loss: 0.00103727
Iteration 7/25 | Loss: 0.00103727
Iteration 8/25 | Loss: 0.00103727
Iteration 9/25 | Loss: 0.00103727
Iteration 10/25 | Loss: 0.00103727
Iteration 11/25 | Loss: 0.00103727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010372725082561374, 0.0010372725082561374, 0.0010372725082561374, 0.0010372725082561374, 0.0010372725082561374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010372725082561374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.26688337
Iteration 2/25 | Loss: 0.00089688
Iteration 3/25 | Loss: 0.00089688
Iteration 4/25 | Loss: 0.00089688
Iteration 5/25 | Loss: 0.00089688
Iteration 6/25 | Loss: 0.00089688
Iteration 7/25 | Loss: 0.00089688
Iteration 8/25 | Loss: 0.00089688
Iteration 9/25 | Loss: 0.00089688
Iteration 10/25 | Loss: 0.00089688
Iteration 11/25 | Loss: 0.00089688
Iteration 12/25 | Loss: 0.00089688
Iteration 13/25 | Loss: 0.00089688
Iteration 14/25 | Loss: 0.00089688
Iteration 15/25 | Loss: 0.00089688
Iteration 16/25 | Loss: 0.00089688
Iteration 17/25 | Loss: 0.00089688
Iteration 18/25 | Loss: 0.00089688
Iteration 19/25 | Loss: 0.00089688
Iteration 20/25 | Loss: 0.00089688
Iteration 21/25 | Loss: 0.00089688
Iteration 22/25 | Loss: 0.00089688
Iteration 23/25 | Loss: 0.00089688
Iteration 24/25 | Loss: 0.00089688
Iteration 25/25 | Loss: 0.00089688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089688
Iteration 2/1000 | Loss: 0.00003432
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001651
Iteration 6/1000 | Loss: 0.00001576
Iteration 7/1000 | Loss: 0.00001516
Iteration 8/1000 | Loss: 0.00001479
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001442
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001428
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001425
Iteration 18/1000 | Loss: 0.00001425
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001419
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001413
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001376
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001371
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001370
Iteration 58/1000 | Loss: 0.00001369
Iteration 59/1000 | Loss: 0.00001369
Iteration 60/1000 | Loss: 0.00001369
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001369
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001367
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001366
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001365
Iteration 91/1000 | Loss: 0.00001365
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001364
Iteration 106/1000 | Loss: 0.00001364
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001360
Iteration 121/1000 | Loss: 0.00001360
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001359
Iteration 134/1000 | Loss: 0.00001359
Iteration 135/1000 | Loss: 0.00001359
Iteration 136/1000 | Loss: 0.00001359
Iteration 137/1000 | Loss: 0.00001359
Iteration 138/1000 | Loss: 0.00001359
Iteration 139/1000 | Loss: 0.00001359
Iteration 140/1000 | Loss: 0.00001359
Iteration 141/1000 | Loss: 0.00001359
Iteration 142/1000 | Loss: 0.00001359
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Iteration 145/1000 | Loss: 0.00001359
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001359
Iteration 158/1000 | Loss: 0.00001359
Iteration 159/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.3585497981694061e-05, 1.3585497981694061e-05, 1.3585497981694061e-05, 1.3585497981694061e-05, 1.3585497981694061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3585497981694061e-05

Optimization complete. Final v2v error: 3.215099811553955 mm

Highest mean error: 3.4449517726898193 mm for frame 170

Lowest mean error: 2.7258291244506836 mm for frame 5

Saving results

Total time: 42.48398041725159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902176
Iteration 2/25 | Loss: 0.00162588
Iteration 3/25 | Loss: 0.00156805
Iteration 4/25 | Loss: 0.00155874
Iteration 5/25 | Loss: 0.00155519
Iteration 6/25 | Loss: 0.00155492
Iteration 7/25 | Loss: 0.00155492
Iteration 8/25 | Loss: 0.00155492
Iteration 9/25 | Loss: 0.00155492
Iteration 10/25 | Loss: 0.00155492
Iteration 11/25 | Loss: 0.00155492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015549175441265106, 0.0015549175441265106, 0.0015549175441265106, 0.0015549175441265106, 0.0015549175441265106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015549175441265106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.77563953
Iteration 2/25 | Loss: 0.00310477
Iteration 3/25 | Loss: 0.00310475
Iteration 4/25 | Loss: 0.00310475
Iteration 5/25 | Loss: 0.00310475
Iteration 6/25 | Loss: 0.00310475
Iteration 7/25 | Loss: 0.00310475
Iteration 8/25 | Loss: 0.00310475
Iteration 9/25 | Loss: 0.00310475
Iteration 10/25 | Loss: 0.00310475
Iteration 11/25 | Loss: 0.00310475
Iteration 12/25 | Loss: 0.00310475
Iteration 13/25 | Loss: 0.00310475
Iteration 14/25 | Loss: 0.00310475
Iteration 15/25 | Loss: 0.00310475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0031047463417053223, 0.0031047463417053223, 0.0031047463417053223, 0.0031047463417053223, 0.0031047463417053223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031047463417053223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310475
Iteration 2/1000 | Loss: 0.00003899
Iteration 3/1000 | Loss: 0.00002959
Iteration 4/1000 | Loss: 0.00002598
Iteration 5/1000 | Loss: 0.00002410
Iteration 6/1000 | Loss: 0.00002276
Iteration 7/1000 | Loss: 0.00002212
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002149
Iteration 10/1000 | Loss: 0.00002146
Iteration 11/1000 | Loss: 0.00002141
Iteration 12/1000 | Loss: 0.00002140
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00002122
Iteration 16/1000 | Loss: 0.00002122
Iteration 17/1000 | Loss: 0.00002118
Iteration 18/1000 | Loss: 0.00002118
Iteration 19/1000 | Loss: 0.00002113
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002110
Iteration 22/1000 | Loss: 0.00002110
Iteration 23/1000 | Loss: 0.00002109
Iteration 24/1000 | Loss: 0.00002109
Iteration 25/1000 | Loss: 0.00002109
Iteration 26/1000 | Loss: 0.00002109
Iteration 27/1000 | Loss: 0.00002109
Iteration 28/1000 | Loss: 0.00002109
Iteration 29/1000 | Loss: 0.00002109
Iteration 30/1000 | Loss: 0.00002109
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002107
Iteration 33/1000 | Loss: 0.00002107
Iteration 34/1000 | Loss: 0.00002106
Iteration 35/1000 | Loss: 0.00002105
Iteration 36/1000 | Loss: 0.00002105
Iteration 37/1000 | Loss: 0.00002104
Iteration 38/1000 | Loss: 0.00002104
Iteration 39/1000 | Loss: 0.00002104
Iteration 40/1000 | Loss: 0.00002104
Iteration 41/1000 | Loss: 0.00002103
Iteration 42/1000 | Loss: 0.00002102
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002102
Iteration 45/1000 | Loss: 0.00002102
Iteration 46/1000 | Loss: 0.00002102
Iteration 47/1000 | Loss: 0.00002102
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00002102
Iteration 50/1000 | Loss: 0.00002102
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002102
Iteration 53/1000 | Loss: 0.00002102
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002101
Iteration 56/1000 | Loss: 0.00002101
Iteration 57/1000 | Loss: 0.00002101
Iteration 58/1000 | Loss: 0.00002101
Iteration 59/1000 | Loss: 0.00002100
Iteration 60/1000 | Loss: 0.00002100
Iteration 61/1000 | Loss: 0.00002100
Iteration 62/1000 | Loss: 0.00002100
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002100
Iteration 65/1000 | Loss: 0.00002100
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002099
Iteration 69/1000 | Loss: 0.00002099
Iteration 70/1000 | Loss: 0.00002099
Iteration 71/1000 | Loss: 0.00002099
Iteration 72/1000 | Loss: 0.00002099
Iteration 73/1000 | Loss: 0.00002099
Iteration 74/1000 | Loss: 0.00002098
Iteration 75/1000 | Loss: 0.00002098
Iteration 76/1000 | Loss: 0.00002098
Iteration 77/1000 | Loss: 0.00002098
Iteration 78/1000 | Loss: 0.00002098
Iteration 79/1000 | Loss: 0.00002098
Iteration 80/1000 | Loss: 0.00002098
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002098
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002097
Iteration 86/1000 | Loss: 0.00002097
Iteration 87/1000 | Loss: 0.00002097
Iteration 88/1000 | Loss: 0.00002097
Iteration 89/1000 | Loss: 0.00002097
Iteration 90/1000 | Loss: 0.00002097
Iteration 91/1000 | Loss: 0.00002097
Iteration 92/1000 | Loss: 0.00002097
Iteration 93/1000 | Loss: 0.00002097
Iteration 94/1000 | Loss: 0.00002097
Iteration 95/1000 | Loss: 0.00002097
Iteration 96/1000 | Loss: 0.00002097
Iteration 97/1000 | Loss: 0.00002097
Iteration 98/1000 | Loss: 0.00002097
Iteration 99/1000 | Loss: 0.00002097
Iteration 100/1000 | Loss: 0.00002097
Iteration 101/1000 | Loss: 0.00002097
Iteration 102/1000 | Loss: 0.00002096
Iteration 103/1000 | Loss: 0.00002096
Iteration 104/1000 | Loss: 0.00002096
Iteration 105/1000 | Loss: 0.00002096
Iteration 106/1000 | Loss: 0.00002096
Iteration 107/1000 | Loss: 0.00002096
Iteration 108/1000 | Loss: 0.00002096
Iteration 109/1000 | Loss: 0.00002096
Iteration 110/1000 | Loss: 0.00002096
Iteration 111/1000 | Loss: 0.00002096
Iteration 112/1000 | Loss: 0.00002096
Iteration 113/1000 | Loss: 0.00002096
Iteration 114/1000 | Loss: 0.00002096
Iteration 115/1000 | Loss: 0.00002095
Iteration 116/1000 | Loss: 0.00002095
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002095
Iteration 120/1000 | Loss: 0.00002095
Iteration 121/1000 | Loss: 0.00002095
Iteration 122/1000 | Loss: 0.00002095
Iteration 123/1000 | Loss: 0.00002095
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002095
Iteration 126/1000 | Loss: 0.00002095
Iteration 127/1000 | Loss: 0.00002095
Iteration 128/1000 | Loss: 0.00002095
Iteration 129/1000 | Loss: 0.00002095
Iteration 130/1000 | Loss: 0.00002095
Iteration 131/1000 | Loss: 0.00002095
Iteration 132/1000 | Loss: 0.00002095
Iteration 133/1000 | Loss: 0.00002095
Iteration 134/1000 | Loss: 0.00002095
Iteration 135/1000 | Loss: 0.00002095
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002095
Iteration 138/1000 | Loss: 0.00002095
Iteration 139/1000 | Loss: 0.00002095
Iteration 140/1000 | Loss: 0.00002095
Iteration 141/1000 | Loss: 0.00002095
Iteration 142/1000 | Loss: 0.00002095
Iteration 143/1000 | Loss: 0.00002095
Iteration 144/1000 | Loss: 0.00002095
Iteration 145/1000 | Loss: 0.00002095
Iteration 146/1000 | Loss: 0.00002095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.095125091727823e-05, 2.095125091727823e-05, 2.095125091727823e-05, 2.095125091727823e-05, 2.095125091727823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.095125091727823e-05

Optimization complete. Final v2v error: 3.8029263019561768 mm

Highest mean error: 4.354209899902344 mm for frame 142

Lowest mean error: 3.358525514602661 mm for frame 21

Saving results

Total time: 37.141928911209106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923322
Iteration 2/25 | Loss: 0.00174854
Iteration 3/25 | Loss: 0.00160065
Iteration 4/25 | Loss: 0.00158846
Iteration 5/25 | Loss: 0.00158578
Iteration 6/25 | Loss: 0.00158578
Iteration 7/25 | Loss: 0.00158578
Iteration 8/25 | Loss: 0.00158578
Iteration 9/25 | Loss: 0.00158578
Iteration 10/25 | Loss: 0.00158578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001585784018971026, 0.001585784018971026, 0.001585784018971026, 0.001585784018971026, 0.001585784018971026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001585784018971026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76106274
Iteration 2/25 | Loss: 0.00176404
Iteration 3/25 | Loss: 0.00176404
Iteration 4/25 | Loss: 0.00176404
Iteration 5/25 | Loss: 0.00176404
Iteration 6/25 | Loss: 0.00176404
Iteration 7/25 | Loss: 0.00176404
Iteration 8/25 | Loss: 0.00176404
Iteration 9/25 | Loss: 0.00176404
Iteration 10/25 | Loss: 0.00176404
Iteration 11/25 | Loss: 0.00176404
Iteration 12/25 | Loss: 0.00176404
Iteration 13/25 | Loss: 0.00176404
Iteration 14/25 | Loss: 0.00176404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017640391597524285, 0.0017640391597524285, 0.0017640391597524285, 0.0017640391597524285, 0.0017640391597524285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017640391597524285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176404
Iteration 2/1000 | Loss: 0.00004664
Iteration 3/1000 | Loss: 0.00003336
Iteration 4/1000 | Loss: 0.00002847
Iteration 5/1000 | Loss: 0.00002650
Iteration 6/1000 | Loss: 0.00002547
Iteration 7/1000 | Loss: 0.00002499
Iteration 8/1000 | Loss: 0.00002465
Iteration 9/1000 | Loss: 0.00002430
Iteration 10/1000 | Loss: 0.00002411
Iteration 11/1000 | Loss: 0.00002409
Iteration 12/1000 | Loss: 0.00002399
Iteration 13/1000 | Loss: 0.00002398
Iteration 14/1000 | Loss: 0.00002393
Iteration 15/1000 | Loss: 0.00002390
Iteration 16/1000 | Loss: 0.00002390
Iteration 17/1000 | Loss: 0.00002390
Iteration 18/1000 | Loss: 0.00002390
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002388
Iteration 23/1000 | Loss: 0.00002388
Iteration 24/1000 | Loss: 0.00002387
Iteration 25/1000 | Loss: 0.00002386
Iteration 26/1000 | Loss: 0.00002386
Iteration 27/1000 | Loss: 0.00002386
Iteration 28/1000 | Loss: 0.00002386
Iteration 29/1000 | Loss: 0.00002386
Iteration 30/1000 | Loss: 0.00002386
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002385
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002385
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00002384
Iteration 40/1000 | Loss: 0.00002384
Iteration 41/1000 | Loss: 0.00002384
Iteration 42/1000 | Loss: 0.00002384
Iteration 43/1000 | Loss: 0.00002384
Iteration 44/1000 | Loss: 0.00002383
Iteration 45/1000 | Loss: 0.00002383
Iteration 46/1000 | Loss: 0.00002383
Iteration 47/1000 | Loss: 0.00002383
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002382
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002382
Iteration 53/1000 | Loss: 0.00002382
Iteration 54/1000 | Loss: 0.00002382
Iteration 55/1000 | Loss: 0.00002380
Iteration 56/1000 | Loss: 0.00002380
Iteration 57/1000 | Loss: 0.00002380
Iteration 58/1000 | Loss: 0.00002379
Iteration 59/1000 | Loss: 0.00002379
Iteration 60/1000 | Loss: 0.00002379
Iteration 61/1000 | Loss: 0.00002379
Iteration 62/1000 | Loss: 0.00002379
Iteration 63/1000 | Loss: 0.00002379
Iteration 64/1000 | Loss: 0.00002379
Iteration 65/1000 | Loss: 0.00002379
Iteration 66/1000 | Loss: 0.00002378
Iteration 67/1000 | Loss: 0.00002378
Iteration 68/1000 | Loss: 0.00002378
Iteration 69/1000 | Loss: 0.00002378
Iteration 70/1000 | Loss: 0.00002378
Iteration 71/1000 | Loss: 0.00002377
Iteration 72/1000 | Loss: 0.00002377
Iteration 73/1000 | Loss: 0.00002377
Iteration 74/1000 | Loss: 0.00002377
Iteration 75/1000 | Loss: 0.00002377
Iteration 76/1000 | Loss: 0.00002377
Iteration 77/1000 | Loss: 0.00002377
Iteration 78/1000 | Loss: 0.00002377
Iteration 79/1000 | Loss: 0.00002377
Iteration 80/1000 | Loss: 0.00002377
Iteration 81/1000 | Loss: 0.00002377
Iteration 82/1000 | Loss: 0.00002377
Iteration 83/1000 | Loss: 0.00002377
Iteration 84/1000 | Loss: 0.00002376
Iteration 85/1000 | Loss: 0.00002376
Iteration 86/1000 | Loss: 0.00002376
Iteration 87/1000 | Loss: 0.00002376
Iteration 88/1000 | Loss: 0.00002376
Iteration 89/1000 | Loss: 0.00002376
Iteration 90/1000 | Loss: 0.00002376
Iteration 91/1000 | Loss: 0.00002376
Iteration 92/1000 | Loss: 0.00002376
Iteration 93/1000 | Loss: 0.00002376
Iteration 94/1000 | Loss: 0.00002376
Iteration 95/1000 | Loss: 0.00002376
Iteration 96/1000 | Loss: 0.00002376
Iteration 97/1000 | Loss: 0.00002376
Iteration 98/1000 | Loss: 0.00002376
Iteration 99/1000 | Loss: 0.00002376
Iteration 100/1000 | Loss: 0.00002376
Iteration 101/1000 | Loss: 0.00002376
Iteration 102/1000 | Loss: 0.00002376
Iteration 103/1000 | Loss: 0.00002376
Iteration 104/1000 | Loss: 0.00002376
Iteration 105/1000 | Loss: 0.00002376
Iteration 106/1000 | Loss: 0.00002376
Iteration 107/1000 | Loss: 0.00002376
Iteration 108/1000 | Loss: 0.00002376
Iteration 109/1000 | Loss: 0.00002376
Iteration 110/1000 | Loss: 0.00002376
Iteration 111/1000 | Loss: 0.00002376
Iteration 112/1000 | Loss: 0.00002376
Iteration 113/1000 | Loss: 0.00002376
Iteration 114/1000 | Loss: 0.00002376
Iteration 115/1000 | Loss: 0.00002376
Iteration 116/1000 | Loss: 0.00002376
Iteration 117/1000 | Loss: 0.00002376
Iteration 118/1000 | Loss: 0.00002376
Iteration 119/1000 | Loss: 0.00002376
Iteration 120/1000 | Loss: 0.00002376
Iteration 121/1000 | Loss: 0.00002376
Iteration 122/1000 | Loss: 0.00002376
Iteration 123/1000 | Loss: 0.00002376
Iteration 124/1000 | Loss: 0.00002376
Iteration 125/1000 | Loss: 0.00002376
Iteration 126/1000 | Loss: 0.00002376
Iteration 127/1000 | Loss: 0.00002376
Iteration 128/1000 | Loss: 0.00002376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.3760640033287928e-05, 2.3760640033287928e-05, 2.3760640033287928e-05, 2.3760640033287928e-05, 2.3760640033287928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3760640033287928e-05

Optimization complete. Final v2v error: 4.110733509063721 mm

Highest mean error: 4.278579235076904 mm for frame 239

Lowest mean error: 4.023197650909424 mm for frame 35

Saving results

Total time: 33.462873697280884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740127
Iteration 2/25 | Loss: 0.00173103
Iteration 3/25 | Loss: 0.00162451
Iteration 4/25 | Loss: 0.00159333
Iteration 5/25 | Loss: 0.00155646
Iteration 6/25 | Loss: 0.00156265
Iteration 7/25 | Loss: 0.00154924
Iteration 8/25 | Loss: 0.00153498
Iteration 9/25 | Loss: 0.00153924
Iteration 10/25 | Loss: 0.00153390
Iteration 11/25 | Loss: 0.00153458
Iteration 12/25 | Loss: 0.00153482
Iteration 13/25 | Loss: 0.00153375
Iteration 14/25 | Loss: 0.00153594
Iteration 15/25 | Loss: 0.00153454
Iteration 16/25 | Loss: 0.00153493
Iteration 17/25 | Loss: 0.00153451
Iteration 18/25 | Loss: 0.00153438
Iteration 19/25 | Loss: 0.00153460
Iteration 20/25 | Loss: 0.00153470
Iteration 21/25 | Loss: 0.00153368
Iteration 22/25 | Loss: 0.00153408
Iteration 23/25 | Loss: 0.00153464
Iteration 24/25 | Loss: 0.00153335
Iteration 25/25 | Loss: 0.00153154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45987201
Iteration 2/25 | Loss: 0.00327513
Iteration 3/25 | Loss: 0.00311182
Iteration 4/25 | Loss: 0.00311181
Iteration 5/25 | Loss: 0.00311181
Iteration 6/25 | Loss: 0.00311181
Iteration 7/25 | Loss: 0.00311181
Iteration 8/25 | Loss: 0.00311181
Iteration 9/25 | Loss: 0.00311181
Iteration 10/25 | Loss: 0.00311181
Iteration 11/25 | Loss: 0.00311181
Iteration 12/25 | Loss: 0.00311181
Iteration 13/25 | Loss: 0.00311181
Iteration 14/25 | Loss: 0.00311181
Iteration 15/25 | Loss: 0.00311181
Iteration 16/25 | Loss: 0.00311181
Iteration 17/25 | Loss: 0.00311181
Iteration 18/25 | Loss: 0.00311181
Iteration 19/25 | Loss: 0.00311181
Iteration 20/25 | Loss: 0.00311181
Iteration 21/25 | Loss: 0.00311181
Iteration 22/25 | Loss: 0.00311181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.003111810889095068, 0.003111810889095068, 0.003111810889095068, 0.003111810889095068, 0.003111810889095068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003111810889095068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311181
Iteration 2/1000 | Loss: 0.00013561
Iteration 3/1000 | Loss: 0.00009126
Iteration 4/1000 | Loss: 0.00008854
Iteration 5/1000 | Loss: 0.00008486
Iteration 6/1000 | Loss: 0.00008938
Iteration 7/1000 | Loss: 0.00008234
Iteration 8/1000 | Loss: 0.00007132
Iteration 9/1000 | Loss: 0.00003859
Iteration 10/1000 | Loss: 0.00002368
Iteration 11/1000 | Loss: 0.00002257
Iteration 12/1000 | Loss: 0.00010496
Iteration 13/1000 | Loss: 0.00006431
Iteration 14/1000 | Loss: 0.00002410
Iteration 15/1000 | Loss: 0.00009737
Iteration 16/1000 | Loss: 0.00010201
Iteration 17/1000 | Loss: 0.00011202
Iteration 18/1000 | Loss: 0.00011600
Iteration 19/1000 | Loss: 0.00008384
Iteration 20/1000 | Loss: 0.00014648
Iteration 21/1000 | Loss: 0.00008532
Iteration 22/1000 | Loss: 0.00008814
Iteration 23/1000 | Loss: 0.00006824
Iteration 24/1000 | Loss: 0.00002279
Iteration 25/1000 | Loss: 0.00002201
Iteration 26/1000 | Loss: 0.00011066
Iteration 27/1000 | Loss: 0.00016229
Iteration 28/1000 | Loss: 0.00008268
Iteration 29/1000 | Loss: 0.00002456
Iteration 30/1000 | Loss: 0.00008836
Iteration 31/1000 | Loss: 0.00007827
Iteration 32/1000 | Loss: 0.00008195
Iteration 33/1000 | Loss: 0.00006898
Iteration 34/1000 | Loss: 0.00007690
Iteration 35/1000 | Loss: 0.00007044
Iteration 36/1000 | Loss: 0.00006959
Iteration 37/1000 | Loss: 0.00004897
Iteration 38/1000 | Loss: 0.00006433
Iteration 39/1000 | Loss: 0.00004363
Iteration 40/1000 | Loss: 0.00004538
Iteration 41/1000 | Loss: 0.00004045
Iteration 42/1000 | Loss: 0.00003587
Iteration 43/1000 | Loss: 0.00006352
Iteration 44/1000 | Loss: 0.00017527
Iteration 45/1000 | Loss: 0.00013642
Iteration 46/1000 | Loss: 0.00004466
Iteration 47/1000 | Loss: 0.00009241
Iteration 48/1000 | Loss: 0.00008074
Iteration 49/1000 | Loss: 0.00010710
Iteration 50/1000 | Loss: 0.00011583
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00006715
Iteration 53/1000 | Loss: 0.00008828
Iteration 54/1000 | Loss: 0.00009859
Iteration 55/1000 | Loss: 0.00011492
Iteration 56/1000 | Loss: 0.00008561
Iteration 57/1000 | Loss: 0.00010531
Iteration 58/1000 | Loss: 0.00006822
Iteration 59/1000 | Loss: 0.00002367
Iteration 60/1000 | Loss: 0.00002876
Iteration 61/1000 | Loss: 0.00002121
Iteration 62/1000 | Loss: 0.00002037
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001963
Iteration 68/1000 | Loss: 0.00001958
Iteration 69/1000 | Loss: 0.00001958
Iteration 70/1000 | Loss: 0.00001957
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001928
Iteration 79/1000 | Loss: 0.00001928
Iteration 80/1000 | Loss: 0.00001928
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001926
Iteration 90/1000 | Loss: 0.00001926
Iteration 91/1000 | Loss: 0.00001926
Iteration 92/1000 | Loss: 0.00001926
Iteration 93/1000 | Loss: 0.00001925
Iteration 94/1000 | Loss: 0.00001925
Iteration 95/1000 | Loss: 0.00001925
Iteration 96/1000 | Loss: 0.00001925
Iteration 97/1000 | Loss: 0.00001925
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001924
Iteration 103/1000 | Loss: 0.00001924
Iteration 104/1000 | Loss: 0.00001924
Iteration 105/1000 | Loss: 0.00001924
Iteration 106/1000 | Loss: 0.00001924
Iteration 107/1000 | Loss: 0.00001924
Iteration 108/1000 | Loss: 0.00001923
Iteration 109/1000 | Loss: 0.00001923
Iteration 110/1000 | Loss: 0.00001923
Iteration 111/1000 | Loss: 0.00001923
Iteration 112/1000 | Loss: 0.00001923
Iteration 113/1000 | Loss: 0.00001922
Iteration 114/1000 | Loss: 0.00001922
Iteration 115/1000 | Loss: 0.00001922
Iteration 116/1000 | Loss: 0.00001922
Iteration 117/1000 | Loss: 0.00001921
Iteration 118/1000 | Loss: 0.00001921
Iteration 119/1000 | Loss: 0.00001921
Iteration 120/1000 | Loss: 0.00001921
Iteration 121/1000 | Loss: 0.00001921
Iteration 122/1000 | Loss: 0.00001921
Iteration 123/1000 | Loss: 0.00001921
Iteration 124/1000 | Loss: 0.00001920
Iteration 125/1000 | Loss: 0.00001920
Iteration 126/1000 | Loss: 0.00001919
Iteration 127/1000 | Loss: 0.00003912
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001917
Iteration 132/1000 | Loss: 0.00001917
Iteration 133/1000 | Loss: 0.00001917
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001915
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00003060
Iteration 141/1000 | Loss: 0.00001918
Iteration 142/1000 | Loss: 0.00001917
Iteration 143/1000 | Loss: 0.00001917
Iteration 144/1000 | Loss: 0.00001917
Iteration 145/1000 | Loss: 0.00001917
Iteration 146/1000 | Loss: 0.00001917
Iteration 147/1000 | Loss: 0.00001917
Iteration 148/1000 | Loss: 0.00001917
Iteration 149/1000 | Loss: 0.00001917
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001917
Iteration 152/1000 | Loss: 0.00001917
Iteration 153/1000 | Loss: 0.00001917
Iteration 154/1000 | Loss: 0.00001917
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001916
Iteration 160/1000 | Loss: 0.00001916
Iteration 161/1000 | Loss: 0.00001916
Iteration 162/1000 | Loss: 0.00001916
Iteration 163/1000 | Loss: 0.00001916
Iteration 164/1000 | Loss: 0.00001915
Iteration 165/1000 | Loss: 0.00001915
Iteration 166/1000 | Loss: 0.00001915
Iteration 167/1000 | Loss: 0.00001915
Iteration 168/1000 | Loss: 0.00001915
Iteration 169/1000 | Loss: 0.00001915
Iteration 170/1000 | Loss: 0.00001915
Iteration 171/1000 | Loss: 0.00001914
Iteration 172/1000 | Loss: 0.00001914
Iteration 173/1000 | Loss: 0.00001914
Iteration 174/1000 | Loss: 0.00001914
Iteration 175/1000 | Loss: 0.00001914
Iteration 176/1000 | Loss: 0.00001914
Iteration 177/1000 | Loss: 0.00001914
Iteration 178/1000 | Loss: 0.00001914
Iteration 179/1000 | Loss: 0.00003932
Iteration 180/1000 | Loss: 0.00001915
Iteration 181/1000 | Loss: 0.00001915
Iteration 182/1000 | Loss: 0.00001915
Iteration 183/1000 | Loss: 0.00001915
Iteration 184/1000 | Loss: 0.00001915
Iteration 185/1000 | Loss: 0.00001915
Iteration 186/1000 | Loss: 0.00001915
Iteration 187/1000 | Loss: 0.00001915
Iteration 188/1000 | Loss: 0.00001915
Iteration 189/1000 | Loss: 0.00001915
Iteration 190/1000 | Loss: 0.00001915
Iteration 191/1000 | Loss: 0.00001915
Iteration 192/1000 | Loss: 0.00001914
Iteration 193/1000 | Loss: 0.00001914
Iteration 194/1000 | Loss: 0.00001914
Iteration 195/1000 | Loss: 0.00001914
Iteration 196/1000 | Loss: 0.00001914
Iteration 197/1000 | Loss: 0.00001914
Iteration 198/1000 | Loss: 0.00001914
Iteration 199/1000 | Loss: 0.00001914
Iteration 200/1000 | Loss: 0.00001914
Iteration 201/1000 | Loss: 0.00001914
Iteration 202/1000 | Loss: 0.00001914
Iteration 203/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.9136165064992383e-05, 1.9136165064992383e-05, 1.9136165064992383e-05, 1.9136165064992383e-05, 1.9136165064992383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9136165064992383e-05

Optimization complete. Final v2v error: 3.7254035472869873 mm

Highest mean error: 9.720793724060059 mm for frame 140

Lowest mean error: 3.3490898609161377 mm for frame 0

Saving results

Total time: 180.874751329422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102099
Iteration 2/25 | Loss: 0.00359889
Iteration 3/25 | Loss: 0.00318922
Iteration 4/25 | Loss: 0.00310863
Iteration 5/25 | Loss: 0.00296562
Iteration 6/25 | Loss: 0.00286541
Iteration 7/25 | Loss: 0.00279440
Iteration 8/25 | Loss: 0.00274270
Iteration 9/25 | Loss: 0.00272382
Iteration 10/25 | Loss: 0.00265071
Iteration 11/25 | Loss: 0.00261363
Iteration 12/25 | Loss: 0.00260003
Iteration 13/25 | Loss: 0.00259485
Iteration 14/25 | Loss: 0.00260113
Iteration 15/25 | Loss: 0.00262614
Iteration 16/25 | Loss: 0.00264212
Iteration 17/25 | Loss: 0.00260243
Iteration 18/25 | Loss: 0.00253221
Iteration 19/25 | Loss: 0.00248475
Iteration 20/25 | Loss: 0.00247139
Iteration 21/25 | Loss: 0.00246616
Iteration 22/25 | Loss: 0.00246200
Iteration 23/25 | Loss: 0.00246503
Iteration 24/25 | Loss: 0.00247200
Iteration 25/25 | Loss: 0.00246832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06042492
Iteration 2/25 | Loss: 0.00666783
Iteration 3/25 | Loss: 0.00666768
Iteration 4/25 | Loss: 0.00666768
Iteration 5/25 | Loss: 0.00666768
Iteration 6/25 | Loss: 0.00666768
Iteration 7/25 | Loss: 0.00666768
Iteration 8/25 | Loss: 0.00666768
Iteration 9/25 | Loss: 0.00666768
Iteration 10/25 | Loss: 0.00666768
Iteration 11/25 | Loss: 0.00666768
Iteration 12/25 | Loss: 0.00666768
Iteration 13/25 | Loss: 0.00666768
Iteration 14/25 | Loss: 0.00666768
Iteration 15/25 | Loss: 0.00666768
Iteration 16/25 | Loss: 0.00666768
Iteration 17/25 | Loss: 0.00666768
Iteration 18/25 | Loss: 0.00666768
Iteration 19/25 | Loss: 0.00666768
Iteration 20/25 | Loss: 0.00666768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.006667676381766796, 0.006667676381766796, 0.006667676381766796, 0.006667676381766796, 0.006667676381766796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006667676381766796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00666768
Iteration 2/1000 | Loss: 0.01914530
Iteration 3/1000 | Loss: 0.01169598
Iteration 4/1000 | Loss: 0.00550477
Iteration 5/1000 | Loss: 0.00463850
Iteration 6/1000 | Loss: 0.00215714
Iteration 7/1000 | Loss: 0.00098214
Iteration 8/1000 | Loss: 0.00190845
Iteration 9/1000 | Loss: 0.00052411
Iteration 10/1000 | Loss: 0.00044297
Iteration 11/1000 | Loss: 0.00045891
Iteration 12/1000 | Loss: 0.00044533
Iteration 13/1000 | Loss: 0.00033312
Iteration 14/1000 | Loss: 0.00037696
Iteration 15/1000 | Loss: 0.00034016
Iteration 16/1000 | Loss: 0.00038547
Iteration 17/1000 | Loss: 0.00047967
Iteration 18/1000 | Loss: 0.00027137
Iteration 19/1000 | Loss: 0.00094049
Iteration 20/1000 | Loss: 0.00063911
Iteration 21/1000 | Loss: 0.00032181
Iteration 22/1000 | Loss: 0.00026673
Iteration 23/1000 | Loss: 0.00061336
Iteration 24/1000 | Loss: 0.00124340
Iteration 25/1000 | Loss: 0.00100112
Iteration 26/1000 | Loss: 0.00094723
Iteration 27/1000 | Loss: 0.00084759
Iteration 28/1000 | Loss: 0.00048095
Iteration 29/1000 | Loss: 0.00031790
Iteration 30/1000 | Loss: 0.00028237
Iteration 31/1000 | Loss: 0.00067926
Iteration 32/1000 | Loss: 0.00037310
Iteration 33/1000 | Loss: 0.00072866
Iteration 34/1000 | Loss: 0.00029956
Iteration 35/1000 | Loss: 0.00063351
Iteration 36/1000 | Loss: 0.00035995
Iteration 37/1000 | Loss: 0.00071356
Iteration 38/1000 | Loss: 0.00039057
Iteration 39/1000 | Loss: 0.00030751
Iteration 40/1000 | Loss: 0.00084212
Iteration 41/1000 | Loss: 0.00082273
Iteration 42/1000 | Loss: 0.00035655
Iteration 43/1000 | Loss: 0.00057187
Iteration 44/1000 | Loss: 0.00068560
Iteration 45/1000 | Loss: 0.00115908
Iteration 46/1000 | Loss: 0.00086817
Iteration 47/1000 | Loss: 0.00063488
Iteration 48/1000 | Loss: 0.00023952
Iteration 49/1000 | Loss: 0.00023178
Iteration 50/1000 | Loss: 0.00029486
Iteration 51/1000 | Loss: 0.00061230
Iteration 52/1000 | Loss: 0.00033466
Iteration 53/1000 | Loss: 0.00031603
Iteration 54/1000 | Loss: 0.00031037
Iteration 55/1000 | Loss: 0.00030857
Iteration 56/1000 | Loss: 0.00030867
Iteration 57/1000 | Loss: 0.00025148
Iteration 58/1000 | Loss: 0.00024568
Iteration 59/1000 | Loss: 0.00033250
Iteration 60/1000 | Loss: 0.00038326
Iteration 61/1000 | Loss: 0.00069558
Iteration 62/1000 | Loss: 0.00028300
Iteration 63/1000 | Loss: 0.00044474
Iteration 64/1000 | Loss: 0.00053208
Iteration 65/1000 | Loss: 0.00025258
Iteration 66/1000 | Loss: 0.00039765
Iteration 67/1000 | Loss: 0.00048100
Iteration 68/1000 | Loss: 0.00039453
Iteration 69/1000 | Loss: 0.00036142
Iteration 70/1000 | Loss: 0.00025279
Iteration 71/1000 | Loss: 0.00035616
Iteration 72/1000 | Loss: 0.00042639
Iteration 73/1000 | Loss: 0.00032603
Iteration 74/1000 | Loss: 0.00072059
Iteration 75/1000 | Loss: 0.00038215
Iteration 76/1000 | Loss: 0.00022603
Iteration 77/1000 | Loss: 0.00023874
Iteration 78/1000 | Loss: 0.00022894
Iteration 79/1000 | Loss: 0.00030865
Iteration 80/1000 | Loss: 0.00112464
Iteration 81/1000 | Loss: 0.00067748
Iteration 82/1000 | Loss: 0.00076740
Iteration 83/1000 | Loss: 0.00085699
Iteration 84/1000 | Loss: 0.00065390
Iteration 85/1000 | Loss: 0.00051495
Iteration 86/1000 | Loss: 0.00026857
Iteration 87/1000 | Loss: 0.00036076
Iteration 88/1000 | Loss: 0.00062378
Iteration 89/1000 | Loss: 0.00048286
Iteration 90/1000 | Loss: 0.00073605
Iteration 91/1000 | Loss: 0.00031905
Iteration 92/1000 | Loss: 0.00026291
Iteration 93/1000 | Loss: 0.00051522
Iteration 94/1000 | Loss: 0.00021051
Iteration 95/1000 | Loss: 0.00019210
Iteration 96/1000 | Loss: 0.00025797
Iteration 97/1000 | Loss: 0.00018281
Iteration 98/1000 | Loss: 0.00019873
Iteration 99/1000 | Loss: 0.00020867
Iteration 100/1000 | Loss: 0.00058537
Iteration 101/1000 | Loss: 0.00016941
Iteration 102/1000 | Loss: 0.00015623
Iteration 103/1000 | Loss: 0.00015107
Iteration 104/1000 | Loss: 0.00014673
Iteration 105/1000 | Loss: 0.00021127
Iteration 106/1000 | Loss: 0.00045800
Iteration 107/1000 | Loss: 0.00041651
Iteration 108/1000 | Loss: 0.00023223
Iteration 109/1000 | Loss: 0.00037764
Iteration 110/1000 | Loss: 0.00016400
Iteration 111/1000 | Loss: 0.00046392
Iteration 112/1000 | Loss: 0.00047687
Iteration 113/1000 | Loss: 0.00033542
Iteration 114/1000 | Loss: 0.00018243
Iteration 115/1000 | Loss: 0.00034868
Iteration 116/1000 | Loss: 0.00034331
Iteration 117/1000 | Loss: 0.00046332
Iteration 118/1000 | Loss: 0.00069416
Iteration 119/1000 | Loss: 0.00034735
Iteration 120/1000 | Loss: 0.00027990
Iteration 121/1000 | Loss: 0.00043522
Iteration 122/1000 | Loss: 0.00015191
Iteration 123/1000 | Loss: 0.00025705
Iteration 124/1000 | Loss: 0.00055559
Iteration 125/1000 | Loss: 0.00086869
Iteration 126/1000 | Loss: 0.00073145
Iteration 127/1000 | Loss: 0.00074364
Iteration 128/1000 | Loss: 0.00103453
Iteration 129/1000 | Loss: 0.00061964
Iteration 130/1000 | Loss: 0.00089176
Iteration 131/1000 | Loss: 0.00028744
Iteration 132/1000 | Loss: 0.00049528
Iteration 133/1000 | Loss: 0.00055321
Iteration 134/1000 | Loss: 0.00059964
Iteration 135/1000 | Loss: 0.00043195
Iteration 136/1000 | Loss: 0.00049782
Iteration 137/1000 | Loss: 0.00100479
Iteration 138/1000 | Loss: 0.00078473
Iteration 139/1000 | Loss: 0.00038445
Iteration 140/1000 | Loss: 0.00040820
Iteration 141/1000 | Loss: 0.00031244
Iteration 142/1000 | Loss: 0.00146499
Iteration 143/1000 | Loss: 0.00229402
Iteration 144/1000 | Loss: 0.00147189
Iteration 145/1000 | Loss: 0.00136332
Iteration 146/1000 | Loss: 0.00078142
Iteration 147/1000 | Loss: 0.00039728
Iteration 148/1000 | Loss: 0.00083504
Iteration 149/1000 | Loss: 0.00078795
Iteration 150/1000 | Loss: 0.00049600
Iteration 151/1000 | Loss: 0.00071602
Iteration 152/1000 | Loss: 0.00040473
Iteration 153/1000 | Loss: 0.00033150
Iteration 154/1000 | Loss: 0.00016229
Iteration 155/1000 | Loss: 0.00041291
Iteration 156/1000 | Loss: 0.00029982
Iteration 157/1000 | Loss: 0.00015732
Iteration 158/1000 | Loss: 0.00011141
Iteration 159/1000 | Loss: 0.00015516
Iteration 160/1000 | Loss: 0.00009326
Iteration 161/1000 | Loss: 0.00013690
Iteration 162/1000 | Loss: 0.00013827
Iteration 163/1000 | Loss: 0.00014285
Iteration 164/1000 | Loss: 0.00013360
Iteration 165/1000 | Loss: 0.00013040
Iteration 166/1000 | Loss: 0.00012347
Iteration 167/1000 | Loss: 0.00008818
Iteration 168/1000 | Loss: 0.00013060
Iteration 169/1000 | Loss: 0.00017532
Iteration 170/1000 | Loss: 0.00008961
Iteration 171/1000 | Loss: 0.00008222
Iteration 172/1000 | Loss: 0.00007851
Iteration 173/1000 | Loss: 0.00007734
Iteration 174/1000 | Loss: 0.00007631
Iteration 175/1000 | Loss: 0.00007572
Iteration 176/1000 | Loss: 0.00007514
Iteration 177/1000 | Loss: 0.00007409
Iteration 178/1000 | Loss: 0.00007364
Iteration 179/1000 | Loss: 0.00007332
Iteration 180/1000 | Loss: 0.00007298
Iteration 181/1000 | Loss: 0.00007268
Iteration 182/1000 | Loss: 0.00007250
Iteration 183/1000 | Loss: 0.00007231
Iteration 184/1000 | Loss: 0.00007217
Iteration 185/1000 | Loss: 0.00007204
Iteration 186/1000 | Loss: 0.00007199
Iteration 187/1000 | Loss: 0.00007194
Iteration 188/1000 | Loss: 0.00007194
Iteration 189/1000 | Loss: 0.00007193
Iteration 190/1000 | Loss: 0.00007193
Iteration 191/1000 | Loss: 0.00007192
Iteration 192/1000 | Loss: 0.00007192
Iteration 193/1000 | Loss: 0.00007191
Iteration 194/1000 | Loss: 0.00007191
Iteration 195/1000 | Loss: 0.00007191
Iteration 196/1000 | Loss: 0.00007190
Iteration 197/1000 | Loss: 0.00007190
Iteration 198/1000 | Loss: 0.00007190
Iteration 199/1000 | Loss: 0.00007189
Iteration 200/1000 | Loss: 0.00007189
Iteration 201/1000 | Loss: 0.00007189
Iteration 202/1000 | Loss: 0.00007189
Iteration 203/1000 | Loss: 0.00007188
Iteration 204/1000 | Loss: 0.00007188
Iteration 205/1000 | Loss: 0.00007188
Iteration 206/1000 | Loss: 0.00007187
Iteration 207/1000 | Loss: 0.00007187
Iteration 208/1000 | Loss: 0.00007187
Iteration 209/1000 | Loss: 0.00007186
Iteration 210/1000 | Loss: 0.00007186
Iteration 211/1000 | Loss: 0.00007186
Iteration 212/1000 | Loss: 0.00007186
Iteration 213/1000 | Loss: 0.00007185
Iteration 214/1000 | Loss: 0.00007185
Iteration 215/1000 | Loss: 0.00007185
Iteration 216/1000 | Loss: 0.00007185
Iteration 217/1000 | Loss: 0.00007185
Iteration 218/1000 | Loss: 0.00007184
Iteration 219/1000 | Loss: 0.00007184
Iteration 220/1000 | Loss: 0.00007183
Iteration 221/1000 | Loss: 0.00007183
Iteration 222/1000 | Loss: 0.00007182
Iteration 223/1000 | Loss: 0.00007182
Iteration 224/1000 | Loss: 0.00007182
Iteration 225/1000 | Loss: 0.00007182
Iteration 226/1000 | Loss: 0.00007182
Iteration 227/1000 | Loss: 0.00007181
Iteration 228/1000 | Loss: 0.00007181
Iteration 229/1000 | Loss: 0.00007181
Iteration 230/1000 | Loss: 0.00007181
Iteration 231/1000 | Loss: 0.00007180
Iteration 232/1000 | Loss: 0.00007179
Iteration 233/1000 | Loss: 0.00007178
Iteration 234/1000 | Loss: 0.00007178
Iteration 235/1000 | Loss: 0.00007177
Iteration 236/1000 | Loss: 0.00007177
Iteration 237/1000 | Loss: 0.00007176
Iteration 238/1000 | Loss: 0.00007176
Iteration 239/1000 | Loss: 0.00007175
Iteration 240/1000 | Loss: 0.00007175
Iteration 241/1000 | Loss: 0.00007175
Iteration 242/1000 | Loss: 0.00007174
Iteration 243/1000 | Loss: 0.00007174
Iteration 244/1000 | Loss: 0.00007174
Iteration 245/1000 | Loss: 0.00007174
Iteration 246/1000 | Loss: 0.00007173
Iteration 247/1000 | Loss: 0.00007173
Iteration 248/1000 | Loss: 0.00007173
Iteration 249/1000 | Loss: 0.00007172
Iteration 250/1000 | Loss: 0.00007172
Iteration 251/1000 | Loss: 0.00007172
Iteration 252/1000 | Loss: 0.00007171
Iteration 253/1000 | Loss: 0.00007171
Iteration 254/1000 | Loss: 0.00007171
Iteration 255/1000 | Loss: 0.00007170
Iteration 256/1000 | Loss: 0.00007170
Iteration 257/1000 | Loss: 0.00007170
Iteration 258/1000 | Loss: 0.00007169
Iteration 259/1000 | Loss: 0.00007169
Iteration 260/1000 | Loss: 0.00007169
Iteration 261/1000 | Loss: 0.00007169
Iteration 262/1000 | Loss: 0.00007168
Iteration 263/1000 | Loss: 0.00007168
Iteration 264/1000 | Loss: 0.00007168
Iteration 265/1000 | Loss: 0.00007167
Iteration 266/1000 | Loss: 0.00007167
Iteration 267/1000 | Loss: 0.00007167
Iteration 268/1000 | Loss: 0.00007167
Iteration 269/1000 | Loss: 0.00007166
Iteration 270/1000 | Loss: 0.00007166
Iteration 271/1000 | Loss: 0.00007166
Iteration 272/1000 | Loss: 0.00007166
Iteration 273/1000 | Loss: 0.00007166
Iteration 274/1000 | Loss: 0.00007165
Iteration 275/1000 | Loss: 0.00007165
Iteration 276/1000 | Loss: 0.00007165
Iteration 277/1000 | Loss: 0.00007164
Iteration 278/1000 | Loss: 0.00007164
Iteration 279/1000 | Loss: 0.00007164
Iteration 280/1000 | Loss: 0.00007163
Iteration 281/1000 | Loss: 0.00007163
Iteration 282/1000 | Loss: 0.00007163
Iteration 283/1000 | Loss: 0.00007163
Iteration 284/1000 | Loss: 0.00007163
Iteration 285/1000 | Loss: 0.00007162
Iteration 286/1000 | Loss: 0.00007162
Iteration 287/1000 | Loss: 0.00007162
Iteration 288/1000 | Loss: 0.00007162
Iteration 289/1000 | Loss: 0.00007162
Iteration 290/1000 | Loss: 0.00007162
Iteration 291/1000 | Loss: 0.00007162
Iteration 292/1000 | Loss: 0.00007162
Iteration 293/1000 | Loss: 0.00007162
Iteration 294/1000 | Loss: 0.00007162
Iteration 295/1000 | Loss: 0.00007161
Iteration 296/1000 | Loss: 0.00007161
Iteration 297/1000 | Loss: 0.00007161
Iteration 298/1000 | Loss: 0.00007161
Iteration 299/1000 | Loss: 0.00007160
Iteration 300/1000 | Loss: 0.00007160
Iteration 301/1000 | Loss: 0.00007160
Iteration 302/1000 | Loss: 0.00007160
Iteration 303/1000 | Loss: 0.00007160
Iteration 304/1000 | Loss: 0.00007160
Iteration 305/1000 | Loss: 0.00007160
Iteration 306/1000 | Loss: 0.00007159
Iteration 307/1000 | Loss: 0.00007159
Iteration 308/1000 | Loss: 0.00007159
Iteration 309/1000 | Loss: 0.00007159
Iteration 310/1000 | Loss: 0.00007159
Iteration 311/1000 | Loss: 0.00007159
Iteration 312/1000 | Loss: 0.00007159
Iteration 313/1000 | Loss: 0.00007159
Iteration 314/1000 | Loss: 0.00007159
Iteration 315/1000 | Loss: 0.00007159
Iteration 316/1000 | Loss: 0.00007159
Iteration 317/1000 | Loss: 0.00007159
Iteration 318/1000 | Loss: 0.00007159
Iteration 319/1000 | Loss: 0.00007159
Iteration 320/1000 | Loss: 0.00007159
Iteration 321/1000 | Loss: 0.00007159
Iteration 322/1000 | Loss: 0.00007159
Iteration 323/1000 | Loss: 0.00007159
Iteration 324/1000 | Loss: 0.00007158
Iteration 325/1000 | Loss: 0.00007158
Iteration 326/1000 | Loss: 0.00007158
Iteration 327/1000 | Loss: 0.00007158
Iteration 328/1000 | Loss: 0.00007158
Iteration 329/1000 | Loss: 0.00007158
Iteration 330/1000 | Loss: 0.00007158
Iteration 331/1000 | Loss: 0.00007158
Iteration 332/1000 | Loss: 0.00007158
Iteration 333/1000 | Loss: 0.00007158
Iteration 334/1000 | Loss: 0.00007158
Iteration 335/1000 | Loss: 0.00007157
Iteration 336/1000 | Loss: 0.00007157
Iteration 337/1000 | Loss: 0.00007157
Iteration 338/1000 | Loss: 0.00007157
Iteration 339/1000 | Loss: 0.00007157
Iteration 340/1000 | Loss: 0.00007157
Iteration 341/1000 | Loss: 0.00007157
Iteration 342/1000 | Loss: 0.00007157
Iteration 343/1000 | Loss: 0.00007156
Iteration 344/1000 | Loss: 0.00007156
Iteration 345/1000 | Loss: 0.00007156
Iteration 346/1000 | Loss: 0.00007156
Iteration 347/1000 | Loss: 0.00007156
Iteration 348/1000 | Loss: 0.00007156
Iteration 349/1000 | Loss: 0.00007156
Iteration 350/1000 | Loss: 0.00007156
Iteration 351/1000 | Loss: 0.00007156
Iteration 352/1000 | Loss: 0.00007156
Iteration 353/1000 | Loss: 0.00007155
Iteration 354/1000 | Loss: 0.00007155
Iteration 355/1000 | Loss: 0.00007155
Iteration 356/1000 | Loss: 0.00007155
Iteration 357/1000 | Loss: 0.00007155
Iteration 358/1000 | Loss: 0.00007155
Iteration 359/1000 | Loss: 0.00007155
Iteration 360/1000 | Loss: 0.00007155
Iteration 361/1000 | Loss: 0.00007155
Iteration 362/1000 | Loss: 0.00007154
Iteration 363/1000 | Loss: 0.00007154
Iteration 364/1000 | Loss: 0.00007154
Iteration 365/1000 | Loss: 0.00007154
Iteration 366/1000 | Loss: 0.00007154
Iteration 367/1000 | Loss: 0.00007154
Iteration 368/1000 | Loss: 0.00007154
Iteration 369/1000 | Loss: 0.00007154
Iteration 370/1000 | Loss: 0.00007154
Iteration 371/1000 | Loss: 0.00007154
Iteration 372/1000 | Loss: 0.00007154
Iteration 373/1000 | Loss: 0.00007154
Iteration 374/1000 | Loss: 0.00007154
Iteration 375/1000 | Loss: 0.00007153
Iteration 376/1000 | Loss: 0.00007153
Iteration 377/1000 | Loss: 0.00007153
Iteration 378/1000 | Loss: 0.00007153
Iteration 379/1000 | Loss: 0.00007153
Iteration 380/1000 | Loss: 0.00007153
Iteration 381/1000 | Loss: 0.00007153
Iteration 382/1000 | Loss: 0.00007153
Iteration 383/1000 | Loss: 0.00007153
Iteration 384/1000 | Loss: 0.00007153
Iteration 385/1000 | Loss: 0.00007152
Iteration 386/1000 | Loss: 0.00007152
Iteration 387/1000 | Loss: 0.00007152
Iteration 388/1000 | Loss: 0.00007152
Iteration 389/1000 | Loss: 0.00007152
Iteration 390/1000 | Loss: 0.00007152
Iteration 391/1000 | Loss: 0.00007152
Iteration 392/1000 | Loss: 0.00007152
Iteration 393/1000 | Loss: 0.00007152
Iteration 394/1000 | Loss: 0.00007152
Iteration 395/1000 | Loss: 0.00007152
Iteration 396/1000 | Loss: 0.00007152
Iteration 397/1000 | Loss: 0.00007152
Iteration 398/1000 | Loss: 0.00007152
Iteration 399/1000 | Loss: 0.00007152
Iteration 400/1000 | Loss: 0.00007152
Iteration 401/1000 | Loss: 0.00007152
Iteration 402/1000 | Loss: 0.00007152
Iteration 403/1000 | Loss: 0.00007151
Iteration 404/1000 | Loss: 0.00007151
Iteration 405/1000 | Loss: 0.00007151
Iteration 406/1000 | Loss: 0.00007151
Iteration 407/1000 | Loss: 0.00007151
Iteration 408/1000 | Loss: 0.00007151
Iteration 409/1000 | Loss: 0.00007151
Iteration 410/1000 | Loss: 0.00007151
Iteration 411/1000 | Loss: 0.00007151
Iteration 412/1000 | Loss: 0.00007151
Iteration 413/1000 | Loss: 0.00007151
Iteration 414/1000 | Loss: 0.00007151
Iteration 415/1000 | Loss: 0.00007151
Iteration 416/1000 | Loss: 0.00007151
Iteration 417/1000 | Loss: 0.00007151
Iteration 418/1000 | Loss: 0.00007151
Iteration 419/1000 | Loss: 0.00007151
Iteration 420/1000 | Loss: 0.00007151
Iteration 421/1000 | Loss: 0.00007151
Iteration 422/1000 | Loss: 0.00007151
Iteration 423/1000 | Loss: 0.00007151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 423. Stopping optimization.
Last 5 losses: [7.151456520659849e-05, 7.151456520659849e-05, 7.151456520659849e-05, 7.151456520659849e-05, 7.151456520659849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.151456520659849e-05

Optimization complete. Final v2v error: 6.0392913818359375 mm

Highest mean error: 13.252764701843262 mm for frame 134

Lowest mean error: 4.647651672363281 mm for frame 10

Saving results

Total time: 369.979777097702
