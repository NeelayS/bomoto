Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=61, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3416-3471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009450
Iteration 2/25 | Loss: 0.00361949
Iteration 3/25 | Loss: 0.00306154
Iteration 4/25 | Loss: 0.00272557
Iteration 5/25 | Loss: 0.00248774
Iteration 6/25 | Loss: 0.00244125
Iteration 7/25 | Loss: 0.00213399
Iteration 8/25 | Loss: 0.00206116
Iteration 9/25 | Loss: 0.00201085
Iteration 10/25 | Loss: 0.00199142
Iteration 11/25 | Loss: 0.00198334
Iteration 12/25 | Loss: 0.00197742
Iteration 13/25 | Loss: 0.00197003
Iteration 14/25 | Loss: 0.00196485
Iteration 15/25 | Loss: 0.00195206
Iteration 16/25 | Loss: 0.00195341
Iteration 17/25 | Loss: 0.00195946
Iteration 18/25 | Loss: 0.00194878
Iteration 19/25 | Loss: 0.00194071
Iteration 20/25 | Loss: 0.00193358
Iteration 21/25 | Loss: 0.00193157
Iteration 22/25 | Loss: 0.00193343
Iteration 23/25 | Loss: 0.00192591
Iteration 24/25 | Loss: 0.00192315
Iteration 25/25 | Loss: 0.00192225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38579845
Iteration 2/25 | Loss: 0.00923770
Iteration 3/25 | Loss: 0.00731912
Iteration 4/25 | Loss: 0.00731910
Iteration 5/25 | Loss: 0.00731909
Iteration 6/25 | Loss: 0.00731909
Iteration 7/25 | Loss: 0.00731909
Iteration 8/25 | Loss: 0.00731909
Iteration 9/25 | Loss: 0.00731909
Iteration 10/25 | Loss: 0.00731909
Iteration 11/25 | Loss: 0.00731909
Iteration 12/25 | Loss: 0.00731909
Iteration 13/25 | Loss: 0.00731909
Iteration 14/25 | Loss: 0.00731909
Iteration 15/25 | Loss: 0.00731909
Iteration 16/25 | Loss: 0.00731909
Iteration 17/25 | Loss: 0.00731909
Iteration 18/25 | Loss: 0.00731909
Iteration 19/25 | Loss: 0.00731909
Iteration 20/25 | Loss: 0.00731909
Iteration 21/25 | Loss: 0.00731909
Iteration 22/25 | Loss: 0.00731909
Iteration 23/25 | Loss: 0.00731909
Iteration 24/25 | Loss: 0.00731909
Iteration 25/25 | Loss: 0.00731909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00731909
Iteration 2/1000 | Loss: 0.00198550
Iteration 3/1000 | Loss: 0.00372750
Iteration 4/1000 | Loss: 0.00141206
Iteration 5/1000 | Loss: 0.00088271
Iteration 6/1000 | Loss: 0.00055556
Iteration 7/1000 | Loss: 0.00047812
Iteration 8/1000 | Loss: 0.00053852
Iteration 9/1000 | Loss: 0.00051064
Iteration 10/1000 | Loss: 0.00071858
Iteration 11/1000 | Loss: 0.00091198
Iteration 12/1000 | Loss: 0.00240946
Iteration 13/1000 | Loss: 0.00045040
Iteration 14/1000 | Loss: 0.00054955
Iteration 15/1000 | Loss: 0.00069492
Iteration 16/1000 | Loss: 0.00091804
Iteration 17/1000 | Loss: 0.00047350
Iteration 18/1000 | Loss: 0.00061945
Iteration 19/1000 | Loss: 0.00066767
Iteration 20/1000 | Loss: 0.00048602
Iteration 21/1000 | Loss: 0.00036708
Iteration 22/1000 | Loss: 0.00035426
Iteration 23/1000 | Loss: 0.00067800
Iteration 24/1000 | Loss: 0.00033899
Iteration 25/1000 | Loss: 0.00037771
Iteration 26/1000 | Loss: 0.00075694
Iteration 27/1000 | Loss: 0.00088623
Iteration 28/1000 | Loss: 0.00055451
Iteration 29/1000 | Loss: 0.00150158
Iteration 30/1000 | Loss: 0.00056623
Iteration 31/1000 | Loss: 0.00035122
Iteration 32/1000 | Loss: 0.00040580
Iteration 33/1000 | Loss: 0.00035509
Iteration 34/1000 | Loss: 0.00052952
Iteration 35/1000 | Loss: 0.00042171
Iteration 36/1000 | Loss: 0.00043078
Iteration 37/1000 | Loss: 0.00037537
Iteration 38/1000 | Loss: 0.00033604
Iteration 39/1000 | Loss: 0.00060505
Iteration 40/1000 | Loss: 0.00065221
Iteration 41/1000 | Loss: 0.00045713
Iteration 42/1000 | Loss: 0.00032040
Iteration 43/1000 | Loss: 0.00065550
Iteration 44/1000 | Loss: 0.00031944
Iteration 45/1000 | Loss: 0.00048569
Iteration 46/1000 | Loss: 0.00206831
Iteration 47/1000 | Loss: 0.00055736
Iteration 48/1000 | Loss: 0.00052113
Iteration 49/1000 | Loss: 0.00032188
Iteration 50/1000 | Loss: 0.00044332
Iteration 51/1000 | Loss: 0.00087977
Iteration 52/1000 | Loss: 0.00047746
Iteration 53/1000 | Loss: 0.00031197
Iteration 54/1000 | Loss: 0.00038570
Iteration 55/1000 | Loss: 0.00037475
Iteration 56/1000 | Loss: 0.00067354
Iteration 57/1000 | Loss: 0.00097812
Iteration 58/1000 | Loss: 0.00035352
Iteration 59/1000 | Loss: 0.00062452
Iteration 60/1000 | Loss: 0.00031166
Iteration 61/1000 | Loss: 0.00030575
Iteration 62/1000 | Loss: 0.00031501
Iteration 63/1000 | Loss: 0.00103251
Iteration 64/1000 | Loss: 0.00051497
Iteration 65/1000 | Loss: 0.00094672
Iteration 66/1000 | Loss: 0.00031195
Iteration 67/1000 | Loss: 0.00029858
Iteration 68/1000 | Loss: 0.00029491
Iteration 69/1000 | Loss: 0.00058671
Iteration 70/1000 | Loss: 0.00037571
Iteration 71/1000 | Loss: 0.00052083
Iteration 72/1000 | Loss: 0.00040061
Iteration 73/1000 | Loss: 0.00072334
Iteration 74/1000 | Loss: 0.00098414
Iteration 75/1000 | Loss: 0.00085668
Iteration 76/1000 | Loss: 0.00062210
Iteration 77/1000 | Loss: 0.00040405
Iteration 78/1000 | Loss: 0.00054385
Iteration 79/1000 | Loss: 0.00030826
Iteration 80/1000 | Loss: 0.00032428
Iteration 81/1000 | Loss: 0.00028388
Iteration 82/1000 | Loss: 0.00028134
Iteration 83/1000 | Loss: 0.00044182
Iteration 84/1000 | Loss: 0.00136476
Iteration 85/1000 | Loss: 0.00042366
Iteration 86/1000 | Loss: 0.00028317
Iteration 87/1000 | Loss: 0.00040690
Iteration 88/1000 | Loss: 0.00027790
Iteration 89/1000 | Loss: 0.00037772
Iteration 90/1000 | Loss: 0.00062434
Iteration 91/1000 | Loss: 0.00045043
Iteration 92/1000 | Loss: 0.00076972
Iteration 93/1000 | Loss: 0.00036292
Iteration 94/1000 | Loss: 0.00060721
Iteration 95/1000 | Loss: 0.00040669
Iteration 96/1000 | Loss: 0.00029408
Iteration 97/1000 | Loss: 0.00027694
Iteration 98/1000 | Loss: 0.00053219
Iteration 99/1000 | Loss: 0.00029413
Iteration 100/1000 | Loss: 0.00040906
Iteration 101/1000 | Loss: 0.00027435
Iteration 102/1000 | Loss: 0.00027219
Iteration 103/1000 | Loss: 0.00029926
Iteration 104/1000 | Loss: 0.00026974
Iteration 105/1000 | Loss: 0.00056078
Iteration 106/1000 | Loss: 0.00170226
Iteration 107/1000 | Loss: 0.00146364
Iteration 108/1000 | Loss: 0.00216201
Iteration 109/1000 | Loss: 0.00093394
Iteration 110/1000 | Loss: 0.00033567
Iteration 111/1000 | Loss: 0.00077746
Iteration 112/1000 | Loss: 0.00027758
Iteration 113/1000 | Loss: 0.00027235
Iteration 114/1000 | Loss: 0.00032737
Iteration 115/1000 | Loss: 0.00043946
Iteration 116/1000 | Loss: 0.00027235
Iteration 117/1000 | Loss: 0.00026839
Iteration 118/1000 | Loss: 0.00026747
Iteration 119/1000 | Loss: 0.00036628
Iteration 120/1000 | Loss: 0.00026680
Iteration 121/1000 | Loss: 0.00026627
Iteration 122/1000 | Loss: 0.00035378
Iteration 123/1000 | Loss: 0.00026608
Iteration 124/1000 | Loss: 0.00038732
Iteration 125/1000 | Loss: 0.00029536
Iteration 126/1000 | Loss: 0.00041287
Iteration 127/1000 | Loss: 0.00034622
Iteration 128/1000 | Loss: 0.00041824
Iteration 129/1000 | Loss: 0.00042577
Iteration 130/1000 | Loss: 0.00055991
Iteration 131/1000 | Loss: 0.00036347
Iteration 132/1000 | Loss: 0.00039000
Iteration 133/1000 | Loss: 0.00027718
Iteration 134/1000 | Loss: 0.00027281
Iteration 135/1000 | Loss: 0.00026438
Iteration 136/1000 | Loss: 0.00038664
Iteration 137/1000 | Loss: 0.00027703
Iteration 138/1000 | Loss: 0.00026385
Iteration 139/1000 | Loss: 0.00026370
Iteration 140/1000 | Loss: 0.00026365
Iteration 141/1000 | Loss: 0.00026365
Iteration 142/1000 | Loss: 0.00026365
Iteration 143/1000 | Loss: 0.00026365
Iteration 144/1000 | Loss: 0.00026364
Iteration 145/1000 | Loss: 0.00026364
Iteration 146/1000 | Loss: 0.00026364
Iteration 147/1000 | Loss: 0.00026364
Iteration 148/1000 | Loss: 0.00026364
Iteration 149/1000 | Loss: 0.00026351
Iteration 150/1000 | Loss: 0.00028227
Iteration 151/1000 | Loss: 0.00028164
Iteration 152/1000 | Loss: 0.00026328
Iteration 153/1000 | Loss: 0.00026321
Iteration 154/1000 | Loss: 0.00026321
Iteration 155/1000 | Loss: 0.00026319
Iteration 156/1000 | Loss: 0.00028145
Iteration 157/1000 | Loss: 0.00026312
Iteration 158/1000 | Loss: 0.00026304
Iteration 159/1000 | Loss: 0.00026303
Iteration 160/1000 | Loss: 0.00026300
Iteration 161/1000 | Loss: 0.00026297
Iteration 162/1000 | Loss: 0.00026297
Iteration 163/1000 | Loss: 0.00026296
Iteration 164/1000 | Loss: 0.00026296
Iteration 165/1000 | Loss: 0.00026294
Iteration 166/1000 | Loss: 0.00026293
Iteration 167/1000 | Loss: 0.00026293
Iteration 168/1000 | Loss: 0.00026293
Iteration 169/1000 | Loss: 0.00026292
Iteration 170/1000 | Loss: 0.00026292
Iteration 171/1000 | Loss: 0.00026292
Iteration 172/1000 | Loss: 0.00026292
Iteration 173/1000 | Loss: 0.00026292
Iteration 174/1000 | Loss: 0.00026292
Iteration 175/1000 | Loss: 0.00026292
Iteration 176/1000 | Loss: 0.00026292
Iteration 177/1000 | Loss: 0.00026292
Iteration 178/1000 | Loss: 0.00026292
Iteration 179/1000 | Loss: 0.00026292
Iteration 180/1000 | Loss: 0.00026291
Iteration 181/1000 | Loss: 0.00026291
Iteration 182/1000 | Loss: 0.00026291
Iteration 183/1000 | Loss: 0.00026288
Iteration 184/1000 | Loss: 0.00028047
Iteration 185/1000 | Loss: 0.00026299
Iteration 186/1000 | Loss: 0.00027713
Iteration 187/1000 | Loss: 0.00026280
Iteration 188/1000 | Loss: 0.00026279
Iteration 189/1000 | Loss: 0.00026278
Iteration 190/1000 | Loss: 0.00026278
Iteration 191/1000 | Loss: 0.00026278
Iteration 192/1000 | Loss: 0.00026278
Iteration 193/1000 | Loss: 0.00026277
Iteration 194/1000 | Loss: 0.00026277
Iteration 195/1000 | Loss: 0.00026277
Iteration 196/1000 | Loss: 0.00026277
Iteration 197/1000 | Loss: 0.00026277
Iteration 198/1000 | Loss: 0.00026277
Iteration 199/1000 | Loss: 0.00026277
Iteration 200/1000 | Loss: 0.00026277
Iteration 201/1000 | Loss: 0.00026277
Iteration 202/1000 | Loss: 0.00026277
Iteration 203/1000 | Loss: 0.00026277
Iteration 204/1000 | Loss: 0.00026277
Iteration 205/1000 | Loss: 0.00026824
Iteration 206/1000 | Loss: 0.00026369
Iteration 207/1000 | Loss: 0.00026276
Iteration 208/1000 | Loss: 0.00026276
Iteration 209/1000 | Loss: 0.00026276
Iteration 210/1000 | Loss: 0.00026276
Iteration 211/1000 | Loss: 0.00026275
Iteration 212/1000 | Loss: 0.00026275
Iteration 213/1000 | Loss: 0.00026275
Iteration 214/1000 | Loss: 0.00026463
Iteration 215/1000 | Loss: 0.00026335
Iteration 216/1000 | Loss: 0.00026420
Iteration 217/1000 | Loss: 0.00026276
Iteration 218/1000 | Loss: 0.00026276
Iteration 219/1000 | Loss: 0.00026276
Iteration 220/1000 | Loss: 0.00026276
Iteration 221/1000 | Loss: 0.00026276
Iteration 222/1000 | Loss: 0.00026276
Iteration 223/1000 | Loss: 0.00026276
Iteration 224/1000 | Loss: 0.00026276
Iteration 225/1000 | Loss: 0.00026276
Iteration 226/1000 | Loss: 0.00026276
Iteration 227/1000 | Loss: 0.00026276
Iteration 228/1000 | Loss: 0.00026276
Iteration 229/1000 | Loss: 0.00026276
Iteration 230/1000 | Loss: 0.00026276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [0.0002627588401082903, 0.0002627588401082903, 0.0002627588401082903, 0.0002627588401082903, 0.0002627588401082903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002627588401082903

Optimization complete. Final v2v error: 9.162693977355957 mm

Highest mean error: 12.266314506530762 mm for frame 188

Lowest mean error: 4.669266223907471 mm for frame 227

Saving results

Total time: 297.75035190582275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00705371
Iteration 2/25 | Loss: 0.00162879
Iteration 3/25 | Loss: 0.00140761
Iteration 4/25 | Loss: 0.00136576
Iteration 5/25 | Loss: 0.00135412
Iteration 6/25 | Loss: 0.00135054
Iteration 7/25 | Loss: 0.00134899
Iteration 8/25 | Loss: 0.00134846
Iteration 9/25 | Loss: 0.00134758
Iteration 10/25 | Loss: 0.00134717
Iteration 11/25 | Loss: 0.00134523
Iteration 12/25 | Loss: 0.00134478
Iteration 13/25 | Loss: 0.00134472
Iteration 14/25 | Loss: 0.00134472
Iteration 15/25 | Loss: 0.00134472
Iteration 16/25 | Loss: 0.00134472
Iteration 17/25 | Loss: 0.00134472
Iteration 18/25 | Loss: 0.00134472
Iteration 19/25 | Loss: 0.00134472
Iteration 20/25 | Loss: 0.00134472
Iteration 21/25 | Loss: 0.00134472
Iteration 22/25 | Loss: 0.00134472
Iteration 23/25 | Loss: 0.00134472
Iteration 24/25 | Loss: 0.00134472
Iteration 25/25 | Loss: 0.00134471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57280016
Iteration 2/25 | Loss: 0.00114743
Iteration 3/25 | Loss: 0.00114742
Iteration 4/25 | Loss: 0.00114742
Iteration 5/25 | Loss: 0.00114742
Iteration 6/25 | Loss: 0.00114742
Iteration 7/25 | Loss: 0.00114741
Iteration 8/25 | Loss: 0.00114741
Iteration 9/25 | Loss: 0.00114741
Iteration 10/25 | Loss: 0.00114741
Iteration 11/25 | Loss: 0.00114741
Iteration 12/25 | Loss: 0.00114741
Iteration 13/25 | Loss: 0.00114741
Iteration 14/25 | Loss: 0.00114741
Iteration 15/25 | Loss: 0.00114741
Iteration 16/25 | Loss: 0.00114741
Iteration 17/25 | Loss: 0.00114741
Iteration 18/25 | Loss: 0.00114741
Iteration 19/25 | Loss: 0.00114741
Iteration 20/25 | Loss: 0.00114741
Iteration 21/25 | Loss: 0.00114741
Iteration 22/25 | Loss: 0.00114741
Iteration 23/25 | Loss: 0.00114741
Iteration 24/25 | Loss: 0.00114741
Iteration 25/25 | Loss: 0.00114741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114741
Iteration 2/1000 | Loss: 0.00007763
Iteration 3/1000 | Loss: 0.00004742
Iteration 4/1000 | Loss: 0.00003798
Iteration 5/1000 | Loss: 0.00003464
Iteration 6/1000 | Loss: 0.00005027
Iteration 7/1000 | Loss: 0.00003490
Iteration 8/1000 | Loss: 0.00004508
Iteration 9/1000 | Loss: 0.00003160
Iteration 10/1000 | Loss: 0.00002980
Iteration 11/1000 | Loss: 0.00002890
Iteration 12/1000 | Loss: 0.00002824
Iteration 13/1000 | Loss: 0.00002784
Iteration 14/1000 | Loss: 0.00002727
Iteration 15/1000 | Loss: 0.00003652
Iteration 16/1000 | Loss: 0.00003224
Iteration 17/1000 | Loss: 0.00003496
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002692
Iteration 20/1000 | Loss: 0.00002647
Iteration 21/1000 | Loss: 0.00002634
Iteration 22/1000 | Loss: 0.00002613
Iteration 23/1000 | Loss: 0.00002583
Iteration 24/1000 | Loss: 0.00003948
Iteration 25/1000 | Loss: 0.00003288
Iteration 26/1000 | Loss: 0.00004058
Iteration 27/1000 | Loss: 0.00003394
Iteration 28/1000 | Loss: 0.00004155
Iteration 29/1000 | Loss: 0.00003317
Iteration 30/1000 | Loss: 0.00004297
Iteration 31/1000 | Loss: 0.00003178
Iteration 32/1000 | Loss: 0.00004083
Iteration 33/1000 | Loss: 0.00002942
Iteration 34/1000 | Loss: 0.00004072
Iteration 35/1000 | Loss: 0.00003438
Iteration 36/1000 | Loss: 0.00004810
Iteration 37/1000 | Loss: 0.00002901
Iteration 38/1000 | Loss: 0.00003772
Iteration 39/1000 | Loss: 0.00002930
Iteration 40/1000 | Loss: 0.00003912
Iteration 41/1000 | Loss: 0.00003439
Iteration 42/1000 | Loss: 0.00004909
Iteration 43/1000 | Loss: 0.00002937
Iteration 44/1000 | Loss: 0.00002673
Iteration 45/1000 | Loss: 0.00003916
Iteration 46/1000 | Loss: 0.00003014
Iteration 47/1000 | Loss: 0.00002758
Iteration 48/1000 | Loss: 0.00002611
Iteration 49/1000 | Loss: 0.00003869
Iteration 50/1000 | Loss: 0.00003401
Iteration 51/1000 | Loss: 0.00004577
Iteration 52/1000 | Loss: 0.00003009
Iteration 53/1000 | Loss: 0.00005134
Iteration 54/1000 | Loss: 0.00008518
Iteration 55/1000 | Loss: 0.00005521
Iteration 56/1000 | Loss: 0.00002633
Iteration 57/1000 | Loss: 0.00002580
Iteration 58/1000 | Loss: 0.00002546
Iteration 59/1000 | Loss: 0.00002515
Iteration 60/1000 | Loss: 0.00002479
Iteration 61/1000 | Loss: 0.00002458
Iteration 62/1000 | Loss: 0.00002458
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00002457
Iteration 65/1000 | Loss: 0.00002456
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002455
Iteration 69/1000 | Loss: 0.00002455
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002455
Iteration 75/1000 | Loss: 0.00002455
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002454
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002454
Iteration 81/1000 | Loss: 0.00002454
Iteration 82/1000 | Loss: 0.00002454
Iteration 83/1000 | Loss: 0.00002454
Iteration 84/1000 | Loss: 0.00002453
Iteration 85/1000 | Loss: 0.00002453
Iteration 86/1000 | Loss: 0.00002453
Iteration 87/1000 | Loss: 0.00002453
Iteration 88/1000 | Loss: 0.00002453
Iteration 89/1000 | Loss: 0.00002453
Iteration 90/1000 | Loss: 0.00002453
Iteration 91/1000 | Loss: 0.00002452
Iteration 92/1000 | Loss: 0.00002452
Iteration 93/1000 | Loss: 0.00002452
Iteration 94/1000 | Loss: 0.00002452
Iteration 95/1000 | Loss: 0.00002452
Iteration 96/1000 | Loss: 0.00002452
Iteration 97/1000 | Loss: 0.00002452
Iteration 98/1000 | Loss: 0.00002452
Iteration 99/1000 | Loss: 0.00002451
Iteration 100/1000 | Loss: 0.00002451
Iteration 101/1000 | Loss: 0.00002451
Iteration 102/1000 | Loss: 0.00002451
Iteration 103/1000 | Loss: 0.00002451
Iteration 104/1000 | Loss: 0.00002451
Iteration 105/1000 | Loss: 0.00002450
Iteration 106/1000 | Loss: 0.00002449
Iteration 107/1000 | Loss: 0.00002449
Iteration 108/1000 | Loss: 0.00002448
Iteration 109/1000 | Loss: 0.00002448
Iteration 110/1000 | Loss: 0.00002448
Iteration 111/1000 | Loss: 0.00002448
Iteration 112/1000 | Loss: 0.00002447
Iteration 113/1000 | Loss: 0.00002447
Iteration 114/1000 | Loss: 0.00002447
Iteration 115/1000 | Loss: 0.00002447
Iteration 116/1000 | Loss: 0.00002446
Iteration 117/1000 | Loss: 0.00002445
Iteration 118/1000 | Loss: 0.00002445
Iteration 119/1000 | Loss: 0.00002445
Iteration 120/1000 | Loss: 0.00002445
Iteration 121/1000 | Loss: 0.00002444
Iteration 122/1000 | Loss: 0.00002436
Iteration 123/1000 | Loss: 0.00002433
Iteration 124/1000 | Loss: 0.00002433
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002422
Iteration 127/1000 | Loss: 0.00002422
Iteration 128/1000 | Loss: 0.00002421
Iteration 129/1000 | Loss: 0.00002420
Iteration 130/1000 | Loss: 0.00002419
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00002417
Iteration 133/1000 | Loss: 0.00002415
Iteration 134/1000 | Loss: 0.00002410
Iteration 135/1000 | Loss: 0.00002410
Iteration 136/1000 | Loss: 0.00002410
Iteration 137/1000 | Loss: 0.00002409
Iteration 138/1000 | Loss: 0.00002408
Iteration 139/1000 | Loss: 0.00002408
Iteration 140/1000 | Loss: 0.00002407
Iteration 141/1000 | Loss: 0.00002407
Iteration 142/1000 | Loss: 0.00002407
Iteration 143/1000 | Loss: 0.00002407
Iteration 144/1000 | Loss: 0.00002406
Iteration 145/1000 | Loss: 0.00002406
Iteration 146/1000 | Loss: 0.00002406
Iteration 147/1000 | Loss: 0.00002406
Iteration 148/1000 | Loss: 0.00002406
Iteration 149/1000 | Loss: 0.00002406
Iteration 150/1000 | Loss: 0.00002406
Iteration 151/1000 | Loss: 0.00002406
Iteration 152/1000 | Loss: 0.00002406
Iteration 153/1000 | Loss: 0.00002406
Iteration 154/1000 | Loss: 0.00002406
Iteration 155/1000 | Loss: 0.00002405
Iteration 156/1000 | Loss: 0.00002405
Iteration 157/1000 | Loss: 0.00002405
Iteration 158/1000 | Loss: 0.00002405
Iteration 159/1000 | Loss: 0.00002404
Iteration 160/1000 | Loss: 0.00002404
Iteration 161/1000 | Loss: 0.00002404
Iteration 162/1000 | Loss: 0.00002404
Iteration 163/1000 | Loss: 0.00002404
Iteration 164/1000 | Loss: 0.00002403
Iteration 165/1000 | Loss: 0.00002403
Iteration 166/1000 | Loss: 0.00002403
Iteration 167/1000 | Loss: 0.00002402
Iteration 168/1000 | Loss: 0.00002402
Iteration 169/1000 | Loss: 0.00002402
Iteration 170/1000 | Loss: 0.00002402
Iteration 171/1000 | Loss: 0.00002402
Iteration 172/1000 | Loss: 0.00002402
Iteration 173/1000 | Loss: 0.00002401
Iteration 174/1000 | Loss: 0.00002401
Iteration 175/1000 | Loss: 0.00002401
Iteration 176/1000 | Loss: 0.00002401
Iteration 177/1000 | Loss: 0.00002401
Iteration 178/1000 | Loss: 0.00002401
Iteration 179/1000 | Loss: 0.00002401
Iteration 180/1000 | Loss: 0.00002401
Iteration 181/1000 | Loss: 0.00002401
Iteration 182/1000 | Loss: 0.00002401
Iteration 183/1000 | Loss: 0.00002401
Iteration 184/1000 | Loss: 0.00002401
Iteration 185/1000 | Loss: 0.00002401
Iteration 186/1000 | Loss: 0.00002401
Iteration 187/1000 | Loss: 0.00002401
Iteration 188/1000 | Loss: 0.00002401
Iteration 189/1000 | Loss: 0.00002400
Iteration 190/1000 | Loss: 0.00002400
Iteration 191/1000 | Loss: 0.00002400
Iteration 192/1000 | Loss: 0.00002400
Iteration 193/1000 | Loss: 0.00002400
Iteration 194/1000 | Loss: 0.00002400
Iteration 195/1000 | Loss: 0.00002400
Iteration 196/1000 | Loss: 0.00002400
Iteration 197/1000 | Loss: 0.00002400
Iteration 198/1000 | Loss: 0.00002400
Iteration 199/1000 | Loss: 0.00002400
Iteration 200/1000 | Loss: 0.00002400
Iteration 201/1000 | Loss: 0.00002400
Iteration 202/1000 | Loss: 0.00002400
Iteration 203/1000 | Loss: 0.00002400
Iteration 204/1000 | Loss: 0.00002400
Iteration 205/1000 | Loss: 0.00002399
Iteration 206/1000 | Loss: 0.00002399
Iteration 207/1000 | Loss: 0.00002399
Iteration 208/1000 | Loss: 0.00002399
Iteration 209/1000 | Loss: 0.00002399
Iteration 210/1000 | Loss: 0.00002399
Iteration 211/1000 | Loss: 0.00002399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.3994463845156133e-05, 2.3994463845156133e-05, 2.3994463845156133e-05, 2.3994463845156133e-05, 2.3994463845156133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3994463845156133e-05

Optimization complete. Final v2v error: 4.006457328796387 mm

Highest mean error: 6.933809757232666 mm for frame 39

Lowest mean error: 3.1283318996429443 mm for frame 238

Saving results

Total time: 137.21106266975403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364474
Iteration 2/25 | Loss: 0.00141036
Iteration 3/25 | Loss: 0.00128997
Iteration 4/25 | Loss: 0.00126127
Iteration 5/25 | Loss: 0.00125148
Iteration 6/25 | Loss: 0.00124932
Iteration 7/25 | Loss: 0.00124932
Iteration 8/25 | Loss: 0.00124932
Iteration 9/25 | Loss: 0.00124932
Iteration 10/25 | Loss: 0.00124932
Iteration 11/25 | Loss: 0.00124932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012493215035647154, 0.0012493215035647154, 0.0012493215035647154, 0.0012493215035647154, 0.0012493215035647154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012493215035647154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42286229
Iteration 2/25 | Loss: 0.00084437
Iteration 3/25 | Loss: 0.00084437
Iteration 4/25 | Loss: 0.00084437
Iteration 5/25 | Loss: 0.00084436
Iteration 6/25 | Loss: 0.00084436
Iteration 7/25 | Loss: 0.00084436
Iteration 8/25 | Loss: 0.00084436
Iteration 9/25 | Loss: 0.00084436
Iteration 10/25 | Loss: 0.00084436
Iteration 11/25 | Loss: 0.00084436
Iteration 12/25 | Loss: 0.00084436
Iteration 13/25 | Loss: 0.00084436
Iteration 14/25 | Loss: 0.00084436
Iteration 15/25 | Loss: 0.00084436
Iteration 16/25 | Loss: 0.00084436
Iteration 17/25 | Loss: 0.00084436
Iteration 18/25 | Loss: 0.00084436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008443631231784821, 0.0008443631231784821, 0.0008443631231784821, 0.0008443631231784821, 0.0008443631231784821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008443631231784821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084436
Iteration 2/1000 | Loss: 0.00005652
Iteration 3/1000 | Loss: 0.00003934
Iteration 4/1000 | Loss: 0.00002998
Iteration 5/1000 | Loss: 0.00002803
Iteration 6/1000 | Loss: 0.00002657
Iteration 7/1000 | Loss: 0.00002554
Iteration 8/1000 | Loss: 0.00002501
Iteration 9/1000 | Loss: 0.00002454
Iteration 10/1000 | Loss: 0.00002421
Iteration 11/1000 | Loss: 0.00002391
Iteration 12/1000 | Loss: 0.00002365
Iteration 13/1000 | Loss: 0.00002349
Iteration 14/1000 | Loss: 0.00002344
Iteration 15/1000 | Loss: 0.00002343
Iteration 16/1000 | Loss: 0.00002342
Iteration 17/1000 | Loss: 0.00002338
Iteration 18/1000 | Loss: 0.00002325
Iteration 19/1000 | Loss: 0.00002322
Iteration 20/1000 | Loss: 0.00002318
Iteration 21/1000 | Loss: 0.00002317
Iteration 22/1000 | Loss: 0.00002316
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002314
Iteration 25/1000 | Loss: 0.00002313
Iteration 26/1000 | Loss: 0.00002312
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002311
Iteration 29/1000 | Loss: 0.00002307
Iteration 30/1000 | Loss: 0.00002307
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002304
Iteration 33/1000 | Loss: 0.00002301
Iteration 34/1000 | Loss: 0.00002300
Iteration 35/1000 | Loss: 0.00002298
Iteration 36/1000 | Loss: 0.00002297
Iteration 37/1000 | Loss: 0.00002296
Iteration 38/1000 | Loss: 0.00002294
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00002293
Iteration 41/1000 | Loss: 0.00002293
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002287
Iteration 45/1000 | Loss: 0.00002287
Iteration 46/1000 | Loss: 0.00002286
Iteration 47/1000 | Loss: 0.00002286
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002284
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002281
Iteration 57/1000 | Loss: 0.00002281
Iteration 58/1000 | Loss: 0.00002280
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002279
Iteration 61/1000 | Loss: 0.00002279
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00002278
Iteration 64/1000 | Loss: 0.00002278
Iteration 65/1000 | Loss: 0.00002277
Iteration 66/1000 | Loss: 0.00002274
Iteration 67/1000 | Loss: 0.00002274
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002272
Iteration 71/1000 | Loss: 0.00002272
Iteration 72/1000 | Loss: 0.00002272
Iteration 73/1000 | Loss: 0.00002271
Iteration 74/1000 | Loss: 0.00002271
Iteration 75/1000 | Loss: 0.00002271
Iteration 76/1000 | Loss: 0.00002271
Iteration 77/1000 | Loss: 0.00002271
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002271
Iteration 80/1000 | Loss: 0.00002270
Iteration 81/1000 | Loss: 0.00002270
Iteration 82/1000 | Loss: 0.00002270
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002269
Iteration 85/1000 | Loss: 0.00002269
Iteration 86/1000 | Loss: 0.00002269
Iteration 87/1000 | Loss: 0.00002269
Iteration 88/1000 | Loss: 0.00002269
Iteration 89/1000 | Loss: 0.00002269
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002268
Iteration 93/1000 | Loss: 0.00002268
Iteration 94/1000 | Loss: 0.00002268
Iteration 95/1000 | Loss: 0.00002268
Iteration 96/1000 | Loss: 0.00002268
Iteration 97/1000 | Loss: 0.00002268
Iteration 98/1000 | Loss: 0.00002268
Iteration 99/1000 | Loss: 0.00002268
Iteration 100/1000 | Loss: 0.00002268
Iteration 101/1000 | Loss: 0.00002268
Iteration 102/1000 | Loss: 0.00002268
Iteration 103/1000 | Loss: 0.00002268
Iteration 104/1000 | Loss: 0.00002268
Iteration 105/1000 | Loss: 0.00002268
Iteration 106/1000 | Loss: 0.00002267
Iteration 107/1000 | Loss: 0.00002267
Iteration 108/1000 | Loss: 0.00002267
Iteration 109/1000 | Loss: 0.00002267
Iteration 110/1000 | Loss: 0.00002267
Iteration 111/1000 | Loss: 0.00002267
Iteration 112/1000 | Loss: 0.00002267
Iteration 113/1000 | Loss: 0.00002267
Iteration 114/1000 | Loss: 0.00002267
Iteration 115/1000 | Loss: 0.00002267
Iteration 116/1000 | Loss: 0.00002267
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002267
Iteration 120/1000 | Loss: 0.00002267
Iteration 121/1000 | Loss: 0.00002267
Iteration 122/1000 | Loss: 0.00002267
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002266
Iteration 126/1000 | Loss: 0.00002266
Iteration 127/1000 | Loss: 0.00002266
Iteration 128/1000 | Loss: 0.00002266
Iteration 129/1000 | Loss: 0.00002266
Iteration 130/1000 | Loss: 0.00002266
Iteration 131/1000 | Loss: 0.00002266
Iteration 132/1000 | Loss: 0.00002266
Iteration 133/1000 | Loss: 0.00002266
Iteration 134/1000 | Loss: 0.00002266
Iteration 135/1000 | Loss: 0.00002266
Iteration 136/1000 | Loss: 0.00002266
Iteration 137/1000 | Loss: 0.00002266
Iteration 138/1000 | Loss: 0.00002266
Iteration 139/1000 | Loss: 0.00002266
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002265
Iteration 142/1000 | Loss: 0.00002265
Iteration 143/1000 | Loss: 0.00002265
Iteration 144/1000 | Loss: 0.00002265
Iteration 145/1000 | Loss: 0.00002265
Iteration 146/1000 | Loss: 0.00002265
Iteration 147/1000 | Loss: 0.00002265
Iteration 148/1000 | Loss: 0.00002265
Iteration 149/1000 | Loss: 0.00002265
Iteration 150/1000 | Loss: 0.00002265
Iteration 151/1000 | Loss: 0.00002265
Iteration 152/1000 | Loss: 0.00002265
Iteration 153/1000 | Loss: 0.00002265
Iteration 154/1000 | Loss: 0.00002265
Iteration 155/1000 | Loss: 0.00002265
Iteration 156/1000 | Loss: 0.00002265
Iteration 157/1000 | Loss: 0.00002265
Iteration 158/1000 | Loss: 0.00002265
Iteration 159/1000 | Loss: 0.00002265
Iteration 160/1000 | Loss: 0.00002264
Iteration 161/1000 | Loss: 0.00002264
Iteration 162/1000 | Loss: 0.00002264
Iteration 163/1000 | Loss: 0.00002264
Iteration 164/1000 | Loss: 0.00002264
Iteration 165/1000 | Loss: 0.00002264
Iteration 166/1000 | Loss: 0.00002264
Iteration 167/1000 | Loss: 0.00002264
Iteration 168/1000 | Loss: 0.00002264
Iteration 169/1000 | Loss: 0.00002264
Iteration 170/1000 | Loss: 0.00002264
Iteration 171/1000 | Loss: 0.00002264
Iteration 172/1000 | Loss: 0.00002264
Iteration 173/1000 | Loss: 0.00002264
Iteration 174/1000 | Loss: 0.00002264
Iteration 175/1000 | Loss: 0.00002264
Iteration 176/1000 | Loss: 0.00002264
Iteration 177/1000 | Loss: 0.00002264
Iteration 178/1000 | Loss: 0.00002264
Iteration 179/1000 | Loss: 0.00002263
Iteration 180/1000 | Loss: 0.00002263
Iteration 181/1000 | Loss: 0.00002263
Iteration 182/1000 | Loss: 0.00002263
Iteration 183/1000 | Loss: 0.00002263
Iteration 184/1000 | Loss: 0.00002263
Iteration 185/1000 | Loss: 0.00002263
Iteration 186/1000 | Loss: 0.00002263
Iteration 187/1000 | Loss: 0.00002263
Iteration 188/1000 | Loss: 0.00002263
Iteration 189/1000 | Loss: 0.00002263
Iteration 190/1000 | Loss: 0.00002263
Iteration 191/1000 | Loss: 0.00002263
Iteration 192/1000 | Loss: 0.00002263
Iteration 193/1000 | Loss: 0.00002263
Iteration 194/1000 | Loss: 0.00002263
Iteration 195/1000 | Loss: 0.00002263
Iteration 196/1000 | Loss: 0.00002263
Iteration 197/1000 | Loss: 0.00002263
Iteration 198/1000 | Loss: 0.00002263
Iteration 199/1000 | Loss: 0.00002263
Iteration 200/1000 | Loss: 0.00002263
Iteration 201/1000 | Loss: 0.00002263
Iteration 202/1000 | Loss: 0.00002263
Iteration 203/1000 | Loss: 0.00002263
Iteration 204/1000 | Loss: 0.00002263
Iteration 205/1000 | Loss: 0.00002263
Iteration 206/1000 | Loss: 0.00002263
Iteration 207/1000 | Loss: 0.00002263
Iteration 208/1000 | Loss: 0.00002263
Iteration 209/1000 | Loss: 0.00002263
Iteration 210/1000 | Loss: 0.00002263
Iteration 211/1000 | Loss: 0.00002263
Iteration 212/1000 | Loss: 0.00002263
Iteration 213/1000 | Loss: 0.00002263
Iteration 214/1000 | Loss: 0.00002263
Iteration 215/1000 | Loss: 0.00002263
Iteration 216/1000 | Loss: 0.00002263
Iteration 217/1000 | Loss: 0.00002263
Iteration 218/1000 | Loss: 0.00002263
Iteration 219/1000 | Loss: 0.00002263
Iteration 220/1000 | Loss: 0.00002263
Iteration 221/1000 | Loss: 0.00002263
Iteration 222/1000 | Loss: 0.00002263
Iteration 223/1000 | Loss: 0.00002263
Iteration 224/1000 | Loss: 0.00002263
Iteration 225/1000 | Loss: 0.00002263
Iteration 226/1000 | Loss: 0.00002263
Iteration 227/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.262723501189612e-05, 2.262723501189612e-05, 2.262723501189612e-05, 2.262723501189612e-05, 2.262723501189612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.262723501189612e-05

Optimization complete. Final v2v error: 3.9466867446899414 mm

Highest mean error: 5.066914081573486 mm for frame 189

Lowest mean error: 2.9038596153259277 mm for frame 223

Saving results

Total time: 51.2089684009552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854707
Iteration 2/25 | Loss: 0.00134613
Iteration 3/25 | Loss: 0.00126623
Iteration 4/25 | Loss: 0.00125423
Iteration 5/25 | Loss: 0.00125152
Iteration 6/25 | Loss: 0.00125152
Iteration 7/25 | Loss: 0.00125152
Iteration 8/25 | Loss: 0.00125152
Iteration 9/25 | Loss: 0.00125152
Iteration 10/25 | Loss: 0.00125152
Iteration 11/25 | Loss: 0.00125152
Iteration 12/25 | Loss: 0.00125152
Iteration 13/25 | Loss: 0.00125152
Iteration 14/25 | Loss: 0.00125152
Iteration 15/25 | Loss: 0.00125152
Iteration 16/25 | Loss: 0.00125152
Iteration 17/25 | Loss: 0.00125152
Iteration 18/25 | Loss: 0.00125152
Iteration 19/25 | Loss: 0.00125152
Iteration 20/25 | Loss: 0.00125152
Iteration 21/25 | Loss: 0.00125152
Iteration 22/25 | Loss: 0.00125152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012515216367319226, 0.0012515216367319226, 0.0012515216367319226, 0.0012515216367319226, 0.0012515216367319226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012515216367319226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46067786
Iteration 2/25 | Loss: 0.00076996
Iteration 3/25 | Loss: 0.00076995
Iteration 4/25 | Loss: 0.00076995
Iteration 5/25 | Loss: 0.00076995
Iteration 6/25 | Loss: 0.00076995
Iteration 7/25 | Loss: 0.00076994
Iteration 8/25 | Loss: 0.00076994
Iteration 9/25 | Loss: 0.00076994
Iteration 10/25 | Loss: 0.00076994
Iteration 11/25 | Loss: 0.00076994
Iteration 12/25 | Loss: 0.00076994
Iteration 13/25 | Loss: 0.00076994
Iteration 14/25 | Loss: 0.00076994
Iteration 15/25 | Loss: 0.00076994
Iteration 16/25 | Loss: 0.00076994
Iteration 17/25 | Loss: 0.00076994
Iteration 18/25 | Loss: 0.00076994
Iteration 19/25 | Loss: 0.00076994
Iteration 20/25 | Loss: 0.00076994
Iteration 21/25 | Loss: 0.00076994
Iteration 22/25 | Loss: 0.00076994
Iteration 23/25 | Loss: 0.00076994
Iteration 24/25 | Loss: 0.00076994
Iteration 25/25 | Loss: 0.00076994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076994
Iteration 2/1000 | Loss: 0.00002992
Iteration 3/1000 | Loss: 0.00002176
Iteration 4/1000 | Loss: 0.00001943
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001561
Iteration 12/1000 | Loss: 0.00001554
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001548
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00001526
Iteration 23/1000 | Loss: 0.00001526
Iteration 24/1000 | Loss: 0.00001520
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001516
Iteration 29/1000 | Loss: 0.00001516
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001515
Iteration 32/1000 | Loss: 0.00001515
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001514
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001513
Iteration 38/1000 | Loss: 0.00001511
Iteration 39/1000 | Loss: 0.00001511
Iteration 40/1000 | Loss: 0.00001511
Iteration 41/1000 | Loss: 0.00001510
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001508
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001504
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001503
Iteration 53/1000 | Loss: 0.00001503
Iteration 54/1000 | Loss: 0.00001503
Iteration 55/1000 | Loss: 0.00001502
Iteration 56/1000 | Loss: 0.00001501
Iteration 57/1000 | Loss: 0.00001501
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001497
Iteration 62/1000 | Loss: 0.00001497
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001489
Iteration 77/1000 | Loss: 0.00001489
Iteration 78/1000 | Loss: 0.00001489
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001486
Iteration 98/1000 | Loss: 0.00001486
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001486
Iteration 101/1000 | Loss: 0.00001486
Iteration 102/1000 | Loss: 0.00001485
Iteration 103/1000 | Loss: 0.00001485
Iteration 104/1000 | Loss: 0.00001485
Iteration 105/1000 | Loss: 0.00001483
Iteration 106/1000 | Loss: 0.00001483
Iteration 107/1000 | Loss: 0.00001483
Iteration 108/1000 | Loss: 0.00001483
Iteration 109/1000 | Loss: 0.00001483
Iteration 110/1000 | Loss: 0.00001482
Iteration 111/1000 | Loss: 0.00001482
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001479
Iteration 120/1000 | Loss: 0.00001479
Iteration 121/1000 | Loss: 0.00001479
Iteration 122/1000 | Loss: 0.00001479
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001478
Iteration 127/1000 | Loss: 0.00001477
Iteration 128/1000 | Loss: 0.00001477
Iteration 129/1000 | Loss: 0.00001477
Iteration 130/1000 | Loss: 0.00001477
Iteration 131/1000 | Loss: 0.00001477
Iteration 132/1000 | Loss: 0.00001477
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00001476
Iteration 136/1000 | Loss: 0.00001476
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001475
Iteration 139/1000 | Loss: 0.00001475
Iteration 140/1000 | Loss: 0.00001475
Iteration 141/1000 | Loss: 0.00001475
Iteration 142/1000 | Loss: 0.00001475
Iteration 143/1000 | Loss: 0.00001475
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001474
Iteration 150/1000 | Loss: 0.00001474
Iteration 151/1000 | Loss: 0.00001474
Iteration 152/1000 | Loss: 0.00001474
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Iteration 156/1000 | Loss: 0.00001474
Iteration 157/1000 | Loss: 0.00001474
Iteration 158/1000 | Loss: 0.00001474
Iteration 159/1000 | Loss: 0.00001474
Iteration 160/1000 | Loss: 0.00001474
Iteration 161/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.4742478015250526e-05, 1.4742478015250526e-05, 1.4742478015250526e-05, 1.4742478015250526e-05, 1.4742478015250526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4742478015250526e-05

Optimization complete. Final v2v error: 3.250507354736328 mm

Highest mean error: 4.160946846008301 mm for frame 92

Lowest mean error: 3.042206048965454 mm for frame 0

Saving results

Total time: 41.31068563461304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00572785
Iteration 2/25 | Loss: 0.00130992
Iteration 3/25 | Loss: 0.00124544
Iteration 4/25 | Loss: 0.00123648
Iteration 5/25 | Loss: 0.00123320
Iteration 6/25 | Loss: 0.00123292
Iteration 7/25 | Loss: 0.00123292
Iteration 8/25 | Loss: 0.00123292
Iteration 9/25 | Loss: 0.00123292
Iteration 10/25 | Loss: 0.00123292
Iteration 11/25 | Loss: 0.00123292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012329155579209328, 0.0012329155579209328, 0.0012329155579209328, 0.0012329155579209328, 0.0012329155579209328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012329155579209328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83376050
Iteration 2/25 | Loss: 0.00085535
Iteration 3/25 | Loss: 0.00085535
Iteration 4/25 | Loss: 0.00085535
Iteration 5/25 | Loss: 0.00085535
Iteration 6/25 | Loss: 0.00085535
Iteration 7/25 | Loss: 0.00085535
Iteration 8/25 | Loss: 0.00085535
Iteration 9/25 | Loss: 0.00085535
Iteration 10/25 | Loss: 0.00085535
Iteration 11/25 | Loss: 0.00085535
Iteration 12/25 | Loss: 0.00085535
Iteration 13/25 | Loss: 0.00085535
Iteration 14/25 | Loss: 0.00085535
Iteration 15/25 | Loss: 0.00085535
Iteration 16/25 | Loss: 0.00085535
Iteration 17/25 | Loss: 0.00085535
Iteration 18/25 | Loss: 0.00085535
Iteration 19/25 | Loss: 0.00085535
Iteration 20/25 | Loss: 0.00085535
Iteration 21/25 | Loss: 0.00085535
Iteration 22/25 | Loss: 0.00085535
Iteration 23/25 | Loss: 0.00085535
Iteration 24/25 | Loss: 0.00085535
Iteration 25/25 | Loss: 0.00085535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085535
Iteration 2/1000 | Loss: 0.00002358
Iteration 3/1000 | Loss: 0.00001594
Iteration 4/1000 | Loss: 0.00001439
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001318
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001243
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001222
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001193
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001165
Iteration 125/1000 | Loss: 0.00001165
Iteration 126/1000 | Loss: 0.00001165
Iteration 127/1000 | Loss: 0.00001165
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001164
Iteration 130/1000 | Loss: 0.00001164
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001164
Iteration 136/1000 | Loss: 0.00001164
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001164
Iteration 139/1000 | Loss: 0.00001164
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001162
Iteration 156/1000 | Loss: 0.00001162
Iteration 157/1000 | Loss: 0.00001162
Iteration 158/1000 | Loss: 0.00001162
Iteration 159/1000 | Loss: 0.00001162
Iteration 160/1000 | Loss: 0.00001162
Iteration 161/1000 | Loss: 0.00001162
Iteration 162/1000 | Loss: 0.00001162
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001161
Iteration 165/1000 | Loss: 0.00001161
Iteration 166/1000 | Loss: 0.00001161
Iteration 167/1000 | Loss: 0.00001161
Iteration 168/1000 | Loss: 0.00001161
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001161
Iteration 171/1000 | Loss: 0.00001161
Iteration 172/1000 | Loss: 0.00001161
Iteration 173/1000 | Loss: 0.00001161
Iteration 174/1000 | Loss: 0.00001161
Iteration 175/1000 | Loss: 0.00001161
Iteration 176/1000 | Loss: 0.00001161
Iteration 177/1000 | Loss: 0.00001161
Iteration 178/1000 | Loss: 0.00001161
Iteration 179/1000 | Loss: 0.00001161
Iteration 180/1000 | Loss: 0.00001161
Iteration 181/1000 | Loss: 0.00001161
Iteration 182/1000 | Loss: 0.00001161
Iteration 183/1000 | Loss: 0.00001161
Iteration 184/1000 | Loss: 0.00001161
Iteration 185/1000 | Loss: 0.00001161
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001161
Iteration 191/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.160675583378179e-05, 1.160675583378179e-05, 1.160675583378179e-05, 1.160675583378179e-05, 1.160675583378179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.160675583378179e-05

Optimization complete. Final v2v error: 2.9318108558654785 mm

Highest mean error: 3.333146810531616 mm for frame 112

Lowest mean error: 2.7776174545288086 mm for frame 39

Saving results

Total time: 37.236117124557495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022067
Iteration 2/25 | Loss: 0.00234395
Iteration 3/25 | Loss: 0.00181739
Iteration 4/25 | Loss: 0.00156557
Iteration 5/25 | Loss: 0.00144755
Iteration 6/25 | Loss: 0.00142620
Iteration 7/25 | Loss: 0.00135301
Iteration 8/25 | Loss: 0.00133404
Iteration 9/25 | Loss: 0.00128888
Iteration 10/25 | Loss: 0.00127757
Iteration 11/25 | Loss: 0.00127238
Iteration 12/25 | Loss: 0.00126916
Iteration 13/25 | Loss: 0.00126755
Iteration 14/25 | Loss: 0.00126811
Iteration 15/25 | Loss: 0.00126904
Iteration 16/25 | Loss: 0.00126730
Iteration 17/25 | Loss: 0.00126578
Iteration 18/25 | Loss: 0.00126726
Iteration 19/25 | Loss: 0.00126516
Iteration 20/25 | Loss: 0.00126545
Iteration 21/25 | Loss: 0.00126448
Iteration 22/25 | Loss: 0.00126447
Iteration 23/25 | Loss: 0.00126511
Iteration 24/25 | Loss: 0.00126443
Iteration 25/25 | Loss: 0.00126443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53460395
Iteration 2/25 | Loss: 0.00098257
Iteration 3/25 | Loss: 0.00098665
Iteration 4/25 | Loss: 0.00098665
Iteration 5/25 | Loss: 0.00098665
Iteration 6/25 | Loss: 0.00098665
Iteration 7/25 | Loss: 0.00098665
Iteration 8/25 | Loss: 0.00098665
Iteration 9/25 | Loss: 0.00096212
Iteration 10/25 | Loss: 0.00094697
Iteration 11/25 | Loss: 0.00091824
Iteration 12/25 | Loss: 0.00091812
Iteration 13/25 | Loss: 0.00091313
Iteration 14/25 | Loss: 0.00091313
Iteration 15/25 | Loss: 0.00091313
Iteration 16/25 | Loss: 0.00091313
Iteration 17/25 | Loss: 0.00091313
Iteration 18/25 | Loss: 0.00091313
Iteration 19/25 | Loss: 0.00091313
Iteration 20/25 | Loss: 0.00091313
Iteration 21/25 | Loss: 0.00091313
Iteration 22/25 | Loss: 0.00091313
Iteration 23/25 | Loss: 0.00091313
Iteration 24/25 | Loss: 0.00091313
Iteration 25/25 | Loss: 0.00091313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091313
Iteration 2/1000 | Loss: 0.00004651
Iteration 3/1000 | Loss: 0.00026591
Iteration 4/1000 | Loss: 0.00025875
Iteration 5/1000 | Loss: 0.00007286
Iteration 6/1000 | Loss: 0.00039301
Iteration 7/1000 | Loss: 0.00030580
Iteration 8/1000 | Loss: 0.00013825
Iteration 9/1000 | Loss: 0.00040118
Iteration 10/1000 | Loss: 0.00050932
Iteration 11/1000 | Loss: 0.00031032
Iteration 12/1000 | Loss: 0.00035578
Iteration 13/1000 | Loss: 0.00015118
Iteration 14/1000 | Loss: 0.00012804
Iteration 15/1000 | Loss: 0.00011827
Iteration 16/1000 | Loss: 0.00011724
Iteration 17/1000 | Loss: 0.00043404
Iteration 18/1000 | Loss: 0.00028821
Iteration 19/1000 | Loss: 0.00035219
Iteration 20/1000 | Loss: 0.00042996
Iteration 21/1000 | Loss: 0.00044386
Iteration 22/1000 | Loss: 0.00029510
Iteration 23/1000 | Loss: 0.00024050
Iteration 24/1000 | Loss: 0.00003340
Iteration 25/1000 | Loss: 0.00002328
Iteration 26/1000 | Loss: 0.00031284
Iteration 27/1000 | Loss: 0.00004637
Iteration 28/1000 | Loss: 0.00002860
Iteration 29/1000 | Loss: 0.00002893
Iteration 30/1000 | Loss: 0.00002329
Iteration 31/1000 | Loss: 0.00019767
Iteration 32/1000 | Loss: 0.00015536
Iteration 33/1000 | Loss: 0.00017250
Iteration 34/1000 | Loss: 0.00025116
Iteration 35/1000 | Loss: 0.00009434
Iteration 36/1000 | Loss: 0.00003093
Iteration 37/1000 | Loss: 0.00020865
Iteration 38/1000 | Loss: 0.00016015
Iteration 39/1000 | Loss: 0.00014787
Iteration 40/1000 | Loss: 0.00002503
Iteration 41/1000 | Loss: 0.00004182
Iteration 42/1000 | Loss: 0.00004783
Iteration 43/1000 | Loss: 0.00002093
Iteration 44/1000 | Loss: 0.00006566
Iteration 45/1000 | Loss: 0.00002101
Iteration 46/1000 | Loss: 0.00002292
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00003578
Iteration 50/1000 | Loss: 0.00001903
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00085266
Iteration 53/1000 | Loss: 0.00062778
Iteration 54/1000 | Loss: 0.00030200
Iteration 55/1000 | Loss: 0.00028077
Iteration 56/1000 | Loss: 0.00003982
Iteration 57/1000 | Loss: 0.00002255
Iteration 58/1000 | Loss: 0.00002057
Iteration 59/1000 | Loss: 0.00005608
Iteration 60/1000 | Loss: 0.00001644
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00002573
Iteration 63/1000 | Loss: 0.00005814
Iteration 64/1000 | Loss: 0.00001516
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00004202
Iteration 67/1000 | Loss: 0.00002517
Iteration 68/1000 | Loss: 0.00003414
Iteration 69/1000 | Loss: 0.00001776
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00003106
Iteration 73/1000 | Loss: 0.00015201
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001324
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00002354
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001315
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001302
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001301
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001301
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001301
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001301
Iteration 144/1000 | Loss: 0.00001301
Iteration 145/1000 | Loss: 0.00001301
Iteration 146/1000 | Loss: 0.00001301
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001301
Iteration 151/1000 | Loss: 0.00001301
Iteration 152/1000 | Loss: 0.00001301
Iteration 153/1000 | Loss: 0.00001301
Iteration 154/1000 | Loss: 0.00001301
Iteration 155/1000 | Loss: 0.00001301
Iteration 156/1000 | Loss: 0.00001301
Iteration 157/1000 | Loss: 0.00001301
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001301
Iteration 160/1000 | Loss: 0.00001301
Iteration 161/1000 | Loss: 0.00001301
Iteration 162/1000 | Loss: 0.00001301
Iteration 163/1000 | Loss: 0.00001301
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Iteration 168/1000 | Loss: 0.00001301
Iteration 169/1000 | Loss: 0.00001301
Iteration 170/1000 | Loss: 0.00001301
Iteration 171/1000 | Loss: 0.00001301
Iteration 172/1000 | Loss: 0.00001301
Iteration 173/1000 | Loss: 0.00001301
Iteration 174/1000 | Loss: 0.00001301
Iteration 175/1000 | Loss: 0.00001301
Iteration 176/1000 | Loss: 0.00001301
Iteration 177/1000 | Loss: 0.00001301
Iteration 178/1000 | Loss: 0.00001301
Iteration 179/1000 | Loss: 0.00001301
Iteration 180/1000 | Loss: 0.00001301
Iteration 181/1000 | Loss: 0.00001301
Iteration 182/1000 | Loss: 0.00001301
Iteration 183/1000 | Loss: 0.00001301
Iteration 184/1000 | Loss: 0.00001301
Iteration 185/1000 | Loss: 0.00001301
Iteration 186/1000 | Loss: 0.00001301
Iteration 187/1000 | Loss: 0.00001301
Iteration 188/1000 | Loss: 0.00001301
Iteration 189/1000 | Loss: 0.00001301
Iteration 190/1000 | Loss: 0.00001301
Iteration 191/1000 | Loss: 0.00001301
Iteration 192/1000 | Loss: 0.00001301
Iteration 193/1000 | Loss: 0.00001301
Iteration 194/1000 | Loss: 0.00001301
Iteration 195/1000 | Loss: 0.00001301
Iteration 196/1000 | Loss: 0.00001301
Iteration 197/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.3009893336857203e-05, 1.3009893336857203e-05, 1.3009893336857203e-05, 1.3009893336857203e-05, 1.3009893336857203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3009893336857203e-05

Optimization complete. Final v2v error: 3.063913106918335 mm

Highest mean error: 4.003642559051514 mm for frame 153

Lowest mean error: 2.843106746673584 mm for frame 27

Saving results

Total time: 157.86673259735107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987757
Iteration 2/25 | Loss: 0.00987757
Iteration 3/25 | Loss: 0.00987756
Iteration 4/25 | Loss: 0.00255643
Iteration 5/25 | Loss: 0.00197241
Iteration 6/25 | Loss: 0.00192237
Iteration 7/25 | Loss: 0.00183821
Iteration 8/25 | Loss: 0.00164347
Iteration 9/25 | Loss: 0.00150812
Iteration 10/25 | Loss: 0.00145209
Iteration 11/25 | Loss: 0.00142881
Iteration 12/25 | Loss: 0.00141668
Iteration 13/25 | Loss: 0.00141199
Iteration 14/25 | Loss: 0.00140883
Iteration 15/25 | Loss: 0.00140841
Iteration 16/25 | Loss: 0.00140616
Iteration 17/25 | Loss: 0.00140443
Iteration 18/25 | Loss: 0.00140548
Iteration 19/25 | Loss: 0.00140169
Iteration 20/25 | Loss: 0.00140327
Iteration 21/25 | Loss: 0.00140273
Iteration 22/25 | Loss: 0.00140253
Iteration 23/25 | Loss: 0.00140797
Iteration 24/25 | Loss: 0.00140396
Iteration 25/25 | Loss: 0.00139951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42277181
Iteration 2/25 | Loss: 0.00114382
Iteration 3/25 | Loss: 0.00104384
Iteration 4/25 | Loss: 0.00104384
Iteration 5/25 | Loss: 0.00104384
Iteration 6/25 | Loss: 0.00104384
Iteration 7/25 | Loss: 0.00104384
Iteration 8/25 | Loss: 0.00104384
Iteration 9/25 | Loss: 0.00104384
Iteration 10/25 | Loss: 0.00104384
Iteration 11/25 | Loss: 0.00104384
Iteration 12/25 | Loss: 0.00104384
Iteration 13/25 | Loss: 0.00104384
Iteration 14/25 | Loss: 0.00104384
Iteration 15/25 | Loss: 0.00104384
Iteration 16/25 | Loss: 0.00104384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001043836586177349, 0.001043836586177349, 0.001043836586177349, 0.001043836586177349, 0.001043836586177349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043836586177349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104384
Iteration 2/1000 | Loss: 0.00127860
Iteration 3/1000 | Loss: 0.00091005
Iteration 4/1000 | Loss: 0.00038913
Iteration 5/1000 | Loss: 0.00011591
Iteration 6/1000 | Loss: 0.00010290
Iteration 7/1000 | Loss: 0.00016542
Iteration 8/1000 | Loss: 0.00019609
Iteration 9/1000 | Loss: 0.00008574
Iteration 10/1000 | Loss: 0.00006053
Iteration 11/1000 | Loss: 0.00006294
Iteration 12/1000 | Loss: 0.00005781
Iteration 13/1000 | Loss: 0.00005099
Iteration 14/1000 | Loss: 0.00004585
Iteration 15/1000 | Loss: 0.00071870
Iteration 16/1000 | Loss: 0.00044711
Iteration 17/1000 | Loss: 0.00010917
Iteration 18/1000 | Loss: 0.00024074
Iteration 19/1000 | Loss: 0.00005399
Iteration 20/1000 | Loss: 0.00010743
Iteration 21/1000 | Loss: 0.00007811
Iteration 22/1000 | Loss: 0.00005029
Iteration 23/1000 | Loss: 0.00004344
Iteration 24/1000 | Loss: 0.00003961
Iteration 25/1000 | Loss: 0.00015854
Iteration 26/1000 | Loss: 0.00003727
Iteration 27/1000 | Loss: 0.00003789
Iteration 28/1000 | Loss: 0.00019144
Iteration 29/1000 | Loss: 0.00023515
Iteration 30/1000 | Loss: 0.00006082
Iteration 31/1000 | Loss: 0.00009644
Iteration 32/1000 | Loss: 0.00004855
Iteration 33/1000 | Loss: 0.00009532
Iteration 34/1000 | Loss: 0.00007503
Iteration 35/1000 | Loss: 0.00003134
Iteration 36/1000 | Loss: 0.00002982
Iteration 37/1000 | Loss: 0.00002903
Iteration 38/1000 | Loss: 0.00023097
Iteration 39/1000 | Loss: 0.00003321
Iteration 40/1000 | Loss: 0.00003108
Iteration 41/1000 | Loss: 0.00003007
Iteration 42/1000 | Loss: 0.00008089
Iteration 43/1000 | Loss: 0.00002842
Iteration 44/1000 | Loss: 0.00002794
Iteration 45/1000 | Loss: 0.00002758
Iteration 46/1000 | Loss: 0.00002735
Iteration 47/1000 | Loss: 0.00002711
Iteration 48/1000 | Loss: 0.00005047
Iteration 49/1000 | Loss: 0.00004350
Iteration 50/1000 | Loss: 0.00004790
Iteration 51/1000 | Loss: 0.00003730
Iteration 52/1000 | Loss: 0.00004811
Iteration 53/1000 | Loss: 0.00003578
Iteration 54/1000 | Loss: 0.00004848
Iteration 55/1000 | Loss: 0.00003409
Iteration 56/1000 | Loss: 0.00004874
Iteration 57/1000 | Loss: 0.00012257
Iteration 58/1000 | Loss: 0.00004218
Iteration 59/1000 | Loss: 0.00002857
Iteration 60/1000 | Loss: 0.00002760
Iteration 61/1000 | Loss: 0.00002700
Iteration 62/1000 | Loss: 0.00002671
Iteration 63/1000 | Loss: 0.00002652
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002630
Iteration 66/1000 | Loss: 0.00002622
Iteration 67/1000 | Loss: 0.00002620
Iteration 68/1000 | Loss: 0.00002619
Iteration 69/1000 | Loss: 0.00002619
Iteration 70/1000 | Loss: 0.00002612
Iteration 71/1000 | Loss: 0.00002612
Iteration 72/1000 | Loss: 0.00002612
Iteration 73/1000 | Loss: 0.00002612
Iteration 74/1000 | Loss: 0.00002612
Iteration 75/1000 | Loss: 0.00002611
Iteration 76/1000 | Loss: 0.00002611
Iteration 77/1000 | Loss: 0.00002611
Iteration 78/1000 | Loss: 0.00002611
Iteration 79/1000 | Loss: 0.00002610
Iteration 80/1000 | Loss: 0.00002610
Iteration 81/1000 | Loss: 0.00002610
Iteration 82/1000 | Loss: 0.00002609
Iteration 83/1000 | Loss: 0.00002609
Iteration 84/1000 | Loss: 0.00002609
Iteration 85/1000 | Loss: 0.00002609
Iteration 86/1000 | Loss: 0.00002609
Iteration 87/1000 | Loss: 0.00002609
Iteration 88/1000 | Loss: 0.00002609
Iteration 89/1000 | Loss: 0.00002609
Iteration 90/1000 | Loss: 0.00002608
Iteration 91/1000 | Loss: 0.00002608
Iteration 92/1000 | Loss: 0.00002608
Iteration 93/1000 | Loss: 0.00002608
Iteration 94/1000 | Loss: 0.00002608
Iteration 95/1000 | Loss: 0.00002607
Iteration 96/1000 | Loss: 0.00002607
Iteration 97/1000 | Loss: 0.00002607
Iteration 98/1000 | Loss: 0.00002606
Iteration 99/1000 | Loss: 0.00002606
Iteration 100/1000 | Loss: 0.00002606
Iteration 101/1000 | Loss: 0.00002606
Iteration 102/1000 | Loss: 0.00002606
Iteration 103/1000 | Loss: 0.00002606
Iteration 104/1000 | Loss: 0.00002606
Iteration 105/1000 | Loss: 0.00002606
Iteration 106/1000 | Loss: 0.00002606
Iteration 107/1000 | Loss: 0.00002605
Iteration 108/1000 | Loss: 0.00002605
Iteration 109/1000 | Loss: 0.00002605
Iteration 110/1000 | Loss: 0.00002605
Iteration 111/1000 | Loss: 0.00002605
Iteration 112/1000 | Loss: 0.00002605
Iteration 113/1000 | Loss: 0.00002605
Iteration 114/1000 | Loss: 0.00002605
Iteration 115/1000 | Loss: 0.00002604
Iteration 116/1000 | Loss: 0.00002603
Iteration 117/1000 | Loss: 0.00002603
Iteration 118/1000 | Loss: 0.00002603
Iteration 119/1000 | Loss: 0.00002603
Iteration 120/1000 | Loss: 0.00002603
Iteration 121/1000 | Loss: 0.00002603
Iteration 122/1000 | Loss: 0.00002603
Iteration 123/1000 | Loss: 0.00002603
Iteration 124/1000 | Loss: 0.00002603
Iteration 125/1000 | Loss: 0.00002602
Iteration 126/1000 | Loss: 0.00002602
Iteration 127/1000 | Loss: 0.00002602
Iteration 128/1000 | Loss: 0.00002602
Iteration 129/1000 | Loss: 0.00002602
Iteration 130/1000 | Loss: 0.00002602
Iteration 131/1000 | Loss: 0.00002602
Iteration 132/1000 | Loss: 0.00002602
Iteration 133/1000 | Loss: 0.00002602
Iteration 134/1000 | Loss: 0.00002602
Iteration 135/1000 | Loss: 0.00002602
Iteration 136/1000 | Loss: 0.00002602
Iteration 137/1000 | Loss: 0.00002601
Iteration 138/1000 | Loss: 0.00002601
Iteration 139/1000 | Loss: 0.00002601
Iteration 140/1000 | Loss: 0.00002601
Iteration 141/1000 | Loss: 0.00002600
Iteration 142/1000 | Loss: 0.00002600
Iteration 143/1000 | Loss: 0.00002600
Iteration 144/1000 | Loss: 0.00002600
Iteration 145/1000 | Loss: 0.00002600
Iteration 146/1000 | Loss: 0.00002600
Iteration 147/1000 | Loss: 0.00002600
Iteration 148/1000 | Loss: 0.00002600
Iteration 149/1000 | Loss: 0.00002599
Iteration 150/1000 | Loss: 0.00002599
Iteration 151/1000 | Loss: 0.00002599
Iteration 152/1000 | Loss: 0.00002598
Iteration 153/1000 | Loss: 0.00002598
Iteration 154/1000 | Loss: 0.00002598
Iteration 155/1000 | Loss: 0.00002598
Iteration 156/1000 | Loss: 0.00002598
Iteration 157/1000 | Loss: 0.00002597
Iteration 158/1000 | Loss: 0.00002597
Iteration 159/1000 | Loss: 0.00002597
Iteration 160/1000 | Loss: 0.00002597
Iteration 161/1000 | Loss: 0.00002597
Iteration 162/1000 | Loss: 0.00002597
Iteration 163/1000 | Loss: 0.00002597
Iteration 164/1000 | Loss: 0.00002596
Iteration 165/1000 | Loss: 0.00002596
Iteration 166/1000 | Loss: 0.00002596
Iteration 167/1000 | Loss: 0.00002596
Iteration 168/1000 | Loss: 0.00002595
Iteration 169/1000 | Loss: 0.00002595
Iteration 170/1000 | Loss: 0.00002595
Iteration 171/1000 | Loss: 0.00002595
Iteration 172/1000 | Loss: 0.00002595
Iteration 173/1000 | Loss: 0.00002594
Iteration 174/1000 | Loss: 0.00002594
Iteration 175/1000 | Loss: 0.00002594
Iteration 176/1000 | Loss: 0.00002593
Iteration 177/1000 | Loss: 0.00002593
Iteration 178/1000 | Loss: 0.00002593
Iteration 179/1000 | Loss: 0.00002593
Iteration 180/1000 | Loss: 0.00002592
Iteration 181/1000 | Loss: 0.00002592
Iteration 182/1000 | Loss: 0.00002592
Iteration 183/1000 | Loss: 0.00002592
Iteration 184/1000 | Loss: 0.00002591
Iteration 185/1000 | Loss: 0.00002591
Iteration 186/1000 | Loss: 0.00002591
Iteration 187/1000 | Loss: 0.00002591
Iteration 188/1000 | Loss: 0.00002591
Iteration 189/1000 | Loss: 0.00002591
Iteration 190/1000 | Loss: 0.00002590
Iteration 191/1000 | Loss: 0.00002590
Iteration 192/1000 | Loss: 0.00002590
Iteration 193/1000 | Loss: 0.00002590
Iteration 194/1000 | Loss: 0.00002590
Iteration 195/1000 | Loss: 0.00002590
Iteration 196/1000 | Loss: 0.00002589
Iteration 197/1000 | Loss: 0.00002589
Iteration 198/1000 | Loss: 0.00002589
Iteration 199/1000 | Loss: 0.00002589
Iteration 200/1000 | Loss: 0.00002589
Iteration 201/1000 | Loss: 0.00002588
Iteration 202/1000 | Loss: 0.00002588
Iteration 203/1000 | Loss: 0.00002588
Iteration 204/1000 | Loss: 0.00002588
Iteration 205/1000 | Loss: 0.00002588
Iteration 206/1000 | Loss: 0.00002588
Iteration 207/1000 | Loss: 0.00002588
Iteration 208/1000 | Loss: 0.00002588
Iteration 209/1000 | Loss: 0.00002588
Iteration 210/1000 | Loss: 0.00002588
Iteration 211/1000 | Loss: 0.00002587
Iteration 212/1000 | Loss: 0.00002587
Iteration 213/1000 | Loss: 0.00008339
Iteration 214/1000 | Loss: 0.00002665
Iteration 215/1000 | Loss: 0.00002605
Iteration 216/1000 | Loss: 0.00002588
Iteration 217/1000 | Loss: 0.00002587
Iteration 218/1000 | Loss: 0.00002585
Iteration 219/1000 | Loss: 0.00002585
Iteration 220/1000 | Loss: 0.00002585
Iteration 221/1000 | Loss: 0.00002584
Iteration 222/1000 | Loss: 0.00002584
Iteration 223/1000 | Loss: 0.00002584
Iteration 224/1000 | Loss: 0.00002584
Iteration 225/1000 | Loss: 0.00002584
Iteration 226/1000 | Loss: 0.00002584
Iteration 227/1000 | Loss: 0.00002584
Iteration 228/1000 | Loss: 0.00002583
Iteration 229/1000 | Loss: 0.00002583
Iteration 230/1000 | Loss: 0.00002583
Iteration 231/1000 | Loss: 0.00002583
Iteration 232/1000 | Loss: 0.00002583
Iteration 233/1000 | Loss: 0.00002583
Iteration 234/1000 | Loss: 0.00002583
Iteration 235/1000 | Loss: 0.00002583
Iteration 236/1000 | Loss: 0.00002583
Iteration 237/1000 | Loss: 0.00002583
Iteration 238/1000 | Loss: 0.00002582
Iteration 239/1000 | Loss: 0.00002582
Iteration 240/1000 | Loss: 0.00002582
Iteration 241/1000 | Loss: 0.00002582
Iteration 242/1000 | Loss: 0.00002582
Iteration 243/1000 | Loss: 0.00002582
Iteration 244/1000 | Loss: 0.00002582
Iteration 245/1000 | Loss: 0.00002582
Iteration 246/1000 | Loss: 0.00002582
Iteration 247/1000 | Loss: 0.00002582
Iteration 248/1000 | Loss: 0.00002582
Iteration 249/1000 | Loss: 0.00002582
Iteration 250/1000 | Loss: 0.00002581
Iteration 251/1000 | Loss: 0.00002581
Iteration 252/1000 | Loss: 0.00002581
Iteration 253/1000 | Loss: 0.00002581
Iteration 254/1000 | Loss: 0.00002581
Iteration 255/1000 | Loss: 0.00002581
Iteration 256/1000 | Loss: 0.00002581
Iteration 257/1000 | Loss: 0.00002580
Iteration 258/1000 | Loss: 0.00002580
Iteration 259/1000 | Loss: 0.00002580
Iteration 260/1000 | Loss: 0.00002580
Iteration 261/1000 | Loss: 0.00002580
Iteration 262/1000 | Loss: 0.00002580
Iteration 263/1000 | Loss: 0.00002580
Iteration 264/1000 | Loss: 0.00002580
Iteration 265/1000 | Loss: 0.00002580
Iteration 266/1000 | Loss: 0.00002580
Iteration 267/1000 | Loss: 0.00002580
Iteration 268/1000 | Loss: 0.00002580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [2.5803423341130838e-05, 2.5803423341130838e-05, 2.5803423341130838e-05, 2.5803423341130838e-05, 2.5803423341130838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5803423341130838e-05

Optimization complete. Final v2v error: 4.166264533996582 mm

Highest mean error: 6.1690449714660645 mm for frame 143

Lowest mean error: 3.555140733718872 mm for frame 66

Saving results

Total time: 172.74274516105652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027461
Iteration 2/25 | Loss: 0.00249697
Iteration 3/25 | Loss: 0.00188465
Iteration 4/25 | Loss: 0.00186764
Iteration 5/25 | Loss: 0.00155486
Iteration 6/25 | Loss: 0.00153156
Iteration 7/25 | Loss: 0.00150366
Iteration 8/25 | Loss: 0.00148847
Iteration 9/25 | Loss: 0.00149218
Iteration 10/25 | Loss: 0.00157908
Iteration 11/25 | Loss: 0.00162354
Iteration 12/25 | Loss: 0.00148720
Iteration 13/25 | Loss: 0.00142241
Iteration 14/25 | Loss: 0.00140844
Iteration 15/25 | Loss: 0.00141284
Iteration 16/25 | Loss: 0.00140389
Iteration 17/25 | Loss: 0.00140091
Iteration 18/25 | Loss: 0.00139654
Iteration 19/25 | Loss: 0.00140373
Iteration 20/25 | Loss: 0.00139932
Iteration 21/25 | Loss: 0.00139317
Iteration 22/25 | Loss: 0.00142686
Iteration 23/25 | Loss: 0.00142552
Iteration 24/25 | Loss: 0.00139084
Iteration 25/25 | Loss: 0.00138122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44473135
Iteration 2/25 | Loss: 0.00254433
Iteration 3/25 | Loss: 0.00191895
Iteration 4/25 | Loss: 0.00191895
Iteration 5/25 | Loss: 0.00191895
Iteration 6/25 | Loss: 0.00191895
Iteration 7/25 | Loss: 0.00191895
Iteration 8/25 | Loss: 0.00191895
Iteration 9/25 | Loss: 0.00191895
Iteration 10/25 | Loss: 0.00191895
Iteration 11/25 | Loss: 0.00191895
Iteration 12/25 | Loss: 0.00191895
Iteration 13/25 | Loss: 0.00191895
Iteration 14/25 | Loss: 0.00191895
Iteration 15/25 | Loss: 0.00191894
Iteration 16/25 | Loss: 0.00191894
Iteration 17/25 | Loss: 0.00191894
Iteration 18/25 | Loss: 0.00191894
Iteration 19/25 | Loss: 0.00191894
Iteration 20/25 | Loss: 0.00191894
Iteration 21/25 | Loss: 0.00191894
Iteration 22/25 | Loss: 0.00191894
Iteration 23/25 | Loss: 0.00191894
Iteration 24/25 | Loss: 0.00191894
Iteration 25/25 | Loss: 0.00191894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191894
Iteration 2/1000 | Loss: 0.00017725
Iteration 3/1000 | Loss: 0.00069813
Iteration 4/1000 | Loss: 0.00011018
Iteration 5/1000 | Loss: 0.00045675
Iteration 6/1000 | Loss: 0.00010719
Iteration 7/1000 | Loss: 0.00009380
Iteration 8/1000 | Loss: 0.00008471
Iteration 9/1000 | Loss: 0.00008041
Iteration 10/1000 | Loss: 0.00009744
Iteration 11/1000 | Loss: 0.00008997
Iteration 12/1000 | Loss: 0.00007689
Iteration 13/1000 | Loss: 0.00051177
Iteration 14/1000 | Loss: 0.00066677
Iteration 15/1000 | Loss: 0.00269046
Iteration 16/1000 | Loss: 0.00246717
Iteration 17/1000 | Loss: 0.00023344
Iteration 18/1000 | Loss: 0.00032056
Iteration 19/1000 | Loss: 0.00016042
Iteration 20/1000 | Loss: 0.00019517
Iteration 21/1000 | Loss: 0.00014520
Iteration 22/1000 | Loss: 0.00015568
Iteration 23/1000 | Loss: 0.00058306
Iteration 24/1000 | Loss: 0.00098214
Iteration 25/1000 | Loss: 0.00016348
Iteration 26/1000 | Loss: 0.00034504
Iteration 27/1000 | Loss: 0.00004728
Iteration 28/1000 | Loss: 0.00003562
Iteration 29/1000 | Loss: 0.00015187
Iteration 30/1000 | Loss: 0.00003186
Iteration 31/1000 | Loss: 0.00013570
Iteration 32/1000 | Loss: 0.00011505
Iteration 33/1000 | Loss: 0.00009464
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002154
Iteration 36/1000 | Loss: 0.00001998
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00022401
Iteration 40/1000 | Loss: 0.00002169
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001571
Iteration 63/1000 | Loss: 0.00001570
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001568
Iteration 68/1000 | Loss: 0.00001568
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001563
Iteration 98/1000 | Loss: 0.00001563
Iteration 99/1000 | Loss: 0.00001563
Iteration 100/1000 | Loss: 0.00001562
Iteration 101/1000 | Loss: 0.00001562
Iteration 102/1000 | Loss: 0.00001562
Iteration 103/1000 | Loss: 0.00001562
Iteration 104/1000 | Loss: 0.00001562
Iteration 105/1000 | Loss: 0.00001562
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001562
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001562
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.5621568309143186e-05, 1.5621568309143186e-05, 1.5621568309143186e-05, 1.5621568309143186e-05, 1.5621568309143186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5621568309143186e-05

Optimization complete. Final v2v error: 3.3698883056640625 mm

Highest mean error: 4.762089729309082 mm for frame 121

Lowest mean error: 3.086111068725586 mm for frame 54

Saving results

Total time: 112.80683135986328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447623
Iteration 2/25 | Loss: 0.00141361
Iteration 3/25 | Loss: 0.00131520
Iteration 4/25 | Loss: 0.00130955
Iteration 5/25 | Loss: 0.00130857
Iteration 6/25 | Loss: 0.00130852
Iteration 7/25 | Loss: 0.00130852
Iteration 8/25 | Loss: 0.00130852
Iteration 9/25 | Loss: 0.00130852
Iteration 10/25 | Loss: 0.00130852
Iteration 11/25 | Loss: 0.00130852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013085202081128955, 0.0013085202081128955, 0.0013085202081128955, 0.0013085202081128955, 0.0013085202081128955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013085202081128955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57451665
Iteration 2/25 | Loss: 0.00075788
Iteration 3/25 | Loss: 0.00075787
Iteration 4/25 | Loss: 0.00075787
Iteration 5/25 | Loss: 0.00075787
Iteration 6/25 | Loss: 0.00075787
Iteration 7/25 | Loss: 0.00075787
Iteration 8/25 | Loss: 0.00075787
Iteration 9/25 | Loss: 0.00075787
Iteration 10/25 | Loss: 0.00075787
Iteration 11/25 | Loss: 0.00075787
Iteration 12/25 | Loss: 0.00075787
Iteration 13/25 | Loss: 0.00075787
Iteration 14/25 | Loss: 0.00075787
Iteration 15/25 | Loss: 0.00075787
Iteration 16/25 | Loss: 0.00075787
Iteration 17/25 | Loss: 0.00075787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007578686345368624, 0.0007578686345368624, 0.0007578686345368624, 0.0007578686345368624, 0.0007578686345368624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007578686345368624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075787
Iteration 2/1000 | Loss: 0.00003317
Iteration 3/1000 | Loss: 0.00002362
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002035
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001881
Iteration 9/1000 | Loss: 0.00001858
Iteration 10/1000 | Loss: 0.00001849
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001824
Iteration 13/1000 | Loss: 0.00001813
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001801
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001793
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001791
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001786
Iteration 33/1000 | Loss: 0.00001786
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001776
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001771
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001770
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001766
Iteration 93/1000 | Loss: 0.00001766
Iteration 94/1000 | Loss: 0.00001766
Iteration 95/1000 | Loss: 0.00001765
Iteration 96/1000 | Loss: 0.00001765
Iteration 97/1000 | Loss: 0.00001765
Iteration 98/1000 | Loss: 0.00001764
Iteration 99/1000 | Loss: 0.00001764
Iteration 100/1000 | Loss: 0.00001764
Iteration 101/1000 | Loss: 0.00001764
Iteration 102/1000 | Loss: 0.00001764
Iteration 103/1000 | Loss: 0.00001764
Iteration 104/1000 | Loss: 0.00001763
Iteration 105/1000 | Loss: 0.00001763
Iteration 106/1000 | Loss: 0.00001763
Iteration 107/1000 | Loss: 0.00001762
Iteration 108/1000 | Loss: 0.00001762
Iteration 109/1000 | Loss: 0.00001762
Iteration 110/1000 | Loss: 0.00001762
Iteration 111/1000 | Loss: 0.00001762
Iteration 112/1000 | Loss: 0.00001762
Iteration 113/1000 | Loss: 0.00001762
Iteration 114/1000 | Loss: 0.00001761
Iteration 115/1000 | Loss: 0.00001761
Iteration 116/1000 | Loss: 0.00001761
Iteration 117/1000 | Loss: 0.00001761
Iteration 118/1000 | Loss: 0.00001761
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001760
Iteration 122/1000 | Loss: 0.00001760
Iteration 123/1000 | Loss: 0.00001760
Iteration 124/1000 | Loss: 0.00001760
Iteration 125/1000 | Loss: 0.00001760
Iteration 126/1000 | Loss: 0.00001759
Iteration 127/1000 | Loss: 0.00001759
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001758
Iteration 139/1000 | Loss: 0.00001758
Iteration 140/1000 | Loss: 0.00001758
Iteration 141/1000 | Loss: 0.00001758
Iteration 142/1000 | Loss: 0.00001758
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001758
Iteration 145/1000 | Loss: 0.00001758
Iteration 146/1000 | Loss: 0.00001758
Iteration 147/1000 | Loss: 0.00001758
Iteration 148/1000 | Loss: 0.00001758
Iteration 149/1000 | Loss: 0.00001758
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001757
Iteration 154/1000 | Loss: 0.00001757
Iteration 155/1000 | Loss: 0.00001757
Iteration 156/1000 | Loss: 0.00001757
Iteration 157/1000 | Loss: 0.00001757
Iteration 158/1000 | Loss: 0.00001757
Iteration 159/1000 | Loss: 0.00001757
Iteration 160/1000 | Loss: 0.00001757
Iteration 161/1000 | Loss: 0.00001757
Iteration 162/1000 | Loss: 0.00001757
Iteration 163/1000 | Loss: 0.00001757
Iteration 164/1000 | Loss: 0.00001757
Iteration 165/1000 | Loss: 0.00001757
Iteration 166/1000 | Loss: 0.00001757
Iteration 167/1000 | Loss: 0.00001757
Iteration 168/1000 | Loss: 0.00001757
Iteration 169/1000 | Loss: 0.00001757
Iteration 170/1000 | Loss: 0.00001756
Iteration 171/1000 | Loss: 0.00001756
Iteration 172/1000 | Loss: 0.00001756
Iteration 173/1000 | Loss: 0.00001756
Iteration 174/1000 | Loss: 0.00001756
Iteration 175/1000 | Loss: 0.00001756
Iteration 176/1000 | Loss: 0.00001756
Iteration 177/1000 | Loss: 0.00001756
Iteration 178/1000 | Loss: 0.00001756
Iteration 179/1000 | Loss: 0.00001756
Iteration 180/1000 | Loss: 0.00001756
Iteration 181/1000 | Loss: 0.00001756
Iteration 182/1000 | Loss: 0.00001755
Iteration 183/1000 | Loss: 0.00001755
Iteration 184/1000 | Loss: 0.00001755
Iteration 185/1000 | Loss: 0.00001755
Iteration 186/1000 | Loss: 0.00001755
Iteration 187/1000 | Loss: 0.00001755
Iteration 188/1000 | Loss: 0.00001755
Iteration 189/1000 | Loss: 0.00001755
Iteration 190/1000 | Loss: 0.00001755
Iteration 191/1000 | Loss: 0.00001755
Iteration 192/1000 | Loss: 0.00001755
Iteration 193/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.7551430573803373e-05, 1.7551430573803373e-05, 1.7551430573803373e-05, 1.7551430573803373e-05, 1.7551430573803373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7551430573803373e-05

Optimization complete. Final v2v error: 3.533784866333008 mm

Highest mean error: 3.82698392868042 mm for frame 106

Lowest mean error: 3.3252522945404053 mm for frame 78

Saving results

Total time: 36.21334218978882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586008
Iteration 2/25 | Loss: 0.00137874
Iteration 3/25 | Loss: 0.00127467
Iteration 4/25 | Loss: 0.00125056
Iteration 5/25 | Loss: 0.00124525
Iteration 6/25 | Loss: 0.00124371
Iteration 7/25 | Loss: 0.00124371
Iteration 8/25 | Loss: 0.00124371
Iteration 9/25 | Loss: 0.00124371
Iteration 10/25 | Loss: 0.00124371
Iteration 11/25 | Loss: 0.00124371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001243708305992186, 0.001243708305992186, 0.001243708305992186, 0.001243708305992186, 0.001243708305992186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243708305992186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58927715
Iteration 2/25 | Loss: 0.00091104
Iteration 3/25 | Loss: 0.00091104
Iteration 4/25 | Loss: 0.00091104
Iteration 5/25 | Loss: 0.00091104
Iteration 6/25 | Loss: 0.00091104
Iteration 7/25 | Loss: 0.00091104
Iteration 8/25 | Loss: 0.00091104
Iteration 9/25 | Loss: 0.00091104
Iteration 10/25 | Loss: 0.00091104
Iteration 11/25 | Loss: 0.00091104
Iteration 12/25 | Loss: 0.00091104
Iteration 13/25 | Loss: 0.00091104
Iteration 14/25 | Loss: 0.00091104
Iteration 15/25 | Loss: 0.00091104
Iteration 16/25 | Loss: 0.00091104
Iteration 17/25 | Loss: 0.00091104
Iteration 18/25 | Loss: 0.00091104
Iteration 19/25 | Loss: 0.00091104
Iteration 20/25 | Loss: 0.00091104
Iteration 21/25 | Loss: 0.00091104
Iteration 22/25 | Loss: 0.00091104
Iteration 23/25 | Loss: 0.00091104
Iteration 24/25 | Loss: 0.00091104
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009110370883718133, 0.0009110370883718133, 0.0009110370883718133, 0.0009110370883718133, 0.0009110370883718133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009110370883718133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091104
Iteration 2/1000 | Loss: 0.00004039
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002448
Iteration 6/1000 | Loss: 0.00002344
Iteration 7/1000 | Loss: 0.00002270
Iteration 8/1000 | Loss: 0.00002219
Iteration 9/1000 | Loss: 0.00002191
Iteration 10/1000 | Loss: 0.00002152
Iteration 11/1000 | Loss: 0.00002130
Iteration 12/1000 | Loss: 0.00002128
Iteration 13/1000 | Loss: 0.00002121
Iteration 14/1000 | Loss: 0.00002120
Iteration 15/1000 | Loss: 0.00002119
Iteration 16/1000 | Loss: 0.00002118
Iteration 17/1000 | Loss: 0.00002116
Iteration 18/1000 | Loss: 0.00002096
Iteration 19/1000 | Loss: 0.00002081
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002051
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002050
Iteration 29/1000 | Loss: 0.00002050
Iteration 30/1000 | Loss: 0.00002050
Iteration 31/1000 | Loss: 0.00002050
Iteration 32/1000 | Loss: 0.00002049
Iteration 33/1000 | Loss: 0.00002049
Iteration 34/1000 | Loss: 0.00002046
Iteration 35/1000 | Loss: 0.00002046
Iteration 36/1000 | Loss: 0.00002045
Iteration 37/1000 | Loss: 0.00002044
Iteration 38/1000 | Loss: 0.00002044
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002043
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002042
Iteration 43/1000 | Loss: 0.00002041
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002040
Iteration 46/1000 | Loss: 0.00002039
Iteration 47/1000 | Loss: 0.00002039
Iteration 48/1000 | Loss: 0.00002039
Iteration 49/1000 | Loss: 0.00002039
Iteration 50/1000 | Loss: 0.00002038
Iteration 51/1000 | Loss: 0.00002037
Iteration 52/1000 | Loss: 0.00002037
Iteration 53/1000 | Loss: 0.00002036
Iteration 54/1000 | Loss: 0.00002036
Iteration 55/1000 | Loss: 0.00002035
Iteration 56/1000 | Loss: 0.00002035
Iteration 57/1000 | Loss: 0.00002035
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002033
Iteration 62/1000 | Loss: 0.00002033
Iteration 63/1000 | Loss: 0.00002032
Iteration 64/1000 | Loss: 0.00002032
Iteration 65/1000 | Loss: 0.00002032
Iteration 66/1000 | Loss: 0.00002031
Iteration 67/1000 | Loss: 0.00002031
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002030
Iteration 77/1000 | Loss: 0.00002030
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002029
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002029
Iteration 84/1000 | Loss: 0.00002029
Iteration 85/1000 | Loss: 0.00002029
Iteration 86/1000 | Loss: 0.00002029
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002028
Iteration 89/1000 | Loss: 0.00002028
Iteration 90/1000 | Loss: 0.00002028
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002027
Iteration 95/1000 | Loss: 0.00002027
Iteration 96/1000 | Loss: 0.00002027
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002026
Iteration 102/1000 | Loss: 0.00002026
Iteration 103/1000 | Loss: 0.00002026
Iteration 104/1000 | Loss: 0.00002026
Iteration 105/1000 | Loss: 0.00002026
Iteration 106/1000 | Loss: 0.00002026
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002025
Iteration 110/1000 | Loss: 0.00002025
Iteration 111/1000 | Loss: 0.00002025
Iteration 112/1000 | Loss: 0.00002025
Iteration 113/1000 | Loss: 0.00002025
Iteration 114/1000 | Loss: 0.00002025
Iteration 115/1000 | Loss: 0.00002025
Iteration 116/1000 | Loss: 0.00002025
Iteration 117/1000 | Loss: 0.00002025
Iteration 118/1000 | Loss: 0.00002025
Iteration 119/1000 | Loss: 0.00002025
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002024
Iteration 125/1000 | Loss: 0.00002024
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002023
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002023
Iteration 132/1000 | Loss: 0.00002023
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00002023
Iteration 135/1000 | Loss: 0.00002022
Iteration 136/1000 | Loss: 0.00002022
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002022
Iteration 139/1000 | Loss: 0.00002022
Iteration 140/1000 | Loss: 0.00002022
Iteration 141/1000 | Loss: 0.00002022
Iteration 142/1000 | Loss: 0.00002022
Iteration 143/1000 | Loss: 0.00002022
Iteration 144/1000 | Loss: 0.00002022
Iteration 145/1000 | Loss: 0.00002022
Iteration 146/1000 | Loss: 0.00002022
Iteration 147/1000 | Loss: 0.00002022
Iteration 148/1000 | Loss: 0.00002022
Iteration 149/1000 | Loss: 0.00002021
Iteration 150/1000 | Loss: 0.00002021
Iteration 151/1000 | Loss: 0.00002021
Iteration 152/1000 | Loss: 0.00002021
Iteration 153/1000 | Loss: 0.00002021
Iteration 154/1000 | Loss: 0.00002021
Iteration 155/1000 | Loss: 0.00002021
Iteration 156/1000 | Loss: 0.00002021
Iteration 157/1000 | Loss: 0.00002021
Iteration 158/1000 | Loss: 0.00002020
Iteration 159/1000 | Loss: 0.00002020
Iteration 160/1000 | Loss: 0.00002020
Iteration 161/1000 | Loss: 0.00002020
Iteration 162/1000 | Loss: 0.00002020
Iteration 163/1000 | Loss: 0.00002020
Iteration 164/1000 | Loss: 0.00002020
Iteration 165/1000 | Loss: 0.00002020
Iteration 166/1000 | Loss: 0.00002020
Iteration 167/1000 | Loss: 0.00002020
Iteration 168/1000 | Loss: 0.00002020
Iteration 169/1000 | Loss: 0.00002020
Iteration 170/1000 | Loss: 0.00002020
Iteration 171/1000 | Loss: 0.00002020
Iteration 172/1000 | Loss: 0.00002020
Iteration 173/1000 | Loss: 0.00002019
Iteration 174/1000 | Loss: 0.00002019
Iteration 175/1000 | Loss: 0.00002019
Iteration 176/1000 | Loss: 0.00002019
Iteration 177/1000 | Loss: 0.00002019
Iteration 178/1000 | Loss: 0.00002019
Iteration 179/1000 | Loss: 0.00002019
Iteration 180/1000 | Loss: 0.00002019
Iteration 181/1000 | Loss: 0.00002019
Iteration 182/1000 | Loss: 0.00002019
Iteration 183/1000 | Loss: 0.00002019
Iteration 184/1000 | Loss: 0.00002018
Iteration 185/1000 | Loss: 0.00002018
Iteration 186/1000 | Loss: 0.00002018
Iteration 187/1000 | Loss: 0.00002018
Iteration 188/1000 | Loss: 0.00002018
Iteration 189/1000 | Loss: 0.00002018
Iteration 190/1000 | Loss: 0.00002018
Iteration 191/1000 | Loss: 0.00002018
Iteration 192/1000 | Loss: 0.00002018
Iteration 193/1000 | Loss: 0.00002018
Iteration 194/1000 | Loss: 0.00002018
Iteration 195/1000 | Loss: 0.00002018
Iteration 196/1000 | Loss: 0.00002018
Iteration 197/1000 | Loss: 0.00002018
Iteration 198/1000 | Loss: 0.00002018
Iteration 199/1000 | Loss: 0.00002018
Iteration 200/1000 | Loss: 0.00002018
Iteration 201/1000 | Loss: 0.00002018
Iteration 202/1000 | Loss: 0.00002018
Iteration 203/1000 | Loss: 0.00002017
Iteration 204/1000 | Loss: 0.00002017
Iteration 205/1000 | Loss: 0.00002017
Iteration 206/1000 | Loss: 0.00002017
Iteration 207/1000 | Loss: 0.00002017
Iteration 208/1000 | Loss: 0.00002017
Iteration 209/1000 | Loss: 0.00002017
Iteration 210/1000 | Loss: 0.00002017
Iteration 211/1000 | Loss: 0.00002016
Iteration 212/1000 | Loss: 0.00002016
Iteration 213/1000 | Loss: 0.00002016
Iteration 214/1000 | Loss: 0.00002016
Iteration 215/1000 | Loss: 0.00002016
Iteration 216/1000 | Loss: 0.00002016
Iteration 217/1000 | Loss: 0.00002016
Iteration 218/1000 | Loss: 0.00002016
Iteration 219/1000 | Loss: 0.00002016
Iteration 220/1000 | Loss: 0.00002016
Iteration 221/1000 | Loss: 0.00002016
Iteration 222/1000 | Loss: 0.00002016
Iteration 223/1000 | Loss: 0.00002016
Iteration 224/1000 | Loss: 0.00002016
Iteration 225/1000 | Loss: 0.00002016
Iteration 226/1000 | Loss: 0.00002016
Iteration 227/1000 | Loss: 0.00002016
Iteration 228/1000 | Loss: 0.00002016
Iteration 229/1000 | Loss: 0.00002016
Iteration 230/1000 | Loss: 0.00002016
Iteration 231/1000 | Loss: 0.00002016
Iteration 232/1000 | Loss: 0.00002016
Iteration 233/1000 | Loss: 0.00002016
Iteration 234/1000 | Loss: 0.00002016
Iteration 235/1000 | Loss: 0.00002016
Iteration 236/1000 | Loss: 0.00002016
Iteration 237/1000 | Loss: 0.00002016
Iteration 238/1000 | Loss: 0.00002016
Iteration 239/1000 | Loss: 0.00002016
Iteration 240/1000 | Loss: 0.00002016
Iteration 241/1000 | Loss: 0.00002016
Iteration 242/1000 | Loss: 0.00002016
Iteration 243/1000 | Loss: 0.00002016
Iteration 244/1000 | Loss: 0.00002016
Iteration 245/1000 | Loss: 0.00002016
Iteration 246/1000 | Loss: 0.00002016
Iteration 247/1000 | Loss: 0.00002016
Iteration 248/1000 | Loss: 0.00002016
Iteration 249/1000 | Loss: 0.00002016
Iteration 250/1000 | Loss: 0.00002016
Iteration 251/1000 | Loss: 0.00002016
Iteration 252/1000 | Loss: 0.00002016
Iteration 253/1000 | Loss: 0.00002016
Iteration 254/1000 | Loss: 0.00002016
Iteration 255/1000 | Loss: 0.00002016
Iteration 256/1000 | Loss: 0.00002016
Iteration 257/1000 | Loss: 0.00002016
Iteration 258/1000 | Loss: 0.00002016
Iteration 259/1000 | Loss: 0.00002016
Iteration 260/1000 | Loss: 0.00002016
Iteration 261/1000 | Loss: 0.00002016
Iteration 262/1000 | Loss: 0.00002016
Iteration 263/1000 | Loss: 0.00002016
Iteration 264/1000 | Loss: 0.00002016
Iteration 265/1000 | Loss: 0.00002016
Iteration 266/1000 | Loss: 0.00002016
Iteration 267/1000 | Loss: 0.00002016
Iteration 268/1000 | Loss: 0.00002016
Iteration 269/1000 | Loss: 0.00002016
Iteration 270/1000 | Loss: 0.00002016
Iteration 271/1000 | Loss: 0.00002016
Iteration 272/1000 | Loss: 0.00002016
Iteration 273/1000 | Loss: 0.00002016
Iteration 274/1000 | Loss: 0.00002016
Iteration 275/1000 | Loss: 0.00002016
Iteration 276/1000 | Loss: 0.00002016
Iteration 277/1000 | Loss: 0.00002016
Iteration 278/1000 | Loss: 0.00002016
Iteration 279/1000 | Loss: 0.00002016
Iteration 280/1000 | Loss: 0.00002016
Iteration 281/1000 | Loss: 0.00002016
Iteration 282/1000 | Loss: 0.00002016
Iteration 283/1000 | Loss: 0.00002016
Iteration 284/1000 | Loss: 0.00002016
Iteration 285/1000 | Loss: 0.00002016
Iteration 286/1000 | Loss: 0.00002016
Iteration 287/1000 | Loss: 0.00002016
Iteration 288/1000 | Loss: 0.00002016
Iteration 289/1000 | Loss: 0.00002016
Iteration 290/1000 | Loss: 0.00002016
Iteration 291/1000 | Loss: 0.00002016
Iteration 292/1000 | Loss: 0.00002016
Iteration 293/1000 | Loss: 0.00002016
Iteration 294/1000 | Loss: 0.00002016
Iteration 295/1000 | Loss: 0.00002016
Iteration 296/1000 | Loss: 0.00002016
Iteration 297/1000 | Loss: 0.00002016
Iteration 298/1000 | Loss: 0.00002016
Iteration 299/1000 | Loss: 0.00002016
Iteration 300/1000 | Loss: 0.00002016
Iteration 301/1000 | Loss: 0.00002016
Iteration 302/1000 | Loss: 0.00002016
Iteration 303/1000 | Loss: 0.00002016
Iteration 304/1000 | Loss: 0.00002016
Iteration 305/1000 | Loss: 0.00002016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [2.0155755919404328e-05, 2.0155755919404328e-05, 2.0155755919404328e-05, 2.0155755919404328e-05, 2.0155755919404328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0155755919404328e-05

Optimization complete. Final v2v error: 3.897502899169922 mm

Highest mean error: 4.029860019683838 mm for frame 20

Lowest mean error: 3.7422313690185547 mm for frame 239

Saving results

Total time: 52.107277154922485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611604
Iteration 2/25 | Loss: 0.00173540
Iteration 3/25 | Loss: 0.00144816
Iteration 4/25 | Loss: 0.00143704
Iteration 5/25 | Loss: 0.00143388
Iteration 6/25 | Loss: 0.00143304
Iteration 7/25 | Loss: 0.00143304
Iteration 8/25 | Loss: 0.00143304
Iteration 9/25 | Loss: 0.00143304
Iteration 10/25 | Loss: 0.00143304
Iteration 11/25 | Loss: 0.00143304
Iteration 12/25 | Loss: 0.00143304
Iteration 13/25 | Loss: 0.00143304
Iteration 14/25 | Loss: 0.00143304
Iteration 15/25 | Loss: 0.00143304
Iteration 16/25 | Loss: 0.00143304
Iteration 17/25 | Loss: 0.00143304
Iteration 18/25 | Loss: 0.00143304
Iteration 19/25 | Loss: 0.00143304
Iteration 20/25 | Loss: 0.00143304
Iteration 21/25 | Loss: 0.00143304
Iteration 22/25 | Loss: 0.00143304
Iteration 23/25 | Loss: 0.00143304
Iteration 24/25 | Loss: 0.00143304
Iteration 25/25 | Loss: 0.00143304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03699279
Iteration 2/25 | Loss: 0.00110541
Iteration 3/25 | Loss: 0.00110539
Iteration 4/25 | Loss: 0.00110539
Iteration 5/25 | Loss: 0.00110539
Iteration 6/25 | Loss: 0.00110539
Iteration 7/25 | Loss: 0.00110539
Iteration 8/25 | Loss: 0.00110539
Iteration 9/25 | Loss: 0.00110539
Iteration 10/25 | Loss: 0.00110539
Iteration 11/25 | Loss: 0.00110539
Iteration 12/25 | Loss: 0.00110539
Iteration 13/25 | Loss: 0.00110539
Iteration 14/25 | Loss: 0.00110539
Iteration 15/25 | Loss: 0.00110539
Iteration 16/25 | Loss: 0.00110539
Iteration 17/25 | Loss: 0.00110539
Iteration 18/25 | Loss: 0.00110539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011053896741941571, 0.0011053896741941571, 0.0011053896741941571, 0.0011053896741941571, 0.0011053896741941571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011053896741941571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110539
Iteration 2/1000 | Loss: 0.00008766
Iteration 3/1000 | Loss: 0.00005461
Iteration 4/1000 | Loss: 0.00004577
Iteration 5/1000 | Loss: 0.00004298
Iteration 6/1000 | Loss: 0.00004121
Iteration 7/1000 | Loss: 0.00004029
Iteration 8/1000 | Loss: 0.00003948
Iteration 9/1000 | Loss: 0.00003858
Iteration 10/1000 | Loss: 0.00003808
Iteration 11/1000 | Loss: 0.00003763
Iteration 12/1000 | Loss: 0.00003714
Iteration 13/1000 | Loss: 0.00003669
Iteration 14/1000 | Loss: 0.00003629
Iteration 15/1000 | Loss: 0.00003590
Iteration 16/1000 | Loss: 0.00003559
Iteration 17/1000 | Loss: 0.00003539
Iteration 18/1000 | Loss: 0.00003518
Iteration 19/1000 | Loss: 0.00003498
Iteration 20/1000 | Loss: 0.00003485
Iteration 21/1000 | Loss: 0.00003472
Iteration 22/1000 | Loss: 0.00003467
Iteration 23/1000 | Loss: 0.00003466
Iteration 24/1000 | Loss: 0.00003461
Iteration 25/1000 | Loss: 0.00003461
Iteration 26/1000 | Loss: 0.00003457
Iteration 27/1000 | Loss: 0.00003457
Iteration 28/1000 | Loss: 0.00003457
Iteration 29/1000 | Loss: 0.00003457
Iteration 30/1000 | Loss: 0.00003457
Iteration 31/1000 | Loss: 0.00003457
Iteration 32/1000 | Loss: 0.00003457
Iteration 33/1000 | Loss: 0.00003457
Iteration 34/1000 | Loss: 0.00003457
Iteration 35/1000 | Loss: 0.00003456
Iteration 36/1000 | Loss: 0.00003456
Iteration 37/1000 | Loss: 0.00003456
Iteration 38/1000 | Loss: 0.00003456
Iteration 39/1000 | Loss: 0.00003455
Iteration 40/1000 | Loss: 0.00003455
Iteration 41/1000 | Loss: 0.00003454
Iteration 42/1000 | Loss: 0.00003454
Iteration 43/1000 | Loss: 0.00003454
Iteration 44/1000 | Loss: 0.00003453
Iteration 45/1000 | Loss: 0.00003453
Iteration 46/1000 | Loss: 0.00003453
Iteration 47/1000 | Loss: 0.00003453
Iteration 48/1000 | Loss: 0.00003453
Iteration 49/1000 | Loss: 0.00003453
Iteration 50/1000 | Loss: 0.00003452
Iteration 51/1000 | Loss: 0.00003452
Iteration 52/1000 | Loss: 0.00003452
Iteration 53/1000 | Loss: 0.00003451
Iteration 54/1000 | Loss: 0.00003451
Iteration 55/1000 | Loss: 0.00003451
Iteration 56/1000 | Loss: 0.00003451
Iteration 57/1000 | Loss: 0.00003450
Iteration 58/1000 | Loss: 0.00003450
Iteration 59/1000 | Loss: 0.00003450
Iteration 60/1000 | Loss: 0.00003450
Iteration 61/1000 | Loss: 0.00003450
Iteration 62/1000 | Loss: 0.00003449
Iteration 63/1000 | Loss: 0.00003449
Iteration 64/1000 | Loss: 0.00003449
Iteration 65/1000 | Loss: 0.00003449
Iteration 66/1000 | Loss: 0.00003448
Iteration 67/1000 | Loss: 0.00003448
Iteration 68/1000 | Loss: 0.00003447
Iteration 69/1000 | Loss: 0.00003447
Iteration 70/1000 | Loss: 0.00003447
Iteration 71/1000 | Loss: 0.00003447
Iteration 72/1000 | Loss: 0.00003446
Iteration 73/1000 | Loss: 0.00003446
Iteration 74/1000 | Loss: 0.00003446
Iteration 75/1000 | Loss: 0.00003446
Iteration 76/1000 | Loss: 0.00003446
Iteration 77/1000 | Loss: 0.00003445
Iteration 78/1000 | Loss: 0.00003445
Iteration 79/1000 | Loss: 0.00003445
Iteration 80/1000 | Loss: 0.00003445
Iteration 81/1000 | Loss: 0.00003444
Iteration 82/1000 | Loss: 0.00003444
Iteration 83/1000 | Loss: 0.00003444
Iteration 84/1000 | Loss: 0.00003444
Iteration 85/1000 | Loss: 0.00003444
Iteration 86/1000 | Loss: 0.00003444
Iteration 87/1000 | Loss: 0.00003443
Iteration 88/1000 | Loss: 0.00003443
Iteration 89/1000 | Loss: 0.00003443
Iteration 90/1000 | Loss: 0.00003443
Iteration 91/1000 | Loss: 0.00003442
Iteration 92/1000 | Loss: 0.00003442
Iteration 93/1000 | Loss: 0.00003442
Iteration 94/1000 | Loss: 0.00003442
Iteration 95/1000 | Loss: 0.00003442
Iteration 96/1000 | Loss: 0.00003442
Iteration 97/1000 | Loss: 0.00003442
Iteration 98/1000 | Loss: 0.00003442
Iteration 99/1000 | Loss: 0.00003442
Iteration 100/1000 | Loss: 0.00003442
Iteration 101/1000 | Loss: 0.00003441
Iteration 102/1000 | Loss: 0.00003441
Iteration 103/1000 | Loss: 0.00003441
Iteration 104/1000 | Loss: 0.00003441
Iteration 105/1000 | Loss: 0.00003440
Iteration 106/1000 | Loss: 0.00003440
Iteration 107/1000 | Loss: 0.00003440
Iteration 108/1000 | Loss: 0.00003440
Iteration 109/1000 | Loss: 0.00003440
Iteration 110/1000 | Loss: 0.00003439
Iteration 111/1000 | Loss: 0.00003439
Iteration 112/1000 | Loss: 0.00003439
Iteration 113/1000 | Loss: 0.00003439
Iteration 114/1000 | Loss: 0.00003439
Iteration 115/1000 | Loss: 0.00003439
Iteration 116/1000 | Loss: 0.00003439
Iteration 117/1000 | Loss: 0.00003439
Iteration 118/1000 | Loss: 0.00003438
Iteration 119/1000 | Loss: 0.00003438
Iteration 120/1000 | Loss: 0.00003438
Iteration 121/1000 | Loss: 0.00003438
Iteration 122/1000 | Loss: 0.00003438
Iteration 123/1000 | Loss: 0.00003438
Iteration 124/1000 | Loss: 0.00003438
Iteration 125/1000 | Loss: 0.00003438
Iteration 126/1000 | Loss: 0.00003438
Iteration 127/1000 | Loss: 0.00003438
Iteration 128/1000 | Loss: 0.00003438
Iteration 129/1000 | Loss: 0.00003438
Iteration 130/1000 | Loss: 0.00003438
Iteration 131/1000 | Loss: 0.00003438
Iteration 132/1000 | Loss: 0.00003438
Iteration 133/1000 | Loss: 0.00003438
Iteration 134/1000 | Loss: 0.00003438
Iteration 135/1000 | Loss: 0.00003438
Iteration 136/1000 | Loss: 0.00003437
Iteration 137/1000 | Loss: 0.00003437
Iteration 138/1000 | Loss: 0.00003437
Iteration 139/1000 | Loss: 0.00003437
Iteration 140/1000 | Loss: 0.00003437
Iteration 141/1000 | Loss: 0.00003437
Iteration 142/1000 | Loss: 0.00003437
Iteration 143/1000 | Loss: 0.00003437
Iteration 144/1000 | Loss: 0.00003437
Iteration 145/1000 | Loss: 0.00003437
Iteration 146/1000 | Loss: 0.00003437
Iteration 147/1000 | Loss: 0.00003437
Iteration 148/1000 | Loss: 0.00003437
Iteration 149/1000 | Loss: 0.00003437
Iteration 150/1000 | Loss: 0.00003437
Iteration 151/1000 | Loss: 0.00003437
Iteration 152/1000 | Loss: 0.00003437
Iteration 153/1000 | Loss: 0.00003437
Iteration 154/1000 | Loss: 0.00003437
Iteration 155/1000 | Loss: 0.00003437
Iteration 156/1000 | Loss: 0.00003437
Iteration 157/1000 | Loss: 0.00003436
Iteration 158/1000 | Loss: 0.00003436
Iteration 159/1000 | Loss: 0.00003436
Iteration 160/1000 | Loss: 0.00003436
Iteration 161/1000 | Loss: 0.00003436
Iteration 162/1000 | Loss: 0.00003436
Iteration 163/1000 | Loss: 0.00003436
Iteration 164/1000 | Loss: 0.00003436
Iteration 165/1000 | Loss: 0.00003436
Iteration 166/1000 | Loss: 0.00003436
Iteration 167/1000 | Loss: 0.00003436
Iteration 168/1000 | Loss: 0.00003436
Iteration 169/1000 | Loss: 0.00003436
Iteration 170/1000 | Loss: 0.00003436
Iteration 171/1000 | Loss: 0.00003436
Iteration 172/1000 | Loss: 0.00003436
Iteration 173/1000 | Loss: 0.00003436
Iteration 174/1000 | Loss: 0.00003436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.436418774072081e-05, 3.436418774072081e-05, 3.436418774072081e-05, 3.436418774072081e-05, 3.436418774072081e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.436418774072081e-05

Optimization complete. Final v2v error: 4.5025105476379395 mm

Highest mean error: 5.544759750366211 mm for frame 136

Lowest mean error: 3.51021671295166 mm for frame 49

Saving results

Total time: 49.1541223526001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364456
Iteration 2/25 | Loss: 0.00140176
Iteration 3/25 | Loss: 0.00128760
Iteration 4/25 | Loss: 0.00126059
Iteration 5/25 | Loss: 0.00125121
Iteration 6/25 | Loss: 0.00124905
Iteration 7/25 | Loss: 0.00124900
Iteration 8/25 | Loss: 0.00124900
Iteration 9/25 | Loss: 0.00124900
Iteration 10/25 | Loss: 0.00124900
Iteration 11/25 | Loss: 0.00124900
Iteration 12/25 | Loss: 0.00124900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001248996239155531, 0.001248996239155531, 0.001248996239155531, 0.001248996239155531, 0.001248996239155531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248996239155531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42265439
Iteration 2/25 | Loss: 0.00084513
Iteration 3/25 | Loss: 0.00084513
Iteration 4/25 | Loss: 0.00084513
Iteration 5/25 | Loss: 0.00084513
Iteration 6/25 | Loss: 0.00084513
Iteration 7/25 | Loss: 0.00084512
Iteration 8/25 | Loss: 0.00084512
Iteration 9/25 | Loss: 0.00084512
Iteration 10/25 | Loss: 0.00084512
Iteration 11/25 | Loss: 0.00084512
Iteration 12/25 | Loss: 0.00084512
Iteration 13/25 | Loss: 0.00084512
Iteration 14/25 | Loss: 0.00084512
Iteration 15/25 | Loss: 0.00084512
Iteration 16/25 | Loss: 0.00084512
Iteration 17/25 | Loss: 0.00084512
Iteration 18/25 | Loss: 0.00084512
Iteration 19/25 | Loss: 0.00084512
Iteration 20/25 | Loss: 0.00084512
Iteration 21/25 | Loss: 0.00084512
Iteration 22/25 | Loss: 0.00084512
Iteration 23/25 | Loss: 0.00084512
Iteration 24/25 | Loss: 0.00084512
Iteration 25/25 | Loss: 0.00084512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084512
Iteration 2/1000 | Loss: 0.00005646
Iteration 3/1000 | Loss: 0.00003744
Iteration 4/1000 | Loss: 0.00002990
Iteration 5/1000 | Loss: 0.00002779
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002556
Iteration 8/1000 | Loss: 0.00002501
Iteration 9/1000 | Loss: 0.00002454
Iteration 10/1000 | Loss: 0.00002423
Iteration 11/1000 | Loss: 0.00002393
Iteration 12/1000 | Loss: 0.00002365
Iteration 13/1000 | Loss: 0.00002348
Iteration 14/1000 | Loss: 0.00002337
Iteration 15/1000 | Loss: 0.00002330
Iteration 16/1000 | Loss: 0.00002324
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002317
Iteration 19/1000 | Loss: 0.00002315
Iteration 20/1000 | Loss: 0.00002314
Iteration 21/1000 | Loss: 0.00002308
Iteration 22/1000 | Loss: 0.00002306
Iteration 23/1000 | Loss: 0.00002305
Iteration 24/1000 | Loss: 0.00002304
Iteration 25/1000 | Loss: 0.00002304
Iteration 26/1000 | Loss: 0.00002303
Iteration 27/1000 | Loss: 0.00002303
Iteration 28/1000 | Loss: 0.00002302
Iteration 29/1000 | Loss: 0.00002302
Iteration 30/1000 | Loss: 0.00002301
Iteration 31/1000 | Loss: 0.00002301
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002300
Iteration 34/1000 | Loss: 0.00002297
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002289
Iteration 38/1000 | Loss: 0.00002288
Iteration 39/1000 | Loss: 0.00002288
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002287
Iteration 42/1000 | Loss: 0.00002286
Iteration 43/1000 | Loss: 0.00002286
Iteration 44/1000 | Loss: 0.00002286
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002285
Iteration 47/1000 | Loss: 0.00002284
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002283
Iteration 50/1000 | Loss: 0.00002283
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002282
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002279
Iteration 57/1000 | Loss: 0.00002279
Iteration 58/1000 | Loss: 0.00002279
Iteration 59/1000 | Loss: 0.00002279
Iteration 60/1000 | Loss: 0.00002279
Iteration 61/1000 | Loss: 0.00002279
Iteration 62/1000 | Loss: 0.00002278
Iteration 63/1000 | Loss: 0.00002277
Iteration 64/1000 | Loss: 0.00002277
Iteration 65/1000 | Loss: 0.00002276
Iteration 66/1000 | Loss: 0.00002276
Iteration 67/1000 | Loss: 0.00002276
Iteration 68/1000 | Loss: 0.00002275
Iteration 69/1000 | Loss: 0.00002275
Iteration 70/1000 | Loss: 0.00002275
Iteration 71/1000 | Loss: 0.00002274
Iteration 72/1000 | Loss: 0.00002274
Iteration 73/1000 | Loss: 0.00002274
Iteration 74/1000 | Loss: 0.00002274
Iteration 75/1000 | Loss: 0.00002273
Iteration 76/1000 | Loss: 0.00002273
Iteration 77/1000 | Loss: 0.00002273
Iteration 78/1000 | Loss: 0.00002273
Iteration 79/1000 | Loss: 0.00002272
Iteration 80/1000 | Loss: 0.00002272
Iteration 81/1000 | Loss: 0.00002272
Iteration 82/1000 | Loss: 0.00002272
Iteration 83/1000 | Loss: 0.00002271
Iteration 84/1000 | Loss: 0.00002271
Iteration 85/1000 | Loss: 0.00002271
Iteration 86/1000 | Loss: 0.00002270
Iteration 87/1000 | Loss: 0.00002270
Iteration 88/1000 | Loss: 0.00002270
Iteration 89/1000 | Loss: 0.00002269
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002269
Iteration 94/1000 | Loss: 0.00002268
Iteration 95/1000 | Loss: 0.00002268
Iteration 96/1000 | Loss: 0.00002268
Iteration 97/1000 | Loss: 0.00002268
Iteration 98/1000 | Loss: 0.00002268
Iteration 99/1000 | Loss: 0.00002268
Iteration 100/1000 | Loss: 0.00002267
Iteration 101/1000 | Loss: 0.00002267
Iteration 102/1000 | Loss: 0.00002267
Iteration 103/1000 | Loss: 0.00002267
Iteration 104/1000 | Loss: 0.00002267
Iteration 105/1000 | Loss: 0.00002267
Iteration 106/1000 | Loss: 0.00002267
Iteration 107/1000 | Loss: 0.00002267
Iteration 108/1000 | Loss: 0.00002267
Iteration 109/1000 | Loss: 0.00002267
Iteration 110/1000 | Loss: 0.00002267
Iteration 111/1000 | Loss: 0.00002267
Iteration 112/1000 | Loss: 0.00002267
Iteration 113/1000 | Loss: 0.00002266
Iteration 114/1000 | Loss: 0.00002266
Iteration 115/1000 | Loss: 0.00002266
Iteration 116/1000 | Loss: 0.00002266
Iteration 117/1000 | Loss: 0.00002266
Iteration 118/1000 | Loss: 0.00002266
Iteration 119/1000 | Loss: 0.00002266
Iteration 120/1000 | Loss: 0.00002266
Iteration 121/1000 | Loss: 0.00002266
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002266
Iteration 126/1000 | Loss: 0.00002266
Iteration 127/1000 | Loss: 0.00002266
Iteration 128/1000 | Loss: 0.00002266
Iteration 129/1000 | Loss: 0.00002266
Iteration 130/1000 | Loss: 0.00002266
Iteration 131/1000 | Loss: 0.00002266
Iteration 132/1000 | Loss: 0.00002266
Iteration 133/1000 | Loss: 0.00002266
Iteration 134/1000 | Loss: 0.00002266
Iteration 135/1000 | Loss: 0.00002266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.265860530314967e-05, 2.265860530314967e-05, 2.265860530314967e-05, 2.265860530314967e-05, 2.265860530314967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.265860530314967e-05

Optimization complete. Final v2v error: 3.9470279216766357 mm

Highest mean error: 5.05997896194458 mm for frame 189

Lowest mean error: 2.9445526599884033 mm for frame 221

Saving results

Total time: 46.8759651184082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00656613
Iteration 2/25 | Loss: 0.00135915
Iteration 3/25 | Loss: 0.00127519
Iteration 4/25 | Loss: 0.00126016
Iteration 5/25 | Loss: 0.00125576
Iteration 6/25 | Loss: 0.00125529
Iteration 7/25 | Loss: 0.00125529
Iteration 8/25 | Loss: 0.00125529
Iteration 9/25 | Loss: 0.00125529
Iteration 10/25 | Loss: 0.00125529
Iteration 11/25 | Loss: 0.00125529
Iteration 12/25 | Loss: 0.00125529
Iteration 13/25 | Loss: 0.00125529
Iteration 14/25 | Loss: 0.00125529
Iteration 15/25 | Loss: 0.00125529
Iteration 16/25 | Loss: 0.00125529
Iteration 17/25 | Loss: 0.00125529
Iteration 18/25 | Loss: 0.00125529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012552911648526788, 0.0012552911648526788, 0.0012552911648526788, 0.0012552911648526788, 0.0012552911648526788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012552911648526788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37570143
Iteration 2/25 | Loss: 0.00084359
Iteration 3/25 | Loss: 0.00084358
Iteration 4/25 | Loss: 0.00084358
Iteration 5/25 | Loss: 0.00084358
Iteration 6/25 | Loss: 0.00084358
Iteration 7/25 | Loss: 0.00084358
Iteration 8/25 | Loss: 0.00084358
Iteration 9/25 | Loss: 0.00084358
Iteration 10/25 | Loss: 0.00084358
Iteration 11/25 | Loss: 0.00084358
Iteration 12/25 | Loss: 0.00084358
Iteration 13/25 | Loss: 0.00084358
Iteration 14/25 | Loss: 0.00084358
Iteration 15/25 | Loss: 0.00084358
Iteration 16/25 | Loss: 0.00084358
Iteration 17/25 | Loss: 0.00084358
Iteration 18/25 | Loss: 0.00084358
Iteration 19/25 | Loss: 0.00084358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008435799391008914, 0.0008435799391008914, 0.0008435799391008914, 0.0008435799391008914, 0.0008435799391008914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008435799391008914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084358
Iteration 2/1000 | Loss: 0.00003411
Iteration 3/1000 | Loss: 0.00002102
Iteration 4/1000 | Loss: 0.00001790
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001642
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001517
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001455
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001451
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.4503287275147159e-05, 1.4503287275147159e-05, 1.4503287275147159e-05, 1.4503287275147159e-05, 1.4503287275147159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4503287275147159e-05

Optimization complete. Final v2v error: 3.2509799003601074 mm

Highest mean error: 3.492017984390259 mm for frame 60

Lowest mean error: 3.04630446434021 mm for frame 11

Saving results

Total time: 28.241652727127075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783779
Iteration 2/25 | Loss: 0.00138232
Iteration 3/25 | Loss: 0.00126397
Iteration 4/25 | Loss: 0.00125544
Iteration 5/25 | Loss: 0.00125452
Iteration 6/25 | Loss: 0.00125452
Iteration 7/25 | Loss: 0.00125452
Iteration 8/25 | Loss: 0.00125452
Iteration 9/25 | Loss: 0.00125452
Iteration 10/25 | Loss: 0.00125452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012545199133455753, 0.0012545199133455753, 0.0012545199133455753, 0.0012545199133455753, 0.0012545199133455753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012545199133455753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41510439
Iteration 2/25 | Loss: 0.00066347
Iteration 3/25 | Loss: 0.00066345
Iteration 4/25 | Loss: 0.00066345
Iteration 5/25 | Loss: 0.00066345
Iteration 6/25 | Loss: 0.00066345
Iteration 7/25 | Loss: 0.00066345
Iteration 8/25 | Loss: 0.00066345
Iteration 9/25 | Loss: 0.00066345
Iteration 10/25 | Loss: 0.00066345
Iteration 11/25 | Loss: 0.00066345
Iteration 12/25 | Loss: 0.00066345
Iteration 13/25 | Loss: 0.00066345
Iteration 14/25 | Loss: 0.00066345
Iteration 15/25 | Loss: 0.00066345
Iteration 16/25 | Loss: 0.00066345
Iteration 17/25 | Loss: 0.00066345
Iteration 18/25 | Loss: 0.00066345
Iteration 19/25 | Loss: 0.00066345
Iteration 20/25 | Loss: 0.00066345
Iteration 21/25 | Loss: 0.00066345
Iteration 22/25 | Loss: 0.00066345
Iteration 23/25 | Loss: 0.00066345
Iteration 24/25 | Loss: 0.00066345
Iteration 25/25 | Loss: 0.00066345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066345
Iteration 2/1000 | Loss: 0.00002871
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001796
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001674
Iteration 8/1000 | Loss: 0.00001632
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001565
Iteration 11/1000 | Loss: 0.00001540
Iteration 12/1000 | Loss: 0.00001516
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001444
Iteration 20/1000 | Loss: 0.00001443
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001435
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001432
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001420
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001418
Iteration 34/1000 | Loss: 0.00001418
Iteration 35/1000 | Loss: 0.00001418
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001410
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001409
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001408
Iteration 64/1000 | Loss: 0.00001408
Iteration 65/1000 | Loss: 0.00001408
Iteration 66/1000 | Loss: 0.00001408
Iteration 67/1000 | Loss: 0.00001408
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001408
Iteration 83/1000 | Loss: 0.00001408
Iteration 84/1000 | Loss: 0.00001408
Iteration 85/1000 | Loss: 0.00001407
Iteration 86/1000 | Loss: 0.00001407
Iteration 87/1000 | Loss: 0.00001407
Iteration 88/1000 | Loss: 0.00001407
Iteration 89/1000 | Loss: 0.00001407
Iteration 90/1000 | Loss: 0.00001407
Iteration 91/1000 | Loss: 0.00001407
Iteration 92/1000 | Loss: 0.00001407
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001406
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001406
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001406
Iteration 106/1000 | Loss: 0.00001406
Iteration 107/1000 | Loss: 0.00001406
Iteration 108/1000 | Loss: 0.00001406
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001405
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001405
Iteration 117/1000 | Loss: 0.00001405
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001404
Iteration 129/1000 | Loss: 0.00001404
Iteration 130/1000 | Loss: 0.00001404
Iteration 131/1000 | Loss: 0.00001404
Iteration 132/1000 | Loss: 0.00001404
Iteration 133/1000 | Loss: 0.00001404
Iteration 134/1000 | Loss: 0.00001404
Iteration 135/1000 | Loss: 0.00001404
Iteration 136/1000 | Loss: 0.00001404
Iteration 137/1000 | Loss: 0.00001404
Iteration 138/1000 | Loss: 0.00001404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.4040398127690423e-05, 1.4040398127690423e-05, 1.4040398127690423e-05, 1.4040398127690423e-05, 1.4040398127690423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4040398127690423e-05

Optimization complete. Final v2v error: 3.1541225910186768 mm

Highest mean error: 3.4966037273406982 mm for frame 62

Lowest mean error: 2.821221113204956 mm for frame 27

Saving results

Total time: 38.23928117752075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898568
Iteration 2/25 | Loss: 0.00145561
Iteration 3/25 | Loss: 0.00134145
Iteration 4/25 | Loss: 0.00132257
Iteration 5/25 | Loss: 0.00131586
Iteration 6/25 | Loss: 0.00131437
Iteration 7/25 | Loss: 0.00131437
Iteration 8/25 | Loss: 0.00131437
Iteration 9/25 | Loss: 0.00131437
Iteration 10/25 | Loss: 0.00131437
Iteration 11/25 | Loss: 0.00131437
Iteration 12/25 | Loss: 0.00131437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013143711257725954, 0.0013143711257725954, 0.0013143711257725954, 0.0013143711257725954, 0.0013143711257725954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013143711257725954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43998075
Iteration 2/25 | Loss: 0.00075971
Iteration 3/25 | Loss: 0.00075969
Iteration 4/25 | Loss: 0.00075969
Iteration 5/25 | Loss: 0.00075969
Iteration 6/25 | Loss: 0.00075969
Iteration 7/25 | Loss: 0.00075969
Iteration 8/25 | Loss: 0.00075969
Iteration 9/25 | Loss: 0.00075969
Iteration 10/25 | Loss: 0.00075969
Iteration 11/25 | Loss: 0.00075969
Iteration 12/25 | Loss: 0.00075969
Iteration 13/25 | Loss: 0.00075969
Iteration 14/25 | Loss: 0.00075969
Iteration 15/25 | Loss: 0.00075969
Iteration 16/25 | Loss: 0.00075969
Iteration 17/25 | Loss: 0.00075969
Iteration 18/25 | Loss: 0.00075969
Iteration 19/25 | Loss: 0.00075969
Iteration 20/25 | Loss: 0.00075969
Iteration 21/25 | Loss: 0.00075969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007596886134706438, 0.0007596886134706438, 0.0007596886134706438, 0.0007596886134706438, 0.0007596886134706438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007596886134706438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075969
Iteration 2/1000 | Loss: 0.00004849
Iteration 3/1000 | Loss: 0.00003413
Iteration 4/1000 | Loss: 0.00002917
Iteration 5/1000 | Loss: 0.00002757
Iteration 6/1000 | Loss: 0.00002640
Iteration 7/1000 | Loss: 0.00002569
Iteration 8/1000 | Loss: 0.00002500
Iteration 9/1000 | Loss: 0.00002448
Iteration 10/1000 | Loss: 0.00002415
Iteration 11/1000 | Loss: 0.00002387
Iteration 12/1000 | Loss: 0.00002369
Iteration 13/1000 | Loss: 0.00002352
Iteration 14/1000 | Loss: 0.00002351
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002334
Iteration 17/1000 | Loss: 0.00002324
Iteration 18/1000 | Loss: 0.00002320
Iteration 19/1000 | Loss: 0.00002317
Iteration 20/1000 | Loss: 0.00002316
Iteration 21/1000 | Loss: 0.00002311
Iteration 22/1000 | Loss: 0.00002310
Iteration 23/1000 | Loss: 0.00002310
Iteration 24/1000 | Loss: 0.00002308
Iteration 25/1000 | Loss: 0.00002307
Iteration 26/1000 | Loss: 0.00002307
Iteration 27/1000 | Loss: 0.00002307
Iteration 28/1000 | Loss: 0.00002306
Iteration 29/1000 | Loss: 0.00002306
Iteration 30/1000 | Loss: 0.00002305
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002305
Iteration 33/1000 | Loss: 0.00002305
Iteration 34/1000 | Loss: 0.00002304
Iteration 35/1000 | Loss: 0.00002304
Iteration 36/1000 | Loss: 0.00002304
Iteration 37/1000 | Loss: 0.00002304
Iteration 38/1000 | Loss: 0.00002303
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002303
Iteration 41/1000 | Loss: 0.00002303
Iteration 42/1000 | Loss: 0.00002302
Iteration 43/1000 | Loss: 0.00002302
Iteration 44/1000 | Loss: 0.00002302
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00002301
Iteration 48/1000 | Loss: 0.00002300
Iteration 49/1000 | Loss: 0.00002299
Iteration 50/1000 | Loss: 0.00002299
Iteration 51/1000 | Loss: 0.00002297
Iteration 52/1000 | Loss: 0.00002296
Iteration 53/1000 | Loss: 0.00002296
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002294
Iteration 57/1000 | Loss: 0.00002293
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002292
Iteration 61/1000 | Loss: 0.00002292
Iteration 62/1000 | Loss: 0.00002291
Iteration 63/1000 | Loss: 0.00002291
Iteration 64/1000 | Loss: 0.00002291
Iteration 65/1000 | Loss: 0.00002290
Iteration 66/1000 | Loss: 0.00002290
Iteration 67/1000 | Loss: 0.00002290
Iteration 68/1000 | Loss: 0.00002289
Iteration 69/1000 | Loss: 0.00002289
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002289
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002287
Iteration 77/1000 | Loss: 0.00002287
Iteration 78/1000 | Loss: 0.00002287
Iteration 79/1000 | Loss: 0.00002287
Iteration 80/1000 | Loss: 0.00002286
Iteration 81/1000 | Loss: 0.00002286
Iteration 82/1000 | Loss: 0.00002285
Iteration 83/1000 | Loss: 0.00002285
Iteration 84/1000 | Loss: 0.00002285
Iteration 85/1000 | Loss: 0.00002285
Iteration 86/1000 | Loss: 0.00002285
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002285
Iteration 89/1000 | Loss: 0.00002285
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002283
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002282
Iteration 103/1000 | Loss: 0.00002282
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002282
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002281
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002280
Iteration 118/1000 | Loss: 0.00002279
Iteration 119/1000 | Loss: 0.00002279
Iteration 120/1000 | Loss: 0.00002279
Iteration 121/1000 | Loss: 0.00002279
Iteration 122/1000 | Loss: 0.00002279
Iteration 123/1000 | Loss: 0.00002279
Iteration 124/1000 | Loss: 0.00002279
Iteration 125/1000 | Loss: 0.00002279
Iteration 126/1000 | Loss: 0.00002279
Iteration 127/1000 | Loss: 0.00002279
Iteration 128/1000 | Loss: 0.00002279
Iteration 129/1000 | Loss: 0.00002279
Iteration 130/1000 | Loss: 0.00002278
Iteration 131/1000 | Loss: 0.00002278
Iteration 132/1000 | Loss: 0.00002278
Iteration 133/1000 | Loss: 0.00002278
Iteration 134/1000 | Loss: 0.00002278
Iteration 135/1000 | Loss: 0.00002278
Iteration 136/1000 | Loss: 0.00002278
Iteration 137/1000 | Loss: 0.00002278
Iteration 138/1000 | Loss: 0.00002278
Iteration 139/1000 | Loss: 0.00002278
Iteration 140/1000 | Loss: 0.00002278
Iteration 141/1000 | Loss: 0.00002278
Iteration 142/1000 | Loss: 0.00002278
Iteration 143/1000 | Loss: 0.00002278
Iteration 144/1000 | Loss: 0.00002278
Iteration 145/1000 | Loss: 0.00002278
Iteration 146/1000 | Loss: 0.00002278
Iteration 147/1000 | Loss: 0.00002278
Iteration 148/1000 | Loss: 0.00002278
Iteration 149/1000 | Loss: 0.00002278
Iteration 150/1000 | Loss: 0.00002278
Iteration 151/1000 | Loss: 0.00002278
Iteration 152/1000 | Loss: 0.00002278
Iteration 153/1000 | Loss: 0.00002278
Iteration 154/1000 | Loss: 0.00002278
Iteration 155/1000 | Loss: 0.00002278
Iteration 156/1000 | Loss: 0.00002278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.2781519874115475e-05, 2.2781519874115475e-05, 2.2781519874115475e-05, 2.2781519874115475e-05, 2.2781519874115475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2781519874115475e-05

Optimization complete. Final v2v error: 4.007541179656982 mm

Highest mean error: 5.38268518447876 mm for frame 67

Lowest mean error: 3.4202394485473633 mm for frame 44

Saving results

Total time: 39.93764615058899
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408749
Iteration 2/25 | Loss: 0.00130316
Iteration 3/25 | Loss: 0.00124729
Iteration 4/25 | Loss: 0.00123780
Iteration 5/25 | Loss: 0.00123483
Iteration 6/25 | Loss: 0.00123433
Iteration 7/25 | Loss: 0.00123433
Iteration 8/25 | Loss: 0.00123433
Iteration 9/25 | Loss: 0.00123433
Iteration 10/25 | Loss: 0.00123433
Iteration 11/25 | Loss: 0.00123433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012343327980488539, 0.0012343327980488539, 0.0012343327980488539, 0.0012343327980488539, 0.0012343327980488539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012343327980488539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86568856
Iteration 2/25 | Loss: 0.00078564
Iteration 3/25 | Loss: 0.00078564
Iteration 4/25 | Loss: 0.00078564
Iteration 5/25 | Loss: 0.00078564
Iteration 6/25 | Loss: 0.00078564
Iteration 7/25 | Loss: 0.00078564
Iteration 8/25 | Loss: 0.00078564
Iteration 9/25 | Loss: 0.00078564
Iteration 10/25 | Loss: 0.00078564
Iteration 11/25 | Loss: 0.00078564
Iteration 12/25 | Loss: 0.00078564
Iteration 13/25 | Loss: 0.00078564
Iteration 14/25 | Loss: 0.00078564
Iteration 15/25 | Loss: 0.00078564
Iteration 16/25 | Loss: 0.00078564
Iteration 17/25 | Loss: 0.00078564
Iteration 18/25 | Loss: 0.00078564
Iteration 19/25 | Loss: 0.00078564
Iteration 20/25 | Loss: 0.00078564
Iteration 21/25 | Loss: 0.00078564
Iteration 22/25 | Loss: 0.00078564
Iteration 23/25 | Loss: 0.00078564
Iteration 24/25 | Loss: 0.00078564
Iteration 25/25 | Loss: 0.00078564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078564
Iteration 2/1000 | Loss: 0.00003344
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001692
Iteration 6/1000 | Loss: 0.00001629
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001515
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001469
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001455
Iteration 23/1000 | Loss: 0.00001451
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001442
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001434
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001432
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001430
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001425
Iteration 62/1000 | Loss: 0.00001425
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001424
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001421
Iteration 71/1000 | Loss: 0.00001421
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001419
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001413
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001410
Iteration 102/1000 | Loss: 0.00001410
Iteration 103/1000 | Loss: 0.00001410
Iteration 104/1000 | Loss: 0.00001410
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001409
Iteration 112/1000 | Loss: 0.00001409
Iteration 113/1000 | Loss: 0.00001409
Iteration 114/1000 | Loss: 0.00001409
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001408
Iteration 117/1000 | Loss: 0.00001408
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001404
Iteration 128/1000 | Loss: 0.00001404
Iteration 129/1000 | Loss: 0.00001404
Iteration 130/1000 | Loss: 0.00001404
Iteration 131/1000 | Loss: 0.00001404
Iteration 132/1000 | Loss: 0.00001403
Iteration 133/1000 | Loss: 0.00001403
Iteration 134/1000 | Loss: 0.00001403
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Iteration 142/1000 | Loss: 0.00001401
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001400
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001399
Iteration 155/1000 | Loss: 0.00001399
Iteration 156/1000 | Loss: 0.00001399
Iteration 157/1000 | Loss: 0.00001399
Iteration 158/1000 | Loss: 0.00001399
Iteration 159/1000 | Loss: 0.00001399
Iteration 160/1000 | Loss: 0.00001399
Iteration 161/1000 | Loss: 0.00001399
Iteration 162/1000 | Loss: 0.00001399
Iteration 163/1000 | Loss: 0.00001399
Iteration 164/1000 | Loss: 0.00001399
Iteration 165/1000 | Loss: 0.00001399
Iteration 166/1000 | Loss: 0.00001399
Iteration 167/1000 | Loss: 0.00001399
Iteration 168/1000 | Loss: 0.00001399
Iteration 169/1000 | Loss: 0.00001399
Iteration 170/1000 | Loss: 0.00001399
Iteration 171/1000 | Loss: 0.00001399
Iteration 172/1000 | Loss: 0.00001399
Iteration 173/1000 | Loss: 0.00001399
Iteration 174/1000 | Loss: 0.00001399
Iteration 175/1000 | Loss: 0.00001399
Iteration 176/1000 | Loss: 0.00001399
Iteration 177/1000 | Loss: 0.00001399
Iteration 178/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3988602404424455e-05, 1.3988602404424455e-05, 1.3988602404424455e-05, 1.3988602404424455e-05, 1.3988602404424455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3988602404424455e-05

Optimization complete. Final v2v error: 3.1993918418884277 mm

Highest mean error: 3.749911069869995 mm for frame 62

Lowest mean error: 3.012918710708618 mm for frame 116

Saving results

Total time: 38.39846205711365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446023
Iteration 2/25 | Loss: 0.00140154
Iteration 3/25 | Loss: 0.00130898
Iteration 4/25 | Loss: 0.00129913
Iteration 5/25 | Loss: 0.00129645
Iteration 6/25 | Loss: 0.00129645
Iteration 7/25 | Loss: 0.00129645
Iteration 8/25 | Loss: 0.00129645
Iteration 9/25 | Loss: 0.00129645
Iteration 10/25 | Loss: 0.00129645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012964505003765225, 0.0012964505003765225, 0.0012964505003765225, 0.0012964505003765225, 0.0012964505003765225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012964505003765225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42745638
Iteration 2/25 | Loss: 0.00074006
Iteration 3/25 | Loss: 0.00074006
Iteration 4/25 | Loss: 0.00074006
Iteration 5/25 | Loss: 0.00074006
Iteration 6/25 | Loss: 0.00074006
Iteration 7/25 | Loss: 0.00074006
Iteration 8/25 | Loss: 0.00074006
Iteration 9/25 | Loss: 0.00074006
Iteration 10/25 | Loss: 0.00074006
Iteration 11/25 | Loss: 0.00074006
Iteration 12/25 | Loss: 0.00074006
Iteration 13/25 | Loss: 0.00074006
Iteration 14/25 | Loss: 0.00074006
Iteration 15/25 | Loss: 0.00074006
Iteration 16/25 | Loss: 0.00074006
Iteration 17/25 | Loss: 0.00074006
Iteration 18/25 | Loss: 0.00074006
Iteration 19/25 | Loss: 0.00074006
Iteration 20/25 | Loss: 0.00074006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007400596514344215, 0.0007400596514344215, 0.0007400596514344215, 0.0007400596514344215, 0.0007400596514344215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007400596514344215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074006
Iteration 2/1000 | Loss: 0.00003215
Iteration 3/1000 | Loss: 0.00002424
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00002020
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001965
Iteration 12/1000 | Loss: 0.00001962
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001943
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001942
Iteration 18/1000 | Loss: 0.00001941
Iteration 19/1000 | Loss: 0.00001935
Iteration 20/1000 | Loss: 0.00001932
Iteration 21/1000 | Loss: 0.00001927
Iteration 22/1000 | Loss: 0.00001926
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001917
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00001916
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001916
Iteration 37/1000 | Loss: 0.00001916
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001916
Iteration 40/1000 | Loss: 0.00001916
Iteration 41/1000 | Loss: 0.00001916
Iteration 42/1000 | Loss: 0.00001916
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001912
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001910
Iteration 58/1000 | Loss: 0.00001910
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001909
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001908
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001907
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001906
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001905
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001903
Iteration 92/1000 | Loss: 0.00001903
Iteration 93/1000 | Loss: 0.00001903
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001903
Iteration 118/1000 | Loss: 0.00001903
Iteration 119/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.9033361240872182e-05, 1.9033361240872182e-05, 1.9033361240872182e-05, 1.9033361240872182e-05, 1.9033361240872182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9033361240872182e-05

Optimization complete. Final v2v error: 3.664454460144043 mm

Highest mean error: 4.7497782707214355 mm for frame 230

Lowest mean error: 3.1300270557403564 mm for frame 95

Saving results

Total time: 38.046292304992676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397997
Iteration 2/25 | Loss: 0.00128509
Iteration 3/25 | Loss: 0.00123152
Iteration 4/25 | Loss: 0.00122595
Iteration 5/25 | Loss: 0.00122468
Iteration 6/25 | Loss: 0.00122468
Iteration 7/25 | Loss: 0.00122468
Iteration 8/25 | Loss: 0.00122468
Iteration 9/25 | Loss: 0.00122468
Iteration 10/25 | Loss: 0.00122468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012246803380548954, 0.0012246803380548954, 0.0012246803380548954, 0.0012246803380548954, 0.0012246803380548954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012246803380548954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42699587
Iteration 2/25 | Loss: 0.00084089
Iteration 3/25 | Loss: 0.00084088
Iteration 4/25 | Loss: 0.00084088
Iteration 5/25 | Loss: 0.00084088
Iteration 6/25 | Loss: 0.00084088
Iteration 7/25 | Loss: 0.00084088
Iteration 8/25 | Loss: 0.00084088
Iteration 9/25 | Loss: 0.00084088
Iteration 10/25 | Loss: 0.00084088
Iteration 11/25 | Loss: 0.00084088
Iteration 12/25 | Loss: 0.00084088
Iteration 13/25 | Loss: 0.00084088
Iteration 14/25 | Loss: 0.00084088
Iteration 15/25 | Loss: 0.00084088
Iteration 16/25 | Loss: 0.00084088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008408764842897654, 0.0008408764842897654, 0.0008408764842897654, 0.0008408764842897654, 0.0008408764842897654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008408764842897654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084088
Iteration 2/1000 | Loss: 0.00003026
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001521
Iteration 5/1000 | Loss: 0.00001357
Iteration 6/1000 | Loss: 0.00001264
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001164
Iteration 9/1000 | Loss: 0.00001142
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001135
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001130
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001130
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001115
Iteration 28/1000 | Loss: 0.00001114
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001112
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001098
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001089
Iteration 54/1000 | Loss: 0.00001089
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001082
Iteration 68/1000 | Loss: 0.00001082
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001078
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001076
Iteration 82/1000 | Loss: 0.00001076
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001074
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001073
Iteration 93/1000 | Loss: 0.00001073
Iteration 94/1000 | Loss: 0.00001072
Iteration 95/1000 | Loss: 0.00001072
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001068
Iteration 111/1000 | Loss: 0.00001068
Iteration 112/1000 | Loss: 0.00001068
Iteration 113/1000 | Loss: 0.00001068
Iteration 114/1000 | Loss: 0.00001068
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001067
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001066
Iteration 126/1000 | Loss: 0.00001066
Iteration 127/1000 | Loss: 0.00001066
Iteration 128/1000 | Loss: 0.00001066
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001065
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001063
Iteration 139/1000 | Loss: 0.00001063
Iteration 140/1000 | Loss: 0.00001063
Iteration 141/1000 | Loss: 0.00001062
Iteration 142/1000 | Loss: 0.00001062
Iteration 143/1000 | Loss: 0.00001061
Iteration 144/1000 | Loss: 0.00001061
Iteration 145/1000 | Loss: 0.00001061
Iteration 146/1000 | Loss: 0.00001061
Iteration 147/1000 | Loss: 0.00001060
Iteration 148/1000 | Loss: 0.00001060
Iteration 149/1000 | Loss: 0.00001060
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001059
Iteration 152/1000 | Loss: 0.00001059
Iteration 153/1000 | Loss: 0.00001058
Iteration 154/1000 | Loss: 0.00001058
Iteration 155/1000 | Loss: 0.00001058
Iteration 156/1000 | Loss: 0.00001057
Iteration 157/1000 | Loss: 0.00001057
Iteration 158/1000 | Loss: 0.00001057
Iteration 159/1000 | Loss: 0.00001056
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001053
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001052
Iteration 171/1000 | Loss: 0.00001052
Iteration 172/1000 | Loss: 0.00001052
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001051
Iteration 175/1000 | Loss: 0.00001051
Iteration 176/1000 | Loss: 0.00001051
Iteration 177/1000 | Loss: 0.00001051
Iteration 178/1000 | Loss: 0.00001051
Iteration 179/1000 | Loss: 0.00001051
Iteration 180/1000 | Loss: 0.00001051
Iteration 181/1000 | Loss: 0.00001051
Iteration 182/1000 | Loss: 0.00001051
Iteration 183/1000 | Loss: 0.00001051
Iteration 184/1000 | Loss: 0.00001050
Iteration 185/1000 | Loss: 0.00001050
Iteration 186/1000 | Loss: 0.00001050
Iteration 187/1000 | Loss: 0.00001050
Iteration 188/1000 | Loss: 0.00001050
Iteration 189/1000 | Loss: 0.00001050
Iteration 190/1000 | Loss: 0.00001050
Iteration 191/1000 | Loss: 0.00001050
Iteration 192/1000 | Loss: 0.00001050
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001049
Iteration 195/1000 | Loss: 0.00001049
Iteration 196/1000 | Loss: 0.00001049
Iteration 197/1000 | Loss: 0.00001049
Iteration 198/1000 | Loss: 0.00001049
Iteration 199/1000 | Loss: 0.00001049
Iteration 200/1000 | Loss: 0.00001049
Iteration 201/1000 | Loss: 0.00001049
Iteration 202/1000 | Loss: 0.00001049
Iteration 203/1000 | Loss: 0.00001049
Iteration 204/1000 | Loss: 0.00001049
Iteration 205/1000 | Loss: 0.00001049
Iteration 206/1000 | Loss: 0.00001049
Iteration 207/1000 | Loss: 0.00001048
Iteration 208/1000 | Loss: 0.00001048
Iteration 209/1000 | Loss: 0.00001048
Iteration 210/1000 | Loss: 0.00001048
Iteration 211/1000 | Loss: 0.00001047
Iteration 212/1000 | Loss: 0.00001047
Iteration 213/1000 | Loss: 0.00001047
Iteration 214/1000 | Loss: 0.00001047
Iteration 215/1000 | Loss: 0.00001047
Iteration 216/1000 | Loss: 0.00001047
Iteration 217/1000 | Loss: 0.00001047
Iteration 218/1000 | Loss: 0.00001047
Iteration 219/1000 | Loss: 0.00001047
Iteration 220/1000 | Loss: 0.00001047
Iteration 221/1000 | Loss: 0.00001047
Iteration 222/1000 | Loss: 0.00001047
Iteration 223/1000 | Loss: 0.00001047
Iteration 224/1000 | Loss: 0.00001047
Iteration 225/1000 | Loss: 0.00001047
Iteration 226/1000 | Loss: 0.00001047
Iteration 227/1000 | Loss: 0.00001047
Iteration 228/1000 | Loss: 0.00001047
Iteration 229/1000 | Loss: 0.00001047
Iteration 230/1000 | Loss: 0.00001046
Iteration 231/1000 | Loss: 0.00001046
Iteration 232/1000 | Loss: 0.00001046
Iteration 233/1000 | Loss: 0.00001046
Iteration 234/1000 | Loss: 0.00001046
Iteration 235/1000 | Loss: 0.00001046
Iteration 236/1000 | Loss: 0.00001046
Iteration 237/1000 | Loss: 0.00001046
Iteration 238/1000 | Loss: 0.00001046
Iteration 239/1000 | Loss: 0.00001046
Iteration 240/1000 | Loss: 0.00001046
Iteration 241/1000 | Loss: 0.00001046
Iteration 242/1000 | Loss: 0.00001046
Iteration 243/1000 | Loss: 0.00001046
Iteration 244/1000 | Loss: 0.00001046
Iteration 245/1000 | Loss: 0.00001045
Iteration 246/1000 | Loss: 0.00001045
Iteration 247/1000 | Loss: 0.00001045
Iteration 248/1000 | Loss: 0.00001045
Iteration 249/1000 | Loss: 0.00001045
Iteration 250/1000 | Loss: 0.00001045
Iteration 251/1000 | Loss: 0.00001045
Iteration 252/1000 | Loss: 0.00001045
Iteration 253/1000 | Loss: 0.00001044
Iteration 254/1000 | Loss: 0.00001044
Iteration 255/1000 | Loss: 0.00001044
Iteration 256/1000 | Loss: 0.00001044
Iteration 257/1000 | Loss: 0.00001044
Iteration 258/1000 | Loss: 0.00001044
Iteration 259/1000 | Loss: 0.00001044
Iteration 260/1000 | Loss: 0.00001044
Iteration 261/1000 | Loss: 0.00001044
Iteration 262/1000 | Loss: 0.00001044
Iteration 263/1000 | Loss: 0.00001044
Iteration 264/1000 | Loss: 0.00001044
Iteration 265/1000 | Loss: 0.00001044
Iteration 266/1000 | Loss: 0.00001044
Iteration 267/1000 | Loss: 0.00001044
Iteration 268/1000 | Loss: 0.00001043
Iteration 269/1000 | Loss: 0.00001043
Iteration 270/1000 | Loss: 0.00001043
Iteration 271/1000 | Loss: 0.00001043
Iteration 272/1000 | Loss: 0.00001043
Iteration 273/1000 | Loss: 0.00001043
Iteration 274/1000 | Loss: 0.00001042
Iteration 275/1000 | Loss: 0.00001042
Iteration 276/1000 | Loss: 0.00001042
Iteration 277/1000 | Loss: 0.00001042
Iteration 278/1000 | Loss: 0.00001042
Iteration 279/1000 | Loss: 0.00001042
Iteration 280/1000 | Loss: 0.00001042
Iteration 281/1000 | Loss: 0.00001042
Iteration 282/1000 | Loss: 0.00001042
Iteration 283/1000 | Loss: 0.00001041
Iteration 284/1000 | Loss: 0.00001041
Iteration 285/1000 | Loss: 0.00001041
Iteration 286/1000 | Loss: 0.00001041
Iteration 287/1000 | Loss: 0.00001041
Iteration 288/1000 | Loss: 0.00001041
Iteration 289/1000 | Loss: 0.00001041
Iteration 290/1000 | Loss: 0.00001041
Iteration 291/1000 | Loss: 0.00001041
Iteration 292/1000 | Loss: 0.00001041
Iteration 293/1000 | Loss: 0.00001041
Iteration 294/1000 | Loss: 0.00001041
Iteration 295/1000 | Loss: 0.00001041
Iteration 296/1000 | Loss: 0.00001041
Iteration 297/1000 | Loss: 0.00001040
Iteration 298/1000 | Loss: 0.00001040
Iteration 299/1000 | Loss: 0.00001040
Iteration 300/1000 | Loss: 0.00001040
Iteration 301/1000 | Loss: 0.00001040
Iteration 302/1000 | Loss: 0.00001040
Iteration 303/1000 | Loss: 0.00001040
Iteration 304/1000 | Loss: 0.00001040
Iteration 305/1000 | Loss: 0.00001040
Iteration 306/1000 | Loss: 0.00001040
Iteration 307/1000 | Loss: 0.00001040
Iteration 308/1000 | Loss: 0.00001040
Iteration 309/1000 | Loss: 0.00001039
Iteration 310/1000 | Loss: 0.00001039
Iteration 311/1000 | Loss: 0.00001039
Iteration 312/1000 | Loss: 0.00001039
Iteration 313/1000 | Loss: 0.00001039
Iteration 314/1000 | Loss: 0.00001039
Iteration 315/1000 | Loss: 0.00001039
Iteration 316/1000 | Loss: 0.00001039
Iteration 317/1000 | Loss: 0.00001038
Iteration 318/1000 | Loss: 0.00001038
Iteration 319/1000 | Loss: 0.00001038
Iteration 320/1000 | Loss: 0.00001038
Iteration 321/1000 | Loss: 0.00001038
Iteration 322/1000 | Loss: 0.00001038
Iteration 323/1000 | Loss: 0.00001038
Iteration 324/1000 | Loss: 0.00001038
Iteration 325/1000 | Loss: 0.00001038
Iteration 326/1000 | Loss: 0.00001038
Iteration 327/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [1.0380133062426466e-05, 1.0380133062426466e-05, 1.0380133062426466e-05, 1.0380133062426466e-05, 1.0380133062426466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0380133062426466e-05

Optimization complete. Final v2v error: 2.7689473628997803 mm

Highest mean error: 2.8498644828796387 mm for frame 75

Lowest mean error: 2.717498779296875 mm for frame 112

Saving results

Total time: 44.37148952484131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00655163
Iteration 2/25 | Loss: 0.00152396
Iteration 3/25 | Loss: 0.00139123
Iteration 4/25 | Loss: 0.00138186
Iteration 5/25 | Loss: 0.00137981
Iteration 6/25 | Loss: 0.00137963
Iteration 7/25 | Loss: 0.00137963
Iteration 8/25 | Loss: 0.00137963
Iteration 9/25 | Loss: 0.00137963
Iteration 10/25 | Loss: 0.00137963
Iteration 11/25 | Loss: 0.00137963
Iteration 12/25 | Loss: 0.00137963
Iteration 13/25 | Loss: 0.00137963
Iteration 14/25 | Loss: 0.00137963
Iteration 15/25 | Loss: 0.00137963
Iteration 16/25 | Loss: 0.00137963
Iteration 17/25 | Loss: 0.00137963
Iteration 18/25 | Loss: 0.00137963
Iteration 19/25 | Loss: 0.00137963
Iteration 20/25 | Loss: 0.00137963
Iteration 21/25 | Loss: 0.00137963
Iteration 22/25 | Loss: 0.00137963
Iteration 23/25 | Loss: 0.00137963
Iteration 24/25 | Loss: 0.00137963
Iteration 25/25 | Loss: 0.00137963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.02450180
Iteration 2/25 | Loss: 0.00090639
Iteration 3/25 | Loss: 0.00090618
Iteration 4/25 | Loss: 0.00090618
Iteration 5/25 | Loss: 0.00090618
Iteration 6/25 | Loss: 0.00090618
Iteration 7/25 | Loss: 0.00090618
Iteration 8/25 | Loss: 0.00090618
Iteration 9/25 | Loss: 0.00090618
Iteration 10/25 | Loss: 0.00090618
Iteration 11/25 | Loss: 0.00090618
Iteration 12/25 | Loss: 0.00090618
Iteration 13/25 | Loss: 0.00090618
Iteration 14/25 | Loss: 0.00090618
Iteration 15/25 | Loss: 0.00090618
Iteration 16/25 | Loss: 0.00090618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009061816963367164, 0.0009061816963367164, 0.0009061816963367164, 0.0009061816963367164, 0.0009061816963367164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009061816963367164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090618
Iteration 2/1000 | Loss: 0.00006454
Iteration 3/1000 | Loss: 0.00003421
Iteration 4/1000 | Loss: 0.00002873
Iteration 5/1000 | Loss: 0.00002649
Iteration 6/1000 | Loss: 0.00002543
Iteration 7/1000 | Loss: 0.00002471
Iteration 8/1000 | Loss: 0.00002404
Iteration 9/1000 | Loss: 0.00002352
Iteration 10/1000 | Loss: 0.00002319
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002273
Iteration 13/1000 | Loss: 0.00002258
Iteration 14/1000 | Loss: 0.00002250
Iteration 15/1000 | Loss: 0.00002244
Iteration 16/1000 | Loss: 0.00002238
Iteration 17/1000 | Loss: 0.00002237
Iteration 18/1000 | Loss: 0.00002237
Iteration 19/1000 | Loss: 0.00002231
Iteration 20/1000 | Loss: 0.00002224
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002221
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002215
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002208
Iteration 28/1000 | Loss: 0.00002207
Iteration 29/1000 | Loss: 0.00002206
Iteration 30/1000 | Loss: 0.00002205
Iteration 31/1000 | Loss: 0.00002205
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002203
Iteration 35/1000 | Loss: 0.00002203
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002203
Iteration 39/1000 | Loss: 0.00002203
Iteration 40/1000 | Loss: 0.00002202
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00002202
Iteration 43/1000 | Loss: 0.00002202
Iteration 44/1000 | Loss: 0.00002202
Iteration 45/1000 | Loss: 0.00002202
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002201
Iteration 48/1000 | Loss: 0.00002201
Iteration 49/1000 | Loss: 0.00002201
Iteration 50/1000 | Loss: 0.00002198
Iteration 51/1000 | Loss: 0.00002198
Iteration 52/1000 | Loss: 0.00002197
Iteration 53/1000 | Loss: 0.00002197
Iteration 54/1000 | Loss: 0.00002197
Iteration 55/1000 | Loss: 0.00002196
Iteration 56/1000 | Loss: 0.00002196
Iteration 57/1000 | Loss: 0.00002196
Iteration 58/1000 | Loss: 0.00002196
Iteration 59/1000 | Loss: 0.00002196
Iteration 60/1000 | Loss: 0.00002196
Iteration 61/1000 | Loss: 0.00002196
Iteration 62/1000 | Loss: 0.00002195
Iteration 63/1000 | Loss: 0.00002195
Iteration 64/1000 | Loss: 0.00002195
Iteration 65/1000 | Loss: 0.00002195
Iteration 66/1000 | Loss: 0.00002194
Iteration 67/1000 | Loss: 0.00002194
Iteration 68/1000 | Loss: 0.00002194
Iteration 69/1000 | Loss: 0.00002193
Iteration 70/1000 | Loss: 0.00002193
Iteration 71/1000 | Loss: 0.00002193
Iteration 72/1000 | Loss: 0.00002193
Iteration 73/1000 | Loss: 0.00002193
Iteration 74/1000 | Loss: 0.00002192
Iteration 75/1000 | Loss: 0.00002192
Iteration 76/1000 | Loss: 0.00002192
Iteration 77/1000 | Loss: 0.00002191
Iteration 78/1000 | Loss: 0.00002191
Iteration 79/1000 | Loss: 0.00002191
Iteration 80/1000 | Loss: 0.00002190
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002189
Iteration 86/1000 | Loss: 0.00002189
Iteration 87/1000 | Loss: 0.00002189
Iteration 88/1000 | Loss: 0.00002189
Iteration 89/1000 | Loss: 0.00002188
Iteration 90/1000 | Loss: 0.00002188
Iteration 91/1000 | Loss: 0.00002188
Iteration 92/1000 | Loss: 0.00002188
Iteration 93/1000 | Loss: 0.00002187
Iteration 94/1000 | Loss: 0.00002187
Iteration 95/1000 | Loss: 0.00002187
Iteration 96/1000 | Loss: 0.00002187
Iteration 97/1000 | Loss: 0.00002186
Iteration 98/1000 | Loss: 0.00002186
Iteration 99/1000 | Loss: 0.00002186
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002184
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00002183
Iteration 111/1000 | Loss: 0.00002183
Iteration 112/1000 | Loss: 0.00002183
Iteration 113/1000 | Loss: 0.00002183
Iteration 114/1000 | Loss: 0.00002183
Iteration 115/1000 | Loss: 0.00002183
Iteration 116/1000 | Loss: 0.00002182
Iteration 117/1000 | Loss: 0.00002182
Iteration 118/1000 | Loss: 0.00002182
Iteration 119/1000 | Loss: 0.00002182
Iteration 120/1000 | Loss: 0.00002182
Iteration 121/1000 | Loss: 0.00002182
Iteration 122/1000 | Loss: 0.00002182
Iteration 123/1000 | Loss: 0.00002182
Iteration 124/1000 | Loss: 0.00002182
Iteration 125/1000 | Loss: 0.00002182
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002181
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002180
Iteration 134/1000 | Loss: 0.00002180
Iteration 135/1000 | Loss: 0.00002180
Iteration 136/1000 | Loss: 0.00002180
Iteration 137/1000 | Loss: 0.00002180
Iteration 138/1000 | Loss: 0.00002180
Iteration 139/1000 | Loss: 0.00002180
Iteration 140/1000 | Loss: 0.00002180
Iteration 141/1000 | Loss: 0.00002180
Iteration 142/1000 | Loss: 0.00002180
Iteration 143/1000 | Loss: 0.00002179
Iteration 144/1000 | Loss: 0.00002179
Iteration 145/1000 | Loss: 0.00002179
Iteration 146/1000 | Loss: 0.00002179
Iteration 147/1000 | Loss: 0.00002179
Iteration 148/1000 | Loss: 0.00002179
Iteration 149/1000 | Loss: 0.00002179
Iteration 150/1000 | Loss: 0.00002179
Iteration 151/1000 | Loss: 0.00002179
Iteration 152/1000 | Loss: 0.00002179
Iteration 153/1000 | Loss: 0.00002179
Iteration 154/1000 | Loss: 0.00002179
Iteration 155/1000 | Loss: 0.00002179
Iteration 156/1000 | Loss: 0.00002179
Iteration 157/1000 | Loss: 0.00002179
Iteration 158/1000 | Loss: 0.00002179
Iteration 159/1000 | Loss: 0.00002179
Iteration 160/1000 | Loss: 0.00002179
Iteration 161/1000 | Loss: 0.00002179
Iteration 162/1000 | Loss: 0.00002179
Iteration 163/1000 | Loss: 0.00002179
Iteration 164/1000 | Loss: 0.00002179
Iteration 165/1000 | Loss: 0.00002179
Iteration 166/1000 | Loss: 0.00002179
Iteration 167/1000 | Loss: 0.00002179
Iteration 168/1000 | Loss: 0.00002179
Iteration 169/1000 | Loss: 0.00002179
Iteration 170/1000 | Loss: 0.00002179
Iteration 171/1000 | Loss: 0.00002179
Iteration 172/1000 | Loss: 0.00002179
Iteration 173/1000 | Loss: 0.00002179
Iteration 174/1000 | Loss: 0.00002179
Iteration 175/1000 | Loss: 0.00002179
Iteration 176/1000 | Loss: 0.00002179
Iteration 177/1000 | Loss: 0.00002179
Iteration 178/1000 | Loss: 0.00002179
Iteration 179/1000 | Loss: 0.00002179
Iteration 180/1000 | Loss: 0.00002179
Iteration 181/1000 | Loss: 0.00002179
Iteration 182/1000 | Loss: 0.00002179
Iteration 183/1000 | Loss: 0.00002179
Iteration 184/1000 | Loss: 0.00002179
Iteration 185/1000 | Loss: 0.00002179
Iteration 186/1000 | Loss: 0.00002179
Iteration 187/1000 | Loss: 0.00002179
Iteration 188/1000 | Loss: 0.00002179
Iteration 189/1000 | Loss: 0.00002179
Iteration 190/1000 | Loss: 0.00002179
Iteration 191/1000 | Loss: 0.00002179
Iteration 192/1000 | Loss: 0.00002179
Iteration 193/1000 | Loss: 0.00002179
Iteration 194/1000 | Loss: 0.00002179
Iteration 195/1000 | Loss: 0.00002179
Iteration 196/1000 | Loss: 0.00002179
Iteration 197/1000 | Loss: 0.00002179
Iteration 198/1000 | Loss: 0.00002179
Iteration 199/1000 | Loss: 0.00002179
Iteration 200/1000 | Loss: 0.00002179
Iteration 201/1000 | Loss: 0.00002179
Iteration 202/1000 | Loss: 0.00002179
Iteration 203/1000 | Loss: 0.00002179
Iteration 204/1000 | Loss: 0.00002179
Iteration 205/1000 | Loss: 0.00002179
Iteration 206/1000 | Loss: 0.00002179
Iteration 207/1000 | Loss: 0.00002179
Iteration 208/1000 | Loss: 0.00002179
Iteration 209/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.1785874196211807e-05, 2.1785874196211807e-05, 2.1785874196211807e-05, 2.1785874196211807e-05, 2.1785874196211807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1785874196211807e-05

Optimization complete. Final v2v error: 3.8802435398101807 mm

Highest mean error: 4.596728801727295 mm for frame 124

Lowest mean error: 3.1453511714935303 mm for frame 21

Saving results

Total time: 42.156991720199585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414458
Iteration 2/25 | Loss: 0.00136903
Iteration 3/25 | Loss: 0.00126682
Iteration 4/25 | Loss: 0.00124834
Iteration 5/25 | Loss: 0.00124199
Iteration 6/25 | Loss: 0.00124025
Iteration 7/25 | Loss: 0.00123984
Iteration 8/25 | Loss: 0.00123984
Iteration 9/25 | Loss: 0.00123984
Iteration 10/25 | Loss: 0.00123984
Iteration 11/25 | Loss: 0.00123984
Iteration 12/25 | Loss: 0.00123984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012398406397551298, 0.0012398406397551298, 0.0012398406397551298, 0.0012398406397551298, 0.0012398406397551298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012398406397551298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42839932
Iteration 2/25 | Loss: 0.00079839
Iteration 3/25 | Loss: 0.00079839
Iteration 4/25 | Loss: 0.00079839
Iteration 5/25 | Loss: 0.00079839
Iteration 6/25 | Loss: 0.00079839
Iteration 7/25 | Loss: 0.00079838
Iteration 8/25 | Loss: 0.00079838
Iteration 9/25 | Loss: 0.00079838
Iteration 10/25 | Loss: 0.00079838
Iteration 11/25 | Loss: 0.00079838
Iteration 12/25 | Loss: 0.00079838
Iteration 13/25 | Loss: 0.00079838
Iteration 14/25 | Loss: 0.00079838
Iteration 15/25 | Loss: 0.00079838
Iteration 16/25 | Loss: 0.00079838
Iteration 17/25 | Loss: 0.00079838
Iteration 18/25 | Loss: 0.00079838
Iteration 19/25 | Loss: 0.00079838
Iteration 20/25 | Loss: 0.00079838
Iteration 21/25 | Loss: 0.00079838
Iteration 22/25 | Loss: 0.00079838
Iteration 23/25 | Loss: 0.00079838
Iteration 24/25 | Loss: 0.00079838
Iteration 25/25 | Loss: 0.00079838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079838
Iteration 2/1000 | Loss: 0.00003726
Iteration 3/1000 | Loss: 0.00002085
Iteration 4/1000 | Loss: 0.00001713
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001369
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001293
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001281
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001277
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001276
Iteration 26/1000 | Loss: 0.00001275
Iteration 27/1000 | Loss: 0.00001275
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001267
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001267
Iteration 35/1000 | Loss: 0.00001267
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001267
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001263
Iteration 44/1000 | Loss: 0.00001263
Iteration 45/1000 | Loss: 0.00001263
Iteration 46/1000 | Loss: 0.00001263
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001262
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001252
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001250
Iteration 103/1000 | Loss: 0.00001250
Iteration 104/1000 | Loss: 0.00001250
Iteration 105/1000 | Loss: 0.00001250
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001250
Iteration 111/1000 | Loss: 0.00001249
Iteration 112/1000 | Loss: 0.00001249
Iteration 113/1000 | Loss: 0.00001249
Iteration 114/1000 | Loss: 0.00001249
Iteration 115/1000 | Loss: 0.00001249
Iteration 116/1000 | Loss: 0.00001249
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001249
Iteration 121/1000 | Loss: 0.00001249
Iteration 122/1000 | Loss: 0.00001249
Iteration 123/1000 | Loss: 0.00001249
Iteration 124/1000 | Loss: 0.00001248
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001247
Iteration 132/1000 | Loss: 0.00001247
Iteration 133/1000 | Loss: 0.00001247
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001247
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Iteration 141/1000 | Loss: 0.00001245
Iteration 142/1000 | Loss: 0.00001245
Iteration 143/1000 | Loss: 0.00001245
Iteration 144/1000 | Loss: 0.00001245
Iteration 145/1000 | Loss: 0.00001245
Iteration 146/1000 | Loss: 0.00001244
Iteration 147/1000 | Loss: 0.00001244
Iteration 148/1000 | Loss: 0.00001244
Iteration 149/1000 | Loss: 0.00001244
Iteration 150/1000 | Loss: 0.00001244
Iteration 151/1000 | Loss: 0.00001244
Iteration 152/1000 | Loss: 0.00001244
Iteration 153/1000 | Loss: 0.00001243
Iteration 154/1000 | Loss: 0.00001243
Iteration 155/1000 | Loss: 0.00001243
Iteration 156/1000 | Loss: 0.00001243
Iteration 157/1000 | Loss: 0.00001243
Iteration 158/1000 | Loss: 0.00001243
Iteration 159/1000 | Loss: 0.00001243
Iteration 160/1000 | Loss: 0.00001243
Iteration 161/1000 | Loss: 0.00001243
Iteration 162/1000 | Loss: 0.00001243
Iteration 163/1000 | Loss: 0.00001243
Iteration 164/1000 | Loss: 0.00001243
Iteration 165/1000 | Loss: 0.00001243
Iteration 166/1000 | Loss: 0.00001243
Iteration 167/1000 | Loss: 0.00001243
Iteration 168/1000 | Loss: 0.00001243
Iteration 169/1000 | Loss: 0.00001243
Iteration 170/1000 | Loss: 0.00001243
Iteration 171/1000 | Loss: 0.00001243
Iteration 172/1000 | Loss: 0.00001242
Iteration 173/1000 | Loss: 0.00001242
Iteration 174/1000 | Loss: 0.00001242
Iteration 175/1000 | Loss: 0.00001242
Iteration 176/1000 | Loss: 0.00001241
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001241
Iteration 182/1000 | Loss: 0.00001240
Iteration 183/1000 | Loss: 0.00001240
Iteration 184/1000 | Loss: 0.00001240
Iteration 185/1000 | Loss: 0.00001239
Iteration 186/1000 | Loss: 0.00001239
Iteration 187/1000 | Loss: 0.00001239
Iteration 188/1000 | Loss: 0.00001239
Iteration 189/1000 | Loss: 0.00001239
Iteration 190/1000 | Loss: 0.00001239
Iteration 191/1000 | Loss: 0.00001239
Iteration 192/1000 | Loss: 0.00001239
Iteration 193/1000 | Loss: 0.00001238
Iteration 194/1000 | Loss: 0.00001238
Iteration 195/1000 | Loss: 0.00001238
Iteration 196/1000 | Loss: 0.00001238
Iteration 197/1000 | Loss: 0.00001237
Iteration 198/1000 | Loss: 0.00001237
Iteration 199/1000 | Loss: 0.00001237
Iteration 200/1000 | Loss: 0.00001237
Iteration 201/1000 | Loss: 0.00001237
Iteration 202/1000 | Loss: 0.00001236
Iteration 203/1000 | Loss: 0.00001236
Iteration 204/1000 | Loss: 0.00001236
Iteration 205/1000 | Loss: 0.00001236
Iteration 206/1000 | Loss: 0.00001236
Iteration 207/1000 | Loss: 0.00001236
Iteration 208/1000 | Loss: 0.00001236
Iteration 209/1000 | Loss: 0.00001236
Iteration 210/1000 | Loss: 0.00001236
Iteration 211/1000 | Loss: 0.00001236
Iteration 212/1000 | Loss: 0.00001236
Iteration 213/1000 | Loss: 0.00001236
Iteration 214/1000 | Loss: 0.00001236
Iteration 215/1000 | Loss: 0.00001236
Iteration 216/1000 | Loss: 0.00001235
Iteration 217/1000 | Loss: 0.00001235
Iteration 218/1000 | Loss: 0.00001235
Iteration 219/1000 | Loss: 0.00001235
Iteration 220/1000 | Loss: 0.00001235
Iteration 221/1000 | Loss: 0.00001235
Iteration 222/1000 | Loss: 0.00001235
Iteration 223/1000 | Loss: 0.00001235
Iteration 224/1000 | Loss: 0.00001235
Iteration 225/1000 | Loss: 0.00001235
Iteration 226/1000 | Loss: 0.00001235
Iteration 227/1000 | Loss: 0.00001235
Iteration 228/1000 | Loss: 0.00001235
Iteration 229/1000 | Loss: 0.00001235
Iteration 230/1000 | Loss: 0.00001235
Iteration 231/1000 | Loss: 0.00001234
Iteration 232/1000 | Loss: 0.00001234
Iteration 233/1000 | Loss: 0.00001234
Iteration 234/1000 | Loss: 0.00001234
Iteration 235/1000 | Loss: 0.00001234
Iteration 236/1000 | Loss: 0.00001234
Iteration 237/1000 | Loss: 0.00001234
Iteration 238/1000 | Loss: 0.00001234
Iteration 239/1000 | Loss: 0.00001234
Iteration 240/1000 | Loss: 0.00001234
Iteration 241/1000 | Loss: 0.00001234
Iteration 242/1000 | Loss: 0.00001234
Iteration 243/1000 | Loss: 0.00001234
Iteration 244/1000 | Loss: 0.00001234
Iteration 245/1000 | Loss: 0.00001234
Iteration 246/1000 | Loss: 0.00001233
Iteration 247/1000 | Loss: 0.00001233
Iteration 248/1000 | Loss: 0.00001233
Iteration 249/1000 | Loss: 0.00001233
Iteration 250/1000 | Loss: 0.00001233
Iteration 251/1000 | Loss: 0.00001233
Iteration 252/1000 | Loss: 0.00001233
Iteration 253/1000 | Loss: 0.00001233
Iteration 254/1000 | Loss: 0.00001233
Iteration 255/1000 | Loss: 0.00001233
Iteration 256/1000 | Loss: 0.00001233
Iteration 257/1000 | Loss: 0.00001233
Iteration 258/1000 | Loss: 0.00001233
Iteration 259/1000 | Loss: 0.00001233
Iteration 260/1000 | Loss: 0.00001233
Iteration 261/1000 | Loss: 0.00001233
Iteration 262/1000 | Loss: 0.00001233
Iteration 263/1000 | Loss: 0.00001233
Iteration 264/1000 | Loss: 0.00001233
Iteration 265/1000 | Loss: 0.00001233
Iteration 266/1000 | Loss: 0.00001233
Iteration 267/1000 | Loss: 0.00001233
Iteration 268/1000 | Loss: 0.00001232
Iteration 269/1000 | Loss: 0.00001232
Iteration 270/1000 | Loss: 0.00001232
Iteration 271/1000 | Loss: 0.00001232
Iteration 272/1000 | Loss: 0.00001232
Iteration 273/1000 | Loss: 0.00001232
Iteration 274/1000 | Loss: 0.00001232
Iteration 275/1000 | Loss: 0.00001232
Iteration 276/1000 | Loss: 0.00001232
Iteration 277/1000 | Loss: 0.00001232
Iteration 278/1000 | Loss: 0.00001232
Iteration 279/1000 | Loss: 0.00001232
Iteration 280/1000 | Loss: 0.00001232
Iteration 281/1000 | Loss: 0.00001232
Iteration 282/1000 | Loss: 0.00001232
Iteration 283/1000 | Loss: 0.00001232
Iteration 284/1000 | Loss: 0.00001232
Iteration 285/1000 | Loss: 0.00001232
Iteration 286/1000 | Loss: 0.00001232
Iteration 287/1000 | Loss: 0.00001232
Iteration 288/1000 | Loss: 0.00001232
Iteration 289/1000 | Loss: 0.00001232
Iteration 290/1000 | Loss: 0.00001232
Iteration 291/1000 | Loss: 0.00001232
Iteration 292/1000 | Loss: 0.00001232
Iteration 293/1000 | Loss: 0.00001232
Iteration 294/1000 | Loss: 0.00001232
Iteration 295/1000 | Loss: 0.00001232
Iteration 296/1000 | Loss: 0.00001232
Iteration 297/1000 | Loss: 0.00001232
Iteration 298/1000 | Loss: 0.00001232
Iteration 299/1000 | Loss: 0.00001232
Iteration 300/1000 | Loss: 0.00001232
Iteration 301/1000 | Loss: 0.00001232
Iteration 302/1000 | Loss: 0.00001232
Iteration 303/1000 | Loss: 0.00001232
Iteration 304/1000 | Loss: 0.00001232
Iteration 305/1000 | Loss: 0.00001232
Iteration 306/1000 | Loss: 0.00001232
Iteration 307/1000 | Loss: 0.00001232
Iteration 308/1000 | Loss: 0.00001232
Iteration 309/1000 | Loss: 0.00001232
Iteration 310/1000 | Loss: 0.00001232
Iteration 311/1000 | Loss: 0.00001232
Iteration 312/1000 | Loss: 0.00001232
Iteration 313/1000 | Loss: 0.00001232
Iteration 314/1000 | Loss: 0.00001232
Iteration 315/1000 | Loss: 0.00001232
Iteration 316/1000 | Loss: 0.00001232
Iteration 317/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [1.2324883755354676e-05, 1.2324883755354676e-05, 1.2324883755354676e-05, 1.2324883755354676e-05, 1.2324883755354676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2324883755354676e-05

Optimization complete. Final v2v error: 2.996521472930908 mm

Highest mean error: 3.677150011062622 mm for frame 28

Lowest mean error: 2.7395315170288086 mm for frame 89

Saving results

Total time: 46.024723052978516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377053
Iteration 2/25 | Loss: 0.00148506
Iteration 3/25 | Loss: 0.00128424
Iteration 4/25 | Loss: 0.00125445
Iteration 5/25 | Loss: 0.00124674
Iteration 6/25 | Loss: 0.00124496
Iteration 7/25 | Loss: 0.00124480
Iteration 8/25 | Loss: 0.00124480
Iteration 9/25 | Loss: 0.00124480
Iteration 10/25 | Loss: 0.00124480
Iteration 11/25 | Loss: 0.00124480
Iteration 12/25 | Loss: 0.00124480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012448030756786466, 0.0012448030756786466, 0.0012448030756786466, 0.0012448030756786466, 0.0012448030756786466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012448030756786466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49343944
Iteration 2/25 | Loss: 0.00079129
Iteration 3/25 | Loss: 0.00079129
Iteration 4/25 | Loss: 0.00079129
Iteration 5/25 | Loss: 0.00079129
Iteration 6/25 | Loss: 0.00079129
Iteration 7/25 | Loss: 0.00079128
Iteration 8/25 | Loss: 0.00079128
Iteration 9/25 | Loss: 0.00079128
Iteration 10/25 | Loss: 0.00079128
Iteration 11/25 | Loss: 0.00079128
Iteration 12/25 | Loss: 0.00079128
Iteration 13/25 | Loss: 0.00079128
Iteration 14/25 | Loss: 0.00079128
Iteration 15/25 | Loss: 0.00079128
Iteration 16/25 | Loss: 0.00079128
Iteration 17/25 | Loss: 0.00079128
Iteration 18/25 | Loss: 0.00079128
Iteration 19/25 | Loss: 0.00079128
Iteration 20/25 | Loss: 0.00079128
Iteration 21/25 | Loss: 0.00079128
Iteration 22/25 | Loss: 0.00079128
Iteration 23/25 | Loss: 0.00079128
Iteration 24/25 | Loss: 0.00079128
Iteration 25/25 | Loss: 0.00079128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079128
Iteration 2/1000 | Loss: 0.00003681
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001620
Iteration 7/1000 | Loss: 0.00001564
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001463
Iteration 11/1000 | Loss: 0.00001444
Iteration 12/1000 | Loss: 0.00001443
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001418
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001415
Iteration 20/1000 | Loss: 0.00001414
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001405
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001399
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001395
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001393
Iteration 81/1000 | Loss: 0.00001393
Iteration 82/1000 | Loss: 0.00001392
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001392
Iteration 89/1000 | Loss: 0.00001392
Iteration 90/1000 | Loss: 0.00001392
Iteration 91/1000 | Loss: 0.00001392
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001391
Iteration 94/1000 | Loss: 0.00001391
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001391
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001389
Iteration 109/1000 | Loss: 0.00001389
Iteration 110/1000 | Loss: 0.00001389
Iteration 111/1000 | Loss: 0.00001389
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001384
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001384
Iteration 156/1000 | Loss: 0.00001384
Iteration 157/1000 | Loss: 0.00001384
Iteration 158/1000 | Loss: 0.00001384
Iteration 159/1000 | Loss: 0.00001384
Iteration 160/1000 | Loss: 0.00001384
Iteration 161/1000 | Loss: 0.00001384
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00001384
Iteration 165/1000 | Loss: 0.00001384
Iteration 166/1000 | Loss: 0.00001384
Iteration 167/1000 | Loss: 0.00001384
Iteration 168/1000 | Loss: 0.00001384
Iteration 169/1000 | Loss: 0.00001384
Iteration 170/1000 | Loss: 0.00001384
Iteration 171/1000 | Loss: 0.00001384
Iteration 172/1000 | Loss: 0.00001384
Iteration 173/1000 | Loss: 0.00001383
Iteration 174/1000 | Loss: 0.00001383
Iteration 175/1000 | Loss: 0.00001383
Iteration 176/1000 | Loss: 0.00001383
Iteration 177/1000 | Loss: 0.00001383
Iteration 178/1000 | Loss: 0.00001383
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001383
Iteration 183/1000 | Loss: 0.00001383
Iteration 184/1000 | Loss: 0.00001383
Iteration 185/1000 | Loss: 0.00001383
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001383
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Iteration 192/1000 | Loss: 0.00001383
Iteration 193/1000 | Loss: 0.00001383
Iteration 194/1000 | Loss: 0.00001383
Iteration 195/1000 | Loss: 0.00001383
Iteration 196/1000 | Loss: 0.00001383
Iteration 197/1000 | Loss: 0.00001383
Iteration 198/1000 | Loss: 0.00001383
Iteration 199/1000 | Loss: 0.00001383
Iteration 200/1000 | Loss: 0.00001383
Iteration 201/1000 | Loss: 0.00001383
Iteration 202/1000 | Loss: 0.00001383
Iteration 203/1000 | Loss: 0.00001383
Iteration 204/1000 | Loss: 0.00001383
Iteration 205/1000 | Loss: 0.00001383
Iteration 206/1000 | Loss: 0.00001383
Iteration 207/1000 | Loss: 0.00001383
Iteration 208/1000 | Loss: 0.00001383
Iteration 209/1000 | Loss: 0.00001383
Iteration 210/1000 | Loss: 0.00001383
Iteration 211/1000 | Loss: 0.00001383
Iteration 212/1000 | Loss: 0.00001383
Iteration 213/1000 | Loss: 0.00001383
Iteration 214/1000 | Loss: 0.00001383
Iteration 215/1000 | Loss: 0.00001383
Iteration 216/1000 | Loss: 0.00001383
Iteration 217/1000 | Loss: 0.00001383
Iteration 218/1000 | Loss: 0.00001383
Iteration 219/1000 | Loss: 0.00001383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.3833607226843014e-05, 1.3833607226843014e-05, 1.3833607226843014e-05, 1.3833607226843014e-05, 1.3833607226843014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3833607226843014e-05

Optimization complete. Final v2v error: 3.221733570098877 mm

Highest mean error: 3.5332844257354736 mm for frame 40

Lowest mean error: 2.7806897163391113 mm for frame 4

Saving results

Total time: 38.54361581802368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911716
Iteration 2/25 | Loss: 0.00141400
Iteration 3/25 | Loss: 0.00130870
Iteration 4/25 | Loss: 0.00129808
Iteration 5/25 | Loss: 0.00129564
Iteration 6/25 | Loss: 0.00129564
Iteration 7/25 | Loss: 0.00129564
Iteration 8/25 | Loss: 0.00129564
Iteration 9/25 | Loss: 0.00129564
Iteration 10/25 | Loss: 0.00129564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012956426944583654, 0.0012956426944583654, 0.0012956426944583654, 0.0012956426944583654, 0.0012956426944583654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012956426944583654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44067788
Iteration 2/25 | Loss: 0.00089186
Iteration 3/25 | Loss: 0.00089185
Iteration 4/25 | Loss: 0.00089185
Iteration 5/25 | Loss: 0.00089185
Iteration 6/25 | Loss: 0.00089185
Iteration 7/25 | Loss: 0.00089185
Iteration 8/25 | Loss: 0.00089185
Iteration 9/25 | Loss: 0.00089185
Iteration 10/25 | Loss: 0.00089185
Iteration 11/25 | Loss: 0.00089185
Iteration 12/25 | Loss: 0.00089185
Iteration 13/25 | Loss: 0.00089185
Iteration 14/25 | Loss: 0.00089185
Iteration 15/25 | Loss: 0.00089185
Iteration 16/25 | Loss: 0.00089185
Iteration 17/25 | Loss: 0.00089185
Iteration 18/25 | Loss: 0.00089185
Iteration 19/25 | Loss: 0.00089185
Iteration 20/25 | Loss: 0.00089185
Iteration 21/25 | Loss: 0.00089185
Iteration 22/25 | Loss: 0.00089185
Iteration 23/25 | Loss: 0.00089185
Iteration 24/25 | Loss: 0.00089185
Iteration 25/25 | Loss: 0.00089185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089185
Iteration 2/1000 | Loss: 0.00002797
Iteration 3/1000 | Loss: 0.00001868
Iteration 4/1000 | Loss: 0.00001698
Iteration 5/1000 | Loss: 0.00001623
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001515
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001463
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001392
Iteration 18/1000 | Loss: 0.00001391
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001389
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001376
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001361
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001360
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001354
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001353
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001340
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.3402301192400046e-05, 1.3402301192400046e-05, 1.3402301192400046e-05, 1.3402301192400046e-05, 1.3402301192400046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3402301192400046e-05

Optimization complete. Final v2v error: 3.120772123336792 mm

Highest mean error: 3.7158873081207275 mm for frame 57

Lowest mean error: 2.897308826446533 mm for frame 103

Saving results

Total time: 36.347984790802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781977
Iteration 2/25 | Loss: 0.00188718
Iteration 3/25 | Loss: 0.00140114
Iteration 4/25 | Loss: 0.00135315
Iteration 5/25 | Loss: 0.00134881
Iteration 6/25 | Loss: 0.00134790
Iteration 7/25 | Loss: 0.00134790
Iteration 8/25 | Loss: 0.00134790
Iteration 9/25 | Loss: 0.00134790
Iteration 10/25 | Loss: 0.00134790
Iteration 11/25 | Loss: 0.00134790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013479037443175912, 0.0013479037443175912, 0.0013479037443175912, 0.0013479037443175912, 0.0013479037443175912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013479037443175912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43580198
Iteration 2/25 | Loss: 0.00088380
Iteration 3/25 | Loss: 0.00088378
Iteration 4/25 | Loss: 0.00088378
Iteration 5/25 | Loss: 0.00088378
Iteration 6/25 | Loss: 0.00088378
Iteration 7/25 | Loss: 0.00088378
Iteration 8/25 | Loss: 0.00088378
Iteration 9/25 | Loss: 0.00088378
Iteration 10/25 | Loss: 0.00088378
Iteration 11/25 | Loss: 0.00088378
Iteration 12/25 | Loss: 0.00088378
Iteration 13/25 | Loss: 0.00088378
Iteration 14/25 | Loss: 0.00088378
Iteration 15/25 | Loss: 0.00088378
Iteration 16/25 | Loss: 0.00088378
Iteration 17/25 | Loss: 0.00088378
Iteration 18/25 | Loss: 0.00088378
Iteration 19/25 | Loss: 0.00088378
Iteration 20/25 | Loss: 0.00088378
Iteration 21/25 | Loss: 0.00088378
Iteration 22/25 | Loss: 0.00088378
Iteration 23/25 | Loss: 0.00088378
Iteration 24/25 | Loss: 0.00088378
Iteration 25/25 | Loss: 0.00088378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088378
Iteration 2/1000 | Loss: 0.00004670
Iteration 3/1000 | Loss: 0.00003105
Iteration 4/1000 | Loss: 0.00002613
Iteration 5/1000 | Loss: 0.00002460
Iteration 6/1000 | Loss: 0.00002383
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002293
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002252
Iteration 11/1000 | Loss: 0.00002249
Iteration 12/1000 | Loss: 0.00002243
Iteration 13/1000 | Loss: 0.00002233
Iteration 14/1000 | Loss: 0.00002223
Iteration 15/1000 | Loss: 0.00002222
Iteration 16/1000 | Loss: 0.00002221
Iteration 17/1000 | Loss: 0.00002221
Iteration 18/1000 | Loss: 0.00002221
Iteration 19/1000 | Loss: 0.00002221
Iteration 20/1000 | Loss: 0.00002221
Iteration 21/1000 | Loss: 0.00002221
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002221
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002220
Iteration 27/1000 | Loss: 0.00002219
Iteration 28/1000 | Loss: 0.00002219
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002217
Iteration 32/1000 | Loss: 0.00002217
Iteration 33/1000 | Loss: 0.00002216
Iteration 34/1000 | Loss: 0.00002216
Iteration 35/1000 | Loss: 0.00002215
Iteration 36/1000 | Loss: 0.00002215
Iteration 37/1000 | Loss: 0.00002215
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002215
Iteration 42/1000 | Loss: 0.00002215
Iteration 43/1000 | Loss: 0.00002214
Iteration 44/1000 | Loss: 0.00002214
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002214
Iteration 49/1000 | Loss: 0.00002213
Iteration 50/1000 | Loss: 0.00002212
Iteration 51/1000 | Loss: 0.00002211
Iteration 52/1000 | Loss: 0.00002211
Iteration 53/1000 | Loss: 0.00002211
Iteration 54/1000 | Loss: 0.00002211
Iteration 55/1000 | Loss: 0.00002211
Iteration 56/1000 | Loss: 0.00002210
Iteration 57/1000 | Loss: 0.00002210
Iteration 58/1000 | Loss: 0.00002210
Iteration 59/1000 | Loss: 0.00002209
Iteration 60/1000 | Loss: 0.00002208
Iteration 61/1000 | Loss: 0.00002208
Iteration 62/1000 | Loss: 0.00002207
Iteration 63/1000 | Loss: 0.00002207
Iteration 64/1000 | Loss: 0.00002207
Iteration 65/1000 | Loss: 0.00002207
Iteration 66/1000 | Loss: 0.00002207
Iteration 67/1000 | Loss: 0.00002206
Iteration 68/1000 | Loss: 0.00002206
Iteration 69/1000 | Loss: 0.00002206
Iteration 70/1000 | Loss: 0.00002206
Iteration 71/1000 | Loss: 0.00002206
Iteration 72/1000 | Loss: 0.00002206
Iteration 73/1000 | Loss: 0.00002206
Iteration 74/1000 | Loss: 0.00002205
Iteration 75/1000 | Loss: 0.00002205
Iteration 76/1000 | Loss: 0.00002205
Iteration 77/1000 | Loss: 0.00002205
Iteration 78/1000 | Loss: 0.00002205
Iteration 79/1000 | Loss: 0.00002205
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002204
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002204
Iteration 91/1000 | Loss: 0.00002204
Iteration 92/1000 | Loss: 0.00002203
Iteration 93/1000 | Loss: 0.00002203
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002203
Iteration 96/1000 | Loss: 0.00002203
Iteration 97/1000 | Loss: 0.00002203
Iteration 98/1000 | Loss: 0.00002203
Iteration 99/1000 | Loss: 0.00002202
Iteration 100/1000 | Loss: 0.00002202
Iteration 101/1000 | Loss: 0.00002202
Iteration 102/1000 | Loss: 0.00002202
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002202
Iteration 108/1000 | Loss: 0.00002201
Iteration 109/1000 | Loss: 0.00002201
Iteration 110/1000 | Loss: 0.00002201
Iteration 111/1000 | Loss: 0.00002201
Iteration 112/1000 | Loss: 0.00002201
Iteration 113/1000 | Loss: 0.00002201
Iteration 114/1000 | Loss: 0.00002201
Iteration 115/1000 | Loss: 0.00002201
Iteration 116/1000 | Loss: 0.00002201
Iteration 117/1000 | Loss: 0.00002201
Iteration 118/1000 | Loss: 0.00002201
Iteration 119/1000 | Loss: 0.00002200
Iteration 120/1000 | Loss: 0.00002200
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002199
Iteration 124/1000 | Loss: 0.00002199
Iteration 125/1000 | Loss: 0.00002199
Iteration 126/1000 | Loss: 0.00002199
Iteration 127/1000 | Loss: 0.00002199
Iteration 128/1000 | Loss: 0.00002199
Iteration 129/1000 | Loss: 0.00002199
Iteration 130/1000 | Loss: 0.00002199
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002198
Iteration 133/1000 | Loss: 0.00002198
Iteration 134/1000 | Loss: 0.00002198
Iteration 135/1000 | Loss: 0.00002198
Iteration 136/1000 | Loss: 0.00002198
Iteration 137/1000 | Loss: 0.00002198
Iteration 138/1000 | Loss: 0.00002198
Iteration 139/1000 | Loss: 0.00002198
Iteration 140/1000 | Loss: 0.00002198
Iteration 141/1000 | Loss: 0.00002198
Iteration 142/1000 | Loss: 0.00002197
Iteration 143/1000 | Loss: 0.00002197
Iteration 144/1000 | Loss: 0.00002197
Iteration 145/1000 | Loss: 0.00002197
Iteration 146/1000 | Loss: 0.00002197
Iteration 147/1000 | Loss: 0.00002197
Iteration 148/1000 | Loss: 0.00002197
Iteration 149/1000 | Loss: 0.00002197
Iteration 150/1000 | Loss: 0.00002197
Iteration 151/1000 | Loss: 0.00002197
Iteration 152/1000 | Loss: 0.00002197
Iteration 153/1000 | Loss: 0.00002197
Iteration 154/1000 | Loss: 0.00002197
Iteration 155/1000 | Loss: 0.00002197
Iteration 156/1000 | Loss: 0.00002197
Iteration 157/1000 | Loss: 0.00002197
Iteration 158/1000 | Loss: 0.00002197
Iteration 159/1000 | Loss: 0.00002197
Iteration 160/1000 | Loss: 0.00002196
Iteration 161/1000 | Loss: 0.00002196
Iteration 162/1000 | Loss: 0.00002196
Iteration 163/1000 | Loss: 0.00002196
Iteration 164/1000 | Loss: 0.00002196
Iteration 165/1000 | Loss: 0.00002196
Iteration 166/1000 | Loss: 0.00002196
Iteration 167/1000 | Loss: 0.00002196
Iteration 168/1000 | Loss: 0.00002196
Iteration 169/1000 | Loss: 0.00002196
Iteration 170/1000 | Loss: 0.00002196
Iteration 171/1000 | Loss: 0.00002196
Iteration 172/1000 | Loss: 0.00002196
Iteration 173/1000 | Loss: 0.00002196
Iteration 174/1000 | Loss: 0.00002196
Iteration 175/1000 | Loss: 0.00002196
Iteration 176/1000 | Loss: 0.00002196
Iteration 177/1000 | Loss: 0.00002196
Iteration 178/1000 | Loss: 0.00002196
Iteration 179/1000 | Loss: 0.00002196
Iteration 180/1000 | Loss: 0.00002196
Iteration 181/1000 | Loss: 0.00002196
Iteration 182/1000 | Loss: 0.00002196
Iteration 183/1000 | Loss: 0.00002196
Iteration 184/1000 | Loss: 0.00002196
Iteration 185/1000 | Loss: 0.00002196
Iteration 186/1000 | Loss: 0.00002196
Iteration 187/1000 | Loss: 0.00002196
Iteration 188/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.1960151570965536e-05, 2.1960151570965536e-05, 2.1960151570965536e-05, 2.1960151570965536e-05, 2.1960151570965536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1960151570965536e-05

Optimization complete. Final v2v error: 3.9358222484588623 mm

Highest mean error: 4.488140106201172 mm for frame 135

Lowest mean error: 3.680863380432129 mm for frame 110

Saving results

Total time: 35.658085346221924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834905
Iteration 2/25 | Loss: 0.00157760
Iteration 3/25 | Loss: 0.00141954
Iteration 4/25 | Loss: 0.00141240
Iteration 5/25 | Loss: 0.00141125
Iteration 6/25 | Loss: 0.00141125
Iteration 7/25 | Loss: 0.00141125
Iteration 8/25 | Loss: 0.00141125
Iteration 9/25 | Loss: 0.00141125
Iteration 10/25 | Loss: 0.00141125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014112533535808325, 0.0014112533535808325, 0.0014112533535808325, 0.0014112533535808325, 0.0014112533535808325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014112533535808325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03235567
Iteration 2/25 | Loss: 0.00068880
Iteration 3/25 | Loss: 0.00068879
Iteration 4/25 | Loss: 0.00068879
Iteration 5/25 | Loss: 0.00068879
Iteration 6/25 | Loss: 0.00068879
Iteration 7/25 | Loss: 0.00068879
Iteration 8/25 | Loss: 0.00068879
Iteration 9/25 | Loss: 0.00068879
Iteration 10/25 | Loss: 0.00068879
Iteration 11/25 | Loss: 0.00068879
Iteration 12/25 | Loss: 0.00068879
Iteration 13/25 | Loss: 0.00068879
Iteration 14/25 | Loss: 0.00068879
Iteration 15/25 | Loss: 0.00068879
Iteration 16/25 | Loss: 0.00068879
Iteration 17/25 | Loss: 0.00068879
Iteration 18/25 | Loss: 0.00068879
Iteration 19/25 | Loss: 0.00068879
Iteration 20/25 | Loss: 0.00068879
Iteration 21/25 | Loss: 0.00068879
Iteration 22/25 | Loss: 0.00068879
Iteration 23/25 | Loss: 0.00068879
Iteration 24/25 | Loss: 0.00068879
Iteration 25/25 | Loss: 0.00068879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068879
Iteration 2/1000 | Loss: 0.00004856
Iteration 3/1000 | Loss: 0.00003464
Iteration 4/1000 | Loss: 0.00003071
Iteration 5/1000 | Loss: 0.00002927
Iteration 6/1000 | Loss: 0.00002847
Iteration 7/1000 | Loss: 0.00002774
Iteration 8/1000 | Loss: 0.00002723
Iteration 9/1000 | Loss: 0.00002687
Iteration 10/1000 | Loss: 0.00002650
Iteration 11/1000 | Loss: 0.00002616
Iteration 12/1000 | Loss: 0.00002585
Iteration 13/1000 | Loss: 0.00002554
Iteration 14/1000 | Loss: 0.00002534
Iteration 15/1000 | Loss: 0.00002531
Iteration 16/1000 | Loss: 0.00002522
Iteration 17/1000 | Loss: 0.00002508
Iteration 18/1000 | Loss: 0.00002508
Iteration 19/1000 | Loss: 0.00002508
Iteration 20/1000 | Loss: 0.00002507
Iteration 21/1000 | Loss: 0.00002504
Iteration 22/1000 | Loss: 0.00002503
Iteration 23/1000 | Loss: 0.00002498
Iteration 24/1000 | Loss: 0.00002498
Iteration 25/1000 | Loss: 0.00002498
Iteration 26/1000 | Loss: 0.00002498
Iteration 27/1000 | Loss: 0.00002498
Iteration 28/1000 | Loss: 0.00002498
Iteration 29/1000 | Loss: 0.00002498
Iteration 30/1000 | Loss: 0.00002498
Iteration 31/1000 | Loss: 0.00002498
Iteration 32/1000 | Loss: 0.00002497
Iteration 33/1000 | Loss: 0.00002497
Iteration 34/1000 | Loss: 0.00002497
Iteration 35/1000 | Loss: 0.00002497
Iteration 36/1000 | Loss: 0.00002497
Iteration 37/1000 | Loss: 0.00002496
Iteration 38/1000 | Loss: 0.00002494
Iteration 39/1000 | Loss: 0.00002494
Iteration 40/1000 | Loss: 0.00002494
Iteration 41/1000 | Loss: 0.00002494
Iteration 42/1000 | Loss: 0.00002494
Iteration 43/1000 | Loss: 0.00002493
Iteration 44/1000 | Loss: 0.00002493
Iteration 45/1000 | Loss: 0.00002493
Iteration 46/1000 | Loss: 0.00002493
Iteration 47/1000 | Loss: 0.00002493
Iteration 48/1000 | Loss: 0.00002493
Iteration 49/1000 | Loss: 0.00002493
Iteration 50/1000 | Loss: 0.00002492
Iteration 51/1000 | Loss: 0.00002492
Iteration 52/1000 | Loss: 0.00002492
Iteration 53/1000 | Loss: 0.00002491
Iteration 54/1000 | Loss: 0.00002491
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002490
Iteration 57/1000 | Loss: 0.00002486
Iteration 58/1000 | Loss: 0.00002486
Iteration 59/1000 | Loss: 0.00002486
Iteration 60/1000 | Loss: 0.00002485
Iteration 61/1000 | Loss: 0.00002485
Iteration 62/1000 | Loss: 0.00002485
Iteration 63/1000 | Loss: 0.00002485
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002485
Iteration 67/1000 | Loss: 0.00002485
Iteration 68/1000 | Loss: 0.00002484
Iteration 69/1000 | Loss: 0.00002484
Iteration 70/1000 | Loss: 0.00002484
Iteration 71/1000 | Loss: 0.00002484
Iteration 72/1000 | Loss: 0.00002484
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002484
Iteration 80/1000 | Loss: 0.00002484
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002483
Iteration 83/1000 | Loss: 0.00002483
Iteration 84/1000 | Loss: 0.00002483
Iteration 85/1000 | Loss: 0.00002483
Iteration 86/1000 | Loss: 0.00002483
Iteration 87/1000 | Loss: 0.00002483
Iteration 88/1000 | Loss: 0.00002482
Iteration 89/1000 | Loss: 0.00002482
Iteration 90/1000 | Loss: 0.00002482
Iteration 91/1000 | Loss: 0.00002482
Iteration 92/1000 | Loss: 0.00002482
Iteration 93/1000 | Loss: 0.00002482
Iteration 94/1000 | Loss: 0.00002482
Iteration 95/1000 | Loss: 0.00002482
Iteration 96/1000 | Loss: 0.00002481
Iteration 97/1000 | Loss: 0.00002481
Iteration 98/1000 | Loss: 0.00002481
Iteration 99/1000 | Loss: 0.00002481
Iteration 100/1000 | Loss: 0.00002481
Iteration 101/1000 | Loss: 0.00002481
Iteration 102/1000 | Loss: 0.00002481
Iteration 103/1000 | Loss: 0.00002481
Iteration 104/1000 | Loss: 0.00002481
Iteration 105/1000 | Loss: 0.00002481
Iteration 106/1000 | Loss: 0.00002481
Iteration 107/1000 | Loss: 0.00002481
Iteration 108/1000 | Loss: 0.00002481
Iteration 109/1000 | Loss: 0.00002481
Iteration 110/1000 | Loss: 0.00002481
Iteration 111/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.4814376956783235e-05, 2.4814376956783235e-05, 2.4814376956783235e-05, 2.4814376956783235e-05, 2.4814376956783235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4814376956783235e-05

Optimization complete. Final v2v error: 4.1979193687438965 mm

Highest mean error: 4.246077537536621 mm for frame 125

Lowest mean error: 4.155884265899658 mm for frame 146

Saving results

Total time: 35.021705627441406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400534
Iteration 2/25 | Loss: 0.00134224
Iteration 3/25 | Loss: 0.00124292
Iteration 4/25 | Loss: 0.00123229
Iteration 5/25 | Loss: 0.00122997
Iteration 6/25 | Loss: 0.00122943
Iteration 7/25 | Loss: 0.00122943
Iteration 8/25 | Loss: 0.00122943
Iteration 9/25 | Loss: 0.00122943
Iteration 10/25 | Loss: 0.00122943
Iteration 11/25 | Loss: 0.00122943
Iteration 12/25 | Loss: 0.00122943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00122943171299994, 0.00122943171299994, 0.00122943171299994, 0.00122943171299994, 0.00122943171299994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00122943171299994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43092430
Iteration 2/25 | Loss: 0.00084008
Iteration 3/25 | Loss: 0.00084008
Iteration 4/25 | Loss: 0.00084008
Iteration 5/25 | Loss: 0.00084008
Iteration 6/25 | Loss: 0.00084008
Iteration 7/25 | Loss: 0.00084008
Iteration 8/25 | Loss: 0.00084008
Iteration 9/25 | Loss: 0.00084008
Iteration 10/25 | Loss: 0.00084008
Iteration 11/25 | Loss: 0.00084008
Iteration 12/25 | Loss: 0.00084008
Iteration 13/25 | Loss: 0.00084008
Iteration 14/25 | Loss: 0.00084008
Iteration 15/25 | Loss: 0.00084008
Iteration 16/25 | Loss: 0.00084008
Iteration 17/25 | Loss: 0.00084008
Iteration 18/25 | Loss: 0.00084008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008400778169743717, 0.0008400778169743717, 0.0008400778169743717, 0.0008400778169743717, 0.0008400778169743717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008400778169743717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084008
Iteration 2/1000 | Loss: 0.00002536
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001406
Iteration 6/1000 | Loss: 0.00001331
Iteration 7/1000 | Loss: 0.00001283
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001220
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001214
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001209
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001207
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001206
Iteration 48/1000 | Loss: 0.00001206
Iteration 49/1000 | Loss: 0.00001205
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001196
Iteration 76/1000 | Loss: 0.00001196
Iteration 77/1000 | Loss: 0.00001196
Iteration 78/1000 | Loss: 0.00001196
Iteration 79/1000 | Loss: 0.00001196
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001183
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001183
Iteration 121/1000 | Loss: 0.00001183
Iteration 122/1000 | Loss: 0.00001182
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001181
Iteration 125/1000 | Loss: 0.00001181
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001180
Iteration 128/1000 | Loss: 0.00001180
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001178
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001176
Iteration 141/1000 | Loss: 0.00001176
Iteration 142/1000 | Loss: 0.00001176
Iteration 143/1000 | Loss: 0.00001176
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001175
Iteration 147/1000 | Loss: 0.00001174
Iteration 148/1000 | Loss: 0.00001174
Iteration 149/1000 | Loss: 0.00001174
Iteration 150/1000 | Loss: 0.00001174
Iteration 151/1000 | Loss: 0.00001174
Iteration 152/1000 | Loss: 0.00001174
Iteration 153/1000 | Loss: 0.00001173
Iteration 154/1000 | Loss: 0.00001173
Iteration 155/1000 | Loss: 0.00001173
Iteration 156/1000 | Loss: 0.00001173
Iteration 157/1000 | Loss: 0.00001173
Iteration 158/1000 | Loss: 0.00001173
Iteration 159/1000 | Loss: 0.00001173
Iteration 160/1000 | Loss: 0.00001173
Iteration 161/1000 | Loss: 0.00001173
Iteration 162/1000 | Loss: 0.00001173
Iteration 163/1000 | Loss: 0.00001173
Iteration 164/1000 | Loss: 0.00001172
Iteration 165/1000 | Loss: 0.00001172
Iteration 166/1000 | Loss: 0.00001172
Iteration 167/1000 | Loss: 0.00001172
Iteration 168/1000 | Loss: 0.00001172
Iteration 169/1000 | Loss: 0.00001172
Iteration 170/1000 | Loss: 0.00001172
Iteration 171/1000 | Loss: 0.00001172
Iteration 172/1000 | Loss: 0.00001171
Iteration 173/1000 | Loss: 0.00001171
Iteration 174/1000 | Loss: 0.00001171
Iteration 175/1000 | Loss: 0.00001171
Iteration 176/1000 | Loss: 0.00001171
Iteration 177/1000 | Loss: 0.00001171
Iteration 178/1000 | Loss: 0.00001171
Iteration 179/1000 | Loss: 0.00001171
Iteration 180/1000 | Loss: 0.00001171
Iteration 181/1000 | Loss: 0.00001171
Iteration 182/1000 | Loss: 0.00001171
Iteration 183/1000 | Loss: 0.00001171
Iteration 184/1000 | Loss: 0.00001171
Iteration 185/1000 | Loss: 0.00001170
Iteration 186/1000 | Loss: 0.00001170
Iteration 187/1000 | Loss: 0.00001170
Iteration 188/1000 | Loss: 0.00001170
Iteration 189/1000 | Loss: 0.00001170
Iteration 190/1000 | Loss: 0.00001170
Iteration 191/1000 | Loss: 0.00001170
Iteration 192/1000 | Loss: 0.00001170
Iteration 193/1000 | Loss: 0.00001170
Iteration 194/1000 | Loss: 0.00001170
Iteration 195/1000 | Loss: 0.00001170
Iteration 196/1000 | Loss: 0.00001170
Iteration 197/1000 | Loss: 0.00001170
Iteration 198/1000 | Loss: 0.00001170
Iteration 199/1000 | Loss: 0.00001170
Iteration 200/1000 | Loss: 0.00001170
Iteration 201/1000 | Loss: 0.00001170
Iteration 202/1000 | Loss: 0.00001170
Iteration 203/1000 | Loss: 0.00001169
Iteration 204/1000 | Loss: 0.00001169
Iteration 205/1000 | Loss: 0.00001169
Iteration 206/1000 | Loss: 0.00001169
Iteration 207/1000 | Loss: 0.00001169
Iteration 208/1000 | Loss: 0.00001169
Iteration 209/1000 | Loss: 0.00001169
Iteration 210/1000 | Loss: 0.00001169
Iteration 211/1000 | Loss: 0.00001169
Iteration 212/1000 | Loss: 0.00001169
Iteration 213/1000 | Loss: 0.00001169
Iteration 214/1000 | Loss: 0.00001169
Iteration 215/1000 | Loss: 0.00001169
Iteration 216/1000 | Loss: 0.00001169
Iteration 217/1000 | Loss: 0.00001169
Iteration 218/1000 | Loss: 0.00001169
Iteration 219/1000 | Loss: 0.00001169
Iteration 220/1000 | Loss: 0.00001169
Iteration 221/1000 | Loss: 0.00001169
Iteration 222/1000 | Loss: 0.00001169
Iteration 223/1000 | Loss: 0.00001169
Iteration 224/1000 | Loss: 0.00001168
Iteration 225/1000 | Loss: 0.00001168
Iteration 226/1000 | Loss: 0.00001168
Iteration 227/1000 | Loss: 0.00001168
Iteration 228/1000 | Loss: 0.00001168
Iteration 229/1000 | Loss: 0.00001168
Iteration 230/1000 | Loss: 0.00001168
Iteration 231/1000 | Loss: 0.00001168
Iteration 232/1000 | Loss: 0.00001168
Iteration 233/1000 | Loss: 0.00001168
Iteration 234/1000 | Loss: 0.00001168
Iteration 235/1000 | Loss: 0.00001168
Iteration 236/1000 | Loss: 0.00001168
Iteration 237/1000 | Loss: 0.00001168
Iteration 238/1000 | Loss: 0.00001167
Iteration 239/1000 | Loss: 0.00001167
Iteration 240/1000 | Loss: 0.00001167
Iteration 241/1000 | Loss: 0.00001167
Iteration 242/1000 | Loss: 0.00001167
Iteration 243/1000 | Loss: 0.00001167
Iteration 244/1000 | Loss: 0.00001167
Iteration 245/1000 | Loss: 0.00001167
Iteration 246/1000 | Loss: 0.00001167
Iteration 247/1000 | Loss: 0.00001167
Iteration 248/1000 | Loss: 0.00001167
Iteration 249/1000 | Loss: 0.00001167
Iteration 250/1000 | Loss: 0.00001167
Iteration 251/1000 | Loss: 0.00001167
Iteration 252/1000 | Loss: 0.00001166
Iteration 253/1000 | Loss: 0.00001166
Iteration 254/1000 | Loss: 0.00001166
Iteration 255/1000 | Loss: 0.00001166
Iteration 256/1000 | Loss: 0.00001166
Iteration 257/1000 | Loss: 0.00001166
Iteration 258/1000 | Loss: 0.00001166
Iteration 259/1000 | Loss: 0.00001165
Iteration 260/1000 | Loss: 0.00001165
Iteration 261/1000 | Loss: 0.00001165
Iteration 262/1000 | Loss: 0.00001165
Iteration 263/1000 | Loss: 0.00001165
Iteration 264/1000 | Loss: 0.00001165
Iteration 265/1000 | Loss: 0.00001165
Iteration 266/1000 | Loss: 0.00001165
Iteration 267/1000 | Loss: 0.00001165
Iteration 268/1000 | Loss: 0.00001165
Iteration 269/1000 | Loss: 0.00001165
Iteration 270/1000 | Loss: 0.00001165
Iteration 271/1000 | Loss: 0.00001165
Iteration 272/1000 | Loss: 0.00001165
Iteration 273/1000 | Loss: 0.00001165
Iteration 274/1000 | Loss: 0.00001165
Iteration 275/1000 | Loss: 0.00001164
Iteration 276/1000 | Loss: 0.00001164
Iteration 277/1000 | Loss: 0.00001164
Iteration 278/1000 | Loss: 0.00001164
Iteration 279/1000 | Loss: 0.00001164
Iteration 280/1000 | Loss: 0.00001164
Iteration 281/1000 | Loss: 0.00001164
Iteration 282/1000 | Loss: 0.00001164
Iteration 283/1000 | Loss: 0.00001164
Iteration 284/1000 | Loss: 0.00001164
Iteration 285/1000 | Loss: 0.00001164
Iteration 286/1000 | Loss: 0.00001164
Iteration 287/1000 | Loss: 0.00001164
Iteration 288/1000 | Loss: 0.00001164
Iteration 289/1000 | Loss: 0.00001164
Iteration 290/1000 | Loss: 0.00001164
Iteration 291/1000 | Loss: 0.00001164
Iteration 292/1000 | Loss: 0.00001163
Iteration 293/1000 | Loss: 0.00001163
Iteration 294/1000 | Loss: 0.00001163
Iteration 295/1000 | Loss: 0.00001163
Iteration 296/1000 | Loss: 0.00001163
Iteration 297/1000 | Loss: 0.00001163
Iteration 298/1000 | Loss: 0.00001163
Iteration 299/1000 | Loss: 0.00001163
Iteration 300/1000 | Loss: 0.00001163
Iteration 301/1000 | Loss: 0.00001163
Iteration 302/1000 | Loss: 0.00001163
Iteration 303/1000 | Loss: 0.00001163
Iteration 304/1000 | Loss: 0.00001163
Iteration 305/1000 | Loss: 0.00001163
Iteration 306/1000 | Loss: 0.00001163
Iteration 307/1000 | Loss: 0.00001163
Iteration 308/1000 | Loss: 0.00001163
Iteration 309/1000 | Loss: 0.00001163
Iteration 310/1000 | Loss: 0.00001163
Iteration 311/1000 | Loss: 0.00001163
Iteration 312/1000 | Loss: 0.00001163
Iteration 313/1000 | Loss: 0.00001163
Iteration 314/1000 | Loss: 0.00001163
Iteration 315/1000 | Loss: 0.00001163
Iteration 316/1000 | Loss: 0.00001163
Iteration 317/1000 | Loss: 0.00001163
Iteration 318/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [1.1634438124019653e-05, 1.1634438124019653e-05, 1.1634438124019653e-05, 1.1634438124019653e-05, 1.1634438124019653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1634438124019653e-05

Optimization complete. Final v2v error: 2.902682304382324 mm

Highest mean error: 3.853086233139038 mm for frame 70

Lowest mean error: 2.7628328800201416 mm for frame 110

Saving results

Total time: 44.20577263832092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991210
Iteration 2/25 | Loss: 0.00308467
Iteration 3/25 | Loss: 0.00213905
Iteration 4/25 | Loss: 0.00201871
Iteration 5/25 | Loss: 0.00203758
Iteration 6/25 | Loss: 0.00199609
Iteration 7/25 | Loss: 0.00192891
Iteration 8/25 | Loss: 0.00190870
Iteration 9/25 | Loss: 0.00180644
Iteration 10/25 | Loss: 0.00176583
Iteration 11/25 | Loss: 0.00178595
Iteration 12/25 | Loss: 0.00177058
Iteration 13/25 | Loss: 0.00171594
Iteration 14/25 | Loss: 0.00164856
Iteration 15/25 | Loss: 0.00164301
Iteration 16/25 | Loss: 0.00162740
Iteration 17/25 | Loss: 0.00162454
Iteration 18/25 | Loss: 0.00162227
Iteration 19/25 | Loss: 0.00161406
Iteration 20/25 | Loss: 0.00161513
Iteration 21/25 | Loss: 0.00160827
Iteration 22/25 | Loss: 0.00160360
Iteration 23/25 | Loss: 0.00160179
Iteration 24/25 | Loss: 0.00160085
Iteration 25/25 | Loss: 0.00160192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41376519
Iteration 2/25 | Loss: 0.00678801
Iteration 3/25 | Loss: 0.00212750
Iteration 4/25 | Loss: 0.00212748
Iteration 5/25 | Loss: 0.00212747
Iteration 6/25 | Loss: 0.00212747
Iteration 7/25 | Loss: 0.00212747
Iteration 8/25 | Loss: 0.00212747
Iteration 9/25 | Loss: 0.00212747
Iteration 10/25 | Loss: 0.00212747
Iteration 11/25 | Loss: 0.00212747
Iteration 12/25 | Loss: 0.00212747
Iteration 13/25 | Loss: 0.00212747
Iteration 14/25 | Loss: 0.00212747
Iteration 15/25 | Loss: 0.00212747
Iteration 16/25 | Loss: 0.00212747
Iteration 17/25 | Loss: 0.00212747
Iteration 18/25 | Loss: 0.00212747
Iteration 19/25 | Loss: 0.00212747
Iteration 20/25 | Loss: 0.00212747
Iteration 21/25 | Loss: 0.00212747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002127470215782523, 0.002127470215782523, 0.002127470215782523, 0.002127470215782523, 0.002127470215782523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002127470215782523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212747
Iteration 2/1000 | Loss: 0.00455629
Iteration 3/1000 | Loss: 0.00319214
Iteration 4/1000 | Loss: 0.00108825
Iteration 5/1000 | Loss: 0.00091329
Iteration 6/1000 | Loss: 0.00184969
Iteration 7/1000 | Loss: 0.00230902
Iteration 8/1000 | Loss: 0.00249176
Iteration 9/1000 | Loss: 0.00421917
Iteration 10/1000 | Loss: 0.00161143
Iteration 11/1000 | Loss: 0.00115419
Iteration 12/1000 | Loss: 0.00337624
Iteration 13/1000 | Loss: 0.00293102
Iteration 14/1000 | Loss: 0.00208548
Iteration 15/1000 | Loss: 0.00116326
Iteration 16/1000 | Loss: 0.00126521
Iteration 17/1000 | Loss: 0.00103901
Iteration 18/1000 | Loss: 0.00100209
Iteration 19/1000 | Loss: 0.00303144
Iteration 20/1000 | Loss: 0.00124778
Iteration 21/1000 | Loss: 0.00185990
Iteration 22/1000 | Loss: 0.00096731
Iteration 23/1000 | Loss: 0.00236351
Iteration 24/1000 | Loss: 0.00056398
Iteration 25/1000 | Loss: 0.00082057
Iteration 26/1000 | Loss: 0.00034686
Iteration 27/1000 | Loss: 0.00048426
Iteration 28/1000 | Loss: 0.00036431
Iteration 29/1000 | Loss: 0.00020555
Iteration 30/1000 | Loss: 0.00022939
Iteration 31/1000 | Loss: 0.00137152
Iteration 32/1000 | Loss: 0.00468352
Iteration 33/1000 | Loss: 0.00220185
Iteration 34/1000 | Loss: 0.00424723
Iteration 35/1000 | Loss: 0.00434288
Iteration 36/1000 | Loss: 0.00244566
Iteration 37/1000 | Loss: 0.00327425
Iteration 38/1000 | Loss: 0.00381876
Iteration 39/1000 | Loss: 0.00304916
Iteration 40/1000 | Loss: 0.00148978
Iteration 41/1000 | Loss: 0.00079927
Iteration 42/1000 | Loss: 0.00074128
Iteration 43/1000 | Loss: 0.00077791
Iteration 44/1000 | Loss: 0.00060574
Iteration 45/1000 | Loss: 0.00015173
Iteration 46/1000 | Loss: 0.00058200
Iteration 47/1000 | Loss: 0.00007354
Iteration 48/1000 | Loss: 0.00013404
Iteration 49/1000 | Loss: 0.00210313
Iteration 50/1000 | Loss: 0.00050726
Iteration 51/1000 | Loss: 0.00173968
Iteration 52/1000 | Loss: 0.00176119
Iteration 53/1000 | Loss: 0.00013777
Iteration 54/1000 | Loss: 0.00012472
Iteration 55/1000 | Loss: 0.00005801
Iteration 56/1000 | Loss: 0.00005283
Iteration 57/1000 | Loss: 0.00012969
Iteration 58/1000 | Loss: 0.00004645
Iteration 59/1000 | Loss: 0.00004337
Iteration 60/1000 | Loss: 0.00045696
Iteration 61/1000 | Loss: 0.00050408
Iteration 62/1000 | Loss: 0.00013402
Iteration 63/1000 | Loss: 0.00004209
Iteration 64/1000 | Loss: 0.00003885
Iteration 65/1000 | Loss: 0.00003742
Iteration 66/1000 | Loss: 0.00037766
Iteration 67/1000 | Loss: 0.00047237
Iteration 68/1000 | Loss: 0.00007477
Iteration 69/1000 | Loss: 0.00006747
Iteration 70/1000 | Loss: 0.00003803
Iteration 71/1000 | Loss: 0.00003490
Iteration 72/1000 | Loss: 0.00003412
Iteration 73/1000 | Loss: 0.00010773
Iteration 74/1000 | Loss: 0.00005959
Iteration 75/1000 | Loss: 0.00005208
Iteration 76/1000 | Loss: 0.00003434
Iteration 77/1000 | Loss: 0.00003331
Iteration 78/1000 | Loss: 0.00003184
Iteration 79/1000 | Loss: 0.00003121
Iteration 80/1000 | Loss: 0.00004077
Iteration 81/1000 | Loss: 0.00003137
Iteration 82/1000 | Loss: 0.00003082
Iteration 83/1000 | Loss: 0.00020784
Iteration 84/1000 | Loss: 0.00003048
Iteration 85/1000 | Loss: 0.00003037
Iteration 86/1000 | Loss: 0.00003034
Iteration 87/1000 | Loss: 0.00012767
Iteration 88/1000 | Loss: 0.00072455
Iteration 89/1000 | Loss: 0.00040568
Iteration 90/1000 | Loss: 0.00017698
Iteration 91/1000 | Loss: 0.00018779
Iteration 92/1000 | Loss: 0.00009042
Iteration 93/1000 | Loss: 0.00006245
Iteration 94/1000 | Loss: 0.00013408
Iteration 95/1000 | Loss: 0.00003207
Iteration 96/1000 | Loss: 0.00003111
Iteration 97/1000 | Loss: 0.00033271
Iteration 98/1000 | Loss: 0.00035994
Iteration 99/1000 | Loss: 0.00013882
Iteration 100/1000 | Loss: 0.00003014
Iteration 101/1000 | Loss: 0.00002915
Iteration 102/1000 | Loss: 0.00002880
Iteration 103/1000 | Loss: 0.00016302
Iteration 104/1000 | Loss: 0.00089066
Iteration 105/1000 | Loss: 0.00059753
Iteration 106/1000 | Loss: 0.00058461
Iteration 107/1000 | Loss: 0.00003772
Iteration 108/1000 | Loss: 0.00017705
Iteration 109/1000 | Loss: 0.00021558
Iteration 110/1000 | Loss: 0.00002926
Iteration 111/1000 | Loss: 0.00002868
Iteration 112/1000 | Loss: 0.00009132
Iteration 113/1000 | Loss: 0.00002873
Iteration 114/1000 | Loss: 0.00002844
Iteration 115/1000 | Loss: 0.00002840
Iteration 116/1000 | Loss: 0.00002840
Iteration 117/1000 | Loss: 0.00002839
Iteration 118/1000 | Loss: 0.00002838
Iteration 119/1000 | Loss: 0.00002837
Iteration 120/1000 | Loss: 0.00002836
Iteration 121/1000 | Loss: 0.00002836
Iteration 122/1000 | Loss: 0.00002836
Iteration 123/1000 | Loss: 0.00002836
Iteration 124/1000 | Loss: 0.00002836
Iteration 125/1000 | Loss: 0.00002836
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002836
Iteration 128/1000 | Loss: 0.00002836
Iteration 129/1000 | Loss: 0.00002836
Iteration 130/1000 | Loss: 0.00002836
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002835
Iteration 134/1000 | Loss: 0.00002835
Iteration 135/1000 | Loss: 0.00002834
Iteration 136/1000 | Loss: 0.00002830
Iteration 137/1000 | Loss: 0.00002830
Iteration 138/1000 | Loss: 0.00002830
Iteration 139/1000 | Loss: 0.00002829
Iteration 140/1000 | Loss: 0.00002829
Iteration 141/1000 | Loss: 0.00002829
Iteration 142/1000 | Loss: 0.00002828
Iteration 143/1000 | Loss: 0.00002828
Iteration 144/1000 | Loss: 0.00002828
Iteration 145/1000 | Loss: 0.00002828
Iteration 146/1000 | Loss: 0.00002828
Iteration 147/1000 | Loss: 0.00002827
Iteration 148/1000 | Loss: 0.00002827
Iteration 149/1000 | Loss: 0.00002826
Iteration 150/1000 | Loss: 0.00002826
Iteration 151/1000 | Loss: 0.00002826
Iteration 152/1000 | Loss: 0.00002825
Iteration 153/1000 | Loss: 0.00002825
Iteration 154/1000 | Loss: 0.00002824
Iteration 155/1000 | Loss: 0.00002824
Iteration 156/1000 | Loss: 0.00002823
Iteration 157/1000 | Loss: 0.00002823
Iteration 158/1000 | Loss: 0.00002823
Iteration 159/1000 | Loss: 0.00002822
Iteration 160/1000 | Loss: 0.00002822
Iteration 161/1000 | Loss: 0.00019799
Iteration 162/1000 | Loss: 0.00003689
Iteration 163/1000 | Loss: 0.00002843
Iteration 164/1000 | Loss: 0.00004347
Iteration 165/1000 | Loss: 0.00002885
Iteration 166/1000 | Loss: 0.00002837
Iteration 167/1000 | Loss: 0.00002837
Iteration 168/1000 | Loss: 0.00002829
Iteration 169/1000 | Loss: 0.00002829
Iteration 170/1000 | Loss: 0.00002828
Iteration 171/1000 | Loss: 0.00002828
Iteration 172/1000 | Loss: 0.00002827
Iteration 173/1000 | Loss: 0.00002827
Iteration 174/1000 | Loss: 0.00002824
Iteration 175/1000 | Loss: 0.00002824
Iteration 176/1000 | Loss: 0.00002824
Iteration 177/1000 | Loss: 0.00002824
Iteration 178/1000 | Loss: 0.00002824
Iteration 179/1000 | Loss: 0.00002824
Iteration 180/1000 | Loss: 0.00002824
Iteration 181/1000 | Loss: 0.00002823
Iteration 182/1000 | Loss: 0.00002823
Iteration 183/1000 | Loss: 0.00002823
Iteration 184/1000 | Loss: 0.00002822
Iteration 185/1000 | Loss: 0.00002820
Iteration 186/1000 | Loss: 0.00002819
Iteration 187/1000 | Loss: 0.00002818
Iteration 188/1000 | Loss: 0.00002817
Iteration 189/1000 | Loss: 0.00002817
Iteration 190/1000 | Loss: 0.00002816
Iteration 191/1000 | Loss: 0.00002816
Iteration 192/1000 | Loss: 0.00002816
Iteration 193/1000 | Loss: 0.00002816
Iteration 194/1000 | Loss: 0.00002815
Iteration 195/1000 | Loss: 0.00002814
Iteration 196/1000 | Loss: 0.00002813
Iteration 197/1000 | Loss: 0.00002813
Iteration 198/1000 | Loss: 0.00002812
Iteration 199/1000 | Loss: 0.00002812
Iteration 200/1000 | Loss: 0.00002812
Iteration 201/1000 | Loss: 0.00002812
Iteration 202/1000 | Loss: 0.00002812
Iteration 203/1000 | Loss: 0.00002812
Iteration 204/1000 | Loss: 0.00002812
Iteration 205/1000 | Loss: 0.00002812
Iteration 206/1000 | Loss: 0.00002812
Iteration 207/1000 | Loss: 0.00002812
Iteration 208/1000 | Loss: 0.00002812
Iteration 209/1000 | Loss: 0.00002812
Iteration 210/1000 | Loss: 0.00002811
Iteration 211/1000 | Loss: 0.00002811
Iteration 212/1000 | Loss: 0.00002811
Iteration 213/1000 | Loss: 0.00002811
Iteration 214/1000 | Loss: 0.00002811
Iteration 215/1000 | Loss: 0.00002811
Iteration 216/1000 | Loss: 0.00002811
Iteration 217/1000 | Loss: 0.00002811
Iteration 218/1000 | Loss: 0.00002811
Iteration 219/1000 | Loss: 0.00002811
Iteration 220/1000 | Loss: 0.00002811
Iteration 221/1000 | Loss: 0.00002810
Iteration 222/1000 | Loss: 0.00002810
Iteration 223/1000 | Loss: 0.00002810
Iteration 224/1000 | Loss: 0.00002810
Iteration 225/1000 | Loss: 0.00002810
Iteration 226/1000 | Loss: 0.00002810
Iteration 227/1000 | Loss: 0.00002810
Iteration 228/1000 | Loss: 0.00002810
Iteration 229/1000 | Loss: 0.00002810
Iteration 230/1000 | Loss: 0.00002809
Iteration 231/1000 | Loss: 0.00002809
Iteration 232/1000 | Loss: 0.00002809
Iteration 233/1000 | Loss: 0.00002809
Iteration 234/1000 | Loss: 0.00002809
Iteration 235/1000 | Loss: 0.00002809
Iteration 236/1000 | Loss: 0.00002809
Iteration 237/1000 | Loss: 0.00002809
Iteration 238/1000 | Loss: 0.00002809
Iteration 239/1000 | Loss: 0.00002808
Iteration 240/1000 | Loss: 0.00006199
Iteration 241/1000 | Loss: 0.00038804
Iteration 242/1000 | Loss: 0.00033744
Iteration 243/1000 | Loss: 0.00003316
Iteration 244/1000 | Loss: 0.00015355
Iteration 245/1000 | Loss: 0.00008613
Iteration 246/1000 | Loss: 0.00005395
Iteration 247/1000 | Loss: 0.00002803
Iteration 248/1000 | Loss: 0.00002750
Iteration 249/1000 | Loss: 0.00002704
Iteration 250/1000 | Loss: 0.00002689
Iteration 251/1000 | Loss: 0.00002683
Iteration 252/1000 | Loss: 0.00002667
Iteration 253/1000 | Loss: 0.00002666
Iteration 254/1000 | Loss: 0.00002665
Iteration 255/1000 | Loss: 0.00002665
Iteration 256/1000 | Loss: 0.00002661
Iteration 257/1000 | Loss: 0.00002661
Iteration 258/1000 | Loss: 0.00002660
Iteration 259/1000 | Loss: 0.00002656
Iteration 260/1000 | Loss: 0.00002654
Iteration 261/1000 | Loss: 0.00002654
Iteration 262/1000 | Loss: 0.00002653
Iteration 263/1000 | Loss: 0.00002650
Iteration 264/1000 | Loss: 0.00002650
Iteration 265/1000 | Loss: 0.00002650
Iteration 266/1000 | Loss: 0.00002650
Iteration 267/1000 | Loss: 0.00002650
Iteration 268/1000 | Loss: 0.00002650
Iteration 269/1000 | Loss: 0.00002650
Iteration 270/1000 | Loss: 0.00002650
Iteration 271/1000 | Loss: 0.00002649
Iteration 272/1000 | Loss: 0.00002649
Iteration 273/1000 | Loss: 0.00002649
Iteration 274/1000 | Loss: 0.00002649
Iteration 275/1000 | Loss: 0.00002649
Iteration 276/1000 | Loss: 0.00002649
Iteration 277/1000 | Loss: 0.00002648
Iteration 278/1000 | Loss: 0.00002648
Iteration 279/1000 | Loss: 0.00002648
Iteration 280/1000 | Loss: 0.00002648
Iteration 281/1000 | Loss: 0.00002647
Iteration 282/1000 | Loss: 0.00002647
Iteration 283/1000 | Loss: 0.00002647
Iteration 284/1000 | Loss: 0.00002647
Iteration 285/1000 | Loss: 0.00002647
Iteration 286/1000 | Loss: 0.00002647
Iteration 287/1000 | Loss: 0.00002647
Iteration 288/1000 | Loss: 0.00002647
Iteration 289/1000 | Loss: 0.00002647
Iteration 290/1000 | Loss: 0.00002647
Iteration 291/1000 | Loss: 0.00002647
Iteration 292/1000 | Loss: 0.00002646
Iteration 293/1000 | Loss: 0.00002646
Iteration 294/1000 | Loss: 0.00002646
Iteration 295/1000 | Loss: 0.00002646
Iteration 296/1000 | Loss: 0.00002646
Iteration 297/1000 | Loss: 0.00002645
Iteration 298/1000 | Loss: 0.00002645
Iteration 299/1000 | Loss: 0.00002645
Iteration 300/1000 | Loss: 0.00002645
Iteration 301/1000 | Loss: 0.00002645
Iteration 302/1000 | Loss: 0.00002645
Iteration 303/1000 | Loss: 0.00002645
Iteration 304/1000 | Loss: 0.00002644
Iteration 305/1000 | Loss: 0.00002644
Iteration 306/1000 | Loss: 0.00002644
Iteration 307/1000 | Loss: 0.00002644
Iteration 308/1000 | Loss: 0.00002644
Iteration 309/1000 | Loss: 0.00002644
Iteration 310/1000 | Loss: 0.00002644
Iteration 311/1000 | Loss: 0.00002644
Iteration 312/1000 | Loss: 0.00002644
Iteration 313/1000 | Loss: 0.00002644
Iteration 314/1000 | Loss: 0.00002644
Iteration 315/1000 | Loss: 0.00002644
Iteration 316/1000 | Loss: 0.00002644
Iteration 317/1000 | Loss: 0.00002643
Iteration 318/1000 | Loss: 0.00002643
Iteration 319/1000 | Loss: 0.00002643
Iteration 320/1000 | Loss: 0.00002643
Iteration 321/1000 | Loss: 0.00002643
Iteration 322/1000 | Loss: 0.00002643
Iteration 323/1000 | Loss: 0.00002643
Iteration 324/1000 | Loss: 0.00002643
Iteration 325/1000 | Loss: 0.00002643
Iteration 326/1000 | Loss: 0.00002643
Iteration 327/1000 | Loss: 0.00002643
Iteration 328/1000 | Loss: 0.00002643
Iteration 329/1000 | Loss: 0.00002643
Iteration 330/1000 | Loss: 0.00002643
Iteration 331/1000 | Loss: 0.00002643
Iteration 332/1000 | Loss: 0.00002643
Iteration 333/1000 | Loss: 0.00002642
Iteration 334/1000 | Loss: 0.00002642
Iteration 335/1000 | Loss: 0.00002642
Iteration 336/1000 | Loss: 0.00002642
Iteration 337/1000 | Loss: 0.00002642
Iteration 338/1000 | Loss: 0.00002642
Iteration 339/1000 | Loss: 0.00002642
Iteration 340/1000 | Loss: 0.00002642
Iteration 341/1000 | Loss: 0.00002642
Iteration 342/1000 | Loss: 0.00002642
Iteration 343/1000 | Loss: 0.00002642
Iteration 344/1000 | Loss: 0.00002642
Iteration 345/1000 | Loss: 0.00002642
Iteration 346/1000 | Loss: 0.00002642
Iteration 347/1000 | Loss: 0.00002641
Iteration 348/1000 | Loss: 0.00002641
Iteration 349/1000 | Loss: 0.00002641
Iteration 350/1000 | Loss: 0.00002641
Iteration 351/1000 | Loss: 0.00002641
Iteration 352/1000 | Loss: 0.00002641
Iteration 353/1000 | Loss: 0.00002641
Iteration 354/1000 | Loss: 0.00002641
Iteration 355/1000 | Loss: 0.00002641
Iteration 356/1000 | Loss: 0.00002641
Iteration 357/1000 | Loss: 0.00002641
Iteration 358/1000 | Loss: 0.00002641
Iteration 359/1000 | Loss: 0.00002641
Iteration 360/1000 | Loss: 0.00002641
Iteration 361/1000 | Loss: 0.00002640
Iteration 362/1000 | Loss: 0.00018583
Iteration 363/1000 | Loss: 0.00015634
Iteration 364/1000 | Loss: 0.00004455
Iteration 365/1000 | Loss: 0.00002655
Iteration 366/1000 | Loss: 0.00002647
Iteration 367/1000 | Loss: 0.00002646
Iteration 368/1000 | Loss: 0.00002643
Iteration 369/1000 | Loss: 0.00002642
Iteration 370/1000 | Loss: 0.00002642
Iteration 371/1000 | Loss: 0.00002641
Iteration 372/1000 | Loss: 0.00002641
Iteration 373/1000 | Loss: 0.00002640
Iteration 374/1000 | Loss: 0.00002640
Iteration 375/1000 | Loss: 0.00002640
Iteration 376/1000 | Loss: 0.00002640
Iteration 377/1000 | Loss: 0.00002640
Iteration 378/1000 | Loss: 0.00002640
Iteration 379/1000 | Loss: 0.00002640
Iteration 380/1000 | Loss: 0.00002640
Iteration 381/1000 | Loss: 0.00002640
Iteration 382/1000 | Loss: 0.00002639
Iteration 383/1000 | Loss: 0.00002639
Iteration 384/1000 | Loss: 0.00002639
Iteration 385/1000 | Loss: 0.00002639
Iteration 386/1000 | Loss: 0.00002639
Iteration 387/1000 | Loss: 0.00002638
Iteration 388/1000 | Loss: 0.00002638
Iteration 389/1000 | Loss: 0.00002638
Iteration 390/1000 | Loss: 0.00002638
Iteration 391/1000 | Loss: 0.00002637
Iteration 392/1000 | Loss: 0.00002637
Iteration 393/1000 | Loss: 0.00002637
Iteration 394/1000 | Loss: 0.00002637
Iteration 395/1000 | Loss: 0.00002636
Iteration 396/1000 | Loss: 0.00002636
Iteration 397/1000 | Loss: 0.00002636
Iteration 398/1000 | Loss: 0.00002636
Iteration 399/1000 | Loss: 0.00002636
Iteration 400/1000 | Loss: 0.00002636
Iteration 401/1000 | Loss: 0.00002635
Iteration 402/1000 | Loss: 0.00002635
Iteration 403/1000 | Loss: 0.00002635
Iteration 404/1000 | Loss: 0.00002635
Iteration 405/1000 | Loss: 0.00002634
Iteration 406/1000 | Loss: 0.00002634
Iteration 407/1000 | Loss: 0.00002634
Iteration 408/1000 | Loss: 0.00002634
Iteration 409/1000 | Loss: 0.00002634
Iteration 410/1000 | Loss: 0.00002634
Iteration 411/1000 | Loss: 0.00002634
Iteration 412/1000 | Loss: 0.00002633
Iteration 413/1000 | Loss: 0.00002633
Iteration 414/1000 | Loss: 0.00002633
Iteration 415/1000 | Loss: 0.00002633
Iteration 416/1000 | Loss: 0.00002633
Iteration 417/1000 | Loss: 0.00002633
Iteration 418/1000 | Loss: 0.00002633
Iteration 419/1000 | Loss: 0.00002633
Iteration 420/1000 | Loss: 0.00002633
Iteration 421/1000 | Loss: 0.00002633
Iteration 422/1000 | Loss: 0.00002633
Iteration 423/1000 | Loss: 0.00002633
Iteration 424/1000 | Loss: 0.00002633
Iteration 425/1000 | Loss: 0.00002633
Iteration 426/1000 | Loss: 0.00002633
Iteration 427/1000 | Loss: 0.00002633
Iteration 428/1000 | Loss: 0.00002633
Iteration 429/1000 | Loss: 0.00002633
Iteration 430/1000 | Loss: 0.00002633
Iteration 431/1000 | Loss: 0.00002633
Iteration 432/1000 | Loss: 0.00002633
Iteration 433/1000 | Loss: 0.00002633
Iteration 434/1000 | Loss: 0.00002632
Iteration 435/1000 | Loss: 0.00002632
Iteration 436/1000 | Loss: 0.00002632
Iteration 437/1000 | Loss: 0.00002632
Iteration 438/1000 | Loss: 0.00002632
Iteration 439/1000 | Loss: 0.00002632
Iteration 440/1000 | Loss: 0.00002632
Iteration 441/1000 | Loss: 0.00002632
Iteration 442/1000 | Loss: 0.00002632
Iteration 443/1000 | Loss: 0.00002632
Iteration 444/1000 | Loss: 0.00002632
Iteration 445/1000 | Loss: 0.00002632
Iteration 446/1000 | Loss: 0.00002632
Iteration 447/1000 | Loss: 0.00002632
Iteration 448/1000 | Loss: 0.00002632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 448. Stopping optimization.
Last 5 losses: [2.6324594728066586e-05, 2.6324594728066586e-05, 2.6324594728066586e-05, 2.6324594728066586e-05, 2.6324594728066586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6324594728066586e-05

Optimization complete. Final v2v error: 3.5283362865448 mm

Highest mean error: 10.247737884521484 mm for frame 35

Lowest mean error: 2.9243054389953613 mm for frame 174

Saving results

Total time: 287.0854263305664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974394
Iteration 2/25 | Loss: 0.00293198
Iteration 3/25 | Loss: 0.00202021
Iteration 4/25 | Loss: 0.00178127
Iteration 5/25 | Loss: 0.00183166
Iteration 6/25 | Loss: 0.00183015
Iteration 7/25 | Loss: 0.00170566
Iteration 8/25 | Loss: 0.00159628
Iteration 9/25 | Loss: 0.00158279
Iteration 10/25 | Loss: 0.00152989
Iteration 11/25 | Loss: 0.00150578
Iteration 12/25 | Loss: 0.00144210
Iteration 13/25 | Loss: 0.00142186
Iteration 14/25 | Loss: 0.00139594
Iteration 15/25 | Loss: 0.00139173
Iteration 16/25 | Loss: 0.00137829
Iteration 17/25 | Loss: 0.00136672
Iteration 18/25 | Loss: 0.00135972
Iteration 19/25 | Loss: 0.00136152
Iteration 20/25 | Loss: 0.00134751
Iteration 21/25 | Loss: 0.00134067
Iteration 22/25 | Loss: 0.00133763
Iteration 23/25 | Loss: 0.00133381
Iteration 24/25 | Loss: 0.00132501
Iteration 25/25 | Loss: 0.00132281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40746474
Iteration 2/25 | Loss: 0.00095907
Iteration 3/25 | Loss: 0.00093298
Iteration 4/25 | Loss: 0.00093298
Iteration 5/25 | Loss: 0.00093297
Iteration 6/25 | Loss: 0.00093297
Iteration 7/25 | Loss: 0.00093297
Iteration 8/25 | Loss: 0.00093297
Iteration 9/25 | Loss: 0.00093297
Iteration 10/25 | Loss: 0.00093297
Iteration 11/25 | Loss: 0.00093297
Iteration 12/25 | Loss: 0.00093297
Iteration 13/25 | Loss: 0.00093297
Iteration 14/25 | Loss: 0.00093297
Iteration 15/25 | Loss: 0.00093297
Iteration 16/25 | Loss: 0.00093297
Iteration 17/25 | Loss: 0.00093297
Iteration 18/25 | Loss: 0.00093297
Iteration 19/25 | Loss: 0.00093297
Iteration 20/25 | Loss: 0.00093297
Iteration 21/25 | Loss: 0.00093297
Iteration 22/25 | Loss: 0.00093297
Iteration 23/25 | Loss: 0.00093297
Iteration 24/25 | Loss: 0.00093297
Iteration 25/25 | Loss: 0.00093297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093297
Iteration 2/1000 | Loss: 0.00108862
Iteration 3/1000 | Loss: 0.00023875
Iteration 4/1000 | Loss: 0.00009395
Iteration 5/1000 | Loss: 0.00019989
Iteration 6/1000 | Loss: 0.00052725
Iteration 7/1000 | Loss: 0.00024338
Iteration 8/1000 | Loss: 0.00031116
Iteration 9/1000 | Loss: 0.00008336
Iteration 10/1000 | Loss: 0.00005951
Iteration 11/1000 | Loss: 0.00016034
Iteration 12/1000 | Loss: 0.00012129
Iteration 13/1000 | Loss: 0.00014866
Iteration 14/1000 | Loss: 0.00021556
Iteration 15/1000 | Loss: 0.00026171
Iteration 16/1000 | Loss: 0.00014538
Iteration 17/1000 | Loss: 0.00023633
Iteration 18/1000 | Loss: 0.00018608
Iteration 19/1000 | Loss: 0.00022392
Iteration 20/1000 | Loss: 0.00021404
Iteration 21/1000 | Loss: 0.00019542
Iteration 22/1000 | Loss: 0.00032849
Iteration 23/1000 | Loss: 0.00021335
Iteration 24/1000 | Loss: 0.00012640
Iteration 25/1000 | Loss: 0.00012552
Iteration 26/1000 | Loss: 0.00027691
Iteration 27/1000 | Loss: 0.00007839
Iteration 28/1000 | Loss: 0.00013578
Iteration 29/1000 | Loss: 0.00023722
Iteration 30/1000 | Loss: 0.00006881
Iteration 31/1000 | Loss: 0.00006709
Iteration 32/1000 | Loss: 0.00006021
Iteration 33/1000 | Loss: 0.00022480
Iteration 34/1000 | Loss: 0.00020556
Iteration 35/1000 | Loss: 0.00005779
Iteration 36/1000 | Loss: 0.00031415
Iteration 37/1000 | Loss: 0.00050203
Iteration 38/1000 | Loss: 0.00027699
Iteration 39/1000 | Loss: 0.00018915
Iteration 40/1000 | Loss: 0.00042415
Iteration 41/1000 | Loss: 0.00036287
Iteration 42/1000 | Loss: 0.00025891
Iteration 43/1000 | Loss: 0.00027060
Iteration 44/1000 | Loss: 0.00022657
Iteration 45/1000 | Loss: 0.00064270
Iteration 46/1000 | Loss: 0.00022912
Iteration 47/1000 | Loss: 0.00012205
Iteration 48/1000 | Loss: 0.00013070
Iteration 49/1000 | Loss: 0.00055778
Iteration 50/1000 | Loss: 0.00021946
Iteration 51/1000 | Loss: 0.00044866
Iteration 52/1000 | Loss: 0.00027166
Iteration 53/1000 | Loss: 0.00029404
Iteration 54/1000 | Loss: 0.00059374
Iteration 55/1000 | Loss: 0.00039654
Iteration 56/1000 | Loss: 0.00020248
Iteration 57/1000 | Loss: 0.00050481
Iteration 58/1000 | Loss: 0.00032354
Iteration 59/1000 | Loss: 0.00010413
Iteration 60/1000 | Loss: 0.00018966
Iteration 61/1000 | Loss: 0.00039747
Iteration 62/1000 | Loss: 0.00038989
Iteration 63/1000 | Loss: 0.00012513
Iteration 64/1000 | Loss: 0.00006003
Iteration 65/1000 | Loss: 0.00006539
Iteration 66/1000 | Loss: 0.00006877
Iteration 67/1000 | Loss: 0.00012735
Iteration 68/1000 | Loss: 0.00006354
Iteration 69/1000 | Loss: 0.00023247
Iteration 70/1000 | Loss: 0.00012363
Iteration 71/1000 | Loss: 0.00008931
Iteration 72/1000 | Loss: 0.00009685
Iteration 73/1000 | Loss: 0.00008526
Iteration 74/1000 | Loss: 0.00006057
Iteration 75/1000 | Loss: 0.00011571
Iteration 76/1000 | Loss: 0.00010129
Iteration 77/1000 | Loss: 0.00010283
Iteration 78/1000 | Loss: 0.00017701
Iteration 79/1000 | Loss: 0.00024622
Iteration 80/1000 | Loss: 0.00031890
Iteration 81/1000 | Loss: 0.00026899
Iteration 82/1000 | Loss: 0.00006213
Iteration 83/1000 | Loss: 0.00006048
Iteration 84/1000 | Loss: 0.00014970
Iteration 85/1000 | Loss: 0.00098563
Iteration 86/1000 | Loss: 0.00066694
Iteration 87/1000 | Loss: 0.00056241
Iteration 88/1000 | Loss: 0.00033558
Iteration 89/1000 | Loss: 0.00040841
Iteration 90/1000 | Loss: 0.00005408
Iteration 91/1000 | Loss: 0.00015242
Iteration 92/1000 | Loss: 0.00004455
Iteration 93/1000 | Loss: 0.00011586
Iteration 94/1000 | Loss: 0.00013597
Iteration 95/1000 | Loss: 0.00003803
Iteration 96/1000 | Loss: 0.00002876
Iteration 97/1000 | Loss: 0.00011419
Iteration 98/1000 | Loss: 0.00002904
Iteration 99/1000 | Loss: 0.00017176
Iteration 100/1000 | Loss: 0.00002374
Iteration 101/1000 | Loss: 0.00002269
Iteration 102/1000 | Loss: 0.00015002
Iteration 103/1000 | Loss: 0.00007337
Iteration 104/1000 | Loss: 0.00022900
Iteration 105/1000 | Loss: 0.00011092
Iteration 106/1000 | Loss: 0.00009424
Iteration 107/1000 | Loss: 0.00003933
Iteration 108/1000 | Loss: 0.00002259
Iteration 109/1000 | Loss: 0.00002098
Iteration 110/1000 | Loss: 0.00014353
Iteration 111/1000 | Loss: 0.00015303
Iteration 112/1000 | Loss: 0.00010839
Iteration 113/1000 | Loss: 0.00007287
Iteration 114/1000 | Loss: 0.00003109
Iteration 115/1000 | Loss: 0.00002535
Iteration 116/1000 | Loss: 0.00002428
Iteration 117/1000 | Loss: 0.00041158
Iteration 118/1000 | Loss: 0.00012058
Iteration 119/1000 | Loss: 0.00003173
Iteration 120/1000 | Loss: 0.00052859
Iteration 121/1000 | Loss: 0.00026574
Iteration 122/1000 | Loss: 0.00013366
Iteration 123/1000 | Loss: 0.00016764
Iteration 124/1000 | Loss: 0.00006827
Iteration 125/1000 | Loss: 0.00014908
Iteration 126/1000 | Loss: 0.00047764
Iteration 127/1000 | Loss: 0.00011511
Iteration 128/1000 | Loss: 0.00004129
Iteration 129/1000 | Loss: 0.00002663
Iteration 130/1000 | Loss: 0.00020013
Iteration 131/1000 | Loss: 0.00013378
Iteration 132/1000 | Loss: 0.00019285
Iteration 133/1000 | Loss: 0.00026034
Iteration 134/1000 | Loss: 0.00002836
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002004
Iteration 137/1000 | Loss: 0.00001957
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001846
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001787
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001736
Iteration 145/1000 | Loss: 0.00001712
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001680
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001679
Iteration 151/1000 | Loss: 0.00001678
Iteration 152/1000 | Loss: 0.00001678
Iteration 153/1000 | Loss: 0.00001677
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001675
Iteration 156/1000 | Loss: 0.00001675
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001673
Iteration 159/1000 | Loss: 0.00001672
Iteration 160/1000 | Loss: 0.00001671
Iteration 161/1000 | Loss: 0.00001670
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001669
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001669
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001669
Iteration 172/1000 | Loss: 0.00001668
Iteration 173/1000 | Loss: 0.00001668
Iteration 174/1000 | Loss: 0.00001668
Iteration 175/1000 | Loss: 0.00001667
Iteration 176/1000 | Loss: 0.00001667
Iteration 177/1000 | Loss: 0.00001667
Iteration 178/1000 | Loss: 0.00001667
Iteration 179/1000 | Loss: 0.00001667
Iteration 180/1000 | Loss: 0.00001667
Iteration 181/1000 | Loss: 0.00001666
Iteration 182/1000 | Loss: 0.00001666
Iteration 183/1000 | Loss: 0.00001666
Iteration 184/1000 | Loss: 0.00001666
Iteration 185/1000 | Loss: 0.00001665
Iteration 186/1000 | Loss: 0.00001665
Iteration 187/1000 | Loss: 0.00001665
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001664
Iteration 190/1000 | Loss: 0.00001664
Iteration 191/1000 | Loss: 0.00001664
Iteration 192/1000 | Loss: 0.00001664
Iteration 193/1000 | Loss: 0.00001663
Iteration 194/1000 | Loss: 0.00001663
Iteration 195/1000 | Loss: 0.00001663
Iteration 196/1000 | Loss: 0.00001663
Iteration 197/1000 | Loss: 0.00001663
Iteration 198/1000 | Loss: 0.00001663
Iteration 199/1000 | Loss: 0.00001663
Iteration 200/1000 | Loss: 0.00001663
Iteration 201/1000 | Loss: 0.00001663
Iteration 202/1000 | Loss: 0.00001663
Iteration 203/1000 | Loss: 0.00001663
Iteration 204/1000 | Loss: 0.00001663
Iteration 205/1000 | Loss: 0.00001663
Iteration 206/1000 | Loss: 0.00001663
Iteration 207/1000 | Loss: 0.00001663
Iteration 208/1000 | Loss: 0.00001662
Iteration 209/1000 | Loss: 0.00001662
Iteration 210/1000 | Loss: 0.00001662
Iteration 211/1000 | Loss: 0.00001662
Iteration 212/1000 | Loss: 0.00001662
Iteration 213/1000 | Loss: 0.00001662
Iteration 214/1000 | Loss: 0.00001662
Iteration 215/1000 | Loss: 0.00001662
Iteration 216/1000 | Loss: 0.00001662
Iteration 217/1000 | Loss: 0.00001662
Iteration 218/1000 | Loss: 0.00001662
Iteration 219/1000 | Loss: 0.00001662
Iteration 220/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.661785972828511e-05, 1.661785972828511e-05, 1.661785972828511e-05, 1.661785972828511e-05, 1.661785972828511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.661785972828511e-05

Optimization complete. Final v2v error: 3.3623270988464355 mm

Highest mean error: 5.132043361663818 mm for frame 99

Lowest mean error: 3.1058366298675537 mm for frame 88

Saving results

Total time: 288.64815044403076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742289
Iteration 2/25 | Loss: 0.00140651
Iteration 3/25 | Loss: 0.00129321
Iteration 4/25 | Loss: 0.00128141
Iteration 5/25 | Loss: 0.00127700
Iteration 6/25 | Loss: 0.00127681
Iteration 7/25 | Loss: 0.00127681
Iteration 8/25 | Loss: 0.00127681
Iteration 9/25 | Loss: 0.00127681
Iteration 10/25 | Loss: 0.00127681
Iteration 11/25 | Loss: 0.00127681
Iteration 12/25 | Loss: 0.00127681
Iteration 13/25 | Loss: 0.00127681
Iteration 14/25 | Loss: 0.00127681
Iteration 15/25 | Loss: 0.00127681
Iteration 16/25 | Loss: 0.00127681
Iteration 17/25 | Loss: 0.00127681
Iteration 18/25 | Loss: 0.00127681
Iteration 19/25 | Loss: 0.00127681
Iteration 20/25 | Loss: 0.00127681
Iteration 21/25 | Loss: 0.00127681
Iteration 22/25 | Loss: 0.00127681
Iteration 23/25 | Loss: 0.00127681
Iteration 24/25 | Loss: 0.00127681
Iteration 25/25 | Loss: 0.00127681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38224995
Iteration 2/25 | Loss: 0.00077342
Iteration 3/25 | Loss: 0.00077340
Iteration 4/25 | Loss: 0.00077340
Iteration 5/25 | Loss: 0.00077340
Iteration 6/25 | Loss: 0.00077340
Iteration 7/25 | Loss: 0.00077340
Iteration 8/25 | Loss: 0.00077340
Iteration 9/25 | Loss: 0.00077340
Iteration 10/25 | Loss: 0.00077340
Iteration 11/25 | Loss: 0.00077340
Iteration 12/25 | Loss: 0.00077340
Iteration 13/25 | Loss: 0.00077340
Iteration 14/25 | Loss: 0.00077340
Iteration 15/25 | Loss: 0.00077340
Iteration 16/25 | Loss: 0.00077340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007733955862931907, 0.0007733955862931907, 0.0007733955862931907, 0.0007733955862931907, 0.0007733955862931907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007733955862931907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077340
Iteration 2/1000 | Loss: 0.00003619
Iteration 3/1000 | Loss: 0.00002729
Iteration 4/1000 | Loss: 0.00002273
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001894
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001840
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001832
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001828
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001826
Iteration 28/1000 | Loss: 0.00001826
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001825
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001819
Iteration 33/1000 | Loss: 0.00001818
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001803
Iteration 36/1000 | Loss: 0.00001803
Iteration 37/1000 | Loss: 0.00001803
Iteration 38/1000 | Loss: 0.00001803
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001802
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001798
Iteration 47/1000 | Loss: 0.00001798
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001796
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001790
Iteration 54/1000 | Loss: 0.00001790
Iteration 55/1000 | Loss: 0.00001789
Iteration 56/1000 | Loss: 0.00001789
Iteration 57/1000 | Loss: 0.00001789
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001785
Iteration 61/1000 | Loss: 0.00001785
Iteration 62/1000 | Loss: 0.00001784
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00001783
Iteration 65/1000 | Loss: 0.00001783
Iteration 66/1000 | Loss: 0.00001782
Iteration 67/1000 | Loss: 0.00001782
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001781
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001778
Iteration 89/1000 | Loss: 0.00001778
Iteration 90/1000 | Loss: 0.00001778
Iteration 91/1000 | Loss: 0.00001778
Iteration 92/1000 | Loss: 0.00001778
Iteration 93/1000 | Loss: 0.00001777
Iteration 94/1000 | Loss: 0.00001777
Iteration 95/1000 | Loss: 0.00001776
Iteration 96/1000 | Loss: 0.00001776
Iteration 97/1000 | Loss: 0.00001776
Iteration 98/1000 | Loss: 0.00001775
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001764
Iteration 137/1000 | Loss: 0.00001764
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001764
Iteration 140/1000 | Loss: 0.00001764
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001764
Iteration 156/1000 | Loss: 0.00001764
Iteration 157/1000 | Loss: 0.00001764
Iteration 158/1000 | Loss: 0.00001764
Iteration 159/1000 | Loss: 0.00001764
Iteration 160/1000 | Loss: 0.00001764
Iteration 161/1000 | Loss: 0.00001764
Iteration 162/1000 | Loss: 0.00001764
Iteration 163/1000 | Loss: 0.00001764
Iteration 164/1000 | Loss: 0.00001764
Iteration 165/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.7638813005760312e-05, 1.7638813005760312e-05, 1.7638813005760312e-05, 1.7638813005760312e-05, 1.7638813005760312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7638813005760312e-05

Optimization complete. Final v2v error: 3.5759825706481934 mm

Highest mean error: 3.9472341537475586 mm for frame 211

Lowest mean error: 3.3554160594940186 mm for frame 7

Saving results

Total time: 43.64881706237793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533041
Iteration 2/25 | Loss: 0.00145793
Iteration 3/25 | Loss: 0.00136526
Iteration 4/25 | Loss: 0.00134640
Iteration 5/25 | Loss: 0.00134003
Iteration 6/25 | Loss: 0.00133882
Iteration 7/25 | Loss: 0.00133865
Iteration 8/25 | Loss: 0.00133865
Iteration 9/25 | Loss: 0.00133865
Iteration 10/25 | Loss: 0.00133865
Iteration 11/25 | Loss: 0.00133865
Iteration 12/25 | Loss: 0.00133865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001338645932264626, 0.001338645932264626, 0.001338645932264626, 0.001338645932264626, 0.001338645932264626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001338645932264626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43297505
Iteration 2/25 | Loss: 0.00098148
Iteration 3/25 | Loss: 0.00098147
Iteration 4/25 | Loss: 0.00098147
Iteration 5/25 | Loss: 0.00098147
Iteration 6/25 | Loss: 0.00098147
Iteration 7/25 | Loss: 0.00098147
Iteration 8/25 | Loss: 0.00098147
Iteration 9/25 | Loss: 0.00098147
Iteration 10/25 | Loss: 0.00098147
Iteration 11/25 | Loss: 0.00098147
Iteration 12/25 | Loss: 0.00098147
Iteration 13/25 | Loss: 0.00098147
Iteration 14/25 | Loss: 0.00098147
Iteration 15/25 | Loss: 0.00098147
Iteration 16/25 | Loss: 0.00098147
Iteration 17/25 | Loss: 0.00098147
Iteration 18/25 | Loss: 0.00098147
Iteration 19/25 | Loss: 0.00098147
Iteration 20/25 | Loss: 0.00098147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000981469638645649, 0.000981469638645649, 0.000981469638645649, 0.000981469638645649, 0.000981469638645649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000981469638645649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098147
Iteration 2/1000 | Loss: 0.00007043
Iteration 3/1000 | Loss: 0.00004760
Iteration 4/1000 | Loss: 0.00003562
Iteration 5/1000 | Loss: 0.00003193
Iteration 6/1000 | Loss: 0.00003027
Iteration 7/1000 | Loss: 0.00002911
Iteration 8/1000 | Loss: 0.00002847
Iteration 9/1000 | Loss: 0.00002799
Iteration 10/1000 | Loss: 0.00002746
Iteration 11/1000 | Loss: 0.00002709
Iteration 12/1000 | Loss: 0.00002681
Iteration 13/1000 | Loss: 0.00002660
Iteration 14/1000 | Loss: 0.00002637
Iteration 15/1000 | Loss: 0.00002626
Iteration 16/1000 | Loss: 0.00002612
Iteration 17/1000 | Loss: 0.00002611
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00002608
Iteration 20/1000 | Loss: 0.00002607
Iteration 21/1000 | Loss: 0.00002607
Iteration 22/1000 | Loss: 0.00002602
Iteration 23/1000 | Loss: 0.00002600
Iteration 24/1000 | Loss: 0.00002593
Iteration 25/1000 | Loss: 0.00002586
Iteration 26/1000 | Loss: 0.00002583
Iteration 27/1000 | Loss: 0.00002580
Iteration 28/1000 | Loss: 0.00002580
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00002574
Iteration 31/1000 | Loss: 0.00002574
Iteration 32/1000 | Loss: 0.00002572
Iteration 33/1000 | Loss: 0.00002572
Iteration 34/1000 | Loss: 0.00002571
Iteration 35/1000 | Loss: 0.00002571
Iteration 36/1000 | Loss: 0.00002571
Iteration 37/1000 | Loss: 0.00002571
Iteration 38/1000 | Loss: 0.00002571
Iteration 39/1000 | Loss: 0.00002571
Iteration 40/1000 | Loss: 0.00002571
Iteration 41/1000 | Loss: 0.00002571
Iteration 42/1000 | Loss: 0.00002571
Iteration 43/1000 | Loss: 0.00002570
Iteration 44/1000 | Loss: 0.00002570
Iteration 45/1000 | Loss: 0.00002570
Iteration 46/1000 | Loss: 0.00002570
Iteration 47/1000 | Loss: 0.00002570
Iteration 48/1000 | Loss: 0.00002568
Iteration 49/1000 | Loss: 0.00002568
Iteration 50/1000 | Loss: 0.00002567
Iteration 51/1000 | Loss: 0.00002567
Iteration 52/1000 | Loss: 0.00002567
Iteration 53/1000 | Loss: 0.00002566
Iteration 54/1000 | Loss: 0.00002566
Iteration 55/1000 | Loss: 0.00002566
Iteration 56/1000 | Loss: 0.00002565
Iteration 57/1000 | Loss: 0.00002565
Iteration 58/1000 | Loss: 0.00002564
Iteration 59/1000 | Loss: 0.00002564
Iteration 60/1000 | Loss: 0.00002564
Iteration 61/1000 | Loss: 0.00002564
Iteration 62/1000 | Loss: 0.00002564
Iteration 63/1000 | Loss: 0.00002563
Iteration 64/1000 | Loss: 0.00002563
Iteration 65/1000 | Loss: 0.00002563
Iteration 66/1000 | Loss: 0.00002563
Iteration 67/1000 | Loss: 0.00002562
Iteration 68/1000 | Loss: 0.00002562
Iteration 69/1000 | Loss: 0.00002562
Iteration 70/1000 | Loss: 0.00002561
Iteration 71/1000 | Loss: 0.00002561
Iteration 72/1000 | Loss: 0.00002561
Iteration 73/1000 | Loss: 0.00002561
Iteration 74/1000 | Loss: 0.00002561
Iteration 75/1000 | Loss: 0.00002561
Iteration 76/1000 | Loss: 0.00002560
Iteration 77/1000 | Loss: 0.00002560
Iteration 78/1000 | Loss: 0.00002560
Iteration 79/1000 | Loss: 0.00002560
Iteration 80/1000 | Loss: 0.00002560
Iteration 81/1000 | Loss: 0.00002560
Iteration 82/1000 | Loss: 0.00002559
Iteration 83/1000 | Loss: 0.00002559
Iteration 84/1000 | Loss: 0.00002559
Iteration 85/1000 | Loss: 0.00002559
Iteration 86/1000 | Loss: 0.00002558
Iteration 87/1000 | Loss: 0.00002558
Iteration 88/1000 | Loss: 0.00002558
Iteration 89/1000 | Loss: 0.00002558
Iteration 90/1000 | Loss: 0.00002558
Iteration 91/1000 | Loss: 0.00002557
Iteration 92/1000 | Loss: 0.00002557
Iteration 93/1000 | Loss: 0.00002557
Iteration 94/1000 | Loss: 0.00002557
Iteration 95/1000 | Loss: 0.00002557
Iteration 96/1000 | Loss: 0.00002557
Iteration 97/1000 | Loss: 0.00002557
Iteration 98/1000 | Loss: 0.00002557
Iteration 99/1000 | Loss: 0.00002557
Iteration 100/1000 | Loss: 0.00002557
Iteration 101/1000 | Loss: 0.00002557
Iteration 102/1000 | Loss: 0.00002557
Iteration 103/1000 | Loss: 0.00002557
Iteration 104/1000 | Loss: 0.00002557
Iteration 105/1000 | Loss: 0.00002556
Iteration 106/1000 | Loss: 0.00002556
Iteration 107/1000 | Loss: 0.00002556
Iteration 108/1000 | Loss: 0.00002556
Iteration 109/1000 | Loss: 0.00002556
Iteration 110/1000 | Loss: 0.00002556
Iteration 111/1000 | Loss: 0.00002556
Iteration 112/1000 | Loss: 0.00002556
Iteration 113/1000 | Loss: 0.00002556
Iteration 114/1000 | Loss: 0.00002556
Iteration 115/1000 | Loss: 0.00002556
Iteration 116/1000 | Loss: 0.00002556
Iteration 117/1000 | Loss: 0.00002556
Iteration 118/1000 | Loss: 0.00002556
Iteration 119/1000 | Loss: 0.00002555
Iteration 120/1000 | Loss: 0.00002555
Iteration 121/1000 | Loss: 0.00002555
Iteration 122/1000 | Loss: 0.00002555
Iteration 123/1000 | Loss: 0.00002555
Iteration 124/1000 | Loss: 0.00002555
Iteration 125/1000 | Loss: 0.00002555
Iteration 126/1000 | Loss: 0.00002555
Iteration 127/1000 | Loss: 0.00002555
Iteration 128/1000 | Loss: 0.00002555
Iteration 129/1000 | Loss: 0.00002555
Iteration 130/1000 | Loss: 0.00002555
Iteration 131/1000 | Loss: 0.00002555
Iteration 132/1000 | Loss: 0.00002554
Iteration 133/1000 | Loss: 0.00002554
Iteration 134/1000 | Loss: 0.00002554
Iteration 135/1000 | Loss: 0.00002554
Iteration 136/1000 | Loss: 0.00002554
Iteration 137/1000 | Loss: 0.00002554
Iteration 138/1000 | Loss: 0.00002554
Iteration 139/1000 | Loss: 0.00002554
Iteration 140/1000 | Loss: 0.00002554
Iteration 141/1000 | Loss: 0.00002554
Iteration 142/1000 | Loss: 0.00002554
Iteration 143/1000 | Loss: 0.00002553
Iteration 144/1000 | Loss: 0.00002553
Iteration 145/1000 | Loss: 0.00002553
Iteration 146/1000 | Loss: 0.00002553
Iteration 147/1000 | Loss: 0.00002553
Iteration 148/1000 | Loss: 0.00002553
Iteration 149/1000 | Loss: 0.00002553
Iteration 150/1000 | Loss: 0.00002553
Iteration 151/1000 | Loss: 0.00002553
Iteration 152/1000 | Loss: 0.00002553
Iteration 153/1000 | Loss: 0.00002553
Iteration 154/1000 | Loss: 0.00002553
Iteration 155/1000 | Loss: 0.00002553
Iteration 156/1000 | Loss: 0.00002552
Iteration 157/1000 | Loss: 0.00002552
Iteration 158/1000 | Loss: 0.00002552
Iteration 159/1000 | Loss: 0.00002552
Iteration 160/1000 | Loss: 0.00002552
Iteration 161/1000 | Loss: 0.00002552
Iteration 162/1000 | Loss: 0.00002552
Iteration 163/1000 | Loss: 0.00002552
Iteration 164/1000 | Loss: 0.00002551
Iteration 165/1000 | Loss: 0.00002551
Iteration 166/1000 | Loss: 0.00002551
Iteration 167/1000 | Loss: 0.00002551
Iteration 168/1000 | Loss: 0.00002551
Iteration 169/1000 | Loss: 0.00002551
Iteration 170/1000 | Loss: 0.00002551
Iteration 171/1000 | Loss: 0.00002551
Iteration 172/1000 | Loss: 0.00002551
Iteration 173/1000 | Loss: 0.00002551
Iteration 174/1000 | Loss: 0.00002550
Iteration 175/1000 | Loss: 0.00002550
Iteration 176/1000 | Loss: 0.00002550
Iteration 177/1000 | Loss: 0.00002550
Iteration 178/1000 | Loss: 0.00002550
Iteration 179/1000 | Loss: 0.00002550
Iteration 180/1000 | Loss: 0.00002550
Iteration 181/1000 | Loss: 0.00002550
Iteration 182/1000 | Loss: 0.00002549
Iteration 183/1000 | Loss: 0.00002549
Iteration 184/1000 | Loss: 0.00002549
Iteration 185/1000 | Loss: 0.00002549
Iteration 186/1000 | Loss: 0.00002549
Iteration 187/1000 | Loss: 0.00002549
Iteration 188/1000 | Loss: 0.00002549
Iteration 189/1000 | Loss: 0.00002548
Iteration 190/1000 | Loss: 0.00002548
Iteration 191/1000 | Loss: 0.00002548
Iteration 192/1000 | Loss: 0.00002548
Iteration 193/1000 | Loss: 0.00002548
Iteration 194/1000 | Loss: 0.00002548
Iteration 195/1000 | Loss: 0.00002547
Iteration 196/1000 | Loss: 0.00002547
Iteration 197/1000 | Loss: 0.00002547
Iteration 198/1000 | Loss: 0.00002547
Iteration 199/1000 | Loss: 0.00002547
Iteration 200/1000 | Loss: 0.00002547
Iteration 201/1000 | Loss: 0.00002547
Iteration 202/1000 | Loss: 0.00002547
Iteration 203/1000 | Loss: 0.00002547
Iteration 204/1000 | Loss: 0.00002546
Iteration 205/1000 | Loss: 0.00002546
Iteration 206/1000 | Loss: 0.00002546
Iteration 207/1000 | Loss: 0.00002546
Iteration 208/1000 | Loss: 0.00002546
Iteration 209/1000 | Loss: 0.00002546
Iteration 210/1000 | Loss: 0.00002545
Iteration 211/1000 | Loss: 0.00002545
Iteration 212/1000 | Loss: 0.00002545
Iteration 213/1000 | Loss: 0.00002545
Iteration 214/1000 | Loss: 0.00002545
Iteration 215/1000 | Loss: 0.00002545
Iteration 216/1000 | Loss: 0.00002545
Iteration 217/1000 | Loss: 0.00002545
Iteration 218/1000 | Loss: 0.00002545
Iteration 219/1000 | Loss: 0.00002545
Iteration 220/1000 | Loss: 0.00002545
Iteration 221/1000 | Loss: 0.00002545
Iteration 222/1000 | Loss: 0.00002545
Iteration 223/1000 | Loss: 0.00002545
Iteration 224/1000 | Loss: 0.00002545
Iteration 225/1000 | Loss: 0.00002545
Iteration 226/1000 | Loss: 0.00002545
Iteration 227/1000 | Loss: 0.00002545
Iteration 228/1000 | Loss: 0.00002545
Iteration 229/1000 | Loss: 0.00002545
Iteration 230/1000 | Loss: 0.00002545
Iteration 231/1000 | Loss: 0.00002545
Iteration 232/1000 | Loss: 0.00002545
Iteration 233/1000 | Loss: 0.00002545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.5445580831728876e-05, 2.5445580831728876e-05, 2.5445580831728876e-05, 2.5445580831728876e-05, 2.5445580831728876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5445580831728876e-05

Optimization complete. Final v2v error: 4.039073944091797 mm

Highest mean error: 4.795143127441406 mm for frame 109

Lowest mean error: 3.1541049480438232 mm for frame 8

Saving results

Total time: 47.71818566322327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793396
Iteration 2/25 | Loss: 0.00150836
Iteration 3/25 | Loss: 0.00141551
Iteration 4/25 | Loss: 0.00140352
Iteration 5/25 | Loss: 0.00139887
Iteration 6/25 | Loss: 0.00139821
Iteration 7/25 | Loss: 0.00139821
Iteration 8/25 | Loss: 0.00139821
Iteration 9/25 | Loss: 0.00139821
Iteration 10/25 | Loss: 0.00139821
Iteration 11/25 | Loss: 0.00139821
Iteration 12/25 | Loss: 0.00139821
Iteration 13/25 | Loss: 0.00139821
Iteration 14/25 | Loss: 0.00139821
Iteration 15/25 | Loss: 0.00139821
Iteration 16/25 | Loss: 0.00139821
Iteration 17/25 | Loss: 0.00139821
Iteration 18/25 | Loss: 0.00139821
Iteration 19/25 | Loss: 0.00139821
Iteration 20/25 | Loss: 0.00139821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013982134405523539, 0.0013982134405523539, 0.0013982134405523539, 0.0013982134405523539, 0.0013982134405523539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013982134405523539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26831436
Iteration 2/25 | Loss: 0.00085943
Iteration 3/25 | Loss: 0.00085940
Iteration 4/25 | Loss: 0.00085940
Iteration 5/25 | Loss: 0.00085940
Iteration 6/25 | Loss: 0.00085940
Iteration 7/25 | Loss: 0.00085940
Iteration 8/25 | Loss: 0.00085940
Iteration 9/25 | Loss: 0.00085940
Iteration 10/25 | Loss: 0.00085940
Iteration 11/25 | Loss: 0.00085940
Iteration 12/25 | Loss: 0.00085940
Iteration 13/25 | Loss: 0.00085940
Iteration 14/25 | Loss: 0.00085940
Iteration 15/25 | Loss: 0.00085940
Iteration 16/25 | Loss: 0.00085940
Iteration 17/25 | Loss: 0.00085940
Iteration 18/25 | Loss: 0.00085940
Iteration 19/25 | Loss: 0.00085940
Iteration 20/25 | Loss: 0.00085940
Iteration 21/25 | Loss: 0.00085940
Iteration 22/25 | Loss: 0.00085940
Iteration 23/25 | Loss: 0.00085940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008593965321779251, 0.0008593965321779251, 0.0008593965321779251, 0.0008593965321779251, 0.0008593965321779251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008593965321779251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085940
Iteration 2/1000 | Loss: 0.00003631
Iteration 3/1000 | Loss: 0.00002903
Iteration 4/1000 | Loss: 0.00002702
Iteration 5/1000 | Loss: 0.00002612
Iteration 6/1000 | Loss: 0.00002559
Iteration 7/1000 | Loss: 0.00002511
Iteration 8/1000 | Loss: 0.00002473
Iteration 9/1000 | Loss: 0.00002444
Iteration 10/1000 | Loss: 0.00002425
Iteration 11/1000 | Loss: 0.00002412
Iteration 12/1000 | Loss: 0.00002395
Iteration 13/1000 | Loss: 0.00002390
Iteration 14/1000 | Loss: 0.00002383
Iteration 15/1000 | Loss: 0.00002383
Iteration 16/1000 | Loss: 0.00002382
Iteration 17/1000 | Loss: 0.00002378
Iteration 18/1000 | Loss: 0.00002375
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002373
Iteration 22/1000 | Loss: 0.00002370
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00002369
Iteration 25/1000 | Loss: 0.00002365
Iteration 26/1000 | Loss: 0.00002361
Iteration 27/1000 | Loss: 0.00002360
Iteration 28/1000 | Loss: 0.00002360
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002350
Iteration 32/1000 | Loss: 0.00002346
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002339
Iteration 35/1000 | Loss: 0.00002338
Iteration 36/1000 | Loss: 0.00002335
Iteration 37/1000 | Loss: 0.00002334
Iteration 38/1000 | Loss: 0.00002333
Iteration 39/1000 | Loss: 0.00002333
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002331
Iteration 42/1000 | Loss: 0.00002331
Iteration 43/1000 | Loss: 0.00002331
Iteration 44/1000 | Loss: 0.00002331
Iteration 45/1000 | Loss: 0.00002330
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002330
Iteration 48/1000 | Loss: 0.00002329
Iteration 49/1000 | Loss: 0.00002328
Iteration 50/1000 | Loss: 0.00002328
Iteration 51/1000 | Loss: 0.00002328
Iteration 52/1000 | Loss: 0.00002327
Iteration 53/1000 | Loss: 0.00002327
Iteration 54/1000 | Loss: 0.00002327
Iteration 55/1000 | Loss: 0.00002327
Iteration 56/1000 | Loss: 0.00002326
Iteration 57/1000 | Loss: 0.00002326
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002326
Iteration 60/1000 | Loss: 0.00002325
Iteration 61/1000 | Loss: 0.00002325
Iteration 62/1000 | Loss: 0.00002325
Iteration 63/1000 | Loss: 0.00002325
Iteration 64/1000 | Loss: 0.00002325
Iteration 65/1000 | Loss: 0.00002324
Iteration 66/1000 | Loss: 0.00002324
Iteration 67/1000 | Loss: 0.00002324
Iteration 68/1000 | Loss: 0.00002324
Iteration 69/1000 | Loss: 0.00002324
Iteration 70/1000 | Loss: 0.00002324
Iteration 71/1000 | Loss: 0.00002324
Iteration 72/1000 | Loss: 0.00002324
Iteration 73/1000 | Loss: 0.00002323
Iteration 74/1000 | Loss: 0.00002323
Iteration 75/1000 | Loss: 0.00002323
Iteration 76/1000 | Loss: 0.00002323
Iteration 77/1000 | Loss: 0.00002323
Iteration 78/1000 | Loss: 0.00002323
Iteration 79/1000 | Loss: 0.00002322
Iteration 80/1000 | Loss: 0.00002322
Iteration 81/1000 | Loss: 0.00002322
Iteration 82/1000 | Loss: 0.00002322
Iteration 83/1000 | Loss: 0.00002322
Iteration 84/1000 | Loss: 0.00002321
Iteration 85/1000 | Loss: 0.00002321
Iteration 86/1000 | Loss: 0.00002321
Iteration 87/1000 | Loss: 0.00002321
Iteration 88/1000 | Loss: 0.00002321
Iteration 89/1000 | Loss: 0.00002320
Iteration 90/1000 | Loss: 0.00002320
Iteration 91/1000 | Loss: 0.00002320
Iteration 92/1000 | Loss: 0.00002320
Iteration 93/1000 | Loss: 0.00002320
Iteration 94/1000 | Loss: 0.00002320
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002319
Iteration 98/1000 | Loss: 0.00002319
Iteration 99/1000 | Loss: 0.00002319
Iteration 100/1000 | Loss: 0.00002319
Iteration 101/1000 | Loss: 0.00002319
Iteration 102/1000 | Loss: 0.00002319
Iteration 103/1000 | Loss: 0.00002319
Iteration 104/1000 | Loss: 0.00002319
Iteration 105/1000 | Loss: 0.00002319
Iteration 106/1000 | Loss: 0.00002319
Iteration 107/1000 | Loss: 0.00002318
Iteration 108/1000 | Loss: 0.00002318
Iteration 109/1000 | Loss: 0.00002318
Iteration 110/1000 | Loss: 0.00002318
Iteration 111/1000 | Loss: 0.00002318
Iteration 112/1000 | Loss: 0.00002318
Iteration 113/1000 | Loss: 0.00002318
Iteration 114/1000 | Loss: 0.00002318
Iteration 115/1000 | Loss: 0.00002318
Iteration 116/1000 | Loss: 0.00002318
Iteration 117/1000 | Loss: 0.00002318
Iteration 118/1000 | Loss: 0.00002318
Iteration 119/1000 | Loss: 0.00002318
Iteration 120/1000 | Loss: 0.00002318
Iteration 121/1000 | Loss: 0.00002318
Iteration 122/1000 | Loss: 0.00002318
Iteration 123/1000 | Loss: 0.00002318
Iteration 124/1000 | Loss: 0.00002318
Iteration 125/1000 | Loss: 0.00002318
Iteration 126/1000 | Loss: 0.00002318
Iteration 127/1000 | Loss: 0.00002318
Iteration 128/1000 | Loss: 0.00002318
Iteration 129/1000 | Loss: 0.00002318
Iteration 130/1000 | Loss: 0.00002318
Iteration 131/1000 | Loss: 0.00002318
Iteration 132/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.3176939066615887e-05, 2.3176939066615887e-05, 2.3176939066615887e-05, 2.3176939066615887e-05, 2.3176939066615887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3176939066615887e-05

Optimization complete. Final v2v error: 4.009398460388184 mm

Highest mean error: 4.948473930358887 mm for frame 52

Lowest mean error: 3.5712153911590576 mm for frame 223

Saving results

Total time: 42.866051197052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941535
Iteration 2/25 | Loss: 0.00367335
Iteration 3/25 | Loss: 0.00250338
Iteration 4/25 | Loss: 0.00186668
Iteration 5/25 | Loss: 0.00180403
Iteration 6/25 | Loss: 0.00177761
Iteration 7/25 | Loss: 0.00175718
Iteration 8/25 | Loss: 0.00176086
Iteration 9/25 | Loss: 0.00173193
Iteration 10/25 | Loss: 0.00171214
Iteration 11/25 | Loss: 0.00167113
Iteration 12/25 | Loss: 0.00166169
Iteration 13/25 | Loss: 0.00165763
Iteration 14/25 | Loss: 0.00164300
Iteration 15/25 | Loss: 0.00164748
Iteration 16/25 | Loss: 0.00164322
Iteration 17/25 | Loss: 0.00163876
Iteration 18/25 | Loss: 0.00164604
Iteration 19/25 | Loss: 0.00164399
Iteration 20/25 | Loss: 0.00164275
Iteration 21/25 | Loss: 0.00163046
Iteration 22/25 | Loss: 0.00160895
Iteration 23/25 | Loss: 0.00160896
Iteration 24/25 | Loss: 0.00160355
Iteration 25/25 | Loss: 0.00159323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.39114666
Iteration 2/25 | Loss: 0.00384748
Iteration 3/25 | Loss: 0.00364304
Iteration 4/25 | Loss: 0.00364304
Iteration 5/25 | Loss: 0.00364304
Iteration 6/25 | Loss: 0.00364304
Iteration 7/25 | Loss: 0.00364304
Iteration 8/25 | Loss: 0.00364304
Iteration 9/25 | Loss: 0.00364304
Iteration 10/25 | Loss: 0.00364304
Iteration 11/25 | Loss: 0.00364304
Iteration 12/25 | Loss: 0.00364304
Iteration 13/25 | Loss: 0.00364304
Iteration 14/25 | Loss: 0.00364304
Iteration 15/25 | Loss: 0.00364304
Iteration 16/25 | Loss: 0.00364304
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003643035888671875, 0.003643035888671875, 0.003643035888671875, 0.003643035888671875, 0.003643035888671875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003643035888671875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00364304
Iteration 2/1000 | Loss: 0.00096789
Iteration 3/1000 | Loss: 0.00174652
Iteration 4/1000 | Loss: 0.00026968
Iteration 5/1000 | Loss: 0.00021547
Iteration 6/1000 | Loss: 0.00069578
Iteration 7/1000 | Loss: 0.00029200
Iteration 8/1000 | Loss: 0.00020094
Iteration 9/1000 | Loss: 0.00082035
Iteration 10/1000 | Loss: 0.00066804
Iteration 11/1000 | Loss: 0.00158528
Iteration 12/1000 | Loss: 0.00227793
Iteration 13/1000 | Loss: 0.00143578
Iteration 14/1000 | Loss: 0.00041224
Iteration 15/1000 | Loss: 0.00041285
Iteration 16/1000 | Loss: 0.00016579
Iteration 17/1000 | Loss: 0.00248054
Iteration 18/1000 | Loss: 0.00215082
Iteration 19/1000 | Loss: 0.00262318
Iteration 20/1000 | Loss: 0.00262573
Iteration 21/1000 | Loss: 0.00101607
Iteration 22/1000 | Loss: 0.00021918
Iteration 23/1000 | Loss: 0.00030549
Iteration 24/1000 | Loss: 0.00014948
Iteration 25/1000 | Loss: 0.00045967
Iteration 26/1000 | Loss: 0.00044013
Iteration 27/1000 | Loss: 0.00026215
Iteration 28/1000 | Loss: 0.00020564
Iteration 29/1000 | Loss: 0.00041018
Iteration 30/1000 | Loss: 0.00082091
Iteration 31/1000 | Loss: 0.00048284
Iteration 32/1000 | Loss: 0.00034235
Iteration 33/1000 | Loss: 0.00043416
Iteration 34/1000 | Loss: 0.00072186
Iteration 35/1000 | Loss: 0.00030136
Iteration 36/1000 | Loss: 0.00009387
Iteration 37/1000 | Loss: 0.00030225
Iteration 38/1000 | Loss: 0.00092084
Iteration 39/1000 | Loss: 0.00035194
Iteration 40/1000 | Loss: 0.00025344
Iteration 41/1000 | Loss: 0.00008601
Iteration 42/1000 | Loss: 0.00053200
Iteration 43/1000 | Loss: 0.00047340
Iteration 44/1000 | Loss: 0.00040290
Iteration 45/1000 | Loss: 0.00007795
Iteration 46/1000 | Loss: 0.00150540
Iteration 47/1000 | Loss: 0.00054250
Iteration 48/1000 | Loss: 0.00018613
Iteration 49/1000 | Loss: 0.00046494
Iteration 50/1000 | Loss: 0.00036943
Iteration 51/1000 | Loss: 0.00039016
Iteration 52/1000 | Loss: 0.00039458
Iteration 53/1000 | Loss: 0.00040285
Iteration 54/1000 | Loss: 0.00032730
Iteration 55/1000 | Loss: 0.00035497
Iteration 56/1000 | Loss: 0.00028378
Iteration 57/1000 | Loss: 0.00036084
Iteration 58/1000 | Loss: 0.00042308
Iteration 59/1000 | Loss: 0.00011552
Iteration 60/1000 | Loss: 0.00007249
Iteration 61/1000 | Loss: 0.00014617
Iteration 62/1000 | Loss: 0.00006542
Iteration 63/1000 | Loss: 0.00034225
Iteration 64/1000 | Loss: 0.00006289
Iteration 65/1000 | Loss: 0.00006011
Iteration 66/1000 | Loss: 0.00032848
Iteration 67/1000 | Loss: 0.00005781
Iteration 68/1000 | Loss: 0.00005496
Iteration 69/1000 | Loss: 0.00005307
Iteration 70/1000 | Loss: 0.00030641
Iteration 71/1000 | Loss: 0.00037467
Iteration 72/1000 | Loss: 0.00005319
Iteration 73/1000 | Loss: 0.00004876
Iteration 74/1000 | Loss: 0.00004681
Iteration 75/1000 | Loss: 0.00075713
Iteration 76/1000 | Loss: 0.00015040
Iteration 77/1000 | Loss: 0.00016333
Iteration 78/1000 | Loss: 0.00014187
Iteration 79/1000 | Loss: 0.00017504
Iteration 80/1000 | Loss: 0.00007852
Iteration 81/1000 | Loss: 0.00023734
Iteration 82/1000 | Loss: 0.00008739
Iteration 83/1000 | Loss: 0.00025504
Iteration 84/1000 | Loss: 0.00012590
Iteration 85/1000 | Loss: 0.00004399
Iteration 86/1000 | Loss: 0.00004176
Iteration 87/1000 | Loss: 0.00022643
Iteration 88/1000 | Loss: 0.00147646
Iteration 89/1000 | Loss: 0.00024730
Iteration 90/1000 | Loss: 0.00006860
Iteration 91/1000 | Loss: 0.00019070
Iteration 92/1000 | Loss: 0.00012355
Iteration 93/1000 | Loss: 0.00004050
Iteration 94/1000 | Loss: 0.00003763
Iteration 95/1000 | Loss: 0.00013802
Iteration 96/1000 | Loss: 0.00003514
Iteration 97/1000 | Loss: 0.00003378
Iteration 98/1000 | Loss: 0.00019293
Iteration 99/1000 | Loss: 0.00039932
Iteration 100/1000 | Loss: 0.00003418
Iteration 101/1000 | Loss: 0.00003265
Iteration 102/1000 | Loss: 0.00012890
Iteration 103/1000 | Loss: 0.00010254
Iteration 104/1000 | Loss: 0.00032591
Iteration 105/1000 | Loss: 0.00039056
Iteration 106/1000 | Loss: 0.00015766
Iteration 107/1000 | Loss: 0.00023447
Iteration 108/1000 | Loss: 0.00017443
Iteration 109/1000 | Loss: 0.00013767
Iteration 110/1000 | Loss: 0.00014065
Iteration 111/1000 | Loss: 0.00010107
Iteration 112/1000 | Loss: 0.00008093
Iteration 113/1000 | Loss: 0.00014718
Iteration 114/1000 | Loss: 0.00011286
Iteration 115/1000 | Loss: 0.00006119
Iteration 116/1000 | Loss: 0.00003277
Iteration 117/1000 | Loss: 0.00009595
Iteration 118/1000 | Loss: 0.00006539
Iteration 119/1000 | Loss: 0.00038660
Iteration 120/1000 | Loss: 0.00035262
Iteration 121/1000 | Loss: 0.00038008
Iteration 122/1000 | Loss: 0.00003474
Iteration 123/1000 | Loss: 0.00003190
Iteration 124/1000 | Loss: 0.00003021
Iteration 125/1000 | Loss: 0.00002889
Iteration 126/1000 | Loss: 0.00002778
Iteration 127/1000 | Loss: 0.00002686
Iteration 128/1000 | Loss: 0.00002646
Iteration 129/1000 | Loss: 0.00002618
Iteration 130/1000 | Loss: 0.00002605
Iteration 131/1000 | Loss: 0.00002597
Iteration 132/1000 | Loss: 0.00002597
Iteration 133/1000 | Loss: 0.00002595
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002594
Iteration 136/1000 | Loss: 0.00002594
Iteration 137/1000 | Loss: 0.00002582
Iteration 138/1000 | Loss: 0.00002581
Iteration 139/1000 | Loss: 0.00002580
Iteration 140/1000 | Loss: 0.00002579
Iteration 141/1000 | Loss: 0.00002578
Iteration 142/1000 | Loss: 0.00002577
Iteration 143/1000 | Loss: 0.00002577
Iteration 144/1000 | Loss: 0.00002577
Iteration 145/1000 | Loss: 0.00002576
Iteration 146/1000 | Loss: 0.00002575
Iteration 147/1000 | Loss: 0.00002575
Iteration 148/1000 | Loss: 0.00002574
Iteration 149/1000 | Loss: 0.00002574
Iteration 150/1000 | Loss: 0.00002573
Iteration 151/1000 | Loss: 0.00002573
Iteration 152/1000 | Loss: 0.00002573
Iteration 153/1000 | Loss: 0.00002572
Iteration 154/1000 | Loss: 0.00002572
Iteration 155/1000 | Loss: 0.00002571
Iteration 156/1000 | Loss: 0.00002570
Iteration 157/1000 | Loss: 0.00002570
Iteration 158/1000 | Loss: 0.00002569
Iteration 159/1000 | Loss: 0.00002569
Iteration 160/1000 | Loss: 0.00002569
Iteration 161/1000 | Loss: 0.00002569
Iteration 162/1000 | Loss: 0.00002569
Iteration 163/1000 | Loss: 0.00002568
Iteration 164/1000 | Loss: 0.00002568
Iteration 165/1000 | Loss: 0.00002568
Iteration 166/1000 | Loss: 0.00002568
Iteration 167/1000 | Loss: 0.00002566
Iteration 168/1000 | Loss: 0.00002563
Iteration 169/1000 | Loss: 0.00002563
Iteration 170/1000 | Loss: 0.00002560
Iteration 171/1000 | Loss: 0.00002559
Iteration 172/1000 | Loss: 0.00002559
Iteration 173/1000 | Loss: 0.00002558
Iteration 174/1000 | Loss: 0.00002557
Iteration 175/1000 | Loss: 0.00002556
Iteration 176/1000 | Loss: 0.00002556
Iteration 177/1000 | Loss: 0.00002555
Iteration 178/1000 | Loss: 0.00002553
Iteration 179/1000 | Loss: 0.00002553
Iteration 180/1000 | Loss: 0.00002552
Iteration 181/1000 | Loss: 0.00002552
Iteration 182/1000 | Loss: 0.00002552
Iteration 183/1000 | Loss: 0.00002552
Iteration 184/1000 | Loss: 0.00002551
Iteration 185/1000 | Loss: 0.00002551
Iteration 186/1000 | Loss: 0.00002551
Iteration 187/1000 | Loss: 0.00002550
Iteration 188/1000 | Loss: 0.00002550
Iteration 189/1000 | Loss: 0.00002549
Iteration 190/1000 | Loss: 0.00002549
Iteration 191/1000 | Loss: 0.00002548
Iteration 192/1000 | Loss: 0.00002548
Iteration 193/1000 | Loss: 0.00002548
Iteration 194/1000 | Loss: 0.00002548
Iteration 195/1000 | Loss: 0.00002547
Iteration 196/1000 | Loss: 0.00002547
Iteration 197/1000 | Loss: 0.00002547
Iteration 198/1000 | Loss: 0.00002547
Iteration 199/1000 | Loss: 0.00002547
Iteration 200/1000 | Loss: 0.00002547
Iteration 201/1000 | Loss: 0.00002546
Iteration 202/1000 | Loss: 0.00002546
Iteration 203/1000 | Loss: 0.00002546
Iteration 204/1000 | Loss: 0.00002546
Iteration 205/1000 | Loss: 0.00002545
Iteration 206/1000 | Loss: 0.00002545
Iteration 207/1000 | Loss: 0.00002544
Iteration 208/1000 | Loss: 0.00002544
Iteration 209/1000 | Loss: 0.00002543
Iteration 210/1000 | Loss: 0.00002543
Iteration 211/1000 | Loss: 0.00002543
Iteration 212/1000 | Loss: 0.00002542
Iteration 213/1000 | Loss: 0.00002542
Iteration 214/1000 | Loss: 0.00002542
Iteration 215/1000 | Loss: 0.00002542
Iteration 216/1000 | Loss: 0.00002541
Iteration 217/1000 | Loss: 0.00002541
Iteration 218/1000 | Loss: 0.00002541
Iteration 219/1000 | Loss: 0.00002541
Iteration 220/1000 | Loss: 0.00002540
Iteration 221/1000 | Loss: 0.00002540
Iteration 222/1000 | Loss: 0.00002540
Iteration 223/1000 | Loss: 0.00002540
Iteration 224/1000 | Loss: 0.00002540
Iteration 225/1000 | Loss: 0.00002539
Iteration 226/1000 | Loss: 0.00002539
Iteration 227/1000 | Loss: 0.00002539
Iteration 228/1000 | Loss: 0.00002538
Iteration 229/1000 | Loss: 0.00002538
Iteration 230/1000 | Loss: 0.00002538
Iteration 231/1000 | Loss: 0.00002538
Iteration 232/1000 | Loss: 0.00002538
Iteration 233/1000 | Loss: 0.00002537
Iteration 234/1000 | Loss: 0.00002537
Iteration 235/1000 | Loss: 0.00002537
Iteration 236/1000 | Loss: 0.00002537
Iteration 237/1000 | Loss: 0.00002537
Iteration 238/1000 | Loss: 0.00002536
Iteration 239/1000 | Loss: 0.00002536
Iteration 240/1000 | Loss: 0.00002536
Iteration 241/1000 | Loss: 0.00002535
Iteration 242/1000 | Loss: 0.00002535
Iteration 243/1000 | Loss: 0.00002534
Iteration 244/1000 | Loss: 0.00002534
Iteration 245/1000 | Loss: 0.00002534
Iteration 246/1000 | Loss: 0.00002534
Iteration 247/1000 | Loss: 0.00002534
Iteration 248/1000 | Loss: 0.00002534
Iteration 249/1000 | Loss: 0.00002534
Iteration 250/1000 | Loss: 0.00002534
Iteration 251/1000 | Loss: 0.00002534
Iteration 252/1000 | Loss: 0.00002533
Iteration 253/1000 | Loss: 0.00002533
Iteration 254/1000 | Loss: 0.00002533
Iteration 255/1000 | Loss: 0.00002533
Iteration 256/1000 | Loss: 0.00002533
Iteration 257/1000 | Loss: 0.00002533
Iteration 258/1000 | Loss: 0.00002533
Iteration 259/1000 | Loss: 0.00002533
Iteration 260/1000 | Loss: 0.00002533
Iteration 261/1000 | Loss: 0.00002532
Iteration 262/1000 | Loss: 0.00002532
Iteration 263/1000 | Loss: 0.00002532
Iteration 264/1000 | Loss: 0.00002532
Iteration 265/1000 | Loss: 0.00002532
Iteration 266/1000 | Loss: 0.00002532
Iteration 267/1000 | Loss: 0.00002532
Iteration 268/1000 | Loss: 0.00002532
Iteration 269/1000 | Loss: 0.00002532
Iteration 270/1000 | Loss: 0.00002532
Iteration 271/1000 | Loss: 0.00002532
Iteration 272/1000 | Loss: 0.00002532
Iteration 273/1000 | Loss: 0.00002532
Iteration 274/1000 | Loss: 0.00002532
Iteration 275/1000 | Loss: 0.00002532
Iteration 276/1000 | Loss: 0.00002532
Iteration 277/1000 | Loss: 0.00002532
Iteration 278/1000 | Loss: 0.00002532
Iteration 279/1000 | Loss: 0.00002532
Iteration 280/1000 | Loss: 0.00002532
Iteration 281/1000 | Loss: 0.00002532
Iteration 282/1000 | Loss: 0.00002532
Iteration 283/1000 | Loss: 0.00002532
Iteration 284/1000 | Loss: 0.00002532
Iteration 285/1000 | Loss: 0.00002532
Iteration 286/1000 | Loss: 0.00002532
Iteration 287/1000 | Loss: 0.00002532
Iteration 288/1000 | Loss: 0.00002532
Iteration 289/1000 | Loss: 0.00002532
Iteration 290/1000 | Loss: 0.00002532
Iteration 291/1000 | Loss: 0.00002532
Iteration 292/1000 | Loss: 0.00002532
Iteration 293/1000 | Loss: 0.00002532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [2.5321294742752798e-05, 2.5321294742752798e-05, 2.5321294742752798e-05, 2.5321294742752798e-05, 2.5321294742752798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5321294742752798e-05

Optimization complete. Final v2v error: 3.8302690982818604 mm

Highest mean error: 11.85344123840332 mm for frame 119

Lowest mean error: 2.8837575912475586 mm for frame 31

Saving results

Total time: 257.67169404029846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478498
Iteration 2/25 | Loss: 0.00135179
Iteration 3/25 | Loss: 0.00127909
Iteration 4/25 | Loss: 0.00126648
Iteration 5/25 | Loss: 0.00126150
Iteration 6/25 | Loss: 0.00126045
Iteration 7/25 | Loss: 0.00126033
Iteration 8/25 | Loss: 0.00126033
Iteration 9/25 | Loss: 0.00126033
Iteration 10/25 | Loss: 0.00126033
Iteration 11/25 | Loss: 0.00126033
Iteration 12/25 | Loss: 0.00126033
Iteration 13/25 | Loss: 0.00126033
Iteration 14/25 | Loss: 0.00126033
Iteration 15/25 | Loss: 0.00126033
Iteration 16/25 | Loss: 0.00126033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012603332288563251, 0.0012603332288563251, 0.0012603332288563251, 0.0012603332288563251, 0.0012603332288563251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012603332288563251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.90681505
Iteration 2/25 | Loss: 0.00084645
Iteration 3/25 | Loss: 0.00084645
Iteration 4/25 | Loss: 0.00084645
Iteration 5/25 | Loss: 0.00084644
Iteration 6/25 | Loss: 0.00084644
Iteration 7/25 | Loss: 0.00084644
Iteration 8/25 | Loss: 0.00084644
Iteration 9/25 | Loss: 0.00084644
Iteration 10/25 | Loss: 0.00084644
Iteration 11/25 | Loss: 0.00084644
Iteration 12/25 | Loss: 0.00084644
Iteration 13/25 | Loss: 0.00084644
Iteration 14/25 | Loss: 0.00084644
Iteration 15/25 | Loss: 0.00084644
Iteration 16/25 | Loss: 0.00084644
Iteration 17/25 | Loss: 0.00084644
Iteration 18/25 | Loss: 0.00084644
Iteration 19/25 | Loss: 0.00084644
Iteration 20/25 | Loss: 0.00084644
Iteration 21/25 | Loss: 0.00084644
Iteration 22/25 | Loss: 0.00084644
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000846442359033972, 0.000846442359033972, 0.000846442359033972, 0.000846442359033972, 0.000846442359033972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000846442359033972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084644
Iteration 2/1000 | Loss: 0.00003818
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001852
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001621
Iteration 8/1000 | Loss: 0.00001584
Iteration 9/1000 | Loss: 0.00001560
Iteration 10/1000 | Loss: 0.00001542
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001525
Iteration 15/1000 | Loss: 0.00001521
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001517
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001496
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001494
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001469
Iteration 78/1000 | Loss: 0.00001469
Iteration 79/1000 | Loss: 0.00001469
Iteration 80/1000 | Loss: 0.00001468
Iteration 81/1000 | Loss: 0.00001468
Iteration 82/1000 | Loss: 0.00001468
Iteration 83/1000 | Loss: 0.00001466
Iteration 84/1000 | Loss: 0.00001466
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001465
Iteration 90/1000 | Loss: 0.00001465
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001464
Iteration 93/1000 | Loss: 0.00001464
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001464
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001463
Iteration 100/1000 | Loss: 0.00001462
Iteration 101/1000 | Loss: 0.00001462
Iteration 102/1000 | Loss: 0.00001462
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001460
Iteration 107/1000 | Loss: 0.00001460
Iteration 108/1000 | Loss: 0.00001460
Iteration 109/1000 | Loss: 0.00001460
Iteration 110/1000 | Loss: 0.00001459
Iteration 111/1000 | Loss: 0.00001459
Iteration 112/1000 | Loss: 0.00001459
Iteration 113/1000 | Loss: 0.00001459
Iteration 114/1000 | Loss: 0.00001459
Iteration 115/1000 | Loss: 0.00001459
Iteration 116/1000 | Loss: 0.00001459
Iteration 117/1000 | Loss: 0.00001459
Iteration 118/1000 | Loss: 0.00001459
Iteration 119/1000 | Loss: 0.00001459
Iteration 120/1000 | Loss: 0.00001458
Iteration 121/1000 | Loss: 0.00001458
Iteration 122/1000 | Loss: 0.00001458
Iteration 123/1000 | Loss: 0.00001458
Iteration 124/1000 | Loss: 0.00001458
Iteration 125/1000 | Loss: 0.00001458
Iteration 126/1000 | Loss: 0.00001458
Iteration 127/1000 | Loss: 0.00001458
Iteration 128/1000 | Loss: 0.00001458
Iteration 129/1000 | Loss: 0.00001458
Iteration 130/1000 | Loss: 0.00001458
Iteration 131/1000 | Loss: 0.00001458
Iteration 132/1000 | Loss: 0.00001457
Iteration 133/1000 | Loss: 0.00001457
Iteration 134/1000 | Loss: 0.00001457
Iteration 135/1000 | Loss: 0.00001457
Iteration 136/1000 | Loss: 0.00001457
Iteration 137/1000 | Loss: 0.00001457
Iteration 138/1000 | Loss: 0.00001457
Iteration 139/1000 | Loss: 0.00001456
Iteration 140/1000 | Loss: 0.00001456
Iteration 141/1000 | Loss: 0.00001456
Iteration 142/1000 | Loss: 0.00001456
Iteration 143/1000 | Loss: 0.00001456
Iteration 144/1000 | Loss: 0.00001456
Iteration 145/1000 | Loss: 0.00001456
Iteration 146/1000 | Loss: 0.00001456
Iteration 147/1000 | Loss: 0.00001456
Iteration 148/1000 | Loss: 0.00001456
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001456
Iteration 151/1000 | Loss: 0.00001456
Iteration 152/1000 | Loss: 0.00001456
Iteration 153/1000 | Loss: 0.00001456
Iteration 154/1000 | Loss: 0.00001456
Iteration 155/1000 | Loss: 0.00001456
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001456
Iteration 160/1000 | Loss: 0.00001456
Iteration 161/1000 | Loss: 0.00001456
Iteration 162/1000 | Loss: 0.00001456
Iteration 163/1000 | Loss: 0.00001456
Iteration 164/1000 | Loss: 0.00001456
Iteration 165/1000 | Loss: 0.00001456
Iteration 166/1000 | Loss: 0.00001456
Iteration 167/1000 | Loss: 0.00001456
Iteration 168/1000 | Loss: 0.00001456
Iteration 169/1000 | Loss: 0.00001456
Iteration 170/1000 | Loss: 0.00001456
Iteration 171/1000 | Loss: 0.00001456
Iteration 172/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4558222574123647e-05, 1.4558222574123647e-05, 1.4558222574123647e-05, 1.4558222574123647e-05, 1.4558222574123647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4558222574123647e-05

Optimization complete. Final v2v error: 3.278506278991699 mm

Highest mean error: 3.6704957485198975 mm for frame 105

Lowest mean error: 3.091259002685547 mm for frame 65

Saving results

Total time: 39.0640013217926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917120
Iteration 2/25 | Loss: 0.00360023
Iteration 3/25 | Loss: 0.00323159
Iteration 4/25 | Loss: 0.00238906
Iteration 5/25 | Loss: 0.00219134
Iteration 6/25 | Loss: 0.00207721
Iteration 7/25 | Loss: 0.00196118
Iteration 8/25 | Loss: 0.00255254
Iteration 9/25 | Loss: 0.00155043
Iteration 10/25 | Loss: 0.00137089
Iteration 11/25 | Loss: 0.00134428
Iteration 12/25 | Loss: 0.00134063
Iteration 13/25 | Loss: 0.00134405
Iteration 14/25 | Loss: 0.00133855
Iteration 15/25 | Loss: 0.00133775
Iteration 16/25 | Loss: 0.00133772
Iteration 17/25 | Loss: 0.00133772
Iteration 18/25 | Loss: 0.00133772
Iteration 19/25 | Loss: 0.00133771
Iteration 20/25 | Loss: 0.00133771
Iteration 21/25 | Loss: 0.00133769
Iteration 22/25 | Loss: 0.00133769
Iteration 23/25 | Loss: 0.00133769
Iteration 24/25 | Loss: 0.00133769
Iteration 25/25 | Loss: 0.00133769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39678335
Iteration 2/25 | Loss: 0.00112361
Iteration 3/25 | Loss: 0.00071274
Iteration 4/25 | Loss: 0.00071268
Iteration 5/25 | Loss: 0.00071268
Iteration 6/25 | Loss: 0.00071268
Iteration 7/25 | Loss: 0.00071268
Iteration 8/25 | Loss: 0.00071268
Iteration 9/25 | Loss: 0.00071268
Iteration 10/25 | Loss: 0.00071268
Iteration 11/25 | Loss: 0.00071268
Iteration 12/25 | Loss: 0.00071268
Iteration 13/25 | Loss: 0.00071268
Iteration 14/25 | Loss: 0.00071268
Iteration 15/25 | Loss: 0.00071268
Iteration 16/25 | Loss: 0.00071268
Iteration 17/25 | Loss: 0.00071268
Iteration 18/25 | Loss: 0.00071268
Iteration 19/25 | Loss: 0.00071268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007126815617084503, 0.0007126815617084503, 0.0007126815617084503, 0.0007126815617084503, 0.0007126815617084503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007126815617084503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071268
Iteration 2/1000 | Loss: 0.00004227
Iteration 3/1000 | Loss: 0.00003093
Iteration 4/1000 | Loss: 0.00114339
Iteration 5/1000 | Loss: 0.00073001
Iteration 6/1000 | Loss: 0.00004676
Iteration 7/1000 | Loss: 0.00003146
Iteration 8/1000 | Loss: 0.00002781
Iteration 9/1000 | Loss: 0.00002602
Iteration 10/1000 | Loss: 0.00002483
Iteration 11/1000 | Loss: 0.00014664
Iteration 12/1000 | Loss: 0.00002408
Iteration 13/1000 | Loss: 0.00002339
Iteration 14/1000 | Loss: 0.00002281
Iteration 15/1000 | Loss: 0.00002241
Iteration 16/1000 | Loss: 0.00002211
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002161
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00002160
Iteration 22/1000 | Loss: 0.00032056
Iteration 23/1000 | Loss: 0.00143433
Iteration 24/1000 | Loss: 0.00038705
Iteration 25/1000 | Loss: 0.00004122
Iteration 26/1000 | Loss: 0.00013533
Iteration 27/1000 | Loss: 0.00004542
Iteration 28/1000 | Loss: 0.00012077
Iteration 29/1000 | Loss: 0.00002718
Iteration 30/1000 | Loss: 0.00002544
Iteration 31/1000 | Loss: 0.00002438
Iteration 32/1000 | Loss: 0.00002358
Iteration 33/1000 | Loss: 0.00002320
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00002250
Iteration 36/1000 | Loss: 0.00002223
Iteration 37/1000 | Loss: 0.00030018
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002198
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002196
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002192
Iteration 48/1000 | Loss: 0.00002192
Iteration 49/1000 | Loss: 0.00002191
Iteration 50/1000 | Loss: 0.00002191
Iteration 51/1000 | Loss: 0.00002191
Iteration 52/1000 | Loss: 0.00002191
Iteration 53/1000 | Loss: 0.00002190
Iteration 54/1000 | Loss: 0.00002190
Iteration 55/1000 | Loss: 0.00002190
Iteration 56/1000 | Loss: 0.00002189
Iteration 57/1000 | Loss: 0.00002189
Iteration 58/1000 | Loss: 0.00002187
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002187
Iteration 61/1000 | Loss: 0.00002186
Iteration 62/1000 | Loss: 0.00002186
Iteration 63/1000 | Loss: 0.00002186
Iteration 64/1000 | Loss: 0.00002186
Iteration 65/1000 | Loss: 0.00002185
Iteration 66/1000 | Loss: 0.00002185
Iteration 67/1000 | Loss: 0.00002185
Iteration 68/1000 | Loss: 0.00002185
Iteration 69/1000 | Loss: 0.00002185
Iteration 70/1000 | Loss: 0.00002185
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002184
Iteration 73/1000 | Loss: 0.00002184
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00002184
Iteration 76/1000 | Loss: 0.00002184
Iteration 77/1000 | Loss: 0.00002184
Iteration 78/1000 | Loss: 0.00002184
Iteration 79/1000 | Loss: 0.00002184
Iteration 80/1000 | Loss: 0.00002184
Iteration 81/1000 | Loss: 0.00002184
Iteration 82/1000 | Loss: 0.00002184
Iteration 83/1000 | Loss: 0.00002183
Iteration 84/1000 | Loss: 0.00002183
Iteration 85/1000 | Loss: 0.00002183
Iteration 86/1000 | Loss: 0.00002183
Iteration 87/1000 | Loss: 0.00002183
Iteration 88/1000 | Loss: 0.00002183
Iteration 89/1000 | Loss: 0.00002183
Iteration 90/1000 | Loss: 0.00002183
Iteration 91/1000 | Loss: 0.00002182
Iteration 92/1000 | Loss: 0.00002182
Iteration 93/1000 | Loss: 0.00002182
Iteration 94/1000 | Loss: 0.00002182
Iteration 95/1000 | Loss: 0.00002182
Iteration 96/1000 | Loss: 0.00002182
Iteration 97/1000 | Loss: 0.00002182
Iteration 98/1000 | Loss: 0.00002182
Iteration 99/1000 | Loss: 0.00002182
Iteration 100/1000 | Loss: 0.00002182
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002181
Iteration 104/1000 | Loss: 0.00002181
Iteration 105/1000 | Loss: 0.00002181
Iteration 106/1000 | Loss: 0.00002181
Iteration 107/1000 | Loss: 0.00002181
Iteration 108/1000 | Loss: 0.00002181
Iteration 109/1000 | Loss: 0.00002181
Iteration 110/1000 | Loss: 0.00002181
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002181
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002180
Iteration 119/1000 | Loss: 0.00002180
Iteration 120/1000 | Loss: 0.00002180
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Iteration 124/1000 | Loss: 0.00002180
Iteration 125/1000 | Loss: 0.00002180
Iteration 126/1000 | Loss: 0.00002180
Iteration 127/1000 | Loss: 0.00002180
Iteration 128/1000 | Loss: 0.00002180
Iteration 129/1000 | Loss: 0.00002180
Iteration 130/1000 | Loss: 0.00002180
Iteration 131/1000 | Loss: 0.00002180
Iteration 132/1000 | Loss: 0.00002180
Iteration 133/1000 | Loss: 0.00002180
Iteration 134/1000 | Loss: 0.00002180
Iteration 135/1000 | Loss: 0.00002180
Iteration 136/1000 | Loss: 0.00002179
Iteration 137/1000 | Loss: 0.00002179
Iteration 138/1000 | Loss: 0.00002179
Iteration 139/1000 | Loss: 0.00002179
Iteration 140/1000 | Loss: 0.00002179
Iteration 141/1000 | Loss: 0.00002179
Iteration 142/1000 | Loss: 0.00002179
Iteration 143/1000 | Loss: 0.00002179
Iteration 144/1000 | Loss: 0.00002179
Iteration 145/1000 | Loss: 0.00002179
Iteration 146/1000 | Loss: 0.00002179
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002178
Iteration 149/1000 | Loss: 0.00002178
Iteration 150/1000 | Loss: 0.00002178
Iteration 151/1000 | Loss: 0.00002178
Iteration 152/1000 | Loss: 0.00002178
Iteration 153/1000 | Loss: 0.00002178
Iteration 154/1000 | Loss: 0.00002177
Iteration 155/1000 | Loss: 0.00002177
Iteration 156/1000 | Loss: 0.00002177
Iteration 157/1000 | Loss: 0.00002177
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002177
Iteration 160/1000 | Loss: 0.00002177
Iteration 161/1000 | Loss: 0.00002177
Iteration 162/1000 | Loss: 0.00002177
Iteration 163/1000 | Loss: 0.00002177
Iteration 164/1000 | Loss: 0.00002177
Iteration 165/1000 | Loss: 0.00002177
Iteration 166/1000 | Loss: 0.00002177
Iteration 167/1000 | Loss: 0.00002177
Iteration 168/1000 | Loss: 0.00002177
Iteration 169/1000 | Loss: 0.00002177
Iteration 170/1000 | Loss: 0.00002177
Iteration 171/1000 | Loss: 0.00002177
Iteration 172/1000 | Loss: 0.00002177
Iteration 173/1000 | Loss: 0.00002177
Iteration 174/1000 | Loss: 0.00002177
Iteration 175/1000 | Loss: 0.00002177
Iteration 176/1000 | Loss: 0.00002177
Iteration 177/1000 | Loss: 0.00002177
Iteration 178/1000 | Loss: 0.00002177
Iteration 179/1000 | Loss: 0.00002177
Iteration 180/1000 | Loss: 0.00002177
Iteration 181/1000 | Loss: 0.00002177
Iteration 182/1000 | Loss: 0.00002177
Iteration 183/1000 | Loss: 0.00002177
Iteration 184/1000 | Loss: 0.00002177
Iteration 185/1000 | Loss: 0.00002177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.1767855287180282e-05, 2.1767855287180282e-05, 2.1767855287180282e-05, 2.1767855287180282e-05, 2.1767855287180282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1767855287180282e-05

Optimization complete. Final v2v error: 3.329517126083374 mm

Highest mean error: 11.758130073547363 mm for frame 3

Lowest mean error: 3.1202926635742188 mm for frame 131

Saving results

Total time: 89.11502122879028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440214
Iteration 2/25 | Loss: 0.00133467
Iteration 3/25 | Loss: 0.00126166
Iteration 4/25 | Loss: 0.00125145
Iteration 5/25 | Loss: 0.00124818
Iteration 6/25 | Loss: 0.00124792
Iteration 7/25 | Loss: 0.00124792
Iteration 8/25 | Loss: 0.00124792
Iteration 9/25 | Loss: 0.00124792
Iteration 10/25 | Loss: 0.00124792
Iteration 11/25 | Loss: 0.00124792
Iteration 12/25 | Loss: 0.00124792
Iteration 13/25 | Loss: 0.00124792
Iteration 14/25 | Loss: 0.00124792
Iteration 15/25 | Loss: 0.00124792
Iteration 16/25 | Loss: 0.00124792
Iteration 17/25 | Loss: 0.00124792
Iteration 18/25 | Loss: 0.00124792
Iteration 19/25 | Loss: 0.00124792
Iteration 20/25 | Loss: 0.00124792
Iteration 21/25 | Loss: 0.00124792
Iteration 22/25 | Loss: 0.00124792
Iteration 23/25 | Loss: 0.00124792
Iteration 24/25 | Loss: 0.00124792
Iteration 25/25 | Loss: 0.00124792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39165354
Iteration 2/25 | Loss: 0.00079858
Iteration 3/25 | Loss: 0.00079858
Iteration 4/25 | Loss: 0.00079858
Iteration 5/25 | Loss: 0.00079858
Iteration 6/25 | Loss: 0.00079858
Iteration 7/25 | Loss: 0.00079858
Iteration 8/25 | Loss: 0.00079858
Iteration 9/25 | Loss: 0.00079858
Iteration 10/25 | Loss: 0.00079858
Iteration 11/25 | Loss: 0.00079858
Iteration 12/25 | Loss: 0.00079858
Iteration 13/25 | Loss: 0.00079858
Iteration 14/25 | Loss: 0.00079858
Iteration 15/25 | Loss: 0.00079858
Iteration 16/25 | Loss: 0.00079858
Iteration 17/25 | Loss: 0.00079858
Iteration 18/25 | Loss: 0.00079858
Iteration 19/25 | Loss: 0.00079858
Iteration 20/25 | Loss: 0.00079858
Iteration 21/25 | Loss: 0.00079858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007985801785252988, 0.0007985801785252988, 0.0007985801785252988, 0.0007985801785252988, 0.0007985801785252988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007985801785252988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079858
Iteration 2/1000 | Loss: 0.00002943
Iteration 3/1000 | Loss: 0.00002203
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00001701
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001629
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001588
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001566
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001556
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001549
Iteration 59/1000 | Loss: 0.00001549
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001548
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001548
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001541
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001541
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001540
Iteration 89/1000 | Loss: 0.00001540
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00001539
Iteration 95/1000 | Loss: 0.00001539
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001538
Iteration 99/1000 | Loss: 0.00001538
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001537
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001536
Iteration 105/1000 | Loss: 0.00001536
Iteration 106/1000 | Loss: 0.00001536
Iteration 107/1000 | Loss: 0.00001535
Iteration 108/1000 | Loss: 0.00001535
Iteration 109/1000 | Loss: 0.00001535
Iteration 110/1000 | Loss: 0.00001535
Iteration 111/1000 | Loss: 0.00001535
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001535
Iteration 117/1000 | Loss: 0.00001534
Iteration 118/1000 | Loss: 0.00001534
Iteration 119/1000 | Loss: 0.00001534
Iteration 120/1000 | Loss: 0.00001534
Iteration 121/1000 | Loss: 0.00001534
Iteration 122/1000 | Loss: 0.00001534
Iteration 123/1000 | Loss: 0.00001533
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001533
Iteration 132/1000 | Loss: 0.00001533
Iteration 133/1000 | Loss: 0.00001533
Iteration 134/1000 | Loss: 0.00001533
Iteration 135/1000 | Loss: 0.00001533
Iteration 136/1000 | Loss: 0.00001533
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001533
Iteration 142/1000 | Loss: 0.00001533
Iteration 143/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.532978239993099e-05, 1.532978239993099e-05, 1.532978239993099e-05, 1.532978239993099e-05, 1.532978239993099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.532978239993099e-05

Optimization complete. Final v2v error: 3.3115878105163574 mm

Highest mean error: 3.6239938735961914 mm for frame 108

Lowest mean error: 3.1518807411193848 mm for frame 4

Saving results

Total time: 38.85452127456665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621271
Iteration 2/25 | Loss: 0.00153781
Iteration 3/25 | Loss: 0.00131637
Iteration 4/25 | Loss: 0.00129372
Iteration 5/25 | Loss: 0.00129132
Iteration 6/25 | Loss: 0.00129129
Iteration 7/25 | Loss: 0.00129129
Iteration 8/25 | Loss: 0.00129129
Iteration 9/25 | Loss: 0.00129129
Iteration 10/25 | Loss: 0.00129129
Iteration 11/25 | Loss: 0.00129129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012912892270833254, 0.0012912892270833254, 0.0012912892270833254, 0.0012912892270833254, 0.0012912892270833254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012912892270833254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13384390
Iteration 2/25 | Loss: 0.00069987
Iteration 3/25 | Loss: 0.00069985
Iteration 4/25 | Loss: 0.00069985
Iteration 5/25 | Loss: 0.00069985
Iteration 6/25 | Loss: 0.00069985
Iteration 7/25 | Loss: 0.00069985
Iteration 8/25 | Loss: 0.00069985
Iteration 9/25 | Loss: 0.00069985
Iteration 10/25 | Loss: 0.00069985
Iteration 11/25 | Loss: 0.00069985
Iteration 12/25 | Loss: 0.00069985
Iteration 13/25 | Loss: 0.00069985
Iteration 14/25 | Loss: 0.00069985
Iteration 15/25 | Loss: 0.00069985
Iteration 16/25 | Loss: 0.00069985
Iteration 17/25 | Loss: 0.00069985
Iteration 18/25 | Loss: 0.00069985
Iteration 19/25 | Loss: 0.00069985
Iteration 20/25 | Loss: 0.00069985
Iteration 21/25 | Loss: 0.00069985
Iteration 22/25 | Loss: 0.00069984
Iteration 23/25 | Loss: 0.00069985
Iteration 24/25 | Loss: 0.00069985
Iteration 25/25 | Loss: 0.00069984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069984
Iteration 2/1000 | Loss: 0.00003607
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001649
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001525
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001501
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001486
Iteration 19/1000 | Loss: 0.00001485
Iteration 20/1000 | Loss: 0.00001484
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001469
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001466
Iteration 32/1000 | Loss: 0.00001466
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001457
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001454
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001437
Iteration 74/1000 | Loss: 0.00001437
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001436
Iteration 81/1000 | Loss: 0.00001436
Iteration 82/1000 | Loss: 0.00001436
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001435
Iteration 85/1000 | Loss: 0.00001435
Iteration 86/1000 | Loss: 0.00001435
Iteration 87/1000 | Loss: 0.00001435
Iteration 88/1000 | Loss: 0.00001435
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001434
Iteration 91/1000 | Loss: 0.00001434
Iteration 92/1000 | Loss: 0.00001434
Iteration 93/1000 | Loss: 0.00001434
Iteration 94/1000 | Loss: 0.00001434
Iteration 95/1000 | Loss: 0.00001434
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.4337436368805356e-05, 1.4337436368805356e-05, 1.4337436368805356e-05, 1.4337436368805356e-05, 1.4337436368805356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4337436368805356e-05

Optimization complete. Final v2v error: 3.231019973754883 mm

Highest mean error: 3.648233652114868 mm for frame 230

Lowest mean error: 2.9269721508026123 mm for frame 185

Saving results

Total time: 43.61731028556824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064479
Iteration 2/25 | Loss: 0.00206741
Iteration 3/25 | Loss: 0.00181443
Iteration 4/25 | Loss: 0.00158684
Iteration 5/25 | Loss: 0.00141612
Iteration 6/25 | Loss: 0.00144695
Iteration 7/25 | Loss: 0.00131822
Iteration 8/25 | Loss: 0.00129876
Iteration 9/25 | Loss: 0.00129307
Iteration 10/25 | Loss: 0.00129035
Iteration 11/25 | Loss: 0.00128556
Iteration 12/25 | Loss: 0.00128546
Iteration 13/25 | Loss: 0.00128407
Iteration 14/25 | Loss: 0.00127564
Iteration 15/25 | Loss: 0.00127469
Iteration 16/25 | Loss: 0.00127423
Iteration 17/25 | Loss: 0.00127410
Iteration 18/25 | Loss: 0.00127409
Iteration 19/25 | Loss: 0.00127409
Iteration 20/25 | Loss: 0.00127409
Iteration 21/25 | Loss: 0.00127409
Iteration 22/25 | Loss: 0.00127409
Iteration 23/25 | Loss: 0.00127409
Iteration 24/25 | Loss: 0.00127409
Iteration 25/25 | Loss: 0.00127409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43889451
Iteration 2/25 | Loss: 0.00092513
Iteration 3/25 | Loss: 0.00092512
Iteration 4/25 | Loss: 0.00092512
Iteration 5/25 | Loss: 0.00092512
Iteration 6/25 | Loss: 0.00092512
Iteration 7/25 | Loss: 0.00092512
Iteration 8/25 | Loss: 0.00092512
Iteration 9/25 | Loss: 0.00092512
Iteration 10/25 | Loss: 0.00092512
Iteration 11/25 | Loss: 0.00092512
Iteration 12/25 | Loss: 0.00092512
Iteration 13/25 | Loss: 0.00092512
Iteration 14/25 | Loss: 0.00092512
Iteration 15/25 | Loss: 0.00092512
Iteration 16/25 | Loss: 0.00092512
Iteration 17/25 | Loss: 0.00092512
Iteration 18/25 | Loss: 0.00092512
Iteration 19/25 | Loss: 0.00092512
Iteration 20/25 | Loss: 0.00092512
Iteration 21/25 | Loss: 0.00092512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000925120955798775, 0.000925120955798775, 0.000925120955798775, 0.000925120955798775, 0.000925120955798775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000925120955798775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092512
Iteration 2/1000 | Loss: 0.00004056
Iteration 3/1000 | Loss: 0.00002842
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002271
Iteration 6/1000 | Loss: 0.00002201
Iteration 7/1000 | Loss: 0.00002138
Iteration 8/1000 | Loss: 0.00002100
Iteration 9/1000 | Loss: 0.00002078
Iteration 10/1000 | Loss: 0.00038625
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001857
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001834
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001821
Iteration 28/1000 | Loss: 0.00001821
Iteration 29/1000 | Loss: 0.00001820
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001793
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001773
Iteration 39/1000 | Loss: 0.00001772
Iteration 40/1000 | Loss: 0.00001772
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001771
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001769
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001767
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001766
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001765
Iteration 69/1000 | Loss: 0.00001765
Iteration 70/1000 | Loss: 0.00001765
Iteration 71/1000 | Loss: 0.00001765
Iteration 72/1000 | Loss: 0.00001764
Iteration 73/1000 | Loss: 0.00001764
Iteration 74/1000 | Loss: 0.00001764
Iteration 75/1000 | Loss: 0.00001764
Iteration 76/1000 | Loss: 0.00001763
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001763
Iteration 79/1000 | Loss: 0.00001763
Iteration 80/1000 | Loss: 0.00001763
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001761
Iteration 94/1000 | Loss: 0.00001761
Iteration 95/1000 | Loss: 0.00001761
Iteration 96/1000 | Loss: 0.00001761
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.7609050701139495e-05, 1.7609050701139495e-05, 1.7609050701139495e-05, 1.7609050701139495e-05, 1.7609050701139495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7609050701139495e-05

Optimization complete. Final v2v error: 3.533381223678589 mm

Highest mean error: 4.673056125640869 mm for frame 66

Lowest mean error: 3.1801235675811768 mm for frame 0

Saving results

Total time: 63.624661445617676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968377
Iteration 2/25 | Loss: 0.00220815
Iteration 3/25 | Loss: 0.00172925
Iteration 4/25 | Loss: 0.00182065
Iteration 5/25 | Loss: 0.00149716
Iteration 6/25 | Loss: 0.00144152
Iteration 7/25 | Loss: 0.00143663
Iteration 8/25 | Loss: 0.00140379
Iteration 9/25 | Loss: 0.00137455
Iteration 10/25 | Loss: 0.00136132
Iteration 11/25 | Loss: 0.00135261
Iteration 12/25 | Loss: 0.00134693
Iteration 13/25 | Loss: 0.00134534
Iteration 14/25 | Loss: 0.00134351
Iteration 15/25 | Loss: 0.00134315
Iteration 16/25 | Loss: 0.00134184
Iteration 17/25 | Loss: 0.00134121
Iteration 18/25 | Loss: 0.00134224
Iteration 19/25 | Loss: 0.00134069
Iteration 20/25 | Loss: 0.00134176
Iteration 21/25 | Loss: 0.00133983
Iteration 22/25 | Loss: 0.00133986
Iteration 23/25 | Loss: 0.00133963
Iteration 24/25 | Loss: 0.00134000
Iteration 25/25 | Loss: 0.00134159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40562594
Iteration 2/25 | Loss: 0.00077978
Iteration 3/25 | Loss: 0.00077978
Iteration 4/25 | Loss: 0.00077978
Iteration 5/25 | Loss: 0.00077978
Iteration 6/25 | Loss: 0.00077978
Iteration 7/25 | Loss: 0.00077978
Iteration 8/25 | Loss: 0.00077978
Iteration 9/25 | Loss: 0.00077978
Iteration 10/25 | Loss: 0.00077978
Iteration 11/25 | Loss: 0.00077978
Iteration 12/25 | Loss: 0.00077978
Iteration 13/25 | Loss: 0.00077978
Iteration 14/25 | Loss: 0.00077978
Iteration 15/25 | Loss: 0.00077978
Iteration 16/25 | Loss: 0.00077978
Iteration 17/25 | Loss: 0.00077978
Iteration 18/25 | Loss: 0.00077978
Iteration 19/25 | Loss: 0.00077978
Iteration 20/25 | Loss: 0.00077978
Iteration 21/25 | Loss: 0.00077978
Iteration 22/25 | Loss: 0.00077978
Iteration 23/25 | Loss: 0.00077978
Iteration 24/25 | Loss: 0.00077978
Iteration 25/25 | Loss: 0.00077978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077978
Iteration 2/1000 | Loss: 0.00005800
Iteration 3/1000 | Loss: 0.00006212
Iteration 4/1000 | Loss: 0.00004808
Iteration 5/1000 | Loss: 0.00003596
Iteration 6/1000 | Loss: 0.00004827
Iteration 7/1000 | Loss: 0.00004245
Iteration 8/1000 | Loss: 0.00004947
Iteration 9/1000 | Loss: 0.00005101
Iteration 10/1000 | Loss: 0.00006445
Iteration 11/1000 | Loss: 0.00006135
Iteration 12/1000 | Loss: 0.00006242
Iteration 13/1000 | Loss: 0.00008349
Iteration 14/1000 | Loss: 0.00007118
Iteration 15/1000 | Loss: 0.00006747
Iteration 16/1000 | Loss: 0.00007588
Iteration 17/1000 | Loss: 0.00006554
Iteration 18/1000 | Loss: 0.00006570
Iteration 19/1000 | Loss: 0.00006836
Iteration 20/1000 | Loss: 0.00006298
Iteration 21/1000 | Loss: 0.00006340
Iteration 22/1000 | Loss: 0.00005848
Iteration 23/1000 | Loss: 0.00006548
Iteration 24/1000 | Loss: 0.00006205
Iteration 25/1000 | Loss: 0.00006264
Iteration 26/1000 | Loss: 0.00006775
Iteration 27/1000 | Loss: 0.00006960
Iteration 28/1000 | Loss: 0.00005762
Iteration 29/1000 | Loss: 0.00007677
Iteration 30/1000 | Loss: 0.00006085
Iteration 31/1000 | Loss: 0.00005561
Iteration 32/1000 | Loss: 0.00006090
Iteration 33/1000 | Loss: 0.00006805
Iteration 34/1000 | Loss: 0.00006550
Iteration 35/1000 | Loss: 0.00007385
Iteration 36/1000 | Loss: 0.00008873
Iteration 37/1000 | Loss: 0.00047666
Iteration 38/1000 | Loss: 0.00007361
Iteration 39/1000 | Loss: 0.00007250
Iteration 40/1000 | Loss: 0.00009219
Iteration 41/1000 | Loss: 0.00034493
Iteration 42/1000 | Loss: 0.00024658
Iteration 43/1000 | Loss: 0.00027458
Iteration 44/1000 | Loss: 0.00022368
Iteration 45/1000 | Loss: 0.00025488
Iteration 46/1000 | Loss: 0.00025391
Iteration 47/1000 | Loss: 0.00006592
Iteration 48/1000 | Loss: 0.00007620
Iteration 49/1000 | Loss: 0.00005920
Iteration 50/1000 | Loss: 0.00006225
Iteration 51/1000 | Loss: 0.00005810
Iteration 52/1000 | Loss: 0.00024017
Iteration 53/1000 | Loss: 0.00006514
Iteration 54/1000 | Loss: 0.00007028
Iteration 55/1000 | Loss: 0.00006030
Iteration 56/1000 | Loss: 0.00006420
Iteration 57/1000 | Loss: 0.00006531
Iteration 58/1000 | Loss: 0.00007908
Iteration 59/1000 | Loss: 0.00008427
Iteration 60/1000 | Loss: 0.00006548
Iteration 61/1000 | Loss: 0.00005713
Iteration 62/1000 | Loss: 0.00005446
Iteration 63/1000 | Loss: 0.00012133
Iteration 64/1000 | Loss: 0.00013922
Iteration 65/1000 | Loss: 0.00011427
Iteration 66/1000 | Loss: 0.00010600
Iteration 67/1000 | Loss: 0.00007016
Iteration 68/1000 | Loss: 0.00007740
Iteration 69/1000 | Loss: 0.00007802
Iteration 70/1000 | Loss: 0.00007215
Iteration 71/1000 | Loss: 0.00006297
Iteration 72/1000 | Loss: 0.00008723
Iteration 73/1000 | Loss: 0.00005594
Iteration 74/1000 | Loss: 0.00006360
Iteration 75/1000 | Loss: 0.00003817
Iteration 76/1000 | Loss: 0.00005215
Iteration 77/1000 | Loss: 0.00005718
Iteration 78/1000 | Loss: 0.00008756
Iteration 79/1000 | Loss: 0.00005922
Iteration 80/1000 | Loss: 0.00004795
Iteration 81/1000 | Loss: 0.00004148
Iteration 82/1000 | Loss: 0.00004960
Iteration 83/1000 | Loss: 0.00003915
Iteration 84/1000 | Loss: 0.00003326
Iteration 85/1000 | Loss: 0.00002676
Iteration 86/1000 | Loss: 0.00003084
Iteration 87/1000 | Loss: 0.00003661
Iteration 88/1000 | Loss: 0.00002874
Iteration 89/1000 | Loss: 0.00003888
Iteration 90/1000 | Loss: 0.00003709
Iteration 91/1000 | Loss: 0.00003602
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00003807
Iteration 94/1000 | Loss: 0.00003845
Iteration 95/1000 | Loss: 0.00003455
Iteration 96/1000 | Loss: 0.00003599
Iteration 97/1000 | Loss: 0.00004837
Iteration 98/1000 | Loss: 0.00004771
Iteration 99/1000 | Loss: 0.00003445
Iteration 100/1000 | Loss: 0.00004837
Iteration 101/1000 | Loss: 0.00004078
Iteration 102/1000 | Loss: 0.00004093
Iteration 103/1000 | Loss: 0.00004740
Iteration 104/1000 | Loss: 0.00003770
Iteration 105/1000 | Loss: 0.00002873
Iteration 106/1000 | Loss: 0.00003123
Iteration 107/1000 | Loss: 0.00002653
Iteration 108/1000 | Loss: 0.00003455
Iteration 109/1000 | Loss: 0.00003204
Iteration 110/1000 | Loss: 0.00003762
Iteration 111/1000 | Loss: 0.00003531
Iteration 112/1000 | Loss: 0.00003251
Iteration 113/1000 | Loss: 0.00003556
Iteration 114/1000 | Loss: 0.00002495
Iteration 115/1000 | Loss: 0.00002882
Iteration 116/1000 | Loss: 0.00002934
Iteration 117/1000 | Loss: 0.00002631
Iteration 118/1000 | Loss: 0.00003687
Iteration 119/1000 | Loss: 0.00002489
Iteration 120/1000 | Loss: 0.00002427
Iteration 121/1000 | Loss: 0.00002393
Iteration 122/1000 | Loss: 0.00002358
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002336
Iteration 127/1000 | Loss: 0.00002328
Iteration 128/1000 | Loss: 0.00002326
Iteration 129/1000 | Loss: 0.00002326
Iteration 130/1000 | Loss: 0.00002326
Iteration 131/1000 | Loss: 0.00002325
Iteration 132/1000 | Loss: 0.00002325
Iteration 133/1000 | Loss: 0.00002325
Iteration 134/1000 | Loss: 0.00002325
Iteration 135/1000 | Loss: 0.00002325
Iteration 136/1000 | Loss: 0.00002325
Iteration 137/1000 | Loss: 0.00002325
Iteration 138/1000 | Loss: 0.00002323
Iteration 139/1000 | Loss: 0.00002319
Iteration 140/1000 | Loss: 0.00002318
Iteration 141/1000 | Loss: 0.00002318
Iteration 142/1000 | Loss: 0.00002318
Iteration 143/1000 | Loss: 0.00002318
Iteration 144/1000 | Loss: 0.00002318
Iteration 145/1000 | Loss: 0.00002315
Iteration 146/1000 | Loss: 0.00002315
Iteration 147/1000 | Loss: 0.00002315
Iteration 148/1000 | Loss: 0.00002314
Iteration 149/1000 | Loss: 0.00002314
Iteration 150/1000 | Loss: 0.00002314
Iteration 151/1000 | Loss: 0.00002313
Iteration 152/1000 | Loss: 0.00002306
Iteration 153/1000 | Loss: 0.00002306
Iteration 154/1000 | Loss: 0.00002305
Iteration 155/1000 | Loss: 0.00002304
Iteration 156/1000 | Loss: 0.00002303
Iteration 157/1000 | Loss: 0.00002303
Iteration 158/1000 | Loss: 0.00002302
Iteration 159/1000 | Loss: 0.00002301
Iteration 160/1000 | Loss: 0.00002300
Iteration 161/1000 | Loss: 0.00002300
Iteration 162/1000 | Loss: 0.00002299
Iteration 163/1000 | Loss: 0.00002299
Iteration 164/1000 | Loss: 0.00002298
Iteration 165/1000 | Loss: 0.00002297
Iteration 166/1000 | Loss: 0.00002297
Iteration 167/1000 | Loss: 0.00002297
Iteration 168/1000 | Loss: 0.00002297
Iteration 169/1000 | Loss: 0.00002297
Iteration 170/1000 | Loss: 0.00002297
Iteration 171/1000 | Loss: 0.00002296
Iteration 172/1000 | Loss: 0.00002296
Iteration 173/1000 | Loss: 0.00002296
Iteration 174/1000 | Loss: 0.00002296
Iteration 175/1000 | Loss: 0.00002296
Iteration 176/1000 | Loss: 0.00002296
Iteration 177/1000 | Loss: 0.00002295
Iteration 178/1000 | Loss: 0.00002295
Iteration 179/1000 | Loss: 0.00002295
Iteration 180/1000 | Loss: 0.00002295
Iteration 181/1000 | Loss: 0.00002295
Iteration 182/1000 | Loss: 0.00002295
Iteration 183/1000 | Loss: 0.00002295
Iteration 184/1000 | Loss: 0.00002295
Iteration 185/1000 | Loss: 0.00002295
Iteration 186/1000 | Loss: 0.00002294
Iteration 187/1000 | Loss: 0.00002294
Iteration 188/1000 | Loss: 0.00002294
Iteration 189/1000 | Loss: 0.00002294
Iteration 190/1000 | Loss: 0.00002294
Iteration 191/1000 | Loss: 0.00002294
Iteration 192/1000 | Loss: 0.00002294
Iteration 193/1000 | Loss: 0.00002294
Iteration 194/1000 | Loss: 0.00002294
Iteration 195/1000 | Loss: 0.00002294
Iteration 196/1000 | Loss: 0.00002294
Iteration 197/1000 | Loss: 0.00002294
Iteration 198/1000 | Loss: 0.00002294
Iteration 199/1000 | Loss: 0.00002294
Iteration 200/1000 | Loss: 0.00002293
Iteration 201/1000 | Loss: 0.00002293
Iteration 202/1000 | Loss: 0.00002293
Iteration 203/1000 | Loss: 0.00002292
Iteration 204/1000 | Loss: 0.00002292
Iteration 205/1000 | Loss: 0.00002292
Iteration 206/1000 | Loss: 0.00002292
Iteration 207/1000 | Loss: 0.00002292
Iteration 208/1000 | Loss: 0.00002291
Iteration 209/1000 | Loss: 0.00002291
Iteration 210/1000 | Loss: 0.00002291
Iteration 211/1000 | Loss: 0.00002291
Iteration 212/1000 | Loss: 0.00002291
Iteration 213/1000 | Loss: 0.00002291
Iteration 214/1000 | Loss: 0.00002291
Iteration 215/1000 | Loss: 0.00002291
Iteration 216/1000 | Loss: 0.00002291
Iteration 217/1000 | Loss: 0.00002291
Iteration 218/1000 | Loss: 0.00002291
Iteration 219/1000 | Loss: 0.00002291
Iteration 220/1000 | Loss: 0.00002291
Iteration 221/1000 | Loss: 0.00002291
Iteration 222/1000 | Loss: 0.00002291
Iteration 223/1000 | Loss: 0.00002291
Iteration 224/1000 | Loss: 0.00002291
Iteration 225/1000 | Loss: 0.00002291
Iteration 226/1000 | Loss: 0.00002291
Iteration 227/1000 | Loss: 0.00002291
Iteration 228/1000 | Loss: 0.00002291
Iteration 229/1000 | Loss: 0.00002291
Iteration 230/1000 | Loss: 0.00002291
Iteration 231/1000 | Loss: 0.00002291
Iteration 232/1000 | Loss: 0.00002291
Iteration 233/1000 | Loss: 0.00002291
Iteration 234/1000 | Loss: 0.00002291
Iteration 235/1000 | Loss: 0.00002291
Iteration 236/1000 | Loss: 0.00002291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.290519478265196e-05, 2.290519478265196e-05, 2.290519478265196e-05, 2.290519478265196e-05, 2.290519478265196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.290519478265196e-05

Optimization complete. Final v2v error: 4.010542869567871 mm

Highest mean error: 4.936347484588623 mm for frame 213

Lowest mean error: 3.851517677307129 mm for frame 223

Saving results

Total time: 258.79256987571716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510128
Iteration 2/25 | Loss: 0.00139447
Iteration 3/25 | Loss: 0.00130077
Iteration 4/25 | Loss: 0.00128244
Iteration 5/25 | Loss: 0.00127789
Iteration 6/25 | Loss: 0.00127723
Iteration 7/25 | Loss: 0.00127723
Iteration 8/25 | Loss: 0.00127723
Iteration 9/25 | Loss: 0.00127723
Iteration 10/25 | Loss: 0.00127723
Iteration 11/25 | Loss: 0.00127723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012772282352671027, 0.0012772282352671027, 0.0012772282352671027, 0.0012772282352671027, 0.0012772282352671027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012772282352671027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24960089
Iteration 2/25 | Loss: 0.00084046
Iteration 3/25 | Loss: 0.00084042
Iteration 4/25 | Loss: 0.00084042
Iteration 5/25 | Loss: 0.00084042
Iteration 6/25 | Loss: 0.00084042
Iteration 7/25 | Loss: 0.00084042
Iteration 8/25 | Loss: 0.00084042
Iteration 9/25 | Loss: 0.00084042
Iteration 10/25 | Loss: 0.00084042
Iteration 11/25 | Loss: 0.00084042
Iteration 12/25 | Loss: 0.00084042
Iteration 13/25 | Loss: 0.00084042
Iteration 14/25 | Loss: 0.00084042
Iteration 15/25 | Loss: 0.00084042
Iteration 16/25 | Loss: 0.00084042
Iteration 17/25 | Loss: 0.00084042
Iteration 18/25 | Loss: 0.00084042
Iteration 19/25 | Loss: 0.00084042
Iteration 20/25 | Loss: 0.00084042
Iteration 21/25 | Loss: 0.00084042
Iteration 22/25 | Loss: 0.00084042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008404219406656921, 0.0008404219406656921, 0.0008404219406656921, 0.0008404219406656921, 0.0008404219406656921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008404219406656921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084042
Iteration 2/1000 | Loss: 0.00003309
Iteration 3/1000 | Loss: 0.00002189
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001873
Iteration 6/1000 | Loss: 0.00001780
Iteration 7/1000 | Loss: 0.00001718
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001638
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001583
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001556
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001551
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001545
Iteration 39/1000 | Loss: 0.00001545
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001543
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001542
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001542
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001541
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001540
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001539
Iteration 58/1000 | Loss: 0.00001539
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001536
Iteration 72/1000 | Loss: 0.00001536
Iteration 73/1000 | Loss: 0.00001535
Iteration 74/1000 | Loss: 0.00001535
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001533
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001531
Iteration 92/1000 | Loss: 0.00001531
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001530
Iteration 95/1000 | Loss: 0.00001530
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001529
Iteration 98/1000 | Loss: 0.00001529
Iteration 99/1000 | Loss: 0.00001528
Iteration 100/1000 | Loss: 0.00001528
Iteration 101/1000 | Loss: 0.00001528
Iteration 102/1000 | Loss: 0.00001528
Iteration 103/1000 | Loss: 0.00001528
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001527
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001526
Iteration 117/1000 | Loss: 0.00001526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5262627130141482e-05, 1.5262627130141482e-05, 1.5262627130141482e-05, 1.5262627130141482e-05, 1.5262627130141482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5262627130141482e-05

Optimization complete. Final v2v error: 3.3287672996520996 mm

Highest mean error: 3.6740167140960693 mm for frame 155

Lowest mean error: 2.832878828048706 mm for frame 31

Saving results

Total time: 37.2458381652832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967584
Iteration 2/25 | Loss: 0.00382560
Iteration 3/25 | Loss: 0.00289913
Iteration 4/25 | Loss: 0.00246180
Iteration 5/25 | Loss: 0.00252894
Iteration 6/25 | Loss: 0.00232811
Iteration 7/25 | Loss: 0.00233031
Iteration 8/25 | Loss: 0.00215119
Iteration 9/25 | Loss: 0.00205180
Iteration 10/25 | Loss: 0.00201041
Iteration 11/25 | Loss: 0.00195825
Iteration 12/25 | Loss: 0.00191306
Iteration 13/25 | Loss: 0.00188736
Iteration 14/25 | Loss: 0.00184712
Iteration 15/25 | Loss: 0.00189485
Iteration 16/25 | Loss: 0.00190954
Iteration 17/25 | Loss: 0.00184995
Iteration 18/25 | Loss: 0.00182763
Iteration 19/25 | Loss: 0.00182209
Iteration 20/25 | Loss: 0.00180865
Iteration 21/25 | Loss: 0.00180047
Iteration 22/25 | Loss: 0.00181691
Iteration 23/25 | Loss: 0.00181611
Iteration 24/25 | Loss: 0.00178866
Iteration 25/25 | Loss: 0.00177836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04465532
Iteration 2/25 | Loss: 0.01331097
Iteration 3/25 | Loss: 0.00714200
Iteration 4/25 | Loss: 0.00713931
Iteration 5/25 | Loss: 0.00713931
Iteration 6/25 | Loss: 0.00713931
Iteration 7/25 | Loss: 0.00713931
Iteration 8/25 | Loss: 0.00713931
Iteration 9/25 | Loss: 0.00713931
Iteration 10/25 | Loss: 0.00713931
Iteration 11/25 | Loss: 0.00713931
Iteration 12/25 | Loss: 0.00713931
Iteration 13/25 | Loss: 0.00713931
Iteration 14/25 | Loss: 0.00713931
Iteration 15/25 | Loss: 0.00713931
Iteration 16/25 | Loss: 0.00713931
Iteration 17/25 | Loss: 0.00713931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.007139306981116533, 0.007139306981116533, 0.007139306981116533, 0.007139306981116533, 0.007139306981116533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007139306981116533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00713931
Iteration 2/1000 | Loss: 0.00536908
Iteration 3/1000 | Loss: 0.00297256
Iteration 4/1000 | Loss: 0.00311387
Iteration 5/1000 | Loss: 0.00365654
Iteration 6/1000 | Loss: 0.00334825
Iteration 7/1000 | Loss: 0.00299043
Iteration 8/1000 | Loss: 0.00288113
Iteration 9/1000 | Loss: 0.00480034
Iteration 10/1000 | Loss: 0.00354213
Iteration 11/1000 | Loss: 0.00305807
Iteration 12/1000 | Loss: 0.00291839
Iteration 13/1000 | Loss: 0.00244535
Iteration 14/1000 | Loss: 0.00532522
Iteration 15/1000 | Loss: 0.00351036
Iteration 16/1000 | Loss: 0.00183161
Iteration 17/1000 | Loss: 0.00259424
Iteration 18/1000 | Loss: 0.00212964
Iteration 19/1000 | Loss: 0.00189782
Iteration 20/1000 | Loss: 0.00555499
Iteration 21/1000 | Loss: 0.00422006
Iteration 22/1000 | Loss: 0.00214241
Iteration 23/1000 | Loss: 0.00647934
Iteration 24/1000 | Loss: 0.00036656
Iteration 25/1000 | Loss: 0.00064766
Iteration 26/1000 | Loss: 0.00099771
Iteration 27/1000 | Loss: 0.00656723
Iteration 28/1000 | Loss: 0.00078803
Iteration 29/1000 | Loss: 0.00027862
Iteration 30/1000 | Loss: 0.00116697
Iteration 31/1000 | Loss: 0.00909112
Iteration 32/1000 | Loss: 0.01493455
Iteration 33/1000 | Loss: 0.00183141
Iteration 34/1000 | Loss: 0.00023328
Iteration 35/1000 | Loss: 0.00225490
Iteration 36/1000 | Loss: 0.00551327
Iteration 37/1000 | Loss: 0.00342273
Iteration 38/1000 | Loss: 0.00061125
Iteration 39/1000 | Loss: 0.00115968
Iteration 40/1000 | Loss: 0.00508968
Iteration 41/1000 | Loss: 0.00047046
Iteration 42/1000 | Loss: 0.00021454
Iteration 43/1000 | Loss: 0.00442567
Iteration 44/1000 | Loss: 0.00020436
Iteration 45/1000 | Loss: 0.00117408
Iteration 46/1000 | Loss: 0.00015893
Iteration 47/1000 | Loss: 0.00014171
Iteration 48/1000 | Loss: 0.00105569
Iteration 49/1000 | Loss: 0.00124589
Iteration 50/1000 | Loss: 0.00014294
Iteration 51/1000 | Loss: 0.00012695
Iteration 52/1000 | Loss: 0.00276474
Iteration 53/1000 | Loss: 0.00014039
Iteration 54/1000 | Loss: 0.00137780
Iteration 55/1000 | Loss: 0.00560157
Iteration 56/1000 | Loss: 0.00015690
Iteration 57/1000 | Loss: 0.00012216
Iteration 58/1000 | Loss: 0.00306765
Iteration 59/1000 | Loss: 0.00181785
Iteration 60/1000 | Loss: 0.00012889
Iteration 61/1000 | Loss: 0.00010017
Iteration 62/1000 | Loss: 0.00094711
Iteration 63/1000 | Loss: 0.00008938
Iteration 64/1000 | Loss: 0.00007948
Iteration 65/1000 | Loss: 0.00053263
Iteration 66/1000 | Loss: 0.00022725
Iteration 67/1000 | Loss: 0.00030909
Iteration 68/1000 | Loss: 0.00007577
Iteration 69/1000 | Loss: 0.00007064
Iteration 70/1000 | Loss: 0.00006797
Iteration 71/1000 | Loss: 0.00006594
Iteration 72/1000 | Loss: 0.00006429
Iteration 73/1000 | Loss: 0.00006315
Iteration 74/1000 | Loss: 0.00006230
Iteration 75/1000 | Loss: 0.00066048
Iteration 76/1000 | Loss: 0.00067159
Iteration 77/1000 | Loss: 0.00064761
Iteration 78/1000 | Loss: 0.00485186
Iteration 79/1000 | Loss: 0.00141583
Iteration 80/1000 | Loss: 0.00011554
Iteration 81/1000 | Loss: 0.00014486
Iteration 82/1000 | Loss: 0.00292504
Iteration 83/1000 | Loss: 0.00012002
Iteration 84/1000 | Loss: 0.00114506
Iteration 85/1000 | Loss: 0.00006017
Iteration 86/1000 | Loss: 0.00005265
Iteration 87/1000 | Loss: 0.00004863
Iteration 88/1000 | Loss: 0.00004674
Iteration 89/1000 | Loss: 0.00004498
Iteration 90/1000 | Loss: 0.00004351
Iteration 91/1000 | Loss: 0.00004266
Iteration 92/1000 | Loss: 0.00004185
Iteration 93/1000 | Loss: 0.00004123
Iteration 94/1000 | Loss: 0.00004074
Iteration 95/1000 | Loss: 0.00097239
Iteration 96/1000 | Loss: 0.00057581
Iteration 97/1000 | Loss: 0.00005207
Iteration 98/1000 | Loss: 0.00004072
Iteration 99/1000 | Loss: 0.00003859
Iteration 100/1000 | Loss: 0.00003681
Iteration 101/1000 | Loss: 0.00003514
Iteration 102/1000 | Loss: 0.00003416
Iteration 103/1000 | Loss: 0.00003335
Iteration 104/1000 | Loss: 0.00003279
Iteration 105/1000 | Loss: 0.00003233
Iteration 106/1000 | Loss: 0.00003187
Iteration 107/1000 | Loss: 0.00003154
Iteration 108/1000 | Loss: 0.00003116
Iteration 109/1000 | Loss: 0.00003089
Iteration 110/1000 | Loss: 0.00003066
Iteration 111/1000 | Loss: 0.00003046
Iteration 112/1000 | Loss: 0.00003027
Iteration 113/1000 | Loss: 0.00003014
Iteration 114/1000 | Loss: 0.00003006
Iteration 115/1000 | Loss: 0.00003000
Iteration 116/1000 | Loss: 0.00002998
Iteration 117/1000 | Loss: 0.00002991
Iteration 118/1000 | Loss: 0.00002989
Iteration 119/1000 | Loss: 0.00002989
Iteration 120/1000 | Loss: 0.00002988
Iteration 121/1000 | Loss: 0.00002988
Iteration 122/1000 | Loss: 0.00002987
Iteration 123/1000 | Loss: 0.00002987
Iteration 124/1000 | Loss: 0.00002986
Iteration 125/1000 | Loss: 0.00002985
Iteration 126/1000 | Loss: 0.00002985
Iteration 127/1000 | Loss: 0.00002985
Iteration 128/1000 | Loss: 0.00002984
Iteration 129/1000 | Loss: 0.00002984
Iteration 130/1000 | Loss: 0.00002984
Iteration 131/1000 | Loss: 0.00002984
Iteration 132/1000 | Loss: 0.00002984
Iteration 133/1000 | Loss: 0.00002984
Iteration 134/1000 | Loss: 0.00002984
Iteration 135/1000 | Loss: 0.00002983
Iteration 136/1000 | Loss: 0.00002983
Iteration 137/1000 | Loss: 0.00002983
Iteration 138/1000 | Loss: 0.00002983
Iteration 139/1000 | Loss: 0.00002983
Iteration 140/1000 | Loss: 0.00002983
Iteration 141/1000 | Loss: 0.00002982
Iteration 142/1000 | Loss: 0.00002982
Iteration 143/1000 | Loss: 0.00002982
Iteration 144/1000 | Loss: 0.00002982
Iteration 145/1000 | Loss: 0.00002981
Iteration 146/1000 | Loss: 0.00002981
Iteration 147/1000 | Loss: 0.00002981
Iteration 148/1000 | Loss: 0.00002980
Iteration 149/1000 | Loss: 0.00002980
Iteration 150/1000 | Loss: 0.00002979
Iteration 151/1000 | Loss: 0.00002979
Iteration 152/1000 | Loss: 0.00002977
Iteration 153/1000 | Loss: 0.00002977
Iteration 154/1000 | Loss: 0.00002977
Iteration 155/1000 | Loss: 0.00002977
Iteration 156/1000 | Loss: 0.00002977
Iteration 157/1000 | Loss: 0.00002977
Iteration 158/1000 | Loss: 0.00002977
Iteration 159/1000 | Loss: 0.00002976
Iteration 160/1000 | Loss: 0.00002976
Iteration 161/1000 | Loss: 0.00002975
Iteration 162/1000 | Loss: 0.00002975
Iteration 163/1000 | Loss: 0.00002975
Iteration 164/1000 | Loss: 0.00002975
Iteration 165/1000 | Loss: 0.00002975
Iteration 166/1000 | Loss: 0.00002974
Iteration 167/1000 | Loss: 0.00002974
Iteration 168/1000 | Loss: 0.00002974
Iteration 169/1000 | Loss: 0.00002974
Iteration 170/1000 | Loss: 0.00002974
Iteration 171/1000 | Loss: 0.00002974
Iteration 172/1000 | Loss: 0.00002974
Iteration 173/1000 | Loss: 0.00002974
Iteration 174/1000 | Loss: 0.00002974
Iteration 175/1000 | Loss: 0.00002974
Iteration 176/1000 | Loss: 0.00002974
Iteration 177/1000 | Loss: 0.00002974
Iteration 178/1000 | Loss: 0.00002974
Iteration 179/1000 | Loss: 0.00002974
Iteration 180/1000 | Loss: 0.00002973
Iteration 181/1000 | Loss: 0.00002973
Iteration 182/1000 | Loss: 0.00002972
Iteration 183/1000 | Loss: 0.00002972
Iteration 184/1000 | Loss: 0.00002972
Iteration 185/1000 | Loss: 0.00002972
Iteration 186/1000 | Loss: 0.00002972
Iteration 187/1000 | Loss: 0.00002971
Iteration 188/1000 | Loss: 0.00002971
Iteration 189/1000 | Loss: 0.00002971
Iteration 190/1000 | Loss: 0.00002971
Iteration 191/1000 | Loss: 0.00002971
Iteration 192/1000 | Loss: 0.00002970
Iteration 193/1000 | Loss: 0.00002970
Iteration 194/1000 | Loss: 0.00002970
Iteration 195/1000 | Loss: 0.00002970
Iteration 196/1000 | Loss: 0.00002970
Iteration 197/1000 | Loss: 0.00002970
Iteration 198/1000 | Loss: 0.00002969
Iteration 199/1000 | Loss: 0.00002969
Iteration 200/1000 | Loss: 0.00002969
Iteration 201/1000 | Loss: 0.00002969
Iteration 202/1000 | Loss: 0.00002969
Iteration 203/1000 | Loss: 0.00002969
Iteration 204/1000 | Loss: 0.00002969
Iteration 205/1000 | Loss: 0.00002969
Iteration 206/1000 | Loss: 0.00002968
Iteration 207/1000 | Loss: 0.00002968
Iteration 208/1000 | Loss: 0.00002968
Iteration 209/1000 | Loss: 0.00002968
Iteration 210/1000 | Loss: 0.00002968
Iteration 211/1000 | Loss: 0.00002968
Iteration 212/1000 | Loss: 0.00002968
Iteration 213/1000 | Loss: 0.00002967
Iteration 214/1000 | Loss: 0.00002967
Iteration 215/1000 | Loss: 0.00002967
Iteration 216/1000 | Loss: 0.00002967
Iteration 217/1000 | Loss: 0.00002967
Iteration 218/1000 | Loss: 0.00002967
Iteration 219/1000 | Loss: 0.00002967
Iteration 220/1000 | Loss: 0.00002966
Iteration 221/1000 | Loss: 0.00002966
Iteration 222/1000 | Loss: 0.00002966
Iteration 223/1000 | Loss: 0.00002966
Iteration 224/1000 | Loss: 0.00002966
Iteration 225/1000 | Loss: 0.00002965
Iteration 226/1000 | Loss: 0.00002965
Iteration 227/1000 | Loss: 0.00002965
Iteration 228/1000 | Loss: 0.00002965
Iteration 229/1000 | Loss: 0.00002964
Iteration 230/1000 | Loss: 0.00002964
Iteration 231/1000 | Loss: 0.00002964
Iteration 232/1000 | Loss: 0.00002964
Iteration 233/1000 | Loss: 0.00002963
Iteration 234/1000 | Loss: 0.00002963
Iteration 235/1000 | Loss: 0.00002963
Iteration 236/1000 | Loss: 0.00002962
Iteration 237/1000 | Loss: 0.00002962
Iteration 238/1000 | Loss: 0.00002962
Iteration 239/1000 | Loss: 0.00002962
Iteration 240/1000 | Loss: 0.00002962
Iteration 241/1000 | Loss: 0.00002962
Iteration 242/1000 | Loss: 0.00002962
Iteration 243/1000 | Loss: 0.00002962
Iteration 244/1000 | Loss: 0.00002962
Iteration 245/1000 | Loss: 0.00002961
Iteration 246/1000 | Loss: 0.00002961
Iteration 247/1000 | Loss: 0.00002961
Iteration 248/1000 | Loss: 0.00002961
Iteration 249/1000 | Loss: 0.00002961
Iteration 250/1000 | Loss: 0.00002960
Iteration 251/1000 | Loss: 0.00002960
Iteration 252/1000 | Loss: 0.00002960
Iteration 253/1000 | Loss: 0.00002960
Iteration 254/1000 | Loss: 0.00002960
Iteration 255/1000 | Loss: 0.00002960
Iteration 256/1000 | Loss: 0.00002960
Iteration 257/1000 | Loss: 0.00002960
Iteration 258/1000 | Loss: 0.00002960
Iteration 259/1000 | Loss: 0.00002960
Iteration 260/1000 | Loss: 0.00002960
Iteration 261/1000 | Loss: 0.00002960
Iteration 262/1000 | Loss: 0.00002960
Iteration 263/1000 | Loss: 0.00002959
Iteration 264/1000 | Loss: 0.00002959
Iteration 265/1000 | Loss: 0.00002959
Iteration 266/1000 | Loss: 0.00002959
Iteration 267/1000 | Loss: 0.00002959
Iteration 268/1000 | Loss: 0.00002959
Iteration 269/1000 | Loss: 0.00002959
Iteration 270/1000 | Loss: 0.00002959
Iteration 271/1000 | Loss: 0.00002958
Iteration 272/1000 | Loss: 0.00002958
Iteration 273/1000 | Loss: 0.00002958
Iteration 274/1000 | Loss: 0.00002958
Iteration 275/1000 | Loss: 0.00002958
Iteration 276/1000 | Loss: 0.00002958
Iteration 277/1000 | Loss: 0.00002958
Iteration 278/1000 | Loss: 0.00002958
Iteration 279/1000 | Loss: 0.00002958
Iteration 280/1000 | Loss: 0.00002958
Iteration 281/1000 | Loss: 0.00002958
Iteration 282/1000 | Loss: 0.00002958
Iteration 283/1000 | Loss: 0.00002958
Iteration 284/1000 | Loss: 0.00002958
Iteration 285/1000 | Loss: 0.00002957
Iteration 286/1000 | Loss: 0.00002957
Iteration 287/1000 | Loss: 0.00002957
Iteration 288/1000 | Loss: 0.00002957
Iteration 289/1000 | Loss: 0.00002957
Iteration 290/1000 | Loss: 0.00002957
Iteration 291/1000 | Loss: 0.00002957
Iteration 292/1000 | Loss: 0.00002957
Iteration 293/1000 | Loss: 0.00002957
Iteration 294/1000 | Loss: 0.00002957
Iteration 295/1000 | Loss: 0.00002957
Iteration 296/1000 | Loss: 0.00002957
Iteration 297/1000 | Loss: 0.00002957
Iteration 298/1000 | Loss: 0.00002957
Iteration 299/1000 | Loss: 0.00002957
Iteration 300/1000 | Loss: 0.00002957
Iteration 301/1000 | Loss: 0.00002957
Iteration 302/1000 | Loss: 0.00002957
Iteration 303/1000 | Loss: 0.00002957
Iteration 304/1000 | Loss: 0.00002956
Iteration 305/1000 | Loss: 0.00002956
Iteration 306/1000 | Loss: 0.00002956
Iteration 307/1000 | Loss: 0.00002956
Iteration 308/1000 | Loss: 0.00002956
Iteration 309/1000 | Loss: 0.00002956
Iteration 310/1000 | Loss: 0.00002956
Iteration 311/1000 | Loss: 0.00002956
Iteration 312/1000 | Loss: 0.00002956
Iteration 313/1000 | Loss: 0.00002956
Iteration 314/1000 | Loss: 0.00002956
Iteration 315/1000 | Loss: 0.00002956
Iteration 316/1000 | Loss: 0.00002956
Iteration 317/1000 | Loss: 0.00002956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.9561022529378533e-05, 2.9561022529378533e-05, 2.9561022529378533e-05, 2.9561022529378533e-05, 2.9561022529378533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9561022529378533e-05

Optimization complete. Final v2v error: 4.0092620849609375 mm

Highest mean error: 10.528024673461914 mm for frame 90

Lowest mean error: 2.9462013244628906 mm for frame 104

Saving results

Total time: 215.4575469493866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826854
Iteration 2/25 | Loss: 0.00145364
Iteration 3/25 | Loss: 0.00130728
Iteration 4/25 | Loss: 0.00127522
Iteration 5/25 | Loss: 0.00126286
Iteration 6/25 | Loss: 0.00126002
Iteration 7/25 | Loss: 0.00125952
Iteration 8/25 | Loss: 0.00125952
Iteration 9/25 | Loss: 0.00125952
Iteration 10/25 | Loss: 0.00125952
Iteration 11/25 | Loss: 0.00125952
Iteration 12/25 | Loss: 0.00125952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012595233274623752, 0.0012595233274623752, 0.0012595233274623752, 0.0012595233274623752, 0.0012595233274623752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012595233274623752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59230816
Iteration 2/25 | Loss: 0.00080499
Iteration 3/25 | Loss: 0.00080499
Iteration 4/25 | Loss: 0.00080499
Iteration 5/25 | Loss: 0.00080499
Iteration 6/25 | Loss: 0.00080499
Iteration 7/25 | Loss: 0.00080499
Iteration 8/25 | Loss: 0.00080499
Iteration 9/25 | Loss: 0.00080499
Iteration 10/25 | Loss: 0.00080499
Iteration 11/25 | Loss: 0.00080499
Iteration 12/25 | Loss: 0.00080499
Iteration 13/25 | Loss: 0.00080499
Iteration 14/25 | Loss: 0.00080499
Iteration 15/25 | Loss: 0.00080499
Iteration 16/25 | Loss: 0.00080499
Iteration 17/25 | Loss: 0.00080499
Iteration 18/25 | Loss: 0.00080499
Iteration 19/25 | Loss: 0.00080499
Iteration 20/25 | Loss: 0.00080499
Iteration 21/25 | Loss: 0.00080499
Iteration 22/25 | Loss: 0.00080499
Iteration 23/25 | Loss: 0.00080499
Iteration 24/25 | Loss: 0.00080499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008049889001995325, 0.0008049889001995325, 0.0008049889001995325, 0.0008049889001995325, 0.0008049889001995325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008049889001995325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080499
Iteration 2/1000 | Loss: 0.00006102
Iteration 3/1000 | Loss: 0.00004340
Iteration 4/1000 | Loss: 0.00003634
Iteration 5/1000 | Loss: 0.00003356
Iteration 6/1000 | Loss: 0.00003192
Iteration 7/1000 | Loss: 0.00003059
Iteration 8/1000 | Loss: 0.00002971
Iteration 9/1000 | Loss: 0.00002905
Iteration 10/1000 | Loss: 0.00002856
Iteration 11/1000 | Loss: 0.00002813
Iteration 12/1000 | Loss: 0.00002773
Iteration 13/1000 | Loss: 0.00002742
Iteration 14/1000 | Loss: 0.00002717
Iteration 15/1000 | Loss: 0.00002711
Iteration 16/1000 | Loss: 0.00002693
Iteration 17/1000 | Loss: 0.00002680
Iteration 18/1000 | Loss: 0.00002676
Iteration 19/1000 | Loss: 0.00002671
Iteration 20/1000 | Loss: 0.00002671
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00002669
Iteration 23/1000 | Loss: 0.00002668
Iteration 24/1000 | Loss: 0.00002668
Iteration 25/1000 | Loss: 0.00002667
Iteration 26/1000 | Loss: 0.00002667
Iteration 27/1000 | Loss: 0.00002667
Iteration 28/1000 | Loss: 0.00002666
Iteration 29/1000 | Loss: 0.00002666
Iteration 30/1000 | Loss: 0.00002665
Iteration 31/1000 | Loss: 0.00002665
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002664
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002663
Iteration 36/1000 | Loss: 0.00002662
Iteration 37/1000 | Loss: 0.00002662
Iteration 38/1000 | Loss: 0.00002661
Iteration 39/1000 | Loss: 0.00002661
Iteration 40/1000 | Loss: 0.00002659
Iteration 41/1000 | Loss: 0.00002659
Iteration 42/1000 | Loss: 0.00002658
Iteration 43/1000 | Loss: 0.00002656
Iteration 44/1000 | Loss: 0.00002656
Iteration 45/1000 | Loss: 0.00002656
Iteration 46/1000 | Loss: 0.00002656
Iteration 47/1000 | Loss: 0.00002656
Iteration 48/1000 | Loss: 0.00002656
Iteration 49/1000 | Loss: 0.00002656
Iteration 50/1000 | Loss: 0.00002656
Iteration 51/1000 | Loss: 0.00002656
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002655
Iteration 56/1000 | Loss: 0.00002655
Iteration 57/1000 | Loss: 0.00002655
Iteration 58/1000 | Loss: 0.00002655
Iteration 59/1000 | Loss: 0.00002655
Iteration 60/1000 | Loss: 0.00002655
Iteration 61/1000 | Loss: 0.00002655
Iteration 62/1000 | Loss: 0.00002654
Iteration 63/1000 | Loss: 0.00002653
Iteration 64/1000 | Loss: 0.00002653
Iteration 65/1000 | Loss: 0.00002652
Iteration 66/1000 | Loss: 0.00002652
Iteration 67/1000 | Loss: 0.00002652
Iteration 68/1000 | Loss: 0.00002651
Iteration 69/1000 | Loss: 0.00002650
Iteration 70/1000 | Loss: 0.00002650
Iteration 71/1000 | Loss: 0.00002650
Iteration 72/1000 | Loss: 0.00002650
Iteration 73/1000 | Loss: 0.00002649
Iteration 74/1000 | Loss: 0.00002649
Iteration 75/1000 | Loss: 0.00002649
Iteration 76/1000 | Loss: 0.00002649
Iteration 77/1000 | Loss: 0.00002648
Iteration 78/1000 | Loss: 0.00002648
Iteration 79/1000 | Loss: 0.00002648
Iteration 80/1000 | Loss: 0.00002648
Iteration 81/1000 | Loss: 0.00002647
Iteration 82/1000 | Loss: 0.00002647
Iteration 83/1000 | Loss: 0.00002647
Iteration 84/1000 | Loss: 0.00002646
Iteration 85/1000 | Loss: 0.00002646
Iteration 86/1000 | Loss: 0.00002646
Iteration 87/1000 | Loss: 0.00002646
Iteration 88/1000 | Loss: 0.00002646
Iteration 89/1000 | Loss: 0.00002645
Iteration 90/1000 | Loss: 0.00002645
Iteration 91/1000 | Loss: 0.00002645
Iteration 92/1000 | Loss: 0.00002645
Iteration 93/1000 | Loss: 0.00002644
Iteration 94/1000 | Loss: 0.00002644
Iteration 95/1000 | Loss: 0.00002644
Iteration 96/1000 | Loss: 0.00002644
Iteration 97/1000 | Loss: 0.00002644
Iteration 98/1000 | Loss: 0.00002644
Iteration 99/1000 | Loss: 0.00002643
Iteration 100/1000 | Loss: 0.00002643
Iteration 101/1000 | Loss: 0.00002643
Iteration 102/1000 | Loss: 0.00002643
Iteration 103/1000 | Loss: 0.00002643
Iteration 104/1000 | Loss: 0.00002642
Iteration 105/1000 | Loss: 0.00002642
Iteration 106/1000 | Loss: 0.00002642
Iteration 107/1000 | Loss: 0.00002642
Iteration 108/1000 | Loss: 0.00002642
Iteration 109/1000 | Loss: 0.00002642
Iteration 110/1000 | Loss: 0.00002641
Iteration 111/1000 | Loss: 0.00002641
Iteration 112/1000 | Loss: 0.00002641
Iteration 113/1000 | Loss: 0.00002641
Iteration 114/1000 | Loss: 0.00002641
Iteration 115/1000 | Loss: 0.00002641
Iteration 116/1000 | Loss: 0.00002641
Iteration 117/1000 | Loss: 0.00002641
Iteration 118/1000 | Loss: 0.00002641
Iteration 119/1000 | Loss: 0.00002641
Iteration 120/1000 | Loss: 0.00002640
Iteration 121/1000 | Loss: 0.00002640
Iteration 122/1000 | Loss: 0.00002640
Iteration 123/1000 | Loss: 0.00002640
Iteration 124/1000 | Loss: 0.00002639
Iteration 125/1000 | Loss: 0.00002639
Iteration 126/1000 | Loss: 0.00002639
Iteration 127/1000 | Loss: 0.00002639
Iteration 128/1000 | Loss: 0.00002639
Iteration 129/1000 | Loss: 0.00002639
Iteration 130/1000 | Loss: 0.00002638
Iteration 131/1000 | Loss: 0.00002638
Iteration 132/1000 | Loss: 0.00002638
Iteration 133/1000 | Loss: 0.00002638
Iteration 134/1000 | Loss: 0.00002637
Iteration 135/1000 | Loss: 0.00002637
Iteration 136/1000 | Loss: 0.00002637
Iteration 137/1000 | Loss: 0.00002637
Iteration 138/1000 | Loss: 0.00002637
Iteration 139/1000 | Loss: 0.00002637
Iteration 140/1000 | Loss: 0.00002637
Iteration 141/1000 | Loss: 0.00002636
Iteration 142/1000 | Loss: 0.00002636
Iteration 143/1000 | Loss: 0.00002636
Iteration 144/1000 | Loss: 0.00002636
Iteration 145/1000 | Loss: 0.00002636
Iteration 146/1000 | Loss: 0.00002636
Iteration 147/1000 | Loss: 0.00002636
Iteration 148/1000 | Loss: 0.00002636
Iteration 149/1000 | Loss: 0.00002636
Iteration 150/1000 | Loss: 0.00002636
Iteration 151/1000 | Loss: 0.00002636
Iteration 152/1000 | Loss: 0.00002636
Iteration 153/1000 | Loss: 0.00002636
Iteration 154/1000 | Loss: 0.00002635
Iteration 155/1000 | Loss: 0.00002635
Iteration 156/1000 | Loss: 0.00002635
Iteration 157/1000 | Loss: 0.00002635
Iteration 158/1000 | Loss: 0.00002635
Iteration 159/1000 | Loss: 0.00002635
Iteration 160/1000 | Loss: 0.00002635
Iteration 161/1000 | Loss: 0.00002635
Iteration 162/1000 | Loss: 0.00002635
Iteration 163/1000 | Loss: 0.00002635
Iteration 164/1000 | Loss: 0.00002635
Iteration 165/1000 | Loss: 0.00002635
Iteration 166/1000 | Loss: 0.00002635
Iteration 167/1000 | Loss: 0.00002635
Iteration 168/1000 | Loss: 0.00002634
Iteration 169/1000 | Loss: 0.00002634
Iteration 170/1000 | Loss: 0.00002634
Iteration 171/1000 | Loss: 0.00002634
Iteration 172/1000 | Loss: 0.00002634
Iteration 173/1000 | Loss: 0.00002634
Iteration 174/1000 | Loss: 0.00002634
Iteration 175/1000 | Loss: 0.00002634
Iteration 176/1000 | Loss: 0.00002634
Iteration 177/1000 | Loss: 0.00002634
Iteration 178/1000 | Loss: 0.00002634
Iteration 179/1000 | Loss: 0.00002633
Iteration 180/1000 | Loss: 0.00002633
Iteration 181/1000 | Loss: 0.00002633
Iteration 182/1000 | Loss: 0.00002633
Iteration 183/1000 | Loss: 0.00002633
Iteration 184/1000 | Loss: 0.00002633
Iteration 185/1000 | Loss: 0.00002633
Iteration 186/1000 | Loss: 0.00002633
Iteration 187/1000 | Loss: 0.00002633
Iteration 188/1000 | Loss: 0.00002633
Iteration 189/1000 | Loss: 0.00002633
Iteration 190/1000 | Loss: 0.00002633
Iteration 191/1000 | Loss: 0.00002633
Iteration 192/1000 | Loss: 0.00002633
Iteration 193/1000 | Loss: 0.00002633
Iteration 194/1000 | Loss: 0.00002633
Iteration 195/1000 | Loss: 0.00002633
Iteration 196/1000 | Loss: 0.00002633
Iteration 197/1000 | Loss: 0.00002633
Iteration 198/1000 | Loss: 0.00002633
Iteration 199/1000 | Loss: 0.00002632
Iteration 200/1000 | Loss: 0.00002632
Iteration 201/1000 | Loss: 0.00002632
Iteration 202/1000 | Loss: 0.00002632
Iteration 203/1000 | Loss: 0.00002632
Iteration 204/1000 | Loss: 0.00002632
Iteration 205/1000 | Loss: 0.00002632
Iteration 206/1000 | Loss: 0.00002632
Iteration 207/1000 | Loss: 0.00002632
Iteration 208/1000 | Loss: 0.00002632
Iteration 209/1000 | Loss: 0.00002632
Iteration 210/1000 | Loss: 0.00002631
Iteration 211/1000 | Loss: 0.00002631
Iteration 212/1000 | Loss: 0.00002631
Iteration 213/1000 | Loss: 0.00002631
Iteration 214/1000 | Loss: 0.00002631
Iteration 215/1000 | Loss: 0.00002631
Iteration 216/1000 | Loss: 0.00002631
Iteration 217/1000 | Loss: 0.00002631
Iteration 218/1000 | Loss: 0.00002631
Iteration 219/1000 | Loss: 0.00002631
Iteration 220/1000 | Loss: 0.00002631
Iteration 221/1000 | Loss: 0.00002631
Iteration 222/1000 | Loss: 0.00002631
Iteration 223/1000 | Loss: 0.00002631
Iteration 224/1000 | Loss: 0.00002630
Iteration 225/1000 | Loss: 0.00002630
Iteration 226/1000 | Loss: 0.00002630
Iteration 227/1000 | Loss: 0.00002630
Iteration 228/1000 | Loss: 0.00002630
Iteration 229/1000 | Loss: 0.00002630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.6304829589207657e-05, 2.6304829589207657e-05, 2.6304829589207657e-05, 2.6304829589207657e-05, 2.6304829589207657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6304829589207657e-05

Optimization complete. Final v2v error: 4.2252302169799805 mm

Highest mean error: 5.7480974197387695 mm for frame 100

Lowest mean error: 2.905773162841797 mm for frame 15

Saving results

Total time: 46.60105323791504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828789
Iteration 2/25 | Loss: 0.00143542
Iteration 3/25 | Loss: 0.00128840
Iteration 4/25 | Loss: 0.00124311
Iteration 5/25 | Loss: 0.00123767
Iteration 6/25 | Loss: 0.00123273
Iteration 7/25 | Loss: 0.00123182
Iteration 8/25 | Loss: 0.00123158
Iteration 9/25 | Loss: 0.00123158
Iteration 10/25 | Loss: 0.00123158
Iteration 11/25 | Loss: 0.00123158
Iteration 12/25 | Loss: 0.00123158
Iteration 13/25 | Loss: 0.00123158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001231582136824727, 0.001231582136824727, 0.001231582136824727, 0.001231582136824727, 0.001231582136824727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001231582136824727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76371646
Iteration 2/25 | Loss: 0.00086956
Iteration 3/25 | Loss: 0.00086862
Iteration 4/25 | Loss: 0.00086862
Iteration 5/25 | Loss: 0.00086861
Iteration 6/25 | Loss: 0.00086861
Iteration 7/25 | Loss: 0.00086861
Iteration 8/25 | Loss: 0.00086861
Iteration 9/25 | Loss: 0.00086861
Iteration 10/25 | Loss: 0.00086861
Iteration 11/25 | Loss: 0.00086861
Iteration 12/25 | Loss: 0.00086861
Iteration 13/25 | Loss: 0.00086861
Iteration 14/25 | Loss: 0.00086861
Iteration 15/25 | Loss: 0.00086861
Iteration 16/25 | Loss: 0.00086861
Iteration 17/25 | Loss: 0.00086861
Iteration 18/25 | Loss: 0.00086861
Iteration 19/25 | Loss: 0.00086861
Iteration 20/25 | Loss: 0.00086861
Iteration 21/25 | Loss: 0.00086861
Iteration 22/25 | Loss: 0.00086861
Iteration 23/25 | Loss: 0.00086861
Iteration 24/25 | Loss: 0.00086861
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008686113869771361, 0.0008686113869771361, 0.0008686113869771361, 0.0008686113869771361, 0.0008686113869771361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008686113869771361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086861
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001651
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001387
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001341
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001279
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001285
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001221
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001388
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001196
Iteration 176/1000 | Loss: 0.00001196
Iteration 177/1000 | Loss: 0.00001196
Iteration 178/1000 | Loss: 0.00001196
Iteration 179/1000 | Loss: 0.00001196
Iteration 180/1000 | Loss: 0.00001196
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001194
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001194
Iteration 197/1000 | Loss: 0.00001194
Iteration 198/1000 | Loss: 0.00001194
Iteration 199/1000 | Loss: 0.00001194
Iteration 200/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.1944684956688434e-05, 1.1944684956688434e-05, 1.1944684956688434e-05, 1.1944684956688434e-05, 1.1944684956688434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1944684956688434e-05

Optimization complete. Final v2v error: 2.9796204566955566 mm

Highest mean error: 3.2344810962677 mm for frame 161

Lowest mean error: 2.7691378593444824 mm for frame 17

Saving results

Total time: 55.497018337249756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817042
Iteration 2/25 | Loss: 0.00150002
Iteration 3/25 | Loss: 0.00130818
Iteration 4/25 | Loss: 0.00128311
Iteration 5/25 | Loss: 0.00127504
Iteration 6/25 | Loss: 0.00127388
Iteration 7/25 | Loss: 0.00127360
Iteration 8/25 | Loss: 0.00127348
Iteration 9/25 | Loss: 0.00127342
Iteration 10/25 | Loss: 0.00127342
Iteration 11/25 | Loss: 0.00127342
Iteration 12/25 | Loss: 0.00127342
Iteration 13/25 | Loss: 0.00127342
Iteration 14/25 | Loss: 0.00127342
Iteration 15/25 | Loss: 0.00127341
Iteration 16/25 | Loss: 0.00127341
Iteration 17/25 | Loss: 0.00127341
Iteration 18/25 | Loss: 0.00127341
Iteration 19/25 | Loss: 0.00127341
Iteration 20/25 | Loss: 0.00127341
Iteration 21/25 | Loss: 0.00127341
Iteration 22/25 | Loss: 0.00127341
Iteration 23/25 | Loss: 0.00127341
Iteration 24/25 | Loss: 0.00127341
Iteration 25/25 | Loss: 0.00127341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62731671
Iteration 2/25 | Loss: 0.00095453
Iteration 3/25 | Loss: 0.00086225
Iteration 4/25 | Loss: 0.00086225
Iteration 5/25 | Loss: 0.00086225
Iteration 6/25 | Loss: 0.00086225
Iteration 7/25 | Loss: 0.00086225
Iteration 8/25 | Loss: 0.00086225
Iteration 9/25 | Loss: 0.00086225
Iteration 10/25 | Loss: 0.00086225
Iteration 11/25 | Loss: 0.00086225
Iteration 12/25 | Loss: 0.00086225
Iteration 13/25 | Loss: 0.00086225
Iteration 14/25 | Loss: 0.00086225
Iteration 15/25 | Loss: 0.00086225
Iteration 16/25 | Loss: 0.00086225
Iteration 17/25 | Loss: 0.00086225
Iteration 18/25 | Loss: 0.00086225
Iteration 19/25 | Loss: 0.00086225
Iteration 20/25 | Loss: 0.00086225
Iteration 21/25 | Loss: 0.00086225
Iteration 22/25 | Loss: 0.00086225
Iteration 23/25 | Loss: 0.00086225
Iteration 24/25 | Loss: 0.00086225
Iteration 25/25 | Loss: 0.00086225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086225
Iteration 2/1000 | Loss: 0.00011305
Iteration 3/1000 | Loss: 0.00002032
Iteration 4/1000 | Loss: 0.00001804
Iteration 5/1000 | Loss: 0.00007608
Iteration 6/1000 | Loss: 0.00001648
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00006029
Iteration 15/1000 | Loss: 0.00002036
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001453
Iteration 20/1000 | Loss: 0.00001453
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001451
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001444
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001444
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001444
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001444
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001432
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001431
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001425
Iteration 81/1000 | Loss: 0.00001425
Iteration 82/1000 | Loss: 0.00001425
Iteration 83/1000 | Loss: 0.00001424
Iteration 84/1000 | Loss: 0.00001424
Iteration 85/1000 | Loss: 0.00001423
Iteration 86/1000 | Loss: 0.00001423
Iteration 87/1000 | Loss: 0.00001423
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001422
Iteration 90/1000 | Loss: 0.00001422
Iteration 91/1000 | Loss: 0.00001422
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001421
Iteration 96/1000 | Loss: 0.00001421
Iteration 97/1000 | Loss: 0.00001421
Iteration 98/1000 | Loss: 0.00001421
Iteration 99/1000 | Loss: 0.00001421
Iteration 100/1000 | Loss: 0.00001421
Iteration 101/1000 | Loss: 0.00001421
Iteration 102/1000 | Loss: 0.00001421
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001421
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001416
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00006019
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001408
Iteration 134/1000 | Loss: 0.00001408
Iteration 135/1000 | Loss: 0.00001408
Iteration 136/1000 | Loss: 0.00001408
Iteration 137/1000 | Loss: 0.00001408
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001407
Iteration 148/1000 | Loss: 0.00001407
Iteration 149/1000 | Loss: 0.00001407
Iteration 150/1000 | Loss: 0.00001407
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001407
Iteration 156/1000 | Loss: 0.00001407
Iteration 157/1000 | Loss: 0.00001407
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4069129974814132e-05, 1.4069129974814132e-05, 1.4069129974814132e-05, 1.4069129974814132e-05, 1.4069129974814132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4069129974814132e-05

Optimization complete. Final v2v error: 3.204324960708618 mm

Highest mean error: 3.4561572074890137 mm for frame 232

Lowest mean error: 2.938662528991699 mm for frame 42

Saving results

Total time: 54.83237910270691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962781
Iteration 2/25 | Loss: 0.00170887
Iteration 3/25 | Loss: 0.00141039
Iteration 4/25 | Loss: 0.00133558
Iteration 5/25 | Loss: 0.00133954
Iteration 6/25 | Loss: 0.00133119
Iteration 7/25 | Loss: 0.00131932
Iteration 8/25 | Loss: 0.00131734
Iteration 9/25 | Loss: 0.00131592
Iteration 10/25 | Loss: 0.00131565
Iteration 11/25 | Loss: 0.00131857
Iteration 12/25 | Loss: 0.00131716
Iteration 13/25 | Loss: 0.00131632
Iteration 14/25 | Loss: 0.00131797
Iteration 15/25 | Loss: 0.00131796
Iteration 16/25 | Loss: 0.00131675
Iteration 17/25 | Loss: 0.00131605
Iteration 18/25 | Loss: 0.00131573
Iteration 19/25 | Loss: 0.00131667
Iteration 20/25 | Loss: 0.00131318
Iteration 21/25 | Loss: 0.00131245
Iteration 22/25 | Loss: 0.00131229
Iteration 23/25 | Loss: 0.00131228
Iteration 24/25 | Loss: 0.00131228
Iteration 25/25 | Loss: 0.00131228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52465427
Iteration 2/25 | Loss: 0.00115357
Iteration 3/25 | Loss: 0.00115356
Iteration 4/25 | Loss: 0.00115356
Iteration 5/25 | Loss: 0.00115356
Iteration 6/25 | Loss: 0.00115355
Iteration 7/25 | Loss: 0.00115355
Iteration 8/25 | Loss: 0.00115355
Iteration 9/25 | Loss: 0.00115355
Iteration 10/25 | Loss: 0.00115355
Iteration 11/25 | Loss: 0.00115355
Iteration 12/25 | Loss: 0.00115355
Iteration 13/25 | Loss: 0.00115355
Iteration 14/25 | Loss: 0.00115355
Iteration 15/25 | Loss: 0.00115355
Iteration 16/25 | Loss: 0.00115355
Iteration 17/25 | Loss: 0.00115355
Iteration 18/25 | Loss: 0.00115355
Iteration 19/25 | Loss: 0.00115355
Iteration 20/25 | Loss: 0.00115355
Iteration 21/25 | Loss: 0.00115355
Iteration 22/25 | Loss: 0.00115355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011535530211403966, 0.0011535530211403966, 0.0011535530211403966, 0.0011535530211403966, 0.0011535530211403966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011535530211403966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115355
Iteration 2/1000 | Loss: 0.00003233
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00002023
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001792
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00012582
Iteration 14/1000 | Loss: 0.00012582
Iteration 15/1000 | Loss: 0.00010092
Iteration 16/1000 | Loss: 0.00001681
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001652
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00012733
Iteration 26/1000 | Loss: 0.00011763
Iteration 27/1000 | Loss: 0.00001983
Iteration 28/1000 | Loss: 0.00005877
Iteration 29/1000 | Loss: 0.00015041
Iteration 30/1000 | Loss: 0.00009833
Iteration 31/1000 | Loss: 0.00003622
Iteration 32/1000 | Loss: 0.00011614
Iteration 33/1000 | Loss: 0.00010035
Iteration 34/1000 | Loss: 0.00005163
Iteration 35/1000 | Loss: 0.00003972
Iteration 36/1000 | Loss: 0.00011110
Iteration 37/1000 | Loss: 0.00006974
Iteration 38/1000 | Loss: 0.00004736
Iteration 39/1000 | Loss: 0.00016351
Iteration 40/1000 | Loss: 0.00002306
Iteration 41/1000 | Loss: 0.00017255
Iteration 42/1000 | Loss: 0.00003559
Iteration 43/1000 | Loss: 0.00002230
Iteration 44/1000 | Loss: 0.00002096
Iteration 45/1000 | Loss: 0.00001976
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001868
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001742
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001727
Iteration 83/1000 | Loss: 0.00001727
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001725
Iteration 88/1000 | Loss: 0.00001725
Iteration 89/1000 | Loss: 0.00001725
Iteration 90/1000 | Loss: 0.00001725
Iteration 91/1000 | Loss: 0.00001724
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001724
Iteration 94/1000 | Loss: 0.00001723
Iteration 95/1000 | Loss: 0.00001723
Iteration 96/1000 | Loss: 0.00001723
Iteration 97/1000 | Loss: 0.00001722
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001722
Iteration 101/1000 | Loss: 0.00001722
Iteration 102/1000 | Loss: 0.00001721
Iteration 103/1000 | Loss: 0.00001721
Iteration 104/1000 | Loss: 0.00001721
Iteration 105/1000 | Loss: 0.00001721
Iteration 106/1000 | Loss: 0.00001721
Iteration 107/1000 | Loss: 0.00001721
Iteration 108/1000 | Loss: 0.00001721
Iteration 109/1000 | Loss: 0.00001720
Iteration 110/1000 | Loss: 0.00001720
Iteration 111/1000 | Loss: 0.00001720
Iteration 112/1000 | Loss: 0.00001720
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001719
Iteration 121/1000 | Loss: 0.00001719
Iteration 122/1000 | Loss: 0.00001719
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00001719
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001718
Iteration 129/1000 | Loss: 0.00001718
Iteration 130/1000 | Loss: 0.00001718
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001718
Iteration 140/1000 | Loss: 0.00001718
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001717
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001717
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001715
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001715
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001715
Iteration 175/1000 | Loss: 0.00001715
Iteration 176/1000 | Loss: 0.00001715
Iteration 177/1000 | Loss: 0.00001714
Iteration 178/1000 | Loss: 0.00001714
Iteration 179/1000 | Loss: 0.00001714
Iteration 180/1000 | Loss: 0.00001714
Iteration 181/1000 | Loss: 0.00001714
Iteration 182/1000 | Loss: 0.00001714
Iteration 183/1000 | Loss: 0.00001714
Iteration 184/1000 | Loss: 0.00001714
Iteration 185/1000 | Loss: 0.00001714
Iteration 186/1000 | Loss: 0.00001714
Iteration 187/1000 | Loss: 0.00001714
Iteration 188/1000 | Loss: 0.00001714
Iteration 189/1000 | Loss: 0.00001714
Iteration 190/1000 | Loss: 0.00001714
Iteration 191/1000 | Loss: 0.00001714
Iteration 192/1000 | Loss: 0.00001714
Iteration 193/1000 | Loss: 0.00001714
Iteration 194/1000 | Loss: 0.00001714
Iteration 195/1000 | Loss: 0.00001714
Iteration 196/1000 | Loss: 0.00001714
Iteration 197/1000 | Loss: 0.00001714
Iteration 198/1000 | Loss: 0.00001714
Iteration 199/1000 | Loss: 0.00001714
Iteration 200/1000 | Loss: 0.00001714
Iteration 201/1000 | Loss: 0.00001714
Iteration 202/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.7141353964689188e-05, 1.7141353964689188e-05, 1.7141353964689188e-05, 1.7141353964689188e-05, 1.7141353964689188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7141353964689188e-05

Optimization complete. Final v2v error: 3.4130587577819824 mm

Highest mean error: 11.683623313903809 mm for frame 181

Lowest mean error: 3.1035945415496826 mm for frame 95

Saving results

Total time: 128.0080554485321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423966
Iteration 2/25 | Loss: 0.00131964
Iteration 3/25 | Loss: 0.00126415
Iteration 4/25 | Loss: 0.00125776
Iteration 5/25 | Loss: 0.00125587
Iteration 6/25 | Loss: 0.00125587
Iteration 7/25 | Loss: 0.00125587
Iteration 8/25 | Loss: 0.00125587
Iteration 9/25 | Loss: 0.00125587
Iteration 10/25 | Loss: 0.00125587
Iteration 11/25 | Loss: 0.00125587
Iteration 12/25 | Loss: 0.00125587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012558746384456754, 0.0012558746384456754, 0.0012558746384456754, 0.0012558746384456754, 0.0012558746384456754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012558746384456754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43130171
Iteration 2/25 | Loss: 0.00083576
Iteration 3/25 | Loss: 0.00083576
Iteration 4/25 | Loss: 0.00083576
Iteration 5/25 | Loss: 0.00083576
Iteration 6/25 | Loss: 0.00083576
Iteration 7/25 | Loss: 0.00083576
Iteration 8/25 | Loss: 0.00083576
Iteration 9/25 | Loss: 0.00083576
Iteration 10/25 | Loss: 0.00083576
Iteration 11/25 | Loss: 0.00083576
Iteration 12/25 | Loss: 0.00083576
Iteration 13/25 | Loss: 0.00083576
Iteration 14/25 | Loss: 0.00083576
Iteration 15/25 | Loss: 0.00083576
Iteration 16/25 | Loss: 0.00083576
Iteration 17/25 | Loss: 0.00083576
Iteration 18/25 | Loss: 0.00083576
Iteration 19/25 | Loss: 0.00083576
Iteration 20/25 | Loss: 0.00083576
Iteration 21/25 | Loss: 0.00083576
Iteration 22/25 | Loss: 0.00083576
Iteration 23/25 | Loss: 0.00083576
Iteration 24/25 | Loss: 0.00083576
Iteration 25/25 | Loss: 0.00083576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083576
Iteration 2/1000 | Loss: 0.00002604
Iteration 3/1000 | Loss: 0.00001680
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001411
Iteration 7/1000 | Loss: 0.00001375
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001293
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001292
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001275
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001233
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.2296817658352666e-05, 1.2296817658352666e-05, 1.2296817658352666e-05, 1.2296817658352666e-05, 1.2296817658352666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2296817658352666e-05

Optimization complete. Final v2v error: 3.045698881149292 mm

Highest mean error: 3.2264010906219482 mm for frame 239

Lowest mean error: 2.965360641479492 mm for frame 198

Saving results

Total time: 34.0167031288147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058430
Iteration 2/25 | Loss: 0.00276867
Iteration 3/25 | Loss: 0.00216417
Iteration 4/25 | Loss: 0.00204671
Iteration 5/25 | Loss: 0.00191803
Iteration 6/25 | Loss: 0.00187438
Iteration 7/25 | Loss: 0.00169507
Iteration 8/25 | Loss: 0.00166473
Iteration 9/25 | Loss: 0.00165244
Iteration 10/25 | Loss: 0.00164955
Iteration 11/25 | Loss: 0.00157487
Iteration 12/25 | Loss: 0.00155893
Iteration 13/25 | Loss: 0.00155170
Iteration 14/25 | Loss: 0.00154666
Iteration 15/25 | Loss: 0.00154272
Iteration 16/25 | Loss: 0.00154691
Iteration 17/25 | Loss: 0.00154751
Iteration 18/25 | Loss: 0.00155483
Iteration 19/25 | Loss: 0.00154970
Iteration 20/25 | Loss: 0.00154627
Iteration 21/25 | Loss: 0.00154610
Iteration 22/25 | Loss: 0.00155229
Iteration 23/25 | Loss: 0.00154740
Iteration 24/25 | Loss: 0.00155088
Iteration 25/25 | Loss: 0.00154897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.57250166
Iteration 2/25 | Loss: 0.00162866
Iteration 3/25 | Loss: 0.00150466
Iteration 4/25 | Loss: 0.00150466
Iteration 5/25 | Loss: 0.00150465
Iteration 6/25 | Loss: 0.00150465
Iteration 7/25 | Loss: 0.00150465
Iteration 8/25 | Loss: 0.00150465
Iteration 9/25 | Loss: 0.00150465
Iteration 10/25 | Loss: 0.00150465
Iteration 11/25 | Loss: 0.00150465
Iteration 12/25 | Loss: 0.00150465
Iteration 13/25 | Loss: 0.00150465
Iteration 14/25 | Loss: 0.00150465
Iteration 15/25 | Loss: 0.00150465
Iteration 16/25 | Loss: 0.00150465
Iteration 17/25 | Loss: 0.00150465
Iteration 18/25 | Loss: 0.00150465
Iteration 19/25 | Loss: 0.00150465
Iteration 20/25 | Loss: 0.00150465
Iteration 21/25 | Loss: 0.00150465
Iteration 22/25 | Loss: 0.00150465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015046519692987204, 0.0015046519692987204, 0.0015046519692987204, 0.0015046519692987204, 0.0015046519692987204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015046519692987204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150465
Iteration 2/1000 | Loss: 0.00050260
Iteration 3/1000 | Loss: 0.00011730
Iteration 4/1000 | Loss: 0.00016084
Iteration 5/1000 | Loss: 0.00015180
Iteration 6/1000 | Loss: 0.00014746
Iteration 7/1000 | Loss: 0.00029109
Iteration 8/1000 | Loss: 0.00010025
Iteration 9/1000 | Loss: 0.00035803
Iteration 10/1000 | Loss: 0.00073553
Iteration 11/1000 | Loss: 0.00057477
Iteration 12/1000 | Loss: 0.00228283
Iteration 13/1000 | Loss: 0.00040516
Iteration 14/1000 | Loss: 0.00008491
Iteration 15/1000 | Loss: 0.00017241
Iteration 16/1000 | Loss: 0.00006634
Iteration 17/1000 | Loss: 0.00010843
Iteration 18/1000 | Loss: 0.00008466
Iteration 19/1000 | Loss: 0.00017213
Iteration 20/1000 | Loss: 0.00024240
Iteration 21/1000 | Loss: 0.00005570
Iteration 22/1000 | Loss: 0.00008262
Iteration 23/1000 | Loss: 0.00006060
Iteration 24/1000 | Loss: 0.00010767
Iteration 25/1000 | Loss: 0.00009001
Iteration 26/1000 | Loss: 0.00009699
Iteration 27/1000 | Loss: 0.00006522
Iteration 28/1000 | Loss: 0.00004650
Iteration 29/1000 | Loss: 0.00004544
Iteration 30/1000 | Loss: 0.00005071
Iteration 31/1000 | Loss: 0.00004461
Iteration 32/1000 | Loss: 0.00008826
Iteration 33/1000 | Loss: 0.00004421
Iteration 34/1000 | Loss: 0.00006618
Iteration 35/1000 | Loss: 0.00004385
Iteration 36/1000 | Loss: 0.00004364
Iteration 37/1000 | Loss: 0.00008362
Iteration 38/1000 | Loss: 0.00011209
Iteration 39/1000 | Loss: 0.00008233
Iteration 40/1000 | Loss: 0.00010436
Iteration 41/1000 | Loss: 0.00004355
Iteration 42/1000 | Loss: 0.00004342
Iteration 43/1000 | Loss: 0.00004339
Iteration 44/1000 | Loss: 0.00004339
Iteration 45/1000 | Loss: 0.00004338
Iteration 46/1000 | Loss: 0.00004338
Iteration 47/1000 | Loss: 0.00004335
Iteration 48/1000 | Loss: 0.00004334
Iteration 49/1000 | Loss: 0.00004333
Iteration 50/1000 | Loss: 0.00004331
Iteration 51/1000 | Loss: 0.00004330
Iteration 52/1000 | Loss: 0.00004330
Iteration 53/1000 | Loss: 0.00004330
Iteration 54/1000 | Loss: 0.00004329
Iteration 55/1000 | Loss: 0.00004329
Iteration 56/1000 | Loss: 0.00004327
Iteration 57/1000 | Loss: 0.00004326
Iteration 58/1000 | Loss: 0.00004325
Iteration 59/1000 | Loss: 0.00004325
Iteration 60/1000 | Loss: 0.00004325
Iteration 61/1000 | Loss: 0.00004325
Iteration 62/1000 | Loss: 0.00004323
Iteration 63/1000 | Loss: 0.00004323
Iteration 64/1000 | Loss: 0.00004323
Iteration 65/1000 | Loss: 0.00004323
Iteration 66/1000 | Loss: 0.00004322
Iteration 67/1000 | Loss: 0.00004322
Iteration 68/1000 | Loss: 0.00004322
Iteration 69/1000 | Loss: 0.00004322
Iteration 70/1000 | Loss: 0.00004322
Iteration 71/1000 | Loss: 0.00004322
Iteration 72/1000 | Loss: 0.00004322
Iteration 73/1000 | Loss: 0.00004321
Iteration 74/1000 | Loss: 0.00004321
Iteration 75/1000 | Loss: 0.00004321
Iteration 76/1000 | Loss: 0.00004321
Iteration 77/1000 | Loss: 0.00004321
Iteration 78/1000 | Loss: 0.00004320
Iteration 79/1000 | Loss: 0.00004320
Iteration 80/1000 | Loss: 0.00004319
Iteration 81/1000 | Loss: 0.00004319
Iteration 82/1000 | Loss: 0.00004319
Iteration 83/1000 | Loss: 0.00004319
Iteration 84/1000 | Loss: 0.00004318
Iteration 85/1000 | Loss: 0.00004318
Iteration 86/1000 | Loss: 0.00004318
Iteration 87/1000 | Loss: 0.00004318
Iteration 88/1000 | Loss: 0.00004317
Iteration 89/1000 | Loss: 0.00004317
Iteration 90/1000 | Loss: 0.00004316
Iteration 91/1000 | Loss: 0.00004316
Iteration 92/1000 | Loss: 0.00004316
Iteration 93/1000 | Loss: 0.00004315
Iteration 94/1000 | Loss: 0.00004315
Iteration 95/1000 | Loss: 0.00004315
Iteration 96/1000 | Loss: 0.00004315
Iteration 97/1000 | Loss: 0.00004314
Iteration 98/1000 | Loss: 0.00004314
Iteration 99/1000 | Loss: 0.00004314
Iteration 100/1000 | Loss: 0.00004314
Iteration 101/1000 | Loss: 0.00004313
Iteration 102/1000 | Loss: 0.00004313
Iteration 103/1000 | Loss: 0.00004313
Iteration 104/1000 | Loss: 0.00004312
Iteration 105/1000 | Loss: 0.00004312
Iteration 106/1000 | Loss: 0.00004312
Iteration 107/1000 | Loss: 0.00004312
Iteration 108/1000 | Loss: 0.00004312
Iteration 109/1000 | Loss: 0.00004311
Iteration 110/1000 | Loss: 0.00004311
Iteration 111/1000 | Loss: 0.00004311
Iteration 112/1000 | Loss: 0.00004311
Iteration 113/1000 | Loss: 0.00004311
Iteration 114/1000 | Loss: 0.00004311
Iteration 115/1000 | Loss: 0.00004310
Iteration 116/1000 | Loss: 0.00004310
Iteration 117/1000 | Loss: 0.00004310
Iteration 118/1000 | Loss: 0.00004310
Iteration 119/1000 | Loss: 0.00004310
Iteration 120/1000 | Loss: 0.00004310
Iteration 121/1000 | Loss: 0.00004310
Iteration 122/1000 | Loss: 0.00004310
Iteration 123/1000 | Loss: 0.00004310
Iteration 124/1000 | Loss: 0.00004310
Iteration 125/1000 | Loss: 0.00004309
Iteration 126/1000 | Loss: 0.00004309
Iteration 127/1000 | Loss: 0.00004309
Iteration 128/1000 | Loss: 0.00004309
Iteration 129/1000 | Loss: 0.00004309
Iteration 130/1000 | Loss: 0.00004309
Iteration 131/1000 | Loss: 0.00004309
Iteration 132/1000 | Loss: 0.00004308
Iteration 133/1000 | Loss: 0.00004308
Iteration 134/1000 | Loss: 0.00004308
Iteration 135/1000 | Loss: 0.00004308
Iteration 136/1000 | Loss: 0.00004308
Iteration 137/1000 | Loss: 0.00004308
Iteration 138/1000 | Loss: 0.00011596
Iteration 139/1000 | Loss: 0.00004562
Iteration 140/1000 | Loss: 0.00006631
Iteration 141/1000 | Loss: 0.00004314
Iteration 142/1000 | Loss: 0.00006570
Iteration 143/1000 | Loss: 0.00007437
Iteration 144/1000 | Loss: 0.00007281
Iteration 145/1000 | Loss: 0.00004403
Iteration 146/1000 | Loss: 0.00005791
Iteration 147/1000 | Loss: 0.00015162
Iteration 148/1000 | Loss: 0.00004318
Iteration 149/1000 | Loss: 0.00004315
Iteration 150/1000 | Loss: 0.00004314
Iteration 151/1000 | Loss: 0.00004312
Iteration 152/1000 | Loss: 0.00004311
Iteration 153/1000 | Loss: 0.00004311
Iteration 154/1000 | Loss: 0.00004310
Iteration 155/1000 | Loss: 0.00004309
Iteration 156/1000 | Loss: 0.00004309
Iteration 157/1000 | Loss: 0.00005813
Iteration 158/1000 | Loss: 0.00004534
Iteration 159/1000 | Loss: 0.00004311
Iteration 160/1000 | Loss: 0.00004310
Iteration 161/1000 | Loss: 0.00004310
Iteration 162/1000 | Loss: 0.00004310
Iteration 163/1000 | Loss: 0.00004310
Iteration 164/1000 | Loss: 0.00004310
Iteration 165/1000 | Loss: 0.00004310
Iteration 166/1000 | Loss: 0.00004310
Iteration 167/1000 | Loss: 0.00004310
Iteration 168/1000 | Loss: 0.00006888
Iteration 169/1000 | Loss: 0.00004381
Iteration 170/1000 | Loss: 0.00005052
Iteration 171/1000 | Loss: 0.00004401
Iteration 172/1000 | Loss: 0.00004314
Iteration 173/1000 | Loss: 0.00004314
Iteration 174/1000 | Loss: 0.00004314
Iteration 175/1000 | Loss: 0.00004398
Iteration 176/1000 | Loss: 0.00004402
Iteration 177/1000 | Loss: 0.00004421
Iteration 178/1000 | Loss: 0.00004313
Iteration 179/1000 | Loss: 0.00004312
Iteration 180/1000 | Loss: 0.00004312
Iteration 181/1000 | Loss: 0.00004312
Iteration 182/1000 | Loss: 0.00004312
Iteration 183/1000 | Loss: 0.00004312
Iteration 184/1000 | Loss: 0.00004312
Iteration 185/1000 | Loss: 0.00004312
Iteration 186/1000 | Loss: 0.00004312
Iteration 187/1000 | Loss: 0.00004312
Iteration 188/1000 | Loss: 0.00004312
Iteration 189/1000 | Loss: 0.00004312
Iteration 190/1000 | Loss: 0.00004312
Iteration 191/1000 | Loss: 0.00004312
Iteration 192/1000 | Loss: 0.00004312
Iteration 193/1000 | Loss: 0.00004312
Iteration 194/1000 | Loss: 0.00004312
Iteration 195/1000 | Loss: 0.00004312
Iteration 196/1000 | Loss: 0.00004312
Iteration 197/1000 | Loss: 0.00004312
Iteration 198/1000 | Loss: 0.00004312
Iteration 199/1000 | Loss: 0.00004312
Iteration 200/1000 | Loss: 0.00004312
Iteration 201/1000 | Loss: 0.00004312
Iteration 202/1000 | Loss: 0.00004312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [4.312153032515198e-05, 4.312153032515198e-05, 4.312153032515198e-05, 4.312153032515198e-05, 4.312153032515198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.312153032515198e-05

Optimization complete. Final v2v error: 5.476496696472168 mm

Highest mean error: 7.1075358390808105 mm for frame 69

Lowest mean error: 5.058834552764893 mm for frame 83

Saving results

Total time: 143.7875578403473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494740
Iteration 2/25 | Loss: 0.00146236
Iteration 3/25 | Loss: 0.00131837
Iteration 4/25 | Loss: 0.00130495
Iteration 5/25 | Loss: 0.00130143
Iteration 6/25 | Loss: 0.00130087
Iteration 7/25 | Loss: 0.00130087
Iteration 8/25 | Loss: 0.00130087
Iteration 9/25 | Loss: 0.00130087
Iteration 10/25 | Loss: 0.00130087
Iteration 11/25 | Loss: 0.00130087
Iteration 12/25 | Loss: 0.00130087
Iteration 13/25 | Loss: 0.00130087
Iteration 14/25 | Loss: 0.00130087
Iteration 15/25 | Loss: 0.00130087
Iteration 16/25 | Loss: 0.00130087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013008692767471075, 0.0013008692767471075, 0.0013008692767471075, 0.0013008692767471075, 0.0013008692767471075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013008692767471075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52710807
Iteration 2/25 | Loss: 0.00081141
Iteration 3/25 | Loss: 0.00081139
Iteration 4/25 | Loss: 0.00081139
Iteration 5/25 | Loss: 0.00081139
Iteration 6/25 | Loss: 0.00081139
Iteration 7/25 | Loss: 0.00081139
Iteration 8/25 | Loss: 0.00081139
Iteration 9/25 | Loss: 0.00081139
Iteration 10/25 | Loss: 0.00081139
Iteration 11/25 | Loss: 0.00081139
Iteration 12/25 | Loss: 0.00081139
Iteration 13/25 | Loss: 0.00081139
Iteration 14/25 | Loss: 0.00081139
Iteration 15/25 | Loss: 0.00081139
Iteration 16/25 | Loss: 0.00081139
Iteration 17/25 | Loss: 0.00081139
Iteration 18/25 | Loss: 0.00081139
Iteration 19/25 | Loss: 0.00081139
Iteration 20/25 | Loss: 0.00081139
Iteration 21/25 | Loss: 0.00081139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008113899966701865, 0.0008113899966701865, 0.0008113899966701865, 0.0008113899966701865, 0.0008113899966701865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008113899966701865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081139
Iteration 2/1000 | Loss: 0.00003642
Iteration 3/1000 | Loss: 0.00002664
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00001974
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001852
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001765
Iteration 12/1000 | Loss: 0.00001758
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001724
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001722
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001720
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001719
Iteration 33/1000 | Loss: 0.00001718
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001714
Iteration 40/1000 | Loss: 0.00001714
Iteration 41/1000 | Loss: 0.00001713
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001711
Iteration 44/1000 | Loss: 0.00001711
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001710
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001707
Iteration 52/1000 | Loss: 0.00001707
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001704
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001700
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001696
Iteration 84/1000 | Loss: 0.00001696
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001694
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001693
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001692
Iteration 96/1000 | Loss: 0.00001692
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001692
Iteration 99/1000 | Loss: 0.00001692
Iteration 100/1000 | Loss: 0.00001692
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001686
Iteration 150/1000 | Loss: 0.00001686
Iteration 151/1000 | Loss: 0.00001686
Iteration 152/1000 | Loss: 0.00001686
Iteration 153/1000 | Loss: 0.00001686
Iteration 154/1000 | Loss: 0.00001686
Iteration 155/1000 | Loss: 0.00001686
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001686
Iteration 161/1000 | Loss: 0.00001686
Iteration 162/1000 | Loss: 0.00001686
Iteration 163/1000 | Loss: 0.00001686
Iteration 164/1000 | Loss: 0.00001686
Iteration 165/1000 | Loss: 0.00001686
Iteration 166/1000 | Loss: 0.00001686
Iteration 167/1000 | Loss: 0.00001686
Iteration 168/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.685832285147626e-05, 1.685832285147626e-05, 1.685832285147626e-05, 1.685832285147626e-05, 1.685832285147626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.685832285147626e-05

Optimization complete. Final v2v error: 3.395664930343628 mm

Highest mean error: 4.4902424812316895 mm for frame 110

Lowest mean error: 2.8164865970611572 mm for frame 166

Saving results

Total time: 38.00772476196289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047225
Iteration 2/25 | Loss: 0.00167156
Iteration 3/25 | Loss: 0.00148752
Iteration 4/25 | Loss: 0.00146798
Iteration 5/25 | Loss: 0.00146101
Iteration 6/25 | Loss: 0.00146051
Iteration 7/25 | Loss: 0.00146051
Iteration 8/25 | Loss: 0.00146051
Iteration 9/25 | Loss: 0.00146048
Iteration 10/25 | Loss: 0.00146048
Iteration 11/25 | Loss: 0.00146048
Iteration 12/25 | Loss: 0.00146048
Iteration 13/25 | Loss: 0.00146048
Iteration 14/25 | Loss: 0.00146048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014604832977056503, 0.0014604832977056503, 0.0014604832977056503, 0.0014604832977056503, 0.0014604832977056503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014604832977056503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29868507
Iteration 2/25 | Loss: 0.00089432
Iteration 3/25 | Loss: 0.00089432
Iteration 4/25 | Loss: 0.00089432
Iteration 5/25 | Loss: 0.00089432
Iteration 6/25 | Loss: 0.00089432
Iteration 7/25 | Loss: 0.00089432
Iteration 8/25 | Loss: 0.00089432
Iteration 9/25 | Loss: 0.00089432
Iteration 10/25 | Loss: 0.00089432
Iteration 11/25 | Loss: 0.00089432
Iteration 12/25 | Loss: 0.00089432
Iteration 13/25 | Loss: 0.00089432
Iteration 14/25 | Loss: 0.00089432
Iteration 15/25 | Loss: 0.00089432
Iteration 16/25 | Loss: 0.00089432
Iteration 17/25 | Loss: 0.00089432
Iteration 18/25 | Loss: 0.00089432
Iteration 19/25 | Loss: 0.00089432
Iteration 20/25 | Loss: 0.00089432
Iteration 21/25 | Loss: 0.00089432
Iteration 22/25 | Loss: 0.00089432
Iteration 23/25 | Loss: 0.00089432
Iteration 24/25 | Loss: 0.00089432
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008943166467361152, 0.0008943166467361152, 0.0008943166467361152, 0.0008943166467361152, 0.0008943166467361152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008943166467361152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089432
Iteration 2/1000 | Loss: 0.00004631
Iteration 3/1000 | Loss: 0.00003487
Iteration 4/1000 | Loss: 0.00003177
Iteration 5/1000 | Loss: 0.00003074
Iteration 6/1000 | Loss: 0.00002992
Iteration 7/1000 | Loss: 0.00002940
Iteration 8/1000 | Loss: 0.00002914
Iteration 9/1000 | Loss: 0.00002889
Iteration 10/1000 | Loss: 0.00002868
Iteration 11/1000 | Loss: 0.00002853
Iteration 12/1000 | Loss: 0.00002847
Iteration 13/1000 | Loss: 0.00002847
Iteration 14/1000 | Loss: 0.00002846
Iteration 15/1000 | Loss: 0.00002845
Iteration 16/1000 | Loss: 0.00002845
Iteration 17/1000 | Loss: 0.00002844
Iteration 18/1000 | Loss: 0.00002837
Iteration 19/1000 | Loss: 0.00002837
Iteration 20/1000 | Loss: 0.00002836
Iteration 21/1000 | Loss: 0.00002836
Iteration 22/1000 | Loss: 0.00002835
Iteration 23/1000 | Loss: 0.00002834
Iteration 24/1000 | Loss: 0.00002834
Iteration 25/1000 | Loss: 0.00002833
Iteration 26/1000 | Loss: 0.00002833
Iteration 27/1000 | Loss: 0.00002833
Iteration 28/1000 | Loss: 0.00002833
Iteration 29/1000 | Loss: 0.00002833
Iteration 30/1000 | Loss: 0.00002833
Iteration 31/1000 | Loss: 0.00002833
Iteration 32/1000 | Loss: 0.00002833
Iteration 33/1000 | Loss: 0.00002833
Iteration 34/1000 | Loss: 0.00002833
Iteration 35/1000 | Loss: 0.00002833
Iteration 36/1000 | Loss: 0.00002833
Iteration 37/1000 | Loss: 0.00002833
Iteration 38/1000 | Loss: 0.00002833
Iteration 39/1000 | Loss: 0.00002833
Iteration 40/1000 | Loss: 0.00002832
Iteration 41/1000 | Loss: 0.00002831
Iteration 42/1000 | Loss: 0.00002831
Iteration 43/1000 | Loss: 0.00002830
Iteration 44/1000 | Loss: 0.00002830
Iteration 45/1000 | Loss: 0.00002830
Iteration 46/1000 | Loss: 0.00002830
Iteration 47/1000 | Loss: 0.00002830
Iteration 48/1000 | Loss: 0.00002830
Iteration 49/1000 | Loss: 0.00002830
Iteration 50/1000 | Loss: 0.00002830
Iteration 51/1000 | Loss: 0.00002830
Iteration 52/1000 | Loss: 0.00002829
Iteration 53/1000 | Loss: 0.00002829
Iteration 54/1000 | Loss: 0.00002827
Iteration 55/1000 | Loss: 0.00002827
Iteration 56/1000 | Loss: 0.00002827
Iteration 57/1000 | Loss: 0.00002827
Iteration 58/1000 | Loss: 0.00002827
Iteration 59/1000 | Loss: 0.00002827
Iteration 60/1000 | Loss: 0.00002827
Iteration 61/1000 | Loss: 0.00002827
Iteration 62/1000 | Loss: 0.00002827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [2.8265323635423556e-05, 2.8265323635423556e-05, 2.8265323635423556e-05, 2.8265323635423556e-05, 2.8265323635423556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8265323635423556e-05

Optimization complete. Final v2v error: 4.300368785858154 mm

Highest mean error: 4.917247295379639 mm for frame 150

Lowest mean error: 3.827528715133667 mm for frame 7

Saving results

Total time: 30.643723249435425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005150
Iteration 2/25 | Loss: 0.00181477
Iteration 3/25 | Loss: 0.00166816
Iteration 4/25 | Loss: 0.00142580
Iteration 5/25 | Loss: 0.00140170
Iteration 6/25 | Loss: 0.00143638
Iteration 7/25 | Loss: 0.00140942
Iteration 8/25 | Loss: 0.00137847
Iteration 9/25 | Loss: 0.00138379
Iteration 10/25 | Loss: 0.00137594
Iteration 11/25 | Loss: 0.00137549
Iteration 12/25 | Loss: 0.00135018
Iteration 13/25 | Loss: 0.00133585
Iteration 14/25 | Loss: 0.00133085
Iteration 15/25 | Loss: 0.00132590
Iteration 16/25 | Loss: 0.00131892
Iteration 17/25 | Loss: 0.00132704
Iteration 18/25 | Loss: 0.00132968
Iteration 19/25 | Loss: 0.00132231
Iteration 20/25 | Loss: 0.00132706
Iteration 21/25 | Loss: 0.00131730
Iteration 22/25 | Loss: 0.00130755
Iteration 23/25 | Loss: 0.00130355
Iteration 24/25 | Loss: 0.00130631
Iteration 25/25 | Loss: 0.00130509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53161216
Iteration 2/25 | Loss: 0.00094830
Iteration 3/25 | Loss: 0.00094829
Iteration 4/25 | Loss: 0.00094829
Iteration 5/25 | Loss: 0.00094829
Iteration 6/25 | Loss: 0.00094829
Iteration 7/25 | Loss: 0.00094829
Iteration 8/25 | Loss: 0.00094829
Iteration 9/25 | Loss: 0.00094829
Iteration 10/25 | Loss: 0.00094829
Iteration 11/25 | Loss: 0.00094829
Iteration 12/25 | Loss: 0.00094829
Iteration 13/25 | Loss: 0.00094829
Iteration 14/25 | Loss: 0.00094829
Iteration 15/25 | Loss: 0.00094829
Iteration 16/25 | Loss: 0.00094829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000948290980886668, 0.000948290980886668, 0.000948290980886668, 0.000948290980886668, 0.000948290980886668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000948290980886668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094829
Iteration 2/1000 | Loss: 0.00103472
Iteration 3/1000 | Loss: 0.00006022
Iteration 4/1000 | Loss: 0.00004364
Iteration 5/1000 | Loss: 0.00005528
Iteration 6/1000 | Loss: 0.00003568
Iteration 7/1000 | Loss: 0.00005100
Iteration 8/1000 | Loss: 0.00005039
Iteration 9/1000 | Loss: 0.00004702
Iteration 10/1000 | Loss: 0.00004336
Iteration 11/1000 | Loss: 0.00004639
Iteration 12/1000 | Loss: 0.00004424
Iteration 13/1000 | Loss: 0.00003932
Iteration 14/1000 | Loss: 0.00004033
Iteration 15/1000 | Loss: 0.00120322
Iteration 16/1000 | Loss: 0.00005602
Iteration 17/1000 | Loss: 0.00002936
Iteration 18/1000 | Loss: 0.00005158
Iteration 19/1000 | Loss: 0.00002908
Iteration 20/1000 | Loss: 0.00002690
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002224
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002173
Iteration 27/1000 | Loss: 0.00002150
Iteration 28/1000 | Loss: 0.00002142
Iteration 29/1000 | Loss: 0.00002138
Iteration 30/1000 | Loss: 0.00002117
Iteration 31/1000 | Loss: 0.00002699
Iteration 32/1000 | Loss: 0.00002160
Iteration 33/1000 | Loss: 0.00030561
Iteration 34/1000 | Loss: 0.00024247
Iteration 35/1000 | Loss: 0.00029445
Iteration 36/1000 | Loss: 0.00005535
Iteration 37/1000 | Loss: 0.00003023
Iteration 38/1000 | Loss: 0.00002553
Iteration 39/1000 | Loss: 0.00002442
Iteration 40/1000 | Loss: 0.00029578
Iteration 41/1000 | Loss: 0.00045175
Iteration 42/1000 | Loss: 0.00045648
Iteration 43/1000 | Loss: 0.00023094
Iteration 44/1000 | Loss: 0.00003054
Iteration 45/1000 | Loss: 0.00002706
Iteration 46/1000 | Loss: 0.00032712
Iteration 47/1000 | Loss: 0.00005007
Iteration 48/1000 | Loss: 0.00003912
Iteration 49/1000 | Loss: 0.00002899
Iteration 50/1000 | Loss: 0.00003466
Iteration 51/1000 | Loss: 0.00002606
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002277
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00025476
Iteration 58/1000 | Loss: 0.00137235
Iteration 59/1000 | Loss: 0.00028012
Iteration 60/1000 | Loss: 0.00011531
Iteration 61/1000 | Loss: 0.00002625
Iteration 62/1000 | Loss: 0.00031399
Iteration 63/1000 | Loss: 0.00004236
Iteration 64/1000 | Loss: 0.00002500
Iteration 65/1000 | Loss: 0.00002358
Iteration 66/1000 | Loss: 0.00002220
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002137
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00032271
Iteration 71/1000 | Loss: 0.00013992
Iteration 72/1000 | Loss: 0.00002082
Iteration 73/1000 | Loss: 0.00036744
Iteration 74/1000 | Loss: 0.00024375
Iteration 75/1000 | Loss: 0.00011635
Iteration 76/1000 | Loss: 0.00002970
Iteration 77/1000 | Loss: 0.00027662
Iteration 78/1000 | Loss: 0.00012490
Iteration 79/1000 | Loss: 0.00028434
Iteration 80/1000 | Loss: 0.00013803
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002018
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001844
Iteration 90/1000 | Loss: 0.00001837
Iteration 91/1000 | Loss: 0.00001836
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001826
Iteration 94/1000 | Loss: 0.00001818
Iteration 95/1000 | Loss: 0.00001818
Iteration 96/1000 | Loss: 0.00001816
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001814
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00001812
Iteration 103/1000 | Loss: 0.00001812
Iteration 104/1000 | Loss: 0.00001811
Iteration 105/1000 | Loss: 0.00001810
Iteration 106/1000 | Loss: 0.00001810
Iteration 107/1000 | Loss: 0.00040891
Iteration 108/1000 | Loss: 0.00003602
Iteration 109/1000 | Loss: 0.00037335
Iteration 110/1000 | Loss: 0.00011999
Iteration 111/1000 | Loss: 0.00051277
Iteration 112/1000 | Loss: 0.00041394
Iteration 113/1000 | Loss: 0.00004002
Iteration 114/1000 | Loss: 0.00003166
Iteration 115/1000 | Loss: 0.00002558
Iteration 116/1000 | Loss: 0.00002337
Iteration 117/1000 | Loss: 0.00002173
Iteration 118/1000 | Loss: 0.00002068
Iteration 119/1000 | Loss: 0.00001988
Iteration 120/1000 | Loss: 0.00001920
Iteration 121/1000 | Loss: 0.00001873
Iteration 122/1000 | Loss: 0.00001848
Iteration 123/1000 | Loss: 0.00001848
Iteration 124/1000 | Loss: 0.00001834
Iteration 125/1000 | Loss: 0.00001832
Iteration 126/1000 | Loss: 0.00001831
Iteration 127/1000 | Loss: 0.00001831
Iteration 128/1000 | Loss: 0.00001831
Iteration 129/1000 | Loss: 0.00001830
Iteration 130/1000 | Loss: 0.00001829
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001822
Iteration 133/1000 | Loss: 0.00001821
Iteration 134/1000 | Loss: 0.00001821
Iteration 135/1000 | Loss: 0.00001821
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001816
Iteration 138/1000 | Loss: 0.00001815
Iteration 139/1000 | Loss: 0.00001815
Iteration 140/1000 | Loss: 0.00001813
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001812
Iteration 143/1000 | Loss: 0.00001812
Iteration 144/1000 | Loss: 0.00001812
Iteration 145/1000 | Loss: 0.00001811
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001810
Iteration 148/1000 | Loss: 0.00001810
Iteration 149/1000 | Loss: 0.00001809
Iteration 150/1000 | Loss: 0.00001809
Iteration 151/1000 | Loss: 0.00001808
Iteration 152/1000 | Loss: 0.00001808
Iteration 153/1000 | Loss: 0.00001807
Iteration 154/1000 | Loss: 0.00001807
Iteration 155/1000 | Loss: 0.00001806
Iteration 156/1000 | Loss: 0.00001806
Iteration 157/1000 | Loss: 0.00001805
Iteration 158/1000 | Loss: 0.00001805
Iteration 159/1000 | Loss: 0.00001804
Iteration 160/1000 | Loss: 0.00001804
Iteration 161/1000 | Loss: 0.00001804
Iteration 162/1000 | Loss: 0.00001803
Iteration 163/1000 | Loss: 0.00001803
Iteration 164/1000 | Loss: 0.00001803
Iteration 165/1000 | Loss: 0.00001802
Iteration 166/1000 | Loss: 0.00001802
Iteration 167/1000 | Loss: 0.00001802
Iteration 168/1000 | Loss: 0.00001801
Iteration 169/1000 | Loss: 0.00001801
Iteration 170/1000 | Loss: 0.00001801
Iteration 171/1000 | Loss: 0.00001801
Iteration 172/1000 | Loss: 0.00001800
Iteration 173/1000 | Loss: 0.00001800
Iteration 174/1000 | Loss: 0.00001800
Iteration 175/1000 | Loss: 0.00001800
Iteration 176/1000 | Loss: 0.00001799
Iteration 177/1000 | Loss: 0.00001799
Iteration 178/1000 | Loss: 0.00001799
Iteration 179/1000 | Loss: 0.00001799
Iteration 180/1000 | Loss: 0.00001799
Iteration 181/1000 | Loss: 0.00001799
Iteration 182/1000 | Loss: 0.00001799
Iteration 183/1000 | Loss: 0.00001799
Iteration 184/1000 | Loss: 0.00001798
Iteration 185/1000 | Loss: 0.00001798
Iteration 186/1000 | Loss: 0.00001798
Iteration 187/1000 | Loss: 0.00001798
Iteration 188/1000 | Loss: 0.00001798
Iteration 189/1000 | Loss: 0.00001798
Iteration 190/1000 | Loss: 0.00001798
Iteration 191/1000 | Loss: 0.00001797
Iteration 192/1000 | Loss: 0.00001797
Iteration 193/1000 | Loss: 0.00001797
Iteration 194/1000 | Loss: 0.00001797
Iteration 195/1000 | Loss: 0.00001797
Iteration 196/1000 | Loss: 0.00001797
Iteration 197/1000 | Loss: 0.00001797
Iteration 198/1000 | Loss: 0.00001797
Iteration 199/1000 | Loss: 0.00001797
Iteration 200/1000 | Loss: 0.00001796
Iteration 201/1000 | Loss: 0.00001796
Iteration 202/1000 | Loss: 0.00001796
Iteration 203/1000 | Loss: 0.00001796
Iteration 204/1000 | Loss: 0.00001796
Iteration 205/1000 | Loss: 0.00001796
Iteration 206/1000 | Loss: 0.00001796
Iteration 207/1000 | Loss: 0.00001796
Iteration 208/1000 | Loss: 0.00001796
Iteration 209/1000 | Loss: 0.00001796
Iteration 210/1000 | Loss: 0.00001796
Iteration 211/1000 | Loss: 0.00001795
Iteration 212/1000 | Loss: 0.00001795
Iteration 213/1000 | Loss: 0.00001795
Iteration 214/1000 | Loss: 0.00001795
Iteration 215/1000 | Loss: 0.00001795
Iteration 216/1000 | Loss: 0.00001795
Iteration 217/1000 | Loss: 0.00001795
Iteration 218/1000 | Loss: 0.00001795
Iteration 219/1000 | Loss: 0.00001795
Iteration 220/1000 | Loss: 0.00001794
Iteration 221/1000 | Loss: 0.00001794
Iteration 222/1000 | Loss: 0.00001794
Iteration 223/1000 | Loss: 0.00001794
Iteration 224/1000 | Loss: 0.00001794
Iteration 225/1000 | Loss: 0.00001793
Iteration 226/1000 | Loss: 0.00001793
Iteration 227/1000 | Loss: 0.00001793
Iteration 228/1000 | Loss: 0.00001793
Iteration 229/1000 | Loss: 0.00001793
Iteration 230/1000 | Loss: 0.00001792
Iteration 231/1000 | Loss: 0.00001792
Iteration 232/1000 | Loss: 0.00001792
Iteration 233/1000 | Loss: 0.00001792
Iteration 234/1000 | Loss: 0.00001792
Iteration 235/1000 | Loss: 0.00001792
Iteration 236/1000 | Loss: 0.00001792
Iteration 237/1000 | Loss: 0.00001792
Iteration 238/1000 | Loss: 0.00001792
Iteration 239/1000 | Loss: 0.00001792
Iteration 240/1000 | Loss: 0.00001792
Iteration 241/1000 | Loss: 0.00001792
Iteration 242/1000 | Loss: 0.00001792
Iteration 243/1000 | Loss: 0.00001792
Iteration 244/1000 | Loss: 0.00001791
Iteration 245/1000 | Loss: 0.00001791
Iteration 246/1000 | Loss: 0.00001791
Iteration 247/1000 | Loss: 0.00001791
Iteration 248/1000 | Loss: 0.00001791
Iteration 249/1000 | Loss: 0.00001791
Iteration 250/1000 | Loss: 0.00001791
Iteration 251/1000 | Loss: 0.00001791
Iteration 252/1000 | Loss: 0.00001791
Iteration 253/1000 | Loss: 0.00001791
Iteration 254/1000 | Loss: 0.00001791
Iteration 255/1000 | Loss: 0.00001790
Iteration 256/1000 | Loss: 0.00001790
Iteration 257/1000 | Loss: 0.00001790
Iteration 258/1000 | Loss: 0.00001790
Iteration 259/1000 | Loss: 0.00001790
Iteration 260/1000 | Loss: 0.00001790
Iteration 261/1000 | Loss: 0.00001790
Iteration 262/1000 | Loss: 0.00001790
Iteration 263/1000 | Loss: 0.00001790
Iteration 264/1000 | Loss: 0.00001790
Iteration 265/1000 | Loss: 0.00001790
Iteration 266/1000 | Loss: 0.00001790
Iteration 267/1000 | Loss: 0.00001790
Iteration 268/1000 | Loss: 0.00001789
Iteration 269/1000 | Loss: 0.00001789
Iteration 270/1000 | Loss: 0.00001789
Iteration 271/1000 | Loss: 0.00001789
Iteration 272/1000 | Loss: 0.00001789
Iteration 273/1000 | Loss: 0.00001789
Iteration 274/1000 | Loss: 0.00001789
Iteration 275/1000 | Loss: 0.00001789
Iteration 276/1000 | Loss: 0.00001789
Iteration 277/1000 | Loss: 0.00001789
Iteration 278/1000 | Loss: 0.00001789
Iteration 279/1000 | Loss: 0.00001789
Iteration 280/1000 | Loss: 0.00001789
Iteration 281/1000 | Loss: 0.00001789
Iteration 282/1000 | Loss: 0.00001789
Iteration 283/1000 | Loss: 0.00001789
Iteration 284/1000 | Loss: 0.00001788
Iteration 285/1000 | Loss: 0.00001788
Iteration 286/1000 | Loss: 0.00001788
Iteration 287/1000 | Loss: 0.00001788
Iteration 288/1000 | Loss: 0.00001788
Iteration 289/1000 | Loss: 0.00001788
Iteration 290/1000 | Loss: 0.00001788
Iteration 291/1000 | Loss: 0.00001788
Iteration 292/1000 | Loss: 0.00001788
Iteration 293/1000 | Loss: 0.00001788
Iteration 294/1000 | Loss: 0.00001788
Iteration 295/1000 | Loss: 0.00001788
Iteration 296/1000 | Loss: 0.00001787
Iteration 297/1000 | Loss: 0.00001787
Iteration 298/1000 | Loss: 0.00001787
Iteration 299/1000 | Loss: 0.00001787
Iteration 300/1000 | Loss: 0.00001787
Iteration 301/1000 | Loss: 0.00001787
Iteration 302/1000 | Loss: 0.00001787
Iteration 303/1000 | Loss: 0.00001787
Iteration 304/1000 | Loss: 0.00001787
Iteration 305/1000 | Loss: 0.00001786
Iteration 306/1000 | Loss: 0.00001786
Iteration 307/1000 | Loss: 0.00001786
Iteration 308/1000 | Loss: 0.00001786
Iteration 309/1000 | Loss: 0.00001786
Iteration 310/1000 | Loss: 0.00001786
Iteration 311/1000 | Loss: 0.00001786
Iteration 312/1000 | Loss: 0.00001785
Iteration 313/1000 | Loss: 0.00001785
Iteration 314/1000 | Loss: 0.00001785
Iteration 315/1000 | Loss: 0.00001785
Iteration 316/1000 | Loss: 0.00001785
Iteration 317/1000 | Loss: 0.00001785
Iteration 318/1000 | Loss: 0.00001785
Iteration 319/1000 | Loss: 0.00001785
Iteration 320/1000 | Loss: 0.00001785
Iteration 321/1000 | Loss: 0.00001785
Iteration 322/1000 | Loss: 0.00001785
Iteration 323/1000 | Loss: 0.00001785
Iteration 324/1000 | Loss: 0.00001784
Iteration 325/1000 | Loss: 0.00001784
Iteration 326/1000 | Loss: 0.00001784
Iteration 327/1000 | Loss: 0.00001784
Iteration 328/1000 | Loss: 0.00001783
Iteration 329/1000 | Loss: 0.00001783
Iteration 330/1000 | Loss: 0.00001783
Iteration 331/1000 | Loss: 0.00001783
Iteration 332/1000 | Loss: 0.00001783
Iteration 333/1000 | Loss: 0.00001783
Iteration 334/1000 | Loss: 0.00001783
Iteration 335/1000 | Loss: 0.00001783
Iteration 336/1000 | Loss: 0.00001783
Iteration 337/1000 | Loss: 0.00001783
Iteration 338/1000 | Loss: 0.00001783
Iteration 339/1000 | Loss: 0.00001783
Iteration 340/1000 | Loss: 0.00001783
Iteration 341/1000 | Loss: 0.00001783
Iteration 342/1000 | Loss: 0.00001783
Iteration 343/1000 | Loss: 0.00001783
Iteration 344/1000 | Loss: 0.00001783
Iteration 345/1000 | Loss: 0.00001783
Iteration 346/1000 | Loss: 0.00001783
Iteration 347/1000 | Loss: 0.00001783
Iteration 348/1000 | Loss: 0.00001783
Iteration 349/1000 | Loss: 0.00001783
Iteration 350/1000 | Loss: 0.00001783
Iteration 351/1000 | Loss: 0.00001783
Iteration 352/1000 | Loss: 0.00001783
Iteration 353/1000 | Loss: 0.00001783
Iteration 354/1000 | Loss: 0.00001783
Iteration 355/1000 | Loss: 0.00001783
Iteration 356/1000 | Loss: 0.00001783
Iteration 357/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [1.783193147275597e-05, 1.783193147275597e-05, 1.783193147275597e-05, 1.783193147275597e-05, 1.783193147275597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.783193147275597e-05

Optimization complete. Final v2v error: 3.3342788219451904 mm

Highest mean error: 5.754583835601807 mm for frame 101

Lowest mean error: 2.8837101459503174 mm for frame 124

Saving results

Total time: 202.3016800880432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777788
Iteration 2/25 | Loss: 0.00133985
Iteration 3/25 | Loss: 0.00124497
Iteration 4/25 | Loss: 0.00123746
Iteration 5/25 | Loss: 0.00123601
Iteration 6/25 | Loss: 0.00123601
Iteration 7/25 | Loss: 0.00123601
Iteration 8/25 | Loss: 0.00123601
Iteration 9/25 | Loss: 0.00123601
Iteration 10/25 | Loss: 0.00123601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012360143009573221, 0.0012360143009573221, 0.0012360143009573221, 0.0012360143009573221, 0.0012360143009573221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012360143009573221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42683291
Iteration 2/25 | Loss: 0.00076782
Iteration 3/25 | Loss: 0.00076781
Iteration 4/25 | Loss: 0.00076781
Iteration 5/25 | Loss: 0.00076781
Iteration 6/25 | Loss: 0.00076781
Iteration 7/25 | Loss: 0.00076781
Iteration 8/25 | Loss: 0.00076781
Iteration 9/25 | Loss: 0.00076781
Iteration 10/25 | Loss: 0.00076781
Iteration 11/25 | Loss: 0.00076781
Iteration 12/25 | Loss: 0.00076781
Iteration 13/25 | Loss: 0.00076781
Iteration 14/25 | Loss: 0.00076781
Iteration 15/25 | Loss: 0.00076781
Iteration 16/25 | Loss: 0.00076781
Iteration 17/25 | Loss: 0.00076781
Iteration 18/25 | Loss: 0.00076781
Iteration 19/25 | Loss: 0.00076781
Iteration 20/25 | Loss: 0.00076781
Iteration 21/25 | Loss: 0.00076781
Iteration 22/25 | Loss: 0.00076781
Iteration 23/25 | Loss: 0.00076781
Iteration 24/25 | Loss: 0.00076781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007678084075450897, 0.0007678084075450897, 0.0007678084075450897, 0.0007678084075450897, 0.0007678084075450897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007678084075450897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076781
Iteration 2/1000 | Loss: 0.00002631
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001461
Iteration 6/1000 | Loss: 0.00001382
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001300
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001241
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001237
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001222
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001199
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001197
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001191
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001166
Iteration 125/1000 | Loss: 0.00001166
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001166
Iteration 128/1000 | Loss: 0.00001166
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001165
Iteration 131/1000 | Loss: 0.00001165
Iteration 132/1000 | Loss: 0.00001165
Iteration 133/1000 | Loss: 0.00001165
Iteration 134/1000 | Loss: 0.00001165
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001165
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001165
Iteration 140/1000 | Loss: 0.00001164
Iteration 141/1000 | Loss: 0.00001164
Iteration 142/1000 | Loss: 0.00001164
Iteration 143/1000 | Loss: 0.00001164
Iteration 144/1000 | Loss: 0.00001164
Iteration 145/1000 | Loss: 0.00001164
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001163
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001163
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001162
Iteration 165/1000 | Loss: 0.00001162
Iteration 166/1000 | Loss: 0.00001162
Iteration 167/1000 | Loss: 0.00001162
Iteration 168/1000 | Loss: 0.00001161
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001159
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001159
Iteration 181/1000 | Loss: 0.00001159
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001158
Iteration 187/1000 | Loss: 0.00001158
Iteration 188/1000 | Loss: 0.00001158
Iteration 189/1000 | Loss: 0.00001158
Iteration 190/1000 | Loss: 0.00001158
Iteration 191/1000 | Loss: 0.00001158
Iteration 192/1000 | Loss: 0.00001158
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Iteration 205/1000 | Loss: 0.00001158
Iteration 206/1000 | Loss: 0.00001158
Iteration 207/1000 | Loss: 0.00001158
Iteration 208/1000 | Loss: 0.00001158
Iteration 209/1000 | Loss: 0.00001158
Iteration 210/1000 | Loss: 0.00001158
Iteration 211/1000 | Loss: 0.00001158
Iteration 212/1000 | Loss: 0.00001158
Iteration 213/1000 | Loss: 0.00001158
Iteration 214/1000 | Loss: 0.00001158
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001158
Iteration 219/1000 | Loss: 0.00001158
Iteration 220/1000 | Loss: 0.00001158
Iteration 221/1000 | Loss: 0.00001158
Iteration 222/1000 | Loss: 0.00001158
Iteration 223/1000 | Loss: 0.00001158
Iteration 224/1000 | Loss: 0.00001158
Iteration 225/1000 | Loss: 0.00001158
Iteration 226/1000 | Loss: 0.00001158
Iteration 227/1000 | Loss: 0.00001158
Iteration 228/1000 | Loss: 0.00001158
Iteration 229/1000 | Loss: 0.00001158
Iteration 230/1000 | Loss: 0.00001158
Iteration 231/1000 | Loss: 0.00001158
Iteration 232/1000 | Loss: 0.00001158
Iteration 233/1000 | Loss: 0.00001158
Iteration 234/1000 | Loss: 0.00001158
Iteration 235/1000 | Loss: 0.00001158
Iteration 236/1000 | Loss: 0.00001158
Iteration 237/1000 | Loss: 0.00001158
Iteration 238/1000 | Loss: 0.00001158
Iteration 239/1000 | Loss: 0.00001158
Iteration 240/1000 | Loss: 0.00001158
Iteration 241/1000 | Loss: 0.00001158
Iteration 242/1000 | Loss: 0.00001158
Iteration 243/1000 | Loss: 0.00001158
Iteration 244/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.1576640645216685e-05, 1.1576640645216685e-05, 1.1576640645216685e-05, 1.1576640645216685e-05, 1.1576640645216685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1576640645216685e-05

Optimization complete. Final v2v error: 2.9056994915008545 mm

Highest mean error: 3.139681816101074 mm for frame 74

Lowest mean error: 2.771869659423828 mm for frame 176

Saving results

Total time: 41.99434971809387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840741
Iteration 2/25 | Loss: 0.00133376
Iteration 3/25 | Loss: 0.00125780
Iteration 4/25 | Loss: 0.00124414
Iteration 5/25 | Loss: 0.00123906
Iteration 6/25 | Loss: 0.00123802
Iteration 7/25 | Loss: 0.00123795
Iteration 8/25 | Loss: 0.00123795
Iteration 9/25 | Loss: 0.00123795
Iteration 10/25 | Loss: 0.00123795
Iteration 11/25 | Loss: 0.00123795
Iteration 12/25 | Loss: 0.00123795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012379466788843274, 0.0012379466788843274, 0.0012379466788843274, 0.0012379466788843274, 0.0012379466788843274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012379466788843274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05488873
Iteration 2/25 | Loss: 0.00083146
Iteration 3/25 | Loss: 0.00083146
Iteration 4/25 | Loss: 0.00083146
Iteration 5/25 | Loss: 0.00083146
Iteration 6/25 | Loss: 0.00083146
Iteration 7/25 | Loss: 0.00083146
Iteration 8/25 | Loss: 0.00083146
Iteration 9/25 | Loss: 0.00083146
Iteration 10/25 | Loss: 0.00083146
Iteration 11/25 | Loss: 0.00083146
Iteration 12/25 | Loss: 0.00083146
Iteration 13/25 | Loss: 0.00083146
Iteration 14/25 | Loss: 0.00083146
Iteration 15/25 | Loss: 0.00083146
Iteration 16/25 | Loss: 0.00083146
Iteration 17/25 | Loss: 0.00083146
Iteration 18/25 | Loss: 0.00083146
Iteration 19/25 | Loss: 0.00083146
Iteration 20/25 | Loss: 0.00083146
Iteration 21/25 | Loss: 0.00083146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000831456040032208, 0.000831456040032208, 0.000831456040032208, 0.000831456040032208, 0.000831456040032208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000831456040032208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083146
Iteration 2/1000 | Loss: 0.00002892
Iteration 3/1000 | Loss: 0.00001986
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001501
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001394
Iteration 20/1000 | Loss: 0.00001393
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001390
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001360
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001350
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001324
Iteration 122/1000 | Loss: 0.00001324
Iteration 123/1000 | Loss: 0.00001324
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001323
Iteration 130/1000 | Loss: 0.00001323
Iteration 131/1000 | Loss: 0.00001323
Iteration 132/1000 | Loss: 0.00001323
Iteration 133/1000 | Loss: 0.00001323
Iteration 134/1000 | Loss: 0.00001323
Iteration 135/1000 | Loss: 0.00001323
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001323
Iteration 142/1000 | Loss: 0.00001323
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Iteration 152/1000 | Loss: 0.00001323
Iteration 153/1000 | Loss: 0.00001323
Iteration 154/1000 | Loss: 0.00001323
Iteration 155/1000 | Loss: 0.00001323
Iteration 156/1000 | Loss: 0.00001323
Iteration 157/1000 | Loss: 0.00001323
Iteration 158/1000 | Loss: 0.00001323
Iteration 159/1000 | Loss: 0.00001323
Iteration 160/1000 | Loss: 0.00001323
Iteration 161/1000 | Loss: 0.00001323
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001323
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001323
Iteration 174/1000 | Loss: 0.00001323
Iteration 175/1000 | Loss: 0.00001323
Iteration 176/1000 | Loss: 0.00001323
Iteration 177/1000 | Loss: 0.00001323
Iteration 178/1000 | Loss: 0.00001323
Iteration 179/1000 | Loss: 0.00001323
Iteration 180/1000 | Loss: 0.00001323
Iteration 181/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.3230099284555763e-05, 1.3230099284555763e-05, 1.3230099284555763e-05, 1.3230099284555763e-05, 1.3230099284555763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3230099284555763e-05

Optimization complete. Final v2v error: 3.0735809803009033 mm

Highest mean error: 3.610708236694336 mm for frame 59

Lowest mean error: 2.8415868282318115 mm for frame 100

Saving results

Total time: 39.62693214416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789339
Iteration 2/25 | Loss: 0.00135463
Iteration 3/25 | Loss: 0.00125408
Iteration 4/25 | Loss: 0.00124046
Iteration 5/25 | Loss: 0.00123632
Iteration 6/25 | Loss: 0.00123554
Iteration 7/25 | Loss: 0.00123554
Iteration 8/25 | Loss: 0.00123554
Iteration 9/25 | Loss: 0.00123554
Iteration 10/25 | Loss: 0.00123554
Iteration 11/25 | Loss: 0.00123554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012355445651337504, 0.0012355445651337504, 0.0012355445651337504, 0.0012355445651337504, 0.0012355445651337504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012355445651337504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42322659
Iteration 2/25 | Loss: 0.00089351
Iteration 3/25 | Loss: 0.00089351
Iteration 4/25 | Loss: 0.00089350
Iteration 5/25 | Loss: 0.00089350
Iteration 6/25 | Loss: 0.00089350
Iteration 7/25 | Loss: 0.00089350
Iteration 8/25 | Loss: 0.00089350
Iteration 9/25 | Loss: 0.00089350
Iteration 10/25 | Loss: 0.00089350
Iteration 11/25 | Loss: 0.00089350
Iteration 12/25 | Loss: 0.00089350
Iteration 13/25 | Loss: 0.00089350
Iteration 14/25 | Loss: 0.00089350
Iteration 15/25 | Loss: 0.00089350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000893502845428884, 0.000893502845428884, 0.000893502845428884, 0.000893502845428884, 0.000893502845428884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000893502845428884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089350
Iteration 2/1000 | Loss: 0.00004434
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001707
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001469
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001437
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001429
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001425
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001425
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001423
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001422
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001420
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001420
Iteration 41/1000 | Loss: 0.00001420
Iteration 42/1000 | Loss: 0.00001420
Iteration 43/1000 | Loss: 0.00001420
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001418
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001416
Iteration 55/1000 | Loss: 0.00001416
Iteration 56/1000 | Loss: 0.00001416
Iteration 57/1000 | Loss: 0.00001416
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001415
Iteration 71/1000 | Loss: 0.00001415
Iteration 72/1000 | Loss: 0.00001415
Iteration 73/1000 | Loss: 0.00001415
Iteration 74/1000 | Loss: 0.00001415
Iteration 75/1000 | Loss: 0.00001415
Iteration 76/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.4154186828818638e-05, 1.4154186828818638e-05, 1.4154186828818638e-05, 1.4154186828818638e-05, 1.4154186828818638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4154186828818638e-05

Optimization complete. Final v2v error: 3.245020866394043 mm

Highest mean error: 3.763093948364258 mm for frame 239

Lowest mean error: 2.9498090744018555 mm for frame 172

Saving results

Total time: 34.04725003242493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810232
Iteration 2/25 | Loss: 0.00172719
Iteration 3/25 | Loss: 0.00141733
Iteration 4/25 | Loss: 0.00138996
Iteration 5/25 | Loss: 0.00138495
Iteration 6/25 | Loss: 0.00138339
Iteration 7/25 | Loss: 0.00138339
Iteration 8/25 | Loss: 0.00138339
Iteration 9/25 | Loss: 0.00138339
Iteration 10/25 | Loss: 0.00138339
Iteration 11/25 | Loss: 0.00138339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013833880657330155, 0.0013833880657330155, 0.0013833880657330155, 0.0013833880657330155, 0.0013833880657330155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013833880657330155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46248269
Iteration 2/25 | Loss: 0.00088066
Iteration 3/25 | Loss: 0.00088066
Iteration 4/25 | Loss: 0.00088066
Iteration 5/25 | Loss: 0.00088066
Iteration 6/25 | Loss: 0.00088066
Iteration 7/25 | Loss: 0.00088065
Iteration 8/25 | Loss: 0.00088065
Iteration 9/25 | Loss: 0.00088065
Iteration 10/25 | Loss: 0.00088065
Iteration 11/25 | Loss: 0.00088065
Iteration 12/25 | Loss: 0.00088065
Iteration 13/25 | Loss: 0.00088065
Iteration 14/25 | Loss: 0.00088065
Iteration 15/25 | Loss: 0.00088065
Iteration 16/25 | Loss: 0.00088065
Iteration 17/25 | Loss: 0.00088065
Iteration 18/25 | Loss: 0.00088065
Iteration 19/25 | Loss: 0.00088065
Iteration 20/25 | Loss: 0.00088065
Iteration 21/25 | Loss: 0.00088065
Iteration 22/25 | Loss: 0.00088065
Iteration 23/25 | Loss: 0.00088065
Iteration 24/25 | Loss: 0.00088065
Iteration 25/25 | Loss: 0.00088065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088065
Iteration 2/1000 | Loss: 0.00005676
Iteration 3/1000 | Loss: 0.00003413
Iteration 4/1000 | Loss: 0.00003003
Iteration 5/1000 | Loss: 0.00002832
Iteration 6/1000 | Loss: 0.00002736
Iteration 7/1000 | Loss: 0.00002647
Iteration 8/1000 | Loss: 0.00002588
Iteration 9/1000 | Loss: 0.00002543
Iteration 10/1000 | Loss: 0.00002507
Iteration 11/1000 | Loss: 0.00002478
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002429
Iteration 14/1000 | Loss: 0.00002411
Iteration 15/1000 | Loss: 0.00002408
Iteration 16/1000 | Loss: 0.00002402
Iteration 17/1000 | Loss: 0.00002395
Iteration 18/1000 | Loss: 0.00002392
Iteration 19/1000 | Loss: 0.00002392
Iteration 20/1000 | Loss: 0.00002392
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002386
Iteration 23/1000 | Loss: 0.00002384
Iteration 24/1000 | Loss: 0.00002379
Iteration 25/1000 | Loss: 0.00002377
Iteration 26/1000 | Loss: 0.00002372
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002370
Iteration 30/1000 | Loss: 0.00002370
Iteration 31/1000 | Loss: 0.00002369
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002369
Iteration 34/1000 | Loss: 0.00002368
Iteration 35/1000 | Loss: 0.00002368
Iteration 36/1000 | Loss: 0.00002367
Iteration 37/1000 | Loss: 0.00002367
Iteration 38/1000 | Loss: 0.00002366
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002364
Iteration 43/1000 | Loss: 0.00002364
Iteration 44/1000 | Loss: 0.00002364
Iteration 45/1000 | Loss: 0.00002364
Iteration 46/1000 | Loss: 0.00002364
Iteration 47/1000 | Loss: 0.00002364
Iteration 48/1000 | Loss: 0.00002363
Iteration 49/1000 | Loss: 0.00002363
Iteration 50/1000 | Loss: 0.00002362
Iteration 51/1000 | Loss: 0.00002361
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002361
Iteration 54/1000 | Loss: 0.00002361
Iteration 55/1000 | Loss: 0.00002361
Iteration 56/1000 | Loss: 0.00002360
Iteration 57/1000 | Loss: 0.00002360
Iteration 58/1000 | Loss: 0.00002360
Iteration 59/1000 | Loss: 0.00002360
Iteration 60/1000 | Loss: 0.00002359
Iteration 61/1000 | Loss: 0.00002359
Iteration 62/1000 | Loss: 0.00002359
Iteration 63/1000 | Loss: 0.00002359
Iteration 64/1000 | Loss: 0.00002358
Iteration 65/1000 | Loss: 0.00002358
Iteration 66/1000 | Loss: 0.00002358
Iteration 67/1000 | Loss: 0.00002358
Iteration 68/1000 | Loss: 0.00002358
Iteration 69/1000 | Loss: 0.00002358
Iteration 70/1000 | Loss: 0.00002358
Iteration 71/1000 | Loss: 0.00002358
Iteration 72/1000 | Loss: 0.00002358
Iteration 73/1000 | Loss: 0.00002358
Iteration 74/1000 | Loss: 0.00002358
Iteration 75/1000 | Loss: 0.00002357
Iteration 76/1000 | Loss: 0.00002357
Iteration 77/1000 | Loss: 0.00002357
Iteration 78/1000 | Loss: 0.00002357
Iteration 79/1000 | Loss: 0.00002357
Iteration 80/1000 | Loss: 0.00002357
Iteration 81/1000 | Loss: 0.00002357
Iteration 82/1000 | Loss: 0.00002356
Iteration 83/1000 | Loss: 0.00002356
Iteration 84/1000 | Loss: 0.00002356
Iteration 85/1000 | Loss: 0.00002356
Iteration 86/1000 | Loss: 0.00002355
Iteration 87/1000 | Loss: 0.00002355
Iteration 88/1000 | Loss: 0.00002355
Iteration 89/1000 | Loss: 0.00002355
Iteration 90/1000 | Loss: 0.00002355
Iteration 91/1000 | Loss: 0.00002355
Iteration 92/1000 | Loss: 0.00002355
Iteration 93/1000 | Loss: 0.00002355
Iteration 94/1000 | Loss: 0.00002355
Iteration 95/1000 | Loss: 0.00002355
Iteration 96/1000 | Loss: 0.00002355
Iteration 97/1000 | Loss: 0.00002355
Iteration 98/1000 | Loss: 0.00002355
Iteration 99/1000 | Loss: 0.00002355
Iteration 100/1000 | Loss: 0.00002355
Iteration 101/1000 | Loss: 0.00002355
Iteration 102/1000 | Loss: 0.00002355
Iteration 103/1000 | Loss: 0.00002355
Iteration 104/1000 | Loss: 0.00002355
Iteration 105/1000 | Loss: 0.00002355
Iteration 106/1000 | Loss: 0.00002355
Iteration 107/1000 | Loss: 0.00002355
Iteration 108/1000 | Loss: 0.00002355
Iteration 109/1000 | Loss: 0.00002355
Iteration 110/1000 | Loss: 0.00002355
Iteration 111/1000 | Loss: 0.00002355
Iteration 112/1000 | Loss: 0.00002355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.3548662284156308e-05, 2.3548662284156308e-05, 2.3548662284156308e-05, 2.3548662284156308e-05, 2.3548662284156308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3548662284156308e-05

Optimization complete. Final v2v error: 3.9853811264038086 mm

Highest mean error: 5.269564151763916 mm for frame 149

Lowest mean error: 3.1909737586975098 mm for frame 8

Saving results

Total time: 38.76878333091736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484571
Iteration 2/25 | Loss: 0.00139653
Iteration 3/25 | Loss: 0.00131778
Iteration 4/25 | Loss: 0.00130642
Iteration 5/25 | Loss: 0.00130324
Iteration 6/25 | Loss: 0.00130324
Iteration 7/25 | Loss: 0.00130324
Iteration 8/25 | Loss: 0.00130324
Iteration 9/25 | Loss: 0.00130324
Iteration 10/25 | Loss: 0.00130324
Iteration 11/25 | Loss: 0.00130324
Iteration 12/25 | Loss: 0.00130324
Iteration 13/25 | Loss: 0.00130324
Iteration 14/25 | Loss: 0.00130324
Iteration 15/25 | Loss: 0.00130324
Iteration 16/25 | Loss: 0.00130324
Iteration 17/25 | Loss: 0.00130324
Iteration 18/25 | Loss: 0.00130324
Iteration 19/25 | Loss: 0.00130324
Iteration 20/25 | Loss: 0.00130324
Iteration 21/25 | Loss: 0.00130324
Iteration 22/25 | Loss: 0.00130324
Iteration 23/25 | Loss: 0.00130324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013032353017479181, 0.0013032353017479181, 0.0013032353017479181, 0.0013032353017479181, 0.0013032353017479181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013032353017479181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45544124
Iteration 2/25 | Loss: 0.00088756
Iteration 3/25 | Loss: 0.00088755
Iteration 4/25 | Loss: 0.00088755
Iteration 5/25 | Loss: 0.00088755
Iteration 6/25 | Loss: 0.00088755
Iteration 7/25 | Loss: 0.00088755
Iteration 8/25 | Loss: 0.00088755
Iteration 9/25 | Loss: 0.00088755
Iteration 10/25 | Loss: 0.00088755
Iteration 11/25 | Loss: 0.00088755
Iteration 12/25 | Loss: 0.00088755
Iteration 13/25 | Loss: 0.00088755
Iteration 14/25 | Loss: 0.00088755
Iteration 15/25 | Loss: 0.00088755
Iteration 16/25 | Loss: 0.00088755
Iteration 17/25 | Loss: 0.00088755
Iteration 18/25 | Loss: 0.00088755
Iteration 19/25 | Loss: 0.00088755
Iteration 20/25 | Loss: 0.00088755
Iteration 21/25 | Loss: 0.00088755
Iteration 22/25 | Loss: 0.00088755
Iteration 23/25 | Loss: 0.00088755
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008875516359694302, 0.0008875516359694302, 0.0008875516359694302, 0.0008875516359694302, 0.0008875516359694302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008875516359694302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088755
Iteration 2/1000 | Loss: 0.00003329
Iteration 3/1000 | Loss: 0.00002332
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00002032
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001829
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001748
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001729
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00001725
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00001703
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001700
Iteration 37/1000 | Loss: 0.00001700
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001696
Iteration 44/1000 | Loss: 0.00001696
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001694
Iteration 48/1000 | Loss: 0.00001694
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001686
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001685
Iteration 58/1000 | Loss: 0.00001684
Iteration 59/1000 | Loss: 0.00001684
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001681
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001679
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001678
Iteration 78/1000 | Loss: 0.00001678
Iteration 79/1000 | Loss: 0.00001678
Iteration 80/1000 | Loss: 0.00001678
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001675
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001675
Iteration 93/1000 | Loss: 0.00001675
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001674
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001673
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001671
Iteration 111/1000 | Loss: 0.00001671
Iteration 112/1000 | Loss: 0.00001671
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001670
Iteration 115/1000 | Loss: 0.00001670
Iteration 116/1000 | Loss: 0.00001670
Iteration 117/1000 | Loss: 0.00001670
Iteration 118/1000 | Loss: 0.00001670
Iteration 119/1000 | Loss: 0.00001670
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001670
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001670
Iteration 140/1000 | Loss: 0.00001670
Iteration 141/1000 | Loss: 0.00001670
Iteration 142/1000 | Loss: 0.00001670
Iteration 143/1000 | Loss: 0.00001670
Iteration 144/1000 | Loss: 0.00001670
Iteration 145/1000 | Loss: 0.00001670
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001670
Iteration 149/1000 | Loss: 0.00001670
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001670
Iteration 152/1000 | Loss: 0.00001670
Iteration 153/1000 | Loss: 0.00001670
Iteration 154/1000 | Loss: 0.00001670
Iteration 155/1000 | Loss: 0.00001670
Iteration 156/1000 | Loss: 0.00001670
Iteration 157/1000 | Loss: 0.00001670
Iteration 158/1000 | Loss: 0.00001670
Iteration 159/1000 | Loss: 0.00001670
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001670
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001670
Iteration 165/1000 | Loss: 0.00001670
Iteration 166/1000 | Loss: 0.00001670
Iteration 167/1000 | Loss: 0.00001670
Iteration 168/1000 | Loss: 0.00001670
Iteration 169/1000 | Loss: 0.00001670
Iteration 170/1000 | Loss: 0.00001670
Iteration 171/1000 | Loss: 0.00001670
Iteration 172/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.669679295446258e-05, 1.669679295446258e-05, 1.669679295446258e-05, 1.669679295446258e-05, 1.669679295446258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.669679295446258e-05

Optimization complete. Final v2v error: 3.444505214691162 mm

Highest mean error: 3.7685720920562744 mm for frame 102

Lowest mean error: 3.214240789413452 mm for frame 119

Saving results

Total time: 43.52901482582092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794802
Iteration 2/25 | Loss: 0.00140150
Iteration 3/25 | Loss: 0.00126585
Iteration 4/25 | Loss: 0.00124351
Iteration 5/25 | Loss: 0.00123834
Iteration 6/25 | Loss: 0.00123805
Iteration 7/25 | Loss: 0.00123805
Iteration 8/25 | Loss: 0.00123805
Iteration 9/25 | Loss: 0.00123805
Iteration 10/25 | Loss: 0.00123805
Iteration 11/25 | Loss: 0.00123805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001238052500411868, 0.001238052500411868, 0.001238052500411868, 0.001238052500411868, 0.001238052500411868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238052500411868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42318678
Iteration 2/25 | Loss: 0.00077714
Iteration 3/25 | Loss: 0.00077714
Iteration 4/25 | Loss: 0.00077713
Iteration 5/25 | Loss: 0.00077713
Iteration 6/25 | Loss: 0.00077713
Iteration 7/25 | Loss: 0.00077713
Iteration 8/25 | Loss: 0.00077713
Iteration 9/25 | Loss: 0.00077713
Iteration 10/25 | Loss: 0.00077713
Iteration 11/25 | Loss: 0.00077713
Iteration 12/25 | Loss: 0.00077713
Iteration 13/25 | Loss: 0.00077713
Iteration 14/25 | Loss: 0.00077713
Iteration 15/25 | Loss: 0.00077713
Iteration 16/25 | Loss: 0.00077713
Iteration 17/25 | Loss: 0.00077713
Iteration 18/25 | Loss: 0.00077713
Iteration 19/25 | Loss: 0.00077713
Iteration 20/25 | Loss: 0.00077713
Iteration 21/25 | Loss: 0.00077713
Iteration 22/25 | Loss: 0.00077713
Iteration 23/25 | Loss: 0.00077713
Iteration 24/25 | Loss: 0.00077713
Iteration 25/25 | Loss: 0.00077713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077713
Iteration 2/1000 | Loss: 0.00002731
Iteration 3/1000 | Loss: 0.00001947
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001510
Iteration 7/1000 | Loss: 0.00001453
Iteration 8/1000 | Loss: 0.00001415
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001345
Iteration 16/1000 | Loss: 0.00001345
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001336
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001331
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001316
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001309
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001306
Iteration 52/1000 | Loss: 0.00001306
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001293
Iteration 78/1000 | Loss: 0.00001293
Iteration 79/1000 | Loss: 0.00001293
Iteration 80/1000 | Loss: 0.00001292
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001290
Iteration 85/1000 | Loss: 0.00001290
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001289
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001288
Iteration 92/1000 | Loss: 0.00001288
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001287
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001286
Iteration 100/1000 | Loss: 0.00001286
Iteration 101/1000 | Loss: 0.00001286
Iteration 102/1000 | Loss: 0.00001286
Iteration 103/1000 | Loss: 0.00001286
Iteration 104/1000 | Loss: 0.00001285
Iteration 105/1000 | Loss: 0.00001285
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001282
Iteration 120/1000 | Loss: 0.00001282
Iteration 121/1000 | Loss: 0.00001281
Iteration 122/1000 | Loss: 0.00001281
Iteration 123/1000 | Loss: 0.00001281
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001280
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001278
Iteration 136/1000 | Loss: 0.00001278
Iteration 137/1000 | Loss: 0.00001278
Iteration 138/1000 | Loss: 0.00001278
Iteration 139/1000 | Loss: 0.00001278
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001277
Iteration 152/1000 | Loss: 0.00001277
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.2771834008162841e-05, 1.2771834008162841e-05, 1.2771834008162841e-05, 1.2771834008162841e-05, 1.2771834008162841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2771834008162841e-05

Optimization complete. Final v2v error: 3.052438974380493 mm

Highest mean error: 3.454341173171997 mm for frame 104

Lowest mean error: 2.8000142574310303 mm for frame 246

Saving results

Total time: 43.287320375442505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470318
Iteration 2/25 | Loss: 0.00136521
Iteration 3/25 | Loss: 0.00128104
Iteration 4/25 | Loss: 0.00127033
Iteration 5/25 | Loss: 0.00126835
Iteration 6/25 | Loss: 0.00126835
Iteration 7/25 | Loss: 0.00126835
Iteration 8/25 | Loss: 0.00126835
Iteration 9/25 | Loss: 0.00126835
Iteration 10/25 | Loss: 0.00126835
Iteration 11/25 | Loss: 0.00126835
Iteration 12/25 | Loss: 0.00126835
Iteration 13/25 | Loss: 0.00126835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012683536624535918, 0.0012683536624535918, 0.0012683536624535918, 0.0012683536624535918, 0.0012683536624535918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012683536624535918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.50181913
Iteration 2/25 | Loss: 0.00086541
Iteration 3/25 | Loss: 0.00086541
Iteration 4/25 | Loss: 0.00086541
Iteration 5/25 | Loss: 0.00086541
Iteration 6/25 | Loss: 0.00086541
Iteration 7/25 | Loss: 0.00086541
Iteration 8/25 | Loss: 0.00086541
Iteration 9/25 | Loss: 0.00086541
Iteration 10/25 | Loss: 0.00086541
Iteration 11/25 | Loss: 0.00086541
Iteration 12/25 | Loss: 0.00086541
Iteration 13/25 | Loss: 0.00086541
Iteration 14/25 | Loss: 0.00086541
Iteration 15/25 | Loss: 0.00086541
Iteration 16/25 | Loss: 0.00086541
Iteration 17/25 | Loss: 0.00086541
Iteration 18/25 | Loss: 0.00086541
Iteration 19/25 | Loss: 0.00086541
Iteration 20/25 | Loss: 0.00086541
Iteration 21/25 | Loss: 0.00086541
Iteration 22/25 | Loss: 0.00086541
Iteration 23/25 | Loss: 0.00086541
Iteration 24/25 | Loss: 0.00086541
Iteration 25/25 | Loss: 0.00086541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086541
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001815
Iteration 5/1000 | Loss: 0.00001699
Iteration 6/1000 | Loss: 0.00001644
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001498
Iteration 15/1000 | Loss: 0.00001496
Iteration 16/1000 | Loss: 0.00001489
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001469
Iteration 19/1000 | Loss: 0.00001469
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001453
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001438
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001437
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001425
Iteration 62/1000 | Loss: 0.00001425
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001419
Iteration 78/1000 | Loss: 0.00001419
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001417
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001416
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001414
Iteration 94/1000 | Loss: 0.00001413
Iteration 95/1000 | Loss: 0.00001413
Iteration 96/1000 | Loss: 0.00001413
Iteration 97/1000 | Loss: 0.00001413
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001412
Iteration 100/1000 | Loss: 0.00001412
Iteration 101/1000 | Loss: 0.00001412
Iteration 102/1000 | Loss: 0.00001412
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001410
Iteration 111/1000 | Loss: 0.00001410
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001410
Iteration 119/1000 | Loss: 0.00001410
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001410
Iteration 130/1000 | Loss: 0.00001410
Iteration 131/1000 | Loss: 0.00001410
Iteration 132/1000 | Loss: 0.00001410
Iteration 133/1000 | Loss: 0.00001410
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001410
Iteration 136/1000 | Loss: 0.00001410
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001410
Iteration 142/1000 | Loss: 0.00001410
Iteration 143/1000 | Loss: 0.00001410
Iteration 144/1000 | Loss: 0.00001410
Iteration 145/1000 | Loss: 0.00001410
Iteration 146/1000 | Loss: 0.00001410
Iteration 147/1000 | Loss: 0.00001410
Iteration 148/1000 | Loss: 0.00001410
Iteration 149/1000 | Loss: 0.00001410
Iteration 150/1000 | Loss: 0.00001410
Iteration 151/1000 | Loss: 0.00001410
Iteration 152/1000 | Loss: 0.00001410
Iteration 153/1000 | Loss: 0.00001410
Iteration 154/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.4101722626946867e-05, 1.4101722626946867e-05, 1.4101722626946867e-05, 1.4101722626946867e-05, 1.4101722626946867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4101722626946867e-05

Optimization complete. Final v2v error: 3.20621395111084 mm

Highest mean error: 3.3637654781341553 mm for frame 246

Lowest mean error: 3.029242992401123 mm for frame 223

Saving results

Total time: 43.50560474395752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_020/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_020/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807653
Iteration 2/25 | Loss: 0.00187252
Iteration 3/25 | Loss: 0.00138348
Iteration 4/25 | Loss: 0.00133232
Iteration 5/25 | Loss: 0.00132568
Iteration 6/25 | Loss: 0.00132550
Iteration 7/25 | Loss: 0.00132550
Iteration 8/25 | Loss: 0.00132550
Iteration 9/25 | Loss: 0.00132550
Iteration 10/25 | Loss: 0.00132550
Iteration 11/25 | Loss: 0.00132550
Iteration 12/25 | Loss: 0.00132550
Iteration 13/25 | Loss: 0.00132550
Iteration 14/25 | Loss: 0.00132550
Iteration 15/25 | Loss: 0.00132550
Iteration 16/25 | Loss: 0.00132550
Iteration 17/25 | Loss: 0.00132550
Iteration 18/25 | Loss: 0.00132550
Iteration 19/25 | Loss: 0.00132550
Iteration 20/25 | Loss: 0.00132550
Iteration 21/25 | Loss: 0.00132550
Iteration 22/25 | Loss: 0.00132550
Iteration 23/25 | Loss: 0.00132550
Iteration 24/25 | Loss: 0.00132550
Iteration 25/25 | Loss: 0.00132550

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42227399
Iteration 2/25 | Loss: 0.00068816
Iteration 3/25 | Loss: 0.00068816
Iteration 4/25 | Loss: 0.00068816
Iteration 5/25 | Loss: 0.00068816
Iteration 6/25 | Loss: 0.00068816
Iteration 7/25 | Loss: 0.00068816
Iteration 8/25 | Loss: 0.00068816
Iteration 9/25 | Loss: 0.00068816
Iteration 10/25 | Loss: 0.00068816
Iteration 11/25 | Loss: 0.00068816
Iteration 12/25 | Loss: 0.00068816
Iteration 13/25 | Loss: 0.00068816
Iteration 14/25 | Loss: 0.00068816
Iteration 15/25 | Loss: 0.00068816
Iteration 16/25 | Loss: 0.00068816
Iteration 17/25 | Loss: 0.00068816
Iteration 18/25 | Loss: 0.00068816
Iteration 19/25 | Loss: 0.00068816
Iteration 20/25 | Loss: 0.00068816
Iteration 21/25 | Loss: 0.00068816
Iteration 22/25 | Loss: 0.00068816
Iteration 23/25 | Loss: 0.00068816
Iteration 24/25 | Loss: 0.00068816
Iteration 25/25 | Loss: 0.00068816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068816
Iteration 2/1000 | Loss: 0.00003224
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002464
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002311
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002193
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002179
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002159
Iteration 18/1000 | Loss: 0.00002158
Iteration 19/1000 | Loss: 0.00002158
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002157
Iteration 22/1000 | Loss: 0.00002157
Iteration 23/1000 | Loss: 0.00002157
Iteration 24/1000 | Loss: 0.00002157
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00002156
Iteration 27/1000 | Loss: 0.00002153
Iteration 28/1000 | Loss: 0.00002151
Iteration 29/1000 | Loss: 0.00002149
Iteration 30/1000 | Loss: 0.00002149
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00002146
Iteration 35/1000 | Loss: 0.00002146
Iteration 36/1000 | Loss: 0.00002146
Iteration 37/1000 | Loss: 0.00002146
Iteration 38/1000 | Loss: 0.00002145
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002144
Iteration 41/1000 | Loss: 0.00002144
Iteration 42/1000 | Loss: 0.00002144
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002143
Iteration 45/1000 | Loss: 0.00002143
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002142
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00002142
Iteration 50/1000 | Loss: 0.00002142
Iteration 51/1000 | Loss: 0.00002142
Iteration 52/1000 | Loss: 0.00002142
Iteration 53/1000 | Loss: 0.00002141
Iteration 54/1000 | Loss: 0.00002141
Iteration 55/1000 | Loss: 0.00002140
Iteration 56/1000 | Loss: 0.00002140
Iteration 57/1000 | Loss: 0.00002140
Iteration 58/1000 | Loss: 0.00002140
Iteration 59/1000 | Loss: 0.00002139
Iteration 60/1000 | Loss: 0.00002139
Iteration 61/1000 | Loss: 0.00002139
Iteration 62/1000 | Loss: 0.00002139
Iteration 63/1000 | Loss: 0.00002139
Iteration 64/1000 | Loss: 0.00002139
Iteration 65/1000 | Loss: 0.00002139
Iteration 66/1000 | Loss: 0.00002138
Iteration 67/1000 | Loss: 0.00002138
Iteration 68/1000 | Loss: 0.00002138
Iteration 69/1000 | Loss: 0.00002138
Iteration 70/1000 | Loss: 0.00002138
Iteration 71/1000 | Loss: 0.00002138
Iteration 72/1000 | Loss: 0.00002138
Iteration 73/1000 | Loss: 0.00002137
Iteration 74/1000 | Loss: 0.00002137
Iteration 75/1000 | Loss: 0.00002137
Iteration 76/1000 | Loss: 0.00002137
Iteration 77/1000 | Loss: 0.00002137
Iteration 78/1000 | Loss: 0.00002137
Iteration 79/1000 | Loss: 0.00002137
Iteration 80/1000 | Loss: 0.00002137
Iteration 81/1000 | Loss: 0.00002137
Iteration 82/1000 | Loss: 0.00002137
Iteration 83/1000 | Loss: 0.00002137
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002136
Iteration 86/1000 | Loss: 0.00002136
Iteration 87/1000 | Loss: 0.00002136
Iteration 88/1000 | Loss: 0.00002136
Iteration 89/1000 | Loss: 0.00002135
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002134
Iteration 95/1000 | Loss: 0.00002134
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002134
Iteration 101/1000 | Loss: 0.00002134
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002133
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002132
Iteration 109/1000 | Loss: 0.00002132
Iteration 110/1000 | Loss: 0.00002132
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002132
Iteration 113/1000 | Loss: 0.00002132
Iteration 114/1000 | Loss: 0.00002132
Iteration 115/1000 | Loss: 0.00002132
Iteration 116/1000 | Loss: 0.00002131
Iteration 117/1000 | Loss: 0.00002131
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002131
Iteration 120/1000 | Loss: 0.00002131
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002131
Iteration 124/1000 | Loss: 0.00002131
Iteration 125/1000 | Loss: 0.00002131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.1313393517630175e-05, 2.1313393517630175e-05, 2.1313393517630175e-05, 2.1313393517630175e-05, 2.1313393517630175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1313393517630175e-05

Optimization complete. Final v2v error: 3.84735369682312 mm

Highest mean error: 3.9187586307525635 mm for frame 76

Lowest mean error: 3.467012643814087 mm for frame 5

Saving results

Total time: 38.89535927772522
