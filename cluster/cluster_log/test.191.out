Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=191, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10696-10751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384045
Iteration 2/25 | Loss: 0.00116199
Iteration 3/25 | Loss: 0.00112132
Iteration 4/25 | Loss: 0.00111665
Iteration 5/25 | Loss: 0.00111595
Iteration 6/25 | Loss: 0.00111595
Iteration 7/25 | Loss: 0.00111595
Iteration 8/25 | Loss: 0.00111595
Iteration 9/25 | Loss: 0.00111595
Iteration 10/25 | Loss: 0.00111595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011159525020048022, 0.0011159525020048022, 0.0011159525020048022, 0.0011159525020048022, 0.0011159525020048022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011159525020048022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28995466
Iteration 2/25 | Loss: 0.00118546
Iteration 3/25 | Loss: 0.00118545
Iteration 4/25 | Loss: 0.00118545
Iteration 5/25 | Loss: 0.00118545
Iteration 6/25 | Loss: 0.00118545
Iteration 7/25 | Loss: 0.00118545
Iteration 8/25 | Loss: 0.00118545
Iteration 9/25 | Loss: 0.00118545
Iteration 10/25 | Loss: 0.00118545
Iteration 11/25 | Loss: 0.00118545
Iteration 12/25 | Loss: 0.00118545
Iteration 13/25 | Loss: 0.00118545
Iteration 14/25 | Loss: 0.00118545
Iteration 15/25 | Loss: 0.00118545
Iteration 16/25 | Loss: 0.00118545
Iteration 17/25 | Loss: 0.00118545
Iteration 18/25 | Loss: 0.00118545
Iteration 19/25 | Loss: 0.00118545
Iteration 20/25 | Loss: 0.00118545
Iteration 21/25 | Loss: 0.00118545
Iteration 22/25 | Loss: 0.00118545
Iteration 23/25 | Loss: 0.00118545
Iteration 24/25 | Loss: 0.00118545
Iteration 25/25 | Loss: 0.00118545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118545
Iteration 2/1000 | Loss: 0.00002414
Iteration 3/1000 | Loss: 0.00001792
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001197
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001102
Iteration 15/1000 | Loss: 0.00001095
Iteration 16/1000 | Loss: 0.00001093
Iteration 17/1000 | Loss: 0.00001090
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001083
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001075
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001070
Iteration 28/1000 | Loss: 0.00001070
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001070
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001069
Iteration 34/1000 | Loss: 0.00001068
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001062
Iteration 40/1000 | Loss: 0.00001062
Iteration 41/1000 | Loss: 0.00001062
Iteration 42/1000 | Loss: 0.00001061
Iteration 43/1000 | Loss: 0.00001061
Iteration 44/1000 | Loss: 0.00001060
Iteration 45/1000 | Loss: 0.00001060
Iteration 46/1000 | Loss: 0.00001053
Iteration 47/1000 | Loss: 0.00001047
Iteration 48/1000 | Loss: 0.00001047
Iteration 49/1000 | Loss: 0.00001046
Iteration 50/1000 | Loss: 0.00001045
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001035
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001033
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001031
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001029
Iteration 69/1000 | Loss: 0.00001029
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001026
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001025
Iteration 74/1000 | Loss: 0.00001025
Iteration 75/1000 | Loss: 0.00001025
Iteration 76/1000 | Loss: 0.00001025
Iteration 77/1000 | Loss: 0.00001024
Iteration 78/1000 | Loss: 0.00001023
Iteration 79/1000 | Loss: 0.00001023
Iteration 80/1000 | Loss: 0.00001023
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001022
Iteration 83/1000 | Loss: 0.00001022
Iteration 84/1000 | Loss: 0.00001022
Iteration 85/1000 | Loss: 0.00001022
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001016
Iteration 91/1000 | Loss: 0.00001016
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001016
Iteration 94/1000 | Loss: 0.00001016
Iteration 95/1000 | Loss: 0.00001016
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001015
Iteration 99/1000 | Loss: 0.00001015
Iteration 100/1000 | Loss: 0.00001015
Iteration 101/1000 | Loss: 0.00001015
Iteration 102/1000 | Loss: 0.00001015
Iteration 103/1000 | Loss: 0.00001015
Iteration 104/1000 | Loss: 0.00001015
Iteration 105/1000 | Loss: 0.00001015
Iteration 106/1000 | Loss: 0.00001014
Iteration 107/1000 | Loss: 0.00001014
Iteration 108/1000 | Loss: 0.00001013
Iteration 109/1000 | Loss: 0.00001013
Iteration 110/1000 | Loss: 0.00001013
Iteration 111/1000 | Loss: 0.00001013
Iteration 112/1000 | Loss: 0.00001013
Iteration 113/1000 | Loss: 0.00001013
Iteration 114/1000 | Loss: 0.00001013
Iteration 115/1000 | Loss: 0.00001013
Iteration 116/1000 | Loss: 0.00001013
Iteration 117/1000 | Loss: 0.00001012
Iteration 118/1000 | Loss: 0.00001012
Iteration 119/1000 | Loss: 0.00001012
Iteration 120/1000 | Loss: 0.00001012
Iteration 121/1000 | Loss: 0.00001012
Iteration 122/1000 | Loss: 0.00001012
Iteration 123/1000 | Loss: 0.00001012
Iteration 124/1000 | Loss: 0.00001012
Iteration 125/1000 | Loss: 0.00001012
Iteration 126/1000 | Loss: 0.00001011
Iteration 127/1000 | Loss: 0.00001011
Iteration 128/1000 | Loss: 0.00001011
Iteration 129/1000 | Loss: 0.00001011
Iteration 130/1000 | Loss: 0.00001011
Iteration 131/1000 | Loss: 0.00001011
Iteration 132/1000 | Loss: 0.00001011
Iteration 133/1000 | Loss: 0.00001011
Iteration 134/1000 | Loss: 0.00001011
Iteration 135/1000 | Loss: 0.00001010
Iteration 136/1000 | Loss: 0.00001010
Iteration 137/1000 | Loss: 0.00001010
Iteration 138/1000 | Loss: 0.00001010
Iteration 139/1000 | Loss: 0.00001010
Iteration 140/1000 | Loss: 0.00001010
Iteration 141/1000 | Loss: 0.00001010
Iteration 142/1000 | Loss: 0.00001010
Iteration 143/1000 | Loss: 0.00001010
Iteration 144/1000 | Loss: 0.00001010
Iteration 145/1000 | Loss: 0.00001010
Iteration 146/1000 | Loss: 0.00001009
Iteration 147/1000 | Loss: 0.00001009
Iteration 148/1000 | Loss: 0.00001009
Iteration 149/1000 | Loss: 0.00001009
Iteration 150/1000 | Loss: 0.00001009
Iteration 151/1000 | Loss: 0.00001009
Iteration 152/1000 | Loss: 0.00001009
Iteration 153/1000 | Loss: 0.00001009
Iteration 154/1000 | Loss: 0.00001009
Iteration 155/1000 | Loss: 0.00001009
Iteration 156/1000 | Loss: 0.00001009
Iteration 157/1000 | Loss: 0.00001009
Iteration 158/1000 | Loss: 0.00001009
Iteration 159/1000 | Loss: 0.00001009
Iteration 160/1000 | Loss: 0.00001009
Iteration 161/1000 | Loss: 0.00001009
Iteration 162/1000 | Loss: 0.00001009
Iteration 163/1000 | Loss: 0.00001009
Iteration 164/1000 | Loss: 0.00001008
Iteration 165/1000 | Loss: 0.00001008
Iteration 166/1000 | Loss: 0.00001008
Iteration 167/1000 | Loss: 0.00001008
Iteration 168/1000 | Loss: 0.00001008
Iteration 169/1000 | Loss: 0.00001008
Iteration 170/1000 | Loss: 0.00001008
Iteration 171/1000 | Loss: 0.00001008
Iteration 172/1000 | Loss: 0.00001007
Iteration 173/1000 | Loss: 0.00001007
Iteration 174/1000 | Loss: 0.00001007
Iteration 175/1000 | Loss: 0.00001007
Iteration 176/1000 | Loss: 0.00001007
Iteration 177/1000 | Loss: 0.00001007
Iteration 178/1000 | Loss: 0.00001007
Iteration 179/1000 | Loss: 0.00001007
Iteration 180/1000 | Loss: 0.00001007
Iteration 181/1000 | Loss: 0.00001007
Iteration 182/1000 | Loss: 0.00001007
Iteration 183/1000 | Loss: 0.00001007
Iteration 184/1000 | Loss: 0.00001007
Iteration 185/1000 | Loss: 0.00001007
Iteration 186/1000 | Loss: 0.00001007
Iteration 187/1000 | Loss: 0.00001006
Iteration 188/1000 | Loss: 0.00001006
Iteration 189/1000 | Loss: 0.00001006
Iteration 190/1000 | Loss: 0.00001006
Iteration 191/1000 | Loss: 0.00001006
Iteration 192/1000 | Loss: 0.00001006
Iteration 193/1000 | Loss: 0.00001006
Iteration 194/1000 | Loss: 0.00001006
Iteration 195/1000 | Loss: 0.00001005
Iteration 196/1000 | Loss: 0.00001005
Iteration 197/1000 | Loss: 0.00001005
Iteration 198/1000 | Loss: 0.00001005
Iteration 199/1000 | Loss: 0.00001005
Iteration 200/1000 | Loss: 0.00001005
Iteration 201/1000 | Loss: 0.00001005
Iteration 202/1000 | Loss: 0.00001005
Iteration 203/1000 | Loss: 0.00001005
Iteration 204/1000 | Loss: 0.00001005
Iteration 205/1000 | Loss: 0.00001005
Iteration 206/1000 | Loss: 0.00001005
Iteration 207/1000 | Loss: 0.00001005
Iteration 208/1000 | Loss: 0.00001004
Iteration 209/1000 | Loss: 0.00001004
Iteration 210/1000 | Loss: 0.00001004
Iteration 211/1000 | Loss: 0.00001004
Iteration 212/1000 | Loss: 0.00001004
Iteration 213/1000 | Loss: 0.00001004
Iteration 214/1000 | Loss: 0.00001004
Iteration 215/1000 | Loss: 0.00001004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0044397640740499e-05, 1.0044397640740499e-05, 1.0044397640740499e-05, 1.0044397640740499e-05, 1.0044397640740499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0044397640740499e-05

Optimization complete. Final v2v error: 2.7486979961395264 mm

Highest mean error: 2.9061343669891357 mm for frame 153

Lowest mean error: 2.626845359802246 mm for frame 46

Saving results

Total time: 43.58251452445984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056177
Iteration 2/25 | Loss: 0.01056177
Iteration 3/25 | Loss: 0.01056177
Iteration 4/25 | Loss: 0.01056177
Iteration 5/25 | Loss: 0.01056177
Iteration 6/25 | Loss: 0.01056177
Iteration 7/25 | Loss: 0.01056177
Iteration 8/25 | Loss: 0.01056177
Iteration 9/25 | Loss: 0.01056177
Iteration 10/25 | Loss: 0.01056177
Iteration 11/25 | Loss: 0.01056177
Iteration 12/25 | Loss: 0.01056177
Iteration 13/25 | Loss: 0.01056176
Iteration 14/25 | Loss: 0.01056176
Iteration 15/25 | Loss: 0.01056176
Iteration 16/25 | Loss: 0.01056176
Iteration 17/25 | Loss: 0.01056176
Iteration 18/25 | Loss: 0.01056176
Iteration 19/25 | Loss: 0.01056176
Iteration 20/25 | Loss: 0.01056176
Iteration 21/25 | Loss: 0.01056176
Iteration 22/25 | Loss: 0.01056176
Iteration 23/25 | Loss: 0.01056176
Iteration 24/25 | Loss: 0.01056176
Iteration 25/25 | Loss: 0.01056176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76423144
Iteration 2/25 | Loss: 0.09303646
Iteration 3/25 | Loss: 0.08849978
Iteration 4/25 | Loss: 0.08798574
Iteration 5/25 | Loss: 0.08798574
Iteration 6/25 | Loss: 0.08798572
Iteration 7/25 | Loss: 0.08798570
Iteration 8/25 | Loss: 0.08798571
Iteration 9/25 | Loss: 0.08798570
Iteration 10/25 | Loss: 0.08798570
Iteration 11/25 | Loss: 0.08798570
Iteration 12/25 | Loss: 0.08798570
Iteration 13/25 | Loss: 0.08798569
Iteration 14/25 | Loss: 0.08798569
Iteration 15/25 | Loss: 0.08798569
Iteration 16/25 | Loss: 0.08798569
Iteration 17/25 | Loss: 0.08798569
Iteration 18/25 | Loss: 0.08798569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.08798568695783615, 0.08798568695783615, 0.08798568695783615, 0.08798568695783615, 0.08798568695783615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08798568695783615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08798569
Iteration 2/1000 | Loss: 0.00424447
Iteration 3/1000 | Loss: 0.01234753
Iteration 4/1000 | Loss: 0.00117369
Iteration 5/1000 | Loss: 0.00374095
Iteration 6/1000 | Loss: 0.00550749
Iteration 7/1000 | Loss: 0.00377861
Iteration 8/1000 | Loss: 0.00017451
Iteration 9/1000 | Loss: 0.00031803
Iteration 10/1000 | Loss: 0.00115920
Iteration 11/1000 | Loss: 0.00026064
Iteration 12/1000 | Loss: 0.00046338
Iteration 13/1000 | Loss: 0.00119934
Iteration 14/1000 | Loss: 0.00031320
Iteration 15/1000 | Loss: 0.00008335
Iteration 16/1000 | Loss: 0.00011269
Iteration 17/1000 | Loss: 0.00006647
Iteration 18/1000 | Loss: 0.00017263
Iteration 19/1000 | Loss: 0.00024710
Iteration 20/1000 | Loss: 0.00003393
Iteration 21/1000 | Loss: 0.00029232
Iteration 22/1000 | Loss: 0.00025031
Iteration 23/1000 | Loss: 0.00002806
Iteration 24/1000 | Loss: 0.00002621
Iteration 25/1000 | Loss: 0.00005897
Iteration 26/1000 | Loss: 0.00003233
Iteration 27/1000 | Loss: 0.00008952
Iteration 28/1000 | Loss: 0.00002274
Iteration 29/1000 | Loss: 0.00028056
Iteration 30/1000 | Loss: 0.00029675
Iteration 31/1000 | Loss: 0.00005160
Iteration 32/1000 | Loss: 0.00008570
Iteration 33/1000 | Loss: 0.00003012
Iteration 34/1000 | Loss: 0.00002050
Iteration 35/1000 | Loss: 0.00005393
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00006159
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001722
Iteration 42/1000 | Loss: 0.00001722
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00003203
Iteration 45/1000 | Loss: 0.00008088
Iteration 46/1000 | Loss: 0.00005640
Iteration 47/1000 | Loss: 0.00020340
Iteration 48/1000 | Loss: 0.00002645
Iteration 49/1000 | Loss: 0.00003194
Iteration 50/1000 | Loss: 0.00009672
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00002807
Iteration 56/1000 | Loss: 0.00004046
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001647
Iteration 59/1000 | Loss: 0.00001646
Iteration 60/1000 | Loss: 0.00001646
Iteration 61/1000 | Loss: 0.00001646
Iteration 62/1000 | Loss: 0.00001646
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001644
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001639
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001637
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001632
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00004526
Iteration 86/1000 | Loss: 0.00004390
Iteration 87/1000 | Loss: 0.00001833
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001625
Iteration 90/1000 | Loss: 0.00001625
Iteration 91/1000 | Loss: 0.00001625
Iteration 92/1000 | Loss: 0.00001625
Iteration 93/1000 | Loss: 0.00001625
Iteration 94/1000 | Loss: 0.00001625
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00001624
Iteration 98/1000 | Loss: 0.00001624
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001623
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00003043
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001619
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001613
Iteration 124/1000 | Loss: 0.00001613
Iteration 125/1000 | Loss: 0.00001613
Iteration 126/1000 | Loss: 0.00001613
Iteration 127/1000 | Loss: 0.00001613
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Iteration 131/1000 | Loss: 0.00001613
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00001613
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001613
Iteration 137/1000 | Loss: 0.00001612
Iteration 138/1000 | Loss: 0.00001612
Iteration 139/1000 | Loss: 0.00001612
Iteration 140/1000 | Loss: 0.00001611
Iteration 141/1000 | Loss: 0.00001611
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001610
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001610
Iteration 147/1000 | Loss: 0.00001610
Iteration 148/1000 | Loss: 0.00001609
Iteration 149/1000 | Loss: 0.00001609
Iteration 150/1000 | Loss: 0.00001609
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001608
Iteration 154/1000 | Loss: 0.00001607
Iteration 155/1000 | Loss: 0.00001607
Iteration 156/1000 | Loss: 0.00001607
Iteration 157/1000 | Loss: 0.00001607
Iteration 158/1000 | Loss: 0.00001607
Iteration 159/1000 | Loss: 0.00001606
Iteration 160/1000 | Loss: 0.00001606
Iteration 161/1000 | Loss: 0.00001606
Iteration 162/1000 | Loss: 0.00001606
Iteration 163/1000 | Loss: 0.00001606
Iteration 164/1000 | Loss: 0.00001606
Iteration 165/1000 | Loss: 0.00001606
Iteration 166/1000 | Loss: 0.00001606
Iteration 167/1000 | Loss: 0.00001606
Iteration 168/1000 | Loss: 0.00001606
Iteration 169/1000 | Loss: 0.00001606
Iteration 170/1000 | Loss: 0.00001606
Iteration 171/1000 | Loss: 0.00001606
Iteration 172/1000 | Loss: 0.00001606
Iteration 173/1000 | Loss: 0.00001606
Iteration 174/1000 | Loss: 0.00001606
Iteration 175/1000 | Loss: 0.00001605
Iteration 176/1000 | Loss: 0.00001605
Iteration 177/1000 | Loss: 0.00001605
Iteration 178/1000 | Loss: 0.00001605
Iteration 179/1000 | Loss: 0.00001605
Iteration 180/1000 | Loss: 0.00001605
Iteration 181/1000 | Loss: 0.00001605
Iteration 182/1000 | Loss: 0.00001605
Iteration 183/1000 | Loss: 0.00001605
Iteration 184/1000 | Loss: 0.00001605
Iteration 185/1000 | Loss: 0.00001605
Iteration 186/1000 | Loss: 0.00001605
Iteration 187/1000 | Loss: 0.00001605
Iteration 188/1000 | Loss: 0.00001605
Iteration 189/1000 | Loss: 0.00001605
Iteration 190/1000 | Loss: 0.00001604
Iteration 191/1000 | Loss: 0.00001604
Iteration 192/1000 | Loss: 0.00001604
Iteration 193/1000 | Loss: 0.00001604
Iteration 194/1000 | Loss: 0.00001604
Iteration 195/1000 | Loss: 0.00001604
Iteration 196/1000 | Loss: 0.00001604
Iteration 197/1000 | Loss: 0.00001604
Iteration 198/1000 | Loss: 0.00001604
Iteration 199/1000 | Loss: 0.00001604
Iteration 200/1000 | Loss: 0.00001604
Iteration 201/1000 | Loss: 0.00001604
Iteration 202/1000 | Loss: 0.00001604
Iteration 203/1000 | Loss: 0.00001604
Iteration 204/1000 | Loss: 0.00001604
Iteration 205/1000 | Loss: 0.00001604
Iteration 206/1000 | Loss: 0.00001604
Iteration 207/1000 | Loss: 0.00001604
Iteration 208/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.603523014637176e-05, 1.603523014637176e-05, 1.603523014637176e-05, 1.603523014637176e-05, 1.603523014637176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.603523014637176e-05

Optimization complete. Final v2v error: 3.40097975730896 mm

Highest mean error: 3.947531223297119 mm for frame 19

Lowest mean error: 2.981285572052002 mm for frame 129

Saving results

Total time: 109.80073070526123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956625
Iteration 2/25 | Loss: 0.00219230
Iteration 3/25 | Loss: 0.00148800
Iteration 4/25 | Loss: 0.00141700
Iteration 5/25 | Loss: 0.00136021
Iteration 6/25 | Loss: 0.00134571
Iteration 7/25 | Loss: 0.00126593
Iteration 8/25 | Loss: 0.00125255
Iteration 9/25 | Loss: 0.00123720
Iteration 10/25 | Loss: 0.00122270
Iteration 11/25 | Loss: 0.00121996
Iteration 12/25 | Loss: 0.00121950
Iteration 13/25 | Loss: 0.00121932
Iteration 14/25 | Loss: 0.00121908
Iteration 15/25 | Loss: 0.00121990
Iteration 16/25 | Loss: 0.00121859
Iteration 17/25 | Loss: 0.00121749
Iteration 18/25 | Loss: 0.00122027
Iteration 19/25 | Loss: 0.00121627
Iteration 20/25 | Loss: 0.00121522
Iteration 21/25 | Loss: 0.00121509
Iteration 22/25 | Loss: 0.00121509
Iteration 23/25 | Loss: 0.00121509
Iteration 24/25 | Loss: 0.00121509
Iteration 25/25 | Loss: 0.00121508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29374945
Iteration 2/25 | Loss: 0.00095351
Iteration 3/25 | Loss: 0.00095351
Iteration 4/25 | Loss: 0.00095351
Iteration 5/25 | Loss: 0.00095351
Iteration 6/25 | Loss: 0.00095351
Iteration 7/25 | Loss: 0.00095351
Iteration 8/25 | Loss: 0.00095351
Iteration 9/25 | Loss: 0.00095351
Iteration 10/25 | Loss: 0.00095351
Iteration 11/25 | Loss: 0.00095351
Iteration 12/25 | Loss: 0.00095351
Iteration 13/25 | Loss: 0.00095351
Iteration 14/25 | Loss: 0.00095351
Iteration 15/25 | Loss: 0.00095351
Iteration 16/25 | Loss: 0.00095351
Iteration 17/25 | Loss: 0.00095351
Iteration 18/25 | Loss: 0.00095351
Iteration 19/25 | Loss: 0.00095351
Iteration 20/25 | Loss: 0.00095351
Iteration 21/25 | Loss: 0.00095351
Iteration 22/25 | Loss: 0.00095351
Iteration 23/25 | Loss: 0.00095351
Iteration 24/25 | Loss: 0.00095351
Iteration 25/25 | Loss: 0.00095351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095351
Iteration 2/1000 | Loss: 0.00003157
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00005951
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001533
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001490
Iteration 16/1000 | Loss: 0.00001489
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001461
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001446
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001434
Iteration 95/1000 | Loss: 0.00001434
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001431
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001427
Iteration 123/1000 | Loss: 0.00001427
Iteration 124/1000 | Loss: 0.00001427
Iteration 125/1000 | Loss: 0.00001427
Iteration 126/1000 | Loss: 0.00001427
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001426
Iteration 130/1000 | Loss: 0.00001426
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001426
Iteration 133/1000 | Loss: 0.00001426
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001425
Iteration 138/1000 | Loss: 0.00001425
Iteration 139/1000 | Loss: 0.00001425
Iteration 140/1000 | Loss: 0.00001425
Iteration 141/1000 | Loss: 0.00001425
Iteration 142/1000 | Loss: 0.00001425
Iteration 143/1000 | Loss: 0.00001425
Iteration 144/1000 | Loss: 0.00001425
Iteration 145/1000 | Loss: 0.00001424
Iteration 146/1000 | Loss: 0.00001424
Iteration 147/1000 | Loss: 0.00001424
Iteration 148/1000 | Loss: 0.00001424
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001423
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001423
Iteration 155/1000 | Loss: 0.00001423
Iteration 156/1000 | Loss: 0.00001423
Iteration 157/1000 | Loss: 0.00001423
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001422
Iteration 161/1000 | Loss: 0.00001422
Iteration 162/1000 | Loss: 0.00001422
Iteration 163/1000 | Loss: 0.00001422
Iteration 164/1000 | Loss: 0.00001422
Iteration 165/1000 | Loss: 0.00001421
Iteration 166/1000 | Loss: 0.00001421
Iteration 167/1000 | Loss: 0.00001421
Iteration 168/1000 | Loss: 0.00001421
Iteration 169/1000 | Loss: 0.00001421
Iteration 170/1000 | Loss: 0.00001421
Iteration 171/1000 | Loss: 0.00001421
Iteration 172/1000 | Loss: 0.00001421
Iteration 173/1000 | Loss: 0.00001421
Iteration 174/1000 | Loss: 0.00001421
Iteration 175/1000 | Loss: 0.00001421
Iteration 176/1000 | Loss: 0.00001420
Iteration 177/1000 | Loss: 0.00001420
Iteration 178/1000 | Loss: 0.00001420
Iteration 179/1000 | Loss: 0.00001420
Iteration 180/1000 | Loss: 0.00001420
Iteration 181/1000 | Loss: 0.00001420
Iteration 182/1000 | Loss: 0.00001420
Iteration 183/1000 | Loss: 0.00001420
Iteration 184/1000 | Loss: 0.00001419
Iteration 185/1000 | Loss: 0.00001419
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001419
Iteration 188/1000 | Loss: 0.00001419
Iteration 189/1000 | Loss: 0.00001419
Iteration 190/1000 | Loss: 0.00001419
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001419
Iteration 193/1000 | Loss: 0.00001419
Iteration 194/1000 | Loss: 0.00001419
Iteration 195/1000 | Loss: 0.00001419
Iteration 196/1000 | Loss: 0.00001419
Iteration 197/1000 | Loss: 0.00001419
Iteration 198/1000 | Loss: 0.00001419
Iteration 199/1000 | Loss: 0.00001419
Iteration 200/1000 | Loss: 0.00001419
Iteration 201/1000 | Loss: 0.00001419
Iteration 202/1000 | Loss: 0.00001419
Iteration 203/1000 | Loss: 0.00001419
Iteration 204/1000 | Loss: 0.00001419
Iteration 205/1000 | Loss: 0.00001419
Iteration 206/1000 | Loss: 0.00001419
Iteration 207/1000 | Loss: 0.00001419
Iteration 208/1000 | Loss: 0.00001419
Iteration 209/1000 | Loss: 0.00001419
Iteration 210/1000 | Loss: 0.00001419
Iteration 211/1000 | Loss: 0.00001419
Iteration 212/1000 | Loss: 0.00001419
Iteration 213/1000 | Loss: 0.00001419
Iteration 214/1000 | Loss: 0.00001419
Iteration 215/1000 | Loss: 0.00001419
Iteration 216/1000 | Loss: 0.00001419
Iteration 217/1000 | Loss: 0.00001419
Iteration 218/1000 | Loss: 0.00001419
Iteration 219/1000 | Loss: 0.00001419
Iteration 220/1000 | Loss: 0.00001419
Iteration 221/1000 | Loss: 0.00001419
Iteration 222/1000 | Loss: 0.00001419
Iteration 223/1000 | Loss: 0.00001419
Iteration 224/1000 | Loss: 0.00001419
Iteration 225/1000 | Loss: 0.00001419
Iteration 226/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.4185337931849062e-05, 1.4185337931849062e-05, 1.4185337931849062e-05, 1.4185337931849062e-05, 1.4185337931849062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4185337931849062e-05

Optimization complete. Final v2v error: 3.167780876159668 mm

Highest mean error: 4.395412921905518 mm for frame 85

Lowest mean error: 2.657500743865967 mm for frame 208

Saving results

Total time: 84.66079378128052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833277
Iteration 2/25 | Loss: 0.00173161
Iteration 3/25 | Loss: 0.00131548
Iteration 4/25 | Loss: 0.00126985
Iteration 5/25 | Loss: 0.00126287
Iteration 6/25 | Loss: 0.00125769
Iteration 7/25 | Loss: 0.00125375
Iteration 8/25 | Loss: 0.00123959
Iteration 9/25 | Loss: 0.00123617
Iteration 10/25 | Loss: 0.00123553
Iteration 11/25 | Loss: 0.00123549
Iteration 12/25 | Loss: 0.00123549
Iteration 13/25 | Loss: 0.00123549
Iteration 14/25 | Loss: 0.00123923
Iteration 15/25 | Loss: 0.00123982
Iteration 16/25 | Loss: 0.00123904
Iteration 17/25 | Loss: 0.00123697
Iteration 18/25 | Loss: 0.00123728
Iteration 19/25 | Loss: 0.00123645
Iteration 20/25 | Loss: 0.00123480
Iteration 21/25 | Loss: 0.00123358
Iteration 22/25 | Loss: 0.00123273
Iteration 23/25 | Loss: 0.00123189
Iteration 24/25 | Loss: 0.00123144
Iteration 25/25 | Loss: 0.00123139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33838439
Iteration 2/25 | Loss: 0.00102747
Iteration 3/25 | Loss: 0.00099401
Iteration 4/25 | Loss: 0.00099401
Iteration 5/25 | Loss: 0.00099401
Iteration 6/25 | Loss: 0.00099401
Iteration 7/25 | Loss: 0.00099401
Iteration 8/25 | Loss: 0.00099401
Iteration 9/25 | Loss: 0.00099401
Iteration 10/25 | Loss: 0.00099401
Iteration 11/25 | Loss: 0.00099401
Iteration 12/25 | Loss: 0.00099401
Iteration 13/25 | Loss: 0.00099401
Iteration 14/25 | Loss: 0.00099401
Iteration 15/25 | Loss: 0.00099401
Iteration 16/25 | Loss: 0.00099401
Iteration 17/25 | Loss: 0.00099401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009940110612660646, 0.0009940110612660646, 0.0009940110612660646, 0.0009940110612660646, 0.0009940110612660646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009940110612660646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099401
Iteration 2/1000 | Loss: 0.00011795
Iteration 3/1000 | Loss: 0.00003063
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002157
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00001948
Iteration 8/1000 | Loss: 0.00001875
Iteration 9/1000 | Loss: 0.00001813
Iteration 10/1000 | Loss: 0.00001773
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001689
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001681
Iteration 17/1000 | Loss: 0.00001678
Iteration 18/1000 | Loss: 0.00001675
Iteration 19/1000 | Loss: 0.00001671
Iteration 20/1000 | Loss: 0.00001671
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001668
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001660
Iteration 39/1000 | Loss: 0.00001660
Iteration 40/1000 | Loss: 0.00001660
Iteration 41/1000 | Loss: 0.00001660
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001659
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001657
Iteration 58/1000 | Loss: 0.00001657
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001656
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001655
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001653
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001653
Iteration 88/1000 | Loss: 0.00001653
Iteration 89/1000 | Loss: 0.00001653
Iteration 90/1000 | Loss: 0.00001653
Iteration 91/1000 | Loss: 0.00001653
Iteration 92/1000 | Loss: 0.00001653
Iteration 93/1000 | Loss: 0.00001653
Iteration 94/1000 | Loss: 0.00001653
Iteration 95/1000 | Loss: 0.00001653
Iteration 96/1000 | Loss: 0.00001653
Iteration 97/1000 | Loss: 0.00001653
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.6527579646208324e-05, 1.6527579646208324e-05, 1.6527579646208324e-05, 1.6527579646208324e-05, 1.6527579646208324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6527579646208324e-05

Optimization complete. Final v2v error: 3.4256200790405273 mm

Highest mean error: 4.003584384918213 mm for frame 200

Lowest mean error: 3.010115623474121 mm for frame 203

Saving results

Total time: 75.25078344345093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625050
Iteration 2/25 | Loss: 0.00123821
Iteration 3/25 | Loss: 0.00115017
Iteration 4/25 | Loss: 0.00113853
Iteration 5/25 | Loss: 0.00113463
Iteration 6/25 | Loss: 0.00113354
Iteration 7/25 | Loss: 0.00113354
Iteration 8/25 | Loss: 0.00113354
Iteration 9/25 | Loss: 0.00113354
Iteration 10/25 | Loss: 0.00113354
Iteration 11/25 | Loss: 0.00113354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001133537502028048, 0.001133537502028048, 0.001133537502028048, 0.001133537502028048, 0.001133537502028048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001133537502028048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33930171
Iteration 2/25 | Loss: 0.00121481
Iteration 3/25 | Loss: 0.00121481
Iteration 4/25 | Loss: 0.00121481
Iteration 5/25 | Loss: 0.00121481
Iteration 6/25 | Loss: 0.00121481
Iteration 7/25 | Loss: 0.00121481
Iteration 8/25 | Loss: 0.00121481
Iteration 9/25 | Loss: 0.00121481
Iteration 10/25 | Loss: 0.00121481
Iteration 11/25 | Loss: 0.00121481
Iteration 12/25 | Loss: 0.00121481
Iteration 13/25 | Loss: 0.00121481
Iteration 14/25 | Loss: 0.00121481
Iteration 15/25 | Loss: 0.00121481
Iteration 16/25 | Loss: 0.00121481
Iteration 17/25 | Loss: 0.00121481
Iteration 18/25 | Loss: 0.00121481
Iteration 19/25 | Loss: 0.00121481
Iteration 20/25 | Loss: 0.00121481
Iteration 21/25 | Loss: 0.00121481
Iteration 22/25 | Loss: 0.00121481
Iteration 23/25 | Loss: 0.00121481
Iteration 24/25 | Loss: 0.00121481
Iteration 25/25 | Loss: 0.00121481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121481
Iteration 2/1000 | Loss: 0.00001886
Iteration 3/1000 | Loss: 0.00001256
Iteration 4/1000 | Loss: 0.00001128
Iteration 5/1000 | Loss: 0.00001073
Iteration 6/1000 | Loss: 0.00001033
Iteration 7/1000 | Loss: 0.00000993
Iteration 8/1000 | Loss: 0.00000985
Iteration 9/1000 | Loss: 0.00000969
Iteration 10/1000 | Loss: 0.00000942
Iteration 11/1000 | Loss: 0.00000928
Iteration 12/1000 | Loss: 0.00000910
Iteration 13/1000 | Loss: 0.00000909
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000907
Iteration 16/1000 | Loss: 0.00000906
Iteration 17/1000 | Loss: 0.00000904
Iteration 18/1000 | Loss: 0.00000903
Iteration 19/1000 | Loss: 0.00000896
Iteration 20/1000 | Loss: 0.00000896
Iteration 21/1000 | Loss: 0.00000896
Iteration 22/1000 | Loss: 0.00000895
Iteration 23/1000 | Loss: 0.00000894
Iteration 24/1000 | Loss: 0.00000893
Iteration 25/1000 | Loss: 0.00000892
Iteration 26/1000 | Loss: 0.00000892
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000891
Iteration 30/1000 | Loss: 0.00000890
Iteration 31/1000 | Loss: 0.00000890
Iteration 32/1000 | Loss: 0.00000889
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000886
Iteration 35/1000 | Loss: 0.00000881
Iteration 36/1000 | Loss: 0.00000881
Iteration 37/1000 | Loss: 0.00000880
Iteration 38/1000 | Loss: 0.00000879
Iteration 39/1000 | Loss: 0.00000878
Iteration 40/1000 | Loss: 0.00000878
Iteration 41/1000 | Loss: 0.00000878
Iteration 42/1000 | Loss: 0.00000877
Iteration 43/1000 | Loss: 0.00000877
Iteration 44/1000 | Loss: 0.00000877
Iteration 45/1000 | Loss: 0.00000876
Iteration 46/1000 | Loss: 0.00000875
Iteration 47/1000 | Loss: 0.00000875
Iteration 48/1000 | Loss: 0.00000875
Iteration 49/1000 | Loss: 0.00000875
Iteration 50/1000 | Loss: 0.00000875
Iteration 51/1000 | Loss: 0.00000875
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000874
Iteration 58/1000 | Loss: 0.00000873
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000871
Iteration 64/1000 | Loss: 0.00000871
Iteration 65/1000 | Loss: 0.00000870
Iteration 66/1000 | Loss: 0.00000870
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000869
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000869
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000867
Iteration 74/1000 | Loss: 0.00000867
Iteration 75/1000 | Loss: 0.00000866
Iteration 76/1000 | Loss: 0.00000866
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000866
Iteration 79/1000 | Loss: 0.00000866
Iteration 80/1000 | Loss: 0.00000866
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000863
Iteration 95/1000 | Loss: 0.00000863
Iteration 96/1000 | Loss: 0.00000863
Iteration 97/1000 | Loss: 0.00000863
Iteration 98/1000 | Loss: 0.00000863
Iteration 99/1000 | Loss: 0.00000863
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000863
Iteration 102/1000 | Loss: 0.00000862
Iteration 103/1000 | Loss: 0.00000862
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000862
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000862
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000862
Iteration 110/1000 | Loss: 0.00000862
Iteration 111/1000 | Loss: 0.00000862
Iteration 112/1000 | Loss: 0.00000862
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000861
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000856
Iteration 145/1000 | Loss: 0.00000856
Iteration 146/1000 | Loss: 0.00000856
Iteration 147/1000 | Loss: 0.00000856
Iteration 148/1000 | Loss: 0.00000856
Iteration 149/1000 | Loss: 0.00000856
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000856
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000855
Iteration 155/1000 | Loss: 0.00000855
Iteration 156/1000 | Loss: 0.00000855
Iteration 157/1000 | Loss: 0.00000855
Iteration 158/1000 | Loss: 0.00000855
Iteration 159/1000 | Loss: 0.00000855
Iteration 160/1000 | Loss: 0.00000855
Iteration 161/1000 | Loss: 0.00000855
Iteration 162/1000 | Loss: 0.00000855
Iteration 163/1000 | Loss: 0.00000855
Iteration 164/1000 | Loss: 0.00000855
Iteration 165/1000 | Loss: 0.00000855
Iteration 166/1000 | Loss: 0.00000855
Iteration 167/1000 | Loss: 0.00000855
Iteration 168/1000 | Loss: 0.00000855
Iteration 169/1000 | Loss: 0.00000855
Iteration 170/1000 | Loss: 0.00000855
Iteration 171/1000 | Loss: 0.00000855
Iteration 172/1000 | Loss: 0.00000855
Iteration 173/1000 | Loss: 0.00000855
Iteration 174/1000 | Loss: 0.00000855
Iteration 175/1000 | Loss: 0.00000855
Iteration 176/1000 | Loss: 0.00000855
Iteration 177/1000 | Loss: 0.00000855
Iteration 178/1000 | Loss: 0.00000855
Iteration 179/1000 | Loss: 0.00000855
Iteration 180/1000 | Loss: 0.00000855
Iteration 181/1000 | Loss: 0.00000855
Iteration 182/1000 | Loss: 0.00000855
Iteration 183/1000 | Loss: 0.00000855
Iteration 184/1000 | Loss: 0.00000855
Iteration 185/1000 | Loss: 0.00000855
Iteration 186/1000 | Loss: 0.00000855
Iteration 187/1000 | Loss: 0.00000855
Iteration 188/1000 | Loss: 0.00000855
Iteration 189/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [8.550392522010952e-06, 8.550392522010952e-06, 8.550392522010952e-06, 8.550392522010952e-06, 8.550392522010952e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.550392522010952e-06

Optimization complete. Final v2v error: 2.5170695781707764 mm

Highest mean error: 3.0970916748046875 mm for frame 80

Lowest mean error: 2.3446524143218994 mm for frame 105

Saving results

Total time: 37.86390805244446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838358
Iteration 2/25 | Loss: 0.00271097
Iteration 3/25 | Loss: 0.00186869
Iteration 4/25 | Loss: 0.00167270
Iteration 5/25 | Loss: 0.00151067
Iteration 6/25 | Loss: 0.00149479
Iteration 7/25 | Loss: 0.00143416
Iteration 8/25 | Loss: 0.00143393
Iteration 9/25 | Loss: 0.00141893
Iteration 10/25 | Loss: 0.00141876
Iteration 11/25 | Loss: 0.00141923
Iteration 12/25 | Loss: 0.00141427
Iteration 13/25 | Loss: 0.00141091
Iteration 14/25 | Loss: 0.00140451
Iteration 15/25 | Loss: 0.00140808
Iteration 16/25 | Loss: 0.00141614
Iteration 17/25 | Loss: 0.00140327
Iteration 18/25 | Loss: 0.00140647
Iteration 19/25 | Loss: 0.00140348
Iteration 20/25 | Loss: 0.00139514
Iteration 21/25 | Loss: 0.00139577
Iteration 22/25 | Loss: 0.00139331
Iteration 23/25 | Loss: 0.00139315
Iteration 24/25 | Loss: 0.00139304
Iteration 25/25 | Loss: 0.00139692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.35660648
Iteration 2/25 | Loss: 0.00429995
Iteration 3/25 | Loss: 0.00333749
Iteration 4/25 | Loss: 0.00333749
Iteration 5/25 | Loss: 0.00333749
Iteration 6/25 | Loss: 0.00333749
Iteration 7/25 | Loss: 0.00333749
Iteration 8/25 | Loss: 0.00333748
Iteration 9/25 | Loss: 0.00333748
Iteration 10/25 | Loss: 0.00333748
Iteration 11/25 | Loss: 0.00333748
Iteration 12/25 | Loss: 0.00333748
Iteration 13/25 | Loss: 0.00333748
Iteration 14/25 | Loss: 0.00333748
Iteration 15/25 | Loss: 0.00333748
Iteration 16/25 | Loss: 0.00333748
Iteration 17/25 | Loss: 0.00333748
Iteration 18/25 | Loss: 0.00333748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0033374829217791557, 0.0033374829217791557, 0.0033374829217791557, 0.0033374829217791557, 0.0033374829217791557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033374829217791557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333748
Iteration 2/1000 | Loss: 0.00084184
Iteration 3/1000 | Loss: 0.00204560
Iteration 4/1000 | Loss: 0.00024190
Iteration 5/1000 | Loss: 0.00079417
Iteration 6/1000 | Loss: 0.00134241
Iteration 7/1000 | Loss: 0.00410233
Iteration 8/1000 | Loss: 0.00048990
Iteration 9/1000 | Loss: 0.00026930
Iteration 10/1000 | Loss: 0.00087276
Iteration 11/1000 | Loss: 0.00070687
Iteration 12/1000 | Loss: 0.00011265
Iteration 13/1000 | Loss: 0.00009115
Iteration 14/1000 | Loss: 0.00008317
Iteration 15/1000 | Loss: 0.00007443
Iteration 16/1000 | Loss: 0.00043881
Iteration 17/1000 | Loss: 0.00023042
Iteration 18/1000 | Loss: 0.00169478
Iteration 19/1000 | Loss: 0.00606262
Iteration 20/1000 | Loss: 0.00180832
Iteration 21/1000 | Loss: 0.00032979
Iteration 22/1000 | Loss: 0.00029600
Iteration 23/1000 | Loss: 0.00039389
Iteration 24/1000 | Loss: 0.00009783
Iteration 25/1000 | Loss: 0.00025371
Iteration 26/1000 | Loss: 0.00008248
Iteration 27/1000 | Loss: 0.00022505
Iteration 28/1000 | Loss: 0.00005026
Iteration 29/1000 | Loss: 0.00006733
Iteration 30/1000 | Loss: 0.00014665
Iteration 31/1000 | Loss: 0.00021684
Iteration 32/1000 | Loss: 0.00013069
Iteration 33/1000 | Loss: 0.00011008
Iteration 34/1000 | Loss: 0.00012201
Iteration 35/1000 | Loss: 0.00007739
Iteration 36/1000 | Loss: 0.00023771
Iteration 37/1000 | Loss: 0.00010384
Iteration 38/1000 | Loss: 0.00017798
Iteration 39/1000 | Loss: 0.00019082
Iteration 40/1000 | Loss: 0.00018879
Iteration 41/1000 | Loss: 0.00015322
Iteration 42/1000 | Loss: 0.00016265
Iteration 43/1000 | Loss: 0.00022171
Iteration 44/1000 | Loss: 0.00006918
Iteration 45/1000 | Loss: 0.00011911
Iteration 46/1000 | Loss: 0.00014955
Iteration 47/1000 | Loss: 0.00013750
Iteration 48/1000 | Loss: 0.00014001
Iteration 49/1000 | Loss: 0.00014785
Iteration 50/1000 | Loss: 0.00053675
Iteration 51/1000 | Loss: 0.00012569
Iteration 52/1000 | Loss: 0.00036540
Iteration 53/1000 | Loss: 0.00012706
Iteration 54/1000 | Loss: 0.00021427
Iteration 55/1000 | Loss: 0.00029275
Iteration 56/1000 | Loss: 0.00024258
Iteration 57/1000 | Loss: 0.00025151
Iteration 58/1000 | Loss: 0.00021470
Iteration 59/1000 | Loss: 0.00018208
Iteration 60/1000 | Loss: 0.00015525
Iteration 61/1000 | Loss: 0.00005538
Iteration 62/1000 | Loss: 0.00006488
Iteration 63/1000 | Loss: 0.00002728
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00004060
Iteration 66/1000 | Loss: 0.00009712
Iteration 67/1000 | Loss: 0.00003308
Iteration 68/1000 | Loss: 0.00003332
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001698
Iteration 73/1000 | Loss: 0.00005059
Iteration 74/1000 | Loss: 0.00001900
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00004493
Iteration 77/1000 | Loss: 0.00001569
Iteration 78/1000 | Loss: 0.00002602
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00002088
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001405
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001405
Iteration 117/1000 | Loss: 0.00001405
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4053849554329645e-05, 1.4053849554329645e-05, 1.4053849554329645e-05, 1.4053849554329645e-05, 1.4053849554329645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4053849554329645e-05

Optimization complete. Final v2v error: 3.141791343688965 mm

Highest mean error: 5.876289367675781 mm for frame 59

Lowest mean error: 2.6043524742126465 mm for frame 153

Saving results

Total time: 171.48325753211975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997537
Iteration 2/25 | Loss: 0.00222141
Iteration 3/25 | Loss: 0.00158126
Iteration 4/25 | Loss: 0.00145477
Iteration 5/25 | Loss: 0.00142282
Iteration 6/25 | Loss: 0.00132104
Iteration 7/25 | Loss: 0.00130814
Iteration 8/25 | Loss: 0.00125284
Iteration 9/25 | Loss: 0.00123804
Iteration 10/25 | Loss: 0.00123393
Iteration 11/25 | Loss: 0.00123016
Iteration 12/25 | Loss: 0.00122430
Iteration 13/25 | Loss: 0.00122317
Iteration 14/25 | Loss: 0.00122104
Iteration 15/25 | Loss: 0.00122078
Iteration 16/25 | Loss: 0.00122343
Iteration 17/25 | Loss: 0.00122565
Iteration 18/25 | Loss: 0.00122374
Iteration 19/25 | Loss: 0.00122226
Iteration 20/25 | Loss: 0.00121918
Iteration 21/25 | Loss: 0.00121989
Iteration 22/25 | Loss: 0.00121750
Iteration 23/25 | Loss: 0.00121729
Iteration 24/25 | Loss: 0.00121721
Iteration 25/25 | Loss: 0.00121711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43672264
Iteration 2/25 | Loss: 0.00146330
Iteration 3/25 | Loss: 0.00135026
Iteration 4/25 | Loss: 0.00135026
Iteration 5/25 | Loss: 0.00135026
Iteration 6/25 | Loss: 0.00135026
Iteration 7/25 | Loss: 0.00135025
Iteration 8/25 | Loss: 0.00135025
Iteration 9/25 | Loss: 0.00135025
Iteration 10/25 | Loss: 0.00135025
Iteration 11/25 | Loss: 0.00135025
Iteration 12/25 | Loss: 0.00135025
Iteration 13/25 | Loss: 0.00135025
Iteration 14/25 | Loss: 0.00135025
Iteration 15/25 | Loss: 0.00135025
Iteration 16/25 | Loss: 0.00135025
Iteration 17/25 | Loss: 0.00135025
Iteration 18/25 | Loss: 0.00135025
Iteration 19/25 | Loss: 0.00135025
Iteration 20/25 | Loss: 0.00135025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001350253471173346, 0.001350253471173346, 0.001350253471173346, 0.001350253471173346, 0.001350253471173346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001350253471173346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135025
Iteration 2/1000 | Loss: 0.00015114
Iteration 3/1000 | Loss: 0.00012596
Iteration 4/1000 | Loss: 0.00007067
Iteration 5/1000 | Loss: 0.00041032
Iteration 6/1000 | Loss: 0.00005054
Iteration 7/1000 | Loss: 0.00019294
Iteration 8/1000 | Loss: 0.00012508
Iteration 9/1000 | Loss: 0.00011440
Iteration 10/1000 | Loss: 0.00003998
Iteration 11/1000 | Loss: 0.00002993
Iteration 12/1000 | Loss: 0.00002329
Iteration 13/1000 | Loss: 0.00005024
Iteration 14/1000 | Loss: 0.00004404
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001730
Iteration 18/1000 | Loss: 0.00002796
Iteration 19/1000 | Loss: 0.00002418
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00002655
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00005871
Iteration 25/1000 | Loss: 0.00002053
Iteration 26/1000 | Loss: 0.00001510
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001994
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001479
Iteration 36/1000 | Loss: 0.00001479
Iteration 37/1000 | Loss: 0.00001479
Iteration 38/1000 | Loss: 0.00001479
Iteration 39/1000 | Loss: 0.00001479
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.4790355635341257e-05, 1.4790355635341257e-05, 1.4790355635341257e-05, 1.4790355635341257e-05, 1.4790355635341257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4790355635341257e-05

Optimization complete. Final v2v error: 3.1372745037078857 mm

Highest mean error: 11.894452095031738 mm for frame 182

Lowest mean error: 2.7411646842956543 mm for frame 14

Saving results

Total time: 95.55711340904236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541533
Iteration 2/25 | Loss: 0.00135377
Iteration 3/25 | Loss: 0.00120020
Iteration 4/25 | Loss: 0.00118632
Iteration 5/25 | Loss: 0.00118236
Iteration 6/25 | Loss: 0.00118294
Iteration 7/25 | Loss: 0.00117752
Iteration 8/25 | Loss: 0.00117532
Iteration 9/25 | Loss: 0.00117524
Iteration 10/25 | Loss: 0.00117522
Iteration 11/25 | Loss: 0.00117522
Iteration 12/25 | Loss: 0.00117522
Iteration 13/25 | Loss: 0.00117521
Iteration 14/25 | Loss: 0.00117521
Iteration 15/25 | Loss: 0.00117521
Iteration 16/25 | Loss: 0.00117521
Iteration 17/25 | Loss: 0.00117521
Iteration 18/25 | Loss: 0.00117521
Iteration 19/25 | Loss: 0.00117521
Iteration 20/25 | Loss: 0.00117521
Iteration 21/25 | Loss: 0.00117521
Iteration 22/25 | Loss: 0.00117521
Iteration 23/25 | Loss: 0.00117521
Iteration 24/25 | Loss: 0.00117521
Iteration 25/25 | Loss: 0.00117521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.86092615
Iteration 2/25 | Loss: 0.00117834
Iteration 3/25 | Loss: 0.00113397
Iteration 4/25 | Loss: 0.00113397
Iteration 5/25 | Loss: 0.00113397
Iteration 6/25 | Loss: 0.00113397
Iteration 7/25 | Loss: 0.00113397
Iteration 8/25 | Loss: 0.00113397
Iteration 9/25 | Loss: 0.00113397
Iteration 10/25 | Loss: 0.00113397
Iteration 11/25 | Loss: 0.00113397
Iteration 12/25 | Loss: 0.00113397
Iteration 13/25 | Loss: 0.00113397
Iteration 14/25 | Loss: 0.00113397
Iteration 15/25 | Loss: 0.00113397
Iteration 16/25 | Loss: 0.00113397
Iteration 17/25 | Loss: 0.00113397
Iteration 18/25 | Loss: 0.00113397
Iteration 19/25 | Loss: 0.00113397
Iteration 20/25 | Loss: 0.00113397
Iteration 21/25 | Loss: 0.00113397
Iteration 22/25 | Loss: 0.00113397
Iteration 23/25 | Loss: 0.00113397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011339695192873478, 0.0011339695192873478, 0.0011339695192873478, 0.0011339695192873478, 0.0011339695192873478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011339695192873478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113397
Iteration 2/1000 | Loss: 0.00006852
Iteration 3/1000 | Loss: 0.00002708
Iteration 4/1000 | Loss: 0.00001458
Iteration 5/1000 | Loss: 0.00002956
Iteration 6/1000 | Loss: 0.00002934
Iteration 7/1000 | Loss: 0.00006022
Iteration 8/1000 | Loss: 0.00001314
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00003325
Iteration 13/1000 | Loss: 0.00002638
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00003061
Iteration 16/1000 | Loss: 0.00001848
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00003013
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001192
Iteration 28/1000 | Loss: 0.00001192
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001179
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001179
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001171
Iteration 72/1000 | Loss: 0.00001171
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001169
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001168
Iteration 82/1000 | Loss: 0.00001168
Iteration 83/1000 | Loss: 0.00001168
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001166
Iteration 88/1000 | Loss: 0.00001166
Iteration 89/1000 | Loss: 0.00001166
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001165
Iteration 92/1000 | Loss: 0.00001165
Iteration 93/1000 | Loss: 0.00001165
Iteration 94/1000 | Loss: 0.00001165
Iteration 95/1000 | Loss: 0.00001165
Iteration 96/1000 | Loss: 0.00001165
Iteration 97/1000 | Loss: 0.00001164
Iteration 98/1000 | Loss: 0.00001164
Iteration 99/1000 | Loss: 0.00001164
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001163
Iteration 102/1000 | Loss: 0.00001163
Iteration 103/1000 | Loss: 0.00003286
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1585093488974962e-05, 1.1585093488974962e-05, 1.1585093488974962e-05, 1.1585093488974962e-05, 1.1585093488974962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1585093488974962e-05

Optimization complete. Final v2v error: 2.906829595565796 mm

Highest mean error: 3.291766405105591 mm for frame 215

Lowest mean error: 2.6749942302703857 mm for frame 129

Saving results

Total time: 57.90987682342529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958415
Iteration 2/25 | Loss: 0.00296354
Iteration 3/25 | Loss: 0.00217805
Iteration 4/25 | Loss: 0.00211256
Iteration 5/25 | Loss: 0.00178192
Iteration 6/25 | Loss: 0.00163027
Iteration 7/25 | Loss: 0.00144317
Iteration 8/25 | Loss: 0.00136479
Iteration 9/25 | Loss: 0.00135236
Iteration 10/25 | Loss: 0.00135934
Iteration 11/25 | Loss: 0.00134762
Iteration 12/25 | Loss: 0.00133661
Iteration 13/25 | Loss: 0.00132372
Iteration 14/25 | Loss: 0.00132342
Iteration 15/25 | Loss: 0.00132251
Iteration 16/25 | Loss: 0.00132114
Iteration 17/25 | Loss: 0.00132007
Iteration 18/25 | Loss: 0.00131980
Iteration 19/25 | Loss: 0.00131887
Iteration 20/25 | Loss: 0.00131623
Iteration 21/25 | Loss: 0.00131556
Iteration 22/25 | Loss: 0.00131535
Iteration 23/25 | Loss: 0.00131525
Iteration 24/25 | Loss: 0.00131525
Iteration 25/25 | Loss: 0.00131525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28561449
Iteration 2/25 | Loss: 0.00178732
Iteration 3/25 | Loss: 0.00133172
Iteration 4/25 | Loss: 0.00133172
Iteration 5/25 | Loss: 0.00133172
Iteration 6/25 | Loss: 0.00133172
Iteration 7/25 | Loss: 0.00133172
Iteration 8/25 | Loss: 0.00133172
Iteration 9/25 | Loss: 0.00133172
Iteration 10/25 | Loss: 0.00133172
Iteration 11/25 | Loss: 0.00133172
Iteration 12/25 | Loss: 0.00133172
Iteration 13/25 | Loss: 0.00133172
Iteration 14/25 | Loss: 0.00133172
Iteration 15/25 | Loss: 0.00133172
Iteration 16/25 | Loss: 0.00133172
Iteration 17/25 | Loss: 0.00133172
Iteration 18/25 | Loss: 0.00133172
Iteration 19/25 | Loss: 0.00133172
Iteration 20/25 | Loss: 0.00133172
Iteration 21/25 | Loss: 0.00133172
Iteration 22/25 | Loss: 0.00133172
Iteration 23/25 | Loss: 0.00133172
Iteration 24/25 | Loss: 0.00133172
Iteration 25/25 | Loss: 0.00133172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133172
Iteration 2/1000 | Loss: 0.00441127
Iteration 3/1000 | Loss: 0.00062957
Iteration 4/1000 | Loss: 0.00024098
Iteration 5/1000 | Loss: 0.00046125
Iteration 6/1000 | Loss: 0.00046952
Iteration 7/1000 | Loss: 0.00061663
Iteration 8/1000 | Loss: 0.00027293
Iteration 9/1000 | Loss: 0.00091047
Iteration 10/1000 | Loss: 0.00009636
Iteration 11/1000 | Loss: 0.00019154
Iteration 12/1000 | Loss: 0.00021484
Iteration 13/1000 | Loss: 0.00007624
Iteration 14/1000 | Loss: 0.00012853
Iteration 15/1000 | Loss: 0.00018475
Iteration 16/1000 | Loss: 0.00004588
Iteration 17/1000 | Loss: 0.00005612
Iteration 18/1000 | Loss: 0.00026019
Iteration 19/1000 | Loss: 0.00003804
Iteration 20/1000 | Loss: 0.00007836
Iteration 21/1000 | Loss: 0.00003511
Iteration 22/1000 | Loss: 0.00009528
Iteration 23/1000 | Loss: 0.00003409
Iteration 24/1000 | Loss: 0.00008446
Iteration 25/1000 | Loss: 0.00013968
Iteration 26/1000 | Loss: 0.00006304
Iteration 27/1000 | Loss: 0.00023428
Iteration 28/1000 | Loss: 0.00003903
Iteration 29/1000 | Loss: 0.00003459
Iteration 30/1000 | Loss: 0.00047447
Iteration 31/1000 | Loss: 0.00071979
Iteration 32/1000 | Loss: 0.00008264
Iteration 33/1000 | Loss: 0.00004956
Iteration 34/1000 | Loss: 0.00007462
Iteration 35/1000 | Loss: 0.00003276
Iteration 36/1000 | Loss: 0.00007240
Iteration 37/1000 | Loss: 0.00017574
Iteration 38/1000 | Loss: 0.00003387
Iteration 39/1000 | Loss: 0.00002214
Iteration 40/1000 | Loss: 0.00002056
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001816
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00002626
Iteration 46/1000 | Loss: 0.00013557
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00004972
Iteration 49/1000 | Loss: 0.00085228
Iteration 50/1000 | Loss: 0.00002693
Iteration 51/1000 | Loss: 0.00009994
Iteration 52/1000 | Loss: 0.00003823
Iteration 53/1000 | Loss: 0.00005394
Iteration 54/1000 | Loss: 0.00021369
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00004298
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00020709
Iteration 60/1000 | Loss: 0.00003042
Iteration 61/1000 | Loss: 0.00005474
Iteration 62/1000 | Loss: 0.00001587
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00005450
Iteration 67/1000 | Loss: 0.00013672
Iteration 68/1000 | Loss: 0.00002290
Iteration 69/1000 | Loss: 0.00001616
Iteration 70/1000 | Loss: 0.00004334
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00006269
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001508
Iteration 81/1000 | Loss: 0.00001508
Iteration 82/1000 | Loss: 0.00001508
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001503
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001502
Iteration 105/1000 | Loss: 0.00001502
Iteration 106/1000 | Loss: 0.00001502
Iteration 107/1000 | Loss: 0.00001502
Iteration 108/1000 | Loss: 0.00001502
Iteration 109/1000 | Loss: 0.00001502
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001501
Iteration 113/1000 | Loss: 0.00001501
Iteration 114/1000 | Loss: 0.00001501
Iteration 115/1000 | Loss: 0.00001501
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001499
Iteration 121/1000 | Loss: 0.00001499
Iteration 122/1000 | Loss: 0.00001499
Iteration 123/1000 | Loss: 0.00001498
Iteration 124/1000 | Loss: 0.00001498
Iteration 125/1000 | Loss: 0.00001498
Iteration 126/1000 | Loss: 0.00001498
Iteration 127/1000 | Loss: 0.00001498
Iteration 128/1000 | Loss: 0.00001498
Iteration 129/1000 | Loss: 0.00001498
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001498
Iteration 137/1000 | Loss: 0.00001497
Iteration 138/1000 | Loss: 0.00001497
Iteration 139/1000 | Loss: 0.00001497
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001493
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001492
Iteration 153/1000 | Loss: 0.00001492
Iteration 154/1000 | Loss: 0.00001492
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001492
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001491
Iteration 164/1000 | Loss: 0.00001491
Iteration 165/1000 | Loss: 0.00001491
Iteration 166/1000 | Loss: 0.00001491
Iteration 167/1000 | Loss: 0.00001491
Iteration 168/1000 | Loss: 0.00001491
Iteration 169/1000 | Loss: 0.00001490
Iteration 170/1000 | Loss: 0.00001490
Iteration 171/1000 | Loss: 0.00007579
Iteration 172/1000 | Loss: 0.00008524
Iteration 173/1000 | Loss: 0.00060212
Iteration 174/1000 | Loss: 0.00002436
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00003118
Iteration 177/1000 | Loss: 0.00001496
Iteration 178/1000 | Loss: 0.00001487
Iteration 179/1000 | Loss: 0.00001487
Iteration 180/1000 | Loss: 0.00001487
Iteration 181/1000 | Loss: 0.00001485
Iteration 182/1000 | Loss: 0.00001485
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001485
Iteration 185/1000 | Loss: 0.00001485
Iteration 186/1000 | Loss: 0.00001485
Iteration 187/1000 | Loss: 0.00001484
Iteration 188/1000 | Loss: 0.00001484
Iteration 189/1000 | Loss: 0.00001484
Iteration 190/1000 | Loss: 0.00001484
Iteration 191/1000 | Loss: 0.00001483
Iteration 192/1000 | Loss: 0.00001483
Iteration 193/1000 | Loss: 0.00001483
Iteration 194/1000 | Loss: 0.00001483
Iteration 195/1000 | Loss: 0.00001482
Iteration 196/1000 | Loss: 0.00001482
Iteration 197/1000 | Loss: 0.00001482
Iteration 198/1000 | Loss: 0.00001482
Iteration 199/1000 | Loss: 0.00001482
Iteration 200/1000 | Loss: 0.00001482
Iteration 201/1000 | Loss: 0.00001482
Iteration 202/1000 | Loss: 0.00001482
Iteration 203/1000 | Loss: 0.00001481
Iteration 204/1000 | Loss: 0.00001481
Iteration 205/1000 | Loss: 0.00001481
Iteration 206/1000 | Loss: 0.00001481
Iteration 207/1000 | Loss: 0.00001481
Iteration 208/1000 | Loss: 0.00001481
Iteration 209/1000 | Loss: 0.00001481
Iteration 210/1000 | Loss: 0.00001481
Iteration 211/1000 | Loss: 0.00001481
Iteration 212/1000 | Loss: 0.00001481
Iteration 213/1000 | Loss: 0.00001480
Iteration 214/1000 | Loss: 0.00001480
Iteration 215/1000 | Loss: 0.00001480
Iteration 216/1000 | Loss: 0.00001480
Iteration 217/1000 | Loss: 0.00001480
Iteration 218/1000 | Loss: 0.00001480
Iteration 219/1000 | Loss: 0.00001480
Iteration 220/1000 | Loss: 0.00001479
Iteration 221/1000 | Loss: 0.00001479
Iteration 222/1000 | Loss: 0.00001479
Iteration 223/1000 | Loss: 0.00001479
Iteration 224/1000 | Loss: 0.00001479
Iteration 225/1000 | Loss: 0.00001479
Iteration 226/1000 | Loss: 0.00001479
Iteration 227/1000 | Loss: 0.00001479
Iteration 228/1000 | Loss: 0.00001479
Iteration 229/1000 | Loss: 0.00001479
Iteration 230/1000 | Loss: 0.00001479
Iteration 231/1000 | Loss: 0.00001479
Iteration 232/1000 | Loss: 0.00001479
Iteration 233/1000 | Loss: 0.00001479
Iteration 234/1000 | Loss: 0.00001478
Iteration 235/1000 | Loss: 0.00001478
Iteration 236/1000 | Loss: 0.00001478
Iteration 237/1000 | Loss: 0.00001478
Iteration 238/1000 | Loss: 0.00001478
Iteration 239/1000 | Loss: 0.00001478
Iteration 240/1000 | Loss: 0.00001478
Iteration 241/1000 | Loss: 0.00001478
Iteration 242/1000 | Loss: 0.00001478
Iteration 243/1000 | Loss: 0.00001478
Iteration 244/1000 | Loss: 0.00001478
Iteration 245/1000 | Loss: 0.00001478
Iteration 246/1000 | Loss: 0.00001478
Iteration 247/1000 | Loss: 0.00001478
Iteration 248/1000 | Loss: 0.00001478
Iteration 249/1000 | Loss: 0.00001478
Iteration 250/1000 | Loss: 0.00001478
Iteration 251/1000 | Loss: 0.00001478
Iteration 252/1000 | Loss: 0.00001478
Iteration 253/1000 | Loss: 0.00001478
Iteration 254/1000 | Loss: 0.00001478
Iteration 255/1000 | Loss: 0.00001478
Iteration 256/1000 | Loss: 0.00001477
Iteration 257/1000 | Loss: 0.00001477
Iteration 258/1000 | Loss: 0.00001477
Iteration 259/1000 | Loss: 0.00001477
Iteration 260/1000 | Loss: 0.00001477
Iteration 261/1000 | Loss: 0.00001477
Iteration 262/1000 | Loss: 0.00001477
Iteration 263/1000 | Loss: 0.00001477
Iteration 264/1000 | Loss: 0.00001477
Iteration 265/1000 | Loss: 0.00001477
Iteration 266/1000 | Loss: 0.00001477
Iteration 267/1000 | Loss: 0.00001477
Iteration 268/1000 | Loss: 0.00001477
Iteration 269/1000 | Loss: 0.00001477
Iteration 270/1000 | Loss: 0.00001477
Iteration 271/1000 | Loss: 0.00001477
Iteration 272/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.4772203940083273e-05, 1.4772203940083273e-05, 1.4772203940083273e-05, 1.4772203940083273e-05, 1.4772203940083273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4772203940083273e-05

Optimization complete. Final v2v error: 3.268798589706421 mm

Highest mean error: 3.6851797103881836 mm for frame 137

Lowest mean error: 3.0618059635162354 mm for frame 102

Saving results

Total time: 173.20409846305847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816798
Iteration 2/25 | Loss: 0.00131240
Iteration 3/25 | Loss: 0.00121819
Iteration 4/25 | Loss: 0.00120667
Iteration 5/25 | Loss: 0.00120367
Iteration 6/25 | Loss: 0.00120319
Iteration 7/25 | Loss: 0.00120319
Iteration 8/25 | Loss: 0.00120319
Iteration 9/25 | Loss: 0.00120319
Iteration 10/25 | Loss: 0.00120319
Iteration 11/25 | Loss: 0.00120319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00120319495908916, 0.00120319495908916, 0.00120319495908916, 0.00120319495908916, 0.00120319495908916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00120319495908916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47060025
Iteration 2/25 | Loss: 0.00126063
Iteration 3/25 | Loss: 0.00126063
Iteration 4/25 | Loss: 0.00126063
Iteration 5/25 | Loss: 0.00126063
Iteration 6/25 | Loss: 0.00126063
Iteration 7/25 | Loss: 0.00126063
Iteration 8/25 | Loss: 0.00126063
Iteration 9/25 | Loss: 0.00126063
Iteration 10/25 | Loss: 0.00126063
Iteration 11/25 | Loss: 0.00126063
Iteration 12/25 | Loss: 0.00126063
Iteration 13/25 | Loss: 0.00126063
Iteration 14/25 | Loss: 0.00126063
Iteration 15/25 | Loss: 0.00126063
Iteration 16/25 | Loss: 0.00126063
Iteration 17/25 | Loss: 0.00126063
Iteration 18/25 | Loss: 0.00126063
Iteration 19/25 | Loss: 0.00126063
Iteration 20/25 | Loss: 0.00126063
Iteration 21/25 | Loss: 0.00126063
Iteration 22/25 | Loss: 0.00126063
Iteration 23/25 | Loss: 0.00126063
Iteration 24/25 | Loss: 0.00126063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012606268282979727, 0.0012606268282979727, 0.0012606268282979727, 0.0012606268282979727, 0.0012606268282979727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012606268282979727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126063
Iteration 2/1000 | Loss: 0.00003126
Iteration 3/1000 | Loss: 0.00002276
Iteration 4/1000 | Loss: 0.00001991
Iteration 5/1000 | Loss: 0.00001870
Iteration 6/1000 | Loss: 0.00001787
Iteration 7/1000 | Loss: 0.00001727
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001654
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001586
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001569
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001556
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001554
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001527
Iteration 43/1000 | Loss: 0.00001525
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001519
Iteration 57/1000 | Loss: 0.00001519
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001517
Iteration 69/1000 | Loss: 0.00001516
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001515
Iteration 74/1000 | Loss: 0.00001515
Iteration 75/1000 | Loss: 0.00001514
Iteration 76/1000 | Loss: 0.00001514
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001511
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001509
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001507
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001502
Iteration 123/1000 | Loss: 0.00001502
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001501
Iteration 126/1000 | Loss: 0.00001501
Iteration 127/1000 | Loss: 0.00001501
Iteration 128/1000 | Loss: 0.00001501
Iteration 129/1000 | Loss: 0.00001501
Iteration 130/1000 | Loss: 0.00001501
Iteration 131/1000 | Loss: 0.00001501
Iteration 132/1000 | Loss: 0.00001501
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001497
Iteration 149/1000 | Loss: 0.00001497
Iteration 150/1000 | Loss: 0.00001497
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001496
Iteration 153/1000 | Loss: 0.00001496
Iteration 154/1000 | Loss: 0.00001496
Iteration 155/1000 | Loss: 0.00001496
Iteration 156/1000 | Loss: 0.00001495
Iteration 157/1000 | Loss: 0.00001495
Iteration 158/1000 | Loss: 0.00001495
Iteration 159/1000 | Loss: 0.00001495
Iteration 160/1000 | Loss: 0.00001495
Iteration 161/1000 | Loss: 0.00001495
Iteration 162/1000 | Loss: 0.00001495
Iteration 163/1000 | Loss: 0.00001495
Iteration 164/1000 | Loss: 0.00001494
Iteration 165/1000 | Loss: 0.00001494
Iteration 166/1000 | Loss: 0.00001494
Iteration 167/1000 | Loss: 0.00001494
Iteration 168/1000 | Loss: 0.00001493
Iteration 169/1000 | Loss: 0.00001493
Iteration 170/1000 | Loss: 0.00001493
Iteration 171/1000 | Loss: 0.00001493
Iteration 172/1000 | Loss: 0.00001493
Iteration 173/1000 | Loss: 0.00001493
Iteration 174/1000 | Loss: 0.00001493
Iteration 175/1000 | Loss: 0.00001493
Iteration 176/1000 | Loss: 0.00001493
Iteration 177/1000 | Loss: 0.00001493
Iteration 178/1000 | Loss: 0.00001493
Iteration 179/1000 | Loss: 0.00001492
Iteration 180/1000 | Loss: 0.00001492
Iteration 181/1000 | Loss: 0.00001492
Iteration 182/1000 | Loss: 0.00001492
Iteration 183/1000 | Loss: 0.00001492
Iteration 184/1000 | Loss: 0.00001492
Iteration 185/1000 | Loss: 0.00001492
Iteration 186/1000 | Loss: 0.00001492
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001491
Iteration 189/1000 | Loss: 0.00001491
Iteration 190/1000 | Loss: 0.00001491
Iteration 191/1000 | Loss: 0.00001491
Iteration 192/1000 | Loss: 0.00001491
Iteration 193/1000 | Loss: 0.00001491
Iteration 194/1000 | Loss: 0.00001491
Iteration 195/1000 | Loss: 0.00001491
Iteration 196/1000 | Loss: 0.00001491
Iteration 197/1000 | Loss: 0.00001491
Iteration 198/1000 | Loss: 0.00001491
Iteration 199/1000 | Loss: 0.00001491
Iteration 200/1000 | Loss: 0.00001491
Iteration 201/1000 | Loss: 0.00001490
Iteration 202/1000 | Loss: 0.00001490
Iteration 203/1000 | Loss: 0.00001490
Iteration 204/1000 | Loss: 0.00001490
Iteration 205/1000 | Loss: 0.00001490
Iteration 206/1000 | Loss: 0.00001490
Iteration 207/1000 | Loss: 0.00001490
Iteration 208/1000 | Loss: 0.00001490
Iteration 209/1000 | Loss: 0.00001490
Iteration 210/1000 | Loss: 0.00001490
Iteration 211/1000 | Loss: 0.00001490
Iteration 212/1000 | Loss: 0.00001490
Iteration 213/1000 | Loss: 0.00001490
Iteration 214/1000 | Loss: 0.00001490
Iteration 215/1000 | Loss: 0.00001490
Iteration 216/1000 | Loss: 0.00001490
Iteration 217/1000 | Loss: 0.00001490
Iteration 218/1000 | Loss: 0.00001489
Iteration 219/1000 | Loss: 0.00001489
Iteration 220/1000 | Loss: 0.00001489
Iteration 221/1000 | Loss: 0.00001489
Iteration 222/1000 | Loss: 0.00001489
Iteration 223/1000 | Loss: 0.00001489
Iteration 224/1000 | Loss: 0.00001489
Iteration 225/1000 | Loss: 0.00001489
Iteration 226/1000 | Loss: 0.00001489
Iteration 227/1000 | Loss: 0.00001489
Iteration 228/1000 | Loss: 0.00001489
Iteration 229/1000 | Loss: 0.00001489
Iteration 230/1000 | Loss: 0.00001489
Iteration 231/1000 | Loss: 0.00001489
Iteration 232/1000 | Loss: 0.00001489
Iteration 233/1000 | Loss: 0.00001489
Iteration 234/1000 | Loss: 0.00001489
Iteration 235/1000 | Loss: 0.00001489
Iteration 236/1000 | Loss: 0.00001489
Iteration 237/1000 | Loss: 0.00001489
Iteration 238/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.4888472833263222e-05, 1.4888472833263222e-05, 1.4888472833263222e-05, 1.4888472833263222e-05, 1.4888472833263222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4888472833263222e-05

Optimization complete. Final v2v error: 3.256591796875 mm

Highest mean error: 4.173360347747803 mm for frame 86

Lowest mean error: 2.6861209869384766 mm for frame 147

Saving results

Total time: 45.71080684661865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989759
Iteration 2/25 | Loss: 0.00178931
Iteration 3/25 | Loss: 0.00146570
Iteration 4/25 | Loss: 0.00134077
Iteration 5/25 | Loss: 0.00131880
Iteration 6/25 | Loss: 0.00135557
Iteration 7/25 | Loss: 0.00131412
Iteration 8/25 | Loss: 0.00130556
Iteration 9/25 | Loss: 0.00125183
Iteration 10/25 | Loss: 0.00125079
Iteration 11/25 | Loss: 0.00124127
Iteration 12/25 | Loss: 0.00122268
Iteration 13/25 | Loss: 0.00121815
Iteration 14/25 | Loss: 0.00119872
Iteration 15/25 | Loss: 0.00120361
Iteration 16/25 | Loss: 0.00120701
Iteration 17/25 | Loss: 0.00119776
Iteration 18/25 | Loss: 0.00119850
Iteration 19/25 | Loss: 0.00119547
Iteration 20/25 | Loss: 0.00120432
Iteration 21/25 | Loss: 0.00119436
Iteration 22/25 | Loss: 0.00119842
Iteration 23/25 | Loss: 0.00119582
Iteration 24/25 | Loss: 0.00119539
Iteration 25/25 | Loss: 0.00120185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28862131
Iteration 2/25 | Loss: 0.00161587
Iteration 3/25 | Loss: 0.00125064
Iteration 4/25 | Loss: 0.00125064
Iteration 5/25 | Loss: 0.00125064
Iteration 6/25 | Loss: 0.00125064
Iteration 7/25 | Loss: 0.00125064
Iteration 8/25 | Loss: 0.00125064
Iteration 9/25 | Loss: 0.00125064
Iteration 10/25 | Loss: 0.00125064
Iteration 11/25 | Loss: 0.00125064
Iteration 12/25 | Loss: 0.00125064
Iteration 13/25 | Loss: 0.00125064
Iteration 14/25 | Loss: 0.00125064
Iteration 15/25 | Loss: 0.00125064
Iteration 16/25 | Loss: 0.00125064
Iteration 17/25 | Loss: 0.00125064
Iteration 18/25 | Loss: 0.00125064
Iteration 19/25 | Loss: 0.00125064
Iteration 20/25 | Loss: 0.00125064
Iteration 21/25 | Loss: 0.00125064
Iteration 22/25 | Loss: 0.00125064
Iteration 23/25 | Loss: 0.00125064
Iteration 24/25 | Loss: 0.00125064
Iteration 25/25 | Loss: 0.00125064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125064
Iteration 2/1000 | Loss: 0.00020146
Iteration 3/1000 | Loss: 0.00061973
Iteration 4/1000 | Loss: 0.00002693
Iteration 5/1000 | Loss: 0.00005164
Iteration 6/1000 | Loss: 0.00005206
Iteration 7/1000 | Loss: 0.00007035
Iteration 8/1000 | Loss: 0.00002844
Iteration 9/1000 | Loss: 0.00002408
Iteration 10/1000 | Loss: 0.00001867
Iteration 11/1000 | Loss: 0.00014409
Iteration 12/1000 | Loss: 0.00002048
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00003599
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00004956
Iteration 18/1000 | Loss: 0.00001864
Iteration 19/1000 | Loss: 0.00007792
Iteration 20/1000 | Loss: 0.00052585
Iteration 21/1000 | Loss: 0.00013139
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00004087
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00004413
Iteration 29/1000 | Loss: 0.00046814
Iteration 30/1000 | Loss: 0.00004403
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001328
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001310
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001306
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001304
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00028521
Iteration 67/1000 | Loss: 0.00011124
Iteration 68/1000 | Loss: 0.00005967
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00004063
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001950
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00021623
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00025178
Iteration 77/1000 | Loss: 0.00005519
Iteration 78/1000 | Loss: 0.00023490
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00021856
Iteration 81/1000 | Loss: 0.00003365
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00020263
Iteration 84/1000 | Loss: 0.00001602
Iteration 85/1000 | Loss: 0.00023718
Iteration 86/1000 | Loss: 0.00015459
Iteration 87/1000 | Loss: 0.00023369
Iteration 88/1000 | Loss: 0.00025052
Iteration 89/1000 | Loss: 0.00030983
Iteration 90/1000 | Loss: 0.00081904
Iteration 91/1000 | Loss: 0.00029370
Iteration 92/1000 | Loss: 0.00021585
Iteration 93/1000 | Loss: 0.00020185
Iteration 94/1000 | Loss: 0.00032472
Iteration 95/1000 | Loss: 0.00018729
Iteration 96/1000 | Loss: 0.00028376
Iteration 97/1000 | Loss: 0.00002330
Iteration 98/1000 | Loss: 0.00003594
Iteration 99/1000 | Loss: 0.00002997
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00003838
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00006066
Iteration 104/1000 | Loss: 0.00056969
Iteration 105/1000 | Loss: 0.00003109
Iteration 106/1000 | Loss: 0.00001738
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00006126
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001385
Iteration 112/1000 | Loss: 0.00001316
Iteration 113/1000 | Loss: 0.00001224
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001224
Iteration 119/1000 | Loss: 0.00001224
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001218
Iteration 123/1000 | Loss: 0.00001218
Iteration 124/1000 | Loss: 0.00001218
Iteration 125/1000 | Loss: 0.00001218
Iteration 126/1000 | Loss: 0.00001218
Iteration 127/1000 | Loss: 0.00001218
Iteration 128/1000 | Loss: 0.00001217
Iteration 129/1000 | Loss: 0.00001217
Iteration 130/1000 | Loss: 0.00001217
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001209
Iteration 142/1000 | Loss: 0.00001209
Iteration 143/1000 | Loss: 0.00001209
Iteration 144/1000 | Loss: 0.00001209
Iteration 145/1000 | Loss: 0.00001208
Iteration 146/1000 | Loss: 0.00001208
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001204
Iteration 157/1000 | Loss: 0.00001204
Iteration 158/1000 | Loss: 0.00001204
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001203
Iteration 163/1000 | Loss: 0.00001203
Iteration 164/1000 | Loss: 0.00001203
Iteration 165/1000 | Loss: 0.00001203
Iteration 166/1000 | Loss: 0.00001202
Iteration 167/1000 | Loss: 0.00001202
Iteration 168/1000 | Loss: 0.00001202
Iteration 169/1000 | Loss: 0.00001202
Iteration 170/1000 | Loss: 0.00001202
Iteration 171/1000 | Loss: 0.00001202
Iteration 172/1000 | Loss: 0.00001202
Iteration 173/1000 | Loss: 0.00001201
Iteration 174/1000 | Loss: 0.00001201
Iteration 175/1000 | Loss: 0.00001201
Iteration 176/1000 | Loss: 0.00001201
Iteration 177/1000 | Loss: 0.00001201
Iteration 178/1000 | Loss: 0.00001201
Iteration 179/1000 | Loss: 0.00001201
Iteration 180/1000 | Loss: 0.00001201
Iteration 181/1000 | Loss: 0.00001200
Iteration 182/1000 | Loss: 0.00001200
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001199
Iteration 185/1000 | Loss: 0.00001198
Iteration 186/1000 | Loss: 0.00001198
Iteration 187/1000 | Loss: 0.00001198
Iteration 188/1000 | Loss: 0.00001198
Iteration 189/1000 | Loss: 0.00001198
Iteration 190/1000 | Loss: 0.00001198
Iteration 191/1000 | Loss: 0.00001198
Iteration 192/1000 | Loss: 0.00001198
Iteration 193/1000 | Loss: 0.00001198
Iteration 194/1000 | Loss: 0.00001198
Iteration 195/1000 | Loss: 0.00001197
Iteration 196/1000 | Loss: 0.00001197
Iteration 197/1000 | Loss: 0.00001197
Iteration 198/1000 | Loss: 0.00001197
Iteration 199/1000 | Loss: 0.00001197
Iteration 200/1000 | Loss: 0.00001197
Iteration 201/1000 | Loss: 0.00001197
Iteration 202/1000 | Loss: 0.00001196
Iteration 203/1000 | Loss: 0.00001196
Iteration 204/1000 | Loss: 0.00001196
Iteration 205/1000 | Loss: 0.00001196
Iteration 206/1000 | Loss: 0.00001196
Iteration 207/1000 | Loss: 0.00001196
Iteration 208/1000 | Loss: 0.00001196
Iteration 209/1000 | Loss: 0.00001196
Iteration 210/1000 | Loss: 0.00001196
Iteration 211/1000 | Loss: 0.00001196
Iteration 212/1000 | Loss: 0.00001196
Iteration 213/1000 | Loss: 0.00001196
Iteration 214/1000 | Loss: 0.00001196
Iteration 215/1000 | Loss: 0.00001196
Iteration 216/1000 | Loss: 0.00001196
Iteration 217/1000 | Loss: 0.00001196
Iteration 218/1000 | Loss: 0.00001196
Iteration 219/1000 | Loss: 0.00001196
Iteration 220/1000 | Loss: 0.00001196
Iteration 221/1000 | Loss: 0.00001196
Iteration 222/1000 | Loss: 0.00001196
Iteration 223/1000 | Loss: 0.00001196
Iteration 224/1000 | Loss: 0.00001196
Iteration 225/1000 | Loss: 0.00001196
Iteration 226/1000 | Loss: 0.00001196
Iteration 227/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.1963122233282775e-05, 1.1963122233282775e-05, 1.1963122233282775e-05, 1.1963122233282775e-05, 1.1963122233282775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1963122233282775e-05

Optimization complete. Final v2v error: 2.924314260482788 mm

Highest mean error: 4.0996809005737305 mm for frame 68

Lowest mean error: 2.613901138305664 mm for frame 37

Saving results

Total time: 164.3981626033783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420754
Iteration 2/25 | Loss: 0.00126113
Iteration 3/25 | Loss: 0.00118351
Iteration 4/25 | Loss: 0.00116428
Iteration 5/25 | Loss: 0.00115742
Iteration 6/25 | Loss: 0.00115594
Iteration 7/25 | Loss: 0.00115594
Iteration 8/25 | Loss: 0.00115594
Iteration 9/25 | Loss: 0.00115594
Iteration 10/25 | Loss: 0.00115594
Iteration 11/25 | Loss: 0.00115594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011559372069314122, 0.0011559372069314122, 0.0011559372069314122, 0.0011559372069314122, 0.0011559372069314122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011559372069314122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39286387
Iteration 2/25 | Loss: 0.00117368
Iteration 3/25 | Loss: 0.00117368
Iteration 4/25 | Loss: 0.00117368
Iteration 5/25 | Loss: 0.00117368
Iteration 6/25 | Loss: 0.00117368
Iteration 7/25 | Loss: 0.00117367
Iteration 8/25 | Loss: 0.00117367
Iteration 9/25 | Loss: 0.00117367
Iteration 10/25 | Loss: 0.00117367
Iteration 11/25 | Loss: 0.00117367
Iteration 12/25 | Loss: 0.00117367
Iteration 13/25 | Loss: 0.00117367
Iteration 14/25 | Loss: 0.00117367
Iteration 15/25 | Loss: 0.00117367
Iteration 16/25 | Loss: 0.00117367
Iteration 17/25 | Loss: 0.00117367
Iteration 18/25 | Loss: 0.00117367
Iteration 19/25 | Loss: 0.00117367
Iteration 20/25 | Loss: 0.00117367
Iteration 21/25 | Loss: 0.00117367
Iteration 22/25 | Loss: 0.00117367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011736737797036767, 0.0011736737797036767, 0.0011736737797036767, 0.0011736737797036767, 0.0011736737797036767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011736737797036767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117367
Iteration 2/1000 | Loss: 0.00002384
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001517
Iteration 6/1000 | Loss: 0.00001469
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001313
Iteration 12/1000 | Loss: 0.00001304
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001264
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001259
Iteration 35/1000 | Loss: 0.00001258
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001238
Iteration 101/1000 | Loss: 0.00001238
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001237
Iteration 104/1000 | Loss: 0.00001237
Iteration 105/1000 | Loss: 0.00001237
Iteration 106/1000 | Loss: 0.00001237
Iteration 107/1000 | Loss: 0.00001237
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001236
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001236
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001235
Iteration 127/1000 | Loss: 0.00001235
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001235
Iteration 130/1000 | Loss: 0.00001235
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001235
Iteration 140/1000 | Loss: 0.00001235
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001235
Iteration 143/1000 | Loss: 0.00001235
Iteration 144/1000 | Loss: 0.00001235
Iteration 145/1000 | Loss: 0.00001235
Iteration 146/1000 | Loss: 0.00001235
Iteration 147/1000 | Loss: 0.00001235
Iteration 148/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2350121323834173e-05, 1.2350121323834173e-05, 1.2350121323834173e-05, 1.2350121323834173e-05, 1.2350121323834173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2350121323834173e-05

Optimization complete. Final v2v error: 3.037358283996582 mm

Highest mean error: 3.4929428100585938 mm for frame 119

Lowest mean error: 2.9112789630889893 mm for frame 38

Saving results

Total time: 38.714733600616455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031776
Iteration 2/25 | Loss: 0.00170067
Iteration 3/25 | Loss: 0.00131119
Iteration 4/25 | Loss: 0.00129528
Iteration 5/25 | Loss: 0.00129148
Iteration 6/25 | Loss: 0.00129089
Iteration 7/25 | Loss: 0.00129089
Iteration 8/25 | Loss: 0.00129089
Iteration 9/25 | Loss: 0.00129089
Iteration 10/25 | Loss: 0.00129089
Iteration 11/25 | Loss: 0.00129089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012908859644085169, 0.0012908859644085169, 0.0012908859644085169, 0.0012908859644085169, 0.0012908859644085169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012908859644085169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39025003
Iteration 2/25 | Loss: 0.00112908
Iteration 3/25 | Loss: 0.00112907
Iteration 4/25 | Loss: 0.00112907
Iteration 5/25 | Loss: 0.00112907
Iteration 6/25 | Loss: 0.00112907
Iteration 7/25 | Loss: 0.00112907
Iteration 8/25 | Loss: 0.00112907
Iteration 9/25 | Loss: 0.00112907
Iteration 10/25 | Loss: 0.00112907
Iteration 11/25 | Loss: 0.00112907
Iteration 12/25 | Loss: 0.00112907
Iteration 13/25 | Loss: 0.00112907
Iteration 14/25 | Loss: 0.00112907
Iteration 15/25 | Loss: 0.00112907
Iteration 16/25 | Loss: 0.00112907
Iteration 17/25 | Loss: 0.00112907
Iteration 18/25 | Loss: 0.00112907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011290714610368013, 0.0011290714610368013, 0.0011290714610368013, 0.0011290714610368013, 0.0011290714610368013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011290714610368013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112907
Iteration 2/1000 | Loss: 0.00005070
Iteration 3/1000 | Loss: 0.00003248
Iteration 4/1000 | Loss: 0.00002683
Iteration 5/1000 | Loss: 0.00002462
Iteration 6/1000 | Loss: 0.00002352
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002216
Iteration 9/1000 | Loss: 0.00002176
Iteration 10/1000 | Loss: 0.00002143
Iteration 11/1000 | Loss: 0.00002123
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002101
Iteration 14/1000 | Loss: 0.00002093
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002080
Iteration 17/1000 | Loss: 0.00002073
Iteration 18/1000 | Loss: 0.00002073
Iteration 19/1000 | Loss: 0.00002068
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002064
Iteration 25/1000 | Loss: 0.00002064
Iteration 26/1000 | Loss: 0.00002063
Iteration 27/1000 | Loss: 0.00002063
Iteration 28/1000 | Loss: 0.00002062
Iteration 29/1000 | Loss: 0.00002062
Iteration 30/1000 | Loss: 0.00002062
Iteration 31/1000 | Loss: 0.00002061
Iteration 32/1000 | Loss: 0.00002061
Iteration 33/1000 | Loss: 0.00002061
Iteration 34/1000 | Loss: 0.00002060
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00002059
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002057
Iteration 43/1000 | Loss: 0.00002055
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002052
Iteration 46/1000 | Loss: 0.00002052
Iteration 47/1000 | Loss: 0.00002052
Iteration 48/1000 | Loss: 0.00002051
Iteration 49/1000 | Loss: 0.00002050
Iteration 50/1000 | Loss: 0.00002050
Iteration 51/1000 | Loss: 0.00002050
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002047
Iteration 59/1000 | Loss: 0.00002047
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002046
Iteration 62/1000 | Loss: 0.00002046
Iteration 63/1000 | Loss: 0.00002046
Iteration 64/1000 | Loss: 0.00002046
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002046
Iteration 67/1000 | Loss: 0.00002046
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002045
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002043
Iteration 75/1000 | Loss: 0.00002043
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002042
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002041
Iteration 81/1000 | Loss: 0.00002041
Iteration 82/1000 | Loss: 0.00002040
Iteration 83/1000 | Loss: 0.00002040
Iteration 84/1000 | Loss: 0.00002040
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002039
Iteration 87/1000 | Loss: 0.00002039
Iteration 88/1000 | Loss: 0.00002038
Iteration 89/1000 | Loss: 0.00002038
Iteration 90/1000 | Loss: 0.00002037
Iteration 91/1000 | Loss: 0.00002037
Iteration 92/1000 | Loss: 0.00002037
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002036
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002036
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002036
Iteration 103/1000 | Loss: 0.00002036
Iteration 104/1000 | Loss: 0.00002036
Iteration 105/1000 | Loss: 0.00002035
Iteration 106/1000 | Loss: 0.00002035
Iteration 107/1000 | Loss: 0.00002035
Iteration 108/1000 | Loss: 0.00002035
Iteration 109/1000 | Loss: 0.00002035
Iteration 110/1000 | Loss: 0.00002035
Iteration 111/1000 | Loss: 0.00002035
Iteration 112/1000 | Loss: 0.00002035
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002035
Iteration 115/1000 | Loss: 0.00002035
Iteration 116/1000 | Loss: 0.00002035
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002034
Iteration 119/1000 | Loss: 0.00002034
Iteration 120/1000 | Loss: 0.00002034
Iteration 121/1000 | Loss: 0.00002034
Iteration 122/1000 | Loss: 0.00002034
Iteration 123/1000 | Loss: 0.00002034
Iteration 124/1000 | Loss: 0.00002033
Iteration 125/1000 | Loss: 0.00002033
Iteration 126/1000 | Loss: 0.00002033
Iteration 127/1000 | Loss: 0.00002033
Iteration 128/1000 | Loss: 0.00002033
Iteration 129/1000 | Loss: 0.00002033
Iteration 130/1000 | Loss: 0.00002033
Iteration 131/1000 | Loss: 0.00002033
Iteration 132/1000 | Loss: 0.00002032
Iteration 133/1000 | Loss: 0.00002032
Iteration 134/1000 | Loss: 0.00002032
Iteration 135/1000 | Loss: 0.00002032
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002031
Iteration 139/1000 | Loss: 0.00002031
Iteration 140/1000 | Loss: 0.00002031
Iteration 141/1000 | Loss: 0.00002031
Iteration 142/1000 | Loss: 0.00002031
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002031
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002031
Iteration 152/1000 | Loss: 0.00002031
Iteration 153/1000 | Loss: 0.00002031
Iteration 154/1000 | Loss: 0.00002031
Iteration 155/1000 | Loss: 0.00002031
Iteration 156/1000 | Loss: 0.00002031
Iteration 157/1000 | Loss: 0.00002031
Iteration 158/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.030539690167643e-05, 2.030539690167643e-05, 2.030539690167643e-05, 2.030539690167643e-05, 2.030539690167643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.030539690167643e-05

Optimization complete. Final v2v error: 3.720714569091797 mm

Highest mean error: 4.471729755401611 mm for frame 24

Lowest mean error: 2.794019937515259 mm for frame 0

Saving results

Total time: 41.40719795227051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714556
Iteration 2/25 | Loss: 0.00166347
Iteration 3/25 | Loss: 0.00134631
Iteration 4/25 | Loss: 0.00129967
Iteration 5/25 | Loss: 0.00130107
Iteration 6/25 | Loss: 0.00129207
Iteration 7/25 | Loss: 0.00128284
Iteration 8/25 | Loss: 0.00128120
Iteration 9/25 | Loss: 0.00128089
Iteration 10/25 | Loss: 0.00128077
Iteration 11/25 | Loss: 0.00128067
Iteration 12/25 | Loss: 0.00128063
Iteration 13/25 | Loss: 0.00128061
Iteration 14/25 | Loss: 0.00128061
Iteration 15/25 | Loss: 0.00128060
Iteration 16/25 | Loss: 0.00128060
Iteration 17/25 | Loss: 0.00128060
Iteration 18/25 | Loss: 0.00128060
Iteration 19/25 | Loss: 0.00128060
Iteration 20/25 | Loss: 0.00128060
Iteration 21/25 | Loss: 0.00128060
Iteration 22/25 | Loss: 0.00128060
Iteration 23/25 | Loss: 0.00128059
Iteration 24/25 | Loss: 0.00128059
Iteration 25/25 | Loss: 0.00128059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40113401
Iteration 2/25 | Loss: 0.00103586
Iteration 3/25 | Loss: 0.00103579
Iteration 4/25 | Loss: 0.00103579
Iteration 5/25 | Loss: 0.00103579
Iteration 6/25 | Loss: 0.00103579
Iteration 7/25 | Loss: 0.00103579
Iteration 8/25 | Loss: 0.00103579
Iteration 9/25 | Loss: 0.00103579
Iteration 10/25 | Loss: 0.00103578
Iteration 11/25 | Loss: 0.00103578
Iteration 12/25 | Loss: 0.00103578
Iteration 13/25 | Loss: 0.00103578
Iteration 14/25 | Loss: 0.00103578
Iteration 15/25 | Loss: 0.00103578
Iteration 16/25 | Loss: 0.00103578
Iteration 17/25 | Loss: 0.00103578
Iteration 18/25 | Loss: 0.00103578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010357844876125455, 0.0010357844876125455, 0.0010357844876125455, 0.0010357844876125455, 0.0010357844876125455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010357844876125455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103578
Iteration 2/1000 | Loss: 0.00004882
Iteration 3/1000 | Loss: 0.00002863
Iteration 4/1000 | Loss: 0.00002585
Iteration 5/1000 | Loss: 0.00002462
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00002295
Iteration 8/1000 | Loss: 0.00011069
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002182
Iteration 11/1000 | Loss: 0.00002108
Iteration 12/1000 | Loss: 0.00002058
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002015
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00002000
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001980
Iteration 20/1000 | Loss: 0.00001977
Iteration 21/1000 | Loss: 0.00001972
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001966
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001961
Iteration 26/1000 | Loss: 0.00001960
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00001958
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001955
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001952
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001951
Iteration 44/1000 | Loss: 0.00001951
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001950
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001949
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001948
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001945
Iteration 79/1000 | Loss: 0.00001945
Iteration 80/1000 | Loss: 0.00001945
Iteration 81/1000 | Loss: 0.00001945
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001945
Iteration 91/1000 | Loss: 0.00001945
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00001945
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001945
Iteration 98/1000 | Loss: 0.00001945
Iteration 99/1000 | Loss: 0.00001945
Iteration 100/1000 | Loss: 0.00001945
Iteration 101/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9448367311269976e-05, 1.9448367311269976e-05, 1.9448367311269976e-05, 1.9448367311269976e-05, 1.9448367311269976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9448367311269976e-05

Optimization complete. Final v2v error: 3.6714329719543457 mm

Highest mean error: 4.552065372467041 mm for frame 23

Lowest mean error: 3.081293821334839 mm for frame 198

Saving results

Total time: 59.324575901031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027605
Iteration 2/25 | Loss: 0.01027605
Iteration 3/25 | Loss: 0.01027605
Iteration 4/25 | Loss: 0.01027605
Iteration 5/25 | Loss: 0.01027605
Iteration 6/25 | Loss: 0.01027605
Iteration 7/25 | Loss: 0.01027605
Iteration 8/25 | Loss: 0.01027605
Iteration 9/25 | Loss: 0.01027605
Iteration 10/25 | Loss: 0.01027605
Iteration 11/25 | Loss: 0.01027605
Iteration 12/25 | Loss: 0.01027605
Iteration 13/25 | Loss: 0.01027605
Iteration 14/25 | Loss: 0.01027604
Iteration 15/25 | Loss: 0.01027604
Iteration 16/25 | Loss: 0.01027604
Iteration 17/25 | Loss: 0.01027604
Iteration 18/25 | Loss: 0.01027604
Iteration 19/25 | Loss: 0.01027604
Iteration 20/25 | Loss: 0.01027604
Iteration 21/25 | Loss: 0.01027604
Iteration 22/25 | Loss: 0.01027604
Iteration 23/25 | Loss: 0.01027604
Iteration 24/25 | Loss: 0.01027604
Iteration 25/25 | Loss: 0.01027603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27162147
Iteration 2/25 | Loss: 0.17663933
Iteration 3/25 | Loss: 0.17557640
Iteration 4/25 | Loss: 0.17549758
Iteration 5/25 | Loss: 0.17549758
Iteration 6/25 | Loss: 0.17549758
Iteration 7/25 | Loss: 0.17549758
Iteration 8/25 | Loss: 0.17549758
Iteration 9/25 | Loss: 0.17549758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.17549757659435272, 0.17549757659435272, 0.17549757659435272, 0.17549757659435272, 0.17549757659435272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17549757659435272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17549758
Iteration 2/1000 | Loss: 0.00355128
Iteration 3/1000 | Loss: 0.00200650
Iteration 4/1000 | Loss: 0.00805292
Iteration 5/1000 | Loss: 0.00213133
Iteration 6/1000 | Loss: 0.00073681
Iteration 7/1000 | Loss: 0.00065748
Iteration 8/1000 | Loss: 0.00008095
Iteration 9/1000 | Loss: 0.00048842
Iteration 10/1000 | Loss: 0.00010443
Iteration 11/1000 | Loss: 0.00010286
Iteration 12/1000 | Loss: 0.00002895
Iteration 13/1000 | Loss: 0.00004061
Iteration 14/1000 | Loss: 0.00036618
Iteration 15/1000 | Loss: 0.00024790
Iteration 16/1000 | Loss: 0.00001972
Iteration 17/1000 | Loss: 0.00006605
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00231185
Iteration 22/1000 | Loss: 0.00020738
Iteration 23/1000 | Loss: 0.00029088
Iteration 24/1000 | Loss: 0.00002145
Iteration 25/1000 | Loss: 0.00001433
Iteration 26/1000 | Loss: 0.00016000
Iteration 27/1000 | Loss: 0.00009310
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001072
Iteration 35/1000 | Loss: 0.00001047
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001040
Iteration 38/1000 | Loss: 0.00001027
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001022
Iteration 41/1000 | Loss: 0.00001021
Iteration 42/1000 | Loss: 0.00001019
Iteration 43/1000 | Loss: 0.00001016
Iteration 44/1000 | Loss: 0.00001014
Iteration 45/1000 | Loss: 0.00001011
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00016136
Iteration 49/1000 | Loss: 0.00003984
Iteration 50/1000 | Loss: 0.00003681
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001023
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000992
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000989
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000989
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000987
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00000985
Iteration 69/1000 | Loss: 0.00000985
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000985
Iteration 75/1000 | Loss: 0.00000985
Iteration 76/1000 | Loss: 0.00000985
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000983
Iteration 80/1000 | Loss: 0.00000983
Iteration 81/1000 | Loss: 0.00000982
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000976
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000974
Iteration 119/1000 | Loss: 0.00000974
Iteration 120/1000 | Loss: 0.00000974
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000971
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000970
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000970
Iteration 137/1000 | Loss: 0.00000970
Iteration 138/1000 | Loss: 0.00000970
Iteration 139/1000 | Loss: 0.00000970
Iteration 140/1000 | Loss: 0.00000970
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000969
Iteration 145/1000 | Loss: 0.00000969
Iteration 146/1000 | Loss: 0.00000969
Iteration 147/1000 | Loss: 0.00000969
Iteration 148/1000 | Loss: 0.00000969
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000968
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000967
Iteration 153/1000 | Loss: 0.00000967
Iteration 154/1000 | Loss: 0.00000967
Iteration 155/1000 | Loss: 0.00000967
Iteration 156/1000 | Loss: 0.00000967
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000966
Iteration 159/1000 | Loss: 0.00000966
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000964
Iteration 162/1000 | Loss: 0.00000963
Iteration 163/1000 | Loss: 0.00000963
Iteration 164/1000 | Loss: 0.00000963
Iteration 165/1000 | Loss: 0.00000963
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000962
Iteration 170/1000 | Loss: 0.00000962
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000962
Iteration 173/1000 | Loss: 0.00000961
Iteration 174/1000 | Loss: 0.00000961
Iteration 175/1000 | Loss: 0.00000961
Iteration 176/1000 | Loss: 0.00000960
Iteration 177/1000 | Loss: 0.00000960
Iteration 178/1000 | Loss: 0.00000960
Iteration 179/1000 | Loss: 0.00000960
Iteration 180/1000 | Loss: 0.00000960
Iteration 181/1000 | Loss: 0.00000960
Iteration 182/1000 | Loss: 0.00000960
Iteration 183/1000 | Loss: 0.00000960
Iteration 184/1000 | Loss: 0.00000960
Iteration 185/1000 | Loss: 0.00000960
Iteration 186/1000 | Loss: 0.00000959
Iteration 187/1000 | Loss: 0.00000959
Iteration 188/1000 | Loss: 0.00000959
Iteration 189/1000 | Loss: 0.00000959
Iteration 190/1000 | Loss: 0.00000958
Iteration 191/1000 | Loss: 0.00000958
Iteration 192/1000 | Loss: 0.00000958
Iteration 193/1000 | Loss: 0.00000957
Iteration 194/1000 | Loss: 0.00000957
Iteration 195/1000 | Loss: 0.00000957
Iteration 196/1000 | Loss: 0.00000957
Iteration 197/1000 | Loss: 0.00000957
Iteration 198/1000 | Loss: 0.00000957
Iteration 199/1000 | Loss: 0.00000957
Iteration 200/1000 | Loss: 0.00000957
Iteration 201/1000 | Loss: 0.00000957
Iteration 202/1000 | Loss: 0.00000957
Iteration 203/1000 | Loss: 0.00000957
Iteration 204/1000 | Loss: 0.00000957
Iteration 205/1000 | Loss: 0.00000957
Iteration 206/1000 | Loss: 0.00000957
Iteration 207/1000 | Loss: 0.00000957
Iteration 208/1000 | Loss: 0.00000957
Iteration 209/1000 | Loss: 0.00000957
Iteration 210/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [9.566950211592484e-06, 9.566950211592484e-06, 9.566950211592484e-06, 9.566950211592484e-06, 9.566950211592484e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.566950211592484e-06

Optimization complete. Final v2v error: 2.645747661590576 mm

Highest mean error: 2.937488317489624 mm for frame 64

Lowest mean error: 2.5290026664733887 mm for frame 165

Saving results

Total time: 86.45840191841125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466146
Iteration 2/25 | Loss: 0.00127952
Iteration 3/25 | Loss: 0.00117869
Iteration 4/25 | Loss: 0.00116571
Iteration 5/25 | Loss: 0.00116322
Iteration 6/25 | Loss: 0.00116322
Iteration 7/25 | Loss: 0.00116322
Iteration 8/25 | Loss: 0.00116322
Iteration 9/25 | Loss: 0.00116322
Iteration 10/25 | Loss: 0.00116322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001163223758339882, 0.001163223758339882, 0.001163223758339882, 0.001163223758339882, 0.001163223758339882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163223758339882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.78227663
Iteration 2/25 | Loss: 0.00109382
Iteration 3/25 | Loss: 0.00109381
Iteration 4/25 | Loss: 0.00109381
Iteration 5/25 | Loss: 0.00109381
Iteration 6/25 | Loss: 0.00109381
Iteration 7/25 | Loss: 0.00109381
Iteration 8/25 | Loss: 0.00109381
Iteration 9/25 | Loss: 0.00109381
Iteration 10/25 | Loss: 0.00109381
Iteration 11/25 | Loss: 0.00109381
Iteration 12/25 | Loss: 0.00109381
Iteration 13/25 | Loss: 0.00109381
Iteration 14/25 | Loss: 0.00109381
Iteration 15/25 | Loss: 0.00109381
Iteration 16/25 | Loss: 0.00109381
Iteration 17/25 | Loss: 0.00109381
Iteration 18/25 | Loss: 0.00109381
Iteration 19/25 | Loss: 0.00109381
Iteration 20/25 | Loss: 0.00109381
Iteration 21/25 | Loss: 0.00109381
Iteration 22/25 | Loss: 0.00109381
Iteration 23/25 | Loss: 0.00109381
Iteration 24/25 | Loss: 0.00109381
Iteration 25/25 | Loss: 0.00109381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109381
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001514
Iteration 5/1000 | Loss: 0.00001431
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001247
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001207
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001171
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001166
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001166
Iteration 40/1000 | Loss: 0.00001166
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001164
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001161
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001148
Iteration 63/1000 | Loss: 0.00001148
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001147
Iteration 67/1000 | Loss: 0.00001147
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001146
Iteration 72/1000 | Loss: 0.00001146
Iteration 73/1000 | Loss: 0.00001146
Iteration 74/1000 | Loss: 0.00001146
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001143
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001141
Iteration 107/1000 | Loss: 0.00001141
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001140
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001140
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001140
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.1392740816518199e-05, 1.1392740816518199e-05, 1.1392740816518199e-05, 1.1392740816518199e-05, 1.1392740816518199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1392740816518199e-05

Optimization complete. Final v2v error: 2.9059157371520996 mm

Highest mean error: 3.0709121227264404 mm for frame 88

Lowest mean error: 2.7098050117492676 mm for frame 211

Saving results

Total time: 42.64721989631653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383782
Iteration 2/25 | Loss: 0.00121479
Iteration 3/25 | Loss: 0.00114422
Iteration 4/25 | Loss: 0.00113415
Iteration 5/25 | Loss: 0.00113060
Iteration 6/25 | Loss: 0.00112955
Iteration 7/25 | Loss: 0.00112953
Iteration 8/25 | Loss: 0.00112953
Iteration 9/25 | Loss: 0.00112953
Iteration 10/25 | Loss: 0.00112953
Iteration 11/25 | Loss: 0.00112953
Iteration 12/25 | Loss: 0.00112953
Iteration 13/25 | Loss: 0.00112953
Iteration 14/25 | Loss: 0.00112953
Iteration 15/25 | Loss: 0.00112953
Iteration 16/25 | Loss: 0.00112953
Iteration 17/25 | Loss: 0.00112952
Iteration 18/25 | Loss: 0.00112952
Iteration 19/25 | Loss: 0.00112952
Iteration 20/25 | Loss: 0.00112952
Iteration 21/25 | Loss: 0.00112952
Iteration 22/25 | Loss: 0.00112952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011295198928564787, 0.0011295198928564787, 0.0011295198928564787, 0.0011295198928564787, 0.0011295198928564787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011295198928564787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80499208
Iteration 2/25 | Loss: 0.00128480
Iteration 3/25 | Loss: 0.00128480
Iteration 4/25 | Loss: 0.00128480
Iteration 5/25 | Loss: 0.00128480
Iteration 6/25 | Loss: 0.00128479
Iteration 7/25 | Loss: 0.00128479
Iteration 8/25 | Loss: 0.00128479
Iteration 9/25 | Loss: 0.00128479
Iteration 10/25 | Loss: 0.00128479
Iteration 11/25 | Loss: 0.00128479
Iteration 12/25 | Loss: 0.00128479
Iteration 13/25 | Loss: 0.00128479
Iteration 14/25 | Loss: 0.00128479
Iteration 15/25 | Loss: 0.00128479
Iteration 16/25 | Loss: 0.00128479
Iteration 17/25 | Loss: 0.00128479
Iteration 18/25 | Loss: 0.00128479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001284793484956026, 0.001284793484956026, 0.001284793484956026, 0.001284793484956026, 0.001284793484956026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001284793484956026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128479
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001534
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001187
Iteration 6/1000 | Loss: 0.00001128
Iteration 7/1000 | Loss: 0.00001083
Iteration 8/1000 | Loss: 0.00001050
Iteration 9/1000 | Loss: 0.00001026
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00000980
Iteration 12/1000 | Loss: 0.00000980
Iteration 13/1000 | Loss: 0.00000977
Iteration 14/1000 | Loss: 0.00000970
Iteration 15/1000 | Loss: 0.00000965
Iteration 16/1000 | Loss: 0.00000961
Iteration 17/1000 | Loss: 0.00000961
Iteration 18/1000 | Loss: 0.00000960
Iteration 19/1000 | Loss: 0.00000958
Iteration 20/1000 | Loss: 0.00000955
Iteration 21/1000 | Loss: 0.00000946
Iteration 22/1000 | Loss: 0.00000945
Iteration 23/1000 | Loss: 0.00000945
Iteration 24/1000 | Loss: 0.00000945
Iteration 25/1000 | Loss: 0.00000945
Iteration 26/1000 | Loss: 0.00000943
Iteration 27/1000 | Loss: 0.00000941
Iteration 28/1000 | Loss: 0.00000941
Iteration 29/1000 | Loss: 0.00000940
Iteration 30/1000 | Loss: 0.00000940
Iteration 31/1000 | Loss: 0.00000940
Iteration 32/1000 | Loss: 0.00000939
Iteration 33/1000 | Loss: 0.00000939
Iteration 34/1000 | Loss: 0.00000939
Iteration 35/1000 | Loss: 0.00000939
Iteration 36/1000 | Loss: 0.00000938
Iteration 37/1000 | Loss: 0.00000936
Iteration 38/1000 | Loss: 0.00000935
Iteration 39/1000 | Loss: 0.00000935
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000933
Iteration 43/1000 | Loss: 0.00000932
Iteration 44/1000 | Loss: 0.00000931
Iteration 45/1000 | Loss: 0.00000930
Iteration 46/1000 | Loss: 0.00000926
Iteration 47/1000 | Loss: 0.00000926
Iteration 48/1000 | Loss: 0.00000925
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000924
Iteration 51/1000 | Loss: 0.00000924
Iteration 52/1000 | Loss: 0.00000924
Iteration 53/1000 | Loss: 0.00000923
Iteration 54/1000 | Loss: 0.00000922
Iteration 55/1000 | Loss: 0.00000922
Iteration 56/1000 | Loss: 0.00000922
Iteration 57/1000 | Loss: 0.00000921
Iteration 58/1000 | Loss: 0.00000921
Iteration 59/1000 | Loss: 0.00000921
Iteration 60/1000 | Loss: 0.00000921
Iteration 61/1000 | Loss: 0.00000921
Iteration 62/1000 | Loss: 0.00000921
Iteration 63/1000 | Loss: 0.00000921
Iteration 64/1000 | Loss: 0.00000920
Iteration 65/1000 | Loss: 0.00000920
Iteration 66/1000 | Loss: 0.00000920
Iteration 67/1000 | Loss: 0.00000920
Iteration 68/1000 | Loss: 0.00000919
Iteration 69/1000 | Loss: 0.00000919
Iteration 70/1000 | Loss: 0.00000918
Iteration 71/1000 | Loss: 0.00000918
Iteration 72/1000 | Loss: 0.00000918
Iteration 73/1000 | Loss: 0.00000917
Iteration 74/1000 | Loss: 0.00000917
Iteration 75/1000 | Loss: 0.00000917
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000916
Iteration 80/1000 | Loss: 0.00000916
Iteration 81/1000 | Loss: 0.00000916
Iteration 82/1000 | Loss: 0.00000915
Iteration 83/1000 | Loss: 0.00000914
Iteration 84/1000 | Loss: 0.00000913
Iteration 85/1000 | Loss: 0.00000913
Iteration 86/1000 | Loss: 0.00000913
Iteration 87/1000 | Loss: 0.00000912
Iteration 88/1000 | Loss: 0.00000912
Iteration 89/1000 | Loss: 0.00000912
Iteration 90/1000 | Loss: 0.00000912
Iteration 91/1000 | Loss: 0.00000912
Iteration 92/1000 | Loss: 0.00000912
Iteration 93/1000 | Loss: 0.00000911
Iteration 94/1000 | Loss: 0.00000911
Iteration 95/1000 | Loss: 0.00000911
Iteration 96/1000 | Loss: 0.00000910
Iteration 97/1000 | Loss: 0.00000910
Iteration 98/1000 | Loss: 0.00000910
Iteration 99/1000 | Loss: 0.00000910
Iteration 100/1000 | Loss: 0.00000909
Iteration 101/1000 | Loss: 0.00000909
Iteration 102/1000 | Loss: 0.00000909
Iteration 103/1000 | Loss: 0.00000909
Iteration 104/1000 | Loss: 0.00000909
Iteration 105/1000 | Loss: 0.00000909
Iteration 106/1000 | Loss: 0.00000908
Iteration 107/1000 | Loss: 0.00000908
Iteration 108/1000 | Loss: 0.00000908
Iteration 109/1000 | Loss: 0.00000908
Iteration 110/1000 | Loss: 0.00000908
Iteration 111/1000 | Loss: 0.00000908
Iteration 112/1000 | Loss: 0.00000907
Iteration 113/1000 | Loss: 0.00000907
Iteration 114/1000 | Loss: 0.00000907
Iteration 115/1000 | Loss: 0.00000907
Iteration 116/1000 | Loss: 0.00000907
Iteration 117/1000 | Loss: 0.00000907
Iteration 118/1000 | Loss: 0.00000907
Iteration 119/1000 | Loss: 0.00000907
Iteration 120/1000 | Loss: 0.00000907
Iteration 121/1000 | Loss: 0.00000907
Iteration 122/1000 | Loss: 0.00000907
Iteration 123/1000 | Loss: 0.00000906
Iteration 124/1000 | Loss: 0.00000906
Iteration 125/1000 | Loss: 0.00000906
Iteration 126/1000 | Loss: 0.00000906
Iteration 127/1000 | Loss: 0.00000906
Iteration 128/1000 | Loss: 0.00000906
Iteration 129/1000 | Loss: 0.00000906
Iteration 130/1000 | Loss: 0.00000906
Iteration 131/1000 | Loss: 0.00000906
Iteration 132/1000 | Loss: 0.00000906
Iteration 133/1000 | Loss: 0.00000906
Iteration 134/1000 | Loss: 0.00000905
Iteration 135/1000 | Loss: 0.00000905
Iteration 136/1000 | Loss: 0.00000905
Iteration 137/1000 | Loss: 0.00000905
Iteration 138/1000 | Loss: 0.00000905
Iteration 139/1000 | Loss: 0.00000905
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000905
Iteration 142/1000 | Loss: 0.00000905
Iteration 143/1000 | Loss: 0.00000905
Iteration 144/1000 | Loss: 0.00000905
Iteration 145/1000 | Loss: 0.00000905
Iteration 146/1000 | Loss: 0.00000905
Iteration 147/1000 | Loss: 0.00000904
Iteration 148/1000 | Loss: 0.00000904
Iteration 149/1000 | Loss: 0.00000904
Iteration 150/1000 | Loss: 0.00000904
Iteration 151/1000 | Loss: 0.00000904
Iteration 152/1000 | Loss: 0.00000904
Iteration 153/1000 | Loss: 0.00000904
Iteration 154/1000 | Loss: 0.00000904
Iteration 155/1000 | Loss: 0.00000904
Iteration 156/1000 | Loss: 0.00000904
Iteration 157/1000 | Loss: 0.00000904
Iteration 158/1000 | Loss: 0.00000904
Iteration 159/1000 | Loss: 0.00000903
Iteration 160/1000 | Loss: 0.00000903
Iteration 161/1000 | Loss: 0.00000903
Iteration 162/1000 | Loss: 0.00000903
Iteration 163/1000 | Loss: 0.00000903
Iteration 164/1000 | Loss: 0.00000903
Iteration 165/1000 | Loss: 0.00000903
Iteration 166/1000 | Loss: 0.00000903
Iteration 167/1000 | Loss: 0.00000903
Iteration 168/1000 | Loss: 0.00000903
Iteration 169/1000 | Loss: 0.00000903
Iteration 170/1000 | Loss: 0.00000903
Iteration 171/1000 | Loss: 0.00000903
Iteration 172/1000 | Loss: 0.00000903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [9.032274647324812e-06, 9.032274647324812e-06, 9.032274647324812e-06, 9.032274647324812e-06, 9.032274647324812e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.032274647324812e-06

Optimization complete. Final v2v error: 2.6007981300354004 mm

Highest mean error: 3.026963233947754 mm for frame 55

Lowest mean error: 2.5084681510925293 mm for frame 71

Saving results

Total time: 40.00121736526489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565946
Iteration 2/25 | Loss: 0.00131509
Iteration 3/25 | Loss: 0.00121299
Iteration 4/25 | Loss: 0.00119431
Iteration 5/25 | Loss: 0.00118752
Iteration 6/25 | Loss: 0.00118556
Iteration 7/25 | Loss: 0.00118550
Iteration 8/25 | Loss: 0.00118550
Iteration 9/25 | Loss: 0.00118550
Iteration 10/25 | Loss: 0.00118550
Iteration 11/25 | Loss: 0.00118550
Iteration 12/25 | Loss: 0.00118550
Iteration 13/25 | Loss: 0.00118550
Iteration 14/25 | Loss: 0.00118550
Iteration 15/25 | Loss: 0.00118550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011855015764012933, 0.0011855015764012933, 0.0011855015764012933, 0.0011855015764012933, 0.0011855015764012933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011855015764012933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41463172
Iteration 2/25 | Loss: 0.00131510
Iteration 3/25 | Loss: 0.00131510
Iteration 4/25 | Loss: 0.00131510
Iteration 5/25 | Loss: 0.00131510
Iteration 6/25 | Loss: 0.00131510
Iteration 7/25 | Loss: 0.00131509
Iteration 8/25 | Loss: 0.00131509
Iteration 9/25 | Loss: 0.00131509
Iteration 10/25 | Loss: 0.00131509
Iteration 11/25 | Loss: 0.00131509
Iteration 12/25 | Loss: 0.00131509
Iteration 13/25 | Loss: 0.00131509
Iteration 14/25 | Loss: 0.00131509
Iteration 15/25 | Loss: 0.00131509
Iteration 16/25 | Loss: 0.00131509
Iteration 17/25 | Loss: 0.00131509
Iteration 18/25 | Loss: 0.00131509
Iteration 19/25 | Loss: 0.00131509
Iteration 20/25 | Loss: 0.00131509
Iteration 21/25 | Loss: 0.00131509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013150937156751752, 0.0013150937156751752, 0.0013150937156751752, 0.0013150937156751752, 0.0013150937156751752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013150937156751752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131509
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001460
Iteration 6/1000 | Loss: 0.00001415
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001308
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001233
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001227
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001215
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001210
Iteration 146/1000 | Loss: 0.00001210
Iteration 147/1000 | Loss: 0.00001210
Iteration 148/1000 | Loss: 0.00001210
Iteration 149/1000 | Loss: 0.00001210
Iteration 150/1000 | Loss: 0.00001210
Iteration 151/1000 | Loss: 0.00001210
Iteration 152/1000 | Loss: 0.00001210
Iteration 153/1000 | Loss: 0.00001210
Iteration 154/1000 | Loss: 0.00001210
Iteration 155/1000 | Loss: 0.00001210
Iteration 156/1000 | Loss: 0.00001210
Iteration 157/1000 | Loss: 0.00001209
Iteration 158/1000 | Loss: 0.00001209
Iteration 159/1000 | Loss: 0.00001209
Iteration 160/1000 | Loss: 0.00001209
Iteration 161/1000 | Loss: 0.00001209
Iteration 162/1000 | Loss: 0.00001209
Iteration 163/1000 | Loss: 0.00001209
Iteration 164/1000 | Loss: 0.00001209
Iteration 165/1000 | Loss: 0.00001209
Iteration 166/1000 | Loss: 0.00001209
Iteration 167/1000 | Loss: 0.00001209
Iteration 168/1000 | Loss: 0.00001209
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001209
Iteration 172/1000 | Loss: 0.00001209
Iteration 173/1000 | Loss: 0.00001208
Iteration 174/1000 | Loss: 0.00001208
Iteration 175/1000 | Loss: 0.00001208
Iteration 176/1000 | Loss: 0.00001208
Iteration 177/1000 | Loss: 0.00001208
Iteration 178/1000 | Loss: 0.00001208
Iteration 179/1000 | Loss: 0.00001208
Iteration 180/1000 | Loss: 0.00001208
Iteration 181/1000 | Loss: 0.00001208
Iteration 182/1000 | Loss: 0.00001208
Iteration 183/1000 | Loss: 0.00001208
Iteration 184/1000 | Loss: 0.00001208
Iteration 185/1000 | Loss: 0.00001208
Iteration 186/1000 | Loss: 0.00001208
Iteration 187/1000 | Loss: 0.00001208
Iteration 188/1000 | Loss: 0.00001208
Iteration 189/1000 | Loss: 0.00001207
Iteration 190/1000 | Loss: 0.00001207
Iteration 191/1000 | Loss: 0.00001207
Iteration 192/1000 | Loss: 0.00001207
Iteration 193/1000 | Loss: 0.00001207
Iteration 194/1000 | Loss: 0.00001207
Iteration 195/1000 | Loss: 0.00001207
Iteration 196/1000 | Loss: 0.00001207
Iteration 197/1000 | Loss: 0.00001207
Iteration 198/1000 | Loss: 0.00001207
Iteration 199/1000 | Loss: 0.00001207
Iteration 200/1000 | Loss: 0.00001207
Iteration 201/1000 | Loss: 0.00001207
Iteration 202/1000 | Loss: 0.00001207
Iteration 203/1000 | Loss: 0.00001207
Iteration 204/1000 | Loss: 0.00001207
Iteration 205/1000 | Loss: 0.00001207
Iteration 206/1000 | Loss: 0.00001207
Iteration 207/1000 | Loss: 0.00001207
Iteration 208/1000 | Loss: 0.00001207
Iteration 209/1000 | Loss: 0.00001207
Iteration 210/1000 | Loss: 0.00001207
Iteration 211/1000 | Loss: 0.00001207
Iteration 212/1000 | Loss: 0.00001207
Iteration 213/1000 | Loss: 0.00001207
Iteration 214/1000 | Loss: 0.00001207
Iteration 215/1000 | Loss: 0.00001207
Iteration 216/1000 | Loss: 0.00001207
Iteration 217/1000 | Loss: 0.00001207
Iteration 218/1000 | Loss: 0.00001207
Iteration 219/1000 | Loss: 0.00001207
Iteration 220/1000 | Loss: 0.00001207
Iteration 221/1000 | Loss: 0.00001207
Iteration 222/1000 | Loss: 0.00001207
Iteration 223/1000 | Loss: 0.00001207
Iteration 224/1000 | Loss: 0.00001207
Iteration 225/1000 | Loss: 0.00001207
Iteration 226/1000 | Loss: 0.00001207
Iteration 227/1000 | Loss: 0.00001207
Iteration 228/1000 | Loss: 0.00001207
Iteration 229/1000 | Loss: 0.00001207
Iteration 230/1000 | Loss: 0.00001207
Iteration 231/1000 | Loss: 0.00001207
Iteration 232/1000 | Loss: 0.00001207
Iteration 233/1000 | Loss: 0.00001207
Iteration 234/1000 | Loss: 0.00001207
Iteration 235/1000 | Loss: 0.00001207
Iteration 236/1000 | Loss: 0.00001207
Iteration 237/1000 | Loss: 0.00001207
Iteration 238/1000 | Loss: 0.00001207
Iteration 239/1000 | Loss: 0.00001207
Iteration 240/1000 | Loss: 0.00001207
Iteration 241/1000 | Loss: 0.00001207
Iteration 242/1000 | Loss: 0.00001207
Iteration 243/1000 | Loss: 0.00001207
Iteration 244/1000 | Loss: 0.00001207
Iteration 245/1000 | Loss: 0.00001207
Iteration 246/1000 | Loss: 0.00001207
Iteration 247/1000 | Loss: 0.00001207
Iteration 248/1000 | Loss: 0.00001207
Iteration 249/1000 | Loss: 0.00001207
Iteration 250/1000 | Loss: 0.00001207
Iteration 251/1000 | Loss: 0.00001207
Iteration 252/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.2069003787473775e-05, 1.2069003787473775e-05, 1.2069003787473775e-05, 1.2069003787473775e-05, 1.2069003787473775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2069003787473775e-05

Optimization complete. Final v2v error: 2.9736287593841553 mm

Highest mean error: 3.407348394393921 mm for frame 1

Lowest mean error: 2.6800129413604736 mm for frame 20

Saving results

Total time: 41.32370972633362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798718
Iteration 2/25 | Loss: 0.00132828
Iteration 3/25 | Loss: 0.00120983
Iteration 4/25 | Loss: 0.00119593
Iteration 5/25 | Loss: 0.00119274
Iteration 6/25 | Loss: 0.00119171
Iteration 7/25 | Loss: 0.00119171
Iteration 8/25 | Loss: 0.00119171
Iteration 9/25 | Loss: 0.00119171
Iteration 10/25 | Loss: 0.00119171
Iteration 11/25 | Loss: 0.00119171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011917082592844963, 0.0011917082592844963, 0.0011917082592844963, 0.0011917082592844963, 0.0011917082592844963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011917082592844963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36176586
Iteration 2/25 | Loss: 0.00122840
Iteration 3/25 | Loss: 0.00122840
Iteration 4/25 | Loss: 0.00122840
Iteration 5/25 | Loss: 0.00122840
Iteration 6/25 | Loss: 0.00122840
Iteration 7/25 | Loss: 0.00122840
Iteration 8/25 | Loss: 0.00122840
Iteration 9/25 | Loss: 0.00122840
Iteration 10/25 | Loss: 0.00122840
Iteration 11/25 | Loss: 0.00122840
Iteration 12/25 | Loss: 0.00122840
Iteration 13/25 | Loss: 0.00122840
Iteration 14/25 | Loss: 0.00122840
Iteration 15/25 | Loss: 0.00122840
Iteration 16/25 | Loss: 0.00122840
Iteration 17/25 | Loss: 0.00122840
Iteration 18/25 | Loss: 0.00122840
Iteration 19/25 | Loss: 0.00122840
Iteration 20/25 | Loss: 0.00122840
Iteration 21/25 | Loss: 0.00122840
Iteration 22/25 | Loss: 0.00122840
Iteration 23/25 | Loss: 0.00122840
Iteration 24/25 | Loss: 0.00122840
Iteration 25/25 | Loss: 0.00122840

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122840
Iteration 2/1000 | Loss: 0.00003849
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001887
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001701
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001640
Iteration 11/1000 | Loss: 0.00001619
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001603
Iteration 14/1000 | Loss: 0.00001588
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001569
Iteration 23/1000 | Loss: 0.00001569
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001562
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00001554
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001551
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001548
Iteration 48/1000 | Loss: 0.00001548
Iteration 49/1000 | Loss: 0.00001548
Iteration 50/1000 | Loss: 0.00001548
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001548
Iteration 54/1000 | Loss: 0.00001547
Iteration 55/1000 | Loss: 0.00001547
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001544
Iteration 71/1000 | Loss: 0.00001544
Iteration 72/1000 | Loss: 0.00001544
Iteration 73/1000 | Loss: 0.00001544
Iteration 74/1000 | Loss: 0.00001543
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001541
Iteration 80/1000 | Loss: 0.00001541
Iteration 81/1000 | Loss: 0.00001541
Iteration 82/1000 | Loss: 0.00001541
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001538
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001537
Iteration 96/1000 | Loss: 0.00001537
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001537
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001537
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001537
Iteration 105/1000 | Loss: 0.00001537
Iteration 106/1000 | Loss: 0.00001537
Iteration 107/1000 | Loss: 0.00001536
Iteration 108/1000 | Loss: 0.00001536
Iteration 109/1000 | Loss: 0.00001536
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001534
Iteration 117/1000 | Loss: 0.00001534
Iteration 118/1000 | Loss: 0.00001534
Iteration 119/1000 | Loss: 0.00001534
Iteration 120/1000 | Loss: 0.00001534
Iteration 121/1000 | Loss: 0.00001534
Iteration 122/1000 | Loss: 0.00001534
Iteration 123/1000 | Loss: 0.00001534
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001532
Iteration 130/1000 | Loss: 0.00001532
Iteration 131/1000 | Loss: 0.00001532
Iteration 132/1000 | Loss: 0.00001532
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001531
Iteration 135/1000 | Loss: 0.00001531
Iteration 136/1000 | Loss: 0.00001531
Iteration 137/1000 | Loss: 0.00001531
Iteration 138/1000 | Loss: 0.00001531
Iteration 139/1000 | Loss: 0.00001531
Iteration 140/1000 | Loss: 0.00001531
Iteration 141/1000 | Loss: 0.00001531
Iteration 142/1000 | Loss: 0.00001531
Iteration 143/1000 | Loss: 0.00001531
Iteration 144/1000 | Loss: 0.00001530
Iteration 145/1000 | Loss: 0.00001530
Iteration 146/1000 | Loss: 0.00001530
Iteration 147/1000 | Loss: 0.00001530
Iteration 148/1000 | Loss: 0.00001530
Iteration 149/1000 | Loss: 0.00001530
Iteration 150/1000 | Loss: 0.00001529
Iteration 151/1000 | Loss: 0.00001529
Iteration 152/1000 | Loss: 0.00001529
Iteration 153/1000 | Loss: 0.00001529
Iteration 154/1000 | Loss: 0.00001528
Iteration 155/1000 | Loss: 0.00001528
Iteration 156/1000 | Loss: 0.00001528
Iteration 157/1000 | Loss: 0.00001527
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001525
Iteration 161/1000 | Loss: 0.00001525
Iteration 162/1000 | Loss: 0.00001524
Iteration 163/1000 | Loss: 0.00001524
Iteration 164/1000 | Loss: 0.00001524
Iteration 165/1000 | Loss: 0.00001524
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001522
Iteration 171/1000 | Loss: 0.00001521
Iteration 172/1000 | Loss: 0.00001521
Iteration 173/1000 | Loss: 0.00001521
Iteration 174/1000 | Loss: 0.00001520
Iteration 175/1000 | Loss: 0.00001520
Iteration 176/1000 | Loss: 0.00001520
Iteration 177/1000 | Loss: 0.00001520
Iteration 178/1000 | Loss: 0.00001520
Iteration 179/1000 | Loss: 0.00001520
Iteration 180/1000 | Loss: 0.00001520
Iteration 181/1000 | Loss: 0.00001520
Iteration 182/1000 | Loss: 0.00001519
Iteration 183/1000 | Loss: 0.00001519
Iteration 184/1000 | Loss: 0.00001519
Iteration 185/1000 | Loss: 0.00001518
Iteration 186/1000 | Loss: 0.00001518
Iteration 187/1000 | Loss: 0.00001518
Iteration 188/1000 | Loss: 0.00001518
Iteration 189/1000 | Loss: 0.00001518
Iteration 190/1000 | Loss: 0.00001517
Iteration 191/1000 | Loss: 0.00001517
Iteration 192/1000 | Loss: 0.00001517
Iteration 193/1000 | Loss: 0.00001517
Iteration 194/1000 | Loss: 0.00001517
Iteration 195/1000 | Loss: 0.00001517
Iteration 196/1000 | Loss: 0.00001517
Iteration 197/1000 | Loss: 0.00001517
Iteration 198/1000 | Loss: 0.00001517
Iteration 199/1000 | Loss: 0.00001517
Iteration 200/1000 | Loss: 0.00001517
Iteration 201/1000 | Loss: 0.00001516
Iteration 202/1000 | Loss: 0.00001516
Iteration 203/1000 | Loss: 0.00001516
Iteration 204/1000 | Loss: 0.00001516
Iteration 205/1000 | Loss: 0.00001516
Iteration 206/1000 | Loss: 0.00001516
Iteration 207/1000 | Loss: 0.00001515
Iteration 208/1000 | Loss: 0.00001515
Iteration 209/1000 | Loss: 0.00001515
Iteration 210/1000 | Loss: 0.00001515
Iteration 211/1000 | Loss: 0.00001515
Iteration 212/1000 | Loss: 0.00001515
Iteration 213/1000 | Loss: 0.00001515
Iteration 214/1000 | Loss: 0.00001515
Iteration 215/1000 | Loss: 0.00001515
Iteration 216/1000 | Loss: 0.00001515
Iteration 217/1000 | Loss: 0.00001515
Iteration 218/1000 | Loss: 0.00001515
Iteration 219/1000 | Loss: 0.00001515
Iteration 220/1000 | Loss: 0.00001515
Iteration 221/1000 | Loss: 0.00001515
Iteration 222/1000 | Loss: 0.00001515
Iteration 223/1000 | Loss: 0.00001515
Iteration 224/1000 | Loss: 0.00001515
Iteration 225/1000 | Loss: 0.00001515
Iteration 226/1000 | Loss: 0.00001514
Iteration 227/1000 | Loss: 0.00001514
Iteration 228/1000 | Loss: 0.00001514
Iteration 229/1000 | Loss: 0.00001514
Iteration 230/1000 | Loss: 0.00001514
Iteration 231/1000 | Loss: 0.00001514
Iteration 232/1000 | Loss: 0.00001514
Iteration 233/1000 | Loss: 0.00001514
Iteration 234/1000 | Loss: 0.00001514
Iteration 235/1000 | Loss: 0.00001514
Iteration 236/1000 | Loss: 0.00001514
Iteration 237/1000 | Loss: 0.00001514
Iteration 238/1000 | Loss: 0.00001514
Iteration 239/1000 | Loss: 0.00001513
Iteration 240/1000 | Loss: 0.00001513
Iteration 241/1000 | Loss: 0.00001513
Iteration 242/1000 | Loss: 0.00001513
Iteration 243/1000 | Loss: 0.00001513
Iteration 244/1000 | Loss: 0.00001513
Iteration 245/1000 | Loss: 0.00001513
Iteration 246/1000 | Loss: 0.00001513
Iteration 247/1000 | Loss: 0.00001513
Iteration 248/1000 | Loss: 0.00001513
Iteration 249/1000 | Loss: 0.00001513
Iteration 250/1000 | Loss: 0.00001513
Iteration 251/1000 | Loss: 0.00001513
Iteration 252/1000 | Loss: 0.00001513
Iteration 253/1000 | Loss: 0.00001513
Iteration 254/1000 | Loss: 0.00001512
Iteration 255/1000 | Loss: 0.00001512
Iteration 256/1000 | Loss: 0.00001512
Iteration 257/1000 | Loss: 0.00001512
Iteration 258/1000 | Loss: 0.00001512
Iteration 259/1000 | Loss: 0.00001512
Iteration 260/1000 | Loss: 0.00001512
Iteration 261/1000 | Loss: 0.00001512
Iteration 262/1000 | Loss: 0.00001512
Iteration 263/1000 | Loss: 0.00001512
Iteration 264/1000 | Loss: 0.00001512
Iteration 265/1000 | Loss: 0.00001512
Iteration 266/1000 | Loss: 0.00001512
Iteration 267/1000 | Loss: 0.00001512
Iteration 268/1000 | Loss: 0.00001512
Iteration 269/1000 | Loss: 0.00001512
Iteration 270/1000 | Loss: 0.00001512
Iteration 271/1000 | Loss: 0.00001512
Iteration 272/1000 | Loss: 0.00001512
Iteration 273/1000 | Loss: 0.00001512
Iteration 274/1000 | Loss: 0.00001512
Iteration 275/1000 | Loss: 0.00001512
Iteration 276/1000 | Loss: 0.00001512
Iteration 277/1000 | Loss: 0.00001512
Iteration 278/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.511832761025289e-05, 1.511832761025289e-05, 1.511832761025289e-05, 1.511832761025289e-05, 1.511832761025289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.511832761025289e-05

Optimization complete. Final v2v error: 3.2544400691986084 mm

Highest mean error: 3.802027463912964 mm for frame 97

Lowest mean error: 2.5519697666168213 mm for frame 2

Saving results

Total time: 48.273404121398926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784355
Iteration 2/25 | Loss: 0.00217336
Iteration 3/25 | Loss: 0.00165930
Iteration 4/25 | Loss: 0.00159406
Iteration 5/25 | Loss: 0.00149269
Iteration 6/25 | Loss: 0.00140149
Iteration 7/25 | Loss: 0.00137680
Iteration 8/25 | Loss: 0.00136055
Iteration 9/25 | Loss: 0.00137020
Iteration 10/25 | Loss: 0.00135066
Iteration 11/25 | Loss: 0.00137384
Iteration 12/25 | Loss: 0.00134627
Iteration 13/25 | Loss: 0.00133532
Iteration 14/25 | Loss: 0.00133144
Iteration 15/25 | Loss: 0.00133029
Iteration 16/25 | Loss: 0.00132985
Iteration 17/25 | Loss: 0.00132982
Iteration 18/25 | Loss: 0.00132982
Iteration 19/25 | Loss: 0.00132982
Iteration 20/25 | Loss: 0.00132982
Iteration 21/25 | Loss: 0.00132982
Iteration 22/25 | Loss: 0.00132982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013298216508701444, 0.0013298216508701444, 0.0013298216508701444, 0.0013298216508701444, 0.0013298216508701444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013298216508701444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28594077
Iteration 2/25 | Loss: 0.00090272
Iteration 3/25 | Loss: 0.00090271
Iteration 4/25 | Loss: 0.00090271
Iteration 5/25 | Loss: 0.00090271
Iteration 6/25 | Loss: 0.00090270
Iteration 7/25 | Loss: 0.00090270
Iteration 8/25 | Loss: 0.00090270
Iteration 9/25 | Loss: 0.00090270
Iteration 10/25 | Loss: 0.00090270
Iteration 11/25 | Loss: 0.00090270
Iteration 12/25 | Loss: 0.00090270
Iteration 13/25 | Loss: 0.00090270
Iteration 14/25 | Loss: 0.00090270
Iteration 15/25 | Loss: 0.00090270
Iteration 16/25 | Loss: 0.00090270
Iteration 17/25 | Loss: 0.00090270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009027037303894758, 0.0009027037303894758, 0.0009027037303894758, 0.0009027037303894758, 0.0009027037303894758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009027037303894758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090270
Iteration 2/1000 | Loss: 0.00004472
Iteration 3/1000 | Loss: 0.00002978
Iteration 4/1000 | Loss: 0.00002749
Iteration 5/1000 | Loss: 0.00002653
Iteration 6/1000 | Loss: 0.00002594
Iteration 7/1000 | Loss: 0.00002564
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002485
Iteration 11/1000 | Loss: 0.00002467
Iteration 12/1000 | Loss: 0.00002461
Iteration 13/1000 | Loss: 0.00002460
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002451
Iteration 16/1000 | Loss: 0.00002451
Iteration 17/1000 | Loss: 0.00002450
Iteration 18/1000 | Loss: 0.00002434
Iteration 19/1000 | Loss: 0.00002434
Iteration 20/1000 | Loss: 0.00002430
Iteration 21/1000 | Loss: 0.00002429
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002429
Iteration 24/1000 | Loss: 0.00002429
Iteration 25/1000 | Loss: 0.00002428
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002427
Iteration 28/1000 | Loss: 0.00002426
Iteration 29/1000 | Loss: 0.00002426
Iteration 30/1000 | Loss: 0.00002425
Iteration 31/1000 | Loss: 0.00002425
Iteration 32/1000 | Loss: 0.00002425
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002423
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002422
Iteration 37/1000 | Loss: 0.00002422
Iteration 38/1000 | Loss: 0.00002421
Iteration 39/1000 | Loss: 0.00002421
Iteration 40/1000 | Loss: 0.00002421
Iteration 41/1000 | Loss: 0.00002421
Iteration 42/1000 | Loss: 0.00002421
Iteration 43/1000 | Loss: 0.00002421
Iteration 44/1000 | Loss: 0.00002421
Iteration 45/1000 | Loss: 0.00002421
Iteration 46/1000 | Loss: 0.00002421
Iteration 47/1000 | Loss: 0.00002420
Iteration 48/1000 | Loss: 0.00002420
Iteration 49/1000 | Loss: 0.00002420
Iteration 50/1000 | Loss: 0.00002420
Iteration 51/1000 | Loss: 0.00002420
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002420
Iteration 54/1000 | Loss: 0.00002419
Iteration 55/1000 | Loss: 0.00002419
Iteration 56/1000 | Loss: 0.00002419
Iteration 57/1000 | Loss: 0.00002419
Iteration 58/1000 | Loss: 0.00002419
Iteration 59/1000 | Loss: 0.00002419
Iteration 60/1000 | Loss: 0.00002419
Iteration 61/1000 | Loss: 0.00002419
Iteration 62/1000 | Loss: 0.00002419
Iteration 63/1000 | Loss: 0.00002419
Iteration 64/1000 | Loss: 0.00002419
Iteration 65/1000 | Loss: 0.00002419
Iteration 66/1000 | Loss: 0.00002418
Iteration 67/1000 | Loss: 0.00002418
Iteration 68/1000 | Loss: 0.00002418
Iteration 69/1000 | Loss: 0.00002418
Iteration 70/1000 | Loss: 0.00002418
Iteration 71/1000 | Loss: 0.00002418
Iteration 72/1000 | Loss: 0.00002418
Iteration 73/1000 | Loss: 0.00002418
Iteration 74/1000 | Loss: 0.00002418
Iteration 75/1000 | Loss: 0.00002418
Iteration 76/1000 | Loss: 0.00002418
Iteration 77/1000 | Loss: 0.00002418
Iteration 78/1000 | Loss: 0.00002418
Iteration 79/1000 | Loss: 0.00002418
Iteration 80/1000 | Loss: 0.00002417
Iteration 81/1000 | Loss: 0.00002417
Iteration 82/1000 | Loss: 0.00002417
Iteration 83/1000 | Loss: 0.00002417
Iteration 84/1000 | Loss: 0.00002417
Iteration 85/1000 | Loss: 0.00002417
Iteration 86/1000 | Loss: 0.00002417
Iteration 87/1000 | Loss: 0.00002417
Iteration 88/1000 | Loss: 0.00002417
Iteration 89/1000 | Loss: 0.00002417
Iteration 90/1000 | Loss: 0.00002417
Iteration 91/1000 | Loss: 0.00002416
Iteration 92/1000 | Loss: 0.00002416
Iteration 93/1000 | Loss: 0.00002416
Iteration 94/1000 | Loss: 0.00002416
Iteration 95/1000 | Loss: 0.00002416
Iteration 96/1000 | Loss: 0.00002416
Iteration 97/1000 | Loss: 0.00002416
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002415
Iteration 104/1000 | Loss: 0.00002415
Iteration 105/1000 | Loss: 0.00002415
Iteration 106/1000 | Loss: 0.00002415
Iteration 107/1000 | Loss: 0.00002415
Iteration 108/1000 | Loss: 0.00002415
Iteration 109/1000 | Loss: 0.00002415
Iteration 110/1000 | Loss: 0.00002415
Iteration 111/1000 | Loss: 0.00002415
Iteration 112/1000 | Loss: 0.00002415
Iteration 113/1000 | Loss: 0.00002415
Iteration 114/1000 | Loss: 0.00002415
Iteration 115/1000 | Loss: 0.00002415
Iteration 116/1000 | Loss: 0.00002415
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002415
Iteration 119/1000 | Loss: 0.00002415
Iteration 120/1000 | Loss: 0.00002415
Iteration 121/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.4147437216015533e-05, 2.4147437216015533e-05, 2.4147437216015533e-05, 2.4147437216015533e-05, 2.4147437216015533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4147437216015533e-05

Optimization complete. Final v2v error: 4.128835201263428 mm

Highest mean error: 4.384824275970459 mm for frame 6

Lowest mean error: 3.828418731689453 mm for frame 155

Saving results

Total time: 54.745052099227905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953537
Iteration 2/25 | Loss: 0.00220133
Iteration 3/25 | Loss: 0.00172145
Iteration 4/25 | Loss: 0.00172394
Iteration 5/25 | Loss: 0.00159332
Iteration 6/25 | Loss: 0.00154061
Iteration 7/25 | Loss: 0.00144975
Iteration 8/25 | Loss: 0.00136895
Iteration 9/25 | Loss: 0.00131966
Iteration 10/25 | Loss: 0.00130109
Iteration 11/25 | Loss: 0.00129224
Iteration 12/25 | Loss: 0.00129605
Iteration 13/25 | Loss: 0.00129146
Iteration 14/25 | Loss: 0.00127545
Iteration 15/25 | Loss: 0.00126527
Iteration 16/25 | Loss: 0.00125634
Iteration 17/25 | Loss: 0.00125046
Iteration 18/25 | Loss: 0.00124962
Iteration 19/25 | Loss: 0.00125614
Iteration 20/25 | Loss: 0.00124898
Iteration 21/25 | Loss: 0.00124739
Iteration 22/25 | Loss: 0.00124696
Iteration 23/25 | Loss: 0.00124691
Iteration 24/25 | Loss: 0.00124691
Iteration 25/25 | Loss: 0.00124691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29503739
Iteration 2/25 | Loss: 0.00113907
Iteration 3/25 | Loss: 0.00104006
Iteration 4/25 | Loss: 0.00104006
Iteration 5/25 | Loss: 0.00104006
Iteration 6/25 | Loss: 0.00104006
Iteration 7/25 | Loss: 0.00104006
Iteration 8/25 | Loss: 0.00104006
Iteration 9/25 | Loss: 0.00104006
Iteration 10/25 | Loss: 0.00104006
Iteration 11/25 | Loss: 0.00104006
Iteration 12/25 | Loss: 0.00104006
Iteration 13/25 | Loss: 0.00104006
Iteration 14/25 | Loss: 0.00104006
Iteration 15/25 | Loss: 0.00104006
Iteration 16/25 | Loss: 0.00104006
Iteration 17/25 | Loss: 0.00104006
Iteration 18/25 | Loss: 0.00104006
Iteration 19/25 | Loss: 0.00104006
Iteration 20/25 | Loss: 0.00104006
Iteration 21/25 | Loss: 0.00104006
Iteration 22/25 | Loss: 0.00104006
Iteration 23/25 | Loss: 0.00104006
Iteration 24/25 | Loss: 0.00104006
Iteration 25/25 | Loss: 0.00104006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104006
Iteration 2/1000 | Loss: 0.00007450
Iteration 3/1000 | Loss: 0.00011459
Iteration 4/1000 | Loss: 0.00043491
Iteration 5/1000 | Loss: 0.00103635
Iteration 6/1000 | Loss: 0.00300839
Iteration 7/1000 | Loss: 0.00025501
Iteration 8/1000 | Loss: 0.00109362
Iteration 9/1000 | Loss: 0.00132390
Iteration 10/1000 | Loss: 0.00004905
Iteration 11/1000 | Loss: 0.00007229
Iteration 12/1000 | Loss: 0.00004690
Iteration 13/1000 | Loss: 0.00002426
Iteration 14/1000 | Loss: 0.00003966
Iteration 15/1000 | Loss: 0.00002049
Iteration 16/1000 | Loss: 0.00007828
Iteration 17/1000 | Loss: 0.00001913
Iteration 18/1000 | Loss: 0.00001827
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00010399
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00008209
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00037387
Iteration 27/1000 | Loss: 0.00007579
Iteration 28/1000 | Loss: 0.00002222
Iteration 29/1000 | Loss: 0.00010367
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00008291
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00024425
Iteration 34/1000 | Loss: 0.00011589
Iteration 35/1000 | Loss: 0.00010313
Iteration 36/1000 | Loss: 0.00003061
Iteration 37/1000 | Loss: 0.00002030
Iteration 38/1000 | Loss: 0.00003015
Iteration 39/1000 | Loss: 0.00003391
Iteration 40/1000 | Loss: 0.00001776
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00007656
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00008805
Iteration 50/1000 | Loss: 0.00002742
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001663
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00037226
Iteration 59/1000 | Loss: 0.00004347
Iteration 60/1000 | Loss: 0.00023597
Iteration 61/1000 | Loss: 0.00013035
Iteration 62/1000 | Loss: 0.00022734
Iteration 63/1000 | Loss: 0.00004324
Iteration 64/1000 | Loss: 0.00002388
Iteration 65/1000 | Loss: 0.00017829
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001861
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00003218
Iteration 70/1000 | Loss: 0.00007545
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00010023
Iteration 75/1000 | Loss: 0.00024301
Iteration 76/1000 | Loss: 0.00002379
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00002891
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00010504
Iteration 81/1000 | Loss: 0.00009208
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00002786
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00005606
Iteration 89/1000 | Loss: 0.00003163
Iteration 90/1000 | Loss: 0.00001619
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00002384
Iteration 93/1000 | Loss: 0.00003678
Iteration 94/1000 | Loss: 0.00002306
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001537
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001514
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001506
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001504
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001498
Iteration 138/1000 | Loss: 0.00001498
Iteration 139/1000 | Loss: 0.00001498
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001497
Iteration 142/1000 | Loss: 0.00001497
Iteration 143/1000 | Loss: 0.00001497
Iteration 144/1000 | Loss: 0.00011440
Iteration 145/1000 | Loss: 0.00002442
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001491
Iteration 151/1000 | Loss: 0.00001491
Iteration 152/1000 | Loss: 0.00001490
Iteration 153/1000 | Loss: 0.00001490
Iteration 154/1000 | Loss: 0.00001490
Iteration 155/1000 | Loss: 0.00001490
Iteration 156/1000 | Loss: 0.00001489
Iteration 157/1000 | Loss: 0.00001489
Iteration 158/1000 | Loss: 0.00001489
Iteration 159/1000 | Loss: 0.00001489
Iteration 160/1000 | Loss: 0.00001489
Iteration 161/1000 | Loss: 0.00001488
Iteration 162/1000 | Loss: 0.00001488
Iteration 163/1000 | Loss: 0.00001488
Iteration 164/1000 | Loss: 0.00001488
Iteration 165/1000 | Loss: 0.00001488
Iteration 166/1000 | Loss: 0.00001488
Iteration 167/1000 | Loss: 0.00001488
Iteration 168/1000 | Loss: 0.00001488
Iteration 169/1000 | Loss: 0.00001488
Iteration 170/1000 | Loss: 0.00001488
Iteration 171/1000 | Loss: 0.00001488
Iteration 172/1000 | Loss: 0.00001488
Iteration 173/1000 | Loss: 0.00001488
Iteration 174/1000 | Loss: 0.00001488
Iteration 175/1000 | Loss: 0.00001488
Iteration 176/1000 | Loss: 0.00001488
Iteration 177/1000 | Loss: 0.00001488
Iteration 178/1000 | Loss: 0.00001488
Iteration 179/1000 | Loss: 0.00001488
Iteration 180/1000 | Loss: 0.00001488
Iteration 181/1000 | Loss: 0.00001488
Iteration 182/1000 | Loss: 0.00001488
Iteration 183/1000 | Loss: 0.00001488
Iteration 184/1000 | Loss: 0.00001488
Iteration 185/1000 | Loss: 0.00001488
Iteration 186/1000 | Loss: 0.00001488
Iteration 187/1000 | Loss: 0.00001488
Iteration 188/1000 | Loss: 0.00001488
Iteration 189/1000 | Loss: 0.00001488
Iteration 190/1000 | Loss: 0.00001488
Iteration 191/1000 | Loss: 0.00001488
Iteration 192/1000 | Loss: 0.00001488
Iteration 193/1000 | Loss: 0.00001488
Iteration 194/1000 | Loss: 0.00001488
Iteration 195/1000 | Loss: 0.00001488
Iteration 196/1000 | Loss: 0.00001488
Iteration 197/1000 | Loss: 0.00001488
Iteration 198/1000 | Loss: 0.00001488
Iteration 199/1000 | Loss: 0.00001488
Iteration 200/1000 | Loss: 0.00001488
Iteration 201/1000 | Loss: 0.00001488
Iteration 202/1000 | Loss: 0.00001488
Iteration 203/1000 | Loss: 0.00001488
Iteration 204/1000 | Loss: 0.00001488
Iteration 205/1000 | Loss: 0.00001488
Iteration 206/1000 | Loss: 0.00001488
Iteration 207/1000 | Loss: 0.00001488
Iteration 208/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.4877051398798358e-05, 1.4877051398798358e-05, 1.4877051398798358e-05, 1.4877051398798358e-05, 1.4877051398798358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4877051398798358e-05

Optimization complete. Final v2v error: 3.1744027137756348 mm

Highest mean error: 5.34417724609375 mm for frame 51

Lowest mean error: 2.811549663543701 mm for frame 33

Saving results

Total time: 174.00159192085266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037197
Iteration 2/25 | Loss: 0.01037197
Iteration 3/25 | Loss: 0.01037197
Iteration 4/25 | Loss: 0.01037197
Iteration 5/25 | Loss: 0.01037197
Iteration 6/25 | Loss: 0.01037197
Iteration 7/25 | Loss: 0.01037197
Iteration 8/25 | Loss: 0.01037197
Iteration 9/25 | Loss: 0.01037196
Iteration 10/25 | Loss: 0.01037196
Iteration 11/25 | Loss: 0.01037196
Iteration 12/25 | Loss: 0.01037196
Iteration 13/25 | Loss: 0.01037196
Iteration 14/25 | Loss: 0.01037196
Iteration 15/25 | Loss: 0.01037196
Iteration 16/25 | Loss: 0.01037196
Iteration 17/25 | Loss: 0.01037196
Iteration 18/25 | Loss: 0.01037195
Iteration 19/25 | Loss: 0.01037195
Iteration 20/25 | Loss: 0.01037195
Iteration 21/25 | Loss: 0.01037195
Iteration 22/25 | Loss: 0.01037195
Iteration 23/25 | Loss: 0.01037195
Iteration 24/25 | Loss: 0.01037195
Iteration 25/25 | Loss: 0.01037195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44581664
Iteration 2/25 | Loss: 0.10109178
Iteration 3/25 | Loss: 0.10107964
Iteration 4/25 | Loss: 0.10102528
Iteration 5/25 | Loss: 0.10102526
Iteration 6/25 | Loss: 0.10102525
Iteration 7/25 | Loss: 0.10102525
Iteration 8/25 | Loss: 0.10102525
Iteration 9/25 | Loss: 0.10102525
Iteration 10/25 | Loss: 0.10102522
Iteration 11/25 | Loss: 0.10102522
Iteration 12/25 | Loss: 0.10102522
Iteration 13/25 | Loss: 0.10102522
Iteration 14/25 | Loss: 0.10102522
Iteration 15/25 | Loss: 0.10102522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.10102522373199463, 0.10102522373199463, 0.10102522373199463, 0.10102522373199463, 0.10102522373199463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10102522373199463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10102522
Iteration 2/1000 | Loss: 0.00050034
Iteration 3/1000 | Loss: 0.00026178
Iteration 4/1000 | Loss: 0.00085854
Iteration 5/1000 | Loss: 0.00009157
Iteration 6/1000 | Loss: 0.00007382
Iteration 7/1000 | Loss: 0.00007967
Iteration 8/1000 | Loss: 0.00003101
Iteration 9/1000 | Loss: 0.00002717
Iteration 10/1000 | Loss: 0.00006526
Iteration 11/1000 | Loss: 0.00002184
Iteration 12/1000 | Loss: 0.00004879
Iteration 13/1000 | Loss: 0.00002747
Iteration 14/1000 | Loss: 0.00007080
Iteration 15/1000 | Loss: 0.00001781
Iteration 16/1000 | Loss: 0.00001669
Iteration 17/1000 | Loss: 0.00007296
Iteration 18/1000 | Loss: 0.00029628
Iteration 19/1000 | Loss: 0.00005597
Iteration 20/1000 | Loss: 0.00003721
Iteration 21/1000 | Loss: 0.00005640
Iteration 22/1000 | Loss: 0.00004263
Iteration 23/1000 | Loss: 0.00002853
Iteration 24/1000 | Loss: 0.00003768
Iteration 25/1000 | Loss: 0.00013987
Iteration 26/1000 | Loss: 0.00009015
Iteration 27/1000 | Loss: 0.00005506
Iteration 28/1000 | Loss: 0.00002121
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00005537
Iteration 31/1000 | Loss: 0.00004829
Iteration 32/1000 | Loss: 0.00003717
Iteration 33/1000 | Loss: 0.00003706
Iteration 34/1000 | Loss: 0.00006695
Iteration 35/1000 | Loss: 0.00002284
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00002600
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00003712
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00009013
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00004963
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00003184
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001086
Iteration 51/1000 | Loss: 0.00001081
Iteration 52/1000 | Loss: 0.00006079
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00004106
Iteration 55/1000 | Loss: 0.00001067
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001060
Iteration 60/1000 | Loss: 0.00001060
Iteration 61/1000 | Loss: 0.00003802
Iteration 62/1000 | Loss: 0.00007157
Iteration 63/1000 | Loss: 0.00005634
Iteration 64/1000 | Loss: 0.00001684
Iteration 65/1000 | Loss: 0.00002866
Iteration 66/1000 | Loss: 0.00001048
Iteration 67/1000 | Loss: 0.00001044
Iteration 68/1000 | Loss: 0.00001044
Iteration 69/1000 | Loss: 0.00001044
Iteration 70/1000 | Loss: 0.00001044
Iteration 71/1000 | Loss: 0.00001044
Iteration 72/1000 | Loss: 0.00001044
Iteration 73/1000 | Loss: 0.00001044
Iteration 74/1000 | Loss: 0.00001043
Iteration 75/1000 | Loss: 0.00001043
Iteration 76/1000 | Loss: 0.00001043
Iteration 77/1000 | Loss: 0.00001043
Iteration 78/1000 | Loss: 0.00001043
Iteration 79/1000 | Loss: 0.00001043
Iteration 80/1000 | Loss: 0.00001043
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001042
Iteration 88/1000 | Loss: 0.00001042
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001042
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001042
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.0415312317491043e-05, 1.0415312317491043e-05, 1.0415312317491043e-05, 1.0415312317491043e-05, 1.0415312317491043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0415312317491043e-05

Optimization complete. Final v2v error: 2.749089241027832 mm

Highest mean error: 9.308969497680664 mm for frame 67

Lowest mean error: 2.564704179763794 mm for frame 15

Saving results

Total time: 103.06870722770691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831154
Iteration 2/25 | Loss: 0.00129262
Iteration 3/25 | Loss: 0.00118228
Iteration 4/25 | Loss: 0.00113740
Iteration 5/25 | Loss: 0.00113312
Iteration 6/25 | Loss: 0.00113073
Iteration 7/25 | Loss: 0.00113023
Iteration 8/25 | Loss: 0.00113006
Iteration 9/25 | Loss: 0.00113005
Iteration 10/25 | Loss: 0.00113004
Iteration 11/25 | Loss: 0.00113004
Iteration 12/25 | Loss: 0.00113004
Iteration 13/25 | Loss: 0.00113004
Iteration 14/25 | Loss: 0.00113004
Iteration 15/25 | Loss: 0.00113004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011300449259579182, 0.0011300449259579182, 0.0011300449259579182, 0.0011300449259579182, 0.0011300449259579182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011300449259579182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63910747
Iteration 2/25 | Loss: 0.00122766
Iteration 3/25 | Loss: 0.00122766
Iteration 4/25 | Loss: 0.00122765
Iteration 5/25 | Loss: 0.00122765
Iteration 6/25 | Loss: 0.00122765
Iteration 7/25 | Loss: 0.00122765
Iteration 8/25 | Loss: 0.00122765
Iteration 9/25 | Loss: 0.00122765
Iteration 10/25 | Loss: 0.00122765
Iteration 11/25 | Loss: 0.00122755
Iteration 12/25 | Loss: 0.00122755
Iteration 13/25 | Loss: 0.00122755
Iteration 14/25 | Loss: 0.00122755
Iteration 15/25 | Loss: 0.00122755
Iteration 16/25 | Loss: 0.00122755
Iteration 17/25 | Loss: 0.00122755
Iteration 18/25 | Loss: 0.00122755
Iteration 19/25 | Loss: 0.00122755
Iteration 20/25 | Loss: 0.00122755
Iteration 21/25 | Loss: 0.00122755
Iteration 22/25 | Loss: 0.00122755
Iteration 23/25 | Loss: 0.00122755
Iteration 24/25 | Loss: 0.00122755
Iteration 25/25 | Loss: 0.00122755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122755
Iteration 2/1000 | Loss: 0.00001776
Iteration 3/1000 | Loss: 0.00001374
Iteration 4/1000 | Loss: 0.00001594
Iteration 5/1000 | Loss: 0.00001178
Iteration 6/1000 | Loss: 0.00001131
Iteration 7/1000 | Loss: 0.00001105
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00004057
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00000968
Iteration 17/1000 | Loss: 0.00000968
Iteration 18/1000 | Loss: 0.00000968
Iteration 19/1000 | Loss: 0.00000968
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000968
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000966
Iteration 25/1000 | Loss: 0.00000965
Iteration 26/1000 | Loss: 0.00000965
Iteration 27/1000 | Loss: 0.00000965
Iteration 28/1000 | Loss: 0.00000964
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00001564
Iteration 31/1000 | Loss: 0.00000953
Iteration 32/1000 | Loss: 0.00000950
Iteration 33/1000 | Loss: 0.00000949
Iteration 34/1000 | Loss: 0.00000948
Iteration 35/1000 | Loss: 0.00000948
Iteration 36/1000 | Loss: 0.00000948
Iteration 37/1000 | Loss: 0.00000947
Iteration 38/1000 | Loss: 0.00000946
Iteration 39/1000 | Loss: 0.00000986
Iteration 40/1000 | Loss: 0.00000941
Iteration 41/1000 | Loss: 0.00000941
Iteration 42/1000 | Loss: 0.00000940
Iteration 43/1000 | Loss: 0.00000940
Iteration 44/1000 | Loss: 0.00000940
Iteration 45/1000 | Loss: 0.00000939
Iteration 46/1000 | Loss: 0.00000939
Iteration 47/1000 | Loss: 0.00000939
Iteration 48/1000 | Loss: 0.00000939
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001017
Iteration 51/1000 | Loss: 0.00000931
Iteration 52/1000 | Loss: 0.00000930
Iteration 53/1000 | Loss: 0.00001016
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000929
Iteration 61/1000 | Loss: 0.00000929
Iteration 62/1000 | Loss: 0.00000929
Iteration 63/1000 | Loss: 0.00000929
Iteration 64/1000 | Loss: 0.00000929
Iteration 65/1000 | Loss: 0.00000929
Iteration 66/1000 | Loss: 0.00000929
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [9.293121365772095e-06, 9.293121365772095e-06, 9.293121365772095e-06, 9.293121365772095e-06, 9.293121365772095e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.293121365772095e-06

Optimization complete. Final v2v error: 2.6582467555999756 mm

Highest mean error: 2.8795166015625 mm for frame 43

Lowest mean error: 2.4730124473571777 mm for frame 234

Saving results

Total time: 50.21997833251953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785143
Iteration 2/25 | Loss: 0.00129746
Iteration 3/25 | Loss: 0.00120353
Iteration 4/25 | Loss: 0.00118712
Iteration 5/25 | Loss: 0.00118113
Iteration 6/25 | Loss: 0.00117969
Iteration 7/25 | Loss: 0.00117969
Iteration 8/25 | Loss: 0.00117969
Iteration 9/25 | Loss: 0.00117969
Iteration 10/25 | Loss: 0.00117969
Iteration 11/25 | Loss: 0.00117969
Iteration 12/25 | Loss: 0.00117969
Iteration 13/25 | Loss: 0.00117969
Iteration 14/25 | Loss: 0.00117969
Iteration 15/25 | Loss: 0.00117969
Iteration 16/25 | Loss: 0.00117969
Iteration 17/25 | Loss: 0.00117969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011796933831647038, 0.0011796933831647038, 0.0011796933831647038, 0.0011796933831647038, 0.0011796933831647038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011796933831647038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.10375595
Iteration 2/25 | Loss: 0.00173297
Iteration 3/25 | Loss: 0.00173282
Iteration 4/25 | Loss: 0.00173282
Iteration 5/25 | Loss: 0.00173282
Iteration 6/25 | Loss: 0.00173282
Iteration 7/25 | Loss: 0.00173282
Iteration 8/25 | Loss: 0.00173282
Iteration 9/25 | Loss: 0.00173282
Iteration 10/25 | Loss: 0.00173282
Iteration 11/25 | Loss: 0.00173282
Iteration 12/25 | Loss: 0.00173282
Iteration 13/25 | Loss: 0.00173282
Iteration 14/25 | Loss: 0.00173282
Iteration 15/25 | Loss: 0.00173282
Iteration 16/25 | Loss: 0.00173282
Iteration 17/25 | Loss: 0.00173282
Iteration 18/25 | Loss: 0.00173282
Iteration 19/25 | Loss: 0.00173282
Iteration 20/25 | Loss: 0.00173282
Iteration 21/25 | Loss: 0.00173282
Iteration 22/25 | Loss: 0.00173282
Iteration 23/25 | Loss: 0.00173282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017328188987448812, 0.0017328188987448812, 0.0017328188987448812, 0.0017328188987448812, 0.0017328188987448812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017328188987448812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173282
Iteration 2/1000 | Loss: 0.00004694
Iteration 3/1000 | Loss: 0.00003158
Iteration 4/1000 | Loss: 0.00002562
Iteration 5/1000 | Loss: 0.00002252
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001935
Iteration 8/1000 | Loss: 0.00001851
Iteration 9/1000 | Loss: 0.00001797
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001707
Iteration 14/1000 | Loss: 0.00001700
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001693
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001687
Iteration 19/1000 | Loss: 0.00001685
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001680
Iteration 23/1000 | Loss: 0.00001679
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001675
Iteration 27/1000 | Loss: 0.00001675
Iteration 28/1000 | Loss: 0.00001674
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001666
Iteration 31/1000 | Loss: 0.00001663
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001659
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001658
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001658
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001657
Iteration 59/1000 | Loss: 0.00001657
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001651
Iteration 85/1000 | Loss: 0.00001651
Iteration 86/1000 | Loss: 0.00001651
Iteration 87/1000 | Loss: 0.00001651
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001651
Iteration 90/1000 | Loss: 0.00001651
Iteration 91/1000 | Loss: 0.00001650
Iteration 92/1000 | Loss: 0.00001650
Iteration 93/1000 | Loss: 0.00001650
Iteration 94/1000 | Loss: 0.00001650
Iteration 95/1000 | Loss: 0.00001650
Iteration 96/1000 | Loss: 0.00001650
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001649
Iteration 99/1000 | Loss: 0.00001649
Iteration 100/1000 | Loss: 0.00001649
Iteration 101/1000 | Loss: 0.00001649
Iteration 102/1000 | Loss: 0.00001649
Iteration 103/1000 | Loss: 0.00001649
Iteration 104/1000 | Loss: 0.00001649
Iteration 105/1000 | Loss: 0.00001648
Iteration 106/1000 | Loss: 0.00001648
Iteration 107/1000 | Loss: 0.00001648
Iteration 108/1000 | Loss: 0.00001648
Iteration 109/1000 | Loss: 0.00001648
Iteration 110/1000 | Loss: 0.00001647
Iteration 111/1000 | Loss: 0.00001647
Iteration 112/1000 | Loss: 0.00001646
Iteration 113/1000 | Loss: 0.00001646
Iteration 114/1000 | Loss: 0.00001646
Iteration 115/1000 | Loss: 0.00001646
Iteration 116/1000 | Loss: 0.00001645
Iteration 117/1000 | Loss: 0.00001645
Iteration 118/1000 | Loss: 0.00001645
Iteration 119/1000 | Loss: 0.00001645
Iteration 120/1000 | Loss: 0.00001644
Iteration 121/1000 | Loss: 0.00001644
Iteration 122/1000 | Loss: 0.00001644
Iteration 123/1000 | Loss: 0.00001644
Iteration 124/1000 | Loss: 0.00001644
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001643
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001642
Iteration 134/1000 | Loss: 0.00001642
Iteration 135/1000 | Loss: 0.00001642
Iteration 136/1000 | Loss: 0.00001642
Iteration 137/1000 | Loss: 0.00001642
Iteration 138/1000 | Loss: 0.00001642
Iteration 139/1000 | Loss: 0.00001641
Iteration 140/1000 | Loss: 0.00001641
Iteration 141/1000 | Loss: 0.00001641
Iteration 142/1000 | Loss: 0.00001641
Iteration 143/1000 | Loss: 0.00001641
Iteration 144/1000 | Loss: 0.00001641
Iteration 145/1000 | Loss: 0.00001641
Iteration 146/1000 | Loss: 0.00001640
Iteration 147/1000 | Loss: 0.00001640
Iteration 148/1000 | Loss: 0.00001640
Iteration 149/1000 | Loss: 0.00001640
Iteration 150/1000 | Loss: 0.00001640
Iteration 151/1000 | Loss: 0.00001640
Iteration 152/1000 | Loss: 0.00001640
Iteration 153/1000 | Loss: 0.00001640
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001639
Iteration 156/1000 | Loss: 0.00001639
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001638
Iteration 159/1000 | Loss: 0.00001638
Iteration 160/1000 | Loss: 0.00001638
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001636
Iteration 166/1000 | Loss: 0.00001636
Iteration 167/1000 | Loss: 0.00001636
Iteration 168/1000 | Loss: 0.00001636
Iteration 169/1000 | Loss: 0.00001636
Iteration 170/1000 | Loss: 0.00001635
Iteration 171/1000 | Loss: 0.00001635
Iteration 172/1000 | Loss: 0.00001635
Iteration 173/1000 | Loss: 0.00001635
Iteration 174/1000 | Loss: 0.00001635
Iteration 175/1000 | Loss: 0.00001635
Iteration 176/1000 | Loss: 0.00001635
Iteration 177/1000 | Loss: 0.00001635
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001634
Iteration 180/1000 | Loss: 0.00001634
Iteration 181/1000 | Loss: 0.00001634
Iteration 182/1000 | Loss: 0.00001634
Iteration 183/1000 | Loss: 0.00001634
Iteration 184/1000 | Loss: 0.00001634
Iteration 185/1000 | Loss: 0.00001634
Iteration 186/1000 | Loss: 0.00001633
Iteration 187/1000 | Loss: 0.00001633
Iteration 188/1000 | Loss: 0.00001633
Iteration 189/1000 | Loss: 0.00001633
Iteration 190/1000 | Loss: 0.00001633
Iteration 191/1000 | Loss: 0.00001633
Iteration 192/1000 | Loss: 0.00001633
Iteration 193/1000 | Loss: 0.00001633
Iteration 194/1000 | Loss: 0.00001633
Iteration 195/1000 | Loss: 0.00001633
Iteration 196/1000 | Loss: 0.00001632
Iteration 197/1000 | Loss: 0.00001632
Iteration 198/1000 | Loss: 0.00001632
Iteration 199/1000 | Loss: 0.00001632
Iteration 200/1000 | Loss: 0.00001632
Iteration 201/1000 | Loss: 0.00001632
Iteration 202/1000 | Loss: 0.00001632
Iteration 203/1000 | Loss: 0.00001632
Iteration 204/1000 | Loss: 0.00001631
Iteration 205/1000 | Loss: 0.00001631
Iteration 206/1000 | Loss: 0.00001631
Iteration 207/1000 | Loss: 0.00001631
Iteration 208/1000 | Loss: 0.00001630
Iteration 209/1000 | Loss: 0.00001630
Iteration 210/1000 | Loss: 0.00001630
Iteration 211/1000 | Loss: 0.00001630
Iteration 212/1000 | Loss: 0.00001630
Iteration 213/1000 | Loss: 0.00001630
Iteration 214/1000 | Loss: 0.00001630
Iteration 215/1000 | Loss: 0.00001630
Iteration 216/1000 | Loss: 0.00001630
Iteration 217/1000 | Loss: 0.00001630
Iteration 218/1000 | Loss: 0.00001629
Iteration 219/1000 | Loss: 0.00001629
Iteration 220/1000 | Loss: 0.00001629
Iteration 221/1000 | Loss: 0.00001629
Iteration 222/1000 | Loss: 0.00001629
Iteration 223/1000 | Loss: 0.00001628
Iteration 224/1000 | Loss: 0.00001628
Iteration 225/1000 | Loss: 0.00001628
Iteration 226/1000 | Loss: 0.00001628
Iteration 227/1000 | Loss: 0.00001628
Iteration 228/1000 | Loss: 0.00001628
Iteration 229/1000 | Loss: 0.00001628
Iteration 230/1000 | Loss: 0.00001628
Iteration 231/1000 | Loss: 0.00001628
Iteration 232/1000 | Loss: 0.00001628
Iteration 233/1000 | Loss: 0.00001628
Iteration 234/1000 | Loss: 0.00001628
Iteration 235/1000 | Loss: 0.00001627
Iteration 236/1000 | Loss: 0.00001627
Iteration 237/1000 | Loss: 0.00001627
Iteration 238/1000 | Loss: 0.00001627
Iteration 239/1000 | Loss: 0.00001627
Iteration 240/1000 | Loss: 0.00001626
Iteration 241/1000 | Loss: 0.00001626
Iteration 242/1000 | Loss: 0.00001626
Iteration 243/1000 | Loss: 0.00001626
Iteration 244/1000 | Loss: 0.00001626
Iteration 245/1000 | Loss: 0.00001626
Iteration 246/1000 | Loss: 0.00001625
Iteration 247/1000 | Loss: 0.00001625
Iteration 248/1000 | Loss: 0.00001625
Iteration 249/1000 | Loss: 0.00001625
Iteration 250/1000 | Loss: 0.00001625
Iteration 251/1000 | Loss: 0.00001625
Iteration 252/1000 | Loss: 0.00001625
Iteration 253/1000 | Loss: 0.00001625
Iteration 254/1000 | Loss: 0.00001625
Iteration 255/1000 | Loss: 0.00001625
Iteration 256/1000 | Loss: 0.00001625
Iteration 257/1000 | Loss: 0.00001625
Iteration 258/1000 | Loss: 0.00001625
Iteration 259/1000 | Loss: 0.00001625
Iteration 260/1000 | Loss: 0.00001624
Iteration 261/1000 | Loss: 0.00001624
Iteration 262/1000 | Loss: 0.00001624
Iteration 263/1000 | Loss: 0.00001624
Iteration 264/1000 | Loss: 0.00001624
Iteration 265/1000 | Loss: 0.00001624
Iteration 266/1000 | Loss: 0.00001624
Iteration 267/1000 | Loss: 0.00001624
Iteration 268/1000 | Loss: 0.00001624
Iteration 269/1000 | Loss: 0.00001624
Iteration 270/1000 | Loss: 0.00001624
Iteration 271/1000 | Loss: 0.00001624
Iteration 272/1000 | Loss: 0.00001624
Iteration 273/1000 | Loss: 0.00001624
Iteration 274/1000 | Loss: 0.00001624
Iteration 275/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.623539901629556e-05, 1.623539901629556e-05, 1.623539901629556e-05, 1.623539901629556e-05, 1.623539901629556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.623539901629556e-05

Optimization complete. Final v2v error: 3.388713836669922 mm

Highest mean error: 5.064303398132324 mm for frame 55

Lowest mean error: 2.702270746231079 mm for frame 40

Saving results

Total time: 48.39219927787781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378997
Iteration 2/25 | Loss: 0.00129789
Iteration 3/25 | Loss: 0.00119237
Iteration 4/25 | Loss: 0.00117889
Iteration 5/25 | Loss: 0.00117425
Iteration 6/25 | Loss: 0.00117347
Iteration 7/25 | Loss: 0.00117347
Iteration 8/25 | Loss: 0.00117347
Iteration 9/25 | Loss: 0.00117347
Iteration 10/25 | Loss: 0.00117347
Iteration 11/25 | Loss: 0.00117347
Iteration 12/25 | Loss: 0.00117347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011734690051525831, 0.0011734690051525831, 0.0011734690051525831, 0.0011734690051525831, 0.0011734690051525831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011734690051525831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40036058
Iteration 2/25 | Loss: 0.00165580
Iteration 3/25 | Loss: 0.00165580
Iteration 4/25 | Loss: 0.00165580
Iteration 5/25 | Loss: 0.00165580
Iteration 6/25 | Loss: 0.00165580
Iteration 7/25 | Loss: 0.00165580
Iteration 8/25 | Loss: 0.00165580
Iteration 9/25 | Loss: 0.00165580
Iteration 10/25 | Loss: 0.00165579
Iteration 11/25 | Loss: 0.00165579
Iteration 12/25 | Loss: 0.00165579
Iteration 13/25 | Loss: 0.00165579
Iteration 14/25 | Loss: 0.00165579
Iteration 15/25 | Loss: 0.00165579
Iteration 16/25 | Loss: 0.00165579
Iteration 17/25 | Loss: 0.00165579
Iteration 18/25 | Loss: 0.00165579
Iteration 19/25 | Loss: 0.00165579
Iteration 20/25 | Loss: 0.00165579
Iteration 21/25 | Loss: 0.00165579
Iteration 22/25 | Loss: 0.00165579
Iteration 23/25 | Loss: 0.00165579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016557942144572735, 0.0016557942144572735, 0.0016557942144572735, 0.0016557942144572735, 0.0016557942144572735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016557942144572735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165579
Iteration 2/1000 | Loss: 0.00003292
Iteration 3/1000 | Loss: 0.00002124
Iteration 4/1000 | Loss: 0.00001893
Iteration 5/1000 | Loss: 0.00001778
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001597
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001486
Iteration 18/1000 | Loss: 0.00001486
Iteration 19/1000 | Loss: 0.00001485
Iteration 20/1000 | Loss: 0.00001484
Iteration 21/1000 | Loss: 0.00001484
Iteration 22/1000 | Loss: 0.00001483
Iteration 23/1000 | Loss: 0.00001483
Iteration 24/1000 | Loss: 0.00001483
Iteration 25/1000 | Loss: 0.00001483
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001477
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001467
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001461
Iteration 56/1000 | Loss: 0.00001461
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001457
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001455
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001449
Iteration 87/1000 | Loss: 0.00001448
Iteration 88/1000 | Loss: 0.00001448
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001440
Iteration 128/1000 | Loss: 0.00001440
Iteration 129/1000 | Loss: 0.00001440
Iteration 130/1000 | Loss: 0.00001440
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001440
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001440
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001440
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001440
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001440
Iteration 159/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4395324797078501e-05, 1.4395324797078501e-05, 1.4395324797078501e-05, 1.4395324797078501e-05, 1.4395324797078501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4395324797078501e-05

Optimization complete. Final v2v error: 3.208880662918091 mm

Highest mean error: 3.4940714836120605 mm for frame 39

Lowest mean error: 2.607947826385498 mm for frame 0

Saving results

Total time: 38.341307163238525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009581
Iteration 2/25 | Loss: 0.00225094
Iteration 3/25 | Loss: 0.00195398
Iteration 4/25 | Loss: 0.00181666
Iteration 5/25 | Loss: 0.00166269
Iteration 6/25 | Loss: 0.00147458
Iteration 7/25 | Loss: 0.00138447
Iteration 8/25 | Loss: 0.00137353
Iteration 9/25 | Loss: 0.00137175
Iteration 10/25 | Loss: 0.00137155
Iteration 11/25 | Loss: 0.00137155
Iteration 12/25 | Loss: 0.00137155
Iteration 13/25 | Loss: 0.00137155
Iteration 14/25 | Loss: 0.00137155
Iteration 15/25 | Loss: 0.00137155
Iteration 16/25 | Loss: 0.00137155
Iteration 17/25 | Loss: 0.00137155
Iteration 18/25 | Loss: 0.00137155
Iteration 19/25 | Loss: 0.00137155
Iteration 20/25 | Loss: 0.00137155
Iteration 21/25 | Loss: 0.00137155
Iteration 22/25 | Loss: 0.00137155
Iteration 23/25 | Loss: 0.00137155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013715458335354924, 0.0013715458335354924, 0.0013715458335354924, 0.0013715458335354924, 0.0013715458335354924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013715458335354924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28225172
Iteration 2/25 | Loss: 0.00101399
Iteration 3/25 | Loss: 0.00101399
Iteration 4/25 | Loss: 0.00101399
Iteration 5/25 | Loss: 0.00101399
Iteration 6/25 | Loss: 0.00101399
Iteration 7/25 | Loss: 0.00101399
Iteration 8/25 | Loss: 0.00101399
Iteration 9/25 | Loss: 0.00101399
Iteration 10/25 | Loss: 0.00101399
Iteration 11/25 | Loss: 0.00101399
Iteration 12/25 | Loss: 0.00101399
Iteration 13/25 | Loss: 0.00101399
Iteration 14/25 | Loss: 0.00101399
Iteration 15/25 | Loss: 0.00101399
Iteration 16/25 | Loss: 0.00101399
Iteration 17/25 | Loss: 0.00101399
Iteration 18/25 | Loss: 0.00101399
Iteration 19/25 | Loss: 0.00101399
Iteration 20/25 | Loss: 0.00101399
Iteration 21/25 | Loss: 0.00101399
Iteration 22/25 | Loss: 0.00101399
Iteration 23/25 | Loss: 0.00101399
Iteration 24/25 | Loss: 0.00101399
Iteration 25/25 | Loss: 0.00101399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101399
Iteration 2/1000 | Loss: 0.00037165
Iteration 3/1000 | Loss: 0.00105980
Iteration 4/1000 | Loss: 0.00028587
Iteration 5/1000 | Loss: 0.00018622
Iteration 6/1000 | Loss: 0.00013228
Iteration 7/1000 | Loss: 0.00010524
Iteration 8/1000 | Loss: 0.00008636
Iteration 9/1000 | Loss: 0.00006913
Iteration 10/1000 | Loss: 0.00005984
Iteration 11/1000 | Loss: 0.00005325
Iteration 12/1000 | Loss: 0.00004964
Iteration 13/1000 | Loss: 0.00004747
Iteration 14/1000 | Loss: 0.00004476
Iteration 15/1000 | Loss: 0.00004327
Iteration 16/1000 | Loss: 0.00004223
Iteration 17/1000 | Loss: 0.00004172
Iteration 18/1000 | Loss: 0.00004110
Iteration 19/1000 | Loss: 0.00004069
Iteration 20/1000 | Loss: 0.00004029
Iteration 21/1000 | Loss: 0.00003972
Iteration 22/1000 | Loss: 0.00003907
Iteration 23/1000 | Loss: 0.00003830
Iteration 24/1000 | Loss: 0.00003765
Iteration 25/1000 | Loss: 0.00003704
Iteration 26/1000 | Loss: 0.00003635
Iteration 27/1000 | Loss: 0.00003572
Iteration 28/1000 | Loss: 0.00003517
Iteration 29/1000 | Loss: 0.00030606
Iteration 30/1000 | Loss: 0.00030996
Iteration 31/1000 | Loss: 0.00024422
Iteration 32/1000 | Loss: 0.00007651
Iteration 33/1000 | Loss: 0.00005611
Iteration 34/1000 | Loss: 0.00004623
Iteration 35/1000 | Loss: 0.00004307
Iteration 36/1000 | Loss: 0.00006689
Iteration 37/1000 | Loss: 0.00005815
Iteration 38/1000 | Loss: 0.00005637
Iteration 39/1000 | Loss: 0.00005191
Iteration 40/1000 | Loss: 0.00004561
Iteration 41/1000 | Loss: 0.00004454
Iteration 42/1000 | Loss: 0.00003909
Iteration 43/1000 | Loss: 0.00003758
Iteration 44/1000 | Loss: 0.00003631
Iteration 45/1000 | Loss: 0.00003550
Iteration 46/1000 | Loss: 0.00003418
Iteration 47/1000 | Loss: 0.00003303
Iteration 48/1000 | Loss: 0.00003226
Iteration 49/1000 | Loss: 0.00003186
Iteration 50/1000 | Loss: 0.00003138
Iteration 51/1000 | Loss: 0.00003088
Iteration 52/1000 | Loss: 0.00003044
Iteration 53/1000 | Loss: 0.00003016
Iteration 54/1000 | Loss: 0.00003011
Iteration 55/1000 | Loss: 0.00003005
Iteration 56/1000 | Loss: 0.00003005
Iteration 57/1000 | Loss: 0.00003004
Iteration 58/1000 | Loss: 0.00003004
Iteration 59/1000 | Loss: 0.00003003
Iteration 60/1000 | Loss: 0.00003002
Iteration 61/1000 | Loss: 0.00003001
Iteration 62/1000 | Loss: 0.00003000
Iteration 63/1000 | Loss: 0.00003000
Iteration 64/1000 | Loss: 0.00003000
Iteration 65/1000 | Loss: 0.00003000
Iteration 66/1000 | Loss: 0.00003000
Iteration 67/1000 | Loss: 0.00002999
Iteration 68/1000 | Loss: 0.00002999
Iteration 69/1000 | Loss: 0.00002999
Iteration 70/1000 | Loss: 0.00002998
Iteration 71/1000 | Loss: 0.00002998
Iteration 72/1000 | Loss: 0.00002998
Iteration 73/1000 | Loss: 0.00002998
Iteration 74/1000 | Loss: 0.00002998
Iteration 75/1000 | Loss: 0.00002998
Iteration 76/1000 | Loss: 0.00002998
Iteration 77/1000 | Loss: 0.00002998
Iteration 78/1000 | Loss: 0.00002998
Iteration 79/1000 | Loss: 0.00002997
Iteration 80/1000 | Loss: 0.00002997
Iteration 81/1000 | Loss: 0.00002997
Iteration 82/1000 | Loss: 0.00002997
Iteration 83/1000 | Loss: 0.00002997
Iteration 84/1000 | Loss: 0.00002997
Iteration 85/1000 | Loss: 0.00002996
Iteration 86/1000 | Loss: 0.00002996
Iteration 87/1000 | Loss: 0.00002995
Iteration 88/1000 | Loss: 0.00002995
Iteration 89/1000 | Loss: 0.00002995
Iteration 90/1000 | Loss: 0.00002995
Iteration 91/1000 | Loss: 0.00002995
Iteration 92/1000 | Loss: 0.00002994
Iteration 93/1000 | Loss: 0.00002994
Iteration 94/1000 | Loss: 0.00002994
Iteration 95/1000 | Loss: 0.00002994
Iteration 96/1000 | Loss: 0.00002993
Iteration 97/1000 | Loss: 0.00002993
Iteration 98/1000 | Loss: 0.00002993
Iteration 99/1000 | Loss: 0.00002993
Iteration 100/1000 | Loss: 0.00002992
Iteration 101/1000 | Loss: 0.00002992
Iteration 102/1000 | Loss: 0.00002991
Iteration 103/1000 | Loss: 0.00002991
Iteration 104/1000 | Loss: 0.00002990
Iteration 105/1000 | Loss: 0.00002990
Iteration 106/1000 | Loss: 0.00002989
Iteration 107/1000 | Loss: 0.00002989
Iteration 108/1000 | Loss: 0.00002989
Iteration 109/1000 | Loss: 0.00002988
Iteration 110/1000 | Loss: 0.00002986
Iteration 111/1000 | Loss: 0.00002986
Iteration 112/1000 | Loss: 0.00002986
Iteration 113/1000 | Loss: 0.00002986
Iteration 114/1000 | Loss: 0.00002984
Iteration 115/1000 | Loss: 0.00002984
Iteration 116/1000 | Loss: 0.00002982
Iteration 117/1000 | Loss: 0.00002982
Iteration 118/1000 | Loss: 0.00002982
Iteration 119/1000 | Loss: 0.00002982
Iteration 120/1000 | Loss: 0.00002982
Iteration 121/1000 | Loss: 0.00002982
Iteration 122/1000 | Loss: 0.00002982
Iteration 123/1000 | Loss: 0.00002982
Iteration 124/1000 | Loss: 0.00002982
Iteration 125/1000 | Loss: 0.00002982
Iteration 126/1000 | Loss: 0.00002981
Iteration 127/1000 | Loss: 0.00002981
Iteration 128/1000 | Loss: 0.00002979
Iteration 129/1000 | Loss: 0.00002979
Iteration 130/1000 | Loss: 0.00002979
Iteration 131/1000 | Loss: 0.00002979
Iteration 132/1000 | Loss: 0.00002979
Iteration 133/1000 | Loss: 0.00002979
Iteration 134/1000 | Loss: 0.00002978
Iteration 135/1000 | Loss: 0.00002978
Iteration 136/1000 | Loss: 0.00002978
Iteration 137/1000 | Loss: 0.00002978
Iteration 138/1000 | Loss: 0.00002978
Iteration 139/1000 | Loss: 0.00002978
Iteration 140/1000 | Loss: 0.00002978
Iteration 141/1000 | Loss: 0.00002978
Iteration 142/1000 | Loss: 0.00002978
Iteration 143/1000 | Loss: 0.00002977
Iteration 144/1000 | Loss: 0.00002977
Iteration 145/1000 | Loss: 0.00002977
Iteration 146/1000 | Loss: 0.00002977
Iteration 147/1000 | Loss: 0.00002977
Iteration 148/1000 | Loss: 0.00002977
Iteration 149/1000 | Loss: 0.00002977
Iteration 150/1000 | Loss: 0.00002977
Iteration 151/1000 | Loss: 0.00002976
Iteration 152/1000 | Loss: 0.00002976
Iteration 153/1000 | Loss: 0.00002976
Iteration 154/1000 | Loss: 0.00002976
Iteration 155/1000 | Loss: 0.00002976
Iteration 156/1000 | Loss: 0.00002975
Iteration 157/1000 | Loss: 0.00002975
Iteration 158/1000 | Loss: 0.00002974
Iteration 159/1000 | Loss: 0.00002974
Iteration 160/1000 | Loss: 0.00002974
Iteration 161/1000 | Loss: 0.00002974
Iteration 162/1000 | Loss: 0.00002974
Iteration 163/1000 | Loss: 0.00002974
Iteration 164/1000 | Loss: 0.00002974
Iteration 165/1000 | Loss: 0.00002974
Iteration 166/1000 | Loss: 0.00002974
Iteration 167/1000 | Loss: 0.00002974
Iteration 168/1000 | Loss: 0.00002974
Iteration 169/1000 | Loss: 0.00002973
Iteration 170/1000 | Loss: 0.00002973
Iteration 171/1000 | Loss: 0.00002973
Iteration 172/1000 | Loss: 0.00002973
Iteration 173/1000 | Loss: 0.00002973
Iteration 174/1000 | Loss: 0.00002973
Iteration 175/1000 | Loss: 0.00002973
Iteration 176/1000 | Loss: 0.00002972
Iteration 177/1000 | Loss: 0.00002972
Iteration 178/1000 | Loss: 0.00002972
Iteration 179/1000 | Loss: 0.00002972
Iteration 180/1000 | Loss: 0.00002972
Iteration 181/1000 | Loss: 0.00002972
Iteration 182/1000 | Loss: 0.00002972
Iteration 183/1000 | Loss: 0.00002972
Iteration 184/1000 | Loss: 0.00002972
Iteration 185/1000 | Loss: 0.00002972
Iteration 186/1000 | Loss: 0.00002971
Iteration 187/1000 | Loss: 0.00002971
Iteration 188/1000 | Loss: 0.00002971
Iteration 189/1000 | Loss: 0.00002971
Iteration 190/1000 | Loss: 0.00002970
Iteration 191/1000 | Loss: 0.00002970
Iteration 192/1000 | Loss: 0.00002970
Iteration 193/1000 | Loss: 0.00002970
Iteration 194/1000 | Loss: 0.00002970
Iteration 195/1000 | Loss: 0.00002970
Iteration 196/1000 | Loss: 0.00002970
Iteration 197/1000 | Loss: 0.00002970
Iteration 198/1000 | Loss: 0.00002970
Iteration 199/1000 | Loss: 0.00002970
Iteration 200/1000 | Loss: 0.00002969
Iteration 201/1000 | Loss: 0.00002969
Iteration 202/1000 | Loss: 0.00002969
Iteration 203/1000 | Loss: 0.00002969
Iteration 204/1000 | Loss: 0.00002969
Iteration 205/1000 | Loss: 0.00002969
Iteration 206/1000 | Loss: 0.00002969
Iteration 207/1000 | Loss: 0.00002969
Iteration 208/1000 | Loss: 0.00002969
Iteration 209/1000 | Loss: 0.00002969
Iteration 210/1000 | Loss: 0.00002969
Iteration 211/1000 | Loss: 0.00002969
Iteration 212/1000 | Loss: 0.00002969
Iteration 213/1000 | Loss: 0.00002968
Iteration 214/1000 | Loss: 0.00002968
Iteration 215/1000 | Loss: 0.00002968
Iteration 216/1000 | Loss: 0.00002968
Iteration 217/1000 | Loss: 0.00002968
Iteration 218/1000 | Loss: 0.00002968
Iteration 219/1000 | Loss: 0.00002968
Iteration 220/1000 | Loss: 0.00002968
Iteration 221/1000 | Loss: 0.00002968
Iteration 222/1000 | Loss: 0.00002968
Iteration 223/1000 | Loss: 0.00002968
Iteration 224/1000 | Loss: 0.00002968
Iteration 225/1000 | Loss: 0.00002968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.9682252716156654e-05, 2.9682252716156654e-05, 2.9682252716156654e-05, 2.9682252716156654e-05, 2.9682252716156654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9682252716156654e-05

Optimization complete. Final v2v error: 4.612393379211426 mm

Highest mean error: 5.741218566894531 mm for frame 146

Lowest mean error: 4.415292263031006 mm for frame 9

Saving results

Total time: 119.41755223274231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773672
Iteration 2/25 | Loss: 0.00144119
Iteration 3/25 | Loss: 0.00123449
Iteration 4/25 | Loss: 0.00120618
Iteration 5/25 | Loss: 0.00121511
Iteration 6/25 | Loss: 0.00120135
Iteration 7/25 | Loss: 0.00119520
Iteration 8/25 | Loss: 0.00119110
Iteration 9/25 | Loss: 0.00119657
Iteration 10/25 | Loss: 0.00119512
Iteration 11/25 | Loss: 0.00119615
Iteration 12/25 | Loss: 0.00119441
Iteration 13/25 | Loss: 0.00119136
Iteration 14/25 | Loss: 0.00119283
Iteration 15/25 | Loss: 0.00119104
Iteration 16/25 | Loss: 0.00119399
Iteration 17/25 | Loss: 0.00119347
Iteration 18/25 | Loss: 0.00119245
Iteration 19/25 | Loss: 0.00118800
Iteration 20/25 | Loss: 0.00118914
Iteration 21/25 | Loss: 0.00118677
Iteration 22/25 | Loss: 0.00118584
Iteration 23/25 | Loss: 0.00118487
Iteration 24/25 | Loss: 0.00118519
Iteration 25/25 | Loss: 0.00118292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82157123
Iteration 2/25 | Loss: 0.00143826
Iteration 3/25 | Loss: 0.00142268
Iteration 4/25 | Loss: 0.00142268
Iteration 5/25 | Loss: 0.00142268
Iteration 6/25 | Loss: 0.00142268
Iteration 7/25 | Loss: 0.00142268
Iteration 8/25 | Loss: 0.00142268
Iteration 9/25 | Loss: 0.00142268
Iteration 10/25 | Loss: 0.00142268
Iteration 11/25 | Loss: 0.00142268
Iteration 12/25 | Loss: 0.00142268
Iteration 13/25 | Loss: 0.00142268
Iteration 14/25 | Loss: 0.00142268
Iteration 15/25 | Loss: 0.00142268
Iteration 16/25 | Loss: 0.00142268
Iteration 17/25 | Loss: 0.00142268
Iteration 18/25 | Loss: 0.00142268
Iteration 19/25 | Loss: 0.00142268
Iteration 20/25 | Loss: 0.00142268
Iteration 21/25 | Loss: 0.00142268
Iteration 22/25 | Loss: 0.00142268
Iteration 23/25 | Loss: 0.00142268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001422679633833468, 0.001422679633833468, 0.001422679633833468, 0.001422679633833468, 0.001422679633833468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001422679633833468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142268
Iteration 2/1000 | Loss: 0.00010688
Iteration 3/1000 | Loss: 0.00009548
Iteration 4/1000 | Loss: 0.00004181
Iteration 5/1000 | Loss: 0.00004029
Iteration 6/1000 | Loss: 0.00001723
Iteration 7/1000 | Loss: 0.00007918
Iteration 8/1000 | Loss: 0.00012033
Iteration 9/1000 | Loss: 0.00007622
Iteration 10/1000 | Loss: 0.00009763
Iteration 11/1000 | Loss: 0.00008784
Iteration 12/1000 | Loss: 0.00009310
Iteration 13/1000 | Loss: 0.00007192
Iteration 14/1000 | Loss: 0.00002304
Iteration 15/1000 | Loss: 0.00008624
Iteration 16/1000 | Loss: 0.00007695
Iteration 17/1000 | Loss: 0.00001942
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00006726
Iteration 20/1000 | Loss: 0.00008126
Iteration 21/1000 | Loss: 0.00006319
Iteration 22/1000 | Loss: 0.00010647
Iteration 23/1000 | Loss: 0.00007401
Iteration 24/1000 | Loss: 0.00013512
Iteration 25/1000 | Loss: 0.00009220
Iteration 26/1000 | Loss: 0.00007437
Iteration 27/1000 | Loss: 0.00010214
Iteration 28/1000 | Loss: 0.00008613
Iteration 29/1000 | Loss: 0.00004368
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00010552
Iteration 32/1000 | Loss: 0.00010558
Iteration 33/1000 | Loss: 0.00007908
Iteration 34/1000 | Loss: 0.00009724
Iteration 35/1000 | Loss: 0.00005943
Iteration 36/1000 | Loss: 0.00011908
Iteration 37/1000 | Loss: 0.00003633
Iteration 38/1000 | Loss: 0.00004533
Iteration 39/1000 | Loss: 0.00006799
Iteration 40/1000 | Loss: 0.00006842
Iteration 41/1000 | Loss: 0.00010885
Iteration 42/1000 | Loss: 0.00006473
Iteration 43/1000 | Loss: 0.00008592
Iteration 44/1000 | Loss: 0.00005560
Iteration 45/1000 | Loss: 0.00007506
Iteration 46/1000 | Loss: 0.00007361
Iteration 47/1000 | Loss: 0.00005966
Iteration 48/1000 | Loss: 0.00003950
Iteration 49/1000 | Loss: 0.00003598
Iteration 50/1000 | Loss: 0.00013274
Iteration 51/1000 | Loss: 0.00013263
Iteration 52/1000 | Loss: 0.00013172
Iteration 53/1000 | Loss: 0.00008372
Iteration 54/1000 | Loss: 0.00012857
Iteration 55/1000 | Loss: 0.00013881
Iteration 56/1000 | Loss: 0.00012697
Iteration 57/1000 | Loss: 0.00009216
Iteration 58/1000 | Loss: 0.00002389
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00001587
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001429
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001390
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001365
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001351
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001345
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001316
Iteration 145/1000 | Loss: 0.00001316
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001312
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001311
Iteration 159/1000 | Loss: 0.00001311
Iteration 160/1000 | Loss: 0.00001311
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001310
Iteration 167/1000 | Loss: 0.00001310
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001309
Iteration 183/1000 | Loss: 0.00001309
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001309
Iteration 186/1000 | Loss: 0.00001309
Iteration 187/1000 | Loss: 0.00001309
Iteration 188/1000 | Loss: 0.00001309
Iteration 189/1000 | Loss: 0.00001309
Iteration 190/1000 | Loss: 0.00001309
Iteration 191/1000 | Loss: 0.00001309
Iteration 192/1000 | Loss: 0.00001309
Iteration 193/1000 | Loss: 0.00001309
Iteration 194/1000 | Loss: 0.00001309
Iteration 195/1000 | Loss: 0.00001309
Iteration 196/1000 | Loss: 0.00001309
Iteration 197/1000 | Loss: 0.00001309
Iteration 198/1000 | Loss: 0.00001309
Iteration 199/1000 | Loss: 0.00001309
Iteration 200/1000 | Loss: 0.00001309
Iteration 201/1000 | Loss: 0.00001309
Iteration 202/1000 | Loss: 0.00001309
Iteration 203/1000 | Loss: 0.00001309
Iteration 204/1000 | Loss: 0.00001309
Iteration 205/1000 | Loss: 0.00001309
Iteration 206/1000 | Loss: 0.00001309
Iteration 207/1000 | Loss: 0.00001309
Iteration 208/1000 | Loss: 0.00001309
Iteration 209/1000 | Loss: 0.00001309
Iteration 210/1000 | Loss: 0.00001309
Iteration 211/1000 | Loss: 0.00001309
Iteration 212/1000 | Loss: 0.00001309
Iteration 213/1000 | Loss: 0.00001309
Iteration 214/1000 | Loss: 0.00001309
Iteration 215/1000 | Loss: 0.00001309
Iteration 216/1000 | Loss: 0.00001309
Iteration 217/1000 | Loss: 0.00001309
Iteration 218/1000 | Loss: 0.00001309
Iteration 219/1000 | Loss: 0.00001309
Iteration 220/1000 | Loss: 0.00001309
Iteration 221/1000 | Loss: 0.00001309
Iteration 222/1000 | Loss: 0.00001309
Iteration 223/1000 | Loss: 0.00001309
Iteration 224/1000 | Loss: 0.00001309
Iteration 225/1000 | Loss: 0.00001309
Iteration 226/1000 | Loss: 0.00001309
Iteration 227/1000 | Loss: 0.00001309
Iteration 228/1000 | Loss: 0.00001309
Iteration 229/1000 | Loss: 0.00001309
Iteration 230/1000 | Loss: 0.00001309
Iteration 231/1000 | Loss: 0.00001309
Iteration 232/1000 | Loss: 0.00001309
Iteration 233/1000 | Loss: 0.00001309
Iteration 234/1000 | Loss: 0.00001309
Iteration 235/1000 | Loss: 0.00001309
Iteration 236/1000 | Loss: 0.00001309
Iteration 237/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.3087541447021067e-05, 1.3087541447021067e-05, 1.3087541447021067e-05, 1.3087541447021067e-05, 1.3087541447021067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3087541447021067e-05

Optimization complete. Final v2v error: 3.0499484539031982 mm

Highest mean error: 3.881303548812866 mm for frame 39

Lowest mean error: 2.68399715423584 mm for frame 187

Saving results

Total time: 174.539080619812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892738
Iteration 2/25 | Loss: 0.00136740
Iteration 3/25 | Loss: 0.00121343
Iteration 4/25 | Loss: 0.00118885
Iteration 5/25 | Loss: 0.00118387
Iteration 6/25 | Loss: 0.00118275
Iteration 7/25 | Loss: 0.00118281
Iteration 8/25 | Loss: 0.00118198
Iteration 9/25 | Loss: 0.00118164
Iteration 10/25 | Loss: 0.00118141
Iteration 11/25 | Loss: 0.00118138
Iteration 12/25 | Loss: 0.00118137
Iteration 13/25 | Loss: 0.00118137
Iteration 14/25 | Loss: 0.00118137
Iteration 15/25 | Loss: 0.00118137
Iteration 16/25 | Loss: 0.00118137
Iteration 17/25 | Loss: 0.00118137
Iteration 18/25 | Loss: 0.00118137
Iteration 19/25 | Loss: 0.00118137
Iteration 20/25 | Loss: 0.00118137
Iteration 21/25 | Loss: 0.00118137
Iteration 22/25 | Loss: 0.00118137
Iteration 23/25 | Loss: 0.00118137
Iteration 24/25 | Loss: 0.00118137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011813684832304716, 0.0011813684832304716, 0.0011813684832304716, 0.0011813684832304716, 0.0011813684832304716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011813684832304716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29421842
Iteration 2/25 | Loss: 0.00111799
Iteration 3/25 | Loss: 0.00111797
Iteration 4/25 | Loss: 0.00111797
Iteration 5/25 | Loss: 0.00111797
Iteration 6/25 | Loss: 0.00111797
Iteration 7/25 | Loss: 0.00111797
Iteration 8/25 | Loss: 0.00111797
Iteration 9/25 | Loss: 0.00111797
Iteration 10/25 | Loss: 0.00111797
Iteration 11/25 | Loss: 0.00111797
Iteration 12/25 | Loss: 0.00111796
Iteration 13/25 | Loss: 0.00111796
Iteration 14/25 | Loss: 0.00111796
Iteration 15/25 | Loss: 0.00111796
Iteration 16/25 | Loss: 0.00111796
Iteration 17/25 | Loss: 0.00111796
Iteration 18/25 | Loss: 0.00111796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011179648572579026, 0.0011179648572579026, 0.0011179648572579026, 0.0011179648572579026, 0.0011179648572579026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011179648572579026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111796
Iteration 2/1000 | Loss: 0.00002217
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001285
Iteration 6/1000 | Loss: 0.00001234
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001140
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001103
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001099
Iteration 17/1000 | Loss: 0.00001098
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001089
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001085
Iteration 23/1000 | Loss: 0.00001084
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001077
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001076
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001065
Iteration 39/1000 | Loss: 0.00001065
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001064
Iteration 42/1000 | Loss: 0.00001064
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001061
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001060
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001059
Iteration 54/1000 | Loss: 0.00001059
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001056
Iteration 61/1000 | Loss: 0.00001055
Iteration 62/1000 | Loss: 0.00001055
Iteration 63/1000 | Loss: 0.00001054
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001053
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001050
Iteration 70/1000 | Loss: 0.00001050
Iteration 71/1000 | Loss: 0.00001050
Iteration 72/1000 | Loss: 0.00001050
Iteration 73/1000 | Loss: 0.00001049
Iteration 74/1000 | Loss: 0.00001049
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001049
Iteration 77/1000 | Loss: 0.00001049
Iteration 78/1000 | Loss: 0.00001048
Iteration 79/1000 | Loss: 0.00001048
Iteration 80/1000 | Loss: 0.00001048
Iteration 81/1000 | Loss: 0.00001048
Iteration 82/1000 | Loss: 0.00001048
Iteration 83/1000 | Loss: 0.00001048
Iteration 84/1000 | Loss: 0.00001047
Iteration 85/1000 | Loss: 0.00001047
Iteration 86/1000 | Loss: 0.00001047
Iteration 87/1000 | Loss: 0.00001047
Iteration 88/1000 | Loss: 0.00001047
Iteration 89/1000 | Loss: 0.00001047
Iteration 90/1000 | Loss: 0.00001047
Iteration 91/1000 | Loss: 0.00001047
Iteration 92/1000 | Loss: 0.00001047
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001045
Iteration 108/1000 | Loss: 0.00001045
Iteration 109/1000 | Loss: 0.00001045
Iteration 110/1000 | Loss: 0.00001045
Iteration 111/1000 | Loss: 0.00001045
Iteration 112/1000 | Loss: 0.00001045
Iteration 113/1000 | Loss: 0.00001045
Iteration 114/1000 | Loss: 0.00001044
Iteration 115/1000 | Loss: 0.00001044
Iteration 116/1000 | Loss: 0.00001044
Iteration 117/1000 | Loss: 0.00001044
Iteration 118/1000 | Loss: 0.00001044
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001043
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001042
Iteration 133/1000 | Loss: 0.00001042
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001041
Iteration 137/1000 | Loss: 0.00001041
Iteration 138/1000 | Loss: 0.00001041
Iteration 139/1000 | Loss: 0.00001040
Iteration 140/1000 | Loss: 0.00001040
Iteration 141/1000 | Loss: 0.00001040
Iteration 142/1000 | Loss: 0.00001040
Iteration 143/1000 | Loss: 0.00001040
Iteration 144/1000 | Loss: 0.00001040
Iteration 145/1000 | Loss: 0.00001040
Iteration 146/1000 | Loss: 0.00001040
Iteration 147/1000 | Loss: 0.00001040
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001040
Iteration 152/1000 | Loss: 0.00001040
Iteration 153/1000 | Loss: 0.00001040
Iteration 154/1000 | Loss: 0.00001040
Iteration 155/1000 | Loss: 0.00001040
Iteration 156/1000 | Loss: 0.00001040
Iteration 157/1000 | Loss: 0.00001040
Iteration 158/1000 | Loss: 0.00001040
Iteration 159/1000 | Loss: 0.00001040
Iteration 160/1000 | Loss: 0.00001040
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001040
Iteration 163/1000 | Loss: 0.00001040
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001040
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001040
Iteration 170/1000 | Loss: 0.00001040
Iteration 171/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.0395966455689631e-05, 1.0395966455689631e-05, 1.0395966455689631e-05, 1.0395966455689631e-05, 1.0395966455689631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0395966455689631e-05

Optimization complete. Final v2v error: 2.7709829807281494 mm

Highest mean error: 3.0233383178710938 mm for frame 74

Lowest mean error: 2.5543227195739746 mm for frame 138

Saving results

Total time: 53.01042699813843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803017
Iteration 2/25 | Loss: 0.00159263
Iteration 3/25 | Loss: 0.00129408
Iteration 4/25 | Loss: 0.00124567
Iteration 5/25 | Loss: 0.00123498
Iteration 6/25 | Loss: 0.00123296
Iteration 7/25 | Loss: 0.00123296
Iteration 8/25 | Loss: 0.00123296
Iteration 9/25 | Loss: 0.00123296
Iteration 10/25 | Loss: 0.00123296
Iteration 11/25 | Loss: 0.00123296
Iteration 12/25 | Loss: 0.00123296
Iteration 13/25 | Loss: 0.00123296
Iteration 14/25 | Loss: 0.00123296
Iteration 15/25 | Loss: 0.00123296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012329560704529285, 0.0012329560704529285, 0.0012329560704529285, 0.0012329560704529285, 0.0012329560704529285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012329560704529285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27974164
Iteration 2/25 | Loss: 0.00118712
Iteration 3/25 | Loss: 0.00118712
Iteration 4/25 | Loss: 0.00118712
Iteration 5/25 | Loss: 0.00118712
Iteration 6/25 | Loss: 0.00118712
Iteration 7/25 | Loss: 0.00118712
Iteration 8/25 | Loss: 0.00118712
Iteration 9/25 | Loss: 0.00118712
Iteration 10/25 | Loss: 0.00118712
Iteration 11/25 | Loss: 0.00118712
Iteration 12/25 | Loss: 0.00118712
Iteration 13/25 | Loss: 0.00118712
Iteration 14/25 | Loss: 0.00118712
Iteration 15/25 | Loss: 0.00118712
Iteration 16/25 | Loss: 0.00118712
Iteration 17/25 | Loss: 0.00118712
Iteration 18/25 | Loss: 0.00118712
Iteration 19/25 | Loss: 0.00118712
Iteration 20/25 | Loss: 0.00118712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011871207971125841, 0.0011871207971125841, 0.0011871207971125841, 0.0011871207971125841, 0.0011871207971125841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011871207971125841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118712
Iteration 2/1000 | Loss: 0.00003930
Iteration 3/1000 | Loss: 0.00002549
Iteration 4/1000 | Loss: 0.00002300
Iteration 5/1000 | Loss: 0.00002190
Iteration 6/1000 | Loss: 0.00002112
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001940
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001914
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001878
Iteration 22/1000 | Loss: 0.00001877
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001876
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001859
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001858
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001857
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001857
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001856
Iteration 45/1000 | Loss: 0.00001856
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001854
Iteration 54/1000 | Loss: 0.00001854
Iteration 55/1000 | Loss: 0.00001854
Iteration 56/1000 | Loss: 0.00001854
Iteration 57/1000 | Loss: 0.00001854
Iteration 58/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.8535361959948204e-05, 1.8535361959948204e-05, 1.8535361959948204e-05, 1.8535361959948204e-05, 1.8535361959948204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8535361959948204e-05

Optimization complete. Final v2v error: 3.639813184738159 mm

Highest mean error: 4.242125034332275 mm for frame 197

Lowest mean error: 3.295194149017334 mm for frame 0

Saving results

Total time: 36.60266709327698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831363
Iteration 2/25 | Loss: 0.00203053
Iteration 3/25 | Loss: 0.00144137
Iteration 4/25 | Loss: 0.00137420
Iteration 5/25 | Loss: 0.00134365
Iteration 6/25 | Loss: 0.00134728
Iteration 7/25 | Loss: 0.00133052
Iteration 8/25 | Loss: 0.00133191
Iteration 9/25 | Loss: 0.00132907
Iteration 10/25 | Loss: 0.00132637
Iteration 11/25 | Loss: 0.00132568
Iteration 12/25 | Loss: 0.00132559
Iteration 13/25 | Loss: 0.00132551
Iteration 14/25 | Loss: 0.00132549
Iteration 15/25 | Loss: 0.00132539
Iteration 16/25 | Loss: 0.00132557
Iteration 17/25 | Loss: 0.00132389
Iteration 18/25 | Loss: 0.00132327
Iteration 19/25 | Loss: 0.00132313
Iteration 20/25 | Loss: 0.00132310
Iteration 21/25 | Loss: 0.00132310
Iteration 22/25 | Loss: 0.00132310
Iteration 23/25 | Loss: 0.00132310
Iteration 24/25 | Loss: 0.00132309
Iteration 25/25 | Loss: 0.00132309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46827984
Iteration 2/25 | Loss: 0.00292699
Iteration 3/25 | Loss: 0.00292697
Iteration 4/25 | Loss: 0.00292697
Iteration 5/25 | Loss: 0.00292697
Iteration 6/25 | Loss: 0.00292696
Iteration 7/25 | Loss: 0.00292696
Iteration 8/25 | Loss: 0.00292696
Iteration 9/25 | Loss: 0.00292696
Iteration 10/25 | Loss: 0.00292696
Iteration 11/25 | Loss: 0.00292696
Iteration 12/25 | Loss: 0.00292696
Iteration 13/25 | Loss: 0.00292696
Iteration 14/25 | Loss: 0.00292696
Iteration 15/25 | Loss: 0.00292696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0029269636142998934, 0.0029269636142998934, 0.0029269636142998934, 0.0029269636142998934, 0.0029269636142998934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029269636142998934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292696
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002755
Iteration 4/1000 | Loss: 0.00002370
Iteration 5/1000 | Loss: 0.00002185
Iteration 6/1000 | Loss: 0.00002110
Iteration 7/1000 | Loss: 0.00002047
Iteration 8/1000 | Loss: 0.00001972
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00001870
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00001846
Iteration 16/1000 | Loss: 0.00001846
Iteration 17/1000 | Loss: 0.00001846
Iteration 18/1000 | Loss: 0.00001845
Iteration 19/1000 | Loss: 0.00001842
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001841
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001838
Iteration 24/1000 | Loss: 0.00001838
Iteration 25/1000 | Loss: 0.00001838
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001837
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001836
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001835
Iteration 32/1000 | Loss: 0.00001835
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001832
Iteration 45/1000 | Loss: 0.00001832
Iteration 46/1000 | Loss: 0.00001832
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001828
Iteration 66/1000 | Loss: 0.00001828
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001828
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00001827
Iteration 79/1000 | Loss: 0.00001827
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001826
Iteration 82/1000 | Loss: 0.00001826
Iteration 83/1000 | Loss: 0.00001826
Iteration 84/1000 | Loss: 0.00001826
Iteration 85/1000 | Loss: 0.00001826
Iteration 86/1000 | Loss: 0.00001826
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001825
Iteration 95/1000 | Loss: 0.00001825
Iteration 96/1000 | Loss: 0.00001825
Iteration 97/1000 | Loss: 0.00001825
Iteration 98/1000 | Loss: 0.00001825
Iteration 99/1000 | Loss: 0.00001825
Iteration 100/1000 | Loss: 0.00001825
Iteration 101/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.8253565940540284e-05, 1.8253565940540284e-05, 1.8253565940540284e-05, 1.8253565940540284e-05, 1.8253565940540284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8253565940540284e-05

Optimization complete. Final v2v error: 3.604614734649658 mm

Highest mean error: 3.957543134689331 mm for frame 8

Lowest mean error: 3.2014200687408447 mm for frame 178

Saving results

Total time: 59.14099454879761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817803
Iteration 2/25 | Loss: 0.00153303
Iteration 3/25 | Loss: 0.00130493
Iteration 4/25 | Loss: 0.00127927
Iteration 5/25 | Loss: 0.00127408
Iteration 6/25 | Loss: 0.00127299
Iteration 7/25 | Loss: 0.00127299
Iteration 8/25 | Loss: 0.00127299
Iteration 9/25 | Loss: 0.00127299
Iteration 10/25 | Loss: 0.00127299
Iteration 11/25 | Loss: 0.00127299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012729853624477983, 0.0012729853624477983, 0.0012729853624477983, 0.0012729853624477983, 0.0012729853624477983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012729853624477983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07089376
Iteration 2/25 | Loss: 0.00445213
Iteration 3/25 | Loss: 0.00445213
Iteration 4/25 | Loss: 0.00445213
Iteration 5/25 | Loss: 0.00445213
Iteration 6/25 | Loss: 0.00445213
Iteration 7/25 | Loss: 0.00445213
Iteration 8/25 | Loss: 0.00445213
Iteration 9/25 | Loss: 0.00445213
Iteration 10/25 | Loss: 0.00445213
Iteration 11/25 | Loss: 0.00445213
Iteration 12/25 | Loss: 0.00445213
Iteration 13/25 | Loss: 0.00445213
Iteration 14/25 | Loss: 0.00445213
Iteration 15/25 | Loss: 0.00445213
Iteration 16/25 | Loss: 0.00445213
Iteration 17/25 | Loss: 0.00445213
Iteration 18/25 | Loss: 0.00445213
Iteration 19/25 | Loss: 0.00445213
Iteration 20/25 | Loss: 0.00445213
Iteration 21/25 | Loss: 0.00445213
Iteration 22/25 | Loss: 0.00445213
Iteration 23/25 | Loss: 0.00445213
Iteration 24/25 | Loss: 0.00445213
Iteration 25/25 | Loss: 0.00445213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004452128894627094, 0.004452128894627094, 0.004452128894627094, 0.004452128894627094, 0.004452128894627094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004452128894627094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00445213
Iteration 2/1000 | Loss: 0.00003500
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001797
Iteration 6/1000 | Loss: 0.00001642
Iteration 7/1000 | Loss: 0.00001564
Iteration 8/1000 | Loss: 0.00001504
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001394
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001375
Iteration 14/1000 | Loss: 0.00001373
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001346
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001339
Iteration 25/1000 | Loss: 0.00001339
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001337
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001335
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001334
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001324
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001320
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001319
Iteration 76/1000 | Loss: 0.00001319
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001309
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.309317030973034e-05, 1.309317030973034e-05, 1.309317030973034e-05, 1.309317030973034e-05, 1.309317030973034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.309317030973034e-05

Optimization complete. Final v2v error: 3.0324435234069824 mm

Highest mean error: 4.254509925842285 mm for frame 0

Lowest mean error: 2.738888740539551 mm for frame 229

Saving results

Total time: 43.12737822532654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489220
Iteration 2/25 | Loss: 0.00147106
Iteration 3/25 | Loss: 0.00130462
Iteration 4/25 | Loss: 0.00129358
Iteration 5/25 | Loss: 0.00129206
Iteration 6/25 | Loss: 0.00129189
Iteration 7/25 | Loss: 0.00129189
Iteration 8/25 | Loss: 0.00129189
Iteration 9/25 | Loss: 0.00129189
Iteration 10/25 | Loss: 0.00129189
Iteration 11/25 | Loss: 0.00129189
Iteration 12/25 | Loss: 0.00129189
Iteration 13/25 | Loss: 0.00129189
Iteration 14/25 | Loss: 0.00129189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012918907450512052, 0.0012918907450512052, 0.0012918907450512052, 0.0012918907450512052, 0.0012918907450512052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012918907450512052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10884547
Iteration 2/25 | Loss: 0.00365704
Iteration 3/25 | Loss: 0.00365704
Iteration 4/25 | Loss: 0.00365704
Iteration 5/25 | Loss: 0.00365704
Iteration 6/25 | Loss: 0.00365704
Iteration 7/25 | Loss: 0.00365704
Iteration 8/25 | Loss: 0.00365704
Iteration 9/25 | Loss: 0.00365704
Iteration 10/25 | Loss: 0.00365704
Iteration 11/25 | Loss: 0.00365704
Iteration 12/25 | Loss: 0.00365704
Iteration 13/25 | Loss: 0.00365704
Iteration 14/25 | Loss: 0.00365704
Iteration 15/25 | Loss: 0.00365704
Iteration 16/25 | Loss: 0.00365704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003657035529613495, 0.003657035529613495, 0.003657035529613495, 0.003657035529613495, 0.003657035529613495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003657035529613495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00365704
Iteration 2/1000 | Loss: 0.00004130
Iteration 3/1000 | Loss: 0.00002520
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00002064
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001833
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001637
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001617
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001600
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001592
Iteration 21/1000 | Loss: 0.00001590
Iteration 22/1000 | Loss: 0.00001590
Iteration 23/1000 | Loss: 0.00001589
Iteration 24/1000 | Loss: 0.00001588
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001585
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001569
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001569
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001568
Iteration 48/1000 | Loss: 0.00001568
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001565
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001564
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001562
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001562
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001560
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001557
Iteration 123/1000 | Loss: 0.00001557
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001556
Iteration 130/1000 | Loss: 0.00001556
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001556
Iteration 133/1000 | Loss: 0.00001556
Iteration 134/1000 | Loss: 0.00001556
Iteration 135/1000 | Loss: 0.00001556
Iteration 136/1000 | Loss: 0.00001556
Iteration 137/1000 | Loss: 0.00001556
Iteration 138/1000 | Loss: 0.00001556
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001555
Iteration 142/1000 | Loss: 0.00001555
Iteration 143/1000 | Loss: 0.00001554
Iteration 144/1000 | Loss: 0.00001554
Iteration 145/1000 | Loss: 0.00001554
Iteration 146/1000 | Loss: 0.00001554
Iteration 147/1000 | Loss: 0.00001554
Iteration 148/1000 | Loss: 0.00001554
Iteration 149/1000 | Loss: 0.00001554
Iteration 150/1000 | Loss: 0.00001554
Iteration 151/1000 | Loss: 0.00001554
Iteration 152/1000 | Loss: 0.00001554
Iteration 153/1000 | Loss: 0.00001554
Iteration 154/1000 | Loss: 0.00001554
Iteration 155/1000 | Loss: 0.00001554
Iteration 156/1000 | Loss: 0.00001554
Iteration 157/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5540095773758367e-05, 1.5540095773758367e-05, 1.5540095773758367e-05, 1.5540095773758367e-05, 1.5540095773758367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5540095773758367e-05

Optimization complete. Final v2v error: 3.312196731567383 mm

Highest mean error: 4.137603282928467 mm for frame 78

Lowest mean error: 2.8041090965270996 mm for frame 45

Saving results

Total time: 38.82139182090759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899089
Iteration 2/25 | Loss: 0.00145170
Iteration 3/25 | Loss: 0.00126971
Iteration 4/25 | Loss: 0.00126091
Iteration 5/25 | Loss: 0.00125943
Iteration 6/25 | Loss: 0.00125943
Iteration 7/25 | Loss: 0.00125943
Iteration 8/25 | Loss: 0.00125943
Iteration 9/25 | Loss: 0.00125943
Iteration 10/25 | Loss: 0.00125943
Iteration 11/25 | Loss: 0.00125943
Iteration 12/25 | Loss: 0.00125943
Iteration 13/25 | Loss: 0.00125943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012594348518177867, 0.0012594348518177867, 0.0012594348518177867, 0.0012594348518177867, 0.0012594348518177867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012594348518177867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15502632
Iteration 2/25 | Loss: 0.00336981
Iteration 3/25 | Loss: 0.00336981
Iteration 4/25 | Loss: 0.00336981
Iteration 5/25 | Loss: 0.00336981
Iteration 6/25 | Loss: 0.00336981
Iteration 7/25 | Loss: 0.00336981
Iteration 8/25 | Loss: 0.00336981
Iteration 9/25 | Loss: 0.00336981
Iteration 10/25 | Loss: 0.00336981
Iteration 11/25 | Loss: 0.00336981
Iteration 12/25 | Loss: 0.00336981
Iteration 13/25 | Loss: 0.00336981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0033698061015456915, 0.0033698061015456915, 0.0033698061015456915, 0.0033698061015456915, 0.0033698061015456915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033698061015456915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336981
Iteration 2/1000 | Loss: 0.00003555
Iteration 3/1000 | Loss: 0.00002672
Iteration 4/1000 | Loss: 0.00002466
Iteration 5/1000 | Loss: 0.00002302
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00001998
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001946
Iteration 12/1000 | Loss: 0.00001944
Iteration 13/1000 | Loss: 0.00001944
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001931
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001911
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001908
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001907
Iteration 28/1000 | Loss: 0.00001907
Iteration 29/1000 | Loss: 0.00001906
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001906
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001905
Iteration 37/1000 | Loss: 0.00001905
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001904
Iteration 43/1000 | Loss: 0.00001904
Iteration 44/1000 | Loss: 0.00001904
Iteration 45/1000 | Loss: 0.00001904
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001903
Iteration 52/1000 | Loss: 0.00001903
Iteration 53/1000 | Loss: 0.00001902
Iteration 54/1000 | Loss: 0.00001902
Iteration 55/1000 | Loss: 0.00001901
Iteration 56/1000 | Loss: 0.00001901
Iteration 57/1000 | Loss: 0.00001901
Iteration 58/1000 | Loss: 0.00001901
Iteration 59/1000 | Loss: 0.00001901
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001901
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001901
Iteration 67/1000 | Loss: 0.00001901
Iteration 68/1000 | Loss: 0.00001901
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.901354335132055e-05, 1.901354335132055e-05, 1.901354335132055e-05, 1.901354335132055e-05, 1.901354335132055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.901354335132055e-05

Optimization complete. Final v2v error: 3.3912999629974365 mm

Highest mean error: 4.357281684875488 mm for frame 197

Lowest mean error: 2.813451051712036 mm for frame 156

Saving results

Total time: 33.69688606262207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480438
Iteration 2/25 | Loss: 0.00139022
Iteration 3/25 | Loss: 0.00129700
Iteration 4/25 | Loss: 0.00127730
Iteration 5/25 | Loss: 0.00127170
Iteration 6/25 | Loss: 0.00126998
Iteration 7/25 | Loss: 0.00126998
Iteration 8/25 | Loss: 0.00126998
Iteration 9/25 | Loss: 0.00126998
Iteration 10/25 | Loss: 0.00126998
Iteration 11/25 | Loss: 0.00126998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012699832441285253, 0.0012699832441285253, 0.0012699832441285253, 0.0012699832441285253, 0.0012699832441285253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012699832441285253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11081851
Iteration 2/25 | Loss: 0.00373042
Iteration 3/25 | Loss: 0.00373040
Iteration 4/25 | Loss: 0.00373040
Iteration 5/25 | Loss: 0.00373040
Iteration 6/25 | Loss: 0.00373040
Iteration 7/25 | Loss: 0.00373040
Iteration 8/25 | Loss: 0.00373040
Iteration 9/25 | Loss: 0.00373040
Iteration 10/25 | Loss: 0.00373040
Iteration 11/25 | Loss: 0.00373040
Iteration 12/25 | Loss: 0.00373040
Iteration 13/25 | Loss: 0.00373040
Iteration 14/25 | Loss: 0.00373040
Iteration 15/25 | Loss: 0.00373040
Iteration 16/25 | Loss: 0.00373040
Iteration 17/25 | Loss: 0.00373040
Iteration 18/25 | Loss: 0.00373040
Iteration 19/25 | Loss: 0.00373040
Iteration 20/25 | Loss: 0.00373040
Iteration 21/25 | Loss: 0.00373040
Iteration 22/25 | Loss: 0.00373040
Iteration 23/25 | Loss: 0.00373040
Iteration 24/25 | Loss: 0.00373040
Iteration 25/25 | Loss: 0.00373040

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00373040
Iteration 2/1000 | Loss: 0.00004303
Iteration 3/1000 | Loss: 0.00002589
Iteration 4/1000 | Loss: 0.00002266
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00001945
Iteration 7/1000 | Loss: 0.00001871
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001733
Iteration 11/1000 | Loss: 0.00001704
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001661
Iteration 20/1000 | Loss: 0.00001658
Iteration 21/1000 | Loss: 0.00001658
Iteration 22/1000 | Loss: 0.00001658
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001657
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001653
Iteration 40/1000 | Loss: 0.00001653
Iteration 41/1000 | Loss: 0.00001653
Iteration 42/1000 | Loss: 0.00001653
Iteration 43/1000 | Loss: 0.00001653
Iteration 44/1000 | Loss: 0.00001652
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001644
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001643
Iteration 81/1000 | Loss: 0.00001643
Iteration 82/1000 | Loss: 0.00001643
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.6403004337917082e-05, 1.6403004337917082e-05, 1.6403004337917082e-05, 1.6403004337917082e-05, 1.6403004337917082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6403004337917082e-05

Optimization complete. Final v2v error: 3.4544126987457275 mm

Highest mean error: 3.984386920928955 mm for frame 177

Lowest mean error: 2.844006299972534 mm for frame 239

Saving results

Total time: 39.59101915359497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887280
Iteration 2/25 | Loss: 0.00132021
Iteration 3/25 | Loss: 0.00123213
Iteration 4/25 | Loss: 0.00122460
Iteration 5/25 | Loss: 0.00122215
Iteration 6/25 | Loss: 0.00122139
Iteration 7/25 | Loss: 0.00122139
Iteration 8/25 | Loss: 0.00122139
Iteration 9/25 | Loss: 0.00122136
Iteration 10/25 | Loss: 0.00122136
Iteration 11/25 | Loss: 0.00122136
Iteration 12/25 | Loss: 0.00122136
Iteration 13/25 | Loss: 0.00122136
Iteration 14/25 | Loss: 0.00122136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012213579611852765, 0.0012213579611852765, 0.0012213579611852765, 0.0012213579611852765, 0.0012213579611852765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012213579611852765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19368410
Iteration 2/25 | Loss: 0.00390874
Iteration 3/25 | Loss: 0.00390874
Iteration 4/25 | Loss: 0.00390874
Iteration 5/25 | Loss: 0.00390874
Iteration 6/25 | Loss: 0.00390874
Iteration 7/25 | Loss: 0.00390874
Iteration 8/25 | Loss: 0.00390874
Iteration 9/25 | Loss: 0.00390874
Iteration 10/25 | Loss: 0.00390874
Iteration 11/25 | Loss: 0.00390874
Iteration 12/25 | Loss: 0.00390874
Iteration 13/25 | Loss: 0.00390874
Iteration 14/25 | Loss: 0.00390874
Iteration 15/25 | Loss: 0.00390874
Iteration 16/25 | Loss: 0.00390874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003908735234290361, 0.003908735234290361, 0.003908735234290361, 0.003908735234290361, 0.003908735234290361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003908735234290361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390874
Iteration 2/1000 | Loss: 0.00002834
Iteration 3/1000 | Loss: 0.00001867
Iteration 4/1000 | Loss: 0.00001653
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001225
Iteration 65/1000 | Loss: 0.00001225
Iteration 66/1000 | Loss: 0.00001225
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001225
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001225
Iteration 72/1000 | Loss: 0.00001225
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.224554125656141e-05, 1.224554125656141e-05, 1.224554125656141e-05, 1.224554125656141e-05, 1.224554125656141e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.224554125656141e-05

Optimization complete. Final v2v error: 2.9074864387512207 mm

Highest mean error: 3.059180736541748 mm for frame 91

Lowest mean error: 2.7525863647460938 mm for frame 108

Saving results

Total time: 29.386643648147583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886546
Iteration 2/25 | Loss: 0.00205703
Iteration 3/25 | Loss: 0.00148878
Iteration 4/25 | Loss: 0.00142467
Iteration 5/25 | Loss: 0.00142065
Iteration 6/25 | Loss: 0.00142063
Iteration 7/25 | Loss: 0.00142063
Iteration 8/25 | Loss: 0.00142063
Iteration 9/25 | Loss: 0.00142063
Iteration 10/25 | Loss: 0.00142063
Iteration 11/25 | Loss: 0.00142063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014206265332177281, 0.0014206265332177281, 0.0014206265332177281, 0.0014206265332177281, 0.0014206265332177281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014206265332177281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93715334
Iteration 2/25 | Loss: 0.00256927
Iteration 3/25 | Loss: 0.00256924
Iteration 4/25 | Loss: 0.00256924
Iteration 5/25 | Loss: 0.00256924
Iteration 6/25 | Loss: 0.00256924
Iteration 7/25 | Loss: 0.00256924
Iteration 8/25 | Loss: 0.00256924
Iteration 9/25 | Loss: 0.00256924
Iteration 10/25 | Loss: 0.00256923
Iteration 11/25 | Loss: 0.00256923
Iteration 12/25 | Loss: 0.00256923
Iteration 13/25 | Loss: 0.00256923
Iteration 14/25 | Loss: 0.00256923
Iteration 15/25 | Loss: 0.00256923
Iteration 16/25 | Loss: 0.00256923
Iteration 17/25 | Loss: 0.00256923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00256923446431756, 0.00256923446431756, 0.00256923446431756, 0.00256923446431756, 0.00256923446431756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00256923446431756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256923
Iteration 2/1000 | Loss: 0.00004892
Iteration 3/1000 | Loss: 0.00003243
Iteration 4/1000 | Loss: 0.00002959
Iteration 5/1000 | Loss: 0.00002779
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002591
Iteration 8/1000 | Loss: 0.00002522
Iteration 9/1000 | Loss: 0.00002460
Iteration 10/1000 | Loss: 0.00002417
Iteration 11/1000 | Loss: 0.00002396
Iteration 12/1000 | Loss: 0.00002386
Iteration 13/1000 | Loss: 0.00002382
Iteration 14/1000 | Loss: 0.00002381
Iteration 15/1000 | Loss: 0.00002380
Iteration 16/1000 | Loss: 0.00002376
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002374
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002363
Iteration 21/1000 | Loss: 0.00002362
Iteration 22/1000 | Loss: 0.00002362
Iteration 23/1000 | Loss: 0.00002361
Iteration 24/1000 | Loss: 0.00002360
Iteration 25/1000 | Loss: 0.00002358
Iteration 26/1000 | Loss: 0.00002357
Iteration 27/1000 | Loss: 0.00002357
Iteration 28/1000 | Loss: 0.00002355
Iteration 29/1000 | Loss: 0.00002354
Iteration 30/1000 | Loss: 0.00002354
Iteration 31/1000 | Loss: 0.00002353
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002350
Iteration 39/1000 | Loss: 0.00002349
Iteration 40/1000 | Loss: 0.00002349
Iteration 41/1000 | Loss: 0.00002349
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002348
Iteration 44/1000 | Loss: 0.00002348
Iteration 45/1000 | Loss: 0.00002347
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00002347
Iteration 48/1000 | Loss: 0.00002347
Iteration 49/1000 | Loss: 0.00002347
Iteration 50/1000 | Loss: 0.00002347
Iteration 51/1000 | Loss: 0.00002347
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002346
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002346
Iteration 62/1000 | Loss: 0.00002346
Iteration 63/1000 | Loss: 0.00002346
Iteration 64/1000 | Loss: 0.00002346
Iteration 65/1000 | Loss: 0.00002345
Iteration 66/1000 | Loss: 0.00002345
Iteration 67/1000 | Loss: 0.00002345
Iteration 68/1000 | Loss: 0.00002345
Iteration 69/1000 | Loss: 0.00002345
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002344
Iteration 76/1000 | Loss: 0.00002344
Iteration 77/1000 | Loss: 0.00002344
Iteration 78/1000 | Loss: 0.00002344
Iteration 79/1000 | Loss: 0.00002343
Iteration 80/1000 | Loss: 0.00002343
Iteration 81/1000 | Loss: 0.00002343
Iteration 82/1000 | Loss: 0.00002343
Iteration 83/1000 | Loss: 0.00002343
Iteration 84/1000 | Loss: 0.00002343
Iteration 85/1000 | Loss: 0.00002343
Iteration 86/1000 | Loss: 0.00002343
Iteration 87/1000 | Loss: 0.00002343
Iteration 88/1000 | Loss: 0.00002343
Iteration 89/1000 | Loss: 0.00002343
Iteration 90/1000 | Loss: 0.00002343
Iteration 91/1000 | Loss: 0.00002343
Iteration 92/1000 | Loss: 0.00002343
Iteration 93/1000 | Loss: 0.00002343
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002343
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00002343
Iteration 105/1000 | Loss: 0.00002343
Iteration 106/1000 | Loss: 0.00002343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.342529478482902e-05, 2.342529478482902e-05, 2.342529478482902e-05, 2.342529478482902e-05, 2.342529478482902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.342529478482902e-05

Optimization complete. Final v2v error: 4.059045791625977 mm

Highest mean error: 4.35851526260376 mm for frame 1

Lowest mean error: 3.837831974029541 mm for frame 199

Saving results

Total time: 36.927939891815186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392330
Iteration 2/25 | Loss: 0.00168212
Iteration 3/25 | Loss: 0.00136228
Iteration 4/25 | Loss: 0.00132626
Iteration 5/25 | Loss: 0.00131893
Iteration 6/25 | Loss: 0.00131744
Iteration 7/25 | Loss: 0.00131744
Iteration 8/25 | Loss: 0.00131744
Iteration 9/25 | Loss: 0.00131744
Iteration 10/25 | Loss: 0.00131744
Iteration 11/25 | Loss: 0.00131744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013174358755350113, 0.0013174358755350113, 0.0013174358755350113, 0.0013174358755350113, 0.0013174358755350113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013174358755350113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03350413
Iteration 2/25 | Loss: 0.00415777
Iteration 3/25 | Loss: 0.00415777
Iteration 4/25 | Loss: 0.00415777
Iteration 5/25 | Loss: 0.00415777
Iteration 6/25 | Loss: 0.00415777
Iteration 7/25 | Loss: 0.00415777
Iteration 8/25 | Loss: 0.00415777
Iteration 9/25 | Loss: 0.00415777
Iteration 10/25 | Loss: 0.00415777
Iteration 11/25 | Loss: 0.00415777
Iteration 12/25 | Loss: 0.00415777
Iteration 13/25 | Loss: 0.00415777
Iteration 14/25 | Loss: 0.00415777
Iteration 15/25 | Loss: 0.00415777
Iteration 16/25 | Loss: 0.00415777
Iteration 17/25 | Loss: 0.00415777
Iteration 18/25 | Loss: 0.00415777
Iteration 19/25 | Loss: 0.00415777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00415776576846838, 0.00415776576846838, 0.00415776576846838, 0.00415776576846838, 0.00415776576846838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00415776576846838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00415777
Iteration 2/1000 | Loss: 0.00004126
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001501
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001445
Iteration 13/1000 | Loss: 0.00001445
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001439
Iteration 18/1000 | Loss: 0.00001439
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001410
Iteration 47/1000 | Loss: 0.00001410
Iteration 48/1000 | Loss: 0.00001410
Iteration 49/1000 | Loss: 0.00001408
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001406
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001405
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001400
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001399
Iteration 86/1000 | Loss: 0.00001399
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001397
Iteration 101/1000 | Loss: 0.00001397
Iteration 102/1000 | Loss: 0.00001397
Iteration 103/1000 | Loss: 0.00001397
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001396
Iteration 110/1000 | Loss: 0.00001396
Iteration 111/1000 | Loss: 0.00001396
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001396
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001395
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001394
Iteration 130/1000 | Loss: 0.00001394
Iteration 131/1000 | Loss: 0.00001394
Iteration 132/1000 | Loss: 0.00001394
Iteration 133/1000 | Loss: 0.00001394
Iteration 134/1000 | Loss: 0.00001394
Iteration 135/1000 | Loss: 0.00001393
Iteration 136/1000 | Loss: 0.00001393
Iteration 137/1000 | Loss: 0.00001393
Iteration 138/1000 | Loss: 0.00001393
Iteration 139/1000 | Loss: 0.00001393
Iteration 140/1000 | Loss: 0.00001393
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001393
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001392
Iteration 147/1000 | Loss: 0.00001392
Iteration 148/1000 | Loss: 0.00001392
Iteration 149/1000 | Loss: 0.00001392
Iteration 150/1000 | Loss: 0.00001392
Iteration 151/1000 | Loss: 0.00001392
Iteration 152/1000 | Loss: 0.00001392
Iteration 153/1000 | Loss: 0.00001392
Iteration 154/1000 | Loss: 0.00001392
Iteration 155/1000 | Loss: 0.00001392
Iteration 156/1000 | Loss: 0.00001392
Iteration 157/1000 | Loss: 0.00001392
Iteration 158/1000 | Loss: 0.00001392
Iteration 159/1000 | Loss: 0.00001392
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001391
Iteration 162/1000 | Loss: 0.00001391
Iteration 163/1000 | Loss: 0.00001391
Iteration 164/1000 | Loss: 0.00001391
Iteration 165/1000 | Loss: 0.00001391
Iteration 166/1000 | Loss: 0.00001391
Iteration 167/1000 | Loss: 0.00001391
Iteration 168/1000 | Loss: 0.00001391
Iteration 169/1000 | Loss: 0.00001391
Iteration 170/1000 | Loss: 0.00001390
Iteration 171/1000 | Loss: 0.00001390
Iteration 172/1000 | Loss: 0.00001390
Iteration 173/1000 | Loss: 0.00001390
Iteration 174/1000 | Loss: 0.00001390
Iteration 175/1000 | Loss: 0.00001390
Iteration 176/1000 | Loss: 0.00001390
Iteration 177/1000 | Loss: 0.00001390
Iteration 178/1000 | Loss: 0.00001390
Iteration 179/1000 | Loss: 0.00001389
Iteration 180/1000 | Loss: 0.00001389
Iteration 181/1000 | Loss: 0.00001389
Iteration 182/1000 | Loss: 0.00001389
Iteration 183/1000 | Loss: 0.00001389
Iteration 184/1000 | Loss: 0.00001389
Iteration 185/1000 | Loss: 0.00001389
Iteration 186/1000 | Loss: 0.00001389
Iteration 187/1000 | Loss: 0.00001389
Iteration 188/1000 | Loss: 0.00001389
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001388
Iteration 193/1000 | Loss: 0.00001388
Iteration 194/1000 | Loss: 0.00001388
Iteration 195/1000 | Loss: 0.00001388
Iteration 196/1000 | Loss: 0.00001388
Iteration 197/1000 | Loss: 0.00001388
Iteration 198/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.387882002745755e-05, 1.387882002745755e-05, 1.387882002745755e-05, 1.387882002745755e-05, 1.387882002745755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.387882002745755e-05

Optimization complete. Final v2v error: 3.180656909942627 mm

Highest mean error: 3.4912962913513184 mm for frame 47

Lowest mean error: 2.8655893802642822 mm for frame 5

Saving results

Total time: 38.70240092277527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038650
Iteration 2/25 | Loss: 0.00218389
Iteration 3/25 | Loss: 0.00180509
Iteration 4/25 | Loss: 0.00172625
Iteration 5/25 | Loss: 0.00174031
Iteration 6/25 | Loss: 0.00170255
Iteration 7/25 | Loss: 0.00166945
Iteration 8/25 | Loss: 0.00165022
Iteration 9/25 | Loss: 0.00164807
Iteration 10/25 | Loss: 0.00163576
Iteration 11/25 | Loss: 0.00163061
Iteration 12/25 | Loss: 0.00162885
Iteration 13/25 | Loss: 0.00164002
Iteration 14/25 | Loss: 0.00163299
Iteration 15/25 | Loss: 0.00163110
Iteration 16/25 | Loss: 0.00162291
Iteration 17/25 | Loss: 0.00163085
Iteration 18/25 | Loss: 0.00163565
Iteration 19/25 | Loss: 0.00162911
Iteration 20/25 | Loss: 0.00163038
Iteration 21/25 | Loss: 0.00164995
Iteration 22/25 | Loss: 0.00164834
Iteration 23/25 | Loss: 0.00163937
Iteration 24/25 | Loss: 0.00162833
Iteration 25/25 | Loss: 0.00162774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08106983
Iteration 2/25 | Loss: 0.00774655
Iteration 3/25 | Loss: 0.00738763
Iteration 4/25 | Loss: 0.00738763
Iteration 5/25 | Loss: 0.00738763
Iteration 6/25 | Loss: 0.00738763
Iteration 7/25 | Loss: 0.00738763
Iteration 8/25 | Loss: 0.00738763
Iteration 9/25 | Loss: 0.00738763
Iteration 10/25 | Loss: 0.00738763
Iteration 11/25 | Loss: 0.00738763
Iteration 12/25 | Loss: 0.00738763
Iteration 13/25 | Loss: 0.00738763
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.007387625053524971, 0.007387625053524971, 0.007387625053524971, 0.007387625053524971, 0.007387625053524971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007387625053524971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00738763
Iteration 2/1000 | Loss: 0.00073889
Iteration 3/1000 | Loss: 0.00049539
Iteration 4/1000 | Loss: 0.00040520
Iteration 5/1000 | Loss: 0.00041673
Iteration 6/1000 | Loss: 0.00029450
Iteration 7/1000 | Loss: 0.00032077
Iteration 8/1000 | Loss: 0.00030387
Iteration 9/1000 | Loss: 0.00024500
Iteration 10/1000 | Loss: 0.00056550
Iteration 11/1000 | Loss: 0.00077704
Iteration 12/1000 | Loss: 0.00080651
Iteration 13/1000 | Loss: 0.00030450
Iteration 14/1000 | Loss: 0.00020048
Iteration 15/1000 | Loss: 0.00019802
Iteration 16/1000 | Loss: 0.00020500
Iteration 17/1000 | Loss: 0.00018528
Iteration 18/1000 | Loss: 0.00019056
Iteration 19/1000 | Loss: 0.00017469
Iteration 20/1000 | Loss: 0.00055181
Iteration 21/1000 | Loss: 0.00563419
Iteration 22/1000 | Loss: 0.00860277
Iteration 23/1000 | Loss: 0.00109739
Iteration 24/1000 | Loss: 0.00085581
Iteration 25/1000 | Loss: 0.00055149
Iteration 26/1000 | Loss: 0.00068252
Iteration 27/1000 | Loss: 0.00036507
Iteration 28/1000 | Loss: 0.00083903
Iteration 29/1000 | Loss: 0.00038690
Iteration 30/1000 | Loss: 0.00064533
Iteration 31/1000 | Loss: 0.00089357
Iteration 32/1000 | Loss: 0.00021726
Iteration 33/1000 | Loss: 0.00045245
Iteration 34/1000 | Loss: 0.00022187
Iteration 35/1000 | Loss: 0.00007502
Iteration 36/1000 | Loss: 0.00011260
Iteration 37/1000 | Loss: 0.00006747
Iteration 38/1000 | Loss: 0.00013808
Iteration 39/1000 | Loss: 0.00023320
Iteration 40/1000 | Loss: 0.00015208
Iteration 41/1000 | Loss: 0.00020502
Iteration 42/1000 | Loss: 0.00023828
Iteration 43/1000 | Loss: 0.00025045
Iteration 44/1000 | Loss: 0.00019328
Iteration 45/1000 | Loss: 0.00011295
Iteration 46/1000 | Loss: 0.00004436
Iteration 47/1000 | Loss: 0.00008445
Iteration 48/1000 | Loss: 0.00004673
Iteration 49/1000 | Loss: 0.00003128
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00002641
Iteration 52/1000 | Loss: 0.00002461
Iteration 53/1000 | Loss: 0.00002339
Iteration 54/1000 | Loss: 0.00003717
Iteration 55/1000 | Loss: 0.00011726
Iteration 56/1000 | Loss: 0.00020768
Iteration 57/1000 | Loss: 0.00003246
Iteration 58/1000 | Loss: 0.00006560
Iteration 59/1000 | Loss: 0.00007655
Iteration 60/1000 | Loss: 0.00002825
Iteration 61/1000 | Loss: 0.00002477
Iteration 62/1000 | Loss: 0.00002394
Iteration 63/1000 | Loss: 0.00002320
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002199
Iteration 66/1000 | Loss: 0.00002153
Iteration 67/1000 | Loss: 0.00002117
Iteration 68/1000 | Loss: 0.00002089
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002042
Iteration 73/1000 | Loss: 0.00002041
Iteration 74/1000 | Loss: 0.00002040
Iteration 75/1000 | Loss: 0.00002039
Iteration 76/1000 | Loss: 0.00002038
Iteration 77/1000 | Loss: 0.00002037
Iteration 78/1000 | Loss: 0.00002037
Iteration 79/1000 | Loss: 0.00002037
Iteration 80/1000 | Loss: 0.00002036
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002033
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002031
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002022
Iteration 89/1000 | Loss: 0.00002013
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001938
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00012207
Iteration 94/1000 | Loss: 0.00021137
Iteration 95/1000 | Loss: 0.00003344
Iteration 96/1000 | Loss: 0.00022489
Iteration 97/1000 | Loss: 0.00022242
Iteration 98/1000 | Loss: 0.00002491
Iteration 99/1000 | Loss: 0.00017440
Iteration 100/1000 | Loss: 0.00015269
Iteration 101/1000 | Loss: 0.00016236
Iteration 102/1000 | Loss: 0.00013617
Iteration 103/1000 | Loss: 0.00011443
Iteration 104/1000 | Loss: 0.00013732
Iteration 105/1000 | Loss: 0.00006654
Iteration 106/1000 | Loss: 0.00012048
Iteration 107/1000 | Loss: 0.00007493
Iteration 108/1000 | Loss: 0.00007630
Iteration 109/1000 | Loss: 0.00001931
Iteration 110/1000 | Loss: 0.00001806
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001686
Iteration 150/1000 | Loss: 0.00001686
Iteration 151/1000 | Loss: 0.00001686
Iteration 152/1000 | Loss: 0.00001686
Iteration 153/1000 | Loss: 0.00001686
Iteration 154/1000 | Loss: 0.00001686
Iteration 155/1000 | Loss: 0.00001686
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001686
Iteration 161/1000 | Loss: 0.00001686
Iteration 162/1000 | Loss: 0.00001686
Iteration 163/1000 | Loss: 0.00001686
Iteration 164/1000 | Loss: 0.00001686
Iteration 165/1000 | Loss: 0.00001686
Iteration 166/1000 | Loss: 0.00001686
Iteration 167/1000 | Loss: 0.00001686
Iteration 168/1000 | Loss: 0.00001686
Iteration 169/1000 | Loss: 0.00001686
Iteration 170/1000 | Loss: 0.00001686
Iteration 171/1000 | Loss: 0.00001686
Iteration 172/1000 | Loss: 0.00001686
Iteration 173/1000 | Loss: 0.00001686
Iteration 174/1000 | Loss: 0.00001686
Iteration 175/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.6856100046425126e-05, 1.6856100046425126e-05, 1.6856100046425126e-05, 1.6856100046425126e-05, 1.6856100046425126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6856100046425126e-05

Optimization complete. Final v2v error: 3.4558465480804443 mm

Highest mean error: 5.95598030090332 mm for frame 1

Lowest mean error: 3.1911582946777344 mm for frame 159

Saving results

Total time: 187.67657375335693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995433
Iteration 2/25 | Loss: 0.00311044
Iteration 3/25 | Loss: 0.00235070
Iteration 4/25 | Loss: 0.00213768
Iteration 5/25 | Loss: 0.00194484
Iteration 6/25 | Loss: 0.00193726
Iteration 7/25 | Loss: 0.00183390
Iteration 8/25 | Loss: 0.00182967
Iteration 9/25 | Loss: 0.00178198
Iteration 10/25 | Loss: 0.00173899
Iteration 11/25 | Loss: 0.00172042
Iteration 12/25 | Loss: 0.00171169
Iteration 13/25 | Loss: 0.00170276
Iteration 14/25 | Loss: 0.00168598
Iteration 15/25 | Loss: 0.00168235
Iteration 16/25 | Loss: 0.00167850
Iteration 17/25 | Loss: 0.00167640
Iteration 18/25 | Loss: 0.00167253
Iteration 19/25 | Loss: 0.00166847
Iteration 20/25 | Loss: 0.00166671
Iteration 21/25 | Loss: 0.00166586
Iteration 22/25 | Loss: 0.00166529
Iteration 23/25 | Loss: 0.00166701
Iteration 24/25 | Loss: 0.00166471
Iteration 25/25 | Loss: 0.00166360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01591384
Iteration 2/25 | Loss: 0.01349227
Iteration 3/25 | Loss: 0.00982138
Iteration 4/25 | Loss: 0.00982137
Iteration 5/25 | Loss: 0.00982136
Iteration 6/25 | Loss: 0.00982136
Iteration 7/25 | Loss: 0.00982136
Iteration 8/25 | Loss: 0.00982136
Iteration 9/25 | Loss: 0.00982136
Iteration 10/25 | Loss: 0.00982136
Iteration 11/25 | Loss: 0.00982136
Iteration 12/25 | Loss: 0.00982136
Iteration 13/25 | Loss: 0.00982136
Iteration 14/25 | Loss: 0.00982136
Iteration 15/25 | Loss: 0.00982136
Iteration 16/25 | Loss: 0.00982136
Iteration 17/25 | Loss: 0.00982136
Iteration 18/25 | Loss: 0.00982136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.009821358136832714, 0.009821358136832714, 0.009821358136832714, 0.009821358136832714, 0.009821358136832714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009821358136832714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00982136
Iteration 2/1000 | Loss: 0.00400059
Iteration 3/1000 | Loss: 0.00088823
Iteration 4/1000 | Loss: 0.00060996
Iteration 5/1000 | Loss: 0.00062109
Iteration 6/1000 | Loss: 0.00039030
Iteration 7/1000 | Loss: 0.00042863
Iteration 8/1000 | Loss: 0.00031958
Iteration 9/1000 | Loss: 0.00029798
Iteration 10/1000 | Loss: 0.00037323
Iteration 11/1000 | Loss: 0.00029846
Iteration 12/1000 | Loss: 0.00026771
Iteration 13/1000 | Loss: 0.00028489
Iteration 14/1000 | Loss: 0.00025400
Iteration 15/1000 | Loss: 0.00024980
Iteration 16/1000 | Loss: 0.00024661
Iteration 17/1000 | Loss: 0.00024436
Iteration 18/1000 | Loss: 0.00024204
Iteration 19/1000 | Loss: 0.00024040
Iteration 20/1000 | Loss: 0.00023891
Iteration 21/1000 | Loss: 0.00023740
Iteration 22/1000 | Loss: 0.00023573
Iteration 23/1000 | Loss: 0.00026966
Iteration 24/1000 | Loss: 0.00023433
Iteration 25/1000 | Loss: 0.00023243
Iteration 26/1000 | Loss: 0.00046487
Iteration 27/1000 | Loss: 0.00063604
Iteration 28/1000 | Loss: 0.00037281
Iteration 29/1000 | Loss: 0.00025335
Iteration 30/1000 | Loss: 0.00024089
Iteration 31/1000 | Loss: 0.00023110
Iteration 32/1000 | Loss: 0.00022743
Iteration 33/1000 | Loss: 0.00024341
Iteration 34/1000 | Loss: 0.00022613
Iteration 35/1000 | Loss: 0.00022333
Iteration 36/1000 | Loss: 0.00041415
Iteration 37/1000 | Loss: 0.00022491
Iteration 38/1000 | Loss: 0.00022212
Iteration 39/1000 | Loss: 0.00022032
Iteration 40/1000 | Loss: 0.00021920
Iteration 41/1000 | Loss: 0.00021852
Iteration 42/1000 | Loss: 0.00021791
Iteration 43/1000 | Loss: 0.00021728
Iteration 44/1000 | Loss: 0.00021661
Iteration 45/1000 | Loss: 0.00021584
Iteration 46/1000 | Loss: 0.00021537
Iteration 47/1000 | Loss: 0.00029190
Iteration 48/1000 | Loss: 0.00022030
Iteration 49/1000 | Loss: 0.00021652
Iteration 50/1000 | Loss: 0.00021517
Iteration 51/1000 | Loss: 0.00021404
Iteration 52/1000 | Loss: 0.00021350
Iteration 53/1000 | Loss: 0.00021304
Iteration 54/1000 | Loss: 0.00021273
Iteration 55/1000 | Loss: 0.00021253
Iteration 56/1000 | Loss: 0.00021229
Iteration 57/1000 | Loss: 0.00021214
Iteration 58/1000 | Loss: 0.00021212
Iteration 59/1000 | Loss: 0.00021199
Iteration 60/1000 | Loss: 0.00021196
Iteration 61/1000 | Loss: 0.00021187
Iteration 62/1000 | Loss: 0.00021170
Iteration 63/1000 | Loss: 0.00021156
Iteration 64/1000 | Loss: 0.00021155
Iteration 65/1000 | Loss: 0.00021151
Iteration 66/1000 | Loss: 0.00021151
Iteration 67/1000 | Loss: 0.00021138
Iteration 68/1000 | Loss: 0.00021137
Iteration 69/1000 | Loss: 0.00021132
Iteration 70/1000 | Loss: 0.00021131
Iteration 71/1000 | Loss: 0.00021131
Iteration 72/1000 | Loss: 0.00021130
Iteration 73/1000 | Loss: 0.00021129
Iteration 74/1000 | Loss: 0.00021129
Iteration 75/1000 | Loss: 0.00021128
Iteration 76/1000 | Loss: 0.00021128
Iteration 77/1000 | Loss: 0.00021128
Iteration 78/1000 | Loss: 0.00021127
Iteration 79/1000 | Loss: 0.00021123
Iteration 80/1000 | Loss: 0.00021123
Iteration 81/1000 | Loss: 0.00021122
Iteration 82/1000 | Loss: 0.00021121
Iteration 83/1000 | Loss: 0.00021121
Iteration 84/1000 | Loss: 0.00021121
Iteration 85/1000 | Loss: 0.00021121
Iteration 86/1000 | Loss: 0.00021120
Iteration 87/1000 | Loss: 0.00021120
Iteration 88/1000 | Loss: 0.00021120
Iteration 89/1000 | Loss: 0.00021120
Iteration 90/1000 | Loss: 0.00021119
Iteration 91/1000 | Loss: 0.00021119
Iteration 92/1000 | Loss: 0.00021118
Iteration 93/1000 | Loss: 0.00021117
Iteration 94/1000 | Loss: 0.00021117
Iteration 95/1000 | Loss: 0.00021116
Iteration 96/1000 | Loss: 0.00021116
Iteration 97/1000 | Loss: 0.00021116
Iteration 98/1000 | Loss: 0.00021116
Iteration 99/1000 | Loss: 0.00021115
Iteration 100/1000 | Loss: 0.00021115
Iteration 101/1000 | Loss: 0.00021115
Iteration 102/1000 | Loss: 0.00021115
Iteration 103/1000 | Loss: 0.00021115
Iteration 104/1000 | Loss: 0.00021115
Iteration 105/1000 | Loss: 0.00021115
Iteration 106/1000 | Loss: 0.00021115
Iteration 107/1000 | Loss: 0.00021114
Iteration 108/1000 | Loss: 0.00021114
Iteration 109/1000 | Loss: 0.00021114
Iteration 110/1000 | Loss: 0.00021114
Iteration 111/1000 | Loss: 0.00021114
Iteration 112/1000 | Loss: 0.00021114
Iteration 113/1000 | Loss: 0.00021114
Iteration 114/1000 | Loss: 0.00021114
Iteration 115/1000 | Loss: 0.00021114
Iteration 116/1000 | Loss: 0.00021114
Iteration 117/1000 | Loss: 0.00021114
Iteration 118/1000 | Loss: 0.00021114
Iteration 119/1000 | Loss: 0.00021113
Iteration 120/1000 | Loss: 0.00021113
Iteration 121/1000 | Loss: 0.00021113
Iteration 122/1000 | Loss: 0.00021113
Iteration 123/1000 | Loss: 0.00021113
Iteration 124/1000 | Loss: 0.00021113
Iteration 125/1000 | Loss: 0.00021113
Iteration 126/1000 | Loss: 0.00021113
Iteration 127/1000 | Loss: 0.00021113
Iteration 128/1000 | Loss: 0.00021113
Iteration 129/1000 | Loss: 0.00021113
Iteration 130/1000 | Loss: 0.00021112
Iteration 131/1000 | Loss: 0.00021112
Iteration 132/1000 | Loss: 0.00021112
Iteration 133/1000 | Loss: 0.00021112
Iteration 134/1000 | Loss: 0.00021112
Iteration 135/1000 | Loss: 0.00021111
Iteration 136/1000 | Loss: 0.00021111
Iteration 137/1000 | Loss: 0.00021111
Iteration 138/1000 | Loss: 0.00021111
Iteration 139/1000 | Loss: 0.00021111
Iteration 140/1000 | Loss: 0.00021111
Iteration 141/1000 | Loss: 0.00021110
Iteration 142/1000 | Loss: 0.00021110
Iteration 143/1000 | Loss: 0.00021110
Iteration 144/1000 | Loss: 0.00021110
Iteration 145/1000 | Loss: 0.00021110
Iteration 146/1000 | Loss: 0.00021110
Iteration 147/1000 | Loss: 0.00021110
Iteration 148/1000 | Loss: 0.00021110
Iteration 149/1000 | Loss: 0.00021110
Iteration 150/1000 | Loss: 0.00021110
Iteration 151/1000 | Loss: 0.00021110
Iteration 152/1000 | Loss: 0.00021110
Iteration 153/1000 | Loss: 0.00021110
Iteration 154/1000 | Loss: 0.00021110
Iteration 155/1000 | Loss: 0.00021110
Iteration 156/1000 | Loss: 0.00021110
Iteration 157/1000 | Loss: 0.00021110
Iteration 158/1000 | Loss: 0.00021110
Iteration 159/1000 | Loss: 0.00021110
Iteration 160/1000 | Loss: 0.00021110
Iteration 161/1000 | Loss: 0.00021110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [0.00021109567023813725, 0.00021109567023813725, 0.00021109567023813725, 0.00021109567023813725, 0.00021109567023813725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021109567023813725

Optimization complete. Final v2v error: 7.666438579559326 mm

Highest mean error: 21.74542808532715 mm for frame 77

Lowest mean error: 4.100656509399414 mm for frame 97

Saving results

Total time: 162.8017294406891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455302
Iteration 2/25 | Loss: 0.00138107
Iteration 3/25 | Loss: 0.00128209
Iteration 4/25 | Loss: 0.00126666
Iteration 5/25 | Loss: 0.00126171
Iteration 6/25 | Loss: 0.00125958
Iteration 7/25 | Loss: 0.00125929
Iteration 8/25 | Loss: 0.00125929
Iteration 9/25 | Loss: 0.00125929
Iteration 10/25 | Loss: 0.00125929
Iteration 11/25 | Loss: 0.00125929
Iteration 12/25 | Loss: 0.00125929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012592870043590665, 0.0012592870043590665, 0.0012592870043590665, 0.0012592870043590665, 0.0012592870043590665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012592870043590665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25736809
Iteration 2/25 | Loss: 0.00343598
Iteration 3/25 | Loss: 0.00343597
Iteration 4/25 | Loss: 0.00343597
Iteration 5/25 | Loss: 0.00343597
Iteration 6/25 | Loss: 0.00343597
Iteration 7/25 | Loss: 0.00343597
Iteration 8/25 | Loss: 0.00343597
Iteration 9/25 | Loss: 0.00343597
Iteration 10/25 | Loss: 0.00343597
Iteration 11/25 | Loss: 0.00343597
Iteration 12/25 | Loss: 0.00343597
Iteration 13/25 | Loss: 0.00343597
Iteration 14/25 | Loss: 0.00343597
Iteration 15/25 | Loss: 0.00343597
Iteration 16/25 | Loss: 0.00343597
Iteration 17/25 | Loss: 0.00343597
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003435966558754444, 0.003435966558754444, 0.003435966558754444, 0.003435966558754444, 0.003435966558754444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003435966558754444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00343597
Iteration 2/1000 | Loss: 0.00003959
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001763
Iteration 5/1000 | Loss: 0.00001651
Iteration 6/1000 | Loss: 0.00001567
Iteration 7/1000 | Loss: 0.00001518
Iteration 8/1000 | Loss: 0.00001471
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001392
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001368
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001356
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001354
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001353
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001353
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001352
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001351
Iteration 57/1000 | Loss: 0.00001351
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001350
Iteration 60/1000 | Loss: 0.00001350
Iteration 61/1000 | Loss: 0.00001350
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001348
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001347
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001346
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.3461986782203894e-05, 1.3461986782203894e-05, 1.3461986782203894e-05, 1.3461986782203894e-05, 1.3461986782203894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3461986782203894e-05

Optimization complete. Final v2v error: 3.119321346282959 mm

Highest mean error: 3.5112953186035156 mm for frame 28

Lowest mean error: 2.729921817779541 mm for frame 13

Saving results

Total time: 35.44547390937805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880235
Iteration 2/25 | Loss: 0.00165738
Iteration 3/25 | Loss: 0.00131578
Iteration 4/25 | Loss: 0.00127418
Iteration 5/25 | Loss: 0.00126857
Iteration 6/25 | Loss: 0.00126735
Iteration 7/25 | Loss: 0.00126735
Iteration 8/25 | Loss: 0.00126735
Iteration 9/25 | Loss: 0.00126735
Iteration 10/25 | Loss: 0.00126735
Iteration 11/25 | Loss: 0.00126735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012673529563471675, 0.0012673529563471675, 0.0012673529563471675, 0.0012673529563471675, 0.0012673529563471675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012673529563471675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05344129
Iteration 2/25 | Loss: 0.00403697
Iteration 3/25 | Loss: 0.00403696
Iteration 4/25 | Loss: 0.00403696
Iteration 5/25 | Loss: 0.00403696
Iteration 6/25 | Loss: 0.00403696
Iteration 7/25 | Loss: 0.00403696
Iteration 8/25 | Loss: 0.00403696
Iteration 9/25 | Loss: 0.00403696
Iteration 10/25 | Loss: 0.00403696
Iteration 11/25 | Loss: 0.00403696
Iteration 12/25 | Loss: 0.00403696
Iteration 13/25 | Loss: 0.00403696
Iteration 14/25 | Loss: 0.00403696
Iteration 15/25 | Loss: 0.00403696
Iteration 16/25 | Loss: 0.00403696
Iteration 17/25 | Loss: 0.00403696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004036960657685995, 0.004036960657685995, 0.004036960657685995, 0.004036960657685995, 0.004036960657685995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004036960657685995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00403696
Iteration 2/1000 | Loss: 0.00002700
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001669
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001409
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001312
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001249
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001229
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001226
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001222
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001218
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.2184050319774542e-05, 1.2184050319774542e-05, 1.2184050319774542e-05, 1.2184050319774542e-05, 1.2184050319774542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2184050319774542e-05

Optimization complete. Final v2v error: 2.9693057537078857 mm

Highest mean error: 3.180908679962158 mm for frame 55

Lowest mean error: 2.7316360473632812 mm for frame 1

Saving results

Total time: 30.536497592926025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909503
Iteration 2/25 | Loss: 0.00360416
Iteration 3/25 | Loss: 0.00243856
Iteration 4/25 | Loss: 0.00180356
Iteration 5/25 | Loss: 0.00180392
Iteration 6/25 | Loss: 0.00167160
Iteration 7/25 | Loss: 0.00166240
Iteration 8/25 | Loss: 0.00153964
Iteration 9/25 | Loss: 0.00152027
Iteration 10/25 | Loss: 0.00151071
Iteration 11/25 | Loss: 0.00150714
Iteration 12/25 | Loss: 0.00150790
Iteration 13/25 | Loss: 0.00150623
Iteration 14/25 | Loss: 0.00149975
Iteration 15/25 | Loss: 0.00147775
Iteration 16/25 | Loss: 0.00147636
Iteration 17/25 | Loss: 0.00147600
Iteration 18/25 | Loss: 0.00147533
Iteration 19/25 | Loss: 0.00147474
Iteration 20/25 | Loss: 0.00147415
Iteration 21/25 | Loss: 0.00147374
Iteration 22/25 | Loss: 0.00147333
Iteration 23/25 | Loss: 0.00147289
Iteration 24/25 | Loss: 0.00147249
Iteration 25/25 | Loss: 0.00147224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.10407066
Iteration 2/25 | Loss: 0.00635122
Iteration 3/25 | Loss: 0.00579966
Iteration 4/25 | Loss: 0.00579965
Iteration 5/25 | Loss: 0.00579965
Iteration 6/25 | Loss: 0.00579965
Iteration 7/25 | Loss: 0.00579965
Iteration 8/25 | Loss: 0.00579965
Iteration 9/25 | Loss: 0.00579965
Iteration 10/25 | Loss: 0.00579965
Iteration 11/25 | Loss: 0.00579965
Iteration 12/25 | Loss: 0.00579965
Iteration 13/25 | Loss: 0.00579965
Iteration 14/25 | Loss: 0.00579965
Iteration 15/25 | Loss: 0.00579965
Iteration 16/25 | Loss: 0.00579965
Iteration 17/25 | Loss: 0.00579965
Iteration 18/25 | Loss: 0.00579965
Iteration 19/25 | Loss: 0.00579965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005799645557999611, 0.005799645557999611, 0.005799645557999611, 0.005799645557999611, 0.005799645557999611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005799645557999611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00579965
Iteration 2/1000 | Loss: 0.00197225
Iteration 3/1000 | Loss: 0.00181138
Iteration 4/1000 | Loss: 0.00316162
Iteration 5/1000 | Loss: 0.00275649
Iteration 6/1000 | Loss: 0.00134140
Iteration 7/1000 | Loss: 0.00094465
Iteration 8/1000 | Loss: 0.00120140
Iteration 9/1000 | Loss: 0.00120121
Iteration 10/1000 | Loss: 0.00215547
Iteration 11/1000 | Loss: 0.00334139
Iteration 12/1000 | Loss: 0.00065079
Iteration 13/1000 | Loss: 0.00113099
Iteration 14/1000 | Loss: 0.00042000
Iteration 15/1000 | Loss: 0.00045349
Iteration 16/1000 | Loss: 0.00085758
Iteration 17/1000 | Loss: 0.00056533
Iteration 18/1000 | Loss: 0.00124069
Iteration 19/1000 | Loss: 0.00092493
Iteration 20/1000 | Loss: 0.00066011
Iteration 21/1000 | Loss: 0.00041503
Iteration 22/1000 | Loss: 0.00018464
Iteration 23/1000 | Loss: 0.00038311
Iteration 24/1000 | Loss: 0.00019228
Iteration 25/1000 | Loss: 0.00041069
Iteration 26/1000 | Loss: 0.00007875
Iteration 27/1000 | Loss: 0.00021058
Iteration 28/1000 | Loss: 0.00236357
Iteration 29/1000 | Loss: 0.00198638
Iteration 30/1000 | Loss: 0.00175294
Iteration 31/1000 | Loss: 0.00056046
Iteration 32/1000 | Loss: 0.00422522
Iteration 33/1000 | Loss: 0.00115819
Iteration 34/1000 | Loss: 0.00201249
Iteration 35/1000 | Loss: 0.00103396
Iteration 36/1000 | Loss: 0.00200315
Iteration 37/1000 | Loss: 0.00098153
Iteration 38/1000 | Loss: 0.00106615
Iteration 39/1000 | Loss: 0.00113976
Iteration 40/1000 | Loss: 0.00216186
Iteration 41/1000 | Loss: 0.00228949
Iteration 42/1000 | Loss: 0.00101593
Iteration 43/1000 | Loss: 0.00071534
Iteration 44/1000 | Loss: 0.00100053
Iteration 45/1000 | Loss: 0.00065753
Iteration 46/1000 | Loss: 0.00013254
Iteration 47/1000 | Loss: 0.00118758
Iteration 48/1000 | Loss: 0.00067357
Iteration 49/1000 | Loss: 0.00120903
Iteration 50/1000 | Loss: 0.00149811
Iteration 51/1000 | Loss: 0.00145279
Iteration 52/1000 | Loss: 0.00163249
Iteration 53/1000 | Loss: 0.00092302
Iteration 54/1000 | Loss: 0.00045542
Iteration 55/1000 | Loss: 0.00036492
Iteration 56/1000 | Loss: 0.00029905
Iteration 57/1000 | Loss: 0.00080700
Iteration 58/1000 | Loss: 0.00103977
Iteration 59/1000 | Loss: 0.00149148
Iteration 60/1000 | Loss: 0.00098210
Iteration 61/1000 | Loss: 0.00108691
Iteration 62/1000 | Loss: 0.00139912
Iteration 63/1000 | Loss: 0.00144781
Iteration 64/1000 | Loss: 0.00072786
Iteration 65/1000 | Loss: 0.00079216
Iteration 66/1000 | Loss: 0.00104358
Iteration 67/1000 | Loss: 0.00073216
Iteration 68/1000 | Loss: 0.00077974
Iteration 69/1000 | Loss: 0.00046327
Iteration 70/1000 | Loss: 0.00040615
Iteration 71/1000 | Loss: 0.00040823
Iteration 72/1000 | Loss: 0.00088221
Iteration 73/1000 | Loss: 0.00042980
Iteration 74/1000 | Loss: 0.00129909
Iteration 75/1000 | Loss: 0.00041580
Iteration 76/1000 | Loss: 0.00082381
Iteration 77/1000 | Loss: 0.00045484
Iteration 78/1000 | Loss: 0.00053518
Iteration 79/1000 | Loss: 0.00070032
Iteration 80/1000 | Loss: 0.00112234
Iteration 81/1000 | Loss: 0.00097527
Iteration 82/1000 | Loss: 0.00057961
Iteration 83/1000 | Loss: 0.00066332
Iteration 84/1000 | Loss: 0.00028869
Iteration 85/1000 | Loss: 0.00049550
Iteration 86/1000 | Loss: 0.00016171
Iteration 87/1000 | Loss: 0.00024468
Iteration 88/1000 | Loss: 0.00032617
Iteration 89/1000 | Loss: 0.00033560
Iteration 90/1000 | Loss: 0.00038621
Iteration 91/1000 | Loss: 0.00033589
Iteration 92/1000 | Loss: 0.00015774
Iteration 93/1000 | Loss: 0.00025846
Iteration 94/1000 | Loss: 0.00042347
Iteration 95/1000 | Loss: 0.00020461
Iteration 96/1000 | Loss: 0.00028498
Iteration 97/1000 | Loss: 0.00023714
Iteration 98/1000 | Loss: 0.00006787
Iteration 99/1000 | Loss: 0.00055280
Iteration 100/1000 | Loss: 0.00067419
Iteration 101/1000 | Loss: 0.00046165
Iteration 102/1000 | Loss: 0.00030078
Iteration 103/1000 | Loss: 0.00005372
Iteration 104/1000 | Loss: 0.00028546
Iteration 105/1000 | Loss: 0.00004161
Iteration 106/1000 | Loss: 0.00020405
Iteration 107/1000 | Loss: 0.00015953
Iteration 108/1000 | Loss: 0.00022111
Iteration 109/1000 | Loss: 0.00016670
Iteration 110/1000 | Loss: 0.00014489
Iteration 111/1000 | Loss: 0.00003212
Iteration 112/1000 | Loss: 0.00002996
Iteration 113/1000 | Loss: 0.00002858
Iteration 114/1000 | Loss: 0.00002770
Iteration 115/1000 | Loss: 0.00029532
Iteration 116/1000 | Loss: 0.00019606
Iteration 117/1000 | Loss: 0.00043465
Iteration 118/1000 | Loss: 0.00031934
Iteration 119/1000 | Loss: 0.00033696
Iteration 120/1000 | Loss: 0.00016723
Iteration 121/1000 | Loss: 0.00012289
Iteration 122/1000 | Loss: 0.00002723
Iteration 123/1000 | Loss: 0.00002643
Iteration 124/1000 | Loss: 0.00002614
Iteration 125/1000 | Loss: 0.00002590
Iteration 126/1000 | Loss: 0.00002580
Iteration 127/1000 | Loss: 0.00028920
Iteration 128/1000 | Loss: 0.00019236
Iteration 129/1000 | Loss: 0.00128156
Iteration 130/1000 | Loss: 0.00034940
Iteration 131/1000 | Loss: 0.00033992
Iteration 132/1000 | Loss: 0.00022162
Iteration 133/1000 | Loss: 0.00053990
Iteration 134/1000 | Loss: 0.00025823
Iteration 135/1000 | Loss: 0.00045646
Iteration 136/1000 | Loss: 0.00016255
Iteration 137/1000 | Loss: 0.00018331
Iteration 138/1000 | Loss: 0.00093543
Iteration 139/1000 | Loss: 0.00058140
Iteration 140/1000 | Loss: 0.00048866
Iteration 141/1000 | Loss: 0.00035606
Iteration 142/1000 | Loss: 0.00005462
Iteration 143/1000 | Loss: 0.00003636
Iteration 144/1000 | Loss: 0.00006638
Iteration 145/1000 | Loss: 0.00024698
Iteration 146/1000 | Loss: 0.00036973
Iteration 147/1000 | Loss: 0.00052233
Iteration 148/1000 | Loss: 0.00043823
Iteration 149/1000 | Loss: 0.00034725
Iteration 150/1000 | Loss: 0.00027086
Iteration 151/1000 | Loss: 0.00033257
Iteration 152/1000 | Loss: 0.00057043
Iteration 153/1000 | Loss: 0.00045777
Iteration 154/1000 | Loss: 0.00024745
Iteration 155/1000 | Loss: 0.00019304
Iteration 156/1000 | Loss: 0.00034525
Iteration 157/1000 | Loss: 0.00042554
Iteration 158/1000 | Loss: 0.00059451
Iteration 159/1000 | Loss: 0.00026167
Iteration 160/1000 | Loss: 0.00044369
Iteration 161/1000 | Loss: 0.00053984
Iteration 162/1000 | Loss: 0.00007623
Iteration 163/1000 | Loss: 0.00043550
Iteration 164/1000 | Loss: 0.00041083
Iteration 165/1000 | Loss: 0.00055827
Iteration 166/1000 | Loss: 0.00085176
Iteration 167/1000 | Loss: 0.00045482
Iteration 168/1000 | Loss: 0.00063795
Iteration 169/1000 | Loss: 0.00088101
Iteration 170/1000 | Loss: 0.00088532
Iteration 171/1000 | Loss: 0.00079183
Iteration 172/1000 | Loss: 0.00027732
Iteration 173/1000 | Loss: 0.00011160
Iteration 174/1000 | Loss: 0.00005452
Iteration 175/1000 | Loss: 0.00025997
Iteration 176/1000 | Loss: 0.00005190
Iteration 177/1000 | Loss: 0.00022864
Iteration 178/1000 | Loss: 0.00002944
Iteration 179/1000 | Loss: 0.00018945
Iteration 180/1000 | Loss: 0.00010719
Iteration 181/1000 | Loss: 0.00031713
Iteration 182/1000 | Loss: 0.00027099
Iteration 183/1000 | Loss: 0.00013081
Iteration 184/1000 | Loss: 0.00012929
Iteration 185/1000 | Loss: 0.00018144
Iteration 186/1000 | Loss: 0.00014311
Iteration 187/1000 | Loss: 0.00029337
Iteration 188/1000 | Loss: 0.00009020
Iteration 189/1000 | Loss: 0.00036008
Iteration 190/1000 | Loss: 0.00040598
Iteration 191/1000 | Loss: 0.00049730
Iteration 192/1000 | Loss: 0.00048553
Iteration 193/1000 | Loss: 0.00059186
Iteration 194/1000 | Loss: 0.00034699
Iteration 195/1000 | Loss: 0.00043632
Iteration 196/1000 | Loss: 0.00046175
Iteration 197/1000 | Loss: 0.00037470
Iteration 198/1000 | Loss: 0.00039152
Iteration 199/1000 | Loss: 0.00011070
Iteration 200/1000 | Loss: 0.00010780
Iteration 201/1000 | Loss: 0.00003245
Iteration 202/1000 | Loss: 0.00002988
Iteration 203/1000 | Loss: 0.00002254
Iteration 204/1000 | Loss: 0.00004434
Iteration 205/1000 | Loss: 0.00001834
Iteration 206/1000 | Loss: 0.00002248
Iteration 207/1000 | Loss: 0.00002114
Iteration 208/1000 | Loss: 0.00001677
Iteration 209/1000 | Loss: 0.00001641
Iteration 210/1000 | Loss: 0.00001626
Iteration 211/1000 | Loss: 0.00001613
Iteration 212/1000 | Loss: 0.00001597
Iteration 213/1000 | Loss: 0.00001590
Iteration 214/1000 | Loss: 0.00001589
Iteration 215/1000 | Loss: 0.00001588
Iteration 216/1000 | Loss: 0.00001588
Iteration 217/1000 | Loss: 0.00001588
Iteration 218/1000 | Loss: 0.00001587
Iteration 219/1000 | Loss: 0.00001585
Iteration 220/1000 | Loss: 0.00001585
Iteration 221/1000 | Loss: 0.00001583
Iteration 222/1000 | Loss: 0.00001583
Iteration 223/1000 | Loss: 0.00001582
Iteration 224/1000 | Loss: 0.00001582
Iteration 225/1000 | Loss: 0.00001580
Iteration 226/1000 | Loss: 0.00001572
Iteration 227/1000 | Loss: 0.00001570
Iteration 228/1000 | Loss: 0.00001567
Iteration 229/1000 | Loss: 0.00001567
Iteration 230/1000 | Loss: 0.00001567
Iteration 231/1000 | Loss: 0.00001567
Iteration 232/1000 | Loss: 0.00001567
Iteration 233/1000 | Loss: 0.00001567
Iteration 234/1000 | Loss: 0.00001566
Iteration 235/1000 | Loss: 0.00001563
Iteration 236/1000 | Loss: 0.00001562
Iteration 237/1000 | Loss: 0.00001562
Iteration 238/1000 | Loss: 0.00001561
Iteration 239/1000 | Loss: 0.00001561
Iteration 240/1000 | Loss: 0.00001560
Iteration 241/1000 | Loss: 0.00001560
Iteration 242/1000 | Loss: 0.00001560
Iteration 243/1000 | Loss: 0.00001560
Iteration 244/1000 | Loss: 0.00001559
Iteration 245/1000 | Loss: 0.00001558
Iteration 246/1000 | Loss: 0.00001558
Iteration 247/1000 | Loss: 0.00001558
Iteration 248/1000 | Loss: 0.00001558
Iteration 249/1000 | Loss: 0.00001558
Iteration 250/1000 | Loss: 0.00001558
Iteration 251/1000 | Loss: 0.00001558
Iteration 252/1000 | Loss: 0.00001558
Iteration 253/1000 | Loss: 0.00001558
Iteration 254/1000 | Loss: 0.00001558
Iteration 255/1000 | Loss: 0.00001558
Iteration 256/1000 | Loss: 0.00001558
Iteration 257/1000 | Loss: 0.00001557
Iteration 258/1000 | Loss: 0.00001557
Iteration 259/1000 | Loss: 0.00001557
Iteration 260/1000 | Loss: 0.00001557
Iteration 261/1000 | Loss: 0.00001557
Iteration 262/1000 | Loss: 0.00001557
Iteration 263/1000 | Loss: 0.00001557
Iteration 264/1000 | Loss: 0.00001556
Iteration 265/1000 | Loss: 0.00001556
Iteration 266/1000 | Loss: 0.00001556
Iteration 267/1000 | Loss: 0.00001556
Iteration 268/1000 | Loss: 0.00001556
Iteration 269/1000 | Loss: 0.00001556
Iteration 270/1000 | Loss: 0.00001556
Iteration 271/1000 | Loss: 0.00001556
Iteration 272/1000 | Loss: 0.00001556
Iteration 273/1000 | Loss: 0.00001555
Iteration 274/1000 | Loss: 0.00001555
Iteration 275/1000 | Loss: 0.00001555
Iteration 276/1000 | Loss: 0.00001555
Iteration 277/1000 | Loss: 0.00001555
Iteration 278/1000 | Loss: 0.00001555
Iteration 279/1000 | Loss: 0.00001555
Iteration 280/1000 | Loss: 0.00001555
Iteration 281/1000 | Loss: 0.00001555
Iteration 282/1000 | Loss: 0.00001555
Iteration 283/1000 | Loss: 0.00001555
Iteration 284/1000 | Loss: 0.00001555
Iteration 285/1000 | Loss: 0.00001554
Iteration 286/1000 | Loss: 0.00001554
Iteration 287/1000 | Loss: 0.00001554
Iteration 288/1000 | Loss: 0.00001554
Iteration 289/1000 | Loss: 0.00001554
Iteration 290/1000 | Loss: 0.00001554
Iteration 291/1000 | Loss: 0.00001554
Iteration 292/1000 | Loss: 0.00001554
Iteration 293/1000 | Loss: 0.00001554
Iteration 294/1000 | Loss: 0.00001554
Iteration 295/1000 | Loss: 0.00001553
Iteration 296/1000 | Loss: 0.00001553
Iteration 297/1000 | Loss: 0.00001553
Iteration 298/1000 | Loss: 0.00001553
Iteration 299/1000 | Loss: 0.00001553
Iteration 300/1000 | Loss: 0.00001553
Iteration 301/1000 | Loss: 0.00001553
Iteration 302/1000 | Loss: 0.00001553
Iteration 303/1000 | Loss: 0.00001553
Iteration 304/1000 | Loss: 0.00001553
Iteration 305/1000 | Loss: 0.00001553
Iteration 306/1000 | Loss: 0.00001552
Iteration 307/1000 | Loss: 0.00001552
Iteration 308/1000 | Loss: 0.00001552
Iteration 309/1000 | Loss: 0.00001552
Iteration 310/1000 | Loss: 0.00001552
Iteration 311/1000 | Loss: 0.00001552
Iteration 312/1000 | Loss: 0.00001552
Iteration 313/1000 | Loss: 0.00001552
Iteration 314/1000 | Loss: 0.00001552
Iteration 315/1000 | Loss: 0.00001552
Iteration 316/1000 | Loss: 0.00001552
Iteration 317/1000 | Loss: 0.00001552
Iteration 318/1000 | Loss: 0.00001552
Iteration 319/1000 | Loss: 0.00001552
Iteration 320/1000 | Loss: 0.00001552
Iteration 321/1000 | Loss: 0.00001552
Iteration 322/1000 | Loss: 0.00001551
Iteration 323/1000 | Loss: 0.00001551
Iteration 324/1000 | Loss: 0.00001551
Iteration 325/1000 | Loss: 0.00001551
Iteration 326/1000 | Loss: 0.00001551
Iteration 327/1000 | Loss: 0.00001550
Iteration 328/1000 | Loss: 0.00001550
Iteration 329/1000 | Loss: 0.00001550
Iteration 330/1000 | Loss: 0.00001550
Iteration 331/1000 | Loss: 0.00001550
Iteration 332/1000 | Loss: 0.00001550
Iteration 333/1000 | Loss: 0.00001550
Iteration 334/1000 | Loss: 0.00001550
Iteration 335/1000 | Loss: 0.00001550
Iteration 336/1000 | Loss: 0.00001550
Iteration 337/1000 | Loss: 0.00001550
Iteration 338/1000 | Loss: 0.00001550
Iteration 339/1000 | Loss: 0.00001550
Iteration 340/1000 | Loss: 0.00001550
Iteration 341/1000 | Loss: 0.00001550
Iteration 342/1000 | Loss: 0.00001550
Iteration 343/1000 | Loss: 0.00001550
Iteration 344/1000 | Loss: 0.00001550
Iteration 345/1000 | Loss: 0.00001550
Iteration 346/1000 | Loss: 0.00001550
Iteration 347/1000 | Loss: 0.00001550
Iteration 348/1000 | Loss: 0.00001550
Iteration 349/1000 | Loss: 0.00001550
Iteration 350/1000 | Loss: 0.00001550
Iteration 351/1000 | Loss: 0.00001550
Iteration 352/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [1.549897569930181e-05, 1.549897569930181e-05, 1.549897569930181e-05, 1.549897569930181e-05, 1.549897569930181e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.549897569930181e-05

Optimization complete. Final v2v error: 3.2074666023254395 mm

Highest mean error: 9.0603609085083 mm for frame 67

Lowest mean error: 2.411181926727295 mm for frame 5

Saving results

Total time: 392.14417910575867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344215
Iteration 2/25 | Loss: 0.00141059
Iteration 3/25 | Loss: 0.00127894
Iteration 4/25 | Loss: 0.00126621
Iteration 5/25 | Loss: 0.00126192
Iteration 6/25 | Loss: 0.00126143
Iteration 7/25 | Loss: 0.00126143
Iteration 8/25 | Loss: 0.00126143
Iteration 9/25 | Loss: 0.00126143
Iteration 10/25 | Loss: 0.00126143
Iteration 11/25 | Loss: 0.00126143
Iteration 12/25 | Loss: 0.00126143
Iteration 13/25 | Loss: 0.00126143
Iteration 14/25 | Loss: 0.00126143
Iteration 15/25 | Loss: 0.00126143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012614298611879349, 0.0012614298611879349, 0.0012614298611879349, 0.0012614298611879349, 0.0012614298611879349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012614298611879349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04791451
Iteration 2/25 | Loss: 0.00511401
Iteration 3/25 | Loss: 0.00511401
Iteration 4/25 | Loss: 0.00511401
Iteration 5/25 | Loss: 0.00511401
Iteration 6/25 | Loss: 0.00511401
Iteration 7/25 | Loss: 0.00511401
Iteration 8/25 | Loss: 0.00511401
Iteration 9/25 | Loss: 0.00511401
Iteration 10/25 | Loss: 0.00511401
Iteration 11/25 | Loss: 0.00511401
Iteration 12/25 | Loss: 0.00511401
Iteration 13/25 | Loss: 0.00511401
Iteration 14/25 | Loss: 0.00511401
Iteration 15/25 | Loss: 0.00511401
Iteration 16/25 | Loss: 0.00511401
Iteration 17/25 | Loss: 0.00511401
Iteration 18/25 | Loss: 0.00511401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00511400680989027, 0.00511400680989027, 0.00511400680989027, 0.00511400680989027, 0.00511400680989027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00511400680989027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00511401
Iteration 2/1000 | Loss: 0.00003947
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001607
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001371
Iteration 12/1000 | Loss: 0.00001351
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001312
Iteration 25/1000 | Loss: 0.00001311
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001306
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001305
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001293
Iteration 84/1000 | Loss: 0.00001293
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001291
Iteration 92/1000 | Loss: 0.00001291
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001290
Iteration 100/1000 | Loss: 0.00001290
Iteration 101/1000 | Loss: 0.00001290
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001290
Iteration 108/1000 | Loss: 0.00001290
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001290
Iteration 111/1000 | Loss: 0.00001290
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.2897292435809504e-05, 1.2897292435809504e-05, 1.2897292435809504e-05, 1.2897292435809504e-05, 1.2897292435809504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2897292435809504e-05

Optimization complete. Final v2v error: 3.0234365463256836 mm

Highest mean error: 3.516017436981201 mm for frame 3

Lowest mean error: 2.6092710494995117 mm for frame 133

Saving results

Total time: 41.80497717857361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817668
Iteration 2/25 | Loss: 0.00170359
Iteration 3/25 | Loss: 0.00136410
Iteration 4/25 | Loss: 0.00132161
Iteration 5/25 | Loss: 0.00131472
Iteration 6/25 | Loss: 0.00131341
Iteration 7/25 | Loss: 0.00131341
Iteration 8/25 | Loss: 0.00131341
Iteration 9/25 | Loss: 0.00131341
Iteration 10/25 | Loss: 0.00131341
Iteration 11/25 | Loss: 0.00131341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013134118635207415, 0.0013134118635207415, 0.0013134118635207415, 0.0013134118635207415, 0.0013134118635207415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013134118635207415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12306249
Iteration 2/25 | Loss: 0.00439190
Iteration 3/25 | Loss: 0.00439190
Iteration 4/25 | Loss: 0.00439189
Iteration 5/25 | Loss: 0.00439189
Iteration 6/25 | Loss: 0.00439189
Iteration 7/25 | Loss: 0.00439189
Iteration 8/25 | Loss: 0.00439189
Iteration 9/25 | Loss: 0.00439189
Iteration 10/25 | Loss: 0.00439189
Iteration 11/25 | Loss: 0.00439189
Iteration 12/25 | Loss: 0.00439189
Iteration 13/25 | Loss: 0.00439189
Iteration 14/25 | Loss: 0.00439189
Iteration 15/25 | Loss: 0.00439189
Iteration 16/25 | Loss: 0.00439189
Iteration 17/25 | Loss: 0.00439189
Iteration 18/25 | Loss: 0.00439189
Iteration 19/25 | Loss: 0.00439189
Iteration 20/25 | Loss: 0.00439189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004391892813146114, 0.004391892813146114, 0.004391892813146114, 0.004391892813146114, 0.004391892813146114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004391892813146114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00439189
Iteration 2/1000 | Loss: 0.00005984
Iteration 3/1000 | Loss: 0.00002860
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00002060
Iteration 6/1000 | Loss: 0.00001927
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00001791
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001658
Iteration 12/1000 | Loss: 0.00001631
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001589
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001553
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001549
Iteration 24/1000 | Loss: 0.00001549
Iteration 25/1000 | Loss: 0.00001545
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001542
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001541
Iteration 34/1000 | Loss: 0.00001541
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001540
Iteration 37/1000 | Loss: 0.00001540
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001538
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001537
Iteration 47/1000 | Loss: 0.00001537
Iteration 48/1000 | Loss: 0.00001537
Iteration 49/1000 | Loss: 0.00001537
Iteration 50/1000 | Loss: 0.00001537
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001536
Iteration 53/1000 | Loss: 0.00001535
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001534
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001533
Iteration 60/1000 | Loss: 0.00001533
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001531
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001530
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001528
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001528
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001525
Iteration 90/1000 | Loss: 0.00001525
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001518
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001514
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001513
Iteration 131/1000 | Loss: 0.00001513
Iteration 132/1000 | Loss: 0.00001513
Iteration 133/1000 | Loss: 0.00001513
Iteration 134/1000 | Loss: 0.00001513
Iteration 135/1000 | Loss: 0.00001513
Iteration 136/1000 | Loss: 0.00001513
Iteration 137/1000 | Loss: 0.00001513
Iteration 138/1000 | Loss: 0.00001512
Iteration 139/1000 | Loss: 0.00001512
Iteration 140/1000 | Loss: 0.00001512
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001511
Iteration 145/1000 | Loss: 0.00001511
Iteration 146/1000 | Loss: 0.00001510
Iteration 147/1000 | Loss: 0.00001510
Iteration 148/1000 | Loss: 0.00001510
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001509
Iteration 151/1000 | Loss: 0.00001509
Iteration 152/1000 | Loss: 0.00001509
Iteration 153/1000 | Loss: 0.00001509
Iteration 154/1000 | Loss: 0.00001509
Iteration 155/1000 | Loss: 0.00001509
Iteration 156/1000 | Loss: 0.00001509
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Iteration 163/1000 | Loss: 0.00001508
Iteration 164/1000 | Loss: 0.00001508
Iteration 165/1000 | Loss: 0.00001508
Iteration 166/1000 | Loss: 0.00001507
Iteration 167/1000 | Loss: 0.00001507
Iteration 168/1000 | Loss: 0.00001507
Iteration 169/1000 | Loss: 0.00001507
Iteration 170/1000 | Loss: 0.00001507
Iteration 171/1000 | Loss: 0.00001506
Iteration 172/1000 | Loss: 0.00001506
Iteration 173/1000 | Loss: 0.00001506
Iteration 174/1000 | Loss: 0.00001506
Iteration 175/1000 | Loss: 0.00001506
Iteration 176/1000 | Loss: 0.00001506
Iteration 177/1000 | Loss: 0.00001506
Iteration 178/1000 | Loss: 0.00001506
Iteration 179/1000 | Loss: 0.00001505
Iteration 180/1000 | Loss: 0.00001505
Iteration 181/1000 | Loss: 0.00001505
Iteration 182/1000 | Loss: 0.00001505
Iteration 183/1000 | Loss: 0.00001505
Iteration 184/1000 | Loss: 0.00001505
Iteration 185/1000 | Loss: 0.00001505
Iteration 186/1000 | Loss: 0.00001505
Iteration 187/1000 | Loss: 0.00001505
Iteration 188/1000 | Loss: 0.00001505
Iteration 189/1000 | Loss: 0.00001505
Iteration 190/1000 | Loss: 0.00001505
Iteration 191/1000 | Loss: 0.00001505
Iteration 192/1000 | Loss: 0.00001505
Iteration 193/1000 | Loss: 0.00001505
Iteration 194/1000 | Loss: 0.00001505
Iteration 195/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.505351610830985e-05, 1.505351610830985e-05, 1.505351610830985e-05, 1.505351610830985e-05, 1.505351610830985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.505351610830985e-05

Optimization complete. Final v2v error: 3.2923593521118164 mm

Highest mean error: 3.786752462387085 mm for frame 59

Lowest mean error: 2.7447335720062256 mm for frame 13

Saving results

Total time: 47.93753004074097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085520
Iteration 2/25 | Loss: 0.00277038
Iteration 3/25 | Loss: 0.00184887
Iteration 4/25 | Loss: 0.00169713
Iteration 5/25 | Loss: 0.00170758
Iteration 6/25 | Loss: 0.00160012
Iteration 7/25 | Loss: 0.00154632
Iteration 8/25 | Loss: 0.00151151
Iteration 9/25 | Loss: 0.00146993
Iteration 10/25 | Loss: 0.00140678
Iteration 11/25 | Loss: 0.00143669
Iteration 12/25 | Loss: 0.00139356
Iteration 13/25 | Loss: 0.00137398
Iteration 14/25 | Loss: 0.00137039
Iteration 15/25 | Loss: 0.00135977
Iteration 16/25 | Loss: 0.00135010
Iteration 17/25 | Loss: 0.00134764
Iteration 18/25 | Loss: 0.00135469
Iteration 19/25 | Loss: 0.00135140
Iteration 20/25 | Loss: 0.00135120
Iteration 21/25 | Loss: 0.00135586
Iteration 22/25 | Loss: 0.00134738
Iteration 23/25 | Loss: 0.00134446
Iteration 24/25 | Loss: 0.00134554
Iteration 25/25 | Loss: 0.00133788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13583672
Iteration 2/25 | Loss: 0.00752398
Iteration 3/25 | Loss: 0.00498589
Iteration 4/25 | Loss: 0.00498585
Iteration 5/25 | Loss: 0.00498585
Iteration 6/25 | Loss: 0.00498585
Iteration 7/25 | Loss: 0.00498585
Iteration 8/25 | Loss: 0.00498585
Iteration 9/25 | Loss: 0.00498584
Iteration 10/25 | Loss: 0.00498584
Iteration 11/25 | Loss: 0.00498584
Iteration 12/25 | Loss: 0.00498584
Iteration 13/25 | Loss: 0.00498584
Iteration 14/25 | Loss: 0.00498584
Iteration 15/25 | Loss: 0.00498584
Iteration 16/25 | Loss: 0.00498584
Iteration 17/25 | Loss: 0.00498584
Iteration 18/25 | Loss: 0.00498584
Iteration 19/25 | Loss: 0.00498584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004985843785107136, 0.004985843785107136, 0.004985843785107136, 0.004985843785107136, 0.004985843785107136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004985843785107136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00498584
Iteration 2/1000 | Loss: 0.00047406
Iteration 3/1000 | Loss: 0.00063666
Iteration 4/1000 | Loss: 0.00083693
Iteration 5/1000 | Loss: 0.00046143
Iteration 6/1000 | Loss: 0.00066753
Iteration 7/1000 | Loss: 0.00060092
Iteration 8/1000 | Loss: 0.00036596
Iteration 9/1000 | Loss: 0.00045906
Iteration 10/1000 | Loss: 0.00036621
Iteration 11/1000 | Loss: 0.00036935
Iteration 12/1000 | Loss: 0.00075204
Iteration 13/1000 | Loss: 0.00051317
Iteration 14/1000 | Loss: 0.00034599
Iteration 15/1000 | Loss: 0.00079305
Iteration 16/1000 | Loss: 0.00036227
Iteration 17/1000 | Loss: 0.00082635
Iteration 18/1000 | Loss: 0.00033124
Iteration 19/1000 | Loss: 0.00061558
Iteration 20/1000 | Loss: 0.00042859
Iteration 21/1000 | Loss: 0.00028599
Iteration 22/1000 | Loss: 0.00019830
Iteration 23/1000 | Loss: 0.00034538
Iteration 24/1000 | Loss: 0.00033403
Iteration 25/1000 | Loss: 0.00035774
Iteration 26/1000 | Loss: 0.00074118
Iteration 27/1000 | Loss: 0.00048536
Iteration 28/1000 | Loss: 0.00045316
Iteration 29/1000 | Loss: 0.00034854
Iteration 30/1000 | Loss: 0.00031518
Iteration 31/1000 | Loss: 0.00027847
Iteration 32/1000 | Loss: 0.00047050
Iteration 33/1000 | Loss: 0.00034820
Iteration 34/1000 | Loss: 0.00033603
Iteration 35/1000 | Loss: 0.00044404
Iteration 36/1000 | Loss: 0.00036380
Iteration 37/1000 | Loss: 0.00058703
Iteration 38/1000 | Loss: 0.00074416
Iteration 39/1000 | Loss: 0.00081520
Iteration 40/1000 | Loss: 0.00080919
Iteration 41/1000 | Loss: 0.00079763
Iteration 42/1000 | Loss: 0.00079845
Iteration 43/1000 | Loss: 0.00070319
Iteration 44/1000 | Loss: 0.00071761
Iteration 45/1000 | Loss: 0.00068270
Iteration 46/1000 | Loss: 0.00094039
Iteration 47/1000 | Loss: 0.00076706
Iteration 48/1000 | Loss: 0.00076239
Iteration 49/1000 | Loss: 0.00058318
Iteration 50/1000 | Loss: 0.00047980
Iteration 51/1000 | Loss: 0.00077809
Iteration 52/1000 | Loss: 0.00109136
Iteration 53/1000 | Loss: 0.00115657
Iteration 54/1000 | Loss: 0.00075119
Iteration 55/1000 | Loss: 0.00092734
Iteration 56/1000 | Loss: 0.00043180
Iteration 57/1000 | Loss: 0.00033941
Iteration 58/1000 | Loss: 0.00024334
Iteration 59/1000 | Loss: 0.00109111
Iteration 60/1000 | Loss: 0.00062538
Iteration 61/1000 | Loss: 0.00052076
Iteration 62/1000 | Loss: 0.00011107
Iteration 63/1000 | Loss: 0.00031095
Iteration 64/1000 | Loss: 0.00042045
Iteration 65/1000 | Loss: 0.00044783
Iteration 66/1000 | Loss: 0.00040638
Iteration 67/1000 | Loss: 0.00041838
Iteration 68/1000 | Loss: 0.00055135
Iteration 69/1000 | Loss: 0.00031461
Iteration 70/1000 | Loss: 0.00021668
Iteration 71/1000 | Loss: 0.00018082
Iteration 72/1000 | Loss: 0.00018216
Iteration 73/1000 | Loss: 0.00035527
Iteration 74/1000 | Loss: 0.00020082
Iteration 75/1000 | Loss: 0.00021101
Iteration 76/1000 | Loss: 0.00066030
Iteration 77/1000 | Loss: 0.00052418
Iteration 78/1000 | Loss: 0.00082332
Iteration 79/1000 | Loss: 0.00041749
Iteration 80/1000 | Loss: 0.00057339
Iteration 81/1000 | Loss: 0.00058087
Iteration 82/1000 | Loss: 0.00036915
Iteration 83/1000 | Loss: 0.00102842
Iteration 84/1000 | Loss: 0.00023316
Iteration 85/1000 | Loss: 0.00040927
Iteration 86/1000 | Loss: 0.00054287
Iteration 87/1000 | Loss: 0.00018687
Iteration 88/1000 | Loss: 0.00027903
Iteration 89/1000 | Loss: 0.00023006
Iteration 90/1000 | Loss: 0.00006254
Iteration 91/1000 | Loss: 0.00014495
Iteration 92/1000 | Loss: 0.00013209
Iteration 93/1000 | Loss: 0.00015926
Iteration 94/1000 | Loss: 0.00014104
Iteration 95/1000 | Loss: 0.00016844
Iteration 96/1000 | Loss: 0.00029950
Iteration 97/1000 | Loss: 0.00085578
Iteration 98/1000 | Loss: 0.00028963
Iteration 99/1000 | Loss: 0.00041297
Iteration 100/1000 | Loss: 0.00056840
Iteration 101/1000 | Loss: 0.00027076
Iteration 102/1000 | Loss: 0.00026345
Iteration 103/1000 | Loss: 0.00030619
Iteration 104/1000 | Loss: 0.00013604
Iteration 105/1000 | Loss: 0.00007844
Iteration 106/1000 | Loss: 0.00022653
Iteration 107/1000 | Loss: 0.00014048
Iteration 108/1000 | Loss: 0.00021156
Iteration 109/1000 | Loss: 0.00013076
Iteration 110/1000 | Loss: 0.00018992
Iteration 111/1000 | Loss: 0.00013103
Iteration 112/1000 | Loss: 0.00008529
Iteration 113/1000 | Loss: 0.00022868
Iteration 114/1000 | Loss: 0.00011712
Iteration 115/1000 | Loss: 0.00026534
Iteration 116/1000 | Loss: 0.00005037
Iteration 117/1000 | Loss: 0.00003346
Iteration 118/1000 | Loss: 0.00002847
Iteration 119/1000 | Loss: 0.00006940
Iteration 120/1000 | Loss: 0.00003782
Iteration 121/1000 | Loss: 0.00004172
Iteration 122/1000 | Loss: 0.00003259
Iteration 123/1000 | Loss: 0.00002450
Iteration 124/1000 | Loss: 0.00004703
Iteration 125/1000 | Loss: 0.00003420
Iteration 126/1000 | Loss: 0.00002509
Iteration 127/1000 | Loss: 0.00005719
Iteration 128/1000 | Loss: 0.00003395
Iteration 129/1000 | Loss: 0.00005629
Iteration 130/1000 | Loss: 0.00003447
Iteration 131/1000 | Loss: 0.00005664
Iteration 132/1000 | Loss: 0.00003360
Iteration 133/1000 | Loss: 0.00005111
Iteration 134/1000 | Loss: 0.00003277
Iteration 135/1000 | Loss: 0.00004884
Iteration 136/1000 | Loss: 0.00003488
Iteration 137/1000 | Loss: 0.00005370
Iteration 138/1000 | Loss: 0.00003490
Iteration 139/1000 | Loss: 0.00005247
Iteration 140/1000 | Loss: 0.00003383
Iteration 141/1000 | Loss: 0.00005467
Iteration 142/1000 | Loss: 0.00003991
Iteration 143/1000 | Loss: 0.00005425
Iteration 144/1000 | Loss: 0.00002438
Iteration 145/1000 | Loss: 0.00015098
Iteration 146/1000 | Loss: 0.00015178
Iteration 147/1000 | Loss: 0.00002857
Iteration 148/1000 | Loss: 0.00013755
Iteration 149/1000 | Loss: 0.00004175
Iteration 150/1000 | Loss: 0.00002520
Iteration 151/1000 | Loss: 0.00018265
Iteration 152/1000 | Loss: 0.00017046
Iteration 153/1000 | Loss: 0.00002332
Iteration 154/1000 | Loss: 0.00013825
Iteration 155/1000 | Loss: 0.00005079
Iteration 156/1000 | Loss: 0.00002093
Iteration 157/1000 | Loss: 0.00012576
Iteration 158/1000 | Loss: 0.00003162
Iteration 159/1000 | Loss: 0.00003276
Iteration 160/1000 | Loss: 0.00008338
Iteration 161/1000 | Loss: 0.00002295
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002173
Iteration 164/1000 | Loss: 0.00002125
Iteration 165/1000 | Loss: 0.00002098
Iteration 166/1000 | Loss: 0.00002053
Iteration 167/1000 | Loss: 0.00002010
Iteration 168/1000 | Loss: 0.00001976
Iteration 169/1000 | Loss: 0.00001938
Iteration 170/1000 | Loss: 0.00001918
Iteration 171/1000 | Loss: 0.00001916
Iteration 172/1000 | Loss: 0.00001905
Iteration 173/1000 | Loss: 0.00001905
Iteration 174/1000 | Loss: 0.00001902
Iteration 175/1000 | Loss: 0.00001902
Iteration 176/1000 | Loss: 0.00001900
Iteration 177/1000 | Loss: 0.00001900
Iteration 178/1000 | Loss: 0.00001900
Iteration 179/1000 | Loss: 0.00001897
Iteration 180/1000 | Loss: 0.00001894
Iteration 181/1000 | Loss: 0.00001893
Iteration 182/1000 | Loss: 0.00001893
Iteration 183/1000 | Loss: 0.00001893
Iteration 184/1000 | Loss: 0.00001892
Iteration 185/1000 | Loss: 0.00001892
Iteration 186/1000 | Loss: 0.00001892
Iteration 187/1000 | Loss: 0.00001892
Iteration 188/1000 | Loss: 0.00001892
Iteration 189/1000 | Loss: 0.00001891
Iteration 190/1000 | Loss: 0.00001891
Iteration 191/1000 | Loss: 0.00001891
Iteration 192/1000 | Loss: 0.00001890
Iteration 193/1000 | Loss: 0.00001890
Iteration 194/1000 | Loss: 0.00001890
Iteration 195/1000 | Loss: 0.00001889
Iteration 196/1000 | Loss: 0.00001889
Iteration 197/1000 | Loss: 0.00001888
Iteration 198/1000 | Loss: 0.00001888
Iteration 199/1000 | Loss: 0.00001888
Iteration 200/1000 | Loss: 0.00001887
Iteration 201/1000 | Loss: 0.00001887
Iteration 202/1000 | Loss: 0.00001886
Iteration 203/1000 | Loss: 0.00001884
Iteration 204/1000 | Loss: 0.00001883
Iteration 205/1000 | Loss: 0.00001883
Iteration 206/1000 | Loss: 0.00001882
Iteration 207/1000 | Loss: 0.00001881
Iteration 208/1000 | Loss: 0.00001881
Iteration 209/1000 | Loss: 0.00001880
Iteration 210/1000 | Loss: 0.00001880
Iteration 211/1000 | Loss: 0.00001880
Iteration 212/1000 | Loss: 0.00001879
Iteration 213/1000 | Loss: 0.00001879
Iteration 214/1000 | Loss: 0.00001879
Iteration 215/1000 | Loss: 0.00001878
Iteration 216/1000 | Loss: 0.00001878
Iteration 217/1000 | Loss: 0.00001878
Iteration 218/1000 | Loss: 0.00001878
Iteration 219/1000 | Loss: 0.00001878
Iteration 220/1000 | Loss: 0.00001878
Iteration 221/1000 | Loss: 0.00001878
Iteration 222/1000 | Loss: 0.00001878
Iteration 223/1000 | Loss: 0.00001878
Iteration 224/1000 | Loss: 0.00001878
Iteration 225/1000 | Loss: 0.00001878
Iteration 226/1000 | Loss: 0.00001877
Iteration 227/1000 | Loss: 0.00001877
Iteration 228/1000 | Loss: 0.00001877
Iteration 229/1000 | Loss: 0.00001877
Iteration 230/1000 | Loss: 0.00001877
Iteration 231/1000 | Loss: 0.00001877
Iteration 232/1000 | Loss: 0.00001876
Iteration 233/1000 | Loss: 0.00001876
Iteration 234/1000 | Loss: 0.00001876
Iteration 235/1000 | Loss: 0.00001876
Iteration 236/1000 | Loss: 0.00001876
Iteration 237/1000 | Loss: 0.00001875
Iteration 238/1000 | Loss: 0.00001875
Iteration 239/1000 | Loss: 0.00001875
Iteration 240/1000 | Loss: 0.00001875
Iteration 241/1000 | Loss: 0.00001875
Iteration 242/1000 | Loss: 0.00001875
Iteration 243/1000 | Loss: 0.00001875
Iteration 244/1000 | Loss: 0.00001875
Iteration 245/1000 | Loss: 0.00001875
Iteration 246/1000 | Loss: 0.00001875
Iteration 247/1000 | Loss: 0.00001874
Iteration 248/1000 | Loss: 0.00001874
Iteration 249/1000 | Loss: 0.00001874
Iteration 250/1000 | Loss: 0.00001874
Iteration 251/1000 | Loss: 0.00001874
Iteration 252/1000 | Loss: 0.00001874
Iteration 253/1000 | Loss: 0.00001874
Iteration 254/1000 | Loss: 0.00001874
Iteration 255/1000 | Loss: 0.00001874
Iteration 256/1000 | Loss: 0.00001874
Iteration 257/1000 | Loss: 0.00001874
Iteration 258/1000 | Loss: 0.00001874
Iteration 259/1000 | Loss: 0.00001874
Iteration 260/1000 | Loss: 0.00001874
Iteration 261/1000 | Loss: 0.00001874
Iteration 262/1000 | Loss: 0.00001874
Iteration 263/1000 | Loss: 0.00001873
Iteration 264/1000 | Loss: 0.00001873
Iteration 265/1000 | Loss: 0.00001873
Iteration 266/1000 | Loss: 0.00001873
Iteration 267/1000 | Loss: 0.00001873
Iteration 268/1000 | Loss: 0.00001873
Iteration 269/1000 | Loss: 0.00001873
Iteration 270/1000 | Loss: 0.00001873
Iteration 271/1000 | Loss: 0.00001873
Iteration 272/1000 | Loss: 0.00001873
Iteration 273/1000 | Loss: 0.00001873
Iteration 274/1000 | Loss: 0.00001873
Iteration 275/1000 | Loss: 0.00001873
Iteration 276/1000 | Loss: 0.00001873
Iteration 277/1000 | Loss: 0.00001873
Iteration 278/1000 | Loss: 0.00001873
Iteration 279/1000 | Loss: 0.00001873
Iteration 280/1000 | Loss: 0.00001873
Iteration 281/1000 | Loss: 0.00001873
Iteration 282/1000 | Loss: 0.00001873
Iteration 283/1000 | Loss: 0.00001873
Iteration 284/1000 | Loss: 0.00001873
Iteration 285/1000 | Loss: 0.00001873
Iteration 286/1000 | Loss: 0.00001873
Iteration 287/1000 | Loss: 0.00001873
Iteration 288/1000 | Loss: 0.00001873
Iteration 289/1000 | Loss: 0.00001873
Iteration 290/1000 | Loss: 0.00001872
Iteration 291/1000 | Loss: 0.00001872
Iteration 292/1000 | Loss: 0.00001872
Iteration 293/1000 | Loss: 0.00001872
Iteration 294/1000 | Loss: 0.00001872
Iteration 295/1000 | Loss: 0.00001872
Iteration 296/1000 | Loss: 0.00001872
Iteration 297/1000 | Loss: 0.00001872
Iteration 298/1000 | Loss: 0.00001872
Iteration 299/1000 | Loss: 0.00001872
Iteration 300/1000 | Loss: 0.00001872
Iteration 301/1000 | Loss: 0.00001872
Iteration 302/1000 | Loss: 0.00001872
Iteration 303/1000 | Loss: 0.00001872
Iteration 304/1000 | Loss: 0.00001872
Iteration 305/1000 | Loss: 0.00001872
Iteration 306/1000 | Loss: 0.00001872
Iteration 307/1000 | Loss: 0.00001872
Iteration 308/1000 | Loss: 0.00001871
Iteration 309/1000 | Loss: 0.00001871
Iteration 310/1000 | Loss: 0.00001871
Iteration 311/1000 | Loss: 0.00001871
Iteration 312/1000 | Loss: 0.00001871
Iteration 313/1000 | Loss: 0.00001871
Iteration 314/1000 | Loss: 0.00001871
Iteration 315/1000 | Loss: 0.00001871
Iteration 316/1000 | Loss: 0.00001871
Iteration 317/1000 | Loss: 0.00001871
Iteration 318/1000 | Loss: 0.00001871
Iteration 319/1000 | Loss: 0.00001871
Iteration 320/1000 | Loss: 0.00001871
Iteration 321/1000 | Loss: 0.00001871
Iteration 322/1000 | Loss: 0.00001871
Iteration 323/1000 | Loss: 0.00001871
Iteration 324/1000 | Loss: 0.00001871
Iteration 325/1000 | Loss: 0.00001871
Iteration 326/1000 | Loss: 0.00001871
Iteration 327/1000 | Loss: 0.00001871
Iteration 328/1000 | Loss: 0.00001871
Iteration 329/1000 | Loss: 0.00001871
Iteration 330/1000 | Loss: 0.00001871
Iteration 331/1000 | Loss: 0.00001871
Iteration 332/1000 | Loss: 0.00001871
Iteration 333/1000 | Loss: 0.00001871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [1.8707572962739505e-05, 1.8707572962739505e-05, 1.8707572962739505e-05, 1.8707572962739505e-05, 1.8707572962739505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8707572962739505e-05

Optimization complete. Final v2v error: 3.3616881370544434 mm

Highest mean error: 12.17016887664795 mm for frame 60

Lowest mean error: 2.700127124786377 mm for frame 107

Saving results

Total time: 295.43775844573975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00734304
Iteration 2/25 | Loss: 0.00171852
Iteration 3/25 | Loss: 0.00136840
Iteration 4/25 | Loss: 0.00134270
Iteration 5/25 | Loss: 0.00135190
Iteration 6/25 | Loss: 0.00134922
Iteration 7/25 | Loss: 0.00133250
Iteration 8/25 | Loss: 0.00131677
Iteration 9/25 | Loss: 0.00131321
Iteration 10/25 | Loss: 0.00131231
Iteration 11/25 | Loss: 0.00131205
Iteration 12/25 | Loss: 0.00131204
Iteration 13/25 | Loss: 0.00131204
Iteration 14/25 | Loss: 0.00131203
Iteration 15/25 | Loss: 0.00131203
Iteration 16/25 | Loss: 0.00131203
Iteration 17/25 | Loss: 0.00131203
Iteration 18/25 | Loss: 0.00131203
Iteration 19/25 | Loss: 0.00131203
Iteration 20/25 | Loss: 0.00131203
Iteration 21/25 | Loss: 0.00131203
Iteration 22/25 | Loss: 0.00131203
Iteration 23/25 | Loss: 0.00131203
Iteration 24/25 | Loss: 0.00131203
Iteration 25/25 | Loss: 0.00131203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03837347
Iteration 2/25 | Loss: 0.00433629
Iteration 3/25 | Loss: 0.00435315
Iteration 4/25 | Loss: 0.00433627
Iteration 5/25 | Loss: 0.00433627
Iteration 6/25 | Loss: 0.00433627
Iteration 7/25 | Loss: 0.00433627
Iteration 8/25 | Loss: 0.00433627
Iteration 9/25 | Loss: 0.00433627
Iteration 10/25 | Loss: 0.00433627
Iteration 11/25 | Loss: 0.00433627
Iteration 12/25 | Loss: 0.00433627
Iteration 13/25 | Loss: 0.00433627
Iteration 14/25 | Loss: 0.00433627
Iteration 15/25 | Loss: 0.00433627
Iteration 16/25 | Loss: 0.00433627
Iteration 17/25 | Loss: 0.00433627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004336265381425619, 0.004336265381425619, 0.004336265381425619, 0.004336265381425619, 0.004336265381425619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004336265381425619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00433627
Iteration 2/1000 | Loss: 0.00008389
Iteration 3/1000 | Loss: 0.00006609
Iteration 4/1000 | Loss: 0.00003983
Iteration 5/1000 | Loss: 0.00002904
Iteration 6/1000 | Loss: 0.00002549
Iteration 7/1000 | Loss: 0.00002283
Iteration 8/1000 | Loss: 0.00002071
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00005919
Iteration 12/1000 | Loss: 0.00013199
Iteration 13/1000 | Loss: 0.00001721
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00013369
Iteration 16/1000 | Loss: 0.00004712
Iteration 17/1000 | Loss: 0.00001957
Iteration 18/1000 | Loss: 0.00005058
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00004712
Iteration 23/1000 | Loss: 0.00001599
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00003815
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00003168
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001518
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001513
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001511
Iteration 75/1000 | Loss: 0.00001511
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001511
Iteration 78/1000 | Loss: 0.00001511
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001509
Iteration 86/1000 | Loss: 0.00001509
Iteration 87/1000 | Loss: 0.00001509
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001508
Iteration 90/1000 | Loss: 0.00001508
Iteration 91/1000 | Loss: 0.00001507
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001504
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00004825
Iteration 113/1000 | Loss: 0.00004614
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001501
Iteration 117/1000 | Loss: 0.00001501
Iteration 118/1000 | Loss: 0.00001501
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001500
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001499
Iteration 124/1000 | Loss: 0.00001499
Iteration 125/1000 | Loss: 0.00001499
Iteration 126/1000 | Loss: 0.00001499
Iteration 127/1000 | Loss: 0.00001499
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001498
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001498
Iteration 137/1000 | Loss: 0.00001498
Iteration 138/1000 | Loss: 0.00001498
Iteration 139/1000 | Loss: 0.00001498
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.4977335013099946e-05, 1.4977335013099946e-05, 1.4977335013099946e-05, 1.4977335013099946e-05, 1.4977335013099946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4977335013099946e-05

Optimization complete. Final v2v error: 3.38578462600708 mm

Highest mean error: 3.9361743927001953 mm for frame 115

Lowest mean error: 3.063704490661621 mm for frame 14

Saving results

Total time: 80.84378790855408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420449
Iteration 2/25 | Loss: 0.00132070
Iteration 3/25 | Loss: 0.00124170
Iteration 4/25 | Loss: 0.00123233
Iteration 5/25 | Loss: 0.00122913
Iteration 6/25 | Loss: 0.00122800
Iteration 7/25 | Loss: 0.00122772
Iteration 8/25 | Loss: 0.00122768
Iteration 9/25 | Loss: 0.00122768
Iteration 10/25 | Loss: 0.00122768
Iteration 11/25 | Loss: 0.00122768
Iteration 12/25 | Loss: 0.00122768
Iteration 13/25 | Loss: 0.00122768
Iteration 14/25 | Loss: 0.00122768
Iteration 15/25 | Loss: 0.00122768
Iteration 16/25 | Loss: 0.00122768
Iteration 17/25 | Loss: 0.00122768
Iteration 18/25 | Loss: 0.00122768
Iteration 19/25 | Loss: 0.00122768
Iteration 20/25 | Loss: 0.00122768
Iteration 21/25 | Loss: 0.00122768
Iteration 22/25 | Loss: 0.00122768
Iteration 23/25 | Loss: 0.00122768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012276762863621116, 0.0012276762863621116, 0.0012276762863621116, 0.0012276762863621116, 0.0012276762863621116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012276762863621116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66098428
Iteration 2/25 | Loss: 0.00412712
Iteration 3/25 | Loss: 0.00412712
Iteration 4/25 | Loss: 0.00412712
Iteration 5/25 | Loss: 0.00412712
Iteration 6/25 | Loss: 0.00412712
Iteration 7/25 | Loss: 0.00412712
Iteration 8/25 | Loss: 0.00412712
Iteration 9/25 | Loss: 0.00412712
Iteration 10/25 | Loss: 0.00412712
Iteration 11/25 | Loss: 0.00412712
Iteration 12/25 | Loss: 0.00412712
Iteration 13/25 | Loss: 0.00412712
Iteration 14/25 | Loss: 0.00412712
Iteration 15/25 | Loss: 0.00412712
Iteration 16/25 | Loss: 0.00412712
Iteration 17/25 | Loss: 0.00412712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00412711501121521, 0.00412711501121521, 0.00412711501121521, 0.00412711501121521, 0.00412711501121521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00412711501121521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00412712
Iteration 2/1000 | Loss: 0.00003136
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001732
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001440
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001393
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001361
Iteration 26/1000 | Loss: 0.00001361
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001358
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001353
Iteration 37/1000 | Loss: 0.00001353
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001351
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001349
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001348
Iteration 52/1000 | Loss: 0.00001348
Iteration 53/1000 | Loss: 0.00001347
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001347
Iteration 56/1000 | Loss: 0.00001347
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001345
Iteration 65/1000 | Loss: 0.00001345
Iteration 66/1000 | Loss: 0.00001345
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001343
Iteration 77/1000 | Loss: 0.00001343
Iteration 78/1000 | Loss: 0.00001343
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.3433230378723238e-05, 1.3433230378723238e-05, 1.3433230378723238e-05, 1.3433230378723238e-05, 1.3433230378723238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3433230378723238e-05

Optimization complete. Final v2v error: 3.028979539871216 mm

Highest mean error: 4.159552574157715 mm for frame 60

Lowest mean error: 2.679445743560791 mm for frame 131

Saving results

Total time: 35.43965792655945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974297
Iteration 2/25 | Loss: 0.00175530
Iteration 3/25 | Loss: 0.00146538
Iteration 4/25 | Loss: 0.00143527
Iteration 5/25 | Loss: 0.00143026
Iteration 6/25 | Loss: 0.00142927
Iteration 7/25 | Loss: 0.00142927
Iteration 8/25 | Loss: 0.00142927
Iteration 9/25 | Loss: 0.00142927
Iteration 10/25 | Loss: 0.00142927
Iteration 11/25 | Loss: 0.00142927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014292745618149638, 0.0014292745618149638, 0.0014292745618149638, 0.0014292745618149638, 0.0014292745618149638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014292745618149638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66193867
Iteration 2/25 | Loss: 0.00339312
Iteration 3/25 | Loss: 0.00339272
Iteration 4/25 | Loss: 0.00339272
Iteration 5/25 | Loss: 0.00339272
Iteration 6/25 | Loss: 0.00339272
Iteration 7/25 | Loss: 0.00339272
Iteration 8/25 | Loss: 0.00339272
Iteration 9/25 | Loss: 0.00339272
Iteration 10/25 | Loss: 0.00339272
Iteration 11/25 | Loss: 0.00339272
Iteration 12/25 | Loss: 0.00339272
Iteration 13/25 | Loss: 0.00339272
Iteration 14/25 | Loss: 0.00339272
Iteration 15/25 | Loss: 0.00339272
Iteration 16/25 | Loss: 0.00339272
Iteration 17/25 | Loss: 0.00339272
Iteration 18/25 | Loss: 0.00339272
Iteration 19/25 | Loss: 0.00339272
Iteration 20/25 | Loss: 0.00339272
Iteration 21/25 | Loss: 0.00339272
Iteration 22/25 | Loss: 0.00339272
Iteration 23/25 | Loss: 0.00339272
Iteration 24/25 | Loss: 0.00339272
Iteration 25/25 | Loss: 0.00339272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00339272
Iteration 2/1000 | Loss: 0.00010672
Iteration 3/1000 | Loss: 0.00005595
Iteration 4/1000 | Loss: 0.00004684
Iteration 5/1000 | Loss: 0.00004332
Iteration 6/1000 | Loss: 0.00004137
Iteration 7/1000 | Loss: 0.00004007
Iteration 8/1000 | Loss: 0.00003893
Iteration 9/1000 | Loss: 0.00003793
Iteration 10/1000 | Loss: 0.00003706
Iteration 11/1000 | Loss: 0.00003653
Iteration 12/1000 | Loss: 0.00003617
Iteration 13/1000 | Loss: 0.00003585
Iteration 14/1000 | Loss: 0.00003560
Iteration 15/1000 | Loss: 0.00003539
Iteration 16/1000 | Loss: 0.00003521
Iteration 17/1000 | Loss: 0.00003520
Iteration 18/1000 | Loss: 0.00003506
Iteration 19/1000 | Loss: 0.00003504
Iteration 20/1000 | Loss: 0.00003491
Iteration 21/1000 | Loss: 0.00003490
Iteration 22/1000 | Loss: 0.00003489
Iteration 23/1000 | Loss: 0.00003485
Iteration 24/1000 | Loss: 0.00003485
Iteration 25/1000 | Loss: 0.00003478
Iteration 26/1000 | Loss: 0.00003474
Iteration 27/1000 | Loss: 0.00003468
Iteration 28/1000 | Loss: 0.00003467
Iteration 29/1000 | Loss: 0.00003465
Iteration 30/1000 | Loss: 0.00003465
Iteration 31/1000 | Loss: 0.00003464
Iteration 32/1000 | Loss: 0.00003461
Iteration 33/1000 | Loss: 0.00003451
Iteration 34/1000 | Loss: 0.00003450
Iteration 35/1000 | Loss: 0.00003449
Iteration 36/1000 | Loss: 0.00003448
Iteration 37/1000 | Loss: 0.00003446
Iteration 38/1000 | Loss: 0.00003446
Iteration 39/1000 | Loss: 0.00003445
Iteration 40/1000 | Loss: 0.00003444
Iteration 41/1000 | Loss: 0.00003443
Iteration 42/1000 | Loss: 0.00003443
Iteration 43/1000 | Loss: 0.00003441
Iteration 44/1000 | Loss: 0.00003440
Iteration 45/1000 | Loss: 0.00003437
Iteration 46/1000 | Loss: 0.00003436
Iteration 47/1000 | Loss: 0.00003435
Iteration 48/1000 | Loss: 0.00003434
Iteration 49/1000 | Loss: 0.00003434
Iteration 50/1000 | Loss: 0.00003434
Iteration 51/1000 | Loss: 0.00003434
Iteration 52/1000 | Loss: 0.00003434
Iteration 53/1000 | Loss: 0.00003434
Iteration 54/1000 | Loss: 0.00003434
Iteration 55/1000 | Loss: 0.00003433
Iteration 56/1000 | Loss: 0.00003433
Iteration 57/1000 | Loss: 0.00003433
Iteration 58/1000 | Loss: 0.00003433
Iteration 59/1000 | Loss: 0.00003433
Iteration 60/1000 | Loss: 0.00003433
Iteration 61/1000 | Loss: 0.00003432
Iteration 62/1000 | Loss: 0.00003431
Iteration 63/1000 | Loss: 0.00003429
Iteration 64/1000 | Loss: 0.00003429
Iteration 65/1000 | Loss: 0.00003429
Iteration 66/1000 | Loss: 0.00003429
Iteration 67/1000 | Loss: 0.00003429
Iteration 68/1000 | Loss: 0.00003429
Iteration 69/1000 | Loss: 0.00003428
Iteration 70/1000 | Loss: 0.00003428
Iteration 71/1000 | Loss: 0.00003428
Iteration 72/1000 | Loss: 0.00003428
Iteration 73/1000 | Loss: 0.00003428
Iteration 74/1000 | Loss: 0.00003426
Iteration 75/1000 | Loss: 0.00003426
Iteration 76/1000 | Loss: 0.00003426
Iteration 77/1000 | Loss: 0.00003426
Iteration 78/1000 | Loss: 0.00003426
Iteration 79/1000 | Loss: 0.00003426
Iteration 80/1000 | Loss: 0.00003426
Iteration 81/1000 | Loss: 0.00003426
Iteration 82/1000 | Loss: 0.00003426
Iteration 83/1000 | Loss: 0.00003426
Iteration 84/1000 | Loss: 0.00003426
Iteration 85/1000 | Loss: 0.00003426
Iteration 86/1000 | Loss: 0.00003425
Iteration 87/1000 | Loss: 0.00003425
Iteration 88/1000 | Loss: 0.00003425
Iteration 89/1000 | Loss: 0.00003425
Iteration 90/1000 | Loss: 0.00003425
Iteration 91/1000 | Loss: 0.00003425
Iteration 92/1000 | Loss: 0.00003425
Iteration 93/1000 | Loss: 0.00003425
Iteration 94/1000 | Loss: 0.00003425
Iteration 95/1000 | Loss: 0.00003425
Iteration 96/1000 | Loss: 0.00003425
Iteration 97/1000 | Loss: 0.00003424
Iteration 98/1000 | Loss: 0.00003424
Iteration 99/1000 | Loss: 0.00003424
Iteration 100/1000 | Loss: 0.00003424
Iteration 101/1000 | Loss: 0.00003424
Iteration 102/1000 | Loss: 0.00003424
Iteration 103/1000 | Loss: 0.00003424
Iteration 104/1000 | Loss: 0.00003424
Iteration 105/1000 | Loss: 0.00003424
Iteration 106/1000 | Loss: 0.00003424
Iteration 107/1000 | Loss: 0.00003424
Iteration 108/1000 | Loss: 0.00003423
Iteration 109/1000 | Loss: 0.00003422
Iteration 110/1000 | Loss: 0.00003422
Iteration 111/1000 | Loss: 0.00003422
Iteration 112/1000 | Loss: 0.00003421
Iteration 113/1000 | Loss: 0.00003421
Iteration 114/1000 | Loss: 0.00003421
Iteration 115/1000 | Loss: 0.00003420
Iteration 116/1000 | Loss: 0.00003420
Iteration 117/1000 | Loss: 0.00003420
Iteration 118/1000 | Loss: 0.00003419
Iteration 119/1000 | Loss: 0.00003419
Iteration 120/1000 | Loss: 0.00003419
Iteration 121/1000 | Loss: 0.00003419
Iteration 122/1000 | Loss: 0.00003418
Iteration 123/1000 | Loss: 0.00003418
Iteration 124/1000 | Loss: 0.00003418
Iteration 125/1000 | Loss: 0.00003417
Iteration 126/1000 | Loss: 0.00003417
Iteration 127/1000 | Loss: 0.00003417
Iteration 128/1000 | Loss: 0.00003417
Iteration 129/1000 | Loss: 0.00003416
Iteration 130/1000 | Loss: 0.00003416
Iteration 131/1000 | Loss: 0.00003416
Iteration 132/1000 | Loss: 0.00003416
Iteration 133/1000 | Loss: 0.00003416
Iteration 134/1000 | Loss: 0.00003415
Iteration 135/1000 | Loss: 0.00003415
Iteration 136/1000 | Loss: 0.00003415
Iteration 137/1000 | Loss: 0.00003415
Iteration 138/1000 | Loss: 0.00003414
Iteration 139/1000 | Loss: 0.00003414
Iteration 140/1000 | Loss: 0.00003414
Iteration 141/1000 | Loss: 0.00003414
Iteration 142/1000 | Loss: 0.00003414
Iteration 143/1000 | Loss: 0.00003414
Iteration 144/1000 | Loss: 0.00003414
Iteration 145/1000 | Loss: 0.00003414
Iteration 146/1000 | Loss: 0.00003413
Iteration 147/1000 | Loss: 0.00003413
Iteration 148/1000 | Loss: 0.00003413
Iteration 149/1000 | Loss: 0.00003413
Iteration 150/1000 | Loss: 0.00003413
Iteration 151/1000 | Loss: 0.00003413
Iteration 152/1000 | Loss: 0.00003412
Iteration 153/1000 | Loss: 0.00003412
Iteration 154/1000 | Loss: 0.00003412
Iteration 155/1000 | Loss: 0.00003412
Iteration 156/1000 | Loss: 0.00003412
Iteration 157/1000 | Loss: 0.00003412
Iteration 158/1000 | Loss: 0.00003412
Iteration 159/1000 | Loss: 0.00003412
Iteration 160/1000 | Loss: 0.00003412
Iteration 161/1000 | Loss: 0.00003412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.4122156648663804e-05, 3.4122156648663804e-05, 3.4122156648663804e-05, 3.4122156648663804e-05, 3.4122156648663804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4122156648663804e-05

Optimization complete. Final v2v error: 4.641406059265137 mm

Highest mean error: 5.829323768615723 mm for frame 46

Lowest mean error: 3.4787652492523193 mm for frame 225

Saving results

Total time: 57.36132788658142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866286
Iteration 2/25 | Loss: 0.00158765
Iteration 3/25 | Loss: 0.00133038
Iteration 4/25 | Loss: 0.00130790
Iteration 5/25 | Loss: 0.00130601
Iteration 6/25 | Loss: 0.00130563
Iteration 7/25 | Loss: 0.00130563
Iteration 8/25 | Loss: 0.00130563
Iteration 9/25 | Loss: 0.00130563
Iteration 10/25 | Loss: 0.00130563
Iteration 11/25 | Loss: 0.00130563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013056310126557946, 0.0013056310126557946, 0.0013056310126557946, 0.0013056310126557946, 0.0013056310126557946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013056310126557946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00752771
Iteration 2/25 | Loss: 0.00287574
Iteration 3/25 | Loss: 0.00287573
Iteration 4/25 | Loss: 0.00287573
Iteration 5/25 | Loss: 0.00287573
Iteration 6/25 | Loss: 0.00287573
Iteration 7/25 | Loss: 0.00287573
Iteration 8/25 | Loss: 0.00287573
Iteration 9/25 | Loss: 0.00287573
Iteration 10/25 | Loss: 0.00287573
Iteration 11/25 | Loss: 0.00287573
Iteration 12/25 | Loss: 0.00287573
Iteration 13/25 | Loss: 0.00287573
Iteration 14/25 | Loss: 0.00287573
Iteration 15/25 | Loss: 0.00287573
Iteration 16/25 | Loss: 0.00287573
Iteration 17/25 | Loss: 0.00287573
Iteration 18/25 | Loss: 0.00287573
Iteration 19/25 | Loss: 0.00287573
Iteration 20/25 | Loss: 0.00287573
Iteration 21/25 | Loss: 0.00287573
Iteration 22/25 | Loss: 0.00287573
Iteration 23/25 | Loss: 0.00287573
Iteration 24/25 | Loss: 0.00287573
Iteration 25/25 | Loss: 0.00287573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287573
Iteration 2/1000 | Loss: 0.00003930
Iteration 3/1000 | Loss: 0.00002345
Iteration 4/1000 | Loss: 0.00002038
Iteration 5/1000 | Loss: 0.00001899
Iteration 6/1000 | Loss: 0.00001837
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001693
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001688
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001681
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001665
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001663
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001663
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001658
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001655
Iteration 46/1000 | Loss: 0.00001655
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001653
Iteration 56/1000 | Loss: 0.00001653
Iteration 57/1000 | Loss: 0.00001652
Iteration 58/1000 | Loss: 0.00001652
Iteration 59/1000 | Loss: 0.00001651
Iteration 60/1000 | Loss: 0.00001651
Iteration 61/1000 | Loss: 0.00001651
Iteration 62/1000 | Loss: 0.00001651
Iteration 63/1000 | Loss: 0.00001650
Iteration 64/1000 | Loss: 0.00001650
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00001650
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001649
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001648
Iteration 83/1000 | Loss: 0.00001648
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001647
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001646
Iteration 92/1000 | Loss: 0.00001646
Iteration 93/1000 | Loss: 0.00001646
Iteration 94/1000 | Loss: 0.00001646
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001645
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001644
Iteration 104/1000 | Loss: 0.00001644
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001644
Iteration 109/1000 | Loss: 0.00001644
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001643
Iteration 117/1000 | Loss: 0.00001643
Iteration 118/1000 | Loss: 0.00001643
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001643
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001641
Iteration 132/1000 | Loss: 0.00001641
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001641
Iteration 136/1000 | Loss: 0.00001641
Iteration 137/1000 | Loss: 0.00001641
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001640
Iteration 144/1000 | Loss: 0.00001640
Iteration 145/1000 | Loss: 0.00001640
Iteration 146/1000 | Loss: 0.00001640
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001639
Iteration 152/1000 | Loss: 0.00001639
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001639
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001638
Iteration 159/1000 | Loss: 0.00001638
Iteration 160/1000 | Loss: 0.00001638
Iteration 161/1000 | Loss: 0.00001638
Iteration 162/1000 | Loss: 0.00001638
Iteration 163/1000 | Loss: 0.00001638
Iteration 164/1000 | Loss: 0.00001638
Iteration 165/1000 | Loss: 0.00001638
Iteration 166/1000 | Loss: 0.00001638
Iteration 167/1000 | Loss: 0.00001638
Iteration 168/1000 | Loss: 0.00001638
Iteration 169/1000 | Loss: 0.00001638
Iteration 170/1000 | Loss: 0.00001637
Iteration 171/1000 | Loss: 0.00001637
Iteration 172/1000 | Loss: 0.00001637
Iteration 173/1000 | Loss: 0.00001637
Iteration 174/1000 | Loss: 0.00001637
Iteration 175/1000 | Loss: 0.00001637
Iteration 176/1000 | Loss: 0.00001637
Iteration 177/1000 | Loss: 0.00001637
Iteration 178/1000 | Loss: 0.00001637
Iteration 179/1000 | Loss: 0.00001637
Iteration 180/1000 | Loss: 0.00001637
Iteration 181/1000 | Loss: 0.00001637
Iteration 182/1000 | Loss: 0.00001637
Iteration 183/1000 | Loss: 0.00001637
Iteration 184/1000 | Loss: 0.00001636
Iteration 185/1000 | Loss: 0.00001636
Iteration 186/1000 | Loss: 0.00001636
Iteration 187/1000 | Loss: 0.00001636
Iteration 188/1000 | Loss: 0.00001636
Iteration 189/1000 | Loss: 0.00001636
Iteration 190/1000 | Loss: 0.00001636
Iteration 191/1000 | Loss: 0.00001636
Iteration 192/1000 | Loss: 0.00001636
Iteration 193/1000 | Loss: 0.00001636
Iteration 194/1000 | Loss: 0.00001635
Iteration 195/1000 | Loss: 0.00001635
Iteration 196/1000 | Loss: 0.00001635
Iteration 197/1000 | Loss: 0.00001635
Iteration 198/1000 | Loss: 0.00001635
Iteration 199/1000 | Loss: 0.00001635
Iteration 200/1000 | Loss: 0.00001635
Iteration 201/1000 | Loss: 0.00001635
Iteration 202/1000 | Loss: 0.00001635
Iteration 203/1000 | Loss: 0.00001635
Iteration 204/1000 | Loss: 0.00001635
Iteration 205/1000 | Loss: 0.00001635
Iteration 206/1000 | Loss: 0.00001635
Iteration 207/1000 | Loss: 0.00001635
Iteration 208/1000 | Loss: 0.00001635
Iteration 209/1000 | Loss: 0.00001635
Iteration 210/1000 | Loss: 0.00001635
Iteration 211/1000 | Loss: 0.00001635
Iteration 212/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.6350633813999593e-05, 1.6350633813999593e-05, 1.6350633813999593e-05, 1.6350633813999593e-05, 1.6350633813999593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6350633813999593e-05

Optimization complete. Final v2v error: 3.428757905960083 mm

Highest mean error: 3.769092559814453 mm for frame 60

Lowest mean error: 3.1329703330993652 mm for frame 6

Saving results

Total time: 38.83517384529114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01140683
Iteration 2/25 | Loss: 0.00228781
Iteration 3/25 | Loss: 0.00186274
Iteration 4/25 | Loss: 0.00167890
Iteration 5/25 | Loss: 0.00142003
Iteration 6/25 | Loss: 0.00140536
Iteration 7/25 | Loss: 0.00140280
Iteration 8/25 | Loss: 0.00140335
Iteration 9/25 | Loss: 0.00140157
Iteration 10/25 | Loss: 0.00140106
Iteration 11/25 | Loss: 0.00140091
Iteration 12/25 | Loss: 0.00140086
Iteration 13/25 | Loss: 0.00140086
Iteration 14/25 | Loss: 0.00140086
Iteration 15/25 | Loss: 0.00140086
Iteration 16/25 | Loss: 0.00140086
Iteration 17/25 | Loss: 0.00140086
Iteration 18/25 | Loss: 0.00140086
Iteration 19/25 | Loss: 0.00140086
Iteration 20/25 | Loss: 0.00140086
Iteration 21/25 | Loss: 0.00140086
Iteration 22/25 | Loss: 0.00140086
Iteration 23/25 | Loss: 0.00140085
Iteration 24/25 | Loss: 0.00140085
Iteration 25/25 | Loss: 0.00140085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00297928
Iteration 2/25 | Loss: 0.00257448
Iteration 3/25 | Loss: 0.00257446
Iteration 4/25 | Loss: 0.00257446
Iteration 5/25 | Loss: 0.00257446
Iteration 6/25 | Loss: 0.00257446
Iteration 7/25 | Loss: 0.00257446
Iteration 8/25 | Loss: 0.00257446
Iteration 9/25 | Loss: 0.00257446
Iteration 10/25 | Loss: 0.00257446
Iteration 11/25 | Loss: 0.00257446
Iteration 12/25 | Loss: 0.00257446
Iteration 13/25 | Loss: 0.00257446
Iteration 14/25 | Loss: 0.00257446
Iteration 15/25 | Loss: 0.00257446
Iteration 16/25 | Loss: 0.00257446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002574460580945015, 0.002574460580945015, 0.002574460580945015, 0.002574460580945015, 0.002574460580945015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002574460580945015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257446
Iteration 2/1000 | Loss: 0.00005915
Iteration 3/1000 | Loss: 0.00003780
Iteration 4/1000 | Loss: 0.00003370
Iteration 5/1000 | Loss: 0.00003230
Iteration 6/1000 | Loss: 0.00003106
Iteration 7/1000 | Loss: 0.00003008
Iteration 8/1000 | Loss: 0.00003144
Iteration 9/1000 | Loss: 0.00002886
Iteration 10/1000 | Loss: 0.00002851
Iteration 11/1000 | Loss: 0.00002812
Iteration 12/1000 | Loss: 0.00002790
Iteration 13/1000 | Loss: 0.00002772
Iteration 14/1000 | Loss: 0.00002773
Iteration 15/1000 | Loss: 0.00002769
Iteration 16/1000 | Loss: 0.00002769
Iteration 17/1000 | Loss: 0.00002764
Iteration 18/1000 | Loss: 0.00002763
Iteration 19/1000 | Loss: 0.00002762
Iteration 20/1000 | Loss: 0.00002762
Iteration 21/1000 | Loss: 0.00002762
Iteration 22/1000 | Loss: 0.00002762
Iteration 23/1000 | Loss: 0.00002761
Iteration 24/1000 | Loss: 0.00002753
Iteration 25/1000 | Loss: 0.00002746
Iteration 26/1000 | Loss: 0.00002744
Iteration 27/1000 | Loss: 0.00002744
Iteration 28/1000 | Loss: 0.00002744
Iteration 29/1000 | Loss: 0.00002744
Iteration 30/1000 | Loss: 0.00002744
Iteration 31/1000 | Loss: 0.00002743
Iteration 32/1000 | Loss: 0.00002743
Iteration 33/1000 | Loss: 0.00002743
Iteration 34/1000 | Loss: 0.00002743
Iteration 35/1000 | Loss: 0.00002743
Iteration 36/1000 | Loss: 0.00002743
Iteration 37/1000 | Loss: 0.00002742
Iteration 38/1000 | Loss: 0.00002742
Iteration 39/1000 | Loss: 0.00002742
Iteration 40/1000 | Loss: 0.00002740
Iteration 41/1000 | Loss: 0.00002740
Iteration 42/1000 | Loss: 0.00002740
Iteration 43/1000 | Loss: 0.00002740
Iteration 44/1000 | Loss: 0.00002739
Iteration 45/1000 | Loss: 0.00002739
Iteration 46/1000 | Loss: 0.00002739
Iteration 47/1000 | Loss: 0.00002739
Iteration 48/1000 | Loss: 0.00002739
Iteration 49/1000 | Loss: 0.00002739
Iteration 50/1000 | Loss: 0.00002738
Iteration 51/1000 | Loss: 0.00002738
Iteration 52/1000 | Loss: 0.00002738
Iteration 53/1000 | Loss: 0.00002738
Iteration 54/1000 | Loss: 0.00002737
Iteration 55/1000 | Loss: 0.00002737
Iteration 56/1000 | Loss: 0.00002737
Iteration 57/1000 | Loss: 0.00002737
Iteration 58/1000 | Loss: 0.00002737
Iteration 59/1000 | Loss: 0.00002736
Iteration 60/1000 | Loss: 0.00002736
Iteration 61/1000 | Loss: 0.00002736
Iteration 62/1000 | Loss: 0.00002736
Iteration 63/1000 | Loss: 0.00002736
Iteration 64/1000 | Loss: 0.00002736
Iteration 65/1000 | Loss: 0.00002736
Iteration 66/1000 | Loss: 0.00002736
Iteration 67/1000 | Loss: 0.00002736
Iteration 68/1000 | Loss: 0.00002736
Iteration 69/1000 | Loss: 0.00002735
Iteration 70/1000 | Loss: 0.00002735
Iteration 71/1000 | Loss: 0.00002735
Iteration 72/1000 | Loss: 0.00002735
Iteration 73/1000 | Loss: 0.00002734
Iteration 74/1000 | Loss: 0.00002734
Iteration 75/1000 | Loss: 0.00002734
Iteration 76/1000 | Loss: 0.00002733
Iteration 77/1000 | Loss: 0.00002733
Iteration 78/1000 | Loss: 0.00002733
Iteration 79/1000 | Loss: 0.00002733
Iteration 80/1000 | Loss: 0.00002733
Iteration 81/1000 | Loss: 0.00002733
Iteration 82/1000 | Loss: 0.00002733
Iteration 83/1000 | Loss: 0.00002733
Iteration 84/1000 | Loss: 0.00002733
Iteration 85/1000 | Loss: 0.00002733
Iteration 86/1000 | Loss: 0.00002733
Iteration 87/1000 | Loss: 0.00002733
Iteration 88/1000 | Loss: 0.00002733
Iteration 89/1000 | Loss: 0.00002733
Iteration 90/1000 | Loss: 0.00002733
Iteration 91/1000 | Loss: 0.00002733
Iteration 92/1000 | Loss: 0.00002733
Iteration 93/1000 | Loss: 0.00002733
Iteration 94/1000 | Loss: 0.00002733
Iteration 95/1000 | Loss: 0.00002733
Iteration 96/1000 | Loss: 0.00002733
Iteration 97/1000 | Loss: 0.00002733
Iteration 98/1000 | Loss: 0.00002733
Iteration 99/1000 | Loss: 0.00002733
Iteration 100/1000 | Loss: 0.00002733
Iteration 101/1000 | Loss: 0.00002733
Iteration 102/1000 | Loss: 0.00002733
Iteration 103/1000 | Loss: 0.00002733
Iteration 104/1000 | Loss: 0.00002733
Iteration 105/1000 | Loss: 0.00002733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.732895518420264e-05, 2.732895518420264e-05, 2.732895518420264e-05, 2.732895518420264e-05, 2.732895518420264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.732895518420264e-05

Optimization complete. Final v2v error: 4.402405261993408 mm

Highest mean error: 10.247029304504395 mm for frame 138

Lowest mean error: 4.174021244049072 mm for frame 154

Saving results

Total time: 53.3144154548645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440748
Iteration 2/25 | Loss: 0.00137740
Iteration 3/25 | Loss: 0.00130911
Iteration 4/25 | Loss: 0.00129884
Iteration 5/25 | Loss: 0.00129558
Iteration 6/25 | Loss: 0.00129427
Iteration 7/25 | Loss: 0.00129427
Iteration 8/25 | Loss: 0.00129427
Iteration 9/25 | Loss: 0.00129427
Iteration 10/25 | Loss: 0.00129427
Iteration 11/25 | Loss: 0.00129427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012942735338583589, 0.0012942735338583589, 0.0012942735338583589, 0.0012942735338583589, 0.0012942735338583589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012942735338583589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54759979
Iteration 2/25 | Loss: 0.00413619
Iteration 3/25 | Loss: 0.00413617
Iteration 4/25 | Loss: 0.00413617
Iteration 5/25 | Loss: 0.00413617
Iteration 6/25 | Loss: 0.00413617
Iteration 7/25 | Loss: 0.00413617
Iteration 8/25 | Loss: 0.00413617
Iteration 9/25 | Loss: 0.00413617
Iteration 10/25 | Loss: 0.00413617
Iteration 11/25 | Loss: 0.00413617
Iteration 12/25 | Loss: 0.00413617
Iteration 13/25 | Loss: 0.00413617
Iteration 14/25 | Loss: 0.00413617
Iteration 15/25 | Loss: 0.00413617
Iteration 16/25 | Loss: 0.00413617
Iteration 17/25 | Loss: 0.00413617
Iteration 18/25 | Loss: 0.00413617
Iteration 19/25 | Loss: 0.00413617
Iteration 20/25 | Loss: 0.00413617
Iteration 21/25 | Loss: 0.00413617
Iteration 22/25 | Loss: 0.00413617
Iteration 23/25 | Loss: 0.00413617
Iteration 24/25 | Loss: 0.00413617
Iteration 25/25 | Loss: 0.00413617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00413617
Iteration 2/1000 | Loss: 0.00005319
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002386
Iteration 5/1000 | Loss: 0.00002240
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002002
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001788
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001758
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001750
Iteration 18/1000 | Loss: 0.00001749
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001738
Iteration 22/1000 | Loss: 0.00001738
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001729
Iteration 26/1000 | Loss: 0.00001729
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001718
Iteration 63/1000 | Loss: 0.00001718
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001711
Iteration 119/1000 | Loss: 0.00001711
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001710
Iteration 126/1000 | Loss: 0.00001710
Iteration 127/1000 | Loss: 0.00001710
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00001710
Iteration 130/1000 | Loss: 0.00001710
Iteration 131/1000 | Loss: 0.00001710
Iteration 132/1000 | Loss: 0.00001710
Iteration 133/1000 | Loss: 0.00001710
Iteration 134/1000 | Loss: 0.00001710
Iteration 135/1000 | Loss: 0.00001710
Iteration 136/1000 | Loss: 0.00001710
Iteration 137/1000 | Loss: 0.00001710
Iteration 138/1000 | Loss: 0.00001710
Iteration 139/1000 | Loss: 0.00001710
Iteration 140/1000 | Loss: 0.00001710
Iteration 141/1000 | Loss: 0.00001710
Iteration 142/1000 | Loss: 0.00001710
Iteration 143/1000 | Loss: 0.00001710
Iteration 144/1000 | Loss: 0.00001710
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001710
Iteration 147/1000 | Loss: 0.00001710
Iteration 148/1000 | Loss: 0.00001710
Iteration 149/1000 | Loss: 0.00001710
Iteration 150/1000 | Loss: 0.00001710
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001710
Iteration 153/1000 | Loss: 0.00001710
Iteration 154/1000 | Loss: 0.00001710
Iteration 155/1000 | Loss: 0.00001710
Iteration 156/1000 | Loss: 0.00001710
Iteration 157/1000 | Loss: 0.00001710
Iteration 158/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.710199285298586e-05, 1.710199285298586e-05, 1.710199285298586e-05, 1.710199285298586e-05, 1.710199285298586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.710199285298586e-05

Optimization complete. Final v2v error: 3.5104854106903076 mm

Highest mean error: 3.93929386138916 mm for frame 118

Lowest mean error: 3.002470016479492 mm for frame 150

Saving results

Total time: 37.21475601196289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510349
Iteration 2/25 | Loss: 0.00162663
Iteration 3/25 | Loss: 0.00134606
Iteration 4/25 | Loss: 0.00132106
Iteration 5/25 | Loss: 0.00131702
Iteration 6/25 | Loss: 0.00131645
Iteration 7/25 | Loss: 0.00131645
Iteration 8/25 | Loss: 0.00131645
Iteration 9/25 | Loss: 0.00131645
Iteration 10/25 | Loss: 0.00131645
Iteration 11/25 | Loss: 0.00131645
Iteration 12/25 | Loss: 0.00131645
Iteration 13/25 | Loss: 0.00131645
Iteration 14/25 | Loss: 0.00131645
Iteration 15/25 | Loss: 0.00131645
Iteration 16/25 | Loss: 0.00131645
Iteration 17/25 | Loss: 0.00131645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013164486736059189, 0.0013164486736059189, 0.0013164486736059189, 0.0013164486736059189, 0.0013164486736059189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013164486736059189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06189835
Iteration 2/25 | Loss: 0.00371372
Iteration 3/25 | Loss: 0.00371372
Iteration 4/25 | Loss: 0.00371372
Iteration 5/25 | Loss: 0.00371372
Iteration 6/25 | Loss: 0.00371372
Iteration 7/25 | Loss: 0.00371372
Iteration 8/25 | Loss: 0.00371371
Iteration 9/25 | Loss: 0.00371371
Iteration 10/25 | Loss: 0.00371371
Iteration 11/25 | Loss: 0.00371371
Iteration 12/25 | Loss: 0.00371371
Iteration 13/25 | Loss: 0.00371371
Iteration 14/25 | Loss: 0.00371371
Iteration 15/25 | Loss: 0.00371371
Iteration 16/25 | Loss: 0.00371371
Iteration 17/25 | Loss: 0.00371371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0037137137260288, 0.0037137137260288, 0.0037137137260288, 0.0037137137260288, 0.0037137137260288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037137137260288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371371
Iteration 2/1000 | Loss: 0.00004791
Iteration 3/1000 | Loss: 0.00003543
Iteration 4/1000 | Loss: 0.00003358
Iteration 5/1000 | Loss: 0.00003219
Iteration 6/1000 | Loss: 0.00003134
Iteration 7/1000 | Loss: 0.00003058
Iteration 8/1000 | Loss: 0.00003012
Iteration 9/1000 | Loss: 0.00002979
Iteration 10/1000 | Loss: 0.00002971
Iteration 11/1000 | Loss: 0.00002955
Iteration 12/1000 | Loss: 0.00002941
Iteration 13/1000 | Loss: 0.00002925
Iteration 14/1000 | Loss: 0.00002914
Iteration 15/1000 | Loss: 0.00002910
Iteration 16/1000 | Loss: 0.00002899
Iteration 17/1000 | Loss: 0.00002896
Iteration 18/1000 | Loss: 0.00002896
Iteration 19/1000 | Loss: 0.00002894
Iteration 20/1000 | Loss: 0.00002893
Iteration 21/1000 | Loss: 0.00002893
Iteration 22/1000 | Loss: 0.00002890
Iteration 23/1000 | Loss: 0.00002881
Iteration 24/1000 | Loss: 0.00002877
Iteration 25/1000 | Loss: 0.00002871
Iteration 26/1000 | Loss: 0.00002866
Iteration 27/1000 | Loss: 0.00002866
Iteration 28/1000 | Loss: 0.00002866
Iteration 29/1000 | Loss: 0.00002865
Iteration 30/1000 | Loss: 0.00002865
Iteration 31/1000 | Loss: 0.00002853
Iteration 32/1000 | Loss: 0.00002849
Iteration 33/1000 | Loss: 0.00002848
Iteration 34/1000 | Loss: 0.00002848
Iteration 35/1000 | Loss: 0.00002847
Iteration 36/1000 | Loss: 0.00002847
Iteration 37/1000 | Loss: 0.00002847
Iteration 38/1000 | Loss: 0.00002843
Iteration 39/1000 | Loss: 0.00002843
Iteration 40/1000 | Loss: 0.00002842
Iteration 41/1000 | Loss: 0.00002840
Iteration 42/1000 | Loss: 0.00002839
Iteration 43/1000 | Loss: 0.00002838
Iteration 44/1000 | Loss: 0.00002838
Iteration 45/1000 | Loss: 0.00002838
Iteration 46/1000 | Loss: 0.00002838
Iteration 47/1000 | Loss: 0.00002838
Iteration 48/1000 | Loss: 0.00002838
Iteration 49/1000 | Loss: 0.00002838
Iteration 50/1000 | Loss: 0.00002838
Iteration 51/1000 | Loss: 0.00002837
Iteration 52/1000 | Loss: 0.00002836
Iteration 53/1000 | Loss: 0.00002836
Iteration 54/1000 | Loss: 0.00002835
Iteration 55/1000 | Loss: 0.00002834
Iteration 56/1000 | Loss: 0.00002834
Iteration 57/1000 | Loss: 0.00002834
Iteration 58/1000 | Loss: 0.00002834
Iteration 59/1000 | Loss: 0.00002834
Iteration 60/1000 | Loss: 0.00002834
Iteration 61/1000 | Loss: 0.00002834
Iteration 62/1000 | Loss: 0.00002834
Iteration 63/1000 | Loss: 0.00002834
Iteration 64/1000 | Loss: 0.00002834
Iteration 65/1000 | Loss: 0.00002834
Iteration 66/1000 | Loss: 0.00002834
Iteration 67/1000 | Loss: 0.00002834
Iteration 68/1000 | Loss: 0.00002834
Iteration 69/1000 | Loss: 0.00002834
Iteration 70/1000 | Loss: 0.00002833
Iteration 71/1000 | Loss: 0.00002833
Iteration 72/1000 | Loss: 0.00002833
Iteration 73/1000 | Loss: 0.00002833
Iteration 74/1000 | Loss: 0.00002833
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002832
Iteration 77/1000 | Loss: 0.00002832
Iteration 78/1000 | Loss: 0.00002832
Iteration 79/1000 | Loss: 0.00002832
Iteration 80/1000 | Loss: 0.00002832
Iteration 81/1000 | Loss: 0.00002832
Iteration 82/1000 | Loss: 0.00002831
Iteration 83/1000 | Loss: 0.00002831
Iteration 84/1000 | Loss: 0.00002831
Iteration 85/1000 | Loss: 0.00002831
Iteration 86/1000 | Loss: 0.00002831
Iteration 87/1000 | Loss: 0.00002831
Iteration 88/1000 | Loss: 0.00002831
Iteration 89/1000 | Loss: 0.00002831
Iteration 90/1000 | Loss: 0.00002831
Iteration 91/1000 | Loss: 0.00002831
Iteration 92/1000 | Loss: 0.00002831
Iteration 93/1000 | Loss: 0.00002831
Iteration 94/1000 | Loss: 0.00002830
Iteration 95/1000 | Loss: 0.00002829
Iteration 96/1000 | Loss: 0.00002829
Iteration 97/1000 | Loss: 0.00002829
Iteration 98/1000 | Loss: 0.00002829
Iteration 99/1000 | Loss: 0.00002829
Iteration 100/1000 | Loss: 0.00002829
Iteration 101/1000 | Loss: 0.00002829
Iteration 102/1000 | Loss: 0.00002829
Iteration 103/1000 | Loss: 0.00002828
Iteration 104/1000 | Loss: 0.00002828
Iteration 105/1000 | Loss: 0.00002828
Iteration 106/1000 | Loss: 0.00002828
Iteration 107/1000 | Loss: 0.00002828
Iteration 108/1000 | Loss: 0.00002828
Iteration 109/1000 | Loss: 0.00002827
Iteration 110/1000 | Loss: 0.00002827
Iteration 111/1000 | Loss: 0.00002827
Iteration 112/1000 | Loss: 0.00002827
Iteration 113/1000 | Loss: 0.00002827
Iteration 114/1000 | Loss: 0.00002827
Iteration 115/1000 | Loss: 0.00002827
Iteration 116/1000 | Loss: 0.00002826
Iteration 117/1000 | Loss: 0.00002826
Iteration 118/1000 | Loss: 0.00002826
Iteration 119/1000 | Loss: 0.00002826
Iteration 120/1000 | Loss: 0.00002826
Iteration 121/1000 | Loss: 0.00002826
Iteration 122/1000 | Loss: 0.00002826
Iteration 123/1000 | Loss: 0.00002826
Iteration 124/1000 | Loss: 0.00002825
Iteration 125/1000 | Loss: 0.00002825
Iteration 126/1000 | Loss: 0.00002825
Iteration 127/1000 | Loss: 0.00002825
Iteration 128/1000 | Loss: 0.00002825
Iteration 129/1000 | Loss: 0.00002824
Iteration 130/1000 | Loss: 0.00002824
Iteration 131/1000 | Loss: 0.00002824
Iteration 132/1000 | Loss: 0.00002824
Iteration 133/1000 | Loss: 0.00002824
Iteration 134/1000 | Loss: 0.00002824
Iteration 135/1000 | Loss: 0.00002824
Iteration 136/1000 | Loss: 0.00002824
Iteration 137/1000 | Loss: 0.00002824
Iteration 138/1000 | Loss: 0.00002824
Iteration 139/1000 | Loss: 0.00002824
Iteration 140/1000 | Loss: 0.00002824
Iteration 141/1000 | Loss: 0.00002824
Iteration 142/1000 | Loss: 0.00002824
Iteration 143/1000 | Loss: 0.00002824
Iteration 144/1000 | Loss: 0.00002824
Iteration 145/1000 | Loss: 0.00002824
Iteration 146/1000 | Loss: 0.00002824
Iteration 147/1000 | Loss: 0.00002824
Iteration 148/1000 | Loss: 0.00002824
Iteration 149/1000 | Loss: 0.00002824
Iteration 150/1000 | Loss: 0.00002824
Iteration 151/1000 | Loss: 0.00002824
Iteration 152/1000 | Loss: 0.00002824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.8240418032510206e-05, 2.8240418032510206e-05, 2.8240418032510206e-05, 2.8240418032510206e-05, 2.8240418032510206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8240418032510206e-05

Optimization complete. Final v2v error: 4.246791362762451 mm

Highest mean error: 4.634223937988281 mm for frame 159

Lowest mean error: 3.8136491775512695 mm for frame 79

Saving results

Total time: 49.8742151260376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474818
Iteration 2/25 | Loss: 0.00171025
Iteration 3/25 | Loss: 0.00150620
Iteration 4/25 | Loss: 0.00144935
Iteration 5/25 | Loss: 0.00142166
Iteration 6/25 | Loss: 0.00142020
Iteration 7/25 | Loss: 0.00138609
Iteration 8/25 | Loss: 0.00138112
Iteration 9/25 | Loss: 0.00136962
Iteration 10/25 | Loss: 0.00136531
Iteration 11/25 | Loss: 0.00135186
Iteration 12/25 | Loss: 0.00136011
Iteration 13/25 | Loss: 0.00135932
Iteration 14/25 | Loss: 0.00135435
Iteration 15/25 | Loss: 0.00134630
Iteration 16/25 | Loss: 0.00135202
Iteration 17/25 | Loss: 0.00135291
Iteration 18/25 | Loss: 0.00134933
Iteration 19/25 | Loss: 0.00134950
Iteration 20/25 | Loss: 0.00134662
Iteration 21/25 | Loss: 0.00134812
Iteration 22/25 | Loss: 0.00134517
Iteration 23/25 | Loss: 0.00134803
Iteration 24/25 | Loss: 0.00134637
Iteration 25/25 | Loss: 0.00135002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13214493
Iteration 2/25 | Loss: 0.00426058
Iteration 3/25 | Loss: 0.00426058
Iteration 4/25 | Loss: 0.00426058
Iteration 5/25 | Loss: 0.00426058
Iteration 6/25 | Loss: 0.00426058
Iteration 7/25 | Loss: 0.00426058
Iteration 8/25 | Loss: 0.00426058
Iteration 9/25 | Loss: 0.00426058
Iteration 10/25 | Loss: 0.00426058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.004260575398802757, 0.004260575398802757, 0.004260575398802757, 0.004260575398802757, 0.004260575398802757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004260575398802757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00426058
Iteration 2/1000 | Loss: 0.00017386
Iteration 3/1000 | Loss: 0.00023740
Iteration 4/1000 | Loss: 0.00019357
Iteration 5/1000 | Loss: 0.00021149
Iteration 6/1000 | Loss: 0.00025760
Iteration 7/1000 | Loss: 0.00028027
Iteration 8/1000 | Loss: 0.00013105
Iteration 9/1000 | Loss: 0.00015234
Iteration 10/1000 | Loss: 0.00020319
Iteration 11/1000 | Loss: 0.00017762
Iteration 12/1000 | Loss: 0.00018416
Iteration 13/1000 | Loss: 0.00018957
Iteration 14/1000 | Loss: 0.00019830
Iteration 15/1000 | Loss: 0.00018330
Iteration 16/1000 | Loss: 0.00019802
Iteration 17/1000 | Loss: 0.00013375
Iteration 18/1000 | Loss: 0.00019817
Iteration 19/1000 | Loss: 0.00024431
Iteration 20/1000 | Loss: 0.00017720
Iteration 21/1000 | Loss: 0.00018097
Iteration 22/1000 | Loss: 0.00014512
Iteration 23/1000 | Loss: 0.00017368
Iteration 24/1000 | Loss: 0.00019220
Iteration 25/1000 | Loss: 0.00022876
Iteration 26/1000 | Loss: 0.00023427
Iteration 27/1000 | Loss: 0.00023784
Iteration 28/1000 | Loss: 0.00025909
Iteration 29/1000 | Loss: 0.00014982
Iteration 30/1000 | Loss: 0.00014255
Iteration 31/1000 | Loss: 0.00016547
Iteration 32/1000 | Loss: 0.00011195
Iteration 33/1000 | Loss: 0.00019874
Iteration 34/1000 | Loss: 0.00022948
Iteration 35/1000 | Loss: 0.00020472
Iteration 36/1000 | Loss: 0.00013243
Iteration 37/1000 | Loss: 0.00006333
Iteration 38/1000 | Loss: 0.00008141
Iteration 39/1000 | Loss: 0.00009813
Iteration 40/1000 | Loss: 0.00011977
Iteration 41/1000 | Loss: 0.00014733
Iteration 42/1000 | Loss: 0.00006720
Iteration 43/1000 | Loss: 0.00013232
Iteration 44/1000 | Loss: 0.00013984
Iteration 45/1000 | Loss: 0.00010546
Iteration 46/1000 | Loss: 0.00014359
Iteration 47/1000 | Loss: 0.00017504
Iteration 48/1000 | Loss: 0.00012518
Iteration 49/1000 | Loss: 0.00016411
Iteration 50/1000 | Loss: 0.00015103
Iteration 51/1000 | Loss: 0.00012910
Iteration 52/1000 | Loss: 0.00015752
Iteration 53/1000 | Loss: 0.00014355
Iteration 54/1000 | Loss: 0.00016253
Iteration 55/1000 | Loss: 0.00009686
Iteration 56/1000 | Loss: 0.00012581
Iteration 57/1000 | Loss: 0.00013327
Iteration 58/1000 | Loss: 0.00011104
Iteration 59/1000 | Loss: 0.00012119
Iteration 60/1000 | Loss: 0.00009831
Iteration 61/1000 | Loss: 0.00010911
Iteration 62/1000 | Loss: 0.00014477
Iteration 63/1000 | Loss: 0.00015055
Iteration 64/1000 | Loss: 0.00011280
Iteration 65/1000 | Loss: 0.00010892
Iteration 66/1000 | Loss: 0.00013406
Iteration 67/1000 | Loss: 0.00011370
Iteration 68/1000 | Loss: 0.00012062
Iteration 69/1000 | Loss: 0.00010794
Iteration 70/1000 | Loss: 0.00010361
Iteration 71/1000 | Loss: 0.00012107
Iteration 72/1000 | Loss: 0.00013786
Iteration 73/1000 | Loss: 0.00012046
Iteration 74/1000 | Loss: 0.00013568
Iteration 75/1000 | Loss: 0.00013736
Iteration 76/1000 | Loss: 0.00013289
Iteration 77/1000 | Loss: 0.00014538
Iteration 78/1000 | Loss: 0.00013113
Iteration 79/1000 | Loss: 0.00016820
Iteration 80/1000 | Loss: 0.00010229
Iteration 81/1000 | Loss: 0.00013176
Iteration 82/1000 | Loss: 0.00014009
Iteration 83/1000 | Loss: 0.00011174
Iteration 84/1000 | Loss: 0.00013038
Iteration 85/1000 | Loss: 0.00012156
Iteration 86/1000 | Loss: 0.00012294
Iteration 87/1000 | Loss: 0.00010432
Iteration 88/1000 | Loss: 0.00012556
Iteration 89/1000 | Loss: 0.00010554
Iteration 90/1000 | Loss: 0.00009346
Iteration 91/1000 | Loss: 0.00010594
Iteration 92/1000 | Loss: 0.00009265
Iteration 93/1000 | Loss: 0.00011941
Iteration 94/1000 | Loss: 0.00011324
Iteration 95/1000 | Loss: 0.00013383
Iteration 96/1000 | Loss: 0.00013333
Iteration 97/1000 | Loss: 0.00013717
Iteration 98/1000 | Loss: 0.00014270
Iteration 99/1000 | Loss: 0.00012145
Iteration 100/1000 | Loss: 0.00012807
Iteration 101/1000 | Loss: 0.00013871
Iteration 102/1000 | Loss: 0.00013060
Iteration 103/1000 | Loss: 0.00013978
Iteration 104/1000 | Loss: 0.00013259
Iteration 105/1000 | Loss: 0.00012742
Iteration 106/1000 | Loss: 0.00013350
Iteration 107/1000 | Loss: 0.00013191
Iteration 108/1000 | Loss: 0.00013531
Iteration 109/1000 | Loss: 0.00013285
Iteration 110/1000 | Loss: 0.00017295
Iteration 111/1000 | Loss: 0.00016424
Iteration 112/1000 | Loss: 0.00007541
Iteration 113/1000 | Loss: 0.00008136
Iteration 114/1000 | Loss: 0.00012062
Iteration 115/1000 | Loss: 0.00007966
Iteration 116/1000 | Loss: 0.00009474
Iteration 117/1000 | Loss: 0.00006364
Iteration 118/1000 | Loss: 0.00008757
Iteration 119/1000 | Loss: 0.00010262
Iteration 120/1000 | Loss: 0.00006310
Iteration 121/1000 | Loss: 0.00005335
Iteration 122/1000 | Loss: 0.00009024
Iteration 123/1000 | Loss: 0.00008313
Iteration 124/1000 | Loss: 0.00008640
Iteration 125/1000 | Loss: 0.00007104
Iteration 126/1000 | Loss: 0.00009089
Iteration 127/1000 | Loss: 0.00007963
Iteration 128/1000 | Loss: 0.00009399
Iteration 129/1000 | Loss: 0.00007523
Iteration 130/1000 | Loss: 0.00009699
Iteration 131/1000 | Loss: 0.00009221
Iteration 132/1000 | Loss: 0.00010496
Iteration 133/1000 | Loss: 0.00008782
Iteration 134/1000 | Loss: 0.00010310
Iteration 135/1000 | Loss: 0.00006666
Iteration 136/1000 | Loss: 0.00005503
Iteration 137/1000 | Loss: 0.00009634
Iteration 138/1000 | Loss: 0.00011002
Iteration 139/1000 | Loss: 0.00010575
Iteration 140/1000 | Loss: 0.00010396
Iteration 141/1000 | Loss: 0.00009886
Iteration 142/1000 | Loss: 0.00009282
Iteration 143/1000 | Loss: 0.00008395
Iteration 144/1000 | Loss: 0.00008158
Iteration 145/1000 | Loss: 0.00008364
Iteration 146/1000 | Loss: 0.00010102
Iteration 147/1000 | Loss: 0.00007502
Iteration 148/1000 | Loss: 0.00009177
Iteration 149/1000 | Loss: 0.00007516
Iteration 150/1000 | Loss: 0.00011020
Iteration 151/1000 | Loss: 0.00010769
Iteration 152/1000 | Loss: 0.00010359
Iteration 153/1000 | Loss: 0.00009514
Iteration 154/1000 | Loss: 0.00009996
Iteration 155/1000 | Loss: 0.00009996
Iteration 156/1000 | Loss: 0.00009717
Iteration 157/1000 | Loss: 0.00008620
Iteration 158/1000 | Loss: 0.00009113
Iteration 159/1000 | Loss: 0.00008426
Iteration 160/1000 | Loss: 0.00010548
Iteration 161/1000 | Loss: 0.00006392
Iteration 162/1000 | Loss: 0.00007780
Iteration 163/1000 | Loss: 0.00007459
Iteration 164/1000 | Loss: 0.00009869
Iteration 165/1000 | Loss: 0.00008874
Iteration 166/1000 | Loss: 0.00009087
Iteration 167/1000 | Loss: 0.00008997
Iteration 168/1000 | Loss: 0.00010233
Iteration 169/1000 | Loss: 0.00010245
Iteration 170/1000 | Loss: 0.00009969
Iteration 171/1000 | Loss: 0.00009388
Iteration 172/1000 | Loss: 0.00009682
Iteration 173/1000 | Loss: 0.00008535
Iteration 174/1000 | Loss: 0.00009240
Iteration 175/1000 | Loss: 0.00007505
Iteration 176/1000 | Loss: 0.00006024
Iteration 177/1000 | Loss: 0.00007550
Iteration 178/1000 | Loss: 0.00005372
Iteration 179/1000 | Loss: 0.00008166
Iteration 180/1000 | Loss: 0.00010099
Iteration 181/1000 | Loss: 0.00007698
Iteration 182/1000 | Loss: 0.00006218
Iteration 183/1000 | Loss: 0.00006897
Iteration 184/1000 | Loss: 0.00010679
Iteration 185/1000 | Loss: 0.00008470
Iteration 186/1000 | Loss: 0.00007953
Iteration 187/1000 | Loss: 0.00009377
Iteration 188/1000 | Loss: 0.00008759
Iteration 189/1000 | Loss: 0.00008959
Iteration 190/1000 | Loss: 0.00007958
Iteration 191/1000 | Loss: 0.00006678
Iteration 192/1000 | Loss: 0.00005107
Iteration 193/1000 | Loss: 0.00002333
Iteration 194/1000 | Loss: 0.00006346
Iteration 195/1000 | Loss: 0.00008767
Iteration 196/1000 | Loss: 0.00006852
Iteration 197/1000 | Loss: 0.00008834
Iteration 198/1000 | Loss: 0.00006093
Iteration 199/1000 | Loss: 0.00008729
Iteration 200/1000 | Loss: 0.00005669
Iteration 201/1000 | Loss: 0.00008880
Iteration 202/1000 | Loss: 0.00005546
Iteration 203/1000 | Loss: 0.00008939
Iteration 204/1000 | Loss: 0.00006497
Iteration 205/1000 | Loss: 0.00008927
Iteration 206/1000 | Loss: 0.00007901
Iteration 207/1000 | Loss: 0.00006940
Iteration 208/1000 | Loss: 0.00009618
Iteration 209/1000 | Loss: 0.00006143
Iteration 210/1000 | Loss: 0.00009283
Iteration 211/1000 | Loss: 0.00006556
Iteration 212/1000 | Loss: 0.00006727
Iteration 213/1000 | Loss: 0.00008185
Iteration 214/1000 | Loss: 0.00003037
Iteration 215/1000 | Loss: 0.00004548
Iteration 216/1000 | Loss: 0.00005380
Iteration 217/1000 | Loss: 0.00004085
Iteration 218/1000 | Loss: 0.00003562
Iteration 219/1000 | Loss: 0.00005064
Iteration 220/1000 | Loss: 0.00002631
Iteration 221/1000 | Loss: 0.00003535
Iteration 222/1000 | Loss: 0.00006326
Iteration 223/1000 | Loss: 0.00003870
Iteration 224/1000 | Loss: 0.00002138
Iteration 225/1000 | Loss: 0.00005590
Iteration 226/1000 | Loss: 0.00001903
Iteration 227/1000 | Loss: 0.00001638
Iteration 228/1000 | Loss: 0.00001503
Iteration 229/1000 | Loss: 0.00001447
Iteration 230/1000 | Loss: 0.00001404
Iteration 231/1000 | Loss: 0.00001376
Iteration 232/1000 | Loss: 0.00001349
Iteration 233/1000 | Loss: 0.00001334
Iteration 234/1000 | Loss: 0.00001330
Iteration 235/1000 | Loss: 0.00001325
Iteration 236/1000 | Loss: 0.00001320
Iteration 237/1000 | Loss: 0.00001316
Iteration 238/1000 | Loss: 0.00001313
Iteration 239/1000 | Loss: 0.00001312
Iteration 240/1000 | Loss: 0.00001310
Iteration 241/1000 | Loss: 0.00001309
Iteration 242/1000 | Loss: 0.00001308
Iteration 243/1000 | Loss: 0.00001307
Iteration 244/1000 | Loss: 0.00001306
Iteration 245/1000 | Loss: 0.00001303
Iteration 246/1000 | Loss: 0.00001303
Iteration 247/1000 | Loss: 0.00001303
Iteration 248/1000 | Loss: 0.00001302
Iteration 249/1000 | Loss: 0.00001302
Iteration 250/1000 | Loss: 0.00001302
Iteration 251/1000 | Loss: 0.00001302
Iteration 252/1000 | Loss: 0.00001302
Iteration 253/1000 | Loss: 0.00001301
Iteration 254/1000 | Loss: 0.00001301
Iteration 255/1000 | Loss: 0.00001301
Iteration 256/1000 | Loss: 0.00001301
Iteration 257/1000 | Loss: 0.00001300
Iteration 258/1000 | Loss: 0.00001300
Iteration 259/1000 | Loss: 0.00001299
Iteration 260/1000 | Loss: 0.00001299
Iteration 261/1000 | Loss: 0.00001299
Iteration 262/1000 | Loss: 0.00001299
Iteration 263/1000 | Loss: 0.00001299
Iteration 264/1000 | Loss: 0.00001299
Iteration 265/1000 | Loss: 0.00001299
Iteration 266/1000 | Loss: 0.00001298
Iteration 267/1000 | Loss: 0.00001298
Iteration 268/1000 | Loss: 0.00001298
Iteration 269/1000 | Loss: 0.00001298
Iteration 270/1000 | Loss: 0.00001298
Iteration 271/1000 | Loss: 0.00001298
Iteration 272/1000 | Loss: 0.00001297
Iteration 273/1000 | Loss: 0.00001297
Iteration 274/1000 | Loss: 0.00001297
Iteration 275/1000 | Loss: 0.00001296
Iteration 276/1000 | Loss: 0.00001296
Iteration 277/1000 | Loss: 0.00001296
Iteration 278/1000 | Loss: 0.00001295
Iteration 279/1000 | Loss: 0.00001295
Iteration 280/1000 | Loss: 0.00001295
Iteration 281/1000 | Loss: 0.00001295
Iteration 282/1000 | Loss: 0.00001295
Iteration 283/1000 | Loss: 0.00001295
Iteration 284/1000 | Loss: 0.00001295
Iteration 285/1000 | Loss: 0.00001295
Iteration 286/1000 | Loss: 0.00001295
Iteration 287/1000 | Loss: 0.00001295
Iteration 288/1000 | Loss: 0.00001295
Iteration 289/1000 | Loss: 0.00001294
Iteration 290/1000 | Loss: 0.00001294
Iteration 291/1000 | Loss: 0.00001294
Iteration 292/1000 | Loss: 0.00001294
Iteration 293/1000 | Loss: 0.00001294
Iteration 294/1000 | Loss: 0.00001294
Iteration 295/1000 | Loss: 0.00001294
Iteration 296/1000 | Loss: 0.00001294
Iteration 297/1000 | Loss: 0.00001294
Iteration 298/1000 | Loss: 0.00001294
Iteration 299/1000 | Loss: 0.00001294
Iteration 300/1000 | Loss: 0.00001294
Iteration 301/1000 | Loss: 0.00001294
Iteration 302/1000 | Loss: 0.00001294
Iteration 303/1000 | Loss: 0.00001294
Iteration 304/1000 | Loss: 0.00001294
Iteration 305/1000 | Loss: 0.00001294
Iteration 306/1000 | Loss: 0.00001294
Iteration 307/1000 | Loss: 0.00001294
Iteration 308/1000 | Loss: 0.00001294
Iteration 309/1000 | Loss: 0.00001294
Iteration 310/1000 | Loss: 0.00001294
Iteration 311/1000 | Loss: 0.00001294
Iteration 312/1000 | Loss: 0.00001294
Iteration 313/1000 | Loss: 0.00001294
Iteration 314/1000 | Loss: 0.00001294
Iteration 315/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.2939341104356572e-05, 1.2939341104356572e-05, 1.2939341104356572e-05, 1.2939341104356572e-05, 1.2939341104356572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2939341104356572e-05

Optimization complete. Final v2v error: 3.057234525680542 mm

Highest mean error: 3.526505470275879 mm for frame 105

Lowest mean error: 2.7790839672088623 mm for frame 65

Saving results

Total time: 364.0121831893921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5702/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5702/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395969
Iteration 2/25 | Loss: 0.00134641
Iteration 3/25 | Loss: 0.00126563
Iteration 4/25 | Loss: 0.00125109
Iteration 5/25 | Loss: 0.00124482
Iteration 6/25 | Loss: 0.00124261
Iteration 7/25 | Loss: 0.00124259
Iteration 8/25 | Loss: 0.00124259
Iteration 9/25 | Loss: 0.00124259
Iteration 10/25 | Loss: 0.00124259
Iteration 11/25 | Loss: 0.00124259
Iteration 12/25 | Loss: 0.00124259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012425946770235896, 0.0012425946770235896, 0.0012425946770235896, 0.0012425946770235896, 0.0012425946770235896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012425946770235896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63927031
Iteration 2/25 | Loss: 0.00413568
Iteration 3/25 | Loss: 0.00413568
Iteration 4/25 | Loss: 0.00413568
Iteration 5/25 | Loss: 0.00413568
Iteration 6/25 | Loss: 0.00413568
Iteration 7/25 | Loss: 0.00413568
Iteration 8/25 | Loss: 0.00413568
Iteration 9/25 | Loss: 0.00413568
Iteration 10/25 | Loss: 0.00413568
Iteration 11/25 | Loss: 0.00413568
Iteration 12/25 | Loss: 0.00413568
Iteration 13/25 | Loss: 0.00413568
Iteration 14/25 | Loss: 0.00413568
Iteration 15/25 | Loss: 0.00413568
Iteration 16/25 | Loss: 0.00413568
Iteration 17/25 | Loss: 0.00413568
Iteration 18/25 | Loss: 0.00413568
Iteration 19/25 | Loss: 0.00413568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004135678056627512, 0.004135678056627512, 0.004135678056627512, 0.004135678056627512, 0.004135678056627512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004135678056627512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00413568
Iteration 2/1000 | Loss: 0.00002688
Iteration 3/1000 | Loss: 0.00002027
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001611
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001504
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001455
Iteration 13/1000 | Loss: 0.00001440
Iteration 14/1000 | Loss: 0.00001439
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001438
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001417
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001416
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001413
Iteration 46/1000 | Loss: 0.00001413
Iteration 47/1000 | Loss: 0.00001413
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001412
Iteration 52/1000 | Loss: 0.00001412
Iteration 53/1000 | Loss: 0.00001412
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001411
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001411
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.4108186405792367e-05, 1.4108186405792367e-05, 1.4108186405792367e-05, 1.4108186405792367e-05, 1.4108186405792367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4108186405792367e-05

Optimization complete. Final v2v error: 3.152411699295044 mm

Highest mean error: 3.577302932739258 mm for frame 25

Lowest mean error: 2.869497776031494 mm for frame 58

Saving results

Total time: 30.931699752807617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952071
Iteration 2/25 | Loss: 0.00952071
Iteration 3/25 | Loss: 0.00952071
Iteration 4/25 | Loss: 0.00952071
Iteration 5/25 | Loss: 0.00952071
Iteration 6/25 | Loss: 0.00952070
Iteration 7/25 | Loss: 0.00952070
Iteration 8/25 | Loss: 0.00952070
Iteration 9/25 | Loss: 0.00952070
Iteration 10/25 | Loss: 0.00952070
Iteration 11/25 | Loss: 0.00952070
Iteration 12/25 | Loss: 0.00952070
Iteration 13/25 | Loss: 0.00952070
Iteration 14/25 | Loss: 0.00952070
Iteration 15/25 | Loss: 0.00952069
Iteration 16/25 | Loss: 0.00952069
Iteration 17/25 | Loss: 0.00952069
Iteration 18/25 | Loss: 0.00952069
Iteration 19/25 | Loss: 0.00952069
Iteration 20/25 | Loss: 0.00952069
Iteration 21/25 | Loss: 0.00952069
Iteration 22/25 | Loss: 0.00952068
Iteration 23/25 | Loss: 0.00952068
Iteration 24/25 | Loss: 0.00952068
Iteration 25/25 | Loss: 0.00952068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72894430
Iteration 2/25 | Loss: 0.18225300
Iteration 3/25 | Loss: 0.18031782
Iteration 4/25 | Loss: 0.17817494
Iteration 5/25 | Loss: 0.17800204
Iteration 6/25 | Loss: 0.17803833
Iteration 7/25 | Loss: 0.17803904
Iteration 8/25 | Loss: 0.17796531
Iteration 9/25 | Loss: 0.17796527
Iteration 10/25 | Loss: 0.17796525
Iteration 11/25 | Loss: 0.17796525
Iteration 12/25 | Loss: 0.17796524
Iteration 13/25 | Loss: 0.17796524
Iteration 14/25 | Loss: 0.17796524
Iteration 15/25 | Loss: 0.17796524
Iteration 16/25 | Loss: 0.17796524
Iteration 17/25 | Loss: 0.17796524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.17796523869037628, 0.17796523869037628, 0.17796523869037628, 0.17796523869037628, 0.17796523869037628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17796523869037628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17796524
Iteration 2/1000 | Loss: 0.02000751
Iteration 3/1000 | Loss: 0.00145241
Iteration 4/1000 | Loss: 0.00075639
Iteration 5/1000 | Loss: 0.00038897
Iteration 6/1000 | Loss: 0.00023895
Iteration 7/1000 | Loss: 0.00015669
Iteration 8/1000 | Loss: 0.00013701
Iteration 9/1000 | Loss: 0.00011879
Iteration 10/1000 | Loss: 0.00009968
Iteration 11/1000 | Loss: 0.00008661
Iteration 12/1000 | Loss: 0.00007579
Iteration 13/1000 | Loss: 0.00006747
Iteration 14/1000 | Loss: 0.00005810
Iteration 15/1000 | Loss: 0.00005069
Iteration 16/1000 | Loss: 0.00004318
Iteration 17/1000 | Loss: 0.00003902
Iteration 18/1000 | Loss: 0.00003559
Iteration 19/1000 | Loss: 0.00003339
Iteration 20/1000 | Loss: 0.00003111
Iteration 21/1000 | Loss: 0.00002973
Iteration 22/1000 | Loss: 0.00002890
Iteration 23/1000 | Loss: 0.00002807
Iteration 24/1000 | Loss: 0.00002747
Iteration 25/1000 | Loss: 0.00002698
Iteration 26/1000 | Loss: 0.00002650
Iteration 27/1000 | Loss: 0.00002604
Iteration 28/1000 | Loss: 0.00002574
Iteration 29/1000 | Loss: 0.00002565
Iteration 30/1000 | Loss: 0.00002541
Iteration 31/1000 | Loss: 0.00002522
Iteration 32/1000 | Loss: 0.00002522
Iteration 33/1000 | Loss: 0.00002513
Iteration 34/1000 | Loss: 0.00002508
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002507
Iteration 37/1000 | Loss: 0.00002506
Iteration 38/1000 | Loss: 0.00002504
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002503
Iteration 41/1000 | Loss: 0.00002502
Iteration 42/1000 | Loss: 0.00002502
Iteration 43/1000 | Loss: 0.00002501
Iteration 44/1000 | Loss: 0.00002500
Iteration 45/1000 | Loss: 0.00002499
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00002497
Iteration 49/1000 | Loss: 0.00002495
Iteration 50/1000 | Loss: 0.00002494
Iteration 51/1000 | Loss: 0.00002494
Iteration 52/1000 | Loss: 0.00002493
Iteration 53/1000 | Loss: 0.00002493
Iteration 54/1000 | Loss: 0.00002493
Iteration 55/1000 | Loss: 0.00002492
Iteration 56/1000 | Loss: 0.00002492
Iteration 57/1000 | Loss: 0.00002491
Iteration 58/1000 | Loss: 0.00002482
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00002480
Iteration 61/1000 | Loss: 0.00002480
Iteration 62/1000 | Loss: 0.00002479
Iteration 63/1000 | Loss: 0.00002479
Iteration 64/1000 | Loss: 0.00002478
Iteration 65/1000 | Loss: 0.00002476
Iteration 66/1000 | Loss: 0.00002476
Iteration 67/1000 | Loss: 0.00002475
Iteration 68/1000 | Loss: 0.00002475
Iteration 69/1000 | Loss: 0.00002475
Iteration 70/1000 | Loss: 0.00002475
Iteration 71/1000 | Loss: 0.00002475
Iteration 72/1000 | Loss: 0.00002475
Iteration 73/1000 | Loss: 0.00002475
Iteration 74/1000 | Loss: 0.00002474
Iteration 75/1000 | Loss: 0.00002474
Iteration 76/1000 | Loss: 0.00002472
Iteration 77/1000 | Loss: 0.00002472
Iteration 78/1000 | Loss: 0.00002472
Iteration 79/1000 | Loss: 0.00002472
Iteration 80/1000 | Loss: 0.00002472
Iteration 81/1000 | Loss: 0.00002471
Iteration 82/1000 | Loss: 0.00002471
Iteration 83/1000 | Loss: 0.00002471
Iteration 84/1000 | Loss: 0.00002471
Iteration 85/1000 | Loss: 0.00002471
Iteration 86/1000 | Loss: 0.00002471
Iteration 87/1000 | Loss: 0.00002471
Iteration 88/1000 | Loss: 0.00002471
Iteration 89/1000 | Loss: 0.00002471
Iteration 90/1000 | Loss: 0.00002471
Iteration 91/1000 | Loss: 0.00002470
Iteration 92/1000 | Loss: 0.00002469
Iteration 93/1000 | Loss: 0.00002469
Iteration 94/1000 | Loss: 0.00002469
Iteration 95/1000 | Loss: 0.00002468
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002468
Iteration 99/1000 | Loss: 0.00002468
Iteration 100/1000 | Loss: 0.00002468
Iteration 101/1000 | Loss: 0.00002467
Iteration 102/1000 | Loss: 0.00002467
Iteration 103/1000 | Loss: 0.00002467
Iteration 104/1000 | Loss: 0.00002467
Iteration 105/1000 | Loss: 0.00002467
Iteration 106/1000 | Loss: 0.00002467
Iteration 107/1000 | Loss: 0.00002467
Iteration 108/1000 | Loss: 0.00002466
Iteration 109/1000 | Loss: 0.00002466
Iteration 110/1000 | Loss: 0.00002466
Iteration 111/1000 | Loss: 0.00002466
Iteration 112/1000 | Loss: 0.00002466
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002466
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002465
Iteration 119/1000 | Loss: 0.00002465
Iteration 120/1000 | Loss: 0.00002464
Iteration 121/1000 | Loss: 0.00002464
Iteration 122/1000 | Loss: 0.00002464
Iteration 123/1000 | Loss: 0.00002463
Iteration 124/1000 | Loss: 0.00002463
Iteration 125/1000 | Loss: 0.00002463
Iteration 126/1000 | Loss: 0.00002463
Iteration 127/1000 | Loss: 0.00002463
Iteration 128/1000 | Loss: 0.00002462
Iteration 129/1000 | Loss: 0.00002462
Iteration 130/1000 | Loss: 0.00002462
Iteration 131/1000 | Loss: 0.00002462
Iteration 132/1000 | Loss: 0.00002462
Iteration 133/1000 | Loss: 0.00002461
Iteration 134/1000 | Loss: 0.00002461
Iteration 135/1000 | Loss: 0.00002461
Iteration 136/1000 | Loss: 0.00002461
Iteration 137/1000 | Loss: 0.00002461
Iteration 138/1000 | Loss: 0.00002460
Iteration 139/1000 | Loss: 0.00002460
Iteration 140/1000 | Loss: 0.00002460
Iteration 141/1000 | Loss: 0.00002460
Iteration 142/1000 | Loss: 0.00002460
Iteration 143/1000 | Loss: 0.00002459
Iteration 144/1000 | Loss: 0.00002459
Iteration 145/1000 | Loss: 0.00002459
Iteration 146/1000 | Loss: 0.00002459
Iteration 147/1000 | Loss: 0.00002459
Iteration 148/1000 | Loss: 0.00002459
Iteration 149/1000 | Loss: 0.00002459
Iteration 150/1000 | Loss: 0.00002459
Iteration 151/1000 | Loss: 0.00002459
Iteration 152/1000 | Loss: 0.00002459
Iteration 153/1000 | Loss: 0.00002459
Iteration 154/1000 | Loss: 0.00002459
Iteration 155/1000 | Loss: 0.00002458
Iteration 156/1000 | Loss: 0.00002458
Iteration 157/1000 | Loss: 0.00002458
Iteration 158/1000 | Loss: 0.00002458
Iteration 159/1000 | Loss: 0.00002458
Iteration 160/1000 | Loss: 0.00002458
Iteration 161/1000 | Loss: 0.00002458
Iteration 162/1000 | Loss: 0.00002458
Iteration 163/1000 | Loss: 0.00002457
Iteration 164/1000 | Loss: 0.00002457
Iteration 165/1000 | Loss: 0.00002457
Iteration 166/1000 | Loss: 0.00002457
Iteration 167/1000 | Loss: 0.00002457
Iteration 168/1000 | Loss: 0.00002457
Iteration 169/1000 | Loss: 0.00002457
Iteration 170/1000 | Loss: 0.00002457
Iteration 171/1000 | Loss: 0.00002457
Iteration 172/1000 | Loss: 0.00002457
Iteration 173/1000 | Loss: 0.00002457
Iteration 174/1000 | Loss: 0.00002457
Iteration 175/1000 | Loss: 0.00002457
Iteration 176/1000 | Loss: 0.00002457
Iteration 177/1000 | Loss: 0.00002457
Iteration 178/1000 | Loss: 0.00002457
Iteration 179/1000 | Loss: 0.00002456
Iteration 180/1000 | Loss: 0.00002456
Iteration 181/1000 | Loss: 0.00002456
Iteration 182/1000 | Loss: 0.00002456
Iteration 183/1000 | Loss: 0.00002456
Iteration 184/1000 | Loss: 0.00002456
Iteration 185/1000 | Loss: 0.00002455
Iteration 186/1000 | Loss: 0.00002455
Iteration 187/1000 | Loss: 0.00002455
Iteration 188/1000 | Loss: 0.00002455
Iteration 189/1000 | Loss: 0.00002455
Iteration 190/1000 | Loss: 0.00002455
Iteration 191/1000 | Loss: 0.00002455
Iteration 192/1000 | Loss: 0.00002455
Iteration 193/1000 | Loss: 0.00002455
Iteration 194/1000 | Loss: 0.00002455
Iteration 195/1000 | Loss: 0.00002455
Iteration 196/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.4546672648284584e-05, 2.4546672648284584e-05, 2.4546672648284584e-05, 2.4546672648284584e-05, 2.4546672648284584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4546672648284584e-05

Optimization complete. Final v2v error: 4.287542343139648 mm

Highest mean error: 4.585844993591309 mm for frame 126

Lowest mean error: 3.6855902671813965 mm for frame 36

Saving results

Total time: 78.61730408668518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503845
Iteration 2/25 | Loss: 0.00135321
Iteration 3/25 | Loss: 0.00126218
Iteration 4/25 | Loss: 0.00124603
Iteration 5/25 | Loss: 0.00123918
Iteration 6/25 | Loss: 0.00123884
Iteration 7/25 | Loss: 0.00123884
Iteration 8/25 | Loss: 0.00123884
Iteration 9/25 | Loss: 0.00123884
Iteration 10/25 | Loss: 0.00123884
Iteration 11/25 | Loss: 0.00123884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012388393515720963, 0.0012388393515720963, 0.0012388393515720963, 0.0012388393515720963, 0.0012388393515720963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012388393515720963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78954095
Iteration 2/25 | Loss: 0.00091171
Iteration 3/25 | Loss: 0.00091171
Iteration 4/25 | Loss: 0.00091171
Iteration 5/25 | Loss: 0.00091171
Iteration 6/25 | Loss: 0.00091171
Iteration 7/25 | Loss: 0.00091171
Iteration 8/25 | Loss: 0.00091171
Iteration 9/25 | Loss: 0.00091171
Iteration 10/25 | Loss: 0.00091171
Iteration 11/25 | Loss: 0.00091171
Iteration 12/25 | Loss: 0.00091171
Iteration 13/25 | Loss: 0.00091171
Iteration 14/25 | Loss: 0.00091171
Iteration 15/25 | Loss: 0.00091171
Iteration 16/25 | Loss: 0.00091171
Iteration 17/25 | Loss: 0.00091171
Iteration 18/25 | Loss: 0.00091171
Iteration 19/25 | Loss: 0.00091171
Iteration 20/25 | Loss: 0.00091171
Iteration 21/25 | Loss: 0.00091171
Iteration 22/25 | Loss: 0.00091171
Iteration 23/25 | Loss: 0.00091171
Iteration 24/25 | Loss: 0.00091171
Iteration 25/25 | Loss: 0.00091171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091171
Iteration 2/1000 | Loss: 0.00003960
Iteration 3/1000 | Loss: 0.00002547
Iteration 4/1000 | Loss: 0.00002334
Iteration 5/1000 | Loss: 0.00002209
Iteration 6/1000 | Loss: 0.00002122
Iteration 7/1000 | Loss: 0.00002064
Iteration 8/1000 | Loss: 0.00002022
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001944
Iteration 11/1000 | Loss: 0.00001911
Iteration 12/1000 | Loss: 0.00001884
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00001846
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001804
Iteration 17/1000 | Loss: 0.00001802
Iteration 18/1000 | Loss: 0.00001802
Iteration 19/1000 | Loss: 0.00001787
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001759
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001754
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001746
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001745
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001744
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001740
Iteration 57/1000 | Loss: 0.00001740
Iteration 58/1000 | Loss: 0.00001739
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001734
Iteration 76/1000 | Loss: 0.00001734
Iteration 77/1000 | Loss: 0.00001734
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001727
Iteration 106/1000 | Loss: 0.00001727
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001722
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001722
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001720
Iteration 146/1000 | Loss: 0.00001720
Iteration 147/1000 | Loss: 0.00001720
Iteration 148/1000 | Loss: 0.00001720
Iteration 149/1000 | Loss: 0.00001720
Iteration 150/1000 | Loss: 0.00001720
Iteration 151/1000 | Loss: 0.00001720
Iteration 152/1000 | Loss: 0.00001720
Iteration 153/1000 | Loss: 0.00001720
Iteration 154/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.720069849397987e-05, 1.720069849397987e-05, 1.720069849397987e-05, 1.720069849397987e-05, 1.720069849397987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720069849397987e-05

Optimization complete. Final v2v error: 3.5050978660583496 mm

Highest mean error: 4.359959125518799 mm for frame 263

Lowest mean error: 3.391386032104492 mm for frame 17

Saving results

Total time: 53.25072360038757
