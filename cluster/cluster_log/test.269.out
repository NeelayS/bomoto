Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=269, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15064-15119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507036
Iteration 2/25 | Loss: 0.00134680
Iteration 3/25 | Loss: 0.00103728
Iteration 4/25 | Loss: 0.00100120
Iteration 5/25 | Loss: 0.00098033
Iteration 6/25 | Loss: 0.00097611
Iteration 7/25 | Loss: 0.00097579
Iteration 8/25 | Loss: 0.00097579
Iteration 9/25 | Loss: 0.00097579
Iteration 10/25 | Loss: 0.00097579
Iteration 11/25 | Loss: 0.00097579
Iteration 12/25 | Loss: 0.00097579
Iteration 13/25 | Loss: 0.00097579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009757876978255808, 0.0009757876978255808, 0.0009757876978255808, 0.0009757876978255808, 0.0009757876978255808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009757876978255808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66535860
Iteration 2/25 | Loss: 0.00079633
Iteration 3/25 | Loss: 0.00079633
Iteration 4/25 | Loss: 0.00079633
Iteration 5/25 | Loss: 0.00079633
Iteration 6/25 | Loss: 0.00079633
Iteration 7/25 | Loss: 0.00079633
Iteration 8/25 | Loss: 0.00079633
Iteration 9/25 | Loss: 0.00079633
Iteration 10/25 | Loss: 0.00079633
Iteration 11/25 | Loss: 0.00079633
Iteration 12/25 | Loss: 0.00079633
Iteration 13/25 | Loss: 0.00079633
Iteration 14/25 | Loss: 0.00079633
Iteration 15/25 | Loss: 0.00079633
Iteration 16/25 | Loss: 0.00079633
Iteration 17/25 | Loss: 0.00079633
Iteration 18/25 | Loss: 0.00079633
Iteration 19/25 | Loss: 0.00079633
Iteration 20/25 | Loss: 0.00079633
Iteration 21/25 | Loss: 0.00079633
Iteration 22/25 | Loss: 0.00079633
Iteration 23/25 | Loss: 0.00079633
Iteration 24/25 | Loss: 0.00079633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007963282405398786, 0.0007963282405398786, 0.0007963282405398786, 0.0007963282405398786, 0.0007963282405398786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007963282405398786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079633
Iteration 2/1000 | Loss: 0.00005734
Iteration 3/1000 | Loss: 0.00003925
Iteration 4/1000 | Loss: 0.00003511
Iteration 5/1000 | Loss: 0.00003325
Iteration 6/1000 | Loss: 0.00003194
Iteration 7/1000 | Loss: 0.00003133
Iteration 8/1000 | Loss: 0.00003068
Iteration 9/1000 | Loss: 0.00003024
Iteration 10/1000 | Loss: 0.00002973
Iteration 11/1000 | Loss: 0.00002948
Iteration 12/1000 | Loss: 0.00002931
Iteration 13/1000 | Loss: 0.00002924
Iteration 14/1000 | Loss: 0.00002921
Iteration 15/1000 | Loss: 0.00002921
Iteration 16/1000 | Loss: 0.00002919
Iteration 17/1000 | Loss: 0.00002919
Iteration 18/1000 | Loss: 0.00002918
Iteration 19/1000 | Loss: 0.00002918
Iteration 20/1000 | Loss: 0.00002917
Iteration 21/1000 | Loss: 0.00002916
Iteration 22/1000 | Loss: 0.00002915
Iteration 23/1000 | Loss: 0.00002909
Iteration 24/1000 | Loss: 0.00002901
Iteration 25/1000 | Loss: 0.00002901
Iteration 26/1000 | Loss: 0.00002896
Iteration 27/1000 | Loss: 0.00002896
Iteration 28/1000 | Loss: 0.00002896
Iteration 29/1000 | Loss: 0.00002894
Iteration 30/1000 | Loss: 0.00002894
Iteration 31/1000 | Loss: 0.00002894
Iteration 32/1000 | Loss: 0.00002893
Iteration 33/1000 | Loss: 0.00002893
Iteration 34/1000 | Loss: 0.00002892
Iteration 35/1000 | Loss: 0.00002891
Iteration 36/1000 | Loss: 0.00002891
Iteration 37/1000 | Loss: 0.00002891
Iteration 38/1000 | Loss: 0.00002891
Iteration 39/1000 | Loss: 0.00002889
Iteration 40/1000 | Loss: 0.00002889
Iteration 41/1000 | Loss: 0.00002889
Iteration 42/1000 | Loss: 0.00002888
Iteration 43/1000 | Loss: 0.00002888
Iteration 44/1000 | Loss: 0.00002888
Iteration 45/1000 | Loss: 0.00002888
Iteration 46/1000 | Loss: 0.00002888
Iteration 47/1000 | Loss: 0.00002888
Iteration 48/1000 | Loss: 0.00002887
Iteration 49/1000 | Loss: 0.00002886
Iteration 50/1000 | Loss: 0.00002885
Iteration 51/1000 | Loss: 0.00002885
Iteration 52/1000 | Loss: 0.00002884
Iteration 53/1000 | Loss: 0.00002884
Iteration 54/1000 | Loss: 0.00002884
Iteration 55/1000 | Loss: 0.00002884
Iteration 56/1000 | Loss: 0.00002884
Iteration 57/1000 | Loss: 0.00002884
Iteration 58/1000 | Loss: 0.00002884
Iteration 59/1000 | Loss: 0.00002883
Iteration 60/1000 | Loss: 0.00002883
Iteration 61/1000 | Loss: 0.00002883
Iteration 62/1000 | Loss: 0.00002883
Iteration 63/1000 | Loss: 0.00002883
Iteration 64/1000 | Loss: 0.00002883
Iteration 65/1000 | Loss: 0.00002882
Iteration 66/1000 | Loss: 0.00002882
Iteration 67/1000 | Loss: 0.00002882
Iteration 68/1000 | Loss: 0.00002882
Iteration 69/1000 | Loss: 0.00002882
Iteration 70/1000 | Loss: 0.00002882
Iteration 71/1000 | Loss: 0.00002882
Iteration 72/1000 | Loss: 0.00002882
Iteration 73/1000 | Loss: 0.00002882
Iteration 74/1000 | Loss: 0.00002882
Iteration 75/1000 | Loss: 0.00002882
Iteration 76/1000 | Loss: 0.00002882
Iteration 77/1000 | Loss: 0.00002882
Iteration 78/1000 | Loss: 0.00002882
Iteration 79/1000 | Loss: 0.00002882
Iteration 80/1000 | Loss: 0.00002882
Iteration 81/1000 | Loss: 0.00002882
Iteration 82/1000 | Loss: 0.00002882
Iteration 83/1000 | Loss: 0.00002882
Iteration 84/1000 | Loss: 0.00002882
Iteration 85/1000 | Loss: 0.00002882
Iteration 86/1000 | Loss: 0.00002882
Iteration 87/1000 | Loss: 0.00002882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.8822123567806557e-05, 2.8822123567806557e-05, 2.8822123567806557e-05, 2.8822123567806557e-05, 2.8822123567806557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8822123567806557e-05

Optimization complete. Final v2v error: 4.578283309936523 mm

Highest mean error: 5.202718257904053 mm for frame 247

Lowest mean error: 4.2700419425964355 mm for frame 8

Saving results

Total time: 42.58526968955994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752865
Iteration 2/25 | Loss: 0.00220634
Iteration 3/25 | Loss: 0.00130536
Iteration 4/25 | Loss: 0.00113419
Iteration 5/25 | Loss: 0.00108894
Iteration 6/25 | Loss: 0.00113231
Iteration 7/25 | Loss: 0.00105839
Iteration 8/25 | Loss: 0.00102036
Iteration 9/25 | Loss: 0.00096669
Iteration 10/25 | Loss: 0.00094475
Iteration 11/25 | Loss: 0.00093491
Iteration 12/25 | Loss: 0.00092034
Iteration 13/25 | Loss: 0.00091872
Iteration 14/25 | Loss: 0.00091266
Iteration 15/25 | Loss: 0.00091557
Iteration 16/25 | Loss: 0.00091070
Iteration 17/25 | Loss: 0.00091040
Iteration 18/25 | Loss: 0.00091029
Iteration 19/25 | Loss: 0.00091029
Iteration 20/25 | Loss: 0.00091029
Iteration 21/25 | Loss: 0.00091025
Iteration 22/25 | Loss: 0.00091025
Iteration 23/25 | Loss: 0.00091025
Iteration 24/25 | Loss: 0.00091025
Iteration 25/25 | Loss: 0.00091025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55133247
Iteration 2/25 | Loss: 0.00126020
Iteration 3/25 | Loss: 0.00126020
Iteration 4/25 | Loss: 0.00126020
Iteration 5/25 | Loss: 0.00126020
Iteration 6/25 | Loss: 0.00126020
Iteration 7/25 | Loss: 0.00126020
Iteration 8/25 | Loss: 0.00126020
Iteration 9/25 | Loss: 0.00126020
Iteration 10/25 | Loss: 0.00126020
Iteration 11/25 | Loss: 0.00126019
Iteration 12/25 | Loss: 0.00126019
Iteration 13/25 | Loss: 0.00126019
Iteration 14/25 | Loss: 0.00126019
Iteration 15/25 | Loss: 0.00126019
Iteration 16/25 | Loss: 0.00126019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012601949274539948, 0.0012601949274539948, 0.0012601949274539948, 0.0012601949274539948, 0.0012601949274539948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012601949274539948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126020
Iteration 2/1000 | Loss: 0.00003632
Iteration 3/1000 | Loss: 0.00002868
Iteration 4/1000 | Loss: 0.00002591
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002379
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002303
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002387
Iteration 13/1000 | Loss: 0.00002347
Iteration 14/1000 | Loss: 0.00002296
Iteration 15/1000 | Loss: 0.00002274
Iteration 16/1000 | Loss: 0.00002274
Iteration 17/1000 | Loss: 0.00002274
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002274
Iteration 20/1000 | Loss: 0.00002274
Iteration 21/1000 | Loss: 0.00002274
Iteration 22/1000 | Loss: 0.00002273
Iteration 23/1000 | Loss: 0.00002273
Iteration 24/1000 | Loss: 0.00002273
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002273
Iteration 28/1000 | Loss: 0.00002273
Iteration 29/1000 | Loss: 0.00002273
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002334
Iteration 32/1000 | Loss: 0.00002294
Iteration 33/1000 | Loss: 0.00002269
Iteration 34/1000 | Loss: 0.00002322
Iteration 35/1000 | Loss: 0.00002296
Iteration 36/1000 | Loss: 0.00002269
Iteration 37/1000 | Loss: 0.00002267
Iteration 38/1000 | Loss: 0.00002317
Iteration 39/1000 | Loss: 0.00004060
Iteration 40/1000 | Loss: 0.00002593
Iteration 41/1000 | Loss: 0.00002434
Iteration 42/1000 | Loss: 0.00002352
Iteration 43/1000 | Loss: 0.00002321
Iteration 44/1000 | Loss: 0.00002323
Iteration 45/1000 | Loss: 0.00002292
Iteration 46/1000 | Loss: 0.00002276
Iteration 47/1000 | Loss: 0.00002276
Iteration 48/1000 | Loss: 0.00002276
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002288
Iteration 51/1000 | Loss: 0.00002272
Iteration 52/1000 | Loss: 0.00002272
Iteration 53/1000 | Loss: 0.00002272
Iteration 54/1000 | Loss: 0.00002272
Iteration 55/1000 | Loss: 0.00002272
Iteration 56/1000 | Loss: 0.00002272
Iteration 57/1000 | Loss: 0.00002272
Iteration 58/1000 | Loss: 0.00002272
Iteration 59/1000 | Loss: 0.00002272
Iteration 60/1000 | Loss: 0.00002272
Iteration 61/1000 | Loss: 0.00002272
Iteration 62/1000 | Loss: 0.00002272
Iteration 63/1000 | Loss: 0.00002272
Iteration 64/1000 | Loss: 0.00002271
Iteration 65/1000 | Loss: 0.00002271
Iteration 66/1000 | Loss: 0.00002271
Iteration 67/1000 | Loss: 0.00002271
Iteration 68/1000 | Loss: 0.00002271
Iteration 69/1000 | Loss: 0.00002271
Iteration 70/1000 | Loss: 0.00002271
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00003719
Iteration 73/1000 | Loss: 0.00002480
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002266
Iteration 77/1000 | Loss: 0.00002265
Iteration 78/1000 | Loss: 0.00002265
Iteration 79/1000 | Loss: 0.00002265
Iteration 80/1000 | Loss: 0.00002265
Iteration 81/1000 | Loss: 0.00002265
Iteration 82/1000 | Loss: 0.00003349
Iteration 83/1000 | Loss: 0.00002450
Iteration 84/1000 | Loss: 0.00002297
Iteration 85/1000 | Loss: 0.00002286
Iteration 86/1000 | Loss: 0.00002295
Iteration 87/1000 | Loss: 0.00003234
Iteration 88/1000 | Loss: 0.00002404
Iteration 89/1000 | Loss: 0.00002266
Iteration 90/1000 | Loss: 0.00003217
Iteration 91/1000 | Loss: 0.00002345
Iteration 92/1000 | Loss: 0.00002500
Iteration 93/1000 | Loss: 0.00002778
Iteration 94/1000 | Loss: 0.00002303
Iteration 95/1000 | Loss: 0.00002417
Iteration 96/1000 | Loss: 0.00002397
Iteration 97/1000 | Loss: 0.00002552
Iteration 98/1000 | Loss: 0.00002304
Iteration 99/1000 | Loss: 0.00002876
Iteration 100/1000 | Loss: 0.00002317
Iteration 101/1000 | Loss: 0.00002285
Iteration 102/1000 | Loss: 0.00002314
Iteration 103/1000 | Loss: 0.00002278
Iteration 104/1000 | Loss: 0.00002278
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00003143
Iteration 107/1000 | Loss: 0.00002347
Iteration 108/1000 | Loss: 0.00002317
Iteration 109/1000 | Loss: 0.00002294
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002858
Iteration 113/1000 | Loss: 0.00002426
Iteration 114/1000 | Loss: 0.00003540
Iteration 115/1000 | Loss: 0.00002317
Iteration 116/1000 | Loss: 0.00002282
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002265
Iteration 120/1000 | Loss: 0.00002265
Iteration 121/1000 | Loss: 0.00002265
Iteration 122/1000 | Loss: 0.00002265
Iteration 123/1000 | Loss: 0.00002265
Iteration 124/1000 | Loss: 0.00002265
Iteration 125/1000 | Loss: 0.00002265
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002265
Iteration 129/1000 | Loss: 0.00002265
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002265
Iteration 132/1000 | Loss: 0.00002265
Iteration 133/1000 | Loss: 0.00002265
Iteration 134/1000 | Loss: 0.00002265
Iteration 135/1000 | Loss: 0.00002265
Iteration 136/1000 | Loss: 0.00002265
Iteration 137/1000 | Loss: 0.00002265
Iteration 138/1000 | Loss: 0.00002265
Iteration 139/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.264791874040384e-05, 2.264791874040384e-05, 2.264791874040384e-05, 2.264791874040384e-05, 2.264791874040384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.264791874040384e-05

Optimization complete. Final v2v error: 3.6656508445739746 mm

Highest mean error: 20.333402633666992 mm for frame 162

Lowest mean error: 3.2052295207977295 mm for frame 160

Saving results

Total time: 124.3612973690033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110398
Iteration 2/25 | Loss: 0.00291880
Iteration 3/25 | Loss: 0.00187317
Iteration 4/25 | Loss: 0.00164426
Iteration 5/25 | Loss: 0.00159563
Iteration 6/25 | Loss: 0.00145588
Iteration 7/25 | Loss: 0.00140357
Iteration 8/25 | Loss: 0.00134821
Iteration 9/25 | Loss: 0.00130301
Iteration 10/25 | Loss: 0.00130138
Iteration 11/25 | Loss: 0.00130340
Iteration 12/25 | Loss: 0.00127955
Iteration 13/25 | Loss: 0.00127348
Iteration 14/25 | Loss: 0.00126217
Iteration 15/25 | Loss: 0.00125811
Iteration 16/25 | Loss: 0.00125169
Iteration 17/25 | Loss: 0.00124832
Iteration 18/25 | Loss: 0.00124726
Iteration 19/25 | Loss: 0.00123843
Iteration 20/25 | Loss: 0.00123773
Iteration 21/25 | Loss: 0.00124348
Iteration 22/25 | Loss: 0.00124166
Iteration 23/25 | Loss: 0.00124440
Iteration 24/25 | Loss: 0.00124118
Iteration 25/25 | Loss: 0.00123600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66890812
Iteration 2/25 | Loss: 0.00345662
Iteration 3/25 | Loss: 0.00345661
Iteration 4/25 | Loss: 0.00345661
Iteration 5/25 | Loss: 0.00345661
Iteration 6/25 | Loss: 0.00345661
Iteration 7/25 | Loss: 0.00345661
Iteration 8/25 | Loss: 0.00345661
Iteration 9/25 | Loss: 0.00345661
Iteration 10/25 | Loss: 0.00345661
Iteration 11/25 | Loss: 0.00345661
Iteration 12/25 | Loss: 0.00345661
Iteration 13/25 | Loss: 0.00345661
Iteration 14/25 | Loss: 0.00345661
Iteration 15/25 | Loss: 0.00345661
Iteration 16/25 | Loss: 0.00345661
Iteration 17/25 | Loss: 0.00345661
Iteration 18/25 | Loss: 0.00345661
Iteration 19/25 | Loss: 0.00345661
Iteration 20/25 | Loss: 0.00345661
Iteration 21/25 | Loss: 0.00345661
Iteration 22/25 | Loss: 0.00345661
Iteration 23/25 | Loss: 0.00345661
Iteration 24/25 | Loss: 0.00345661
Iteration 25/25 | Loss: 0.00345661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345661
Iteration 2/1000 | Loss: 0.00047230
Iteration 3/1000 | Loss: 0.00037310
Iteration 4/1000 | Loss: 0.00032675
Iteration 5/1000 | Loss: 0.00029160
Iteration 6/1000 | Loss: 0.00025150
Iteration 7/1000 | Loss: 0.00024436
Iteration 8/1000 | Loss: 0.00237242
Iteration 9/1000 | Loss: 0.00108565
Iteration 10/1000 | Loss: 0.00153302
Iteration 11/1000 | Loss: 0.00121034
Iteration 12/1000 | Loss: 0.00402956
Iteration 13/1000 | Loss: 0.00248601
Iteration 14/1000 | Loss: 0.00419935
Iteration 15/1000 | Loss: 0.00181993
Iteration 16/1000 | Loss: 0.00296354
Iteration 17/1000 | Loss: 0.00157696
Iteration 18/1000 | Loss: 0.00085742
Iteration 19/1000 | Loss: 0.00169224
Iteration 20/1000 | Loss: 0.00056055
Iteration 21/1000 | Loss: 0.00121040
Iteration 22/1000 | Loss: 0.00079783
Iteration 23/1000 | Loss: 0.00048564
Iteration 24/1000 | Loss: 0.00082348
Iteration 25/1000 | Loss: 0.00097600
Iteration 26/1000 | Loss: 0.00077452
Iteration 27/1000 | Loss: 0.00099447
Iteration 28/1000 | Loss: 0.00053070
Iteration 29/1000 | Loss: 0.00053163
Iteration 30/1000 | Loss: 0.00063947
Iteration 31/1000 | Loss: 0.00084830
Iteration 32/1000 | Loss: 0.00065874
Iteration 33/1000 | Loss: 0.00088634
Iteration 34/1000 | Loss: 0.00023011
Iteration 35/1000 | Loss: 0.00108191
Iteration 36/1000 | Loss: 0.00066326
Iteration 37/1000 | Loss: 0.00056176
Iteration 38/1000 | Loss: 0.00039122
Iteration 39/1000 | Loss: 0.00071563
Iteration 40/1000 | Loss: 0.00099717
Iteration 41/1000 | Loss: 0.00102896
Iteration 42/1000 | Loss: 0.00090701
Iteration 43/1000 | Loss: 0.00043945
Iteration 44/1000 | Loss: 0.00108035
Iteration 45/1000 | Loss: 0.00079868
Iteration 46/1000 | Loss: 0.00096715
Iteration 47/1000 | Loss: 0.00097172
Iteration 48/1000 | Loss: 0.00065343
Iteration 49/1000 | Loss: 0.00033645
Iteration 50/1000 | Loss: 0.00036728
Iteration 51/1000 | Loss: 0.00024030
Iteration 52/1000 | Loss: 0.00065384
Iteration 53/1000 | Loss: 0.00126986
Iteration 54/1000 | Loss: 0.00045823
Iteration 55/1000 | Loss: 0.00018430
Iteration 56/1000 | Loss: 0.00019334
Iteration 57/1000 | Loss: 0.00058451
Iteration 58/1000 | Loss: 0.00022786
Iteration 59/1000 | Loss: 0.00038552
Iteration 60/1000 | Loss: 0.00042192
Iteration 61/1000 | Loss: 0.00020124
Iteration 62/1000 | Loss: 0.00033855
Iteration 63/1000 | Loss: 0.00054485
Iteration 64/1000 | Loss: 0.00076859
Iteration 65/1000 | Loss: 0.00020653
Iteration 66/1000 | Loss: 0.00025091
Iteration 67/1000 | Loss: 0.00046385
Iteration 68/1000 | Loss: 0.00044376
Iteration 69/1000 | Loss: 0.00022238
Iteration 70/1000 | Loss: 0.00101292
Iteration 71/1000 | Loss: 0.00031066
Iteration 72/1000 | Loss: 0.00088896
Iteration 73/1000 | Loss: 0.00078014
Iteration 74/1000 | Loss: 0.00115815
Iteration 75/1000 | Loss: 0.00084050
Iteration 76/1000 | Loss: 0.00201867
Iteration 77/1000 | Loss: 0.00075433
Iteration 78/1000 | Loss: 0.00082134
Iteration 79/1000 | Loss: 0.00100192
Iteration 80/1000 | Loss: 0.00078206
Iteration 81/1000 | Loss: 0.00161622
Iteration 82/1000 | Loss: 0.00133304
Iteration 83/1000 | Loss: 0.00055266
Iteration 84/1000 | Loss: 0.00034362
Iteration 85/1000 | Loss: 0.00026109
Iteration 86/1000 | Loss: 0.00019546
Iteration 87/1000 | Loss: 0.00019069
Iteration 88/1000 | Loss: 0.00020092
Iteration 89/1000 | Loss: 0.00017714
Iteration 90/1000 | Loss: 0.00107323
Iteration 91/1000 | Loss: 0.00015420
Iteration 92/1000 | Loss: 0.00087548
Iteration 93/1000 | Loss: 0.00085788
Iteration 94/1000 | Loss: 0.00046778
Iteration 95/1000 | Loss: 0.00021563
Iteration 96/1000 | Loss: 0.00018325
Iteration 97/1000 | Loss: 0.00017565
Iteration 98/1000 | Loss: 0.00137875
Iteration 99/1000 | Loss: 0.00162984
Iteration 100/1000 | Loss: 0.00055502
Iteration 101/1000 | Loss: 0.00055424
Iteration 102/1000 | Loss: 0.00051799
Iteration 103/1000 | Loss: 0.00041008
Iteration 104/1000 | Loss: 0.00050133
Iteration 105/1000 | Loss: 0.00035683
Iteration 106/1000 | Loss: 0.00026089
Iteration 107/1000 | Loss: 0.00025994
Iteration 108/1000 | Loss: 0.00034396
Iteration 109/1000 | Loss: 0.00021404
Iteration 110/1000 | Loss: 0.00120045
Iteration 111/1000 | Loss: 0.00040649
Iteration 112/1000 | Loss: 0.00070468
Iteration 113/1000 | Loss: 0.00042151
Iteration 114/1000 | Loss: 0.00014833
Iteration 115/1000 | Loss: 0.00029542
Iteration 116/1000 | Loss: 0.00027445
Iteration 117/1000 | Loss: 0.00027174
Iteration 118/1000 | Loss: 0.00198428
Iteration 119/1000 | Loss: 0.00051641
Iteration 120/1000 | Loss: 0.00146485
Iteration 121/1000 | Loss: 0.00109250
Iteration 122/1000 | Loss: 0.00198093
Iteration 123/1000 | Loss: 0.00152101
Iteration 124/1000 | Loss: 0.00198799
Iteration 125/1000 | Loss: 0.00330513
Iteration 126/1000 | Loss: 0.00285126
Iteration 127/1000 | Loss: 0.00162422
Iteration 128/1000 | Loss: 0.00063531
Iteration 129/1000 | Loss: 0.00144562
Iteration 130/1000 | Loss: 0.00165920
Iteration 131/1000 | Loss: 0.00159703
Iteration 132/1000 | Loss: 0.00185799
Iteration 133/1000 | Loss: 0.00128036
Iteration 134/1000 | Loss: 0.00123741
Iteration 135/1000 | Loss: 0.00175404
Iteration 136/1000 | Loss: 0.00142502
Iteration 137/1000 | Loss: 0.00141988
Iteration 138/1000 | Loss: 0.00132031
Iteration 139/1000 | Loss: 0.00167685
Iteration 140/1000 | Loss: 0.00091016
Iteration 141/1000 | Loss: 0.00139054
Iteration 142/1000 | Loss: 0.00256406
Iteration 143/1000 | Loss: 0.00099084
Iteration 144/1000 | Loss: 0.00186006
Iteration 145/1000 | Loss: 0.00093224
Iteration 146/1000 | Loss: 0.00071306
Iteration 147/1000 | Loss: 0.00232851
Iteration 148/1000 | Loss: 0.00084960
Iteration 149/1000 | Loss: 0.00060726
Iteration 150/1000 | Loss: 0.00113500
Iteration 151/1000 | Loss: 0.00092493
Iteration 152/1000 | Loss: 0.00062157
Iteration 153/1000 | Loss: 0.00079377
Iteration 154/1000 | Loss: 0.00052957
Iteration 155/1000 | Loss: 0.00066994
Iteration 156/1000 | Loss: 0.00136824
Iteration 157/1000 | Loss: 0.00070390
Iteration 158/1000 | Loss: 0.00059519
Iteration 159/1000 | Loss: 0.00086892
Iteration 160/1000 | Loss: 0.00096279
Iteration 161/1000 | Loss: 0.00092805
Iteration 162/1000 | Loss: 0.00080189
Iteration 163/1000 | Loss: 0.00085601
Iteration 164/1000 | Loss: 0.00135726
Iteration 165/1000 | Loss: 0.00111410
Iteration 166/1000 | Loss: 0.00115338
Iteration 167/1000 | Loss: 0.00109480
Iteration 168/1000 | Loss: 0.00096909
Iteration 169/1000 | Loss: 0.00097279
Iteration 170/1000 | Loss: 0.00145602
Iteration 171/1000 | Loss: 0.00150393
Iteration 172/1000 | Loss: 0.00087567
Iteration 173/1000 | Loss: 0.00096290
Iteration 174/1000 | Loss: 0.00094981
Iteration 175/1000 | Loss: 0.00087048
Iteration 176/1000 | Loss: 0.00099923
Iteration 177/1000 | Loss: 0.00072119
Iteration 178/1000 | Loss: 0.00065586
Iteration 179/1000 | Loss: 0.00154545
Iteration 180/1000 | Loss: 0.00137570
Iteration 181/1000 | Loss: 0.00069357
Iteration 182/1000 | Loss: 0.00016680
Iteration 183/1000 | Loss: 0.00013436
Iteration 184/1000 | Loss: 0.00063015
Iteration 185/1000 | Loss: 0.00033346
Iteration 186/1000 | Loss: 0.00035583
Iteration 187/1000 | Loss: 0.00028044
Iteration 188/1000 | Loss: 0.00014931
Iteration 189/1000 | Loss: 0.00049784
Iteration 190/1000 | Loss: 0.00011754
Iteration 191/1000 | Loss: 0.00158757
Iteration 192/1000 | Loss: 0.00048790
Iteration 193/1000 | Loss: 0.00017704
Iteration 194/1000 | Loss: 0.00015123
Iteration 195/1000 | Loss: 0.00013353
Iteration 196/1000 | Loss: 0.00012856
Iteration 197/1000 | Loss: 0.00011577
Iteration 198/1000 | Loss: 0.00010545
Iteration 199/1000 | Loss: 0.00011244
Iteration 200/1000 | Loss: 0.00009740
Iteration 201/1000 | Loss: 0.00074959
Iteration 202/1000 | Loss: 0.00241042
Iteration 203/1000 | Loss: 0.00115612
Iteration 204/1000 | Loss: 0.00076013
Iteration 205/1000 | Loss: 0.00076832
Iteration 206/1000 | Loss: 0.00022544
Iteration 207/1000 | Loss: 0.00041288
Iteration 208/1000 | Loss: 0.00034966
Iteration 209/1000 | Loss: 0.00060581
Iteration 210/1000 | Loss: 0.00048945
Iteration 211/1000 | Loss: 0.00037385
Iteration 212/1000 | Loss: 0.00012874
Iteration 213/1000 | Loss: 0.00011213
Iteration 214/1000 | Loss: 0.00045128
Iteration 215/1000 | Loss: 0.00015776
Iteration 216/1000 | Loss: 0.00035024
Iteration 217/1000 | Loss: 0.00042335
Iteration 218/1000 | Loss: 0.00042717
Iteration 219/1000 | Loss: 0.00035680
Iteration 220/1000 | Loss: 0.00040799
Iteration 221/1000 | Loss: 0.00044333
Iteration 222/1000 | Loss: 0.00017677
Iteration 223/1000 | Loss: 0.00018775
Iteration 224/1000 | Loss: 0.00023795
Iteration 225/1000 | Loss: 0.00020003
Iteration 226/1000 | Loss: 0.00035592
Iteration 227/1000 | Loss: 0.00021058
Iteration 228/1000 | Loss: 0.00022714
Iteration 229/1000 | Loss: 0.00022584
Iteration 230/1000 | Loss: 0.00145281
Iteration 231/1000 | Loss: 0.00057227
Iteration 232/1000 | Loss: 0.00052553
Iteration 233/1000 | Loss: 0.00031125
Iteration 234/1000 | Loss: 0.00055054
Iteration 235/1000 | Loss: 0.00080291
Iteration 236/1000 | Loss: 0.00059501
Iteration 237/1000 | Loss: 0.00052840
Iteration 238/1000 | Loss: 0.00055778
Iteration 239/1000 | Loss: 0.00043016
Iteration 240/1000 | Loss: 0.00074295
Iteration 241/1000 | Loss: 0.00040988
Iteration 242/1000 | Loss: 0.00029078
Iteration 243/1000 | Loss: 0.00038276
Iteration 244/1000 | Loss: 0.00037132
Iteration 245/1000 | Loss: 0.00068929
Iteration 246/1000 | Loss: 0.00088024
Iteration 247/1000 | Loss: 0.00074527
Iteration 248/1000 | Loss: 0.00012802
Iteration 249/1000 | Loss: 0.00016636
Iteration 250/1000 | Loss: 0.00105130
Iteration 251/1000 | Loss: 0.00045421
Iteration 252/1000 | Loss: 0.00069499
Iteration 253/1000 | Loss: 0.00019205
Iteration 254/1000 | Loss: 0.00043259
Iteration 255/1000 | Loss: 0.00025365
Iteration 256/1000 | Loss: 0.00056322
Iteration 257/1000 | Loss: 0.00069135
Iteration 258/1000 | Loss: 0.00105451
Iteration 259/1000 | Loss: 0.00049677
Iteration 260/1000 | Loss: 0.00100121
Iteration 261/1000 | Loss: 0.00045892
Iteration 262/1000 | Loss: 0.00150022
Iteration 263/1000 | Loss: 0.00044797
Iteration 264/1000 | Loss: 0.00011788
Iteration 265/1000 | Loss: 0.00012667
Iteration 266/1000 | Loss: 0.00010720
Iteration 267/1000 | Loss: 0.00033357
Iteration 268/1000 | Loss: 0.00012043
Iteration 269/1000 | Loss: 0.00021639
Iteration 270/1000 | Loss: 0.00019410
Iteration 271/1000 | Loss: 0.00018704
Iteration 272/1000 | Loss: 0.00016536
Iteration 273/1000 | Loss: 0.00021364
Iteration 274/1000 | Loss: 0.00012776
Iteration 275/1000 | Loss: 0.00020376
Iteration 276/1000 | Loss: 0.00035198
Iteration 277/1000 | Loss: 0.00023703
Iteration 278/1000 | Loss: 0.00009012
Iteration 279/1000 | Loss: 0.00032951
Iteration 280/1000 | Loss: 0.00009045
Iteration 281/1000 | Loss: 0.00008700
Iteration 282/1000 | Loss: 0.00008490
Iteration 283/1000 | Loss: 0.00008358
Iteration 284/1000 | Loss: 0.00008251
Iteration 285/1000 | Loss: 0.00008217
Iteration 286/1000 | Loss: 0.00008317
Iteration 287/1000 | Loss: 0.00083701
Iteration 288/1000 | Loss: 0.00141218
Iteration 289/1000 | Loss: 0.00097866
Iteration 290/1000 | Loss: 0.00010369
Iteration 291/1000 | Loss: 0.00008309
Iteration 292/1000 | Loss: 0.00007961
Iteration 293/1000 | Loss: 0.00007784
Iteration 294/1000 | Loss: 0.00007673
Iteration 295/1000 | Loss: 0.00007615
Iteration 296/1000 | Loss: 0.00007565
Iteration 297/1000 | Loss: 0.00007543
Iteration 298/1000 | Loss: 0.00007517
Iteration 299/1000 | Loss: 0.00007507
Iteration 300/1000 | Loss: 0.00007505
Iteration 301/1000 | Loss: 0.00007505
Iteration 302/1000 | Loss: 0.00007503
Iteration 303/1000 | Loss: 0.00007498
Iteration 304/1000 | Loss: 0.00083320
Iteration 305/1000 | Loss: 0.00215524
Iteration 306/1000 | Loss: 0.00052141
Iteration 307/1000 | Loss: 0.00009223
Iteration 308/1000 | Loss: 0.00067898
Iteration 309/1000 | Loss: 0.00040424
Iteration 310/1000 | Loss: 0.00058669
Iteration 311/1000 | Loss: 0.00008808
Iteration 312/1000 | Loss: 0.00007931
Iteration 313/1000 | Loss: 0.00007512
Iteration 314/1000 | Loss: 0.00007328
Iteration 315/1000 | Loss: 0.00007245
Iteration 316/1000 | Loss: 0.00007183
Iteration 317/1000 | Loss: 0.00007148
Iteration 318/1000 | Loss: 0.00007147
Iteration 319/1000 | Loss: 0.00007142
Iteration 320/1000 | Loss: 0.00007139
Iteration 321/1000 | Loss: 0.00007138
Iteration 322/1000 | Loss: 0.00007135
Iteration 323/1000 | Loss: 0.00007135
Iteration 324/1000 | Loss: 0.00007135
Iteration 325/1000 | Loss: 0.00007134
Iteration 326/1000 | Loss: 0.00007134
Iteration 327/1000 | Loss: 0.00007133
Iteration 328/1000 | Loss: 0.00007133
Iteration 329/1000 | Loss: 0.00007132
Iteration 330/1000 | Loss: 0.00007132
Iteration 331/1000 | Loss: 0.00007132
Iteration 332/1000 | Loss: 0.00007131
Iteration 333/1000 | Loss: 0.00007131
Iteration 334/1000 | Loss: 0.00007131
Iteration 335/1000 | Loss: 0.00007131
Iteration 336/1000 | Loss: 0.00007131
Iteration 337/1000 | Loss: 0.00007129
Iteration 338/1000 | Loss: 0.00007129
Iteration 339/1000 | Loss: 0.00007127
Iteration 340/1000 | Loss: 0.00007125
Iteration 341/1000 | Loss: 0.00007125
Iteration 342/1000 | Loss: 0.00007125
Iteration 343/1000 | Loss: 0.00007124
Iteration 344/1000 | Loss: 0.00007124
Iteration 345/1000 | Loss: 0.00007124
Iteration 346/1000 | Loss: 0.00007124
Iteration 347/1000 | Loss: 0.00007123
Iteration 348/1000 | Loss: 0.00007122
Iteration 349/1000 | Loss: 0.00007122
Iteration 350/1000 | Loss: 0.00007122
Iteration 351/1000 | Loss: 0.00007122
Iteration 352/1000 | Loss: 0.00007121
Iteration 353/1000 | Loss: 0.00007118
Iteration 354/1000 | Loss: 0.00007118
Iteration 355/1000 | Loss: 0.00007118
Iteration 356/1000 | Loss: 0.00007118
Iteration 357/1000 | Loss: 0.00007118
Iteration 358/1000 | Loss: 0.00007118
Iteration 359/1000 | Loss: 0.00007118
Iteration 360/1000 | Loss: 0.00007117
Iteration 361/1000 | Loss: 0.00007117
Iteration 362/1000 | Loss: 0.00007117
Iteration 363/1000 | Loss: 0.00007117
Iteration 364/1000 | Loss: 0.00007116
Iteration 365/1000 | Loss: 0.00007116
Iteration 366/1000 | Loss: 0.00007116
Iteration 367/1000 | Loss: 0.00007115
Iteration 368/1000 | Loss: 0.00007115
Iteration 369/1000 | Loss: 0.00007115
Iteration 370/1000 | Loss: 0.00007115
Iteration 371/1000 | Loss: 0.00007115
Iteration 372/1000 | Loss: 0.00007115
Iteration 373/1000 | Loss: 0.00007114
Iteration 374/1000 | Loss: 0.00007114
Iteration 375/1000 | Loss: 0.00007114
Iteration 376/1000 | Loss: 0.00007114
Iteration 377/1000 | Loss: 0.00007114
Iteration 378/1000 | Loss: 0.00007114
Iteration 379/1000 | Loss: 0.00007113
Iteration 380/1000 | Loss: 0.00007113
Iteration 381/1000 | Loss: 0.00007113
Iteration 382/1000 | Loss: 0.00007113
Iteration 383/1000 | Loss: 0.00007113
Iteration 384/1000 | Loss: 0.00007113
Iteration 385/1000 | Loss: 0.00007112
Iteration 386/1000 | Loss: 0.00007112
Iteration 387/1000 | Loss: 0.00007112
Iteration 388/1000 | Loss: 0.00007112
Iteration 389/1000 | Loss: 0.00007112
Iteration 390/1000 | Loss: 0.00007112
Iteration 391/1000 | Loss: 0.00007112
Iteration 392/1000 | Loss: 0.00007112
Iteration 393/1000 | Loss: 0.00007112
Iteration 394/1000 | Loss: 0.00007112
Iteration 395/1000 | Loss: 0.00007112
Iteration 396/1000 | Loss: 0.00007111
Iteration 397/1000 | Loss: 0.00007111
Iteration 398/1000 | Loss: 0.00007111
Iteration 399/1000 | Loss: 0.00007111
Iteration 400/1000 | Loss: 0.00007111
Iteration 401/1000 | Loss: 0.00007111
Iteration 402/1000 | Loss: 0.00007111
Iteration 403/1000 | Loss: 0.00007111
Iteration 404/1000 | Loss: 0.00007111
Iteration 405/1000 | Loss: 0.00007110
Iteration 406/1000 | Loss: 0.00007110
Iteration 407/1000 | Loss: 0.00007110
Iteration 408/1000 | Loss: 0.00007110
Iteration 409/1000 | Loss: 0.00007110
Iteration 410/1000 | Loss: 0.00007110
Iteration 411/1000 | Loss: 0.00007110
Iteration 412/1000 | Loss: 0.00007109
Iteration 413/1000 | Loss: 0.00007109
Iteration 414/1000 | Loss: 0.00007109
Iteration 415/1000 | Loss: 0.00007109
Iteration 416/1000 | Loss: 0.00007109
Iteration 417/1000 | Loss: 0.00007109
Iteration 418/1000 | Loss: 0.00007108
Iteration 419/1000 | Loss: 0.00007108
Iteration 420/1000 | Loss: 0.00007108
Iteration 421/1000 | Loss: 0.00007108
Iteration 422/1000 | Loss: 0.00007108
Iteration 423/1000 | Loss: 0.00007108
Iteration 424/1000 | Loss: 0.00007108
Iteration 425/1000 | Loss: 0.00007107
Iteration 426/1000 | Loss: 0.00007107
Iteration 427/1000 | Loss: 0.00007107
Iteration 428/1000 | Loss: 0.00007107
Iteration 429/1000 | Loss: 0.00007107
Iteration 430/1000 | Loss: 0.00007107
Iteration 431/1000 | Loss: 0.00007107
Iteration 432/1000 | Loss: 0.00007107
Iteration 433/1000 | Loss: 0.00007107
Iteration 434/1000 | Loss: 0.00007106
Iteration 435/1000 | Loss: 0.00007106
Iteration 436/1000 | Loss: 0.00007106
Iteration 437/1000 | Loss: 0.00007106
Iteration 438/1000 | Loss: 0.00007106
Iteration 439/1000 | Loss: 0.00007106
Iteration 440/1000 | Loss: 0.00007106
Iteration 441/1000 | Loss: 0.00007106
Iteration 442/1000 | Loss: 0.00007106
Iteration 443/1000 | Loss: 0.00007106
Iteration 444/1000 | Loss: 0.00007106
Iteration 445/1000 | Loss: 0.00007106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 445. Stopping optimization.
Last 5 losses: [7.106162229320034e-05, 7.106162229320034e-05, 7.106162229320034e-05, 7.106162229320034e-05, 7.106162229320034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.106162229320034e-05

Optimization complete. Final v2v error: 5.236535549163818 mm

Highest mean error: 13.143118858337402 mm for frame 50

Lowest mean error: 3.8931281566619873 mm for frame 79

Saving results

Total time: 498.0284848213196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813327
Iteration 2/25 | Loss: 0.00140695
Iteration 3/25 | Loss: 0.00121067
Iteration 4/25 | Loss: 0.00113257
Iteration 5/25 | Loss: 0.00106280
Iteration 6/25 | Loss: 0.00104933
Iteration 7/25 | Loss: 0.00106222
Iteration 8/25 | Loss: 0.00103454
Iteration 9/25 | Loss: 0.00103022
Iteration 10/25 | Loss: 0.00102198
Iteration 11/25 | Loss: 0.00102185
Iteration 12/25 | Loss: 0.00102185
Iteration 13/25 | Loss: 0.00102185
Iteration 14/25 | Loss: 0.00102185
Iteration 15/25 | Loss: 0.00102185
Iteration 16/25 | Loss: 0.00102185
Iteration 17/25 | Loss: 0.00102184
Iteration 18/25 | Loss: 0.00102184
Iteration 19/25 | Loss: 0.00102184
Iteration 20/25 | Loss: 0.00102184
Iteration 21/25 | Loss: 0.00102184
Iteration 22/25 | Loss: 0.00102184
Iteration 23/25 | Loss: 0.00102183
Iteration 24/25 | Loss: 0.00102183
Iteration 25/25 | Loss: 0.00102183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67492044
Iteration 2/25 | Loss: 0.00104393
Iteration 3/25 | Loss: 0.00104393
Iteration 4/25 | Loss: 0.00104393
Iteration 5/25 | Loss: 0.00104393
Iteration 6/25 | Loss: 0.00104393
Iteration 7/25 | Loss: 0.00104393
Iteration 8/25 | Loss: 0.00104393
Iteration 9/25 | Loss: 0.00104392
Iteration 10/25 | Loss: 0.00104392
Iteration 11/25 | Loss: 0.00104392
Iteration 12/25 | Loss: 0.00104392
Iteration 13/25 | Loss: 0.00104392
Iteration 14/25 | Loss: 0.00104392
Iteration 15/25 | Loss: 0.00104392
Iteration 16/25 | Loss: 0.00104392
Iteration 17/25 | Loss: 0.00104392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010439243633300066, 0.0010439243633300066, 0.0010439243633300066, 0.0010439243633300066, 0.0010439243633300066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010439243633300066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104392
Iteration 2/1000 | Loss: 0.00006289
Iteration 3/1000 | Loss: 0.00004042
Iteration 4/1000 | Loss: 0.00003412
Iteration 5/1000 | Loss: 0.00003253
Iteration 6/1000 | Loss: 0.00003144
Iteration 7/1000 | Loss: 0.00003065
Iteration 8/1000 | Loss: 0.00003010
Iteration 9/1000 | Loss: 0.00002940
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002901
Iteration 12/1000 | Loss: 0.00002893
Iteration 13/1000 | Loss: 0.00002883
Iteration 14/1000 | Loss: 0.00002883
Iteration 15/1000 | Loss: 0.00002868
Iteration 16/1000 | Loss: 0.00002868
Iteration 17/1000 | Loss: 0.00002866
Iteration 18/1000 | Loss: 0.00002865
Iteration 19/1000 | Loss: 0.00002860
Iteration 20/1000 | Loss: 0.00002855
Iteration 21/1000 | Loss: 0.00002854
Iteration 22/1000 | Loss: 0.00002854
Iteration 23/1000 | Loss: 0.00002853
Iteration 24/1000 | Loss: 0.00002853
Iteration 25/1000 | Loss: 0.00002852
Iteration 26/1000 | Loss: 0.00002852
Iteration 27/1000 | Loss: 0.00002852
Iteration 28/1000 | Loss: 0.00002851
Iteration 29/1000 | Loss: 0.00002851
Iteration 30/1000 | Loss: 0.00002850
Iteration 31/1000 | Loss: 0.00002850
Iteration 32/1000 | Loss: 0.00002850
Iteration 33/1000 | Loss: 0.00002849
Iteration 34/1000 | Loss: 0.00002848
Iteration 35/1000 | Loss: 0.00002848
Iteration 36/1000 | Loss: 0.00002848
Iteration 37/1000 | Loss: 0.00002847
Iteration 38/1000 | Loss: 0.00002846
Iteration 39/1000 | Loss: 0.00002845
Iteration 40/1000 | Loss: 0.00002844
Iteration 41/1000 | Loss: 0.00002842
Iteration 42/1000 | Loss: 0.00002842
Iteration 43/1000 | Loss: 0.00002841
Iteration 44/1000 | Loss: 0.00002841
Iteration 45/1000 | Loss: 0.00002841
Iteration 46/1000 | Loss: 0.00002840
Iteration 47/1000 | Loss: 0.00002840
Iteration 48/1000 | Loss: 0.00002840
Iteration 49/1000 | Loss: 0.00002839
Iteration 50/1000 | Loss: 0.00002839
Iteration 51/1000 | Loss: 0.00002839
Iteration 52/1000 | Loss: 0.00002838
Iteration 53/1000 | Loss: 0.00002838
Iteration 54/1000 | Loss: 0.00002837
Iteration 55/1000 | Loss: 0.00002836
Iteration 56/1000 | Loss: 0.00002836
Iteration 57/1000 | Loss: 0.00002835
Iteration 58/1000 | Loss: 0.00002835
Iteration 59/1000 | Loss: 0.00002835
Iteration 60/1000 | Loss: 0.00002835
Iteration 61/1000 | Loss: 0.00002834
Iteration 62/1000 | Loss: 0.00002834
Iteration 63/1000 | Loss: 0.00002833
Iteration 64/1000 | Loss: 0.00002833
Iteration 65/1000 | Loss: 0.00002833
Iteration 66/1000 | Loss: 0.00002832
Iteration 67/1000 | Loss: 0.00002832
Iteration 68/1000 | Loss: 0.00002832
Iteration 69/1000 | Loss: 0.00002832
Iteration 70/1000 | Loss: 0.00002832
Iteration 71/1000 | Loss: 0.00002832
Iteration 72/1000 | Loss: 0.00002832
Iteration 73/1000 | Loss: 0.00002832
Iteration 74/1000 | Loss: 0.00002831
Iteration 75/1000 | Loss: 0.00002831
Iteration 76/1000 | Loss: 0.00002831
Iteration 77/1000 | Loss: 0.00002831
Iteration 78/1000 | Loss: 0.00002831
Iteration 79/1000 | Loss: 0.00002831
Iteration 80/1000 | Loss: 0.00002831
Iteration 81/1000 | Loss: 0.00002831
Iteration 82/1000 | Loss: 0.00002830
Iteration 83/1000 | Loss: 0.00002830
Iteration 84/1000 | Loss: 0.00002830
Iteration 85/1000 | Loss: 0.00002830
Iteration 86/1000 | Loss: 0.00002830
Iteration 87/1000 | Loss: 0.00002830
Iteration 88/1000 | Loss: 0.00002830
Iteration 89/1000 | Loss: 0.00002829
Iteration 90/1000 | Loss: 0.00002829
Iteration 91/1000 | Loss: 0.00002829
Iteration 92/1000 | Loss: 0.00002829
Iteration 93/1000 | Loss: 0.00002828
Iteration 94/1000 | Loss: 0.00002828
Iteration 95/1000 | Loss: 0.00002828
Iteration 96/1000 | Loss: 0.00002828
Iteration 97/1000 | Loss: 0.00002828
Iteration 98/1000 | Loss: 0.00002828
Iteration 99/1000 | Loss: 0.00002827
Iteration 100/1000 | Loss: 0.00002827
Iteration 101/1000 | Loss: 0.00002827
Iteration 102/1000 | Loss: 0.00002827
Iteration 103/1000 | Loss: 0.00002827
Iteration 104/1000 | Loss: 0.00002826
Iteration 105/1000 | Loss: 0.00002826
Iteration 106/1000 | Loss: 0.00002826
Iteration 107/1000 | Loss: 0.00002825
Iteration 108/1000 | Loss: 0.00002825
Iteration 109/1000 | Loss: 0.00002825
Iteration 110/1000 | Loss: 0.00002825
Iteration 111/1000 | Loss: 0.00002825
Iteration 112/1000 | Loss: 0.00002824
Iteration 113/1000 | Loss: 0.00002824
Iteration 114/1000 | Loss: 0.00002824
Iteration 115/1000 | Loss: 0.00002824
Iteration 116/1000 | Loss: 0.00002824
Iteration 117/1000 | Loss: 0.00002823
Iteration 118/1000 | Loss: 0.00002823
Iteration 119/1000 | Loss: 0.00002823
Iteration 120/1000 | Loss: 0.00002823
Iteration 121/1000 | Loss: 0.00002823
Iteration 122/1000 | Loss: 0.00002823
Iteration 123/1000 | Loss: 0.00002823
Iteration 124/1000 | Loss: 0.00002823
Iteration 125/1000 | Loss: 0.00002823
Iteration 126/1000 | Loss: 0.00002822
Iteration 127/1000 | Loss: 0.00002822
Iteration 128/1000 | Loss: 0.00002822
Iteration 129/1000 | Loss: 0.00002822
Iteration 130/1000 | Loss: 0.00002822
Iteration 131/1000 | Loss: 0.00002822
Iteration 132/1000 | Loss: 0.00002822
Iteration 133/1000 | Loss: 0.00002822
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002822
Iteration 136/1000 | Loss: 0.00002822
Iteration 137/1000 | Loss: 0.00002821
Iteration 138/1000 | Loss: 0.00002821
Iteration 139/1000 | Loss: 0.00002821
Iteration 140/1000 | Loss: 0.00002821
Iteration 141/1000 | Loss: 0.00002821
Iteration 142/1000 | Loss: 0.00002821
Iteration 143/1000 | Loss: 0.00002821
Iteration 144/1000 | Loss: 0.00002821
Iteration 145/1000 | Loss: 0.00002821
Iteration 146/1000 | Loss: 0.00002821
Iteration 147/1000 | Loss: 0.00002821
Iteration 148/1000 | Loss: 0.00002821
Iteration 149/1000 | Loss: 0.00002821
Iteration 150/1000 | Loss: 0.00002821
Iteration 151/1000 | Loss: 0.00002820
Iteration 152/1000 | Loss: 0.00002820
Iteration 153/1000 | Loss: 0.00002820
Iteration 154/1000 | Loss: 0.00002820
Iteration 155/1000 | Loss: 0.00002820
Iteration 156/1000 | Loss: 0.00002820
Iteration 157/1000 | Loss: 0.00002820
Iteration 158/1000 | Loss: 0.00002820
Iteration 159/1000 | Loss: 0.00002820
Iteration 160/1000 | Loss: 0.00002820
Iteration 161/1000 | Loss: 0.00002820
Iteration 162/1000 | Loss: 0.00002820
Iteration 163/1000 | Loss: 0.00002820
Iteration 164/1000 | Loss: 0.00002819
Iteration 165/1000 | Loss: 0.00002819
Iteration 166/1000 | Loss: 0.00002819
Iteration 167/1000 | Loss: 0.00002819
Iteration 168/1000 | Loss: 0.00002819
Iteration 169/1000 | Loss: 0.00002819
Iteration 170/1000 | Loss: 0.00002819
Iteration 171/1000 | Loss: 0.00002819
Iteration 172/1000 | Loss: 0.00002819
Iteration 173/1000 | Loss: 0.00002819
Iteration 174/1000 | Loss: 0.00002819
Iteration 175/1000 | Loss: 0.00002819
Iteration 176/1000 | Loss: 0.00002819
Iteration 177/1000 | Loss: 0.00002819
Iteration 178/1000 | Loss: 0.00002819
Iteration 179/1000 | Loss: 0.00002819
Iteration 180/1000 | Loss: 0.00002818
Iteration 181/1000 | Loss: 0.00002818
Iteration 182/1000 | Loss: 0.00002818
Iteration 183/1000 | Loss: 0.00002818
Iteration 184/1000 | Loss: 0.00002818
Iteration 185/1000 | Loss: 0.00002818
Iteration 186/1000 | Loss: 0.00002818
Iteration 187/1000 | Loss: 0.00002818
Iteration 188/1000 | Loss: 0.00002818
Iteration 189/1000 | Loss: 0.00002818
Iteration 190/1000 | Loss: 0.00002818
Iteration 191/1000 | Loss: 0.00002818
Iteration 192/1000 | Loss: 0.00002818
Iteration 193/1000 | Loss: 0.00002818
Iteration 194/1000 | Loss: 0.00002818
Iteration 195/1000 | Loss: 0.00002818
Iteration 196/1000 | Loss: 0.00002818
Iteration 197/1000 | Loss: 0.00002818
Iteration 198/1000 | Loss: 0.00002818
Iteration 199/1000 | Loss: 0.00002818
Iteration 200/1000 | Loss: 0.00002818
Iteration 201/1000 | Loss: 0.00002818
Iteration 202/1000 | Loss: 0.00002818
Iteration 203/1000 | Loss: 0.00002818
Iteration 204/1000 | Loss: 0.00002818
Iteration 205/1000 | Loss: 0.00002818
Iteration 206/1000 | Loss: 0.00002818
Iteration 207/1000 | Loss: 0.00002818
Iteration 208/1000 | Loss: 0.00002818
Iteration 209/1000 | Loss: 0.00002818
Iteration 210/1000 | Loss: 0.00002818
Iteration 211/1000 | Loss: 0.00002818
Iteration 212/1000 | Loss: 0.00002818
Iteration 213/1000 | Loss: 0.00002818
Iteration 214/1000 | Loss: 0.00002818
Iteration 215/1000 | Loss: 0.00002818
Iteration 216/1000 | Loss: 0.00002818
Iteration 217/1000 | Loss: 0.00002818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.81815755442949e-05, 2.81815755442949e-05, 2.81815755442949e-05, 2.81815755442949e-05, 2.81815755442949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.81815755442949e-05

Optimization complete. Final v2v error: 4.379148483276367 mm

Highest mean error: 5.123566150665283 mm for frame 5

Lowest mean error: 3.562225103378296 mm for frame 137

Saving results

Total time: 50.01461100578308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901720
Iteration 2/25 | Loss: 0.00177650
Iteration 3/25 | Loss: 0.00107976
Iteration 4/25 | Loss: 0.00097046
Iteration 5/25 | Loss: 0.00094581
Iteration 6/25 | Loss: 0.00093290
Iteration 7/25 | Loss: 0.00091818
Iteration 8/25 | Loss: 0.00091659
Iteration 9/25 | Loss: 0.00091902
Iteration 10/25 | Loss: 0.00091167
Iteration 11/25 | Loss: 0.00090630
Iteration 12/25 | Loss: 0.00090151
Iteration 13/25 | Loss: 0.00089850
Iteration 14/25 | Loss: 0.00089949
Iteration 15/25 | Loss: 0.00089891
Iteration 16/25 | Loss: 0.00089576
Iteration 17/25 | Loss: 0.00089552
Iteration 18/25 | Loss: 0.00089538
Iteration 19/25 | Loss: 0.00089535
Iteration 20/25 | Loss: 0.00089535
Iteration 21/25 | Loss: 0.00089534
Iteration 22/25 | Loss: 0.00089534
Iteration 23/25 | Loss: 0.00089534
Iteration 24/25 | Loss: 0.00089534
Iteration 25/25 | Loss: 0.00089534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.21787930
Iteration 2/25 | Loss: 0.00131257
Iteration 3/25 | Loss: 0.00120315
Iteration 4/25 | Loss: 0.00120315
Iteration 5/25 | Loss: 0.00120315
Iteration 6/25 | Loss: 0.00120315
Iteration 7/25 | Loss: 0.00120315
Iteration 8/25 | Loss: 0.00120315
Iteration 9/25 | Loss: 0.00120315
Iteration 10/25 | Loss: 0.00120315
Iteration 11/25 | Loss: 0.00120315
Iteration 12/25 | Loss: 0.00120314
Iteration 13/25 | Loss: 0.00120314
Iteration 14/25 | Loss: 0.00120314
Iteration 15/25 | Loss: 0.00120314
Iteration 16/25 | Loss: 0.00120314
Iteration 17/25 | Loss: 0.00120314
Iteration 18/25 | Loss: 0.00120314
Iteration 19/25 | Loss: 0.00120314
Iteration 20/25 | Loss: 0.00120314
Iteration 21/25 | Loss: 0.00120314
Iteration 22/25 | Loss: 0.00120314
Iteration 23/25 | Loss: 0.00120314
Iteration 24/25 | Loss: 0.00120314
Iteration 25/25 | Loss: 0.00120314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012031447840854526, 0.0012031447840854526, 0.0012031447840854526, 0.0012031447840854526, 0.0012031447840854526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012031447840854526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120314
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00009395
Iteration 4/1000 | Loss: 0.00002631
Iteration 5/1000 | Loss: 0.00002478
Iteration 6/1000 | Loss: 0.00015353
Iteration 7/1000 | Loss: 0.00008317
Iteration 8/1000 | Loss: 0.00005553
Iteration 9/1000 | Loss: 0.00005263
Iteration 10/1000 | Loss: 0.00002326
Iteration 11/1000 | Loss: 0.00002280
Iteration 12/1000 | Loss: 0.00007766
Iteration 13/1000 | Loss: 0.00002238
Iteration 14/1000 | Loss: 0.00002213
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002196
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002185
Iteration 19/1000 | Loss: 0.00002182
Iteration 20/1000 | Loss: 0.00002181
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002180
Iteration 23/1000 | Loss: 0.00002179
Iteration 24/1000 | Loss: 0.00002179
Iteration 25/1000 | Loss: 0.00002176
Iteration 26/1000 | Loss: 0.00002174
Iteration 27/1000 | Loss: 0.00002173
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002170
Iteration 34/1000 | Loss: 0.00002169
Iteration 35/1000 | Loss: 0.00002169
Iteration 36/1000 | Loss: 0.00002169
Iteration 37/1000 | Loss: 0.00002169
Iteration 38/1000 | Loss: 0.00002169
Iteration 39/1000 | Loss: 0.00002169
Iteration 40/1000 | Loss: 0.00002168
Iteration 41/1000 | Loss: 0.00002167
Iteration 42/1000 | Loss: 0.00002165
Iteration 43/1000 | Loss: 0.00002165
Iteration 44/1000 | Loss: 0.00002164
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002163
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002163
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002162
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002159
Iteration 64/1000 | Loss: 0.00002159
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002157
Iteration 67/1000 | Loss: 0.00002157
Iteration 68/1000 | Loss: 0.00002157
Iteration 69/1000 | Loss: 0.00002157
Iteration 70/1000 | Loss: 0.00002156
Iteration 71/1000 | Loss: 0.00002156
Iteration 72/1000 | Loss: 0.00002156
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002155
Iteration 75/1000 | Loss: 0.00002155
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002154
Iteration 79/1000 | Loss: 0.00002154
Iteration 80/1000 | Loss: 0.00002154
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002153
Iteration 85/1000 | Loss: 0.00002153
Iteration 86/1000 | Loss: 0.00002152
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002152
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00006582
Iteration 95/1000 | Loss: 0.00027760
Iteration 96/1000 | Loss: 0.00002162
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00006471
Iteration 103/1000 | Loss: 0.00029524
Iteration 104/1000 | Loss: 0.00025689
Iteration 105/1000 | Loss: 0.00028114
Iteration 106/1000 | Loss: 0.00022260
Iteration 107/1000 | Loss: 0.00026532
Iteration 108/1000 | Loss: 0.00009456
Iteration 109/1000 | Loss: 0.00002210
Iteration 110/1000 | Loss: 0.00002169
Iteration 111/1000 | Loss: 0.00002158
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002150
Iteration 114/1000 | Loss: 0.00002150
Iteration 115/1000 | Loss: 0.00002149
Iteration 116/1000 | Loss: 0.00002149
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002147
Iteration 119/1000 | Loss: 0.00002147
Iteration 120/1000 | Loss: 0.00002146
Iteration 121/1000 | Loss: 0.00008941
Iteration 122/1000 | Loss: 0.00007842
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002148
Iteration 125/1000 | Loss: 0.00002147
Iteration 126/1000 | Loss: 0.00002147
Iteration 127/1000 | Loss: 0.00002147
Iteration 128/1000 | Loss: 0.00002146
Iteration 129/1000 | Loss: 0.00002146
Iteration 130/1000 | Loss: 0.00002146
Iteration 131/1000 | Loss: 0.00002145
Iteration 132/1000 | Loss: 0.00002145
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002144
Iteration 135/1000 | Loss: 0.00002144
Iteration 136/1000 | Loss: 0.00002144
Iteration 137/1000 | Loss: 0.00002144
Iteration 138/1000 | Loss: 0.00002144
Iteration 139/1000 | Loss: 0.00002143
Iteration 140/1000 | Loss: 0.00002143
Iteration 141/1000 | Loss: 0.00002143
Iteration 142/1000 | Loss: 0.00002143
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002142
Iteration 145/1000 | Loss: 0.00002142
Iteration 146/1000 | Loss: 0.00002142
Iteration 147/1000 | Loss: 0.00002142
Iteration 148/1000 | Loss: 0.00002142
Iteration 149/1000 | Loss: 0.00002142
Iteration 150/1000 | Loss: 0.00002142
Iteration 151/1000 | Loss: 0.00002142
Iteration 152/1000 | Loss: 0.00002142
Iteration 153/1000 | Loss: 0.00002142
Iteration 154/1000 | Loss: 0.00002142
Iteration 155/1000 | Loss: 0.00002142
Iteration 156/1000 | Loss: 0.00002142
Iteration 157/1000 | Loss: 0.00002142
Iteration 158/1000 | Loss: 0.00002142
Iteration 159/1000 | Loss: 0.00002141
Iteration 160/1000 | Loss: 0.00002141
Iteration 161/1000 | Loss: 0.00002141
Iteration 162/1000 | Loss: 0.00002141
Iteration 163/1000 | Loss: 0.00002141
Iteration 164/1000 | Loss: 0.00002141
Iteration 165/1000 | Loss: 0.00002141
Iteration 166/1000 | Loss: 0.00002141
Iteration 167/1000 | Loss: 0.00002141
Iteration 168/1000 | Loss: 0.00002141
Iteration 169/1000 | Loss: 0.00002141
Iteration 170/1000 | Loss: 0.00002141
Iteration 171/1000 | Loss: 0.00002141
Iteration 172/1000 | Loss: 0.00002141
Iteration 173/1000 | Loss: 0.00002141
Iteration 174/1000 | Loss: 0.00002141
Iteration 175/1000 | Loss: 0.00002141
Iteration 176/1000 | Loss: 0.00002141
Iteration 177/1000 | Loss: 0.00002141
Iteration 178/1000 | Loss: 0.00002141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.141321965609677e-05, 2.141321965609677e-05, 2.141321965609677e-05, 2.141321965609677e-05, 2.141321965609677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.141321965609677e-05

Optimization complete. Final v2v error: 3.914743185043335 mm

Highest mean error: 10.439911842346191 mm for frame 52

Lowest mean error: 3.332369565963745 mm for frame 15

Saving results

Total time: 91.57110095024109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412967
Iteration 2/25 | Loss: 0.00103688
Iteration 3/25 | Loss: 0.00093876
Iteration 4/25 | Loss: 0.00092146
Iteration 5/25 | Loss: 0.00091260
Iteration 6/25 | Loss: 0.00091050
Iteration 7/25 | Loss: 0.00091008
Iteration 8/25 | Loss: 0.00091008
Iteration 9/25 | Loss: 0.00091008
Iteration 10/25 | Loss: 0.00091008
Iteration 11/25 | Loss: 0.00091008
Iteration 12/25 | Loss: 0.00091008
Iteration 13/25 | Loss: 0.00091008
Iteration 14/25 | Loss: 0.00091008
Iteration 15/25 | Loss: 0.00091008
Iteration 16/25 | Loss: 0.00091008
Iteration 17/25 | Loss: 0.00091008
Iteration 18/25 | Loss: 0.00091008
Iteration 19/25 | Loss: 0.00091008
Iteration 20/25 | Loss: 0.00091008
Iteration 21/25 | Loss: 0.00091008
Iteration 22/25 | Loss: 0.00091008
Iteration 23/25 | Loss: 0.00091008
Iteration 24/25 | Loss: 0.00091008
Iteration 25/25 | Loss: 0.00091008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76601636
Iteration 2/25 | Loss: 0.00118248
Iteration 3/25 | Loss: 0.00118248
Iteration 4/25 | Loss: 0.00118248
Iteration 5/25 | Loss: 0.00118248
Iteration 6/25 | Loss: 0.00118248
Iteration 7/25 | Loss: 0.00118248
Iteration 8/25 | Loss: 0.00118248
Iteration 9/25 | Loss: 0.00118248
Iteration 10/25 | Loss: 0.00118248
Iteration 11/25 | Loss: 0.00118248
Iteration 12/25 | Loss: 0.00118248
Iteration 13/25 | Loss: 0.00118248
Iteration 14/25 | Loss: 0.00118248
Iteration 15/25 | Loss: 0.00118248
Iteration 16/25 | Loss: 0.00118248
Iteration 17/25 | Loss: 0.00118248
Iteration 18/25 | Loss: 0.00118248
Iteration 19/25 | Loss: 0.00118248
Iteration 20/25 | Loss: 0.00118248
Iteration 21/25 | Loss: 0.00118248
Iteration 22/25 | Loss: 0.00118248
Iteration 23/25 | Loss: 0.00118248
Iteration 24/25 | Loss: 0.00118248
Iteration 25/25 | Loss: 0.00118248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118248
Iteration 2/1000 | Loss: 0.00003900
Iteration 3/1000 | Loss: 0.00002991
Iteration 4/1000 | Loss: 0.00002533
Iteration 5/1000 | Loss: 0.00002366
Iteration 6/1000 | Loss: 0.00002225
Iteration 7/1000 | Loss: 0.00002144
Iteration 8/1000 | Loss: 0.00002087
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002012
Iteration 18/1000 | Loss: 0.00002012
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002010
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002004
Iteration 23/1000 | Loss: 0.00002003
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002002
Iteration 26/1000 | Loss: 0.00002002
Iteration 27/1000 | Loss: 0.00002000
Iteration 28/1000 | Loss: 0.00001998
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001997
Iteration 32/1000 | Loss: 0.00001997
Iteration 33/1000 | Loss: 0.00001997
Iteration 34/1000 | Loss: 0.00001997
Iteration 35/1000 | Loss: 0.00001997
Iteration 36/1000 | Loss: 0.00001997
Iteration 37/1000 | Loss: 0.00001997
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001997
Iteration 41/1000 | Loss: 0.00001997
Iteration 42/1000 | Loss: 0.00001997
Iteration 43/1000 | Loss: 0.00001997
Iteration 44/1000 | Loss: 0.00001997
Iteration 45/1000 | Loss: 0.00001997
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001994
Iteration 51/1000 | Loss: 0.00001994
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001993
Iteration 55/1000 | Loss: 0.00001992
Iteration 56/1000 | Loss: 0.00001992
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001990
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001987
Iteration 76/1000 | Loss: 0.00001987
Iteration 77/1000 | Loss: 0.00001987
Iteration 78/1000 | Loss: 0.00001987
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001986
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001986
Iteration 85/1000 | Loss: 0.00001986
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001985
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001985
Iteration 94/1000 | Loss: 0.00001985
Iteration 95/1000 | Loss: 0.00001985
Iteration 96/1000 | Loss: 0.00001985
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001983
Iteration 108/1000 | Loss: 0.00001983
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001979
Iteration 123/1000 | Loss: 0.00001979
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001978
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001978
Iteration 137/1000 | Loss: 0.00001978
Iteration 138/1000 | Loss: 0.00001978
Iteration 139/1000 | Loss: 0.00001978
Iteration 140/1000 | Loss: 0.00001977
Iteration 141/1000 | Loss: 0.00001977
Iteration 142/1000 | Loss: 0.00001977
Iteration 143/1000 | Loss: 0.00001977
Iteration 144/1000 | Loss: 0.00001977
Iteration 145/1000 | Loss: 0.00001977
Iteration 146/1000 | Loss: 0.00001977
Iteration 147/1000 | Loss: 0.00001977
Iteration 148/1000 | Loss: 0.00001977
Iteration 149/1000 | Loss: 0.00001977
Iteration 150/1000 | Loss: 0.00001977
Iteration 151/1000 | Loss: 0.00001977
Iteration 152/1000 | Loss: 0.00001976
Iteration 153/1000 | Loss: 0.00001976
Iteration 154/1000 | Loss: 0.00001976
Iteration 155/1000 | Loss: 0.00001976
Iteration 156/1000 | Loss: 0.00001976
Iteration 157/1000 | Loss: 0.00001976
Iteration 158/1000 | Loss: 0.00001976
Iteration 159/1000 | Loss: 0.00001976
Iteration 160/1000 | Loss: 0.00001976
Iteration 161/1000 | Loss: 0.00001976
Iteration 162/1000 | Loss: 0.00001976
Iteration 163/1000 | Loss: 0.00001976
Iteration 164/1000 | Loss: 0.00001975
Iteration 165/1000 | Loss: 0.00001975
Iteration 166/1000 | Loss: 0.00001975
Iteration 167/1000 | Loss: 0.00001975
Iteration 168/1000 | Loss: 0.00001975
Iteration 169/1000 | Loss: 0.00001975
Iteration 170/1000 | Loss: 0.00001975
Iteration 171/1000 | Loss: 0.00001975
Iteration 172/1000 | Loss: 0.00001975
Iteration 173/1000 | Loss: 0.00001975
Iteration 174/1000 | Loss: 0.00001975
Iteration 175/1000 | Loss: 0.00001975
Iteration 176/1000 | Loss: 0.00001975
Iteration 177/1000 | Loss: 0.00001975
Iteration 178/1000 | Loss: 0.00001975
Iteration 179/1000 | Loss: 0.00001975
Iteration 180/1000 | Loss: 0.00001975
Iteration 181/1000 | Loss: 0.00001975
Iteration 182/1000 | Loss: 0.00001975
Iteration 183/1000 | Loss: 0.00001975
Iteration 184/1000 | Loss: 0.00001975
Iteration 185/1000 | Loss: 0.00001975
Iteration 186/1000 | Loss: 0.00001975
Iteration 187/1000 | Loss: 0.00001975
Iteration 188/1000 | Loss: 0.00001975
Iteration 189/1000 | Loss: 0.00001975
Iteration 190/1000 | Loss: 0.00001975
Iteration 191/1000 | Loss: 0.00001975
Iteration 192/1000 | Loss: 0.00001975
Iteration 193/1000 | Loss: 0.00001975
Iteration 194/1000 | Loss: 0.00001975
Iteration 195/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.9746961697819643e-05, 1.9746961697819643e-05, 1.9746961697819643e-05, 1.9746961697819643e-05, 1.9746961697819643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9746961697819643e-05

Optimization complete. Final v2v error: 3.8087475299835205 mm

Highest mean error: 4.260162830352783 mm for frame 73

Lowest mean error: 3.507373094558716 mm for frame 86

Saving results

Total time: 39.35240459442139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688762
Iteration 2/25 | Loss: 0.00118344
Iteration 3/25 | Loss: 0.00097464
Iteration 4/25 | Loss: 0.00095449
Iteration 5/25 | Loss: 0.00094666
Iteration 6/25 | Loss: 0.00094476
Iteration 7/25 | Loss: 0.00094476
Iteration 8/25 | Loss: 0.00094476
Iteration 9/25 | Loss: 0.00094476
Iteration 10/25 | Loss: 0.00094476
Iteration 11/25 | Loss: 0.00094476
Iteration 12/25 | Loss: 0.00094476
Iteration 13/25 | Loss: 0.00094476
Iteration 14/25 | Loss: 0.00094476
Iteration 15/25 | Loss: 0.00094476
Iteration 16/25 | Loss: 0.00094476
Iteration 17/25 | Loss: 0.00094476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000944764818996191, 0.000944764818996191, 0.000944764818996191, 0.000944764818996191, 0.000944764818996191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000944764818996191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90238667
Iteration 2/25 | Loss: 0.00084521
Iteration 3/25 | Loss: 0.00084521
Iteration 4/25 | Loss: 0.00084521
Iteration 5/25 | Loss: 0.00084521
Iteration 6/25 | Loss: 0.00084521
Iteration 7/25 | Loss: 0.00084521
Iteration 8/25 | Loss: 0.00084521
Iteration 9/25 | Loss: 0.00084521
Iteration 10/25 | Loss: 0.00084521
Iteration 11/25 | Loss: 0.00084521
Iteration 12/25 | Loss: 0.00084521
Iteration 13/25 | Loss: 0.00084521
Iteration 14/25 | Loss: 0.00084521
Iteration 15/25 | Loss: 0.00084521
Iteration 16/25 | Loss: 0.00084521
Iteration 17/25 | Loss: 0.00084521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008452060865238309, 0.0008452060865238309, 0.0008452060865238309, 0.0008452060865238309, 0.0008452060865238309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008452060865238309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084521
Iteration 2/1000 | Loss: 0.00003845
Iteration 3/1000 | Loss: 0.00002735
Iteration 4/1000 | Loss: 0.00002537
Iteration 5/1000 | Loss: 0.00002441
Iteration 6/1000 | Loss: 0.00002384
Iteration 7/1000 | Loss: 0.00002342
Iteration 8/1000 | Loss: 0.00002305
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00002285
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002277
Iteration 13/1000 | Loss: 0.00002264
Iteration 14/1000 | Loss: 0.00002263
Iteration 15/1000 | Loss: 0.00002262
Iteration 16/1000 | Loss: 0.00002258
Iteration 17/1000 | Loss: 0.00002258
Iteration 18/1000 | Loss: 0.00002256
Iteration 19/1000 | Loss: 0.00002255
Iteration 20/1000 | Loss: 0.00002255
Iteration 21/1000 | Loss: 0.00002254
Iteration 22/1000 | Loss: 0.00002254
Iteration 23/1000 | Loss: 0.00002254
Iteration 24/1000 | Loss: 0.00002253
Iteration 25/1000 | Loss: 0.00002253
Iteration 26/1000 | Loss: 0.00002253
Iteration 27/1000 | Loss: 0.00002253
Iteration 28/1000 | Loss: 0.00002252
Iteration 29/1000 | Loss: 0.00002252
Iteration 30/1000 | Loss: 0.00002252
Iteration 31/1000 | Loss: 0.00002252
Iteration 32/1000 | Loss: 0.00002251
Iteration 33/1000 | Loss: 0.00002251
Iteration 34/1000 | Loss: 0.00002250
Iteration 35/1000 | Loss: 0.00002250
Iteration 36/1000 | Loss: 0.00002249
Iteration 37/1000 | Loss: 0.00002249
Iteration 38/1000 | Loss: 0.00002249
Iteration 39/1000 | Loss: 0.00002248
Iteration 40/1000 | Loss: 0.00002248
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00002248
Iteration 44/1000 | Loss: 0.00002247
Iteration 45/1000 | Loss: 0.00002247
Iteration 46/1000 | Loss: 0.00002247
Iteration 47/1000 | Loss: 0.00002247
Iteration 48/1000 | Loss: 0.00002247
Iteration 49/1000 | Loss: 0.00002247
Iteration 50/1000 | Loss: 0.00002247
Iteration 51/1000 | Loss: 0.00002247
Iteration 52/1000 | Loss: 0.00002247
Iteration 53/1000 | Loss: 0.00002247
Iteration 54/1000 | Loss: 0.00002246
Iteration 55/1000 | Loss: 0.00002245
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00002244
Iteration 59/1000 | Loss: 0.00002244
Iteration 60/1000 | Loss: 0.00002243
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002242
Iteration 63/1000 | Loss: 0.00002242
Iteration 64/1000 | Loss: 0.00002241
Iteration 65/1000 | Loss: 0.00002241
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00002240
Iteration 68/1000 | Loss: 0.00002239
Iteration 69/1000 | Loss: 0.00002239
Iteration 70/1000 | Loss: 0.00002238
Iteration 71/1000 | Loss: 0.00002238
Iteration 72/1000 | Loss: 0.00002238
Iteration 73/1000 | Loss: 0.00002238
Iteration 74/1000 | Loss: 0.00002237
Iteration 75/1000 | Loss: 0.00002237
Iteration 76/1000 | Loss: 0.00002237
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002235
Iteration 82/1000 | Loss: 0.00002235
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00002235
Iteration 85/1000 | Loss: 0.00002235
Iteration 86/1000 | Loss: 0.00002235
Iteration 87/1000 | Loss: 0.00002235
Iteration 88/1000 | Loss: 0.00002235
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002232
Iteration 109/1000 | Loss: 0.00002231
Iteration 110/1000 | Loss: 0.00002231
Iteration 111/1000 | Loss: 0.00002231
Iteration 112/1000 | Loss: 0.00002231
Iteration 113/1000 | Loss: 0.00002231
Iteration 114/1000 | Loss: 0.00002231
Iteration 115/1000 | Loss: 0.00002231
Iteration 116/1000 | Loss: 0.00002231
Iteration 117/1000 | Loss: 0.00002231
Iteration 118/1000 | Loss: 0.00002231
Iteration 119/1000 | Loss: 0.00002231
Iteration 120/1000 | Loss: 0.00002231
Iteration 121/1000 | Loss: 0.00002231
Iteration 122/1000 | Loss: 0.00002231
Iteration 123/1000 | Loss: 0.00002231
Iteration 124/1000 | Loss: 0.00002230
Iteration 125/1000 | Loss: 0.00002230
Iteration 126/1000 | Loss: 0.00002230
Iteration 127/1000 | Loss: 0.00002230
Iteration 128/1000 | Loss: 0.00002230
Iteration 129/1000 | Loss: 0.00002230
Iteration 130/1000 | Loss: 0.00002230
Iteration 131/1000 | Loss: 0.00002230
Iteration 132/1000 | Loss: 0.00002230
Iteration 133/1000 | Loss: 0.00002229
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002229
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002229
Iteration 138/1000 | Loss: 0.00002229
Iteration 139/1000 | Loss: 0.00002229
Iteration 140/1000 | Loss: 0.00002229
Iteration 141/1000 | Loss: 0.00002229
Iteration 142/1000 | Loss: 0.00002229
Iteration 143/1000 | Loss: 0.00002229
Iteration 144/1000 | Loss: 0.00002229
Iteration 145/1000 | Loss: 0.00002229
Iteration 146/1000 | Loss: 0.00002229
Iteration 147/1000 | Loss: 0.00002229
Iteration 148/1000 | Loss: 0.00002229
Iteration 149/1000 | Loss: 0.00002229
Iteration 150/1000 | Loss: 0.00002229
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.2290862034424208e-05, 2.2290862034424208e-05, 2.2290862034424208e-05, 2.2290862034424208e-05, 2.2290862034424208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2290862034424208e-05

Optimization complete. Final v2v error: 4.060375213623047 mm

Highest mean error: 4.496504783630371 mm for frame 61

Lowest mean error: 3.7416586875915527 mm for frame 4

Saving results

Total time: 35.462923526763916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089660
Iteration 2/25 | Loss: 0.00379529
Iteration 3/25 | Loss: 0.00255480
Iteration 4/25 | Loss: 0.00228078
Iteration 5/25 | Loss: 0.00194575
Iteration 6/25 | Loss: 0.00183214
Iteration 7/25 | Loss: 0.00172807
Iteration 8/25 | Loss: 0.00172111
Iteration 9/25 | Loss: 0.00164428
Iteration 10/25 | Loss: 0.00152724
Iteration 11/25 | Loss: 0.00143621
Iteration 12/25 | Loss: 0.00138750
Iteration 13/25 | Loss: 0.00137471
Iteration 14/25 | Loss: 0.00138758
Iteration 15/25 | Loss: 0.00135927
Iteration 16/25 | Loss: 0.00135178
Iteration 17/25 | Loss: 0.00134715
Iteration 18/25 | Loss: 0.00134668
Iteration 19/25 | Loss: 0.00134651
Iteration 20/25 | Loss: 0.00133986
Iteration 21/25 | Loss: 0.00133456
Iteration 22/25 | Loss: 0.00133334
Iteration 23/25 | Loss: 0.00133301
Iteration 24/25 | Loss: 0.00133283
Iteration 25/25 | Loss: 0.00133417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24168992
Iteration 2/25 | Loss: 0.00520411
Iteration 3/25 | Loss: 0.00355440
Iteration 4/25 | Loss: 0.00355440
Iteration 5/25 | Loss: 0.00355440
Iteration 6/25 | Loss: 0.00355440
Iteration 7/25 | Loss: 0.00355440
Iteration 8/25 | Loss: 0.00355439
Iteration 9/25 | Loss: 0.00355439
Iteration 10/25 | Loss: 0.00355439
Iteration 11/25 | Loss: 0.00355439
Iteration 12/25 | Loss: 0.00355439
Iteration 13/25 | Loss: 0.00355439
Iteration 14/25 | Loss: 0.00355439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0035543947014957666, 0.0035543947014957666, 0.0035543947014957666, 0.0035543947014957666, 0.0035543947014957666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035543947014957666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00355439
Iteration 2/1000 | Loss: 0.00212592
Iteration 3/1000 | Loss: 0.00038235
Iteration 4/1000 | Loss: 0.00033106
Iteration 5/1000 | Loss: 0.00046377
Iteration 6/1000 | Loss: 0.00028296
Iteration 7/1000 | Loss: 0.00051003
Iteration 8/1000 | Loss: 0.00052900
Iteration 9/1000 | Loss: 0.00023200
Iteration 10/1000 | Loss: 0.00050850
Iteration 11/1000 | Loss: 0.00039820
Iteration 12/1000 | Loss: 0.00020690
Iteration 13/1000 | Loss: 0.00019640
Iteration 14/1000 | Loss: 0.00018834
Iteration 15/1000 | Loss: 0.00018204
Iteration 16/1000 | Loss: 0.00017729
Iteration 17/1000 | Loss: 0.00095238
Iteration 18/1000 | Loss: 0.00425755
Iteration 19/1000 | Loss: 0.00164917
Iteration 20/1000 | Loss: 0.00357397
Iteration 21/1000 | Loss: 0.00188172
Iteration 22/1000 | Loss: 0.00239568
Iteration 23/1000 | Loss: 0.00106926
Iteration 24/1000 | Loss: 0.00180642
Iteration 25/1000 | Loss: 0.00128736
Iteration 26/1000 | Loss: 0.00066186
Iteration 27/1000 | Loss: 0.00086026
Iteration 28/1000 | Loss: 0.00100098
Iteration 29/1000 | Loss: 0.00075293
Iteration 30/1000 | Loss: 0.00059997
Iteration 31/1000 | Loss: 0.00173981
Iteration 32/1000 | Loss: 0.00049216
Iteration 33/1000 | Loss: 0.00030530
Iteration 34/1000 | Loss: 0.00044276
Iteration 35/1000 | Loss: 0.00044422
Iteration 36/1000 | Loss: 0.00036543
Iteration 37/1000 | Loss: 0.00080001
Iteration 38/1000 | Loss: 0.00035558
Iteration 39/1000 | Loss: 0.00034208
Iteration 40/1000 | Loss: 0.00021264
Iteration 41/1000 | Loss: 0.00016254
Iteration 42/1000 | Loss: 0.00019152
Iteration 43/1000 | Loss: 0.00030278
Iteration 44/1000 | Loss: 0.00038375
Iteration 45/1000 | Loss: 0.00021954
Iteration 46/1000 | Loss: 0.00042356
Iteration 47/1000 | Loss: 0.00122912
Iteration 48/1000 | Loss: 0.00030264
Iteration 49/1000 | Loss: 0.00024057
Iteration 50/1000 | Loss: 0.00031816
Iteration 51/1000 | Loss: 0.00025634
Iteration 52/1000 | Loss: 0.00011229
Iteration 53/1000 | Loss: 0.00025514
Iteration 54/1000 | Loss: 0.00028710
Iteration 55/1000 | Loss: 0.00041838
Iteration 56/1000 | Loss: 0.00021646
Iteration 57/1000 | Loss: 0.00033990
Iteration 58/1000 | Loss: 0.00018692
Iteration 59/1000 | Loss: 0.00029687
Iteration 60/1000 | Loss: 0.00024221
Iteration 61/1000 | Loss: 0.00016896
Iteration 62/1000 | Loss: 0.00017431
Iteration 63/1000 | Loss: 0.00017158
Iteration 64/1000 | Loss: 0.00015906
Iteration 65/1000 | Loss: 0.00009408
Iteration 66/1000 | Loss: 0.00031241
Iteration 67/1000 | Loss: 0.00010087
Iteration 68/1000 | Loss: 0.00009163
Iteration 69/1000 | Loss: 0.00008666
Iteration 70/1000 | Loss: 0.00008283
Iteration 71/1000 | Loss: 0.00007982
Iteration 72/1000 | Loss: 0.00007800
Iteration 73/1000 | Loss: 0.00007702
Iteration 74/1000 | Loss: 0.00007655
Iteration 75/1000 | Loss: 0.00007600
Iteration 76/1000 | Loss: 0.00007565
Iteration 77/1000 | Loss: 0.00007537
Iteration 78/1000 | Loss: 0.00007512
Iteration 79/1000 | Loss: 0.00007497
Iteration 80/1000 | Loss: 0.00007489
Iteration 81/1000 | Loss: 0.00007482
Iteration 82/1000 | Loss: 0.00007478
Iteration 83/1000 | Loss: 0.00007477
Iteration 84/1000 | Loss: 0.00007477
Iteration 85/1000 | Loss: 0.00007476
Iteration 86/1000 | Loss: 0.00007475
Iteration 87/1000 | Loss: 0.00007475
Iteration 88/1000 | Loss: 0.00007474
Iteration 89/1000 | Loss: 0.00007473
Iteration 90/1000 | Loss: 0.00007472
Iteration 91/1000 | Loss: 0.00007472
Iteration 92/1000 | Loss: 0.00007472
Iteration 93/1000 | Loss: 0.00007471
Iteration 94/1000 | Loss: 0.00007471
Iteration 95/1000 | Loss: 0.00007471
Iteration 96/1000 | Loss: 0.00007471
Iteration 97/1000 | Loss: 0.00007471
Iteration 98/1000 | Loss: 0.00007471
Iteration 99/1000 | Loss: 0.00007470
Iteration 100/1000 | Loss: 0.00007470
Iteration 101/1000 | Loss: 0.00007470
Iteration 102/1000 | Loss: 0.00007470
Iteration 103/1000 | Loss: 0.00007470
Iteration 104/1000 | Loss: 0.00007470
Iteration 105/1000 | Loss: 0.00007470
Iteration 106/1000 | Loss: 0.00007470
Iteration 107/1000 | Loss: 0.00007470
Iteration 108/1000 | Loss: 0.00007470
Iteration 109/1000 | Loss: 0.00007469
Iteration 110/1000 | Loss: 0.00007469
Iteration 111/1000 | Loss: 0.00007469
Iteration 112/1000 | Loss: 0.00007469
Iteration 113/1000 | Loss: 0.00007468
Iteration 114/1000 | Loss: 0.00007468
Iteration 115/1000 | Loss: 0.00007468
Iteration 116/1000 | Loss: 0.00007468
Iteration 117/1000 | Loss: 0.00007468
Iteration 118/1000 | Loss: 0.00007467
Iteration 119/1000 | Loss: 0.00007467
Iteration 120/1000 | Loss: 0.00007467
Iteration 121/1000 | Loss: 0.00007467
Iteration 122/1000 | Loss: 0.00007467
Iteration 123/1000 | Loss: 0.00007467
Iteration 124/1000 | Loss: 0.00007467
Iteration 125/1000 | Loss: 0.00007467
Iteration 126/1000 | Loss: 0.00007467
Iteration 127/1000 | Loss: 0.00007467
Iteration 128/1000 | Loss: 0.00007467
Iteration 129/1000 | Loss: 0.00007467
Iteration 130/1000 | Loss: 0.00007467
Iteration 131/1000 | Loss: 0.00007467
Iteration 132/1000 | Loss: 0.00007467
Iteration 133/1000 | Loss: 0.00007466
Iteration 134/1000 | Loss: 0.00007466
Iteration 135/1000 | Loss: 0.00007466
Iteration 136/1000 | Loss: 0.00007466
Iteration 137/1000 | Loss: 0.00007466
Iteration 138/1000 | Loss: 0.00007466
Iteration 139/1000 | Loss: 0.00007466
Iteration 140/1000 | Loss: 0.00007466
Iteration 141/1000 | Loss: 0.00007466
Iteration 142/1000 | Loss: 0.00007466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [7.466323586413637e-05, 7.466323586413637e-05, 7.466323586413637e-05, 7.466323586413637e-05, 7.466323586413637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.466323586413637e-05

Optimization complete. Final v2v error: 5.594244956970215 mm

Highest mean error: 11.967204093933105 mm for frame 86

Lowest mean error: 4.319169521331787 mm for frame 127

Saving results

Total time: 161.22709798812866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827579
Iteration 2/25 | Loss: 0.00157370
Iteration 3/25 | Loss: 0.00109079
Iteration 4/25 | Loss: 0.00109004
Iteration 5/25 | Loss: 0.00113758
Iteration 6/25 | Loss: 0.00101599
Iteration 7/25 | Loss: 0.00096401
Iteration 8/25 | Loss: 0.00095947
Iteration 9/25 | Loss: 0.00094037
Iteration 10/25 | Loss: 0.00095257
Iteration 11/25 | Loss: 0.00093625
Iteration 12/25 | Loss: 0.00092866
Iteration 13/25 | Loss: 0.00092400
Iteration 14/25 | Loss: 0.00092213
Iteration 15/25 | Loss: 0.00091952
Iteration 16/25 | Loss: 0.00091702
Iteration 17/25 | Loss: 0.00091662
Iteration 18/25 | Loss: 0.00092219
Iteration 19/25 | Loss: 0.00091982
Iteration 20/25 | Loss: 0.00092082
Iteration 21/25 | Loss: 0.00091534
Iteration 22/25 | Loss: 0.00091335
Iteration 23/25 | Loss: 0.00090841
Iteration 24/25 | Loss: 0.00090691
Iteration 25/25 | Loss: 0.00090609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08340955
Iteration 2/25 | Loss: 0.00108379
Iteration 3/25 | Loss: 0.00108377
Iteration 4/25 | Loss: 0.00108377
Iteration 5/25 | Loss: 0.00108377
Iteration 6/25 | Loss: 0.00108377
Iteration 7/25 | Loss: 0.00108377
Iteration 8/25 | Loss: 0.00108377
Iteration 9/25 | Loss: 0.00108377
Iteration 10/25 | Loss: 0.00108377
Iteration 11/25 | Loss: 0.00108377
Iteration 12/25 | Loss: 0.00108377
Iteration 13/25 | Loss: 0.00108377
Iteration 14/25 | Loss: 0.00108377
Iteration 15/25 | Loss: 0.00108377
Iteration 16/25 | Loss: 0.00108377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001083770883269608, 0.001083770883269608, 0.001083770883269608, 0.001083770883269608, 0.001083770883269608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083770883269608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108377
Iteration 2/1000 | Loss: 0.00003966
Iteration 3/1000 | Loss: 0.00002612
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002043
Iteration 7/1000 | Loss: 0.00001990
Iteration 8/1000 | Loss: 0.00001940
Iteration 9/1000 | Loss: 0.00001903
Iteration 10/1000 | Loss: 0.00001878
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001856
Iteration 13/1000 | Loss: 0.00001847
Iteration 14/1000 | Loss: 0.00001846
Iteration 15/1000 | Loss: 0.00001837
Iteration 16/1000 | Loss: 0.00001834
Iteration 17/1000 | Loss: 0.00001832
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001827
Iteration 22/1000 | Loss: 0.00001824
Iteration 23/1000 | Loss: 0.00001823
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001820
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001814
Iteration 31/1000 | Loss: 0.00001810
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001804
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001802
Iteration 38/1000 | Loss: 0.00001801
Iteration 39/1000 | Loss: 0.00001800
Iteration 40/1000 | Loss: 0.00001800
Iteration 41/1000 | Loss: 0.00001799
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001795
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001792
Iteration 59/1000 | Loss: 0.00001792
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001789
Iteration 77/1000 | Loss: 0.00001789
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001788
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001787
Iteration 84/1000 | Loss: 0.00001787
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001786
Iteration 89/1000 | Loss: 0.00001786
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.7864495021058246e-05, 1.7864495021058246e-05, 1.7864495021058246e-05, 1.7864495021058246e-05, 1.7864495021058246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7864495021058246e-05

Optimization complete. Final v2v error: 3.621225118637085 mm

Highest mean error: 4.533712387084961 mm for frame 22

Lowest mean error: 3.0166213512420654 mm for frame 114

Saving results

Total time: 82.88176536560059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084975
Iteration 2/25 | Loss: 0.00272720
Iteration 3/25 | Loss: 0.00188732
Iteration 4/25 | Loss: 0.00161628
Iteration 5/25 | Loss: 0.00151433
Iteration 6/25 | Loss: 0.00139350
Iteration 7/25 | Loss: 0.00132983
Iteration 8/25 | Loss: 0.00132442
Iteration 9/25 | Loss: 0.00126271
Iteration 10/25 | Loss: 0.00117653
Iteration 11/25 | Loss: 0.00112635
Iteration 12/25 | Loss: 0.00109029
Iteration 13/25 | Loss: 0.00106459
Iteration 14/25 | Loss: 0.00106746
Iteration 15/25 | Loss: 0.00105571
Iteration 16/25 | Loss: 0.00103553
Iteration 17/25 | Loss: 0.00102432
Iteration 18/25 | Loss: 0.00101945
Iteration 19/25 | Loss: 0.00102362
Iteration 20/25 | Loss: 0.00102097
Iteration 21/25 | Loss: 0.00102122
Iteration 22/25 | Loss: 0.00101214
Iteration 23/25 | Loss: 0.00100784
Iteration 24/25 | Loss: 0.00100924
Iteration 25/25 | Loss: 0.00100809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26115859
Iteration 2/25 | Loss: 0.00127741
Iteration 3/25 | Loss: 0.00117450
Iteration 4/25 | Loss: 0.00117450
Iteration 5/25 | Loss: 0.00117450
Iteration 6/25 | Loss: 0.00117450
Iteration 7/25 | Loss: 0.00117450
Iteration 8/25 | Loss: 0.00117450
Iteration 9/25 | Loss: 0.00117450
Iteration 10/25 | Loss: 0.00117450
Iteration 11/25 | Loss: 0.00117450
Iteration 12/25 | Loss: 0.00117450
Iteration 13/25 | Loss: 0.00117450
Iteration 14/25 | Loss: 0.00117450
Iteration 15/25 | Loss: 0.00117450
Iteration 16/25 | Loss: 0.00117450
Iteration 17/25 | Loss: 0.00117450
Iteration 18/25 | Loss: 0.00117450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011744954390451312, 0.0011744954390451312, 0.0011744954390451312, 0.0011744954390451312, 0.0011744954390451312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011744954390451312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117450
Iteration 2/1000 | Loss: 0.00011749
Iteration 3/1000 | Loss: 0.00005568
Iteration 4/1000 | Loss: 0.00004736
Iteration 5/1000 | Loss: 0.00015743
Iteration 6/1000 | Loss: 0.00004275
Iteration 7/1000 | Loss: 0.00018580
Iteration 8/1000 | Loss: 0.00004033
Iteration 9/1000 | Loss: 0.00032630
Iteration 10/1000 | Loss: 0.00030545
Iteration 11/1000 | Loss: 0.00005127
Iteration 12/1000 | Loss: 0.00003741
Iteration 13/1000 | Loss: 0.00009577
Iteration 14/1000 | Loss: 0.00043996
Iteration 15/1000 | Loss: 0.00004073
Iteration 16/1000 | Loss: 0.00003669
Iteration 17/1000 | Loss: 0.00003527
Iteration 18/1000 | Loss: 0.00012264
Iteration 19/1000 | Loss: 0.00025567
Iteration 20/1000 | Loss: 0.00003576
Iteration 21/1000 | Loss: 0.00008752
Iteration 22/1000 | Loss: 0.00003494
Iteration 23/1000 | Loss: 0.00003458
Iteration 24/1000 | Loss: 0.00003451
Iteration 25/1000 | Loss: 0.00003437
Iteration 26/1000 | Loss: 0.00003432
Iteration 27/1000 | Loss: 0.00018393
Iteration 28/1000 | Loss: 0.00042765
Iteration 29/1000 | Loss: 0.00003902
Iteration 30/1000 | Loss: 0.00018299
Iteration 31/1000 | Loss: 0.00012534
Iteration 32/1000 | Loss: 0.00006726
Iteration 33/1000 | Loss: 0.00014464
Iteration 34/1000 | Loss: 0.00004206
Iteration 35/1000 | Loss: 0.00003506
Iteration 36/1000 | Loss: 0.00020055
Iteration 37/1000 | Loss: 0.00007231
Iteration 38/1000 | Loss: 0.00015547
Iteration 39/1000 | Loss: 0.00015509
Iteration 40/1000 | Loss: 0.00013700
Iteration 41/1000 | Loss: 0.00015535
Iteration 42/1000 | Loss: 0.00004323
Iteration 43/1000 | Loss: 0.00003524
Iteration 44/1000 | Loss: 0.00003362
Iteration 45/1000 | Loss: 0.00011821
Iteration 46/1000 | Loss: 0.00011790
Iteration 47/1000 | Loss: 0.00046987
Iteration 48/1000 | Loss: 0.00003304
Iteration 49/1000 | Loss: 0.00003227
Iteration 50/1000 | Loss: 0.00003200
Iteration 51/1000 | Loss: 0.00003186
Iteration 52/1000 | Loss: 0.00003184
Iteration 53/1000 | Loss: 0.00003182
Iteration 54/1000 | Loss: 0.00003181
Iteration 55/1000 | Loss: 0.00003181
Iteration 56/1000 | Loss: 0.00003181
Iteration 57/1000 | Loss: 0.00003180
Iteration 58/1000 | Loss: 0.00003180
Iteration 59/1000 | Loss: 0.00003180
Iteration 60/1000 | Loss: 0.00003180
Iteration 61/1000 | Loss: 0.00003179
Iteration 62/1000 | Loss: 0.00003179
Iteration 63/1000 | Loss: 0.00003179
Iteration 64/1000 | Loss: 0.00003177
Iteration 65/1000 | Loss: 0.00003177
Iteration 66/1000 | Loss: 0.00003177
Iteration 67/1000 | Loss: 0.00003176
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003175
Iteration 72/1000 | Loss: 0.00003175
Iteration 73/1000 | Loss: 0.00003175
Iteration 74/1000 | Loss: 0.00003175
Iteration 75/1000 | Loss: 0.00003175
Iteration 76/1000 | Loss: 0.00003174
Iteration 77/1000 | Loss: 0.00003174
Iteration 78/1000 | Loss: 0.00003174
Iteration 79/1000 | Loss: 0.00003174
Iteration 80/1000 | Loss: 0.00003173
Iteration 81/1000 | Loss: 0.00003173
Iteration 82/1000 | Loss: 0.00003173
Iteration 83/1000 | Loss: 0.00003173
Iteration 84/1000 | Loss: 0.00003173
Iteration 85/1000 | Loss: 0.00003172
Iteration 86/1000 | Loss: 0.00003172
Iteration 87/1000 | Loss: 0.00003172
Iteration 88/1000 | Loss: 0.00003172
Iteration 89/1000 | Loss: 0.00003172
Iteration 90/1000 | Loss: 0.00003172
Iteration 91/1000 | Loss: 0.00003172
Iteration 92/1000 | Loss: 0.00003172
Iteration 93/1000 | Loss: 0.00003172
Iteration 94/1000 | Loss: 0.00003172
Iteration 95/1000 | Loss: 0.00003172
Iteration 96/1000 | Loss: 0.00003172
Iteration 97/1000 | Loss: 0.00003172
Iteration 98/1000 | Loss: 0.00003171
Iteration 99/1000 | Loss: 0.00003171
Iteration 100/1000 | Loss: 0.00003171
Iteration 101/1000 | Loss: 0.00003171
Iteration 102/1000 | Loss: 0.00003171
Iteration 103/1000 | Loss: 0.00003171
Iteration 104/1000 | Loss: 0.00003171
Iteration 105/1000 | Loss: 0.00003171
Iteration 106/1000 | Loss: 0.00003171
Iteration 107/1000 | Loss: 0.00003171
Iteration 108/1000 | Loss: 0.00003171
Iteration 109/1000 | Loss: 0.00003171
Iteration 110/1000 | Loss: 0.00003171
Iteration 111/1000 | Loss: 0.00003171
Iteration 112/1000 | Loss: 0.00003171
Iteration 113/1000 | Loss: 0.00003171
Iteration 114/1000 | Loss: 0.00003170
Iteration 115/1000 | Loss: 0.00003170
Iteration 116/1000 | Loss: 0.00003170
Iteration 117/1000 | Loss: 0.00003170
Iteration 118/1000 | Loss: 0.00003170
Iteration 119/1000 | Loss: 0.00003170
Iteration 120/1000 | Loss: 0.00003170
Iteration 121/1000 | Loss: 0.00003170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.170471973135136e-05, 3.170471973135136e-05, 3.170471973135136e-05, 3.170471973135136e-05, 3.170471973135136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.170471973135136e-05

Optimization complete. Final v2v error: 4.644420623779297 mm

Highest mean error: 9.681504249572754 mm for frame 74

Lowest mean error: 4.198995590209961 mm for frame 56

Saving results

Total time: 137.61784291267395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446086
Iteration 2/25 | Loss: 0.00109071
Iteration 3/25 | Loss: 0.00098888
Iteration 4/25 | Loss: 0.00096632
Iteration 5/25 | Loss: 0.00095733
Iteration 6/25 | Loss: 0.00095517
Iteration 7/25 | Loss: 0.00095500
Iteration 8/25 | Loss: 0.00095500
Iteration 9/25 | Loss: 0.00095500
Iteration 10/25 | Loss: 0.00095500
Iteration 11/25 | Loss: 0.00095500
Iteration 12/25 | Loss: 0.00095500
Iteration 13/25 | Loss: 0.00095500
Iteration 14/25 | Loss: 0.00095500
Iteration 15/25 | Loss: 0.00095500
Iteration 16/25 | Loss: 0.00095500
Iteration 17/25 | Loss: 0.00095500
Iteration 18/25 | Loss: 0.00095500
Iteration 19/25 | Loss: 0.00095500
Iteration 20/25 | Loss: 0.00095500
Iteration 21/25 | Loss: 0.00095500
Iteration 22/25 | Loss: 0.00095500
Iteration 23/25 | Loss: 0.00095500
Iteration 24/25 | Loss: 0.00095500
Iteration 25/25 | Loss: 0.00095500

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62546253
Iteration 2/25 | Loss: 0.00123734
Iteration 3/25 | Loss: 0.00123734
Iteration 4/25 | Loss: 0.00123734
Iteration 5/25 | Loss: 0.00123734
Iteration 6/25 | Loss: 0.00123734
Iteration 7/25 | Loss: 0.00123733
Iteration 8/25 | Loss: 0.00123733
Iteration 9/25 | Loss: 0.00123733
Iteration 10/25 | Loss: 0.00123733
Iteration 11/25 | Loss: 0.00123733
Iteration 12/25 | Loss: 0.00123733
Iteration 13/25 | Loss: 0.00123733
Iteration 14/25 | Loss: 0.00123733
Iteration 15/25 | Loss: 0.00123733
Iteration 16/25 | Loss: 0.00123733
Iteration 17/25 | Loss: 0.00123733
Iteration 18/25 | Loss: 0.00123733
Iteration 19/25 | Loss: 0.00123733
Iteration 20/25 | Loss: 0.00123733
Iteration 21/25 | Loss: 0.00123733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012373338686302304, 0.0012373338686302304, 0.0012373338686302304, 0.0012373338686302304, 0.0012373338686302304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012373338686302304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123733
Iteration 2/1000 | Loss: 0.00003708
Iteration 3/1000 | Loss: 0.00003067
Iteration 4/1000 | Loss: 0.00002805
Iteration 5/1000 | Loss: 0.00002648
Iteration 6/1000 | Loss: 0.00002544
Iteration 7/1000 | Loss: 0.00002452
Iteration 8/1000 | Loss: 0.00002412
Iteration 9/1000 | Loss: 0.00002393
Iteration 10/1000 | Loss: 0.00002386
Iteration 11/1000 | Loss: 0.00002372
Iteration 12/1000 | Loss: 0.00002372
Iteration 13/1000 | Loss: 0.00002363
Iteration 14/1000 | Loss: 0.00002361
Iteration 15/1000 | Loss: 0.00002355
Iteration 16/1000 | Loss: 0.00002355
Iteration 17/1000 | Loss: 0.00002352
Iteration 18/1000 | Loss: 0.00002352
Iteration 19/1000 | Loss: 0.00002351
Iteration 20/1000 | Loss: 0.00002351
Iteration 21/1000 | Loss: 0.00002351
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002350
Iteration 24/1000 | Loss: 0.00002348
Iteration 25/1000 | Loss: 0.00002347
Iteration 26/1000 | Loss: 0.00002347
Iteration 27/1000 | Loss: 0.00002347
Iteration 28/1000 | Loss: 0.00002346
Iteration 29/1000 | Loss: 0.00002346
Iteration 30/1000 | Loss: 0.00002344
Iteration 31/1000 | Loss: 0.00002343
Iteration 32/1000 | Loss: 0.00002343
Iteration 33/1000 | Loss: 0.00002342
Iteration 34/1000 | Loss: 0.00002342
Iteration 35/1000 | Loss: 0.00002342
Iteration 36/1000 | Loss: 0.00002342
Iteration 37/1000 | Loss: 0.00002342
Iteration 38/1000 | Loss: 0.00002341
Iteration 39/1000 | Loss: 0.00002341
Iteration 40/1000 | Loss: 0.00002341
Iteration 41/1000 | Loss: 0.00002341
Iteration 42/1000 | Loss: 0.00002341
Iteration 43/1000 | Loss: 0.00002341
Iteration 44/1000 | Loss: 0.00002340
Iteration 45/1000 | Loss: 0.00002340
Iteration 46/1000 | Loss: 0.00002340
Iteration 47/1000 | Loss: 0.00002340
Iteration 48/1000 | Loss: 0.00002340
Iteration 49/1000 | Loss: 0.00002340
Iteration 50/1000 | Loss: 0.00002340
Iteration 51/1000 | Loss: 0.00002340
Iteration 52/1000 | Loss: 0.00002340
Iteration 53/1000 | Loss: 0.00002340
Iteration 54/1000 | Loss: 0.00002340
Iteration 55/1000 | Loss: 0.00002340
Iteration 56/1000 | Loss: 0.00002340
Iteration 57/1000 | Loss: 0.00002340
Iteration 58/1000 | Loss: 0.00002340
Iteration 59/1000 | Loss: 0.00002339
Iteration 60/1000 | Loss: 0.00002339
Iteration 61/1000 | Loss: 0.00002339
Iteration 62/1000 | Loss: 0.00002339
Iteration 63/1000 | Loss: 0.00002339
Iteration 64/1000 | Loss: 0.00002339
Iteration 65/1000 | Loss: 0.00002339
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002338
Iteration 70/1000 | Loss: 0.00002338
Iteration 71/1000 | Loss: 0.00002338
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002338
Iteration 75/1000 | Loss: 0.00002338
Iteration 76/1000 | Loss: 0.00002338
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002338
Iteration 79/1000 | Loss: 0.00002338
Iteration 80/1000 | Loss: 0.00002337
Iteration 81/1000 | Loss: 0.00002337
Iteration 82/1000 | Loss: 0.00002337
Iteration 83/1000 | Loss: 0.00002337
Iteration 84/1000 | Loss: 0.00002337
Iteration 85/1000 | Loss: 0.00002337
Iteration 86/1000 | Loss: 0.00002337
Iteration 87/1000 | Loss: 0.00002337
Iteration 88/1000 | Loss: 0.00002337
Iteration 89/1000 | Loss: 0.00002337
Iteration 90/1000 | Loss: 0.00002337
Iteration 91/1000 | Loss: 0.00002337
Iteration 92/1000 | Loss: 0.00002337
Iteration 93/1000 | Loss: 0.00002337
Iteration 94/1000 | Loss: 0.00002337
Iteration 95/1000 | Loss: 0.00002337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.3373981093754992e-05, 2.3373981093754992e-05, 2.3373981093754992e-05, 2.3373981093754992e-05, 2.3373981093754992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3373981093754992e-05

Optimization complete. Final v2v error: 4.125020503997803 mm

Highest mean error: 4.434721946716309 mm for frame 6

Lowest mean error: 3.837156057357788 mm for frame 166

Saving results

Total time: 35.776593923568726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924077
Iteration 2/25 | Loss: 0.00116507
Iteration 3/25 | Loss: 0.00099225
Iteration 4/25 | Loss: 0.00097516
Iteration 5/25 | Loss: 0.00096991
Iteration 6/25 | Loss: 0.00096843
Iteration 7/25 | Loss: 0.00096834
Iteration 8/25 | Loss: 0.00096834
Iteration 9/25 | Loss: 0.00096834
Iteration 10/25 | Loss: 0.00096834
Iteration 11/25 | Loss: 0.00096834
Iteration 12/25 | Loss: 0.00096834
Iteration 13/25 | Loss: 0.00096834
Iteration 14/25 | Loss: 0.00096834
Iteration 15/25 | Loss: 0.00096834
Iteration 16/25 | Loss: 0.00096834
Iteration 17/25 | Loss: 0.00096834
Iteration 18/25 | Loss: 0.00096834
Iteration 19/25 | Loss: 0.00096834
Iteration 20/25 | Loss: 0.00096834
Iteration 21/25 | Loss: 0.00096834
Iteration 22/25 | Loss: 0.00096834
Iteration 23/25 | Loss: 0.00096834
Iteration 24/25 | Loss: 0.00096834
Iteration 25/25 | Loss: 0.00096834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009683379903435707, 0.0009683379903435707, 0.0009683379903435707, 0.0009683379903435707, 0.0009683379903435707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009683379903435707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61363780
Iteration 2/25 | Loss: 0.00087404
Iteration 3/25 | Loss: 0.00087404
Iteration 4/25 | Loss: 0.00087404
Iteration 5/25 | Loss: 0.00087404
Iteration 6/25 | Loss: 0.00087404
Iteration 7/25 | Loss: 0.00087404
Iteration 8/25 | Loss: 0.00087404
Iteration 9/25 | Loss: 0.00087404
Iteration 10/25 | Loss: 0.00087404
Iteration 11/25 | Loss: 0.00087404
Iteration 12/25 | Loss: 0.00087404
Iteration 13/25 | Loss: 0.00087404
Iteration 14/25 | Loss: 0.00087404
Iteration 15/25 | Loss: 0.00087404
Iteration 16/25 | Loss: 0.00087404
Iteration 17/25 | Loss: 0.00087404
Iteration 18/25 | Loss: 0.00087404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008740352350287139, 0.0008740352350287139, 0.0008740352350287139, 0.0008740352350287139, 0.0008740352350287139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008740352350287139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087404
Iteration 2/1000 | Loss: 0.00003396
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001497
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001450
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001442
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001440
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001438
Iteration 22/1000 | Loss: 0.00001437
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001435
Iteration 29/1000 | Loss: 0.00001435
Iteration 30/1000 | Loss: 0.00001434
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001433
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001431
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001424
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001423
Iteration 86/1000 | Loss: 0.00001423
Iteration 87/1000 | Loss: 0.00001423
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001422
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001421
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001421
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4213824215403292e-05, 1.4213824215403292e-05, 1.4213824215403292e-05, 1.4213824215403292e-05, 1.4213824215403292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4213824215403292e-05

Optimization complete. Final v2v error: 3.3418335914611816 mm

Highest mean error: 4.176443576812744 mm for frame 111

Lowest mean error: 3.0559654235839844 mm for frame 194

Saving results

Total time: 33.38760328292847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00543843
Iteration 2/25 | Loss: 0.00124377
Iteration 3/25 | Loss: 0.00113742
Iteration 4/25 | Loss: 0.00110762
Iteration 5/25 | Loss: 0.00109813
Iteration 6/25 | Loss: 0.00109519
Iteration 7/25 | Loss: 0.00109432
Iteration 8/25 | Loss: 0.00109432
Iteration 9/25 | Loss: 0.00109432
Iteration 10/25 | Loss: 0.00109432
Iteration 11/25 | Loss: 0.00109432
Iteration 12/25 | Loss: 0.00109432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001094319624826312, 0.001094319624826312, 0.001094319624826312, 0.001094319624826312, 0.001094319624826312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001094319624826312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.15936041
Iteration 2/25 | Loss: 0.00095943
Iteration 3/25 | Loss: 0.00095943
Iteration 4/25 | Loss: 0.00095943
Iteration 5/25 | Loss: 0.00095943
Iteration 6/25 | Loss: 0.00095943
Iteration 7/25 | Loss: 0.00095942
Iteration 8/25 | Loss: 0.00095942
Iteration 9/25 | Loss: 0.00095942
Iteration 10/25 | Loss: 0.00095942
Iteration 11/25 | Loss: 0.00095942
Iteration 12/25 | Loss: 0.00095942
Iteration 13/25 | Loss: 0.00095942
Iteration 14/25 | Loss: 0.00095942
Iteration 15/25 | Loss: 0.00095942
Iteration 16/25 | Loss: 0.00095942
Iteration 17/25 | Loss: 0.00095942
Iteration 18/25 | Loss: 0.00095942
Iteration 19/25 | Loss: 0.00095942
Iteration 20/25 | Loss: 0.00095942
Iteration 21/25 | Loss: 0.00095942
Iteration 22/25 | Loss: 0.00095942
Iteration 23/25 | Loss: 0.00095942
Iteration 24/25 | Loss: 0.00095942
Iteration 25/25 | Loss: 0.00095942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095942
Iteration 2/1000 | Loss: 0.00005729
Iteration 3/1000 | Loss: 0.00003851
Iteration 4/1000 | Loss: 0.00003502
Iteration 5/1000 | Loss: 0.00003368
Iteration 6/1000 | Loss: 0.00003270
Iteration 7/1000 | Loss: 0.00003224
Iteration 8/1000 | Loss: 0.00003153
Iteration 9/1000 | Loss: 0.00003114
Iteration 10/1000 | Loss: 0.00003093
Iteration 11/1000 | Loss: 0.00003078
Iteration 12/1000 | Loss: 0.00003076
Iteration 13/1000 | Loss: 0.00003069
Iteration 14/1000 | Loss: 0.00003067
Iteration 15/1000 | Loss: 0.00003062
Iteration 16/1000 | Loss: 0.00003062
Iteration 17/1000 | Loss: 0.00003060
Iteration 18/1000 | Loss: 0.00003060
Iteration 19/1000 | Loss: 0.00003060
Iteration 20/1000 | Loss: 0.00003059
Iteration 21/1000 | Loss: 0.00003059
Iteration 22/1000 | Loss: 0.00003059
Iteration 23/1000 | Loss: 0.00003059
Iteration 24/1000 | Loss: 0.00003057
Iteration 25/1000 | Loss: 0.00003057
Iteration 26/1000 | Loss: 0.00003057
Iteration 27/1000 | Loss: 0.00003057
Iteration 28/1000 | Loss: 0.00003057
Iteration 29/1000 | Loss: 0.00003057
Iteration 30/1000 | Loss: 0.00003056
Iteration 31/1000 | Loss: 0.00003056
Iteration 32/1000 | Loss: 0.00003056
Iteration 33/1000 | Loss: 0.00003056
Iteration 34/1000 | Loss: 0.00003055
Iteration 35/1000 | Loss: 0.00003055
Iteration 36/1000 | Loss: 0.00003055
Iteration 37/1000 | Loss: 0.00003054
Iteration 38/1000 | Loss: 0.00003054
Iteration 39/1000 | Loss: 0.00003054
Iteration 40/1000 | Loss: 0.00003054
Iteration 41/1000 | Loss: 0.00003053
Iteration 42/1000 | Loss: 0.00003053
Iteration 43/1000 | Loss: 0.00003053
Iteration 44/1000 | Loss: 0.00003053
Iteration 45/1000 | Loss: 0.00003052
Iteration 46/1000 | Loss: 0.00003052
Iteration 47/1000 | Loss: 0.00003052
Iteration 48/1000 | Loss: 0.00003052
Iteration 49/1000 | Loss: 0.00003051
Iteration 50/1000 | Loss: 0.00003051
Iteration 51/1000 | Loss: 0.00003051
Iteration 52/1000 | Loss: 0.00003051
Iteration 53/1000 | Loss: 0.00003051
Iteration 54/1000 | Loss: 0.00003051
Iteration 55/1000 | Loss: 0.00003051
Iteration 56/1000 | Loss: 0.00003051
Iteration 57/1000 | Loss: 0.00003050
Iteration 58/1000 | Loss: 0.00003050
Iteration 59/1000 | Loss: 0.00003050
Iteration 60/1000 | Loss: 0.00003050
Iteration 61/1000 | Loss: 0.00003050
Iteration 62/1000 | Loss: 0.00003050
Iteration 63/1000 | Loss: 0.00003050
Iteration 64/1000 | Loss: 0.00003050
Iteration 65/1000 | Loss: 0.00003050
Iteration 66/1000 | Loss: 0.00003049
Iteration 67/1000 | Loss: 0.00003049
Iteration 68/1000 | Loss: 0.00003049
Iteration 69/1000 | Loss: 0.00003049
Iteration 70/1000 | Loss: 0.00003049
Iteration 71/1000 | Loss: 0.00003049
Iteration 72/1000 | Loss: 0.00003048
Iteration 73/1000 | Loss: 0.00003048
Iteration 74/1000 | Loss: 0.00003048
Iteration 75/1000 | Loss: 0.00003048
Iteration 76/1000 | Loss: 0.00003048
Iteration 77/1000 | Loss: 0.00003048
Iteration 78/1000 | Loss: 0.00003047
Iteration 79/1000 | Loss: 0.00003047
Iteration 80/1000 | Loss: 0.00003047
Iteration 81/1000 | Loss: 0.00003047
Iteration 82/1000 | Loss: 0.00003046
Iteration 83/1000 | Loss: 0.00003046
Iteration 84/1000 | Loss: 0.00003046
Iteration 85/1000 | Loss: 0.00003046
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003045
Iteration 90/1000 | Loss: 0.00003045
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003045
Iteration 93/1000 | Loss: 0.00003045
Iteration 94/1000 | Loss: 0.00003045
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003044
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003043
Iteration 102/1000 | Loss: 0.00003043
Iteration 103/1000 | Loss: 0.00003043
Iteration 104/1000 | Loss: 0.00003043
Iteration 105/1000 | Loss: 0.00003043
Iteration 106/1000 | Loss: 0.00003042
Iteration 107/1000 | Loss: 0.00003042
Iteration 108/1000 | Loss: 0.00003042
Iteration 109/1000 | Loss: 0.00003042
Iteration 110/1000 | Loss: 0.00003042
Iteration 111/1000 | Loss: 0.00003042
Iteration 112/1000 | Loss: 0.00003042
Iteration 113/1000 | Loss: 0.00003042
Iteration 114/1000 | Loss: 0.00003042
Iteration 115/1000 | Loss: 0.00003042
Iteration 116/1000 | Loss: 0.00003042
Iteration 117/1000 | Loss: 0.00003042
Iteration 118/1000 | Loss: 0.00003042
Iteration 119/1000 | Loss: 0.00003042
Iteration 120/1000 | Loss: 0.00003042
Iteration 121/1000 | Loss: 0.00003041
Iteration 122/1000 | Loss: 0.00003041
Iteration 123/1000 | Loss: 0.00003041
Iteration 124/1000 | Loss: 0.00003041
Iteration 125/1000 | Loss: 0.00003041
Iteration 126/1000 | Loss: 0.00003041
Iteration 127/1000 | Loss: 0.00003041
Iteration 128/1000 | Loss: 0.00003041
Iteration 129/1000 | Loss: 0.00003040
Iteration 130/1000 | Loss: 0.00003040
Iteration 131/1000 | Loss: 0.00003040
Iteration 132/1000 | Loss: 0.00003040
Iteration 133/1000 | Loss: 0.00003040
Iteration 134/1000 | Loss: 0.00003040
Iteration 135/1000 | Loss: 0.00003040
Iteration 136/1000 | Loss: 0.00003040
Iteration 137/1000 | Loss: 0.00003040
Iteration 138/1000 | Loss: 0.00003040
Iteration 139/1000 | Loss: 0.00003039
Iteration 140/1000 | Loss: 0.00003039
Iteration 141/1000 | Loss: 0.00003039
Iteration 142/1000 | Loss: 0.00003039
Iteration 143/1000 | Loss: 0.00003039
Iteration 144/1000 | Loss: 0.00003039
Iteration 145/1000 | Loss: 0.00003039
Iteration 146/1000 | Loss: 0.00003039
Iteration 147/1000 | Loss: 0.00003039
Iteration 148/1000 | Loss: 0.00003039
Iteration 149/1000 | Loss: 0.00003039
Iteration 150/1000 | Loss: 0.00003039
Iteration 151/1000 | Loss: 0.00003038
Iteration 152/1000 | Loss: 0.00003038
Iteration 153/1000 | Loss: 0.00003038
Iteration 154/1000 | Loss: 0.00003038
Iteration 155/1000 | Loss: 0.00003038
Iteration 156/1000 | Loss: 0.00003038
Iteration 157/1000 | Loss: 0.00003038
Iteration 158/1000 | Loss: 0.00003038
Iteration 159/1000 | Loss: 0.00003038
Iteration 160/1000 | Loss: 0.00003038
Iteration 161/1000 | Loss: 0.00003038
Iteration 162/1000 | Loss: 0.00003038
Iteration 163/1000 | Loss: 0.00003038
Iteration 164/1000 | Loss: 0.00003038
Iteration 165/1000 | Loss: 0.00003038
Iteration 166/1000 | Loss: 0.00003038
Iteration 167/1000 | Loss: 0.00003038
Iteration 168/1000 | Loss: 0.00003038
Iteration 169/1000 | Loss: 0.00003038
Iteration 170/1000 | Loss: 0.00003038
Iteration 171/1000 | Loss: 0.00003038
Iteration 172/1000 | Loss: 0.00003038
Iteration 173/1000 | Loss: 0.00003038
Iteration 174/1000 | Loss: 0.00003038
Iteration 175/1000 | Loss: 0.00003038
Iteration 176/1000 | Loss: 0.00003038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.0377457733266056e-05, 3.0377457733266056e-05, 3.0377457733266056e-05, 3.0377457733266056e-05, 3.0377457733266056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0377457733266056e-05

Optimization complete. Final v2v error: 4.680736064910889 mm

Highest mean error: 5.47352933883667 mm for frame 68

Lowest mean error: 4.2346577644348145 mm for frame 128

Saving results

Total time: 38.093291997909546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088132
Iteration 2/25 | Loss: 0.00409938
Iteration 3/25 | Loss: 0.00264403
Iteration 4/25 | Loss: 0.00225343
Iteration 5/25 | Loss: 0.00186036
Iteration 6/25 | Loss: 0.00154511
Iteration 7/25 | Loss: 0.00144879
Iteration 8/25 | Loss: 0.00135753
Iteration 9/25 | Loss: 0.00128416
Iteration 10/25 | Loss: 0.00126960
Iteration 11/25 | Loss: 0.00125617
Iteration 12/25 | Loss: 0.00124371
Iteration 13/25 | Loss: 0.00124246
Iteration 14/25 | Loss: 0.00123878
Iteration 15/25 | Loss: 0.00123338
Iteration 16/25 | Loss: 0.00122945
Iteration 17/25 | Loss: 0.00122606
Iteration 18/25 | Loss: 0.00123769
Iteration 19/25 | Loss: 0.00121561
Iteration 20/25 | Loss: 0.00120876
Iteration 21/25 | Loss: 0.00120731
Iteration 22/25 | Loss: 0.00120759
Iteration 23/25 | Loss: 0.00120642
Iteration 24/25 | Loss: 0.00120678
Iteration 25/25 | Loss: 0.00120696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49163878
Iteration 2/25 | Loss: 0.00406189
Iteration 3/25 | Loss: 0.00384856
Iteration 4/25 | Loss: 0.00384856
Iteration 5/25 | Loss: 0.00384856
Iteration 6/25 | Loss: 0.00384856
Iteration 7/25 | Loss: 0.00384856
Iteration 8/25 | Loss: 0.00384856
Iteration 9/25 | Loss: 0.00384856
Iteration 10/25 | Loss: 0.00384856
Iteration 11/25 | Loss: 0.00384856
Iteration 12/25 | Loss: 0.00384856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.003848564112558961, 0.003848564112558961, 0.003848564112558961, 0.003848564112558961, 0.003848564112558961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003848564112558961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00384856
Iteration 2/1000 | Loss: 0.00127415
Iteration 3/1000 | Loss: 0.00077152
Iteration 4/1000 | Loss: 0.00107100
Iteration 5/1000 | Loss: 0.00144946
Iteration 6/1000 | Loss: 0.00053515
Iteration 7/1000 | Loss: 0.00044913
Iteration 8/1000 | Loss: 0.00034034
Iteration 9/1000 | Loss: 0.00040512
Iteration 10/1000 | Loss: 0.00028942
Iteration 11/1000 | Loss: 0.00039150
Iteration 12/1000 | Loss: 0.00068918
Iteration 13/1000 | Loss: 0.00171979
Iteration 14/1000 | Loss: 0.00036464
Iteration 15/1000 | Loss: 0.00062670
Iteration 16/1000 | Loss: 0.00044347
Iteration 17/1000 | Loss: 0.00026927
Iteration 18/1000 | Loss: 0.00026761
Iteration 19/1000 | Loss: 0.00027506
Iteration 20/1000 | Loss: 0.00060995
Iteration 21/1000 | Loss: 0.00040229
Iteration 22/1000 | Loss: 0.00037987
Iteration 23/1000 | Loss: 0.00025716
Iteration 24/1000 | Loss: 0.00024512
Iteration 25/1000 | Loss: 0.00024801
Iteration 26/1000 | Loss: 0.00074458
Iteration 27/1000 | Loss: 0.00053011
Iteration 28/1000 | Loss: 0.00054418
Iteration 29/1000 | Loss: 0.00038406
Iteration 30/1000 | Loss: 0.00051131
Iteration 31/1000 | Loss: 0.00071082
Iteration 32/1000 | Loss: 0.00096296
Iteration 33/1000 | Loss: 0.00028515
Iteration 34/1000 | Loss: 0.00028095
Iteration 35/1000 | Loss: 0.00023847
Iteration 36/1000 | Loss: 0.00023758
Iteration 37/1000 | Loss: 0.00095775
Iteration 38/1000 | Loss: 0.00054691
Iteration 39/1000 | Loss: 0.00048777
Iteration 40/1000 | Loss: 0.00023096
Iteration 41/1000 | Loss: 0.00021983
Iteration 42/1000 | Loss: 0.00042986
Iteration 43/1000 | Loss: 0.00027106
Iteration 44/1000 | Loss: 0.00022488
Iteration 45/1000 | Loss: 0.00021349
Iteration 46/1000 | Loss: 0.00038082
Iteration 47/1000 | Loss: 0.00021347
Iteration 48/1000 | Loss: 0.00021392
Iteration 49/1000 | Loss: 0.00020955
Iteration 50/1000 | Loss: 0.00020860
Iteration 51/1000 | Loss: 0.00043421
Iteration 52/1000 | Loss: 0.00021120
Iteration 53/1000 | Loss: 0.00020735
Iteration 54/1000 | Loss: 0.00020567
Iteration 55/1000 | Loss: 0.00020451
Iteration 56/1000 | Loss: 0.00020378
Iteration 57/1000 | Loss: 0.00020328
Iteration 58/1000 | Loss: 0.00055146
Iteration 59/1000 | Loss: 0.00038321
Iteration 60/1000 | Loss: 0.00020924
Iteration 61/1000 | Loss: 0.00020294
Iteration 62/1000 | Loss: 0.00019912
Iteration 63/1000 | Loss: 0.00039702
Iteration 64/1000 | Loss: 0.00019956
Iteration 65/1000 | Loss: 0.00021129
Iteration 66/1000 | Loss: 0.00037183
Iteration 67/1000 | Loss: 0.00028333
Iteration 68/1000 | Loss: 0.00038587
Iteration 69/1000 | Loss: 0.00035840
Iteration 70/1000 | Loss: 0.00020145
Iteration 71/1000 | Loss: 0.00019420
Iteration 72/1000 | Loss: 0.00019201
Iteration 73/1000 | Loss: 0.00019099
Iteration 74/1000 | Loss: 0.00019537
Iteration 75/1000 | Loss: 0.00019019
Iteration 76/1000 | Loss: 0.00018991
Iteration 77/1000 | Loss: 0.00018969
Iteration 78/1000 | Loss: 0.00019373
Iteration 79/1000 | Loss: 0.00018954
Iteration 80/1000 | Loss: 0.00018952
Iteration 81/1000 | Loss: 0.00018950
Iteration 82/1000 | Loss: 0.00018947
Iteration 83/1000 | Loss: 0.00018942
Iteration 84/1000 | Loss: 0.00018942
Iteration 85/1000 | Loss: 0.00018941
Iteration 86/1000 | Loss: 0.00019232
Iteration 87/1000 | Loss: 0.00018933
Iteration 88/1000 | Loss: 0.00018931
Iteration 89/1000 | Loss: 0.00018931
Iteration 90/1000 | Loss: 0.00018931
Iteration 91/1000 | Loss: 0.00018931
Iteration 92/1000 | Loss: 0.00018931
Iteration 93/1000 | Loss: 0.00018931
Iteration 94/1000 | Loss: 0.00018931
Iteration 95/1000 | Loss: 0.00018931
Iteration 96/1000 | Loss: 0.00018931
Iteration 97/1000 | Loss: 0.00018931
Iteration 98/1000 | Loss: 0.00018930
Iteration 99/1000 | Loss: 0.00018930
Iteration 100/1000 | Loss: 0.00018930
Iteration 101/1000 | Loss: 0.00018930
Iteration 102/1000 | Loss: 0.00018930
Iteration 103/1000 | Loss: 0.00018930
Iteration 104/1000 | Loss: 0.00018930
Iteration 105/1000 | Loss: 0.00018930
Iteration 106/1000 | Loss: 0.00018930
Iteration 107/1000 | Loss: 0.00018930
Iteration 108/1000 | Loss: 0.00018930
Iteration 109/1000 | Loss: 0.00018929
Iteration 110/1000 | Loss: 0.00018929
Iteration 111/1000 | Loss: 0.00018929
Iteration 112/1000 | Loss: 0.00018929
Iteration 113/1000 | Loss: 0.00018929
Iteration 114/1000 | Loss: 0.00018929
Iteration 115/1000 | Loss: 0.00018929
Iteration 116/1000 | Loss: 0.00018929
Iteration 117/1000 | Loss: 0.00018929
Iteration 118/1000 | Loss: 0.00018929
Iteration 119/1000 | Loss: 0.00018929
Iteration 120/1000 | Loss: 0.00018929
Iteration 121/1000 | Loss: 0.00018929
Iteration 122/1000 | Loss: 0.00018929
Iteration 123/1000 | Loss: 0.00018929
Iteration 124/1000 | Loss: 0.00018929
Iteration 125/1000 | Loss: 0.00018929
Iteration 126/1000 | Loss: 0.00018929
Iteration 127/1000 | Loss: 0.00018929
Iteration 128/1000 | Loss: 0.00018929
Iteration 129/1000 | Loss: 0.00019164
Iteration 130/1000 | Loss: 0.00019164
Iteration 131/1000 | Loss: 0.00018929
Iteration 132/1000 | Loss: 0.00018929
Iteration 133/1000 | Loss: 0.00018929
Iteration 134/1000 | Loss: 0.00018927
Iteration 135/1000 | Loss: 0.00018925
Iteration 136/1000 | Loss: 0.00018988
Iteration 137/1000 | Loss: 0.00018924
Iteration 138/1000 | Loss: 0.00018923
Iteration 139/1000 | Loss: 0.00018923
Iteration 140/1000 | Loss: 0.00018923
Iteration 141/1000 | Loss: 0.00018923
Iteration 142/1000 | Loss: 0.00018923
Iteration 143/1000 | Loss: 0.00018923
Iteration 144/1000 | Loss: 0.00018923
Iteration 145/1000 | Loss: 0.00018923
Iteration 146/1000 | Loss: 0.00018923
Iteration 147/1000 | Loss: 0.00018923
Iteration 148/1000 | Loss: 0.00018923
Iteration 149/1000 | Loss: 0.00018923
Iteration 150/1000 | Loss: 0.00018923
Iteration 151/1000 | Loss: 0.00018923
Iteration 152/1000 | Loss: 0.00018923
Iteration 153/1000 | Loss: 0.00018923
Iteration 154/1000 | Loss: 0.00018922
Iteration 155/1000 | Loss: 0.00018922
Iteration 156/1000 | Loss: 0.00018922
Iteration 157/1000 | Loss: 0.00018922
Iteration 158/1000 | Loss: 0.00018922
Iteration 159/1000 | Loss: 0.00018922
Iteration 160/1000 | Loss: 0.00018922
Iteration 161/1000 | Loss: 0.00018922
Iteration 162/1000 | Loss: 0.00018922
Iteration 163/1000 | Loss: 0.00018922
Iteration 164/1000 | Loss: 0.00018922
Iteration 165/1000 | Loss: 0.00018922
Iteration 166/1000 | Loss: 0.00018922
Iteration 167/1000 | Loss: 0.00018922
Iteration 168/1000 | Loss: 0.00018922
Iteration 169/1000 | Loss: 0.00018922
Iteration 170/1000 | Loss: 0.00018922
Iteration 171/1000 | Loss: 0.00018922
Iteration 172/1000 | Loss: 0.00018922
Iteration 173/1000 | Loss: 0.00018922
Iteration 174/1000 | Loss: 0.00018922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [0.00018921504670288414, 0.00018921504670288414, 0.00018921504670288414, 0.00018921504670288414, 0.00018921504670288414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018921504670288414

Optimization complete. Final v2v error: 7.2856035232543945 mm

Highest mean error: 19.276473999023438 mm for frame 209

Lowest mean error: 4.394500732421875 mm for frame 83

Saving results

Total time: 184.98475813865662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880574
Iteration 2/25 | Loss: 0.00113735
Iteration 3/25 | Loss: 0.00098702
Iteration 4/25 | Loss: 0.00096663
Iteration 5/25 | Loss: 0.00096190
Iteration 6/25 | Loss: 0.00096080
Iteration 7/25 | Loss: 0.00096080
Iteration 8/25 | Loss: 0.00096080
Iteration 9/25 | Loss: 0.00096080
Iteration 10/25 | Loss: 0.00096080
Iteration 11/25 | Loss: 0.00096080
Iteration 12/25 | Loss: 0.00096080
Iteration 13/25 | Loss: 0.00096080
Iteration 14/25 | Loss: 0.00096080
Iteration 15/25 | Loss: 0.00096080
Iteration 16/25 | Loss: 0.00096080
Iteration 17/25 | Loss: 0.00096080
Iteration 18/25 | Loss: 0.00096080
Iteration 19/25 | Loss: 0.00096080
Iteration 20/25 | Loss: 0.00096080
Iteration 21/25 | Loss: 0.00096080
Iteration 22/25 | Loss: 0.00096080
Iteration 23/25 | Loss: 0.00096080
Iteration 24/25 | Loss: 0.00096080
Iteration 25/25 | Loss: 0.00096080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50402558
Iteration 2/25 | Loss: 0.00074649
Iteration 3/25 | Loss: 0.00074649
Iteration 4/25 | Loss: 0.00074649
Iteration 5/25 | Loss: 0.00074649
Iteration 6/25 | Loss: 0.00074649
Iteration 7/25 | Loss: 0.00074649
Iteration 8/25 | Loss: 0.00074649
Iteration 9/25 | Loss: 0.00074649
Iteration 10/25 | Loss: 0.00074649
Iteration 11/25 | Loss: 0.00074649
Iteration 12/25 | Loss: 0.00074649
Iteration 13/25 | Loss: 0.00074649
Iteration 14/25 | Loss: 0.00074649
Iteration 15/25 | Loss: 0.00074649
Iteration 16/25 | Loss: 0.00074649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007464907830581069, 0.0007464907830581069, 0.0007464907830581069, 0.0007464907830581069, 0.0007464907830581069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007464907830581069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074649
Iteration 2/1000 | Loss: 0.00002757
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001461
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001451
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001433
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001433
Iteration 128/1000 | Loss: 0.00001433
Iteration 129/1000 | Loss: 0.00001433
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.432676253898535e-05, 1.432676253898535e-05, 1.432676253898535e-05, 1.432676253898535e-05, 1.432676253898535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.432676253898535e-05

Optimization complete. Final v2v error: 3.277759075164795 mm

Highest mean error: 3.4463613033294678 mm for frame 25

Lowest mean error: 2.9694228172302246 mm for frame 0

Saving results

Total time: 30.046876192092896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876015
Iteration 2/25 | Loss: 0.00140467
Iteration 3/25 | Loss: 0.00121281
Iteration 4/25 | Loss: 0.00115323
Iteration 5/25 | Loss: 0.00114175
Iteration 6/25 | Loss: 0.00113950
Iteration 7/25 | Loss: 0.00113853
Iteration 8/25 | Loss: 0.00113845
Iteration 9/25 | Loss: 0.00113845
Iteration 10/25 | Loss: 0.00113845
Iteration 11/25 | Loss: 0.00113845
Iteration 12/25 | Loss: 0.00113845
Iteration 13/25 | Loss: 0.00113845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011384504614397883, 0.0011384504614397883, 0.0011384504614397883, 0.0011384504614397883, 0.0011384504614397883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011384504614397883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49250388
Iteration 2/25 | Loss: 0.00133984
Iteration 3/25 | Loss: 0.00133984
Iteration 4/25 | Loss: 0.00133984
Iteration 5/25 | Loss: 0.00133984
Iteration 6/25 | Loss: 0.00133984
Iteration 7/25 | Loss: 0.00133984
Iteration 8/25 | Loss: 0.00133984
Iteration 9/25 | Loss: 0.00133984
Iteration 10/25 | Loss: 0.00133984
Iteration 11/25 | Loss: 0.00133984
Iteration 12/25 | Loss: 0.00133984
Iteration 13/25 | Loss: 0.00133984
Iteration 14/25 | Loss: 0.00133984
Iteration 15/25 | Loss: 0.00133984
Iteration 16/25 | Loss: 0.00133984
Iteration 17/25 | Loss: 0.00133984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013398369774222374, 0.0013398369774222374, 0.0013398369774222374, 0.0013398369774222374, 0.0013398369774222374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013398369774222374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133984
Iteration 2/1000 | Loss: 0.00005806
Iteration 3/1000 | Loss: 0.00004148
Iteration 4/1000 | Loss: 0.00003234
Iteration 5/1000 | Loss: 0.00002901
Iteration 6/1000 | Loss: 0.00002782
Iteration 7/1000 | Loss: 0.00002687
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002569
Iteration 10/1000 | Loss: 0.00002534
Iteration 11/1000 | Loss: 0.00002504
Iteration 12/1000 | Loss: 0.00002491
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002466
Iteration 15/1000 | Loss: 0.00002466
Iteration 16/1000 | Loss: 0.00002464
Iteration 17/1000 | Loss: 0.00002463
Iteration 18/1000 | Loss: 0.00002463
Iteration 19/1000 | Loss: 0.00002457
Iteration 20/1000 | Loss: 0.00002451
Iteration 21/1000 | Loss: 0.00002448
Iteration 22/1000 | Loss: 0.00002447
Iteration 23/1000 | Loss: 0.00002447
Iteration 24/1000 | Loss: 0.00002447
Iteration 25/1000 | Loss: 0.00002447
Iteration 26/1000 | Loss: 0.00002447
Iteration 27/1000 | Loss: 0.00002447
Iteration 28/1000 | Loss: 0.00002447
Iteration 29/1000 | Loss: 0.00002447
Iteration 30/1000 | Loss: 0.00002447
Iteration 31/1000 | Loss: 0.00002447
Iteration 32/1000 | Loss: 0.00002446
Iteration 33/1000 | Loss: 0.00002446
Iteration 34/1000 | Loss: 0.00002446
Iteration 35/1000 | Loss: 0.00002446
Iteration 36/1000 | Loss: 0.00002446
Iteration 37/1000 | Loss: 0.00002446
Iteration 38/1000 | Loss: 0.00002446
Iteration 39/1000 | Loss: 0.00002445
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002443
Iteration 42/1000 | Loss: 0.00002442
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002441
Iteration 46/1000 | Loss: 0.00002441
Iteration 47/1000 | Loss: 0.00002441
Iteration 48/1000 | Loss: 0.00002440
Iteration 49/1000 | Loss: 0.00002439
Iteration 50/1000 | Loss: 0.00002439
Iteration 51/1000 | Loss: 0.00002439
Iteration 52/1000 | Loss: 0.00002439
Iteration 53/1000 | Loss: 0.00002439
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002438
Iteration 56/1000 | Loss: 0.00002438
Iteration 57/1000 | Loss: 0.00002438
Iteration 58/1000 | Loss: 0.00002438
Iteration 59/1000 | Loss: 0.00002438
Iteration 60/1000 | Loss: 0.00002438
Iteration 61/1000 | Loss: 0.00002438
Iteration 62/1000 | Loss: 0.00002438
Iteration 63/1000 | Loss: 0.00002438
Iteration 64/1000 | Loss: 0.00002438
Iteration 65/1000 | Loss: 0.00002438
Iteration 66/1000 | Loss: 0.00002438
Iteration 67/1000 | Loss: 0.00002438
Iteration 68/1000 | Loss: 0.00002438
Iteration 69/1000 | Loss: 0.00002438
Iteration 70/1000 | Loss: 0.00002438
Iteration 71/1000 | Loss: 0.00002438
Iteration 72/1000 | Loss: 0.00002438
Iteration 73/1000 | Loss: 0.00002438
Iteration 74/1000 | Loss: 0.00002438
Iteration 75/1000 | Loss: 0.00002438
Iteration 76/1000 | Loss: 0.00002438
Iteration 77/1000 | Loss: 0.00002438
Iteration 78/1000 | Loss: 0.00002438
Iteration 79/1000 | Loss: 0.00002438
Iteration 80/1000 | Loss: 0.00002438
Iteration 81/1000 | Loss: 0.00002438
Iteration 82/1000 | Loss: 0.00002438
Iteration 83/1000 | Loss: 0.00002438
Iteration 84/1000 | Loss: 0.00002438
Iteration 85/1000 | Loss: 0.00002438
Iteration 86/1000 | Loss: 0.00002438
Iteration 87/1000 | Loss: 0.00002438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.4380879040108994e-05, 2.4380879040108994e-05, 2.4380879040108994e-05, 2.4380879040108994e-05, 2.4380879040108994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4380879040108994e-05

Optimization complete. Final v2v error: 4.165868282318115 mm

Highest mean error: 4.4894866943359375 mm for frame 74

Lowest mean error: 3.6141040325164795 mm for frame 133

Saving results

Total time: 34.18186330795288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908563
Iteration 2/25 | Loss: 0.00137198
Iteration 3/25 | Loss: 0.00105001
Iteration 4/25 | Loss: 0.00103699
Iteration 5/25 | Loss: 0.00100600
Iteration 6/25 | Loss: 0.00100276
Iteration 7/25 | Loss: 0.00100187
Iteration 8/25 | Loss: 0.00100168
Iteration 9/25 | Loss: 0.00100168
Iteration 10/25 | Loss: 0.00100168
Iteration 11/25 | Loss: 0.00100168
Iteration 12/25 | Loss: 0.00100168
Iteration 13/25 | Loss: 0.00100168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010016820160672069, 0.0010016820160672069, 0.0010016820160672069, 0.0010016820160672069, 0.0010016820160672069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010016820160672069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75787854
Iteration 2/25 | Loss: 0.00075282
Iteration 3/25 | Loss: 0.00075281
Iteration 4/25 | Loss: 0.00075281
Iteration 5/25 | Loss: 0.00075281
Iteration 6/25 | Loss: 0.00075281
Iteration 7/25 | Loss: 0.00075281
Iteration 8/25 | Loss: 0.00075281
Iteration 9/25 | Loss: 0.00075281
Iteration 10/25 | Loss: 0.00075280
Iteration 11/25 | Loss: 0.00075280
Iteration 12/25 | Loss: 0.00075280
Iteration 13/25 | Loss: 0.00075280
Iteration 14/25 | Loss: 0.00075280
Iteration 15/25 | Loss: 0.00075280
Iteration 16/25 | Loss: 0.00075280
Iteration 17/25 | Loss: 0.00075280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007528045680373907, 0.0007528045680373907, 0.0007528045680373907, 0.0007528045680373907, 0.0007528045680373907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007528045680373907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075280
Iteration 2/1000 | Loss: 0.00003057
Iteration 3/1000 | Loss: 0.00002012
Iteration 4/1000 | Loss: 0.00001858
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001740
Iteration 7/1000 | Loss: 0.00001709
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001683
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001681
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001677
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001676
Iteration 23/1000 | Loss: 0.00001676
Iteration 24/1000 | Loss: 0.00001676
Iteration 25/1000 | Loss: 0.00001676
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001673
Iteration 29/1000 | Loss: 0.00001673
Iteration 30/1000 | Loss: 0.00001672
Iteration 31/1000 | Loss: 0.00001670
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00001668
Iteration 36/1000 | Loss: 0.00001668
Iteration 37/1000 | Loss: 0.00001668
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001666
Iteration 53/1000 | Loss: 0.00001666
Iteration 54/1000 | Loss: 0.00001666
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001665
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001664
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00001664
Iteration 62/1000 | Loss: 0.00001664
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001662
Iteration 71/1000 | Loss: 0.00001662
Iteration 72/1000 | Loss: 0.00001662
Iteration 73/1000 | Loss: 0.00001662
Iteration 74/1000 | Loss: 0.00001662
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001661
Iteration 78/1000 | Loss: 0.00001661
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001660
Iteration 81/1000 | Loss: 0.00001660
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001659
Iteration 88/1000 | Loss: 0.00001659
Iteration 89/1000 | Loss: 0.00001659
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001659
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001658
Iteration 105/1000 | Loss: 0.00001658
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001658
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001657
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001657
Iteration 140/1000 | Loss: 0.00001657
Iteration 141/1000 | Loss: 0.00001657
Iteration 142/1000 | Loss: 0.00001657
Iteration 143/1000 | Loss: 0.00001657
Iteration 144/1000 | Loss: 0.00001657
Iteration 145/1000 | Loss: 0.00001657
Iteration 146/1000 | Loss: 0.00001657
Iteration 147/1000 | Loss: 0.00001657
Iteration 148/1000 | Loss: 0.00001657
Iteration 149/1000 | Loss: 0.00001657
Iteration 150/1000 | Loss: 0.00001657
Iteration 151/1000 | Loss: 0.00001657
Iteration 152/1000 | Loss: 0.00001657
Iteration 153/1000 | Loss: 0.00001657
Iteration 154/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.6565178157179616e-05, 1.6565178157179616e-05, 1.6565178157179616e-05, 1.6565178157179616e-05, 1.6565178157179616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6565178157179616e-05

Optimization complete. Final v2v error: 3.4649548530578613 mm

Highest mean error: 3.7967581748962402 mm for frame 227

Lowest mean error: 3.1210384368896484 mm for frame 175

Saving results

Total time: 37.81869292259216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392716
Iteration 2/25 | Loss: 0.00131866
Iteration 3/25 | Loss: 0.00108284
Iteration 4/25 | Loss: 0.00103138
Iteration 5/25 | Loss: 0.00101751
Iteration 6/25 | Loss: 0.00101480
Iteration 7/25 | Loss: 0.00101421
Iteration 8/25 | Loss: 0.00101421
Iteration 9/25 | Loss: 0.00101421
Iteration 10/25 | Loss: 0.00101421
Iteration 11/25 | Loss: 0.00101421
Iteration 12/25 | Loss: 0.00101421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010142087703570724, 0.0010142087703570724, 0.0010142087703570724, 0.0010142087703570724, 0.0010142087703570724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010142087703570724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59686315
Iteration 2/25 | Loss: 0.00091886
Iteration 3/25 | Loss: 0.00091886
Iteration 4/25 | Loss: 0.00091886
Iteration 5/25 | Loss: 0.00091886
Iteration 6/25 | Loss: 0.00091886
Iteration 7/25 | Loss: 0.00091886
Iteration 8/25 | Loss: 0.00091886
Iteration 9/25 | Loss: 0.00091886
Iteration 10/25 | Loss: 0.00091886
Iteration 11/25 | Loss: 0.00091886
Iteration 12/25 | Loss: 0.00091886
Iteration 13/25 | Loss: 0.00091886
Iteration 14/25 | Loss: 0.00091886
Iteration 15/25 | Loss: 0.00091886
Iteration 16/25 | Loss: 0.00091886
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009188575786538422, 0.0009188575786538422, 0.0009188575786538422, 0.0009188575786538422, 0.0009188575786538422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009188575786538422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091886
Iteration 2/1000 | Loss: 0.00004614
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001651
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001648
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001625
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001624
Iteration 29/1000 | Loss: 0.00001624
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001624
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001623
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001622
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001620
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001620
Iteration 45/1000 | Loss: 0.00001620
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001619
Iteration 48/1000 | Loss: 0.00001619
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001615
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001614
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001611
Iteration 105/1000 | Loss: 0.00001611
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001611
Iteration 122/1000 | Loss: 0.00001611
Iteration 123/1000 | Loss: 0.00001611
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6107180272229016e-05, 1.6107180272229016e-05, 1.6107180272229016e-05, 1.6107180272229016e-05, 1.6107180272229016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6107180272229016e-05

Optimization complete. Final v2v error: 3.4965507984161377 mm

Highest mean error: 3.664412498474121 mm for frame 18

Lowest mean error: 3.3401901721954346 mm for frame 88

Saving results

Total time: 34.37749624252319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996101
Iteration 2/25 | Loss: 0.00112988
Iteration 3/25 | Loss: 0.00103751
Iteration 4/25 | Loss: 0.00100998
Iteration 5/25 | Loss: 0.00100045
Iteration 6/25 | Loss: 0.00099916
Iteration 7/25 | Loss: 0.00099916
Iteration 8/25 | Loss: 0.00099916
Iteration 9/25 | Loss: 0.00099916
Iteration 10/25 | Loss: 0.00099916
Iteration 11/25 | Loss: 0.00099916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009991578990593553, 0.0009991578990593553, 0.0009991578990593553, 0.0009991578990593553, 0.0009991578990593553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009991578990593553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55117178
Iteration 2/25 | Loss: 0.00074529
Iteration 3/25 | Loss: 0.00074528
Iteration 4/25 | Loss: 0.00074528
Iteration 5/25 | Loss: 0.00074528
Iteration 6/25 | Loss: 0.00074528
Iteration 7/25 | Loss: 0.00074528
Iteration 8/25 | Loss: 0.00074528
Iteration 9/25 | Loss: 0.00074528
Iteration 10/25 | Loss: 0.00074528
Iteration 11/25 | Loss: 0.00074528
Iteration 12/25 | Loss: 0.00074528
Iteration 13/25 | Loss: 0.00074528
Iteration 14/25 | Loss: 0.00074528
Iteration 15/25 | Loss: 0.00074528
Iteration 16/25 | Loss: 0.00074528
Iteration 17/25 | Loss: 0.00074528
Iteration 18/25 | Loss: 0.00074528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007452830323018134, 0.0007452830323018134, 0.0007452830323018134, 0.0007452830323018134, 0.0007452830323018134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007452830323018134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074528
Iteration 2/1000 | Loss: 0.00003558
Iteration 3/1000 | Loss: 0.00002810
Iteration 4/1000 | Loss: 0.00002570
Iteration 5/1000 | Loss: 0.00002446
Iteration 6/1000 | Loss: 0.00002335
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002238
Iteration 9/1000 | Loss: 0.00002238
Iteration 10/1000 | Loss: 0.00002237
Iteration 11/1000 | Loss: 0.00002236
Iteration 12/1000 | Loss: 0.00002226
Iteration 13/1000 | Loss: 0.00002217
Iteration 14/1000 | Loss: 0.00002216
Iteration 15/1000 | Loss: 0.00002212
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002212
Iteration 18/1000 | Loss: 0.00002211
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002210
Iteration 22/1000 | Loss: 0.00002210
Iteration 23/1000 | Loss: 0.00002209
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002209
Iteration 29/1000 | Loss: 0.00002209
Iteration 30/1000 | Loss: 0.00002208
Iteration 31/1000 | Loss: 0.00002208
Iteration 32/1000 | Loss: 0.00002207
Iteration 33/1000 | Loss: 0.00002207
Iteration 34/1000 | Loss: 0.00002207
Iteration 35/1000 | Loss: 0.00002206
Iteration 36/1000 | Loss: 0.00002205
Iteration 37/1000 | Loss: 0.00002205
Iteration 38/1000 | Loss: 0.00002205
Iteration 39/1000 | Loss: 0.00002204
Iteration 40/1000 | Loss: 0.00002204
Iteration 41/1000 | Loss: 0.00002204
Iteration 42/1000 | Loss: 0.00002204
Iteration 43/1000 | Loss: 0.00002204
Iteration 44/1000 | Loss: 0.00002203
Iteration 45/1000 | Loss: 0.00002203
Iteration 46/1000 | Loss: 0.00002203
Iteration 47/1000 | Loss: 0.00002203
Iteration 48/1000 | Loss: 0.00002203
Iteration 49/1000 | Loss: 0.00002202
Iteration 50/1000 | Loss: 0.00002202
Iteration 51/1000 | Loss: 0.00002202
Iteration 52/1000 | Loss: 0.00002202
Iteration 53/1000 | Loss: 0.00002201
Iteration 54/1000 | Loss: 0.00002201
Iteration 55/1000 | Loss: 0.00002201
Iteration 56/1000 | Loss: 0.00002201
Iteration 57/1000 | Loss: 0.00002201
Iteration 58/1000 | Loss: 0.00002201
Iteration 59/1000 | Loss: 0.00002201
Iteration 60/1000 | Loss: 0.00002201
Iteration 61/1000 | Loss: 0.00002201
Iteration 62/1000 | Loss: 0.00002201
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002200
Iteration 65/1000 | Loss: 0.00002200
Iteration 66/1000 | Loss: 0.00002200
Iteration 67/1000 | Loss: 0.00002200
Iteration 68/1000 | Loss: 0.00002200
Iteration 69/1000 | Loss: 0.00002200
Iteration 70/1000 | Loss: 0.00002200
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002199
Iteration 75/1000 | Loss: 0.00002199
Iteration 76/1000 | Loss: 0.00002199
Iteration 77/1000 | Loss: 0.00002198
Iteration 78/1000 | Loss: 0.00002198
Iteration 79/1000 | Loss: 0.00002198
Iteration 80/1000 | Loss: 0.00002198
Iteration 81/1000 | Loss: 0.00002198
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002198
Iteration 84/1000 | Loss: 0.00002198
Iteration 85/1000 | Loss: 0.00002197
Iteration 86/1000 | Loss: 0.00002197
Iteration 87/1000 | Loss: 0.00002197
Iteration 88/1000 | Loss: 0.00002197
Iteration 89/1000 | Loss: 0.00002197
Iteration 90/1000 | Loss: 0.00002197
Iteration 91/1000 | Loss: 0.00002197
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002197
Iteration 94/1000 | Loss: 0.00002197
Iteration 95/1000 | Loss: 0.00002197
Iteration 96/1000 | Loss: 0.00002197
Iteration 97/1000 | Loss: 0.00002197
Iteration 98/1000 | Loss: 0.00002196
Iteration 99/1000 | Loss: 0.00002196
Iteration 100/1000 | Loss: 0.00002196
Iteration 101/1000 | Loss: 0.00002196
Iteration 102/1000 | Loss: 0.00002196
Iteration 103/1000 | Loss: 0.00002196
Iteration 104/1000 | Loss: 0.00002196
Iteration 105/1000 | Loss: 0.00002196
Iteration 106/1000 | Loss: 0.00002195
Iteration 107/1000 | Loss: 0.00002195
Iteration 108/1000 | Loss: 0.00002195
Iteration 109/1000 | Loss: 0.00002195
Iteration 110/1000 | Loss: 0.00002195
Iteration 111/1000 | Loss: 0.00002195
Iteration 112/1000 | Loss: 0.00002195
Iteration 113/1000 | Loss: 0.00002195
Iteration 114/1000 | Loss: 0.00002195
Iteration 115/1000 | Loss: 0.00002195
Iteration 116/1000 | Loss: 0.00002195
Iteration 117/1000 | Loss: 0.00002195
Iteration 118/1000 | Loss: 0.00002195
Iteration 119/1000 | Loss: 0.00002195
Iteration 120/1000 | Loss: 0.00002195
Iteration 121/1000 | Loss: 0.00002195
Iteration 122/1000 | Loss: 0.00002195
Iteration 123/1000 | Loss: 0.00002195
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002195
Iteration 126/1000 | Loss: 0.00002195
Iteration 127/1000 | Loss: 0.00002195
Iteration 128/1000 | Loss: 0.00002195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.1948697394691408e-05, 2.1948697394691408e-05, 2.1948697394691408e-05, 2.1948697394691408e-05, 2.1948697394691408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1948697394691408e-05

Optimization complete. Final v2v error: 4.023614406585693 mm

Highest mean error: 4.2806830406188965 mm for frame 165

Lowest mean error: 3.753730535507202 mm for frame 132

Saving results

Total time: 34.44211721420288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916673
Iteration 2/25 | Loss: 0.00139616
Iteration 3/25 | Loss: 0.00113516
Iteration 4/25 | Loss: 0.00108954
Iteration 5/25 | Loss: 0.00107437
Iteration 6/25 | Loss: 0.00107138
Iteration 7/25 | Loss: 0.00107074
Iteration 8/25 | Loss: 0.00107074
Iteration 9/25 | Loss: 0.00107070
Iteration 10/25 | Loss: 0.00107070
Iteration 11/25 | Loss: 0.00107070
Iteration 12/25 | Loss: 0.00107070
Iteration 13/25 | Loss: 0.00107070
Iteration 14/25 | Loss: 0.00107070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010707047767937183, 0.0010707047767937183, 0.0010707047767937183, 0.0010707047767937183, 0.0010707047767937183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010707047767937183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49756336
Iteration 2/25 | Loss: 0.00092564
Iteration 3/25 | Loss: 0.00092559
Iteration 4/25 | Loss: 0.00092559
Iteration 5/25 | Loss: 0.00092559
Iteration 6/25 | Loss: 0.00092559
Iteration 7/25 | Loss: 0.00092558
Iteration 8/25 | Loss: 0.00092558
Iteration 9/25 | Loss: 0.00092558
Iteration 10/25 | Loss: 0.00092558
Iteration 11/25 | Loss: 0.00092558
Iteration 12/25 | Loss: 0.00092558
Iteration 13/25 | Loss: 0.00092558
Iteration 14/25 | Loss: 0.00092558
Iteration 15/25 | Loss: 0.00092558
Iteration 16/25 | Loss: 0.00092558
Iteration 17/25 | Loss: 0.00092558
Iteration 18/25 | Loss: 0.00092558
Iteration 19/25 | Loss: 0.00092558
Iteration 20/25 | Loss: 0.00092558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009255842305719852, 0.0009255842305719852, 0.0009255842305719852, 0.0009255842305719852, 0.0009255842305719852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009255842305719852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092558
Iteration 2/1000 | Loss: 0.00005077
Iteration 3/1000 | Loss: 0.00003219
Iteration 4/1000 | Loss: 0.00002668
Iteration 5/1000 | Loss: 0.00002524
Iteration 6/1000 | Loss: 0.00002411
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002295
Iteration 9/1000 | Loss: 0.00002244
Iteration 10/1000 | Loss: 0.00002213
Iteration 11/1000 | Loss: 0.00002191
Iteration 12/1000 | Loss: 0.00002183
Iteration 13/1000 | Loss: 0.00002170
Iteration 14/1000 | Loss: 0.00002168
Iteration 15/1000 | Loss: 0.00002166
Iteration 16/1000 | Loss: 0.00002166
Iteration 17/1000 | Loss: 0.00002164
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002155
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002149
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002147
Iteration 28/1000 | Loss: 0.00002147
Iteration 29/1000 | Loss: 0.00002146
Iteration 30/1000 | Loss: 0.00002145
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002142
Iteration 33/1000 | Loss: 0.00002141
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002140
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002139
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002136
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002135
Iteration 50/1000 | Loss: 0.00002135
Iteration 51/1000 | Loss: 0.00002134
Iteration 52/1000 | Loss: 0.00002134
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002133
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002132
Iteration 58/1000 | Loss: 0.00002132
Iteration 59/1000 | Loss: 0.00002132
Iteration 60/1000 | Loss: 0.00002131
Iteration 61/1000 | Loss: 0.00002131
Iteration 62/1000 | Loss: 0.00002131
Iteration 63/1000 | Loss: 0.00002131
Iteration 64/1000 | Loss: 0.00002131
Iteration 65/1000 | Loss: 0.00002130
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002130
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002127
Iteration 78/1000 | Loss: 0.00002127
Iteration 79/1000 | Loss: 0.00002127
Iteration 80/1000 | Loss: 0.00002127
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002126
Iteration 84/1000 | Loss: 0.00002126
Iteration 85/1000 | Loss: 0.00002126
Iteration 86/1000 | Loss: 0.00002126
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002125
Iteration 104/1000 | Loss: 0.00002125
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002124
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002124
Iteration 127/1000 | Loss: 0.00002124
Iteration 128/1000 | Loss: 0.00002124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.1244626623229124e-05, 2.1244626623229124e-05, 2.1244626623229124e-05, 2.1244626623229124e-05, 2.1244626623229124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1244626623229124e-05

Optimization complete. Final v2v error: 3.9505772590637207 mm

Highest mean error: 4.67751407623291 mm for frame 218

Lowest mean error: 3.4197022914886475 mm for frame 161

Saving results

Total time: 42.53197765350342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432755
Iteration 2/25 | Loss: 0.00135492
Iteration 3/25 | Loss: 0.00101417
Iteration 4/25 | Loss: 0.00096637
Iteration 5/25 | Loss: 0.00095680
Iteration 6/25 | Loss: 0.00095460
Iteration 7/25 | Loss: 0.00095400
Iteration 8/25 | Loss: 0.00095400
Iteration 9/25 | Loss: 0.00095400
Iteration 10/25 | Loss: 0.00095400
Iteration 11/25 | Loss: 0.00095400
Iteration 12/25 | Loss: 0.00095400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000954004586674273, 0.000954004586674273, 0.000954004586674273, 0.000954004586674273, 0.000954004586674273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000954004586674273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50650203
Iteration 2/25 | Loss: 0.00073512
Iteration 3/25 | Loss: 0.00073512
Iteration 4/25 | Loss: 0.00073512
Iteration 5/25 | Loss: 0.00073512
Iteration 6/25 | Loss: 0.00073512
Iteration 7/25 | Loss: 0.00073512
Iteration 8/25 | Loss: 0.00073512
Iteration 9/25 | Loss: 0.00073512
Iteration 10/25 | Loss: 0.00073512
Iteration 11/25 | Loss: 0.00073512
Iteration 12/25 | Loss: 0.00073512
Iteration 13/25 | Loss: 0.00073512
Iteration 14/25 | Loss: 0.00073512
Iteration 15/25 | Loss: 0.00073512
Iteration 16/25 | Loss: 0.00073512
Iteration 17/25 | Loss: 0.00073512
Iteration 18/25 | Loss: 0.00073512
Iteration 19/25 | Loss: 0.00073512
Iteration 20/25 | Loss: 0.00073512
Iteration 21/25 | Loss: 0.00073512
Iteration 22/25 | Loss: 0.00073512
Iteration 23/25 | Loss: 0.00073512
Iteration 24/25 | Loss: 0.00073512
Iteration 25/25 | Loss: 0.00073512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073512
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00001878
Iteration 4/1000 | Loss: 0.00001558
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001408
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001306
Iteration 19/1000 | Loss: 0.00001305
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001302
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001300
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001297
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00001281
Iteration 109/1000 | Loss: 0.00001281
Iteration 110/1000 | Loss: 0.00001281
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001280
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001280
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001280
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001278
Iteration 136/1000 | Loss: 0.00001278
Iteration 137/1000 | Loss: 0.00001278
Iteration 138/1000 | Loss: 0.00001278
Iteration 139/1000 | Loss: 0.00001278
Iteration 140/1000 | Loss: 0.00001278
Iteration 141/1000 | Loss: 0.00001278
Iteration 142/1000 | Loss: 0.00001278
Iteration 143/1000 | Loss: 0.00001278
Iteration 144/1000 | Loss: 0.00001278
Iteration 145/1000 | Loss: 0.00001278
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001277
Iteration 152/1000 | Loss: 0.00001277
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Iteration 163/1000 | Loss: 0.00001277
Iteration 164/1000 | Loss: 0.00001277
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001276
Iteration 174/1000 | Loss: 0.00001276
Iteration 175/1000 | Loss: 0.00001276
Iteration 176/1000 | Loss: 0.00001276
Iteration 177/1000 | Loss: 0.00001276
Iteration 178/1000 | Loss: 0.00001276
Iteration 179/1000 | Loss: 0.00001276
Iteration 180/1000 | Loss: 0.00001276
Iteration 181/1000 | Loss: 0.00001276
Iteration 182/1000 | Loss: 0.00001276
Iteration 183/1000 | Loss: 0.00001276
Iteration 184/1000 | Loss: 0.00001276
Iteration 185/1000 | Loss: 0.00001276
Iteration 186/1000 | Loss: 0.00001275
Iteration 187/1000 | Loss: 0.00001275
Iteration 188/1000 | Loss: 0.00001275
Iteration 189/1000 | Loss: 0.00001275
Iteration 190/1000 | Loss: 0.00001275
Iteration 191/1000 | Loss: 0.00001275
Iteration 192/1000 | Loss: 0.00001275
Iteration 193/1000 | Loss: 0.00001275
Iteration 194/1000 | Loss: 0.00001275
Iteration 195/1000 | Loss: 0.00001275
Iteration 196/1000 | Loss: 0.00001275
Iteration 197/1000 | Loss: 0.00001275
Iteration 198/1000 | Loss: 0.00001275
Iteration 199/1000 | Loss: 0.00001275
Iteration 200/1000 | Loss: 0.00001275
Iteration 201/1000 | Loss: 0.00001275
Iteration 202/1000 | Loss: 0.00001275
Iteration 203/1000 | Loss: 0.00001275
Iteration 204/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.2752127986459527e-05, 1.2752127986459527e-05, 1.2752127986459527e-05, 1.2752127986459527e-05, 1.2752127986459527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2752127986459527e-05

Optimization complete. Final v2v error: 3.1014084815979004 mm

Highest mean error: 3.7165584564208984 mm for frame 98

Lowest mean error: 2.833954095840454 mm for frame 6

Saving results

Total time: 39.15247106552124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079052
Iteration 2/25 | Loss: 0.00326116
Iteration 3/25 | Loss: 0.00256511
Iteration 4/25 | Loss: 0.00247792
Iteration 5/25 | Loss: 0.00276724
Iteration 6/25 | Loss: 0.00238061
Iteration 7/25 | Loss: 0.00225406
Iteration 8/25 | Loss: 0.00208074
Iteration 9/25 | Loss: 0.00197063
Iteration 10/25 | Loss: 0.00204110
Iteration 11/25 | Loss: 0.00206414
Iteration 12/25 | Loss: 0.00191951
Iteration 13/25 | Loss: 0.00192542
Iteration 14/25 | Loss: 0.00189252
Iteration 15/25 | Loss: 0.00189362
Iteration 16/25 | Loss: 0.00188781
Iteration 17/25 | Loss: 0.00188183
Iteration 18/25 | Loss: 0.00187525
Iteration 19/25 | Loss: 0.00187880
Iteration 20/25 | Loss: 0.00187437
Iteration 21/25 | Loss: 0.00187434
Iteration 22/25 | Loss: 0.00187434
Iteration 23/25 | Loss: 0.00187434
Iteration 24/25 | Loss: 0.00187433
Iteration 25/25 | Loss: 0.00187433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44668865
Iteration 2/25 | Loss: 0.01129533
Iteration 3/25 | Loss: 0.00696683
Iteration 4/25 | Loss: 0.00696681
Iteration 5/25 | Loss: 0.00696681
Iteration 6/25 | Loss: 0.00696681
Iteration 7/25 | Loss: 0.00696681
Iteration 8/25 | Loss: 0.00696681
Iteration 9/25 | Loss: 0.00696681
Iteration 10/25 | Loss: 0.00696681
Iteration 11/25 | Loss: 0.00696681
Iteration 12/25 | Loss: 0.00696681
Iteration 13/25 | Loss: 0.00696681
Iteration 14/25 | Loss: 0.00696681
Iteration 15/25 | Loss: 0.00696681
Iteration 16/25 | Loss: 0.00696681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.006966810207813978, 0.006966810207813978, 0.006966810207813978, 0.006966810207813978, 0.006966810207813978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006966810207813978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00696681
Iteration 2/1000 | Loss: 0.00997749
Iteration 3/1000 | Loss: 0.00143229
Iteration 4/1000 | Loss: 0.00243466
Iteration 5/1000 | Loss: 0.00259923
Iteration 6/1000 | Loss: 0.00113298
Iteration 7/1000 | Loss: 0.00074957
Iteration 8/1000 | Loss: 0.00096803
Iteration 9/1000 | Loss: 0.00596063
Iteration 10/1000 | Loss: 0.02202918
Iteration 11/1000 | Loss: 0.00465269
Iteration 12/1000 | Loss: 0.00394668
Iteration 13/1000 | Loss: 0.00179759
Iteration 14/1000 | Loss: 0.00171416
Iteration 15/1000 | Loss: 0.00219417
Iteration 16/1000 | Loss: 0.00128110
Iteration 17/1000 | Loss: 0.00038388
Iteration 18/1000 | Loss: 0.00058055
Iteration 19/1000 | Loss: 0.00121279
Iteration 20/1000 | Loss: 0.00103054
Iteration 21/1000 | Loss: 0.00067675
Iteration 22/1000 | Loss: 0.00015179
Iteration 23/1000 | Loss: 0.00010640
Iteration 24/1000 | Loss: 0.00008356
Iteration 25/1000 | Loss: 0.00046436
Iteration 26/1000 | Loss: 0.00111216
Iteration 27/1000 | Loss: 0.00042810
Iteration 28/1000 | Loss: 0.00042067
Iteration 29/1000 | Loss: 0.00026758
Iteration 30/1000 | Loss: 0.00021465
Iteration 31/1000 | Loss: 0.00126693
Iteration 32/1000 | Loss: 0.00003682
Iteration 33/1000 | Loss: 0.00003297
Iteration 34/1000 | Loss: 0.00003134
Iteration 35/1000 | Loss: 0.00002994
Iteration 36/1000 | Loss: 0.00051239
Iteration 37/1000 | Loss: 0.00036446
Iteration 38/1000 | Loss: 0.00026565
Iteration 39/1000 | Loss: 0.00026661
Iteration 40/1000 | Loss: 0.00012705
Iteration 41/1000 | Loss: 0.00004519
Iteration 42/1000 | Loss: 0.00006658
Iteration 43/1000 | Loss: 0.00004566
Iteration 44/1000 | Loss: 0.00003349
Iteration 45/1000 | Loss: 0.00007271
Iteration 46/1000 | Loss: 0.00003837
Iteration 47/1000 | Loss: 0.00002755
Iteration 48/1000 | Loss: 0.00002595
Iteration 49/1000 | Loss: 0.00011274
Iteration 50/1000 | Loss: 0.00002430
Iteration 51/1000 | Loss: 0.00002332
Iteration 52/1000 | Loss: 0.00002297
Iteration 53/1000 | Loss: 0.00002283
Iteration 54/1000 | Loss: 0.00002273
Iteration 55/1000 | Loss: 0.00002269
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002269
Iteration 58/1000 | Loss: 0.00002268
Iteration 59/1000 | Loss: 0.00002268
Iteration 60/1000 | Loss: 0.00002268
Iteration 61/1000 | Loss: 0.00002266
Iteration 62/1000 | Loss: 0.00002265
Iteration 63/1000 | Loss: 0.00002265
Iteration 64/1000 | Loss: 0.00002264
Iteration 65/1000 | Loss: 0.00002263
Iteration 66/1000 | Loss: 0.00002263
Iteration 67/1000 | Loss: 0.00002262
Iteration 68/1000 | Loss: 0.00002262
Iteration 69/1000 | Loss: 0.00002262
Iteration 70/1000 | Loss: 0.00002262
Iteration 71/1000 | Loss: 0.00002261
Iteration 72/1000 | Loss: 0.00002261
Iteration 73/1000 | Loss: 0.00002261
Iteration 74/1000 | Loss: 0.00002261
Iteration 75/1000 | Loss: 0.00002261
Iteration 76/1000 | Loss: 0.00002261
Iteration 77/1000 | Loss: 0.00002261
Iteration 78/1000 | Loss: 0.00002260
Iteration 79/1000 | Loss: 0.00002260
Iteration 80/1000 | Loss: 0.00002260
Iteration 81/1000 | Loss: 0.00002260
Iteration 82/1000 | Loss: 0.00002260
Iteration 83/1000 | Loss: 0.00002260
Iteration 84/1000 | Loss: 0.00002260
Iteration 85/1000 | Loss: 0.00002260
Iteration 86/1000 | Loss: 0.00002260
Iteration 87/1000 | Loss: 0.00002260
Iteration 88/1000 | Loss: 0.00002260
Iteration 89/1000 | Loss: 0.00002260
Iteration 90/1000 | Loss: 0.00002259
Iteration 91/1000 | Loss: 0.00002259
Iteration 92/1000 | Loss: 0.00002259
Iteration 93/1000 | Loss: 0.00002259
Iteration 94/1000 | Loss: 0.00002259
Iteration 95/1000 | Loss: 0.00002258
Iteration 96/1000 | Loss: 0.00002258
Iteration 97/1000 | Loss: 0.00002258
Iteration 98/1000 | Loss: 0.00002257
Iteration 99/1000 | Loss: 0.00002257
Iteration 100/1000 | Loss: 0.00002257
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002256
Iteration 103/1000 | Loss: 0.00006396
Iteration 104/1000 | Loss: 0.00002256
Iteration 105/1000 | Loss: 0.00002254
Iteration 106/1000 | Loss: 0.00002253
Iteration 107/1000 | Loss: 0.00002253
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002253
Iteration 112/1000 | Loss: 0.00002253
Iteration 113/1000 | Loss: 0.00002253
Iteration 114/1000 | Loss: 0.00002252
Iteration 115/1000 | Loss: 0.00002252
Iteration 116/1000 | Loss: 0.00002252
Iteration 117/1000 | Loss: 0.00002252
Iteration 118/1000 | Loss: 0.00002252
Iteration 119/1000 | Loss: 0.00002252
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002251
Iteration 122/1000 | Loss: 0.00002251
Iteration 123/1000 | Loss: 0.00002251
Iteration 124/1000 | Loss: 0.00002251
Iteration 125/1000 | Loss: 0.00002251
Iteration 126/1000 | Loss: 0.00002251
Iteration 127/1000 | Loss: 0.00002251
Iteration 128/1000 | Loss: 0.00002251
Iteration 129/1000 | Loss: 0.00002251
Iteration 130/1000 | Loss: 0.00002251
Iteration 131/1000 | Loss: 0.00002251
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002251
Iteration 141/1000 | Loss: 0.00002251
Iteration 142/1000 | Loss: 0.00002251
Iteration 143/1000 | Loss: 0.00002251
Iteration 144/1000 | Loss: 0.00002251
Iteration 145/1000 | Loss: 0.00002251
Iteration 146/1000 | Loss: 0.00002251
Iteration 147/1000 | Loss: 0.00002251
Iteration 148/1000 | Loss: 0.00002251
Iteration 149/1000 | Loss: 0.00002251
Iteration 150/1000 | Loss: 0.00002251
Iteration 151/1000 | Loss: 0.00002251
Iteration 152/1000 | Loss: 0.00002251
Iteration 153/1000 | Loss: 0.00002251
Iteration 154/1000 | Loss: 0.00002251
Iteration 155/1000 | Loss: 0.00002251
Iteration 156/1000 | Loss: 0.00002251
Iteration 157/1000 | Loss: 0.00002251
Iteration 158/1000 | Loss: 0.00002251
Iteration 159/1000 | Loss: 0.00002251
Iteration 160/1000 | Loss: 0.00002251
Iteration 161/1000 | Loss: 0.00002251
Iteration 162/1000 | Loss: 0.00002251
Iteration 163/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.2508695110445842e-05, 2.2508695110445842e-05, 2.2508695110445842e-05, 2.2508695110445842e-05, 2.2508695110445842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2508695110445842e-05

Optimization complete. Final v2v error: 3.9750988483428955 mm

Highest mean error: 5.284262180328369 mm for frame 0

Lowest mean error: 3.7978031635284424 mm for frame 96

Saving results

Total time: 125.7588381767273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061393
Iteration 2/25 | Loss: 0.00189946
Iteration 3/25 | Loss: 0.00132605
Iteration 4/25 | Loss: 0.00127689
Iteration 5/25 | Loss: 0.00125732
Iteration 6/25 | Loss: 0.00125405
Iteration 7/25 | Loss: 0.00125388
Iteration 8/25 | Loss: 0.00125388
Iteration 9/25 | Loss: 0.00125388
Iteration 10/25 | Loss: 0.00125388
Iteration 11/25 | Loss: 0.00125388
Iteration 12/25 | Loss: 0.00125388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012538840528577566, 0.0012538840528577566, 0.0012538840528577566, 0.0012538840528577566, 0.0012538840528577566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012538840528577566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96351707
Iteration 2/25 | Loss: 0.00092479
Iteration 3/25 | Loss: 0.00092479
Iteration 4/25 | Loss: 0.00092479
Iteration 5/25 | Loss: 0.00092479
Iteration 6/25 | Loss: 0.00092479
Iteration 7/25 | Loss: 0.00092478
Iteration 8/25 | Loss: 0.00092478
Iteration 9/25 | Loss: 0.00092478
Iteration 10/25 | Loss: 0.00092478
Iteration 11/25 | Loss: 0.00092478
Iteration 12/25 | Loss: 0.00092478
Iteration 13/25 | Loss: 0.00092478
Iteration 14/25 | Loss: 0.00092478
Iteration 15/25 | Loss: 0.00092478
Iteration 16/25 | Loss: 0.00092478
Iteration 17/25 | Loss: 0.00092478
Iteration 18/25 | Loss: 0.00092478
Iteration 19/25 | Loss: 0.00092478
Iteration 20/25 | Loss: 0.00092478
Iteration 21/25 | Loss: 0.00092478
Iteration 22/25 | Loss: 0.00092478
Iteration 23/25 | Loss: 0.00092478
Iteration 24/25 | Loss: 0.00092478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009247845155186951, 0.0009247845155186951, 0.0009247845155186951, 0.0009247845155186951, 0.0009247845155186951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009247845155186951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092478
Iteration 2/1000 | Loss: 0.00009670
Iteration 3/1000 | Loss: 0.00007417
Iteration 4/1000 | Loss: 0.00006890
Iteration 5/1000 | Loss: 0.00006527
Iteration 6/1000 | Loss: 0.00006277
Iteration 7/1000 | Loss: 0.00006125
Iteration 8/1000 | Loss: 0.00006034
Iteration 9/1000 | Loss: 0.00005955
Iteration 10/1000 | Loss: 0.00005910
Iteration 11/1000 | Loss: 0.00005876
Iteration 12/1000 | Loss: 0.00005856
Iteration 13/1000 | Loss: 0.00005832
Iteration 14/1000 | Loss: 0.00005810
Iteration 15/1000 | Loss: 0.00005791
Iteration 16/1000 | Loss: 0.00005779
Iteration 17/1000 | Loss: 0.00005765
Iteration 18/1000 | Loss: 0.00005755
Iteration 19/1000 | Loss: 0.00005754
Iteration 20/1000 | Loss: 0.00005749
Iteration 21/1000 | Loss: 0.00005748
Iteration 22/1000 | Loss: 0.00005747
Iteration 23/1000 | Loss: 0.00005740
Iteration 24/1000 | Loss: 0.00005737
Iteration 25/1000 | Loss: 0.00005737
Iteration 26/1000 | Loss: 0.00005736
Iteration 27/1000 | Loss: 0.00005731
Iteration 28/1000 | Loss: 0.00005729
Iteration 29/1000 | Loss: 0.00005727
Iteration 30/1000 | Loss: 0.00005727
Iteration 31/1000 | Loss: 0.00005727
Iteration 32/1000 | Loss: 0.00005727
Iteration 33/1000 | Loss: 0.00005727
Iteration 34/1000 | Loss: 0.00005727
Iteration 35/1000 | Loss: 0.00005726
Iteration 36/1000 | Loss: 0.00005726
Iteration 37/1000 | Loss: 0.00005724
Iteration 38/1000 | Loss: 0.00005724
Iteration 39/1000 | Loss: 0.00005724
Iteration 40/1000 | Loss: 0.00005724
Iteration 41/1000 | Loss: 0.00005724
Iteration 42/1000 | Loss: 0.00005724
Iteration 43/1000 | Loss: 0.00005724
Iteration 44/1000 | Loss: 0.00005723
Iteration 45/1000 | Loss: 0.00005723
Iteration 46/1000 | Loss: 0.00005723
Iteration 47/1000 | Loss: 0.00005722
Iteration 48/1000 | Loss: 0.00005722
Iteration 49/1000 | Loss: 0.00005722
Iteration 50/1000 | Loss: 0.00005721
Iteration 51/1000 | Loss: 0.00005720
Iteration 52/1000 | Loss: 0.00005720
Iteration 53/1000 | Loss: 0.00005720
Iteration 54/1000 | Loss: 0.00005719
Iteration 55/1000 | Loss: 0.00005719
Iteration 56/1000 | Loss: 0.00005719
Iteration 57/1000 | Loss: 0.00005719
Iteration 58/1000 | Loss: 0.00005718
Iteration 59/1000 | Loss: 0.00005718
Iteration 60/1000 | Loss: 0.00005718
Iteration 61/1000 | Loss: 0.00005716
Iteration 62/1000 | Loss: 0.00005716
Iteration 63/1000 | Loss: 0.00005716
Iteration 64/1000 | Loss: 0.00005716
Iteration 65/1000 | Loss: 0.00005716
Iteration 66/1000 | Loss: 0.00005716
Iteration 67/1000 | Loss: 0.00005716
Iteration 68/1000 | Loss: 0.00005716
Iteration 69/1000 | Loss: 0.00005716
Iteration 70/1000 | Loss: 0.00005714
Iteration 71/1000 | Loss: 0.00005714
Iteration 72/1000 | Loss: 0.00005714
Iteration 73/1000 | Loss: 0.00005713
Iteration 74/1000 | Loss: 0.00005713
Iteration 75/1000 | Loss: 0.00005713
Iteration 76/1000 | Loss: 0.00005713
Iteration 77/1000 | Loss: 0.00005713
Iteration 78/1000 | Loss: 0.00005713
Iteration 79/1000 | Loss: 0.00005713
Iteration 80/1000 | Loss: 0.00005713
Iteration 81/1000 | Loss: 0.00005712
Iteration 82/1000 | Loss: 0.00005712
Iteration 83/1000 | Loss: 0.00005710
Iteration 84/1000 | Loss: 0.00005709
Iteration 85/1000 | Loss: 0.00005709
Iteration 86/1000 | Loss: 0.00005708
Iteration 87/1000 | Loss: 0.00005708
Iteration 88/1000 | Loss: 0.00005708
Iteration 89/1000 | Loss: 0.00005708
Iteration 90/1000 | Loss: 0.00005707
Iteration 91/1000 | Loss: 0.00005707
Iteration 92/1000 | Loss: 0.00005707
Iteration 93/1000 | Loss: 0.00005704
Iteration 94/1000 | Loss: 0.00005704
Iteration 95/1000 | Loss: 0.00005704
Iteration 96/1000 | Loss: 0.00005704
Iteration 97/1000 | Loss: 0.00005704
Iteration 98/1000 | Loss: 0.00005704
Iteration 99/1000 | Loss: 0.00005704
Iteration 100/1000 | Loss: 0.00005704
Iteration 101/1000 | Loss: 0.00005703
Iteration 102/1000 | Loss: 0.00005702
Iteration 103/1000 | Loss: 0.00005702
Iteration 104/1000 | Loss: 0.00005702
Iteration 105/1000 | Loss: 0.00005701
Iteration 106/1000 | Loss: 0.00005701
Iteration 107/1000 | Loss: 0.00005701
Iteration 108/1000 | Loss: 0.00005701
Iteration 109/1000 | Loss: 0.00005701
Iteration 110/1000 | Loss: 0.00005701
Iteration 111/1000 | Loss: 0.00005701
Iteration 112/1000 | Loss: 0.00005701
Iteration 113/1000 | Loss: 0.00005701
Iteration 114/1000 | Loss: 0.00005701
Iteration 115/1000 | Loss: 0.00005701
Iteration 116/1000 | Loss: 0.00005701
Iteration 117/1000 | Loss: 0.00005701
Iteration 118/1000 | Loss: 0.00005701
Iteration 119/1000 | Loss: 0.00005700
Iteration 120/1000 | Loss: 0.00005700
Iteration 121/1000 | Loss: 0.00005700
Iteration 122/1000 | Loss: 0.00005700
Iteration 123/1000 | Loss: 0.00005700
Iteration 124/1000 | Loss: 0.00005699
Iteration 125/1000 | Loss: 0.00005699
Iteration 126/1000 | Loss: 0.00005699
Iteration 127/1000 | Loss: 0.00005699
Iteration 128/1000 | Loss: 0.00005698
Iteration 129/1000 | Loss: 0.00005698
Iteration 130/1000 | Loss: 0.00005698
Iteration 131/1000 | Loss: 0.00005698
Iteration 132/1000 | Loss: 0.00005698
Iteration 133/1000 | Loss: 0.00005698
Iteration 134/1000 | Loss: 0.00005698
Iteration 135/1000 | Loss: 0.00005697
Iteration 136/1000 | Loss: 0.00005697
Iteration 137/1000 | Loss: 0.00005697
Iteration 138/1000 | Loss: 0.00005697
Iteration 139/1000 | Loss: 0.00005697
Iteration 140/1000 | Loss: 0.00005697
Iteration 141/1000 | Loss: 0.00005697
Iteration 142/1000 | Loss: 0.00005697
Iteration 143/1000 | Loss: 0.00005697
Iteration 144/1000 | Loss: 0.00005696
Iteration 145/1000 | Loss: 0.00005696
Iteration 146/1000 | Loss: 0.00005696
Iteration 147/1000 | Loss: 0.00005696
Iteration 148/1000 | Loss: 0.00005696
Iteration 149/1000 | Loss: 0.00005696
Iteration 150/1000 | Loss: 0.00005696
Iteration 151/1000 | Loss: 0.00005695
Iteration 152/1000 | Loss: 0.00005695
Iteration 153/1000 | Loss: 0.00005695
Iteration 154/1000 | Loss: 0.00005695
Iteration 155/1000 | Loss: 0.00005695
Iteration 156/1000 | Loss: 0.00005695
Iteration 157/1000 | Loss: 0.00005694
Iteration 158/1000 | Loss: 0.00005694
Iteration 159/1000 | Loss: 0.00005694
Iteration 160/1000 | Loss: 0.00005694
Iteration 161/1000 | Loss: 0.00005693
Iteration 162/1000 | Loss: 0.00005693
Iteration 163/1000 | Loss: 0.00005693
Iteration 164/1000 | Loss: 0.00005693
Iteration 165/1000 | Loss: 0.00005693
Iteration 166/1000 | Loss: 0.00005693
Iteration 167/1000 | Loss: 0.00005693
Iteration 168/1000 | Loss: 0.00005692
Iteration 169/1000 | Loss: 0.00005692
Iteration 170/1000 | Loss: 0.00005692
Iteration 171/1000 | Loss: 0.00005692
Iteration 172/1000 | Loss: 0.00005692
Iteration 173/1000 | Loss: 0.00005692
Iteration 174/1000 | Loss: 0.00005692
Iteration 175/1000 | Loss: 0.00005692
Iteration 176/1000 | Loss: 0.00005691
Iteration 177/1000 | Loss: 0.00005691
Iteration 178/1000 | Loss: 0.00005691
Iteration 179/1000 | Loss: 0.00005691
Iteration 180/1000 | Loss: 0.00005691
Iteration 181/1000 | Loss: 0.00005691
Iteration 182/1000 | Loss: 0.00005691
Iteration 183/1000 | Loss: 0.00005691
Iteration 184/1000 | Loss: 0.00005691
Iteration 185/1000 | Loss: 0.00005691
Iteration 186/1000 | Loss: 0.00005691
Iteration 187/1000 | Loss: 0.00005691
Iteration 188/1000 | Loss: 0.00005691
Iteration 189/1000 | Loss: 0.00005691
Iteration 190/1000 | Loss: 0.00005691
Iteration 191/1000 | Loss: 0.00005691
Iteration 192/1000 | Loss: 0.00005691
Iteration 193/1000 | Loss: 0.00005691
Iteration 194/1000 | Loss: 0.00005691
Iteration 195/1000 | Loss: 0.00005691
Iteration 196/1000 | Loss: 0.00005691
Iteration 197/1000 | Loss: 0.00005691
Iteration 198/1000 | Loss: 0.00005691
Iteration 199/1000 | Loss: 0.00005691
Iteration 200/1000 | Loss: 0.00005691
Iteration 201/1000 | Loss: 0.00005691
Iteration 202/1000 | Loss: 0.00005691
Iteration 203/1000 | Loss: 0.00005691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [5.690687976311892e-05, 5.690687976311892e-05, 5.690687976311892e-05, 5.690687976311892e-05, 5.690687976311892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.690687976311892e-05

Optimization complete. Final v2v error: 6.283027172088623 mm

Highest mean error: 7.140410900115967 mm for frame 95

Lowest mean error: 5.749070167541504 mm for frame 45

Saving results

Total time: 59.16273307800293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936784
Iteration 2/25 | Loss: 0.00123199
Iteration 3/25 | Loss: 0.00112269
Iteration 4/25 | Loss: 0.00107968
Iteration 5/25 | Loss: 0.00106701
Iteration 6/25 | Loss: 0.00106452
Iteration 7/25 | Loss: 0.00106424
Iteration 8/25 | Loss: 0.00106424
Iteration 9/25 | Loss: 0.00106424
Iteration 10/25 | Loss: 0.00106424
Iteration 11/25 | Loss: 0.00106424
Iteration 12/25 | Loss: 0.00106424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010642356937751174, 0.0010642356937751174, 0.0010642356937751174, 0.0010642356937751174, 0.0010642356937751174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010642356937751174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45761597
Iteration 2/25 | Loss: 0.00063935
Iteration 3/25 | Loss: 0.00063935
Iteration 4/25 | Loss: 0.00063935
Iteration 5/25 | Loss: 0.00063935
Iteration 6/25 | Loss: 0.00063935
Iteration 7/25 | Loss: 0.00063935
Iteration 8/25 | Loss: 0.00063935
Iteration 9/25 | Loss: 0.00063935
Iteration 10/25 | Loss: 0.00063935
Iteration 11/25 | Loss: 0.00063935
Iteration 12/25 | Loss: 0.00063935
Iteration 13/25 | Loss: 0.00063935
Iteration 14/25 | Loss: 0.00063935
Iteration 15/25 | Loss: 0.00063935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006393525400198996, 0.0006393525400198996, 0.0006393525400198996, 0.0006393525400198996, 0.0006393525400198996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006393525400198996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063935
Iteration 2/1000 | Loss: 0.00005209
Iteration 3/1000 | Loss: 0.00003422
Iteration 4/1000 | Loss: 0.00003156
Iteration 5/1000 | Loss: 0.00003040
Iteration 6/1000 | Loss: 0.00002948
Iteration 7/1000 | Loss: 0.00002891
Iteration 8/1000 | Loss: 0.00002855
Iteration 9/1000 | Loss: 0.00002827
Iteration 10/1000 | Loss: 0.00002820
Iteration 11/1000 | Loss: 0.00002819
Iteration 12/1000 | Loss: 0.00002818
Iteration 13/1000 | Loss: 0.00002816
Iteration 14/1000 | Loss: 0.00002814
Iteration 15/1000 | Loss: 0.00002814
Iteration 16/1000 | Loss: 0.00002813
Iteration 17/1000 | Loss: 0.00002813
Iteration 18/1000 | Loss: 0.00002813
Iteration 19/1000 | Loss: 0.00002813
Iteration 20/1000 | Loss: 0.00002813
Iteration 21/1000 | Loss: 0.00002813
Iteration 22/1000 | Loss: 0.00002813
Iteration 23/1000 | Loss: 0.00002813
Iteration 24/1000 | Loss: 0.00002813
Iteration 25/1000 | Loss: 0.00002813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [2.813148603308946e-05, 2.813148603308946e-05, 2.813148603308946e-05, 2.813148603308946e-05, 2.813148603308946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.813148603308946e-05

Optimization complete. Final v2v error: 4.500464916229248 mm

Highest mean error: 4.717386245727539 mm for frame 7

Lowest mean error: 4.34037971496582 mm for frame 238

Saving results

Total time: 26.44157361984253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900996
Iteration 2/25 | Loss: 0.00131428
Iteration 3/25 | Loss: 0.00105294
Iteration 4/25 | Loss: 0.00101227
Iteration 5/25 | Loss: 0.00101075
Iteration 6/25 | Loss: 0.00100308
Iteration 7/25 | Loss: 0.00100272
Iteration 8/25 | Loss: 0.00100264
Iteration 9/25 | Loss: 0.00100264
Iteration 10/25 | Loss: 0.00100264
Iteration 11/25 | Loss: 0.00100264
Iteration 12/25 | Loss: 0.00100264
Iteration 13/25 | Loss: 0.00100264
Iteration 14/25 | Loss: 0.00100264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010026427917182446, 0.0010026427917182446, 0.0010026427917182446, 0.0010026427917182446, 0.0010026427917182446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010026427917182446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67245770
Iteration 2/25 | Loss: 0.00075229
Iteration 3/25 | Loss: 0.00075228
Iteration 4/25 | Loss: 0.00075228
Iteration 5/25 | Loss: 0.00075228
Iteration 6/25 | Loss: 0.00075227
Iteration 7/25 | Loss: 0.00075227
Iteration 8/25 | Loss: 0.00075227
Iteration 9/25 | Loss: 0.00075227
Iteration 10/25 | Loss: 0.00075227
Iteration 11/25 | Loss: 0.00075227
Iteration 12/25 | Loss: 0.00075227
Iteration 13/25 | Loss: 0.00075227
Iteration 14/25 | Loss: 0.00075227
Iteration 15/25 | Loss: 0.00075227
Iteration 16/25 | Loss: 0.00075227
Iteration 17/25 | Loss: 0.00075227
Iteration 18/25 | Loss: 0.00075227
Iteration 19/25 | Loss: 0.00075227
Iteration 20/25 | Loss: 0.00075227
Iteration 21/25 | Loss: 0.00075227
Iteration 22/25 | Loss: 0.00075227
Iteration 23/25 | Loss: 0.00075227
Iteration 24/25 | Loss: 0.00075227
Iteration 25/25 | Loss: 0.00075227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075227
Iteration 2/1000 | Loss: 0.00003239
Iteration 3/1000 | Loss: 0.00002361
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00002061
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001994
Iteration 9/1000 | Loss: 0.00001987
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001980
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001971
Iteration 24/1000 | Loss: 0.00001971
Iteration 25/1000 | Loss: 0.00001971
Iteration 26/1000 | Loss: 0.00001970
Iteration 27/1000 | Loss: 0.00001970
Iteration 28/1000 | Loss: 0.00001970
Iteration 29/1000 | Loss: 0.00001969
Iteration 30/1000 | Loss: 0.00001969
Iteration 31/1000 | Loss: 0.00001968
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001967
Iteration 34/1000 | Loss: 0.00001967
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001967
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001967
Iteration 44/1000 | Loss: 0.00001966
Iteration 45/1000 | Loss: 0.00001966
Iteration 46/1000 | Loss: 0.00001966
Iteration 47/1000 | Loss: 0.00001966
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001965
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001965
Iteration 60/1000 | Loss: 0.00001965
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001963
Iteration 98/1000 | Loss: 0.00001963
Iteration 99/1000 | Loss: 0.00001963
Iteration 100/1000 | Loss: 0.00001963
Iteration 101/1000 | Loss: 0.00001963
Iteration 102/1000 | Loss: 0.00001963
Iteration 103/1000 | Loss: 0.00001963
Iteration 104/1000 | Loss: 0.00001963
Iteration 105/1000 | Loss: 0.00001963
Iteration 106/1000 | Loss: 0.00001963
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001963
Iteration 113/1000 | Loss: 0.00001963
Iteration 114/1000 | Loss: 0.00001963
Iteration 115/1000 | Loss: 0.00001963
Iteration 116/1000 | Loss: 0.00001963
Iteration 117/1000 | Loss: 0.00001963
Iteration 118/1000 | Loss: 0.00001963
Iteration 119/1000 | Loss: 0.00001963
Iteration 120/1000 | Loss: 0.00001963
Iteration 121/1000 | Loss: 0.00001963
Iteration 122/1000 | Loss: 0.00001963
Iteration 123/1000 | Loss: 0.00001963
Iteration 124/1000 | Loss: 0.00001963
Iteration 125/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.962608439498581e-05, 1.962608439498581e-05, 1.962608439498581e-05, 1.962608439498581e-05, 1.962608439498581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.962608439498581e-05

Optimization complete. Final v2v error: 3.827558755874634 mm

Highest mean error: 4.107475280761719 mm for frame 233

Lowest mean error: 3.4436275959014893 mm for frame 88

Saving results

Total time: 36.548474073410034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410461
Iteration 2/25 | Loss: 0.00110199
Iteration 3/25 | Loss: 0.00101385
Iteration 4/25 | Loss: 0.00098967
Iteration 5/25 | Loss: 0.00098040
Iteration 6/25 | Loss: 0.00097820
Iteration 7/25 | Loss: 0.00097798
Iteration 8/25 | Loss: 0.00097798
Iteration 9/25 | Loss: 0.00097798
Iteration 10/25 | Loss: 0.00097798
Iteration 11/25 | Loss: 0.00097798
Iteration 12/25 | Loss: 0.00097798
Iteration 13/25 | Loss: 0.00097798
Iteration 14/25 | Loss: 0.00097798
Iteration 15/25 | Loss: 0.00097798
Iteration 16/25 | Loss: 0.00097798
Iteration 17/25 | Loss: 0.00097798
Iteration 18/25 | Loss: 0.00097798
Iteration 19/25 | Loss: 0.00097798
Iteration 20/25 | Loss: 0.00097798
Iteration 21/25 | Loss: 0.00097798
Iteration 22/25 | Loss: 0.00097798
Iteration 23/25 | Loss: 0.00097798
Iteration 24/25 | Loss: 0.00097798
Iteration 25/25 | Loss: 0.00097798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57080066
Iteration 2/25 | Loss: 0.00074185
Iteration 3/25 | Loss: 0.00074185
Iteration 4/25 | Loss: 0.00074185
Iteration 5/25 | Loss: 0.00074184
Iteration 6/25 | Loss: 0.00074184
Iteration 7/25 | Loss: 0.00074184
Iteration 8/25 | Loss: 0.00074184
Iteration 9/25 | Loss: 0.00074184
Iteration 10/25 | Loss: 0.00074184
Iteration 11/25 | Loss: 0.00074184
Iteration 12/25 | Loss: 0.00074184
Iteration 13/25 | Loss: 0.00074184
Iteration 14/25 | Loss: 0.00074184
Iteration 15/25 | Loss: 0.00074184
Iteration 16/25 | Loss: 0.00074184
Iteration 17/25 | Loss: 0.00074184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007418429013341665, 0.0007418429013341665, 0.0007418429013341665, 0.0007418429013341665, 0.0007418429013341665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007418429013341665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074184
Iteration 2/1000 | Loss: 0.00002577
Iteration 3/1000 | Loss: 0.00001818
Iteration 4/1000 | Loss: 0.00001660
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001506
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001502
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001502
Iteration 13/1000 | Loss: 0.00001502
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001490
Iteration 43/1000 | Loss: 0.00001490
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001488
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.4876823115628213e-05, 1.4876823115628213e-05, 1.4876823115628213e-05, 1.4876823115628213e-05, 1.4876823115628213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4876823115628213e-05

Optimization complete. Final v2v error: 3.349896192550659 mm

Highest mean error: 3.483579635620117 mm for frame 29

Lowest mean error: 3.1278557777404785 mm for frame 0

Saving results

Total time: 26.133653163909912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876333
Iteration 2/25 | Loss: 0.00119651
Iteration 3/25 | Loss: 0.00103665
Iteration 4/25 | Loss: 0.00102266
Iteration 5/25 | Loss: 0.00099321
Iteration 6/25 | Loss: 0.00099083
Iteration 7/25 | Loss: 0.00098777
Iteration 8/25 | Loss: 0.00098005
Iteration 9/25 | Loss: 0.00097879
Iteration 10/25 | Loss: 0.00097816
Iteration 11/25 | Loss: 0.00097779
Iteration 12/25 | Loss: 0.00097755
Iteration 13/25 | Loss: 0.00097744
Iteration 14/25 | Loss: 0.00097732
Iteration 15/25 | Loss: 0.00097710
Iteration 16/25 | Loss: 0.00097690
Iteration 17/25 | Loss: 0.00097679
Iteration 18/25 | Loss: 0.00097672
Iteration 19/25 | Loss: 0.00097665
Iteration 20/25 | Loss: 0.00097665
Iteration 21/25 | Loss: 0.00097657
Iteration 22/25 | Loss: 0.00097642
Iteration 23/25 | Loss: 0.00097796
Iteration 24/25 | Loss: 0.00097428
Iteration 25/25 | Loss: 0.00097346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51583195
Iteration 2/25 | Loss: 0.00079597
Iteration 3/25 | Loss: 0.00079597
Iteration 4/25 | Loss: 0.00079597
Iteration 5/25 | Loss: 0.00079597
Iteration 6/25 | Loss: 0.00079596
Iteration 7/25 | Loss: 0.00079596
Iteration 8/25 | Loss: 0.00079596
Iteration 9/25 | Loss: 0.00079596
Iteration 10/25 | Loss: 0.00079596
Iteration 11/25 | Loss: 0.00079596
Iteration 12/25 | Loss: 0.00079596
Iteration 13/25 | Loss: 0.00079596
Iteration 14/25 | Loss: 0.00079596
Iteration 15/25 | Loss: 0.00079596
Iteration 16/25 | Loss: 0.00079596
Iteration 17/25 | Loss: 0.00079596
Iteration 18/25 | Loss: 0.00079596
Iteration 19/25 | Loss: 0.00079596
Iteration 20/25 | Loss: 0.00079596
Iteration 21/25 | Loss: 0.00079596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007959644426591694, 0.0007959644426591694, 0.0007959644426591694, 0.0007959644426591694, 0.0007959644426591694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007959644426591694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079596
Iteration 2/1000 | Loss: 0.00004131
Iteration 3/1000 | Loss: 0.00002655
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001735
Iteration 9/1000 | Loss: 0.00001703
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001666
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001649
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001647
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001646
Iteration 43/1000 | Loss: 0.00001646
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001645
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001645
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001641
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001641
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001634
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001632
Iteration 81/1000 | Loss: 0.00001632
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001632
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001632
Iteration 87/1000 | Loss: 0.00001632
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001631
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00001631
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001630
Iteration 99/1000 | Loss: 0.00001630
Iteration 100/1000 | Loss: 0.00001630
Iteration 101/1000 | Loss: 0.00001630
Iteration 102/1000 | Loss: 0.00001630
Iteration 103/1000 | Loss: 0.00001629
Iteration 104/1000 | Loss: 0.00001629
Iteration 105/1000 | Loss: 0.00001629
Iteration 106/1000 | Loss: 0.00001629
Iteration 107/1000 | Loss: 0.00001629
Iteration 108/1000 | Loss: 0.00001629
Iteration 109/1000 | Loss: 0.00001629
Iteration 110/1000 | Loss: 0.00001629
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001629
Iteration 114/1000 | Loss: 0.00001629
Iteration 115/1000 | Loss: 0.00001628
Iteration 116/1000 | Loss: 0.00001628
Iteration 117/1000 | Loss: 0.00001628
Iteration 118/1000 | Loss: 0.00001628
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001627
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001626
Iteration 130/1000 | Loss: 0.00001626
Iteration 131/1000 | Loss: 0.00001626
Iteration 132/1000 | Loss: 0.00001626
Iteration 133/1000 | Loss: 0.00001626
Iteration 134/1000 | Loss: 0.00001626
Iteration 135/1000 | Loss: 0.00001626
Iteration 136/1000 | Loss: 0.00001626
Iteration 137/1000 | Loss: 0.00001626
Iteration 138/1000 | Loss: 0.00001626
Iteration 139/1000 | Loss: 0.00001626
Iteration 140/1000 | Loss: 0.00001626
Iteration 141/1000 | Loss: 0.00001626
Iteration 142/1000 | Loss: 0.00001626
Iteration 143/1000 | Loss: 0.00001626
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001625
Iteration 146/1000 | Loss: 0.00001625
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001624
Iteration 150/1000 | Loss: 0.00001624
Iteration 151/1000 | Loss: 0.00001624
Iteration 152/1000 | Loss: 0.00001624
Iteration 153/1000 | Loss: 0.00001624
Iteration 154/1000 | Loss: 0.00001624
Iteration 155/1000 | Loss: 0.00001624
Iteration 156/1000 | Loss: 0.00001624
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001624
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001624
Iteration 163/1000 | Loss: 0.00001624
Iteration 164/1000 | Loss: 0.00001624
Iteration 165/1000 | Loss: 0.00001623
Iteration 166/1000 | Loss: 0.00001623
Iteration 167/1000 | Loss: 0.00001623
Iteration 168/1000 | Loss: 0.00001623
Iteration 169/1000 | Loss: 0.00001623
Iteration 170/1000 | Loss: 0.00001623
Iteration 171/1000 | Loss: 0.00001623
Iteration 172/1000 | Loss: 0.00001623
Iteration 173/1000 | Loss: 0.00001623
Iteration 174/1000 | Loss: 0.00001623
Iteration 175/1000 | Loss: 0.00001623
Iteration 176/1000 | Loss: 0.00001623
Iteration 177/1000 | Loss: 0.00001623
Iteration 178/1000 | Loss: 0.00001623
Iteration 179/1000 | Loss: 0.00001623
Iteration 180/1000 | Loss: 0.00001622
Iteration 181/1000 | Loss: 0.00001622
Iteration 182/1000 | Loss: 0.00001622
Iteration 183/1000 | Loss: 0.00001622
Iteration 184/1000 | Loss: 0.00001622
Iteration 185/1000 | Loss: 0.00001622
Iteration 186/1000 | Loss: 0.00001622
Iteration 187/1000 | Loss: 0.00001622
Iteration 188/1000 | Loss: 0.00001622
Iteration 189/1000 | Loss: 0.00001622
Iteration 190/1000 | Loss: 0.00001622
Iteration 191/1000 | Loss: 0.00001622
Iteration 192/1000 | Loss: 0.00001622
Iteration 193/1000 | Loss: 0.00001622
Iteration 194/1000 | Loss: 0.00001622
Iteration 195/1000 | Loss: 0.00001622
Iteration 196/1000 | Loss: 0.00001622
Iteration 197/1000 | Loss: 0.00001622
Iteration 198/1000 | Loss: 0.00001622
Iteration 199/1000 | Loss: 0.00001622
Iteration 200/1000 | Loss: 0.00001622
Iteration 201/1000 | Loss: 0.00001622
Iteration 202/1000 | Loss: 0.00001622
Iteration 203/1000 | Loss: 0.00001622
Iteration 204/1000 | Loss: 0.00001622
Iteration 205/1000 | Loss: 0.00001622
Iteration 206/1000 | Loss: 0.00001622
Iteration 207/1000 | Loss: 0.00001622
Iteration 208/1000 | Loss: 0.00001622
Iteration 209/1000 | Loss: 0.00001622
Iteration 210/1000 | Loss: 0.00001622
Iteration 211/1000 | Loss: 0.00001622
Iteration 212/1000 | Loss: 0.00001622
Iteration 213/1000 | Loss: 0.00001622
Iteration 214/1000 | Loss: 0.00001622
Iteration 215/1000 | Loss: 0.00001622
Iteration 216/1000 | Loss: 0.00001622
Iteration 217/1000 | Loss: 0.00001622
Iteration 218/1000 | Loss: 0.00001622
Iteration 219/1000 | Loss: 0.00001622
Iteration 220/1000 | Loss: 0.00001622
Iteration 221/1000 | Loss: 0.00001622
Iteration 222/1000 | Loss: 0.00001622
Iteration 223/1000 | Loss: 0.00001622
Iteration 224/1000 | Loss: 0.00001622
Iteration 225/1000 | Loss: 0.00001622
Iteration 226/1000 | Loss: 0.00001622
Iteration 227/1000 | Loss: 0.00001622
Iteration 228/1000 | Loss: 0.00001622
Iteration 229/1000 | Loss: 0.00001622
Iteration 230/1000 | Loss: 0.00001622
Iteration 231/1000 | Loss: 0.00001622
Iteration 232/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.6217038137256168e-05, 1.6217038137256168e-05, 1.6217038137256168e-05, 1.6217038137256168e-05, 1.6217038137256168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6217038137256168e-05

Optimization complete. Final v2v error: 3.425661325454712 mm

Highest mean error: 4.742375373840332 mm for frame 223

Lowest mean error: 3.0197694301605225 mm for frame 176

Saving results

Total time: 82.09692716598511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01132162
Iteration 2/25 | Loss: 0.00220746
Iteration 3/25 | Loss: 0.00157769
Iteration 4/25 | Loss: 0.00133136
Iteration 5/25 | Loss: 0.00123853
Iteration 6/25 | Loss: 0.00127832
Iteration 7/25 | Loss: 0.00121990
Iteration 8/25 | Loss: 0.00118645
Iteration 9/25 | Loss: 0.00117594
Iteration 10/25 | Loss: 0.00117269
Iteration 11/25 | Loss: 0.00116982
Iteration 12/25 | Loss: 0.00116645
Iteration 13/25 | Loss: 0.00116492
Iteration 14/25 | Loss: 0.00116289
Iteration 15/25 | Loss: 0.00116283
Iteration 16/25 | Loss: 0.00116323
Iteration 17/25 | Loss: 0.00116313
Iteration 18/25 | Loss: 0.00116247
Iteration 19/25 | Loss: 0.00116273
Iteration 20/25 | Loss: 0.00116280
Iteration 21/25 | Loss: 0.00116339
Iteration 22/25 | Loss: 0.00116268
Iteration 23/25 | Loss: 0.00116381
Iteration 24/25 | Loss: 0.00116151
Iteration 25/25 | Loss: 0.00116356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46992862
Iteration 2/25 | Loss: 0.00111554
Iteration 3/25 | Loss: 0.00111554
Iteration 4/25 | Loss: 0.00111554
Iteration 5/25 | Loss: 0.00111554
Iteration 6/25 | Loss: 0.00111554
Iteration 7/25 | Loss: 0.00111554
Iteration 8/25 | Loss: 0.00111554
Iteration 9/25 | Loss: 0.00111554
Iteration 10/25 | Loss: 0.00111554
Iteration 11/25 | Loss: 0.00111554
Iteration 12/25 | Loss: 0.00111554
Iteration 13/25 | Loss: 0.00111554
Iteration 14/25 | Loss: 0.00111554
Iteration 15/25 | Loss: 0.00111554
Iteration 16/25 | Loss: 0.00111554
Iteration 17/25 | Loss: 0.00111554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001115542370826006, 0.001115542370826006, 0.001115542370826006, 0.001115542370826006, 0.001115542370826006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001115542370826006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111554
Iteration 2/1000 | Loss: 0.00007696
Iteration 3/1000 | Loss: 0.00005632
Iteration 4/1000 | Loss: 0.00004301
Iteration 5/1000 | Loss: 0.00005061
Iteration 6/1000 | Loss: 0.00003157
Iteration 7/1000 | Loss: 0.00004026
Iteration 8/1000 | Loss: 0.00003396
Iteration 9/1000 | Loss: 0.00004443
Iteration 10/1000 | Loss: 0.00004533
Iteration 11/1000 | Loss: 0.00005463
Iteration 12/1000 | Loss: 0.00004529
Iteration 13/1000 | Loss: 0.00005125
Iteration 14/1000 | Loss: 0.00004531
Iteration 15/1000 | Loss: 0.00005535
Iteration 16/1000 | Loss: 0.00004440
Iteration 17/1000 | Loss: 0.00005044
Iteration 18/1000 | Loss: 0.00004974
Iteration 19/1000 | Loss: 0.00003245
Iteration 20/1000 | Loss: 0.00003468
Iteration 21/1000 | Loss: 0.00005123
Iteration 22/1000 | Loss: 0.00003945
Iteration 23/1000 | Loss: 0.00004064
Iteration 24/1000 | Loss: 0.00005100
Iteration 25/1000 | Loss: 0.00003774
Iteration 26/1000 | Loss: 0.00005068
Iteration 27/1000 | Loss: 0.00002973
Iteration 28/1000 | Loss: 0.00003648
Iteration 29/1000 | Loss: 0.00003748
Iteration 30/1000 | Loss: 0.00004226
Iteration 31/1000 | Loss: 0.00004441
Iteration 32/1000 | Loss: 0.00004172
Iteration 33/1000 | Loss: 0.00004389
Iteration 34/1000 | Loss: 0.00004161
Iteration 35/1000 | Loss: 0.00005501
Iteration 36/1000 | Loss: 0.00006628
Iteration 37/1000 | Loss: 0.00004743
Iteration 38/1000 | Loss: 0.00004307
Iteration 39/1000 | Loss: 0.00003301
Iteration 40/1000 | Loss: 0.00004074
Iteration 41/1000 | Loss: 0.00003778
Iteration 42/1000 | Loss: 0.00004043
Iteration 43/1000 | Loss: 0.00004102
Iteration 44/1000 | Loss: 0.00004740
Iteration 45/1000 | Loss: 0.00004970
Iteration 46/1000 | Loss: 0.00004702
Iteration 47/1000 | Loss: 0.00004851
Iteration 48/1000 | Loss: 0.00004523
Iteration 49/1000 | Loss: 0.00004240
Iteration 50/1000 | Loss: 0.00004134
Iteration 51/1000 | Loss: 0.00004864
Iteration 52/1000 | Loss: 0.00004146
Iteration 53/1000 | Loss: 0.00003796
Iteration 54/1000 | Loss: 0.00004701
Iteration 55/1000 | Loss: 0.00005226
Iteration 56/1000 | Loss: 0.00006015
Iteration 57/1000 | Loss: 0.00003861
Iteration 58/1000 | Loss: 0.00004660
Iteration 59/1000 | Loss: 0.00005616
Iteration 60/1000 | Loss: 0.00003620
Iteration 61/1000 | Loss: 0.00005394
Iteration 62/1000 | Loss: 0.00019618
Iteration 63/1000 | Loss: 0.00003730
Iteration 64/1000 | Loss: 0.00017494
Iteration 65/1000 | Loss: 0.00004212
Iteration 66/1000 | Loss: 0.00003671
Iteration 67/1000 | Loss: 0.00003135
Iteration 68/1000 | Loss: 0.00002932
Iteration 69/1000 | Loss: 0.00002823
Iteration 70/1000 | Loss: 0.00002769
Iteration 71/1000 | Loss: 0.00002708
Iteration 72/1000 | Loss: 0.00002655
Iteration 73/1000 | Loss: 0.00002611
Iteration 74/1000 | Loss: 0.00002572
Iteration 75/1000 | Loss: 0.00002551
Iteration 76/1000 | Loss: 0.00002542
Iteration 77/1000 | Loss: 0.00002537
Iteration 78/1000 | Loss: 0.00002536
Iteration 79/1000 | Loss: 0.00002529
Iteration 80/1000 | Loss: 0.00002528
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002527
Iteration 85/1000 | Loss: 0.00002526
Iteration 86/1000 | Loss: 0.00002526
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002524
Iteration 90/1000 | Loss: 0.00002524
Iteration 91/1000 | Loss: 0.00002524
Iteration 92/1000 | Loss: 0.00002523
Iteration 93/1000 | Loss: 0.00002523
Iteration 94/1000 | Loss: 0.00002523
Iteration 95/1000 | Loss: 0.00002523
Iteration 96/1000 | Loss: 0.00002522
Iteration 97/1000 | Loss: 0.00002522
Iteration 98/1000 | Loss: 0.00002522
Iteration 99/1000 | Loss: 0.00002522
Iteration 100/1000 | Loss: 0.00002522
Iteration 101/1000 | Loss: 0.00002522
Iteration 102/1000 | Loss: 0.00002522
Iteration 103/1000 | Loss: 0.00002522
Iteration 104/1000 | Loss: 0.00002522
Iteration 105/1000 | Loss: 0.00002522
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002522
Iteration 109/1000 | Loss: 0.00002522
Iteration 110/1000 | Loss: 0.00002522
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002522
Iteration 117/1000 | Loss: 0.00002522
Iteration 118/1000 | Loss: 0.00002522
Iteration 119/1000 | Loss: 0.00002522
Iteration 120/1000 | Loss: 0.00002522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.5215587811544538e-05, 2.5215587811544538e-05, 2.5215587811544538e-05, 2.5215587811544538e-05, 2.5215587811544538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5215587811544538e-05

Optimization complete. Final v2v error: 4.204456806182861 mm

Highest mean error: 5.115699291229248 mm for frame 128

Lowest mean error: 3.907820224761963 mm for frame 132

Saving results

Total time: 154.90308380126953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628353
Iteration 2/25 | Loss: 0.00137944
Iteration 3/25 | Loss: 0.00108243
Iteration 4/25 | Loss: 0.00099452
Iteration 5/25 | Loss: 0.00095727
Iteration 6/25 | Loss: 0.00095412
Iteration 7/25 | Loss: 0.00094722
Iteration 8/25 | Loss: 0.00094472
Iteration 9/25 | Loss: 0.00094580
Iteration 10/25 | Loss: 0.00094093
Iteration 11/25 | Loss: 0.00094370
Iteration 12/25 | Loss: 0.00093970
Iteration 13/25 | Loss: 0.00094419
Iteration 14/25 | Loss: 0.00093984
Iteration 15/25 | Loss: 0.00093933
Iteration 16/25 | Loss: 0.00094365
Iteration 17/25 | Loss: 0.00093944
Iteration 18/25 | Loss: 0.00093981
Iteration 19/25 | Loss: 0.00094365
Iteration 20/25 | Loss: 0.00094182
Iteration 21/25 | Loss: 0.00093955
Iteration 22/25 | Loss: 0.00093955
Iteration 23/25 | Loss: 0.00093954
Iteration 24/25 | Loss: 0.00093954
Iteration 25/25 | Loss: 0.00093954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.68240261
Iteration 2/25 | Loss: 0.00072909
Iteration 3/25 | Loss: 0.00072905
Iteration 4/25 | Loss: 0.00072905
Iteration 5/25 | Loss: 0.00072905
Iteration 6/25 | Loss: 0.00072905
Iteration 7/25 | Loss: 0.00072905
Iteration 8/25 | Loss: 0.00072905
Iteration 9/25 | Loss: 0.00072905
Iteration 10/25 | Loss: 0.00072905
Iteration 11/25 | Loss: 0.00072905
Iteration 12/25 | Loss: 0.00072905
Iteration 13/25 | Loss: 0.00072905
Iteration 14/25 | Loss: 0.00072905
Iteration 15/25 | Loss: 0.00072905
Iteration 16/25 | Loss: 0.00072905
Iteration 17/25 | Loss: 0.00072905
Iteration 18/25 | Loss: 0.00072905
Iteration 19/25 | Loss: 0.00072905
Iteration 20/25 | Loss: 0.00072905
Iteration 21/25 | Loss: 0.00072905
Iteration 22/25 | Loss: 0.00072905
Iteration 23/25 | Loss: 0.00072905
Iteration 24/25 | Loss: 0.00072905
Iteration 25/25 | Loss: 0.00072905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072905
Iteration 2/1000 | Loss: 0.00002805
Iteration 3/1000 | Loss: 0.00001986
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001613
Iteration 9/1000 | Loss: 0.00001602
Iteration 10/1000 | Loss: 0.00001599
Iteration 11/1000 | Loss: 0.00001598
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001572
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001571
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001570
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001569
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001622
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001561
Iteration 71/1000 | Loss: 0.00001561
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001561
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.5604746295139194e-05, 1.5604746295139194e-05, 1.5604746295139194e-05, 1.5604746295139194e-05, 1.5604746295139194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5604746295139194e-05

Optimization complete. Final v2v error: 3.397982358932495 mm

Highest mean error: 9.293971061706543 mm for frame 215

Lowest mean error: 3.0416345596313477 mm for frame 113

Saving results

Total time: 69.17736148834229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936018
Iteration 2/25 | Loss: 0.00153086
Iteration 3/25 | Loss: 0.00111467
Iteration 4/25 | Loss: 0.00105719
Iteration 5/25 | Loss: 0.00104814
Iteration 6/25 | Loss: 0.00104664
Iteration 7/25 | Loss: 0.00104664
Iteration 8/25 | Loss: 0.00104664
Iteration 9/25 | Loss: 0.00104664
Iteration 10/25 | Loss: 0.00104664
Iteration 11/25 | Loss: 0.00104664
Iteration 12/25 | Loss: 0.00104664
Iteration 13/25 | Loss: 0.00104664
Iteration 14/25 | Loss: 0.00104664
Iteration 15/25 | Loss: 0.00104664
Iteration 16/25 | Loss: 0.00104664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001046635676175356, 0.001046635676175356, 0.001046635676175356, 0.001046635676175356, 0.001046635676175356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001046635676175356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.13643742
Iteration 2/25 | Loss: 0.00082296
Iteration 3/25 | Loss: 0.00082293
Iteration 4/25 | Loss: 0.00082293
Iteration 5/25 | Loss: 0.00082293
Iteration 6/25 | Loss: 0.00082293
Iteration 7/25 | Loss: 0.00082293
Iteration 8/25 | Loss: 0.00082293
Iteration 9/25 | Loss: 0.00082293
Iteration 10/25 | Loss: 0.00082293
Iteration 11/25 | Loss: 0.00082293
Iteration 12/25 | Loss: 0.00082293
Iteration 13/25 | Loss: 0.00082293
Iteration 14/25 | Loss: 0.00082293
Iteration 15/25 | Loss: 0.00082293
Iteration 16/25 | Loss: 0.00082293
Iteration 17/25 | Loss: 0.00082293
Iteration 18/25 | Loss: 0.00082293
Iteration 19/25 | Loss: 0.00082293
Iteration 20/25 | Loss: 0.00082293
Iteration 21/25 | Loss: 0.00082293
Iteration 22/25 | Loss: 0.00082293
Iteration 23/25 | Loss: 0.00082293
Iteration 24/25 | Loss: 0.00082293
Iteration 25/25 | Loss: 0.00082293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082293
Iteration 2/1000 | Loss: 0.00003656
Iteration 3/1000 | Loss: 0.00002388
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001870
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00001788
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001749
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001745
Iteration 20/1000 | Loss: 0.00001745
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00001744
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001733
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001733
Iteration 36/1000 | Loss: 0.00001733
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001729
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001728
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001728
Iteration 48/1000 | Loss: 0.00001728
Iteration 49/1000 | Loss: 0.00001728
Iteration 50/1000 | Loss: 0.00001728
Iteration 51/1000 | Loss: 0.00001728
Iteration 52/1000 | Loss: 0.00001728
Iteration 53/1000 | Loss: 0.00001728
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00001727
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001724
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001718
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001713
Iteration 133/1000 | Loss: 0.00001713
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001712
Iteration 139/1000 | Loss: 0.00001712
Iteration 140/1000 | Loss: 0.00001712
Iteration 141/1000 | Loss: 0.00001712
Iteration 142/1000 | Loss: 0.00001712
Iteration 143/1000 | Loss: 0.00001712
Iteration 144/1000 | Loss: 0.00001712
Iteration 145/1000 | Loss: 0.00001712
Iteration 146/1000 | Loss: 0.00001712
Iteration 147/1000 | Loss: 0.00001712
Iteration 148/1000 | Loss: 0.00001712
Iteration 149/1000 | Loss: 0.00001712
Iteration 150/1000 | Loss: 0.00001712
Iteration 151/1000 | Loss: 0.00001712
Iteration 152/1000 | Loss: 0.00001712
Iteration 153/1000 | Loss: 0.00001712
Iteration 154/1000 | Loss: 0.00001712
Iteration 155/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.711678305582609e-05, 1.711678305582609e-05, 1.711678305582609e-05, 1.711678305582609e-05, 1.711678305582609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711678305582609e-05

Optimization complete. Final v2v error: 3.5875017642974854 mm

Highest mean error: 4.066681861877441 mm for frame 0

Lowest mean error: 3.2915310859680176 mm for frame 228

Saving results

Total time: 40.658448934555054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898632
Iteration 2/25 | Loss: 0.00176674
Iteration 3/25 | Loss: 0.00124750
Iteration 4/25 | Loss: 0.00122164
Iteration 5/25 | Loss: 0.00121012
Iteration 6/25 | Loss: 0.00120757
Iteration 7/25 | Loss: 0.00120726
Iteration 8/25 | Loss: 0.00120726
Iteration 9/25 | Loss: 0.00120726
Iteration 10/25 | Loss: 0.00120726
Iteration 11/25 | Loss: 0.00120726
Iteration 12/25 | Loss: 0.00120726
Iteration 13/25 | Loss: 0.00120726
Iteration 14/25 | Loss: 0.00120726
Iteration 15/25 | Loss: 0.00120726
Iteration 16/25 | Loss: 0.00120726
Iteration 17/25 | Loss: 0.00120726
Iteration 18/25 | Loss: 0.00120726
Iteration 19/25 | Loss: 0.00120726
Iteration 20/25 | Loss: 0.00120726
Iteration 21/25 | Loss: 0.00120726
Iteration 22/25 | Loss: 0.00120726
Iteration 23/25 | Loss: 0.00120726
Iteration 24/25 | Loss: 0.00120726
Iteration 25/25 | Loss: 0.00120726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62658376
Iteration 2/25 | Loss: 0.00064962
Iteration 3/25 | Loss: 0.00064962
Iteration 4/25 | Loss: 0.00064962
Iteration 5/25 | Loss: 0.00064962
Iteration 6/25 | Loss: 0.00064962
Iteration 7/25 | Loss: 0.00064962
Iteration 8/25 | Loss: 0.00064961
Iteration 9/25 | Loss: 0.00064961
Iteration 10/25 | Loss: 0.00064961
Iteration 11/25 | Loss: 0.00064961
Iteration 12/25 | Loss: 0.00064961
Iteration 13/25 | Loss: 0.00064961
Iteration 14/25 | Loss: 0.00064961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006496146670542657, 0.0006496146670542657, 0.0006496146670542657, 0.0006496146670542657, 0.0006496146670542657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006496146670542657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064961
Iteration 2/1000 | Loss: 0.00009510
Iteration 3/1000 | Loss: 0.00007235
Iteration 4/1000 | Loss: 0.00006067
Iteration 5/1000 | Loss: 0.00005626
Iteration 6/1000 | Loss: 0.00005393
Iteration 7/1000 | Loss: 0.00005296
Iteration 8/1000 | Loss: 0.00005168
Iteration 9/1000 | Loss: 0.00005066
Iteration 10/1000 | Loss: 0.00004994
Iteration 11/1000 | Loss: 0.00004931
Iteration 12/1000 | Loss: 0.00004875
Iteration 13/1000 | Loss: 0.00004824
Iteration 14/1000 | Loss: 0.00004783
Iteration 15/1000 | Loss: 0.00004760
Iteration 16/1000 | Loss: 0.00004737
Iteration 17/1000 | Loss: 0.00004716
Iteration 18/1000 | Loss: 0.00004692
Iteration 19/1000 | Loss: 0.00004672
Iteration 20/1000 | Loss: 0.00004659
Iteration 21/1000 | Loss: 0.00004658
Iteration 22/1000 | Loss: 0.00004644
Iteration 23/1000 | Loss: 0.00004640
Iteration 24/1000 | Loss: 0.00004639
Iteration 25/1000 | Loss: 0.00004636
Iteration 26/1000 | Loss: 0.00004635
Iteration 27/1000 | Loss: 0.00004633
Iteration 28/1000 | Loss: 0.00004629
Iteration 29/1000 | Loss: 0.00004623
Iteration 30/1000 | Loss: 0.00004623
Iteration 31/1000 | Loss: 0.00004623
Iteration 32/1000 | Loss: 0.00004618
Iteration 33/1000 | Loss: 0.00004618
Iteration 34/1000 | Loss: 0.00004616
Iteration 35/1000 | Loss: 0.00004616
Iteration 36/1000 | Loss: 0.00004615
Iteration 37/1000 | Loss: 0.00004614
Iteration 38/1000 | Loss: 0.00004614
Iteration 39/1000 | Loss: 0.00004611
Iteration 40/1000 | Loss: 0.00004605
Iteration 41/1000 | Loss: 0.00004605
Iteration 42/1000 | Loss: 0.00004600
Iteration 43/1000 | Loss: 0.00004600
Iteration 44/1000 | Loss: 0.00004600
Iteration 45/1000 | Loss: 0.00004598
Iteration 46/1000 | Loss: 0.00004590
Iteration 47/1000 | Loss: 0.00004588
Iteration 48/1000 | Loss: 0.00004587
Iteration 49/1000 | Loss: 0.00004587
Iteration 50/1000 | Loss: 0.00004587
Iteration 51/1000 | Loss: 0.00004587
Iteration 52/1000 | Loss: 0.00004587
Iteration 53/1000 | Loss: 0.00004585
Iteration 54/1000 | Loss: 0.00004585
Iteration 55/1000 | Loss: 0.00004584
Iteration 56/1000 | Loss: 0.00004584
Iteration 57/1000 | Loss: 0.00004584
Iteration 58/1000 | Loss: 0.00004584
Iteration 59/1000 | Loss: 0.00004584
Iteration 60/1000 | Loss: 0.00004583
Iteration 61/1000 | Loss: 0.00004582
Iteration 62/1000 | Loss: 0.00004581
Iteration 63/1000 | Loss: 0.00004581
Iteration 64/1000 | Loss: 0.00004580
Iteration 65/1000 | Loss: 0.00004580
Iteration 66/1000 | Loss: 0.00004578
Iteration 67/1000 | Loss: 0.00004577
Iteration 68/1000 | Loss: 0.00004576
Iteration 69/1000 | Loss: 0.00004576
Iteration 70/1000 | Loss: 0.00004576
Iteration 71/1000 | Loss: 0.00004575
Iteration 72/1000 | Loss: 0.00004575
Iteration 73/1000 | Loss: 0.00004575
Iteration 74/1000 | Loss: 0.00004575
Iteration 75/1000 | Loss: 0.00004575
Iteration 76/1000 | Loss: 0.00004575
Iteration 77/1000 | Loss: 0.00004575
Iteration 78/1000 | Loss: 0.00004575
Iteration 79/1000 | Loss: 0.00004575
Iteration 80/1000 | Loss: 0.00004575
Iteration 81/1000 | Loss: 0.00004574
Iteration 82/1000 | Loss: 0.00004574
Iteration 83/1000 | Loss: 0.00004574
Iteration 84/1000 | Loss: 0.00004574
Iteration 85/1000 | Loss: 0.00004574
Iteration 86/1000 | Loss: 0.00004574
Iteration 87/1000 | Loss: 0.00004574
Iteration 88/1000 | Loss: 0.00004573
Iteration 89/1000 | Loss: 0.00004573
Iteration 90/1000 | Loss: 0.00004573
Iteration 91/1000 | Loss: 0.00004573
Iteration 92/1000 | Loss: 0.00004573
Iteration 93/1000 | Loss: 0.00004573
Iteration 94/1000 | Loss: 0.00004573
Iteration 95/1000 | Loss: 0.00004573
Iteration 96/1000 | Loss: 0.00004573
Iteration 97/1000 | Loss: 0.00004573
Iteration 98/1000 | Loss: 0.00004572
Iteration 99/1000 | Loss: 0.00004572
Iteration 100/1000 | Loss: 0.00004572
Iteration 101/1000 | Loss: 0.00004572
Iteration 102/1000 | Loss: 0.00004572
Iteration 103/1000 | Loss: 0.00004572
Iteration 104/1000 | Loss: 0.00004572
Iteration 105/1000 | Loss: 0.00004572
Iteration 106/1000 | Loss: 0.00004572
Iteration 107/1000 | Loss: 0.00004572
Iteration 108/1000 | Loss: 0.00004572
Iteration 109/1000 | Loss: 0.00004572
Iteration 110/1000 | Loss: 0.00004572
Iteration 111/1000 | Loss: 0.00004572
Iteration 112/1000 | Loss: 0.00004572
Iteration 113/1000 | Loss: 0.00004572
Iteration 114/1000 | Loss: 0.00004572
Iteration 115/1000 | Loss: 0.00004571
Iteration 116/1000 | Loss: 0.00004571
Iteration 117/1000 | Loss: 0.00004570
Iteration 118/1000 | Loss: 0.00004570
Iteration 119/1000 | Loss: 0.00004570
Iteration 120/1000 | Loss: 0.00004570
Iteration 121/1000 | Loss: 0.00004570
Iteration 122/1000 | Loss: 0.00004570
Iteration 123/1000 | Loss: 0.00004569
Iteration 124/1000 | Loss: 0.00004569
Iteration 125/1000 | Loss: 0.00004569
Iteration 126/1000 | Loss: 0.00004569
Iteration 127/1000 | Loss: 0.00004569
Iteration 128/1000 | Loss: 0.00004569
Iteration 129/1000 | Loss: 0.00004569
Iteration 130/1000 | Loss: 0.00004569
Iteration 131/1000 | Loss: 0.00004569
Iteration 132/1000 | Loss: 0.00004568
Iteration 133/1000 | Loss: 0.00004568
Iteration 134/1000 | Loss: 0.00004568
Iteration 135/1000 | Loss: 0.00004568
Iteration 136/1000 | Loss: 0.00004568
Iteration 137/1000 | Loss: 0.00004568
Iteration 138/1000 | Loss: 0.00004568
Iteration 139/1000 | Loss: 0.00004568
Iteration 140/1000 | Loss: 0.00004567
Iteration 141/1000 | Loss: 0.00004567
Iteration 142/1000 | Loss: 0.00004567
Iteration 143/1000 | Loss: 0.00004567
Iteration 144/1000 | Loss: 0.00004566
Iteration 145/1000 | Loss: 0.00004566
Iteration 146/1000 | Loss: 0.00004566
Iteration 147/1000 | Loss: 0.00004566
Iteration 148/1000 | Loss: 0.00004566
Iteration 149/1000 | Loss: 0.00004566
Iteration 150/1000 | Loss: 0.00004566
Iteration 151/1000 | Loss: 0.00004565
Iteration 152/1000 | Loss: 0.00004565
Iteration 153/1000 | Loss: 0.00004565
Iteration 154/1000 | Loss: 0.00004565
Iteration 155/1000 | Loss: 0.00004565
Iteration 156/1000 | Loss: 0.00004565
Iteration 157/1000 | Loss: 0.00004565
Iteration 158/1000 | Loss: 0.00004564
Iteration 159/1000 | Loss: 0.00004564
Iteration 160/1000 | Loss: 0.00004564
Iteration 161/1000 | Loss: 0.00004564
Iteration 162/1000 | Loss: 0.00004564
Iteration 163/1000 | Loss: 0.00004564
Iteration 164/1000 | Loss: 0.00004564
Iteration 165/1000 | Loss: 0.00004564
Iteration 166/1000 | Loss: 0.00004564
Iteration 167/1000 | Loss: 0.00004564
Iteration 168/1000 | Loss: 0.00004563
Iteration 169/1000 | Loss: 0.00004563
Iteration 170/1000 | Loss: 0.00004563
Iteration 171/1000 | Loss: 0.00004563
Iteration 172/1000 | Loss: 0.00004563
Iteration 173/1000 | Loss: 0.00004563
Iteration 174/1000 | Loss: 0.00004563
Iteration 175/1000 | Loss: 0.00004563
Iteration 176/1000 | Loss: 0.00004563
Iteration 177/1000 | Loss: 0.00004563
Iteration 178/1000 | Loss: 0.00004562
Iteration 179/1000 | Loss: 0.00004562
Iteration 180/1000 | Loss: 0.00004562
Iteration 181/1000 | Loss: 0.00004562
Iteration 182/1000 | Loss: 0.00004562
Iteration 183/1000 | Loss: 0.00004562
Iteration 184/1000 | Loss: 0.00004562
Iteration 185/1000 | Loss: 0.00004562
Iteration 186/1000 | Loss: 0.00004562
Iteration 187/1000 | Loss: 0.00004562
Iteration 188/1000 | Loss: 0.00004562
Iteration 189/1000 | Loss: 0.00004562
Iteration 190/1000 | Loss: 0.00004562
Iteration 191/1000 | Loss: 0.00004562
Iteration 192/1000 | Loss: 0.00004562
Iteration 193/1000 | Loss: 0.00004562
Iteration 194/1000 | Loss: 0.00004562
Iteration 195/1000 | Loss: 0.00004562
Iteration 196/1000 | Loss: 0.00004562
Iteration 197/1000 | Loss: 0.00004562
Iteration 198/1000 | Loss: 0.00004562
Iteration 199/1000 | Loss: 0.00004562
Iteration 200/1000 | Loss: 0.00004562
Iteration 201/1000 | Loss: 0.00004562
Iteration 202/1000 | Loss: 0.00004562
Iteration 203/1000 | Loss: 0.00004562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [4.5616470742970705e-05, 4.5616470742970705e-05, 4.5616470742970705e-05, 4.5616470742970705e-05, 4.5616470742970705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5616470742970705e-05

Optimization complete. Final v2v error: 5.585570812225342 mm

Highest mean error: 6.713139533996582 mm for frame 164

Lowest mean error: 4.412351608276367 mm for frame 8

Saving results

Total time: 60.69822573661804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494783
Iteration 2/25 | Loss: 0.00121608
Iteration 3/25 | Loss: 0.00109891
Iteration 4/25 | Loss: 0.00106453
Iteration 5/25 | Loss: 0.00104982
Iteration 6/25 | Loss: 0.00104624
Iteration 7/25 | Loss: 0.00104556
Iteration 8/25 | Loss: 0.00104556
Iteration 9/25 | Loss: 0.00104556
Iteration 10/25 | Loss: 0.00104556
Iteration 11/25 | Loss: 0.00104556
Iteration 12/25 | Loss: 0.00104556
Iteration 13/25 | Loss: 0.00104556
Iteration 14/25 | Loss: 0.00104556
Iteration 15/25 | Loss: 0.00104556
Iteration 16/25 | Loss: 0.00104556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010455574374645948, 0.0010455574374645948, 0.0010455574374645948, 0.0010455574374645948, 0.0010455574374645948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010455574374645948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50476444
Iteration 2/25 | Loss: 0.00091135
Iteration 3/25 | Loss: 0.00091133
Iteration 4/25 | Loss: 0.00091133
Iteration 5/25 | Loss: 0.00091133
Iteration 6/25 | Loss: 0.00091133
Iteration 7/25 | Loss: 0.00091133
Iteration 8/25 | Loss: 0.00091133
Iteration 9/25 | Loss: 0.00091133
Iteration 10/25 | Loss: 0.00091133
Iteration 11/25 | Loss: 0.00091133
Iteration 12/25 | Loss: 0.00091133
Iteration 13/25 | Loss: 0.00091133
Iteration 14/25 | Loss: 0.00091133
Iteration 15/25 | Loss: 0.00091133
Iteration 16/25 | Loss: 0.00091133
Iteration 17/25 | Loss: 0.00091133
Iteration 18/25 | Loss: 0.00091133
Iteration 19/25 | Loss: 0.00091133
Iteration 20/25 | Loss: 0.00091133
Iteration 21/25 | Loss: 0.00091133
Iteration 22/25 | Loss: 0.00091133
Iteration 23/25 | Loss: 0.00091133
Iteration 24/25 | Loss: 0.00091133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009113285923376679, 0.0009113285923376679, 0.0009113285923376679, 0.0009113285923376679, 0.0009113285923376679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009113285923376679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091133
Iteration 2/1000 | Loss: 0.00003475
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00002305
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00002022
Iteration 9/1000 | Loss: 0.00002003
Iteration 10/1000 | Loss: 0.00001998
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001988
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001986
Iteration 21/1000 | Loss: 0.00001986
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001986
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001986
Iteration 27/1000 | Loss: 0.00001985
Iteration 28/1000 | Loss: 0.00001985
Iteration 29/1000 | Loss: 0.00001984
Iteration 30/1000 | Loss: 0.00001983
Iteration 31/1000 | Loss: 0.00001982
Iteration 32/1000 | Loss: 0.00001982
Iteration 33/1000 | Loss: 0.00001982
Iteration 34/1000 | Loss: 0.00001982
Iteration 35/1000 | Loss: 0.00001982
Iteration 36/1000 | Loss: 0.00001982
Iteration 37/1000 | Loss: 0.00001982
Iteration 38/1000 | Loss: 0.00001982
Iteration 39/1000 | Loss: 0.00001982
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001981
Iteration 43/1000 | Loss: 0.00001981
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001979
Iteration 48/1000 | Loss: 0.00001979
Iteration 49/1000 | Loss: 0.00001979
Iteration 50/1000 | Loss: 0.00001978
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001977
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001977
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001975
Iteration 60/1000 | Loss: 0.00001975
Iteration 61/1000 | Loss: 0.00001975
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001973
Iteration 67/1000 | Loss: 0.00001973
Iteration 68/1000 | Loss: 0.00001973
Iteration 69/1000 | Loss: 0.00001973
Iteration 70/1000 | Loss: 0.00001973
Iteration 71/1000 | Loss: 0.00001973
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001972
Iteration 74/1000 | Loss: 0.00001972
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001971
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001970
Iteration 82/1000 | Loss: 0.00001970
Iteration 83/1000 | Loss: 0.00001970
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001969
Iteration 91/1000 | Loss: 0.00001969
Iteration 92/1000 | Loss: 0.00001969
Iteration 93/1000 | Loss: 0.00001969
Iteration 94/1000 | Loss: 0.00001969
Iteration 95/1000 | Loss: 0.00001968
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001968
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001967
Iteration 104/1000 | Loss: 0.00001967
Iteration 105/1000 | Loss: 0.00001967
Iteration 106/1000 | Loss: 0.00001967
Iteration 107/1000 | Loss: 0.00001967
Iteration 108/1000 | Loss: 0.00001967
Iteration 109/1000 | Loss: 0.00001967
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001966
Iteration 113/1000 | Loss: 0.00001966
Iteration 114/1000 | Loss: 0.00001966
Iteration 115/1000 | Loss: 0.00001966
Iteration 116/1000 | Loss: 0.00001966
Iteration 117/1000 | Loss: 0.00001966
Iteration 118/1000 | Loss: 0.00001965
Iteration 119/1000 | Loss: 0.00001965
Iteration 120/1000 | Loss: 0.00001965
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00001965
Iteration 123/1000 | Loss: 0.00001965
Iteration 124/1000 | Loss: 0.00001965
Iteration 125/1000 | Loss: 0.00001965
Iteration 126/1000 | Loss: 0.00001965
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001964
Iteration 133/1000 | Loss: 0.00001964
Iteration 134/1000 | Loss: 0.00001964
Iteration 135/1000 | Loss: 0.00001964
Iteration 136/1000 | Loss: 0.00001964
Iteration 137/1000 | Loss: 0.00001963
Iteration 138/1000 | Loss: 0.00001963
Iteration 139/1000 | Loss: 0.00001963
Iteration 140/1000 | Loss: 0.00001963
Iteration 141/1000 | Loss: 0.00001963
Iteration 142/1000 | Loss: 0.00001963
Iteration 143/1000 | Loss: 0.00001963
Iteration 144/1000 | Loss: 0.00001963
Iteration 145/1000 | Loss: 0.00001963
Iteration 146/1000 | Loss: 0.00001963
Iteration 147/1000 | Loss: 0.00001963
Iteration 148/1000 | Loss: 0.00001963
Iteration 149/1000 | Loss: 0.00001962
Iteration 150/1000 | Loss: 0.00001962
Iteration 151/1000 | Loss: 0.00001962
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001962
Iteration 154/1000 | Loss: 0.00001962
Iteration 155/1000 | Loss: 0.00001962
Iteration 156/1000 | Loss: 0.00001962
Iteration 157/1000 | Loss: 0.00001962
Iteration 158/1000 | Loss: 0.00001962
Iteration 159/1000 | Loss: 0.00001962
Iteration 160/1000 | Loss: 0.00001962
Iteration 161/1000 | Loss: 0.00001962
Iteration 162/1000 | Loss: 0.00001962
Iteration 163/1000 | Loss: 0.00001962
Iteration 164/1000 | Loss: 0.00001962
Iteration 165/1000 | Loss: 0.00001962
Iteration 166/1000 | Loss: 0.00001962
Iteration 167/1000 | Loss: 0.00001962
Iteration 168/1000 | Loss: 0.00001961
Iteration 169/1000 | Loss: 0.00001961
Iteration 170/1000 | Loss: 0.00001961
Iteration 171/1000 | Loss: 0.00001961
Iteration 172/1000 | Loss: 0.00001961
Iteration 173/1000 | Loss: 0.00001961
Iteration 174/1000 | Loss: 0.00001961
Iteration 175/1000 | Loss: 0.00001961
Iteration 176/1000 | Loss: 0.00001961
Iteration 177/1000 | Loss: 0.00001961
Iteration 178/1000 | Loss: 0.00001961
Iteration 179/1000 | Loss: 0.00001961
Iteration 180/1000 | Loss: 0.00001961
Iteration 181/1000 | Loss: 0.00001961
Iteration 182/1000 | Loss: 0.00001961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.961120688065421e-05, 1.961120688065421e-05, 1.961120688065421e-05, 1.961120688065421e-05, 1.961120688065421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.961120688065421e-05

Optimization complete. Final v2v error: 3.8346943855285645 mm

Highest mean error: 4.208970069885254 mm for frame 194

Lowest mean error: 3.5687143802642822 mm for frame 66

Saving results

Total time: 40.87549638748169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470942
Iteration 2/25 | Loss: 0.00115189
Iteration 3/25 | Loss: 0.00106381
Iteration 4/25 | Loss: 0.00103489
Iteration 5/25 | Loss: 0.00102411
Iteration 6/25 | Loss: 0.00102204
Iteration 7/25 | Loss: 0.00102167
Iteration 8/25 | Loss: 0.00102167
Iteration 9/25 | Loss: 0.00102167
Iteration 10/25 | Loss: 0.00102167
Iteration 11/25 | Loss: 0.00102167
Iteration 12/25 | Loss: 0.00102167
Iteration 13/25 | Loss: 0.00102167
Iteration 14/25 | Loss: 0.00102167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010216690134257078, 0.0010216690134257078, 0.0010216690134257078, 0.0010216690134257078, 0.0010216690134257078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010216690134257078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54964125
Iteration 2/25 | Loss: 0.00075783
Iteration 3/25 | Loss: 0.00075782
Iteration 4/25 | Loss: 0.00075782
Iteration 5/25 | Loss: 0.00075782
Iteration 6/25 | Loss: 0.00075782
Iteration 7/25 | Loss: 0.00075782
Iteration 8/25 | Loss: 0.00075782
Iteration 9/25 | Loss: 0.00075782
Iteration 10/25 | Loss: 0.00075782
Iteration 11/25 | Loss: 0.00075782
Iteration 12/25 | Loss: 0.00075782
Iteration 13/25 | Loss: 0.00075782
Iteration 14/25 | Loss: 0.00075782
Iteration 15/25 | Loss: 0.00075782
Iteration 16/25 | Loss: 0.00075782
Iteration 17/25 | Loss: 0.00075782
Iteration 18/25 | Loss: 0.00075782
Iteration 19/25 | Loss: 0.00075782
Iteration 20/25 | Loss: 0.00075782
Iteration 21/25 | Loss: 0.00075782
Iteration 22/25 | Loss: 0.00075782
Iteration 23/25 | Loss: 0.00075782
Iteration 24/25 | Loss: 0.00075782
Iteration 25/25 | Loss: 0.00075782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075782
Iteration 2/1000 | Loss: 0.00005368
Iteration 3/1000 | Loss: 0.00002824
Iteration 4/1000 | Loss: 0.00002555
Iteration 5/1000 | Loss: 0.00002386
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002239
Iteration 8/1000 | Loss: 0.00002200
Iteration 9/1000 | Loss: 0.00002183
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002179
Iteration 12/1000 | Loss: 0.00002176
Iteration 13/1000 | Loss: 0.00002176
Iteration 14/1000 | Loss: 0.00002175
Iteration 15/1000 | Loss: 0.00002174
Iteration 16/1000 | Loss: 0.00002174
Iteration 17/1000 | Loss: 0.00002173
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002171
Iteration 20/1000 | Loss: 0.00002171
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002171
Iteration 23/1000 | Loss: 0.00002170
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002169
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002168
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00002167
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002165
Iteration 41/1000 | Loss: 0.00002165
Iteration 42/1000 | Loss: 0.00002164
Iteration 43/1000 | Loss: 0.00002164
Iteration 44/1000 | Loss: 0.00002164
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002164
Iteration 47/1000 | Loss: 0.00002164
Iteration 48/1000 | Loss: 0.00002164
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002163
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002162
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002162
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002161
Iteration 69/1000 | Loss: 0.00002161
Iteration 70/1000 | Loss: 0.00002161
Iteration 71/1000 | Loss: 0.00002161
Iteration 72/1000 | Loss: 0.00002161
Iteration 73/1000 | Loss: 0.00002161
Iteration 74/1000 | Loss: 0.00002160
Iteration 75/1000 | Loss: 0.00002160
Iteration 76/1000 | Loss: 0.00002160
Iteration 77/1000 | Loss: 0.00002160
Iteration 78/1000 | Loss: 0.00002160
Iteration 79/1000 | Loss: 0.00002160
Iteration 80/1000 | Loss: 0.00002160
Iteration 81/1000 | Loss: 0.00002160
Iteration 82/1000 | Loss: 0.00002160
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00002160
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.1602441847790033e-05, 2.1602441847790033e-05, 2.1602441847790033e-05, 2.1602441847790033e-05, 2.1602441847790033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1602441847790033e-05

Optimization complete. Final v2v error: 3.939786434173584 mm

Highest mean error: 4.4672064781188965 mm for frame 77

Lowest mean error: 3.5949623584747314 mm for frame 100

Saving results

Total time: 28.44452691078186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882382
Iteration 2/25 | Loss: 0.00113567
Iteration 3/25 | Loss: 0.00099373
Iteration 4/25 | Loss: 0.00097065
Iteration 5/25 | Loss: 0.00096328
Iteration 6/25 | Loss: 0.00096123
Iteration 7/25 | Loss: 0.00096079
Iteration 8/25 | Loss: 0.00096079
Iteration 9/25 | Loss: 0.00096079
Iteration 10/25 | Loss: 0.00096079
Iteration 11/25 | Loss: 0.00096079
Iteration 12/25 | Loss: 0.00096079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009607923566363752, 0.0009607923566363752, 0.0009607923566363752, 0.0009607923566363752, 0.0009607923566363752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009607923566363752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50745940
Iteration 2/25 | Loss: 0.00074752
Iteration 3/25 | Loss: 0.00074752
Iteration 4/25 | Loss: 0.00074752
Iteration 5/25 | Loss: 0.00074752
Iteration 6/25 | Loss: 0.00074752
Iteration 7/25 | Loss: 0.00074752
Iteration 8/25 | Loss: 0.00074752
Iteration 9/25 | Loss: 0.00074752
Iteration 10/25 | Loss: 0.00074752
Iteration 11/25 | Loss: 0.00074752
Iteration 12/25 | Loss: 0.00074752
Iteration 13/25 | Loss: 0.00074752
Iteration 14/25 | Loss: 0.00074752
Iteration 15/25 | Loss: 0.00074752
Iteration 16/25 | Loss: 0.00074752
Iteration 17/25 | Loss: 0.00074752
Iteration 18/25 | Loss: 0.00074752
Iteration 19/25 | Loss: 0.00074752
Iteration 20/25 | Loss: 0.00074752
Iteration 21/25 | Loss: 0.00074752
Iteration 22/25 | Loss: 0.00074752
Iteration 23/25 | Loss: 0.00074752
Iteration 24/25 | Loss: 0.00074752
Iteration 25/25 | Loss: 0.00074752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007475164602510631, 0.0007475164602510631, 0.0007475164602510631, 0.0007475164602510631, 0.0007475164602510631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007475164602510631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074752
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00001655
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001392
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001374
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001372
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001369
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001366
Iteration 46/1000 | Loss: 0.00001366
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001365
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001360
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001360
Iteration 83/1000 | Loss: 0.00001360
Iteration 84/1000 | Loss: 0.00001360
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001359
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001358
Iteration 94/1000 | Loss: 0.00001358
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001358
Iteration 102/1000 | Loss: 0.00001358
Iteration 103/1000 | Loss: 0.00001358
Iteration 104/1000 | Loss: 0.00001358
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001357
Iteration 108/1000 | Loss: 0.00001357
Iteration 109/1000 | Loss: 0.00001357
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001357
Iteration 112/1000 | Loss: 0.00001357
Iteration 113/1000 | Loss: 0.00001357
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001356
Iteration 118/1000 | Loss: 0.00001356
Iteration 119/1000 | Loss: 0.00001356
Iteration 120/1000 | Loss: 0.00001356
Iteration 121/1000 | Loss: 0.00001356
Iteration 122/1000 | Loss: 0.00001356
Iteration 123/1000 | Loss: 0.00001356
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001356
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001356
Iteration 132/1000 | Loss: 0.00001356
Iteration 133/1000 | Loss: 0.00001356
Iteration 134/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.3557553756982088e-05, 1.3557553756982088e-05, 1.3557553756982088e-05, 1.3557553756982088e-05, 1.3557553756982088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3557553756982088e-05

Optimization complete. Final v2v error: 3.1622300148010254 mm

Highest mean error: 3.4129061698913574 mm for frame 69

Lowest mean error: 2.7229928970336914 mm for frame 176

Saving results

Total time: 30.873659372329712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810236
Iteration 2/25 | Loss: 0.00128672
Iteration 3/25 | Loss: 0.00106502
Iteration 4/25 | Loss: 0.00104187
Iteration 5/25 | Loss: 0.00100477
Iteration 6/25 | Loss: 0.00100178
Iteration 7/25 | Loss: 0.00100000
Iteration 8/25 | Loss: 0.00099939
Iteration 9/25 | Loss: 0.00099724
Iteration 10/25 | Loss: 0.00099658
Iteration 11/25 | Loss: 0.00099586
Iteration 12/25 | Loss: 0.00099487
Iteration 13/25 | Loss: 0.00099420
Iteration 14/25 | Loss: 0.00099397
Iteration 15/25 | Loss: 0.00099382
Iteration 16/25 | Loss: 0.00099377
Iteration 17/25 | Loss: 0.00099377
Iteration 18/25 | Loss: 0.00099377
Iteration 19/25 | Loss: 0.00099377
Iteration 20/25 | Loss: 0.00099376
Iteration 21/25 | Loss: 0.00099376
Iteration 22/25 | Loss: 0.00099376
Iteration 23/25 | Loss: 0.00099376
Iteration 24/25 | Loss: 0.00099376
Iteration 25/25 | Loss: 0.00099376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93413711
Iteration 2/25 | Loss: 0.00073515
Iteration 3/25 | Loss: 0.00073515
Iteration 4/25 | Loss: 0.00073515
Iteration 5/25 | Loss: 0.00073515
Iteration 6/25 | Loss: 0.00073515
Iteration 7/25 | Loss: 0.00073515
Iteration 8/25 | Loss: 0.00073515
Iteration 9/25 | Loss: 0.00073515
Iteration 10/25 | Loss: 0.00073515
Iteration 11/25 | Loss: 0.00073515
Iteration 12/25 | Loss: 0.00073515
Iteration 13/25 | Loss: 0.00073515
Iteration 14/25 | Loss: 0.00073515
Iteration 15/25 | Loss: 0.00073515
Iteration 16/25 | Loss: 0.00073515
Iteration 17/25 | Loss: 0.00073515
Iteration 18/25 | Loss: 0.00073515
Iteration 19/25 | Loss: 0.00073515
Iteration 20/25 | Loss: 0.00073515
Iteration 21/25 | Loss: 0.00073515
Iteration 22/25 | Loss: 0.00073515
Iteration 23/25 | Loss: 0.00073515
Iteration 24/25 | Loss: 0.00073515
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007351480307988822, 0.0007351480307988822, 0.0007351480307988822, 0.0007351480307988822, 0.0007351480307988822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007351480307988822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073515
Iteration 2/1000 | Loss: 0.00003397
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002086
Iteration 7/1000 | Loss: 0.00002023
Iteration 8/1000 | Loss: 0.00001991
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001980
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001977
Iteration 15/1000 | Loss: 0.00001977
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001976
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001975
Iteration 20/1000 | Loss: 0.00001975
Iteration 21/1000 | Loss: 0.00001973
Iteration 22/1000 | Loss: 0.00001973
Iteration 23/1000 | Loss: 0.00001973
Iteration 24/1000 | Loss: 0.00001972
Iteration 25/1000 | Loss: 0.00001971
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001963
Iteration 28/1000 | Loss: 0.00001963
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001962
Iteration 31/1000 | Loss: 0.00001962
Iteration 32/1000 | Loss: 0.00001962
Iteration 33/1000 | Loss: 0.00001962
Iteration 34/1000 | Loss: 0.00001962
Iteration 35/1000 | Loss: 0.00001962
Iteration 36/1000 | Loss: 0.00001962
Iteration 37/1000 | Loss: 0.00001962
Iteration 38/1000 | Loss: 0.00001962
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001961
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001960
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001959
Iteration 46/1000 | Loss: 0.00001959
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001957
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001957
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001956
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001955
Iteration 77/1000 | Loss: 0.00001955
Iteration 78/1000 | Loss: 0.00001955
Iteration 79/1000 | Loss: 0.00001955
Iteration 80/1000 | Loss: 0.00001955
Iteration 81/1000 | Loss: 0.00001955
Iteration 82/1000 | Loss: 0.00001955
Iteration 83/1000 | Loss: 0.00001955
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001954
Iteration 87/1000 | Loss: 0.00001954
Iteration 88/1000 | Loss: 0.00001953
Iteration 89/1000 | Loss: 0.00001953
Iteration 90/1000 | Loss: 0.00001953
Iteration 91/1000 | Loss: 0.00001953
Iteration 92/1000 | Loss: 0.00001953
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001952
Iteration 95/1000 | Loss: 0.00001952
Iteration 96/1000 | Loss: 0.00001952
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001952
Iteration 99/1000 | Loss: 0.00001952
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001951
Iteration 105/1000 | Loss: 0.00001951
Iteration 106/1000 | Loss: 0.00001951
Iteration 107/1000 | Loss: 0.00001951
Iteration 108/1000 | Loss: 0.00001951
Iteration 109/1000 | Loss: 0.00001951
Iteration 110/1000 | Loss: 0.00001951
Iteration 111/1000 | Loss: 0.00001951
Iteration 112/1000 | Loss: 0.00001951
Iteration 113/1000 | Loss: 0.00001951
Iteration 114/1000 | Loss: 0.00001950
Iteration 115/1000 | Loss: 0.00001950
Iteration 116/1000 | Loss: 0.00001950
Iteration 117/1000 | Loss: 0.00001950
Iteration 118/1000 | Loss: 0.00001950
Iteration 119/1000 | Loss: 0.00001950
Iteration 120/1000 | Loss: 0.00001950
Iteration 121/1000 | Loss: 0.00001950
Iteration 122/1000 | Loss: 0.00001950
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001949
Iteration 126/1000 | Loss: 0.00001949
Iteration 127/1000 | Loss: 0.00001949
Iteration 128/1000 | Loss: 0.00001949
Iteration 129/1000 | Loss: 0.00001949
Iteration 130/1000 | Loss: 0.00001949
Iteration 131/1000 | Loss: 0.00001949
Iteration 132/1000 | Loss: 0.00001949
Iteration 133/1000 | Loss: 0.00001949
Iteration 134/1000 | Loss: 0.00001949
Iteration 135/1000 | Loss: 0.00001948
Iteration 136/1000 | Loss: 0.00001948
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001948
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001948
Iteration 141/1000 | Loss: 0.00001948
Iteration 142/1000 | Loss: 0.00001948
Iteration 143/1000 | Loss: 0.00001948
Iteration 144/1000 | Loss: 0.00001948
Iteration 145/1000 | Loss: 0.00001948
Iteration 146/1000 | Loss: 0.00001948
Iteration 147/1000 | Loss: 0.00001948
Iteration 148/1000 | Loss: 0.00001948
Iteration 149/1000 | Loss: 0.00001948
Iteration 150/1000 | Loss: 0.00001948
Iteration 151/1000 | Loss: 0.00001948
Iteration 152/1000 | Loss: 0.00001948
Iteration 153/1000 | Loss: 0.00001948
Iteration 154/1000 | Loss: 0.00001947
Iteration 155/1000 | Loss: 0.00001947
Iteration 156/1000 | Loss: 0.00001947
Iteration 157/1000 | Loss: 0.00001947
Iteration 158/1000 | Loss: 0.00001947
Iteration 159/1000 | Loss: 0.00001947
Iteration 160/1000 | Loss: 0.00001947
Iteration 161/1000 | Loss: 0.00001947
Iteration 162/1000 | Loss: 0.00001947
Iteration 163/1000 | Loss: 0.00001947
Iteration 164/1000 | Loss: 0.00001947
Iteration 165/1000 | Loss: 0.00001947
Iteration 166/1000 | Loss: 0.00001946
Iteration 167/1000 | Loss: 0.00001946
Iteration 168/1000 | Loss: 0.00001946
Iteration 169/1000 | Loss: 0.00001946
Iteration 170/1000 | Loss: 0.00001946
Iteration 171/1000 | Loss: 0.00001946
Iteration 172/1000 | Loss: 0.00001946
Iteration 173/1000 | Loss: 0.00001946
Iteration 174/1000 | Loss: 0.00001946
Iteration 175/1000 | Loss: 0.00001946
Iteration 176/1000 | Loss: 0.00001946
Iteration 177/1000 | Loss: 0.00001946
Iteration 178/1000 | Loss: 0.00001946
Iteration 179/1000 | Loss: 0.00001946
Iteration 180/1000 | Loss: 0.00001946
Iteration 181/1000 | Loss: 0.00001946
Iteration 182/1000 | Loss: 0.00001946
Iteration 183/1000 | Loss: 0.00001946
Iteration 184/1000 | Loss: 0.00001946
Iteration 185/1000 | Loss: 0.00001945
Iteration 186/1000 | Loss: 0.00001945
Iteration 187/1000 | Loss: 0.00001945
Iteration 188/1000 | Loss: 0.00001945
Iteration 189/1000 | Loss: 0.00001945
Iteration 190/1000 | Loss: 0.00001945
Iteration 191/1000 | Loss: 0.00001945
Iteration 192/1000 | Loss: 0.00001945
Iteration 193/1000 | Loss: 0.00001945
Iteration 194/1000 | Loss: 0.00001945
Iteration 195/1000 | Loss: 0.00001945
Iteration 196/1000 | Loss: 0.00001945
Iteration 197/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.9453862478258088e-05, 1.9453862478258088e-05, 1.9453862478258088e-05, 1.9453862478258088e-05, 1.9453862478258088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9453862478258088e-05

Optimization complete. Final v2v error: 3.772244453430176 mm

Highest mean error: 3.981098175048828 mm for frame 6

Lowest mean error: 3.523097515106201 mm for frame 71

Saving results

Total time: 53.148780822753906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_0629/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_0629/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460559
Iteration 2/25 | Loss: 0.00123042
Iteration 3/25 | Loss: 0.00112726
Iteration 4/25 | Loss: 0.00110901
Iteration 5/25 | Loss: 0.00110202
Iteration 6/25 | Loss: 0.00110117
Iteration 7/25 | Loss: 0.00110101
Iteration 8/25 | Loss: 0.00110101
Iteration 9/25 | Loss: 0.00110101
Iteration 10/25 | Loss: 0.00110101
Iteration 11/25 | Loss: 0.00110101
Iteration 12/25 | Loss: 0.00110101
Iteration 13/25 | Loss: 0.00110101
Iteration 14/25 | Loss: 0.00110101
Iteration 15/25 | Loss: 0.00110101
Iteration 16/25 | Loss: 0.00110101
Iteration 17/25 | Loss: 0.00110101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011010089656338096, 0.0011010089656338096, 0.0011010089656338096, 0.0011010089656338096, 0.0011010089656338096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011010089656338096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26669264
Iteration 2/25 | Loss: 0.00122120
Iteration 3/25 | Loss: 0.00122118
Iteration 4/25 | Loss: 0.00122118
Iteration 5/25 | Loss: 0.00122118
Iteration 6/25 | Loss: 0.00122118
Iteration 7/25 | Loss: 0.00122118
Iteration 8/25 | Loss: 0.00122118
Iteration 9/25 | Loss: 0.00122118
Iteration 10/25 | Loss: 0.00122118
Iteration 11/25 | Loss: 0.00122118
Iteration 12/25 | Loss: 0.00122118
Iteration 13/25 | Loss: 0.00122118
Iteration 14/25 | Loss: 0.00122118
Iteration 15/25 | Loss: 0.00122118
Iteration 16/25 | Loss: 0.00122118
Iteration 17/25 | Loss: 0.00122118
Iteration 18/25 | Loss: 0.00122118
Iteration 19/25 | Loss: 0.00122118
Iteration 20/25 | Loss: 0.00122118
Iteration 21/25 | Loss: 0.00122118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012211808934807777, 0.0012211808934807777, 0.0012211808934807777, 0.0012211808934807777, 0.0012211808934807777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012211808934807777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122118
Iteration 2/1000 | Loss: 0.00003781
Iteration 3/1000 | Loss: 0.00002485
Iteration 4/1000 | Loss: 0.00002146
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001907
Iteration 9/1000 | Loss: 0.00001876
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001844
Iteration 13/1000 | Loss: 0.00001838
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001830
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001828
Iteration 25/1000 | Loss: 0.00001828
Iteration 26/1000 | Loss: 0.00001828
Iteration 27/1000 | Loss: 0.00001827
Iteration 28/1000 | Loss: 0.00001827
Iteration 29/1000 | Loss: 0.00001827
Iteration 30/1000 | Loss: 0.00001827
Iteration 31/1000 | Loss: 0.00001827
Iteration 32/1000 | Loss: 0.00001827
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001827
Iteration 37/1000 | Loss: 0.00001827
Iteration 38/1000 | Loss: 0.00001827
Iteration 39/1000 | Loss: 0.00001827
Iteration 40/1000 | Loss: 0.00001827
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001827
Iteration 43/1000 | Loss: 0.00001827
Iteration 44/1000 | Loss: 0.00001827
Iteration 45/1000 | Loss: 0.00001827
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001827
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001827
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001827
Iteration 58/1000 | Loss: 0.00001827
Iteration 59/1000 | Loss: 0.00001827
Iteration 60/1000 | Loss: 0.00001827
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001827
Iteration 66/1000 | Loss: 0.00001827
Iteration 67/1000 | Loss: 0.00001827
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00001827
Iteration 79/1000 | Loss: 0.00001827
Iteration 80/1000 | Loss: 0.00001827
Iteration 81/1000 | Loss: 0.00001827
Iteration 82/1000 | Loss: 0.00001827
Iteration 83/1000 | Loss: 0.00001827
Iteration 84/1000 | Loss: 0.00001827
Iteration 85/1000 | Loss: 0.00001827
Iteration 86/1000 | Loss: 0.00001827
Iteration 87/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.8266913684783503e-05, 1.8266913684783503e-05, 1.8266913684783503e-05, 1.8266913684783503e-05, 1.8266913684783503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8266913684783503e-05

Optimization complete. Final v2v error: 3.6766726970672607 mm

Highest mean error: 3.836243152618408 mm for frame 94

Lowest mean error: 3.416618824005127 mm for frame 1

Saving results

Total time: 28.52850866317749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991400
Iteration 2/25 | Loss: 0.00140755
Iteration 3/25 | Loss: 0.00108998
Iteration 4/25 | Loss: 0.00104782
Iteration 5/25 | Loss: 0.00103549
Iteration 6/25 | Loss: 0.00103359
Iteration 7/25 | Loss: 0.00103359
Iteration 8/25 | Loss: 0.00103359
Iteration 9/25 | Loss: 0.00103359
Iteration 10/25 | Loss: 0.00103359
Iteration 11/25 | Loss: 0.00103359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010335918050259352, 0.0010335918050259352, 0.0010335918050259352, 0.0010335918050259352, 0.0010335918050259352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010335918050259352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45971847
Iteration 2/25 | Loss: 0.00150997
Iteration 3/25 | Loss: 0.00150985
Iteration 4/25 | Loss: 0.00150985
Iteration 5/25 | Loss: 0.00150985
Iteration 6/25 | Loss: 0.00150985
Iteration 7/25 | Loss: 0.00150985
Iteration 8/25 | Loss: 0.00150985
Iteration 9/25 | Loss: 0.00150985
Iteration 10/25 | Loss: 0.00150985
Iteration 11/25 | Loss: 0.00150985
Iteration 12/25 | Loss: 0.00150985
Iteration 13/25 | Loss: 0.00150985
Iteration 14/25 | Loss: 0.00150985
Iteration 15/25 | Loss: 0.00150985
Iteration 16/25 | Loss: 0.00150985
Iteration 17/25 | Loss: 0.00150985
Iteration 18/25 | Loss: 0.00150985
Iteration 19/25 | Loss: 0.00150985
Iteration 20/25 | Loss: 0.00150985
Iteration 21/25 | Loss: 0.00150985
Iteration 22/25 | Loss: 0.00150985
Iteration 23/25 | Loss: 0.00150985
Iteration 24/25 | Loss: 0.00150985
Iteration 25/25 | Loss: 0.00150985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150985
Iteration 2/1000 | Loss: 0.00004322
Iteration 3/1000 | Loss: 0.00003174
Iteration 4/1000 | Loss: 0.00002934
Iteration 5/1000 | Loss: 0.00002739
Iteration 6/1000 | Loss: 0.00002629
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002480
Iteration 9/1000 | Loss: 0.00002447
Iteration 10/1000 | Loss: 0.00002423
Iteration 11/1000 | Loss: 0.00002404
Iteration 12/1000 | Loss: 0.00002400
Iteration 13/1000 | Loss: 0.00002397
Iteration 14/1000 | Loss: 0.00002384
Iteration 15/1000 | Loss: 0.00002383
Iteration 16/1000 | Loss: 0.00002382
Iteration 17/1000 | Loss: 0.00002382
Iteration 18/1000 | Loss: 0.00002382
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00002382
Iteration 21/1000 | Loss: 0.00002382
Iteration 22/1000 | Loss: 0.00002382
Iteration 23/1000 | Loss: 0.00002382
Iteration 24/1000 | Loss: 0.00002381
Iteration 25/1000 | Loss: 0.00002381
Iteration 26/1000 | Loss: 0.00002381
Iteration 27/1000 | Loss: 0.00002381
Iteration 28/1000 | Loss: 0.00002381
Iteration 29/1000 | Loss: 0.00002380
Iteration 30/1000 | Loss: 0.00002380
Iteration 31/1000 | Loss: 0.00002380
Iteration 32/1000 | Loss: 0.00002380
Iteration 33/1000 | Loss: 0.00002380
Iteration 34/1000 | Loss: 0.00002380
Iteration 35/1000 | Loss: 0.00002380
Iteration 36/1000 | Loss: 0.00002380
Iteration 37/1000 | Loss: 0.00002380
Iteration 38/1000 | Loss: 0.00002380
Iteration 39/1000 | Loss: 0.00002380
Iteration 40/1000 | Loss: 0.00002379
Iteration 41/1000 | Loss: 0.00002379
Iteration 42/1000 | Loss: 0.00002379
Iteration 43/1000 | Loss: 0.00002379
Iteration 44/1000 | Loss: 0.00002378
Iteration 45/1000 | Loss: 0.00002378
Iteration 46/1000 | Loss: 0.00002378
Iteration 47/1000 | Loss: 0.00002378
Iteration 48/1000 | Loss: 0.00002378
Iteration 49/1000 | Loss: 0.00002378
Iteration 50/1000 | Loss: 0.00002377
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002377
Iteration 53/1000 | Loss: 0.00002377
Iteration 54/1000 | Loss: 0.00002377
Iteration 55/1000 | Loss: 0.00002377
Iteration 56/1000 | Loss: 0.00002377
Iteration 57/1000 | Loss: 0.00002377
Iteration 58/1000 | Loss: 0.00002377
Iteration 59/1000 | Loss: 0.00002377
Iteration 60/1000 | Loss: 0.00002377
Iteration 61/1000 | Loss: 0.00002377
Iteration 62/1000 | Loss: 0.00002377
Iteration 63/1000 | Loss: 0.00002376
Iteration 64/1000 | Loss: 0.00002376
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002376
Iteration 67/1000 | Loss: 0.00002376
Iteration 68/1000 | Loss: 0.00002376
Iteration 69/1000 | Loss: 0.00002376
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002376
Iteration 72/1000 | Loss: 0.00002376
Iteration 73/1000 | Loss: 0.00002376
Iteration 74/1000 | Loss: 0.00002376
Iteration 75/1000 | Loss: 0.00002376
Iteration 76/1000 | Loss: 0.00002376
Iteration 77/1000 | Loss: 0.00002375
Iteration 78/1000 | Loss: 0.00002375
Iteration 79/1000 | Loss: 0.00002375
Iteration 80/1000 | Loss: 0.00002375
Iteration 81/1000 | Loss: 0.00002375
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00002375
Iteration 84/1000 | Loss: 0.00002375
Iteration 85/1000 | Loss: 0.00002375
Iteration 86/1000 | Loss: 0.00002375
Iteration 87/1000 | Loss: 0.00002374
Iteration 88/1000 | Loss: 0.00002374
Iteration 89/1000 | Loss: 0.00002374
Iteration 90/1000 | Loss: 0.00002374
Iteration 91/1000 | Loss: 0.00002374
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002374
Iteration 95/1000 | Loss: 0.00002374
Iteration 96/1000 | Loss: 0.00002374
Iteration 97/1000 | Loss: 0.00002374
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002373
Iteration 100/1000 | Loss: 0.00002373
Iteration 101/1000 | Loss: 0.00002373
Iteration 102/1000 | Loss: 0.00002373
Iteration 103/1000 | Loss: 0.00002373
Iteration 104/1000 | Loss: 0.00002373
Iteration 105/1000 | Loss: 0.00002373
Iteration 106/1000 | Loss: 0.00002373
Iteration 107/1000 | Loss: 0.00002372
Iteration 108/1000 | Loss: 0.00002372
Iteration 109/1000 | Loss: 0.00002372
Iteration 110/1000 | Loss: 0.00002372
Iteration 111/1000 | Loss: 0.00002372
Iteration 112/1000 | Loss: 0.00002372
Iteration 113/1000 | Loss: 0.00002372
Iteration 114/1000 | Loss: 0.00002372
Iteration 115/1000 | Loss: 0.00002372
Iteration 116/1000 | Loss: 0.00002372
Iteration 117/1000 | Loss: 0.00002372
Iteration 118/1000 | Loss: 0.00002372
Iteration 119/1000 | Loss: 0.00002372
Iteration 120/1000 | Loss: 0.00002372
Iteration 121/1000 | Loss: 0.00002372
Iteration 122/1000 | Loss: 0.00002372
Iteration 123/1000 | Loss: 0.00002372
Iteration 124/1000 | Loss: 0.00002372
Iteration 125/1000 | Loss: 0.00002372
Iteration 126/1000 | Loss: 0.00002372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.3722106561763212e-05, 2.3722106561763212e-05, 2.3722106561763212e-05, 2.3722106561763212e-05, 2.3722106561763212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3722106561763212e-05

Optimization complete. Final v2v error: 4.214561939239502 mm

Highest mean error: 4.3376898765563965 mm for frame 27

Lowest mean error: 4.027324676513672 mm for frame 177

Saving results

Total time: 37.9304518699646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793570
Iteration 2/25 | Loss: 0.00107360
Iteration 3/25 | Loss: 0.00097632
Iteration 4/25 | Loss: 0.00096428
Iteration 5/25 | Loss: 0.00096197
Iteration 6/25 | Loss: 0.00096112
Iteration 7/25 | Loss: 0.00096107
Iteration 8/25 | Loss: 0.00096107
Iteration 9/25 | Loss: 0.00096107
Iteration 10/25 | Loss: 0.00096107
Iteration 11/25 | Loss: 0.00096107
Iteration 12/25 | Loss: 0.00096107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009610738488845527, 0.0009610738488845527, 0.0009610738488845527, 0.0009610738488845527, 0.0009610738488845527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009610738488845527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36360359
Iteration 2/25 | Loss: 0.00230497
Iteration 3/25 | Loss: 0.00230495
Iteration 4/25 | Loss: 0.00230495
Iteration 5/25 | Loss: 0.00230495
Iteration 6/25 | Loss: 0.00230495
Iteration 7/25 | Loss: 0.00230495
Iteration 8/25 | Loss: 0.00230495
Iteration 9/25 | Loss: 0.00230495
Iteration 10/25 | Loss: 0.00230495
Iteration 11/25 | Loss: 0.00230495
Iteration 12/25 | Loss: 0.00230495
Iteration 13/25 | Loss: 0.00230495
Iteration 14/25 | Loss: 0.00230495
Iteration 15/25 | Loss: 0.00230495
Iteration 16/25 | Loss: 0.00230495
Iteration 17/25 | Loss: 0.00230495
Iteration 18/25 | Loss: 0.00230495
Iteration 19/25 | Loss: 0.00230495
Iteration 20/25 | Loss: 0.00230495
Iteration 21/25 | Loss: 0.00230495
Iteration 22/25 | Loss: 0.00230495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002304946770891547, 0.002304946770891547, 0.002304946770891547, 0.002304946770891547, 0.002304946770891547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002304946770891547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230495
Iteration 2/1000 | Loss: 0.00003891
Iteration 3/1000 | Loss: 0.00002654
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00001983
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001923
Iteration 13/1000 | Loss: 0.00001917
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001909
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001906
Iteration 21/1000 | Loss: 0.00001906
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001906
Iteration 26/1000 | Loss: 0.00001906
Iteration 27/1000 | Loss: 0.00001906
Iteration 28/1000 | Loss: 0.00001906
Iteration 29/1000 | Loss: 0.00001906
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001905
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001904
Iteration 39/1000 | Loss: 0.00001904
Iteration 40/1000 | Loss: 0.00001904
Iteration 41/1000 | Loss: 0.00001903
Iteration 42/1000 | Loss: 0.00001903
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001903
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001903
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00001902
Iteration 49/1000 | Loss: 0.00001901
Iteration 50/1000 | Loss: 0.00001901
Iteration 51/1000 | Loss: 0.00001901
Iteration 52/1000 | Loss: 0.00001901
Iteration 53/1000 | Loss: 0.00001901
Iteration 54/1000 | Loss: 0.00001901
Iteration 55/1000 | Loss: 0.00001901
Iteration 56/1000 | Loss: 0.00001901
Iteration 57/1000 | Loss: 0.00001901
Iteration 58/1000 | Loss: 0.00001901
Iteration 59/1000 | Loss: 0.00001900
Iteration 60/1000 | Loss: 0.00001900
Iteration 61/1000 | Loss: 0.00001900
Iteration 62/1000 | Loss: 0.00001900
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001899
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001899
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001898
Iteration 82/1000 | Loss: 0.00001898
Iteration 83/1000 | Loss: 0.00001898
Iteration 84/1000 | Loss: 0.00001898
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001897
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001896
Iteration 96/1000 | Loss: 0.00001896
Iteration 97/1000 | Loss: 0.00001896
Iteration 98/1000 | Loss: 0.00001896
Iteration 99/1000 | Loss: 0.00001896
Iteration 100/1000 | Loss: 0.00001896
Iteration 101/1000 | Loss: 0.00001896
Iteration 102/1000 | Loss: 0.00001896
Iteration 103/1000 | Loss: 0.00001895
Iteration 104/1000 | Loss: 0.00001895
Iteration 105/1000 | Loss: 0.00001894
Iteration 106/1000 | Loss: 0.00001894
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001894
Iteration 110/1000 | Loss: 0.00001894
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001893
Iteration 115/1000 | Loss: 0.00001893
Iteration 116/1000 | Loss: 0.00001893
Iteration 117/1000 | Loss: 0.00001893
Iteration 118/1000 | Loss: 0.00001893
Iteration 119/1000 | Loss: 0.00001893
Iteration 120/1000 | Loss: 0.00001893
Iteration 121/1000 | Loss: 0.00001893
Iteration 122/1000 | Loss: 0.00001893
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001892
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001892
Iteration 129/1000 | Loss: 0.00001892
Iteration 130/1000 | Loss: 0.00001892
Iteration 131/1000 | Loss: 0.00001892
Iteration 132/1000 | Loss: 0.00001892
Iteration 133/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.8923850802821107e-05, 1.8923850802821107e-05, 1.8923850802821107e-05, 1.8923850802821107e-05, 1.8923850802821107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8923850802821107e-05

Optimization complete. Final v2v error: 3.803006649017334 mm

Highest mean error: 3.951054096221924 mm for frame 91

Lowest mean error: 3.499037027359009 mm for frame 134

Saving results

Total time: 33.3931348323822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574959
Iteration 2/25 | Loss: 0.00107268
Iteration 3/25 | Loss: 0.00095509
Iteration 4/25 | Loss: 0.00092735
Iteration 5/25 | Loss: 0.00091811
Iteration 6/25 | Loss: 0.00091600
Iteration 7/25 | Loss: 0.00091533
Iteration 8/25 | Loss: 0.00091533
Iteration 9/25 | Loss: 0.00091533
Iteration 10/25 | Loss: 0.00091533
Iteration 11/25 | Loss: 0.00091533
Iteration 12/25 | Loss: 0.00091533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009153271093964577, 0.0009153271093964577, 0.0009153271093964577, 0.0009153271093964577, 0.0009153271093964577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009153271093964577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.60281897
Iteration 2/25 | Loss: 0.00217783
Iteration 3/25 | Loss: 0.00217782
Iteration 4/25 | Loss: 0.00217782
Iteration 5/25 | Loss: 0.00217782
Iteration 6/25 | Loss: 0.00217781
Iteration 7/25 | Loss: 0.00217781
Iteration 8/25 | Loss: 0.00217781
Iteration 9/25 | Loss: 0.00217781
Iteration 10/25 | Loss: 0.00217781
Iteration 11/25 | Loss: 0.00217781
Iteration 12/25 | Loss: 0.00217781
Iteration 13/25 | Loss: 0.00217781
Iteration 14/25 | Loss: 0.00217781
Iteration 15/25 | Loss: 0.00217781
Iteration 16/25 | Loss: 0.00217781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00217781332321465, 0.00217781332321465, 0.00217781332321465, 0.00217781332321465, 0.00217781332321465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00217781332321465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217781
Iteration 2/1000 | Loss: 0.00003404
Iteration 3/1000 | Loss: 0.00002305
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00001979
Iteration 6/1000 | Loss: 0.00001891
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001783
Iteration 10/1000 | Loss: 0.00001778
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001768
Iteration 13/1000 | Loss: 0.00001767
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001766
Iteration 16/1000 | Loss: 0.00001765
Iteration 17/1000 | Loss: 0.00001763
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001759
Iteration 25/1000 | Loss: 0.00001759
Iteration 26/1000 | Loss: 0.00001758
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001758
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001758
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001757
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001755
Iteration 42/1000 | Loss: 0.00001755
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001753
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001752
Iteration 67/1000 | Loss: 0.00001752
Iteration 68/1000 | Loss: 0.00001752
Iteration 69/1000 | Loss: 0.00001752
Iteration 70/1000 | Loss: 0.00001752
Iteration 71/1000 | Loss: 0.00001752
Iteration 72/1000 | Loss: 0.00001752
Iteration 73/1000 | Loss: 0.00001752
Iteration 74/1000 | Loss: 0.00001752
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.751411946315784e-05, 1.751411946315784e-05, 1.751411946315784e-05, 1.751411946315784e-05, 1.751411946315784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.751411946315784e-05

Optimization complete. Final v2v error: 3.5553956031799316 mm

Highest mean error: 4.2748026847839355 mm for frame 60

Lowest mean error: 3.1567490100860596 mm for frame 13

Saving results

Total time: 30.510773420333862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875004
Iteration 2/25 | Loss: 0.00111236
Iteration 3/25 | Loss: 0.00097261
Iteration 4/25 | Loss: 0.00094806
Iteration 5/25 | Loss: 0.00093988
Iteration 6/25 | Loss: 0.00093776
Iteration 7/25 | Loss: 0.00093724
Iteration 8/25 | Loss: 0.00093724
Iteration 9/25 | Loss: 0.00093724
Iteration 10/25 | Loss: 0.00093724
Iteration 11/25 | Loss: 0.00093724
Iteration 12/25 | Loss: 0.00093724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009372440981678665, 0.0009372440981678665, 0.0009372440981678665, 0.0009372440981678665, 0.0009372440981678665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009372440981678665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67470062
Iteration 2/25 | Loss: 0.00259665
Iteration 3/25 | Loss: 0.00259665
Iteration 4/25 | Loss: 0.00259665
Iteration 5/25 | Loss: 0.00259665
Iteration 6/25 | Loss: 0.00259665
Iteration 7/25 | Loss: 0.00259665
Iteration 8/25 | Loss: 0.00259665
Iteration 9/25 | Loss: 0.00259665
Iteration 10/25 | Loss: 0.00259665
Iteration 11/25 | Loss: 0.00259665
Iteration 12/25 | Loss: 0.00259665
Iteration 13/25 | Loss: 0.00259665
Iteration 14/25 | Loss: 0.00259665
Iteration 15/25 | Loss: 0.00259665
Iteration 16/25 | Loss: 0.00259665
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002596645848825574, 0.002596645848825574, 0.002596645848825574, 0.002596645848825574, 0.002596645848825574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002596645848825574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259665
Iteration 2/1000 | Loss: 0.00003895
Iteration 3/1000 | Loss: 0.00002737
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00001935
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001761
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001644
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001642
Iteration 19/1000 | Loss: 0.00001640
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001638
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001637
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001635
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001632
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001628
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001626
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001625
Iteration 85/1000 | Loss: 0.00001625
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001624
Iteration 93/1000 | Loss: 0.00001624
Iteration 94/1000 | Loss: 0.00001624
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001623
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001620
Iteration 120/1000 | Loss: 0.00001620
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00001620
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001619
Iteration 127/1000 | Loss: 0.00001619
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001619
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Iteration 140/1000 | Loss: 0.00001618
Iteration 141/1000 | Loss: 0.00001618
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001618
Iteration 144/1000 | Loss: 0.00001618
Iteration 145/1000 | Loss: 0.00001618
Iteration 146/1000 | Loss: 0.00001618
Iteration 147/1000 | Loss: 0.00001618
Iteration 148/1000 | Loss: 0.00001618
Iteration 149/1000 | Loss: 0.00001618
Iteration 150/1000 | Loss: 0.00001618
Iteration 151/1000 | Loss: 0.00001618
Iteration 152/1000 | Loss: 0.00001618
Iteration 153/1000 | Loss: 0.00001618
Iteration 154/1000 | Loss: 0.00001618
Iteration 155/1000 | Loss: 0.00001618
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.6177633369807154e-05, 1.6177633369807154e-05, 1.6177633369807154e-05, 1.6177633369807154e-05, 1.6177633369807154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6177633369807154e-05

Optimization complete. Final v2v error: 3.463627338409424 mm

Highest mean error: 4.137143135070801 mm for frame 239

Lowest mean error: 3.006286859512329 mm for frame 225

Saving results

Total time: 41.68427753448486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886482
Iteration 2/25 | Loss: 0.00142095
Iteration 3/25 | Loss: 0.00116960
Iteration 4/25 | Loss: 0.00111351
Iteration 5/25 | Loss: 0.00110439
Iteration 6/25 | Loss: 0.00110248
Iteration 7/25 | Loss: 0.00110206
Iteration 8/25 | Loss: 0.00110206
Iteration 9/25 | Loss: 0.00110206
Iteration 10/25 | Loss: 0.00110206
Iteration 11/25 | Loss: 0.00110206
Iteration 12/25 | Loss: 0.00110206
Iteration 13/25 | Loss: 0.00110206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011020594974979758, 0.0011020594974979758, 0.0011020594974979758, 0.0011020594974979758, 0.0011020594974979758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011020594974979758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73841250
Iteration 2/25 | Loss: 0.00231108
Iteration 3/25 | Loss: 0.00231102
Iteration 4/25 | Loss: 0.00231102
Iteration 5/25 | Loss: 0.00231102
Iteration 6/25 | Loss: 0.00231102
Iteration 7/25 | Loss: 0.00231102
Iteration 8/25 | Loss: 0.00231102
Iteration 9/25 | Loss: 0.00231102
Iteration 10/25 | Loss: 0.00231102
Iteration 11/25 | Loss: 0.00231102
Iteration 12/25 | Loss: 0.00231102
Iteration 13/25 | Loss: 0.00231102
Iteration 14/25 | Loss: 0.00231102
Iteration 15/25 | Loss: 0.00231102
Iteration 16/25 | Loss: 0.00231102
Iteration 17/25 | Loss: 0.00231102
Iteration 18/25 | Loss: 0.00231102
Iteration 19/25 | Loss: 0.00231102
Iteration 20/25 | Loss: 0.00231102
Iteration 21/25 | Loss: 0.00231102
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023110187612473965, 0.0023110187612473965, 0.0023110187612473965, 0.0023110187612473965, 0.0023110187612473965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023110187612473965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231102
Iteration 2/1000 | Loss: 0.00009501
Iteration 3/1000 | Loss: 0.00006567
Iteration 4/1000 | Loss: 0.00004925
Iteration 5/1000 | Loss: 0.00004336
Iteration 6/1000 | Loss: 0.00004089
Iteration 7/1000 | Loss: 0.00003943
Iteration 8/1000 | Loss: 0.00003826
Iteration 9/1000 | Loss: 0.00003699
Iteration 10/1000 | Loss: 0.00003594
Iteration 11/1000 | Loss: 0.00003511
Iteration 12/1000 | Loss: 0.00003444
Iteration 13/1000 | Loss: 0.00003384
Iteration 14/1000 | Loss: 0.00003339
Iteration 15/1000 | Loss: 0.00003304
Iteration 16/1000 | Loss: 0.00003274
Iteration 17/1000 | Loss: 0.00003249
Iteration 18/1000 | Loss: 0.00003230
Iteration 19/1000 | Loss: 0.00003212
Iteration 20/1000 | Loss: 0.00003206
Iteration 21/1000 | Loss: 0.00003203
Iteration 22/1000 | Loss: 0.00003202
Iteration 23/1000 | Loss: 0.00003202
Iteration 24/1000 | Loss: 0.00003202
Iteration 25/1000 | Loss: 0.00003201
Iteration 26/1000 | Loss: 0.00003201
Iteration 27/1000 | Loss: 0.00003200
Iteration 28/1000 | Loss: 0.00003200
Iteration 29/1000 | Loss: 0.00003199
Iteration 30/1000 | Loss: 0.00003199
Iteration 31/1000 | Loss: 0.00003199
Iteration 32/1000 | Loss: 0.00003193
Iteration 33/1000 | Loss: 0.00003193
Iteration 34/1000 | Loss: 0.00003190
Iteration 35/1000 | Loss: 0.00003190
Iteration 36/1000 | Loss: 0.00003190
Iteration 37/1000 | Loss: 0.00003190
Iteration 38/1000 | Loss: 0.00003190
Iteration 39/1000 | Loss: 0.00003190
Iteration 40/1000 | Loss: 0.00003190
Iteration 41/1000 | Loss: 0.00003190
Iteration 42/1000 | Loss: 0.00003189
Iteration 43/1000 | Loss: 0.00003189
Iteration 44/1000 | Loss: 0.00003189
Iteration 45/1000 | Loss: 0.00003189
Iteration 46/1000 | Loss: 0.00003189
Iteration 47/1000 | Loss: 0.00003189
Iteration 48/1000 | Loss: 0.00003188
Iteration 49/1000 | Loss: 0.00003187
Iteration 50/1000 | Loss: 0.00003187
Iteration 51/1000 | Loss: 0.00003186
Iteration 52/1000 | Loss: 0.00003186
Iteration 53/1000 | Loss: 0.00003185
Iteration 54/1000 | Loss: 0.00003185
Iteration 55/1000 | Loss: 0.00003185
Iteration 56/1000 | Loss: 0.00003184
Iteration 57/1000 | Loss: 0.00003184
Iteration 58/1000 | Loss: 0.00003184
Iteration 59/1000 | Loss: 0.00003183
Iteration 60/1000 | Loss: 0.00003183
Iteration 61/1000 | Loss: 0.00003183
Iteration 62/1000 | Loss: 0.00003182
Iteration 63/1000 | Loss: 0.00003182
Iteration 64/1000 | Loss: 0.00003182
Iteration 65/1000 | Loss: 0.00003181
Iteration 66/1000 | Loss: 0.00003181
Iteration 67/1000 | Loss: 0.00003181
Iteration 68/1000 | Loss: 0.00003181
Iteration 69/1000 | Loss: 0.00003180
Iteration 70/1000 | Loss: 0.00003180
Iteration 71/1000 | Loss: 0.00003180
Iteration 72/1000 | Loss: 0.00003180
Iteration 73/1000 | Loss: 0.00003180
Iteration 74/1000 | Loss: 0.00003180
Iteration 75/1000 | Loss: 0.00003179
Iteration 76/1000 | Loss: 0.00003179
Iteration 77/1000 | Loss: 0.00003179
Iteration 78/1000 | Loss: 0.00003178
Iteration 79/1000 | Loss: 0.00003178
Iteration 80/1000 | Loss: 0.00003178
Iteration 81/1000 | Loss: 0.00003178
Iteration 82/1000 | Loss: 0.00003177
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00003177
Iteration 85/1000 | Loss: 0.00003177
Iteration 86/1000 | Loss: 0.00003176
Iteration 87/1000 | Loss: 0.00003176
Iteration 88/1000 | Loss: 0.00003176
Iteration 89/1000 | Loss: 0.00003176
Iteration 90/1000 | Loss: 0.00003175
Iteration 91/1000 | Loss: 0.00003175
Iteration 92/1000 | Loss: 0.00003175
Iteration 93/1000 | Loss: 0.00003174
Iteration 94/1000 | Loss: 0.00003174
Iteration 95/1000 | Loss: 0.00003174
Iteration 96/1000 | Loss: 0.00003174
Iteration 97/1000 | Loss: 0.00003174
Iteration 98/1000 | Loss: 0.00003173
Iteration 99/1000 | Loss: 0.00003173
Iteration 100/1000 | Loss: 0.00003173
Iteration 101/1000 | Loss: 0.00003173
Iteration 102/1000 | Loss: 0.00003172
Iteration 103/1000 | Loss: 0.00003172
Iteration 104/1000 | Loss: 0.00003172
Iteration 105/1000 | Loss: 0.00003172
Iteration 106/1000 | Loss: 0.00003172
Iteration 107/1000 | Loss: 0.00003171
Iteration 108/1000 | Loss: 0.00003171
Iteration 109/1000 | Loss: 0.00003170
Iteration 110/1000 | Loss: 0.00003170
Iteration 111/1000 | Loss: 0.00003170
Iteration 112/1000 | Loss: 0.00003170
Iteration 113/1000 | Loss: 0.00003170
Iteration 114/1000 | Loss: 0.00003169
Iteration 115/1000 | Loss: 0.00003169
Iteration 116/1000 | Loss: 0.00003169
Iteration 117/1000 | Loss: 0.00003169
Iteration 118/1000 | Loss: 0.00003169
Iteration 119/1000 | Loss: 0.00003169
Iteration 120/1000 | Loss: 0.00003169
Iteration 121/1000 | Loss: 0.00003168
Iteration 122/1000 | Loss: 0.00003168
Iteration 123/1000 | Loss: 0.00003168
Iteration 124/1000 | Loss: 0.00003168
Iteration 125/1000 | Loss: 0.00003167
Iteration 126/1000 | Loss: 0.00003167
Iteration 127/1000 | Loss: 0.00003167
Iteration 128/1000 | Loss: 0.00003167
Iteration 129/1000 | Loss: 0.00003167
Iteration 130/1000 | Loss: 0.00003167
Iteration 131/1000 | Loss: 0.00003167
Iteration 132/1000 | Loss: 0.00003166
Iteration 133/1000 | Loss: 0.00003166
Iteration 134/1000 | Loss: 0.00003166
Iteration 135/1000 | Loss: 0.00003166
Iteration 136/1000 | Loss: 0.00003166
Iteration 137/1000 | Loss: 0.00003166
Iteration 138/1000 | Loss: 0.00003166
Iteration 139/1000 | Loss: 0.00003166
Iteration 140/1000 | Loss: 0.00003166
Iteration 141/1000 | Loss: 0.00003165
Iteration 142/1000 | Loss: 0.00003165
Iteration 143/1000 | Loss: 0.00003165
Iteration 144/1000 | Loss: 0.00003165
Iteration 145/1000 | Loss: 0.00003165
Iteration 146/1000 | Loss: 0.00003165
Iteration 147/1000 | Loss: 0.00003165
Iteration 148/1000 | Loss: 0.00003165
Iteration 149/1000 | Loss: 0.00003165
Iteration 150/1000 | Loss: 0.00003165
Iteration 151/1000 | Loss: 0.00003165
Iteration 152/1000 | Loss: 0.00003165
Iteration 153/1000 | Loss: 0.00003165
Iteration 154/1000 | Loss: 0.00003164
Iteration 155/1000 | Loss: 0.00003164
Iteration 156/1000 | Loss: 0.00003164
Iteration 157/1000 | Loss: 0.00003164
Iteration 158/1000 | Loss: 0.00003164
Iteration 159/1000 | Loss: 0.00003164
Iteration 160/1000 | Loss: 0.00003164
Iteration 161/1000 | Loss: 0.00003164
Iteration 162/1000 | Loss: 0.00003164
Iteration 163/1000 | Loss: 0.00003164
Iteration 164/1000 | Loss: 0.00003163
Iteration 165/1000 | Loss: 0.00003163
Iteration 166/1000 | Loss: 0.00003163
Iteration 167/1000 | Loss: 0.00003163
Iteration 168/1000 | Loss: 0.00003163
Iteration 169/1000 | Loss: 0.00003163
Iteration 170/1000 | Loss: 0.00003163
Iteration 171/1000 | Loss: 0.00003163
Iteration 172/1000 | Loss: 0.00003162
Iteration 173/1000 | Loss: 0.00003162
Iteration 174/1000 | Loss: 0.00003162
Iteration 175/1000 | Loss: 0.00003162
Iteration 176/1000 | Loss: 0.00003162
Iteration 177/1000 | Loss: 0.00003162
Iteration 178/1000 | Loss: 0.00003162
Iteration 179/1000 | Loss: 0.00003162
Iteration 180/1000 | Loss: 0.00003162
Iteration 181/1000 | Loss: 0.00003162
Iteration 182/1000 | Loss: 0.00003162
Iteration 183/1000 | Loss: 0.00003162
Iteration 184/1000 | Loss: 0.00003162
Iteration 185/1000 | Loss: 0.00003162
Iteration 186/1000 | Loss: 0.00003162
Iteration 187/1000 | Loss: 0.00003162
Iteration 188/1000 | Loss: 0.00003162
Iteration 189/1000 | Loss: 0.00003162
Iteration 190/1000 | Loss: 0.00003162
Iteration 191/1000 | Loss: 0.00003162
Iteration 192/1000 | Loss: 0.00003162
Iteration 193/1000 | Loss: 0.00003162
Iteration 194/1000 | Loss: 0.00003162
Iteration 195/1000 | Loss: 0.00003162
Iteration 196/1000 | Loss: 0.00003162
Iteration 197/1000 | Loss: 0.00003162
Iteration 198/1000 | Loss: 0.00003162
Iteration 199/1000 | Loss: 0.00003162
Iteration 200/1000 | Loss: 0.00003162
Iteration 201/1000 | Loss: 0.00003162
Iteration 202/1000 | Loss: 0.00003162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.162074426654726e-05, 3.162074426654726e-05, 3.162074426654726e-05, 3.162074426654726e-05, 3.162074426654726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.162074426654726e-05

Optimization complete. Final v2v error: 4.538106441497803 mm

Highest mean error: 6.333745002746582 mm for frame 163

Lowest mean error: 3.250501871109009 mm for frame 104

Saving results

Total time: 52.87264966964722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137454
Iteration 2/25 | Loss: 0.01137454
Iteration 3/25 | Loss: 0.01137453
Iteration 4/25 | Loss: 0.01137453
Iteration 5/25 | Loss: 0.01137452
Iteration 6/25 | Loss: 0.00200979
Iteration 7/25 | Loss: 0.00140014
Iteration 8/25 | Loss: 0.00119185
Iteration 9/25 | Loss: 0.00116005
Iteration 10/25 | Loss: 0.00114720
Iteration 11/25 | Loss: 0.00108160
Iteration 12/25 | Loss: 0.00105090
Iteration 13/25 | Loss: 0.00103129
Iteration 14/25 | Loss: 0.00102126
Iteration 15/25 | Loss: 0.00103756
Iteration 16/25 | Loss: 0.00103149
Iteration 17/25 | Loss: 0.00101803
Iteration 18/25 | Loss: 0.00100748
Iteration 19/25 | Loss: 0.00099962
Iteration 20/25 | Loss: 0.00099554
Iteration 21/25 | Loss: 0.00099067
Iteration 22/25 | Loss: 0.00098970
Iteration 23/25 | Loss: 0.00098259
Iteration 24/25 | Loss: 0.00097820
Iteration 25/25 | Loss: 0.00097253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96576500
Iteration 2/25 | Loss: 0.00268949
Iteration 3/25 | Loss: 0.00268949
Iteration 4/25 | Loss: 0.00268949
Iteration 5/25 | Loss: 0.00268949
Iteration 6/25 | Loss: 0.00268949
Iteration 7/25 | Loss: 0.00268949
Iteration 8/25 | Loss: 0.00268949
Iteration 9/25 | Loss: 0.00268949
Iteration 10/25 | Loss: 0.00268949
Iteration 11/25 | Loss: 0.00268949
Iteration 12/25 | Loss: 0.00268949
Iteration 13/25 | Loss: 0.00268949
Iteration 14/25 | Loss: 0.00268949
Iteration 15/25 | Loss: 0.00268949
Iteration 16/25 | Loss: 0.00268949
Iteration 17/25 | Loss: 0.00268949
Iteration 18/25 | Loss: 0.00268949
Iteration 19/25 | Loss: 0.00268949
Iteration 20/25 | Loss: 0.00268949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002689487300813198, 0.002689487300813198, 0.002689487300813198, 0.002689487300813198, 0.002689487300813198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002689487300813198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268949
Iteration 2/1000 | Loss: 0.00021275
Iteration 3/1000 | Loss: 0.00027430
Iteration 4/1000 | Loss: 0.00016350
Iteration 5/1000 | Loss: 0.00004159
Iteration 6/1000 | Loss: 0.00016270
Iteration 7/1000 | Loss: 0.00014650
Iteration 8/1000 | Loss: 0.00023766
Iteration 9/1000 | Loss: 0.00016127
Iteration 10/1000 | Loss: 0.00003716
Iteration 11/1000 | Loss: 0.00015397
Iteration 12/1000 | Loss: 0.00007500
Iteration 13/1000 | Loss: 0.00008761
Iteration 14/1000 | Loss: 0.00011014
Iteration 15/1000 | Loss: 0.00017954
Iteration 16/1000 | Loss: 0.00015052
Iteration 17/1000 | Loss: 0.00017462
Iteration 18/1000 | Loss: 0.00006066
Iteration 19/1000 | Loss: 0.00019403
Iteration 20/1000 | Loss: 0.00013201
Iteration 21/1000 | Loss: 0.00007708
Iteration 22/1000 | Loss: 0.00004358
Iteration 23/1000 | Loss: 0.00014813
Iteration 24/1000 | Loss: 0.00019414
Iteration 25/1000 | Loss: 0.00030463
Iteration 26/1000 | Loss: 0.00036413
Iteration 27/1000 | Loss: 0.00027351
Iteration 28/1000 | Loss: 0.00039974
Iteration 29/1000 | Loss: 0.00054471
Iteration 30/1000 | Loss: 0.00039796
Iteration 31/1000 | Loss: 0.00014186
Iteration 32/1000 | Loss: 0.00017410
Iteration 33/1000 | Loss: 0.00005372
Iteration 34/1000 | Loss: 0.00008808
Iteration 35/1000 | Loss: 0.00016320
Iteration 36/1000 | Loss: 0.00009398
Iteration 37/1000 | Loss: 0.00008272
Iteration 38/1000 | Loss: 0.00009610
Iteration 39/1000 | Loss: 0.00009712
Iteration 40/1000 | Loss: 0.00010524
Iteration 41/1000 | Loss: 0.00015735
Iteration 42/1000 | Loss: 0.00003441
Iteration 43/1000 | Loss: 0.00012076
Iteration 44/1000 | Loss: 0.00011987
Iteration 45/1000 | Loss: 0.00004252
Iteration 46/1000 | Loss: 0.00024496
Iteration 47/1000 | Loss: 0.00007097
Iteration 48/1000 | Loss: 0.00014817
Iteration 49/1000 | Loss: 0.00014476
Iteration 50/1000 | Loss: 0.00024578
Iteration 51/1000 | Loss: 0.00025138
Iteration 52/1000 | Loss: 0.00010792
Iteration 53/1000 | Loss: 0.00006961
Iteration 54/1000 | Loss: 0.00007887
Iteration 55/1000 | Loss: 0.00006356
Iteration 56/1000 | Loss: 0.00008406
Iteration 57/1000 | Loss: 0.00004559
Iteration 58/1000 | Loss: 0.00011787
Iteration 59/1000 | Loss: 0.00029462
Iteration 60/1000 | Loss: 0.00008001
Iteration 61/1000 | Loss: 0.00013541
Iteration 62/1000 | Loss: 0.00012296
Iteration 63/1000 | Loss: 0.00035738
Iteration 64/1000 | Loss: 0.00024044
Iteration 65/1000 | Loss: 0.00010118
Iteration 66/1000 | Loss: 0.00004013
Iteration 67/1000 | Loss: 0.00011142
Iteration 68/1000 | Loss: 0.00018099
Iteration 69/1000 | Loss: 0.00005672
Iteration 70/1000 | Loss: 0.00007580
Iteration 71/1000 | Loss: 0.00005987
Iteration 72/1000 | Loss: 0.00019814
Iteration 73/1000 | Loss: 0.00013204
Iteration 74/1000 | Loss: 0.00020076
Iteration 75/1000 | Loss: 0.00005478
Iteration 76/1000 | Loss: 0.00010746
Iteration 77/1000 | Loss: 0.00014988
Iteration 78/1000 | Loss: 0.00019072
Iteration 79/1000 | Loss: 0.00010796
Iteration 80/1000 | Loss: 0.00002718
Iteration 81/1000 | Loss: 0.00019033
Iteration 82/1000 | Loss: 0.00004310
Iteration 83/1000 | Loss: 0.00003256
Iteration 84/1000 | Loss: 0.00003926
Iteration 85/1000 | Loss: 0.00003841
Iteration 86/1000 | Loss: 0.00003807
Iteration 87/1000 | Loss: 0.00022169
Iteration 88/1000 | Loss: 0.00010306
Iteration 89/1000 | Loss: 0.00004289
Iteration 90/1000 | Loss: 0.00002767
Iteration 91/1000 | Loss: 0.00002629
Iteration 92/1000 | Loss: 0.00014880
Iteration 93/1000 | Loss: 0.00002505
Iteration 94/1000 | Loss: 0.00022234
Iteration 95/1000 | Loss: 0.00011891
Iteration 96/1000 | Loss: 0.00010230
Iteration 97/1000 | Loss: 0.00021768
Iteration 98/1000 | Loss: 0.00002757
Iteration 99/1000 | Loss: 0.00002286
Iteration 100/1000 | Loss: 0.00002193
Iteration 101/1000 | Loss: 0.00002152
Iteration 102/1000 | Loss: 0.00002115
Iteration 103/1000 | Loss: 0.00002091
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002073
Iteration 106/1000 | Loss: 0.00002070
Iteration 107/1000 | Loss: 0.00002069
Iteration 108/1000 | Loss: 0.00015618
Iteration 109/1000 | Loss: 0.00002826
Iteration 110/1000 | Loss: 0.00002472
Iteration 111/1000 | Loss: 0.00018743
Iteration 112/1000 | Loss: 0.00003102
Iteration 113/1000 | Loss: 0.00007766
Iteration 114/1000 | Loss: 0.00011650
Iteration 115/1000 | Loss: 0.00002259
Iteration 116/1000 | Loss: 0.00002132
Iteration 117/1000 | Loss: 0.00002090
Iteration 118/1000 | Loss: 0.00002075
Iteration 119/1000 | Loss: 0.00002065
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002064
Iteration 122/1000 | Loss: 0.00002063
Iteration 123/1000 | Loss: 0.00002063
Iteration 124/1000 | Loss: 0.00002062
Iteration 125/1000 | Loss: 0.00002062
Iteration 126/1000 | Loss: 0.00002062
Iteration 127/1000 | Loss: 0.00002061
Iteration 128/1000 | Loss: 0.00002061
Iteration 129/1000 | Loss: 0.00002061
Iteration 130/1000 | Loss: 0.00002061
Iteration 131/1000 | Loss: 0.00002060
Iteration 132/1000 | Loss: 0.00002060
Iteration 133/1000 | Loss: 0.00002060
Iteration 134/1000 | Loss: 0.00002060
Iteration 135/1000 | Loss: 0.00002060
Iteration 136/1000 | Loss: 0.00002060
Iteration 137/1000 | Loss: 0.00002060
Iteration 138/1000 | Loss: 0.00002060
Iteration 139/1000 | Loss: 0.00002059
Iteration 140/1000 | Loss: 0.00002059
Iteration 141/1000 | Loss: 0.00002059
Iteration 142/1000 | Loss: 0.00002059
Iteration 143/1000 | Loss: 0.00002059
Iteration 144/1000 | Loss: 0.00002059
Iteration 145/1000 | Loss: 0.00002059
Iteration 146/1000 | Loss: 0.00002059
Iteration 147/1000 | Loss: 0.00002059
Iteration 148/1000 | Loss: 0.00002058
Iteration 149/1000 | Loss: 0.00002058
Iteration 150/1000 | Loss: 0.00002058
Iteration 151/1000 | Loss: 0.00002058
Iteration 152/1000 | Loss: 0.00002058
Iteration 153/1000 | Loss: 0.00002058
Iteration 154/1000 | Loss: 0.00002057
Iteration 155/1000 | Loss: 0.00002057
Iteration 156/1000 | Loss: 0.00002057
Iteration 157/1000 | Loss: 0.00002057
Iteration 158/1000 | Loss: 0.00002056
Iteration 159/1000 | Loss: 0.00002056
Iteration 160/1000 | Loss: 0.00002056
Iteration 161/1000 | Loss: 0.00002055
Iteration 162/1000 | Loss: 0.00002055
Iteration 163/1000 | Loss: 0.00002054
Iteration 164/1000 | Loss: 0.00002054
Iteration 165/1000 | Loss: 0.00002054
Iteration 166/1000 | Loss: 0.00018283
Iteration 167/1000 | Loss: 0.00011733
Iteration 168/1000 | Loss: 0.00002570
Iteration 169/1000 | Loss: 0.00002423
Iteration 170/1000 | Loss: 0.00002335
Iteration 171/1000 | Loss: 0.00002239
Iteration 172/1000 | Loss: 0.00002145
Iteration 173/1000 | Loss: 0.00002098
Iteration 174/1000 | Loss: 0.00002056
Iteration 175/1000 | Loss: 0.00002029
Iteration 176/1000 | Loss: 0.00002021
Iteration 177/1000 | Loss: 0.00002014
Iteration 178/1000 | Loss: 0.00002014
Iteration 179/1000 | Loss: 0.00002013
Iteration 180/1000 | Loss: 0.00002013
Iteration 181/1000 | Loss: 0.00002013
Iteration 182/1000 | Loss: 0.00002012
Iteration 183/1000 | Loss: 0.00002012
Iteration 184/1000 | Loss: 0.00002012
Iteration 185/1000 | Loss: 0.00002012
Iteration 186/1000 | Loss: 0.00002011
Iteration 187/1000 | Loss: 0.00002011
Iteration 188/1000 | Loss: 0.00002011
Iteration 189/1000 | Loss: 0.00002010
Iteration 190/1000 | Loss: 0.00002010
Iteration 191/1000 | Loss: 0.00002010
Iteration 192/1000 | Loss: 0.00002010
Iteration 193/1000 | Loss: 0.00002010
Iteration 194/1000 | Loss: 0.00002009
Iteration 195/1000 | Loss: 0.00002009
Iteration 196/1000 | Loss: 0.00002009
Iteration 197/1000 | Loss: 0.00002009
Iteration 198/1000 | Loss: 0.00002009
Iteration 199/1000 | Loss: 0.00002009
Iteration 200/1000 | Loss: 0.00002009
Iteration 201/1000 | Loss: 0.00002009
Iteration 202/1000 | Loss: 0.00002008
Iteration 203/1000 | Loss: 0.00002008
Iteration 204/1000 | Loss: 0.00002008
Iteration 205/1000 | Loss: 0.00002008
Iteration 206/1000 | Loss: 0.00002008
Iteration 207/1000 | Loss: 0.00002007
Iteration 208/1000 | Loss: 0.00002007
Iteration 209/1000 | Loss: 0.00002007
Iteration 210/1000 | Loss: 0.00002007
Iteration 211/1000 | Loss: 0.00002007
Iteration 212/1000 | Loss: 0.00002007
Iteration 213/1000 | Loss: 0.00002007
Iteration 214/1000 | Loss: 0.00002007
Iteration 215/1000 | Loss: 0.00002007
Iteration 216/1000 | Loss: 0.00002007
Iteration 217/1000 | Loss: 0.00002006
Iteration 218/1000 | Loss: 0.00002006
Iteration 219/1000 | Loss: 0.00002006
Iteration 220/1000 | Loss: 0.00002006
Iteration 221/1000 | Loss: 0.00002006
Iteration 222/1000 | Loss: 0.00002006
Iteration 223/1000 | Loss: 0.00002006
Iteration 224/1000 | Loss: 0.00002006
Iteration 225/1000 | Loss: 0.00002006
Iteration 226/1000 | Loss: 0.00002006
Iteration 227/1000 | Loss: 0.00002006
Iteration 228/1000 | Loss: 0.00002006
Iteration 229/1000 | Loss: 0.00002006
Iteration 230/1000 | Loss: 0.00002006
Iteration 231/1000 | Loss: 0.00002006
Iteration 232/1000 | Loss: 0.00002006
Iteration 233/1000 | Loss: 0.00002006
Iteration 234/1000 | Loss: 0.00002006
Iteration 235/1000 | Loss: 0.00002006
Iteration 236/1000 | Loss: 0.00002006
Iteration 237/1000 | Loss: 0.00002006
Iteration 238/1000 | Loss: 0.00002006
Iteration 239/1000 | Loss: 0.00002006
Iteration 240/1000 | Loss: 0.00002006
Iteration 241/1000 | Loss: 0.00002006
Iteration 242/1000 | Loss: 0.00002006
Iteration 243/1000 | Loss: 0.00002006
Iteration 244/1000 | Loss: 0.00002006
Iteration 245/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [2.006174145208206e-05, 2.006174145208206e-05, 2.006174145208206e-05, 2.006174145208206e-05, 2.006174145208206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.006174145208206e-05

Optimization complete. Final v2v error: 3.8947389125823975 mm

Highest mean error: 11.102646827697754 mm for frame 101

Lowest mean error: 3.503330707550049 mm for frame 2

Saving results

Total time: 257.6795766353607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642101
Iteration 2/25 | Loss: 0.00139105
Iteration 3/25 | Loss: 0.00117018
Iteration 4/25 | Loss: 0.00111788
Iteration 5/25 | Loss: 0.00111055
Iteration 6/25 | Loss: 0.00110942
Iteration 7/25 | Loss: 0.00110942
Iteration 8/25 | Loss: 0.00110942
Iteration 9/25 | Loss: 0.00110942
Iteration 10/25 | Loss: 0.00110942
Iteration 11/25 | Loss: 0.00110942
Iteration 12/25 | Loss: 0.00110942
Iteration 13/25 | Loss: 0.00110942
Iteration 14/25 | Loss: 0.00110942
Iteration 15/25 | Loss: 0.00110942
Iteration 16/25 | Loss: 0.00110942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011094206711277366, 0.0011094206711277366, 0.0011094206711277366, 0.0011094206711277366, 0.0011094206711277366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011094206711277366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76442140
Iteration 2/25 | Loss: 0.00184427
Iteration 3/25 | Loss: 0.00184426
Iteration 4/25 | Loss: 0.00184426
Iteration 5/25 | Loss: 0.00184426
Iteration 6/25 | Loss: 0.00184426
Iteration 7/25 | Loss: 0.00184426
Iteration 8/25 | Loss: 0.00184426
Iteration 9/25 | Loss: 0.00184426
Iteration 10/25 | Loss: 0.00184426
Iteration 11/25 | Loss: 0.00184426
Iteration 12/25 | Loss: 0.00184426
Iteration 13/25 | Loss: 0.00184426
Iteration 14/25 | Loss: 0.00184426
Iteration 15/25 | Loss: 0.00184426
Iteration 16/25 | Loss: 0.00184426
Iteration 17/25 | Loss: 0.00184426
Iteration 18/25 | Loss: 0.00184426
Iteration 19/25 | Loss: 0.00184426
Iteration 20/25 | Loss: 0.00184426
Iteration 21/25 | Loss: 0.00184426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001844261190854013, 0.001844261190854013, 0.001844261190854013, 0.001844261190854013, 0.001844261190854013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001844261190854013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184426
Iteration 2/1000 | Loss: 0.00005055
Iteration 3/1000 | Loss: 0.00004065
Iteration 4/1000 | Loss: 0.00003778
Iteration 5/1000 | Loss: 0.00003586
Iteration 6/1000 | Loss: 0.00003516
Iteration 7/1000 | Loss: 0.00003447
Iteration 8/1000 | Loss: 0.00003427
Iteration 9/1000 | Loss: 0.00003417
Iteration 10/1000 | Loss: 0.00003407
Iteration 11/1000 | Loss: 0.00003401
Iteration 12/1000 | Loss: 0.00003401
Iteration 13/1000 | Loss: 0.00003401
Iteration 14/1000 | Loss: 0.00003401
Iteration 15/1000 | Loss: 0.00003401
Iteration 16/1000 | Loss: 0.00003401
Iteration 17/1000 | Loss: 0.00003400
Iteration 18/1000 | Loss: 0.00003400
Iteration 19/1000 | Loss: 0.00003400
Iteration 20/1000 | Loss: 0.00003400
Iteration 21/1000 | Loss: 0.00003400
Iteration 22/1000 | Loss: 0.00003400
Iteration 23/1000 | Loss: 0.00003400
Iteration 24/1000 | Loss: 0.00003400
Iteration 25/1000 | Loss: 0.00003400
Iteration 26/1000 | Loss: 0.00003400
Iteration 27/1000 | Loss: 0.00003397
Iteration 28/1000 | Loss: 0.00003395
Iteration 29/1000 | Loss: 0.00003394
Iteration 30/1000 | Loss: 0.00003391
Iteration 31/1000 | Loss: 0.00003391
Iteration 32/1000 | Loss: 0.00003388
Iteration 33/1000 | Loss: 0.00003386
Iteration 34/1000 | Loss: 0.00003386
Iteration 35/1000 | Loss: 0.00003386
Iteration 36/1000 | Loss: 0.00003386
Iteration 37/1000 | Loss: 0.00003386
Iteration 38/1000 | Loss: 0.00003386
Iteration 39/1000 | Loss: 0.00003386
Iteration 40/1000 | Loss: 0.00003386
Iteration 41/1000 | Loss: 0.00003386
Iteration 42/1000 | Loss: 0.00003386
Iteration 43/1000 | Loss: 0.00003385
Iteration 44/1000 | Loss: 0.00003385
Iteration 45/1000 | Loss: 0.00003385
Iteration 46/1000 | Loss: 0.00003385
Iteration 47/1000 | Loss: 0.00003385
Iteration 48/1000 | Loss: 0.00003385
Iteration 49/1000 | Loss: 0.00003385
Iteration 50/1000 | Loss: 0.00003385
Iteration 51/1000 | Loss: 0.00003385
Iteration 52/1000 | Loss: 0.00003385
Iteration 53/1000 | Loss: 0.00003385
Iteration 54/1000 | Loss: 0.00003384
Iteration 55/1000 | Loss: 0.00003384
Iteration 56/1000 | Loss: 0.00003384
Iteration 57/1000 | Loss: 0.00003384
Iteration 58/1000 | Loss: 0.00003384
Iteration 59/1000 | Loss: 0.00003384
Iteration 60/1000 | Loss: 0.00003383
Iteration 61/1000 | Loss: 0.00003383
Iteration 62/1000 | Loss: 0.00003383
Iteration 63/1000 | Loss: 0.00003383
Iteration 64/1000 | Loss: 0.00003383
Iteration 65/1000 | Loss: 0.00003383
Iteration 66/1000 | Loss: 0.00003382
Iteration 67/1000 | Loss: 0.00003380
Iteration 68/1000 | Loss: 0.00003380
Iteration 69/1000 | Loss: 0.00003380
Iteration 70/1000 | Loss: 0.00003380
Iteration 71/1000 | Loss: 0.00003379
Iteration 72/1000 | Loss: 0.00003379
Iteration 73/1000 | Loss: 0.00003378
Iteration 74/1000 | Loss: 0.00003378
Iteration 75/1000 | Loss: 0.00003378
Iteration 76/1000 | Loss: 0.00003378
Iteration 77/1000 | Loss: 0.00003378
Iteration 78/1000 | Loss: 0.00003378
Iteration 79/1000 | Loss: 0.00003378
Iteration 80/1000 | Loss: 0.00003377
Iteration 81/1000 | Loss: 0.00003377
Iteration 82/1000 | Loss: 0.00003377
Iteration 83/1000 | Loss: 0.00003376
Iteration 84/1000 | Loss: 0.00003376
Iteration 85/1000 | Loss: 0.00003376
Iteration 86/1000 | Loss: 0.00003376
Iteration 87/1000 | Loss: 0.00003376
Iteration 88/1000 | Loss: 0.00003376
Iteration 89/1000 | Loss: 0.00003375
Iteration 90/1000 | Loss: 0.00003375
Iteration 91/1000 | Loss: 0.00003375
Iteration 92/1000 | Loss: 0.00003375
Iteration 93/1000 | Loss: 0.00003375
Iteration 94/1000 | Loss: 0.00003374
Iteration 95/1000 | Loss: 0.00003374
Iteration 96/1000 | Loss: 0.00003374
Iteration 97/1000 | Loss: 0.00003373
Iteration 98/1000 | Loss: 0.00003373
Iteration 99/1000 | Loss: 0.00003373
Iteration 100/1000 | Loss: 0.00003373
Iteration 101/1000 | Loss: 0.00003373
Iteration 102/1000 | Loss: 0.00003372
Iteration 103/1000 | Loss: 0.00003372
Iteration 104/1000 | Loss: 0.00003372
Iteration 105/1000 | Loss: 0.00003372
Iteration 106/1000 | Loss: 0.00003372
Iteration 107/1000 | Loss: 0.00003372
Iteration 108/1000 | Loss: 0.00003372
Iteration 109/1000 | Loss: 0.00003372
Iteration 110/1000 | Loss: 0.00003372
Iteration 111/1000 | Loss: 0.00003372
Iteration 112/1000 | Loss: 0.00003372
Iteration 113/1000 | Loss: 0.00003372
Iteration 114/1000 | Loss: 0.00003372
Iteration 115/1000 | Loss: 0.00003372
Iteration 116/1000 | Loss: 0.00003372
Iteration 117/1000 | Loss: 0.00003372
Iteration 118/1000 | Loss: 0.00003372
Iteration 119/1000 | Loss: 0.00003372
Iteration 120/1000 | Loss: 0.00003372
Iteration 121/1000 | Loss: 0.00003372
Iteration 122/1000 | Loss: 0.00003372
Iteration 123/1000 | Loss: 0.00003372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.372195715201087e-05, 3.372195715201087e-05, 3.372195715201087e-05, 3.372195715201087e-05, 3.372195715201087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.372195715201087e-05

Optimization complete. Final v2v error: 4.8109002113342285 mm

Highest mean error: 5.033073902130127 mm for frame 71

Lowest mean error: 4.585465431213379 mm for frame 134

Saving results

Total time: 36.241283655166626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849096
Iteration 2/25 | Loss: 0.00137623
Iteration 3/25 | Loss: 0.00109314
Iteration 4/25 | Loss: 0.00102514
Iteration 5/25 | Loss: 0.00100843
Iteration 6/25 | Loss: 0.00100268
Iteration 7/25 | Loss: 0.00100089
Iteration 8/25 | Loss: 0.00100081
Iteration 9/25 | Loss: 0.00100081
Iteration 10/25 | Loss: 0.00100081
Iteration 11/25 | Loss: 0.00100081
Iteration 12/25 | Loss: 0.00100081
Iteration 13/25 | Loss: 0.00100081
Iteration 14/25 | Loss: 0.00100081
Iteration 15/25 | Loss: 0.00100081
Iteration 16/25 | Loss: 0.00100081
Iteration 17/25 | Loss: 0.00100081
Iteration 18/25 | Loss: 0.00100081
Iteration 19/25 | Loss: 0.00100081
Iteration 20/25 | Loss: 0.00100081
Iteration 21/25 | Loss: 0.00100081
Iteration 22/25 | Loss: 0.00100081
Iteration 23/25 | Loss: 0.00100081
Iteration 24/25 | Loss: 0.00100081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010008071549236774, 0.0010008071549236774, 0.0010008071549236774, 0.0010008071549236774, 0.0010008071549236774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010008071549236774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84253454
Iteration 2/25 | Loss: 0.00290624
Iteration 3/25 | Loss: 0.00290624
Iteration 4/25 | Loss: 0.00290624
Iteration 5/25 | Loss: 0.00290624
Iteration 6/25 | Loss: 0.00290624
Iteration 7/25 | Loss: 0.00290624
Iteration 8/25 | Loss: 0.00290624
Iteration 9/25 | Loss: 0.00290624
Iteration 10/25 | Loss: 0.00290624
Iteration 11/25 | Loss: 0.00290624
Iteration 12/25 | Loss: 0.00290624
Iteration 13/25 | Loss: 0.00290624
Iteration 14/25 | Loss: 0.00290624
Iteration 15/25 | Loss: 0.00290624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002906237496063113, 0.002906237496063113, 0.002906237496063113, 0.002906237496063113, 0.002906237496063113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002906237496063113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290624
Iteration 2/1000 | Loss: 0.00007356
Iteration 3/1000 | Loss: 0.00004261
Iteration 4/1000 | Loss: 0.00003202
Iteration 5/1000 | Loss: 0.00002908
Iteration 6/1000 | Loss: 0.00002734
Iteration 7/1000 | Loss: 0.00002633
Iteration 8/1000 | Loss: 0.00002548
Iteration 9/1000 | Loss: 0.00002475
Iteration 10/1000 | Loss: 0.00002423
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002354
Iteration 13/1000 | Loss: 0.00002332
Iteration 14/1000 | Loss: 0.00002322
Iteration 15/1000 | Loss: 0.00002306
Iteration 16/1000 | Loss: 0.00002302
Iteration 17/1000 | Loss: 0.00002297
Iteration 18/1000 | Loss: 0.00002284
Iteration 19/1000 | Loss: 0.00002283
Iteration 20/1000 | Loss: 0.00002282
Iteration 21/1000 | Loss: 0.00002281
Iteration 22/1000 | Loss: 0.00002280
Iteration 23/1000 | Loss: 0.00002279
Iteration 24/1000 | Loss: 0.00002274
Iteration 25/1000 | Loss: 0.00002274
Iteration 26/1000 | Loss: 0.00002271
Iteration 27/1000 | Loss: 0.00002271
Iteration 28/1000 | Loss: 0.00002269
Iteration 29/1000 | Loss: 0.00002269
Iteration 30/1000 | Loss: 0.00002267
Iteration 31/1000 | Loss: 0.00002267
Iteration 32/1000 | Loss: 0.00002266
Iteration 33/1000 | Loss: 0.00002266
Iteration 34/1000 | Loss: 0.00002265
Iteration 35/1000 | Loss: 0.00002265
Iteration 36/1000 | Loss: 0.00002265
Iteration 37/1000 | Loss: 0.00002265
Iteration 38/1000 | Loss: 0.00002265
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002265
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002262
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002261
Iteration 51/1000 | Loss: 0.00002260
Iteration 52/1000 | Loss: 0.00002260
Iteration 53/1000 | Loss: 0.00002260
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002258
Iteration 61/1000 | Loss: 0.00002258
Iteration 62/1000 | Loss: 0.00002258
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002257
Iteration 66/1000 | Loss: 0.00002257
Iteration 67/1000 | Loss: 0.00002257
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002256
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002256
Iteration 74/1000 | Loss: 0.00002256
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002255
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002255
Iteration 79/1000 | Loss: 0.00002255
Iteration 80/1000 | Loss: 0.00002254
Iteration 81/1000 | Loss: 0.00002254
Iteration 82/1000 | Loss: 0.00002254
Iteration 83/1000 | Loss: 0.00002254
Iteration 84/1000 | Loss: 0.00002254
Iteration 85/1000 | Loss: 0.00002253
Iteration 86/1000 | Loss: 0.00002253
Iteration 87/1000 | Loss: 0.00002253
Iteration 88/1000 | Loss: 0.00002253
Iteration 89/1000 | Loss: 0.00002253
Iteration 90/1000 | Loss: 0.00002253
Iteration 91/1000 | Loss: 0.00002253
Iteration 92/1000 | Loss: 0.00002253
Iteration 93/1000 | Loss: 0.00002253
Iteration 94/1000 | Loss: 0.00002253
Iteration 95/1000 | Loss: 0.00002253
Iteration 96/1000 | Loss: 0.00002253
Iteration 97/1000 | Loss: 0.00002253
Iteration 98/1000 | Loss: 0.00002253
Iteration 99/1000 | Loss: 0.00002253
Iteration 100/1000 | Loss: 0.00002253
Iteration 101/1000 | Loss: 0.00002253
Iteration 102/1000 | Loss: 0.00002253
Iteration 103/1000 | Loss: 0.00002253
Iteration 104/1000 | Loss: 0.00002253
Iteration 105/1000 | Loss: 0.00002253
Iteration 106/1000 | Loss: 0.00002253
Iteration 107/1000 | Loss: 0.00002253
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002253
Iteration 112/1000 | Loss: 0.00002253
Iteration 113/1000 | Loss: 0.00002253
Iteration 114/1000 | Loss: 0.00002253
Iteration 115/1000 | Loss: 0.00002253
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.253023922094144e-05, 2.253023922094144e-05, 2.253023922094144e-05, 2.253023922094144e-05, 2.253023922094144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.253023922094144e-05

Optimization complete. Final v2v error: 4.112192630767822 mm

Highest mean error: 5.47728967666626 mm for frame 149

Lowest mean error: 3.2414844036102295 mm for frame 129

Saving results

Total time: 46.53304958343506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01179194
Iteration 2/25 | Loss: 0.01179194
Iteration 3/25 | Loss: 0.00422650
Iteration 4/25 | Loss: 0.00285301
Iteration 5/25 | Loss: 0.00234070
Iteration 6/25 | Loss: 0.00216704
Iteration 7/25 | Loss: 0.00214398
Iteration 8/25 | Loss: 0.00224664
Iteration 9/25 | Loss: 0.00232651
Iteration 10/25 | Loss: 0.00205489
Iteration 11/25 | Loss: 0.00178801
Iteration 12/25 | Loss: 0.00158386
Iteration 13/25 | Loss: 0.00149406
Iteration 14/25 | Loss: 0.00147776
Iteration 15/25 | Loss: 0.00146918
Iteration 16/25 | Loss: 0.00147242
Iteration 17/25 | Loss: 0.00147123
Iteration 18/25 | Loss: 0.00146830
Iteration 19/25 | Loss: 0.00146183
Iteration 20/25 | Loss: 0.00145333
Iteration 21/25 | Loss: 0.00145119
Iteration 22/25 | Loss: 0.00145375
Iteration 23/25 | Loss: 0.00145449
Iteration 24/25 | Loss: 0.00145262
Iteration 25/25 | Loss: 0.00145337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74608761
Iteration 2/25 | Loss: 0.00329080
Iteration 3/25 | Loss: 0.00294902
Iteration 4/25 | Loss: 0.00294902
Iteration 5/25 | Loss: 0.00294902
Iteration 6/25 | Loss: 0.00294901
Iteration 7/25 | Loss: 0.00294901
Iteration 8/25 | Loss: 0.00294901
Iteration 9/25 | Loss: 0.00294901
Iteration 10/25 | Loss: 0.00294901
Iteration 11/25 | Loss: 0.00294901
Iteration 12/25 | Loss: 0.00294901
Iteration 13/25 | Loss: 0.00294901
Iteration 14/25 | Loss: 0.00294901
Iteration 15/25 | Loss: 0.00294901
Iteration 16/25 | Loss: 0.00294901
Iteration 17/25 | Loss: 0.00294901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002949013141915202, 0.002949013141915202, 0.002949013141915202, 0.002949013141915202, 0.002949013141915202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002949013141915202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294901
Iteration 2/1000 | Loss: 0.00078052
Iteration 3/1000 | Loss: 0.00211297
Iteration 4/1000 | Loss: 0.00440801
Iteration 5/1000 | Loss: 0.00172254
Iteration 6/1000 | Loss: 0.00307235
Iteration 7/1000 | Loss: 0.00503921
Iteration 8/1000 | Loss: 0.00236011
Iteration 9/1000 | Loss: 0.00200386
Iteration 10/1000 | Loss: 0.00315855
Iteration 11/1000 | Loss: 0.00201837
Iteration 12/1000 | Loss: 0.00106396
Iteration 13/1000 | Loss: 0.00228770
Iteration 14/1000 | Loss: 0.00104287
Iteration 15/1000 | Loss: 0.00223217
Iteration 16/1000 | Loss: 0.00206334
Iteration 17/1000 | Loss: 0.00101530
Iteration 18/1000 | Loss: 0.00118929
Iteration 19/1000 | Loss: 0.00125309
Iteration 20/1000 | Loss: 0.00148796
Iteration 21/1000 | Loss: 0.00150974
Iteration 22/1000 | Loss: 0.00242152
Iteration 23/1000 | Loss: 0.00135934
Iteration 24/1000 | Loss: 0.00103412
Iteration 25/1000 | Loss: 0.00070885
Iteration 26/1000 | Loss: 0.00201273
Iteration 27/1000 | Loss: 0.00231375
Iteration 28/1000 | Loss: 0.00156002
Iteration 29/1000 | Loss: 0.00150948
Iteration 30/1000 | Loss: 0.00069487
Iteration 31/1000 | Loss: 0.00058935
Iteration 32/1000 | Loss: 0.00039190
Iteration 33/1000 | Loss: 0.00051820
Iteration 34/1000 | Loss: 0.00088780
Iteration 35/1000 | Loss: 0.00084660
Iteration 36/1000 | Loss: 0.00041831
Iteration 37/1000 | Loss: 0.00039128
Iteration 38/1000 | Loss: 0.00059229
Iteration 39/1000 | Loss: 0.00066205
Iteration 40/1000 | Loss: 0.00071587
Iteration 41/1000 | Loss: 0.00082956
Iteration 42/1000 | Loss: 0.00156879
Iteration 43/1000 | Loss: 0.00079608
Iteration 44/1000 | Loss: 0.00109391
Iteration 45/1000 | Loss: 0.00050279
Iteration 46/1000 | Loss: 0.00039456
Iteration 47/1000 | Loss: 0.00039298
Iteration 48/1000 | Loss: 0.00056713
Iteration 49/1000 | Loss: 0.00060521
Iteration 50/1000 | Loss: 0.00051268
Iteration 51/1000 | Loss: 0.00101797
Iteration 52/1000 | Loss: 0.00102548
Iteration 53/1000 | Loss: 0.00109910
Iteration 54/1000 | Loss: 0.00091179
Iteration 55/1000 | Loss: 0.00079331
Iteration 56/1000 | Loss: 0.00105613
Iteration 57/1000 | Loss: 0.00061345
Iteration 58/1000 | Loss: 0.00048655
Iteration 59/1000 | Loss: 0.00088997
Iteration 60/1000 | Loss: 0.00058052
Iteration 61/1000 | Loss: 0.00048461
Iteration 62/1000 | Loss: 0.00051890
Iteration 63/1000 | Loss: 0.00091352
Iteration 64/1000 | Loss: 0.00092139
Iteration 65/1000 | Loss: 0.00076134
Iteration 66/1000 | Loss: 0.00104544
Iteration 67/1000 | Loss: 0.00100697
Iteration 68/1000 | Loss: 0.00107028
Iteration 69/1000 | Loss: 0.00083901
Iteration 70/1000 | Loss: 0.00109920
Iteration 71/1000 | Loss: 0.00057126
Iteration 72/1000 | Loss: 0.00042957
Iteration 73/1000 | Loss: 0.00041856
Iteration 74/1000 | Loss: 0.00093564
Iteration 75/1000 | Loss: 0.00144469
Iteration 76/1000 | Loss: 0.00098384
Iteration 77/1000 | Loss: 0.00102836
Iteration 78/1000 | Loss: 0.00077671
Iteration 79/1000 | Loss: 0.00022009
Iteration 80/1000 | Loss: 0.00065865
Iteration 81/1000 | Loss: 0.00041281
Iteration 82/1000 | Loss: 0.00013369
Iteration 83/1000 | Loss: 0.00012675
Iteration 84/1000 | Loss: 0.00015634
Iteration 85/1000 | Loss: 0.00012261
Iteration 86/1000 | Loss: 0.00026517
Iteration 87/1000 | Loss: 0.00012482
Iteration 88/1000 | Loss: 0.00013144
Iteration 89/1000 | Loss: 0.00010782
Iteration 90/1000 | Loss: 0.00009782
Iteration 91/1000 | Loss: 0.00039946
Iteration 92/1000 | Loss: 0.00021961
Iteration 93/1000 | Loss: 0.00020153
Iteration 94/1000 | Loss: 0.00022449
Iteration 95/1000 | Loss: 0.00022407
Iteration 96/1000 | Loss: 0.00012643
Iteration 97/1000 | Loss: 0.00012929
Iteration 98/1000 | Loss: 0.00012742
Iteration 99/1000 | Loss: 0.00040338
Iteration 100/1000 | Loss: 0.00022399
Iteration 101/1000 | Loss: 0.00012394
Iteration 102/1000 | Loss: 0.00029478
Iteration 103/1000 | Loss: 0.00012492
Iteration 104/1000 | Loss: 0.00010242
Iteration 105/1000 | Loss: 0.00009096
Iteration 106/1000 | Loss: 0.00012163
Iteration 107/1000 | Loss: 0.00012020
Iteration 108/1000 | Loss: 0.00010857
Iteration 109/1000 | Loss: 0.00018521
Iteration 110/1000 | Loss: 0.00029737
Iteration 111/1000 | Loss: 0.00013602
Iteration 112/1000 | Loss: 0.00012153
Iteration 113/1000 | Loss: 0.00039973
Iteration 114/1000 | Loss: 0.00051987
Iteration 115/1000 | Loss: 0.00039150
Iteration 116/1000 | Loss: 0.00017285
Iteration 117/1000 | Loss: 0.00029660
Iteration 118/1000 | Loss: 0.00038648
Iteration 119/1000 | Loss: 0.00026603
Iteration 120/1000 | Loss: 0.00026811
Iteration 121/1000 | Loss: 0.00019266
Iteration 122/1000 | Loss: 0.00023571
Iteration 123/1000 | Loss: 0.00046052
Iteration 124/1000 | Loss: 0.00010924
Iteration 125/1000 | Loss: 0.00032527
Iteration 126/1000 | Loss: 0.00010896
Iteration 127/1000 | Loss: 0.00020454
Iteration 128/1000 | Loss: 0.00009741
Iteration 129/1000 | Loss: 0.00009755
Iteration 130/1000 | Loss: 0.00034024
Iteration 131/1000 | Loss: 0.00037064
Iteration 132/1000 | Loss: 0.00026638
Iteration 133/1000 | Loss: 0.00039733
Iteration 134/1000 | Loss: 0.00023779
Iteration 135/1000 | Loss: 0.00010180
Iteration 136/1000 | Loss: 0.00009070
Iteration 137/1000 | Loss: 0.00016753
Iteration 138/1000 | Loss: 0.00052927
Iteration 139/1000 | Loss: 0.00025768
Iteration 140/1000 | Loss: 0.00032326
Iteration 141/1000 | Loss: 0.00024441
Iteration 142/1000 | Loss: 0.00034287
Iteration 143/1000 | Loss: 0.00046648
Iteration 144/1000 | Loss: 0.00037480
Iteration 145/1000 | Loss: 0.00039336
Iteration 146/1000 | Loss: 0.00021235
Iteration 147/1000 | Loss: 0.00018796
Iteration 148/1000 | Loss: 0.00016011
Iteration 149/1000 | Loss: 0.00010663
Iteration 150/1000 | Loss: 0.00014472
Iteration 151/1000 | Loss: 0.00020857
Iteration 152/1000 | Loss: 0.00012468
Iteration 153/1000 | Loss: 0.00011023
Iteration 154/1000 | Loss: 0.00051316
Iteration 155/1000 | Loss: 0.00050025
Iteration 156/1000 | Loss: 0.00041763
Iteration 157/1000 | Loss: 0.00037973
Iteration 158/1000 | Loss: 0.00030913
Iteration 159/1000 | Loss: 0.00021085
Iteration 160/1000 | Loss: 0.00028453
Iteration 161/1000 | Loss: 0.00040936
Iteration 162/1000 | Loss: 0.00022786
Iteration 163/1000 | Loss: 0.00045660
Iteration 164/1000 | Loss: 0.00021473
Iteration 165/1000 | Loss: 0.00047275
Iteration 166/1000 | Loss: 0.00021492
Iteration 167/1000 | Loss: 0.00047991
Iteration 168/1000 | Loss: 0.00017626
Iteration 169/1000 | Loss: 0.00010733
Iteration 170/1000 | Loss: 0.00010169
Iteration 171/1000 | Loss: 0.00008847
Iteration 172/1000 | Loss: 0.00028447
Iteration 173/1000 | Loss: 0.00021645
Iteration 174/1000 | Loss: 0.00036164
Iteration 175/1000 | Loss: 0.00030336
Iteration 176/1000 | Loss: 0.00021339
Iteration 177/1000 | Loss: 0.00024707
Iteration 178/1000 | Loss: 0.00044156
Iteration 179/1000 | Loss: 0.00061646
Iteration 180/1000 | Loss: 0.00068890
Iteration 181/1000 | Loss: 0.00009079
Iteration 182/1000 | Loss: 0.00008213
Iteration 183/1000 | Loss: 0.00007562
Iteration 184/1000 | Loss: 0.00007336
Iteration 185/1000 | Loss: 0.00007215
Iteration 186/1000 | Loss: 0.00007107
Iteration 187/1000 | Loss: 0.00007015
Iteration 188/1000 | Loss: 0.00006958
Iteration 189/1000 | Loss: 0.00006917
Iteration 190/1000 | Loss: 0.00006871
Iteration 191/1000 | Loss: 0.00006833
Iteration 192/1000 | Loss: 0.00006802
Iteration 193/1000 | Loss: 0.00006782
Iteration 194/1000 | Loss: 0.00006770
Iteration 195/1000 | Loss: 0.00006757
Iteration 196/1000 | Loss: 0.00006753
Iteration 197/1000 | Loss: 0.00006753
Iteration 198/1000 | Loss: 0.00006753
Iteration 199/1000 | Loss: 0.00006752
Iteration 200/1000 | Loss: 0.00006752
Iteration 201/1000 | Loss: 0.00006751
Iteration 202/1000 | Loss: 0.00006750
Iteration 203/1000 | Loss: 0.00006749
Iteration 204/1000 | Loss: 0.00006749
Iteration 205/1000 | Loss: 0.00006748
Iteration 206/1000 | Loss: 0.00006748
Iteration 207/1000 | Loss: 0.00006747
Iteration 208/1000 | Loss: 0.00006747
Iteration 209/1000 | Loss: 0.00006747
Iteration 210/1000 | Loss: 0.00006746
Iteration 211/1000 | Loss: 0.00006746
Iteration 212/1000 | Loss: 0.00006745
Iteration 213/1000 | Loss: 0.00006745
Iteration 214/1000 | Loss: 0.00006744
Iteration 215/1000 | Loss: 0.00006743
Iteration 216/1000 | Loss: 0.00006743
Iteration 217/1000 | Loss: 0.00006743
Iteration 218/1000 | Loss: 0.00006743
Iteration 219/1000 | Loss: 0.00006743
Iteration 220/1000 | Loss: 0.00006743
Iteration 221/1000 | Loss: 0.00006743
Iteration 222/1000 | Loss: 0.00006742
Iteration 223/1000 | Loss: 0.00006740
Iteration 224/1000 | Loss: 0.00006736
Iteration 225/1000 | Loss: 0.00006736
Iteration 226/1000 | Loss: 0.00006736
Iteration 227/1000 | Loss: 0.00006736
Iteration 228/1000 | Loss: 0.00006736
Iteration 229/1000 | Loss: 0.00006736
Iteration 230/1000 | Loss: 0.00006735
Iteration 231/1000 | Loss: 0.00006735
Iteration 232/1000 | Loss: 0.00006735
Iteration 233/1000 | Loss: 0.00006735
Iteration 234/1000 | Loss: 0.00006735
Iteration 235/1000 | Loss: 0.00006735
Iteration 236/1000 | Loss: 0.00006735
Iteration 237/1000 | Loss: 0.00006735
Iteration 238/1000 | Loss: 0.00006734
Iteration 239/1000 | Loss: 0.00006734
Iteration 240/1000 | Loss: 0.00006734
Iteration 241/1000 | Loss: 0.00006733
Iteration 242/1000 | Loss: 0.00006733
Iteration 243/1000 | Loss: 0.00006733
Iteration 244/1000 | Loss: 0.00006733
Iteration 245/1000 | Loss: 0.00006733
Iteration 246/1000 | Loss: 0.00006733
Iteration 247/1000 | Loss: 0.00006733
Iteration 248/1000 | Loss: 0.00006733
Iteration 249/1000 | Loss: 0.00006733
Iteration 250/1000 | Loss: 0.00006732
Iteration 251/1000 | Loss: 0.00006732
Iteration 252/1000 | Loss: 0.00006732
Iteration 253/1000 | Loss: 0.00006732
Iteration 254/1000 | Loss: 0.00006732
Iteration 255/1000 | Loss: 0.00006731
Iteration 256/1000 | Loss: 0.00006731
Iteration 257/1000 | Loss: 0.00006731
Iteration 258/1000 | Loss: 0.00006730
Iteration 259/1000 | Loss: 0.00006730
Iteration 260/1000 | Loss: 0.00006730
Iteration 261/1000 | Loss: 0.00006730
Iteration 262/1000 | Loss: 0.00006730
Iteration 263/1000 | Loss: 0.00006730
Iteration 264/1000 | Loss: 0.00006729
Iteration 265/1000 | Loss: 0.00006729
Iteration 266/1000 | Loss: 0.00006729
Iteration 267/1000 | Loss: 0.00006729
Iteration 268/1000 | Loss: 0.00006729
Iteration 269/1000 | Loss: 0.00006729
Iteration 270/1000 | Loss: 0.00006729
Iteration 271/1000 | Loss: 0.00006729
Iteration 272/1000 | Loss: 0.00006729
Iteration 273/1000 | Loss: 0.00006729
Iteration 274/1000 | Loss: 0.00006729
Iteration 275/1000 | Loss: 0.00006728
Iteration 276/1000 | Loss: 0.00006728
Iteration 277/1000 | Loss: 0.00006728
Iteration 278/1000 | Loss: 0.00006728
Iteration 279/1000 | Loss: 0.00006728
Iteration 280/1000 | Loss: 0.00006728
Iteration 281/1000 | Loss: 0.00006728
Iteration 282/1000 | Loss: 0.00006728
Iteration 283/1000 | Loss: 0.00006728
Iteration 284/1000 | Loss: 0.00006728
Iteration 285/1000 | Loss: 0.00006728
Iteration 286/1000 | Loss: 0.00006728
Iteration 287/1000 | Loss: 0.00006728
Iteration 288/1000 | Loss: 0.00006728
Iteration 289/1000 | Loss: 0.00006728
Iteration 290/1000 | Loss: 0.00006727
Iteration 291/1000 | Loss: 0.00006727
Iteration 292/1000 | Loss: 0.00006727
Iteration 293/1000 | Loss: 0.00006727
Iteration 294/1000 | Loss: 0.00006727
Iteration 295/1000 | Loss: 0.00006727
Iteration 296/1000 | Loss: 0.00006727
Iteration 297/1000 | Loss: 0.00006727
Iteration 298/1000 | Loss: 0.00006727
Iteration 299/1000 | Loss: 0.00006727
Iteration 300/1000 | Loss: 0.00006727
Iteration 301/1000 | Loss: 0.00006727
Iteration 302/1000 | Loss: 0.00006727
Iteration 303/1000 | Loss: 0.00006727
Iteration 304/1000 | Loss: 0.00006726
Iteration 305/1000 | Loss: 0.00006726
Iteration 306/1000 | Loss: 0.00006726
Iteration 307/1000 | Loss: 0.00006726
Iteration 308/1000 | Loss: 0.00006726
Iteration 309/1000 | Loss: 0.00006726
Iteration 310/1000 | Loss: 0.00006726
Iteration 311/1000 | Loss: 0.00006726
Iteration 312/1000 | Loss: 0.00006726
Iteration 313/1000 | Loss: 0.00006726
Iteration 314/1000 | Loss: 0.00006726
Iteration 315/1000 | Loss: 0.00006726
Iteration 316/1000 | Loss: 0.00006725
Iteration 317/1000 | Loss: 0.00006725
Iteration 318/1000 | Loss: 0.00006725
Iteration 319/1000 | Loss: 0.00006724
Iteration 320/1000 | Loss: 0.00006724
Iteration 321/1000 | Loss: 0.00006724
Iteration 322/1000 | Loss: 0.00006724
Iteration 323/1000 | Loss: 0.00006724
Iteration 324/1000 | Loss: 0.00006724
Iteration 325/1000 | Loss: 0.00006724
Iteration 326/1000 | Loss: 0.00006724
Iteration 327/1000 | Loss: 0.00006724
Iteration 328/1000 | Loss: 0.00006724
Iteration 329/1000 | Loss: 0.00006724
Iteration 330/1000 | Loss: 0.00006724
Iteration 331/1000 | Loss: 0.00006723
Iteration 332/1000 | Loss: 0.00006723
Iteration 333/1000 | Loss: 0.00006723
Iteration 334/1000 | Loss: 0.00006723
Iteration 335/1000 | Loss: 0.00006723
Iteration 336/1000 | Loss: 0.00006723
Iteration 337/1000 | Loss: 0.00006723
Iteration 338/1000 | Loss: 0.00006723
Iteration 339/1000 | Loss: 0.00006723
Iteration 340/1000 | Loss: 0.00006723
Iteration 341/1000 | Loss: 0.00006723
Iteration 342/1000 | Loss: 0.00006723
Iteration 343/1000 | Loss: 0.00006723
Iteration 344/1000 | Loss: 0.00006722
Iteration 345/1000 | Loss: 0.00006722
Iteration 346/1000 | Loss: 0.00006722
Iteration 347/1000 | Loss: 0.00006722
Iteration 348/1000 | Loss: 0.00006722
Iteration 349/1000 | Loss: 0.00006722
Iteration 350/1000 | Loss: 0.00006722
Iteration 351/1000 | Loss: 0.00006722
Iteration 352/1000 | Loss: 0.00006722
Iteration 353/1000 | Loss: 0.00006722
Iteration 354/1000 | Loss: 0.00006722
Iteration 355/1000 | Loss: 0.00006722
Iteration 356/1000 | Loss: 0.00006722
Iteration 357/1000 | Loss: 0.00006722
Iteration 358/1000 | Loss: 0.00006722
Iteration 359/1000 | Loss: 0.00006722
Iteration 360/1000 | Loss: 0.00006722
Iteration 361/1000 | Loss: 0.00006721
Iteration 362/1000 | Loss: 0.00006721
Iteration 363/1000 | Loss: 0.00006721
Iteration 364/1000 | Loss: 0.00006721
Iteration 365/1000 | Loss: 0.00006721
Iteration 366/1000 | Loss: 0.00006721
Iteration 367/1000 | Loss: 0.00006721
Iteration 368/1000 | Loss: 0.00006721
Iteration 369/1000 | Loss: 0.00006721
Iteration 370/1000 | Loss: 0.00006721
Iteration 371/1000 | Loss: 0.00006721
Iteration 372/1000 | Loss: 0.00006720
Iteration 373/1000 | Loss: 0.00006720
Iteration 374/1000 | Loss: 0.00006720
Iteration 375/1000 | Loss: 0.00006720
Iteration 376/1000 | Loss: 0.00006720
Iteration 377/1000 | Loss: 0.00006720
Iteration 378/1000 | Loss: 0.00006720
Iteration 379/1000 | Loss: 0.00006720
Iteration 380/1000 | Loss: 0.00006720
Iteration 381/1000 | Loss: 0.00006720
Iteration 382/1000 | Loss: 0.00006720
Iteration 383/1000 | Loss: 0.00006720
Iteration 384/1000 | Loss: 0.00006720
Iteration 385/1000 | Loss: 0.00006720
Iteration 386/1000 | Loss: 0.00006720
Iteration 387/1000 | Loss: 0.00006720
Iteration 388/1000 | Loss: 0.00006720
Iteration 389/1000 | Loss: 0.00006720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 389. Stopping optimization.
Last 5 losses: [6.719961675116792e-05, 6.719961675116792e-05, 6.719961675116792e-05, 6.719961675116792e-05, 6.719961675116792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.719961675116792e-05

Optimization complete. Final v2v error: 6.067295551300049 mm

Highest mean error: 7.15108060836792 mm for frame 228

Lowest mean error: 4.029894828796387 mm for frame 59

Saving results

Total time: 386.6768488883972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388136
Iteration 2/25 | Loss: 0.00101551
Iteration 3/25 | Loss: 0.00089809
Iteration 4/25 | Loss: 0.00087313
Iteration 5/25 | Loss: 0.00086856
Iteration 6/25 | Loss: 0.00086744
Iteration 7/25 | Loss: 0.00086726
Iteration 8/25 | Loss: 0.00086726
Iteration 9/25 | Loss: 0.00086726
Iteration 10/25 | Loss: 0.00086726
Iteration 11/25 | Loss: 0.00086726
Iteration 12/25 | Loss: 0.00086726
Iteration 13/25 | Loss: 0.00086726
Iteration 14/25 | Loss: 0.00086726
Iteration 15/25 | Loss: 0.00086726
Iteration 16/25 | Loss: 0.00086726
Iteration 17/25 | Loss: 0.00086726
Iteration 18/25 | Loss: 0.00086726
Iteration 19/25 | Loss: 0.00086726
Iteration 20/25 | Loss: 0.00086726
Iteration 21/25 | Loss: 0.00086726
Iteration 22/25 | Loss: 0.00086726
Iteration 23/25 | Loss: 0.00086726
Iteration 24/25 | Loss: 0.00086726
Iteration 25/25 | Loss: 0.00086726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73796201
Iteration 2/25 | Loss: 0.00232729
Iteration 3/25 | Loss: 0.00232729
Iteration 4/25 | Loss: 0.00232729
Iteration 5/25 | Loss: 0.00232729
Iteration 6/25 | Loss: 0.00232728
Iteration 7/25 | Loss: 0.00232728
Iteration 8/25 | Loss: 0.00232728
Iteration 9/25 | Loss: 0.00232728
Iteration 10/25 | Loss: 0.00232728
Iteration 11/25 | Loss: 0.00232728
Iteration 12/25 | Loss: 0.00232728
Iteration 13/25 | Loss: 0.00232728
Iteration 14/25 | Loss: 0.00232728
Iteration 15/25 | Loss: 0.00232728
Iteration 16/25 | Loss: 0.00232728
Iteration 17/25 | Loss: 0.00232728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023272831458598375, 0.0023272831458598375, 0.0023272831458598375, 0.0023272831458598375, 0.0023272831458598375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023272831458598375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232728
Iteration 2/1000 | Loss: 0.00002979
Iteration 3/1000 | Loss: 0.00001671
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001280
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001217
Iteration 12/1000 | Loss: 0.00001214
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001212
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001199
Iteration 64/1000 | Loss: 0.00001199
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001197
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001196
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001196
Iteration 85/1000 | Loss: 0.00001196
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001195
Iteration 90/1000 | Loss: 0.00001195
Iteration 91/1000 | Loss: 0.00001195
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001194
Iteration 97/1000 | Loss: 0.00001194
Iteration 98/1000 | Loss: 0.00001194
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001193
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001192
Iteration 149/1000 | Loss: 0.00001192
Iteration 150/1000 | Loss: 0.00001192
Iteration 151/1000 | Loss: 0.00001192
Iteration 152/1000 | Loss: 0.00001192
Iteration 153/1000 | Loss: 0.00001192
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001192
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001192
Iteration 159/1000 | Loss: 0.00001192
Iteration 160/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.1917271876882296e-05, 1.1917271876882296e-05, 1.1917271876882296e-05, 1.1917271876882296e-05, 1.1917271876882296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1917271876882296e-05

Optimization complete. Final v2v error: 2.995863437652588 mm

Highest mean error: 3.309943437576294 mm for frame 111

Lowest mean error: 2.776940107345581 mm for frame 26

Saving results

Total time: 31.62211561203003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425543
Iteration 2/25 | Loss: 0.00113407
Iteration 3/25 | Loss: 0.00096558
Iteration 4/25 | Loss: 0.00094764
Iteration 5/25 | Loss: 0.00093814
Iteration 6/25 | Loss: 0.00093601
Iteration 7/25 | Loss: 0.00093601
Iteration 8/25 | Loss: 0.00093601
Iteration 9/25 | Loss: 0.00093601
Iteration 10/25 | Loss: 0.00093601
Iteration 11/25 | Loss: 0.00093601
Iteration 12/25 | Loss: 0.00093601
Iteration 13/25 | Loss: 0.00093601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009360142285004258, 0.0009360142285004258, 0.0009360142285004258, 0.0009360142285004258, 0.0009360142285004258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009360142285004258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88608420
Iteration 2/25 | Loss: 0.00268349
Iteration 3/25 | Loss: 0.00268349
Iteration 4/25 | Loss: 0.00268349
Iteration 5/25 | Loss: 0.00268349
Iteration 6/25 | Loss: 0.00268348
Iteration 7/25 | Loss: 0.00268348
Iteration 8/25 | Loss: 0.00268348
Iteration 9/25 | Loss: 0.00268348
Iteration 10/25 | Loss: 0.00268348
Iteration 11/25 | Loss: 0.00268348
Iteration 12/25 | Loss: 0.00268348
Iteration 13/25 | Loss: 0.00268348
Iteration 14/25 | Loss: 0.00268348
Iteration 15/25 | Loss: 0.00268348
Iteration 16/25 | Loss: 0.00268348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002683483762666583, 0.002683483762666583, 0.002683483762666583, 0.002683483762666583, 0.002683483762666583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002683483762666583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268348
Iteration 2/1000 | Loss: 0.00003579
Iteration 3/1000 | Loss: 0.00002561
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001691
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001564
Iteration 8/1000 | Loss: 0.00001536
Iteration 9/1000 | Loss: 0.00001527
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001486
Iteration 14/1000 | Loss: 0.00001470
Iteration 15/1000 | Loss: 0.00001466
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001463
Iteration 18/1000 | Loss: 0.00001463
Iteration 19/1000 | Loss: 0.00001462
Iteration 20/1000 | Loss: 0.00001462
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001454
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001451
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001450
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001449
Iteration 37/1000 | Loss: 0.00001449
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001446
Iteration 41/1000 | Loss: 0.00001446
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001435
Iteration 87/1000 | Loss: 0.00001435
Iteration 88/1000 | Loss: 0.00001435
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001434
Iteration 94/1000 | Loss: 0.00001434
Iteration 95/1000 | Loss: 0.00001434
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001433
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001433
Iteration 107/1000 | Loss: 0.00001433
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001431
Iteration 119/1000 | Loss: 0.00001431
Iteration 120/1000 | Loss: 0.00001431
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001429
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001428
Iteration 135/1000 | Loss: 0.00001428
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001428
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001427
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001426
Iteration 158/1000 | Loss: 0.00001426
Iteration 159/1000 | Loss: 0.00001426
Iteration 160/1000 | Loss: 0.00001426
Iteration 161/1000 | Loss: 0.00001426
Iteration 162/1000 | Loss: 0.00001426
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001426
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.4262443073675968e-05, 1.4262443073675968e-05, 1.4262443073675968e-05, 1.4262443073675968e-05, 1.4262443073675968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4262443073675968e-05

Optimization complete. Final v2v error: 3.2814598083496094 mm

Highest mean error: 3.702018976211548 mm for frame 25

Lowest mean error: 2.966829538345337 mm for frame 205

Saving results

Total time: 43.697261810302734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850111
Iteration 2/25 | Loss: 0.00105570
Iteration 3/25 | Loss: 0.00091742
Iteration 4/25 | Loss: 0.00090181
Iteration 5/25 | Loss: 0.00089652
Iteration 6/25 | Loss: 0.00089520
Iteration 7/25 | Loss: 0.00089520
Iteration 8/25 | Loss: 0.00089520
Iteration 9/25 | Loss: 0.00089520
Iteration 10/25 | Loss: 0.00089520
Iteration 11/25 | Loss: 0.00089520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008951962227001786, 0.0008951962227001786, 0.0008951962227001786, 0.0008951962227001786, 0.0008951962227001786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008951962227001786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63026154
Iteration 2/25 | Loss: 0.00218129
Iteration 3/25 | Loss: 0.00218128
Iteration 4/25 | Loss: 0.00218128
Iteration 5/25 | Loss: 0.00218128
Iteration 6/25 | Loss: 0.00218128
Iteration 7/25 | Loss: 0.00218128
Iteration 8/25 | Loss: 0.00218128
Iteration 9/25 | Loss: 0.00218128
Iteration 10/25 | Loss: 0.00218128
Iteration 11/25 | Loss: 0.00218128
Iteration 12/25 | Loss: 0.00218128
Iteration 13/25 | Loss: 0.00218128
Iteration 14/25 | Loss: 0.00218128
Iteration 15/25 | Loss: 0.00218128
Iteration 16/25 | Loss: 0.00218128
Iteration 17/25 | Loss: 0.00218128
Iteration 18/25 | Loss: 0.00218128
Iteration 19/25 | Loss: 0.00218128
Iteration 20/25 | Loss: 0.00218128
Iteration 21/25 | Loss: 0.00218128
Iteration 22/25 | Loss: 0.00218128
Iteration 23/25 | Loss: 0.00218128
Iteration 24/25 | Loss: 0.00218128
Iteration 25/25 | Loss: 0.00218128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0021812799386680126, 0.0021812799386680126, 0.0021812799386680126, 0.0021812799386680126, 0.0021812799386680126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021812799386680126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218128
Iteration 2/1000 | Loss: 0.00003811
Iteration 3/1000 | Loss: 0.00002228
Iteration 4/1000 | Loss: 0.00001851
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001523
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001477
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001475
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001464
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001461
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001460
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001457
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001456
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001455
Iteration 48/1000 | Loss: 0.00001455
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001454
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001454
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001444
Iteration 107/1000 | Loss: 0.00001444
Iteration 108/1000 | Loss: 0.00001444
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001443
Iteration 114/1000 | Loss: 0.00001443
Iteration 115/1000 | Loss: 0.00001443
Iteration 116/1000 | Loss: 0.00001443
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001443
Iteration 120/1000 | Loss: 0.00001443
Iteration 121/1000 | Loss: 0.00001443
Iteration 122/1000 | Loss: 0.00001443
Iteration 123/1000 | Loss: 0.00001443
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001443
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001442
Iteration 128/1000 | Loss: 0.00001442
Iteration 129/1000 | Loss: 0.00001442
Iteration 130/1000 | Loss: 0.00001442
Iteration 131/1000 | Loss: 0.00001442
Iteration 132/1000 | Loss: 0.00001442
Iteration 133/1000 | Loss: 0.00001442
Iteration 134/1000 | Loss: 0.00001442
Iteration 135/1000 | Loss: 0.00001442
Iteration 136/1000 | Loss: 0.00001441
Iteration 137/1000 | Loss: 0.00001441
Iteration 138/1000 | Loss: 0.00001441
Iteration 139/1000 | Loss: 0.00001441
Iteration 140/1000 | Loss: 0.00001441
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001440
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001440
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001440
Iteration 159/1000 | Loss: 0.00001440
Iteration 160/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.440365031157853e-05, 1.440365031157853e-05, 1.440365031157853e-05, 1.440365031157853e-05, 1.440365031157853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.440365031157853e-05

Optimization complete. Final v2v error: 3.2424542903900146 mm

Highest mean error: 3.4827542304992676 mm for frame 239

Lowest mean error: 3.0292153358459473 mm for frame 5

Saving results

Total time: 41.9022216796875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430690
Iteration 2/25 | Loss: 0.00111229
Iteration 3/25 | Loss: 0.00096607
Iteration 4/25 | Loss: 0.00094088
Iteration 5/25 | Loss: 0.00093361
Iteration 6/25 | Loss: 0.00093155
Iteration 7/25 | Loss: 0.00093128
Iteration 8/25 | Loss: 0.00093128
Iteration 9/25 | Loss: 0.00093128
Iteration 10/25 | Loss: 0.00093128
Iteration 11/25 | Loss: 0.00093128
Iteration 12/25 | Loss: 0.00093127
Iteration 13/25 | Loss: 0.00093127
Iteration 14/25 | Loss: 0.00093127
Iteration 15/25 | Loss: 0.00093127
Iteration 16/25 | Loss: 0.00093127
Iteration 17/25 | Loss: 0.00093127
Iteration 18/25 | Loss: 0.00093127
Iteration 19/25 | Loss: 0.00093127
Iteration 20/25 | Loss: 0.00093127
Iteration 21/25 | Loss: 0.00093127
Iteration 22/25 | Loss: 0.00093127
Iteration 23/25 | Loss: 0.00093127
Iteration 24/25 | Loss: 0.00093127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009312666370533407, 0.0009312666370533407, 0.0009312666370533407, 0.0009312666370533407, 0.0009312666370533407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009312666370533407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62624502
Iteration 2/25 | Loss: 0.00233959
Iteration 3/25 | Loss: 0.00233958
Iteration 4/25 | Loss: 0.00233958
Iteration 5/25 | Loss: 0.00233958
Iteration 6/25 | Loss: 0.00233958
Iteration 7/25 | Loss: 0.00233958
Iteration 8/25 | Loss: 0.00233958
Iteration 9/25 | Loss: 0.00233958
Iteration 10/25 | Loss: 0.00233958
Iteration 11/25 | Loss: 0.00233958
Iteration 12/25 | Loss: 0.00233958
Iteration 13/25 | Loss: 0.00233958
Iteration 14/25 | Loss: 0.00233958
Iteration 15/25 | Loss: 0.00233958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023395814932882786, 0.0023395814932882786, 0.0023395814932882786, 0.0023395814932882786, 0.0023395814932882786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023395814932882786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233958
Iteration 2/1000 | Loss: 0.00003762
Iteration 3/1000 | Loss: 0.00002478
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00001955
Iteration 6/1000 | Loss: 0.00001842
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001673
Iteration 17/1000 | Loss: 0.00001666
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001660
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001659
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001658
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001658
Iteration 45/1000 | Loss: 0.00001658
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001657
Iteration 49/1000 | Loss: 0.00001657
Iteration 50/1000 | Loss: 0.00001657
Iteration 51/1000 | Loss: 0.00001657
Iteration 52/1000 | Loss: 0.00001657
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001656
Iteration 56/1000 | Loss: 0.00001656
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001652
Iteration 84/1000 | Loss: 0.00001652
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00001652
Iteration 87/1000 | Loss: 0.00001652
Iteration 88/1000 | Loss: 0.00001652
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001652
Iteration 92/1000 | Loss: 0.00001652
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001652
Iteration 96/1000 | Loss: 0.00001652
Iteration 97/1000 | Loss: 0.00001651
Iteration 98/1000 | Loss: 0.00001651
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00001651
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001651
Iteration 108/1000 | Loss: 0.00001651
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001651
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001651
Iteration 124/1000 | Loss: 0.00001651
Iteration 125/1000 | Loss: 0.00001651
Iteration 126/1000 | Loss: 0.00001651
Iteration 127/1000 | Loss: 0.00001651
Iteration 128/1000 | Loss: 0.00001651
Iteration 129/1000 | Loss: 0.00001651
Iteration 130/1000 | Loss: 0.00001651
Iteration 131/1000 | Loss: 0.00001651
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001651
Iteration 134/1000 | Loss: 0.00001651
Iteration 135/1000 | Loss: 0.00001651
Iteration 136/1000 | Loss: 0.00001651
Iteration 137/1000 | Loss: 0.00001651
Iteration 138/1000 | Loss: 0.00001651
Iteration 139/1000 | Loss: 0.00001651
Iteration 140/1000 | Loss: 0.00001651
Iteration 141/1000 | Loss: 0.00001651
Iteration 142/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.6509178749402054e-05, 1.6509178749402054e-05, 1.6509178749402054e-05, 1.6509178749402054e-05, 1.6509178749402054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6509178749402054e-05

Optimization complete. Final v2v error: 3.393608570098877 mm

Highest mean error: 3.764352798461914 mm for frame 74

Lowest mean error: 2.967656135559082 mm for frame 24

Saving results

Total time: 34.846062660217285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431698
Iteration 2/25 | Loss: 0.00112611
Iteration 3/25 | Loss: 0.00098830
Iteration 4/25 | Loss: 0.00097153
Iteration 5/25 | Loss: 0.00096624
Iteration 6/25 | Loss: 0.00096499
Iteration 7/25 | Loss: 0.00096465
Iteration 8/25 | Loss: 0.00096465
Iteration 9/25 | Loss: 0.00096465
Iteration 10/25 | Loss: 0.00096465
Iteration 11/25 | Loss: 0.00096465
Iteration 12/25 | Loss: 0.00096465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009646532125771046, 0.0009646532125771046, 0.0009646532125771046, 0.0009646532125771046, 0.0009646532125771046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009646532125771046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64480615
Iteration 2/25 | Loss: 0.00269551
Iteration 3/25 | Loss: 0.00269551
Iteration 4/25 | Loss: 0.00269551
Iteration 5/25 | Loss: 0.00269551
Iteration 6/25 | Loss: 0.00269551
Iteration 7/25 | Loss: 0.00269551
Iteration 8/25 | Loss: 0.00269551
Iteration 9/25 | Loss: 0.00269551
Iteration 10/25 | Loss: 0.00269551
Iteration 11/25 | Loss: 0.00269551
Iteration 12/25 | Loss: 0.00269551
Iteration 13/25 | Loss: 0.00269551
Iteration 14/25 | Loss: 0.00269551
Iteration 15/25 | Loss: 0.00269551
Iteration 16/25 | Loss: 0.00269551
Iteration 17/25 | Loss: 0.00269551
Iteration 18/25 | Loss: 0.00269551
Iteration 19/25 | Loss: 0.00269551
Iteration 20/25 | Loss: 0.00269551
Iteration 21/25 | Loss: 0.00269551
Iteration 22/25 | Loss: 0.00269551
Iteration 23/25 | Loss: 0.00269551
Iteration 24/25 | Loss: 0.00269551
Iteration 25/25 | Loss: 0.00269551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269551
Iteration 2/1000 | Loss: 0.00005517
Iteration 3/1000 | Loss: 0.00003824
Iteration 4/1000 | Loss: 0.00002953
Iteration 5/1000 | Loss: 0.00002581
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002199
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002067
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00001997
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001979
Iteration 15/1000 | Loss: 0.00001966
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001958
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001950
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001947
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001946
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001944
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001943
Iteration 42/1000 | Loss: 0.00001942
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001940
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001933
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001931
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001924
Iteration 114/1000 | Loss: 0.00001923
Iteration 115/1000 | Loss: 0.00001923
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001923
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001922
Iteration 122/1000 | Loss: 0.00001922
Iteration 123/1000 | Loss: 0.00001922
Iteration 124/1000 | Loss: 0.00001922
Iteration 125/1000 | Loss: 0.00001922
Iteration 126/1000 | Loss: 0.00001922
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001921
Iteration 130/1000 | Loss: 0.00001921
Iteration 131/1000 | Loss: 0.00001921
Iteration 132/1000 | Loss: 0.00001921
Iteration 133/1000 | Loss: 0.00001921
Iteration 134/1000 | Loss: 0.00001921
Iteration 135/1000 | Loss: 0.00001921
Iteration 136/1000 | Loss: 0.00001921
Iteration 137/1000 | Loss: 0.00001920
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001920
Iteration 140/1000 | Loss: 0.00001920
Iteration 141/1000 | Loss: 0.00001920
Iteration 142/1000 | Loss: 0.00001920
Iteration 143/1000 | Loss: 0.00001920
Iteration 144/1000 | Loss: 0.00001920
Iteration 145/1000 | Loss: 0.00001920
Iteration 146/1000 | Loss: 0.00001920
Iteration 147/1000 | Loss: 0.00001920
Iteration 148/1000 | Loss: 0.00001920
Iteration 149/1000 | Loss: 0.00001920
Iteration 150/1000 | Loss: 0.00001920
Iteration 151/1000 | Loss: 0.00001919
Iteration 152/1000 | Loss: 0.00001919
Iteration 153/1000 | Loss: 0.00001919
Iteration 154/1000 | Loss: 0.00001919
Iteration 155/1000 | Loss: 0.00001919
Iteration 156/1000 | Loss: 0.00001919
Iteration 157/1000 | Loss: 0.00001919
Iteration 158/1000 | Loss: 0.00001919
Iteration 159/1000 | Loss: 0.00001919
Iteration 160/1000 | Loss: 0.00001919
Iteration 161/1000 | Loss: 0.00001919
Iteration 162/1000 | Loss: 0.00001919
Iteration 163/1000 | Loss: 0.00001919
Iteration 164/1000 | Loss: 0.00001919
Iteration 165/1000 | Loss: 0.00001919
Iteration 166/1000 | Loss: 0.00001919
Iteration 167/1000 | Loss: 0.00001919
Iteration 168/1000 | Loss: 0.00001919
Iteration 169/1000 | Loss: 0.00001919
Iteration 170/1000 | Loss: 0.00001919
Iteration 171/1000 | Loss: 0.00001919
Iteration 172/1000 | Loss: 0.00001919
Iteration 173/1000 | Loss: 0.00001919
Iteration 174/1000 | Loss: 0.00001919
Iteration 175/1000 | Loss: 0.00001918
Iteration 176/1000 | Loss: 0.00001918
Iteration 177/1000 | Loss: 0.00001918
Iteration 178/1000 | Loss: 0.00001918
Iteration 179/1000 | Loss: 0.00001918
Iteration 180/1000 | Loss: 0.00001918
Iteration 181/1000 | Loss: 0.00001918
Iteration 182/1000 | Loss: 0.00001918
Iteration 183/1000 | Loss: 0.00001918
Iteration 184/1000 | Loss: 0.00001918
Iteration 185/1000 | Loss: 0.00001918
Iteration 186/1000 | Loss: 0.00001918
Iteration 187/1000 | Loss: 0.00001918
Iteration 188/1000 | Loss: 0.00001918
Iteration 189/1000 | Loss: 0.00001918
Iteration 190/1000 | Loss: 0.00001918
Iteration 191/1000 | Loss: 0.00001918
Iteration 192/1000 | Loss: 0.00001918
Iteration 193/1000 | Loss: 0.00001918
Iteration 194/1000 | Loss: 0.00001918
Iteration 195/1000 | Loss: 0.00001918
Iteration 196/1000 | Loss: 0.00001918
Iteration 197/1000 | Loss: 0.00001918
Iteration 198/1000 | Loss: 0.00001918
Iteration 199/1000 | Loss: 0.00001918
Iteration 200/1000 | Loss: 0.00001918
Iteration 201/1000 | Loss: 0.00001918
Iteration 202/1000 | Loss: 0.00001918
Iteration 203/1000 | Loss: 0.00001918
Iteration 204/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.9180220988346264e-05, 1.9180220988346264e-05, 1.9180220988346264e-05, 1.9180220988346264e-05, 1.9180220988346264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9180220988346264e-05

Optimization complete. Final v2v error: 3.747077703475952 mm

Highest mean error: 4.214009761810303 mm for frame 112

Lowest mean error: 3.341508150100708 mm for frame 34

Saving results

Total time: 42.64303708076477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916790
Iteration 2/25 | Loss: 0.00108161
Iteration 3/25 | Loss: 0.00097287
Iteration 4/25 | Loss: 0.00093390
Iteration 5/25 | Loss: 0.00092821
Iteration 6/25 | Loss: 0.00092680
Iteration 7/25 | Loss: 0.00092672
Iteration 8/25 | Loss: 0.00092672
Iteration 9/25 | Loss: 0.00092672
Iteration 10/25 | Loss: 0.00092672
Iteration 11/25 | Loss: 0.00092672
Iteration 12/25 | Loss: 0.00092672
Iteration 13/25 | Loss: 0.00092672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000926722539588809, 0.000926722539588809, 0.000926722539588809, 0.000926722539588809, 0.000926722539588809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000926722539588809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62495410
Iteration 2/25 | Loss: 0.00200462
Iteration 3/25 | Loss: 0.00200461
Iteration 4/25 | Loss: 0.00200461
Iteration 5/25 | Loss: 0.00200461
Iteration 6/25 | Loss: 0.00200461
Iteration 7/25 | Loss: 0.00200461
Iteration 8/25 | Loss: 0.00200461
Iteration 9/25 | Loss: 0.00200461
Iteration 10/25 | Loss: 0.00200461
Iteration 11/25 | Loss: 0.00200461
Iteration 12/25 | Loss: 0.00200461
Iteration 13/25 | Loss: 0.00200461
Iteration 14/25 | Loss: 0.00200461
Iteration 15/25 | Loss: 0.00200461
Iteration 16/25 | Loss: 0.00200461
Iteration 17/25 | Loss: 0.00200461
Iteration 18/25 | Loss: 0.00200461
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020046106074005365, 0.0020046106074005365, 0.0020046106074005365, 0.0020046106074005365, 0.0020046106074005365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020046106074005365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200461
Iteration 2/1000 | Loss: 0.00004485
Iteration 3/1000 | Loss: 0.00003119
Iteration 4/1000 | Loss: 0.00002613
Iteration 5/1000 | Loss: 0.00002447
Iteration 6/1000 | Loss: 0.00002290
Iteration 7/1000 | Loss: 0.00002209
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002083
Iteration 11/1000 | Loss: 0.00002067
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002060
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00002054
Iteration 18/1000 | Loss: 0.00002054
Iteration 19/1000 | Loss: 0.00002053
Iteration 20/1000 | Loss: 0.00002053
Iteration 21/1000 | Loss: 0.00002050
Iteration 22/1000 | Loss: 0.00002050
Iteration 23/1000 | Loss: 0.00002050
Iteration 24/1000 | Loss: 0.00002050
Iteration 25/1000 | Loss: 0.00002050
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002048
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002047
Iteration 33/1000 | Loss: 0.00002047
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002046
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002045
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002044
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002042
Iteration 56/1000 | Loss: 0.00002042
Iteration 57/1000 | Loss: 0.00002042
Iteration 58/1000 | Loss: 0.00002042
Iteration 59/1000 | Loss: 0.00002042
Iteration 60/1000 | Loss: 0.00002042
Iteration 61/1000 | Loss: 0.00002042
Iteration 62/1000 | Loss: 0.00002042
Iteration 63/1000 | Loss: 0.00002042
Iteration 64/1000 | Loss: 0.00002042
Iteration 65/1000 | Loss: 0.00002042
Iteration 66/1000 | Loss: 0.00002042
Iteration 67/1000 | Loss: 0.00002042
Iteration 68/1000 | Loss: 0.00002042
Iteration 69/1000 | Loss: 0.00002042
Iteration 70/1000 | Loss: 0.00002042
Iteration 71/1000 | Loss: 0.00002042
Iteration 72/1000 | Loss: 0.00002042
Iteration 73/1000 | Loss: 0.00002042
Iteration 74/1000 | Loss: 0.00002042
Iteration 75/1000 | Loss: 0.00002042
Iteration 76/1000 | Loss: 0.00002042
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002042
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002042
Iteration 84/1000 | Loss: 0.00002042
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002042
Iteration 87/1000 | Loss: 0.00002042
Iteration 88/1000 | Loss: 0.00002042
Iteration 89/1000 | Loss: 0.00002042
Iteration 90/1000 | Loss: 0.00002042
Iteration 91/1000 | Loss: 0.00002042
Iteration 92/1000 | Loss: 0.00002042
Iteration 93/1000 | Loss: 0.00002042
Iteration 94/1000 | Loss: 0.00002042
Iteration 95/1000 | Loss: 0.00002042
Iteration 96/1000 | Loss: 0.00002042
Iteration 97/1000 | Loss: 0.00002042
Iteration 98/1000 | Loss: 0.00002042
Iteration 99/1000 | Loss: 0.00002042
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002042
Iteration 112/1000 | Loss: 0.00002042
Iteration 113/1000 | Loss: 0.00002042
Iteration 114/1000 | Loss: 0.00002042
Iteration 115/1000 | Loss: 0.00002042
Iteration 116/1000 | Loss: 0.00002042
Iteration 117/1000 | Loss: 0.00002042
Iteration 118/1000 | Loss: 0.00002042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.0415060134837404e-05, 2.0415060134837404e-05, 2.0415060134837404e-05, 2.0415060134837404e-05, 2.0415060134837404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0415060134837404e-05

Optimization complete. Final v2v error: 3.79988431930542 mm

Highest mean error: 4.365273952484131 mm for frame 98

Lowest mean error: 3.48364520072937 mm for frame 51

Saving results

Total time: 31.738826990127563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039810
Iteration 2/25 | Loss: 0.00171429
Iteration 3/25 | Loss: 0.00118530
Iteration 4/25 | Loss: 0.00113725
Iteration 5/25 | Loss: 0.00111837
Iteration 6/25 | Loss: 0.00111339
Iteration 7/25 | Loss: 0.00111265
Iteration 8/25 | Loss: 0.00111265
Iteration 9/25 | Loss: 0.00111265
Iteration 10/25 | Loss: 0.00111265
Iteration 11/25 | Loss: 0.00111265
Iteration 12/25 | Loss: 0.00111265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011126549215987325, 0.0011126549215987325, 0.0011126549215987325, 0.0011126549215987325, 0.0011126549215987325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011126549215987325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97653753
Iteration 2/25 | Loss: 0.00159110
Iteration 3/25 | Loss: 0.00159110
Iteration 4/25 | Loss: 0.00159110
Iteration 5/25 | Loss: 0.00159110
Iteration 6/25 | Loss: 0.00159110
Iteration 7/25 | Loss: 0.00159110
Iteration 8/25 | Loss: 0.00159110
Iteration 9/25 | Loss: 0.00159110
Iteration 10/25 | Loss: 0.00159110
Iteration 11/25 | Loss: 0.00159110
Iteration 12/25 | Loss: 0.00159110
Iteration 13/25 | Loss: 0.00159110
Iteration 14/25 | Loss: 0.00159110
Iteration 15/25 | Loss: 0.00159110
Iteration 16/25 | Loss: 0.00159110
Iteration 17/25 | Loss: 0.00159110
Iteration 18/25 | Loss: 0.00159110
Iteration 19/25 | Loss: 0.00159110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015911005903035402, 0.0015911005903035402, 0.0015911005903035402, 0.0015911005903035402, 0.0015911005903035402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015911005903035402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159110
Iteration 2/1000 | Loss: 0.00008073
Iteration 3/1000 | Loss: 0.00005106
Iteration 4/1000 | Loss: 0.00004688
Iteration 5/1000 | Loss: 0.00004498
Iteration 6/1000 | Loss: 0.00004371
Iteration 7/1000 | Loss: 0.00004273
Iteration 8/1000 | Loss: 0.00004209
Iteration 9/1000 | Loss: 0.00004169
Iteration 10/1000 | Loss: 0.00004144
Iteration 11/1000 | Loss: 0.00004124
Iteration 12/1000 | Loss: 0.00004116
Iteration 13/1000 | Loss: 0.00004105
Iteration 14/1000 | Loss: 0.00004104
Iteration 15/1000 | Loss: 0.00004098
Iteration 16/1000 | Loss: 0.00004090
Iteration 17/1000 | Loss: 0.00004090
Iteration 18/1000 | Loss: 0.00004086
Iteration 19/1000 | Loss: 0.00004086
Iteration 20/1000 | Loss: 0.00004086
Iteration 21/1000 | Loss: 0.00004086
Iteration 22/1000 | Loss: 0.00004086
Iteration 23/1000 | Loss: 0.00004086
Iteration 24/1000 | Loss: 0.00004086
Iteration 25/1000 | Loss: 0.00004086
Iteration 26/1000 | Loss: 0.00004086
Iteration 27/1000 | Loss: 0.00004086
Iteration 28/1000 | Loss: 0.00004086
Iteration 29/1000 | Loss: 0.00004085
Iteration 30/1000 | Loss: 0.00004085
Iteration 31/1000 | Loss: 0.00004085
Iteration 32/1000 | Loss: 0.00004085
Iteration 33/1000 | Loss: 0.00004085
Iteration 34/1000 | Loss: 0.00004085
Iteration 35/1000 | Loss: 0.00004085
Iteration 36/1000 | Loss: 0.00004084
Iteration 37/1000 | Loss: 0.00004083
Iteration 38/1000 | Loss: 0.00004083
Iteration 39/1000 | Loss: 0.00004082
Iteration 40/1000 | Loss: 0.00004080
Iteration 41/1000 | Loss: 0.00004080
Iteration 42/1000 | Loss: 0.00004080
Iteration 43/1000 | Loss: 0.00004080
Iteration 44/1000 | Loss: 0.00004080
Iteration 45/1000 | Loss: 0.00004080
Iteration 46/1000 | Loss: 0.00004080
Iteration 47/1000 | Loss: 0.00004079
Iteration 48/1000 | Loss: 0.00004079
Iteration 49/1000 | Loss: 0.00004079
Iteration 50/1000 | Loss: 0.00004076
Iteration 51/1000 | Loss: 0.00004076
Iteration 52/1000 | Loss: 0.00004076
Iteration 53/1000 | Loss: 0.00004076
Iteration 54/1000 | Loss: 0.00004076
Iteration 55/1000 | Loss: 0.00004075
Iteration 56/1000 | Loss: 0.00004075
Iteration 57/1000 | Loss: 0.00004075
Iteration 58/1000 | Loss: 0.00004075
Iteration 59/1000 | Loss: 0.00004075
Iteration 60/1000 | Loss: 0.00004075
Iteration 61/1000 | Loss: 0.00004075
Iteration 62/1000 | Loss: 0.00004075
Iteration 63/1000 | Loss: 0.00004075
Iteration 64/1000 | Loss: 0.00004075
Iteration 65/1000 | Loss: 0.00004074
Iteration 66/1000 | Loss: 0.00004074
Iteration 67/1000 | Loss: 0.00004073
Iteration 68/1000 | Loss: 0.00004072
Iteration 69/1000 | Loss: 0.00004072
Iteration 70/1000 | Loss: 0.00004072
Iteration 71/1000 | Loss: 0.00004072
Iteration 72/1000 | Loss: 0.00004072
Iteration 73/1000 | Loss: 0.00004072
Iteration 74/1000 | Loss: 0.00004072
Iteration 75/1000 | Loss: 0.00004072
Iteration 76/1000 | Loss: 0.00004072
Iteration 77/1000 | Loss: 0.00004072
Iteration 78/1000 | Loss: 0.00004072
Iteration 79/1000 | Loss: 0.00004072
Iteration 80/1000 | Loss: 0.00004072
Iteration 81/1000 | Loss: 0.00004071
Iteration 82/1000 | Loss: 0.00004071
Iteration 83/1000 | Loss: 0.00004071
Iteration 84/1000 | Loss: 0.00004070
Iteration 85/1000 | Loss: 0.00004070
Iteration 86/1000 | Loss: 0.00004070
Iteration 87/1000 | Loss: 0.00004070
Iteration 88/1000 | Loss: 0.00004070
Iteration 89/1000 | Loss: 0.00004069
Iteration 90/1000 | Loss: 0.00004069
Iteration 91/1000 | Loss: 0.00004069
Iteration 92/1000 | Loss: 0.00004069
Iteration 93/1000 | Loss: 0.00004068
Iteration 94/1000 | Loss: 0.00004068
Iteration 95/1000 | Loss: 0.00004068
Iteration 96/1000 | Loss: 0.00004068
Iteration 97/1000 | Loss: 0.00004068
Iteration 98/1000 | Loss: 0.00004067
Iteration 99/1000 | Loss: 0.00004067
Iteration 100/1000 | Loss: 0.00004067
Iteration 101/1000 | Loss: 0.00004067
Iteration 102/1000 | Loss: 0.00004067
Iteration 103/1000 | Loss: 0.00004066
Iteration 104/1000 | Loss: 0.00004066
Iteration 105/1000 | Loss: 0.00004066
Iteration 106/1000 | Loss: 0.00004065
Iteration 107/1000 | Loss: 0.00004065
Iteration 108/1000 | Loss: 0.00004065
Iteration 109/1000 | Loss: 0.00004065
Iteration 110/1000 | Loss: 0.00004064
Iteration 111/1000 | Loss: 0.00004064
Iteration 112/1000 | Loss: 0.00004064
Iteration 113/1000 | Loss: 0.00004064
Iteration 114/1000 | Loss: 0.00004063
Iteration 115/1000 | Loss: 0.00004063
Iteration 116/1000 | Loss: 0.00004063
Iteration 117/1000 | Loss: 0.00004063
Iteration 118/1000 | Loss: 0.00004063
Iteration 119/1000 | Loss: 0.00004063
Iteration 120/1000 | Loss: 0.00004063
Iteration 121/1000 | Loss: 0.00004063
Iteration 122/1000 | Loss: 0.00004062
Iteration 123/1000 | Loss: 0.00004062
Iteration 124/1000 | Loss: 0.00004062
Iteration 125/1000 | Loss: 0.00004062
Iteration 126/1000 | Loss: 0.00004061
Iteration 127/1000 | Loss: 0.00004061
Iteration 128/1000 | Loss: 0.00004061
Iteration 129/1000 | Loss: 0.00004061
Iteration 130/1000 | Loss: 0.00004060
Iteration 131/1000 | Loss: 0.00004060
Iteration 132/1000 | Loss: 0.00004060
Iteration 133/1000 | Loss: 0.00004060
Iteration 134/1000 | Loss: 0.00004060
Iteration 135/1000 | Loss: 0.00004060
Iteration 136/1000 | Loss: 0.00004060
Iteration 137/1000 | Loss: 0.00004059
Iteration 138/1000 | Loss: 0.00004059
Iteration 139/1000 | Loss: 0.00004059
Iteration 140/1000 | Loss: 0.00004059
Iteration 141/1000 | Loss: 0.00004059
Iteration 142/1000 | Loss: 0.00004059
Iteration 143/1000 | Loss: 0.00004059
Iteration 144/1000 | Loss: 0.00004059
Iteration 145/1000 | Loss: 0.00004059
Iteration 146/1000 | Loss: 0.00004058
Iteration 147/1000 | Loss: 0.00004058
Iteration 148/1000 | Loss: 0.00004058
Iteration 149/1000 | Loss: 0.00004058
Iteration 150/1000 | Loss: 0.00004058
Iteration 151/1000 | Loss: 0.00004058
Iteration 152/1000 | Loss: 0.00004058
Iteration 153/1000 | Loss: 0.00004058
Iteration 154/1000 | Loss: 0.00004058
Iteration 155/1000 | Loss: 0.00004057
Iteration 156/1000 | Loss: 0.00004057
Iteration 157/1000 | Loss: 0.00004057
Iteration 158/1000 | Loss: 0.00004057
Iteration 159/1000 | Loss: 0.00004057
Iteration 160/1000 | Loss: 0.00004057
Iteration 161/1000 | Loss: 0.00004057
Iteration 162/1000 | Loss: 0.00004057
Iteration 163/1000 | Loss: 0.00004057
Iteration 164/1000 | Loss: 0.00004057
Iteration 165/1000 | Loss: 0.00004057
Iteration 166/1000 | Loss: 0.00004057
Iteration 167/1000 | Loss: 0.00004057
Iteration 168/1000 | Loss: 0.00004057
Iteration 169/1000 | Loss: 0.00004057
Iteration 170/1000 | Loss: 0.00004057
Iteration 171/1000 | Loss: 0.00004057
Iteration 172/1000 | Loss: 0.00004057
Iteration 173/1000 | Loss: 0.00004057
Iteration 174/1000 | Loss: 0.00004057
Iteration 175/1000 | Loss: 0.00004057
Iteration 176/1000 | Loss: 0.00004057
Iteration 177/1000 | Loss: 0.00004057
Iteration 178/1000 | Loss: 0.00004057
Iteration 179/1000 | Loss: 0.00004057
Iteration 180/1000 | Loss: 0.00004057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [4.057103069499135e-05, 4.057103069499135e-05, 4.057103069499135e-05, 4.057103069499135e-05, 4.057103069499135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.057103069499135e-05

Optimization complete. Final v2v error: 5.162671089172363 mm

Highest mean error: 6.380304336547852 mm for frame 89

Lowest mean error: 4.137352466583252 mm for frame 0

Saving results

Total time: 48.43461847305298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879113
Iteration 2/25 | Loss: 0.00120665
Iteration 3/25 | Loss: 0.00105076
Iteration 4/25 | Loss: 0.00100904
Iteration 5/25 | Loss: 0.00099605
Iteration 6/25 | Loss: 0.00099473
Iteration 7/25 | Loss: 0.00099473
Iteration 8/25 | Loss: 0.00099473
Iteration 9/25 | Loss: 0.00099473
Iteration 10/25 | Loss: 0.00099473
Iteration 11/25 | Loss: 0.00099473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00099473400041461, 0.00099473400041461, 0.00099473400041461, 0.00099473400041461, 0.00099473400041461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00099473400041461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60227346
Iteration 2/25 | Loss: 0.00237754
Iteration 3/25 | Loss: 0.00237751
Iteration 4/25 | Loss: 0.00237751
Iteration 5/25 | Loss: 0.00237751
Iteration 6/25 | Loss: 0.00237751
Iteration 7/25 | Loss: 0.00237751
Iteration 8/25 | Loss: 0.00237751
Iteration 9/25 | Loss: 0.00237751
Iteration 10/25 | Loss: 0.00237751
Iteration 11/25 | Loss: 0.00237751
Iteration 12/25 | Loss: 0.00237751
Iteration 13/25 | Loss: 0.00237751
Iteration 14/25 | Loss: 0.00237751
Iteration 15/25 | Loss: 0.00237751
Iteration 16/25 | Loss: 0.00237751
Iteration 17/25 | Loss: 0.00237751
Iteration 18/25 | Loss: 0.00237751
Iteration 19/25 | Loss: 0.00237751
Iteration 20/25 | Loss: 0.00237751
Iteration 21/25 | Loss: 0.00237751
Iteration 22/25 | Loss: 0.00237751
Iteration 23/25 | Loss: 0.00237751
Iteration 24/25 | Loss: 0.00237751
Iteration 25/25 | Loss: 0.00237751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237751
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00003161
Iteration 4/1000 | Loss: 0.00002845
Iteration 5/1000 | Loss: 0.00002710
Iteration 6/1000 | Loss: 0.00002577
Iteration 7/1000 | Loss: 0.00002479
Iteration 8/1000 | Loss: 0.00002414
Iteration 9/1000 | Loss: 0.00002365
Iteration 10/1000 | Loss: 0.00002341
Iteration 11/1000 | Loss: 0.00002332
Iteration 12/1000 | Loss: 0.00002321
Iteration 13/1000 | Loss: 0.00002315
Iteration 14/1000 | Loss: 0.00002314
Iteration 15/1000 | Loss: 0.00002305
Iteration 16/1000 | Loss: 0.00002294
Iteration 17/1000 | Loss: 0.00002294
Iteration 18/1000 | Loss: 0.00002282
Iteration 19/1000 | Loss: 0.00002281
Iteration 20/1000 | Loss: 0.00002281
Iteration 21/1000 | Loss: 0.00002277
Iteration 22/1000 | Loss: 0.00002277
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00002276
Iteration 25/1000 | Loss: 0.00002276
Iteration 26/1000 | Loss: 0.00002274
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002271
Iteration 29/1000 | Loss: 0.00002271
Iteration 30/1000 | Loss: 0.00002271
Iteration 31/1000 | Loss: 0.00002271
Iteration 32/1000 | Loss: 0.00002271
Iteration 33/1000 | Loss: 0.00002270
Iteration 34/1000 | Loss: 0.00002269
Iteration 35/1000 | Loss: 0.00002266
Iteration 36/1000 | Loss: 0.00002266
Iteration 37/1000 | Loss: 0.00002266
Iteration 38/1000 | Loss: 0.00002266
Iteration 39/1000 | Loss: 0.00002266
Iteration 40/1000 | Loss: 0.00002266
Iteration 41/1000 | Loss: 0.00002266
Iteration 42/1000 | Loss: 0.00002266
Iteration 43/1000 | Loss: 0.00002266
Iteration 44/1000 | Loss: 0.00002265
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002262
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002262
Iteration 52/1000 | Loss: 0.00002262
Iteration 53/1000 | Loss: 0.00002262
Iteration 54/1000 | Loss: 0.00002262
Iteration 55/1000 | Loss: 0.00002262
Iteration 56/1000 | Loss: 0.00002262
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00002260
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002258
Iteration 62/1000 | Loss: 0.00002258
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002258
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002257
Iteration 73/1000 | Loss: 0.00002257
Iteration 74/1000 | Loss: 0.00002257
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002254
Iteration 77/1000 | Loss: 0.00002253
Iteration 78/1000 | Loss: 0.00002253
Iteration 79/1000 | Loss: 0.00002253
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002252
Iteration 83/1000 | Loss: 0.00002251
Iteration 84/1000 | Loss: 0.00002251
Iteration 85/1000 | Loss: 0.00002251
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00002251
Iteration 88/1000 | Loss: 0.00002251
Iteration 89/1000 | Loss: 0.00002251
Iteration 90/1000 | Loss: 0.00002250
Iteration 91/1000 | Loss: 0.00002250
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002250
Iteration 94/1000 | Loss: 0.00002250
Iteration 95/1000 | Loss: 0.00002250
Iteration 96/1000 | Loss: 0.00002250
Iteration 97/1000 | Loss: 0.00002250
Iteration 98/1000 | Loss: 0.00002250
Iteration 99/1000 | Loss: 0.00002250
Iteration 100/1000 | Loss: 0.00002250
Iteration 101/1000 | Loss: 0.00002250
Iteration 102/1000 | Loss: 0.00002250
Iteration 103/1000 | Loss: 0.00002250
Iteration 104/1000 | Loss: 0.00002250
Iteration 105/1000 | Loss: 0.00002250
Iteration 106/1000 | Loss: 0.00002250
Iteration 107/1000 | Loss: 0.00002249
Iteration 108/1000 | Loss: 0.00002249
Iteration 109/1000 | Loss: 0.00002249
Iteration 110/1000 | Loss: 0.00002249
Iteration 111/1000 | Loss: 0.00002249
Iteration 112/1000 | Loss: 0.00002249
Iteration 113/1000 | Loss: 0.00002249
Iteration 114/1000 | Loss: 0.00002249
Iteration 115/1000 | Loss: 0.00002249
Iteration 116/1000 | Loss: 0.00002249
Iteration 117/1000 | Loss: 0.00002248
Iteration 118/1000 | Loss: 0.00002248
Iteration 119/1000 | Loss: 0.00002248
Iteration 120/1000 | Loss: 0.00002248
Iteration 121/1000 | Loss: 0.00002248
Iteration 122/1000 | Loss: 0.00002248
Iteration 123/1000 | Loss: 0.00002248
Iteration 124/1000 | Loss: 0.00002248
Iteration 125/1000 | Loss: 0.00002248
Iteration 126/1000 | Loss: 0.00002248
Iteration 127/1000 | Loss: 0.00002248
Iteration 128/1000 | Loss: 0.00002248
Iteration 129/1000 | Loss: 0.00002247
Iteration 130/1000 | Loss: 0.00002247
Iteration 131/1000 | Loss: 0.00002247
Iteration 132/1000 | Loss: 0.00002247
Iteration 133/1000 | Loss: 0.00002247
Iteration 134/1000 | Loss: 0.00002247
Iteration 135/1000 | Loss: 0.00002247
Iteration 136/1000 | Loss: 0.00002246
Iteration 137/1000 | Loss: 0.00002246
Iteration 138/1000 | Loss: 0.00002246
Iteration 139/1000 | Loss: 0.00002246
Iteration 140/1000 | Loss: 0.00002246
Iteration 141/1000 | Loss: 0.00002246
Iteration 142/1000 | Loss: 0.00002246
Iteration 143/1000 | Loss: 0.00002246
Iteration 144/1000 | Loss: 0.00002246
Iteration 145/1000 | Loss: 0.00002246
Iteration 146/1000 | Loss: 0.00002246
Iteration 147/1000 | Loss: 0.00002246
Iteration 148/1000 | Loss: 0.00002246
Iteration 149/1000 | Loss: 0.00002246
Iteration 150/1000 | Loss: 0.00002245
Iteration 151/1000 | Loss: 0.00002245
Iteration 152/1000 | Loss: 0.00002245
Iteration 153/1000 | Loss: 0.00002245
Iteration 154/1000 | Loss: 0.00002245
Iteration 155/1000 | Loss: 0.00002245
Iteration 156/1000 | Loss: 0.00002245
Iteration 157/1000 | Loss: 0.00002245
Iteration 158/1000 | Loss: 0.00002245
Iteration 159/1000 | Loss: 0.00002245
Iteration 160/1000 | Loss: 0.00002245
Iteration 161/1000 | Loss: 0.00002245
Iteration 162/1000 | Loss: 0.00002245
Iteration 163/1000 | Loss: 0.00002245
Iteration 164/1000 | Loss: 0.00002245
Iteration 165/1000 | Loss: 0.00002245
Iteration 166/1000 | Loss: 0.00002245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.2451109543908387e-05, 2.2451109543908387e-05, 2.2451109543908387e-05, 2.2451109543908387e-05, 2.2451109543908387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2451109543908387e-05

Optimization complete. Final v2v error: 4.041322708129883 mm

Highest mean error: 4.392560958862305 mm for frame 75

Lowest mean error: 3.687542200088501 mm for frame 33

Saving results

Total time: 41.28321695327759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968999
Iteration 2/25 | Loss: 0.00118951
Iteration 3/25 | Loss: 0.00104102
Iteration 4/25 | Loss: 0.00100660
Iteration 5/25 | Loss: 0.00100163
Iteration 6/25 | Loss: 0.00100038
Iteration 7/25 | Loss: 0.00099995
Iteration 8/25 | Loss: 0.00099995
Iteration 9/25 | Loss: 0.00099995
Iteration 10/25 | Loss: 0.00099995
Iteration 11/25 | Loss: 0.00099995
Iteration 12/25 | Loss: 0.00099995
Iteration 13/25 | Loss: 0.00099995
Iteration 14/25 | Loss: 0.00099995
Iteration 15/25 | Loss: 0.00099995
Iteration 16/25 | Loss: 0.00099995
Iteration 17/25 | Loss: 0.00099995
Iteration 18/25 | Loss: 0.00099995
Iteration 19/25 | Loss: 0.00099995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009999488247558475, 0.0009999488247558475, 0.0009999488247558475, 0.0009999488247558475, 0.0009999488247558475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009999488247558475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62679315
Iteration 2/25 | Loss: 0.00218784
Iteration 3/25 | Loss: 0.00218783
Iteration 4/25 | Loss: 0.00218783
Iteration 5/25 | Loss: 0.00218783
Iteration 6/25 | Loss: 0.00218783
Iteration 7/25 | Loss: 0.00218783
Iteration 8/25 | Loss: 0.00218783
Iteration 9/25 | Loss: 0.00218783
Iteration 10/25 | Loss: 0.00218783
Iteration 11/25 | Loss: 0.00218783
Iteration 12/25 | Loss: 0.00218783
Iteration 13/25 | Loss: 0.00218783
Iteration 14/25 | Loss: 0.00218783
Iteration 15/25 | Loss: 0.00218783
Iteration 16/25 | Loss: 0.00218783
Iteration 17/25 | Loss: 0.00218783
Iteration 18/25 | Loss: 0.00218783
Iteration 19/25 | Loss: 0.00218783
Iteration 20/25 | Loss: 0.00218783
Iteration 21/25 | Loss: 0.00218783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021878317929804325, 0.0021878317929804325, 0.0021878317929804325, 0.0021878317929804325, 0.0021878317929804325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021878317929804325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218783
Iteration 2/1000 | Loss: 0.00004856
Iteration 3/1000 | Loss: 0.00003450
Iteration 4/1000 | Loss: 0.00003029
Iteration 5/1000 | Loss: 0.00002807
Iteration 6/1000 | Loss: 0.00002701
Iteration 7/1000 | Loss: 0.00002632
Iteration 8/1000 | Loss: 0.00002597
Iteration 9/1000 | Loss: 0.00002579
Iteration 10/1000 | Loss: 0.00002579
Iteration 11/1000 | Loss: 0.00002578
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00002574
Iteration 14/1000 | Loss: 0.00002574
Iteration 15/1000 | Loss: 0.00002573
Iteration 16/1000 | Loss: 0.00002573
Iteration 17/1000 | Loss: 0.00002573
Iteration 18/1000 | Loss: 0.00002573
Iteration 19/1000 | Loss: 0.00002573
Iteration 20/1000 | Loss: 0.00002573
Iteration 21/1000 | Loss: 0.00002573
Iteration 22/1000 | Loss: 0.00002573
Iteration 23/1000 | Loss: 0.00002572
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002572
Iteration 26/1000 | Loss: 0.00002572
Iteration 27/1000 | Loss: 0.00002572
Iteration 28/1000 | Loss: 0.00002572
Iteration 29/1000 | Loss: 0.00002569
Iteration 30/1000 | Loss: 0.00002569
Iteration 31/1000 | Loss: 0.00002569
Iteration 32/1000 | Loss: 0.00002569
Iteration 33/1000 | Loss: 0.00002569
Iteration 34/1000 | Loss: 0.00002569
Iteration 35/1000 | Loss: 0.00002568
Iteration 36/1000 | Loss: 0.00002568
Iteration 37/1000 | Loss: 0.00002568
Iteration 38/1000 | Loss: 0.00002568
Iteration 39/1000 | Loss: 0.00002568
Iteration 40/1000 | Loss: 0.00002568
Iteration 41/1000 | Loss: 0.00002568
Iteration 42/1000 | Loss: 0.00002568
Iteration 43/1000 | Loss: 0.00002568
Iteration 44/1000 | Loss: 0.00002567
Iteration 45/1000 | Loss: 0.00002567
Iteration 46/1000 | Loss: 0.00002567
Iteration 47/1000 | Loss: 0.00002567
Iteration 48/1000 | Loss: 0.00002566
Iteration 49/1000 | Loss: 0.00002566
Iteration 50/1000 | Loss: 0.00002565
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002564
Iteration 53/1000 | Loss: 0.00002564
Iteration 54/1000 | Loss: 0.00002564
Iteration 55/1000 | Loss: 0.00002563
Iteration 56/1000 | Loss: 0.00002563
Iteration 57/1000 | Loss: 0.00002562
Iteration 58/1000 | Loss: 0.00002562
Iteration 59/1000 | Loss: 0.00002561
Iteration 60/1000 | Loss: 0.00002561
Iteration 61/1000 | Loss: 0.00002561
Iteration 62/1000 | Loss: 0.00002560
Iteration 63/1000 | Loss: 0.00002560
Iteration 64/1000 | Loss: 0.00002560
Iteration 65/1000 | Loss: 0.00002560
Iteration 66/1000 | Loss: 0.00002560
Iteration 67/1000 | Loss: 0.00002560
Iteration 68/1000 | Loss: 0.00002560
Iteration 69/1000 | Loss: 0.00002560
Iteration 70/1000 | Loss: 0.00002560
Iteration 71/1000 | Loss: 0.00002559
Iteration 72/1000 | Loss: 0.00002559
Iteration 73/1000 | Loss: 0.00002559
Iteration 74/1000 | Loss: 0.00002559
Iteration 75/1000 | Loss: 0.00002559
Iteration 76/1000 | Loss: 0.00002559
Iteration 77/1000 | Loss: 0.00002559
Iteration 78/1000 | Loss: 0.00002559
Iteration 79/1000 | Loss: 0.00002559
Iteration 80/1000 | Loss: 0.00002559
Iteration 81/1000 | Loss: 0.00002559
Iteration 82/1000 | Loss: 0.00002558
Iteration 83/1000 | Loss: 0.00002558
Iteration 84/1000 | Loss: 0.00002558
Iteration 85/1000 | Loss: 0.00002558
Iteration 86/1000 | Loss: 0.00002558
Iteration 87/1000 | Loss: 0.00002558
Iteration 88/1000 | Loss: 0.00002558
Iteration 89/1000 | Loss: 0.00002558
Iteration 90/1000 | Loss: 0.00002558
Iteration 91/1000 | Loss: 0.00002558
Iteration 92/1000 | Loss: 0.00002557
Iteration 93/1000 | Loss: 0.00002557
Iteration 94/1000 | Loss: 0.00002557
Iteration 95/1000 | Loss: 0.00002557
Iteration 96/1000 | Loss: 0.00002557
Iteration 97/1000 | Loss: 0.00002557
Iteration 98/1000 | Loss: 0.00002557
Iteration 99/1000 | Loss: 0.00002557
Iteration 100/1000 | Loss: 0.00002557
Iteration 101/1000 | Loss: 0.00002557
Iteration 102/1000 | Loss: 0.00002557
Iteration 103/1000 | Loss: 0.00002557
Iteration 104/1000 | Loss: 0.00002557
Iteration 105/1000 | Loss: 0.00002557
Iteration 106/1000 | Loss: 0.00002557
Iteration 107/1000 | Loss: 0.00002557
Iteration 108/1000 | Loss: 0.00002556
Iteration 109/1000 | Loss: 0.00002556
Iteration 110/1000 | Loss: 0.00002556
Iteration 111/1000 | Loss: 0.00002556
Iteration 112/1000 | Loss: 0.00002556
Iteration 113/1000 | Loss: 0.00002556
Iteration 114/1000 | Loss: 0.00002556
Iteration 115/1000 | Loss: 0.00002556
Iteration 116/1000 | Loss: 0.00002555
Iteration 117/1000 | Loss: 0.00002555
Iteration 118/1000 | Loss: 0.00002554
Iteration 119/1000 | Loss: 0.00002554
Iteration 120/1000 | Loss: 0.00002553
Iteration 121/1000 | Loss: 0.00002553
Iteration 122/1000 | Loss: 0.00002553
Iteration 123/1000 | Loss: 0.00002553
Iteration 124/1000 | Loss: 0.00002553
Iteration 125/1000 | Loss: 0.00002553
Iteration 126/1000 | Loss: 0.00002552
Iteration 127/1000 | Loss: 0.00002552
Iteration 128/1000 | Loss: 0.00002552
Iteration 129/1000 | Loss: 0.00002552
Iteration 130/1000 | Loss: 0.00002551
Iteration 131/1000 | Loss: 0.00002551
Iteration 132/1000 | Loss: 0.00002551
Iteration 133/1000 | Loss: 0.00002551
Iteration 134/1000 | Loss: 0.00002551
Iteration 135/1000 | Loss: 0.00002551
Iteration 136/1000 | Loss: 0.00002551
Iteration 137/1000 | Loss: 0.00002551
Iteration 138/1000 | Loss: 0.00002551
Iteration 139/1000 | Loss: 0.00002550
Iteration 140/1000 | Loss: 0.00002550
Iteration 141/1000 | Loss: 0.00002550
Iteration 142/1000 | Loss: 0.00002549
Iteration 143/1000 | Loss: 0.00002549
Iteration 144/1000 | Loss: 0.00002549
Iteration 145/1000 | Loss: 0.00002549
Iteration 146/1000 | Loss: 0.00002549
Iteration 147/1000 | Loss: 0.00002549
Iteration 148/1000 | Loss: 0.00002549
Iteration 149/1000 | Loss: 0.00002549
Iteration 150/1000 | Loss: 0.00002549
Iteration 151/1000 | Loss: 0.00002549
Iteration 152/1000 | Loss: 0.00002549
Iteration 153/1000 | Loss: 0.00002549
Iteration 154/1000 | Loss: 0.00002549
Iteration 155/1000 | Loss: 0.00002549
Iteration 156/1000 | Loss: 0.00002549
Iteration 157/1000 | Loss: 0.00002549
Iteration 158/1000 | Loss: 0.00002549
Iteration 159/1000 | Loss: 0.00002549
Iteration 160/1000 | Loss: 0.00002549
Iteration 161/1000 | Loss: 0.00002549
Iteration 162/1000 | Loss: 0.00002549
Iteration 163/1000 | Loss: 0.00002549
Iteration 164/1000 | Loss: 0.00002549
Iteration 165/1000 | Loss: 0.00002549
Iteration 166/1000 | Loss: 0.00002549
Iteration 167/1000 | Loss: 0.00002549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.5490158805041574e-05, 2.5490158805041574e-05, 2.5490158805041574e-05, 2.5490158805041574e-05, 2.5490158805041574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5490158805041574e-05

Optimization complete. Final v2v error: 4.154171943664551 mm

Highest mean error: 4.781314373016357 mm for frame 112

Lowest mean error: 3.780416250228882 mm for frame 40

Saving results

Total time: 34.18498086929321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797475
Iteration 2/25 | Loss: 0.00213427
Iteration 3/25 | Loss: 0.00160932
Iteration 4/25 | Loss: 0.00152437
Iteration 5/25 | Loss: 0.00145144
Iteration 6/25 | Loss: 0.00141680
Iteration 7/25 | Loss: 0.00170630
Iteration 8/25 | Loss: 0.00141725
Iteration 9/25 | Loss: 0.00131239
Iteration 10/25 | Loss: 0.00106157
Iteration 11/25 | Loss: 0.00105571
Iteration 12/25 | Loss: 0.00104846
Iteration 13/25 | Loss: 0.00104783
Iteration 14/25 | Loss: 0.00104631
Iteration 15/25 | Loss: 0.00104487
Iteration 16/25 | Loss: 0.00104782
Iteration 17/25 | Loss: 0.00104477
Iteration 18/25 | Loss: 0.00104623
Iteration 19/25 | Loss: 0.00104662
Iteration 20/25 | Loss: 0.00104607
Iteration 21/25 | Loss: 0.00104559
Iteration 22/25 | Loss: 0.00104828
Iteration 23/25 | Loss: 0.00104551
Iteration 24/25 | Loss: 0.00104484
Iteration 25/25 | Loss: 0.00104720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74050450
Iteration 2/25 | Loss: 0.00309926
Iteration 3/25 | Loss: 0.00309887
Iteration 4/25 | Loss: 0.00309887
Iteration 5/25 | Loss: 0.00309887
Iteration 6/25 | Loss: 0.00309887
Iteration 7/25 | Loss: 0.00309887
Iteration 8/25 | Loss: 0.00309887
Iteration 9/25 | Loss: 0.00309887
Iteration 10/25 | Loss: 0.00309887
Iteration 11/25 | Loss: 0.00309887
Iteration 12/25 | Loss: 0.00309887
Iteration 13/25 | Loss: 0.00309887
Iteration 14/25 | Loss: 0.00309887
Iteration 15/25 | Loss: 0.00309887
Iteration 16/25 | Loss: 0.00309887
Iteration 17/25 | Loss: 0.00309887
Iteration 18/25 | Loss: 0.00309887
Iteration 19/25 | Loss: 0.00309887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0030988664366304874, 0.0030988664366304874, 0.0030988664366304874, 0.0030988664366304874, 0.0030988664366304874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030988664366304874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309887
Iteration 2/1000 | Loss: 0.00099113
Iteration 3/1000 | Loss: 0.00026086
Iteration 4/1000 | Loss: 0.00134338
Iteration 5/1000 | Loss: 0.00020028
Iteration 6/1000 | Loss: 0.00041409
Iteration 7/1000 | Loss: 0.00017818
Iteration 8/1000 | Loss: 0.00042219
Iteration 9/1000 | Loss: 0.00014493
Iteration 10/1000 | Loss: 0.00029982
Iteration 11/1000 | Loss: 0.00029006
Iteration 12/1000 | Loss: 0.00019086
Iteration 13/1000 | Loss: 0.00013252
Iteration 14/1000 | Loss: 0.00006396
Iteration 15/1000 | Loss: 0.00027137
Iteration 16/1000 | Loss: 0.00011965
Iteration 17/1000 | Loss: 0.00018743
Iteration 18/1000 | Loss: 0.00021112
Iteration 19/1000 | Loss: 0.00016189
Iteration 20/1000 | Loss: 0.00003996
Iteration 21/1000 | Loss: 0.00044241
Iteration 22/1000 | Loss: 0.00003782
Iteration 23/1000 | Loss: 0.00023470
Iteration 24/1000 | Loss: 0.00016395
Iteration 25/1000 | Loss: 0.00003425
Iteration 26/1000 | Loss: 0.00003329
Iteration 27/1000 | Loss: 0.00003267
Iteration 28/1000 | Loss: 0.00003197
Iteration 29/1000 | Loss: 0.00003068
Iteration 30/1000 | Loss: 0.00002888
Iteration 31/1000 | Loss: 0.00002755
Iteration 32/1000 | Loss: 0.00002710
Iteration 33/1000 | Loss: 0.00002699
Iteration 34/1000 | Loss: 0.00002697
Iteration 35/1000 | Loss: 0.00002697
Iteration 36/1000 | Loss: 0.00002697
Iteration 37/1000 | Loss: 0.00002696
Iteration 38/1000 | Loss: 0.00002696
Iteration 39/1000 | Loss: 0.00002696
Iteration 40/1000 | Loss: 0.00002696
Iteration 41/1000 | Loss: 0.00002696
Iteration 42/1000 | Loss: 0.00002695
Iteration 43/1000 | Loss: 0.00002694
Iteration 44/1000 | Loss: 0.00002694
Iteration 45/1000 | Loss: 0.00002694
Iteration 46/1000 | Loss: 0.00002693
Iteration 47/1000 | Loss: 0.00002693
Iteration 48/1000 | Loss: 0.00002693
Iteration 49/1000 | Loss: 0.00002692
Iteration 50/1000 | Loss: 0.00002692
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00002691
Iteration 54/1000 | Loss: 0.00002691
Iteration 55/1000 | Loss: 0.00002690
Iteration 56/1000 | Loss: 0.00002690
Iteration 57/1000 | Loss: 0.00002690
Iteration 58/1000 | Loss: 0.00002689
Iteration 59/1000 | Loss: 0.00002688
Iteration 60/1000 | Loss: 0.00002688
Iteration 61/1000 | Loss: 0.00002688
Iteration 62/1000 | Loss: 0.00002687
Iteration 63/1000 | Loss: 0.00002687
Iteration 64/1000 | Loss: 0.00002687
Iteration 65/1000 | Loss: 0.00002687
Iteration 66/1000 | Loss: 0.00002687
Iteration 67/1000 | Loss: 0.00002686
Iteration 68/1000 | Loss: 0.00002686
Iteration 69/1000 | Loss: 0.00002686
Iteration 70/1000 | Loss: 0.00002686
Iteration 71/1000 | Loss: 0.00002685
Iteration 72/1000 | Loss: 0.00002684
Iteration 73/1000 | Loss: 0.00002683
Iteration 74/1000 | Loss: 0.00002683
Iteration 75/1000 | Loss: 0.00002681
Iteration 76/1000 | Loss: 0.00002677
Iteration 77/1000 | Loss: 0.00002670
Iteration 78/1000 | Loss: 0.00002668
Iteration 79/1000 | Loss: 0.00002657
Iteration 80/1000 | Loss: 0.00002656
Iteration 81/1000 | Loss: 0.00002656
Iteration 82/1000 | Loss: 0.00002643
Iteration 83/1000 | Loss: 0.00002643
Iteration 84/1000 | Loss: 0.00002642
Iteration 85/1000 | Loss: 0.00002642
Iteration 86/1000 | Loss: 0.00002642
Iteration 87/1000 | Loss: 0.00002641
Iteration 88/1000 | Loss: 0.00002641
Iteration 89/1000 | Loss: 0.00002641
Iteration 90/1000 | Loss: 0.00002640
Iteration 91/1000 | Loss: 0.00002639
Iteration 92/1000 | Loss: 0.00002639
Iteration 93/1000 | Loss: 0.00002638
Iteration 94/1000 | Loss: 0.00002638
Iteration 95/1000 | Loss: 0.00002637
Iteration 96/1000 | Loss: 0.00002637
Iteration 97/1000 | Loss: 0.00002637
Iteration 98/1000 | Loss: 0.00002636
Iteration 99/1000 | Loss: 0.00002633
Iteration 100/1000 | Loss: 0.00002632
Iteration 101/1000 | Loss: 0.00002632
Iteration 102/1000 | Loss: 0.00002632
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002631
Iteration 105/1000 | Loss: 0.00002631
Iteration 106/1000 | Loss: 0.00002631
Iteration 107/1000 | Loss: 0.00002630
Iteration 108/1000 | Loss: 0.00002630
Iteration 109/1000 | Loss: 0.00002630
Iteration 110/1000 | Loss: 0.00002630
Iteration 111/1000 | Loss: 0.00002629
Iteration 112/1000 | Loss: 0.00002628
Iteration 113/1000 | Loss: 0.00002627
Iteration 114/1000 | Loss: 0.00002627
Iteration 115/1000 | Loss: 0.00002627
Iteration 116/1000 | Loss: 0.00002626
Iteration 117/1000 | Loss: 0.00002626
Iteration 118/1000 | Loss: 0.00002626
Iteration 119/1000 | Loss: 0.00002626
Iteration 120/1000 | Loss: 0.00002626
Iteration 121/1000 | Loss: 0.00002626
Iteration 122/1000 | Loss: 0.00002625
Iteration 123/1000 | Loss: 0.00002625
Iteration 124/1000 | Loss: 0.00002625
Iteration 125/1000 | Loss: 0.00002624
Iteration 126/1000 | Loss: 0.00002624
Iteration 127/1000 | Loss: 0.00002624
Iteration 128/1000 | Loss: 0.00002623
Iteration 129/1000 | Loss: 0.00002623
Iteration 130/1000 | Loss: 0.00002623
Iteration 131/1000 | Loss: 0.00002623
Iteration 132/1000 | Loss: 0.00002623
Iteration 133/1000 | Loss: 0.00002623
Iteration 134/1000 | Loss: 0.00002623
Iteration 135/1000 | Loss: 0.00002623
Iteration 136/1000 | Loss: 0.00002623
Iteration 137/1000 | Loss: 0.00002623
Iteration 138/1000 | Loss: 0.00002622
Iteration 139/1000 | Loss: 0.00002622
Iteration 140/1000 | Loss: 0.00002622
Iteration 141/1000 | Loss: 0.00002622
Iteration 142/1000 | Loss: 0.00002622
Iteration 143/1000 | Loss: 0.00002622
Iteration 144/1000 | Loss: 0.00002622
Iteration 145/1000 | Loss: 0.00002622
Iteration 146/1000 | Loss: 0.00002621
Iteration 147/1000 | Loss: 0.00002621
Iteration 148/1000 | Loss: 0.00002621
Iteration 149/1000 | Loss: 0.00002621
Iteration 150/1000 | Loss: 0.00002621
Iteration 151/1000 | Loss: 0.00002621
Iteration 152/1000 | Loss: 0.00002621
Iteration 153/1000 | Loss: 0.00002621
Iteration 154/1000 | Loss: 0.00002621
Iteration 155/1000 | Loss: 0.00002620
Iteration 156/1000 | Loss: 0.00002620
Iteration 157/1000 | Loss: 0.00002620
Iteration 158/1000 | Loss: 0.00002620
Iteration 159/1000 | Loss: 0.00002620
Iteration 160/1000 | Loss: 0.00002620
Iteration 161/1000 | Loss: 0.00002620
Iteration 162/1000 | Loss: 0.00002620
Iteration 163/1000 | Loss: 0.00002620
Iteration 164/1000 | Loss: 0.00002620
Iteration 165/1000 | Loss: 0.00002620
Iteration 166/1000 | Loss: 0.00002620
Iteration 167/1000 | Loss: 0.00002620
Iteration 168/1000 | Loss: 0.00002620
Iteration 169/1000 | Loss: 0.00002620
Iteration 170/1000 | Loss: 0.00002620
Iteration 171/1000 | Loss: 0.00002620
Iteration 172/1000 | Loss: 0.00002620
Iteration 173/1000 | Loss: 0.00002620
Iteration 174/1000 | Loss: 0.00002620
Iteration 175/1000 | Loss: 0.00002619
Iteration 176/1000 | Loss: 0.00002619
Iteration 177/1000 | Loss: 0.00002619
Iteration 178/1000 | Loss: 0.00002619
Iteration 179/1000 | Loss: 0.00002619
Iteration 180/1000 | Loss: 0.00002619
Iteration 181/1000 | Loss: 0.00002619
Iteration 182/1000 | Loss: 0.00002619
Iteration 183/1000 | Loss: 0.00002619
Iteration 184/1000 | Loss: 0.00002619
Iteration 185/1000 | Loss: 0.00002619
Iteration 186/1000 | Loss: 0.00002619
Iteration 187/1000 | Loss: 0.00002619
Iteration 188/1000 | Loss: 0.00002619
Iteration 189/1000 | Loss: 0.00002619
Iteration 190/1000 | Loss: 0.00002619
Iteration 191/1000 | Loss: 0.00002619
Iteration 192/1000 | Loss: 0.00002619
Iteration 193/1000 | Loss: 0.00002619
Iteration 194/1000 | Loss: 0.00002619
Iteration 195/1000 | Loss: 0.00002619
Iteration 196/1000 | Loss: 0.00002619
Iteration 197/1000 | Loss: 0.00002619
Iteration 198/1000 | Loss: 0.00002619
Iteration 199/1000 | Loss: 0.00002619
Iteration 200/1000 | Loss: 0.00002619
Iteration 201/1000 | Loss: 0.00002619
Iteration 202/1000 | Loss: 0.00002619
Iteration 203/1000 | Loss: 0.00002619
Iteration 204/1000 | Loss: 0.00002619
Iteration 205/1000 | Loss: 0.00002619
Iteration 206/1000 | Loss: 0.00002619
Iteration 207/1000 | Loss: 0.00002619
Iteration 208/1000 | Loss: 0.00002619
Iteration 209/1000 | Loss: 0.00002619
Iteration 210/1000 | Loss: 0.00002619
Iteration 211/1000 | Loss: 0.00002619
Iteration 212/1000 | Loss: 0.00002619
Iteration 213/1000 | Loss: 0.00002619
Iteration 214/1000 | Loss: 0.00002619
Iteration 215/1000 | Loss: 0.00002619
Iteration 216/1000 | Loss: 0.00002619
Iteration 217/1000 | Loss: 0.00002619
Iteration 218/1000 | Loss: 0.00002619
Iteration 219/1000 | Loss: 0.00002619
Iteration 220/1000 | Loss: 0.00002619
Iteration 221/1000 | Loss: 0.00002619
Iteration 222/1000 | Loss: 0.00002619
Iteration 223/1000 | Loss: 0.00002619
Iteration 224/1000 | Loss: 0.00002619
Iteration 225/1000 | Loss: 0.00002619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.619293263705913e-05, 2.619293263705913e-05, 2.619293263705913e-05, 2.619293263705913e-05, 2.619293263705913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.619293263705913e-05

Optimization complete. Final v2v error: 4.198727607727051 mm

Highest mean error: 6.32490348815918 mm for frame 154

Lowest mean error: 3.3324246406555176 mm for frame 196

Saving results

Total time: 128.14372730255127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949304
Iteration 2/25 | Loss: 0.00120015
Iteration 3/25 | Loss: 0.00100759
Iteration 4/25 | Loss: 0.00096870
Iteration 5/25 | Loss: 0.00096198
Iteration 6/25 | Loss: 0.00095979
Iteration 7/25 | Loss: 0.00095926
Iteration 8/25 | Loss: 0.00095926
Iteration 9/25 | Loss: 0.00095926
Iteration 10/25 | Loss: 0.00095926
Iteration 11/25 | Loss: 0.00095926
Iteration 12/25 | Loss: 0.00095926
Iteration 13/25 | Loss: 0.00095926
Iteration 14/25 | Loss: 0.00095926
Iteration 15/25 | Loss: 0.00095926
Iteration 16/25 | Loss: 0.00095926
Iteration 17/25 | Loss: 0.00095926
Iteration 18/25 | Loss: 0.00095926
Iteration 19/25 | Loss: 0.00095926
Iteration 20/25 | Loss: 0.00095926
Iteration 21/25 | Loss: 0.00095926
Iteration 22/25 | Loss: 0.00095926
Iteration 23/25 | Loss: 0.00095926
Iteration 24/25 | Loss: 0.00095926
Iteration 25/25 | Loss: 0.00095926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77626324
Iteration 2/25 | Loss: 0.00207149
Iteration 3/25 | Loss: 0.00207146
Iteration 4/25 | Loss: 0.00207146
Iteration 5/25 | Loss: 0.00207146
Iteration 6/25 | Loss: 0.00207146
Iteration 7/25 | Loss: 0.00207146
Iteration 8/25 | Loss: 0.00207146
Iteration 9/25 | Loss: 0.00207146
Iteration 10/25 | Loss: 0.00207146
Iteration 11/25 | Loss: 0.00207146
Iteration 12/25 | Loss: 0.00207146
Iteration 13/25 | Loss: 0.00207146
Iteration 14/25 | Loss: 0.00207146
Iteration 15/25 | Loss: 0.00207146
Iteration 16/25 | Loss: 0.00207146
Iteration 17/25 | Loss: 0.00207146
Iteration 18/25 | Loss: 0.00207146
Iteration 19/25 | Loss: 0.00207146
Iteration 20/25 | Loss: 0.00207146
Iteration 21/25 | Loss: 0.00207146
Iteration 22/25 | Loss: 0.00207146
Iteration 23/25 | Loss: 0.00207146
Iteration 24/25 | Loss: 0.00207146
Iteration 25/25 | Loss: 0.00207146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207146
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00002250
Iteration 4/1000 | Loss: 0.00002062
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001855
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001851
Iteration 15/1000 | Loss: 0.00001850
Iteration 16/1000 | Loss: 0.00001849
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001846
Iteration 20/1000 | Loss: 0.00001845
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001844
Iteration 23/1000 | Loss: 0.00001844
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001843
Iteration 26/1000 | Loss: 0.00001843
Iteration 27/1000 | Loss: 0.00001842
Iteration 28/1000 | Loss: 0.00001841
Iteration 29/1000 | Loss: 0.00001841
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001840
Iteration 34/1000 | Loss: 0.00001839
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001839
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001833
Iteration 76/1000 | Loss: 0.00001833
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00001833
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001832
Iteration 84/1000 | Loss: 0.00001832
Iteration 85/1000 | Loss: 0.00001832
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001831
Iteration 89/1000 | Loss: 0.00001831
Iteration 90/1000 | Loss: 0.00001831
Iteration 91/1000 | Loss: 0.00001831
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001830
Iteration 95/1000 | Loss: 0.00001830
Iteration 96/1000 | Loss: 0.00001830
Iteration 97/1000 | Loss: 0.00001830
Iteration 98/1000 | Loss: 0.00001830
Iteration 99/1000 | Loss: 0.00001830
Iteration 100/1000 | Loss: 0.00001830
Iteration 101/1000 | Loss: 0.00001830
Iteration 102/1000 | Loss: 0.00001830
Iteration 103/1000 | Loss: 0.00001830
Iteration 104/1000 | Loss: 0.00001830
Iteration 105/1000 | Loss: 0.00001830
Iteration 106/1000 | Loss: 0.00001830
Iteration 107/1000 | Loss: 0.00001830
Iteration 108/1000 | Loss: 0.00001830
Iteration 109/1000 | Loss: 0.00001830
Iteration 110/1000 | Loss: 0.00001830
Iteration 111/1000 | Loss: 0.00001830
Iteration 112/1000 | Loss: 0.00001830
Iteration 113/1000 | Loss: 0.00001830
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001830
Iteration 116/1000 | Loss: 0.00001830
Iteration 117/1000 | Loss: 0.00001830
Iteration 118/1000 | Loss: 0.00001830
Iteration 119/1000 | Loss: 0.00001830
Iteration 120/1000 | Loss: 0.00001830
Iteration 121/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.830017390602734e-05, 1.830017390602734e-05, 1.830017390602734e-05, 1.830017390602734e-05, 1.830017390602734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.830017390602734e-05

Optimization complete. Final v2v error: 3.6044397354125977 mm

Highest mean error: 3.9169085025787354 mm for frame 34

Lowest mean error: 3.167989492416382 mm for frame 0

Saving results

Total time: 31.51236128807068
