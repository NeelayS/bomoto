Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=13, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 728-783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647628
Iteration 2/25 | Loss: 0.00248865
Iteration 3/25 | Loss: 0.00238387
Iteration 4/25 | Loss: 0.00236357
Iteration 5/25 | Loss: 0.00235867
Iteration 6/25 | Loss: 0.00235740
Iteration 7/25 | Loss: 0.00235740
Iteration 8/25 | Loss: 0.00235740
Iteration 9/25 | Loss: 0.00235740
Iteration 10/25 | Loss: 0.00235740
Iteration 11/25 | Loss: 0.00235740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0023574004881083965, 0.0023574004881083965, 0.0023574004881083965, 0.0023574004881083965, 0.0023574004881083965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023574004881083965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48669207
Iteration 2/25 | Loss: 0.00304946
Iteration 3/25 | Loss: 0.00304946
Iteration 4/25 | Loss: 0.00304946
Iteration 5/25 | Loss: 0.00304946
Iteration 6/25 | Loss: 0.00304946
Iteration 7/25 | Loss: 0.00304946
Iteration 8/25 | Loss: 0.00304946
Iteration 9/25 | Loss: 0.00304946
Iteration 10/25 | Loss: 0.00304946
Iteration 11/25 | Loss: 0.00304946
Iteration 12/25 | Loss: 0.00304946
Iteration 13/25 | Loss: 0.00304946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0030494597740471363, 0.0030494597740471363, 0.0030494597740471363, 0.0030494597740471363, 0.0030494597740471363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030494597740471363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304946
Iteration 2/1000 | Loss: 0.00015088
Iteration 3/1000 | Loss: 0.00010373
Iteration 4/1000 | Loss: 0.00008230
Iteration 5/1000 | Loss: 0.00007643
Iteration 6/1000 | Loss: 0.00007298
Iteration 7/1000 | Loss: 0.00007062
Iteration 8/1000 | Loss: 0.00006953
Iteration 9/1000 | Loss: 0.00006857
Iteration 10/1000 | Loss: 0.00006789
Iteration 11/1000 | Loss: 0.00006735
Iteration 12/1000 | Loss: 0.00006699
Iteration 13/1000 | Loss: 0.00006676
Iteration 14/1000 | Loss: 0.00006658
Iteration 15/1000 | Loss: 0.00006653
Iteration 16/1000 | Loss: 0.00006643
Iteration 17/1000 | Loss: 0.00006636
Iteration 18/1000 | Loss: 0.00006627
Iteration 19/1000 | Loss: 0.00006626
Iteration 20/1000 | Loss: 0.00006621
Iteration 21/1000 | Loss: 0.00006621
Iteration 22/1000 | Loss: 0.00006618
Iteration 23/1000 | Loss: 0.00006618
Iteration 24/1000 | Loss: 0.00006617
Iteration 25/1000 | Loss: 0.00006617
Iteration 26/1000 | Loss: 0.00006617
Iteration 27/1000 | Loss: 0.00006617
Iteration 28/1000 | Loss: 0.00006617
Iteration 29/1000 | Loss: 0.00006617
Iteration 30/1000 | Loss: 0.00006617
Iteration 31/1000 | Loss: 0.00006617
Iteration 32/1000 | Loss: 0.00006617
Iteration 33/1000 | Loss: 0.00006617
Iteration 34/1000 | Loss: 0.00006617
Iteration 35/1000 | Loss: 0.00006616
Iteration 36/1000 | Loss: 0.00006616
Iteration 37/1000 | Loss: 0.00006616
Iteration 38/1000 | Loss: 0.00006616
Iteration 39/1000 | Loss: 0.00006615
Iteration 40/1000 | Loss: 0.00006615
Iteration 41/1000 | Loss: 0.00006615
Iteration 42/1000 | Loss: 0.00006614
Iteration 43/1000 | Loss: 0.00006614
Iteration 44/1000 | Loss: 0.00006614
Iteration 45/1000 | Loss: 0.00006613
Iteration 46/1000 | Loss: 0.00006613
Iteration 47/1000 | Loss: 0.00006612
Iteration 48/1000 | Loss: 0.00006612
Iteration 49/1000 | Loss: 0.00006611
Iteration 50/1000 | Loss: 0.00006611
Iteration 51/1000 | Loss: 0.00006611
Iteration 52/1000 | Loss: 0.00006610
Iteration 53/1000 | Loss: 0.00006610
Iteration 54/1000 | Loss: 0.00006610
Iteration 55/1000 | Loss: 0.00006610
Iteration 56/1000 | Loss: 0.00006609
Iteration 57/1000 | Loss: 0.00006609
Iteration 58/1000 | Loss: 0.00006609
Iteration 59/1000 | Loss: 0.00006608
Iteration 60/1000 | Loss: 0.00006608
Iteration 61/1000 | Loss: 0.00006608
Iteration 62/1000 | Loss: 0.00006607
Iteration 63/1000 | Loss: 0.00006607
Iteration 64/1000 | Loss: 0.00006607
Iteration 65/1000 | Loss: 0.00006607
Iteration 66/1000 | Loss: 0.00006607
Iteration 67/1000 | Loss: 0.00006607
Iteration 68/1000 | Loss: 0.00006606
Iteration 69/1000 | Loss: 0.00006606
Iteration 70/1000 | Loss: 0.00006606
Iteration 71/1000 | Loss: 0.00006605
Iteration 72/1000 | Loss: 0.00006605
Iteration 73/1000 | Loss: 0.00006605
Iteration 74/1000 | Loss: 0.00006605
Iteration 75/1000 | Loss: 0.00006604
Iteration 76/1000 | Loss: 0.00006604
Iteration 77/1000 | Loss: 0.00006604
Iteration 78/1000 | Loss: 0.00006604
Iteration 79/1000 | Loss: 0.00006603
Iteration 80/1000 | Loss: 0.00006603
Iteration 81/1000 | Loss: 0.00006603
Iteration 82/1000 | Loss: 0.00006603
Iteration 83/1000 | Loss: 0.00006603
Iteration 84/1000 | Loss: 0.00006603
Iteration 85/1000 | Loss: 0.00006603
Iteration 86/1000 | Loss: 0.00006603
Iteration 87/1000 | Loss: 0.00006603
Iteration 88/1000 | Loss: 0.00006603
Iteration 89/1000 | Loss: 0.00006603
Iteration 90/1000 | Loss: 0.00006603
Iteration 91/1000 | Loss: 0.00006603
Iteration 92/1000 | Loss: 0.00006602
Iteration 93/1000 | Loss: 0.00006602
Iteration 94/1000 | Loss: 0.00006602
Iteration 95/1000 | Loss: 0.00006602
Iteration 96/1000 | Loss: 0.00006602
Iteration 97/1000 | Loss: 0.00006602
Iteration 98/1000 | Loss: 0.00006602
Iteration 99/1000 | Loss: 0.00006602
Iteration 100/1000 | Loss: 0.00006602
Iteration 101/1000 | Loss: 0.00006602
Iteration 102/1000 | Loss: 0.00006602
Iteration 103/1000 | Loss: 0.00006602
Iteration 104/1000 | Loss: 0.00006602
Iteration 105/1000 | Loss: 0.00006602
Iteration 106/1000 | Loss: 0.00006602
Iteration 107/1000 | Loss: 0.00006602
Iteration 108/1000 | Loss: 0.00006602
Iteration 109/1000 | Loss: 0.00006602
Iteration 110/1000 | Loss: 0.00006602
Iteration 111/1000 | Loss: 0.00006602
Iteration 112/1000 | Loss: 0.00006602
Iteration 113/1000 | Loss: 0.00006601
Iteration 114/1000 | Loss: 0.00006601
Iteration 115/1000 | Loss: 0.00006601
Iteration 116/1000 | Loss: 0.00006601
Iteration 117/1000 | Loss: 0.00006601
Iteration 118/1000 | Loss: 0.00006601
Iteration 119/1000 | Loss: 0.00006601
Iteration 120/1000 | Loss: 0.00006601
Iteration 121/1000 | Loss: 0.00006601
Iteration 122/1000 | Loss: 0.00006601
Iteration 123/1000 | Loss: 0.00006601
Iteration 124/1000 | Loss: 0.00006601
Iteration 125/1000 | Loss: 0.00006601
Iteration 126/1000 | Loss: 0.00006601
Iteration 127/1000 | Loss: 0.00006601
Iteration 128/1000 | Loss: 0.00006601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [6.601170025533065e-05, 6.601170025533065e-05, 6.601170025533065e-05, 6.601170025533065e-05, 6.601170025533065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.601170025533065e-05

Optimization complete. Final v2v error: 7.196157455444336 mm

Highest mean error: 7.454442977905273 mm for frame 109

Lowest mean error: 6.781314373016357 mm for frame 72

Saving results

Total time: 39.44141483306885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01231228
Iteration 2/25 | Loss: 0.01231228
Iteration 3/25 | Loss: 0.01231227
Iteration 4/25 | Loss: 0.00243916
Iteration 5/25 | Loss: 0.00203922
Iteration 6/25 | Loss: 0.00195916
Iteration 7/25 | Loss: 0.00193180
Iteration 8/25 | Loss: 0.00192307
Iteration 9/25 | Loss: 0.00191927
Iteration 10/25 | Loss: 0.00192326
Iteration 11/25 | Loss: 0.00190796
Iteration 12/25 | Loss: 0.00190635
Iteration 13/25 | Loss: 0.00191165
Iteration 14/25 | Loss: 0.00191283
Iteration 15/25 | Loss: 0.00190601
Iteration 16/25 | Loss: 0.00190433
Iteration 17/25 | Loss: 0.00190467
Iteration 18/25 | Loss: 0.00190440
Iteration 19/25 | Loss: 0.00190420
Iteration 20/25 | Loss: 0.00190493
Iteration 21/25 | Loss: 0.00190479
Iteration 22/25 | Loss: 0.00190536
Iteration 23/25 | Loss: 0.00190399
Iteration 24/25 | Loss: 0.00190356
Iteration 25/25 | Loss: 0.00190325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.25448799
Iteration 2/25 | Loss: 0.00182071
Iteration 3/25 | Loss: 0.00180591
Iteration 4/25 | Loss: 0.00180591
Iteration 5/25 | Loss: 0.00180591
Iteration 6/25 | Loss: 0.00180591
Iteration 7/25 | Loss: 0.00180591
Iteration 8/25 | Loss: 0.00180591
Iteration 9/25 | Loss: 0.00180591
Iteration 10/25 | Loss: 0.00180591
Iteration 11/25 | Loss: 0.00180591
Iteration 12/25 | Loss: 0.00180591
Iteration 13/25 | Loss: 0.00180591
Iteration 14/25 | Loss: 0.00180591
Iteration 15/25 | Loss: 0.00180591
Iteration 16/25 | Loss: 0.00180591
Iteration 17/25 | Loss: 0.00180591
Iteration 18/25 | Loss: 0.00180591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018059122376143932, 0.0018059122376143932, 0.0018059122376143932, 0.0018059122376143932, 0.0018059122376143932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018059122376143932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180591
Iteration 2/1000 | Loss: 0.00015817
Iteration 3/1000 | Loss: 0.00019537
Iteration 4/1000 | Loss: 0.00011101
Iteration 5/1000 | Loss: 0.00006977
Iteration 6/1000 | Loss: 0.00009083
Iteration 7/1000 | Loss: 0.00008556
Iteration 8/1000 | Loss: 0.00006333
Iteration 9/1000 | Loss: 0.00006246
Iteration 10/1000 | Loss: 0.00006166
Iteration 11/1000 | Loss: 0.00006115
Iteration 12/1000 | Loss: 0.00006056
Iteration 13/1000 | Loss: 0.00010167
Iteration 14/1000 | Loss: 0.00006016
Iteration 15/1000 | Loss: 0.00005994
Iteration 16/1000 | Loss: 0.00009021
Iteration 17/1000 | Loss: 0.00005953
Iteration 18/1000 | Loss: 0.00006268
Iteration 19/1000 | Loss: 0.00006125
Iteration 20/1000 | Loss: 0.00010353
Iteration 21/1000 | Loss: 0.00005917
Iteration 22/1000 | Loss: 0.00006050
Iteration 23/1000 | Loss: 0.00005930
Iteration 24/1000 | Loss: 0.00005981
Iteration 25/1000 | Loss: 0.00005931
Iteration 26/1000 | Loss: 0.00010137
Iteration 27/1000 | Loss: 0.00006506
Iteration 28/1000 | Loss: 0.00007440
Iteration 29/1000 | Loss: 0.00005947
Iteration 30/1000 | Loss: 0.00006002
Iteration 31/1000 | Loss: 0.00007672
Iteration 32/1000 | Loss: 0.00006033
Iteration 33/1000 | Loss: 0.00005901
Iteration 34/1000 | Loss: 0.00005951
Iteration 35/1000 | Loss: 0.00008375
Iteration 36/1000 | Loss: 0.00006636
Iteration 37/1000 | Loss: 0.00006355
Iteration 38/1000 | Loss: 0.00006026
Iteration 39/1000 | Loss: 0.00006003
Iteration 40/1000 | Loss: 0.00005994
Iteration 41/1000 | Loss: 0.00005979
Iteration 42/1000 | Loss: 0.00007626
Iteration 43/1000 | Loss: 0.00006135
Iteration 44/1000 | Loss: 0.00006515
Iteration 45/1000 | Loss: 0.00006551
Iteration 46/1000 | Loss: 0.00005984
Iteration 47/1000 | Loss: 0.00006003
Iteration 48/1000 | Loss: 0.00006441
Iteration 49/1000 | Loss: 0.00006061
Iteration 50/1000 | Loss: 0.00006021
Iteration 51/1000 | Loss: 0.00006024
Iteration 52/1000 | Loss: 0.00006007
Iteration 53/1000 | Loss: 0.00009391
Iteration 54/1000 | Loss: 0.00011679
Iteration 55/1000 | Loss: 0.00007213
Iteration 56/1000 | Loss: 0.00007967
Iteration 57/1000 | Loss: 0.00005981
Iteration 58/1000 | Loss: 0.00006023
Iteration 59/1000 | Loss: 0.00005998
Iteration 60/1000 | Loss: 0.00006011
Iteration 61/1000 | Loss: 0.00005980
Iteration 62/1000 | Loss: 0.00005927
Iteration 63/1000 | Loss: 0.00005961
Iteration 64/1000 | Loss: 0.00005961
Iteration 65/1000 | Loss: 0.00005913
Iteration 66/1000 | Loss: 0.00005925
Iteration 67/1000 | Loss: 0.00005988
Iteration 68/1000 | Loss: 0.00005979
Iteration 69/1000 | Loss: 0.00005998
Iteration 70/1000 | Loss: 0.00005958
Iteration 71/1000 | Loss: 0.00006025
Iteration 72/1000 | Loss: 0.00006010
Iteration 73/1000 | Loss: 0.00006010
Iteration 74/1000 | Loss: 0.00005984
Iteration 75/1000 | Loss: 0.00005811
Iteration 76/1000 | Loss: 0.00005810
Iteration 77/1000 | Loss: 0.00005810
Iteration 78/1000 | Loss: 0.00005809
Iteration 79/1000 | Loss: 0.00005808
Iteration 80/1000 | Loss: 0.00005808
Iteration 81/1000 | Loss: 0.00005808
Iteration 82/1000 | Loss: 0.00005808
Iteration 83/1000 | Loss: 0.00005808
Iteration 84/1000 | Loss: 0.00005808
Iteration 85/1000 | Loss: 0.00005808
Iteration 86/1000 | Loss: 0.00005808
Iteration 87/1000 | Loss: 0.00005808
Iteration 88/1000 | Loss: 0.00005808
Iteration 89/1000 | Loss: 0.00005808
Iteration 90/1000 | Loss: 0.00005807
Iteration 91/1000 | Loss: 0.00005807
Iteration 92/1000 | Loss: 0.00005807
Iteration 93/1000 | Loss: 0.00005807
Iteration 94/1000 | Loss: 0.00005806
Iteration 95/1000 | Loss: 0.00005806
Iteration 96/1000 | Loss: 0.00005806
Iteration 97/1000 | Loss: 0.00005805
Iteration 98/1000 | Loss: 0.00005805
Iteration 99/1000 | Loss: 0.00005804
Iteration 100/1000 | Loss: 0.00005804
Iteration 101/1000 | Loss: 0.00005804
Iteration 102/1000 | Loss: 0.00005804
Iteration 103/1000 | Loss: 0.00005804
Iteration 104/1000 | Loss: 0.00005804
Iteration 105/1000 | Loss: 0.00005804
Iteration 106/1000 | Loss: 0.00005804
Iteration 107/1000 | Loss: 0.00005804
Iteration 108/1000 | Loss: 0.00005804
Iteration 109/1000 | Loss: 0.00005804
Iteration 110/1000 | Loss: 0.00005803
Iteration 111/1000 | Loss: 0.00005803
Iteration 112/1000 | Loss: 0.00005803
Iteration 113/1000 | Loss: 0.00005803
Iteration 114/1000 | Loss: 0.00005803
Iteration 115/1000 | Loss: 0.00005800
Iteration 116/1000 | Loss: 0.00005800
Iteration 117/1000 | Loss: 0.00005799
Iteration 118/1000 | Loss: 0.00005798
Iteration 119/1000 | Loss: 0.00005798
Iteration 120/1000 | Loss: 0.00005798
Iteration 121/1000 | Loss: 0.00005796
Iteration 122/1000 | Loss: 0.00005795
Iteration 123/1000 | Loss: 0.00005795
Iteration 124/1000 | Loss: 0.00005795
Iteration 125/1000 | Loss: 0.00005794
Iteration 126/1000 | Loss: 0.00005794
Iteration 127/1000 | Loss: 0.00005794
Iteration 128/1000 | Loss: 0.00005794
Iteration 129/1000 | Loss: 0.00005794
Iteration 130/1000 | Loss: 0.00005794
Iteration 131/1000 | Loss: 0.00005794
Iteration 132/1000 | Loss: 0.00005794
Iteration 133/1000 | Loss: 0.00005793
Iteration 134/1000 | Loss: 0.00005793
Iteration 135/1000 | Loss: 0.00005793
Iteration 136/1000 | Loss: 0.00005793
Iteration 137/1000 | Loss: 0.00005793
Iteration 138/1000 | Loss: 0.00005793
Iteration 139/1000 | Loss: 0.00005792
Iteration 140/1000 | Loss: 0.00005792
Iteration 141/1000 | Loss: 0.00005792
Iteration 142/1000 | Loss: 0.00005792
Iteration 143/1000 | Loss: 0.00005792
Iteration 144/1000 | Loss: 0.00005792
Iteration 145/1000 | Loss: 0.00005792
Iteration 146/1000 | Loss: 0.00005792
Iteration 147/1000 | Loss: 0.00005792
Iteration 148/1000 | Loss: 0.00005792
Iteration 149/1000 | Loss: 0.00005792
Iteration 150/1000 | Loss: 0.00005792
Iteration 151/1000 | Loss: 0.00005792
Iteration 152/1000 | Loss: 0.00005792
Iteration 153/1000 | Loss: 0.00005792
Iteration 154/1000 | Loss: 0.00005792
Iteration 155/1000 | Loss: 0.00005792
Iteration 156/1000 | Loss: 0.00005792
Iteration 157/1000 | Loss: 0.00005792
Iteration 158/1000 | Loss: 0.00005792
Iteration 159/1000 | Loss: 0.00005792
Iteration 160/1000 | Loss: 0.00005792
Iteration 161/1000 | Loss: 0.00005792
Iteration 162/1000 | Loss: 0.00005792
Iteration 163/1000 | Loss: 0.00005792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [5.791681178379804e-05, 5.791681178379804e-05, 5.791681178379804e-05, 5.791681178379804e-05, 5.791681178379804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.791681178379804e-05

Optimization complete. Final v2v error: 6.616516590118408 mm

Highest mean error: 13.620298385620117 mm for frame 93

Lowest mean error: 6.182825565338135 mm for frame 201

Saving results

Total time: 157.120361328125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841306
Iteration 2/25 | Loss: 0.00242853
Iteration 3/25 | Loss: 0.00234379
Iteration 4/25 | Loss: 0.00218401
Iteration 5/25 | Loss: 0.00217155
Iteration 6/25 | Loss: 0.00216492
Iteration 7/25 | Loss: 0.00216326
Iteration 8/25 | Loss: 0.00216281
Iteration 9/25 | Loss: 0.00216246
Iteration 10/25 | Loss: 0.00216205
Iteration 11/25 | Loss: 0.00216250
Iteration 12/25 | Loss: 0.00216223
Iteration 13/25 | Loss: 0.00216214
Iteration 14/25 | Loss: 0.00216200
Iteration 15/25 | Loss: 0.00216175
Iteration 16/25 | Loss: 0.00216216
Iteration 17/25 | Loss: 0.00216153
Iteration 18/25 | Loss: 0.00216137
Iteration 19/25 | Loss: 0.00216127
Iteration 20/25 | Loss: 0.00216122
Iteration 21/25 | Loss: 0.00216117
Iteration 22/25 | Loss: 0.00216117
Iteration 23/25 | Loss: 0.00216117
Iteration 24/25 | Loss: 0.00216117
Iteration 25/25 | Loss: 0.00216116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69882131
Iteration 2/25 | Loss: 0.00250495
Iteration 3/25 | Loss: 0.00250495
Iteration 4/25 | Loss: 0.00250495
Iteration 5/25 | Loss: 0.00250495
Iteration 6/25 | Loss: 0.00250495
Iteration 7/25 | Loss: 0.00250495
Iteration 8/25 | Loss: 0.00250495
Iteration 9/25 | Loss: 0.00250495
Iteration 10/25 | Loss: 0.00250495
Iteration 11/25 | Loss: 0.00250495
Iteration 12/25 | Loss: 0.00250495
Iteration 13/25 | Loss: 0.00250495
Iteration 14/25 | Loss: 0.00250495
Iteration 15/25 | Loss: 0.00250495
Iteration 16/25 | Loss: 0.00250495
Iteration 17/25 | Loss: 0.00250495
Iteration 18/25 | Loss: 0.00250495
Iteration 19/25 | Loss: 0.00250495
Iteration 20/25 | Loss: 0.00250495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0025049468968063593, 0.0025049468968063593, 0.0025049468968063593, 0.0025049468968063593, 0.0025049468968063593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025049468968063593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250495
Iteration 2/1000 | Loss: 0.00019563
Iteration 3/1000 | Loss: 0.00015559
Iteration 4/1000 | Loss: 0.00011078
Iteration 5/1000 | Loss: 0.00006913
Iteration 6/1000 | Loss: 0.00006531
Iteration 7/1000 | Loss: 0.00009510
Iteration 8/1000 | Loss: 0.00006353
Iteration 9/1000 | Loss: 0.00006659
Iteration 10/1000 | Loss: 0.00006131
Iteration 11/1000 | Loss: 0.00006337
Iteration 12/1000 | Loss: 0.00005992
Iteration 13/1000 | Loss: 0.00005913
Iteration 14/1000 | Loss: 0.00005866
Iteration 15/1000 | Loss: 0.00005837
Iteration 16/1000 | Loss: 0.00005817
Iteration 17/1000 | Loss: 0.00005815
Iteration 18/1000 | Loss: 0.00005800
Iteration 19/1000 | Loss: 0.00005799
Iteration 20/1000 | Loss: 0.00005797
Iteration 21/1000 | Loss: 0.00005796
Iteration 22/1000 | Loss: 0.00005796
Iteration 23/1000 | Loss: 0.00005794
Iteration 24/1000 | Loss: 0.00005793
Iteration 25/1000 | Loss: 0.00005791
Iteration 26/1000 | Loss: 0.00005791
Iteration 27/1000 | Loss: 0.00005790
Iteration 28/1000 | Loss: 0.00005789
Iteration 29/1000 | Loss: 0.00012363
Iteration 30/1000 | Loss: 0.00012363
Iteration 31/1000 | Loss: 0.00028382
Iteration 32/1000 | Loss: 0.00005786
Iteration 33/1000 | Loss: 0.00005782
Iteration 34/1000 | Loss: 0.00005782
Iteration 35/1000 | Loss: 0.00005780
Iteration 36/1000 | Loss: 0.00005780
Iteration 37/1000 | Loss: 0.00005780
Iteration 38/1000 | Loss: 0.00005780
Iteration 39/1000 | Loss: 0.00005780
Iteration 40/1000 | Loss: 0.00005780
Iteration 41/1000 | Loss: 0.00012116
Iteration 42/1000 | Loss: 0.00007574
Iteration 43/1000 | Loss: 0.00005799
Iteration 44/1000 | Loss: 0.00006225
Iteration 45/1000 | Loss: 0.00005790
Iteration 46/1000 | Loss: 0.00005784
Iteration 47/1000 | Loss: 0.00005778
Iteration 48/1000 | Loss: 0.00005778
Iteration 49/1000 | Loss: 0.00005778
Iteration 50/1000 | Loss: 0.00005773
Iteration 51/1000 | Loss: 0.00005771
Iteration 52/1000 | Loss: 0.00005770
Iteration 53/1000 | Loss: 0.00005770
Iteration 54/1000 | Loss: 0.00005770
Iteration 55/1000 | Loss: 0.00005769
Iteration 56/1000 | Loss: 0.00005769
Iteration 57/1000 | Loss: 0.00005769
Iteration 58/1000 | Loss: 0.00005769
Iteration 59/1000 | Loss: 0.00005769
Iteration 60/1000 | Loss: 0.00005768
Iteration 61/1000 | Loss: 0.00005768
Iteration 62/1000 | Loss: 0.00005768
Iteration 63/1000 | Loss: 0.00005768
Iteration 64/1000 | Loss: 0.00005768
Iteration 65/1000 | Loss: 0.00005768
Iteration 66/1000 | Loss: 0.00005768
Iteration 67/1000 | Loss: 0.00005767
Iteration 68/1000 | Loss: 0.00005767
Iteration 69/1000 | Loss: 0.00005767
Iteration 70/1000 | Loss: 0.00005767
Iteration 71/1000 | Loss: 0.00005767
Iteration 72/1000 | Loss: 0.00005767
Iteration 73/1000 | Loss: 0.00005767
Iteration 74/1000 | Loss: 0.00005767
Iteration 75/1000 | Loss: 0.00005767
Iteration 76/1000 | Loss: 0.00005767
Iteration 77/1000 | Loss: 0.00005767
Iteration 78/1000 | Loss: 0.00005767
Iteration 79/1000 | Loss: 0.00005767
Iteration 80/1000 | Loss: 0.00005766
Iteration 81/1000 | Loss: 0.00005766
Iteration 82/1000 | Loss: 0.00005766
Iteration 83/1000 | Loss: 0.00005766
Iteration 84/1000 | Loss: 0.00005766
Iteration 85/1000 | Loss: 0.00005766
Iteration 86/1000 | Loss: 0.00005766
Iteration 87/1000 | Loss: 0.00005766
Iteration 88/1000 | Loss: 0.00005766
Iteration 89/1000 | Loss: 0.00005765
Iteration 90/1000 | Loss: 0.00005765
Iteration 91/1000 | Loss: 0.00005765
Iteration 92/1000 | Loss: 0.00005765
Iteration 93/1000 | Loss: 0.00005765
Iteration 94/1000 | Loss: 0.00005765
Iteration 95/1000 | Loss: 0.00005765
Iteration 96/1000 | Loss: 0.00005765
Iteration 97/1000 | Loss: 0.00005765
Iteration 98/1000 | Loss: 0.00005765
Iteration 99/1000 | Loss: 0.00005765
Iteration 100/1000 | Loss: 0.00005765
Iteration 101/1000 | Loss: 0.00005764
Iteration 102/1000 | Loss: 0.00005764
Iteration 103/1000 | Loss: 0.00005764
Iteration 104/1000 | Loss: 0.00005764
Iteration 105/1000 | Loss: 0.00005764
Iteration 106/1000 | Loss: 0.00005764
Iteration 107/1000 | Loss: 0.00005764
Iteration 108/1000 | Loss: 0.00005763
Iteration 109/1000 | Loss: 0.00005763
Iteration 110/1000 | Loss: 0.00005763
Iteration 111/1000 | Loss: 0.00005763
Iteration 112/1000 | Loss: 0.00005763
Iteration 113/1000 | Loss: 0.00005763
Iteration 114/1000 | Loss: 0.00005763
Iteration 115/1000 | Loss: 0.00005763
Iteration 116/1000 | Loss: 0.00005763
Iteration 117/1000 | Loss: 0.00005763
Iteration 118/1000 | Loss: 0.00005763
Iteration 119/1000 | Loss: 0.00005763
Iteration 120/1000 | Loss: 0.00005763
Iteration 121/1000 | Loss: 0.00005762
Iteration 122/1000 | Loss: 0.00005762
Iteration 123/1000 | Loss: 0.00005762
Iteration 124/1000 | Loss: 0.00005762
Iteration 125/1000 | Loss: 0.00005762
Iteration 126/1000 | Loss: 0.00005761
Iteration 127/1000 | Loss: 0.00005761
Iteration 128/1000 | Loss: 0.00005761
Iteration 129/1000 | Loss: 0.00005761
Iteration 130/1000 | Loss: 0.00005761
Iteration 131/1000 | Loss: 0.00005761
Iteration 132/1000 | Loss: 0.00005761
Iteration 133/1000 | Loss: 0.00005761
Iteration 134/1000 | Loss: 0.00005761
Iteration 135/1000 | Loss: 0.00005761
Iteration 136/1000 | Loss: 0.00005761
Iteration 137/1000 | Loss: 0.00005761
Iteration 138/1000 | Loss: 0.00005761
Iteration 139/1000 | Loss: 0.00005761
Iteration 140/1000 | Loss: 0.00005761
Iteration 141/1000 | Loss: 0.00005760
Iteration 142/1000 | Loss: 0.00005760
Iteration 143/1000 | Loss: 0.00005760
Iteration 144/1000 | Loss: 0.00005760
Iteration 145/1000 | Loss: 0.00005760
Iteration 146/1000 | Loss: 0.00005760
Iteration 147/1000 | Loss: 0.00005760
Iteration 148/1000 | Loss: 0.00005760
Iteration 149/1000 | Loss: 0.00005760
Iteration 150/1000 | Loss: 0.00005760
Iteration 151/1000 | Loss: 0.00005760
Iteration 152/1000 | Loss: 0.00005760
Iteration 153/1000 | Loss: 0.00005760
Iteration 154/1000 | Loss: 0.00005760
Iteration 155/1000 | Loss: 0.00005760
Iteration 156/1000 | Loss: 0.00005760
Iteration 157/1000 | Loss: 0.00005760
Iteration 158/1000 | Loss: 0.00005760
Iteration 159/1000 | Loss: 0.00005760
Iteration 160/1000 | Loss: 0.00005760
Iteration 161/1000 | Loss: 0.00005760
Iteration 162/1000 | Loss: 0.00005760
Iteration 163/1000 | Loss: 0.00005760
Iteration 164/1000 | Loss: 0.00005760
Iteration 165/1000 | Loss: 0.00005760
Iteration 166/1000 | Loss: 0.00005760
Iteration 167/1000 | Loss: 0.00005760
Iteration 168/1000 | Loss: 0.00005760
Iteration 169/1000 | Loss: 0.00005760
Iteration 170/1000 | Loss: 0.00005760
Iteration 171/1000 | Loss: 0.00005760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [5.759779014624655e-05, 5.759779014624655e-05, 5.759779014624655e-05, 5.759779014624655e-05, 5.759779014624655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.759779014624655e-05

Optimization complete. Final v2v error: 6.7007012367248535 mm

Highest mean error: 12.89768123626709 mm for frame 138

Lowest mean error: 6.326176166534424 mm for frame 0

Saving results

Total time: 79.2025899887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929196
Iteration 2/25 | Loss: 0.00271891
Iteration 3/25 | Loss: 0.00249575
Iteration 4/25 | Loss: 0.00244349
Iteration 5/25 | Loss: 0.00247243
Iteration 6/25 | Loss: 0.00239402
Iteration 7/25 | Loss: 0.00236097
Iteration 8/25 | Loss: 0.00235041
Iteration 9/25 | Loss: 0.00235135
Iteration 10/25 | Loss: 0.00234983
Iteration 11/25 | Loss: 0.00235068
Iteration 12/25 | Loss: 0.00235212
Iteration 13/25 | Loss: 0.00235342
Iteration 14/25 | Loss: 0.00234551
Iteration 15/25 | Loss: 0.00235006
Iteration 16/25 | Loss: 0.00234632
Iteration 17/25 | Loss: 0.00234545
Iteration 18/25 | Loss: 0.00234415
Iteration 19/25 | Loss: 0.00234631
Iteration 20/25 | Loss: 0.00234453
Iteration 21/25 | Loss: 0.00234920
Iteration 22/25 | Loss: 0.00234540
Iteration 23/25 | Loss: 0.00234583
Iteration 24/25 | Loss: 0.00233879
Iteration 25/25 | Loss: 0.00234183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65840244
Iteration 2/25 | Loss: 0.00359454
Iteration 3/25 | Loss: 0.00359454
Iteration 4/25 | Loss: 0.00359454
Iteration 5/25 | Loss: 0.00359453
Iteration 6/25 | Loss: 0.00359453
Iteration 7/25 | Loss: 0.00359453
Iteration 8/25 | Loss: 0.00359453
Iteration 9/25 | Loss: 0.00359453
Iteration 10/25 | Loss: 0.00359453
Iteration 11/25 | Loss: 0.00359453
Iteration 12/25 | Loss: 0.00359453
Iteration 13/25 | Loss: 0.00359453
Iteration 14/25 | Loss: 0.00359453
Iteration 15/25 | Loss: 0.00359453
Iteration 16/25 | Loss: 0.00359453
Iteration 17/25 | Loss: 0.00359453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0035945335403084755, 0.0035945335403084755, 0.0035945335403084755, 0.0035945335403084755, 0.0035945335403084755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035945335403084755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00359453
Iteration 2/1000 | Loss: 0.00417307
Iteration 3/1000 | Loss: 0.00028452
Iteration 4/1000 | Loss: 0.00054137
Iteration 5/1000 | Loss: 0.00035418
Iteration 6/1000 | Loss: 0.00139393
Iteration 7/1000 | Loss: 0.00022735
Iteration 8/1000 | Loss: 0.00032910
Iteration 9/1000 | Loss: 0.00058693
Iteration 10/1000 | Loss: 0.00037976
Iteration 11/1000 | Loss: 0.00060722
Iteration 12/1000 | Loss: 0.00466892
Iteration 13/1000 | Loss: 0.00055536
Iteration 14/1000 | Loss: 0.00011734
Iteration 15/1000 | Loss: 0.00009018
Iteration 16/1000 | Loss: 0.00007893
Iteration 17/1000 | Loss: 0.00045754
Iteration 18/1000 | Loss: 0.00229683
Iteration 19/1000 | Loss: 0.00026656
Iteration 20/1000 | Loss: 0.00007299
Iteration 21/1000 | Loss: 0.00032099
Iteration 22/1000 | Loss: 0.00031124
Iteration 23/1000 | Loss: 0.00046281
Iteration 24/1000 | Loss: 0.00010216
Iteration 25/1000 | Loss: 0.00006881
Iteration 26/1000 | Loss: 0.00006300
Iteration 27/1000 | Loss: 0.00006134
Iteration 28/1000 | Loss: 0.00006060
Iteration 29/1000 | Loss: 0.00006004
Iteration 30/1000 | Loss: 0.00005949
Iteration 31/1000 | Loss: 0.00005894
Iteration 32/1000 | Loss: 0.00005844
Iteration 33/1000 | Loss: 0.00005805
Iteration 34/1000 | Loss: 0.00005777
Iteration 35/1000 | Loss: 0.00005765
Iteration 36/1000 | Loss: 0.00005757
Iteration 37/1000 | Loss: 0.00005747
Iteration 38/1000 | Loss: 0.00005732
Iteration 39/1000 | Loss: 0.00005726
Iteration 40/1000 | Loss: 0.00005725
Iteration 41/1000 | Loss: 0.00005725
Iteration 42/1000 | Loss: 0.00005720
Iteration 43/1000 | Loss: 0.00005720
Iteration 44/1000 | Loss: 0.00005719
Iteration 45/1000 | Loss: 0.00005718
Iteration 46/1000 | Loss: 0.00005718
Iteration 47/1000 | Loss: 0.00005718
Iteration 48/1000 | Loss: 0.00005717
Iteration 49/1000 | Loss: 0.00005716
Iteration 50/1000 | Loss: 0.00005716
Iteration 51/1000 | Loss: 0.00005716
Iteration 52/1000 | Loss: 0.00005715
Iteration 53/1000 | Loss: 0.00005715
Iteration 54/1000 | Loss: 0.00005715
Iteration 55/1000 | Loss: 0.00005715
Iteration 56/1000 | Loss: 0.00005714
Iteration 57/1000 | Loss: 0.00005714
Iteration 58/1000 | Loss: 0.00005714
Iteration 59/1000 | Loss: 0.00005714
Iteration 60/1000 | Loss: 0.00005711
Iteration 61/1000 | Loss: 0.00005711
Iteration 62/1000 | Loss: 0.00005711
Iteration 63/1000 | Loss: 0.00005711
Iteration 64/1000 | Loss: 0.00005711
Iteration 65/1000 | Loss: 0.00005710
Iteration 66/1000 | Loss: 0.00005709
Iteration 67/1000 | Loss: 0.00005709
Iteration 68/1000 | Loss: 0.00005709
Iteration 69/1000 | Loss: 0.00005708
Iteration 70/1000 | Loss: 0.00005708
Iteration 71/1000 | Loss: 0.00005707
Iteration 72/1000 | Loss: 0.00005707
Iteration 73/1000 | Loss: 0.00005706
Iteration 74/1000 | Loss: 0.00005706
Iteration 75/1000 | Loss: 0.00005706
Iteration 76/1000 | Loss: 0.00005705
Iteration 77/1000 | Loss: 0.00005705
Iteration 78/1000 | Loss: 0.00005705
Iteration 79/1000 | Loss: 0.00005705
Iteration 80/1000 | Loss: 0.00005704
Iteration 81/1000 | Loss: 0.00005704
Iteration 82/1000 | Loss: 0.00005704
Iteration 83/1000 | Loss: 0.00005704
Iteration 84/1000 | Loss: 0.00005704
Iteration 85/1000 | Loss: 0.00005704
Iteration 86/1000 | Loss: 0.00005704
Iteration 87/1000 | Loss: 0.00005704
Iteration 88/1000 | Loss: 0.00005704
Iteration 89/1000 | Loss: 0.00005703
Iteration 90/1000 | Loss: 0.00005703
Iteration 91/1000 | Loss: 0.00005703
Iteration 92/1000 | Loss: 0.00005703
Iteration 93/1000 | Loss: 0.00005703
Iteration 94/1000 | Loss: 0.00005703
Iteration 95/1000 | Loss: 0.00005703
Iteration 96/1000 | Loss: 0.00005703
Iteration 97/1000 | Loss: 0.00005703
Iteration 98/1000 | Loss: 0.00005703
Iteration 99/1000 | Loss: 0.00005703
Iteration 100/1000 | Loss: 0.00005703
Iteration 101/1000 | Loss: 0.00005703
Iteration 102/1000 | Loss: 0.00005703
Iteration 103/1000 | Loss: 0.00005702
Iteration 104/1000 | Loss: 0.00005702
Iteration 105/1000 | Loss: 0.00005702
Iteration 106/1000 | Loss: 0.00005702
Iteration 107/1000 | Loss: 0.00005702
Iteration 108/1000 | Loss: 0.00005702
Iteration 109/1000 | Loss: 0.00005702
Iteration 110/1000 | Loss: 0.00005702
Iteration 111/1000 | Loss: 0.00005702
Iteration 112/1000 | Loss: 0.00005702
Iteration 113/1000 | Loss: 0.00005702
Iteration 114/1000 | Loss: 0.00005702
Iteration 115/1000 | Loss: 0.00005702
Iteration 116/1000 | Loss: 0.00005702
Iteration 117/1000 | Loss: 0.00005702
Iteration 118/1000 | Loss: 0.00005702
Iteration 119/1000 | Loss: 0.00005702
Iteration 120/1000 | Loss: 0.00005702
Iteration 121/1000 | Loss: 0.00005702
Iteration 122/1000 | Loss: 0.00005702
Iteration 123/1000 | Loss: 0.00005702
Iteration 124/1000 | Loss: 0.00005702
Iteration 125/1000 | Loss: 0.00005702
Iteration 126/1000 | Loss: 0.00005702
Iteration 127/1000 | Loss: 0.00005702
Iteration 128/1000 | Loss: 0.00005702
Iteration 129/1000 | Loss: 0.00005702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [5.702400812879205e-05, 5.702400812879205e-05, 5.702400812879205e-05, 5.702400812879205e-05, 5.702400812879205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.702400812879205e-05

Optimization complete. Final v2v error: 6.740609169006348 mm

Highest mean error: 7.594231605529785 mm for frame 43

Lowest mean error: 6.243813514709473 mm for frame 63

Saving results

Total time: 122.72012329101562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513132
Iteration 2/25 | Loss: 0.00232018
Iteration 3/25 | Loss: 0.00227731
Iteration 4/25 | Loss: 0.00227068
Iteration 5/25 | Loss: 0.00226798
Iteration 6/25 | Loss: 0.00226798
Iteration 7/25 | Loss: 0.00226798
Iteration 8/25 | Loss: 0.00226798
Iteration 9/25 | Loss: 0.00226798
Iteration 10/25 | Loss: 0.00226798
Iteration 11/25 | Loss: 0.00226798
Iteration 12/25 | Loss: 0.00226798
Iteration 13/25 | Loss: 0.00226798
Iteration 14/25 | Loss: 0.00226798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0022679755929857492, 0.0022679755929857492, 0.0022679755929857492, 0.0022679755929857492, 0.0022679755929857492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022679755929857492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37554872
Iteration 2/25 | Loss: 0.00278439
Iteration 3/25 | Loss: 0.00278438
Iteration 4/25 | Loss: 0.00278438
Iteration 5/25 | Loss: 0.00278438
Iteration 6/25 | Loss: 0.00278438
Iteration 7/25 | Loss: 0.00278438
Iteration 8/25 | Loss: 0.00278438
Iteration 9/25 | Loss: 0.00278438
Iteration 10/25 | Loss: 0.00278438
Iteration 11/25 | Loss: 0.00278438
Iteration 12/25 | Loss: 0.00278438
Iteration 13/25 | Loss: 0.00278438
Iteration 14/25 | Loss: 0.00278438
Iteration 15/25 | Loss: 0.00278438
Iteration 16/25 | Loss: 0.00278438
Iteration 17/25 | Loss: 0.00278438
Iteration 18/25 | Loss: 0.00278438
Iteration 19/25 | Loss: 0.00278438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002784379990771413, 0.002784379990771413, 0.002784379990771413, 0.002784379990771413, 0.002784379990771413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002784379990771413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278438
Iteration 2/1000 | Loss: 0.00010507
Iteration 3/1000 | Loss: 0.00007589
Iteration 4/1000 | Loss: 0.00006794
Iteration 5/1000 | Loss: 0.00006419
Iteration 6/1000 | Loss: 0.00006269
Iteration 7/1000 | Loss: 0.00006193
Iteration 8/1000 | Loss: 0.00006128
Iteration 9/1000 | Loss: 0.00006077
Iteration 10/1000 | Loss: 0.00006041
Iteration 11/1000 | Loss: 0.00006016
Iteration 12/1000 | Loss: 0.00005995
Iteration 13/1000 | Loss: 0.00005989
Iteration 14/1000 | Loss: 0.00005977
Iteration 15/1000 | Loss: 0.00005971
Iteration 16/1000 | Loss: 0.00005970
Iteration 17/1000 | Loss: 0.00005967
Iteration 18/1000 | Loss: 0.00005966
Iteration 19/1000 | Loss: 0.00005965
Iteration 20/1000 | Loss: 0.00005965
Iteration 21/1000 | Loss: 0.00005965
Iteration 22/1000 | Loss: 0.00005964
Iteration 23/1000 | Loss: 0.00005964
Iteration 24/1000 | Loss: 0.00005963
Iteration 25/1000 | Loss: 0.00005963
Iteration 26/1000 | Loss: 0.00005963
Iteration 27/1000 | Loss: 0.00005963
Iteration 28/1000 | Loss: 0.00005962
Iteration 29/1000 | Loss: 0.00005962
Iteration 30/1000 | Loss: 0.00005962
Iteration 31/1000 | Loss: 0.00005962
Iteration 32/1000 | Loss: 0.00005962
Iteration 33/1000 | Loss: 0.00005962
Iteration 34/1000 | Loss: 0.00005962
Iteration 35/1000 | Loss: 0.00005962
Iteration 36/1000 | Loss: 0.00005961
Iteration 37/1000 | Loss: 0.00005961
Iteration 38/1000 | Loss: 0.00005959
Iteration 39/1000 | Loss: 0.00005959
Iteration 40/1000 | Loss: 0.00005959
Iteration 41/1000 | Loss: 0.00005959
Iteration 42/1000 | Loss: 0.00005959
Iteration 43/1000 | Loss: 0.00005959
Iteration 44/1000 | Loss: 0.00005958
Iteration 45/1000 | Loss: 0.00005958
Iteration 46/1000 | Loss: 0.00005958
Iteration 47/1000 | Loss: 0.00005958
Iteration 48/1000 | Loss: 0.00005956
Iteration 49/1000 | Loss: 0.00005956
Iteration 50/1000 | Loss: 0.00005956
Iteration 51/1000 | Loss: 0.00005956
Iteration 52/1000 | Loss: 0.00005956
Iteration 53/1000 | Loss: 0.00005955
Iteration 54/1000 | Loss: 0.00005955
Iteration 55/1000 | Loss: 0.00005955
Iteration 56/1000 | Loss: 0.00005954
Iteration 57/1000 | Loss: 0.00005954
Iteration 58/1000 | Loss: 0.00005953
Iteration 59/1000 | Loss: 0.00005953
Iteration 60/1000 | Loss: 0.00005953
Iteration 61/1000 | Loss: 0.00005951
Iteration 62/1000 | Loss: 0.00005949
Iteration 63/1000 | Loss: 0.00005949
Iteration 64/1000 | Loss: 0.00005949
Iteration 65/1000 | Loss: 0.00005948
Iteration 66/1000 | Loss: 0.00005948
Iteration 67/1000 | Loss: 0.00005948
Iteration 68/1000 | Loss: 0.00005948
Iteration 69/1000 | Loss: 0.00005948
Iteration 70/1000 | Loss: 0.00005948
Iteration 71/1000 | Loss: 0.00005948
Iteration 72/1000 | Loss: 0.00005947
Iteration 73/1000 | Loss: 0.00005947
Iteration 74/1000 | Loss: 0.00005946
Iteration 75/1000 | Loss: 0.00005946
Iteration 76/1000 | Loss: 0.00005946
Iteration 77/1000 | Loss: 0.00005946
Iteration 78/1000 | Loss: 0.00005946
Iteration 79/1000 | Loss: 0.00005946
Iteration 80/1000 | Loss: 0.00005946
Iteration 81/1000 | Loss: 0.00005945
Iteration 82/1000 | Loss: 0.00005945
Iteration 83/1000 | Loss: 0.00005945
Iteration 84/1000 | Loss: 0.00005945
Iteration 85/1000 | Loss: 0.00005945
Iteration 86/1000 | Loss: 0.00005945
Iteration 87/1000 | Loss: 0.00005945
Iteration 88/1000 | Loss: 0.00005944
Iteration 89/1000 | Loss: 0.00005944
Iteration 90/1000 | Loss: 0.00005944
Iteration 91/1000 | Loss: 0.00005944
Iteration 92/1000 | Loss: 0.00005944
Iteration 93/1000 | Loss: 0.00005944
Iteration 94/1000 | Loss: 0.00005944
Iteration 95/1000 | Loss: 0.00005944
Iteration 96/1000 | Loss: 0.00005944
Iteration 97/1000 | Loss: 0.00005944
Iteration 98/1000 | Loss: 0.00005944
Iteration 99/1000 | Loss: 0.00005944
Iteration 100/1000 | Loss: 0.00005944
Iteration 101/1000 | Loss: 0.00005944
Iteration 102/1000 | Loss: 0.00005944
Iteration 103/1000 | Loss: 0.00005944
Iteration 104/1000 | Loss: 0.00005944
Iteration 105/1000 | Loss: 0.00005944
Iteration 106/1000 | Loss: 0.00005944
Iteration 107/1000 | Loss: 0.00005944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [5.9439174947328866e-05, 5.9439174947328866e-05, 5.9439174947328866e-05, 5.9439174947328866e-05, 5.9439174947328866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.9439174947328866e-05

Optimization complete. Final v2v error: 6.883288383483887 mm

Highest mean error: 7.018248081207275 mm for frame 47

Lowest mean error: 6.722688674926758 mm for frame 239

Saving results

Total time: 38.3003294467926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710242
Iteration 2/25 | Loss: 0.00257475
Iteration 3/25 | Loss: 0.00243240
Iteration 4/25 | Loss: 0.00239130
Iteration 5/25 | Loss: 0.00238110
Iteration 6/25 | Loss: 0.00237966
Iteration 7/25 | Loss: 0.00238556
Iteration 8/25 | Loss: 0.00237997
Iteration 9/25 | Loss: 0.00237892
Iteration 10/25 | Loss: 0.00237869
Iteration 11/25 | Loss: 0.00237794
Iteration 12/25 | Loss: 0.00237735
Iteration 13/25 | Loss: 0.00237718
Iteration 14/25 | Loss: 0.00237711
Iteration 15/25 | Loss: 0.00237711
Iteration 16/25 | Loss: 0.00237710
Iteration 17/25 | Loss: 0.00237709
Iteration 18/25 | Loss: 0.00237707
Iteration 19/25 | Loss: 0.00237706
Iteration 20/25 | Loss: 0.00237694
Iteration 21/25 | Loss: 0.00238359
Iteration 22/25 | Loss: 0.00237466
Iteration 23/25 | Loss: 0.00237347
Iteration 24/25 | Loss: 0.00237321
Iteration 25/25 | Loss: 0.00237310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10013175
Iteration 2/25 | Loss: 0.00304874
Iteration 3/25 | Loss: 0.00304869
Iteration 4/25 | Loss: 0.00304869
Iteration 5/25 | Loss: 0.00304869
Iteration 6/25 | Loss: 0.00304869
Iteration 7/25 | Loss: 0.00304869
Iteration 8/25 | Loss: 0.00304869
Iteration 9/25 | Loss: 0.00304869
Iteration 10/25 | Loss: 0.00304869
Iteration 11/25 | Loss: 0.00304869
Iteration 12/25 | Loss: 0.00304869
Iteration 13/25 | Loss: 0.00304869
Iteration 14/25 | Loss: 0.00304869
Iteration 15/25 | Loss: 0.00304869
Iteration 16/25 | Loss: 0.00304869
Iteration 17/25 | Loss: 0.00304869
Iteration 18/25 | Loss: 0.00304869
Iteration 19/25 | Loss: 0.00304869
Iteration 20/25 | Loss: 0.00304869
Iteration 21/25 | Loss: 0.00304869
Iteration 22/25 | Loss: 0.00304869
Iteration 23/25 | Loss: 0.00304869
Iteration 24/25 | Loss: 0.00304869
Iteration 25/25 | Loss: 0.00304869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304869
Iteration 2/1000 | Loss: 0.00017186
Iteration 3/1000 | Loss: 0.00011175
Iteration 4/1000 | Loss: 0.00008866
Iteration 5/1000 | Loss: 0.00008040
Iteration 6/1000 | Loss: 0.00007596
Iteration 7/1000 | Loss: 0.00007345
Iteration 8/1000 | Loss: 0.00007184
Iteration 9/1000 | Loss: 0.00007086
Iteration 10/1000 | Loss: 0.00007016
Iteration 11/1000 | Loss: 0.00006950
Iteration 12/1000 | Loss: 0.00006901
Iteration 13/1000 | Loss: 0.00006869
Iteration 14/1000 | Loss: 0.00006849
Iteration 15/1000 | Loss: 0.00006830
Iteration 16/1000 | Loss: 0.00006820
Iteration 17/1000 | Loss: 0.00006811
Iteration 18/1000 | Loss: 0.00006804
Iteration 19/1000 | Loss: 0.00006799
Iteration 20/1000 | Loss: 0.00006799
Iteration 21/1000 | Loss: 0.00006795
Iteration 22/1000 | Loss: 0.00006794
Iteration 23/1000 | Loss: 0.00006793
Iteration 24/1000 | Loss: 0.00006793
Iteration 25/1000 | Loss: 0.00006793
Iteration 26/1000 | Loss: 0.00006791
Iteration 27/1000 | Loss: 0.00006790
Iteration 28/1000 | Loss: 0.00006789
Iteration 29/1000 | Loss: 0.00006789
Iteration 30/1000 | Loss: 0.00006788
Iteration 31/1000 | Loss: 0.00006788
Iteration 32/1000 | Loss: 0.00006788
Iteration 33/1000 | Loss: 0.00006788
Iteration 34/1000 | Loss: 0.00006788
Iteration 35/1000 | Loss: 0.00006787
Iteration 36/1000 | Loss: 0.00006787
Iteration 37/1000 | Loss: 0.00006787
Iteration 38/1000 | Loss: 0.00006786
Iteration 39/1000 | Loss: 0.00006786
Iteration 40/1000 | Loss: 0.00006786
Iteration 41/1000 | Loss: 0.00006786
Iteration 42/1000 | Loss: 0.00006786
Iteration 43/1000 | Loss: 0.00006786
Iteration 44/1000 | Loss: 0.00006786
Iteration 45/1000 | Loss: 0.00006786
Iteration 46/1000 | Loss: 0.00006785
Iteration 47/1000 | Loss: 0.00006785
Iteration 48/1000 | Loss: 0.00006784
Iteration 49/1000 | Loss: 0.00006784
Iteration 50/1000 | Loss: 0.00006784
Iteration 51/1000 | Loss: 0.00006784
Iteration 52/1000 | Loss: 0.00006784
Iteration 53/1000 | Loss: 0.00006784
Iteration 54/1000 | Loss: 0.00006784
Iteration 55/1000 | Loss: 0.00006784
Iteration 56/1000 | Loss: 0.00006783
Iteration 57/1000 | Loss: 0.00006783
Iteration 58/1000 | Loss: 0.00006783
Iteration 59/1000 | Loss: 0.00006783
Iteration 60/1000 | Loss: 0.00006782
Iteration 61/1000 | Loss: 0.00006782
Iteration 62/1000 | Loss: 0.00006782
Iteration 63/1000 | Loss: 0.00006782
Iteration 64/1000 | Loss: 0.00006782
Iteration 65/1000 | Loss: 0.00006782
Iteration 66/1000 | Loss: 0.00006782
Iteration 67/1000 | Loss: 0.00006781
Iteration 68/1000 | Loss: 0.00006781
Iteration 69/1000 | Loss: 0.00006781
Iteration 70/1000 | Loss: 0.00006781
Iteration 71/1000 | Loss: 0.00006781
Iteration 72/1000 | Loss: 0.00006781
Iteration 73/1000 | Loss: 0.00006781
Iteration 74/1000 | Loss: 0.00006781
Iteration 75/1000 | Loss: 0.00006781
Iteration 76/1000 | Loss: 0.00006781
Iteration 77/1000 | Loss: 0.00006781
Iteration 78/1000 | Loss: 0.00006781
Iteration 79/1000 | Loss: 0.00006781
Iteration 80/1000 | Loss: 0.00006780
Iteration 81/1000 | Loss: 0.00006780
Iteration 82/1000 | Loss: 0.00006780
Iteration 83/1000 | Loss: 0.00006780
Iteration 84/1000 | Loss: 0.00006780
Iteration 85/1000 | Loss: 0.00006780
Iteration 86/1000 | Loss: 0.00006780
Iteration 87/1000 | Loss: 0.00006780
Iteration 88/1000 | Loss: 0.00006780
Iteration 89/1000 | Loss: 0.00006780
Iteration 90/1000 | Loss: 0.00006780
Iteration 91/1000 | Loss: 0.00006780
Iteration 92/1000 | Loss: 0.00006780
Iteration 93/1000 | Loss: 0.00006780
Iteration 94/1000 | Loss: 0.00006780
Iteration 95/1000 | Loss: 0.00006780
Iteration 96/1000 | Loss: 0.00006780
Iteration 97/1000 | Loss: 0.00006780
Iteration 98/1000 | Loss: 0.00006780
Iteration 99/1000 | Loss: 0.00006780
Iteration 100/1000 | Loss: 0.00006780
Iteration 101/1000 | Loss: 0.00006780
Iteration 102/1000 | Loss: 0.00006780
Iteration 103/1000 | Loss: 0.00006780
Iteration 104/1000 | Loss: 0.00006780
Iteration 105/1000 | Loss: 0.00006780
Iteration 106/1000 | Loss: 0.00006780
Iteration 107/1000 | Loss: 0.00006780
Iteration 108/1000 | Loss: 0.00006780
Iteration 109/1000 | Loss: 0.00006780
Iteration 110/1000 | Loss: 0.00006780
Iteration 111/1000 | Loss: 0.00006780
Iteration 112/1000 | Loss: 0.00006780
Iteration 113/1000 | Loss: 0.00006780
Iteration 114/1000 | Loss: 0.00006780
Iteration 115/1000 | Loss: 0.00006780
Iteration 116/1000 | Loss: 0.00006780
Iteration 117/1000 | Loss: 0.00006780
Iteration 118/1000 | Loss: 0.00006780
Iteration 119/1000 | Loss: 0.00006780
Iteration 120/1000 | Loss: 0.00006780
Iteration 121/1000 | Loss: 0.00006780
Iteration 122/1000 | Loss: 0.00006780
Iteration 123/1000 | Loss: 0.00006780
Iteration 124/1000 | Loss: 0.00006780
Iteration 125/1000 | Loss: 0.00006780
Iteration 126/1000 | Loss: 0.00006780
Iteration 127/1000 | Loss: 0.00006780
Iteration 128/1000 | Loss: 0.00006780
Iteration 129/1000 | Loss: 0.00006780
Iteration 130/1000 | Loss: 0.00006780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [6.779835530323908e-05, 6.779835530323908e-05, 6.779835530323908e-05, 6.779835530323908e-05, 6.779835530323908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.779835530323908e-05

Optimization complete. Final v2v error: 7.299776077270508 mm

Highest mean error: 8.922983169555664 mm for frame 115

Lowest mean error: 6.676499843597412 mm for frame 98

Saving results

Total time: 67.71662473678589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01223928
Iteration 2/25 | Loss: 0.01223928
Iteration 3/25 | Loss: 0.01223927
Iteration 4/25 | Loss: 0.01223927
Iteration 5/25 | Loss: 0.00381011
Iteration 6/25 | Loss: 0.00209418
Iteration 7/25 | Loss: 0.00192856
Iteration 8/25 | Loss: 0.00184028
Iteration 9/25 | Loss: 0.00179097
Iteration 10/25 | Loss: 0.00177469
Iteration 11/25 | Loss: 0.00176691
Iteration 12/25 | Loss: 0.00177371
Iteration 13/25 | Loss: 0.00177238
Iteration 14/25 | Loss: 0.00176568
Iteration 15/25 | Loss: 0.00175512
Iteration 16/25 | Loss: 0.00174361
Iteration 17/25 | Loss: 0.00172786
Iteration 18/25 | Loss: 0.00172224
Iteration 19/25 | Loss: 0.00172167
Iteration 20/25 | Loss: 0.00171555
Iteration 21/25 | Loss: 0.00171860
Iteration 22/25 | Loss: 0.00171429
Iteration 23/25 | Loss: 0.00171158
Iteration 24/25 | Loss: 0.00171197
Iteration 25/25 | Loss: 0.00170960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44943523
Iteration 2/25 | Loss: 0.00146951
Iteration 3/25 | Loss: 0.00146951
Iteration 4/25 | Loss: 0.00146951
Iteration 5/25 | Loss: 0.00146951
Iteration 6/25 | Loss: 0.00146951
Iteration 7/25 | Loss: 0.00146951
Iteration 8/25 | Loss: 0.00146951
Iteration 9/25 | Loss: 0.00146950
Iteration 10/25 | Loss: 0.00146950
Iteration 11/25 | Loss: 0.00146950
Iteration 12/25 | Loss: 0.00146950
Iteration 13/25 | Loss: 0.00146950
Iteration 14/25 | Loss: 0.00146951
Iteration 15/25 | Loss: 0.00146951
Iteration 16/25 | Loss: 0.00146951
Iteration 17/25 | Loss: 0.00146951
Iteration 18/25 | Loss: 0.00146951
Iteration 19/25 | Loss: 0.00146951
Iteration 20/25 | Loss: 0.00146951
Iteration 21/25 | Loss: 0.00146951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014695051359012723, 0.0014695051359012723, 0.0014695051359012723, 0.0014695051359012723, 0.0014695051359012723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014695051359012723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146951
Iteration 2/1000 | Loss: 0.00054520
Iteration 3/1000 | Loss: 0.00020818
Iteration 4/1000 | Loss: 0.00022146
Iteration 5/1000 | Loss: 0.00009143
Iteration 6/1000 | Loss: 0.00014796
Iteration 7/1000 | Loss: 0.00008083
Iteration 8/1000 | Loss: 0.00007833
Iteration 9/1000 | Loss: 0.00022988
Iteration 10/1000 | Loss: 0.00171342
Iteration 11/1000 | Loss: 0.00069290
Iteration 12/1000 | Loss: 0.00008834
Iteration 13/1000 | Loss: 0.00007973
Iteration 14/1000 | Loss: 0.00007573
Iteration 15/1000 | Loss: 0.00024298
Iteration 16/1000 | Loss: 0.00007550
Iteration 17/1000 | Loss: 0.00007446
Iteration 18/1000 | Loss: 0.00007390
Iteration 19/1000 | Loss: 0.00007325
Iteration 20/1000 | Loss: 0.00007476
Iteration 21/1000 | Loss: 0.00007488
Iteration 22/1000 | Loss: 0.00007412
Iteration 23/1000 | Loss: 0.00007282
Iteration 24/1000 | Loss: 0.00007213
Iteration 25/1000 | Loss: 0.00007183
Iteration 26/1000 | Loss: 0.00007165
Iteration 27/1000 | Loss: 0.00007266
Iteration 28/1000 | Loss: 0.00007266
Iteration 29/1000 | Loss: 0.00007220
Iteration 30/1000 | Loss: 0.00007195
Iteration 31/1000 | Loss: 0.00007193
Iteration 32/1000 | Loss: 0.00007193
Iteration 33/1000 | Loss: 0.00007193
Iteration 34/1000 | Loss: 0.00007193
Iteration 35/1000 | Loss: 0.00007193
Iteration 36/1000 | Loss: 0.00007193
Iteration 37/1000 | Loss: 0.00007193
Iteration 38/1000 | Loss: 0.00007193
Iteration 39/1000 | Loss: 0.00007193
Iteration 40/1000 | Loss: 0.00007192
Iteration 41/1000 | Loss: 0.00007192
Iteration 42/1000 | Loss: 0.00007300
Iteration 43/1000 | Loss: 0.00007300
Iteration 44/1000 | Loss: 0.00007233
Iteration 45/1000 | Loss: 0.00007193
Iteration 46/1000 | Loss: 0.00007190
Iteration 47/1000 | Loss: 0.00007183
Iteration 48/1000 | Loss: 0.00007181
Iteration 49/1000 | Loss: 0.00007181
Iteration 50/1000 | Loss: 0.00007180
Iteration 51/1000 | Loss: 0.00007162
Iteration 52/1000 | Loss: 0.00007119
Iteration 53/1000 | Loss: 0.00007219
Iteration 54/1000 | Loss: 0.00007219
Iteration 55/1000 | Loss: 0.00007153
Iteration 56/1000 | Loss: 0.00007116
Iteration 57/1000 | Loss: 0.00007114
Iteration 58/1000 | Loss: 0.00007113
Iteration 59/1000 | Loss: 0.00007113
Iteration 60/1000 | Loss: 0.00007113
Iteration 61/1000 | Loss: 0.00007113
Iteration 62/1000 | Loss: 0.00007113
Iteration 63/1000 | Loss: 0.00007113
Iteration 64/1000 | Loss: 0.00007113
Iteration 65/1000 | Loss: 0.00007113
Iteration 66/1000 | Loss: 0.00007113
Iteration 67/1000 | Loss: 0.00007113
Iteration 68/1000 | Loss: 0.00007113
Iteration 69/1000 | Loss: 0.00007112
Iteration 70/1000 | Loss: 0.00007112
Iteration 71/1000 | Loss: 0.00007112
Iteration 72/1000 | Loss: 0.00007112
Iteration 73/1000 | Loss: 0.00007112
Iteration 74/1000 | Loss: 0.00007112
Iteration 75/1000 | Loss: 0.00007112
Iteration 76/1000 | Loss: 0.00007111
Iteration 77/1000 | Loss: 0.00007111
Iteration 78/1000 | Loss: 0.00007111
Iteration 79/1000 | Loss: 0.00007111
Iteration 80/1000 | Loss: 0.00007111
Iteration 81/1000 | Loss: 0.00007111
Iteration 82/1000 | Loss: 0.00007111
Iteration 83/1000 | Loss: 0.00007111
Iteration 84/1000 | Loss: 0.00007111
Iteration 85/1000 | Loss: 0.00007111
Iteration 86/1000 | Loss: 0.00007110
Iteration 87/1000 | Loss: 0.00007110
Iteration 88/1000 | Loss: 0.00007110
Iteration 89/1000 | Loss: 0.00007110
Iteration 90/1000 | Loss: 0.00007110
Iteration 91/1000 | Loss: 0.00007110
Iteration 92/1000 | Loss: 0.00007110
Iteration 93/1000 | Loss: 0.00007110
Iteration 94/1000 | Loss: 0.00007110
Iteration 95/1000 | Loss: 0.00007110
Iteration 96/1000 | Loss: 0.00007110
Iteration 97/1000 | Loss: 0.00007109
Iteration 98/1000 | Loss: 0.00007109
Iteration 99/1000 | Loss: 0.00007109
Iteration 100/1000 | Loss: 0.00007109
Iteration 101/1000 | Loss: 0.00007109
Iteration 102/1000 | Loss: 0.00007109
Iteration 103/1000 | Loss: 0.00007109
Iteration 104/1000 | Loss: 0.00007109
Iteration 105/1000 | Loss: 0.00007109
Iteration 106/1000 | Loss: 0.00007109
Iteration 107/1000 | Loss: 0.00007109
Iteration 108/1000 | Loss: 0.00007109
Iteration 109/1000 | Loss: 0.00007109
Iteration 110/1000 | Loss: 0.00007109
Iteration 111/1000 | Loss: 0.00007109
Iteration 112/1000 | Loss: 0.00007109
Iteration 113/1000 | Loss: 0.00007109
Iteration 114/1000 | Loss: 0.00007109
Iteration 115/1000 | Loss: 0.00007109
Iteration 116/1000 | Loss: 0.00007109
Iteration 117/1000 | Loss: 0.00007109
Iteration 118/1000 | Loss: 0.00007108
Iteration 119/1000 | Loss: 0.00007108
Iteration 120/1000 | Loss: 0.00007108
Iteration 121/1000 | Loss: 0.00007108
Iteration 122/1000 | Loss: 0.00007108
Iteration 123/1000 | Loss: 0.00007108
Iteration 124/1000 | Loss: 0.00007108
Iteration 125/1000 | Loss: 0.00007108
Iteration 126/1000 | Loss: 0.00007108
Iteration 127/1000 | Loss: 0.00007108
Iteration 128/1000 | Loss: 0.00007108
Iteration 129/1000 | Loss: 0.00007108
Iteration 130/1000 | Loss: 0.00007108
Iteration 131/1000 | Loss: 0.00007108
Iteration 132/1000 | Loss: 0.00007108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [7.108481077011675e-05, 7.108481077011675e-05, 7.108481077011675e-05, 7.108481077011675e-05, 7.108481077011675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.108481077011675e-05

Optimization complete. Final v2v error: 7.119670867919922 mm

Highest mean error: 14.50228500366211 mm for frame 102

Lowest mean error: 6.4878621101379395 mm for frame 50

Saving results

Total time: 92.09268927574158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972094
Iteration 2/25 | Loss: 0.00251808
Iteration 3/25 | Loss: 0.00238674
Iteration 4/25 | Loss: 0.00236786
Iteration 5/25 | Loss: 0.00236077
Iteration 6/25 | Loss: 0.00235923
Iteration 7/25 | Loss: 0.00235923
Iteration 8/25 | Loss: 0.00235923
Iteration 9/25 | Loss: 0.00235923
Iteration 10/25 | Loss: 0.00235923
Iteration 11/25 | Loss: 0.00235923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00235922634601593, 0.00235922634601593, 0.00235922634601593, 0.00235922634601593, 0.00235922634601593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00235922634601593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74928379
Iteration 2/25 | Loss: 0.00296090
Iteration 3/25 | Loss: 0.00296089
Iteration 4/25 | Loss: 0.00296089
Iteration 5/25 | Loss: 0.00296089
Iteration 6/25 | Loss: 0.00296089
Iteration 7/25 | Loss: 0.00296089
Iteration 8/25 | Loss: 0.00296089
Iteration 9/25 | Loss: 0.00296089
Iteration 10/25 | Loss: 0.00296089
Iteration 11/25 | Loss: 0.00296089
Iteration 12/25 | Loss: 0.00296089
Iteration 13/25 | Loss: 0.00296089
Iteration 14/25 | Loss: 0.00296089
Iteration 15/25 | Loss: 0.00296089
Iteration 16/25 | Loss: 0.00296089
Iteration 17/25 | Loss: 0.00296089
Iteration 18/25 | Loss: 0.00296089
Iteration 19/25 | Loss: 0.00296089
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029608882032334805, 0.0029608882032334805, 0.0029608882032334805, 0.0029608882032334805, 0.0029608882032334805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029608882032334805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296089
Iteration 2/1000 | Loss: 0.00011221
Iteration 3/1000 | Loss: 0.00008246
Iteration 4/1000 | Loss: 0.00006754
Iteration 5/1000 | Loss: 0.00006190
Iteration 6/1000 | Loss: 0.00005837
Iteration 7/1000 | Loss: 0.00005696
Iteration 8/1000 | Loss: 0.00005616
Iteration 9/1000 | Loss: 0.00005540
Iteration 10/1000 | Loss: 0.00005460
Iteration 11/1000 | Loss: 0.00005404
Iteration 12/1000 | Loss: 0.00005349
Iteration 13/1000 | Loss: 0.00005313
Iteration 14/1000 | Loss: 0.00005294
Iteration 15/1000 | Loss: 0.00005271
Iteration 16/1000 | Loss: 0.00005265
Iteration 17/1000 | Loss: 0.00005253
Iteration 18/1000 | Loss: 0.00005250
Iteration 19/1000 | Loss: 0.00005249
Iteration 20/1000 | Loss: 0.00005249
Iteration 21/1000 | Loss: 0.00005248
Iteration 22/1000 | Loss: 0.00005247
Iteration 23/1000 | Loss: 0.00005246
Iteration 24/1000 | Loss: 0.00005246
Iteration 25/1000 | Loss: 0.00005246
Iteration 26/1000 | Loss: 0.00005246
Iteration 27/1000 | Loss: 0.00005246
Iteration 28/1000 | Loss: 0.00005246
Iteration 29/1000 | Loss: 0.00005246
Iteration 30/1000 | Loss: 0.00005246
Iteration 31/1000 | Loss: 0.00005246
Iteration 32/1000 | Loss: 0.00005244
Iteration 33/1000 | Loss: 0.00005242
Iteration 34/1000 | Loss: 0.00005242
Iteration 35/1000 | Loss: 0.00005242
Iteration 36/1000 | Loss: 0.00005242
Iteration 37/1000 | Loss: 0.00005242
Iteration 38/1000 | Loss: 0.00005242
Iteration 39/1000 | Loss: 0.00005242
Iteration 40/1000 | Loss: 0.00005242
Iteration 41/1000 | Loss: 0.00005241
Iteration 42/1000 | Loss: 0.00005241
Iteration 43/1000 | Loss: 0.00005240
Iteration 44/1000 | Loss: 0.00005239
Iteration 45/1000 | Loss: 0.00005239
Iteration 46/1000 | Loss: 0.00005239
Iteration 47/1000 | Loss: 0.00005238
Iteration 48/1000 | Loss: 0.00005238
Iteration 49/1000 | Loss: 0.00005238
Iteration 50/1000 | Loss: 0.00005238
Iteration 51/1000 | Loss: 0.00005238
Iteration 52/1000 | Loss: 0.00005238
Iteration 53/1000 | Loss: 0.00005238
Iteration 54/1000 | Loss: 0.00005238
Iteration 55/1000 | Loss: 0.00005237
Iteration 56/1000 | Loss: 0.00005237
Iteration 57/1000 | Loss: 0.00005237
Iteration 58/1000 | Loss: 0.00005237
Iteration 59/1000 | Loss: 0.00005236
Iteration 60/1000 | Loss: 0.00005236
Iteration 61/1000 | Loss: 0.00005236
Iteration 62/1000 | Loss: 0.00005236
Iteration 63/1000 | Loss: 0.00005236
Iteration 64/1000 | Loss: 0.00005236
Iteration 65/1000 | Loss: 0.00005236
Iteration 66/1000 | Loss: 0.00005236
Iteration 67/1000 | Loss: 0.00005236
Iteration 68/1000 | Loss: 0.00005236
Iteration 69/1000 | Loss: 0.00005236
Iteration 70/1000 | Loss: 0.00005236
Iteration 71/1000 | Loss: 0.00005236
Iteration 72/1000 | Loss: 0.00005236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [5.235514254309237e-05, 5.235514254309237e-05, 5.235514254309237e-05, 5.235514254309237e-05, 5.235514254309237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.235514254309237e-05

Optimization complete. Final v2v error: 6.507199287414551 mm

Highest mean error: 7.070545196533203 mm for frame 200

Lowest mean error: 6.088828086853027 mm for frame 102

Saving results

Total time: 40.397215604782104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759461
Iteration 2/25 | Loss: 0.00234931
Iteration 3/25 | Loss: 0.00230193
Iteration 4/25 | Loss: 0.00229359
Iteration 5/25 | Loss: 0.00229056
Iteration 6/25 | Loss: 0.00229056
Iteration 7/25 | Loss: 0.00229056
Iteration 8/25 | Loss: 0.00229056
Iteration 9/25 | Loss: 0.00229056
Iteration 10/25 | Loss: 0.00229056
Iteration 11/25 | Loss: 0.00229056
Iteration 12/25 | Loss: 0.00229056
Iteration 13/25 | Loss: 0.00229056
Iteration 14/25 | Loss: 0.00229056
Iteration 15/25 | Loss: 0.00229056
Iteration 16/25 | Loss: 0.00229056
Iteration 17/25 | Loss: 0.00229056
Iteration 18/25 | Loss: 0.00229056
Iteration 19/25 | Loss: 0.00229056
Iteration 20/25 | Loss: 0.00229056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002290556440129876, 0.002290556440129876, 0.002290556440129876, 0.002290556440129876, 0.002290556440129876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002290556440129876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66034126
Iteration 2/25 | Loss: 0.00283019
Iteration 3/25 | Loss: 0.00283019
Iteration 4/25 | Loss: 0.00283019
Iteration 5/25 | Loss: 0.00283019
Iteration 6/25 | Loss: 0.00283019
Iteration 7/25 | Loss: 0.00283019
Iteration 8/25 | Loss: 0.00283019
Iteration 9/25 | Loss: 0.00283019
Iteration 10/25 | Loss: 0.00283019
Iteration 11/25 | Loss: 0.00283019
Iteration 12/25 | Loss: 0.00283019
Iteration 13/25 | Loss: 0.00283019
Iteration 14/25 | Loss: 0.00283019
Iteration 15/25 | Loss: 0.00283019
Iteration 16/25 | Loss: 0.00283019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002830187324434519, 0.002830187324434519, 0.002830187324434519, 0.002830187324434519, 0.002830187324434519]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002830187324434519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00283019
Iteration 2/1000 | Loss: 0.00010176
Iteration 3/1000 | Loss: 0.00007495
Iteration 4/1000 | Loss: 0.00006437
Iteration 5/1000 | Loss: 0.00006069
Iteration 6/1000 | Loss: 0.00005875
Iteration 7/1000 | Loss: 0.00005772
Iteration 8/1000 | Loss: 0.00005718
Iteration 9/1000 | Loss: 0.00005661
Iteration 10/1000 | Loss: 0.00005600
Iteration 11/1000 | Loss: 0.00005558
Iteration 12/1000 | Loss: 0.00005530
Iteration 13/1000 | Loss: 0.00005518
Iteration 14/1000 | Loss: 0.00005499
Iteration 15/1000 | Loss: 0.00005498
Iteration 16/1000 | Loss: 0.00005482
Iteration 17/1000 | Loss: 0.00005470
Iteration 18/1000 | Loss: 0.00005470
Iteration 19/1000 | Loss: 0.00005466
Iteration 20/1000 | Loss: 0.00005466
Iteration 21/1000 | Loss: 0.00005466
Iteration 22/1000 | Loss: 0.00005465
Iteration 23/1000 | Loss: 0.00005464
Iteration 24/1000 | Loss: 0.00005464
Iteration 25/1000 | Loss: 0.00005464
Iteration 26/1000 | Loss: 0.00005463
Iteration 27/1000 | Loss: 0.00005462
Iteration 28/1000 | Loss: 0.00005462
Iteration 29/1000 | Loss: 0.00005462
Iteration 30/1000 | Loss: 0.00005461
Iteration 31/1000 | Loss: 0.00005460
Iteration 32/1000 | Loss: 0.00005460
Iteration 33/1000 | Loss: 0.00005459
Iteration 34/1000 | Loss: 0.00005459
Iteration 35/1000 | Loss: 0.00005459
Iteration 36/1000 | Loss: 0.00005459
Iteration 37/1000 | Loss: 0.00005459
Iteration 38/1000 | Loss: 0.00005459
Iteration 39/1000 | Loss: 0.00005459
Iteration 40/1000 | Loss: 0.00005459
Iteration 41/1000 | Loss: 0.00005459
Iteration 42/1000 | Loss: 0.00005459
Iteration 43/1000 | Loss: 0.00005458
Iteration 44/1000 | Loss: 0.00005458
Iteration 45/1000 | Loss: 0.00005458
Iteration 46/1000 | Loss: 0.00005458
Iteration 47/1000 | Loss: 0.00005458
Iteration 48/1000 | Loss: 0.00005457
Iteration 49/1000 | Loss: 0.00005457
Iteration 50/1000 | Loss: 0.00005457
Iteration 51/1000 | Loss: 0.00005457
Iteration 52/1000 | Loss: 0.00005457
Iteration 53/1000 | Loss: 0.00005457
Iteration 54/1000 | Loss: 0.00005457
Iteration 55/1000 | Loss: 0.00005457
Iteration 56/1000 | Loss: 0.00005457
Iteration 57/1000 | Loss: 0.00005456
Iteration 58/1000 | Loss: 0.00005456
Iteration 59/1000 | Loss: 0.00005456
Iteration 60/1000 | Loss: 0.00005456
Iteration 61/1000 | Loss: 0.00005456
Iteration 62/1000 | Loss: 0.00005455
Iteration 63/1000 | Loss: 0.00005455
Iteration 64/1000 | Loss: 0.00005455
Iteration 65/1000 | Loss: 0.00005455
Iteration 66/1000 | Loss: 0.00005455
Iteration 67/1000 | Loss: 0.00005455
Iteration 68/1000 | Loss: 0.00005455
Iteration 69/1000 | Loss: 0.00005455
Iteration 70/1000 | Loss: 0.00005455
Iteration 71/1000 | Loss: 0.00005454
Iteration 72/1000 | Loss: 0.00005454
Iteration 73/1000 | Loss: 0.00005453
Iteration 74/1000 | Loss: 0.00005453
Iteration 75/1000 | Loss: 0.00005453
Iteration 76/1000 | Loss: 0.00005453
Iteration 77/1000 | Loss: 0.00005453
Iteration 78/1000 | Loss: 0.00005453
Iteration 79/1000 | Loss: 0.00005452
Iteration 80/1000 | Loss: 0.00005452
Iteration 81/1000 | Loss: 0.00005452
Iteration 82/1000 | Loss: 0.00005451
Iteration 83/1000 | Loss: 0.00005451
Iteration 84/1000 | Loss: 0.00005451
Iteration 85/1000 | Loss: 0.00005451
Iteration 86/1000 | Loss: 0.00005450
Iteration 87/1000 | Loss: 0.00005450
Iteration 88/1000 | Loss: 0.00005450
Iteration 89/1000 | Loss: 0.00005450
Iteration 90/1000 | Loss: 0.00005450
Iteration 91/1000 | Loss: 0.00005450
Iteration 92/1000 | Loss: 0.00005450
Iteration 93/1000 | Loss: 0.00005450
Iteration 94/1000 | Loss: 0.00005450
Iteration 95/1000 | Loss: 0.00005450
Iteration 96/1000 | Loss: 0.00005450
Iteration 97/1000 | Loss: 0.00005450
Iteration 98/1000 | Loss: 0.00005450
Iteration 99/1000 | Loss: 0.00005450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [5.4503369028680027e-05, 5.4503369028680027e-05, 5.4503369028680027e-05, 5.4503369028680027e-05, 5.4503369028680027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.4503369028680027e-05

Optimization complete. Final v2v error: 6.521393775939941 mm

Highest mean error: 6.870453357696533 mm for frame 176

Lowest mean error: 6.217879295349121 mm for frame 81

Saving results

Total time: 35.4054594039917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532018
Iteration 2/25 | Loss: 0.00239726
Iteration 3/25 | Loss: 0.00234456
Iteration 4/25 | Loss: 0.00232937
Iteration 5/25 | Loss: 0.00232549
Iteration 6/25 | Loss: 0.00232549
Iteration 7/25 | Loss: 0.00232549
Iteration 8/25 | Loss: 0.00232549
Iteration 9/25 | Loss: 0.00232549
Iteration 10/25 | Loss: 0.00232549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0023254933767020702, 0.0023254933767020702, 0.0023254933767020702, 0.0023254933767020702, 0.0023254933767020702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023254933767020702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69780231
Iteration 2/25 | Loss: 0.00278450
Iteration 3/25 | Loss: 0.00278450
Iteration 4/25 | Loss: 0.00278450
Iteration 5/25 | Loss: 0.00278450
Iteration 6/25 | Loss: 0.00278450
Iteration 7/25 | Loss: 0.00278450
Iteration 8/25 | Loss: 0.00278450
Iteration 9/25 | Loss: 0.00278450
Iteration 10/25 | Loss: 0.00278450
Iteration 11/25 | Loss: 0.00278450
Iteration 12/25 | Loss: 0.00278450
Iteration 13/25 | Loss: 0.00278450
Iteration 14/25 | Loss: 0.00278450
Iteration 15/25 | Loss: 0.00278450
Iteration 16/25 | Loss: 0.00278450
Iteration 17/25 | Loss: 0.00278450
Iteration 18/25 | Loss: 0.00278450
Iteration 19/25 | Loss: 0.00278450
Iteration 20/25 | Loss: 0.00278450
Iteration 21/25 | Loss: 0.00278450
Iteration 22/25 | Loss: 0.00278450
Iteration 23/25 | Loss: 0.00278450
Iteration 24/25 | Loss: 0.00278450
Iteration 25/25 | Loss: 0.00278450

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278450
Iteration 2/1000 | Loss: 0.00014109
Iteration 3/1000 | Loss: 0.00009048
Iteration 4/1000 | Loss: 0.00007423
Iteration 5/1000 | Loss: 0.00006688
Iteration 6/1000 | Loss: 0.00006150
Iteration 7/1000 | Loss: 0.00005937
Iteration 8/1000 | Loss: 0.00005803
Iteration 9/1000 | Loss: 0.00005704
Iteration 10/1000 | Loss: 0.00005579
Iteration 11/1000 | Loss: 0.00005498
Iteration 12/1000 | Loss: 0.00005439
Iteration 13/1000 | Loss: 0.00005393
Iteration 14/1000 | Loss: 0.00005365
Iteration 15/1000 | Loss: 0.00005346
Iteration 16/1000 | Loss: 0.00005343
Iteration 17/1000 | Loss: 0.00005342
Iteration 18/1000 | Loss: 0.00005341
Iteration 19/1000 | Loss: 0.00005341
Iteration 20/1000 | Loss: 0.00005337
Iteration 21/1000 | Loss: 0.00005337
Iteration 22/1000 | Loss: 0.00005337
Iteration 23/1000 | Loss: 0.00005336
Iteration 24/1000 | Loss: 0.00005336
Iteration 25/1000 | Loss: 0.00005336
Iteration 26/1000 | Loss: 0.00005335
Iteration 27/1000 | Loss: 0.00005335
Iteration 28/1000 | Loss: 0.00005335
Iteration 29/1000 | Loss: 0.00005334
Iteration 30/1000 | Loss: 0.00005334
Iteration 31/1000 | Loss: 0.00005334
Iteration 32/1000 | Loss: 0.00005333
Iteration 33/1000 | Loss: 0.00005333
Iteration 34/1000 | Loss: 0.00005333
Iteration 35/1000 | Loss: 0.00005333
Iteration 36/1000 | Loss: 0.00005332
Iteration 37/1000 | Loss: 0.00005331
Iteration 38/1000 | Loss: 0.00005331
Iteration 39/1000 | Loss: 0.00005330
Iteration 40/1000 | Loss: 0.00005330
Iteration 41/1000 | Loss: 0.00005330
Iteration 42/1000 | Loss: 0.00005330
Iteration 43/1000 | Loss: 0.00005330
Iteration 44/1000 | Loss: 0.00005330
Iteration 45/1000 | Loss: 0.00005329
Iteration 46/1000 | Loss: 0.00005329
Iteration 47/1000 | Loss: 0.00005329
Iteration 48/1000 | Loss: 0.00005329
Iteration 49/1000 | Loss: 0.00005328
Iteration 50/1000 | Loss: 0.00005328
Iteration 51/1000 | Loss: 0.00005327
Iteration 52/1000 | Loss: 0.00005326
Iteration 53/1000 | Loss: 0.00005326
Iteration 54/1000 | Loss: 0.00005326
Iteration 55/1000 | Loss: 0.00005326
Iteration 56/1000 | Loss: 0.00005326
Iteration 57/1000 | Loss: 0.00005326
Iteration 58/1000 | Loss: 0.00005326
Iteration 59/1000 | Loss: 0.00005326
Iteration 60/1000 | Loss: 0.00005326
Iteration 61/1000 | Loss: 0.00005326
Iteration 62/1000 | Loss: 0.00005325
Iteration 63/1000 | Loss: 0.00005325
Iteration 64/1000 | Loss: 0.00005325
Iteration 65/1000 | Loss: 0.00005324
Iteration 66/1000 | Loss: 0.00005324
Iteration 67/1000 | Loss: 0.00005324
Iteration 68/1000 | Loss: 0.00005324
Iteration 69/1000 | Loss: 0.00005324
Iteration 70/1000 | Loss: 0.00005324
Iteration 71/1000 | Loss: 0.00005324
Iteration 72/1000 | Loss: 0.00005324
Iteration 73/1000 | Loss: 0.00005323
Iteration 74/1000 | Loss: 0.00005323
Iteration 75/1000 | Loss: 0.00005323
Iteration 76/1000 | Loss: 0.00005323
Iteration 77/1000 | Loss: 0.00005323
Iteration 78/1000 | Loss: 0.00005323
Iteration 79/1000 | Loss: 0.00005323
Iteration 80/1000 | Loss: 0.00005323
Iteration 81/1000 | Loss: 0.00005323
Iteration 82/1000 | Loss: 0.00005323
Iteration 83/1000 | Loss: 0.00005323
Iteration 84/1000 | Loss: 0.00005323
Iteration 85/1000 | Loss: 0.00005323
Iteration 86/1000 | Loss: 0.00005323
Iteration 87/1000 | Loss: 0.00005323
Iteration 88/1000 | Loss: 0.00005323
Iteration 89/1000 | Loss: 0.00005323
Iteration 90/1000 | Loss: 0.00005323
Iteration 91/1000 | Loss: 0.00005323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [5.323262303136289e-05, 5.323262303136289e-05, 5.323262303136289e-05, 5.323262303136289e-05, 5.323262303136289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.323262303136289e-05

Optimization complete. Final v2v error: 6.552848815917969 mm

Highest mean error: 6.770762920379639 mm for frame 24

Lowest mean error: 6.398588180541992 mm for frame 162

Saving results

Total time: 38.03551530838013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588822
Iteration 2/25 | Loss: 0.00233111
Iteration 3/25 | Loss: 0.00224063
Iteration 4/25 | Loss: 0.00223121
Iteration 5/25 | Loss: 0.00222747
Iteration 6/25 | Loss: 0.00222747
Iteration 7/25 | Loss: 0.00222747
Iteration 8/25 | Loss: 0.00222747
Iteration 9/25 | Loss: 0.00222747
Iteration 10/25 | Loss: 0.00222747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0022274742368608713, 0.0022274742368608713, 0.0022274742368608713, 0.0022274742368608713, 0.0022274742368608713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022274742368608713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35483646
Iteration 2/25 | Loss: 0.00247824
Iteration 3/25 | Loss: 0.00247824
Iteration 4/25 | Loss: 0.00247824
Iteration 5/25 | Loss: 0.00247824
Iteration 6/25 | Loss: 0.00247824
Iteration 7/25 | Loss: 0.00247824
Iteration 8/25 | Loss: 0.00247824
Iteration 9/25 | Loss: 0.00247824
Iteration 10/25 | Loss: 0.00247824
Iteration 11/25 | Loss: 0.00247824
Iteration 12/25 | Loss: 0.00247824
Iteration 13/25 | Loss: 0.00247824
Iteration 14/25 | Loss: 0.00247824
Iteration 15/25 | Loss: 0.00247824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0024782363325357437, 0.0024782363325357437, 0.0024782363325357437, 0.0024782363325357437, 0.0024782363325357437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024782363325357437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247824
Iteration 2/1000 | Loss: 0.00011665
Iteration 3/1000 | Loss: 0.00007927
Iteration 4/1000 | Loss: 0.00006605
Iteration 5/1000 | Loss: 0.00006201
Iteration 6/1000 | Loss: 0.00005950
Iteration 7/1000 | Loss: 0.00005859
Iteration 8/1000 | Loss: 0.00005786
Iteration 9/1000 | Loss: 0.00005733
Iteration 10/1000 | Loss: 0.00005665
Iteration 11/1000 | Loss: 0.00005608
Iteration 12/1000 | Loss: 0.00005582
Iteration 13/1000 | Loss: 0.00005558
Iteration 14/1000 | Loss: 0.00005537
Iteration 15/1000 | Loss: 0.00005533
Iteration 16/1000 | Loss: 0.00005533
Iteration 17/1000 | Loss: 0.00005531
Iteration 18/1000 | Loss: 0.00005530
Iteration 19/1000 | Loss: 0.00005526
Iteration 20/1000 | Loss: 0.00005523
Iteration 21/1000 | Loss: 0.00005522
Iteration 22/1000 | Loss: 0.00005522
Iteration 23/1000 | Loss: 0.00005522
Iteration 24/1000 | Loss: 0.00005521
Iteration 25/1000 | Loss: 0.00005520
Iteration 26/1000 | Loss: 0.00005518
Iteration 27/1000 | Loss: 0.00005514
Iteration 28/1000 | Loss: 0.00005514
Iteration 29/1000 | Loss: 0.00005511
Iteration 30/1000 | Loss: 0.00005510
Iteration 31/1000 | Loss: 0.00005510
Iteration 32/1000 | Loss: 0.00005510
Iteration 33/1000 | Loss: 0.00005510
Iteration 34/1000 | Loss: 0.00005510
Iteration 35/1000 | Loss: 0.00005510
Iteration 36/1000 | Loss: 0.00005510
Iteration 37/1000 | Loss: 0.00005510
Iteration 38/1000 | Loss: 0.00005510
Iteration 39/1000 | Loss: 0.00005509
Iteration 40/1000 | Loss: 0.00005509
Iteration 41/1000 | Loss: 0.00005509
Iteration 42/1000 | Loss: 0.00005509
Iteration 43/1000 | Loss: 0.00005509
Iteration 44/1000 | Loss: 0.00005509
Iteration 45/1000 | Loss: 0.00005509
Iteration 46/1000 | Loss: 0.00005509
Iteration 47/1000 | Loss: 0.00005509
Iteration 48/1000 | Loss: 0.00005509
Iteration 49/1000 | Loss: 0.00005509
Iteration 50/1000 | Loss: 0.00005509
Iteration 51/1000 | Loss: 0.00005509
Iteration 52/1000 | Loss: 0.00005509
Iteration 53/1000 | Loss: 0.00005509
Iteration 54/1000 | Loss: 0.00005509
Iteration 55/1000 | Loss: 0.00005509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [5.5094369599828497e-05, 5.5094369599828497e-05, 5.5094369599828497e-05, 5.5094369599828497e-05, 5.5094369599828497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.5094369599828497e-05

Optimization complete. Final v2v error: 6.5410284996032715 mm

Highest mean error: 6.712861061096191 mm for frame 59

Lowest mean error: 6.403421401977539 mm for frame 154

Saving results

Total time: 32.765809059143066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513081
Iteration 2/25 | Loss: 0.00246983
Iteration 3/25 | Loss: 0.00236009
Iteration 4/25 | Loss: 0.00233665
Iteration 5/25 | Loss: 0.00232792
Iteration 6/25 | Loss: 0.00232677
Iteration 7/25 | Loss: 0.00232677
Iteration 8/25 | Loss: 0.00232677
Iteration 9/25 | Loss: 0.00232677
Iteration 10/25 | Loss: 0.00232677
Iteration 11/25 | Loss: 0.00232677
Iteration 12/25 | Loss: 0.00232677
Iteration 13/25 | Loss: 0.00232677
Iteration 14/25 | Loss: 0.00232677
Iteration 15/25 | Loss: 0.00232677
Iteration 16/25 | Loss: 0.00232677
Iteration 17/25 | Loss: 0.00232677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023267685901373625, 0.0023267685901373625, 0.0023267685901373625, 0.0023267685901373625, 0.0023267685901373625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023267685901373625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38271570
Iteration 2/25 | Loss: 0.00273922
Iteration 3/25 | Loss: 0.00273922
Iteration 4/25 | Loss: 0.00273922
Iteration 5/25 | Loss: 0.00273922
Iteration 6/25 | Loss: 0.00273922
Iteration 7/25 | Loss: 0.00273922
Iteration 8/25 | Loss: 0.00273922
Iteration 9/25 | Loss: 0.00273922
Iteration 10/25 | Loss: 0.00273922
Iteration 11/25 | Loss: 0.00273922
Iteration 12/25 | Loss: 0.00273922
Iteration 13/25 | Loss: 0.00273922
Iteration 14/25 | Loss: 0.00273922
Iteration 15/25 | Loss: 0.00273922
Iteration 16/25 | Loss: 0.00273922
Iteration 17/25 | Loss: 0.00273922
Iteration 18/25 | Loss: 0.00273922
Iteration 19/25 | Loss: 0.00273922
Iteration 20/25 | Loss: 0.00273922
Iteration 21/25 | Loss: 0.00273922
Iteration 22/25 | Loss: 0.00273922
Iteration 23/25 | Loss: 0.00273922
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0027392152696847916, 0.0027392152696847916, 0.0027392152696847916, 0.0027392152696847916, 0.0027392152696847916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027392152696847916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273922
Iteration 2/1000 | Loss: 0.00015103
Iteration 3/1000 | Loss: 0.00009609
Iteration 4/1000 | Loss: 0.00007494
Iteration 5/1000 | Loss: 0.00006563
Iteration 6/1000 | Loss: 0.00006164
Iteration 7/1000 | Loss: 0.00005975
Iteration 8/1000 | Loss: 0.00005807
Iteration 9/1000 | Loss: 0.00005722
Iteration 10/1000 | Loss: 0.00005612
Iteration 11/1000 | Loss: 0.00005526
Iteration 12/1000 | Loss: 0.00005474
Iteration 13/1000 | Loss: 0.00005430
Iteration 14/1000 | Loss: 0.00005411
Iteration 15/1000 | Loss: 0.00005393
Iteration 16/1000 | Loss: 0.00005376
Iteration 17/1000 | Loss: 0.00005363
Iteration 18/1000 | Loss: 0.00005360
Iteration 19/1000 | Loss: 0.00005355
Iteration 20/1000 | Loss: 0.00005354
Iteration 21/1000 | Loss: 0.00005354
Iteration 22/1000 | Loss: 0.00005354
Iteration 23/1000 | Loss: 0.00005353
Iteration 24/1000 | Loss: 0.00005351
Iteration 25/1000 | Loss: 0.00005351
Iteration 26/1000 | Loss: 0.00005351
Iteration 27/1000 | Loss: 0.00005350
Iteration 28/1000 | Loss: 0.00005350
Iteration 29/1000 | Loss: 0.00005350
Iteration 30/1000 | Loss: 0.00005349
Iteration 31/1000 | Loss: 0.00005349
Iteration 32/1000 | Loss: 0.00005349
Iteration 33/1000 | Loss: 0.00005349
Iteration 34/1000 | Loss: 0.00005348
Iteration 35/1000 | Loss: 0.00005348
Iteration 36/1000 | Loss: 0.00005347
Iteration 37/1000 | Loss: 0.00005347
Iteration 38/1000 | Loss: 0.00005347
Iteration 39/1000 | Loss: 0.00005347
Iteration 40/1000 | Loss: 0.00005347
Iteration 41/1000 | Loss: 0.00005346
Iteration 42/1000 | Loss: 0.00005346
Iteration 43/1000 | Loss: 0.00005345
Iteration 44/1000 | Loss: 0.00005345
Iteration 45/1000 | Loss: 0.00005345
Iteration 46/1000 | Loss: 0.00005345
Iteration 47/1000 | Loss: 0.00005345
Iteration 48/1000 | Loss: 0.00005344
Iteration 49/1000 | Loss: 0.00005344
Iteration 50/1000 | Loss: 0.00005344
Iteration 51/1000 | Loss: 0.00005344
Iteration 52/1000 | Loss: 0.00005344
Iteration 53/1000 | Loss: 0.00005344
Iteration 54/1000 | Loss: 0.00005344
Iteration 55/1000 | Loss: 0.00005343
Iteration 56/1000 | Loss: 0.00005343
Iteration 57/1000 | Loss: 0.00005343
Iteration 58/1000 | Loss: 0.00005343
Iteration 59/1000 | Loss: 0.00005343
Iteration 60/1000 | Loss: 0.00005343
Iteration 61/1000 | Loss: 0.00005343
Iteration 62/1000 | Loss: 0.00005343
Iteration 63/1000 | Loss: 0.00005343
Iteration 64/1000 | Loss: 0.00005343
Iteration 65/1000 | Loss: 0.00005343
Iteration 66/1000 | Loss: 0.00005343
Iteration 67/1000 | Loss: 0.00005343
Iteration 68/1000 | Loss: 0.00005343
Iteration 69/1000 | Loss: 0.00005342
Iteration 70/1000 | Loss: 0.00005342
Iteration 71/1000 | Loss: 0.00005342
Iteration 72/1000 | Loss: 0.00005342
Iteration 73/1000 | Loss: 0.00005342
Iteration 74/1000 | Loss: 0.00005342
Iteration 75/1000 | Loss: 0.00005342
Iteration 76/1000 | Loss: 0.00005342
Iteration 77/1000 | Loss: 0.00005342
Iteration 78/1000 | Loss: 0.00005342
Iteration 79/1000 | Loss: 0.00005342
Iteration 80/1000 | Loss: 0.00005342
Iteration 81/1000 | Loss: 0.00005342
Iteration 82/1000 | Loss: 0.00005342
Iteration 83/1000 | Loss: 0.00005342
Iteration 84/1000 | Loss: 0.00005342
Iteration 85/1000 | Loss: 0.00005342
Iteration 86/1000 | Loss: 0.00005342
Iteration 87/1000 | Loss: 0.00005342
Iteration 88/1000 | Loss: 0.00005342
Iteration 89/1000 | Loss: 0.00005341
Iteration 90/1000 | Loss: 0.00005341
Iteration 91/1000 | Loss: 0.00005341
Iteration 92/1000 | Loss: 0.00005341
Iteration 93/1000 | Loss: 0.00005341
Iteration 94/1000 | Loss: 0.00005341
Iteration 95/1000 | Loss: 0.00005341
Iteration 96/1000 | Loss: 0.00005341
Iteration 97/1000 | Loss: 0.00005341
Iteration 98/1000 | Loss: 0.00005341
Iteration 99/1000 | Loss: 0.00005341
Iteration 100/1000 | Loss: 0.00005341
Iteration 101/1000 | Loss: 0.00005340
Iteration 102/1000 | Loss: 0.00005340
Iteration 103/1000 | Loss: 0.00005340
Iteration 104/1000 | Loss: 0.00005340
Iteration 105/1000 | Loss: 0.00005340
Iteration 106/1000 | Loss: 0.00005340
Iteration 107/1000 | Loss: 0.00005340
Iteration 108/1000 | Loss: 0.00005340
Iteration 109/1000 | Loss: 0.00005340
Iteration 110/1000 | Loss: 0.00005340
Iteration 111/1000 | Loss: 0.00005340
Iteration 112/1000 | Loss: 0.00005340
Iteration 113/1000 | Loss: 0.00005340
Iteration 114/1000 | Loss: 0.00005340
Iteration 115/1000 | Loss: 0.00005340
Iteration 116/1000 | Loss: 0.00005340
Iteration 117/1000 | Loss: 0.00005340
Iteration 118/1000 | Loss: 0.00005340
Iteration 119/1000 | Loss: 0.00005340
Iteration 120/1000 | Loss: 0.00005339
Iteration 121/1000 | Loss: 0.00005339
Iteration 122/1000 | Loss: 0.00005339
Iteration 123/1000 | Loss: 0.00005339
Iteration 124/1000 | Loss: 0.00005339
Iteration 125/1000 | Loss: 0.00005339
Iteration 126/1000 | Loss: 0.00005339
Iteration 127/1000 | Loss: 0.00005339
Iteration 128/1000 | Loss: 0.00005339
Iteration 129/1000 | Loss: 0.00005339
Iteration 130/1000 | Loss: 0.00005339
Iteration 131/1000 | Loss: 0.00005339
Iteration 132/1000 | Loss: 0.00005338
Iteration 133/1000 | Loss: 0.00005338
Iteration 134/1000 | Loss: 0.00005338
Iteration 135/1000 | Loss: 0.00005338
Iteration 136/1000 | Loss: 0.00005338
Iteration 137/1000 | Loss: 0.00005338
Iteration 138/1000 | Loss: 0.00005338
Iteration 139/1000 | Loss: 0.00005338
Iteration 140/1000 | Loss: 0.00005338
Iteration 141/1000 | Loss: 0.00005338
Iteration 142/1000 | Loss: 0.00005338
Iteration 143/1000 | Loss: 0.00005338
Iteration 144/1000 | Loss: 0.00005338
Iteration 145/1000 | Loss: 0.00005338
Iteration 146/1000 | Loss: 0.00005338
Iteration 147/1000 | Loss: 0.00005338
Iteration 148/1000 | Loss: 0.00005338
Iteration 149/1000 | Loss: 0.00005338
Iteration 150/1000 | Loss: 0.00005338
Iteration 151/1000 | Loss: 0.00005338
Iteration 152/1000 | Loss: 0.00005338
Iteration 153/1000 | Loss: 0.00005338
Iteration 154/1000 | Loss: 0.00005338
Iteration 155/1000 | Loss: 0.00005338
Iteration 156/1000 | Loss: 0.00005338
Iteration 157/1000 | Loss: 0.00005338
Iteration 158/1000 | Loss: 0.00005338
Iteration 159/1000 | Loss: 0.00005338
Iteration 160/1000 | Loss: 0.00005338
Iteration 161/1000 | Loss: 0.00005338
Iteration 162/1000 | Loss: 0.00005338
Iteration 163/1000 | Loss: 0.00005338
Iteration 164/1000 | Loss: 0.00005338
Iteration 165/1000 | Loss: 0.00005338
Iteration 166/1000 | Loss: 0.00005338
Iteration 167/1000 | Loss: 0.00005338
Iteration 168/1000 | Loss: 0.00005338
Iteration 169/1000 | Loss: 0.00005338
Iteration 170/1000 | Loss: 0.00005338
Iteration 171/1000 | Loss: 0.00005338
Iteration 172/1000 | Loss: 0.00005338
Iteration 173/1000 | Loss: 0.00005338
Iteration 174/1000 | Loss: 0.00005338
Iteration 175/1000 | Loss: 0.00005338
Iteration 176/1000 | Loss: 0.00005338
Iteration 177/1000 | Loss: 0.00005338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [5.337688708095811e-05, 5.337688708095811e-05, 5.337688708095811e-05, 5.337688708095811e-05, 5.337688708095811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.337688708095811e-05

Optimization complete. Final v2v error: 6.435097694396973 mm

Highest mean error: 6.574400901794434 mm for frame 54

Lowest mean error: 6.307483673095703 mm for frame 5

Saving results

Total time: 41.78706693649292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00562837
Iteration 2/25 | Loss: 0.00239881
Iteration 3/25 | Loss: 0.00231507
Iteration 4/25 | Loss: 0.00230332
Iteration 5/25 | Loss: 0.00229912
Iteration 6/25 | Loss: 0.00229881
Iteration 7/25 | Loss: 0.00229881
Iteration 8/25 | Loss: 0.00229881
Iteration 9/25 | Loss: 0.00229881
Iteration 10/25 | Loss: 0.00229881
Iteration 11/25 | Loss: 0.00229881
Iteration 12/25 | Loss: 0.00229881
Iteration 13/25 | Loss: 0.00229881
Iteration 14/25 | Loss: 0.00229881
Iteration 15/25 | Loss: 0.00229881
Iteration 16/25 | Loss: 0.00229881
Iteration 17/25 | Loss: 0.00229881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022988058626651764, 0.0022988058626651764, 0.0022988058626651764, 0.0022988058626651764, 0.0022988058626651764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022988058626651764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.39234543
Iteration 2/25 | Loss: 0.00268112
Iteration 3/25 | Loss: 0.00268109
Iteration 4/25 | Loss: 0.00268109
Iteration 5/25 | Loss: 0.00268109
Iteration 6/25 | Loss: 0.00268109
Iteration 7/25 | Loss: 0.00268109
Iteration 8/25 | Loss: 0.00268109
Iteration 9/25 | Loss: 0.00268109
Iteration 10/25 | Loss: 0.00268109
Iteration 11/25 | Loss: 0.00268109
Iteration 12/25 | Loss: 0.00268109
Iteration 13/25 | Loss: 0.00268109
Iteration 14/25 | Loss: 0.00268109
Iteration 15/25 | Loss: 0.00268109
Iteration 16/25 | Loss: 0.00268109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026810860726982355, 0.0026810860726982355, 0.0026810860726982355, 0.0026810860726982355, 0.0026810860726982355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026810860726982355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268109
Iteration 2/1000 | Loss: 0.00011164
Iteration 3/1000 | Loss: 0.00007848
Iteration 4/1000 | Loss: 0.00006870
Iteration 5/1000 | Loss: 0.00006489
Iteration 6/1000 | Loss: 0.00006278
Iteration 7/1000 | Loss: 0.00006154
Iteration 8/1000 | Loss: 0.00006087
Iteration 9/1000 | Loss: 0.00006013
Iteration 10/1000 | Loss: 0.00005955
Iteration 11/1000 | Loss: 0.00005916
Iteration 12/1000 | Loss: 0.00005888
Iteration 13/1000 | Loss: 0.00005868
Iteration 14/1000 | Loss: 0.00005847
Iteration 15/1000 | Loss: 0.00005841
Iteration 16/1000 | Loss: 0.00005835
Iteration 17/1000 | Loss: 0.00005835
Iteration 18/1000 | Loss: 0.00005834
Iteration 19/1000 | Loss: 0.00005831
Iteration 20/1000 | Loss: 0.00005831
Iteration 21/1000 | Loss: 0.00005831
Iteration 22/1000 | Loss: 0.00005830
Iteration 23/1000 | Loss: 0.00005830
Iteration 24/1000 | Loss: 0.00005829
Iteration 25/1000 | Loss: 0.00005829
Iteration 26/1000 | Loss: 0.00005829
Iteration 27/1000 | Loss: 0.00005829
Iteration 28/1000 | Loss: 0.00005829
Iteration 29/1000 | Loss: 0.00005829
Iteration 30/1000 | Loss: 0.00005829
Iteration 31/1000 | Loss: 0.00005828
Iteration 32/1000 | Loss: 0.00005828
Iteration 33/1000 | Loss: 0.00005828
Iteration 34/1000 | Loss: 0.00005828
Iteration 35/1000 | Loss: 0.00005827
Iteration 36/1000 | Loss: 0.00005827
Iteration 37/1000 | Loss: 0.00005827
Iteration 38/1000 | Loss: 0.00005827
Iteration 39/1000 | Loss: 0.00005827
Iteration 40/1000 | Loss: 0.00005827
Iteration 41/1000 | Loss: 0.00005827
Iteration 42/1000 | Loss: 0.00005826
Iteration 43/1000 | Loss: 0.00005826
Iteration 44/1000 | Loss: 0.00005826
Iteration 45/1000 | Loss: 0.00005826
Iteration 46/1000 | Loss: 0.00005826
Iteration 47/1000 | Loss: 0.00005826
Iteration 48/1000 | Loss: 0.00005826
Iteration 49/1000 | Loss: 0.00005826
Iteration 50/1000 | Loss: 0.00005826
Iteration 51/1000 | Loss: 0.00005826
Iteration 52/1000 | Loss: 0.00005826
Iteration 53/1000 | Loss: 0.00005826
Iteration 54/1000 | Loss: 0.00005826
Iteration 55/1000 | Loss: 0.00005826
Iteration 56/1000 | Loss: 0.00005825
Iteration 57/1000 | Loss: 0.00005825
Iteration 58/1000 | Loss: 0.00005825
Iteration 59/1000 | Loss: 0.00005825
Iteration 60/1000 | Loss: 0.00005825
Iteration 61/1000 | Loss: 0.00005824
Iteration 62/1000 | Loss: 0.00005824
Iteration 63/1000 | Loss: 0.00005824
Iteration 64/1000 | Loss: 0.00005824
Iteration 65/1000 | Loss: 0.00005824
Iteration 66/1000 | Loss: 0.00005824
Iteration 67/1000 | Loss: 0.00005824
Iteration 68/1000 | Loss: 0.00005823
Iteration 69/1000 | Loss: 0.00005823
Iteration 70/1000 | Loss: 0.00005823
Iteration 71/1000 | Loss: 0.00005823
Iteration 72/1000 | Loss: 0.00005823
Iteration 73/1000 | Loss: 0.00005823
Iteration 74/1000 | Loss: 0.00005823
Iteration 75/1000 | Loss: 0.00005823
Iteration 76/1000 | Loss: 0.00005822
Iteration 77/1000 | Loss: 0.00005822
Iteration 78/1000 | Loss: 0.00005822
Iteration 79/1000 | Loss: 0.00005822
Iteration 80/1000 | Loss: 0.00005822
Iteration 81/1000 | Loss: 0.00005822
Iteration 82/1000 | Loss: 0.00005822
Iteration 83/1000 | Loss: 0.00005821
Iteration 84/1000 | Loss: 0.00005821
Iteration 85/1000 | Loss: 0.00005821
Iteration 86/1000 | Loss: 0.00005821
Iteration 87/1000 | Loss: 0.00005821
Iteration 88/1000 | Loss: 0.00005820
Iteration 89/1000 | Loss: 0.00005820
Iteration 90/1000 | Loss: 0.00005820
Iteration 91/1000 | Loss: 0.00005820
Iteration 92/1000 | Loss: 0.00005820
Iteration 93/1000 | Loss: 0.00005820
Iteration 94/1000 | Loss: 0.00005820
Iteration 95/1000 | Loss: 0.00005820
Iteration 96/1000 | Loss: 0.00005820
Iteration 97/1000 | Loss: 0.00005820
Iteration 98/1000 | Loss: 0.00005820
Iteration 99/1000 | Loss: 0.00005820
Iteration 100/1000 | Loss: 0.00005820
Iteration 101/1000 | Loss: 0.00005820
Iteration 102/1000 | Loss: 0.00005820
Iteration 103/1000 | Loss: 0.00005820
Iteration 104/1000 | Loss: 0.00005820
Iteration 105/1000 | Loss: 0.00005820
Iteration 106/1000 | Loss: 0.00005820
Iteration 107/1000 | Loss: 0.00005820
Iteration 108/1000 | Loss: 0.00005820
Iteration 109/1000 | Loss: 0.00005820
Iteration 110/1000 | Loss: 0.00005820
Iteration 111/1000 | Loss: 0.00005820
Iteration 112/1000 | Loss: 0.00005820
Iteration 113/1000 | Loss: 0.00005820
Iteration 114/1000 | Loss: 0.00005820
Iteration 115/1000 | Loss: 0.00005820
Iteration 116/1000 | Loss: 0.00005820
Iteration 117/1000 | Loss: 0.00005820
Iteration 118/1000 | Loss: 0.00005820
Iteration 119/1000 | Loss: 0.00005820
Iteration 120/1000 | Loss: 0.00005820
Iteration 121/1000 | Loss: 0.00005820
Iteration 122/1000 | Loss: 0.00005820
Iteration 123/1000 | Loss: 0.00005820
Iteration 124/1000 | Loss: 0.00005820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [5.820109436172061e-05, 5.820109436172061e-05, 5.820109436172061e-05, 5.820109436172061e-05, 5.820109436172061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.820109436172061e-05

Optimization complete. Final v2v error: 6.768264293670654 mm

Highest mean error: 7.003761291503906 mm for frame 148

Lowest mean error: 6.553475856781006 mm for frame 3

Saving results

Total time: 36.881316900253296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01196130
Iteration 2/25 | Loss: 0.00264112
Iteration 3/25 | Loss: 0.00231831
Iteration 4/25 | Loss: 0.00229610
Iteration 5/25 | Loss: 0.00225317
Iteration 6/25 | Loss: 0.00221426
Iteration 7/25 | Loss: 0.00220063
Iteration 8/25 | Loss: 0.00217905
Iteration 9/25 | Loss: 0.00218177
Iteration 10/25 | Loss: 0.00217759
Iteration 11/25 | Loss: 0.00216319
Iteration 12/25 | Loss: 0.00217030
Iteration 13/25 | Loss: 0.00216731
Iteration 14/25 | Loss: 0.00216612
Iteration 15/25 | Loss: 0.00216398
Iteration 16/25 | Loss: 0.00216070
Iteration 17/25 | Loss: 0.00216550
Iteration 18/25 | Loss: 0.00216137
Iteration 19/25 | Loss: 0.00215545
Iteration 20/25 | Loss: 0.00216094
Iteration 21/25 | Loss: 0.00215528
Iteration 22/25 | Loss: 0.00215526
Iteration 23/25 | Loss: 0.00215526
Iteration 24/25 | Loss: 0.00215526
Iteration 25/25 | Loss: 0.00215526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47797441
Iteration 2/25 | Loss: 0.00241025
Iteration 3/25 | Loss: 0.00241025
Iteration 4/25 | Loss: 0.00241025
Iteration 5/25 | Loss: 0.00241025
Iteration 6/25 | Loss: 0.00241025
Iteration 7/25 | Loss: 0.00241025
Iteration 8/25 | Loss: 0.00241025
Iteration 9/25 | Loss: 0.00241025
Iteration 10/25 | Loss: 0.00241025
Iteration 11/25 | Loss: 0.00241025
Iteration 12/25 | Loss: 0.00241025
Iteration 13/25 | Loss: 0.00241025
Iteration 14/25 | Loss: 0.00241025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002410248154774308, 0.002410248154774308, 0.002410248154774308, 0.002410248154774308, 0.002410248154774308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002410248154774308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241025
Iteration 2/1000 | Loss: 0.00012353
Iteration 3/1000 | Loss: 0.00008508
Iteration 4/1000 | Loss: 0.00007340
Iteration 5/1000 | Loss: 0.00006759
Iteration 6/1000 | Loss: 0.00006428
Iteration 7/1000 | Loss: 0.00006230
Iteration 8/1000 | Loss: 0.00095317
Iteration 9/1000 | Loss: 0.00007319
Iteration 10/1000 | Loss: 0.00006038
Iteration 11/1000 | Loss: 0.00005657
Iteration 12/1000 | Loss: 0.00005427
Iteration 13/1000 | Loss: 0.00005321
Iteration 14/1000 | Loss: 0.00005270
Iteration 15/1000 | Loss: 0.00005238
Iteration 16/1000 | Loss: 0.00005201
Iteration 17/1000 | Loss: 0.00005173
Iteration 18/1000 | Loss: 0.00005147
Iteration 19/1000 | Loss: 0.00005141
Iteration 20/1000 | Loss: 0.00005140
Iteration 21/1000 | Loss: 0.00005138
Iteration 22/1000 | Loss: 0.00005138
Iteration 23/1000 | Loss: 0.00005135
Iteration 24/1000 | Loss: 0.00005135
Iteration 25/1000 | Loss: 0.00005134
Iteration 26/1000 | Loss: 0.00005134
Iteration 27/1000 | Loss: 0.00005134
Iteration 28/1000 | Loss: 0.00005134
Iteration 29/1000 | Loss: 0.00005133
Iteration 30/1000 | Loss: 0.00005133
Iteration 31/1000 | Loss: 0.00005133
Iteration 32/1000 | Loss: 0.00005133
Iteration 33/1000 | Loss: 0.00005133
Iteration 34/1000 | Loss: 0.00005132
Iteration 35/1000 | Loss: 0.00005132
Iteration 36/1000 | Loss: 0.00005130
Iteration 37/1000 | Loss: 0.00005129
Iteration 38/1000 | Loss: 0.00005128
Iteration 39/1000 | Loss: 0.00005128
Iteration 40/1000 | Loss: 0.00005127
Iteration 41/1000 | Loss: 0.00005127
Iteration 42/1000 | Loss: 0.00005127
Iteration 43/1000 | Loss: 0.00005127
Iteration 44/1000 | Loss: 0.00005126
Iteration 45/1000 | Loss: 0.00005126
Iteration 46/1000 | Loss: 0.00005126
Iteration 47/1000 | Loss: 0.00005126
Iteration 48/1000 | Loss: 0.00005125
Iteration 49/1000 | Loss: 0.00005125
Iteration 50/1000 | Loss: 0.00005125
Iteration 51/1000 | Loss: 0.00005125
Iteration 52/1000 | Loss: 0.00005125
Iteration 53/1000 | Loss: 0.00005124
Iteration 54/1000 | Loss: 0.00005124
Iteration 55/1000 | Loss: 0.00005124
Iteration 56/1000 | Loss: 0.00005123
Iteration 57/1000 | Loss: 0.00005123
Iteration 58/1000 | Loss: 0.00005123
Iteration 59/1000 | Loss: 0.00005123
Iteration 60/1000 | Loss: 0.00005122
Iteration 61/1000 | Loss: 0.00005122
Iteration 62/1000 | Loss: 0.00005122
Iteration 63/1000 | Loss: 0.00005122
Iteration 64/1000 | Loss: 0.00005122
Iteration 65/1000 | Loss: 0.00005122
Iteration 66/1000 | Loss: 0.00005122
Iteration 67/1000 | Loss: 0.00005122
Iteration 68/1000 | Loss: 0.00005122
Iteration 69/1000 | Loss: 0.00005122
Iteration 70/1000 | Loss: 0.00005122
Iteration 71/1000 | Loss: 0.00005122
Iteration 72/1000 | Loss: 0.00005122
Iteration 73/1000 | Loss: 0.00005122
Iteration 74/1000 | Loss: 0.00005122
Iteration 75/1000 | Loss: 0.00005122
Iteration 76/1000 | Loss: 0.00005122
Iteration 77/1000 | Loss: 0.00005122
Iteration 78/1000 | Loss: 0.00005122
Iteration 79/1000 | Loss: 0.00005122
Iteration 80/1000 | Loss: 0.00005122
Iteration 81/1000 | Loss: 0.00005122
Iteration 82/1000 | Loss: 0.00005122
Iteration 83/1000 | Loss: 0.00005122
Iteration 84/1000 | Loss: 0.00005122
Iteration 85/1000 | Loss: 0.00005122
Iteration 86/1000 | Loss: 0.00005122
Iteration 87/1000 | Loss: 0.00005122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [5.121721187606454e-05, 5.121721187606454e-05, 5.121721187606454e-05, 5.121721187606454e-05, 5.121721187606454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.121721187606454e-05

Optimization complete. Final v2v error: 6.320399284362793 mm

Highest mean error: 12.303186416625977 mm for frame 21

Lowest mean error: 6.046290397644043 mm for frame 13

Saving results

Total time: 67.08858156204224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01132326
Iteration 2/25 | Loss: 0.00373175
Iteration 3/25 | Loss: 0.00298011
Iteration 4/25 | Loss: 0.00296849
Iteration 5/25 | Loss: 0.00289181
Iteration 6/25 | Loss: 0.00283974
Iteration 7/25 | Loss: 0.00273081
Iteration 8/25 | Loss: 0.00265147
Iteration 9/25 | Loss: 0.00265417
Iteration 10/25 | Loss: 0.00261715
Iteration 11/25 | Loss: 0.00260314
Iteration 12/25 | Loss: 0.00259560
Iteration 13/25 | Loss: 0.00257536
Iteration 14/25 | Loss: 0.00259696
Iteration 15/25 | Loss: 0.00258627
Iteration 16/25 | Loss: 0.00256875
Iteration 17/25 | Loss: 0.00255760
Iteration 18/25 | Loss: 0.00254715
Iteration 19/25 | Loss: 0.00254567
Iteration 20/25 | Loss: 0.00254438
Iteration 21/25 | Loss: 0.00254655
Iteration 22/25 | Loss: 0.00254240
Iteration 23/25 | Loss: 0.00254447
Iteration 24/25 | Loss: 0.00253905
Iteration 25/25 | Loss: 0.00254787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23065293
Iteration 2/25 | Loss: 0.00507848
Iteration 3/25 | Loss: 0.00507846
Iteration 4/25 | Loss: 0.00507846
Iteration 5/25 | Loss: 0.00507846
Iteration 6/25 | Loss: 0.00507846
Iteration 7/25 | Loss: 0.00507846
Iteration 8/25 | Loss: 0.00507846
Iteration 9/25 | Loss: 0.00507846
Iteration 10/25 | Loss: 0.00507846
Iteration 11/25 | Loss: 0.00507846
Iteration 12/25 | Loss: 0.00507846
Iteration 13/25 | Loss: 0.00507846
Iteration 14/25 | Loss: 0.00507846
Iteration 15/25 | Loss: 0.00507846
Iteration 16/25 | Loss: 0.00507846
Iteration 17/25 | Loss: 0.00507846
Iteration 18/25 | Loss: 0.00507846
Iteration 19/25 | Loss: 0.00507846
Iteration 20/25 | Loss: 0.00507846
Iteration 21/25 | Loss: 0.00507846
Iteration 22/25 | Loss: 0.00507846
Iteration 23/25 | Loss: 0.00507846
Iteration 24/25 | Loss: 0.00507846
Iteration 25/25 | Loss: 0.00507846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00507846
Iteration 2/1000 | Loss: 0.00101647
Iteration 3/1000 | Loss: 0.00084143
Iteration 4/1000 | Loss: 0.00231487
Iteration 5/1000 | Loss: 0.00139695
Iteration 6/1000 | Loss: 0.00039039
Iteration 7/1000 | Loss: 0.00181812
Iteration 8/1000 | Loss: 0.00159629
Iteration 9/1000 | Loss: 0.00040145
Iteration 10/1000 | Loss: 0.00184512
Iteration 11/1000 | Loss: 0.00035932
Iteration 12/1000 | Loss: 0.00094558
Iteration 13/1000 | Loss: 0.00184559
Iteration 14/1000 | Loss: 0.00086364
Iteration 15/1000 | Loss: 0.00026166
Iteration 16/1000 | Loss: 0.00116608
Iteration 17/1000 | Loss: 0.00094608
Iteration 18/1000 | Loss: 0.00020257
Iteration 19/1000 | Loss: 0.00305326
Iteration 20/1000 | Loss: 0.00189462
Iteration 21/1000 | Loss: 0.00456029
Iteration 22/1000 | Loss: 0.00268341
Iteration 23/1000 | Loss: 0.00415931
Iteration 24/1000 | Loss: 0.00124045
Iteration 25/1000 | Loss: 0.00148744
Iteration 26/1000 | Loss: 0.00128282
Iteration 27/1000 | Loss: 0.00193209
Iteration 28/1000 | Loss: 0.00161541
Iteration 29/1000 | Loss: 0.00217863
Iteration 30/1000 | Loss: 0.00230331
Iteration 31/1000 | Loss: 0.00340262
Iteration 32/1000 | Loss: 0.00401121
Iteration 33/1000 | Loss: 0.00160504
Iteration 34/1000 | Loss: 0.00166895
Iteration 35/1000 | Loss: 0.00117465
Iteration 36/1000 | Loss: 0.00191959
Iteration 37/1000 | Loss: 0.00297204
Iteration 38/1000 | Loss: 0.00205350
Iteration 39/1000 | Loss: 0.00162883
Iteration 40/1000 | Loss: 0.00185843
Iteration 41/1000 | Loss: 0.00170385
Iteration 42/1000 | Loss: 0.00240906
Iteration 43/1000 | Loss: 0.00090843
Iteration 44/1000 | Loss: 0.00120866
Iteration 45/1000 | Loss: 0.00084018
Iteration 46/1000 | Loss: 0.00084437
Iteration 47/1000 | Loss: 0.00068823
Iteration 48/1000 | Loss: 0.00236749
Iteration 49/1000 | Loss: 0.00196856
Iteration 50/1000 | Loss: 0.00103285
Iteration 51/1000 | Loss: 0.00145099
Iteration 52/1000 | Loss: 0.00119475
Iteration 53/1000 | Loss: 0.00053490
Iteration 54/1000 | Loss: 0.00144104
Iteration 55/1000 | Loss: 0.00196616
Iteration 56/1000 | Loss: 0.00188329
Iteration 57/1000 | Loss: 0.00101513
Iteration 58/1000 | Loss: 0.00089173
Iteration 59/1000 | Loss: 0.00045695
Iteration 60/1000 | Loss: 0.00067274
Iteration 61/1000 | Loss: 0.00107569
Iteration 62/1000 | Loss: 0.00131095
Iteration 63/1000 | Loss: 0.00052045
Iteration 64/1000 | Loss: 0.00095213
Iteration 65/1000 | Loss: 0.00083154
Iteration 66/1000 | Loss: 0.00080035
Iteration 67/1000 | Loss: 0.00038043
Iteration 68/1000 | Loss: 0.00097987
Iteration 69/1000 | Loss: 0.00144821
Iteration 70/1000 | Loss: 0.00061412
Iteration 71/1000 | Loss: 0.00107421
Iteration 72/1000 | Loss: 0.00281626
Iteration 73/1000 | Loss: 0.00057436
Iteration 74/1000 | Loss: 0.00086870
Iteration 75/1000 | Loss: 0.00115060
Iteration 76/1000 | Loss: 0.00047114
Iteration 77/1000 | Loss: 0.00039633
Iteration 78/1000 | Loss: 0.00046965
Iteration 79/1000 | Loss: 0.00091915
Iteration 80/1000 | Loss: 0.00097400
Iteration 81/1000 | Loss: 0.00054575
Iteration 82/1000 | Loss: 0.00093697
Iteration 83/1000 | Loss: 0.00023952
Iteration 84/1000 | Loss: 0.00037000
Iteration 85/1000 | Loss: 0.00066372
Iteration 86/1000 | Loss: 0.00038178
Iteration 87/1000 | Loss: 0.00062975
Iteration 88/1000 | Loss: 0.00050011
Iteration 89/1000 | Loss: 0.00071276
Iteration 90/1000 | Loss: 0.00087300
Iteration 91/1000 | Loss: 0.00019249
Iteration 92/1000 | Loss: 0.00035603
Iteration 93/1000 | Loss: 0.00020885
Iteration 94/1000 | Loss: 0.00015696
Iteration 95/1000 | Loss: 0.00025581
Iteration 96/1000 | Loss: 0.00018428
Iteration 97/1000 | Loss: 0.00027852
Iteration 98/1000 | Loss: 0.00017286
Iteration 99/1000 | Loss: 0.00080133
Iteration 100/1000 | Loss: 0.00025220
Iteration 101/1000 | Loss: 0.00022167
Iteration 102/1000 | Loss: 0.00023119
Iteration 103/1000 | Loss: 0.00026195
Iteration 104/1000 | Loss: 0.00058639
Iteration 105/1000 | Loss: 0.00031703
Iteration 106/1000 | Loss: 0.00012117
Iteration 107/1000 | Loss: 0.00014015
Iteration 108/1000 | Loss: 0.00013692
Iteration 109/1000 | Loss: 0.00013503
Iteration 110/1000 | Loss: 0.00052280
Iteration 111/1000 | Loss: 0.00030874
Iteration 112/1000 | Loss: 0.00051513
Iteration 113/1000 | Loss: 0.00021370
Iteration 114/1000 | Loss: 0.00019546
Iteration 115/1000 | Loss: 0.00016990
Iteration 116/1000 | Loss: 0.00025539
Iteration 117/1000 | Loss: 0.00020371
Iteration 118/1000 | Loss: 0.00014058
Iteration 119/1000 | Loss: 0.00014614
Iteration 120/1000 | Loss: 0.00015122
Iteration 121/1000 | Loss: 0.00013209
Iteration 122/1000 | Loss: 0.00016630
Iteration 123/1000 | Loss: 0.00014362
Iteration 124/1000 | Loss: 0.00013160
Iteration 125/1000 | Loss: 0.00017525
Iteration 126/1000 | Loss: 0.00011652
Iteration 127/1000 | Loss: 0.00010308
Iteration 128/1000 | Loss: 0.00009933
Iteration 129/1000 | Loss: 0.00009717
Iteration 130/1000 | Loss: 0.00009553
Iteration 131/1000 | Loss: 0.00010798
Iteration 132/1000 | Loss: 0.00013707
Iteration 133/1000 | Loss: 0.00010557
Iteration 134/1000 | Loss: 0.00009588
Iteration 135/1000 | Loss: 0.00009329
Iteration 136/1000 | Loss: 0.00013754
Iteration 137/1000 | Loss: 0.00009775
Iteration 138/1000 | Loss: 0.00009270
Iteration 139/1000 | Loss: 0.00009233
Iteration 140/1000 | Loss: 0.00009206
Iteration 141/1000 | Loss: 0.00013422
Iteration 142/1000 | Loss: 0.00009368
Iteration 143/1000 | Loss: 0.00010064
Iteration 144/1000 | Loss: 0.00009178
Iteration 145/1000 | Loss: 0.00009176
Iteration 146/1000 | Loss: 0.00009173
Iteration 147/1000 | Loss: 0.00010010
Iteration 148/1000 | Loss: 0.00009237
Iteration 149/1000 | Loss: 0.00009178
Iteration 150/1000 | Loss: 0.00009167
Iteration 151/1000 | Loss: 0.00009166
Iteration 152/1000 | Loss: 0.00009166
Iteration 153/1000 | Loss: 0.00009164
Iteration 154/1000 | Loss: 0.00009164
Iteration 155/1000 | Loss: 0.00009164
Iteration 156/1000 | Loss: 0.00009161
Iteration 157/1000 | Loss: 0.00009161
Iteration 158/1000 | Loss: 0.00009154
Iteration 159/1000 | Loss: 0.00009154
Iteration 160/1000 | Loss: 0.00009151
Iteration 161/1000 | Loss: 0.00009151
Iteration 162/1000 | Loss: 0.00009151
Iteration 163/1000 | Loss: 0.00009151
Iteration 164/1000 | Loss: 0.00009151
Iteration 165/1000 | Loss: 0.00009151
Iteration 166/1000 | Loss: 0.00009151
Iteration 167/1000 | Loss: 0.00009151
Iteration 168/1000 | Loss: 0.00009151
Iteration 169/1000 | Loss: 0.00009151
Iteration 170/1000 | Loss: 0.00009150
Iteration 171/1000 | Loss: 0.00009150
Iteration 172/1000 | Loss: 0.00009150
Iteration 173/1000 | Loss: 0.00009150
Iteration 174/1000 | Loss: 0.00009150
Iteration 175/1000 | Loss: 0.00009150
Iteration 176/1000 | Loss: 0.00009149
Iteration 177/1000 | Loss: 0.00009149
Iteration 178/1000 | Loss: 0.00009149
Iteration 179/1000 | Loss: 0.00009148
Iteration 180/1000 | Loss: 0.00009148
Iteration 181/1000 | Loss: 0.00009148
Iteration 182/1000 | Loss: 0.00009148
Iteration 183/1000 | Loss: 0.00009148
Iteration 184/1000 | Loss: 0.00009147
Iteration 185/1000 | Loss: 0.00009147
Iteration 186/1000 | Loss: 0.00009147
Iteration 187/1000 | Loss: 0.00009147
Iteration 188/1000 | Loss: 0.00009147
Iteration 189/1000 | Loss: 0.00009147
Iteration 190/1000 | Loss: 0.00009146
Iteration 191/1000 | Loss: 0.00009146
Iteration 192/1000 | Loss: 0.00009392
Iteration 193/1000 | Loss: 0.00009392
Iteration 194/1000 | Loss: 0.00009392
Iteration 195/1000 | Loss: 0.00009392
Iteration 196/1000 | Loss: 0.00009392
Iteration 197/1000 | Loss: 0.00009392
Iteration 198/1000 | Loss: 0.00009324
Iteration 199/1000 | Loss: 0.00049437
Iteration 200/1000 | Loss: 0.00014089
Iteration 201/1000 | Loss: 0.00032502
Iteration 202/1000 | Loss: 0.00057130
Iteration 203/1000 | Loss: 0.00030710
Iteration 204/1000 | Loss: 0.00014779
Iteration 205/1000 | Loss: 0.00009546
Iteration 206/1000 | Loss: 0.00009343
Iteration 207/1000 | Loss: 0.00009198
Iteration 208/1000 | Loss: 0.00009267
Iteration 209/1000 | Loss: 0.00009146
Iteration 210/1000 | Loss: 0.00009137
Iteration 211/1000 | Loss: 0.00009135
Iteration 212/1000 | Loss: 0.00009134
Iteration 213/1000 | Loss: 0.00009134
Iteration 214/1000 | Loss: 0.00009134
Iteration 215/1000 | Loss: 0.00009133
Iteration 216/1000 | Loss: 0.00009133
Iteration 217/1000 | Loss: 0.00009132
Iteration 218/1000 | Loss: 0.00009132
Iteration 219/1000 | Loss: 0.00009132
Iteration 220/1000 | Loss: 0.00009132
Iteration 221/1000 | Loss: 0.00009132
Iteration 222/1000 | Loss: 0.00009132
Iteration 223/1000 | Loss: 0.00009132
Iteration 224/1000 | Loss: 0.00009131
Iteration 225/1000 | Loss: 0.00009131
Iteration 226/1000 | Loss: 0.00009131
Iteration 227/1000 | Loss: 0.00009131
Iteration 228/1000 | Loss: 0.00009131
Iteration 229/1000 | Loss: 0.00009131
Iteration 230/1000 | Loss: 0.00009131
Iteration 231/1000 | Loss: 0.00009131
Iteration 232/1000 | Loss: 0.00009131
Iteration 233/1000 | Loss: 0.00009131
Iteration 234/1000 | Loss: 0.00009131
Iteration 235/1000 | Loss: 0.00009130
Iteration 236/1000 | Loss: 0.00009130
Iteration 237/1000 | Loss: 0.00009146
Iteration 238/1000 | Loss: 0.00009129
Iteration 239/1000 | Loss: 0.00009129
Iteration 240/1000 | Loss: 0.00009129
Iteration 241/1000 | Loss: 0.00009129
Iteration 242/1000 | Loss: 0.00009129
Iteration 243/1000 | Loss: 0.00009129
Iteration 244/1000 | Loss: 0.00009129
Iteration 245/1000 | Loss: 0.00009129
Iteration 246/1000 | Loss: 0.00009129
Iteration 247/1000 | Loss: 0.00009129
Iteration 248/1000 | Loss: 0.00009129
Iteration 249/1000 | Loss: 0.00009129
Iteration 250/1000 | Loss: 0.00009129
Iteration 251/1000 | Loss: 0.00009129
Iteration 252/1000 | Loss: 0.00009129
Iteration 253/1000 | Loss: 0.00009129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [9.12883406272158e-05, 9.12883406272158e-05, 9.12883406272158e-05, 9.12883406272158e-05, 9.12883406272158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.12883406272158e-05

Optimization complete. Final v2v error: 7.3969645500183105 mm

Highest mean error: 15.513028144836426 mm for frame 87

Lowest mean error: 6.221591472625732 mm for frame 165

Saving results

Total time: 286.14703941345215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849574
Iteration 2/25 | Loss: 0.00255898
Iteration 3/25 | Loss: 0.00236051
Iteration 4/25 | Loss: 0.00234116
Iteration 5/25 | Loss: 0.00233420
Iteration 6/25 | Loss: 0.00233315
Iteration 7/25 | Loss: 0.00233315
Iteration 8/25 | Loss: 0.00233315
Iteration 9/25 | Loss: 0.00233315
Iteration 10/25 | Loss: 0.00233315
Iteration 11/25 | Loss: 0.00233315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002333148382604122, 0.002333148382604122, 0.002333148382604122, 0.002333148382604122, 0.002333148382604122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002333148382604122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49611688
Iteration 2/25 | Loss: 0.00267683
Iteration 3/25 | Loss: 0.00267681
Iteration 4/25 | Loss: 0.00267681
Iteration 5/25 | Loss: 0.00267681
Iteration 6/25 | Loss: 0.00267681
Iteration 7/25 | Loss: 0.00267681
Iteration 8/25 | Loss: 0.00267681
Iteration 9/25 | Loss: 0.00267681
Iteration 10/25 | Loss: 0.00267681
Iteration 11/25 | Loss: 0.00267681
Iteration 12/25 | Loss: 0.00267681
Iteration 13/25 | Loss: 0.00267681
Iteration 14/25 | Loss: 0.00267681
Iteration 15/25 | Loss: 0.00267681
Iteration 16/25 | Loss: 0.00267681
Iteration 17/25 | Loss: 0.00267681
Iteration 18/25 | Loss: 0.00267681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002676805481314659, 0.002676805481314659, 0.002676805481314659, 0.002676805481314659, 0.002676805481314659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002676805481314659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267681
Iteration 2/1000 | Loss: 0.00010347
Iteration 3/1000 | Loss: 0.00007791
Iteration 4/1000 | Loss: 0.00007046
Iteration 5/1000 | Loss: 0.00006766
Iteration 6/1000 | Loss: 0.00006603
Iteration 7/1000 | Loss: 0.00006522
Iteration 8/1000 | Loss: 0.00006439
Iteration 9/1000 | Loss: 0.00006385
Iteration 10/1000 | Loss: 0.00006341
Iteration 11/1000 | Loss: 0.00006306
Iteration 12/1000 | Loss: 0.00006278
Iteration 13/1000 | Loss: 0.00006257
Iteration 14/1000 | Loss: 0.00006255
Iteration 15/1000 | Loss: 0.00006248
Iteration 16/1000 | Loss: 0.00006242
Iteration 17/1000 | Loss: 0.00006237
Iteration 18/1000 | Loss: 0.00006237
Iteration 19/1000 | Loss: 0.00006236
Iteration 20/1000 | Loss: 0.00006235
Iteration 21/1000 | Loss: 0.00006234
Iteration 22/1000 | Loss: 0.00006234
Iteration 23/1000 | Loss: 0.00006233
Iteration 24/1000 | Loss: 0.00006232
Iteration 25/1000 | Loss: 0.00006232
Iteration 26/1000 | Loss: 0.00006230
Iteration 27/1000 | Loss: 0.00006230
Iteration 28/1000 | Loss: 0.00006229
Iteration 29/1000 | Loss: 0.00006229
Iteration 30/1000 | Loss: 0.00006229
Iteration 31/1000 | Loss: 0.00006228
Iteration 32/1000 | Loss: 0.00006227
Iteration 33/1000 | Loss: 0.00006227
Iteration 34/1000 | Loss: 0.00006227
Iteration 35/1000 | Loss: 0.00006227
Iteration 36/1000 | Loss: 0.00006226
Iteration 37/1000 | Loss: 0.00006226
Iteration 38/1000 | Loss: 0.00006226
Iteration 39/1000 | Loss: 0.00006226
Iteration 40/1000 | Loss: 0.00006226
Iteration 41/1000 | Loss: 0.00006226
Iteration 42/1000 | Loss: 0.00006225
Iteration 43/1000 | Loss: 0.00006224
Iteration 44/1000 | Loss: 0.00006223
Iteration 45/1000 | Loss: 0.00006223
Iteration 46/1000 | Loss: 0.00006223
Iteration 47/1000 | Loss: 0.00006222
Iteration 48/1000 | Loss: 0.00006222
Iteration 49/1000 | Loss: 0.00006222
Iteration 50/1000 | Loss: 0.00006222
Iteration 51/1000 | Loss: 0.00006221
Iteration 52/1000 | Loss: 0.00006221
Iteration 53/1000 | Loss: 0.00006220
Iteration 54/1000 | Loss: 0.00006220
Iteration 55/1000 | Loss: 0.00006219
Iteration 56/1000 | Loss: 0.00006219
Iteration 57/1000 | Loss: 0.00006219
Iteration 58/1000 | Loss: 0.00006219
Iteration 59/1000 | Loss: 0.00006219
Iteration 60/1000 | Loss: 0.00006219
Iteration 61/1000 | Loss: 0.00006219
Iteration 62/1000 | Loss: 0.00006219
Iteration 63/1000 | Loss: 0.00006219
Iteration 64/1000 | Loss: 0.00006219
Iteration 65/1000 | Loss: 0.00006219
Iteration 66/1000 | Loss: 0.00006219
Iteration 67/1000 | Loss: 0.00006218
Iteration 68/1000 | Loss: 0.00006217
Iteration 69/1000 | Loss: 0.00006217
Iteration 70/1000 | Loss: 0.00006217
Iteration 71/1000 | Loss: 0.00006216
Iteration 72/1000 | Loss: 0.00006216
Iteration 73/1000 | Loss: 0.00006216
Iteration 74/1000 | Loss: 0.00006216
Iteration 75/1000 | Loss: 0.00006216
Iteration 76/1000 | Loss: 0.00006216
Iteration 77/1000 | Loss: 0.00006216
Iteration 78/1000 | Loss: 0.00006216
Iteration 79/1000 | Loss: 0.00006216
Iteration 80/1000 | Loss: 0.00006215
Iteration 81/1000 | Loss: 0.00006215
Iteration 82/1000 | Loss: 0.00006215
Iteration 83/1000 | Loss: 0.00006214
Iteration 84/1000 | Loss: 0.00006214
Iteration 85/1000 | Loss: 0.00006214
Iteration 86/1000 | Loss: 0.00006214
Iteration 87/1000 | Loss: 0.00006214
Iteration 88/1000 | Loss: 0.00006214
Iteration 89/1000 | Loss: 0.00006214
Iteration 90/1000 | Loss: 0.00006214
Iteration 91/1000 | Loss: 0.00006214
Iteration 92/1000 | Loss: 0.00006214
Iteration 93/1000 | Loss: 0.00006214
Iteration 94/1000 | Loss: 0.00006214
Iteration 95/1000 | Loss: 0.00006214
Iteration 96/1000 | Loss: 0.00006214
Iteration 97/1000 | Loss: 0.00006214
Iteration 98/1000 | Loss: 0.00006214
Iteration 99/1000 | Loss: 0.00006214
Iteration 100/1000 | Loss: 0.00006214
Iteration 101/1000 | Loss: 0.00006214
Iteration 102/1000 | Loss: 0.00006214
Iteration 103/1000 | Loss: 0.00006214
Iteration 104/1000 | Loss: 0.00006214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [6.214432505657896e-05, 6.214432505657896e-05, 6.214432505657896e-05, 6.214432505657896e-05, 6.214432505657896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.214432505657896e-05

Optimization complete. Final v2v error: 6.7237138748168945 mm

Highest mean error: 7.265741348266602 mm for frame 226

Lowest mean error: 6.320423126220703 mm for frame 158

Saving results

Total time: 39.60607957839966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042628
Iteration 2/25 | Loss: 0.00288633
Iteration 3/25 | Loss: 0.00269799
Iteration 4/25 | Loss: 0.00266042
Iteration 5/25 | Loss: 0.00265692
Iteration 6/25 | Loss: 0.00265027
Iteration 7/25 | Loss: 0.00263910
Iteration 8/25 | Loss: 0.00263961
Iteration 9/25 | Loss: 0.00263116
Iteration 10/25 | Loss: 0.00263487
Iteration 11/25 | Loss: 0.00262313
Iteration 12/25 | Loss: 0.00262619
Iteration 13/25 | Loss: 0.00263072
Iteration 14/25 | Loss: 0.00262880
Iteration 15/25 | Loss: 0.00262912
Iteration 16/25 | Loss: 0.00262912
Iteration 17/25 | Loss: 0.00263003
Iteration 18/25 | Loss: 0.00262904
Iteration 19/25 | Loss: 0.00263028
Iteration 20/25 | Loss: 0.00262423
Iteration 21/25 | Loss: 0.00262203
Iteration 22/25 | Loss: 0.00261653
Iteration 23/25 | Loss: 0.00261588
Iteration 24/25 | Loss: 0.00261565
Iteration 25/25 | Loss: 0.00262445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30831039
Iteration 2/25 | Loss: 0.00569477
Iteration 3/25 | Loss: 0.00569477
Iteration 4/25 | Loss: 0.00569477
Iteration 5/25 | Loss: 0.00569477
Iteration 6/25 | Loss: 0.00569477
Iteration 7/25 | Loss: 0.00569477
Iteration 8/25 | Loss: 0.00569477
Iteration 9/25 | Loss: 0.00569477
Iteration 10/25 | Loss: 0.00569477
Iteration 11/25 | Loss: 0.00569477
Iteration 12/25 | Loss: 0.00569477
Iteration 13/25 | Loss: 0.00569477
Iteration 14/25 | Loss: 0.00569477
Iteration 15/25 | Loss: 0.00569477
Iteration 16/25 | Loss: 0.00569477
Iteration 17/25 | Loss: 0.00569477
Iteration 18/25 | Loss: 0.00569477
Iteration 19/25 | Loss: 0.00569477
Iteration 20/25 | Loss: 0.00569477
Iteration 21/25 | Loss: 0.00569477
Iteration 22/25 | Loss: 0.00569477
Iteration 23/25 | Loss: 0.00569477
Iteration 24/25 | Loss: 0.00569477
Iteration 25/25 | Loss: 0.00569477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00569477
Iteration 2/1000 | Loss: 0.00070735
Iteration 3/1000 | Loss: 0.00337816
Iteration 4/1000 | Loss: 0.00194453
Iteration 5/1000 | Loss: 0.00060964
Iteration 6/1000 | Loss: 0.00072460
Iteration 7/1000 | Loss: 0.00051614
Iteration 8/1000 | Loss: 0.00039164
Iteration 9/1000 | Loss: 0.00080426
Iteration 10/1000 | Loss: 0.00091951
Iteration 11/1000 | Loss: 0.00015826
Iteration 12/1000 | Loss: 0.00047822
Iteration 13/1000 | Loss: 0.00050552
Iteration 14/1000 | Loss: 0.00044217
Iteration 15/1000 | Loss: 0.00046234
Iteration 16/1000 | Loss: 0.00012206
Iteration 17/1000 | Loss: 0.00010336
Iteration 18/1000 | Loss: 0.00097294
Iteration 19/1000 | Loss: 0.00011890
Iteration 20/1000 | Loss: 0.00009323
Iteration 21/1000 | Loss: 0.00008822
Iteration 22/1000 | Loss: 0.00008443
Iteration 23/1000 | Loss: 0.00008133
Iteration 24/1000 | Loss: 0.00007908
Iteration 25/1000 | Loss: 0.00087552
Iteration 26/1000 | Loss: 0.00146948
Iteration 27/1000 | Loss: 0.00015179
Iteration 28/1000 | Loss: 0.00008590
Iteration 29/1000 | Loss: 0.00063493
Iteration 30/1000 | Loss: 0.00075897
Iteration 31/1000 | Loss: 0.00008728
Iteration 32/1000 | Loss: 0.00007696
Iteration 33/1000 | Loss: 0.00007370
Iteration 34/1000 | Loss: 0.00007061
Iteration 35/1000 | Loss: 0.00006894
Iteration 36/1000 | Loss: 0.00006826
Iteration 37/1000 | Loss: 0.00006794
Iteration 38/1000 | Loss: 0.00006767
Iteration 39/1000 | Loss: 0.00006749
Iteration 40/1000 | Loss: 0.00006738
Iteration 41/1000 | Loss: 0.00006737
Iteration 42/1000 | Loss: 0.00006732
Iteration 43/1000 | Loss: 0.00006726
Iteration 44/1000 | Loss: 0.00006726
Iteration 45/1000 | Loss: 0.00006725
Iteration 46/1000 | Loss: 0.00006725
Iteration 47/1000 | Loss: 0.00006725
Iteration 48/1000 | Loss: 0.00006725
Iteration 49/1000 | Loss: 0.00006725
Iteration 50/1000 | Loss: 0.00006725
Iteration 51/1000 | Loss: 0.00006724
Iteration 52/1000 | Loss: 0.00006724
Iteration 53/1000 | Loss: 0.00006724
Iteration 54/1000 | Loss: 0.00006724
Iteration 55/1000 | Loss: 0.00006724
Iteration 56/1000 | Loss: 0.00006724
Iteration 57/1000 | Loss: 0.00006724
Iteration 58/1000 | Loss: 0.00006724
Iteration 59/1000 | Loss: 0.00006723
Iteration 60/1000 | Loss: 0.00006723
Iteration 61/1000 | Loss: 0.00006723
Iteration 62/1000 | Loss: 0.00006723
Iteration 63/1000 | Loss: 0.00006723
Iteration 64/1000 | Loss: 0.00006723
Iteration 65/1000 | Loss: 0.00006723
Iteration 66/1000 | Loss: 0.00006723
Iteration 67/1000 | Loss: 0.00006723
Iteration 68/1000 | Loss: 0.00006723
Iteration 69/1000 | Loss: 0.00006723
Iteration 70/1000 | Loss: 0.00006723
Iteration 71/1000 | Loss: 0.00006723
Iteration 72/1000 | Loss: 0.00006722
Iteration 73/1000 | Loss: 0.00006722
Iteration 74/1000 | Loss: 0.00006721
Iteration 75/1000 | Loss: 0.00006721
Iteration 76/1000 | Loss: 0.00006721
Iteration 77/1000 | Loss: 0.00006721
Iteration 78/1000 | Loss: 0.00006721
Iteration 79/1000 | Loss: 0.00006721
Iteration 80/1000 | Loss: 0.00006721
Iteration 81/1000 | Loss: 0.00006721
Iteration 82/1000 | Loss: 0.00006721
Iteration 83/1000 | Loss: 0.00006720
Iteration 84/1000 | Loss: 0.00006720
Iteration 85/1000 | Loss: 0.00006720
Iteration 86/1000 | Loss: 0.00006720
Iteration 87/1000 | Loss: 0.00006720
Iteration 88/1000 | Loss: 0.00006720
Iteration 89/1000 | Loss: 0.00006720
Iteration 90/1000 | Loss: 0.00006720
Iteration 91/1000 | Loss: 0.00006720
Iteration 92/1000 | Loss: 0.00006720
Iteration 93/1000 | Loss: 0.00006720
Iteration 94/1000 | Loss: 0.00006720
Iteration 95/1000 | Loss: 0.00006720
Iteration 96/1000 | Loss: 0.00006720
Iteration 97/1000 | Loss: 0.00006720
Iteration 98/1000 | Loss: 0.00006720
Iteration 99/1000 | Loss: 0.00006720
Iteration 100/1000 | Loss: 0.00006720
Iteration 101/1000 | Loss: 0.00006720
Iteration 102/1000 | Loss: 0.00006720
Iteration 103/1000 | Loss: 0.00006720
Iteration 104/1000 | Loss: 0.00006720
Iteration 105/1000 | Loss: 0.00006720
Iteration 106/1000 | Loss: 0.00006720
Iteration 107/1000 | Loss: 0.00006720
Iteration 108/1000 | Loss: 0.00006719
Iteration 109/1000 | Loss: 0.00006719
Iteration 110/1000 | Loss: 0.00006719
Iteration 111/1000 | Loss: 0.00006719
Iteration 112/1000 | Loss: 0.00006719
Iteration 113/1000 | Loss: 0.00006718
Iteration 114/1000 | Loss: 0.00006718
Iteration 115/1000 | Loss: 0.00006718
Iteration 116/1000 | Loss: 0.00006718
Iteration 117/1000 | Loss: 0.00006718
Iteration 118/1000 | Loss: 0.00006718
Iteration 119/1000 | Loss: 0.00006718
Iteration 120/1000 | Loss: 0.00006718
Iteration 121/1000 | Loss: 0.00006718
Iteration 122/1000 | Loss: 0.00006718
Iteration 123/1000 | Loss: 0.00006717
Iteration 124/1000 | Loss: 0.00006717
Iteration 125/1000 | Loss: 0.00006717
Iteration 126/1000 | Loss: 0.00006717
Iteration 127/1000 | Loss: 0.00006717
Iteration 128/1000 | Loss: 0.00006717
Iteration 129/1000 | Loss: 0.00006717
Iteration 130/1000 | Loss: 0.00006717
Iteration 131/1000 | Loss: 0.00006717
Iteration 132/1000 | Loss: 0.00006717
Iteration 133/1000 | Loss: 0.00006717
Iteration 134/1000 | Loss: 0.00006717
Iteration 135/1000 | Loss: 0.00006717
Iteration 136/1000 | Loss: 0.00006717
Iteration 137/1000 | Loss: 0.00006717
Iteration 138/1000 | Loss: 0.00006717
Iteration 139/1000 | Loss: 0.00006716
Iteration 140/1000 | Loss: 0.00006716
Iteration 141/1000 | Loss: 0.00006716
Iteration 142/1000 | Loss: 0.00006716
Iteration 143/1000 | Loss: 0.00006716
Iteration 144/1000 | Loss: 0.00006716
Iteration 145/1000 | Loss: 0.00006716
Iteration 146/1000 | Loss: 0.00006716
Iteration 147/1000 | Loss: 0.00006716
Iteration 148/1000 | Loss: 0.00006716
Iteration 149/1000 | Loss: 0.00006716
Iteration 150/1000 | Loss: 0.00006716
Iteration 151/1000 | Loss: 0.00006716
Iteration 152/1000 | Loss: 0.00006716
Iteration 153/1000 | Loss: 0.00006716
Iteration 154/1000 | Loss: 0.00006716
Iteration 155/1000 | Loss: 0.00006716
Iteration 156/1000 | Loss: 0.00006716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [6.715721247019246e-05, 6.715721247019246e-05, 6.715721247019246e-05, 6.715721247019246e-05, 6.715721247019246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.715721247019246e-05

Optimization complete. Final v2v error: 7.21937894821167 mm

Highest mean error: 7.7067999839782715 mm for frame 57

Lowest mean error: 6.667200565338135 mm for frame 87

Saving results

Total time: 108.97773623466492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943810
Iteration 2/25 | Loss: 0.00243111
Iteration 3/25 | Loss: 0.00232475
Iteration 4/25 | Loss: 0.00226287
Iteration 5/25 | Loss: 0.00226120
Iteration 6/25 | Loss: 0.00225814
Iteration 7/25 | Loss: 0.00225937
Iteration 8/25 | Loss: 0.00225486
Iteration 9/25 | Loss: 0.00225325
Iteration 10/25 | Loss: 0.00225481
Iteration 11/25 | Loss: 0.00225259
Iteration 12/25 | Loss: 0.00225178
Iteration 13/25 | Loss: 0.00225149
Iteration 14/25 | Loss: 0.00225146
Iteration 15/25 | Loss: 0.00225146
Iteration 16/25 | Loss: 0.00225145
Iteration 17/25 | Loss: 0.00225145
Iteration 18/25 | Loss: 0.00225145
Iteration 19/25 | Loss: 0.00225145
Iteration 20/25 | Loss: 0.00225145
Iteration 21/25 | Loss: 0.00225145
Iteration 22/25 | Loss: 0.00225145
Iteration 23/25 | Loss: 0.00225145
Iteration 24/25 | Loss: 0.00225145
Iteration 25/25 | Loss: 0.00225145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12553930
Iteration 2/25 | Loss: 0.00269033
Iteration 3/25 | Loss: 0.00269032
Iteration 4/25 | Loss: 0.00269032
Iteration 5/25 | Loss: 0.00269032
Iteration 6/25 | Loss: 0.00269032
Iteration 7/25 | Loss: 0.00269032
Iteration 8/25 | Loss: 0.00269032
Iteration 9/25 | Loss: 0.00269032
Iteration 10/25 | Loss: 0.00269032
Iteration 11/25 | Loss: 0.00269032
Iteration 12/25 | Loss: 0.00269032
Iteration 13/25 | Loss: 0.00269032
Iteration 14/25 | Loss: 0.00269032
Iteration 15/25 | Loss: 0.00269032
Iteration 16/25 | Loss: 0.00269032
Iteration 17/25 | Loss: 0.00269032
Iteration 18/25 | Loss: 0.00269032
Iteration 19/25 | Loss: 0.00269032
Iteration 20/25 | Loss: 0.00269032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002690319437533617, 0.002690319437533617, 0.002690319437533617, 0.002690319437533617, 0.002690319437533617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002690319437533617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269032
Iteration 2/1000 | Loss: 0.00016403
Iteration 3/1000 | Loss: 0.00009069
Iteration 4/1000 | Loss: 0.00009201
Iteration 5/1000 | Loss: 0.00007286
Iteration 6/1000 | Loss: 0.00006325
Iteration 7/1000 | Loss: 0.00006086
Iteration 8/1000 | Loss: 0.00005959
Iteration 9/1000 | Loss: 0.00005864
Iteration 10/1000 | Loss: 0.00005797
Iteration 11/1000 | Loss: 0.00005736
Iteration 12/1000 | Loss: 0.00005682
Iteration 13/1000 | Loss: 0.00012135
Iteration 14/1000 | Loss: 0.00005658
Iteration 15/1000 | Loss: 0.00005612
Iteration 16/1000 | Loss: 0.00005605
Iteration 17/1000 | Loss: 0.00005587
Iteration 18/1000 | Loss: 0.00005584
Iteration 19/1000 | Loss: 0.00005582
Iteration 20/1000 | Loss: 0.00005578
Iteration 21/1000 | Loss: 0.00005573
Iteration 22/1000 | Loss: 0.00013116
Iteration 23/1000 | Loss: 0.00005569
Iteration 24/1000 | Loss: 0.00005565
Iteration 25/1000 | Loss: 0.00005564
Iteration 26/1000 | Loss: 0.00005564
Iteration 27/1000 | Loss: 0.00005564
Iteration 28/1000 | Loss: 0.00005564
Iteration 29/1000 | Loss: 0.00005564
Iteration 30/1000 | Loss: 0.00005563
Iteration 31/1000 | Loss: 0.00005563
Iteration 32/1000 | Loss: 0.00005563
Iteration 33/1000 | Loss: 0.00005563
Iteration 34/1000 | Loss: 0.00005562
Iteration 35/1000 | Loss: 0.00005562
Iteration 36/1000 | Loss: 0.00005562
Iteration 37/1000 | Loss: 0.00005562
Iteration 38/1000 | Loss: 0.00005562
Iteration 39/1000 | Loss: 0.00005562
Iteration 40/1000 | Loss: 0.00005562
Iteration 41/1000 | Loss: 0.00005562
Iteration 42/1000 | Loss: 0.00005561
Iteration 43/1000 | Loss: 0.00005561
Iteration 44/1000 | Loss: 0.00005561
Iteration 45/1000 | Loss: 0.00005561
Iteration 46/1000 | Loss: 0.00005560
Iteration 47/1000 | Loss: 0.00005558
Iteration 48/1000 | Loss: 0.00005557
Iteration 49/1000 | Loss: 0.00005557
Iteration 50/1000 | Loss: 0.00005557
Iteration 51/1000 | Loss: 0.00005557
Iteration 52/1000 | Loss: 0.00005556
Iteration 53/1000 | Loss: 0.00005556
Iteration 54/1000 | Loss: 0.00005553
Iteration 55/1000 | Loss: 0.00005552
Iteration 56/1000 | Loss: 0.00005552
Iteration 57/1000 | Loss: 0.00005552
Iteration 58/1000 | Loss: 0.00005551
Iteration 59/1000 | Loss: 0.00005551
Iteration 60/1000 | Loss: 0.00005551
Iteration 61/1000 | Loss: 0.00005551
Iteration 62/1000 | Loss: 0.00005551
Iteration 63/1000 | Loss: 0.00005550
Iteration 64/1000 | Loss: 0.00005550
Iteration 65/1000 | Loss: 0.00005550
Iteration 66/1000 | Loss: 0.00005550
Iteration 67/1000 | Loss: 0.00005549
Iteration 68/1000 | Loss: 0.00005549
Iteration 69/1000 | Loss: 0.00005549
Iteration 70/1000 | Loss: 0.00005549
Iteration 71/1000 | Loss: 0.00005548
Iteration 72/1000 | Loss: 0.00005548
Iteration 73/1000 | Loss: 0.00005548
Iteration 74/1000 | Loss: 0.00005548
Iteration 75/1000 | Loss: 0.00005548
Iteration 76/1000 | Loss: 0.00005548
Iteration 77/1000 | Loss: 0.00005547
Iteration 78/1000 | Loss: 0.00005547
Iteration 79/1000 | Loss: 0.00005547
Iteration 80/1000 | Loss: 0.00005547
Iteration 81/1000 | Loss: 0.00005547
Iteration 82/1000 | Loss: 0.00005547
Iteration 83/1000 | Loss: 0.00005547
Iteration 84/1000 | Loss: 0.00005546
Iteration 85/1000 | Loss: 0.00005546
Iteration 86/1000 | Loss: 0.00005546
Iteration 87/1000 | Loss: 0.00005546
Iteration 88/1000 | Loss: 0.00005546
Iteration 89/1000 | Loss: 0.00005546
Iteration 90/1000 | Loss: 0.00005546
Iteration 91/1000 | Loss: 0.00005546
Iteration 92/1000 | Loss: 0.00005546
Iteration 93/1000 | Loss: 0.00005546
Iteration 94/1000 | Loss: 0.00005546
Iteration 95/1000 | Loss: 0.00005546
Iteration 96/1000 | Loss: 0.00005545
Iteration 97/1000 | Loss: 0.00005545
Iteration 98/1000 | Loss: 0.00005545
Iteration 99/1000 | Loss: 0.00005545
Iteration 100/1000 | Loss: 0.00005545
Iteration 101/1000 | Loss: 0.00005545
Iteration 102/1000 | Loss: 0.00005544
Iteration 103/1000 | Loss: 0.00005544
Iteration 104/1000 | Loss: 0.00005544
Iteration 105/1000 | Loss: 0.00005544
Iteration 106/1000 | Loss: 0.00005544
Iteration 107/1000 | Loss: 0.00005544
Iteration 108/1000 | Loss: 0.00005544
Iteration 109/1000 | Loss: 0.00005544
Iteration 110/1000 | Loss: 0.00005544
Iteration 111/1000 | Loss: 0.00005544
Iteration 112/1000 | Loss: 0.00005544
Iteration 113/1000 | Loss: 0.00005544
Iteration 114/1000 | Loss: 0.00005544
Iteration 115/1000 | Loss: 0.00005544
Iteration 116/1000 | Loss: 0.00005544
Iteration 117/1000 | Loss: 0.00005544
Iteration 118/1000 | Loss: 0.00005544
Iteration 119/1000 | Loss: 0.00005543
Iteration 120/1000 | Loss: 0.00005543
Iteration 121/1000 | Loss: 0.00005543
Iteration 122/1000 | Loss: 0.00005543
Iteration 123/1000 | Loss: 0.00005543
Iteration 124/1000 | Loss: 0.00005543
Iteration 125/1000 | Loss: 0.00005543
Iteration 126/1000 | Loss: 0.00005543
Iteration 127/1000 | Loss: 0.00005543
Iteration 128/1000 | Loss: 0.00005543
Iteration 129/1000 | Loss: 0.00005543
Iteration 130/1000 | Loss: 0.00005543
Iteration 131/1000 | Loss: 0.00005543
Iteration 132/1000 | Loss: 0.00005543
Iteration 133/1000 | Loss: 0.00005543
Iteration 134/1000 | Loss: 0.00005543
Iteration 135/1000 | Loss: 0.00005543
Iteration 136/1000 | Loss: 0.00005543
Iteration 137/1000 | Loss: 0.00005543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [5.543060979107395e-05, 5.543060979107395e-05, 5.543060979107395e-05, 5.543060979107395e-05, 5.543060979107395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.543060979107395e-05

Optimization complete. Final v2v error: 6.600901126861572 mm

Highest mean error: 12.3246431350708 mm for frame 2

Lowest mean error: 6.328330993652344 mm for frame 201

Saving results

Total time: 65.8548812866211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00939100
Iteration 2/25 | Loss: 0.00236799
Iteration 3/25 | Loss: 0.00230578
Iteration 4/25 | Loss: 0.00227421
Iteration 5/25 | Loss: 0.00226910
Iteration 6/25 | Loss: 0.00226603
Iteration 7/25 | Loss: 0.00226567
Iteration 8/25 | Loss: 0.00226555
Iteration 9/25 | Loss: 0.00226552
Iteration 10/25 | Loss: 0.00226550
Iteration 11/25 | Loss: 0.00226550
Iteration 12/25 | Loss: 0.00226550
Iteration 13/25 | Loss: 0.00226550
Iteration 14/25 | Loss: 0.00226550
Iteration 15/25 | Loss: 0.00226549
Iteration 16/25 | Loss: 0.00226549
Iteration 17/25 | Loss: 0.00226549
Iteration 18/25 | Loss: 0.00226549
Iteration 19/25 | Loss: 0.00226549
Iteration 20/25 | Loss: 0.00226549
Iteration 21/25 | Loss: 0.00226549
Iteration 22/25 | Loss: 0.00226549
Iteration 23/25 | Loss: 0.00226548
Iteration 24/25 | Loss: 0.00226548
Iteration 25/25 | Loss: 0.00226548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.86264086
Iteration 2/25 | Loss: 0.00301342
Iteration 3/25 | Loss: 0.00259597
Iteration 4/25 | Loss: 0.00259597
Iteration 5/25 | Loss: 0.00259597
Iteration 6/25 | Loss: 0.00259597
Iteration 7/25 | Loss: 0.00259597
Iteration 8/25 | Loss: 0.00259597
Iteration 9/25 | Loss: 0.00259597
Iteration 10/25 | Loss: 0.00259597
Iteration 11/25 | Loss: 0.00259597
Iteration 12/25 | Loss: 0.00259597
Iteration 13/25 | Loss: 0.00259597
Iteration 14/25 | Loss: 0.00259597
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025959708727896214, 0.0025959708727896214, 0.0025959708727896214, 0.0025959708727896214, 0.0025959708727896214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025959708727896214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259597
Iteration 2/1000 | Loss: 0.00013904
Iteration 3/1000 | Loss: 0.00008607
Iteration 4/1000 | Loss: 0.00007352
Iteration 5/1000 | Loss: 0.00006767
Iteration 6/1000 | Loss: 0.00006445
Iteration 7/1000 | Loss: 0.00006252
Iteration 8/1000 | Loss: 0.00006145
Iteration 9/1000 | Loss: 0.00006067
Iteration 10/1000 | Loss: 0.00005983
Iteration 11/1000 | Loss: 0.00005924
Iteration 12/1000 | Loss: 0.00005883
Iteration 13/1000 | Loss: 0.00005862
Iteration 14/1000 | Loss: 0.00005835
Iteration 15/1000 | Loss: 0.00005810
Iteration 16/1000 | Loss: 0.00005793
Iteration 17/1000 | Loss: 0.00005787
Iteration 18/1000 | Loss: 0.00005786
Iteration 19/1000 | Loss: 0.00005785
Iteration 20/1000 | Loss: 0.00005783
Iteration 21/1000 | Loss: 0.00005782
Iteration 22/1000 | Loss: 0.00005781
Iteration 23/1000 | Loss: 0.00005781
Iteration 24/1000 | Loss: 0.00005777
Iteration 25/1000 | Loss: 0.00005776
Iteration 26/1000 | Loss: 0.00005776
Iteration 27/1000 | Loss: 0.00005775
Iteration 28/1000 | Loss: 0.00005771
Iteration 29/1000 | Loss: 0.00005770
Iteration 30/1000 | Loss: 0.00005767
Iteration 31/1000 | Loss: 0.00005766
Iteration 32/1000 | Loss: 0.00005766
Iteration 33/1000 | Loss: 0.00005765
Iteration 34/1000 | Loss: 0.00005764
Iteration 35/1000 | Loss: 0.00005764
Iteration 36/1000 | Loss: 0.00005763
Iteration 37/1000 | Loss: 0.00005763
Iteration 38/1000 | Loss: 0.00005762
Iteration 39/1000 | Loss: 0.00005761
Iteration 40/1000 | Loss: 0.00005760
Iteration 41/1000 | Loss: 0.00005759
Iteration 42/1000 | Loss: 0.00005759
Iteration 43/1000 | Loss: 0.00005759
Iteration 44/1000 | Loss: 0.00005758
Iteration 45/1000 | Loss: 0.00005758
Iteration 46/1000 | Loss: 0.00005758
Iteration 47/1000 | Loss: 0.00005757
Iteration 48/1000 | Loss: 0.00005757
Iteration 49/1000 | Loss: 0.00005757
Iteration 50/1000 | Loss: 0.00005756
Iteration 51/1000 | Loss: 0.00005756
Iteration 52/1000 | Loss: 0.00005756
Iteration 53/1000 | Loss: 0.00005756
Iteration 54/1000 | Loss: 0.00005756
Iteration 55/1000 | Loss: 0.00005756
Iteration 56/1000 | Loss: 0.00005755
Iteration 57/1000 | Loss: 0.00005755
Iteration 58/1000 | Loss: 0.00005755
Iteration 59/1000 | Loss: 0.00005755
Iteration 60/1000 | Loss: 0.00005755
Iteration 61/1000 | Loss: 0.00005754
Iteration 62/1000 | Loss: 0.00005754
Iteration 63/1000 | Loss: 0.00005754
Iteration 64/1000 | Loss: 0.00005754
Iteration 65/1000 | Loss: 0.00005754
Iteration 66/1000 | Loss: 0.00005754
Iteration 67/1000 | Loss: 0.00005754
Iteration 68/1000 | Loss: 0.00005754
Iteration 69/1000 | Loss: 0.00005753
Iteration 70/1000 | Loss: 0.00005753
Iteration 71/1000 | Loss: 0.00005753
Iteration 72/1000 | Loss: 0.00005753
Iteration 73/1000 | Loss: 0.00005753
Iteration 74/1000 | Loss: 0.00005753
Iteration 75/1000 | Loss: 0.00005753
Iteration 76/1000 | Loss: 0.00005753
Iteration 77/1000 | Loss: 0.00005753
Iteration 78/1000 | Loss: 0.00005752
Iteration 79/1000 | Loss: 0.00005752
Iteration 80/1000 | Loss: 0.00005752
Iteration 81/1000 | Loss: 0.00005752
Iteration 82/1000 | Loss: 0.00005752
Iteration 83/1000 | Loss: 0.00005752
Iteration 84/1000 | Loss: 0.00005752
Iteration 85/1000 | Loss: 0.00005752
Iteration 86/1000 | Loss: 0.00005751
Iteration 87/1000 | Loss: 0.00005751
Iteration 88/1000 | Loss: 0.00005751
Iteration 89/1000 | Loss: 0.00005751
Iteration 90/1000 | Loss: 0.00005751
Iteration 91/1000 | Loss: 0.00005751
Iteration 92/1000 | Loss: 0.00005751
Iteration 93/1000 | Loss: 0.00005751
Iteration 94/1000 | Loss: 0.00005751
Iteration 95/1000 | Loss: 0.00005751
Iteration 96/1000 | Loss: 0.00005751
Iteration 97/1000 | Loss: 0.00005750
Iteration 98/1000 | Loss: 0.00005750
Iteration 99/1000 | Loss: 0.00005750
Iteration 100/1000 | Loss: 0.00005750
Iteration 101/1000 | Loss: 0.00005750
Iteration 102/1000 | Loss: 0.00005750
Iteration 103/1000 | Loss: 0.00005750
Iteration 104/1000 | Loss: 0.00005750
Iteration 105/1000 | Loss: 0.00005750
Iteration 106/1000 | Loss: 0.00005750
Iteration 107/1000 | Loss: 0.00005750
Iteration 108/1000 | Loss: 0.00005750
Iteration 109/1000 | Loss: 0.00005750
Iteration 110/1000 | Loss: 0.00005750
Iteration 111/1000 | Loss: 0.00005750
Iteration 112/1000 | Loss: 0.00005750
Iteration 113/1000 | Loss: 0.00005749
Iteration 114/1000 | Loss: 0.00005749
Iteration 115/1000 | Loss: 0.00005749
Iteration 116/1000 | Loss: 0.00005749
Iteration 117/1000 | Loss: 0.00005749
Iteration 118/1000 | Loss: 0.00005749
Iteration 119/1000 | Loss: 0.00005749
Iteration 120/1000 | Loss: 0.00005749
Iteration 121/1000 | Loss: 0.00005749
Iteration 122/1000 | Loss: 0.00005749
Iteration 123/1000 | Loss: 0.00005749
Iteration 124/1000 | Loss: 0.00005749
Iteration 125/1000 | Loss: 0.00005749
Iteration 126/1000 | Loss: 0.00005749
Iteration 127/1000 | Loss: 0.00005749
Iteration 128/1000 | Loss: 0.00005749
Iteration 129/1000 | Loss: 0.00005749
Iteration 130/1000 | Loss: 0.00005749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [5.748965486418456e-05, 5.748965486418456e-05, 5.748965486418456e-05, 5.748965486418456e-05, 5.748965486418456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.748965486418456e-05

Optimization complete. Final v2v error: 6.709283828735352 mm

Highest mean error: 12.348164558410645 mm for frame 51

Lowest mean error: 6.304559230804443 mm for frame 4

Saving results

Total time: 51.04247570037842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593003
Iteration 2/25 | Loss: 0.00244373
Iteration 3/25 | Loss: 0.00230291
Iteration 4/25 | Loss: 0.00228906
Iteration 5/25 | Loss: 0.00228356
Iteration 6/25 | Loss: 0.00228252
Iteration 7/25 | Loss: 0.00228252
Iteration 8/25 | Loss: 0.00228252
Iteration 9/25 | Loss: 0.00228252
Iteration 10/25 | Loss: 0.00228252
Iteration 11/25 | Loss: 0.00228252
Iteration 12/25 | Loss: 0.00228252
Iteration 13/25 | Loss: 0.00228252
Iteration 14/25 | Loss: 0.00228252
Iteration 15/25 | Loss: 0.00228252
Iteration 16/25 | Loss: 0.00228252
Iteration 17/25 | Loss: 0.00228252
Iteration 18/25 | Loss: 0.00228252
Iteration 19/25 | Loss: 0.00228252
Iteration 20/25 | Loss: 0.00228252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002282520290464163, 0.002282520290464163, 0.002282520290464163, 0.002282520290464163, 0.002282520290464163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002282520290464163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36390162
Iteration 2/25 | Loss: 0.00279668
Iteration 3/25 | Loss: 0.00279668
Iteration 4/25 | Loss: 0.00279668
Iteration 5/25 | Loss: 0.00279668
Iteration 6/25 | Loss: 0.00279668
Iteration 7/25 | Loss: 0.00279668
Iteration 8/25 | Loss: 0.00279668
Iteration 9/25 | Loss: 0.00279668
Iteration 10/25 | Loss: 0.00279668
Iteration 11/25 | Loss: 0.00279668
Iteration 12/25 | Loss: 0.00279668
Iteration 13/25 | Loss: 0.00279668
Iteration 14/25 | Loss: 0.00279668
Iteration 15/25 | Loss: 0.00279668
Iteration 16/25 | Loss: 0.00279668
Iteration 17/25 | Loss: 0.00279668
Iteration 18/25 | Loss: 0.00279668
Iteration 19/25 | Loss: 0.00279668
Iteration 20/25 | Loss: 0.00279668
Iteration 21/25 | Loss: 0.00279668
Iteration 22/25 | Loss: 0.00279668
Iteration 23/25 | Loss: 0.00279668
Iteration 24/25 | Loss: 0.00279668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002796680899336934, 0.002796680899336934, 0.002796680899336934, 0.002796680899336934, 0.002796680899336934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002796680899336934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279668
Iteration 2/1000 | Loss: 0.00011695
Iteration 3/1000 | Loss: 0.00008369
Iteration 4/1000 | Loss: 0.00007089
Iteration 5/1000 | Loss: 0.00006547
Iteration 6/1000 | Loss: 0.00006280
Iteration 7/1000 | Loss: 0.00006161
Iteration 8/1000 | Loss: 0.00006063
Iteration 9/1000 | Loss: 0.00005972
Iteration 10/1000 | Loss: 0.00005909
Iteration 11/1000 | Loss: 0.00005860
Iteration 12/1000 | Loss: 0.00005825
Iteration 13/1000 | Loss: 0.00005799
Iteration 14/1000 | Loss: 0.00005797
Iteration 15/1000 | Loss: 0.00005777
Iteration 16/1000 | Loss: 0.00005772
Iteration 17/1000 | Loss: 0.00005767
Iteration 18/1000 | Loss: 0.00005762
Iteration 19/1000 | Loss: 0.00005760
Iteration 20/1000 | Loss: 0.00005759
Iteration 21/1000 | Loss: 0.00005758
Iteration 22/1000 | Loss: 0.00005758
Iteration 23/1000 | Loss: 0.00005757
Iteration 24/1000 | Loss: 0.00005756
Iteration 25/1000 | Loss: 0.00005755
Iteration 26/1000 | Loss: 0.00005753
Iteration 27/1000 | Loss: 0.00005751
Iteration 28/1000 | Loss: 0.00005751
Iteration 29/1000 | Loss: 0.00005748
Iteration 30/1000 | Loss: 0.00005748
Iteration 31/1000 | Loss: 0.00005747
Iteration 32/1000 | Loss: 0.00005747
Iteration 33/1000 | Loss: 0.00005746
Iteration 34/1000 | Loss: 0.00005745
Iteration 35/1000 | Loss: 0.00005745
Iteration 36/1000 | Loss: 0.00005744
Iteration 37/1000 | Loss: 0.00005744
Iteration 38/1000 | Loss: 0.00005743
Iteration 39/1000 | Loss: 0.00005743
Iteration 40/1000 | Loss: 0.00005742
Iteration 41/1000 | Loss: 0.00005741
Iteration 42/1000 | Loss: 0.00005741
Iteration 43/1000 | Loss: 0.00005740
Iteration 44/1000 | Loss: 0.00005740
Iteration 45/1000 | Loss: 0.00005740
Iteration 46/1000 | Loss: 0.00005739
Iteration 47/1000 | Loss: 0.00005739
Iteration 48/1000 | Loss: 0.00005739
Iteration 49/1000 | Loss: 0.00005738
Iteration 50/1000 | Loss: 0.00005738
Iteration 51/1000 | Loss: 0.00005738
Iteration 52/1000 | Loss: 0.00005738
Iteration 53/1000 | Loss: 0.00005738
Iteration 54/1000 | Loss: 0.00005737
Iteration 55/1000 | Loss: 0.00005737
Iteration 56/1000 | Loss: 0.00005737
Iteration 57/1000 | Loss: 0.00005737
Iteration 58/1000 | Loss: 0.00005736
Iteration 59/1000 | Loss: 0.00005736
Iteration 60/1000 | Loss: 0.00005736
Iteration 61/1000 | Loss: 0.00005735
Iteration 62/1000 | Loss: 0.00005735
Iteration 63/1000 | Loss: 0.00005735
Iteration 64/1000 | Loss: 0.00005735
Iteration 65/1000 | Loss: 0.00005735
Iteration 66/1000 | Loss: 0.00005735
Iteration 67/1000 | Loss: 0.00005735
Iteration 68/1000 | Loss: 0.00005735
Iteration 69/1000 | Loss: 0.00005735
Iteration 70/1000 | Loss: 0.00005734
Iteration 71/1000 | Loss: 0.00005734
Iteration 72/1000 | Loss: 0.00005734
Iteration 73/1000 | Loss: 0.00005734
Iteration 74/1000 | Loss: 0.00005734
Iteration 75/1000 | Loss: 0.00005734
Iteration 76/1000 | Loss: 0.00005734
Iteration 77/1000 | Loss: 0.00005734
Iteration 78/1000 | Loss: 0.00005734
Iteration 79/1000 | Loss: 0.00005734
Iteration 80/1000 | Loss: 0.00005734
Iteration 81/1000 | Loss: 0.00005734
Iteration 82/1000 | Loss: 0.00005734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [5.7341541833011433e-05, 5.7341541833011433e-05, 5.7341541833011433e-05, 5.7341541833011433e-05, 5.7341541833011433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.7341541833011433e-05

Optimization complete. Final v2v error: 6.76184606552124 mm

Highest mean error: 7.254316329956055 mm for frame 230

Lowest mean error: 6.472015380859375 mm for frame 70

Saving results

Total time: 41.46186661720276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01169556
Iteration 2/25 | Loss: 0.00283801
Iteration 3/25 | Loss: 0.00240785
Iteration 4/25 | Loss: 0.00247861
Iteration 5/25 | Loss: 0.00227443
Iteration 6/25 | Loss: 0.00235091
Iteration 7/25 | Loss: 0.00223991
Iteration 8/25 | Loss: 0.00214921
Iteration 9/25 | Loss: 0.00212308
Iteration 10/25 | Loss: 0.00209676
Iteration 11/25 | Loss: 0.00209039
Iteration 12/25 | Loss: 0.00207444
Iteration 13/25 | Loss: 0.00207326
Iteration 14/25 | Loss: 0.00205844
Iteration 15/25 | Loss: 0.00205057
Iteration 16/25 | Loss: 0.00204542
Iteration 17/25 | Loss: 0.00204931
Iteration 18/25 | Loss: 0.00204992
Iteration 19/25 | Loss: 0.00204546
Iteration 20/25 | Loss: 0.00207859
Iteration 21/25 | Loss: 0.00207567
Iteration 22/25 | Loss: 0.00201940
Iteration 23/25 | Loss: 0.00200968
Iteration 24/25 | Loss: 0.00200154
Iteration 25/25 | Loss: 0.00200052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36100721
Iteration 2/25 | Loss: 0.00211256
Iteration 3/25 | Loss: 0.00211256
Iteration 4/25 | Loss: 0.00211256
Iteration 5/25 | Loss: 0.00211256
Iteration 6/25 | Loss: 0.00211256
Iteration 7/25 | Loss: 0.00211256
Iteration 8/25 | Loss: 0.00211256
Iteration 9/25 | Loss: 0.00211256
Iteration 10/25 | Loss: 0.00211256
Iteration 11/25 | Loss: 0.00211255
Iteration 12/25 | Loss: 0.00211255
Iteration 13/25 | Loss: 0.00211255
Iteration 14/25 | Loss: 0.00211255
Iteration 15/25 | Loss: 0.00211256
Iteration 16/25 | Loss: 0.00211255
Iteration 17/25 | Loss: 0.00211255
Iteration 18/25 | Loss: 0.00211255
Iteration 19/25 | Loss: 0.00211256
Iteration 20/25 | Loss: 0.00211256
Iteration 21/25 | Loss: 0.00211256
Iteration 22/25 | Loss: 0.00211256
Iteration 23/25 | Loss: 0.00211256
Iteration 24/25 | Loss: 0.00211256
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021125550847500563, 0.0021125550847500563, 0.0021125550847500563, 0.0021125550847500563, 0.0021125550847500563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021125550847500563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211256
Iteration 2/1000 | Loss: 0.00019618
Iteration 3/1000 | Loss: 0.00014543
Iteration 4/1000 | Loss: 0.00010361
Iteration 5/1000 | Loss: 0.00011227
Iteration 6/1000 | Loss: 0.00009589
Iteration 7/1000 | Loss: 0.00011807
Iteration 8/1000 | Loss: 0.00010514
Iteration 9/1000 | Loss: 0.00010483
Iteration 10/1000 | Loss: 0.00008051
Iteration 11/1000 | Loss: 0.00010457
Iteration 12/1000 | Loss: 0.00007812
Iteration 13/1000 | Loss: 0.00009917
Iteration 14/1000 | Loss: 0.00010062
Iteration 15/1000 | Loss: 0.00009298
Iteration 16/1000 | Loss: 0.00009345
Iteration 17/1000 | Loss: 0.00009245
Iteration 18/1000 | Loss: 0.00009368
Iteration 19/1000 | Loss: 0.00009183
Iteration 20/1000 | Loss: 0.00009273
Iteration 21/1000 | Loss: 0.00009072
Iteration 22/1000 | Loss: 0.00009130
Iteration 23/1000 | Loss: 0.00009002
Iteration 24/1000 | Loss: 0.00233659
Iteration 25/1000 | Loss: 0.00012069
Iteration 26/1000 | Loss: 0.00012007
Iteration 27/1000 | Loss: 0.00008994
Iteration 28/1000 | Loss: 0.00008622
Iteration 29/1000 | Loss: 0.00008420
Iteration 30/1000 | Loss: 0.00009211
Iteration 31/1000 | Loss: 0.00008068
Iteration 32/1000 | Loss: 0.00008149
Iteration 33/1000 | Loss: 0.00008207
Iteration 34/1000 | Loss: 0.00007988
Iteration 35/1000 | Loss: 0.00008087
Iteration 36/1000 | Loss: 0.00008391
Iteration 37/1000 | Loss: 0.00007429
Iteration 38/1000 | Loss: 0.00009143
Iteration 39/1000 | Loss: 0.00006842
Iteration 40/1000 | Loss: 0.00008361
Iteration 41/1000 | Loss: 0.00009536
Iteration 42/1000 | Loss: 0.00008421
Iteration 43/1000 | Loss: 0.00008507
Iteration 44/1000 | Loss: 0.00008600
Iteration 45/1000 | Loss: 0.00008388
Iteration 46/1000 | Loss: 0.00011239
Iteration 47/1000 | Loss: 0.00006327
Iteration 48/1000 | Loss: 0.00005926
Iteration 49/1000 | Loss: 0.00005814
Iteration 50/1000 | Loss: 0.00005754
Iteration 51/1000 | Loss: 0.00005707
Iteration 52/1000 | Loss: 0.00005662
Iteration 53/1000 | Loss: 0.00005634
Iteration 54/1000 | Loss: 0.00005615
Iteration 55/1000 | Loss: 0.00005605
Iteration 56/1000 | Loss: 0.00005605
Iteration 57/1000 | Loss: 0.00005604
Iteration 58/1000 | Loss: 0.00005604
Iteration 59/1000 | Loss: 0.00005604
Iteration 60/1000 | Loss: 0.00005603
Iteration 61/1000 | Loss: 0.00005603
Iteration 62/1000 | Loss: 0.00005602
Iteration 63/1000 | Loss: 0.00005602
Iteration 64/1000 | Loss: 0.00005601
Iteration 65/1000 | Loss: 0.00005601
Iteration 66/1000 | Loss: 0.00005601
Iteration 67/1000 | Loss: 0.00005600
Iteration 68/1000 | Loss: 0.00005600
Iteration 69/1000 | Loss: 0.00005600
Iteration 70/1000 | Loss: 0.00005599
Iteration 71/1000 | Loss: 0.00005599
Iteration 72/1000 | Loss: 0.00005599
Iteration 73/1000 | Loss: 0.00005599
Iteration 74/1000 | Loss: 0.00005599
Iteration 75/1000 | Loss: 0.00005599
Iteration 76/1000 | Loss: 0.00005599
Iteration 77/1000 | Loss: 0.00005599
Iteration 78/1000 | Loss: 0.00005599
Iteration 79/1000 | Loss: 0.00005598
Iteration 80/1000 | Loss: 0.00005598
Iteration 81/1000 | Loss: 0.00005597
Iteration 82/1000 | Loss: 0.00005597
Iteration 83/1000 | Loss: 0.00005597
Iteration 84/1000 | Loss: 0.00005597
Iteration 85/1000 | Loss: 0.00005597
Iteration 86/1000 | Loss: 0.00005597
Iteration 87/1000 | Loss: 0.00005597
Iteration 88/1000 | Loss: 0.00005597
Iteration 89/1000 | Loss: 0.00005597
Iteration 90/1000 | Loss: 0.00005597
Iteration 91/1000 | Loss: 0.00005596
Iteration 92/1000 | Loss: 0.00005596
Iteration 93/1000 | Loss: 0.00005596
Iteration 94/1000 | Loss: 0.00005596
Iteration 95/1000 | Loss: 0.00005595
Iteration 96/1000 | Loss: 0.00005595
Iteration 97/1000 | Loss: 0.00005595
Iteration 98/1000 | Loss: 0.00005595
Iteration 99/1000 | Loss: 0.00005595
Iteration 100/1000 | Loss: 0.00005595
Iteration 101/1000 | Loss: 0.00005595
Iteration 102/1000 | Loss: 0.00005595
Iteration 103/1000 | Loss: 0.00005594
Iteration 104/1000 | Loss: 0.00005594
Iteration 105/1000 | Loss: 0.00005594
Iteration 106/1000 | Loss: 0.00005594
Iteration 107/1000 | Loss: 0.00005594
Iteration 108/1000 | Loss: 0.00005594
Iteration 109/1000 | Loss: 0.00005594
Iteration 110/1000 | Loss: 0.00005594
Iteration 111/1000 | Loss: 0.00005594
Iteration 112/1000 | Loss: 0.00005593
Iteration 113/1000 | Loss: 0.00005593
Iteration 114/1000 | Loss: 0.00005593
Iteration 115/1000 | Loss: 0.00005593
Iteration 116/1000 | Loss: 0.00005593
Iteration 117/1000 | Loss: 0.00005593
Iteration 118/1000 | Loss: 0.00005593
Iteration 119/1000 | Loss: 0.00005593
Iteration 120/1000 | Loss: 0.00005593
Iteration 121/1000 | Loss: 0.00005593
Iteration 122/1000 | Loss: 0.00005593
Iteration 123/1000 | Loss: 0.00005592
Iteration 124/1000 | Loss: 0.00005592
Iteration 125/1000 | Loss: 0.00005592
Iteration 126/1000 | Loss: 0.00005592
Iteration 127/1000 | Loss: 0.00005592
Iteration 128/1000 | Loss: 0.00005592
Iteration 129/1000 | Loss: 0.00005592
Iteration 130/1000 | Loss: 0.00005592
Iteration 131/1000 | Loss: 0.00005591
Iteration 132/1000 | Loss: 0.00005591
Iteration 133/1000 | Loss: 0.00005591
Iteration 134/1000 | Loss: 0.00005591
Iteration 135/1000 | Loss: 0.00005591
Iteration 136/1000 | Loss: 0.00005591
Iteration 137/1000 | Loss: 0.00005591
Iteration 138/1000 | Loss: 0.00005591
Iteration 139/1000 | Loss: 0.00005591
Iteration 140/1000 | Loss: 0.00005591
Iteration 141/1000 | Loss: 0.00005591
Iteration 142/1000 | Loss: 0.00005591
Iteration 143/1000 | Loss: 0.00005591
Iteration 144/1000 | Loss: 0.00005591
Iteration 145/1000 | Loss: 0.00005591
Iteration 146/1000 | Loss: 0.00005591
Iteration 147/1000 | Loss: 0.00005591
Iteration 148/1000 | Loss: 0.00005590
Iteration 149/1000 | Loss: 0.00005590
Iteration 150/1000 | Loss: 0.00005590
Iteration 151/1000 | Loss: 0.00005590
Iteration 152/1000 | Loss: 0.00005590
Iteration 153/1000 | Loss: 0.00005590
Iteration 154/1000 | Loss: 0.00005590
Iteration 155/1000 | Loss: 0.00005590
Iteration 156/1000 | Loss: 0.00005589
Iteration 157/1000 | Loss: 0.00005589
Iteration 158/1000 | Loss: 0.00005589
Iteration 159/1000 | Loss: 0.00005589
Iteration 160/1000 | Loss: 0.00005589
Iteration 161/1000 | Loss: 0.00005589
Iteration 162/1000 | Loss: 0.00005589
Iteration 163/1000 | Loss: 0.00005589
Iteration 164/1000 | Loss: 0.00005589
Iteration 165/1000 | Loss: 0.00005589
Iteration 166/1000 | Loss: 0.00005589
Iteration 167/1000 | Loss: 0.00005589
Iteration 168/1000 | Loss: 0.00005589
Iteration 169/1000 | Loss: 0.00005589
Iteration 170/1000 | Loss: 0.00005589
Iteration 171/1000 | Loss: 0.00005589
Iteration 172/1000 | Loss: 0.00005589
Iteration 173/1000 | Loss: 0.00005589
Iteration 174/1000 | Loss: 0.00005589
Iteration 175/1000 | Loss: 0.00005589
Iteration 176/1000 | Loss: 0.00005588
Iteration 177/1000 | Loss: 0.00005588
Iteration 178/1000 | Loss: 0.00005588
Iteration 179/1000 | Loss: 0.00005588
Iteration 180/1000 | Loss: 0.00005588
Iteration 181/1000 | Loss: 0.00005588
Iteration 182/1000 | Loss: 0.00005588
Iteration 183/1000 | Loss: 0.00005588
Iteration 184/1000 | Loss: 0.00005588
Iteration 185/1000 | Loss: 0.00005588
Iteration 186/1000 | Loss: 0.00005588
Iteration 187/1000 | Loss: 0.00005588
Iteration 188/1000 | Loss: 0.00005588
Iteration 189/1000 | Loss: 0.00005588
Iteration 190/1000 | Loss: 0.00005588
Iteration 191/1000 | Loss: 0.00005588
Iteration 192/1000 | Loss: 0.00005588
Iteration 193/1000 | Loss: 0.00005588
Iteration 194/1000 | Loss: 0.00005588
Iteration 195/1000 | Loss: 0.00005588
Iteration 196/1000 | Loss: 0.00005588
Iteration 197/1000 | Loss: 0.00005588
Iteration 198/1000 | Loss: 0.00005588
Iteration 199/1000 | Loss: 0.00005588
Iteration 200/1000 | Loss: 0.00005588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [5.5882937886053696e-05, 5.5882937886053696e-05, 5.5882937886053696e-05, 5.5882937886053696e-05, 5.5882937886053696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.5882937886053696e-05

Optimization complete. Final v2v error: 6.524527549743652 mm

Highest mean error: 12.399435997009277 mm for frame 121

Lowest mean error: 6.1034464836120605 mm for frame 32

Saving results

Total time: 130.17008066177368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520349
Iteration 2/25 | Loss: 0.00232939
Iteration 3/25 | Loss: 0.00228336
Iteration 4/25 | Loss: 0.00227506
Iteration 5/25 | Loss: 0.00227207
Iteration 6/25 | Loss: 0.00227183
Iteration 7/25 | Loss: 0.00227183
Iteration 8/25 | Loss: 0.00227183
Iteration 9/25 | Loss: 0.00227183
Iteration 10/25 | Loss: 0.00227183
Iteration 11/25 | Loss: 0.00227183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002271832898259163, 0.002271832898259163, 0.002271832898259163, 0.002271832898259163, 0.002271832898259163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002271832898259163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60200620
Iteration 2/25 | Loss: 0.00275989
Iteration 3/25 | Loss: 0.00275987
Iteration 4/25 | Loss: 0.00275987
Iteration 5/25 | Loss: 0.00275987
Iteration 6/25 | Loss: 0.00275987
Iteration 7/25 | Loss: 0.00275987
Iteration 8/25 | Loss: 0.00275987
Iteration 9/25 | Loss: 0.00275987
Iteration 10/25 | Loss: 0.00275987
Iteration 11/25 | Loss: 0.00275987
Iteration 12/25 | Loss: 0.00275987
Iteration 13/25 | Loss: 0.00275987
Iteration 14/25 | Loss: 0.00275987
Iteration 15/25 | Loss: 0.00275987
Iteration 16/25 | Loss: 0.00275987
Iteration 17/25 | Loss: 0.00275987
Iteration 18/25 | Loss: 0.00275987
Iteration 19/25 | Loss: 0.00275987
Iteration 20/25 | Loss: 0.00275987
Iteration 21/25 | Loss: 0.00275987
Iteration 22/25 | Loss: 0.00275987
Iteration 23/25 | Loss: 0.00275987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002759867114946246, 0.002759867114946246, 0.002759867114946246, 0.002759867114946246, 0.002759867114946246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002759867114946246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275987
Iteration 2/1000 | Loss: 0.00011360
Iteration 3/1000 | Loss: 0.00007837
Iteration 4/1000 | Loss: 0.00006568
Iteration 5/1000 | Loss: 0.00006174
Iteration 6/1000 | Loss: 0.00005919
Iteration 7/1000 | Loss: 0.00005815
Iteration 8/1000 | Loss: 0.00005752
Iteration 9/1000 | Loss: 0.00005688
Iteration 10/1000 | Loss: 0.00005626
Iteration 11/1000 | Loss: 0.00005576
Iteration 12/1000 | Loss: 0.00005557
Iteration 13/1000 | Loss: 0.00005532
Iteration 14/1000 | Loss: 0.00005514
Iteration 15/1000 | Loss: 0.00005495
Iteration 16/1000 | Loss: 0.00005491
Iteration 17/1000 | Loss: 0.00005490
Iteration 18/1000 | Loss: 0.00005488
Iteration 19/1000 | Loss: 0.00005485
Iteration 20/1000 | Loss: 0.00005485
Iteration 21/1000 | Loss: 0.00005485
Iteration 22/1000 | Loss: 0.00005482
Iteration 23/1000 | Loss: 0.00005482
Iteration 24/1000 | Loss: 0.00005482
Iteration 25/1000 | Loss: 0.00005482
Iteration 26/1000 | Loss: 0.00005482
Iteration 27/1000 | Loss: 0.00005482
Iteration 28/1000 | Loss: 0.00005482
Iteration 29/1000 | Loss: 0.00005482
Iteration 30/1000 | Loss: 0.00005481
Iteration 31/1000 | Loss: 0.00005481
Iteration 32/1000 | Loss: 0.00005481
Iteration 33/1000 | Loss: 0.00005481
Iteration 34/1000 | Loss: 0.00005481
Iteration 35/1000 | Loss: 0.00005480
Iteration 36/1000 | Loss: 0.00005480
Iteration 37/1000 | Loss: 0.00005479
Iteration 38/1000 | Loss: 0.00005479
Iteration 39/1000 | Loss: 0.00005478
Iteration 40/1000 | Loss: 0.00005478
Iteration 41/1000 | Loss: 0.00005478
Iteration 42/1000 | Loss: 0.00005478
Iteration 43/1000 | Loss: 0.00005478
Iteration 44/1000 | Loss: 0.00005478
Iteration 45/1000 | Loss: 0.00005477
Iteration 46/1000 | Loss: 0.00005477
Iteration 47/1000 | Loss: 0.00005477
Iteration 48/1000 | Loss: 0.00005477
Iteration 49/1000 | Loss: 0.00005477
Iteration 50/1000 | Loss: 0.00005477
Iteration 51/1000 | Loss: 0.00005476
Iteration 52/1000 | Loss: 0.00005476
Iteration 53/1000 | Loss: 0.00005476
Iteration 54/1000 | Loss: 0.00005476
Iteration 55/1000 | Loss: 0.00005476
Iteration 56/1000 | Loss: 0.00005475
Iteration 57/1000 | Loss: 0.00005475
Iteration 58/1000 | Loss: 0.00005475
Iteration 59/1000 | Loss: 0.00005474
Iteration 60/1000 | Loss: 0.00005474
Iteration 61/1000 | Loss: 0.00005474
Iteration 62/1000 | Loss: 0.00005474
Iteration 63/1000 | Loss: 0.00005473
Iteration 64/1000 | Loss: 0.00005473
Iteration 65/1000 | Loss: 0.00005473
Iteration 66/1000 | Loss: 0.00005473
Iteration 67/1000 | Loss: 0.00005473
Iteration 68/1000 | Loss: 0.00005472
Iteration 69/1000 | Loss: 0.00005472
Iteration 70/1000 | Loss: 0.00005471
Iteration 71/1000 | Loss: 0.00005471
Iteration 72/1000 | Loss: 0.00005471
Iteration 73/1000 | Loss: 0.00005471
Iteration 74/1000 | Loss: 0.00005471
Iteration 75/1000 | Loss: 0.00005471
Iteration 76/1000 | Loss: 0.00005471
Iteration 77/1000 | Loss: 0.00005470
Iteration 78/1000 | Loss: 0.00005470
Iteration 79/1000 | Loss: 0.00005470
Iteration 80/1000 | Loss: 0.00005470
Iteration 81/1000 | Loss: 0.00005470
Iteration 82/1000 | Loss: 0.00005469
Iteration 83/1000 | Loss: 0.00005469
Iteration 84/1000 | Loss: 0.00005468
Iteration 85/1000 | Loss: 0.00005468
Iteration 86/1000 | Loss: 0.00005468
Iteration 87/1000 | Loss: 0.00005467
Iteration 88/1000 | Loss: 0.00005467
Iteration 89/1000 | Loss: 0.00005467
Iteration 90/1000 | Loss: 0.00005466
Iteration 91/1000 | Loss: 0.00005466
Iteration 92/1000 | Loss: 0.00005466
Iteration 93/1000 | Loss: 0.00005466
Iteration 94/1000 | Loss: 0.00005466
Iteration 95/1000 | Loss: 0.00005466
Iteration 96/1000 | Loss: 0.00005465
Iteration 97/1000 | Loss: 0.00005465
Iteration 98/1000 | Loss: 0.00005465
Iteration 99/1000 | Loss: 0.00005465
Iteration 100/1000 | Loss: 0.00005465
Iteration 101/1000 | Loss: 0.00005464
Iteration 102/1000 | Loss: 0.00005464
Iteration 103/1000 | Loss: 0.00005464
Iteration 104/1000 | Loss: 0.00005464
Iteration 105/1000 | Loss: 0.00005464
Iteration 106/1000 | Loss: 0.00005464
Iteration 107/1000 | Loss: 0.00005464
Iteration 108/1000 | Loss: 0.00005464
Iteration 109/1000 | Loss: 0.00005463
Iteration 110/1000 | Loss: 0.00005463
Iteration 111/1000 | Loss: 0.00005463
Iteration 112/1000 | Loss: 0.00005463
Iteration 113/1000 | Loss: 0.00005463
Iteration 114/1000 | Loss: 0.00005463
Iteration 115/1000 | Loss: 0.00005463
Iteration 116/1000 | Loss: 0.00005463
Iteration 117/1000 | Loss: 0.00005463
Iteration 118/1000 | Loss: 0.00005463
Iteration 119/1000 | Loss: 0.00005463
Iteration 120/1000 | Loss: 0.00005463
Iteration 121/1000 | Loss: 0.00005463
Iteration 122/1000 | Loss: 0.00005463
Iteration 123/1000 | Loss: 0.00005463
Iteration 124/1000 | Loss: 0.00005462
Iteration 125/1000 | Loss: 0.00005462
Iteration 126/1000 | Loss: 0.00005462
Iteration 127/1000 | Loss: 0.00005462
Iteration 128/1000 | Loss: 0.00005462
Iteration 129/1000 | Loss: 0.00005462
Iteration 130/1000 | Loss: 0.00005462
Iteration 131/1000 | Loss: 0.00005462
Iteration 132/1000 | Loss: 0.00005462
Iteration 133/1000 | Loss: 0.00005462
Iteration 134/1000 | Loss: 0.00005462
Iteration 135/1000 | Loss: 0.00005462
Iteration 136/1000 | Loss: 0.00005461
Iteration 137/1000 | Loss: 0.00005461
Iteration 138/1000 | Loss: 0.00005461
Iteration 139/1000 | Loss: 0.00005461
Iteration 140/1000 | Loss: 0.00005461
Iteration 141/1000 | Loss: 0.00005461
Iteration 142/1000 | Loss: 0.00005461
Iteration 143/1000 | Loss: 0.00005461
Iteration 144/1000 | Loss: 0.00005461
Iteration 145/1000 | Loss: 0.00005461
Iteration 146/1000 | Loss: 0.00005461
Iteration 147/1000 | Loss: 0.00005461
Iteration 148/1000 | Loss: 0.00005461
Iteration 149/1000 | Loss: 0.00005461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [5.4609336075372994e-05, 5.4609336075372994e-05, 5.4609336075372994e-05, 5.4609336075372994e-05, 5.4609336075372994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.4609336075372994e-05

Optimization complete. Final v2v error: 6.530932426452637 mm

Highest mean error: 6.813237190246582 mm for frame 177

Lowest mean error: 6.245070934295654 mm for frame 155

Saving results

Total time: 39.85555982589722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859838
Iteration 2/25 | Loss: 0.00274814
Iteration 3/25 | Loss: 0.00181393
Iteration 4/25 | Loss: 0.00155143
Iteration 5/25 | Loss: 0.00140644
Iteration 6/25 | Loss: 0.00142307
Iteration 7/25 | Loss: 0.00133888
Iteration 8/25 | Loss: 0.00130630
Iteration 9/25 | Loss: 0.00129635
Iteration 10/25 | Loss: 0.00129105
Iteration 11/25 | Loss: 0.00128924
Iteration 12/25 | Loss: 0.00128845
Iteration 13/25 | Loss: 0.00128800
Iteration 14/25 | Loss: 0.00128764
Iteration 15/25 | Loss: 0.00128716
Iteration 16/25 | Loss: 0.00128634
Iteration 17/25 | Loss: 0.00128545
Iteration 18/25 | Loss: 0.00128497
Iteration 19/25 | Loss: 0.00128470
Iteration 20/25 | Loss: 0.00128454
Iteration 21/25 | Loss: 0.00128445
Iteration 22/25 | Loss: 0.00128435
Iteration 23/25 | Loss: 0.00128426
Iteration 24/25 | Loss: 0.00128418
Iteration 25/25 | Loss: 0.00128416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28330421
Iteration 2/25 | Loss: 0.00471467
Iteration 3/25 | Loss: 0.00358803
Iteration 4/25 | Loss: 0.00358803
Iteration 5/25 | Loss: 0.00358803
Iteration 6/25 | Loss: 0.00358803
Iteration 7/25 | Loss: 0.00358803
Iteration 8/25 | Loss: 0.00358803
Iteration 9/25 | Loss: 0.00358803
Iteration 10/25 | Loss: 0.00358803
Iteration 11/25 | Loss: 0.00358803
Iteration 12/25 | Loss: 0.00358803
Iteration 13/25 | Loss: 0.00358803
Iteration 14/25 | Loss: 0.00358803
Iteration 15/25 | Loss: 0.00358803
Iteration 16/25 | Loss: 0.00358803
Iteration 17/25 | Loss: 0.00358803
Iteration 18/25 | Loss: 0.00358803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0035880296491086483, 0.0035880296491086483, 0.0035880296491086483, 0.0035880296491086483, 0.0035880296491086483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035880296491086483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00358803
Iteration 2/1000 | Loss: 0.00243014
Iteration 3/1000 | Loss: 0.00141588
Iteration 4/1000 | Loss: 0.00175135
Iteration 5/1000 | Loss: 0.00024024
Iteration 6/1000 | Loss: 0.00236048
Iteration 7/1000 | Loss: 0.00284207
Iteration 8/1000 | Loss: 0.00028646
Iteration 9/1000 | Loss: 0.00022728
Iteration 10/1000 | Loss: 0.00009299
Iteration 11/1000 | Loss: 0.00071502
Iteration 12/1000 | Loss: 0.00094202
Iteration 13/1000 | Loss: 0.00023768
Iteration 14/1000 | Loss: 0.00005893
Iteration 15/1000 | Loss: 0.00057402
Iteration 16/1000 | Loss: 0.00037147
Iteration 17/1000 | Loss: 0.00097990
Iteration 18/1000 | Loss: 0.00062113
Iteration 19/1000 | Loss: 0.00117132
Iteration 20/1000 | Loss: 0.00080726
Iteration 21/1000 | Loss: 0.00060088
Iteration 22/1000 | Loss: 0.00064469
Iteration 23/1000 | Loss: 0.00074372
Iteration 24/1000 | Loss: 0.00088623
Iteration 25/1000 | Loss: 0.00011642
Iteration 26/1000 | Loss: 0.00004569
Iteration 27/1000 | Loss: 0.00004062
Iteration 28/1000 | Loss: 0.00059613
Iteration 29/1000 | Loss: 0.00151629
Iteration 30/1000 | Loss: 0.00094615
Iteration 31/1000 | Loss: 0.00082539
Iteration 32/1000 | Loss: 0.00040159
Iteration 33/1000 | Loss: 0.00084774
Iteration 34/1000 | Loss: 0.00137591
Iteration 35/1000 | Loss: 0.00161896
Iteration 36/1000 | Loss: 0.00118880
Iteration 37/1000 | Loss: 0.00156090
Iteration 38/1000 | Loss: 0.00071643
Iteration 39/1000 | Loss: 0.00053457
Iteration 40/1000 | Loss: 0.00060208
Iteration 41/1000 | Loss: 0.00038950
Iteration 42/1000 | Loss: 0.00016484
Iteration 43/1000 | Loss: 0.00017379
Iteration 44/1000 | Loss: 0.00084040
Iteration 45/1000 | Loss: 0.00018385
Iteration 46/1000 | Loss: 0.00019263
Iteration 47/1000 | Loss: 0.00019211
Iteration 48/1000 | Loss: 0.00004137
Iteration 49/1000 | Loss: 0.00003816
Iteration 50/1000 | Loss: 0.00069516
Iteration 51/1000 | Loss: 0.00024921
Iteration 52/1000 | Loss: 0.00036114
Iteration 53/1000 | Loss: 0.00042266
Iteration 54/1000 | Loss: 0.00016024
Iteration 55/1000 | Loss: 0.00003424
Iteration 56/1000 | Loss: 0.00003009
Iteration 57/1000 | Loss: 0.00056536
Iteration 58/1000 | Loss: 0.00027394
Iteration 59/1000 | Loss: 0.00066690
Iteration 60/1000 | Loss: 0.00046062
Iteration 61/1000 | Loss: 0.00049574
Iteration 62/1000 | Loss: 0.00006510
Iteration 63/1000 | Loss: 0.00003880
Iteration 64/1000 | Loss: 0.00106908
Iteration 65/1000 | Loss: 0.00035213
Iteration 66/1000 | Loss: 0.00019586
Iteration 67/1000 | Loss: 0.00037045
Iteration 68/1000 | Loss: 0.00012978
Iteration 69/1000 | Loss: 0.00018465
Iteration 70/1000 | Loss: 0.00016547
Iteration 71/1000 | Loss: 0.00013523
Iteration 72/1000 | Loss: 0.00028795
Iteration 73/1000 | Loss: 0.00012937
Iteration 74/1000 | Loss: 0.00002999
Iteration 75/1000 | Loss: 0.00034607
Iteration 76/1000 | Loss: 0.00086081
Iteration 77/1000 | Loss: 0.00036234
Iteration 78/1000 | Loss: 0.00016011
Iteration 79/1000 | Loss: 0.00022212
Iteration 80/1000 | Loss: 0.00016647
Iteration 81/1000 | Loss: 0.00075189
Iteration 82/1000 | Loss: 0.00009614
Iteration 83/1000 | Loss: 0.00002700
Iteration 84/1000 | Loss: 0.00002488
Iteration 85/1000 | Loss: 0.00004359
Iteration 86/1000 | Loss: 0.00016934
Iteration 87/1000 | Loss: 0.00040692
Iteration 88/1000 | Loss: 0.00029982
Iteration 89/1000 | Loss: 0.00025118
Iteration 90/1000 | Loss: 0.00027428
Iteration 91/1000 | Loss: 0.00002432
Iteration 92/1000 | Loss: 0.00027495
Iteration 93/1000 | Loss: 0.00037786
Iteration 94/1000 | Loss: 0.00002436
Iteration 95/1000 | Loss: 0.00050728
Iteration 96/1000 | Loss: 0.00059778
Iteration 97/1000 | Loss: 0.00042333
Iteration 98/1000 | Loss: 0.00046051
Iteration 99/1000 | Loss: 0.00038407
Iteration 100/1000 | Loss: 0.00065678
Iteration 101/1000 | Loss: 0.00071551
Iteration 102/1000 | Loss: 0.00061841
Iteration 103/1000 | Loss: 0.00022034
Iteration 104/1000 | Loss: 0.00003423
Iteration 105/1000 | Loss: 0.00032757
Iteration 106/1000 | Loss: 0.00013599
Iteration 107/1000 | Loss: 0.00002613
Iteration 108/1000 | Loss: 0.00002387
Iteration 109/1000 | Loss: 0.00002305
Iteration 110/1000 | Loss: 0.00002368
Iteration 111/1000 | Loss: 0.00014175
Iteration 112/1000 | Loss: 0.00062312
Iteration 113/1000 | Loss: 0.00030964
Iteration 114/1000 | Loss: 0.00014781
Iteration 115/1000 | Loss: 0.00002967
Iteration 116/1000 | Loss: 0.00051191
Iteration 117/1000 | Loss: 0.00028309
Iteration 118/1000 | Loss: 0.00016427
Iteration 119/1000 | Loss: 0.00002733
Iteration 120/1000 | Loss: 0.00002370
Iteration 121/1000 | Loss: 0.00002157
Iteration 122/1000 | Loss: 0.00002054
Iteration 123/1000 | Loss: 0.00066897
Iteration 124/1000 | Loss: 0.00032473
Iteration 125/1000 | Loss: 0.00041468
Iteration 126/1000 | Loss: 0.00028820
Iteration 127/1000 | Loss: 0.00040908
Iteration 128/1000 | Loss: 0.00003873
Iteration 129/1000 | Loss: 0.00040627
Iteration 130/1000 | Loss: 0.00022279
Iteration 131/1000 | Loss: 0.00006285
Iteration 132/1000 | Loss: 0.00007469
Iteration 133/1000 | Loss: 0.00015569
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00001932
Iteration 136/1000 | Loss: 0.00001863
Iteration 137/1000 | Loss: 0.00001805
Iteration 138/1000 | Loss: 0.00015338
Iteration 139/1000 | Loss: 0.00023645
Iteration 140/1000 | Loss: 0.00004961
Iteration 141/1000 | Loss: 0.00035367
Iteration 142/1000 | Loss: 0.00041138
Iteration 143/1000 | Loss: 0.00032890
Iteration 144/1000 | Loss: 0.00002039
Iteration 145/1000 | Loss: 0.00001732
Iteration 146/1000 | Loss: 0.00019930
Iteration 147/1000 | Loss: 0.00043136
Iteration 148/1000 | Loss: 0.00030217
Iteration 149/1000 | Loss: 0.00012578
Iteration 150/1000 | Loss: 0.00002771
Iteration 151/1000 | Loss: 0.00002352
Iteration 152/1000 | Loss: 0.00002181
Iteration 153/1000 | Loss: 0.00002228
Iteration 154/1000 | Loss: 0.00001973
Iteration 155/1000 | Loss: 0.00001828
Iteration 156/1000 | Loss: 0.00002147
Iteration 157/1000 | Loss: 0.00001596
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001387
Iteration 162/1000 | Loss: 0.00001380
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001375
Iteration 166/1000 | Loss: 0.00001375
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001371
Iteration 169/1000 | Loss: 0.00001370
Iteration 170/1000 | Loss: 0.00001370
Iteration 171/1000 | Loss: 0.00001361
Iteration 172/1000 | Loss: 0.00001357
Iteration 173/1000 | Loss: 0.00001356
Iteration 174/1000 | Loss: 0.00001355
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001353
Iteration 177/1000 | Loss: 0.00001351
Iteration 178/1000 | Loss: 0.00001350
Iteration 179/1000 | Loss: 0.00001349
Iteration 180/1000 | Loss: 0.00001348
Iteration 181/1000 | Loss: 0.00001348
Iteration 182/1000 | Loss: 0.00001348
Iteration 183/1000 | Loss: 0.00001347
Iteration 184/1000 | Loss: 0.00001347
Iteration 185/1000 | Loss: 0.00001346
Iteration 186/1000 | Loss: 0.00001346
Iteration 187/1000 | Loss: 0.00001346
Iteration 188/1000 | Loss: 0.00001346
Iteration 189/1000 | Loss: 0.00001346
Iteration 190/1000 | Loss: 0.00001345
Iteration 191/1000 | Loss: 0.00001345
Iteration 192/1000 | Loss: 0.00001345
Iteration 193/1000 | Loss: 0.00001345
Iteration 194/1000 | Loss: 0.00001345
Iteration 195/1000 | Loss: 0.00001345
Iteration 196/1000 | Loss: 0.00001345
Iteration 197/1000 | Loss: 0.00001345
Iteration 198/1000 | Loss: 0.00001345
Iteration 199/1000 | Loss: 0.00001345
Iteration 200/1000 | Loss: 0.00001345
Iteration 201/1000 | Loss: 0.00001345
Iteration 202/1000 | Loss: 0.00001345
Iteration 203/1000 | Loss: 0.00001345
Iteration 204/1000 | Loss: 0.00001345
Iteration 205/1000 | Loss: 0.00001345
Iteration 206/1000 | Loss: 0.00001345
Iteration 207/1000 | Loss: 0.00001345
Iteration 208/1000 | Loss: 0.00001345
Iteration 209/1000 | Loss: 0.00001345
Iteration 210/1000 | Loss: 0.00001345
Iteration 211/1000 | Loss: 0.00001345
Iteration 212/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.3448776371660642e-05, 1.3448776371660642e-05, 1.3448776371660642e-05, 1.3448776371660642e-05, 1.3448776371660642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3448776371660642e-05

Optimization complete. Final v2v error: 2.997332811355591 mm

Highest mean error: 6.319059371948242 mm for frame 68

Lowest mean error: 2.1131935119628906 mm for frame 5

Saving results

Total time: 291.7653214931488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775578
Iteration 2/25 | Loss: 0.00146231
Iteration 3/25 | Loss: 0.00119669
Iteration 4/25 | Loss: 0.00116755
Iteration 5/25 | Loss: 0.00115827
Iteration 6/25 | Loss: 0.00115566
Iteration 7/25 | Loss: 0.00115566
Iteration 8/25 | Loss: 0.00115566
Iteration 9/25 | Loss: 0.00115566
Iteration 10/25 | Loss: 0.00115566
Iteration 11/25 | Loss: 0.00115566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011556593235582113, 0.0011556593235582113, 0.0011556593235582113, 0.0011556593235582113, 0.0011556593235582113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011556593235582113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.80569744
Iteration 2/25 | Loss: 0.00136362
Iteration 3/25 | Loss: 0.00136341
Iteration 4/25 | Loss: 0.00136341
Iteration 5/25 | Loss: 0.00136341
Iteration 6/25 | Loss: 0.00136341
Iteration 7/25 | Loss: 0.00136341
Iteration 8/25 | Loss: 0.00136341
Iteration 9/25 | Loss: 0.00136341
Iteration 10/25 | Loss: 0.00136341
Iteration 11/25 | Loss: 0.00136341
Iteration 12/25 | Loss: 0.00136341
Iteration 13/25 | Loss: 0.00136341
Iteration 14/25 | Loss: 0.00136341
Iteration 15/25 | Loss: 0.00136341
Iteration 16/25 | Loss: 0.00136341
Iteration 17/25 | Loss: 0.00136341
Iteration 18/25 | Loss: 0.00136341
Iteration 19/25 | Loss: 0.00136341
Iteration 20/25 | Loss: 0.00136341
Iteration 21/25 | Loss: 0.00136341
Iteration 22/25 | Loss: 0.00136341
Iteration 23/25 | Loss: 0.00136341
Iteration 24/25 | Loss: 0.00136341
Iteration 25/25 | Loss: 0.00136341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136341
Iteration 2/1000 | Loss: 0.00007345
Iteration 3/1000 | Loss: 0.00004823
Iteration 4/1000 | Loss: 0.00004144
Iteration 5/1000 | Loss: 0.00003941
Iteration 6/1000 | Loss: 0.00003834
Iteration 7/1000 | Loss: 0.00003755
Iteration 8/1000 | Loss: 0.00003692
Iteration 9/1000 | Loss: 0.00003636
Iteration 10/1000 | Loss: 0.00003600
Iteration 11/1000 | Loss: 0.00003570
Iteration 12/1000 | Loss: 0.00003544
Iteration 13/1000 | Loss: 0.00003526
Iteration 14/1000 | Loss: 0.00003505
Iteration 15/1000 | Loss: 0.00003489
Iteration 16/1000 | Loss: 0.00003474
Iteration 17/1000 | Loss: 0.00003462
Iteration 18/1000 | Loss: 0.00003453
Iteration 19/1000 | Loss: 0.00003449
Iteration 20/1000 | Loss: 0.00003436
Iteration 21/1000 | Loss: 0.00003435
Iteration 22/1000 | Loss: 0.00003435
Iteration 23/1000 | Loss: 0.00003434
Iteration 24/1000 | Loss: 0.00003433
Iteration 25/1000 | Loss: 0.00003428
Iteration 26/1000 | Loss: 0.00003423
Iteration 27/1000 | Loss: 0.00003423
Iteration 28/1000 | Loss: 0.00003420
Iteration 29/1000 | Loss: 0.00003414
Iteration 30/1000 | Loss: 0.00003409
Iteration 31/1000 | Loss: 0.00003408
Iteration 32/1000 | Loss: 0.00003408
Iteration 33/1000 | Loss: 0.00003406
Iteration 34/1000 | Loss: 0.00003406
Iteration 35/1000 | Loss: 0.00003405
Iteration 36/1000 | Loss: 0.00003405
Iteration 37/1000 | Loss: 0.00003404
Iteration 38/1000 | Loss: 0.00003404
Iteration 39/1000 | Loss: 0.00003403
Iteration 40/1000 | Loss: 0.00003403
Iteration 41/1000 | Loss: 0.00003403
Iteration 42/1000 | Loss: 0.00003403
Iteration 43/1000 | Loss: 0.00003401
Iteration 44/1000 | Loss: 0.00003401
Iteration 45/1000 | Loss: 0.00003400
Iteration 46/1000 | Loss: 0.00003400
Iteration 47/1000 | Loss: 0.00003400
Iteration 48/1000 | Loss: 0.00003399
Iteration 49/1000 | Loss: 0.00003399
Iteration 50/1000 | Loss: 0.00003399
Iteration 51/1000 | Loss: 0.00003399
Iteration 52/1000 | Loss: 0.00003398
Iteration 53/1000 | Loss: 0.00003398
Iteration 54/1000 | Loss: 0.00003398
Iteration 55/1000 | Loss: 0.00003398
Iteration 56/1000 | Loss: 0.00003398
Iteration 57/1000 | Loss: 0.00003398
Iteration 58/1000 | Loss: 0.00003398
Iteration 59/1000 | Loss: 0.00003398
Iteration 60/1000 | Loss: 0.00003398
Iteration 61/1000 | Loss: 0.00003397
Iteration 62/1000 | Loss: 0.00003397
Iteration 63/1000 | Loss: 0.00003397
Iteration 64/1000 | Loss: 0.00003396
Iteration 65/1000 | Loss: 0.00003396
Iteration 66/1000 | Loss: 0.00003396
Iteration 67/1000 | Loss: 0.00003395
Iteration 68/1000 | Loss: 0.00003395
Iteration 69/1000 | Loss: 0.00003395
Iteration 70/1000 | Loss: 0.00003394
Iteration 71/1000 | Loss: 0.00003394
Iteration 72/1000 | Loss: 0.00003394
Iteration 73/1000 | Loss: 0.00003393
Iteration 74/1000 | Loss: 0.00003393
Iteration 75/1000 | Loss: 0.00003392
Iteration 76/1000 | Loss: 0.00003392
Iteration 77/1000 | Loss: 0.00003392
Iteration 78/1000 | Loss: 0.00003392
Iteration 79/1000 | Loss: 0.00003392
Iteration 80/1000 | Loss: 0.00003392
Iteration 81/1000 | Loss: 0.00003392
Iteration 82/1000 | Loss: 0.00003392
Iteration 83/1000 | Loss: 0.00003392
Iteration 84/1000 | Loss: 0.00003391
Iteration 85/1000 | Loss: 0.00003391
Iteration 86/1000 | Loss: 0.00003391
Iteration 87/1000 | Loss: 0.00003391
Iteration 88/1000 | Loss: 0.00003390
Iteration 89/1000 | Loss: 0.00003390
Iteration 90/1000 | Loss: 0.00003390
Iteration 91/1000 | Loss: 0.00003390
Iteration 92/1000 | Loss: 0.00003390
Iteration 93/1000 | Loss: 0.00003389
Iteration 94/1000 | Loss: 0.00003389
Iteration 95/1000 | Loss: 0.00003389
Iteration 96/1000 | Loss: 0.00003389
Iteration 97/1000 | Loss: 0.00003389
Iteration 98/1000 | Loss: 0.00003388
Iteration 99/1000 | Loss: 0.00003388
Iteration 100/1000 | Loss: 0.00003388
Iteration 101/1000 | Loss: 0.00003388
Iteration 102/1000 | Loss: 0.00003387
Iteration 103/1000 | Loss: 0.00003387
Iteration 104/1000 | Loss: 0.00003387
Iteration 105/1000 | Loss: 0.00003387
Iteration 106/1000 | Loss: 0.00003387
Iteration 107/1000 | Loss: 0.00003387
Iteration 108/1000 | Loss: 0.00003386
Iteration 109/1000 | Loss: 0.00003386
Iteration 110/1000 | Loss: 0.00003386
Iteration 111/1000 | Loss: 0.00003386
Iteration 112/1000 | Loss: 0.00003386
Iteration 113/1000 | Loss: 0.00003385
Iteration 114/1000 | Loss: 0.00003385
Iteration 115/1000 | Loss: 0.00003385
Iteration 116/1000 | Loss: 0.00003385
Iteration 117/1000 | Loss: 0.00003384
Iteration 118/1000 | Loss: 0.00003384
Iteration 119/1000 | Loss: 0.00003384
Iteration 120/1000 | Loss: 0.00003384
Iteration 121/1000 | Loss: 0.00003384
Iteration 122/1000 | Loss: 0.00003384
Iteration 123/1000 | Loss: 0.00003383
Iteration 124/1000 | Loss: 0.00003383
Iteration 125/1000 | Loss: 0.00003383
Iteration 126/1000 | Loss: 0.00003382
Iteration 127/1000 | Loss: 0.00003382
Iteration 128/1000 | Loss: 0.00003382
Iteration 129/1000 | Loss: 0.00003382
Iteration 130/1000 | Loss: 0.00003382
Iteration 131/1000 | Loss: 0.00003382
Iteration 132/1000 | Loss: 0.00003382
Iteration 133/1000 | Loss: 0.00003381
Iteration 134/1000 | Loss: 0.00003381
Iteration 135/1000 | Loss: 0.00003381
Iteration 136/1000 | Loss: 0.00003381
Iteration 137/1000 | Loss: 0.00003381
Iteration 138/1000 | Loss: 0.00003381
Iteration 139/1000 | Loss: 0.00003380
Iteration 140/1000 | Loss: 0.00003380
Iteration 141/1000 | Loss: 0.00003380
Iteration 142/1000 | Loss: 0.00003380
Iteration 143/1000 | Loss: 0.00003380
Iteration 144/1000 | Loss: 0.00003380
Iteration 145/1000 | Loss: 0.00003380
Iteration 146/1000 | Loss: 0.00003380
Iteration 147/1000 | Loss: 0.00003380
Iteration 148/1000 | Loss: 0.00003380
Iteration 149/1000 | Loss: 0.00003380
Iteration 150/1000 | Loss: 0.00003380
Iteration 151/1000 | Loss: 0.00003380
Iteration 152/1000 | Loss: 0.00003379
Iteration 153/1000 | Loss: 0.00003379
Iteration 154/1000 | Loss: 0.00003379
Iteration 155/1000 | Loss: 0.00003379
Iteration 156/1000 | Loss: 0.00003379
Iteration 157/1000 | Loss: 0.00003379
Iteration 158/1000 | Loss: 0.00003379
Iteration 159/1000 | Loss: 0.00003379
Iteration 160/1000 | Loss: 0.00003379
Iteration 161/1000 | Loss: 0.00003379
Iteration 162/1000 | Loss: 0.00003379
Iteration 163/1000 | Loss: 0.00003379
Iteration 164/1000 | Loss: 0.00003379
Iteration 165/1000 | Loss: 0.00003379
Iteration 166/1000 | Loss: 0.00003379
Iteration 167/1000 | Loss: 0.00003379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.3791868190746754e-05, 3.3791868190746754e-05, 3.3791868190746754e-05, 3.3791868190746754e-05, 3.3791868190746754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3791868190746754e-05

Optimization complete. Final v2v error: 4.5929131507873535 mm

Highest mean error: 5.949885845184326 mm for frame 41

Lowest mean error: 3.151167392730713 mm for frame 18

Saving results

Total time: 58.20704221725464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607212
Iteration 2/25 | Loss: 0.00144302
Iteration 3/25 | Loss: 0.00114192
Iteration 4/25 | Loss: 0.00111527
Iteration 5/25 | Loss: 0.00111164
Iteration 6/25 | Loss: 0.00111107
Iteration 7/25 | Loss: 0.00111107
Iteration 8/25 | Loss: 0.00111107
Iteration 9/25 | Loss: 0.00111107
Iteration 10/25 | Loss: 0.00111107
Iteration 11/25 | Loss: 0.00111107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011110723717138171, 0.0011110723717138171, 0.0011110723717138171, 0.0011110723717138171, 0.0011110723717138171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011110723717138171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08299351
Iteration 2/25 | Loss: 0.00122437
Iteration 3/25 | Loss: 0.00122437
Iteration 4/25 | Loss: 0.00122436
Iteration 5/25 | Loss: 0.00122436
Iteration 6/25 | Loss: 0.00122436
Iteration 7/25 | Loss: 0.00122436
Iteration 8/25 | Loss: 0.00122436
Iteration 9/25 | Loss: 0.00122436
Iteration 10/25 | Loss: 0.00122436
Iteration 11/25 | Loss: 0.00122436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012243648525327444, 0.0012243648525327444, 0.0012243648525327444, 0.0012243648525327444, 0.0012243648525327444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012243648525327444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122436
Iteration 2/1000 | Loss: 0.00003079
Iteration 3/1000 | Loss: 0.00002086
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001695
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001519
Iteration 11/1000 | Loss: 0.00001513
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001472
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001458
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001454
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001443
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001427
Iteration 69/1000 | Loss: 0.00001427
Iteration 70/1000 | Loss: 0.00001427
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001426
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001425
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001424
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00001424
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001422
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001421
Iteration 102/1000 | Loss: 0.00001421
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001421
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001420
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001420
Iteration 113/1000 | Loss: 0.00001420
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001418
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001418
Iteration 123/1000 | Loss: 0.00001418
Iteration 124/1000 | Loss: 0.00001418
Iteration 125/1000 | Loss: 0.00001417
Iteration 126/1000 | Loss: 0.00001417
Iteration 127/1000 | Loss: 0.00001417
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001417
Iteration 130/1000 | Loss: 0.00001417
Iteration 131/1000 | Loss: 0.00001417
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001416
Iteration 138/1000 | Loss: 0.00001416
Iteration 139/1000 | Loss: 0.00001416
Iteration 140/1000 | Loss: 0.00001416
Iteration 141/1000 | Loss: 0.00001416
Iteration 142/1000 | Loss: 0.00001416
Iteration 143/1000 | Loss: 0.00001416
Iteration 144/1000 | Loss: 0.00001416
Iteration 145/1000 | Loss: 0.00001416
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001415
Iteration 148/1000 | Loss: 0.00001415
Iteration 149/1000 | Loss: 0.00001415
Iteration 150/1000 | Loss: 0.00001415
Iteration 151/1000 | Loss: 0.00001415
Iteration 152/1000 | Loss: 0.00001415
Iteration 153/1000 | Loss: 0.00001415
Iteration 154/1000 | Loss: 0.00001414
Iteration 155/1000 | Loss: 0.00001414
Iteration 156/1000 | Loss: 0.00001414
Iteration 157/1000 | Loss: 0.00001414
Iteration 158/1000 | Loss: 0.00001414
Iteration 159/1000 | Loss: 0.00001413
Iteration 160/1000 | Loss: 0.00001413
Iteration 161/1000 | Loss: 0.00001413
Iteration 162/1000 | Loss: 0.00001413
Iteration 163/1000 | Loss: 0.00001413
Iteration 164/1000 | Loss: 0.00001413
Iteration 165/1000 | Loss: 0.00001413
Iteration 166/1000 | Loss: 0.00001413
Iteration 167/1000 | Loss: 0.00001413
Iteration 168/1000 | Loss: 0.00001413
Iteration 169/1000 | Loss: 0.00001413
Iteration 170/1000 | Loss: 0.00001413
Iteration 171/1000 | Loss: 0.00001413
Iteration 172/1000 | Loss: 0.00001412
Iteration 173/1000 | Loss: 0.00001412
Iteration 174/1000 | Loss: 0.00001412
Iteration 175/1000 | Loss: 0.00001412
Iteration 176/1000 | Loss: 0.00001412
Iteration 177/1000 | Loss: 0.00001412
Iteration 178/1000 | Loss: 0.00001412
Iteration 179/1000 | Loss: 0.00001412
Iteration 180/1000 | Loss: 0.00001412
Iteration 181/1000 | Loss: 0.00001412
Iteration 182/1000 | Loss: 0.00001412
Iteration 183/1000 | Loss: 0.00001412
Iteration 184/1000 | Loss: 0.00001412
Iteration 185/1000 | Loss: 0.00001412
Iteration 186/1000 | Loss: 0.00001412
Iteration 187/1000 | Loss: 0.00001412
Iteration 188/1000 | Loss: 0.00001412
Iteration 189/1000 | Loss: 0.00001412
Iteration 190/1000 | Loss: 0.00001412
Iteration 191/1000 | Loss: 0.00001412
Iteration 192/1000 | Loss: 0.00001412
Iteration 193/1000 | Loss: 0.00001411
Iteration 194/1000 | Loss: 0.00001411
Iteration 195/1000 | Loss: 0.00001411
Iteration 196/1000 | Loss: 0.00001411
Iteration 197/1000 | Loss: 0.00001411
Iteration 198/1000 | Loss: 0.00001411
Iteration 199/1000 | Loss: 0.00001411
Iteration 200/1000 | Loss: 0.00001411
Iteration 201/1000 | Loss: 0.00001411
Iteration 202/1000 | Loss: 0.00001411
Iteration 203/1000 | Loss: 0.00001411
Iteration 204/1000 | Loss: 0.00001411
Iteration 205/1000 | Loss: 0.00001411
Iteration 206/1000 | Loss: 0.00001411
Iteration 207/1000 | Loss: 0.00001411
Iteration 208/1000 | Loss: 0.00001411
Iteration 209/1000 | Loss: 0.00001411
Iteration 210/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.41148721013451e-05, 1.41148721013451e-05, 1.41148721013451e-05, 1.41148721013451e-05, 1.41148721013451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.41148721013451e-05

Optimization complete. Final v2v error: 2.9880120754241943 mm

Highest mean error: 5.0946574211120605 mm for frame 55

Lowest mean error: 2.3876116275787354 mm for frame 147

Saving results

Total time: 40.67579388618469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581666
Iteration 2/25 | Loss: 0.00150074
Iteration 3/25 | Loss: 0.00118657
Iteration 4/25 | Loss: 0.00115210
Iteration 5/25 | Loss: 0.00114487
Iteration 6/25 | Loss: 0.00115279
Iteration 7/25 | Loss: 0.00113492
Iteration 8/25 | Loss: 0.00113081
Iteration 9/25 | Loss: 0.00113339
Iteration 10/25 | Loss: 0.00112988
Iteration 11/25 | Loss: 0.00112761
Iteration 12/25 | Loss: 0.00112729
Iteration 13/25 | Loss: 0.00112719
Iteration 14/25 | Loss: 0.00112718
Iteration 15/25 | Loss: 0.00112717
Iteration 16/25 | Loss: 0.00112717
Iteration 17/25 | Loss: 0.00112717
Iteration 18/25 | Loss: 0.00112717
Iteration 19/25 | Loss: 0.00112716
Iteration 20/25 | Loss: 0.00112716
Iteration 21/25 | Loss: 0.00112716
Iteration 22/25 | Loss: 0.00112716
Iteration 23/25 | Loss: 0.00112715
Iteration 24/25 | Loss: 0.00112715
Iteration 25/25 | Loss: 0.00112715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15568256
Iteration 2/25 | Loss: 0.00171029
Iteration 3/25 | Loss: 0.00171026
Iteration 4/25 | Loss: 0.00171026
Iteration 5/25 | Loss: 0.00171026
Iteration 6/25 | Loss: 0.00171025
Iteration 7/25 | Loss: 0.00171025
Iteration 8/25 | Loss: 0.00171025
Iteration 9/25 | Loss: 0.00171025
Iteration 10/25 | Loss: 0.00171025
Iteration 11/25 | Loss: 0.00171025
Iteration 12/25 | Loss: 0.00171025
Iteration 13/25 | Loss: 0.00171025
Iteration 14/25 | Loss: 0.00171025
Iteration 15/25 | Loss: 0.00171025
Iteration 16/25 | Loss: 0.00171025
Iteration 17/25 | Loss: 0.00171025
Iteration 18/25 | Loss: 0.00171025
Iteration 19/25 | Loss: 0.00171025
Iteration 20/25 | Loss: 0.00171025
Iteration 21/25 | Loss: 0.00171025
Iteration 22/25 | Loss: 0.00171025
Iteration 23/25 | Loss: 0.00171025
Iteration 24/25 | Loss: 0.00171025
Iteration 25/25 | Loss: 0.00171025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171025
Iteration 2/1000 | Loss: 0.00011530
Iteration 3/1000 | Loss: 0.00008453
Iteration 4/1000 | Loss: 0.00007524
Iteration 5/1000 | Loss: 0.00006737
Iteration 6/1000 | Loss: 0.00006311
Iteration 7/1000 | Loss: 0.00005993
Iteration 8/1000 | Loss: 0.00071440
Iteration 9/1000 | Loss: 0.00033229
Iteration 10/1000 | Loss: 0.00068192
Iteration 11/1000 | Loss: 0.00032866
Iteration 12/1000 | Loss: 0.00005913
Iteration 13/1000 | Loss: 0.00005580
Iteration 14/1000 | Loss: 0.00115332
Iteration 15/1000 | Loss: 0.00026145
Iteration 16/1000 | Loss: 0.00006081
Iteration 17/1000 | Loss: 0.00053395
Iteration 18/1000 | Loss: 0.00032487
Iteration 19/1000 | Loss: 0.00052612
Iteration 20/1000 | Loss: 0.00142052
Iteration 21/1000 | Loss: 0.00029768
Iteration 22/1000 | Loss: 0.00029694
Iteration 23/1000 | Loss: 0.00005470
Iteration 24/1000 | Loss: 0.00027609
Iteration 25/1000 | Loss: 0.00015848
Iteration 26/1000 | Loss: 0.00019644
Iteration 27/1000 | Loss: 0.00005780
Iteration 28/1000 | Loss: 0.00029758
Iteration 29/1000 | Loss: 0.00004941
Iteration 30/1000 | Loss: 0.00004643
Iteration 31/1000 | Loss: 0.00041131
Iteration 32/1000 | Loss: 0.00013771
Iteration 33/1000 | Loss: 0.00004485
Iteration 34/1000 | Loss: 0.00043570
Iteration 35/1000 | Loss: 0.00030417
Iteration 36/1000 | Loss: 0.00043591
Iteration 37/1000 | Loss: 0.00030367
Iteration 38/1000 | Loss: 0.00005082
Iteration 39/1000 | Loss: 0.00031802
Iteration 40/1000 | Loss: 0.00024958
Iteration 41/1000 | Loss: 0.00004803
Iteration 42/1000 | Loss: 0.00004149
Iteration 43/1000 | Loss: 0.00004007
Iteration 44/1000 | Loss: 0.00003888
Iteration 45/1000 | Loss: 0.00003819
Iteration 46/1000 | Loss: 0.00003765
Iteration 47/1000 | Loss: 0.00003730
Iteration 48/1000 | Loss: 0.00003689
Iteration 49/1000 | Loss: 0.00003661
Iteration 50/1000 | Loss: 0.00003632
Iteration 51/1000 | Loss: 0.00003612
Iteration 52/1000 | Loss: 0.00003585
Iteration 53/1000 | Loss: 0.00003567
Iteration 54/1000 | Loss: 0.00003550
Iteration 55/1000 | Loss: 0.00003547
Iteration 56/1000 | Loss: 0.00019020
Iteration 57/1000 | Loss: 0.00003608
Iteration 58/1000 | Loss: 0.00003443
Iteration 59/1000 | Loss: 0.00003414
Iteration 60/1000 | Loss: 0.00003403
Iteration 61/1000 | Loss: 0.00003383
Iteration 62/1000 | Loss: 0.00003375
Iteration 63/1000 | Loss: 0.00003353
Iteration 64/1000 | Loss: 0.00003332
Iteration 65/1000 | Loss: 0.00003321
Iteration 66/1000 | Loss: 0.00003321
Iteration 67/1000 | Loss: 0.00003320
Iteration 68/1000 | Loss: 0.00003318
Iteration 69/1000 | Loss: 0.00003318
Iteration 70/1000 | Loss: 0.00003317
Iteration 71/1000 | Loss: 0.00003316
Iteration 72/1000 | Loss: 0.00003315
Iteration 73/1000 | Loss: 0.00003314
Iteration 74/1000 | Loss: 0.00003314
Iteration 75/1000 | Loss: 0.00003313
Iteration 76/1000 | Loss: 0.00003313
Iteration 77/1000 | Loss: 0.00003312
Iteration 78/1000 | Loss: 0.00003312
Iteration 79/1000 | Loss: 0.00003312
Iteration 80/1000 | Loss: 0.00003312
Iteration 81/1000 | Loss: 0.00003312
Iteration 82/1000 | Loss: 0.00003311
Iteration 83/1000 | Loss: 0.00003311
Iteration 84/1000 | Loss: 0.00003311
Iteration 85/1000 | Loss: 0.00003311
Iteration 86/1000 | Loss: 0.00003311
Iteration 87/1000 | Loss: 0.00003311
Iteration 88/1000 | Loss: 0.00003310
Iteration 89/1000 | Loss: 0.00003310
Iteration 90/1000 | Loss: 0.00003310
Iteration 91/1000 | Loss: 0.00003310
Iteration 92/1000 | Loss: 0.00003310
Iteration 93/1000 | Loss: 0.00003310
Iteration 94/1000 | Loss: 0.00003310
Iteration 95/1000 | Loss: 0.00003310
Iteration 96/1000 | Loss: 0.00003310
Iteration 97/1000 | Loss: 0.00003310
Iteration 98/1000 | Loss: 0.00003310
Iteration 99/1000 | Loss: 0.00003309
Iteration 100/1000 | Loss: 0.00003309
Iteration 101/1000 | Loss: 0.00003309
Iteration 102/1000 | Loss: 0.00003309
Iteration 103/1000 | Loss: 0.00003309
Iteration 104/1000 | Loss: 0.00003309
Iteration 105/1000 | Loss: 0.00003308
Iteration 106/1000 | Loss: 0.00003308
Iteration 107/1000 | Loss: 0.00003308
Iteration 108/1000 | Loss: 0.00003308
Iteration 109/1000 | Loss: 0.00003308
Iteration 110/1000 | Loss: 0.00003308
Iteration 111/1000 | Loss: 0.00003308
Iteration 112/1000 | Loss: 0.00003308
Iteration 113/1000 | Loss: 0.00003308
Iteration 114/1000 | Loss: 0.00003308
Iteration 115/1000 | Loss: 0.00003308
Iteration 116/1000 | Loss: 0.00003308
Iteration 117/1000 | Loss: 0.00003308
Iteration 118/1000 | Loss: 0.00003308
Iteration 119/1000 | Loss: 0.00003308
Iteration 120/1000 | Loss: 0.00003307
Iteration 121/1000 | Loss: 0.00003307
Iteration 122/1000 | Loss: 0.00003307
Iteration 123/1000 | Loss: 0.00003307
Iteration 124/1000 | Loss: 0.00003307
Iteration 125/1000 | Loss: 0.00003307
Iteration 126/1000 | Loss: 0.00003307
Iteration 127/1000 | Loss: 0.00003307
Iteration 128/1000 | Loss: 0.00003307
Iteration 129/1000 | Loss: 0.00003307
Iteration 130/1000 | Loss: 0.00003307
Iteration 131/1000 | Loss: 0.00003307
Iteration 132/1000 | Loss: 0.00003307
Iteration 133/1000 | Loss: 0.00003307
Iteration 134/1000 | Loss: 0.00003307
Iteration 135/1000 | Loss: 0.00003307
Iteration 136/1000 | Loss: 0.00003307
Iteration 137/1000 | Loss: 0.00003307
Iteration 138/1000 | Loss: 0.00003307
Iteration 139/1000 | Loss: 0.00003307
Iteration 140/1000 | Loss: 0.00003307
Iteration 141/1000 | Loss: 0.00003307
Iteration 142/1000 | Loss: 0.00003307
Iteration 143/1000 | Loss: 0.00003307
Iteration 144/1000 | Loss: 0.00003307
Iteration 145/1000 | Loss: 0.00003307
Iteration 146/1000 | Loss: 0.00003307
Iteration 147/1000 | Loss: 0.00003307
Iteration 148/1000 | Loss: 0.00003307
Iteration 149/1000 | Loss: 0.00003307
Iteration 150/1000 | Loss: 0.00003307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.306952930870466e-05, 3.306952930870466e-05, 3.306952930870466e-05, 3.306952930870466e-05, 3.306952930870466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.306952930870466e-05

Optimization complete. Final v2v error: 3.4666521549224854 mm

Highest mean error: 12.128900527954102 mm for frame 93

Lowest mean error: 2.2303168773651123 mm for frame 127

Saving results

Total time: 133.18263816833496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807064
Iteration 2/25 | Loss: 0.00132682
Iteration 3/25 | Loss: 0.00108944
Iteration 4/25 | Loss: 0.00104923
Iteration 5/25 | Loss: 0.00104261
Iteration 6/25 | Loss: 0.00104149
Iteration 7/25 | Loss: 0.00104149
Iteration 8/25 | Loss: 0.00104149
Iteration 9/25 | Loss: 0.00104149
Iteration 10/25 | Loss: 0.00104149
Iteration 11/25 | Loss: 0.00104149
Iteration 12/25 | Loss: 0.00104149
Iteration 13/25 | Loss: 0.00104149
Iteration 14/25 | Loss: 0.00104149
Iteration 15/25 | Loss: 0.00104149
Iteration 16/25 | Loss: 0.00104149
Iteration 17/25 | Loss: 0.00104149
Iteration 18/25 | Loss: 0.00104149
Iteration 19/25 | Loss: 0.00104149
Iteration 20/25 | Loss: 0.00104149
Iteration 21/25 | Loss: 0.00104149
Iteration 22/25 | Loss: 0.00104149
Iteration 23/25 | Loss: 0.00104149
Iteration 24/25 | Loss: 0.00104149
Iteration 25/25 | Loss: 0.00104149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22559595
Iteration 2/25 | Loss: 0.00142935
Iteration 3/25 | Loss: 0.00142935
Iteration 4/25 | Loss: 0.00142935
Iteration 5/25 | Loss: 0.00142935
Iteration 6/25 | Loss: 0.00142935
Iteration 7/25 | Loss: 0.00142935
Iteration 8/25 | Loss: 0.00142935
Iteration 9/25 | Loss: 0.00142935
Iteration 10/25 | Loss: 0.00142935
Iteration 11/25 | Loss: 0.00142935
Iteration 12/25 | Loss: 0.00142935
Iteration 13/25 | Loss: 0.00142935
Iteration 14/25 | Loss: 0.00142935
Iteration 15/25 | Loss: 0.00142935
Iteration 16/25 | Loss: 0.00142935
Iteration 17/25 | Loss: 0.00142935
Iteration 18/25 | Loss: 0.00142935
Iteration 19/25 | Loss: 0.00142935
Iteration 20/25 | Loss: 0.00142935
Iteration 21/25 | Loss: 0.00142935
Iteration 22/25 | Loss: 0.00142935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014293453423306346, 0.0014293453423306346, 0.0014293453423306346, 0.0014293453423306346, 0.0014293453423306346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014293453423306346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142935
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001511
Iteration 4/1000 | Loss: 0.00001363
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001197
Iteration 7/1000 | Loss: 0.00001168
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001122
Iteration 10/1000 | Loss: 0.00001111
Iteration 11/1000 | Loss: 0.00001104
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001095
Iteration 14/1000 | Loss: 0.00001088
Iteration 15/1000 | Loss: 0.00001086
Iteration 16/1000 | Loss: 0.00001085
Iteration 17/1000 | Loss: 0.00001084
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001073
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001073
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001067
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001066
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001065
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001065
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001065
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001064
Iteration 49/1000 | Loss: 0.00001064
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001064
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001063
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001062
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001061
Iteration 59/1000 | Loss: 0.00001061
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001060
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001059
Iteration 76/1000 | Loss: 0.00001059
Iteration 77/1000 | Loss: 0.00001059
Iteration 78/1000 | Loss: 0.00001059
Iteration 79/1000 | Loss: 0.00001058
Iteration 80/1000 | Loss: 0.00001058
Iteration 81/1000 | Loss: 0.00001058
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001056
Iteration 96/1000 | Loss: 0.00001056
Iteration 97/1000 | Loss: 0.00001056
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001055
Iteration 102/1000 | Loss: 0.00001055
Iteration 103/1000 | Loss: 0.00001055
Iteration 104/1000 | Loss: 0.00001055
Iteration 105/1000 | Loss: 0.00001055
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001055
Iteration 108/1000 | Loss: 0.00001055
Iteration 109/1000 | Loss: 0.00001055
Iteration 110/1000 | Loss: 0.00001055
Iteration 111/1000 | Loss: 0.00001055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.055162192642456e-05, 1.055162192642456e-05, 1.055162192642456e-05, 1.055162192642456e-05, 1.055162192642456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.055162192642456e-05

Optimization complete. Final v2v error: 2.7132625579833984 mm

Highest mean error: 3.172470808029175 mm for frame 189

Lowest mean error: 2.05930495262146 mm for frame 4

Saving results

Total time: 37.861958503723145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851412
Iteration 2/25 | Loss: 0.00112728
Iteration 3/25 | Loss: 0.00101766
Iteration 4/25 | Loss: 0.00100576
Iteration 5/25 | Loss: 0.00100359
Iteration 6/25 | Loss: 0.00100359
Iteration 7/25 | Loss: 0.00100359
Iteration 8/25 | Loss: 0.00100359
Iteration 9/25 | Loss: 0.00100359
Iteration 10/25 | Loss: 0.00100359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010035880841314793, 0.0010035880841314793, 0.0010035880841314793, 0.0010035880841314793, 0.0010035880841314793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010035880841314793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24345493
Iteration 2/25 | Loss: 0.00152252
Iteration 3/25 | Loss: 0.00152252
Iteration 4/25 | Loss: 0.00152252
Iteration 5/25 | Loss: 0.00152252
Iteration 6/25 | Loss: 0.00152252
Iteration 7/25 | Loss: 0.00152252
Iteration 8/25 | Loss: 0.00152252
Iteration 9/25 | Loss: 0.00152252
Iteration 10/25 | Loss: 0.00152252
Iteration 11/25 | Loss: 0.00152252
Iteration 12/25 | Loss: 0.00152252
Iteration 13/25 | Loss: 0.00152252
Iteration 14/25 | Loss: 0.00152252
Iteration 15/25 | Loss: 0.00152251
Iteration 16/25 | Loss: 0.00152252
Iteration 17/25 | Loss: 0.00152252
Iteration 18/25 | Loss: 0.00152251
Iteration 19/25 | Loss: 0.00152251
Iteration 20/25 | Loss: 0.00152251
Iteration 21/25 | Loss: 0.00152251
Iteration 22/25 | Loss: 0.00152251
Iteration 23/25 | Loss: 0.00152251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015225147362798452, 0.0015225147362798452, 0.0015225147362798452, 0.0015225147362798452, 0.0015225147362798452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015225147362798452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152251
Iteration 2/1000 | Loss: 0.00001680
Iteration 3/1000 | Loss: 0.00001189
Iteration 4/1000 | Loss: 0.00001064
Iteration 5/1000 | Loss: 0.00001011
Iteration 6/1000 | Loss: 0.00000962
Iteration 7/1000 | Loss: 0.00000938
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000893
Iteration 10/1000 | Loss: 0.00000884
Iteration 11/1000 | Loss: 0.00000884
Iteration 12/1000 | Loss: 0.00000877
Iteration 13/1000 | Loss: 0.00000876
Iteration 14/1000 | Loss: 0.00000876
Iteration 15/1000 | Loss: 0.00000876
Iteration 16/1000 | Loss: 0.00000873
Iteration 17/1000 | Loss: 0.00000871
Iteration 18/1000 | Loss: 0.00000871
Iteration 19/1000 | Loss: 0.00000870
Iteration 20/1000 | Loss: 0.00000870
Iteration 21/1000 | Loss: 0.00000870
Iteration 22/1000 | Loss: 0.00000870
Iteration 23/1000 | Loss: 0.00000869
Iteration 24/1000 | Loss: 0.00000869
Iteration 25/1000 | Loss: 0.00000869
Iteration 26/1000 | Loss: 0.00000868
Iteration 27/1000 | Loss: 0.00000868
Iteration 28/1000 | Loss: 0.00000868
Iteration 29/1000 | Loss: 0.00000867
Iteration 30/1000 | Loss: 0.00000867
Iteration 31/1000 | Loss: 0.00000867
Iteration 32/1000 | Loss: 0.00000867
Iteration 33/1000 | Loss: 0.00000867
Iteration 34/1000 | Loss: 0.00000866
Iteration 35/1000 | Loss: 0.00000866
Iteration 36/1000 | Loss: 0.00000866
Iteration 37/1000 | Loss: 0.00000866
Iteration 38/1000 | Loss: 0.00000866
Iteration 39/1000 | Loss: 0.00000866
Iteration 40/1000 | Loss: 0.00000865
Iteration 41/1000 | Loss: 0.00000865
Iteration 42/1000 | Loss: 0.00000865
Iteration 43/1000 | Loss: 0.00000865
Iteration 44/1000 | Loss: 0.00000865
Iteration 45/1000 | Loss: 0.00000864
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000862
Iteration 49/1000 | Loss: 0.00000862
Iteration 50/1000 | Loss: 0.00000862
Iteration 51/1000 | Loss: 0.00000862
Iteration 52/1000 | Loss: 0.00000861
Iteration 53/1000 | Loss: 0.00000861
Iteration 54/1000 | Loss: 0.00000861
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00000861
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000859
Iteration 60/1000 | Loss: 0.00000859
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000858
Iteration 64/1000 | Loss: 0.00000858
Iteration 65/1000 | Loss: 0.00000858
Iteration 66/1000 | Loss: 0.00000858
Iteration 67/1000 | Loss: 0.00000858
Iteration 68/1000 | Loss: 0.00000857
Iteration 69/1000 | Loss: 0.00000857
Iteration 70/1000 | Loss: 0.00000857
Iteration 71/1000 | Loss: 0.00000857
Iteration 72/1000 | Loss: 0.00000857
Iteration 73/1000 | Loss: 0.00000857
Iteration 74/1000 | Loss: 0.00000857
Iteration 75/1000 | Loss: 0.00000857
Iteration 76/1000 | Loss: 0.00000857
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000856
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000856
Iteration 82/1000 | Loss: 0.00000856
Iteration 83/1000 | Loss: 0.00000855
Iteration 84/1000 | Loss: 0.00000855
Iteration 85/1000 | Loss: 0.00000855
Iteration 86/1000 | Loss: 0.00000855
Iteration 87/1000 | Loss: 0.00000854
Iteration 88/1000 | Loss: 0.00000854
Iteration 89/1000 | Loss: 0.00000854
Iteration 90/1000 | Loss: 0.00000854
Iteration 91/1000 | Loss: 0.00000854
Iteration 92/1000 | Loss: 0.00000853
Iteration 93/1000 | Loss: 0.00000853
Iteration 94/1000 | Loss: 0.00000853
Iteration 95/1000 | Loss: 0.00000853
Iteration 96/1000 | Loss: 0.00000853
Iteration 97/1000 | Loss: 0.00000852
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000852
Iteration 102/1000 | Loss: 0.00000852
Iteration 103/1000 | Loss: 0.00000852
Iteration 104/1000 | Loss: 0.00000852
Iteration 105/1000 | Loss: 0.00000852
Iteration 106/1000 | Loss: 0.00000852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [8.519747098034713e-06, 8.519747098034713e-06, 8.519747098034713e-06, 8.519747098034713e-06, 8.519747098034713e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.519747098034713e-06

Optimization complete. Final v2v error: 2.4908130168914795 mm

Highest mean error: 2.78904128074646 mm for frame 219

Lowest mean error: 2.020569086074829 mm for frame 0

Saving results

Total time: 33.56385517120361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041611
Iteration 2/25 | Loss: 0.01041611
Iteration 3/25 | Loss: 0.00292795
Iteration 4/25 | Loss: 0.00238781
Iteration 5/25 | Loss: 0.00231651
Iteration 6/25 | Loss: 0.00220835
Iteration 7/25 | Loss: 0.00207834
Iteration 8/25 | Loss: 0.00185950
Iteration 9/25 | Loss: 0.00178589
Iteration 10/25 | Loss: 0.00173034
Iteration 11/25 | Loss: 0.00169279
Iteration 12/25 | Loss: 0.00167112
Iteration 13/25 | Loss: 0.00166856
Iteration 14/25 | Loss: 0.00166563
Iteration 15/25 | Loss: 0.00166349
Iteration 16/25 | Loss: 0.00166494
Iteration 17/25 | Loss: 0.00166120
Iteration 18/25 | Loss: 0.00165628
Iteration 19/25 | Loss: 0.00165190
Iteration 20/25 | Loss: 0.00164851
Iteration 21/25 | Loss: 0.00165117
Iteration 22/25 | Loss: 0.00165406
Iteration 23/25 | Loss: 0.00165244
Iteration 24/25 | Loss: 0.00164852
Iteration 25/25 | Loss: 0.00164674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21403599
Iteration 2/25 | Loss: 0.00471806
Iteration 3/25 | Loss: 0.00471806
Iteration 4/25 | Loss: 0.00471806
Iteration 5/25 | Loss: 0.00471806
Iteration 6/25 | Loss: 0.00471806
Iteration 7/25 | Loss: 0.00471806
Iteration 8/25 | Loss: 0.00471806
Iteration 9/25 | Loss: 0.00471806
Iteration 10/25 | Loss: 0.00471806
Iteration 11/25 | Loss: 0.00471806
Iteration 12/25 | Loss: 0.00471806
Iteration 13/25 | Loss: 0.00471806
Iteration 14/25 | Loss: 0.00471806
Iteration 15/25 | Loss: 0.00471806
Iteration 16/25 | Loss: 0.00471806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0047180564142763615, 0.0047180564142763615, 0.0047180564142763615, 0.0047180564142763615, 0.0047180564142763615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047180564142763615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00471806
Iteration 2/1000 | Loss: 0.00068257
Iteration 3/1000 | Loss: 0.00048254
Iteration 4/1000 | Loss: 0.00042042
Iteration 5/1000 | Loss: 0.00038923
Iteration 6/1000 | Loss: 0.00036407
Iteration 7/1000 | Loss: 0.00034139
Iteration 8/1000 | Loss: 0.00032331
Iteration 9/1000 | Loss: 0.00030957
Iteration 10/1000 | Loss: 0.00059531
Iteration 11/1000 | Loss: 0.00041813
Iteration 12/1000 | Loss: 0.00039447
Iteration 13/1000 | Loss: 0.00027886
Iteration 14/1000 | Loss: 0.00026882
Iteration 15/1000 | Loss: 0.00026199
Iteration 16/1000 | Loss: 0.00058872
Iteration 17/1000 | Loss: 0.00111791
Iteration 18/1000 | Loss: 0.01453644
Iteration 19/1000 | Loss: 0.00435807
Iteration 20/1000 | Loss: 0.00095424
Iteration 21/1000 | Loss: 0.00056955
Iteration 22/1000 | Loss: 0.00088033
Iteration 23/1000 | Loss: 0.00032273
Iteration 24/1000 | Loss: 0.00023007
Iteration 25/1000 | Loss: 0.00015589
Iteration 26/1000 | Loss: 0.00011468
Iteration 27/1000 | Loss: 0.00008090
Iteration 28/1000 | Loss: 0.00006282
Iteration 29/1000 | Loss: 0.00005136
Iteration 30/1000 | Loss: 0.00004394
Iteration 31/1000 | Loss: 0.00003913
Iteration 32/1000 | Loss: 0.00025286
Iteration 33/1000 | Loss: 0.00003437
Iteration 34/1000 | Loss: 0.00003038
Iteration 35/1000 | Loss: 0.00002673
Iteration 36/1000 | Loss: 0.00002436
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00001986
Iteration 39/1000 | Loss: 0.00001859
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001681
Iteration 47/1000 | Loss: 0.00001674
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001669
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001666
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001665
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001663
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001663
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001663
Iteration 75/1000 | Loss: 0.00001663
Iteration 76/1000 | Loss: 0.00001662
Iteration 77/1000 | Loss: 0.00001662
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001661
Iteration 80/1000 | Loss: 0.00001661
Iteration 81/1000 | Loss: 0.00001661
Iteration 82/1000 | Loss: 0.00001661
Iteration 83/1000 | Loss: 0.00001661
Iteration 84/1000 | Loss: 0.00001661
Iteration 85/1000 | Loss: 0.00001661
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001660
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001659
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001658
Iteration 105/1000 | Loss: 0.00001658
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001658
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.6576756024733186e-05, 1.6576756024733186e-05, 1.6576756024733186e-05, 1.6576756024733186e-05, 1.6576756024733186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6576756024733186e-05

Optimization complete. Final v2v error: 3.4375438690185547 mm

Highest mean error: 4.1187357902526855 mm for frame 112

Lowest mean error: 3.178941488265991 mm for frame 239

Saving results

Total time: 126.38847470283508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534136
Iteration 2/25 | Loss: 0.00133556
Iteration 3/25 | Loss: 0.00105233
Iteration 4/25 | Loss: 0.00103877
Iteration 5/25 | Loss: 0.00103720
Iteration 6/25 | Loss: 0.00103673
Iteration 7/25 | Loss: 0.00103673
Iteration 8/25 | Loss: 0.00103673
Iteration 9/25 | Loss: 0.00103673
Iteration 10/25 | Loss: 0.00103673
Iteration 11/25 | Loss: 0.00103673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001036729197949171, 0.001036729197949171, 0.001036729197949171, 0.001036729197949171, 0.001036729197949171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001036729197949171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69880682
Iteration 2/25 | Loss: 0.00106118
Iteration 3/25 | Loss: 0.00106118
Iteration 4/25 | Loss: 0.00106118
Iteration 5/25 | Loss: 0.00106118
Iteration 6/25 | Loss: 0.00106118
Iteration 7/25 | Loss: 0.00106118
Iteration 8/25 | Loss: 0.00106118
Iteration 9/25 | Loss: 0.00106118
Iteration 10/25 | Loss: 0.00106118
Iteration 11/25 | Loss: 0.00106117
Iteration 12/25 | Loss: 0.00106117
Iteration 13/25 | Loss: 0.00106117
Iteration 14/25 | Loss: 0.00106117
Iteration 15/25 | Loss: 0.00106117
Iteration 16/25 | Loss: 0.00106117
Iteration 17/25 | Loss: 0.00106117
Iteration 18/25 | Loss: 0.00106117
Iteration 19/25 | Loss: 0.00106117
Iteration 20/25 | Loss: 0.00106117
Iteration 21/25 | Loss: 0.00106117
Iteration 22/25 | Loss: 0.00106117
Iteration 23/25 | Loss: 0.00106117
Iteration 24/25 | Loss: 0.00106117
Iteration 25/25 | Loss: 0.00106117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106117
Iteration 2/1000 | Loss: 0.00002997
Iteration 3/1000 | Loss: 0.00002353
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00002043
Iteration 7/1000 | Loss: 0.00002009
Iteration 8/1000 | Loss: 0.00001974
Iteration 9/1000 | Loss: 0.00001949
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001807
Iteration 15/1000 | Loss: 0.00001792
Iteration 16/1000 | Loss: 0.00001776
Iteration 17/1000 | Loss: 0.00001754
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001717
Iteration 22/1000 | Loss: 0.00001717
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001691
Iteration 26/1000 | Loss: 0.00001683
Iteration 27/1000 | Loss: 0.00001681
Iteration 28/1000 | Loss: 0.00001680
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001680
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001675
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001673
Iteration 47/1000 | Loss: 0.00001673
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00001672
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001672
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001671
Iteration 57/1000 | Loss: 0.00001671
Iteration 58/1000 | Loss: 0.00001671
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001670
Iteration 63/1000 | Loss: 0.00001670
Iteration 64/1000 | Loss: 0.00001670
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001670
Iteration 67/1000 | Loss: 0.00001669
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001669
Iteration 70/1000 | Loss: 0.00001669
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001669
Iteration 74/1000 | Loss: 0.00001669
Iteration 75/1000 | Loss: 0.00001669
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001668
Iteration 80/1000 | Loss: 0.00001668
Iteration 81/1000 | Loss: 0.00001668
Iteration 82/1000 | Loss: 0.00001668
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001668
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001668
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001667
Iteration 95/1000 | Loss: 0.00001667
Iteration 96/1000 | Loss: 0.00001667
Iteration 97/1000 | Loss: 0.00001667
Iteration 98/1000 | Loss: 0.00001667
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001667
Iteration 106/1000 | Loss: 0.00001667
Iteration 107/1000 | Loss: 0.00001667
Iteration 108/1000 | Loss: 0.00001667
Iteration 109/1000 | Loss: 0.00001667
Iteration 110/1000 | Loss: 0.00001667
Iteration 111/1000 | Loss: 0.00001667
Iteration 112/1000 | Loss: 0.00001667
Iteration 113/1000 | Loss: 0.00001667
Iteration 114/1000 | Loss: 0.00001667
Iteration 115/1000 | Loss: 0.00001667
Iteration 116/1000 | Loss: 0.00001667
Iteration 117/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.666702337388415e-05, 1.666702337388415e-05, 1.666702337388415e-05, 1.666702337388415e-05, 1.666702337388415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.666702337388415e-05

Optimization complete. Final v2v error: 3.247990608215332 mm

Highest mean error: 3.6701979637145996 mm for frame 10

Lowest mean error: 2.97652530670166 mm for frame 47

Saving results

Total time: 45.92825889587402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086284
Iteration 2/25 | Loss: 0.00276859
Iteration 3/25 | Loss: 0.00225441
Iteration 4/25 | Loss: 0.00207108
Iteration 5/25 | Loss: 0.00174627
Iteration 6/25 | Loss: 0.00155720
Iteration 7/25 | Loss: 0.00144380
Iteration 8/25 | Loss: 0.00137267
Iteration 9/25 | Loss: 0.00131246
Iteration 10/25 | Loss: 0.00129495
Iteration 11/25 | Loss: 0.00128724
Iteration 12/25 | Loss: 0.00133040
Iteration 13/25 | Loss: 0.00123622
Iteration 14/25 | Loss: 0.00123464
Iteration 15/25 | Loss: 0.00120729
Iteration 16/25 | Loss: 0.00121405
Iteration 17/25 | Loss: 0.00122604
Iteration 18/25 | Loss: 0.00121975
Iteration 19/25 | Loss: 0.00116982
Iteration 20/25 | Loss: 0.00120093
Iteration 21/25 | Loss: 0.00113480
Iteration 22/25 | Loss: 0.00111774
Iteration 23/25 | Loss: 0.00111522
Iteration 24/25 | Loss: 0.00111624
Iteration 25/25 | Loss: 0.00112052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28805101
Iteration 2/25 | Loss: 0.00216606
Iteration 3/25 | Loss: 0.00216606
Iteration 4/25 | Loss: 0.00216606
Iteration 5/25 | Loss: 0.00216606
Iteration 6/25 | Loss: 0.00216606
Iteration 7/25 | Loss: 0.00216606
Iteration 8/25 | Loss: 0.00216606
Iteration 9/25 | Loss: 0.00216605
Iteration 10/25 | Loss: 0.00216605
Iteration 11/25 | Loss: 0.00216605
Iteration 12/25 | Loss: 0.00216605
Iteration 13/25 | Loss: 0.00216605
Iteration 14/25 | Loss: 0.00216605
Iteration 15/25 | Loss: 0.00216605
Iteration 16/25 | Loss: 0.00216605
Iteration 17/25 | Loss: 0.00216605
Iteration 18/25 | Loss: 0.00216605
Iteration 19/25 | Loss: 0.00216605
Iteration 20/25 | Loss: 0.00216605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002166054444387555, 0.002166054444387555, 0.002166054444387555, 0.002166054444387555, 0.002166054444387555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002166054444387555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216605
Iteration 2/1000 | Loss: 0.00057179
Iteration 3/1000 | Loss: 0.00128715
Iteration 4/1000 | Loss: 0.00071479
Iteration 5/1000 | Loss: 0.00059685
Iteration 6/1000 | Loss: 0.00102662
Iteration 7/1000 | Loss: 0.00075834
Iteration 8/1000 | Loss: 0.00098285
Iteration 9/1000 | Loss: 0.00112185
Iteration 10/1000 | Loss: 0.00101141
Iteration 11/1000 | Loss: 0.00067935
Iteration 12/1000 | Loss: 0.00129933
Iteration 13/1000 | Loss: 0.00046759
Iteration 14/1000 | Loss: 0.00025387
Iteration 15/1000 | Loss: 0.00013484
Iteration 16/1000 | Loss: 0.00024116
Iteration 17/1000 | Loss: 0.00024125
Iteration 18/1000 | Loss: 0.00017542
Iteration 19/1000 | Loss: 0.00024519
Iteration 20/1000 | Loss: 0.00030317
Iteration 21/1000 | Loss: 0.00028196
Iteration 22/1000 | Loss: 0.00039504
Iteration 23/1000 | Loss: 0.00031482
Iteration 24/1000 | Loss: 0.00018319
Iteration 25/1000 | Loss: 0.00022811
Iteration 26/1000 | Loss: 0.00095153
Iteration 27/1000 | Loss: 0.00028977
Iteration 28/1000 | Loss: 0.00031754
Iteration 29/1000 | Loss: 0.00040648
Iteration 30/1000 | Loss: 0.00045244
Iteration 31/1000 | Loss: 0.00043002
Iteration 32/1000 | Loss: 0.00039814
Iteration 33/1000 | Loss: 0.00068070
Iteration 34/1000 | Loss: 0.00035863
Iteration 35/1000 | Loss: 0.00040784
Iteration 36/1000 | Loss: 0.00050059
Iteration 37/1000 | Loss: 0.00036178
Iteration 38/1000 | Loss: 0.00104197
Iteration 39/1000 | Loss: 0.00074333
Iteration 40/1000 | Loss: 0.00036386
Iteration 41/1000 | Loss: 0.00051680
Iteration 42/1000 | Loss: 0.00017602
Iteration 43/1000 | Loss: 0.00064920
Iteration 44/1000 | Loss: 0.00034562
Iteration 45/1000 | Loss: 0.00081636
Iteration 46/1000 | Loss: 0.00042436
Iteration 47/1000 | Loss: 0.00039133
Iteration 48/1000 | Loss: 0.00091055
Iteration 49/1000 | Loss: 0.00022914
Iteration 50/1000 | Loss: 0.00072749
Iteration 51/1000 | Loss: 0.00011910
Iteration 52/1000 | Loss: 0.00027650
Iteration 53/1000 | Loss: 0.00020283
Iteration 54/1000 | Loss: 0.00018857
Iteration 55/1000 | Loss: 0.00018166
Iteration 56/1000 | Loss: 0.00019071
Iteration 57/1000 | Loss: 0.00021691
Iteration 58/1000 | Loss: 0.00023122
Iteration 59/1000 | Loss: 0.00044222
Iteration 60/1000 | Loss: 0.00006710
Iteration 61/1000 | Loss: 0.00063558
Iteration 62/1000 | Loss: 0.00099405
Iteration 63/1000 | Loss: 0.00037605
Iteration 64/1000 | Loss: 0.00018694
Iteration 65/1000 | Loss: 0.00009323
Iteration 66/1000 | Loss: 0.00009976
Iteration 67/1000 | Loss: 0.00010470
Iteration 68/1000 | Loss: 0.00008474
Iteration 69/1000 | Loss: 0.00011914
Iteration 70/1000 | Loss: 0.00011103
Iteration 71/1000 | Loss: 0.00009751
Iteration 72/1000 | Loss: 0.00063481
Iteration 73/1000 | Loss: 0.00017375
Iteration 74/1000 | Loss: 0.00009795
Iteration 75/1000 | Loss: 0.00017270
Iteration 76/1000 | Loss: 0.00023831
Iteration 77/1000 | Loss: 0.00035010
Iteration 78/1000 | Loss: 0.00033573
Iteration 79/1000 | Loss: 0.00027624
Iteration 80/1000 | Loss: 0.00020345
Iteration 81/1000 | Loss: 0.00034008
Iteration 82/1000 | Loss: 0.00031776
Iteration 83/1000 | Loss: 0.00025944
Iteration 84/1000 | Loss: 0.00021246
Iteration 85/1000 | Loss: 0.00007488
Iteration 86/1000 | Loss: 0.00066006
Iteration 87/1000 | Loss: 0.00154935
Iteration 88/1000 | Loss: 0.00046292
Iteration 89/1000 | Loss: 0.00045325
Iteration 90/1000 | Loss: 0.00066259
Iteration 91/1000 | Loss: 0.00054474
Iteration 92/1000 | Loss: 0.00153567
Iteration 93/1000 | Loss: 0.00063863
Iteration 94/1000 | Loss: 0.00058578
Iteration 95/1000 | Loss: 0.00009159
Iteration 96/1000 | Loss: 0.00014331
Iteration 97/1000 | Loss: 0.00013738
Iteration 98/1000 | Loss: 0.00005809
Iteration 99/1000 | Loss: 0.00005947
Iteration 100/1000 | Loss: 0.00013223
Iteration 101/1000 | Loss: 0.00057769
Iteration 102/1000 | Loss: 0.00015048
Iteration 103/1000 | Loss: 0.00010829
Iteration 104/1000 | Loss: 0.00008040
Iteration 105/1000 | Loss: 0.00028031
Iteration 106/1000 | Loss: 0.00013014
Iteration 107/1000 | Loss: 0.00008028
Iteration 108/1000 | Loss: 0.00034031
Iteration 109/1000 | Loss: 0.00026648
Iteration 110/1000 | Loss: 0.00018156
Iteration 111/1000 | Loss: 0.00008601
Iteration 112/1000 | Loss: 0.00009587
Iteration 113/1000 | Loss: 0.00010320
Iteration 114/1000 | Loss: 0.00014039
Iteration 115/1000 | Loss: 0.00011454
Iteration 116/1000 | Loss: 0.00015674
Iteration 117/1000 | Loss: 0.00009398
Iteration 118/1000 | Loss: 0.00013019
Iteration 119/1000 | Loss: 0.00009461
Iteration 120/1000 | Loss: 0.00011663
Iteration 121/1000 | Loss: 0.00015733
Iteration 122/1000 | Loss: 0.00013105
Iteration 123/1000 | Loss: 0.00015671
Iteration 124/1000 | Loss: 0.00010363
Iteration 125/1000 | Loss: 0.00010072
Iteration 126/1000 | Loss: 0.00023046
Iteration 127/1000 | Loss: 0.00015602
Iteration 128/1000 | Loss: 0.00016156
Iteration 129/1000 | Loss: 0.00011605
Iteration 130/1000 | Loss: 0.00010399
Iteration 131/1000 | Loss: 0.00026063
Iteration 132/1000 | Loss: 0.00009339
Iteration 133/1000 | Loss: 0.00022158
Iteration 134/1000 | Loss: 0.00012165
Iteration 135/1000 | Loss: 0.00010531
Iteration 136/1000 | Loss: 0.00023219
Iteration 137/1000 | Loss: 0.00015258
Iteration 138/1000 | Loss: 0.00022583
Iteration 139/1000 | Loss: 0.00015945
Iteration 140/1000 | Loss: 0.00010130
Iteration 141/1000 | Loss: 0.00008640
Iteration 142/1000 | Loss: 0.00009198
Iteration 143/1000 | Loss: 0.00028640
Iteration 144/1000 | Loss: 0.00074331
Iteration 145/1000 | Loss: 0.00014463
Iteration 146/1000 | Loss: 0.00006208
Iteration 147/1000 | Loss: 0.00015044
Iteration 148/1000 | Loss: 0.00010081
Iteration 149/1000 | Loss: 0.00009019
Iteration 150/1000 | Loss: 0.00019281
Iteration 151/1000 | Loss: 0.00084297
Iteration 152/1000 | Loss: 0.00148718
Iteration 153/1000 | Loss: 0.00138962
Iteration 154/1000 | Loss: 0.00164017
Iteration 155/1000 | Loss: 0.00053280
Iteration 156/1000 | Loss: 0.00016069
Iteration 157/1000 | Loss: 0.00008114
Iteration 158/1000 | Loss: 0.00008744
Iteration 159/1000 | Loss: 0.00005137
Iteration 160/1000 | Loss: 0.00006783
Iteration 161/1000 | Loss: 0.00068193
Iteration 162/1000 | Loss: 0.00050247
Iteration 163/1000 | Loss: 0.00007730
Iteration 164/1000 | Loss: 0.00006970
Iteration 165/1000 | Loss: 0.00006991
Iteration 166/1000 | Loss: 0.00008448
Iteration 167/1000 | Loss: 0.00046386
Iteration 168/1000 | Loss: 0.00050202
Iteration 169/1000 | Loss: 0.00008437
Iteration 170/1000 | Loss: 0.00009224
Iteration 171/1000 | Loss: 0.00061048
Iteration 172/1000 | Loss: 0.00251707
Iteration 173/1000 | Loss: 0.00016907
Iteration 174/1000 | Loss: 0.00039593
Iteration 175/1000 | Loss: 0.00022427
Iteration 176/1000 | Loss: 0.00022142
Iteration 177/1000 | Loss: 0.00010611
Iteration 178/1000 | Loss: 0.00008703
Iteration 179/1000 | Loss: 0.00032189
Iteration 180/1000 | Loss: 0.00038174
Iteration 181/1000 | Loss: 0.00007294
Iteration 182/1000 | Loss: 0.00004939
Iteration 183/1000 | Loss: 0.00052122
Iteration 184/1000 | Loss: 0.00005248
Iteration 185/1000 | Loss: 0.00006583
Iteration 186/1000 | Loss: 0.00005250
Iteration 187/1000 | Loss: 0.00031293
Iteration 188/1000 | Loss: 0.00004322
Iteration 189/1000 | Loss: 0.00005297
Iteration 190/1000 | Loss: 0.00035215
Iteration 191/1000 | Loss: 0.00010202
Iteration 192/1000 | Loss: 0.00018366
Iteration 193/1000 | Loss: 0.00009167
Iteration 194/1000 | Loss: 0.00005293
Iteration 195/1000 | Loss: 0.00055021
Iteration 196/1000 | Loss: 0.00008321
Iteration 197/1000 | Loss: 0.00013668
Iteration 198/1000 | Loss: 0.00003988
Iteration 199/1000 | Loss: 0.00006362
Iteration 200/1000 | Loss: 0.00004574
Iteration 201/1000 | Loss: 0.00005407
Iteration 202/1000 | Loss: 0.00004077
Iteration 203/1000 | Loss: 0.00005312
Iteration 204/1000 | Loss: 0.00006544
Iteration 205/1000 | Loss: 0.00006424
Iteration 206/1000 | Loss: 0.00046505
Iteration 207/1000 | Loss: 0.00008647
Iteration 208/1000 | Loss: 0.00005837
Iteration 209/1000 | Loss: 0.00014055
Iteration 210/1000 | Loss: 0.00007474
Iteration 211/1000 | Loss: 0.00011518
Iteration 212/1000 | Loss: 0.00007716
Iteration 213/1000 | Loss: 0.00005628
Iteration 214/1000 | Loss: 0.00009677
Iteration 215/1000 | Loss: 0.00005048
Iteration 216/1000 | Loss: 0.00005861
Iteration 217/1000 | Loss: 0.00008303
Iteration 218/1000 | Loss: 0.00005488
Iteration 219/1000 | Loss: 0.00002672
Iteration 220/1000 | Loss: 0.00003833
Iteration 221/1000 | Loss: 0.00006570
Iteration 222/1000 | Loss: 0.00005214
Iteration 223/1000 | Loss: 0.00002854
Iteration 224/1000 | Loss: 0.00005255
Iteration 225/1000 | Loss: 0.00005853
Iteration 226/1000 | Loss: 0.00006875
Iteration 227/1000 | Loss: 0.00004522
Iteration 228/1000 | Loss: 0.00006615
Iteration 229/1000 | Loss: 0.00014363
Iteration 230/1000 | Loss: 0.00007048
Iteration 231/1000 | Loss: 0.00003852
Iteration 232/1000 | Loss: 0.00003329
Iteration 233/1000 | Loss: 0.00004940
Iteration 234/1000 | Loss: 0.00011079
Iteration 235/1000 | Loss: 0.00007065
Iteration 236/1000 | Loss: 0.00005897
Iteration 237/1000 | Loss: 0.00005816
Iteration 238/1000 | Loss: 0.00004052
Iteration 239/1000 | Loss: 0.00003367
Iteration 240/1000 | Loss: 0.00005469
Iteration 241/1000 | Loss: 0.00002634
Iteration 242/1000 | Loss: 0.00005998
Iteration 243/1000 | Loss: 0.00005358
Iteration 244/1000 | Loss: 0.00006461
Iteration 245/1000 | Loss: 0.00005508
Iteration 246/1000 | Loss: 0.00004347
Iteration 247/1000 | Loss: 0.00005208
Iteration 248/1000 | Loss: 0.00005849
Iteration 249/1000 | Loss: 0.00006040
Iteration 250/1000 | Loss: 0.00005521
Iteration 251/1000 | Loss: 0.00003681
Iteration 252/1000 | Loss: 0.00004436
Iteration 253/1000 | Loss: 0.00002356
Iteration 254/1000 | Loss: 0.00002177
Iteration 255/1000 | Loss: 0.00002947
Iteration 256/1000 | Loss: 0.00043711
Iteration 257/1000 | Loss: 0.00045849
Iteration 258/1000 | Loss: 0.00046417
Iteration 259/1000 | Loss: 0.00006118
Iteration 260/1000 | Loss: 0.00005508
Iteration 261/1000 | Loss: 0.00005433
Iteration 262/1000 | Loss: 0.00004402
Iteration 263/1000 | Loss: 0.00005362
Iteration 264/1000 | Loss: 0.00003789
Iteration 265/1000 | Loss: 0.00004699
Iteration 266/1000 | Loss: 0.00003394
Iteration 267/1000 | Loss: 0.00004368
Iteration 268/1000 | Loss: 0.00002837
Iteration 269/1000 | Loss: 0.00003651
Iteration 270/1000 | Loss: 0.00003884
Iteration 271/1000 | Loss: 0.00003012
Iteration 272/1000 | Loss: 0.00002179
Iteration 273/1000 | Loss: 0.00003235
Iteration 274/1000 | Loss: 0.00004087
Iteration 275/1000 | Loss: 0.00004313
Iteration 276/1000 | Loss: 0.00003479
Iteration 277/1000 | Loss: 0.00005116
Iteration 278/1000 | Loss: 0.00003348
Iteration 279/1000 | Loss: 0.00005032
Iteration 280/1000 | Loss: 0.00003441
Iteration 281/1000 | Loss: 0.00005002
Iteration 282/1000 | Loss: 0.00003361
Iteration 283/1000 | Loss: 0.00004698
Iteration 284/1000 | Loss: 0.00003232
Iteration 285/1000 | Loss: 0.00004506
Iteration 286/1000 | Loss: 0.00003391
Iteration 287/1000 | Loss: 0.00004539
Iteration 288/1000 | Loss: 0.00003308
Iteration 289/1000 | Loss: 0.00004374
Iteration 290/1000 | Loss: 0.00005138
Iteration 291/1000 | Loss: 0.00005137
Iteration 292/1000 | Loss: 0.00004762
Iteration 293/1000 | Loss: 0.00004840
Iteration 294/1000 | Loss: 0.00004222
Iteration 295/1000 | Loss: 0.00003277
Iteration 296/1000 | Loss: 0.00003810
Iteration 297/1000 | Loss: 0.00002986
Iteration 298/1000 | Loss: 0.00004317
Iteration 299/1000 | Loss: 0.00003165
Iteration 300/1000 | Loss: 0.00003836
Iteration 301/1000 | Loss: 0.00002696
Iteration 302/1000 | Loss: 0.00003031
Iteration 303/1000 | Loss: 0.00002505
Iteration 304/1000 | Loss: 0.00004111
Iteration 305/1000 | Loss: 0.00003402
Iteration 306/1000 | Loss: 0.00002139
Iteration 307/1000 | Loss: 0.00003781
Iteration 308/1000 | Loss: 0.00003543
Iteration 309/1000 | Loss: 0.00004278
Iteration 310/1000 | Loss: 0.00003591
Iteration 311/1000 | Loss: 0.00005736
Iteration 312/1000 | Loss: 0.00002428
Iteration 313/1000 | Loss: 0.00002171
Iteration 314/1000 | Loss: 0.00001999
Iteration 315/1000 | Loss: 0.00025972
Iteration 316/1000 | Loss: 0.00001906
Iteration 317/1000 | Loss: 0.00012707
Iteration 318/1000 | Loss: 0.00002273
Iteration 319/1000 | Loss: 0.00005659
Iteration 320/1000 | Loss: 0.00001798
Iteration 321/1000 | Loss: 0.00006892
Iteration 322/1000 | Loss: 0.00002161
Iteration 323/1000 | Loss: 0.00010064
Iteration 324/1000 | Loss: 0.00001736
Iteration 325/1000 | Loss: 0.00001692
Iteration 326/1000 | Loss: 0.00001656
Iteration 327/1000 | Loss: 0.00001642
Iteration 328/1000 | Loss: 0.00001640
Iteration 329/1000 | Loss: 0.00001639
Iteration 330/1000 | Loss: 0.00001639
Iteration 331/1000 | Loss: 0.00001638
Iteration 332/1000 | Loss: 0.00001638
Iteration 333/1000 | Loss: 0.00001638
Iteration 334/1000 | Loss: 0.00001638
Iteration 335/1000 | Loss: 0.00001638
Iteration 336/1000 | Loss: 0.00001638
Iteration 337/1000 | Loss: 0.00001638
Iteration 338/1000 | Loss: 0.00001638
Iteration 339/1000 | Loss: 0.00001638
Iteration 340/1000 | Loss: 0.00001638
Iteration 341/1000 | Loss: 0.00001638
Iteration 342/1000 | Loss: 0.00001637
Iteration 343/1000 | Loss: 0.00001637
Iteration 344/1000 | Loss: 0.00001636
Iteration 345/1000 | Loss: 0.00001636
Iteration 346/1000 | Loss: 0.00001635
Iteration 347/1000 | Loss: 0.00001635
Iteration 348/1000 | Loss: 0.00001635
Iteration 349/1000 | Loss: 0.00001634
Iteration 350/1000 | Loss: 0.00001634
Iteration 351/1000 | Loss: 0.00001634
Iteration 352/1000 | Loss: 0.00001634
Iteration 353/1000 | Loss: 0.00001634
Iteration 354/1000 | Loss: 0.00001634
Iteration 355/1000 | Loss: 0.00001634
Iteration 356/1000 | Loss: 0.00001633
Iteration 357/1000 | Loss: 0.00001632
Iteration 358/1000 | Loss: 0.00001632
Iteration 359/1000 | Loss: 0.00001632
Iteration 360/1000 | Loss: 0.00001632
Iteration 361/1000 | Loss: 0.00001632
Iteration 362/1000 | Loss: 0.00001632
Iteration 363/1000 | Loss: 0.00001632
Iteration 364/1000 | Loss: 0.00001631
Iteration 365/1000 | Loss: 0.00001631
Iteration 366/1000 | Loss: 0.00001629
Iteration 367/1000 | Loss: 0.00001629
Iteration 368/1000 | Loss: 0.00001629
Iteration 369/1000 | Loss: 0.00001628
Iteration 370/1000 | Loss: 0.00001626
Iteration 371/1000 | Loss: 0.00001624
Iteration 372/1000 | Loss: 0.00001616
Iteration 373/1000 | Loss: 0.00001616
Iteration 374/1000 | Loss: 0.00001616
Iteration 375/1000 | Loss: 0.00001616
Iteration 376/1000 | Loss: 0.00001616
Iteration 377/1000 | Loss: 0.00001616
Iteration 378/1000 | Loss: 0.00001616
Iteration 379/1000 | Loss: 0.00001616
Iteration 380/1000 | Loss: 0.00001616
Iteration 381/1000 | Loss: 0.00001615
Iteration 382/1000 | Loss: 0.00001614
Iteration 383/1000 | Loss: 0.00001613
Iteration 384/1000 | Loss: 0.00001612
Iteration 385/1000 | Loss: 0.00001611
Iteration 386/1000 | Loss: 0.00001611
Iteration 387/1000 | Loss: 0.00001610
Iteration 388/1000 | Loss: 0.00001606
Iteration 389/1000 | Loss: 0.00001604
Iteration 390/1000 | Loss: 0.00001603
Iteration 391/1000 | Loss: 0.00001603
Iteration 392/1000 | Loss: 0.00001603
Iteration 393/1000 | Loss: 0.00001602
Iteration 394/1000 | Loss: 0.00001602
Iteration 395/1000 | Loss: 0.00001602
Iteration 396/1000 | Loss: 0.00001602
Iteration 397/1000 | Loss: 0.00001601
Iteration 398/1000 | Loss: 0.00001601
Iteration 399/1000 | Loss: 0.00001601
Iteration 400/1000 | Loss: 0.00001601
Iteration 401/1000 | Loss: 0.00001601
Iteration 402/1000 | Loss: 0.00001601
Iteration 403/1000 | Loss: 0.00001601
Iteration 404/1000 | Loss: 0.00001601
Iteration 405/1000 | Loss: 0.00001601
Iteration 406/1000 | Loss: 0.00001601
Iteration 407/1000 | Loss: 0.00001600
Iteration 408/1000 | Loss: 0.00001600
Iteration 409/1000 | Loss: 0.00001600
Iteration 410/1000 | Loss: 0.00001600
Iteration 411/1000 | Loss: 0.00001599
Iteration 412/1000 | Loss: 0.00001599
Iteration 413/1000 | Loss: 0.00001599
Iteration 414/1000 | Loss: 0.00001599
Iteration 415/1000 | Loss: 0.00001599
Iteration 416/1000 | Loss: 0.00001599
Iteration 417/1000 | Loss: 0.00001599
Iteration 418/1000 | Loss: 0.00001599
Iteration 419/1000 | Loss: 0.00001598
Iteration 420/1000 | Loss: 0.00001598
Iteration 421/1000 | Loss: 0.00001598
Iteration 422/1000 | Loss: 0.00001598
Iteration 423/1000 | Loss: 0.00001597
Iteration 424/1000 | Loss: 0.00001597
Iteration 425/1000 | Loss: 0.00001597
Iteration 426/1000 | Loss: 0.00001597
Iteration 427/1000 | Loss: 0.00001597
Iteration 428/1000 | Loss: 0.00001597
Iteration 429/1000 | Loss: 0.00001597
Iteration 430/1000 | Loss: 0.00001597
Iteration 431/1000 | Loss: 0.00001597
Iteration 432/1000 | Loss: 0.00001597
Iteration 433/1000 | Loss: 0.00001596
Iteration 434/1000 | Loss: 0.00001596
Iteration 435/1000 | Loss: 0.00001596
Iteration 436/1000 | Loss: 0.00001596
Iteration 437/1000 | Loss: 0.00001596
Iteration 438/1000 | Loss: 0.00001596
Iteration 439/1000 | Loss: 0.00001596
Iteration 440/1000 | Loss: 0.00001596
Iteration 441/1000 | Loss: 0.00001596
Iteration 442/1000 | Loss: 0.00001596
Iteration 443/1000 | Loss: 0.00001596
Iteration 444/1000 | Loss: 0.00001596
Iteration 445/1000 | Loss: 0.00001596
Iteration 446/1000 | Loss: 0.00001596
Iteration 447/1000 | Loss: 0.00001596
Iteration 448/1000 | Loss: 0.00001596
Iteration 449/1000 | Loss: 0.00001596
Iteration 450/1000 | Loss: 0.00001596
Iteration 451/1000 | Loss: 0.00001596
Iteration 452/1000 | Loss: 0.00001596
Iteration 453/1000 | Loss: 0.00001596
Iteration 454/1000 | Loss: 0.00001595
Iteration 455/1000 | Loss: 0.00001595
Iteration 456/1000 | Loss: 0.00001595
Iteration 457/1000 | Loss: 0.00001595
Iteration 458/1000 | Loss: 0.00001595
Iteration 459/1000 | Loss: 0.00001595
Iteration 460/1000 | Loss: 0.00001595
Iteration 461/1000 | Loss: 0.00001595
Iteration 462/1000 | Loss: 0.00001595
Iteration 463/1000 | Loss: 0.00001595
Iteration 464/1000 | Loss: 0.00001595
Iteration 465/1000 | Loss: 0.00001595
Iteration 466/1000 | Loss: 0.00001594
Iteration 467/1000 | Loss: 0.00001594
Iteration 468/1000 | Loss: 0.00001594
Iteration 469/1000 | Loss: 0.00001594
Iteration 470/1000 | Loss: 0.00001594
Iteration 471/1000 | Loss: 0.00001594
Iteration 472/1000 | Loss: 0.00001594
Iteration 473/1000 | Loss: 0.00001594
Iteration 474/1000 | Loss: 0.00001594
Iteration 475/1000 | Loss: 0.00001594
Iteration 476/1000 | Loss: 0.00001594
Iteration 477/1000 | Loss: 0.00001594
Iteration 478/1000 | Loss: 0.00001594
Iteration 479/1000 | Loss: 0.00001594
Iteration 480/1000 | Loss: 0.00001594
Iteration 481/1000 | Loss: 0.00001594
Iteration 482/1000 | Loss: 0.00001594
Iteration 483/1000 | Loss: 0.00001593
Iteration 484/1000 | Loss: 0.00001593
Iteration 485/1000 | Loss: 0.00001593
Iteration 486/1000 | Loss: 0.00001593
Iteration 487/1000 | Loss: 0.00001593
Iteration 488/1000 | Loss: 0.00001593
Iteration 489/1000 | Loss: 0.00001593
Iteration 490/1000 | Loss: 0.00001593
Iteration 491/1000 | Loss: 0.00001593
Iteration 492/1000 | Loss: 0.00001593
Iteration 493/1000 | Loss: 0.00001593
Iteration 494/1000 | Loss: 0.00001593
Iteration 495/1000 | Loss: 0.00001593
Iteration 496/1000 | Loss: 0.00001593
Iteration 497/1000 | Loss: 0.00001593
Iteration 498/1000 | Loss: 0.00001593
Iteration 499/1000 | Loss: 0.00001593
Iteration 500/1000 | Loss: 0.00001593
Iteration 501/1000 | Loss: 0.00001593
Iteration 502/1000 | Loss: 0.00001593
Iteration 503/1000 | Loss: 0.00001592
Iteration 504/1000 | Loss: 0.00001592
Iteration 505/1000 | Loss: 0.00001592
Iteration 506/1000 | Loss: 0.00001592
Iteration 507/1000 | Loss: 0.00001592
Iteration 508/1000 | Loss: 0.00001592
Iteration 509/1000 | Loss: 0.00001592
Iteration 510/1000 | Loss: 0.00001592
Iteration 511/1000 | Loss: 0.00001592
Iteration 512/1000 | Loss: 0.00001592
Iteration 513/1000 | Loss: 0.00001592
Iteration 514/1000 | Loss: 0.00001592
Iteration 515/1000 | Loss: 0.00001592
Iteration 516/1000 | Loss: 0.00001592
Iteration 517/1000 | Loss: 0.00001592
Iteration 518/1000 | Loss: 0.00001592
Iteration 519/1000 | Loss: 0.00001592
Iteration 520/1000 | Loss: 0.00001592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 520. Stopping optimization.
Last 5 losses: [1.5923464161460288e-05, 1.5923464161460288e-05, 1.5923464161460288e-05, 1.5923464161460288e-05, 1.5923464161460288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5923464161460288e-05

Optimization complete. Final v2v error: 3.228872299194336 mm

Highest mean error: 4.080484867095947 mm for frame 110

Lowest mean error: 2.5855653285980225 mm for frame 122

Saving results

Total time: 515.611659526825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805581
Iteration 2/25 | Loss: 0.00133366
Iteration 3/25 | Loss: 0.00114662
Iteration 4/25 | Loss: 0.00111461
Iteration 5/25 | Loss: 0.00109839
Iteration 6/25 | Loss: 0.00110553
Iteration 7/25 | Loss: 0.00110047
Iteration 8/25 | Loss: 0.00109189
Iteration 9/25 | Loss: 0.00108719
Iteration 10/25 | Loss: 0.00108470
Iteration 11/25 | Loss: 0.00108372
Iteration 12/25 | Loss: 0.00108332
Iteration 13/25 | Loss: 0.00108292
Iteration 14/25 | Loss: 0.00108249
Iteration 15/25 | Loss: 0.00108159
Iteration 16/25 | Loss: 0.00108125
Iteration 17/25 | Loss: 0.00108114
Iteration 18/25 | Loss: 0.00108110
Iteration 19/25 | Loss: 0.00108110
Iteration 20/25 | Loss: 0.00108110
Iteration 21/25 | Loss: 0.00108109
Iteration 22/25 | Loss: 0.00108109
Iteration 23/25 | Loss: 0.00108109
Iteration 24/25 | Loss: 0.00108109
Iteration 25/25 | Loss: 0.00108109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53571093
Iteration 2/25 | Loss: 0.00147705
Iteration 3/25 | Loss: 0.00147705
Iteration 4/25 | Loss: 0.00147704
Iteration 5/25 | Loss: 0.00147704
Iteration 6/25 | Loss: 0.00147704
Iteration 7/25 | Loss: 0.00147704
Iteration 8/25 | Loss: 0.00147704
Iteration 9/25 | Loss: 0.00147704
Iteration 10/25 | Loss: 0.00147704
Iteration 11/25 | Loss: 0.00147704
Iteration 12/25 | Loss: 0.00147704
Iteration 13/25 | Loss: 0.00147704
Iteration 14/25 | Loss: 0.00147704
Iteration 15/25 | Loss: 0.00147704
Iteration 16/25 | Loss: 0.00147704
Iteration 17/25 | Loss: 0.00147704
Iteration 18/25 | Loss: 0.00147704
Iteration 19/25 | Loss: 0.00147704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014770409325137734, 0.0014770409325137734, 0.0014770409325137734, 0.0014770409325137734, 0.0014770409325137734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014770409325137734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147704
Iteration 2/1000 | Loss: 0.00002305
Iteration 3/1000 | Loss: 0.00001514
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001275
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001221
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001212
Iteration 67/1000 | Loss: 0.00001212
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.201404938910855e-05, 1.201404938910855e-05, 1.201404938910855e-05, 1.201404938910855e-05, 1.201404938910855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.201404938910855e-05

Optimization complete. Final v2v error: 2.820993185043335 mm

Highest mean error: 3.704798460006714 mm for frame 44

Lowest mean error: 2.323521852493286 mm for frame 164

Saving results

Total time: 52.274399280548096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422902
Iteration 2/25 | Loss: 0.00122866
Iteration 3/25 | Loss: 0.00105871
Iteration 4/25 | Loss: 0.00103730
Iteration 5/25 | Loss: 0.00103214
Iteration 6/25 | Loss: 0.00103091
Iteration 7/25 | Loss: 0.00103091
Iteration 8/25 | Loss: 0.00103091
Iteration 9/25 | Loss: 0.00103091
Iteration 10/25 | Loss: 0.00103091
Iteration 11/25 | Loss: 0.00103091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010309070348739624, 0.0010309070348739624, 0.0010309070348739624, 0.0010309070348739624, 0.0010309070348739624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010309070348739624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.22719955
Iteration 2/25 | Loss: 0.00133940
Iteration 3/25 | Loss: 0.00133937
Iteration 4/25 | Loss: 0.00133937
Iteration 5/25 | Loss: 0.00133937
Iteration 6/25 | Loss: 0.00133937
Iteration 7/25 | Loss: 0.00133937
Iteration 8/25 | Loss: 0.00133937
Iteration 9/25 | Loss: 0.00133937
Iteration 10/25 | Loss: 0.00133937
Iteration 11/25 | Loss: 0.00133937
Iteration 12/25 | Loss: 0.00133936
Iteration 13/25 | Loss: 0.00133936
Iteration 14/25 | Loss: 0.00133936
Iteration 15/25 | Loss: 0.00133936
Iteration 16/25 | Loss: 0.00133936
Iteration 17/25 | Loss: 0.00133936
Iteration 18/25 | Loss: 0.00133936
Iteration 19/25 | Loss: 0.00133936
Iteration 20/25 | Loss: 0.00133936
Iteration 21/25 | Loss: 0.00133936
Iteration 22/25 | Loss: 0.00133936
Iteration 23/25 | Loss: 0.00133936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013393644476309419, 0.0013393644476309419, 0.0013393644476309419, 0.0013393644476309419, 0.0013393644476309419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013393644476309419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133936
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001712
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001399
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001370
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001361
Iteration 26/1000 | Loss: 0.00001361
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001360
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001359
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001358
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001357
Iteration 39/1000 | Loss: 0.00001357
Iteration 40/1000 | Loss: 0.00001357
Iteration 41/1000 | Loss: 0.00001357
Iteration 42/1000 | Loss: 0.00001357
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001356
Iteration 45/1000 | Loss: 0.00001356
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001351
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001351
Iteration 76/1000 | Loss: 0.00001351
Iteration 77/1000 | Loss: 0.00001351
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001350
Iteration 81/1000 | Loss: 0.00001350
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001349
Iteration 104/1000 | Loss: 0.00001349
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001348
Iteration 108/1000 | Loss: 0.00001348
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001348
Iteration 121/1000 | Loss: 0.00001348
Iteration 122/1000 | Loss: 0.00001348
Iteration 123/1000 | Loss: 0.00001348
Iteration 124/1000 | Loss: 0.00001348
Iteration 125/1000 | Loss: 0.00001348
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001347
Iteration 130/1000 | Loss: 0.00001347
Iteration 131/1000 | Loss: 0.00001347
Iteration 132/1000 | Loss: 0.00001347
Iteration 133/1000 | Loss: 0.00001347
Iteration 134/1000 | Loss: 0.00001347
Iteration 135/1000 | Loss: 0.00001347
Iteration 136/1000 | Loss: 0.00001347
Iteration 137/1000 | Loss: 0.00001347
Iteration 138/1000 | Loss: 0.00001347
Iteration 139/1000 | Loss: 0.00001347
Iteration 140/1000 | Loss: 0.00001347
Iteration 141/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3474474144459236e-05, 1.3474474144459236e-05, 1.3474474144459236e-05, 1.3474474144459236e-05, 1.3474474144459236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3474474144459236e-05

Optimization complete. Final v2v error: 3.0257840156555176 mm

Highest mean error: 3.722532272338867 mm for frame 142

Lowest mean error: 2.5809099674224854 mm for frame 177

Saving results

Total time: 33.58210897445679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422240
Iteration 2/25 | Loss: 0.00119073
Iteration 3/25 | Loss: 0.00111109
Iteration 4/25 | Loss: 0.00109673
Iteration 5/25 | Loss: 0.00109207
Iteration 6/25 | Loss: 0.00109171
Iteration 7/25 | Loss: 0.00109171
Iteration 8/25 | Loss: 0.00109171
Iteration 9/25 | Loss: 0.00109171
Iteration 10/25 | Loss: 0.00109171
Iteration 11/25 | Loss: 0.00109171
Iteration 12/25 | Loss: 0.00109171
Iteration 13/25 | Loss: 0.00109171
Iteration 14/25 | Loss: 0.00109171
Iteration 15/25 | Loss: 0.00109171
Iteration 16/25 | Loss: 0.00109171
Iteration 17/25 | Loss: 0.00109171
Iteration 18/25 | Loss: 0.00109171
Iteration 19/25 | Loss: 0.00109171
Iteration 20/25 | Loss: 0.00109171
Iteration 21/25 | Loss: 0.00109171
Iteration 22/25 | Loss: 0.00109171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001091709011234343, 0.001091709011234343, 0.001091709011234343, 0.001091709011234343, 0.001091709011234343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001091709011234343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43501735
Iteration 2/25 | Loss: 0.00166578
Iteration 3/25 | Loss: 0.00166577
Iteration 4/25 | Loss: 0.00166577
Iteration 5/25 | Loss: 0.00166577
Iteration 6/25 | Loss: 0.00166577
Iteration 7/25 | Loss: 0.00166577
Iteration 8/25 | Loss: 0.00166577
Iteration 9/25 | Loss: 0.00166577
Iteration 10/25 | Loss: 0.00166577
Iteration 11/25 | Loss: 0.00166577
Iteration 12/25 | Loss: 0.00166577
Iteration 13/25 | Loss: 0.00166577
Iteration 14/25 | Loss: 0.00166577
Iteration 15/25 | Loss: 0.00166577
Iteration 16/25 | Loss: 0.00166577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016657718224450946, 0.0016657718224450946, 0.0016657718224450946, 0.0016657718224450946, 0.0016657718224450946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016657718224450946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166577
Iteration 2/1000 | Loss: 0.00003628
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001689
Iteration 8/1000 | Loss: 0.00001644
Iteration 9/1000 | Loss: 0.00001611
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001547
Iteration 17/1000 | Loss: 0.00001546
Iteration 18/1000 | Loss: 0.00001546
Iteration 19/1000 | Loss: 0.00001545
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001531
Iteration 23/1000 | Loss: 0.00001530
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001528
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001526
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001520
Iteration 31/1000 | Loss: 0.00001520
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001518
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001518
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001516
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001513
Iteration 70/1000 | Loss: 0.00001513
Iteration 71/1000 | Loss: 0.00001513
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001509
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001509
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Iteration 130/1000 | Loss: 0.00001508
Iteration 131/1000 | Loss: 0.00001508
Iteration 132/1000 | Loss: 0.00001508
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001507
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001507
Iteration 143/1000 | Loss: 0.00001507
Iteration 144/1000 | Loss: 0.00001507
Iteration 145/1000 | Loss: 0.00001507
Iteration 146/1000 | Loss: 0.00001507
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001506
Iteration 149/1000 | Loss: 0.00001506
Iteration 150/1000 | Loss: 0.00001506
Iteration 151/1000 | Loss: 0.00001506
Iteration 152/1000 | Loss: 0.00001506
Iteration 153/1000 | Loss: 0.00001506
Iteration 154/1000 | Loss: 0.00001506
Iteration 155/1000 | Loss: 0.00001506
Iteration 156/1000 | Loss: 0.00001506
Iteration 157/1000 | Loss: 0.00001506
Iteration 158/1000 | Loss: 0.00001506
Iteration 159/1000 | Loss: 0.00001506
Iteration 160/1000 | Loss: 0.00001506
Iteration 161/1000 | Loss: 0.00001506
Iteration 162/1000 | Loss: 0.00001506
Iteration 163/1000 | Loss: 0.00001506
Iteration 164/1000 | Loss: 0.00001506
Iteration 165/1000 | Loss: 0.00001506
Iteration 166/1000 | Loss: 0.00001506
Iteration 167/1000 | Loss: 0.00001506
Iteration 168/1000 | Loss: 0.00001506
Iteration 169/1000 | Loss: 0.00001506
Iteration 170/1000 | Loss: 0.00001506
Iteration 171/1000 | Loss: 0.00001506
Iteration 172/1000 | Loss: 0.00001506
Iteration 173/1000 | Loss: 0.00001506
Iteration 174/1000 | Loss: 0.00001506
Iteration 175/1000 | Loss: 0.00001506
Iteration 176/1000 | Loss: 0.00001506
Iteration 177/1000 | Loss: 0.00001506
Iteration 178/1000 | Loss: 0.00001506
Iteration 179/1000 | Loss: 0.00001506
Iteration 180/1000 | Loss: 0.00001506
Iteration 181/1000 | Loss: 0.00001506
Iteration 182/1000 | Loss: 0.00001506
Iteration 183/1000 | Loss: 0.00001506
Iteration 184/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.5062286365719046e-05, 1.5062286365719046e-05, 1.5062286365719046e-05, 1.5062286365719046e-05, 1.5062286365719046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5062286365719046e-05

Optimization complete. Final v2v error: 3.3508565425872803 mm

Highest mean error: 3.686065912246704 mm for frame 86

Lowest mean error: 3.027326822280884 mm for frame 0

Saving results

Total time: 36.801071882247925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680416
Iteration 2/25 | Loss: 0.00136191
Iteration 3/25 | Loss: 0.00111890
Iteration 4/25 | Loss: 0.00109654
Iteration 5/25 | Loss: 0.00109345
Iteration 6/25 | Loss: 0.00109310
Iteration 7/25 | Loss: 0.00109310
Iteration 8/25 | Loss: 0.00109310
Iteration 9/25 | Loss: 0.00109310
Iteration 10/25 | Loss: 0.00109310
Iteration 11/25 | Loss: 0.00109310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010931004071608186, 0.0010931004071608186, 0.0010931004071608186, 0.0010931004071608186, 0.0010931004071608186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010931004071608186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55955696
Iteration 2/25 | Loss: 0.00141875
Iteration 3/25 | Loss: 0.00141873
Iteration 4/25 | Loss: 0.00141873
Iteration 5/25 | Loss: 0.00141873
Iteration 6/25 | Loss: 0.00141873
Iteration 7/25 | Loss: 0.00141873
Iteration 8/25 | Loss: 0.00141873
Iteration 9/25 | Loss: 0.00141873
Iteration 10/25 | Loss: 0.00141873
Iteration 11/25 | Loss: 0.00141873
Iteration 12/25 | Loss: 0.00141873
Iteration 13/25 | Loss: 0.00141873
Iteration 14/25 | Loss: 0.00141873
Iteration 15/25 | Loss: 0.00141873
Iteration 16/25 | Loss: 0.00141873
Iteration 17/25 | Loss: 0.00141873
Iteration 18/25 | Loss: 0.00141873
Iteration 19/25 | Loss: 0.00141873
Iteration 20/25 | Loss: 0.00141873
Iteration 21/25 | Loss: 0.00141873
Iteration 22/25 | Loss: 0.00141873
Iteration 23/25 | Loss: 0.00141873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014187266351655126, 0.0014187266351655126, 0.0014187266351655126, 0.0014187266351655126, 0.0014187266351655126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014187266351655126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141873
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00001768
Iteration 4/1000 | Loss: 0.00001514
Iteration 5/1000 | Loss: 0.00001407
Iteration 6/1000 | Loss: 0.00001353
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001245
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001238
Iteration 38/1000 | Loss: 0.00001237
Iteration 39/1000 | Loss: 0.00001237
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001237
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001234
Iteration 51/1000 | Loss: 0.00001234
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001233
Iteration 54/1000 | Loss: 0.00001233
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001233
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001232
Iteration 60/1000 | Loss: 0.00001232
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001229
Iteration 73/1000 | Loss: 0.00001229
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001228
Iteration 77/1000 | Loss: 0.00001228
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001219
Iteration 123/1000 | Loss: 0.00001219
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001218
Iteration 131/1000 | Loss: 0.00001218
Iteration 132/1000 | Loss: 0.00001218
Iteration 133/1000 | Loss: 0.00001218
Iteration 134/1000 | Loss: 0.00001218
Iteration 135/1000 | Loss: 0.00001218
Iteration 136/1000 | Loss: 0.00001218
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001217
Iteration 144/1000 | Loss: 0.00001217
Iteration 145/1000 | Loss: 0.00001217
Iteration 146/1000 | Loss: 0.00001217
Iteration 147/1000 | Loss: 0.00001217
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001216
Iteration 155/1000 | Loss: 0.00001216
Iteration 156/1000 | Loss: 0.00001216
Iteration 157/1000 | Loss: 0.00001216
Iteration 158/1000 | Loss: 0.00001216
Iteration 159/1000 | Loss: 0.00001216
Iteration 160/1000 | Loss: 0.00001216
Iteration 161/1000 | Loss: 0.00001216
Iteration 162/1000 | Loss: 0.00001216
Iteration 163/1000 | Loss: 0.00001216
Iteration 164/1000 | Loss: 0.00001215
Iteration 165/1000 | Loss: 0.00001215
Iteration 166/1000 | Loss: 0.00001215
Iteration 167/1000 | Loss: 0.00001215
Iteration 168/1000 | Loss: 0.00001215
Iteration 169/1000 | Loss: 0.00001215
Iteration 170/1000 | Loss: 0.00001215
Iteration 171/1000 | Loss: 0.00001215
Iteration 172/1000 | Loss: 0.00001215
Iteration 173/1000 | Loss: 0.00001215
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001215
Iteration 176/1000 | Loss: 0.00001215
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001214
Iteration 179/1000 | Loss: 0.00001214
Iteration 180/1000 | Loss: 0.00001214
Iteration 181/1000 | Loss: 0.00001214
Iteration 182/1000 | Loss: 0.00001214
Iteration 183/1000 | Loss: 0.00001214
Iteration 184/1000 | Loss: 0.00001214
Iteration 185/1000 | Loss: 0.00001214
Iteration 186/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2143776984885335e-05, 1.2143776984885335e-05, 1.2143776984885335e-05, 1.2143776984885335e-05, 1.2143776984885335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2143776984885335e-05

Optimization complete. Final v2v error: 2.8631250858306885 mm

Highest mean error: 3.42722749710083 mm for frame 176

Lowest mean error: 2.4831621646881104 mm for frame 5

Saving results

Total time: 36.1735520362854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097266
Iteration 2/25 | Loss: 0.00309327
Iteration 3/25 | Loss: 0.00243814
Iteration 4/25 | Loss: 0.00158807
Iteration 5/25 | Loss: 0.00163552
Iteration 6/25 | Loss: 0.00139667
Iteration 7/25 | Loss: 0.00128614
Iteration 8/25 | Loss: 0.00114564
Iteration 9/25 | Loss: 0.00106179
Iteration 10/25 | Loss: 0.00103816
Iteration 11/25 | Loss: 0.00102877
Iteration 12/25 | Loss: 0.00102229
Iteration 13/25 | Loss: 0.00102780
Iteration 14/25 | Loss: 0.00102127
Iteration 15/25 | Loss: 0.00102740
Iteration 16/25 | Loss: 0.00102892
Iteration 17/25 | Loss: 0.00102592
Iteration 18/25 | Loss: 0.00102557
Iteration 19/25 | Loss: 0.00102611
Iteration 20/25 | Loss: 0.00102157
Iteration 21/25 | Loss: 0.00102807
Iteration 22/25 | Loss: 0.00101973
Iteration 23/25 | Loss: 0.00101935
Iteration 24/25 | Loss: 0.00101931
Iteration 25/25 | Loss: 0.00101929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23340595
Iteration 2/25 | Loss: 0.00164370
Iteration 3/25 | Loss: 0.00164370
Iteration 4/25 | Loss: 0.00164370
Iteration 5/25 | Loss: 0.00164370
Iteration 6/25 | Loss: 0.00164370
Iteration 7/25 | Loss: 0.00164370
Iteration 8/25 | Loss: 0.00164370
Iteration 9/25 | Loss: 0.00164370
Iteration 10/25 | Loss: 0.00164370
Iteration 11/25 | Loss: 0.00164370
Iteration 12/25 | Loss: 0.00164370
Iteration 13/25 | Loss: 0.00164370
Iteration 14/25 | Loss: 0.00164370
Iteration 15/25 | Loss: 0.00164370
Iteration 16/25 | Loss: 0.00164370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016436957521364093, 0.0016436957521364093, 0.0016436957521364093, 0.0016436957521364093, 0.0016436957521364093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016436957521364093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164370
Iteration 2/1000 | Loss: 0.00004016
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00002208
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001781
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00054208
Iteration 12/1000 | Loss: 0.00032093
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001293
Iteration 20/1000 | Loss: 0.00001274
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001226
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001215
Iteration 58/1000 | Loss: 0.00001215
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.2104096640541684e-05, 1.2104096640541684e-05, 1.2104096640541684e-05, 1.2104096640541684e-05, 1.2104096640541684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2104096640541684e-05

Optimization complete. Final v2v error: 2.8437626361846924 mm

Highest mean error: 3.9240503311157227 mm for frame 19

Lowest mean error: 2.3872413635253906 mm for frame 155

Saving results

Total time: 79.62596035003662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784305
Iteration 2/25 | Loss: 0.00140098
Iteration 3/25 | Loss: 0.00106972
Iteration 4/25 | Loss: 0.00105729
Iteration 5/25 | Loss: 0.00105600
Iteration 6/25 | Loss: 0.00105600
Iteration 7/25 | Loss: 0.00105600
Iteration 8/25 | Loss: 0.00105600
Iteration 9/25 | Loss: 0.00105600
Iteration 10/25 | Loss: 0.00105600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010560032678768039, 0.0010560032678768039, 0.0010560032678768039, 0.0010560032678768039, 0.0010560032678768039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010560032678768039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21029794
Iteration 2/25 | Loss: 0.00139879
Iteration 3/25 | Loss: 0.00139878
Iteration 4/25 | Loss: 0.00139878
Iteration 5/25 | Loss: 0.00139878
Iteration 6/25 | Loss: 0.00139878
Iteration 7/25 | Loss: 0.00139878
Iteration 8/25 | Loss: 0.00139878
Iteration 9/25 | Loss: 0.00139878
Iteration 10/25 | Loss: 0.00139878
Iteration 11/25 | Loss: 0.00139878
Iteration 12/25 | Loss: 0.00139878
Iteration 13/25 | Loss: 0.00139878
Iteration 14/25 | Loss: 0.00139878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013987766578793526, 0.0013987766578793526, 0.0013987766578793526, 0.0013987766578793526, 0.0013987766578793526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013987766578793526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139878
Iteration 2/1000 | Loss: 0.00001748
Iteration 3/1000 | Loss: 0.00001170
Iteration 4/1000 | Loss: 0.00001036
Iteration 5/1000 | Loss: 0.00000980
Iteration 6/1000 | Loss: 0.00000941
Iteration 7/1000 | Loss: 0.00000918
Iteration 8/1000 | Loss: 0.00000895
Iteration 9/1000 | Loss: 0.00000882
Iteration 10/1000 | Loss: 0.00000880
Iteration 11/1000 | Loss: 0.00000879
Iteration 12/1000 | Loss: 0.00000879
Iteration 13/1000 | Loss: 0.00000877
Iteration 14/1000 | Loss: 0.00000874
Iteration 15/1000 | Loss: 0.00000872
Iteration 16/1000 | Loss: 0.00000868
Iteration 17/1000 | Loss: 0.00000868
Iteration 18/1000 | Loss: 0.00000867
Iteration 19/1000 | Loss: 0.00000867
Iteration 20/1000 | Loss: 0.00000867
Iteration 21/1000 | Loss: 0.00000867
Iteration 22/1000 | Loss: 0.00000867
Iteration 23/1000 | Loss: 0.00000866
Iteration 24/1000 | Loss: 0.00000866
Iteration 25/1000 | Loss: 0.00000865
Iteration 26/1000 | Loss: 0.00000865
Iteration 27/1000 | Loss: 0.00000865
Iteration 28/1000 | Loss: 0.00000865
Iteration 29/1000 | Loss: 0.00000864
Iteration 30/1000 | Loss: 0.00000864
Iteration 31/1000 | Loss: 0.00000864
Iteration 32/1000 | Loss: 0.00000863
Iteration 33/1000 | Loss: 0.00000863
Iteration 34/1000 | Loss: 0.00000863
Iteration 35/1000 | Loss: 0.00000862
Iteration 36/1000 | Loss: 0.00000861
Iteration 37/1000 | Loss: 0.00000861
Iteration 38/1000 | Loss: 0.00000861
Iteration 39/1000 | Loss: 0.00000860
Iteration 40/1000 | Loss: 0.00000860
Iteration 41/1000 | Loss: 0.00000858
Iteration 42/1000 | Loss: 0.00000858
Iteration 43/1000 | Loss: 0.00000858
Iteration 44/1000 | Loss: 0.00000858
Iteration 45/1000 | Loss: 0.00000858
Iteration 46/1000 | Loss: 0.00000857
Iteration 47/1000 | Loss: 0.00000857
Iteration 48/1000 | Loss: 0.00000856
Iteration 49/1000 | Loss: 0.00000855
Iteration 50/1000 | Loss: 0.00000855
Iteration 51/1000 | Loss: 0.00000855
Iteration 52/1000 | Loss: 0.00000855
Iteration 53/1000 | Loss: 0.00000855
Iteration 54/1000 | Loss: 0.00000855
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000855
Iteration 57/1000 | Loss: 0.00000855
Iteration 58/1000 | Loss: 0.00000855
Iteration 59/1000 | Loss: 0.00000855
Iteration 60/1000 | Loss: 0.00000855
Iteration 61/1000 | Loss: 0.00000855
Iteration 62/1000 | Loss: 0.00000855
Iteration 63/1000 | Loss: 0.00000855
Iteration 64/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [8.553651241527405e-06, 8.553651241527405e-06, 8.553651241527405e-06, 8.553651241527405e-06, 8.553651241527405e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.553651241527405e-06

Optimization complete. Final v2v error: 2.492065191268921 mm

Highest mean error: 2.7133431434631348 mm for frame 44

Lowest mean error: 2.2417473793029785 mm for frame 99

Saving results

Total time: 27.22013545036316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621635
Iteration 2/25 | Loss: 0.00158485
Iteration 3/25 | Loss: 0.00132056
Iteration 4/25 | Loss: 0.00127753
Iteration 5/25 | Loss: 0.00126655
Iteration 6/25 | Loss: 0.00125218
Iteration 7/25 | Loss: 0.00124466
Iteration 8/25 | Loss: 0.00124272
Iteration 9/25 | Loss: 0.00123968
Iteration 10/25 | Loss: 0.00123835
Iteration 11/25 | Loss: 0.00125044
Iteration 12/25 | Loss: 0.00124053
Iteration 13/25 | Loss: 0.00122980
Iteration 14/25 | Loss: 0.00122637
Iteration 15/25 | Loss: 0.00122548
Iteration 16/25 | Loss: 0.00122529
Iteration 17/25 | Loss: 0.00122510
Iteration 18/25 | Loss: 0.00122497
Iteration 19/25 | Loss: 0.00122482
Iteration 20/25 | Loss: 0.00122474
Iteration 21/25 | Loss: 0.00122465
Iteration 22/25 | Loss: 0.00122453
Iteration 23/25 | Loss: 0.00122443
Iteration 24/25 | Loss: 0.00122435
Iteration 25/25 | Loss: 0.00122431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21400547
Iteration 2/25 | Loss: 0.00231429
Iteration 3/25 | Loss: 0.00231429
Iteration 4/25 | Loss: 0.00231429
Iteration 5/25 | Loss: 0.00231428
Iteration 6/25 | Loss: 0.00231428
Iteration 7/25 | Loss: 0.00231428
Iteration 8/25 | Loss: 0.00231428
Iteration 9/25 | Loss: 0.00231428
Iteration 10/25 | Loss: 0.00231428
Iteration 11/25 | Loss: 0.00231428
Iteration 12/25 | Loss: 0.00231428
Iteration 13/25 | Loss: 0.00231428
Iteration 14/25 | Loss: 0.00231428
Iteration 15/25 | Loss: 0.00231428
Iteration 16/25 | Loss: 0.00231428
Iteration 17/25 | Loss: 0.00231428
Iteration 18/25 | Loss: 0.00231428
Iteration 19/25 | Loss: 0.00231428
Iteration 20/25 | Loss: 0.00231428
Iteration 21/25 | Loss: 0.00231428
Iteration 22/25 | Loss: 0.00231428
Iteration 23/25 | Loss: 0.00231428
Iteration 24/25 | Loss: 0.00231428
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00231428281404078, 0.00231428281404078, 0.00231428281404078, 0.00231428281404078, 0.00231428281404078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00231428281404078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231428
Iteration 2/1000 | Loss: 0.00069370
Iteration 3/1000 | Loss: 0.00078702
Iteration 4/1000 | Loss: 0.00063092
Iteration 5/1000 | Loss: 0.00045894
Iteration 6/1000 | Loss: 0.00012281
Iteration 7/1000 | Loss: 0.00098289
Iteration 8/1000 | Loss: 0.00056703
Iteration 9/1000 | Loss: 0.00021097
Iteration 10/1000 | Loss: 0.00039095
Iteration 11/1000 | Loss: 0.00112504
Iteration 12/1000 | Loss: 0.00084357
Iteration 13/1000 | Loss: 0.00018215
Iteration 14/1000 | Loss: 0.00011557
Iteration 15/1000 | Loss: 0.00142131
Iteration 16/1000 | Loss: 0.00136060
Iteration 17/1000 | Loss: 0.00056073
Iteration 18/1000 | Loss: 0.00014829
Iteration 19/1000 | Loss: 0.00026423
Iteration 20/1000 | Loss: 0.00043719
Iteration 21/1000 | Loss: 0.00068429
Iteration 22/1000 | Loss: 0.00057539
Iteration 23/1000 | Loss: 0.00009190
Iteration 24/1000 | Loss: 0.00008525
Iteration 25/1000 | Loss: 0.00136493
Iteration 26/1000 | Loss: 0.00081460
Iteration 27/1000 | Loss: 0.00038486
Iteration 28/1000 | Loss: 0.00008591
Iteration 29/1000 | Loss: 0.00047591
Iteration 30/1000 | Loss: 0.00007504
Iteration 31/1000 | Loss: 0.00059522
Iteration 32/1000 | Loss: 0.00015835
Iteration 33/1000 | Loss: 0.00006553
Iteration 34/1000 | Loss: 0.00006250
Iteration 35/1000 | Loss: 0.00052937
Iteration 36/1000 | Loss: 0.00014269
Iteration 37/1000 | Loss: 0.00051527
Iteration 38/1000 | Loss: 0.00062851
Iteration 39/1000 | Loss: 0.00017438
Iteration 40/1000 | Loss: 0.00011502
Iteration 41/1000 | Loss: 0.00014808
Iteration 42/1000 | Loss: 0.00017719
Iteration 43/1000 | Loss: 0.00005947
Iteration 44/1000 | Loss: 0.00017394
Iteration 45/1000 | Loss: 0.00012781
Iteration 46/1000 | Loss: 0.00005622
Iteration 47/1000 | Loss: 0.00018785
Iteration 48/1000 | Loss: 0.00008997
Iteration 49/1000 | Loss: 0.00018504
Iteration 50/1000 | Loss: 0.00018552
Iteration 51/1000 | Loss: 0.00016774
Iteration 52/1000 | Loss: 0.00007182
Iteration 53/1000 | Loss: 0.00005818
Iteration 54/1000 | Loss: 0.00005241
Iteration 55/1000 | Loss: 0.00004990
Iteration 56/1000 | Loss: 0.00004800
Iteration 57/1000 | Loss: 0.00004689
Iteration 58/1000 | Loss: 0.00004618
Iteration 59/1000 | Loss: 0.00012083
Iteration 60/1000 | Loss: 0.00023535
Iteration 61/1000 | Loss: 0.00004830
Iteration 62/1000 | Loss: 0.00004487
Iteration 63/1000 | Loss: 0.00004399
Iteration 64/1000 | Loss: 0.00033092
Iteration 65/1000 | Loss: 0.00004426
Iteration 66/1000 | Loss: 0.00004238
Iteration 67/1000 | Loss: 0.00004169
Iteration 68/1000 | Loss: 0.00004087
Iteration 69/1000 | Loss: 0.00004022
Iteration 70/1000 | Loss: 0.00003988
Iteration 71/1000 | Loss: 0.00003963
Iteration 72/1000 | Loss: 0.00003958
Iteration 73/1000 | Loss: 0.00003956
Iteration 74/1000 | Loss: 0.00003949
Iteration 75/1000 | Loss: 0.00003939
Iteration 76/1000 | Loss: 0.00003936
Iteration 77/1000 | Loss: 0.00003936
Iteration 78/1000 | Loss: 0.00003935
Iteration 79/1000 | Loss: 0.00003933
Iteration 80/1000 | Loss: 0.00003932
Iteration 81/1000 | Loss: 0.00003932
Iteration 82/1000 | Loss: 0.00003931
Iteration 83/1000 | Loss: 0.00003931
Iteration 84/1000 | Loss: 0.00003930
Iteration 85/1000 | Loss: 0.00003929
Iteration 86/1000 | Loss: 0.00003928
Iteration 87/1000 | Loss: 0.00003928
Iteration 88/1000 | Loss: 0.00003927
Iteration 89/1000 | Loss: 0.00003927
Iteration 90/1000 | Loss: 0.00003927
Iteration 91/1000 | Loss: 0.00003927
Iteration 92/1000 | Loss: 0.00003927
Iteration 93/1000 | Loss: 0.00003926
Iteration 94/1000 | Loss: 0.00003926
Iteration 95/1000 | Loss: 0.00003926
Iteration 96/1000 | Loss: 0.00003925
Iteration 97/1000 | Loss: 0.00003925
Iteration 98/1000 | Loss: 0.00003925
Iteration 99/1000 | Loss: 0.00003924
Iteration 100/1000 | Loss: 0.00003924
Iteration 101/1000 | Loss: 0.00003920
Iteration 102/1000 | Loss: 0.00003917
Iteration 103/1000 | Loss: 0.00018878
Iteration 104/1000 | Loss: 0.00003905
Iteration 105/1000 | Loss: 0.00003829
Iteration 106/1000 | Loss: 0.00003799
Iteration 107/1000 | Loss: 0.00003777
Iteration 108/1000 | Loss: 0.00003756
Iteration 109/1000 | Loss: 0.00003724
Iteration 110/1000 | Loss: 0.00003705
Iteration 111/1000 | Loss: 0.00003703
Iteration 112/1000 | Loss: 0.00003700
Iteration 113/1000 | Loss: 0.00003699
Iteration 114/1000 | Loss: 0.00003698
Iteration 115/1000 | Loss: 0.00003697
Iteration 116/1000 | Loss: 0.00003695
Iteration 117/1000 | Loss: 0.00003695
Iteration 118/1000 | Loss: 0.00003694
Iteration 119/1000 | Loss: 0.00003694
Iteration 120/1000 | Loss: 0.00003694
Iteration 121/1000 | Loss: 0.00003694
Iteration 122/1000 | Loss: 0.00003693
Iteration 123/1000 | Loss: 0.00003693
Iteration 124/1000 | Loss: 0.00003692
Iteration 125/1000 | Loss: 0.00003692
Iteration 126/1000 | Loss: 0.00003692
Iteration 127/1000 | Loss: 0.00003691
Iteration 128/1000 | Loss: 0.00003691
Iteration 129/1000 | Loss: 0.00003691
Iteration 130/1000 | Loss: 0.00003691
Iteration 131/1000 | Loss: 0.00003691
Iteration 132/1000 | Loss: 0.00003691
Iteration 133/1000 | Loss: 0.00003690
Iteration 134/1000 | Loss: 0.00003690
Iteration 135/1000 | Loss: 0.00003690
Iteration 136/1000 | Loss: 0.00003690
Iteration 137/1000 | Loss: 0.00003690
Iteration 138/1000 | Loss: 0.00003690
Iteration 139/1000 | Loss: 0.00003690
Iteration 140/1000 | Loss: 0.00003690
Iteration 141/1000 | Loss: 0.00003690
Iteration 142/1000 | Loss: 0.00003690
Iteration 143/1000 | Loss: 0.00003690
Iteration 144/1000 | Loss: 0.00003690
Iteration 145/1000 | Loss: 0.00003690
Iteration 146/1000 | Loss: 0.00003690
Iteration 147/1000 | Loss: 0.00003689
Iteration 148/1000 | Loss: 0.00003689
Iteration 149/1000 | Loss: 0.00003689
Iteration 150/1000 | Loss: 0.00003689
Iteration 151/1000 | Loss: 0.00003689
Iteration 152/1000 | Loss: 0.00003689
Iteration 153/1000 | Loss: 0.00003689
Iteration 154/1000 | Loss: 0.00003689
Iteration 155/1000 | Loss: 0.00003689
Iteration 156/1000 | Loss: 0.00003689
Iteration 157/1000 | Loss: 0.00003689
Iteration 158/1000 | Loss: 0.00003688
Iteration 159/1000 | Loss: 0.00003688
Iteration 160/1000 | Loss: 0.00003688
Iteration 161/1000 | Loss: 0.00003688
Iteration 162/1000 | Loss: 0.00003688
Iteration 163/1000 | Loss: 0.00003688
Iteration 164/1000 | Loss: 0.00003688
Iteration 165/1000 | Loss: 0.00003688
Iteration 166/1000 | Loss: 0.00003688
Iteration 167/1000 | Loss: 0.00003688
Iteration 168/1000 | Loss: 0.00003688
Iteration 169/1000 | Loss: 0.00003688
Iteration 170/1000 | Loss: 0.00003688
Iteration 171/1000 | Loss: 0.00003688
Iteration 172/1000 | Loss: 0.00003688
Iteration 173/1000 | Loss: 0.00003687
Iteration 174/1000 | Loss: 0.00003687
Iteration 175/1000 | Loss: 0.00003687
Iteration 176/1000 | Loss: 0.00003687
Iteration 177/1000 | Loss: 0.00003687
Iteration 178/1000 | Loss: 0.00003687
Iteration 179/1000 | Loss: 0.00003687
Iteration 180/1000 | Loss: 0.00003687
Iteration 181/1000 | Loss: 0.00003687
Iteration 182/1000 | Loss: 0.00003687
Iteration 183/1000 | Loss: 0.00003687
Iteration 184/1000 | Loss: 0.00003687
Iteration 185/1000 | Loss: 0.00003687
Iteration 186/1000 | Loss: 0.00003686
Iteration 187/1000 | Loss: 0.00003686
Iteration 188/1000 | Loss: 0.00003686
Iteration 189/1000 | Loss: 0.00003686
Iteration 190/1000 | Loss: 0.00003686
Iteration 191/1000 | Loss: 0.00003686
Iteration 192/1000 | Loss: 0.00003686
Iteration 193/1000 | Loss: 0.00003686
Iteration 194/1000 | Loss: 0.00003685
Iteration 195/1000 | Loss: 0.00003685
Iteration 196/1000 | Loss: 0.00003685
Iteration 197/1000 | Loss: 0.00003685
Iteration 198/1000 | Loss: 0.00003685
Iteration 199/1000 | Loss: 0.00003685
Iteration 200/1000 | Loss: 0.00003685
Iteration 201/1000 | Loss: 0.00003685
Iteration 202/1000 | Loss: 0.00003685
Iteration 203/1000 | Loss: 0.00003685
Iteration 204/1000 | Loss: 0.00003685
Iteration 205/1000 | Loss: 0.00003685
Iteration 206/1000 | Loss: 0.00003685
Iteration 207/1000 | Loss: 0.00003685
Iteration 208/1000 | Loss: 0.00003685
Iteration 209/1000 | Loss: 0.00003685
Iteration 210/1000 | Loss: 0.00003685
Iteration 211/1000 | Loss: 0.00003685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.6854871723335236e-05, 3.6854871723335236e-05, 3.6854871723335236e-05, 3.6854871723335236e-05, 3.6854871723335236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6854871723335236e-05

Optimization complete. Final v2v error: 3.5372214317321777 mm

Highest mean error: 11.887451171875 mm for frame 17

Lowest mean error: 2.3607945442199707 mm for frame 63

Saving results

Total time: 188.68682765960693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423162
Iteration 2/25 | Loss: 0.00112606
Iteration 3/25 | Loss: 0.00100889
Iteration 4/25 | Loss: 0.00099557
Iteration 5/25 | Loss: 0.00099278
Iteration 6/25 | Loss: 0.00099256
Iteration 7/25 | Loss: 0.00099256
Iteration 8/25 | Loss: 0.00099256
Iteration 9/25 | Loss: 0.00099256
Iteration 10/25 | Loss: 0.00099256
Iteration 11/25 | Loss: 0.00099256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009925630874931812, 0.0009925630874931812, 0.0009925630874931812, 0.0009925630874931812, 0.0009925630874931812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009925630874931812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21784616
Iteration 2/25 | Loss: 0.00172237
Iteration 3/25 | Loss: 0.00172237
Iteration 4/25 | Loss: 0.00172237
Iteration 5/25 | Loss: 0.00172237
Iteration 6/25 | Loss: 0.00172237
Iteration 7/25 | Loss: 0.00172237
Iteration 8/25 | Loss: 0.00172237
Iteration 9/25 | Loss: 0.00172237
Iteration 10/25 | Loss: 0.00172237
Iteration 11/25 | Loss: 0.00172237
Iteration 12/25 | Loss: 0.00172237
Iteration 13/25 | Loss: 0.00172237
Iteration 14/25 | Loss: 0.00172237
Iteration 15/25 | Loss: 0.00172237
Iteration 16/25 | Loss: 0.00172237
Iteration 17/25 | Loss: 0.00172237
Iteration 18/25 | Loss: 0.00172237
Iteration 19/25 | Loss: 0.00172237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017223685281351209, 0.0017223685281351209, 0.0017223685281351209, 0.0017223685281351209, 0.0017223685281351209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017223685281351209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172237
Iteration 2/1000 | Loss: 0.00002570
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001275
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001179
Iteration 8/1000 | Loss: 0.00001155
Iteration 9/1000 | Loss: 0.00001136
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001120
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001115
Iteration 15/1000 | Loss: 0.00001114
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001107
Iteration 29/1000 | Loss: 0.00001107
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001106
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001105
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001104
Iteration 43/1000 | Loss: 0.00001104
Iteration 44/1000 | Loss: 0.00001104
Iteration 45/1000 | Loss: 0.00001104
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001102
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001101
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.0993934665748384e-05, 1.0993934665748384e-05, 1.0993934665748384e-05, 1.0993934665748384e-05, 1.0993934665748384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0993934665748384e-05

Optimization complete. Final v2v error: 2.7216646671295166 mm

Highest mean error: 3.38502836227417 mm for frame 0

Lowest mean error: 1.9762955904006958 mm for frame 82

Saving results

Total time: 31.993834733963013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00632746
Iteration 2/25 | Loss: 0.00128867
Iteration 3/25 | Loss: 0.00107692
Iteration 4/25 | Loss: 0.00105615
Iteration 5/25 | Loss: 0.00105457
Iteration 6/25 | Loss: 0.00105457
Iteration 7/25 | Loss: 0.00105457
Iteration 8/25 | Loss: 0.00105457
Iteration 9/25 | Loss: 0.00105457
Iteration 10/25 | Loss: 0.00105457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001054569729603827, 0.001054569729603827, 0.001054569729603827, 0.001054569729603827, 0.001054569729603827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001054569729603827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23756528
Iteration 2/25 | Loss: 0.00131895
Iteration 3/25 | Loss: 0.00131892
Iteration 4/25 | Loss: 0.00131892
Iteration 5/25 | Loss: 0.00131892
Iteration 6/25 | Loss: 0.00131892
Iteration 7/25 | Loss: 0.00131892
Iteration 8/25 | Loss: 0.00131892
Iteration 9/25 | Loss: 0.00131892
Iteration 10/25 | Loss: 0.00131892
Iteration 11/25 | Loss: 0.00131892
Iteration 12/25 | Loss: 0.00131892
Iteration 13/25 | Loss: 0.00131892
Iteration 14/25 | Loss: 0.00131892
Iteration 15/25 | Loss: 0.00131892
Iteration 16/25 | Loss: 0.00131892
Iteration 17/25 | Loss: 0.00131892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013189197052270174, 0.0013189197052270174, 0.0013189197052270174, 0.0013189197052270174, 0.0013189197052270174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013189197052270174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131892
Iteration 2/1000 | Loss: 0.00001961
Iteration 3/1000 | Loss: 0.00001348
Iteration 4/1000 | Loss: 0.00001199
Iteration 5/1000 | Loss: 0.00001115
Iteration 6/1000 | Loss: 0.00001067
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00000984
Iteration 9/1000 | Loss: 0.00000955
Iteration 10/1000 | Loss: 0.00000943
Iteration 11/1000 | Loss: 0.00000940
Iteration 12/1000 | Loss: 0.00000937
Iteration 13/1000 | Loss: 0.00000936
Iteration 14/1000 | Loss: 0.00000935
Iteration 15/1000 | Loss: 0.00000931
Iteration 16/1000 | Loss: 0.00000930
Iteration 17/1000 | Loss: 0.00000929
Iteration 18/1000 | Loss: 0.00000929
Iteration 19/1000 | Loss: 0.00000928
Iteration 20/1000 | Loss: 0.00000928
Iteration 21/1000 | Loss: 0.00000928
Iteration 22/1000 | Loss: 0.00000925
Iteration 23/1000 | Loss: 0.00000925
Iteration 24/1000 | Loss: 0.00000924
Iteration 25/1000 | Loss: 0.00000924
Iteration 26/1000 | Loss: 0.00000923
Iteration 27/1000 | Loss: 0.00000923
Iteration 28/1000 | Loss: 0.00000921
Iteration 29/1000 | Loss: 0.00000920
Iteration 30/1000 | Loss: 0.00000919
Iteration 31/1000 | Loss: 0.00000919
Iteration 32/1000 | Loss: 0.00000919
Iteration 33/1000 | Loss: 0.00000918
Iteration 34/1000 | Loss: 0.00000918
Iteration 35/1000 | Loss: 0.00000918
Iteration 36/1000 | Loss: 0.00000917
Iteration 37/1000 | Loss: 0.00000917
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000913
Iteration 40/1000 | Loss: 0.00000912
Iteration 41/1000 | Loss: 0.00000911
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000910
Iteration 44/1000 | Loss: 0.00000909
Iteration 45/1000 | Loss: 0.00000908
Iteration 46/1000 | Loss: 0.00000908
Iteration 47/1000 | Loss: 0.00000908
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000908
Iteration 50/1000 | Loss: 0.00000908
Iteration 51/1000 | Loss: 0.00000907
Iteration 52/1000 | Loss: 0.00000907
Iteration 53/1000 | Loss: 0.00000907
Iteration 54/1000 | Loss: 0.00000906
Iteration 55/1000 | Loss: 0.00000906
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000905
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000904
Iteration 61/1000 | Loss: 0.00000904
Iteration 62/1000 | Loss: 0.00000904
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000902
Iteration 65/1000 | Loss: 0.00000902
Iteration 66/1000 | Loss: 0.00000902
Iteration 67/1000 | Loss: 0.00000901
Iteration 68/1000 | Loss: 0.00000901
Iteration 69/1000 | Loss: 0.00000901
Iteration 70/1000 | Loss: 0.00000901
Iteration 71/1000 | Loss: 0.00000900
Iteration 72/1000 | Loss: 0.00000900
Iteration 73/1000 | Loss: 0.00000900
Iteration 74/1000 | Loss: 0.00000900
Iteration 75/1000 | Loss: 0.00000900
Iteration 76/1000 | Loss: 0.00000900
Iteration 77/1000 | Loss: 0.00000900
Iteration 78/1000 | Loss: 0.00000900
Iteration 79/1000 | Loss: 0.00000900
Iteration 80/1000 | Loss: 0.00000900
Iteration 81/1000 | Loss: 0.00000900
Iteration 82/1000 | Loss: 0.00000900
Iteration 83/1000 | Loss: 0.00000900
Iteration 84/1000 | Loss: 0.00000900
Iteration 85/1000 | Loss: 0.00000900
Iteration 86/1000 | Loss: 0.00000900
Iteration 87/1000 | Loss: 0.00000900
Iteration 88/1000 | Loss: 0.00000900
Iteration 89/1000 | Loss: 0.00000900
Iteration 90/1000 | Loss: 0.00000899
Iteration 91/1000 | Loss: 0.00000899
Iteration 92/1000 | Loss: 0.00000899
Iteration 93/1000 | Loss: 0.00000899
Iteration 94/1000 | Loss: 0.00000898
Iteration 95/1000 | Loss: 0.00000898
Iteration 96/1000 | Loss: 0.00000898
Iteration 97/1000 | Loss: 0.00000898
Iteration 98/1000 | Loss: 0.00000898
Iteration 99/1000 | Loss: 0.00000898
Iteration 100/1000 | Loss: 0.00000898
Iteration 101/1000 | Loss: 0.00000897
Iteration 102/1000 | Loss: 0.00000897
Iteration 103/1000 | Loss: 0.00000897
Iteration 104/1000 | Loss: 0.00000897
Iteration 105/1000 | Loss: 0.00000897
Iteration 106/1000 | Loss: 0.00000897
Iteration 107/1000 | Loss: 0.00000897
Iteration 108/1000 | Loss: 0.00000897
Iteration 109/1000 | Loss: 0.00000897
Iteration 110/1000 | Loss: 0.00000896
Iteration 111/1000 | Loss: 0.00000896
Iteration 112/1000 | Loss: 0.00000896
Iteration 113/1000 | Loss: 0.00000896
Iteration 114/1000 | Loss: 0.00000896
Iteration 115/1000 | Loss: 0.00000896
Iteration 116/1000 | Loss: 0.00000896
Iteration 117/1000 | Loss: 0.00000896
Iteration 118/1000 | Loss: 0.00000895
Iteration 119/1000 | Loss: 0.00000895
Iteration 120/1000 | Loss: 0.00000895
Iteration 121/1000 | Loss: 0.00000895
Iteration 122/1000 | Loss: 0.00000895
Iteration 123/1000 | Loss: 0.00000895
Iteration 124/1000 | Loss: 0.00000895
Iteration 125/1000 | Loss: 0.00000895
Iteration 126/1000 | Loss: 0.00000895
Iteration 127/1000 | Loss: 0.00000894
Iteration 128/1000 | Loss: 0.00000894
Iteration 129/1000 | Loss: 0.00000894
Iteration 130/1000 | Loss: 0.00000894
Iteration 131/1000 | Loss: 0.00000894
Iteration 132/1000 | Loss: 0.00000894
Iteration 133/1000 | Loss: 0.00000894
Iteration 134/1000 | Loss: 0.00000894
Iteration 135/1000 | Loss: 0.00000894
Iteration 136/1000 | Loss: 0.00000894
Iteration 137/1000 | Loss: 0.00000894
Iteration 138/1000 | Loss: 0.00000894
Iteration 139/1000 | Loss: 0.00000894
Iteration 140/1000 | Loss: 0.00000894
Iteration 141/1000 | Loss: 0.00000894
Iteration 142/1000 | Loss: 0.00000894
Iteration 143/1000 | Loss: 0.00000894
Iteration 144/1000 | Loss: 0.00000894
Iteration 145/1000 | Loss: 0.00000894
Iteration 146/1000 | Loss: 0.00000894
Iteration 147/1000 | Loss: 0.00000894
Iteration 148/1000 | Loss: 0.00000894
Iteration 149/1000 | Loss: 0.00000894
Iteration 150/1000 | Loss: 0.00000894
Iteration 151/1000 | Loss: 0.00000894
Iteration 152/1000 | Loss: 0.00000894
Iteration 153/1000 | Loss: 0.00000894
Iteration 154/1000 | Loss: 0.00000894
Iteration 155/1000 | Loss: 0.00000894
Iteration 156/1000 | Loss: 0.00000894
Iteration 157/1000 | Loss: 0.00000894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [8.938366590882652e-06, 8.938366590882652e-06, 8.938366590882652e-06, 8.938366590882652e-06, 8.938366590882652e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.938366590882652e-06

Optimization complete. Final v2v error: 2.527745008468628 mm

Highest mean error: 2.906371831893921 mm for frame 145

Lowest mean error: 2.1440539360046387 mm for frame 29

Saving results

Total time: 34.59397530555725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475103
Iteration 2/25 | Loss: 0.00122788
Iteration 3/25 | Loss: 0.00105949
Iteration 4/25 | Loss: 0.00104562
Iteration 5/25 | Loss: 0.00104297
Iteration 6/25 | Loss: 0.00104297
Iteration 7/25 | Loss: 0.00104297
Iteration 8/25 | Loss: 0.00104297
Iteration 9/25 | Loss: 0.00104297
Iteration 10/25 | Loss: 0.00104297
Iteration 11/25 | Loss: 0.00104297
Iteration 12/25 | Loss: 0.00104297
Iteration 13/25 | Loss: 0.00104297
Iteration 14/25 | Loss: 0.00104297
Iteration 15/25 | Loss: 0.00104297
Iteration 16/25 | Loss: 0.00104297
Iteration 17/25 | Loss: 0.00104297
Iteration 18/25 | Loss: 0.00104297
Iteration 19/25 | Loss: 0.00104297
Iteration 20/25 | Loss: 0.00104297
Iteration 21/25 | Loss: 0.00104297
Iteration 22/25 | Loss: 0.00104297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010429660324007273, 0.0010429660324007273, 0.0010429660324007273, 0.0010429660324007273, 0.0010429660324007273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010429660324007273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23224342
Iteration 2/25 | Loss: 0.00149754
Iteration 3/25 | Loss: 0.00149752
Iteration 4/25 | Loss: 0.00149752
Iteration 5/25 | Loss: 0.00149752
Iteration 6/25 | Loss: 0.00149752
Iteration 7/25 | Loss: 0.00149752
Iteration 8/25 | Loss: 0.00149752
Iteration 9/25 | Loss: 0.00149752
Iteration 10/25 | Loss: 0.00149752
Iteration 11/25 | Loss: 0.00149752
Iteration 12/25 | Loss: 0.00149752
Iteration 13/25 | Loss: 0.00149752
Iteration 14/25 | Loss: 0.00149752
Iteration 15/25 | Loss: 0.00149752
Iteration 16/25 | Loss: 0.00149752
Iteration 17/25 | Loss: 0.00149752
Iteration 18/25 | Loss: 0.00149752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014975189696997404, 0.0014975189696997404, 0.0014975189696997404, 0.0014975189696997404, 0.0014975189696997404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014975189696997404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149752
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00001998
Iteration 4/1000 | Loss: 0.00001583
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001320
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001127
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001123
Iteration 25/1000 | Loss: 0.00001123
Iteration 26/1000 | Loss: 0.00001122
Iteration 27/1000 | Loss: 0.00001122
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001122
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001119
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001118
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001113
Iteration 49/1000 | Loss: 0.00001113
Iteration 50/1000 | Loss: 0.00001112
Iteration 51/1000 | Loss: 0.00001112
Iteration 52/1000 | Loss: 0.00001112
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001112
Iteration 56/1000 | Loss: 0.00001112
Iteration 57/1000 | Loss: 0.00001112
Iteration 58/1000 | Loss: 0.00001112
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001111
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001110
Iteration 64/1000 | Loss: 0.00001110
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001109
Iteration 68/1000 | Loss: 0.00001109
Iteration 69/1000 | Loss: 0.00001109
Iteration 70/1000 | Loss: 0.00001109
Iteration 71/1000 | Loss: 0.00001108
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001104
Iteration 82/1000 | Loss: 0.00001104
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001100
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001098
Iteration 102/1000 | Loss: 0.00001097
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001097
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001097
Iteration 107/1000 | Loss: 0.00001097
Iteration 108/1000 | Loss: 0.00001097
Iteration 109/1000 | Loss: 0.00001097
Iteration 110/1000 | Loss: 0.00001097
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001096
Iteration 113/1000 | Loss: 0.00001096
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001095
Iteration 116/1000 | Loss: 0.00001094
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001094
Iteration 119/1000 | Loss: 0.00001093
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001092
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001091
Iteration 126/1000 | Loss: 0.00001091
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Iteration 130/1000 | Loss: 0.00001090
Iteration 131/1000 | Loss: 0.00001090
Iteration 132/1000 | Loss: 0.00001090
Iteration 133/1000 | Loss: 0.00001090
Iteration 134/1000 | Loss: 0.00001090
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001089
Iteration 138/1000 | Loss: 0.00001089
Iteration 139/1000 | Loss: 0.00001089
Iteration 140/1000 | Loss: 0.00001088
Iteration 141/1000 | Loss: 0.00001088
Iteration 142/1000 | Loss: 0.00001088
Iteration 143/1000 | Loss: 0.00001088
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001086
Iteration 148/1000 | Loss: 0.00001086
Iteration 149/1000 | Loss: 0.00001086
Iteration 150/1000 | Loss: 0.00001086
Iteration 151/1000 | Loss: 0.00001086
Iteration 152/1000 | Loss: 0.00001085
Iteration 153/1000 | Loss: 0.00001085
Iteration 154/1000 | Loss: 0.00001085
Iteration 155/1000 | Loss: 0.00001085
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001084
Iteration 158/1000 | Loss: 0.00001084
Iteration 159/1000 | Loss: 0.00001083
Iteration 160/1000 | Loss: 0.00001083
Iteration 161/1000 | Loss: 0.00001083
Iteration 162/1000 | Loss: 0.00001083
Iteration 163/1000 | Loss: 0.00001083
Iteration 164/1000 | Loss: 0.00001083
Iteration 165/1000 | Loss: 0.00001083
Iteration 166/1000 | Loss: 0.00001082
Iteration 167/1000 | Loss: 0.00001082
Iteration 168/1000 | Loss: 0.00001082
Iteration 169/1000 | Loss: 0.00001082
Iteration 170/1000 | Loss: 0.00001082
Iteration 171/1000 | Loss: 0.00001082
Iteration 172/1000 | Loss: 0.00001082
Iteration 173/1000 | Loss: 0.00001082
Iteration 174/1000 | Loss: 0.00001082
Iteration 175/1000 | Loss: 0.00001082
Iteration 176/1000 | Loss: 0.00001082
Iteration 177/1000 | Loss: 0.00001081
Iteration 178/1000 | Loss: 0.00001081
Iteration 179/1000 | Loss: 0.00001081
Iteration 180/1000 | Loss: 0.00001081
Iteration 181/1000 | Loss: 0.00001081
Iteration 182/1000 | Loss: 0.00001081
Iteration 183/1000 | Loss: 0.00001081
Iteration 184/1000 | Loss: 0.00001081
Iteration 185/1000 | Loss: 0.00001080
Iteration 186/1000 | Loss: 0.00001080
Iteration 187/1000 | Loss: 0.00001080
Iteration 188/1000 | Loss: 0.00001080
Iteration 189/1000 | Loss: 0.00001080
Iteration 190/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.0803101758938283e-05, 1.0803101758938283e-05, 1.0803101758938283e-05, 1.0803101758938283e-05, 1.0803101758938283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0803101758938283e-05

Optimization complete. Final v2v error: 2.74330472946167 mm

Highest mean error: 3.276900053024292 mm for frame 239

Lowest mean error: 2.3184964656829834 mm for frame 210

Saving results

Total time: 44.42436337471008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033679
Iteration 2/25 | Loss: 0.01033679
Iteration 3/25 | Loss: 0.00248248
Iteration 4/25 | Loss: 0.00190422
Iteration 5/25 | Loss: 0.00185351
Iteration 6/25 | Loss: 0.00157873
Iteration 7/25 | Loss: 0.00137095
Iteration 8/25 | Loss: 0.00131658
Iteration 9/25 | Loss: 0.00128668
Iteration 10/25 | Loss: 0.00124171
Iteration 11/25 | Loss: 0.00119765
Iteration 12/25 | Loss: 0.00117948
Iteration 13/25 | Loss: 0.00117498
Iteration 14/25 | Loss: 0.00117609
Iteration 15/25 | Loss: 0.00117263
Iteration 16/25 | Loss: 0.00117065
Iteration 17/25 | Loss: 0.00117413
Iteration 18/25 | Loss: 0.00117388
Iteration 19/25 | Loss: 0.00117297
Iteration 20/25 | Loss: 0.00116911
Iteration 21/25 | Loss: 0.00116858
Iteration 22/25 | Loss: 0.00116839
Iteration 23/25 | Loss: 0.00117116
Iteration 24/25 | Loss: 0.00116646
Iteration 25/25 | Loss: 0.00116458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21345103
Iteration 2/25 | Loss: 0.00250169
Iteration 3/25 | Loss: 0.00250169
Iteration 4/25 | Loss: 0.00250169
Iteration 5/25 | Loss: 0.00250169
Iteration 6/25 | Loss: 0.00250169
Iteration 7/25 | Loss: 0.00250169
Iteration 8/25 | Loss: 0.00250169
Iteration 9/25 | Loss: 0.00250169
Iteration 10/25 | Loss: 0.00250169
Iteration 11/25 | Loss: 0.00250169
Iteration 12/25 | Loss: 0.00250169
Iteration 13/25 | Loss: 0.00250169
Iteration 14/25 | Loss: 0.00250169
Iteration 15/25 | Loss: 0.00250169
Iteration 16/25 | Loss: 0.00250169
Iteration 17/25 | Loss: 0.00250169
Iteration 18/25 | Loss: 0.00250169
Iteration 19/25 | Loss: 0.00250169
Iteration 20/25 | Loss: 0.00250169
Iteration 21/25 | Loss: 0.00250169
Iteration 22/25 | Loss: 0.00250169
Iteration 23/25 | Loss: 0.00250169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00250169076025486, 0.00250169076025486, 0.00250169076025486, 0.00250169076025486, 0.00250169076025486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00250169076025486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250169
Iteration 2/1000 | Loss: 0.00023400
Iteration 3/1000 | Loss: 0.00016066
Iteration 4/1000 | Loss: 0.00012531
Iteration 5/1000 | Loss: 0.00011301
Iteration 6/1000 | Loss: 0.00010132
Iteration 7/1000 | Loss: 0.00009306
Iteration 8/1000 | Loss: 0.00008935
Iteration 9/1000 | Loss: 0.00008611
Iteration 10/1000 | Loss: 0.00008433
Iteration 11/1000 | Loss: 0.00008247
Iteration 12/1000 | Loss: 0.00008159
Iteration 13/1000 | Loss: 0.00008092
Iteration 14/1000 | Loss: 0.00008038
Iteration 15/1000 | Loss: 0.00008004
Iteration 16/1000 | Loss: 0.00007981
Iteration 17/1000 | Loss: 0.00007958
Iteration 18/1000 | Loss: 0.00007944
Iteration 19/1000 | Loss: 0.00007939
Iteration 20/1000 | Loss: 0.00007937
Iteration 21/1000 | Loss: 0.00007936
Iteration 22/1000 | Loss: 0.00007928
Iteration 23/1000 | Loss: 0.00007920
Iteration 24/1000 | Loss: 0.00007919
Iteration 25/1000 | Loss: 0.00007912
Iteration 26/1000 | Loss: 0.00007909
Iteration 27/1000 | Loss: 0.00007908
Iteration 28/1000 | Loss: 0.00007908
Iteration 29/1000 | Loss: 0.00007908
Iteration 30/1000 | Loss: 0.00007908
Iteration 31/1000 | Loss: 0.00007906
Iteration 32/1000 | Loss: 0.00007906
Iteration 33/1000 | Loss: 0.00007906
Iteration 34/1000 | Loss: 0.00007906
Iteration 35/1000 | Loss: 0.00007906
Iteration 36/1000 | Loss: 0.00007906
Iteration 37/1000 | Loss: 0.00007906
Iteration 38/1000 | Loss: 0.00007906
Iteration 39/1000 | Loss: 0.00007906
Iteration 40/1000 | Loss: 0.00007906
Iteration 41/1000 | Loss: 0.00007906
Iteration 42/1000 | Loss: 0.00007905
Iteration 43/1000 | Loss: 0.00007905
Iteration 44/1000 | Loss: 0.00007905
Iteration 45/1000 | Loss: 0.00007905
Iteration 46/1000 | Loss: 0.00007905
Iteration 47/1000 | Loss: 0.00007904
Iteration 48/1000 | Loss: 0.00007904
Iteration 49/1000 | Loss: 0.00007904
Iteration 50/1000 | Loss: 0.00007904
Iteration 51/1000 | Loss: 0.00007904
Iteration 52/1000 | Loss: 0.00007904
Iteration 53/1000 | Loss: 0.00007903
Iteration 54/1000 | Loss: 0.00007903
Iteration 55/1000 | Loss: 0.00007903
Iteration 56/1000 | Loss: 0.00007903
Iteration 57/1000 | Loss: 0.00007903
Iteration 58/1000 | Loss: 0.00007903
Iteration 59/1000 | Loss: 0.00007902
Iteration 60/1000 | Loss: 0.00007902
Iteration 61/1000 | Loss: 0.00007902
Iteration 62/1000 | Loss: 0.00007902
Iteration 63/1000 | Loss: 0.00007902
Iteration 64/1000 | Loss: 0.00007902
Iteration 65/1000 | Loss: 0.00007902
Iteration 66/1000 | Loss: 0.00007902
Iteration 67/1000 | Loss: 0.00007902
Iteration 68/1000 | Loss: 0.00007901
Iteration 69/1000 | Loss: 0.00007901
Iteration 70/1000 | Loss: 0.00007901
Iteration 71/1000 | Loss: 0.00007901
Iteration 72/1000 | Loss: 0.00007901
Iteration 73/1000 | Loss: 0.00007901
Iteration 74/1000 | Loss: 0.00007901
Iteration 75/1000 | Loss: 0.00007901
Iteration 76/1000 | Loss: 0.00007901
Iteration 77/1000 | Loss: 0.00007901
Iteration 78/1000 | Loss: 0.00007901
Iteration 79/1000 | Loss: 0.00007901
Iteration 80/1000 | Loss: 0.00007900
Iteration 81/1000 | Loss: 0.00007900
Iteration 82/1000 | Loss: 0.00007900
Iteration 83/1000 | Loss: 0.00007900
Iteration 84/1000 | Loss: 0.00007900
Iteration 85/1000 | Loss: 0.00007900
Iteration 86/1000 | Loss: 0.00007900
Iteration 87/1000 | Loss: 0.00007899
Iteration 88/1000 | Loss: 0.00007899
Iteration 89/1000 | Loss: 0.00007899
Iteration 90/1000 | Loss: 0.00007899
Iteration 91/1000 | Loss: 0.00007899
Iteration 92/1000 | Loss: 0.00007899
Iteration 93/1000 | Loss: 0.00007898
Iteration 94/1000 | Loss: 0.00007898
Iteration 95/1000 | Loss: 0.00007898
Iteration 96/1000 | Loss: 0.00007898
Iteration 97/1000 | Loss: 0.00007898
Iteration 98/1000 | Loss: 0.00007898
Iteration 99/1000 | Loss: 0.00007898
Iteration 100/1000 | Loss: 0.00007898
Iteration 101/1000 | Loss: 0.00007898
Iteration 102/1000 | Loss: 0.00007898
Iteration 103/1000 | Loss: 0.00007898
Iteration 104/1000 | Loss: 0.00007898
Iteration 105/1000 | Loss: 0.00007898
Iteration 106/1000 | Loss: 0.00007898
Iteration 107/1000 | Loss: 0.00007898
Iteration 108/1000 | Loss: 0.00007898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [7.898172771092504e-05, 7.898172771092504e-05, 7.898172771092504e-05, 7.898172771092504e-05, 7.898172771092504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.898172771092504e-05

Optimization complete. Final v2v error: 4.270393371582031 mm

Highest mean error: 12.1115140914917 mm for frame 3

Lowest mean error: 2.867739200592041 mm for frame 146

Saving results

Total time: 88.76492810249329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997145
Iteration 2/25 | Loss: 0.00168464
Iteration 3/25 | Loss: 0.00122857
Iteration 4/25 | Loss: 0.00120556
Iteration 5/25 | Loss: 0.00119925
Iteration 6/25 | Loss: 0.00119736
Iteration 7/25 | Loss: 0.00119736
Iteration 8/25 | Loss: 0.00119736
Iteration 9/25 | Loss: 0.00119736
Iteration 10/25 | Loss: 0.00119736
Iteration 11/25 | Loss: 0.00119736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001197362900711596, 0.001197362900711596, 0.001197362900711596, 0.001197362900711596, 0.001197362900711596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001197362900711596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78802735
Iteration 2/25 | Loss: 0.00149112
Iteration 3/25 | Loss: 0.00149111
Iteration 4/25 | Loss: 0.00149111
Iteration 5/25 | Loss: 0.00149111
Iteration 6/25 | Loss: 0.00149111
Iteration 7/25 | Loss: 0.00149111
Iteration 8/25 | Loss: 0.00149111
Iteration 9/25 | Loss: 0.00149111
Iteration 10/25 | Loss: 0.00149111
Iteration 11/25 | Loss: 0.00149111
Iteration 12/25 | Loss: 0.00149111
Iteration 13/25 | Loss: 0.00149111
Iteration 14/25 | Loss: 0.00149111
Iteration 15/25 | Loss: 0.00149111
Iteration 16/25 | Loss: 0.00149111
Iteration 17/25 | Loss: 0.00149111
Iteration 18/25 | Loss: 0.00149111
Iteration 19/25 | Loss: 0.00149111
Iteration 20/25 | Loss: 0.00149111
Iteration 21/25 | Loss: 0.00149111
Iteration 22/25 | Loss: 0.00149111
Iteration 23/25 | Loss: 0.00149111
Iteration 24/25 | Loss: 0.00149111
Iteration 25/25 | Loss: 0.00149111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149111
Iteration 2/1000 | Loss: 0.00006999
Iteration 3/1000 | Loss: 0.00004597
Iteration 4/1000 | Loss: 0.00003760
Iteration 5/1000 | Loss: 0.00003532
Iteration 6/1000 | Loss: 0.00003409
Iteration 7/1000 | Loss: 0.00003348
Iteration 8/1000 | Loss: 0.00003277
Iteration 9/1000 | Loss: 0.00003228
Iteration 10/1000 | Loss: 0.00003203
Iteration 11/1000 | Loss: 0.00003170
Iteration 12/1000 | Loss: 0.00003150
Iteration 13/1000 | Loss: 0.00003128
Iteration 14/1000 | Loss: 0.00003104
Iteration 15/1000 | Loss: 0.00003080
Iteration 16/1000 | Loss: 0.00003060
Iteration 17/1000 | Loss: 0.00003037
Iteration 18/1000 | Loss: 0.00003021
Iteration 19/1000 | Loss: 0.00003013
Iteration 20/1000 | Loss: 0.00003009
Iteration 21/1000 | Loss: 0.00003001
Iteration 22/1000 | Loss: 0.00002993
Iteration 23/1000 | Loss: 0.00002991
Iteration 24/1000 | Loss: 0.00002984
Iteration 25/1000 | Loss: 0.00002980
Iteration 26/1000 | Loss: 0.00002975
Iteration 27/1000 | Loss: 0.00002974
Iteration 28/1000 | Loss: 0.00002973
Iteration 29/1000 | Loss: 0.00002972
Iteration 30/1000 | Loss: 0.00002972
Iteration 31/1000 | Loss: 0.00002971
Iteration 32/1000 | Loss: 0.00002970
Iteration 33/1000 | Loss: 0.00002970
Iteration 34/1000 | Loss: 0.00002969
Iteration 35/1000 | Loss: 0.00002968
Iteration 36/1000 | Loss: 0.00002968
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00002966
Iteration 40/1000 | Loss: 0.00002966
Iteration 41/1000 | Loss: 0.00002966
Iteration 42/1000 | Loss: 0.00002965
Iteration 43/1000 | Loss: 0.00002964
Iteration 44/1000 | Loss: 0.00002961
Iteration 45/1000 | Loss: 0.00002961
Iteration 46/1000 | Loss: 0.00002961
Iteration 47/1000 | Loss: 0.00002961
Iteration 48/1000 | Loss: 0.00002961
Iteration 49/1000 | Loss: 0.00002961
Iteration 50/1000 | Loss: 0.00002961
Iteration 51/1000 | Loss: 0.00002961
Iteration 52/1000 | Loss: 0.00002961
Iteration 53/1000 | Loss: 0.00002960
Iteration 54/1000 | Loss: 0.00002960
Iteration 55/1000 | Loss: 0.00002959
Iteration 56/1000 | Loss: 0.00002959
Iteration 57/1000 | Loss: 0.00002959
Iteration 58/1000 | Loss: 0.00002959
Iteration 59/1000 | Loss: 0.00002959
Iteration 60/1000 | Loss: 0.00002959
Iteration 61/1000 | Loss: 0.00002959
Iteration 62/1000 | Loss: 0.00002958
Iteration 63/1000 | Loss: 0.00002958
Iteration 64/1000 | Loss: 0.00002958
Iteration 65/1000 | Loss: 0.00002958
Iteration 66/1000 | Loss: 0.00002958
Iteration 67/1000 | Loss: 0.00002957
Iteration 68/1000 | Loss: 0.00002957
Iteration 69/1000 | Loss: 0.00002957
Iteration 70/1000 | Loss: 0.00002957
Iteration 71/1000 | Loss: 0.00002956
Iteration 72/1000 | Loss: 0.00002956
Iteration 73/1000 | Loss: 0.00002956
Iteration 74/1000 | Loss: 0.00002956
Iteration 75/1000 | Loss: 0.00002956
Iteration 76/1000 | Loss: 0.00002956
Iteration 77/1000 | Loss: 0.00002955
Iteration 78/1000 | Loss: 0.00002955
Iteration 79/1000 | Loss: 0.00002955
Iteration 80/1000 | Loss: 0.00002955
Iteration 81/1000 | Loss: 0.00002955
Iteration 82/1000 | Loss: 0.00002954
Iteration 83/1000 | Loss: 0.00002954
Iteration 84/1000 | Loss: 0.00002954
Iteration 85/1000 | Loss: 0.00002954
Iteration 86/1000 | Loss: 0.00002954
Iteration 87/1000 | Loss: 0.00002954
Iteration 88/1000 | Loss: 0.00002954
Iteration 89/1000 | Loss: 0.00002954
Iteration 90/1000 | Loss: 0.00002954
Iteration 91/1000 | Loss: 0.00002954
Iteration 92/1000 | Loss: 0.00002953
Iteration 93/1000 | Loss: 0.00002953
Iteration 94/1000 | Loss: 0.00002953
Iteration 95/1000 | Loss: 0.00002953
Iteration 96/1000 | Loss: 0.00002953
Iteration 97/1000 | Loss: 0.00002953
Iteration 98/1000 | Loss: 0.00002953
Iteration 99/1000 | Loss: 0.00002953
Iteration 100/1000 | Loss: 0.00002952
Iteration 101/1000 | Loss: 0.00002952
Iteration 102/1000 | Loss: 0.00002952
Iteration 103/1000 | Loss: 0.00002952
Iteration 104/1000 | Loss: 0.00002952
Iteration 105/1000 | Loss: 0.00002952
Iteration 106/1000 | Loss: 0.00002952
Iteration 107/1000 | Loss: 0.00002952
Iteration 108/1000 | Loss: 0.00002952
Iteration 109/1000 | Loss: 0.00002952
Iteration 110/1000 | Loss: 0.00002952
Iteration 111/1000 | Loss: 0.00002951
Iteration 112/1000 | Loss: 0.00002951
Iteration 113/1000 | Loss: 0.00002951
Iteration 114/1000 | Loss: 0.00002951
Iteration 115/1000 | Loss: 0.00002951
Iteration 116/1000 | Loss: 0.00002951
Iteration 117/1000 | Loss: 0.00002951
Iteration 118/1000 | Loss: 0.00002951
Iteration 119/1000 | Loss: 0.00002951
Iteration 120/1000 | Loss: 0.00002951
Iteration 121/1000 | Loss: 0.00002951
Iteration 122/1000 | Loss: 0.00002951
Iteration 123/1000 | Loss: 0.00002951
Iteration 124/1000 | Loss: 0.00002951
Iteration 125/1000 | Loss: 0.00002951
Iteration 126/1000 | Loss: 0.00002950
Iteration 127/1000 | Loss: 0.00002950
Iteration 128/1000 | Loss: 0.00002950
Iteration 129/1000 | Loss: 0.00002950
Iteration 130/1000 | Loss: 0.00002950
Iteration 131/1000 | Loss: 0.00002950
Iteration 132/1000 | Loss: 0.00002950
Iteration 133/1000 | Loss: 0.00002950
Iteration 134/1000 | Loss: 0.00002950
Iteration 135/1000 | Loss: 0.00002950
Iteration 136/1000 | Loss: 0.00002950
Iteration 137/1000 | Loss: 0.00002950
Iteration 138/1000 | Loss: 0.00002950
Iteration 139/1000 | Loss: 0.00002950
Iteration 140/1000 | Loss: 0.00002950
Iteration 141/1000 | Loss: 0.00002950
Iteration 142/1000 | Loss: 0.00002950
Iteration 143/1000 | Loss: 0.00002950
Iteration 144/1000 | Loss: 0.00002950
Iteration 145/1000 | Loss: 0.00002950
Iteration 146/1000 | Loss: 0.00002950
Iteration 147/1000 | Loss: 0.00002950
Iteration 148/1000 | Loss: 0.00002950
Iteration 149/1000 | Loss: 0.00002950
Iteration 150/1000 | Loss: 0.00002950
Iteration 151/1000 | Loss: 0.00002950
Iteration 152/1000 | Loss: 0.00002950
Iteration 153/1000 | Loss: 0.00002950
Iteration 154/1000 | Loss: 0.00002950
Iteration 155/1000 | Loss: 0.00002950
Iteration 156/1000 | Loss: 0.00002950
Iteration 157/1000 | Loss: 0.00002950
Iteration 158/1000 | Loss: 0.00002950
Iteration 159/1000 | Loss: 0.00002950
Iteration 160/1000 | Loss: 0.00002950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.949638474092353e-05, 2.949638474092353e-05, 2.949638474092353e-05, 2.949638474092353e-05, 2.949638474092353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.949638474092353e-05

Optimization complete. Final v2v error: 4.215845108032227 mm

Highest mean error: 5.224395751953125 mm for frame 79

Lowest mean error: 3.0578200817108154 mm for frame 29

Saving results

Total time: 50.688273429870605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031099
Iteration 2/25 | Loss: 0.00248384
Iteration 3/25 | Loss: 0.00187443
Iteration 4/25 | Loss: 0.00169698
Iteration 5/25 | Loss: 0.00153505
Iteration 6/25 | Loss: 0.00135245
Iteration 7/25 | Loss: 0.00121405
Iteration 8/25 | Loss: 0.00117673
Iteration 9/25 | Loss: 0.00115543
Iteration 10/25 | Loss: 0.00112620
Iteration 11/25 | Loss: 0.00111009
Iteration 12/25 | Loss: 0.00110443
Iteration 13/25 | Loss: 0.00109931
Iteration 14/25 | Loss: 0.00109819
Iteration 15/25 | Loss: 0.00109790
Iteration 16/25 | Loss: 0.00109787
Iteration 17/25 | Loss: 0.00109786
Iteration 18/25 | Loss: 0.00109786
Iteration 19/25 | Loss: 0.00109782
Iteration 20/25 | Loss: 0.00109782
Iteration 21/25 | Loss: 0.00109782
Iteration 22/25 | Loss: 0.00109781
Iteration 23/25 | Loss: 0.00109781
Iteration 24/25 | Loss: 0.00109781
Iteration 25/25 | Loss: 0.00109781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15642881
Iteration 2/25 | Loss: 0.00091858
Iteration 3/25 | Loss: 0.00091857
Iteration 4/25 | Loss: 0.00091857
Iteration 5/25 | Loss: 0.00091857
Iteration 6/25 | Loss: 0.00091857
Iteration 7/25 | Loss: 0.00091857
Iteration 8/25 | Loss: 0.00091857
Iteration 9/25 | Loss: 0.00091857
Iteration 10/25 | Loss: 0.00091857
Iteration 11/25 | Loss: 0.00091857
Iteration 12/25 | Loss: 0.00091857
Iteration 13/25 | Loss: 0.00091857
Iteration 14/25 | Loss: 0.00091857
Iteration 15/25 | Loss: 0.00091857
Iteration 16/25 | Loss: 0.00091857
Iteration 17/25 | Loss: 0.00091857
Iteration 18/25 | Loss: 0.00091857
Iteration 19/25 | Loss: 0.00091857
Iteration 20/25 | Loss: 0.00091857
Iteration 21/25 | Loss: 0.00091857
Iteration 22/25 | Loss: 0.00091857
Iteration 23/25 | Loss: 0.00091857
Iteration 24/25 | Loss: 0.00091857
Iteration 25/25 | Loss: 0.00091857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009185699746012688, 0.0009185699746012688, 0.0009185699746012688, 0.0009185699746012688, 0.0009185699746012688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009185699746012688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091857
Iteration 2/1000 | Loss: 0.00004346
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00001778
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001431
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001313
Iteration 14/1000 | Loss: 0.00001301
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001296
Iteration 17/1000 | Loss: 0.00001295
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001293
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001289
Iteration 30/1000 | Loss: 0.00001289
Iteration 31/1000 | Loss: 0.00001289
Iteration 32/1000 | Loss: 0.00001289
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001284
Iteration 45/1000 | Loss: 0.00001284
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001283
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001281
Iteration 56/1000 | Loss: 0.00001281
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001277
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001275
Iteration 83/1000 | Loss: 0.00001275
Iteration 84/1000 | Loss: 0.00001275
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001275
Iteration 91/1000 | Loss: 0.00001274
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001273
Iteration 112/1000 | Loss: 0.00001273
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001271
Iteration 131/1000 | Loss: 0.00001271
Iteration 132/1000 | Loss: 0.00001271
Iteration 133/1000 | Loss: 0.00001271
Iteration 134/1000 | Loss: 0.00001271
Iteration 135/1000 | Loss: 0.00001271
Iteration 136/1000 | Loss: 0.00001271
Iteration 137/1000 | Loss: 0.00001271
Iteration 138/1000 | Loss: 0.00001270
Iteration 139/1000 | Loss: 0.00001270
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001270
Iteration 142/1000 | Loss: 0.00001270
Iteration 143/1000 | Loss: 0.00001270
Iteration 144/1000 | Loss: 0.00001269
Iteration 145/1000 | Loss: 0.00001269
Iteration 146/1000 | Loss: 0.00001269
Iteration 147/1000 | Loss: 0.00001269
Iteration 148/1000 | Loss: 0.00001269
Iteration 149/1000 | Loss: 0.00001269
Iteration 150/1000 | Loss: 0.00001269
Iteration 151/1000 | Loss: 0.00001269
Iteration 152/1000 | Loss: 0.00001269
Iteration 153/1000 | Loss: 0.00001269
Iteration 154/1000 | Loss: 0.00001269
Iteration 155/1000 | Loss: 0.00001269
Iteration 156/1000 | Loss: 0.00001268
Iteration 157/1000 | Loss: 0.00001268
Iteration 158/1000 | Loss: 0.00001268
Iteration 159/1000 | Loss: 0.00001268
Iteration 160/1000 | Loss: 0.00001268
Iteration 161/1000 | Loss: 0.00001268
Iteration 162/1000 | Loss: 0.00001268
Iteration 163/1000 | Loss: 0.00001268
Iteration 164/1000 | Loss: 0.00001268
Iteration 165/1000 | Loss: 0.00001268
Iteration 166/1000 | Loss: 0.00001268
Iteration 167/1000 | Loss: 0.00001268
Iteration 168/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2681509360845666e-05, 1.2681509360845666e-05, 1.2681509360845666e-05, 1.2681509360845666e-05, 1.2681509360845666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2681509360845666e-05

Optimization complete. Final v2v error: 2.9575698375701904 mm

Highest mean error: 3.599111557006836 mm for frame 11

Lowest mean error: 2.487891912460327 mm for frame 135

Saving results

Total time: 56.569737672805786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795945
Iteration 2/25 | Loss: 0.00128147
Iteration 3/25 | Loss: 0.00107886
Iteration 4/25 | Loss: 0.00106445
Iteration 5/25 | Loss: 0.00106276
Iteration 6/25 | Loss: 0.00106276
Iteration 7/25 | Loss: 0.00106276
Iteration 8/25 | Loss: 0.00106276
Iteration 9/25 | Loss: 0.00106276
Iteration 10/25 | Loss: 0.00106276
Iteration 11/25 | Loss: 0.00106276
Iteration 12/25 | Loss: 0.00106276
Iteration 13/25 | Loss: 0.00106276
Iteration 14/25 | Loss: 0.00106276
Iteration 15/25 | Loss: 0.00106276
Iteration 16/25 | Loss: 0.00106276
Iteration 17/25 | Loss: 0.00106276
Iteration 18/25 | Loss: 0.00106276
Iteration 19/25 | Loss: 0.00106276
Iteration 20/25 | Loss: 0.00106276
Iteration 21/25 | Loss: 0.00106276
Iteration 22/25 | Loss: 0.00106276
Iteration 23/25 | Loss: 0.00106276
Iteration 24/25 | Loss: 0.00106276
Iteration 25/25 | Loss: 0.00106276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010627578012645245, 0.0010627578012645245, 0.0010627578012645245, 0.0010627578012645245, 0.0010627578012645245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010627578012645245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22571957
Iteration 2/25 | Loss: 0.00110152
Iteration 3/25 | Loss: 0.00110151
Iteration 4/25 | Loss: 0.00110151
Iteration 5/25 | Loss: 0.00110151
Iteration 6/25 | Loss: 0.00110151
Iteration 7/25 | Loss: 0.00110151
Iteration 8/25 | Loss: 0.00110151
Iteration 9/25 | Loss: 0.00110151
Iteration 10/25 | Loss: 0.00110151
Iteration 11/25 | Loss: 0.00110151
Iteration 12/25 | Loss: 0.00110151
Iteration 13/25 | Loss: 0.00110151
Iteration 14/25 | Loss: 0.00110151
Iteration 15/25 | Loss: 0.00110151
Iteration 16/25 | Loss: 0.00110151
Iteration 17/25 | Loss: 0.00110151
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001101509784348309, 0.001101509784348309, 0.001101509784348309, 0.001101509784348309, 0.001101509784348309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001101509784348309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110151
Iteration 2/1000 | Loss: 0.00002280
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001380
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001277
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001207
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001197
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001174
Iteration 56/1000 | Loss: 0.00001174
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001173
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.1731979611795396e-05, 1.1731979611795396e-05, 1.1731979611795396e-05, 1.1731979611795396e-05, 1.1731979611795396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1731979611795396e-05

Optimization complete. Final v2v error: 2.904665231704712 mm

Highest mean error: 3.3378677368164062 mm for frame 57

Lowest mean error: 2.5145576000213623 mm for frame 239

Saving results

Total time: 29.9274799823761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610824
Iteration 2/25 | Loss: 0.00123986
Iteration 3/25 | Loss: 0.00115608
Iteration 4/25 | Loss: 0.00114184
Iteration 5/25 | Loss: 0.00113918
Iteration 6/25 | Loss: 0.00113865
Iteration 7/25 | Loss: 0.00113865
Iteration 8/25 | Loss: 0.00113865
Iteration 9/25 | Loss: 0.00113865
Iteration 10/25 | Loss: 0.00113865
Iteration 11/25 | Loss: 0.00113865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011386496480554342, 0.0011386496480554342, 0.0011386496480554342, 0.0011386496480554342, 0.0011386496480554342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011386496480554342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64011848
Iteration 2/25 | Loss: 0.00163322
Iteration 3/25 | Loss: 0.00163320
Iteration 4/25 | Loss: 0.00163320
Iteration 5/25 | Loss: 0.00163320
Iteration 6/25 | Loss: 0.00163320
Iteration 7/25 | Loss: 0.00163320
Iteration 8/25 | Loss: 0.00163320
Iteration 9/25 | Loss: 0.00163320
Iteration 10/25 | Loss: 0.00163320
Iteration 11/25 | Loss: 0.00163320
Iteration 12/25 | Loss: 0.00163320
Iteration 13/25 | Loss: 0.00163320
Iteration 14/25 | Loss: 0.00163320
Iteration 15/25 | Loss: 0.00163320
Iteration 16/25 | Loss: 0.00163320
Iteration 17/25 | Loss: 0.00163320
Iteration 18/25 | Loss: 0.00163320
Iteration 19/25 | Loss: 0.00163320
Iteration 20/25 | Loss: 0.00163320
Iteration 21/25 | Loss: 0.00163320
Iteration 22/25 | Loss: 0.00163320
Iteration 23/25 | Loss: 0.00163320
Iteration 24/25 | Loss: 0.00163320
Iteration 25/25 | Loss: 0.00163320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163320
Iteration 2/1000 | Loss: 0.00006344
Iteration 3/1000 | Loss: 0.00003769
Iteration 4/1000 | Loss: 0.00002564
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001812
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001761
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001712
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001708
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00001705
Iteration 32/1000 | Loss: 0.00001705
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001705
Iteration 36/1000 | Loss: 0.00001705
Iteration 37/1000 | Loss: 0.00001705
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001704
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001697
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.6943231457844377e-05, 1.6943231457844377e-05, 1.6943231457844377e-05, 1.6943231457844377e-05, 1.6943231457844377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6943231457844377e-05

Optimization complete. Final v2v error: 3.493759870529175 mm

Highest mean error: 4.081259250640869 mm for frame 0

Lowest mean error: 3.3557958602905273 mm for frame 9

Saving results

Total time: 35.73680806159973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_us_2588/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_us_2588/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984592
Iteration 2/25 | Loss: 0.00137605
Iteration 3/25 | Loss: 0.00106431
Iteration 4/25 | Loss: 0.00104358
Iteration 5/25 | Loss: 0.00103828
Iteration 6/25 | Loss: 0.00103654
Iteration 7/25 | Loss: 0.00103609
Iteration 8/25 | Loss: 0.00103609
Iteration 9/25 | Loss: 0.00103609
Iteration 10/25 | Loss: 0.00103609
Iteration 11/25 | Loss: 0.00103609
Iteration 12/25 | Loss: 0.00103609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010360896121710539, 0.0010360896121710539, 0.0010360896121710539, 0.0010360896121710539, 0.0010360896121710539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010360896121710539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53445601
Iteration 2/25 | Loss: 0.00160157
Iteration 3/25 | Loss: 0.00160157
Iteration 4/25 | Loss: 0.00160157
Iteration 5/25 | Loss: 0.00160157
Iteration 6/25 | Loss: 0.00160157
Iteration 7/25 | Loss: 0.00160157
Iteration 8/25 | Loss: 0.00160157
Iteration 9/25 | Loss: 0.00160157
Iteration 10/25 | Loss: 0.00160157
Iteration 11/25 | Loss: 0.00160157
Iteration 12/25 | Loss: 0.00160157
Iteration 13/25 | Loss: 0.00160157
Iteration 14/25 | Loss: 0.00160157
Iteration 15/25 | Loss: 0.00160157
Iteration 16/25 | Loss: 0.00160157
Iteration 17/25 | Loss: 0.00160157
Iteration 18/25 | Loss: 0.00160157
Iteration 19/25 | Loss: 0.00160157
Iteration 20/25 | Loss: 0.00160157
Iteration 21/25 | Loss: 0.00160157
Iteration 22/25 | Loss: 0.00160157
Iteration 23/25 | Loss: 0.00160157
Iteration 24/25 | Loss: 0.00160157
Iteration 25/25 | Loss: 0.00160157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016015662113204598, 0.0016015662113204598, 0.0016015662113204598, 0.0016015662113204598, 0.0016015662113204598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016015662113204598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160157
Iteration 2/1000 | Loss: 0.00001772
Iteration 3/1000 | Loss: 0.00001099
Iteration 4/1000 | Loss: 0.00000945
Iteration 5/1000 | Loss: 0.00000873
Iteration 6/1000 | Loss: 0.00000840
Iteration 7/1000 | Loss: 0.00000824
Iteration 8/1000 | Loss: 0.00000808
Iteration 9/1000 | Loss: 0.00000804
Iteration 10/1000 | Loss: 0.00000803
Iteration 11/1000 | Loss: 0.00000803
Iteration 12/1000 | Loss: 0.00000803
Iteration 13/1000 | Loss: 0.00000802
Iteration 14/1000 | Loss: 0.00000802
Iteration 15/1000 | Loss: 0.00000798
Iteration 16/1000 | Loss: 0.00000798
Iteration 17/1000 | Loss: 0.00000798
Iteration 18/1000 | Loss: 0.00000798
Iteration 19/1000 | Loss: 0.00000798
Iteration 20/1000 | Loss: 0.00000798
Iteration 21/1000 | Loss: 0.00000798
Iteration 22/1000 | Loss: 0.00000797
Iteration 23/1000 | Loss: 0.00000797
Iteration 24/1000 | Loss: 0.00000796
Iteration 25/1000 | Loss: 0.00000796
Iteration 26/1000 | Loss: 0.00000796
Iteration 27/1000 | Loss: 0.00000796
Iteration 28/1000 | Loss: 0.00000795
Iteration 29/1000 | Loss: 0.00000795
Iteration 30/1000 | Loss: 0.00000795
Iteration 31/1000 | Loss: 0.00000795
Iteration 32/1000 | Loss: 0.00000794
Iteration 33/1000 | Loss: 0.00000794
Iteration 34/1000 | Loss: 0.00000794
Iteration 35/1000 | Loss: 0.00000794
Iteration 36/1000 | Loss: 0.00000794
Iteration 37/1000 | Loss: 0.00000794
Iteration 38/1000 | Loss: 0.00000794
Iteration 39/1000 | Loss: 0.00000794
Iteration 40/1000 | Loss: 0.00000794
Iteration 41/1000 | Loss: 0.00000793
Iteration 42/1000 | Loss: 0.00000793
Iteration 43/1000 | Loss: 0.00000793
Iteration 44/1000 | Loss: 0.00000793
Iteration 45/1000 | Loss: 0.00000793
Iteration 46/1000 | Loss: 0.00000793
Iteration 47/1000 | Loss: 0.00000793
Iteration 48/1000 | Loss: 0.00000793
Iteration 49/1000 | Loss: 0.00000793
Iteration 50/1000 | Loss: 0.00000792
Iteration 51/1000 | Loss: 0.00000792
Iteration 52/1000 | Loss: 0.00000792
Iteration 53/1000 | Loss: 0.00000792
Iteration 54/1000 | Loss: 0.00000792
Iteration 55/1000 | Loss: 0.00000792
Iteration 56/1000 | Loss: 0.00000792
Iteration 57/1000 | Loss: 0.00000792
Iteration 58/1000 | Loss: 0.00000792
Iteration 59/1000 | Loss: 0.00000792
Iteration 60/1000 | Loss: 0.00000792
Iteration 61/1000 | Loss: 0.00000792
Iteration 62/1000 | Loss: 0.00000791
Iteration 63/1000 | Loss: 0.00000791
Iteration 64/1000 | Loss: 0.00000791
Iteration 65/1000 | Loss: 0.00000790
Iteration 66/1000 | Loss: 0.00000790
Iteration 67/1000 | Loss: 0.00000790
Iteration 68/1000 | Loss: 0.00000790
Iteration 69/1000 | Loss: 0.00000790
Iteration 70/1000 | Loss: 0.00000790
Iteration 71/1000 | Loss: 0.00000790
Iteration 72/1000 | Loss: 0.00000790
Iteration 73/1000 | Loss: 0.00000789
Iteration 74/1000 | Loss: 0.00000789
Iteration 75/1000 | Loss: 0.00000789
Iteration 76/1000 | Loss: 0.00000789
Iteration 77/1000 | Loss: 0.00000789
Iteration 78/1000 | Loss: 0.00000789
Iteration 79/1000 | Loss: 0.00000789
Iteration 80/1000 | Loss: 0.00000789
Iteration 81/1000 | Loss: 0.00000789
Iteration 82/1000 | Loss: 0.00000789
Iteration 83/1000 | Loss: 0.00000789
Iteration 84/1000 | Loss: 0.00000789
Iteration 85/1000 | Loss: 0.00000789
Iteration 86/1000 | Loss: 0.00000789
Iteration 87/1000 | Loss: 0.00000789
Iteration 88/1000 | Loss: 0.00000789
Iteration 89/1000 | Loss: 0.00000789
Iteration 90/1000 | Loss: 0.00000789
Iteration 91/1000 | Loss: 0.00000789
Iteration 92/1000 | Loss: 0.00000789
Iteration 93/1000 | Loss: 0.00000789
Iteration 94/1000 | Loss: 0.00000789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [7.890917913755402e-06, 7.890917913755402e-06, 7.890917913755402e-06, 7.890917913755402e-06, 7.890917913755402e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.890917913755402e-06

Optimization complete. Final v2v error: 2.3689417839050293 mm

Highest mean error: 2.635570764541626 mm for frame 120

Lowest mean error: 2.120509624481201 mm for frame 1

Saving results

Total time: 25.839552879333496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871710
Iteration 2/25 | Loss: 0.00195372
Iteration 3/25 | Loss: 0.00130198
Iteration 4/25 | Loss: 0.00120496
Iteration 5/25 | Loss: 0.00120073
Iteration 6/25 | Loss: 0.00120073
Iteration 7/25 | Loss: 0.00120073
Iteration 8/25 | Loss: 0.00120073
Iteration 9/25 | Loss: 0.00120073
Iteration 10/25 | Loss: 0.00120073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012007324257865548, 0.0012007324257865548, 0.0012007324257865548, 0.0012007324257865548, 0.0012007324257865548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012007324257865548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20148289
Iteration 2/25 | Loss: 0.00081799
Iteration 3/25 | Loss: 0.00081799
Iteration 4/25 | Loss: 0.00081799
Iteration 5/25 | Loss: 0.00081799
Iteration 6/25 | Loss: 0.00081799
Iteration 7/25 | Loss: 0.00081799
Iteration 8/25 | Loss: 0.00081799
Iteration 9/25 | Loss: 0.00081799
Iteration 10/25 | Loss: 0.00081799
Iteration 11/25 | Loss: 0.00081799
Iteration 12/25 | Loss: 0.00081799
Iteration 13/25 | Loss: 0.00081799
Iteration 14/25 | Loss: 0.00081799
Iteration 15/25 | Loss: 0.00081799
Iteration 16/25 | Loss: 0.00081799
Iteration 17/25 | Loss: 0.00081799
Iteration 18/25 | Loss: 0.00081799
Iteration 19/25 | Loss: 0.00081799
Iteration 20/25 | Loss: 0.00081799
Iteration 21/25 | Loss: 0.00081799
Iteration 22/25 | Loss: 0.00081799
Iteration 23/25 | Loss: 0.00081799
Iteration 24/25 | Loss: 0.00081799
Iteration 25/25 | Loss: 0.00081799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081799
Iteration 2/1000 | Loss: 0.00006185
Iteration 3/1000 | Loss: 0.00003558
Iteration 4/1000 | Loss: 0.00002581
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002212
Iteration 7/1000 | Loss: 0.00002109
Iteration 8/1000 | Loss: 0.00002047
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001866
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001864
Iteration 110/1000 | Loss: 0.00001864
Iteration 111/1000 | Loss: 0.00001864
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001864
Iteration 114/1000 | Loss: 0.00001864
Iteration 115/1000 | Loss: 0.00001864
Iteration 116/1000 | Loss: 0.00001864
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001864
Iteration 122/1000 | Loss: 0.00001864
Iteration 123/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.863742363639176e-05, 1.863742363639176e-05, 1.863742363639176e-05, 1.863742363639176e-05, 1.863742363639176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.863742363639176e-05

Optimization complete. Final v2v error: 3.624865770339966 mm

Highest mean error: 3.9619133472442627 mm for frame 99

Lowest mean error: 3.068085193634033 mm for frame 0

Saving results

Total time: 38.2157986164093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703579
Iteration 2/25 | Loss: 0.00136779
Iteration 3/25 | Loss: 0.00117980
Iteration 4/25 | Loss: 0.00105164
Iteration 5/25 | Loss: 0.00104162
Iteration 6/25 | Loss: 0.00103078
Iteration 7/25 | Loss: 0.00102860
Iteration 8/25 | Loss: 0.00102809
Iteration 9/25 | Loss: 0.00102793
Iteration 10/25 | Loss: 0.00102785
Iteration 11/25 | Loss: 0.00102782
Iteration 12/25 | Loss: 0.00102782
Iteration 13/25 | Loss: 0.00102781
Iteration 14/25 | Loss: 0.00102781
Iteration 15/25 | Loss: 0.00102781
Iteration 16/25 | Loss: 0.00102781
Iteration 17/25 | Loss: 0.00102781
Iteration 18/25 | Loss: 0.00102781
Iteration 19/25 | Loss: 0.00102781
Iteration 20/25 | Loss: 0.00102781
Iteration 21/25 | Loss: 0.00102781
Iteration 22/25 | Loss: 0.00102781
Iteration 23/25 | Loss: 0.00102781
Iteration 24/25 | Loss: 0.00102781
Iteration 25/25 | Loss: 0.00102781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61776721
Iteration 2/25 | Loss: 0.00171813
Iteration 3/25 | Loss: 0.00171812
Iteration 4/25 | Loss: 0.00171812
Iteration 5/25 | Loss: 0.00171812
Iteration 6/25 | Loss: 0.00171812
Iteration 7/25 | Loss: 0.00171812
Iteration 8/25 | Loss: 0.00171812
Iteration 9/25 | Loss: 0.00171812
Iteration 10/25 | Loss: 0.00171812
Iteration 11/25 | Loss: 0.00171812
Iteration 12/25 | Loss: 0.00171812
Iteration 13/25 | Loss: 0.00171812
Iteration 14/25 | Loss: 0.00171812
Iteration 15/25 | Loss: 0.00171812
Iteration 16/25 | Loss: 0.00171812
Iteration 17/25 | Loss: 0.00171812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017181214643642306, 0.0017181214643642306, 0.0017181214643642306, 0.0017181214643642306, 0.0017181214643642306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017181214643642306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171812
Iteration 2/1000 | Loss: 0.00002470
Iteration 3/1000 | Loss: 0.00001693
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001308
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001296
Iteration 22/1000 | Loss: 0.00001295
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001294
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001293
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001291
Iteration 30/1000 | Loss: 0.00001290
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001284
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001284
Iteration 73/1000 | Loss: 0.00001284
Iteration 74/1000 | Loss: 0.00001284
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001283
Iteration 79/1000 | Loss: 0.00001283
Iteration 80/1000 | Loss: 0.00001283
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001282
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001280
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001278
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001277
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001276
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001276
Iteration 155/1000 | Loss: 0.00001276
Iteration 156/1000 | Loss: 0.00001276
Iteration 157/1000 | Loss: 0.00001276
Iteration 158/1000 | Loss: 0.00001276
Iteration 159/1000 | Loss: 0.00001275
Iteration 160/1000 | Loss: 0.00001275
Iteration 161/1000 | Loss: 0.00001275
Iteration 162/1000 | Loss: 0.00001275
Iteration 163/1000 | Loss: 0.00001275
Iteration 164/1000 | Loss: 0.00001275
Iteration 165/1000 | Loss: 0.00001275
Iteration 166/1000 | Loss: 0.00001275
Iteration 167/1000 | Loss: 0.00001275
Iteration 168/1000 | Loss: 0.00001275
Iteration 169/1000 | Loss: 0.00001275
Iteration 170/1000 | Loss: 0.00001275
Iteration 171/1000 | Loss: 0.00001275
Iteration 172/1000 | Loss: 0.00001274
Iteration 173/1000 | Loss: 0.00001274
Iteration 174/1000 | Loss: 0.00001274
Iteration 175/1000 | Loss: 0.00001274
Iteration 176/1000 | Loss: 0.00001274
Iteration 177/1000 | Loss: 0.00001274
Iteration 178/1000 | Loss: 0.00001274
Iteration 179/1000 | Loss: 0.00001274
Iteration 180/1000 | Loss: 0.00001274
Iteration 181/1000 | Loss: 0.00001274
Iteration 182/1000 | Loss: 0.00001274
Iteration 183/1000 | Loss: 0.00001274
Iteration 184/1000 | Loss: 0.00001274
Iteration 185/1000 | Loss: 0.00001274
Iteration 186/1000 | Loss: 0.00001274
Iteration 187/1000 | Loss: 0.00001274
Iteration 188/1000 | Loss: 0.00001274
Iteration 189/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2741253158310428e-05, 1.2741253158310428e-05, 1.2741253158310428e-05, 1.2741253158310428e-05, 1.2741253158310428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2741253158310428e-05

Optimization complete. Final v2v error: 3.0402801036834717 mm

Highest mean error: 3.6229474544525146 mm for frame 112

Lowest mean error: 2.4968998432159424 mm for frame 0

Saving results

Total time: 43.7916202545166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055104
Iteration 2/25 | Loss: 0.01055104
Iteration 3/25 | Loss: 0.01055104
Iteration 4/25 | Loss: 0.01055104
Iteration 5/25 | Loss: 0.00176942
Iteration 6/25 | Loss: 0.00139210
Iteration 7/25 | Loss: 0.00118073
Iteration 8/25 | Loss: 0.00117456
Iteration 9/25 | Loss: 0.00115110
Iteration 10/25 | Loss: 0.00114342
Iteration 11/25 | Loss: 0.00111972
Iteration 12/25 | Loss: 0.00111158
Iteration 13/25 | Loss: 0.00110714
Iteration 14/25 | Loss: 0.00110625
Iteration 15/25 | Loss: 0.00110299
Iteration 16/25 | Loss: 0.00110451
Iteration 17/25 | Loss: 0.00110368
Iteration 18/25 | Loss: 0.00110252
Iteration 19/25 | Loss: 0.00110199
Iteration 20/25 | Loss: 0.00110154
Iteration 21/25 | Loss: 0.00110140
Iteration 22/25 | Loss: 0.00110140
Iteration 23/25 | Loss: 0.00110140
Iteration 24/25 | Loss: 0.00110139
Iteration 25/25 | Loss: 0.00110139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29011881
Iteration 2/25 | Loss: 0.00207184
Iteration 3/25 | Loss: 0.00207183
Iteration 4/25 | Loss: 0.00207183
Iteration 5/25 | Loss: 0.00207183
Iteration 6/25 | Loss: 0.00207183
Iteration 7/25 | Loss: 0.00207183
Iteration 8/25 | Loss: 0.00200805
Iteration 9/25 | Loss: 0.00200805
Iteration 10/25 | Loss: 0.00200805
Iteration 11/25 | Loss: 0.00200805
Iteration 12/25 | Loss: 0.00200805
Iteration 13/25 | Loss: 0.00200805
Iteration 14/25 | Loss: 0.00200805
Iteration 15/25 | Loss: 0.00200805
Iteration 16/25 | Loss: 0.00200805
Iteration 17/25 | Loss: 0.00200805
Iteration 18/25 | Loss: 0.00200805
Iteration 19/25 | Loss: 0.00200805
Iteration 20/25 | Loss: 0.00200805
Iteration 21/25 | Loss: 0.00200805
Iteration 22/25 | Loss: 0.00200805
Iteration 23/25 | Loss: 0.00200805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002008046256378293, 0.002008046256378293, 0.002008046256378293, 0.002008046256378293, 0.002008046256378293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002008046256378293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200805
Iteration 2/1000 | Loss: 0.00004205
Iteration 3/1000 | Loss: 0.00027366
Iteration 4/1000 | Loss: 0.00002448
Iteration 5/1000 | Loss: 0.00011200
Iteration 6/1000 | Loss: 0.00006465
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002483
Iteration 9/1000 | Loss: 0.00005037
Iteration 10/1000 | Loss: 0.00001911
Iteration 11/1000 | Loss: 0.00001866
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00007469
Iteration 16/1000 | Loss: 0.00007201
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00004003
Iteration 19/1000 | Loss: 0.00003366
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00003887
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00002459
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001745
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00003681
Iteration 45/1000 | Loss: 0.00003625
Iteration 46/1000 | Loss: 0.00002313
Iteration 47/1000 | Loss: 0.00001980
Iteration 48/1000 | Loss: 0.00009484
Iteration 49/1000 | Loss: 0.00002443
Iteration 50/1000 | Loss: 0.00003467
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00004553
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001725
Iteration 94/1000 | Loss: 0.00001725
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001724
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001724
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.7237558495253325e-05, 1.7237558495253325e-05, 1.7237558495253325e-05, 1.7237558495253325e-05, 1.7237558495253325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7237558495253325e-05

Optimization complete. Final v2v error: 3.556359052658081 mm

Highest mean error: 4.3278069496154785 mm for frame 159

Lowest mean error: 3.085083484649658 mm for frame 112

Saving results

Total time: 98.73801302909851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005873
Iteration 2/25 | Loss: 0.00152076
Iteration 3/25 | Loss: 0.00125586
Iteration 4/25 | Loss: 0.00126904
Iteration 5/25 | Loss: 0.00120279
Iteration 6/25 | Loss: 0.00121472
Iteration 7/25 | Loss: 0.00119805
Iteration 8/25 | Loss: 0.00117899
Iteration 9/25 | Loss: 0.00117121
Iteration 10/25 | Loss: 0.00116766
Iteration 11/25 | Loss: 0.00116015
Iteration 12/25 | Loss: 0.00115765
Iteration 13/25 | Loss: 0.00115678
Iteration 14/25 | Loss: 0.00115467
Iteration 15/25 | Loss: 0.00115029
Iteration 16/25 | Loss: 0.00114901
Iteration 17/25 | Loss: 0.00114772
Iteration 18/25 | Loss: 0.00115471
Iteration 19/25 | Loss: 0.00115261
Iteration 20/25 | Loss: 0.00114906
Iteration 21/25 | Loss: 0.00114826
Iteration 22/25 | Loss: 0.00115131
Iteration 23/25 | Loss: 0.00115206
Iteration 24/25 | Loss: 0.00114909
Iteration 25/25 | Loss: 0.00115120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22671807
Iteration 2/25 | Loss: 0.00215838
Iteration 3/25 | Loss: 0.00215835
Iteration 4/25 | Loss: 0.00215835
Iteration 5/25 | Loss: 0.00215834
Iteration 6/25 | Loss: 0.00215834
Iteration 7/25 | Loss: 0.00215834
Iteration 8/25 | Loss: 0.00215834
Iteration 9/25 | Loss: 0.00215834
Iteration 10/25 | Loss: 0.00215834
Iteration 11/25 | Loss: 0.00215834
Iteration 12/25 | Loss: 0.00215834
Iteration 13/25 | Loss: 0.00215834
Iteration 14/25 | Loss: 0.00215834
Iteration 15/25 | Loss: 0.00215834
Iteration 16/25 | Loss: 0.00215834
Iteration 17/25 | Loss: 0.00215834
Iteration 18/25 | Loss: 0.00215834
Iteration 19/25 | Loss: 0.00215834
Iteration 20/25 | Loss: 0.00215834
Iteration 21/25 | Loss: 0.00215834
Iteration 22/25 | Loss: 0.00215834
Iteration 23/25 | Loss: 0.00215834
Iteration 24/25 | Loss: 0.00215834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021583426278084517, 0.0021583426278084517, 0.0021583426278084517, 0.0021583426278084517, 0.0021583426278084517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021583426278084517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215834
Iteration 2/1000 | Loss: 0.00008161
Iteration 3/1000 | Loss: 0.00004691
Iteration 4/1000 | Loss: 0.00005277
Iteration 5/1000 | Loss: 0.00003151
Iteration 6/1000 | Loss: 0.00014156
Iteration 7/1000 | Loss: 0.00010567
Iteration 8/1000 | Loss: 0.00007108
Iteration 9/1000 | Loss: 0.00005335
Iteration 10/1000 | Loss: 0.00013123
Iteration 11/1000 | Loss: 0.00014997
Iteration 12/1000 | Loss: 0.00011818
Iteration 13/1000 | Loss: 0.00006670
Iteration 14/1000 | Loss: 0.00010812
Iteration 15/1000 | Loss: 0.00005627
Iteration 16/1000 | Loss: 0.00012417
Iteration 17/1000 | Loss: 0.00016136
Iteration 18/1000 | Loss: 0.00013389
Iteration 19/1000 | Loss: 0.00003533
Iteration 20/1000 | Loss: 0.00010266
Iteration 21/1000 | Loss: 0.00003194
Iteration 22/1000 | Loss: 0.00015999
Iteration 23/1000 | Loss: 0.00011687
Iteration 24/1000 | Loss: 0.00014220
Iteration 25/1000 | Loss: 0.00014067
Iteration 26/1000 | Loss: 0.00017552
Iteration 27/1000 | Loss: 0.00003363
Iteration 28/1000 | Loss: 0.00002860
Iteration 29/1000 | Loss: 0.00007286
Iteration 30/1000 | Loss: 0.00002605
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00002374
Iteration 33/1000 | Loss: 0.00002338
Iteration 34/1000 | Loss: 0.00002302
Iteration 35/1000 | Loss: 0.00002279
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00002235
Iteration 39/1000 | Loss: 0.00002231
Iteration 40/1000 | Loss: 0.00002231
Iteration 41/1000 | Loss: 0.00002230
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002229
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002225
Iteration 46/1000 | Loss: 0.00002225
Iteration 47/1000 | Loss: 0.00002223
Iteration 48/1000 | Loss: 0.00002222
Iteration 49/1000 | Loss: 0.00002222
Iteration 50/1000 | Loss: 0.00002221
Iteration 51/1000 | Loss: 0.00002221
Iteration 52/1000 | Loss: 0.00002221
Iteration 53/1000 | Loss: 0.00002221
Iteration 54/1000 | Loss: 0.00002221
Iteration 55/1000 | Loss: 0.00002220
Iteration 56/1000 | Loss: 0.00002220
Iteration 57/1000 | Loss: 0.00002220
Iteration 58/1000 | Loss: 0.00002219
Iteration 59/1000 | Loss: 0.00002219
Iteration 60/1000 | Loss: 0.00002219
Iteration 61/1000 | Loss: 0.00002219
Iteration 62/1000 | Loss: 0.00002219
Iteration 63/1000 | Loss: 0.00002218
Iteration 64/1000 | Loss: 0.00002218
Iteration 65/1000 | Loss: 0.00002218
Iteration 66/1000 | Loss: 0.00002218
Iteration 67/1000 | Loss: 0.00002217
Iteration 68/1000 | Loss: 0.00002217
Iteration 69/1000 | Loss: 0.00002217
Iteration 70/1000 | Loss: 0.00002217
Iteration 71/1000 | Loss: 0.00002217
Iteration 72/1000 | Loss: 0.00002217
Iteration 73/1000 | Loss: 0.00002217
Iteration 74/1000 | Loss: 0.00002217
Iteration 75/1000 | Loss: 0.00002216
Iteration 76/1000 | Loss: 0.00002216
Iteration 77/1000 | Loss: 0.00002215
Iteration 78/1000 | Loss: 0.00002215
Iteration 79/1000 | Loss: 0.00002214
Iteration 80/1000 | Loss: 0.00002213
Iteration 81/1000 | Loss: 0.00002213
Iteration 82/1000 | Loss: 0.00002212
Iteration 83/1000 | Loss: 0.00002212
Iteration 84/1000 | Loss: 0.00002212
Iteration 85/1000 | Loss: 0.00002211
Iteration 86/1000 | Loss: 0.00002211
Iteration 87/1000 | Loss: 0.00002211
Iteration 88/1000 | Loss: 0.00002210
Iteration 89/1000 | Loss: 0.00002210
Iteration 90/1000 | Loss: 0.00002210
Iteration 91/1000 | Loss: 0.00002210
Iteration 92/1000 | Loss: 0.00002210
Iteration 93/1000 | Loss: 0.00002210
Iteration 94/1000 | Loss: 0.00002210
Iteration 95/1000 | Loss: 0.00002210
Iteration 96/1000 | Loss: 0.00002210
Iteration 97/1000 | Loss: 0.00002210
Iteration 98/1000 | Loss: 0.00002210
Iteration 99/1000 | Loss: 0.00002209
Iteration 100/1000 | Loss: 0.00002209
Iteration 101/1000 | Loss: 0.00002209
Iteration 102/1000 | Loss: 0.00002209
Iteration 103/1000 | Loss: 0.00002208
Iteration 104/1000 | Loss: 0.00002208
Iteration 105/1000 | Loss: 0.00002208
Iteration 106/1000 | Loss: 0.00002208
Iteration 107/1000 | Loss: 0.00002208
Iteration 108/1000 | Loss: 0.00002207
Iteration 109/1000 | Loss: 0.00002207
Iteration 110/1000 | Loss: 0.00002207
Iteration 111/1000 | Loss: 0.00002207
Iteration 112/1000 | Loss: 0.00002206
Iteration 113/1000 | Loss: 0.00002206
Iteration 114/1000 | Loss: 0.00002206
Iteration 115/1000 | Loss: 0.00002206
Iteration 116/1000 | Loss: 0.00002206
Iteration 117/1000 | Loss: 0.00002205
Iteration 118/1000 | Loss: 0.00002205
Iteration 119/1000 | Loss: 0.00002205
Iteration 120/1000 | Loss: 0.00002205
Iteration 121/1000 | Loss: 0.00002205
Iteration 122/1000 | Loss: 0.00002205
Iteration 123/1000 | Loss: 0.00002205
Iteration 124/1000 | Loss: 0.00002205
Iteration 125/1000 | Loss: 0.00002204
Iteration 126/1000 | Loss: 0.00002204
Iteration 127/1000 | Loss: 0.00002204
Iteration 128/1000 | Loss: 0.00002204
Iteration 129/1000 | Loss: 0.00002204
Iteration 130/1000 | Loss: 0.00002204
Iteration 131/1000 | Loss: 0.00002203
Iteration 132/1000 | Loss: 0.00002203
Iteration 133/1000 | Loss: 0.00002203
Iteration 134/1000 | Loss: 0.00002203
Iteration 135/1000 | Loss: 0.00002203
Iteration 136/1000 | Loss: 0.00002202
Iteration 137/1000 | Loss: 0.00002202
Iteration 138/1000 | Loss: 0.00002202
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002200
Iteration 144/1000 | Loss: 0.00002200
Iteration 145/1000 | Loss: 0.00002200
Iteration 146/1000 | Loss: 0.00002200
Iteration 147/1000 | Loss: 0.00002200
Iteration 148/1000 | Loss: 0.00002200
Iteration 149/1000 | Loss: 0.00002199
Iteration 150/1000 | Loss: 0.00002199
Iteration 151/1000 | Loss: 0.00002199
Iteration 152/1000 | Loss: 0.00002198
Iteration 153/1000 | Loss: 0.00002198
Iteration 154/1000 | Loss: 0.00002198
Iteration 155/1000 | Loss: 0.00002198
Iteration 156/1000 | Loss: 0.00002198
Iteration 157/1000 | Loss: 0.00002198
Iteration 158/1000 | Loss: 0.00002198
Iteration 159/1000 | Loss: 0.00002198
Iteration 160/1000 | Loss: 0.00002198
Iteration 161/1000 | Loss: 0.00002198
Iteration 162/1000 | Loss: 0.00002197
Iteration 163/1000 | Loss: 0.00002197
Iteration 164/1000 | Loss: 0.00002197
Iteration 165/1000 | Loss: 0.00002197
Iteration 166/1000 | Loss: 0.00002197
Iteration 167/1000 | Loss: 0.00002197
Iteration 168/1000 | Loss: 0.00002197
Iteration 169/1000 | Loss: 0.00002197
Iteration 170/1000 | Loss: 0.00002197
Iteration 171/1000 | Loss: 0.00002197
Iteration 172/1000 | Loss: 0.00002197
Iteration 173/1000 | Loss: 0.00002196
Iteration 174/1000 | Loss: 0.00002196
Iteration 175/1000 | Loss: 0.00002196
Iteration 176/1000 | Loss: 0.00002196
Iteration 177/1000 | Loss: 0.00002196
Iteration 178/1000 | Loss: 0.00002196
Iteration 179/1000 | Loss: 0.00002196
Iteration 180/1000 | Loss: 0.00002196
Iteration 181/1000 | Loss: 0.00002196
Iteration 182/1000 | Loss: 0.00002196
Iteration 183/1000 | Loss: 0.00002196
Iteration 184/1000 | Loss: 0.00002196
Iteration 185/1000 | Loss: 0.00002196
Iteration 186/1000 | Loss: 0.00002195
Iteration 187/1000 | Loss: 0.00002195
Iteration 188/1000 | Loss: 0.00002195
Iteration 189/1000 | Loss: 0.00002195
Iteration 190/1000 | Loss: 0.00002195
Iteration 191/1000 | Loss: 0.00002194
Iteration 192/1000 | Loss: 0.00002194
Iteration 193/1000 | Loss: 0.00002194
Iteration 194/1000 | Loss: 0.00002194
Iteration 195/1000 | Loss: 0.00002193
Iteration 196/1000 | Loss: 0.00002193
Iteration 197/1000 | Loss: 0.00002193
Iteration 198/1000 | Loss: 0.00002192
Iteration 199/1000 | Loss: 0.00002192
Iteration 200/1000 | Loss: 0.00002192
Iteration 201/1000 | Loss: 0.00002191
Iteration 202/1000 | Loss: 0.00002191
Iteration 203/1000 | Loss: 0.00002191
Iteration 204/1000 | Loss: 0.00002190
Iteration 205/1000 | Loss: 0.00002190
Iteration 206/1000 | Loss: 0.00002189
Iteration 207/1000 | Loss: 0.00002188
Iteration 208/1000 | Loss: 0.00002188
Iteration 209/1000 | Loss: 0.00002188
Iteration 210/1000 | Loss: 0.00002188
Iteration 211/1000 | Loss: 0.00002187
Iteration 212/1000 | Loss: 0.00002187
Iteration 213/1000 | Loss: 0.00002187
Iteration 214/1000 | Loss: 0.00002187
Iteration 215/1000 | Loss: 0.00002187
Iteration 216/1000 | Loss: 0.00002187
Iteration 217/1000 | Loss: 0.00002186
Iteration 218/1000 | Loss: 0.00002186
Iteration 219/1000 | Loss: 0.00002186
Iteration 220/1000 | Loss: 0.00002186
Iteration 221/1000 | Loss: 0.00002186
Iteration 222/1000 | Loss: 0.00002186
Iteration 223/1000 | Loss: 0.00002186
Iteration 224/1000 | Loss: 0.00002186
Iteration 225/1000 | Loss: 0.00002186
Iteration 226/1000 | Loss: 0.00002186
Iteration 227/1000 | Loss: 0.00002185
Iteration 228/1000 | Loss: 0.00002185
Iteration 229/1000 | Loss: 0.00002185
Iteration 230/1000 | Loss: 0.00002185
Iteration 231/1000 | Loss: 0.00002185
Iteration 232/1000 | Loss: 0.00002185
Iteration 233/1000 | Loss: 0.00002185
Iteration 234/1000 | Loss: 0.00002185
Iteration 235/1000 | Loss: 0.00002185
Iteration 236/1000 | Loss: 0.00002185
Iteration 237/1000 | Loss: 0.00002184
Iteration 238/1000 | Loss: 0.00002184
Iteration 239/1000 | Loss: 0.00002184
Iteration 240/1000 | Loss: 0.00002184
Iteration 241/1000 | Loss: 0.00002184
Iteration 242/1000 | Loss: 0.00002184
Iteration 243/1000 | Loss: 0.00002184
Iteration 244/1000 | Loss: 0.00002184
Iteration 245/1000 | Loss: 0.00002184
Iteration 246/1000 | Loss: 0.00002184
Iteration 247/1000 | Loss: 0.00002183
Iteration 248/1000 | Loss: 0.00002183
Iteration 249/1000 | Loss: 0.00002183
Iteration 250/1000 | Loss: 0.00002183
Iteration 251/1000 | Loss: 0.00002183
Iteration 252/1000 | Loss: 0.00002183
Iteration 253/1000 | Loss: 0.00002183
Iteration 254/1000 | Loss: 0.00002183
Iteration 255/1000 | Loss: 0.00002183
Iteration 256/1000 | Loss: 0.00002182
Iteration 257/1000 | Loss: 0.00002182
Iteration 258/1000 | Loss: 0.00002182
Iteration 259/1000 | Loss: 0.00002182
Iteration 260/1000 | Loss: 0.00002182
Iteration 261/1000 | Loss: 0.00002182
Iteration 262/1000 | Loss: 0.00002182
Iteration 263/1000 | Loss: 0.00002182
Iteration 264/1000 | Loss: 0.00002182
Iteration 265/1000 | Loss: 0.00002182
Iteration 266/1000 | Loss: 0.00002181
Iteration 267/1000 | Loss: 0.00002181
Iteration 268/1000 | Loss: 0.00002181
Iteration 269/1000 | Loss: 0.00002181
Iteration 270/1000 | Loss: 0.00002181
Iteration 271/1000 | Loss: 0.00002181
Iteration 272/1000 | Loss: 0.00002181
Iteration 273/1000 | Loss: 0.00002181
Iteration 274/1000 | Loss: 0.00002181
Iteration 275/1000 | Loss: 0.00002181
Iteration 276/1000 | Loss: 0.00002181
Iteration 277/1000 | Loss: 0.00002181
Iteration 278/1000 | Loss: 0.00002181
Iteration 279/1000 | Loss: 0.00002181
Iteration 280/1000 | Loss: 0.00002181
Iteration 281/1000 | Loss: 0.00002181
Iteration 282/1000 | Loss: 0.00002181
Iteration 283/1000 | Loss: 0.00002181
Iteration 284/1000 | Loss: 0.00002181
Iteration 285/1000 | Loss: 0.00002181
Iteration 286/1000 | Loss: 0.00002180
Iteration 287/1000 | Loss: 0.00002180
Iteration 288/1000 | Loss: 0.00002180
Iteration 289/1000 | Loss: 0.00002180
Iteration 290/1000 | Loss: 0.00002180
Iteration 291/1000 | Loss: 0.00002180
Iteration 292/1000 | Loss: 0.00002180
Iteration 293/1000 | Loss: 0.00002180
Iteration 294/1000 | Loss: 0.00002180
Iteration 295/1000 | Loss: 0.00002180
Iteration 296/1000 | Loss: 0.00002180
Iteration 297/1000 | Loss: 0.00002180
Iteration 298/1000 | Loss: 0.00002180
Iteration 299/1000 | Loss: 0.00002180
Iteration 300/1000 | Loss: 0.00002180
Iteration 301/1000 | Loss: 0.00002180
Iteration 302/1000 | Loss: 0.00002180
Iteration 303/1000 | Loss: 0.00002179
Iteration 304/1000 | Loss: 0.00002179
Iteration 305/1000 | Loss: 0.00002179
Iteration 306/1000 | Loss: 0.00002179
Iteration 307/1000 | Loss: 0.00002179
Iteration 308/1000 | Loss: 0.00002179
Iteration 309/1000 | Loss: 0.00002179
Iteration 310/1000 | Loss: 0.00002179
Iteration 311/1000 | Loss: 0.00002179
Iteration 312/1000 | Loss: 0.00002179
Iteration 313/1000 | Loss: 0.00002179
Iteration 314/1000 | Loss: 0.00002179
Iteration 315/1000 | Loss: 0.00002179
Iteration 316/1000 | Loss: 0.00002179
Iteration 317/1000 | Loss: 0.00002179
Iteration 318/1000 | Loss: 0.00002179
Iteration 319/1000 | Loss: 0.00002179
Iteration 320/1000 | Loss: 0.00002179
Iteration 321/1000 | Loss: 0.00002179
Iteration 322/1000 | Loss: 0.00002179
Iteration 323/1000 | Loss: 0.00002179
Iteration 324/1000 | Loss: 0.00002179
Iteration 325/1000 | Loss: 0.00002179
Iteration 326/1000 | Loss: 0.00002179
Iteration 327/1000 | Loss: 0.00002179
Iteration 328/1000 | Loss: 0.00002179
Iteration 329/1000 | Loss: 0.00002179
Iteration 330/1000 | Loss: 0.00002179
Iteration 331/1000 | Loss: 0.00002179
Iteration 332/1000 | Loss: 0.00002179
Iteration 333/1000 | Loss: 0.00002179
Iteration 334/1000 | Loss: 0.00002179
Iteration 335/1000 | Loss: 0.00002179
Iteration 336/1000 | Loss: 0.00002179
Iteration 337/1000 | Loss: 0.00002179
Iteration 338/1000 | Loss: 0.00002179
Iteration 339/1000 | Loss: 0.00002179
Iteration 340/1000 | Loss: 0.00002179
Iteration 341/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [2.179075454478152e-05, 2.179075454478152e-05, 2.179075454478152e-05, 2.179075454478152e-05, 2.179075454478152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.179075454478152e-05

Optimization complete. Final v2v error: 3.77274489402771 mm

Highest mean error: 4.72134256362915 mm for frame 64

Lowest mean error: 2.9456427097320557 mm for frame 108

Saving results

Total time: 128.16148495674133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440601
Iteration 2/25 | Loss: 0.00122178
Iteration 3/25 | Loss: 0.00107854
Iteration 4/25 | Loss: 0.00106567
Iteration 5/25 | Loss: 0.00106387
Iteration 6/25 | Loss: 0.00106348
Iteration 7/25 | Loss: 0.00106348
Iteration 8/25 | Loss: 0.00106348
Iteration 9/25 | Loss: 0.00106348
Iteration 10/25 | Loss: 0.00106348
Iteration 11/25 | Loss: 0.00106348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010634849313646555, 0.0010634849313646555, 0.0010634849313646555, 0.0010634849313646555, 0.0010634849313646555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010634849313646555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.70048428
Iteration 2/25 | Loss: 0.00107831
Iteration 3/25 | Loss: 0.00107827
Iteration 4/25 | Loss: 0.00107827
Iteration 5/25 | Loss: 0.00107827
Iteration 6/25 | Loss: 0.00107827
Iteration 7/25 | Loss: 0.00107827
Iteration 8/25 | Loss: 0.00107827
Iteration 9/25 | Loss: 0.00107827
Iteration 10/25 | Loss: 0.00107827
Iteration 11/25 | Loss: 0.00107827
Iteration 12/25 | Loss: 0.00107827
Iteration 13/25 | Loss: 0.00107827
Iteration 14/25 | Loss: 0.00107827
Iteration 15/25 | Loss: 0.00107827
Iteration 16/25 | Loss: 0.00107827
Iteration 17/25 | Loss: 0.00107827
Iteration 18/25 | Loss: 0.00107827
Iteration 19/25 | Loss: 0.00107827
Iteration 20/25 | Loss: 0.00107827
Iteration 21/25 | Loss: 0.00107827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010782674653455615, 0.0010782674653455615, 0.0010782674653455615, 0.0010782674653455615, 0.0010782674653455615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010782674653455615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107827
Iteration 2/1000 | Loss: 0.00002966
Iteration 3/1000 | Loss: 0.00001793
Iteration 4/1000 | Loss: 0.00001504
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001318
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001184
Iteration 11/1000 | Loss: 0.00001176
Iteration 12/1000 | Loss: 0.00001163
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001138
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001138
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001137
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001136
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001133
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001133
Iteration 62/1000 | Loss: 0.00001133
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001132
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001131
Iteration 68/1000 | Loss: 0.00001131
Iteration 69/1000 | Loss: 0.00001131
Iteration 70/1000 | Loss: 0.00001131
Iteration 71/1000 | Loss: 0.00001131
Iteration 72/1000 | Loss: 0.00001131
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001130
Iteration 75/1000 | Loss: 0.00001130
Iteration 76/1000 | Loss: 0.00001130
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001129
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001129
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001128
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001124
Iteration 113/1000 | Loss: 0.00001124
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001123
Iteration 122/1000 | Loss: 0.00001123
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001122
Iteration 125/1000 | Loss: 0.00001122
Iteration 126/1000 | Loss: 0.00001122
Iteration 127/1000 | Loss: 0.00001122
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001122
Iteration 130/1000 | Loss: 0.00001122
Iteration 131/1000 | Loss: 0.00001122
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001122
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001121
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001119
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001116
Iteration 155/1000 | Loss: 0.00001116
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001115
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001115
Iteration 163/1000 | Loss: 0.00001115
Iteration 164/1000 | Loss: 0.00001114
Iteration 165/1000 | Loss: 0.00001114
Iteration 166/1000 | Loss: 0.00001114
Iteration 167/1000 | Loss: 0.00001114
Iteration 168/1000 | Loss: 0.00001114
Iteration 169/1000 | Loss: 0.00001114
Iteration 170/1000 | Loss: 0.00001114
Iteration 171/1000 | Loss: 0.00001114
Iteration 172/1000 | Loss: 0.00001114
Iteration 173/1000 | Loss: 0.00001114
Iteration 174/1000 | Loss: 0.00001114
Iteration 175/1000 | Loss: 0.00001113
Iteration 176/1000 | Loss: 0.00001113
Iteration 177/1000 | Loss: 0.00001113
Iteration 178/1000 | Loss: 0.00001113
Iteration 179/1000 | Loss: 0.00001113
Iteration 180/1000 | Loss: 0.00001113
Iteration 181/1000 | Loss: 0.00001113
Iteration 182/1000 | Loss: 0.00001113
Iteration 183/1000 | Loss: 0.00001113
Iteration 184/1000 | Loss: 0.00001113
Iteration 185/1000 | Loss: 0.00001113
Iteration 186/1000 | Loss: 0.00001113
Iteration 187/1000 | Loss: 0.00001113
Iteration 188/1000 | Loss: 0.00001113
Iteration 189/1000 | Loss: 0.00001113
Iteration 190/1000 | Loss: 0.00001113
Iteration 191/1000 | Loss: 0.00001113
Iteration 192/1000 | Loss: 0.00001113
Iteration 193/1000 | Loss: 0.00001113
Iteration 194/1000 | Loss: 0.00001113
Iteration 195/1000 | Loss: 0.00001113
Iteration 196/1000 | Loss: 0.00001113
Iteration 197/1000 | Loss: 0.00001113
Iteration 198/1000 | Loss: 0.00001113
Iteration 199/1000 | Loss: 0.00001113
Iteration 200/1000 | Loss: 0.00001113
Iteration 201/1000 | Loss: 0.00001113
Iteration 202/1000 | Loss: 0.00001113
Iteration 203/1000 | Loss: 0.00001113
Iteration 204/1000 | Loss: 0.00001113
Iteration 205/1000 | Loss: 0.00001113
Iteration 206/1000 | Loss: 0.00001113
Iteration 207/1000 | Loss: 0.00001113
Iteration 208/1000 | Loss: 0.00001113
Iteration 209/1000 | Loss: 0.00001113
Iteration 210/1000 | Loss: 0.00001113
Iteration 211/1000 | Loss: 0.00001113
Iteration 212/1000 | Loss: 0.00001113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.1126546269224491e-05, 1.1126546269224491e-05, 1.1126546269224491e-05, 1.1126546269224491e-05, 1.1126546269224491e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1126546269224491e-05

Optimization complete. Final v2v error: 2.8766732215881348 mm

Highest mean error: 3.3459789752960205 mm for frame 107

Lowest mean error: 2.5656144618988037 mm for frame 23

Saving results

Total time: 38.972909688949585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782538
Iteration 2/25 | Loss: 0.00126058
Iteration 3/25 | Loss: 0.00112006
Iteration 4/25 | Loss: 0.00109469
Iteration 5/25 | Loss: 0.00108774
Iteration 6/25 | Loss: 0.00108526
Iteration 7/25 | Loss: 0.00108522
Iteration 8/25 | Loss: 0.00108522
Iteration 9/25 | Loss: 0.00108522
Iteration 10/25 | Loss: 0.00108522
Iteration 11/25 | Loss: 0.00108522
Iteration 12/25 | Loss: 0.00108522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010852166451513767, 0.0010852166451513767, 0.0010852166451513767, 0.0010852166451513767, 0.0010852166451513767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010852166451513767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26015329
Iteration 2/25 | Loss: 0.00212059
Iteration 3/25 | Loss: 0.00212058
Iteration 4/25 | Loss: 0.00212058
Iteration 5/25 | Loss: 0.00212058
Iteration 6/25 | Loss: 0.00212058
Iteration 7/25 | Loss: 0.00212058
Iteration 8/25 | Loss: 0.00212058
Iteration 9/25 | Loss: 0.00212058
Iteration 10/25 | Loss: 0.00212058
Iteration 11/25 | Loss: 0.00212058
Iteration 12/25 | Loss: 0.00212058
Iteration 13/25 | Loss: 0.00212058
Iteration 14/25 | Loss: 0.00212058
Iteration 15/25 | Loss: 0.00212058
Iteration 16/25 | Loss: 0.00212058
Iteration 17/25 | Loss: 0.00212058
Iteration 18/25 | Loss: 0.00212058
Iteration 19/25 | Loss: 0.00212058
Iteration 20/25 | Loss: 0.00212058
Iteration 21/25 | Loss: 0.00212058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021205805242061615, 0.0021205805242061615, 0.0021205805242061615, 0.0021205805242061615, 0.0021205805242061615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021205805242061615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212058
Iteration 2/1000 | Loss: 0.00005395
Iteration 3/1000 | Loss: 0.00003326
Iteration 4/1000 | Loss: 0.00002490
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00001990
Iteration 8/1000 | Loss: 0.00001949
Iteration 9/1000 | Loss: 0.00001915
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001827
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001817
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001813
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001811
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001810
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001807
Iteration 41/1000 | Loss: 0.00001807
Iteration 42/1000 | Loss: 0.00001807
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001806
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001805
Iteration 48/1000 | Loss: 0.00001805
Iteration 49/1000 | Loss: 0.00001805
Iteration 50/1000 | Loss: 0.00001805
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001804
Iteration 53/1000 | Loss: 0.00001804
Iteration 54/1000 | Loss: 0.00001804
Iteration 55/1000 | Loss: 0.00001804
Iteration 56/1000 | Loss: 0.00001804
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001803
Iteration 59/1000 | Loss: 0.00001803
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001802
Iteration 62/1000 | Loss: 0.00001802
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001802
Iteration 65/1000 | Loss: 0.00001802
Iteration 66/1000 | Loss: 0.00001802
Iteration 67/1000 | Loss: 0.00001802
Iteration 68/1000 | Loss: 0.00001802
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001801
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001801
Iteration 74/1000 | Loss: 0.00001801
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001800
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001800
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001800
Iteration 82/1000 | Loss: 0.00001800
Iteration 83/1000 | Loss: 0.00001800
Iteration 84/1000 | Loss: 0.00001800
Iteration 85/1000 | Loss: 0.00001800
Iteration 86/1000 | Loss: 0.00001800
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001799
Iteration 91/1000 | Loss: 0.00001799
Iteration 92/1000 | Loss: 0.00001799
Iteration 93/1000 | Loss: 0.00001799
Iteration 94/1000 | Loss: 0.00001799
Iteration 95/1000 | Loss: 0.00001799
Iteration 96/1000 | Loss: 0.00001799
Iteration 97/1000 | Loss: 0.00001798
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001797
Iteration 103/1000 | Loss: 0.00001797
Iteration 104/1000 | Loss: 0.00001797
Iteration 105/1000 | Loss: 0.00001797
Iteration 106/1000 | Loss: 0.00001797
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001797
Iteration 113/1000 | Loss: 0.00001797
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001795
Iteration 121/1000 | Loss: 0.00001795
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001795
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001795
Iteration 128/1000 | Loss: 0.00001795
Iteration 129/1000 | Loss: 0.00001795
Iteration 130/1000 | Loss: 0.00001794
Iteration 131/1000 | Loss: 0.00001794
Iteration 132/1000 | Loss: 0.00001794
Iteration 133/1000 | Loss: 0.00001794
Iteration 134/1000 | Loss: 0.00001794
Iteration 135/1000 | Loss: 0.00001794
Iteration 136/1000 | Loss: 0.00001794
Iteration 137/1000 | Loss: 0.00001794
Iteration 138/1000 | Loss: 0.00001794
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001794
Iteration 142/1000 | Loss: 0.00001794
Iteration 143/1000 | Loss: 0.00001794
Iteration 144/1000 | Loss: 0.00001794
Iteration 145/1000 | Loss: 0.00001794
Iteration 146/1000 | Loss: 0.00001794
Iteration 147/1000 | Loss: 0.00001794
Iteration 148/1000 | Loss: 0.00001794
Iteration 149/1000 | Loss: 0.00001794
Iteration 150/1000 | Loss: 0.00001794
Iteration 151/1000 | Loss: 0.00001794
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001793
Iteration 155/1000 | Loss: 0.00001793
Iteration 156/1000 | Loss: 0.00001793
Iteration 157/1000 | Loss: 0.00001793
Iteration 158/1000 | Loss: 0.00001793
Iteration 159/1000 | Loss: 0.00001793
Iteration 160/1000 | Loss: 0.00001793
Iteration 161/1000 | Loss: 0.00001793
Iteration 162/1000 | Loss: 0.00001793
Iteration 163/1000 | Loss: 0.00001793
Iteration 164/1000 | Loss: 0.00001793
Iteration 165/1000 | Loss: 0.00001793
Iteration 166/1000 | Loss: 0.00001793
Iteration 167/1000 | Loss: 0.00001793
Iteration 168/1000 | Loss: 0.00001793
Iteration 169/1000 | Loss: 0.00001792
Iteration 170/1000 | Loss: 0.00001792
Iteration 171/1000 | Loss: 0.00001792
Iteration 172/1000 | Loss: 0.00001792
Iteration 173/1000 | Loss: 0.00001792
Iteration 174/1000 | Loss: 0.00001792
Iteration 175/1000 | Loss: 0.00001792
Iteration 176/1000 | Loss: 0.00001792
Iteration 177/1000 | Loss: 0.00001792
Iteration 178/1000 | Loss: 0.00001792
Iteration 179/1000 | Loss: 0.00001792
Iteration 180/1000 | Loss: 0.00001792
Iteration 181/1000 | Loss: 0.00001792
Iteration 182/1000 | Loss: 0.00001792
Iteration 183/1000 | Loss: 0.00001792
Iteration 184/1000 | Loss: 0.00001792
Iteration 185/1000 | Loss: 0.00001792
Iteration 186/1000 | Loss: 0.00001792
Iteration 187/1000 | Loss: 0.00001792
Iteration 188/1000 | Loss: 0.00001792
Iteration 189/1000 | Loss: 0.00001792
Iteration 190/1000 | Loss: 0.00001792
Iteration 191/1000 | Loss: 0.00001792
Iteration 192/1000 | Loss: 0.00001792
Iteration 193/1000 | Loss: 0.00001792
Iteration 194/1000 | Loss: 0.00001792
Iteration 195/1000 | Loss: 0.00001792
Iteration 196/1000 | Loss: 0.00001792
Iteration 197/1000 | Loss: 0.00001792
Iteration 198/1000 | Loss: 0.00001792
Iteration 199/1000 | Loss: 0.00001792
Iteration 200/1000 | Loss: 0.00001792
Iteration 201/1000 | Loss: 0.00001792
Iteration 202/1000 | Loss: 0.00001792
Iteration 203/1000 | Loss: 0.00001792
Iteration 204/1000 | Loss: 0.00001792
Iteration 205/1000 | Loss: 0.00001792
Iteration 206/1000 | Loss: 0.00001792
Iteration 207/1000 | Loss: 0.00001792
Iteration 208/1000 | Loss: 0.00001792
Iteration 209/1000 | Loss: 0.00001792
Iteration 210/1000 | Loss: 0.00001792
Iteration 211/1000 | Loss: 0.00001792
Iteration 212/1000 | Loss: 0.00001792
Iteration 213/1000 | Loss: 0.00001792
Iteration 214/1000 | Loss: 0.00001792
Iteration 215/1000 | Loss: 0.00001792
Iteration 216/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.7922775441547856e-05, 1.7922775441547856e-05, 1.7922775441547856e-05, 1.7922775441547856e-05, 1.7922775441547856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7922775441547856e-05

Optimization complete. Final v2v error: 3.504944324493408 mm

Highest mean error: 4.339964866638184 mm for frame 21

Lowest mean error: 2.5424370765686035 mm for frame 171

Saving results

Total time: 40.923168897628784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477569
Iteration 2/25 | Loss: 0.00110005
Iteration 3/25 | Loss: 0.00101154
Iteration 4/25 | Loss: 0.00100513
Iteration 5/25 | Loss: 0.00100353
Iteration 6/25 | Loss: 0.00100280
Iteration 7/25 | Loss: 0.00100280
Iteration 8/25 | Loss: 0.00100280
Iteration 9/25 | Loss: 0.00100280
Iteration 10/25 | Loss: 0.00100280
Iteration 11/25 | Loss: 0.00100280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001002795877866447, 0.001002795877866447, 0.001002795877866447, 0.001002795877866447, 0.001002795877866447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001002795877866447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20663035
Iteration 2/25 | Loss: 0.00147394
Iteration 3/25 | Loss: 0.00147392
Iteration 4/25 | Loss: 0.00147392
Iteration 5/25 | Loss: 0.00147392
Iteration 6/25 | Loss: 0.00147392
Iteration 7/25 | Loss: 0.00147392
Iteration 8/25 | Loss: 0.00147392
Iteration 9/25 | Loss: 0.00147391
Iteration 10/25 | Loss: 0.00147391
Iteration 11/25 | Loss: 0.00147391
Iteration 12/25 | Loss: 0.00147391
Iteration 13/25 | Loss: 0.00147391
Iteration 14/25 | Loss: 0.00147391
Iteration 15/25 | Loss: 0.00147391
Iteration 16/25 | Loss: 0.00147391
Iteration 17/25 | Loss: 0.00147391
Iteration 18/25 | Loss: 0.00147391
Iteration 19/25 | Loss: 0.00147391
Iteration 20/25 | Loss: 0.00147391
Iteration 21/25 | Loss: 0.00147391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014739142498001456, 0.0014739142498001456, 0.0014739142498001456, 0.0014739142498001456, 0.0014739142498001456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014739142498001456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147391
Iteration 2/1000 | Loss: 0.00003607
Iteration 3/1000 | Loss: 0.00002006
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001328
Iteration 6/1000 | Loss: 0.00001201
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001115
Iteration 9/1000 | Loss: 0.00001089
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001077
Iteration 13/1000 | Loss: 0.00001072
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001051
Iteration 17/1000 | Loss: 0.00001050
Iteration 18/1000 | Loss: 0.00001049
Iteration 19/1000 | Loss: 0.00001048
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001046
Iteration 24/1000 | Loss: 0.00001044
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001043
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001043
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001038
Iteration 43/1000 | Loss: 0.00001038
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001034
Iteration 50/1000 | Loss: 0.00001034
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001033
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001031
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001026
Iteration 87/1000 | Loss: 0.00001026
Iteration 88/1000 | Loss: 0.00001026
Iteration 89/1000 | Loss: 0.00001026
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001026
Iteration 93/1000 | Loss: 0.00001026
Iteration 94/1000 | Loss: 0.00001026
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.0261129318678286e-05, 1.0261129318678286e-05, 1.0261129318678286e-05, 1.0261129318678286e-05, 1.0261129318678286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0261129318678286e-05

Optimization complete. Final v2v error: 2.7113146781921387 mm

Highest mean error: 3.0714597702026367 mm for frame 30

Lowest mean error: 2.4592835903167725 mm for frame 133

Saving results

Total time: 31.154006958007812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122009
Iteration 2/25 | Loss: 0.00170666
Iteration 3/25 | Loss: 0.00125039
Iteration 4/25 | Loss: 0.00120860
Iteration 5/25 | Loss: 0.00121679
Iteration 6/25 | Loss: 0.00121183
Iteration 7/25 | Loss: 0.00121152
Iteration 8/25 | Loss: 0.00120819
Iteration 9/25 | Loss: 0.00120930
Iteration 10/25 | Loss: 0.00120369
Iteration 11/25 | Loss: 0.00119007
Iteration 12/25 | Loss: 0.00118500
Iteration 13/25 | Loss: 0.00117956
Iteration 14/25 | Loss: 0.00117724
Iteration 15/25 | Loss: 0.00117630
Iteration 16/25 | Loss: 0.00117592
Iteration 17/25 | Loss: 0.00117162
Iteration 18/25 | Loss: 0.00117603
Iteration 19/25 | Loss: 0.00117731
Iteration 20/25 | Loss: 0.00117428
Iteration 21/25 | Loss: 0.00117443
Iteration 22/25 | Loss: 0.00117585
Iteration 23/25 | Loss: 0.00117375
Iteration 24/25 | Loss: 0.00117452
Iteration 25/25 | Loss: 0.00117297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99917358
Iteration 2/25 | Loss: 0.00149343
Iteration 3/25 | Loss: 0.00149343
Iteration 4/25 | Loss: 0.00149343
Iteration 5/25 | Loss: 0.00149343
Iteration 6/25 | Loss: 0.00149342
Iteration 7/25 | Loss: 0.00149342
Iteration 8/25 | Loss: 0.00149342
Iteration 9/25 | Loss: 0.00149342
Iteration 10/25 | Loss: 0.00149342
Iteration 11/25 | Loss: 0.00149342
Iteration 12/25 | Loss: 0.00149342
Iteration 13/25 | Loss: 0.00149342
Iteration 14/25 | Loss: 0.00149342
Iteration 15/25 | Loss: 0.00149342
Iteration 16/25 | Loss: 0.00149342
Iteration 17/25 | Loss: 0.00149342
Iteration 18/25 | Loss: 0.00149342
Iteration 19/25 | Loss: 0.00149342
Iteration 20/25 | Loss: 0.00149342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014934231294319034, 0.0014934231294319034, 0.0014934231294319034, 0.0014934231294319034, 0.0014934231294319034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014934231294319034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149342
Iteration 2/1000 | Loss: 0.00019155
Iteration 3/1000 | Loss: 0.00005079
Iteration 4/1000 | Loss: 0.00010026
Iteration 5/1000 | Loss: 0.00003911
Iteration 6/1000 | Loss: 0.00006058
Iteration 7/1000 | Loss: 0.00005890
Iteration 8/1000 | Loss: 0.00004744
Iteration 9/1000 | Loss: 0.00011188
Iteration 10/1000 | Loss: 0.00011166
Iteration 11/1000 | Loss: 0.00004007
Iteration 12/1000 | Loss: 0.00004806
Iteration 13/1000 | Loss: 0.00003837
Iteration 14/1000 | Loss: 0.00004211
Iteration 15/1000 | Loss: 0.00003667
Iteration 16/1000 | Loss: 0.00004639
Iteration 17/1000 | Loss: 0.00004257
Iteration 18/1000 | Loss: 0.00004182
Iteration 19/1000 | Loss: 0.00003521
Iteration 20/1000 | Loss: 0.00023969
Iteration 21/1000 | Loss: 0.00018367
Iteration 22/1000 | Loss: 0.00013027
Iteration 23/1000 | Loss: 0.00032619
Iteration 24/1000 | Loss: 0.00025932
Iteration 25/1000 | Loss: 0.00015298
Iteration 26/1000 | Loss: 0.00008905
Iteration 27/1000 | Loss: 0.00013301
Iteration 28/1000 | Loss: 0.00017930
Iteration 29/1000 | Loss: 0.00029291
Iteration 30/1000 | Loss: 0.00022436
Iteration 31/1000 | Loss: 0.00020788
Iteration 32/1000 | Loss: 0.00030629
Iteration 33/1000 | Loss: 0.00017164
Iteration 34/1000 | Loss: 0.00017884
Iteration 35/1000 | Loss: 0.00012242
Iteration 36/1000 | Loss: 0.00012953
Iteration 37/1000 | Loss: 0.00029706
Iteration 38/1000 | Loss: 0.00012503
Iteration 39/1000 | Loss: 0.00021161
Iteration 40/1000 | Loss: 0.00013944
Iteration 41/1000 | Loss: 0.00026987
Iteration 42/1000 | Loss: 0.00023592
Iteration 43/1000 | Loss: 0.00018605
Iteration 44/1000 | Loss: 0.00030203
Iteration 45/1000 | Loss: 0.00034029
Iteration 46/1000 | Loss: 0.00015053
Iteration 47/1000 | Loss: 0.00004848
Iteration 48/1000 | Loss: 0.00004069
Iteration 49/1000 | Loss: 0.00015052
Iteration 50/1000 | Loss: 0.00004984
Iteration 51/1000 | Loss: 0.00005748
Iteration 52/1000 | Loss: 0.00012989
Iteration 53/1000 | Loss: 0.00014656
Iteration 54/1000 | Loss: 0.00013030
Iteration 55/1000 | Loss: 0.00013694
Iteration 56/1000 | Loss: 0.00010858
Iteration 57/1000 | Loss: 0.00014694
Iteration 58/1000 | Loss: 0.00014129
Iteration 59/1000 | Loss: 0.00016555
Iteration 60/1000 | Loss: 0.00017155
Iteration 61/1000 | Loss: 0.00013711
Iteration 62/1000 | Loss: 0.00011310
Iteration 63/1000 | Loss: 0.00011996
Iteration 64/1000 | Loss: 0.00016740
Iteration 65/1000 | Loss: 0.00013373
Iteration 66/1000 | Loss: 0.00009709
Iteration 67/1000 | Loss: 0.00018670
Iteration 68/1000 | Loss: 0.00010754
Iteration 69/1000 | Loss: 0.00012064
Iteration 70/1000 | Loss: 0.00014087
Iteration 71/1000 | Loss: 0.00009847
Iteration 72/1000 | Loss: 0.00011334
Iteration 73/1000 | Loss: 0.00014208
Iteration 74/1000 | Loss: 0.00015626
Iteration 75/1000 | Loss: 0.00012887
Iteration 76/1000 | Loss: 0.00016071
Iteration 77/1000 | Loss: 0.00014353
Iteration 78/1000 | Loss: 0.00010224
Iteration 79/1000 | Loss: 0.00005519
Iteration 80/1000 | Loss: 0.00019709
Iteration 81/1000 | Loss: 0.00015978
Iteration 82/1000 | Loss: 0.00011113
Iteration 83/1000 | Loss: 0.00013925
Iteration 84/1000 | Loss: 0.00013922
Iteration 85/1000 | Loss: 0.00017148
Iteration 86/1000 | Loss: 0.00014894
Iteration 87/1000 | Loss: 0.00013927
Iteration 88/1000 | Loss: 0.00005795
Iteration 89/1000 | Loss: 0.00010639
Iteration 90/1000 | Loss: 0.00016975
Iteration 91/1000 | Loss: 0.00015225
Iteration 92/1000 | Loss: 0.00008409
Iteration 93/1000 | Loss: 0.00005450
Iteration 94/1000 | Loss: 0.00004603
Iteration 95/1000 | Loss: 0.00008041
Iteration 96/1000 | Loss: 0.00015173
Iteration 97/1000 | Loss: 0.00006220
Iteration 98/1000 | Loss: 0.00005110
Iteration 99/1000 | Loss: 0.00003682
Iteration 100/1000 | Loss: 0.00004514
Iteration 101/1000 | Loss: 0.00015180
Iteration 102/1000 | Loss: 0.00004578
Iteration 103/1000 | Loss: 0.00003941
Iteration 104/1000 | Loss: 0.00003708
Iteration 105/1000 | Loss: 0.00004337
Iteration 106/1000 | Loss: 0.00003702
Iteration 107/1000 | Loss: 0.00003533
Iteration 108/1000 | Loss: 0.00005794
Iteration 109/1000 | Loss: 0.00014273
Iteration 110/1000 | Loss: 0.00007642
Iteration 111/1000 | Loss: 0.00007133
Iteration 112/1000 | Loss: 0.00010364
Iteration 113/1000 | Loss: 0.00005156
Iteration 114/1000 | Loss: 0.00011231
Iteration 115/1000 | Loss: 0.00008718
Iteration 116/1000 | Loss: 0.00014605
Iteration 117/1000 | Loss: 0.00010895
Iteration 118/1000 | Loss: 0.00013368
Iteration 119/1000 | Loss: 0.00007338
Iteration 120/1000 | Loss: 0.00014608
Iteration 121/1000 | Loss: 0.00009065
Iteration 122/1000 | Loss: 0.00011316
Iteration 123/1000 | Loss: 0.00010372
Iteration 124/1000 | Loss: 0.00011264
Iteration 125/1000 | Loss: 0.00010475
Iteration 126/1000 | Loss: 0.00013426
Iteration 127/1000 | Loss: 0.00009297
Iteration 128/1000 | Loss: 0.00013427
Iteration 129/1000 | Loss: 0.00009806
Iteration 130/1000 | Loss: 0.00005766
Iteration 131/1000 | Loss: 0.00013815
Iteration 132/1000 | Loss: 0.00006582
Iteration 133/1000 | Loss: 0.00010294
Iteration 134/1000 | Loss: 0.00004100
Iteration 135/1000 | Loss: 0.00012136
Iteration 136/1000 | Loss: 0.00015184
Iteration 137/1000 | Loss: 0.00013182
Iteration 138/1000 | Loss: 0.00013284
Iteration 139/1000 | Loss: 0.00007982
Iteration 140/1000 | Loss: 0.00009220
Iteration 141/1000 | Loss: 0.00015294
Iteration 142/1000 | Loss: 0.00011798
Iteration 143/1000 | Loss: 0.00015163
Iteration 144/1000 | Loss: 0.00011629
Iteration 145/1000 | Loss: 0.00007154
Iteration 146/1000 | Loss: 0.00010622
Iteration 147/1000 | Loss: 0.00012904
Iteration 148/1000 | Loss: 0.00012282
Iteration 149/1000 | Loss: 0.00003603
Iteration 150/1000 | Loss: 0.00020289
Iteration 151/1000 | Loss: 0.00004915
Iteration 152/1000 | Loss: 0.00017815
Iteration 153/1000 | Loss: 0.00011589
Iteration 154/1000 | Loss: 0.00012841
Iteration 155/1000 | Loss: 0.00062058
Iteration 156/1000 | Loss: 0.00017592
Iteration 157/1000 | Loss: 0.00004346
Iteration 158/1000 | Loss: 0.00021110
Iteration 159/1000 | Loss: 0.00056936
Iteration 160/1000 | Loss: 0.00008881
Iteration 161/1000 | Loss: 0.00055591
Iteration 162/1000 | Loss: 0.00007444
Iteration 163/1000 | Loss: 0.00051792
Iteration 164/1000 | Loss: 0.00016733
Iteration 165/1000 | Loss: 0.00012848
Iteration 166/1000 | Loss: 0.00021517
Iteration 167/1000 | Loss: 0.00013355
Iteration 168/1000 | Loss: 0.00022504
Iteration 169/1000 | Loss: 0.00030907
Iteration 170/1000 | Loss: 0.00003784
Iteration 171/1000 | Loss: 0.00003387
Iteration 172/1000 | Loss: 0.00003753
Iteration 173/1000 | Loss: 0.00003504
Iteration 174/1000 | Loss: 0.00004109
Iteration 175/1000 | Loss: 0.00003923
Iteration 176/1000 | Loss: 0.00003937
Iteration 177/1000 | Loss: 0.00004109
Iteration 178/1000 | Loss: 0.00003772
Iteration 179/1000 | Loss: 0.00003834
Iteration 180/1000 | Loss: 0.00002984
Iteration 181/1000 | Loss: 0.00004128
Iteration 182/1000 | Loss: 0.00004179
Iteration 183/1000 | Loss: 0.00003611
Iteration 184/1000 | Loss: 0.00003351
Iteration 185/1000 | Loss: 0.00003673
Iteration 186/1000 | Loss: 0.00003968
Iteration 187/1000 | Loss: 0.00003319
Iteration 188/1000 | Loss: 0.00004461
Iteration 189/1000 | Loss: 0.00003746
Iteration 190/1000 | Loss: 0.00003449
Iteration 191/1000 | Loss: 0.00003362
Iteration 192/1000 | Loss: 0.00003495
Iteration 193/1000 | Loss: 0.00003958
Iteration 194/1000 | Loss: 0.00004177
Iteration 195/1000 | Loss: 0.00003982
Iteration 196/1000 | Loss: 0.00004044
Iteration 197/1000 | Loss: 0.00003820
Iteration 198/1000 | Loss: 0.00003955
Iteration 199/1000 | Loss: 0.00003905
Iteration 200/1000 | Loss: 0.00003713
Iteration 201/1000 | Loss: 0.00003719
Iteration 202/1000 | Loss: 0.00003988
Iteration 203/1000 | Loss: 0.00003941
Iteration 204/1000 | Loss: 0.00003621
Iteration 205/1000 | Loss: 0.00003616
Iteration 206/1000 | Loss: 0.00003884
Iteration 207/1000 | Loss: 0.00004017
Iteration 208/1000 | Loss: 0.00002811
Iteration 209/1000 | Loss: 0.00003895
Iteration 210/1000 | Loss: 0.00004766
Iteration 211/1000 | Loss: 0.00003326
Iteration 212/1000 | Loss: 0.00003000
Iteration 213/1000 | Loss: 0.00002786
Iteration 214/1000 | Loss: 0.00002679
Iteration 215/1000 | Loss: 0.00002631
Iteration 216/1000 | Loss: 0.00002602
Iteration 217/1000 | Loss: 0.00002586
Iteration 218/1000 | Loss: 0.00002583
Iteration 219/1000 | Loss: 0.00002583
Iteration 220/1000 | Loss: 0.00002582
Iteration 221/1000 | Loss: 0.00002582
Iteration 222/1000 | Loss: 0.00002581
Iteration 223/1000 | Loss: 0.00002581
Iteration 224/1000 | Loss: 0.00002580
Iteration 225/1000 | Loss: 0.00002580
Iteration 226/1000 | Loss: 0.00002580
Iteration 227/1000 | Loss: 0.00002578
Iteration 228/1000 | Loss: 0.00002578
Iteration 229/1000 | Loss: 0.00002578
Iteration 230/1000 | Loss: 0.00002578
Iteration 231/1000 | Loss: 0.00002577
Iteration 232/1000 | Loss: 0.00002577
Iteration 233/1000 | Loss: 0.00002577
Iteration 234/1000 | Loss: 0.00002574
Iteration 235/1000 | Loss: 0.00002574
Iteration 236/1000 | Loss: 0.00002564
Iteration 237/1000 | Loss: 0.00002563
Iteration 238/1000 | Loss: 0.00002563
Iteration 239/1000 | Loss: 0.00002563
Iteration 240/1000 | Loss: 0.00002563
Iteration 241/1000 | Loss: 0.00002563
Iteration 242/1000 | Loss: 0.00002562
Iteration 243/1000 | Loss: 0.00002562
Iteration 244/1000 | Loss: 0.00002562
Iteration 245/1000 | Loss: 0.00002561
Iteration 246/1000 | Loss: 0.00002560
Iteration 247/1000 | Loss: 0.00002560
Iteration 248/1000 | Loss: 0.00002560
Iteration 249/1000 | Loss: 0.00002560
Iteration 250/1000 | Loss: 0.00002560
Iteration 251/1000 | Loss: 0.00002560
Iteration 252/1000 | Loss: 0.00002560
Iteration 253/1000 | Loss: 0.00002560
Iteration 254/1000 | Loss: 0.00002560
Iteration 255/1000 | Loss: 0.00002559
Iteration 256/1000 | Loss: 0.00002559
Iteration 257/1000 | Loss: 0.00002559
Iteration 258/1000 | Loss: 0.00002559
Iteration 259/1000 | Loss: 0.00002559
Iteration 260/1000 | Loss: 0.00002559
Iteration 261/1000 | Loss: 0.00002559
Iteration 262/1000 | Loss: 0.00002557
Iteration 263/1000 | Loss: 0.00002557
Iteration 264/1000 | Loss: 0.00002556
Iteration 265/1000 | Loss: 0.00002556
Iteration 266/1000 | Loss: 0.00002556
Iteration 267/1000 | Loss: 0.00002556
Iteration 268/1000 | Loss: 0.00002556
Iteration 269/1000 | Loss: 0.00002556
Iteration 270/1000 | Loss: 0.00002556
Iteration 271/1000 | Loss: 0.00002556
Iteration 272/1000 | Loss: 0.00002556
Iteration 273/1000 | Loss: 0.00002555
Iteration 274/1000 | Loss: 0.00002555
Iteration 275/1000 | Loss: 0.00002555
Iteration 276/1000 | Loss: 0.00002555
Iteration 277/1000 | Loss: 0.00002555
Iteration 278/1000 | Loss: 0.00002555
Iteration 279/1000 | Loss: 0.00002554
Iteration 280/1000 | Loss: 0.00002554
Iteration 281/1000 | Loss: 0.00002554
Iteration 282/1000 | Loss: 0.00002554
Iteration 283/1000 | Loss: 0.00002554
Iteration 284/1000 | Loss: 0.00002554
Iteration 285/1000 | Loss: 0.00002554
Iteration 286/1000 | Loss: 0.00002553
Iteration 287/1000 | Loss: 0.00002553
Iteration 288/1000 | Loss: 0.00002553
Iteration 289/1000 | Loss: 0.00002553
Iteration 290/1000 | Loss: 0.00002553
Iteration 291/1000 | Loss: 0.00002553
Iteration 292/1000 | Loss: 0.00002553
Iteration 293/1000 | Loss: 0.00002553
Iteration 294/1000 | Loss: 0.00002553
Iteration 295/1000 | Loss: 0.00002553
Iteration 296/1000 | Loss: 0.00002552
Iteration 297/1000 | Loss: 0.00002552
Iteration 298/1000 | Loss: 0.00002552
Iteration 299/1000 | Loss: 0.00002552
Iteration 300/1000 | Loss: 0.00002552
Iteration 301/1000 | Loss: 0.00002552
Iteration 302/1000 | Loss: 0.00002552
Iteration 303/1000 | Loss: 0.00002552
Iteration 304/1000 | Loss: 0.00002552
Iteration 305/1000 | Loss: 0.00002552
Iteration 306/1000 | Loss: 0.00002552
Iteration 307/1000 | Loss: 0.00002551
Iteration 308/1000 | Loss: 0.00002551
Iteration 309/1000 | Loss: 0.00002551
Iteration 310/1000 | Loss: 0.00002551
Iteration 311/1000 | Loss: 0.00002551
Iteration 312/1000 | Loss: 0.00002551
Iteration 313/1000 | Loss: 0.00002550
Iteration 314/1000 | Loss: 0.00002550
Iteration 315/1000 | Loss: 0.00002550
Iteration 316/1000 | Loss: 0.00002550
Iteration 317/1000 | Loss: 0.00002550
Iteration 318/1000 | Loss: 0.00002549
Iteration 319/1000 | Loss: 0.00002549
Iteration 320/1000 | Loss: 0.00002549
Iteration 321/1000 | Loss: 0.00002549
Iteration 322/1000 | Loss: 0.00002549
Iteration 323/1000 | Loss: 0.00002549
Iteration 324/1000 | Loss: 0.00002549
Iteration 325/1000 | Loss: 0.00002549
Iteration 326/1000 | Loss: 0.00002549
Iteration 327/1000 | Loss: 0.00002549
Iteration 328/1000 | Loss: 0.00002549
Iteration 329/1000 | Loss: 0.00002549
Iteration 330/1000 | Loss: 0.00002549
Iteration 331/1000 | Loss: 0.00002549
Iteration 332/1000 | Loss: 0.00002549
Iteration 333/1000 | Loss: 0.00002549
Iteration 334/1000 | Loss: 0.00002549
Iteration 335/1000 | Loss: 0.00002549
Iteration 336/1000 | Loss: 0.00002549
Iteration 337/1000 | Loss: 0.00002549
Iteration 338/1000 | Loss: 0.00002549
Iteration 339/1000 | Loss: 0.00002549
Iteration 340/1000 | Loss: 0.00002549
Iteration 341/1000 | Loss: 0.00002549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [2.5489323888905346e-05, 2.5489323888905346e-05, 2.5489323888905346e-05, 2.5489323888905346e-05, 2.5489323888905346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5489323888905346e-05

Optimization complete. Final v2v error: 4.0934576988220215 mm

Highest mean error: 5.59950590133667 mm for frame 150

Lowest mean error: 3.2689714431762695 mm for frame 0

Saving results

Total time: 379.14591336250305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2087/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2087/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412674
Iteration 2/25 | Loss: 0.00119212
Iteration 3/25 | Loss: 0.00110378
Iteration 4/25 | Loss: 0.00108901
Iteration 5/25 | Loss: 0.00108490
Iteration 6/25 | Loss: 0.00108368
Iteration 7/25 | Loss: 0.00108362
Iteration 8/25 | Loss: 0.00108362
Iteration 9/25 | Loss: 0.00108362
Iteration 10/25 | Loss: 0.00108362
Iteration 11/25 | Loss: 0.00108362
Iteration 12/25 | Loss: 0.00108362
Iteration 13/25 | Loss: 0.00108362
Iteration 14/25 | Loss: 0.00108362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010836165165528655, 0.0010836165165528655, 0.0010836165165528655, 0.0010836165165528655, 0.0010836165165528655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010836165165528655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32278073
Iteration 2/25 | Loss: 0.00163781
Iteration 3/25 | Loss: 0.00163781
Iteration 4/25 | Loss: 0.00163781
Iteration 5/25 | Loss: 0.00163781
Iteration 6/25 | Loss: 0.00163780
Iteration 7/25 | Loss: 0.00163780
Iteration 8/25 | Loss: 0.00163780
Iteration 9/25 | Loss: 0.00163780
Iteration 10/25 | Loss: 0.00163780
Iteration 11/25 | Loss: 0.00163780
Iteration 12/25 | Loss: 0.00163780
Iteration 13/25 | Loss: 0.00163780
Iteration 14/25 | Loss: 0.00163780
Iteration 15/25 | Loss: 0.00163780
Iteration 16/25 | Loss: 0.00163780
Iteration 17/25 | Loss: 0.00163780
Iteration 18/25 | Loss: 0.00163780
Iteration 19/25 | Loss: 0.00163780
Iteration 20/25 | Loss: 0.00163780
Iteration 21/25 | Loss: 0.00163780
Iteration 22/25 | Loss: 0.00163780
Iteration 23/25 | Loss: 0.00163780
Iteration 24/25 | Loss: 0.00163780
Iteration 25/25 | Loss: 0.00163780

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163780
Iteration 2/1000 | Loss: 0.00004173
Iteration 3/1000 | Loss: 0.00002672
Iteration 4/1000 | Loss: 0.00002360
Iteration 5/1000 | Loss: 0.00002223
Iteration 6/1000 | Loss: 0.00002131
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00002031
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001970
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001947
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001929
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001926
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00001924
Iteration 25/1000 | Loss: 0.00001924
Iteration 26/1000 | Loss: 0.00001924
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001921
Iteration 44/1000 | Loss: 0.00001921
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001918
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001914
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001913
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001911
Iteration 79/1000 | Loss: 0.00001911
Iteration 80/1000 | Loss: 0.00001911
Iteration 81/1000 | Loss: 0.00001911
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001911
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001911
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001909
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001908
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001907
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.90405517059844e-05, 1.90405517059844e-05, 1.90405517059844e-05, 1.90405517059844e-05, 1.90405517059844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.90405517059844e-05

Optimization complete. Final v2v error: 3.6668167114257812 mm

Highest mean error: 4.072307586669922 mm for frame 13

Lowest mean error: 3.2343320846557617 mm for frame 1

Saving results

Total time: 35.77799654006958
