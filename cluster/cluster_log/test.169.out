Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=169, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9464-9519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449667
Iteration 2/25 | Loss: 0.00114953
Iteration 3/25 | Loss: 0.00101929
Iteration 4/25 | Loss: 0.00099856
Iteration 5/25 | Loss: 0.00099216
Iteration 6/25 | Loss: 0.00099039
Iteration 7/25 | Loss: 0.00099031
Iteration 8/25 | Loss: 0.00099031
Iteration 9/25 | Loss: 0.00099031
Iteration 10/25 | Loss: 0.00099031
Iteration 11/25 | Loss: 0.00099031
Iteration 12/25 | Loss: 0.00099031
Iteration 13/25 | Loss: 0.00099031
Iteration 14/25 | Loss: 0.00099031
Iteration 15/25 | Loss: 0.00099031
Iteration 16/25 | Loss: 0.00099031
Iteration 17/25 | Loss: 0.00099031
Iteration 18/25 | Loss: 0.00099031
Iteration 19/25 | Loss: 0.00099031
Iteration 20/25 | Loss: 0.00099031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009903095196932554, 0.0009903095196932554, 0.0009903095196932554, 0.0009903095196932554, 0.0009903095196932554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009903095196932554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32917690
Iteration 2/25 | Loss: 0.00067037
Iteration 3/25 | Loss: 0.00067036
Iteration 4/25 | Loss: 0.00067036
Iteration 5/25 | Loss: 0.00067036
Iteration 6/25 | Loss: 0.00067036
Iteration 7/25 | Loss: 0.00067036
Iteration 8/25 | Loss: 0.00067036
Iteration 9/25 | Loss: 0.00067036
Iteration 10/25 | Loss: 0.00067036
Iteration 11/25 | Loss: 0.00067036
Iteration 12/25 | Loss: 0.00067036
Iteration 13/25 | Loss: 0.00067036
Iteration 14/25 | Loss: 0.00067036
Iteration 15/25 | Loss: 0.00067036
Iteration 16/25 | Loss: 0.00067036
Iteration 17/25 | Loss: 0.00067036
Iteration 18/25 | Loss: 0.00067036
Iteration 19/25 | Loss: 0.00067036
Iteration 20/25 | Loss: 0.00067036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006703594117425382, 0.0006703594117425382, 0.0006703594117425382, 0.0006703594117425382, 0.0006703594117425382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006703594117425382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067036
Iteration 2/1000 | Loss: 0.00003012
Iteration 3/1000 | Loss: 0.00001860
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001283
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001218
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001189
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001188
Iteration 94/1000 | Loss: 0.00001188
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001185
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001180
Iteration 150/1000 | Loss: 0.00001180
Iteration 151/1000 | Loss: 0.00001180
Iteration 152/1000 | Loss: 0.00001180
Iteration 153/1000 | Loss: 0.00001179
Iteration 154/1000 | Loss: 0.00001179
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001178
Iteration 157/1000 | Loss: 0.00001178
Iteration 158/1000 | Loss: 0.00001177
Iteration 159/1000 | Loss: 0.00001177
Iteration 160/1000 | Loss: 0.00001177
Iteration 161/1000 | Loss: 0.00001177
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001175
Iteration 172/1000 | Loss: 0.00001175
Iteration 173/1000 | Loss: 0.00001175
Iteration 174/1000 | Loss: 0.00001175
Iteration 175/1000 | Loss: 0.00001175
Iteration 176/1000 | Loss: 0.00001175
Iteration 177/1000 | Loss: 0.00001175
Iteration 178/1000 | Loss: 0.00001175
Iteration 179/1000 | Loss: 0.00001174
Iteration 180/1000 | Loss: 0.00001174
Iteration 181/1000 | Loss: 0.00001174
Iteration 182/1000 | Loss: 0.00001174
Iteration 183/1000 | Loss: 0.00001174
Iteration 184/1000 | Loss: 0.00001174
Iteration 185/1000 | Loss: 0.00001174
Iteration 186/1000 | Loss: 0.00001173
Iteration 187/1000 | Loss: 0.00001173
Iteration 188/1000 | Loss: 0.00001173
Iteration 189/1000 | Loss: 0.00001173
Iteration 190/1000 | Loss: 0.00001173
Iteration 191/1000 | Loss: 0.00001173
Iteration 192/1000 | Loss: 0.00001173
Iteration 193/1000 | Loss: 0.00001173
Iteration 194/1000 | Loss: 0.00001173
Iteration 195/1000 | Loss: 0.00001173
Iteration 196/1000 | Loss: 0.00001172
Iteration 197/1000 | Loss: 0.00001172
Iteration 198/1000 | Loss: 0.00001172
Iteration 199/1000 | Loss: 0.00001172
Iteration 200/1000 | Loss: 0.00001172
Iteration 201/1000 | Loss: 0.00001172
Iteration 202/1000 | Loss: 0.00001172
Iteration 203/1000 | Loss: 0.00001172
Iteration 204/1000 | Loss: 0.00001172
Iteration 205/1000 | Loss: 0.00001172
Iteration 206/1000 | Loss: 0.00001172
Iteration 207/1000 | Loss: 0.00001171
Iteration 208/1000 | Loss: 0.00001171
Iteration 209/1000 | Loss: 0.00001171
Iteration 210/1000 | Loss: 0.00001171
Iteration 211/1000 | Loss: 0.00001171
Iteration 212/1000 | Loss: 0.00001171
Iteration 213/1000 | Loss: 0.00001171
Iteration 214/1000 | Loss: 0.00001171
Iteration 215/1000 | Loss: 0.00001171
Iteration 216/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.1714959327946417e-05, 1.1714959327946417e-05, 1.1714959327946417e-05, 1.1714959327946417e-05, 1.1714959327946417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1714959327946417e-05

Optimization complete. Final v2v error: 2.9162757396698 mm

Highest mean error: 3.4044418334960938 mm for frame 41

Lowest mean error: 2.491239070892334 mm for frame 83

Saving results

Total time: 41.414183139801025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971611
Iteration 2/25 | Loss: 0.00167247
Iteration 3/25 | Loss: 0.00127923
Iteration 4/25 | Loss: 0.00123109
Iteration 5/25 | Loss: 0.00121938
Iteration 6/25 | Loss: 0.00121573
Iteration 7/25 | Loss: 0.00121474
Iteration 8/25 | Loss: 0.00121454
Iteration 9/25 | Loss: 0.00121454
Iteration 10/25 | Loss: 0.00121454
Iteration 11/25 | Loss: 0.00121454
Iteration 12/25 | Loss: 0.00121454
Iteration 13/25 | Loss: 0.00121454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012145355576649308, 0.0012145355576649308, 0.0012145355576649308, 0.0012145355576649308, 0.0012145355576649308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012145355576649308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23612928
Iteration 2/25 | Loss: 0.00082399
Iteration 3/25 | Loss: 0.00082396
Iteration 4/25 | Loss: 0.00082395
Iteration 5/25 | Loss: 0.00082395
Iteration 6/25 | Loss: 0.00082395
Iteration 7/25 | Loss: 0.00082395
Iteration 8/25 | Loss: 0.00082395
Iteration 9/25 | Loss: 0.00082395
Iteration 10/25 | Loss: 0.00082395
Iteration 11/25 | Loss: 0.00082395
Iteration 12/25 | Loss: 0.00082395
Iteration 13/25 | Loss: 0.00082395
Iteration 14/25 | Loss: 0.00082395
Iteration 15/25 | Loss: 0.00082395
Iteration 16/25 | Loss: 0.00082395
Iteration 17/25 | Loss: 0.00082395
Iteration 18/25 | Loss: 0.00082395
Iteration 19/25 | Loss: 0.00082395
Iteration 20/25 | Loss: 0.00082395
Iteration 21/25 | Loss: 0.00082395
Iteration 22/25 | Loss: 0.00082395
Iteration 23/25 | Loss: 0.00082395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008239530143328011, 0.0008239530143328011, 0.0008239530143328011, 0.0008239530143328011, 0.0008239530143328011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008239530143328011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082395
Iteration 2/1000 | Loss: 0.00008197
Iteration 3/1000 | Loss: 0.00005083
Iteration 4/1000 | Loss: 0.00004406
Iteration 5/1000 | Loss: 0.00004203
Iteration 6/1000 | Loss: 0.00004077
Iteration 7/1000 | Loss: 0.00003978
Iteration 8/1000 | Loss: 0.00003908
Iteration 9/1000 | Loss: 0.00003849
Iteration 10/1000 | Loss: 0.00003794
Iteration 11/1000 | Loss: 0.00003751
Iteration 12/1000 | Loss: 0.00003719
Iteration 13/1000 | Loss: 0.00003690
Iteration 14/1000 | Loss: 0.00003673
Iteration 15/1000 | Loss: 0.00003654
Iteration 16/1000 | Loss: 0.00003641
Iteration 17/1000 | Loss: 0.00003625
Iteration 18/1000 | Loss: 0.00003611
Iteration 19/1000 | Loss: 0.00003609
Iteration 20/1000 | Loss: 0.00003601
Iteration 21/1000 | Loss: 0.00003590
Iteration 22/1000 | Loss: 0.00003587
Iteration 23/1000 | Loss: 0.00003586
Iteration 24/1000 | Loss: 0.00003581
Iteration 25/1000 | Loss: 0.00003578
Iteration 26/1000 | Loss: 0.00003577
Iteration 27/1000 | Loss: 0.00003577
Iteration 28/1000 | Loss: 0.00003576
Iteration 29/1000 | Loss: 0.00003572
Iteration 30/1000 | Loss: 0.00003572
Iteration 31/1000 | Loss: 0.00003569
Iteration 32/1000 | Loss: 0.00003569
Iteration 33/1000 | Loss: 0.00003569
Iteration 34/1000 | Loss: 0.00003569
Iteration 35/1000 | Loss: 0.00003569
Iteration 36/1000 | Loss: 0.00003569
Iteration 37/1000 | Loss: 0.00003569
Iteration 38/1000 | Loss: 0.00003569
Iteration 39/1000 | Loss: 0.00003568
Iteration 40/1000 | Loss: 0.00003568
Iteration 41/1000 | Loss: 0.00003568
Iteration 42/1000 | Loss: 0.00003568
Iteration 43/1000 | Loss: 0.00003568
Iteration 44/1000 | Loss: 0.00003568
Iteration 45/1000 | Loss: 0.00003568
Iteration 46/1000 | Loss: 0.00003568
Iteration 47/1000 | Loss: 0.00003567
Iteration 48/1000 | Loss: 0.00003566
Iteration 49/1000 | Loss: 0.00003565
Iteration 50/1000 | Loss: 0.00003565
Iteration 51/1000 | Loss: 0.00003564
Iteration 52/1000 | Loss: 0.00003564
Iteration 53/1000 | Loss: 0.00003564
Iteration 54/1000 | Loss: 0.00003564
Iteration 55/1000 | Loss: 0.00003564
Iteration 56/1000 | Loss: 0.00003563
Iteration 57/1000 | Loss: 0.00003563
Iteration 58/1000 | Loss: 0.00003563
Iteration 59/1000 | Loss: 0.00003563
Iteration 60/1000 | Loss: 0.00003562
Iteration 61/1000 | Loss: 0.00003562
Iteration 62/1000 | Loss: 0.00003562
Iteration 63/1000 | Loss: 0.00003562
Iteration 64/1000 | Loss: 0.00003562
Iteration 65/1000 | Loss: 0.00003561
Iteration 66/1000 | Loss: 0.00003561
Iteration 67/1000 | Loss: 0.00003561
Iteration 68/1000 | Loss: 0.00003561
Iteration 69/1000 | Loss: 0.00003560
Iteration 70/1000 | Loss: 0.00003560
Iteration 71/1000 | Loss: 0.00003560
Iteration 72/1000 | Loss: 0.00003559
Iteration 73/1000 | Loss: 0.00003559
Iteration 74/1000 | Loss: 0.00003559
Iteration 75/1000 | Loss: 0.00003559
Iteration 76/1000 | Loss: 0.00003559
Iteration 77/1000 | Loss: 0.00003559
Iteration 78/1000 | Loss: 0.00003559
Iteration 79/1000 | Loss: 0.00003559
Iteration 80/1000 | Loss: 0.00003559
Iteration 81/1000 | Loss: 0.00003559
Iteration 82/1000 | Loss: 0.00003558
Iteration 83/1000 | Loss: 0.00003558
Iteration 84/1000 | Loss: 0.00003558
Iteration 85/1000 | Loss: 0.00003558
Iteration 86/1000 | Loss: 0.00003558
Iteration 87/1000 | Loss: 0.00003557
Iteration 88/1000 | Loss: 0.00003557
Iteration 89/1000 | Loss: 0.00003557
Iteration 90/1000 | Loss: 0.00003557
Iteration 91/1000 | Loss: 0.00003557
Iteration 92/1000 | Loss: 0.00003557
Iteration 93/1000 | Loss: 0.00003557
Iteration 94/1000 | Loss: 0.00003557
Iteration 95/1000 | Loss: 0.00003557
Iteration 96/1000 | Loss: 0.00003556
Iteration 97/1000 | Loss: 0.00003556
Iteration 98/1000 | Loss: 0.00003556
Iteration 99/1000 | Loss: 0.00003556
Iteration 100/1000 | Loss: 0.00003556
Iteration 101/1000 | Loss: 0.00003556
Iteration 102/1000 | Loss: 0.00003556
Iteration 103/1000 | Loss: 0.00003556
Iteration 104/1000 | Loss: 0.00003556
Iteration 105/1000 | Loss: 0.00003556
Iteration 106/1000 | Loss: 0.00003556
Iteration 107/1000 | Loss: 0.00003555
Iteration 108/1000 | Loss: 0.00003555
Iteration 109/1000 | Loss: 0.00003555
Iteration 110/1000 | Loss: 0.00003555
Iteration 111/1000 | Loss: 0.00003555
Iteration 112/1000 | Loss: 0.00003555
Iteration 113/1000 | Loss: 0.00003555
Iteration 114/1000 | Loss: 0.00003555
Iteration 115/1000 | Loss: 0.00003555
Iteration 116/1000 | Loss: 0.00003555
Iteration 117/1000 | Loss: 0.00003555
Iteration 118/1000 | Loss: 0.00003554
Iteration 119/1000 | Loss: 0.00003554
Iteration 120/1000 | Loss: 0.00003554
Iteration 121/1000 | Loss: 0.00003554
Iteration 122/1000 | Loss: 0.00003554
Iteration 123/1000 | Loss: 0.00003554
Iteration 124/1000 | Loss: 0.00003554
Iteration 125/1000 | Loss: 0.00003554
Iteration 126/1000 | Loss: 0.00003554
Iteration 127/1000 | Loss: 0.00003554
Iteration 128/1000 | Loss: 0.00003554
Iteration 129/1000 | Loss: 0.00003554
Iteration 130/1000 | Loss: 0.00003553
Iteration 131/1000 | Loss: 0.00003553
Iteration 132/1000 | Loss: 0.00003553
Iteration 133/1000 | Loss: 0.00003553
Iteration 134/1000 | Loss: 0.00003553
Iteration 135/1000 | Loss: 0.00003553
Iteration 136/1000 | Loss: 0.00003553
Iteration 137/1000 | Loss: 0.00003552
Iteration 138/1000 | Loss: 0.00003552
Iteration 139/1000 | Loss: 0.00003552
Iteration 140/1000 | Loss: 0.00003552
Iteration 141/1000 | Loss: 0.00003552
Iteration 142/1000 | Loss: 0.00003552
Iteration 143/1000 | Loss: 0.00003552
Iteration 144/1000 | Loss: 0.00003551
Iteration 145/1000 | Loss: 0.00003551
Iteration 146/1000 | Loss: 0.00003551
Iteration 147/1000 | Loss: 0.00003551
Iteration 148/1000 | Loss: 0.00003551
Iteration 149/1000 | Loss: 0.00003551
Iteration 150/1000 | Loss: 0.00003551
Iteration 151/1000 | Loss: 0.00003551
Iteration 152/1000 | Loss: 0.00003551
Iteration 153/1000 | Loss: 0.00003551
Iteration 154/1000 | Loss: 0.00003551
Iteration 155/1000 | Loss: 0.00003551
Iteration 156/1000 | Loss: 0.00003551
Iteration 157/1000 | Loss: 0.00003550
Iteration 158/1000 | Loss: 0.00003550
Iteration 159/1000 | Loss: 0.00003550
Iteration 160/1000 | Loss: 0.00003550
Iteration 161/1000 | Loss: 0.00003550
Iteration 162/1000 | Loss: 0.00003550
Iteration 163/1000 | Loss: 0.00003550
Iteration 164/1000 | Loss: 0.00003550
Iteration 165/1000 | Loss: 0.00003550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.5502012906363234e-05, 3.5502012906363234e-05, 3.5502012906363234e-05, 3.5502012906363234e-05, 3.5502012906363234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5502012906363234e-05

Optimization complete. Final v2v error: 4.701916694641113 mm

Highest mean error: 5.8594651222229 mm for frame 123

Lowest mean error: 3.6382765769958496 mm for frame 111

Saving results

Total time: 59.81594729423523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693648
Iteration 2/25 | Loss: 0.00134779
Iteration 3/25 | Loss: 0.00112171
Iteration 4/25 | Loss: 0.00100687
Iteration 5/25 | Loss: 0.00100925
Iteration 6/25 | Loss: 0.00098447
Iteration 7/25 | Loss: 0.00098227
Iteration 8/25 | Loss: 0.00098170
Iteration 9/25 | Loss: 0.00098141
Iteration 10/25 | Loss: 0.00098129
Iteration 11/25 | Loss: 0.00098128
Iteration 12/25 | Loss: 0.00098128
Iteration 13/25 | Loss: 0.00098128
Iteration 14/25 | Loss: 0.00098128
Iteration 15/25 | Loss: 0.00098128
Iteration 16/25 | Loss: 0.00098127
Iteration 17/25 | Loss: 0.00098127
Iteration 18/25 | Loss: 0.00098127
Iteration 19/25 | Loss: 0.00098127
Iteration 20/25 | Loss: 0.00098127
Iteration 21/25 | Loss: 0.00098127
Iteration 22/25 | Loss: 0.00098127
Iteration 23/25 | Loss: 0.00098127
Iteration 24/25 | Loss: 0.00098127
Iteration 25/25 | Loss: 0.00098127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70164704
Iteration 2/25 | Loss: 0.00087868
Iteration 3/25 | Loss: 0.00085569
Iteration 4/25 | Loss: 0.00085569
Iteration 5/25 | Loss: 0.00085569
Iteration 6/25 | Loss: 0.00085569
Iteration 7/25 | Loss: 0.00085569
Iteration 8/25 | Loss: 0.00085569
Iteration 9/25 | Loss: 0.00085569
Iteration 10/25 | Loss: 0.00085569
Iteration 11/25 | Loss: 0.00085569
Iteration 12/25 | Loss: 0.00085569
Iteration 13/25 | Loss: 0.00085569
Iteration 14/25 | Loss: 0.00085569
Iteration 15/25 | Loss: 0.00085569
Iteration 16/25 | Loss: 0.00085569
Iteration 17/25 | Loss: 0.00085569
Iteration 18/25 | Loss: 0.00085569
Iteration 19/25 | Loss: 0.00085569
Iteration 20/25 | Loss: 0.00085569
Iteration 21/25 | Loss: 0.00085569
Iteration 22/25 | Loss: 0.00085569
Iteration 23/25 | Loss: 0.00085569
Iteration 24/25 | Loss: 0.00085569
Iteration 25/25 | Loss: 0.00085569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008556860848329961, 0.0008556860848329961, 0.0008556860848329961, 0.0008556860848329961, 0.0008556860848329961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008556860848329961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085569
Iteration 2/1000 | Loss: 0.00005381
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001311
Iteration 5/1000 | Loss: 0.00001228
Iteration 6/1000 | Loss: 0.00001193
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001143
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001129
Iteration 11/1000 | Loss: 0.00003763
Iteration 12/1000 | Loss: 0.00001110
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001097
Iteration 15/1000 | Loss: 0.00001096
Iteration 16/1000 | Loss: 0.00001090
Iteration 17/1000 | Loss: 0.00001089
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001083
Iteration 24/1000 | Loss: 0.00001082
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001081
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001080
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001076
Iteration 45/1000 | Loss: 0.00001076
Iteration 46/1000 | Loss: 0.00001076
Iteration 47/1000 | Loss: 0.00001076
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001072
Iteration 60/1000 | Loss: 0.00001072
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001071
Iteration 63/1000 | Loss: 0.00001071
Iteration 64/1000 | Loss: 0.00004640
Iteration 65/1000 | Loss: 0.00001071
Iteration 66/1000 | Loss: 0.00001070
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001069
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001068
Iteration 75/1000 | Loss: 0.00001068
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001067
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00001066
Iteration 90/1000 | Loss: 0.00001065
Iteration 91/1000 | Loss: 0.00001065
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001065
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001065
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001062
Iteration 121/1000 | Loss: 0.00001062
Iteration 122/1000 | Loss: 0.00001062
Iteration 123/1000 | Loss: 0.00001062
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001062
Iteration 126/1000 | Loss: 0.00001062
Iteration 127/1000 | Loss: 0.00001062
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001061
Iteration 132/1000 | Loss: 0.00001061
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001058
Iteration 142/1000 | Loss: 0.00001058
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001057
Iteration 145/1000 | Loss: 0.00001057
Iteration 146/1000 | Loss: 0.00001057
Iteration 147/1000 | Loss: 0.00001057
Iteration 148/1000 | Loss: 0.00001057
Iteration 149/1000 | Loss: 0.00001057
Iteration 150/1000 | Loss: 0.00001057
Iteration 151/1000 | Loss: 0.00001057
Iteration 152/1000 | Loss: 0.00001057
Iteration 153/1000 | Loss: 0.00001057
Iteration 154/1000 | Loss: 0.00001057
Iteration 155/1000 | Loss: 0.00001056
Iteration 156/1000 | Loss: 0.00001056
Iteration 157/1000 | Loss: 0.00001056
Iteration 158/1000 | Loss: 0.00001056
Iteration 159/1000 | Loss: 0.00001056
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001056
Iteration 164/1000 | Loss: 0.00001056
Iteration 165/1000 | Loss: 0.00001056
Iteration 166/1000 | Loss: 0.00001056
Iteration 167/1000 | Loss: 0.00001056
Iteration 168/1000 | Loss: 0.00001056
Iteration 169/1000 | Loss: 0.00001056
Iteration 170/1000 | Loss: 0.00001056
Iteration 171/1000 | Loss: 0.00001056
Iteration 172/1000 | Loss: 0.00001056
Iteration 173/1000 | Loss: 0.00001056
Iteration 174/1000 | Loss: 0.00001055
Iteration 175/1000 | Loss: 0.00001055
Iteration 176/1000 | Loss: 0.00001055
Iteration 177/1000 | Loss: 0.00001055
Iteration 178/1000 | Loss: 0.00001055
Iteration 179/1000 | Loss: 0.00001055
Iteration 180/1000 | Loss: 0.00001055
Iteration 181/1000 | Loss: 0.00001055
Iteration 182/1000 | Loss: 0.00001055
Iteration 183/1000 | Loss: 0.00001055
Iteration 184/1000 | Loss: 0.00001055
Iteration 185/1000 | Loss: 0.00001055
Iteration 186/1000 | Loss: 0.00001055
Iteration 187/1000 | Loss: 0.00001055
Iteration 188/1000 | Loss: 0.00001055
Iteration 189/1000 | Loss: 0.00001055
Iteration 190/1000 | Loss: 0.00001055
Iteration 191/1000 | Loss: 0.00001055
Iteration 192/1000 | Loss: 0.00001054
Iteration 193/1000 | Loss: 0.00001054
Iteration 194/1000 | Loss: 0.00001054
Iteration 195/1000 | Loss: 0.00001054
Iteration 196/1000 | Loss: 0.00001054
Iteration 197/1000 | Loss: 0.00001054
Iteration 198/1000 | Loss: 0.00001054
Iteration 199/1000 | Loss: 0.00001054
Iteration 200/1000 | Loss: 0.00001054
Iteration 201/1000 | Loss: 0.00001054
Iteration 202/1000 | Loss: 0.00001054
Iteration 203/1000 | Loss: 0.00001054
Iteration 204/1000 | Loss: 0.00001054
Iteration 205/1000 | Loss: 0.00001054
Iteration 206/1000 | Loss: 0.00001054
Iteration 207/1000 | Loss: 0.00001054
Iteration 208/1000 | Loss: 0.00001054
Iteration 209/1000 | Loss: 0.00001054
Iteration 210/1000 | Loss: 0.00001054
Iteration 211/1000 | Loss: 0.00001054
Iteration 212/1000 | Loss: 0.00001054
Iteration 213/1000 | Loss: 0.00001054
Iteration 214/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.0538614333199803e-05, 1.0538614333199803e-05, 1.0538614333199803e-05, 1.0538614333199803e-05, 1.0538614333199803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0538614333199803e-05

Optimization complete. Final v2v error: 2.7574753761291504 mm

Highest mean error: 3.1938278675079346 mm for frame 83

Lowest mean error: 2.3751566410064697 mm for frame 44

Saving results

Total time: 49.92809438705444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801908
Iteration 2/25 | Loss: 0.00106892
Iteration 3/25 | Loss: 0.00095890
Iteration 4/25 | Loss: 0.00094616
Iteration 5/25 | Loss: 0.00094358
Iteration 6/25 | Loss: 0.00094335
Iteration 7/25 | Loss: 0.00094335
Iteration 8/25 | Loss: 0.00094335
Iteration 9/25 | Loss: 0.00094335
Iteration 10/25 | Loss: 0.00094335
Iteration 11/25 | Loss: 0.00094335
Iteration 12/25 | Loss: 0.00094335
Iteration 13/25 | Loss: 0.00094335
Iteration 14/25 | Loss: 0.00094335
Iteration 15/25 | Loss: 0.00094335
Iteration 16/25 | Loss: 0.00094335
Iteration 17/25 | Loss: 0.00094335
Iteration 18/25 | Loss: 0.00094335
Iteration 19/25 | Loss: 0.00094335
Iteration 20/25 | Loss: 0.00094335
Iteration 21/25 | Loss: 0.00094335
Iteration 22/25 | Loss: 0.00094335
Iteration 23/25 | Loss: 0.00094335
Iteration 24/25 | Loss: 0.00094335
Iteration 25/25 | Loss: 0.00094335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38721013
Iteration 2/25 | Loss: 0.00067831
Iteration 3/25 | Loss: 0.00067831
Iteration 4/25 | Loss: 0.00067831
Iteration 5/25 | Loss: 0.00067831
Iteration 6/25 | Loss: 0.00067831
Iteration 7/25 | Loss: 0.00067831
Iteration 8/25 | Loss: 0.00067831
Iteration 9/25 | Loss: 0.00067831
Iteration 10/25 | Loss: 0.00067831
Iteration 11/25 | Loss: 0.00067831
Iteration 12/25 | Loss: 0.00067831
Iteration 13/25 | Loss: 0.00067831
Iteration 14/25 | Loss: 0.00067831
Iteration 15/25 | Loss: 0.00067831
Iteration 16/25 | Loss: 0.00067831
Iteration 17/25 | Loss: 0.00067831
Iteration 18/25 | Loss: 0.00067831
Iteration 19/25 | Loss: 0.00067831
Iteration 20/25 | Loss: 0.00067831
Iteration 21/25 | Loss: 0.00067830
Iteration 22/25 | Loss: 0.00067830
Iteration 23/25 | Loss: 0.00067831
Iteration 24/25 | Loss: 0.00067830
Iteration 25/25 | Loss: 0.00067831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067830
Iteration 2/1000 | Loss: 0.00001746
Iteration 3/1000 | Loss: 0.00001155
Iteration 4/1000 | Loss: 0.00001007
Iteration 5/1000 | Loss: 0.00000938
Iteration 6/1000 | Loss: 0.00000888
Iteration 7/1000 | Loss: 0.00000850
Iteration 8/1000 | Loss: 0.00000839
Iteration 9/1000 | Loss: 0.00000838
Iteration 10/1000 | Loss: 0.00000816
Iteration 11/1000 | Loss: 0.00000812
Iteration 12/1000 | Loss: 0.00000798
Iteration 13/1000 | Loss: 0.00000796
Iteration 14/1000 | Loss: 0.00000796
Iteration 15/1000 | Loss: 0.00000795
Iteration 16/1000 | Loss: 0.00000795
Iteration 17/1000 | Loss: 0.00000795
Iteration 18/1000 | Loss: 0.00000795
Iteration 19/1000 | Loss: 0.00000793
Iteration 20/1000 | Loss: 0.00000790
Iteration 21/1000 | Loss: 0.00000789
Iteration 22/1000 | Loss: 0.00000789
Iteration 23/1000 | Loss: 0.00000789
Iteration 24/1000 | Loss: 0.00000788
Iteration 25/1000 | Loss: 0.00000787
Iteration 26/1000 | Loss: 0.00000786
Iteration 27/1000 | Loss: 0.00000784
Iteration 28/1000 | Loss: 0.00000784
Iteration 29/1000 | Loss: 0.00000784
Iteration 30/1000 | Loss: 0.00000783
Iteration 31/1000 | Loss: 0.00000783
Iteration 32/1000 | Loss: 0.00000783
Iteration 33/1000 | Loss: 0.00000783
Iteration 34/1000 | Loss: 0.00000783
Iteration 35/1000 | Loss: 0.00000783
Iteration 36/1000 | Loss: 0.00000783
Iteration 37/1000 | Loss: 0.00000782
Iteration 38/1000 | Loss: 0.00000782
Iteration 39/1000 | Loss: 0.00000782
Iteration 40/1000 | Loss: 0.00000781
Iteration 41/1000 | Loss: 0.00000780
Iteration 42/1000 | Loss: 0.00000780
Iteration 43/1000 | Loss: 0.00000780
Iteration 44/1000 | Loss: 0.00000779
Iteration 45/1000 | Loss: 0.00000779
Iteration 46/1000 | Loss: 0.00000779
Iteration 47/1000 | Loss: 0.00000778
Iteration 48/1000 | Loss: 0.00000777
Iteration 49/1000 | Loss: 0.00000776
Iteration 50/1000 | Loss: 0.00000776
Iteration 51/1000 | Loss: 0.00000775
Iteration 52/1000 | Loss: 0.00000775
Iteration 53/1000 | Loss: 0.00000775
Iteration 54/1000 | Loss: 0.00000775
Iteration 55/1000 | Loss: 0.00000775
Iteration 56/1000 | Loss: 0.00000775
Iteration 57/1000 | Loss: 0.00000775
Iteration 58/1000 | Loss: 0.00000774
Iteration 59/1000 | Loss: 0.00000774
Iteration 60/1000 | Loss: 0.00000774
Iteration 61/1000 | Loss: 0.00000774
Iteration 62/1000 | Loss: 0.00000774
Iteration 63/1000 | Loss: 0.00000774
Iteration 64/1000 | Loss: 0.00000774
Iteration 65/1000 | Loss: 0.00000774
Iteration 66/1000 | Loss: 0.00000774
Iteration 67/1000 | Loss: 0.00000774
Iteration 68/1000 | Loss: 0.00000773
Iteration 69/1000 | Loss: 0.00000773
Iteration 70/1000 | Loss: 0.00000773
Iteration 71/1000 | Loss: 0.00000773
Iteration 72/1000 | Loss: 0.00000773
Iteration 73/1000 | Loss: 0.00000773
Iteration 74/1000 | Loss: 0.00000773
Iteration 75/1000 | Loss: 0.00000772
Iteration 76/1000 | Loss: 0.00000772
Iteration 77/1000 | Loss: 0.00000772
Iteration 78/1000 | Loss: 0.00000772
Iteration 79/1000 | Loss: 0.00000772
Iteration 80/1000 | Loss: 0.00000771
Iteration 81/1000 | Loss: 0.00000771
Iteration 82/1000 | Loss: 0.00000771
Iteration 83/1000 | Loss: 0.00000771
Iteration 84/1000 | Loss: 0.00000771
Iteration 85/1000 | Loss: 0.00000771
Iteration 86/1000 | Loss: 0.00000770
Iteration 87/1000 | Loss: 0.00000770
Iteration 88/1000 | Loss: 0.00000770
Iteration 89/1000 | Loss: 0.00000770
Iteration 90/1000 | Loss: 0.00000770
Iteration 91/1000 | Loss: 0.00000770
Iteration 92/1000 | Loss: 0.00000769
Iteration 93/1000 | Loss: 0.00000769
Iteration 94/1000 | Loss: 0.00000769
Iteration 95/1000 | Loss: 0.00000769
Iteration 96/1000 | Loss: 0.00000769
Iteration 97/1000 | Loss: 0.00000769
Iteration 98/1000 | Loss: 0.00000768
Iteration 99/1000 | Loss: 0.00000768
Iteration 100/1000 | Loss: 0.00000768
Iteration 101/1000 | Loss: 0.00000768
Iteration 102/1000 | Loss: 0.00000767
Iteration 103/1000 | Loss: 0.00000767
Iteration 104/1000 | Loss: 0.00000767
Iteration 105/1000 | Loss: 0.00000767
Iteration 106/1000 | Loss: 0.00000766
Iteration 107/1000 | Loss: 0.00000766
Iteration 108/1000 | Loss: 0.00000765
Iteration 109/1000 | Loss: 0.00000765
Iteration 110/1000 | Loss: 0.00000764
Iteration 111/1000 | Loss: 0.00000764
Iteration 112/1000 | Loss: 0.00000764
Iteration 113/1000 | Loss: 0.00000764
Iteration 114/1000 | Loss: 0.00000764
Iteration 115/1000 | Loss: 0.00000763
Iteration 116/1000 | Loss: 0.00000763
Iteration 117/1000 | Loss: 0.00000763
Iteration 118/1000 | Loss: 0.00000763
Iteration 119/1000 | Loss: 0.00000762
Iteration 120/1000 | Loss: 0.00000762
Iteration 121/1000 | Loss: 0.00000762
Iteration 122/1000 | Loss: 0.00000762
Iteration 123/1000 | Loss: 0.00000762
Iteration 124/1000 | Loss: 0.00000761
Iteration 125/1000 | Loss: 0.00000761
Iteration 126/1000 | Loss: 0.00000761
Iteration 127/1000 | Loss: 0.00000761
Iteration 128/1000 | Loss: 0.00000761
Iteration 129/1000 | Loss: 0.00000761
Iteration 130/1000 | Loss: 0.00000761
Iteration 131/1000 | Loss: 0.00000761
Iteration 132/1000 | Loss: 0.00000761
Iteration 133/1000 | Loss: 0.00000761
Iteration 134/1000 | Loss: 0.00000760
Iteration 135/1000 | Loss: 0.00000760
Iteration 136/1000 | Loss: 0.00000759
Iteration 137/1000 | Loss: 0.00000759
Iteration 138/1000 | Loss: 0.00000759
Iteration 139/1000 | Loss: 0.00000759
Iteration 140/1000 | Loss: 0.00000759
Iteration 141/1000 | Loss: 0.00000759
Iteration 142/1000 | Loss: 0.00000759
Iteration 143/1000 | Loss: 0.00000759
Iteration 144/1000 | Loss: 0.00000759
Iteration 145/1000 | Loss: 0.00000759
Iteration 146/1000 | Loss: 0.00000759
Iteration 147/1000 | Loss: 0.00000759
Iteration 148/1000 | Loss: 0.00000759
Iteration 149/1000 | Loss: 0.00000759
Iteration 150/1000 | Loss: 0.00000758
Iteration 151/1000 | Loss: 0.00000758
Iteration 152/1000 | Loss: 0.00000758
Iteration 153/1000 | Loss: 0.00000758
Iteration 154/1000 | Loss: 0.00000758
Iteration 155/1000 | Loss: 0.00000758
Iteration 156/1000 | Loss: 0.00000758
Iteration 157/1000 | Loss: 0.00000758
Iteration 158/1000 | Loss: 0.00000757
Iteration 159/1000 | Loss: 0.00000757
Iteration 160/1000 | Loss: 0.00000757
Iteration 161/1000 | Loss: 0.00000757
Iteration 162/1000 | Loss: 0.00000757
Iteration 163/1000 | Loss: 0.00000756
Iteration 164/1000 | Loss: 0.00000756
Iteration 165/1000 | Loss: 0.00000756
Iteration 166/1000 | Loss: 0.00000756
Iteration 167/1000 | Loss: 0.00000756
Iteration 168/1000 | Loss: 0.00000756
Iteration 169/1000 | Loss: 0.00000756
Iteration 170/1000 | Loss: 0.00000756
Iteration 171/1000 | Loss: 0.00000756
Iteration 172/1000 | Loss: 0.00000756
Iteration 173/1000 | Loss: 0.00000755
Iteration 174/1000 | Loss: 0.00000755
Iteration 175/1000 | Loss: 0.00000755
Iteration 176/1000 | Loss: 0.00000754
Iteration 177/1000 | Loss: 0.00000754
Iteration 178/1000 | Loss: 0.00000754
Iteration 179/1000 | Loss: 0.00000754
Iteration 180/1000 | Loss: 0.00000753
Iteration 181/1000 | Loss: 0.00000753
Iteration 182/1000 | Loss: 0.00000753
Iteration 183/1000 | Loss: 0.00000753
Iteration 184/1000 | Loss: 0.00000753
Iteration 185/1000 | Loss: 0.00000753
Iteration 186/1000 | Loss: 0.00000753
Iteration 187/1000 | Loss: 0.00000753
Iteration 188/1000 | Loss: 0.00000752
Iteration 189/1000 | Loss: 0.00000752
Iteration 190/1000 | Loss: 0.00000752
Iteration 191/1000 | Loss: 0.00000752
Iteration 192/1000 | Loss: 0.00000752
Iteration 193/1000 | Loss: 0.00000752
Iteration 194/1000 | Loss: 0.00000752
Iteration 195/1000 | Loss: 0.00000751
Iteration 196/1000 | Loss: 0.00000751
Iteration 197/1000 | Loss: 0.00000751
Iteration 198/1000 | Loss: 0.00000751
Iteration 199/1000 | Loss: 0.00000751
Iteration 200/1000 | Loss: 0.00000751
Iteration 201/1000 | Loss: 0.00000751
Iteration 202/1000 | Loss: 0.00000751
Iteration 203/1000 | Loss: 0.00000751
Iteration 204/1000 | Loss: 0.00000751
Iteration 205/1000 | Loss: 0.00000751
Iteration 206/1000 | Loss: 0.00000750
Iteration 207/1000 | Loss: 0.00000750
Iteration 208/1000 | Loss: 0.00000750
Iteration 209/1000 | Loss: 0.00000750
Iteration 210/1000 | Loss: 0.00000750
Iteration 211/1000 | Loss: 0.00000750
Iteration 212/1000 | Loss: 0.00000750
Iteration 213/1000 | Loss: 0.00000750
Iteration 214/1000 | Loss: 0.00000750
Iteration 215/1000 | Loss: 0.00000749
Iteration 216/1000 | Loss: 0.00000749
Iteration 217/1000 | Loss: 0.00000749
Iteration 218/1000 | Loss: 0.00000749
Iteration 219/1000 | Loss: 0.00000749
Iteration 220/1000 | Loss: 0.00000749
Iteration 221/1000 | Loss: 0.00000749
Iteration 222/1000 | Loss: 0.00000749
Iteration 223/1000 | Loss: 0.00000749
Iteration 224/1000 | Loss: 0.00000749
Iteration 225/1000 | Loss: 0.00000749
Iteration 226/1000 | Loss: 0.00000749
Iteration 227/1000 | Loss: 0.00000749
Iteration 228/1000 | Loss: 0.00000749
Iteration 229/1000 | Loss: 0.00000749
Iteration 230/1000 | Loss: 0.00000749
Iteration 231/1000 | Loss: 0.00000749
Iteration 232/1000 | Loss: 0.00000749
Iteration 233/1000 | Loss: 0.00000748
Iteration 234/1000 | Loss: 0.00000748
Iteration 235/1000 | Loss: 0.00000748
Iteration 236/1000 | Loss: 0.00000748
Iteration 237/1000 | Loss: 0.00000748
Iteration 238/1000 | Loss: 0.00000748
Iteration 239/1000 | Loss: 0.00000748
Iteration 240/1000 | Loss: 0.00000748
Iteration 241/1000 | Loss: 0.00000748
Iteration 242/1000 | Loss: 0.00000748
Iteration 243/1000 | Loss: 0.00000747
Iteration 244/1000 | Loss: 0.00000747
Iteration 245/1000 | Loss: 0.00000747
Iteration 246/1000 | Loss: 0.00000747
Iteration 247/1000 | Loss: 0.00000747
Iteration 248/1000 | Loss: 0.00000747
Iteration 249/1000 | Loss: 0.00000747
Iteration 250/1000 | Loss: 0.00000747
Iteration 251/1000 | Loss: 0.00000747
Iteration 252/1000 | Loss: 0.00000747
Iteration 253/1000 | Loss: 0.00000747
Iteration 254/1000 | Loss: 0.00000747
Iteration 255/1000 | Loss: 0.00000747
Iteration 256/1000 | Loss: 0.00000747
Iteration 257/1000 | Loss: 0.00000747
Iteration 258/1000 | Loss: 0.00000747
Iteration 259/1000 | Loss: 0.00000747
Iteration 260/1000 | Loss: 0.00000747
Iteration 261/1000 | Loss: 0.00000746
Iteration 262/1000 | Loss: 0.00000746
Iteration 263/1000 | Loss: 0.00000746
Iteration 264/1000 | Loss: 0.00000746
Iteration 265/1000 | Loss: 0.00000746
Iteration 266/1000 | Loss: 0.00000746
Iteration 267/1000 | Loss: 0.00000746
Iteration 268/1000 | Loss: 0.00000746
Iteration 269/1000 | Loss: 0.00000746
Iteration 270/1000 | Loss: 0.00000746
Iteration 271/1000 | Loss: 0.00000746
Iteration 272/1000 | Loss: 0.00000746
Iteration 273/1000 | Loss: 0.00000746
Iteration 274/1000 | Loss: 0.00000746
Iteration 275/1000 | Loss: 0.00000746
Iteration 276/1000 | Loss: 0.00000746
Iteration 277/1000 | Loss: 0.00000746
Iteration 278/1000 | Loss: 0.00000746
Iteration 279/1000 | Loss: 0.00000745
Iteration 280/1000 | Loss: 0.00000745
Iteration 281/1000 | Loss: 0.00000745
Iteration 282/1000 | Loss: 0.00000745
Iteration 283/1000 | Loss: 0.00000745
Iteration 284/1000 | Loss: 0.00000745
Iteration 285/1000 | Loss: 0.00000745
Iteration 286/1000 | Loss: 0.00000745
Iteration 287/1000 | Loss: 0.00000745
Iteration 288/1000 | Loss: 0.00000745
Iteration 289/1000 | Loss: 0.00000745
Iteration 290/1000 | Loss: 0.00000745
Iteration 291/1000 | Loss: 0.00000745
Iteration 292/1000 | Loss: 0.00000745
Iteration 293/1000 | Loss: 0.00000745
Iteration 294/1000 | Loss: 0.00000745
Iteration 295/1000 | Loss: 0.00000745
Iteration 296/1000 | Loss: 0.00000745
Iteration 297/1000 | Loss: 0.00000745
Iteration 298/1000 | Loss: 0.00000745
Iteration 299/1000 | Loss: 0.00000745
Iteration 300/1000 | Loss: 0.00000745
Iteration 301/1000 | Loss: 0.00000745
Iteration 302/1000 | Loss: 0.00000745
Iteration 303/1000 | Loss: 0.00000745
Iteration 304/1000 | Loss: 0.00000745
Iteration 305/1000 | Loss: 0.00000745
Iteration 306/1000 | Loss: 0.00000745
Iteration 307/1000 | Loss: 0.00000745
Iteration 308/1000 | Loss: 0.00000745
Iteration 309/1000 | Loss: 0.00000745
Iteration 310/1000 | Loss: 0.00000745
Iteration 311/1000 | Loss: 0.00000745
Iteration 312/1000 | Loss: 0.00000745
Iteration 313/1000 | Loss: 0.00000745
Iteration 314/1000 | Loss: 0.00000745
Iteration 315/1000 | Loss: 0.00000745
Iteration 316/1000 | Loss: 0.00000745
Iteration 317/1000 | Loss: 0.00000745
Iteration 318/1000 | Loss: 0.00000745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [7.445673418260412e-06, 7.445673418260412e-06, 7.445673418260412e-06, 7.445673418260412e-06, 7.445673418260412e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.445673418260412e-06

Optimization complete. Final v2v error: 2.3203320503234863 mm

Highest mean error: 2.620856761932373 mm for frame 55

Lowest mean error: 2.1925008296966553 mm for frame 185

Saving results

Total time: 44.28768515586853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919022
Iteration 2/25 | Loss: 0.00117301
Iteration 3/25 | Loss: 0.00105375
Iteration 4/25 | Loss: 0.00102968
Iteration 5/25 | Loss: 0.00102120
Iteration 6/25 | Loss: 0.00101880
Iteration 7/25 | Loss: 0.00101875
Iteration 8/25 | Loss: 0.00101875
Iteration 9/25 | Loss: 0.00101875
Iteration 10/25 | Loss: 0.00101875
Iteration 11/25 | Loss: 0.00101875
Iteration 12/25 | Loss: 0.00101875
Iteration 13/25 | Loss: 0.00101875
Iteration 14/25 | Loss: 0.00101875
Iteration 15/25 | Loss: 0.00101875
Iteration 16/25 | Loss: 0.00101875
Iteration 17/25 | Loss: 0.00101875
Iteration 18/25 | Loss: 0.00101875
Iteration 19/25 | Loss: 0.00101875
Iteration 20/25 | Loss: 0.00101875
Iteration 21/25 | Loss: 0.00101875
Iteration 22/25 | Loss: 0.00101875
Iteration 23/25 | Loss: 0.00101875
Iteration 24/25 | Loss: 0.00101875
Iteration 25/25 | Loss: 0.00101875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38365114
Iteration 2/25 | Loss: 0.00072041
Iteration 3/25 | Loss: 0.00072040
Iteration 4/25 | Loss: 0.00072040
Iteration 5/25 | Loss: 0.00072040
Iteration 6/25 | Loss: 0.00072040
Iteration 7/25 | Loss: 0.00072040
Iteration 8/25 | Loss: 0.00072039
Iteration 9/25 | Loss: 0.00072039
Iteration 10/25 | Loss: 0.00072039
Iteration 11/25 | Loss: 0.00072039
Iteration 12/25 | Loss: 0.00072039
Iteration 13/25 | Loss: 0.00072039
Iteration 14/25 | Loss: 0.00072039
Iteration 15/25 | Loss: 0.00072039
Iteration 16/25 | Loss: 0.00072039
Iteration 17/25 | Loss: 0.00072039
Iteration 18/25 | Loss: 0.00072039
Iteration 19/25 | Loss: 0.00072039
Iteration 20/25 | Loss: 0.00072039
Iteration 21/25 | Loss: 0.00072039
Iteration 22/25 | Loss: 0.00072039
Iteration 23/25 | Loss: 0.00072039
Iteration 24/25 | Loss: 0.00072039
Iteration 25/25 | Loss: 0.00072039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072039
Iteration 2/1000 | Loss: 0.00004774
Iteration 3/1000 | Loss: 0.00003081
Iteration 4/1000 | Loss: 0.00002346
Iteration 5/1000 | Loss: 0.00002178
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00001996
Iteration 8/1000 | Loss: 0.00001940
Iteration 9/1000 | Loss: 0.00001892
Iteration 10/1000 | Loss: 0.00001868
Iteration 11/1000 | Loss: 0.00001854
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001835
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001834
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001832
Iteration 18/1000 | Loss: 0.00001831
Iteration 19/1000 | Loss: 0.00001830
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001828
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001822
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001816
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001804
Iteration 42/1000 | Loss: 0.00001804
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00001802
Iteration 46/1000 | Loss: 0.00001802
Iteration 47/1000 | Loss: 0.00001802
Iteration 48/1000 | Loss: 0.00001801
Iteration 49/1000 | Loss: 0.00001801
Iteration 50/1000 | Loss: 0.00001801
Iteration 51/1000 | Loss: 0.00001801
Iteration 52/1000 | Loss: 0.00001801
Iteration 53/1000 | Loss: 0.00001801
Iteration 54/1000 | Loss: 0.00001801
Iteration 55/1000 | Loss: 0.00001801
Iteration 56/1000 | Loss: 0.00001801
Iteration 57/1000 | Loss: 0.00001801
Iteration 58/1000 | Loss: 0.00001801
Iteration 59/1000 | Loss: 0.00001800
Iteration 60/1000 | Loss: 0.00001800
Iteration 61/1000 | Loss: 0.00001800
Iteration 62/1000 | Loss: 0.00001799
Iteration 63/1000 | Loss: 0.00001799
Iteration 64/1000 | Loss: 0.00001799
Iteration 65/1000 | Loss: 0.00001798
Iteration 66/1000 | Loss: 0.00001798
Iteration 67/1000 | Loss: 0.00001798
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001797
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001795
Iteration 82/1000 | Loss: 0.00001795
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001794
Iteration 85/1000 | Loss: 0.00001794
Iteration 86/1000 | Loss: 0.00001794
Iteration 87/1000 | Loss: 0.00001794
Iteration 88/1000 | Loss: 0.00001794
Iteration 89/1000 | Loss: 0.00001793
Iteration 90/1000 | Loss: 0.00001793
Iteration 91/1000 | Loss: 0.00001793
Iteration 92/1000 | Loss: 0.00001792
Iteration 93/1000 | Loss: 0.00001792
Iteration 94/1000 | Loss: 0.00001792
Iteration 95/1000 | Loss: 0.00001792
Iteration 96/1000 | Loss: 0.00001791
Iteration 97/1000 | Loss: 0.00001791
Iteration 98/1000 | Loss: 0.00001791
Iteration 99/1000 | Loss: 0.00001791
Iteration 100/1000 | Loss: 0.00001791
Iteration 101/1000 | Loss: 0.00001791
Iteration 102/1000 | Loss: 0.00001790
Iteration 103/1000 | Loss: 0.00001790
Iteration 104/1000 | Loss: 0.00001790
Iteration 105/1000 | Loss: 0.00001789
Iteration 106/1000 | Loss: 0.00001789
Iteration 107/1000 | Loss: 0.00001789
Iteration 108/1000 | Loss: 0.00001789
Iteration 109/1000 | Loss: 0.00001788
Iteration 110/1000 | Loss: 0.00001788
Iteration 111/1000 | Loss: 0.00001788
Iteration 112/1000 | Loss: 0.00001787
Iteration 113/1000 | Loss: 0.00001787
Iteration 114/1000 | Loss: 0.00001787
Iteration 115/1000 | Loss: 0.00001787
Iteration 116/1000 | Loss: 0.00001787
Iteration 117/1000 | Loss: 0.00001786
Iteration 118/1000 | Loss: 0.00001786
Iteration 119/1000 | Loss: 0.00001786
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001786
Iteration 123/1000 | Loss: 0.00001786
Iteration 124/1000 | Loss: 0.00001786
Iteration 125/1000 | Loss: 0.00001786
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001785
Iteration 128/1000 | Loss: 0.00001785
Iteration 129/1000 | Loss: 0.00001785
Iteration 130/1000 | Loss: 0.00001785
Iteration 131/1000 | Loss: 0.00001785
Iteration 132/1000 | Loss: 0.00001785
Iteration 133/1000 | Loss: 0.00001785
Iteration 134/1000 | Loss: 0.00001785
Iteration 135/1000 | Loss: 0.00001785
Iteration 136/1000 | Loss: 0.00001785
Iteration 137/1000 | Loss: 0.00001785
Iteration 138/1000 | Loss: 0.00001785
Iteration 139/1000 | Loss: 0.00001785
Iteration 140/1000 | Loss: 0.00001785
Iteration 141/1000 | Loss: 0.00001785
Iteration 142/1000 | Loss: 0.00001785
Iteration 143/1000 | Loss: 0.00001785
Iteration 144/1000 | Loss: 0.00001785
Iteration 145/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.7852664313977584e-05, 1.7852664313977584e-05, 1.7852664313977584e-05, 1.7852664313977584e-05, 1.7852664313977584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7852664313977584e-05

Optimization complete. Final v2v error: 3.49000883102417 mm

Highest mean error: 5.533634185791016 mm for frame 70

Lowest mean error: 2.9849579334259033 mm for frame 17

Saving results

Total time: 37.95381236076355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393856
Iteration 2/25 | Loss: 0.00115577
Iteration 3/25 | Loss: 0.00100586
Iteration 4/25 | Loss: 0.00098600
Iteration 5/25 | Loss: 0.00098018
Iteration 6/25 | Loss: 0.00097814
Iteration 7/25 | Loss: 0.00097782
Iteration 8/25 | Loss: 0.00097782
Iteration 9/25 | Loss: 0.00097782
Iteration 10/25 | Loss: 0.00097782
Iteration 11/25 | Loss: 0.00097782
Iteration 12/25 | Loss: 0.00097782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009778192033991218, 0.0009778192033991218, 0.0009778192033991218, 0.0009778192033991218, 0.0009778192033991218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009778192033991218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35533547
Iteration 2/25 | Loss: 0.00082681
Iteration 3/25 | Loss: 0.00082679
Iteration 4/25 | Loss: 0.00082679
Iteration 5/25 | Loss: 0.00082679
Iteration 6/25 | Loss: 0.00082679
Iteration 7/25 | Loss: 0.00082679
Iteration 8/25 | Loss: 0.00082679
Iteration 9/25 | Loss: 0.00082678
Iteration 10/25 | Loss: 0.00082678
Iteration 11/25 | Loss: 0.00082678
Iteration 12/25 | Loss: 0.00082678
Iteration 13/25 | Loss: 0.00082678
Iteration 14/25 | Loss: 0.00082678
Iteration 15/25 | Loss: 0.00082678
Iteration 16/25 | Loss: 0.00082678
Iteration 17/25 | Loss: 0.00082678
Iteration 18/25 | Loss: 0.00082678
Iteration 19/25 | Loss: 0.00082678
Iteration 20/25 | Loss: 0.00082678
Iteration 21/25 | Loss: 0.00082678
Iteration 22/25 | Loss: 0.00082678
Iteration 23/25 | Loss: 0.00082678
Iteration 24/25 | Loss: 0.00082678
Iteration 25/25 | Loss: 0.00082678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082678
Iteration 2/1000 | Loss: 0.00004689
Iteration 3/1000 | Loss: 0.00002760
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001433
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001327
Iteration 14/1000 | Loss: 0.00001325
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001298
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001297
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001296
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001295
Iteration 31/1000 | Loss: 0.00001294
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001292
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001290
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001280
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001277
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001276
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001275
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001274
Iteration 82/1000 | Loss: 0.00001274
Iteration 83/1000 | Loss: 0.00001274
Iteration 84/1000 | Loss: 0.00001273
Iteration 85/1000 | Loss: 0.00001273
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001272
Iteration 89/1000 | Loss: 0.00001272
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001270
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001269
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001267
Iteration 111/1000 | Loss: 0.00001267
Iteration 112/1000 | Loss: 0.00001267
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001265
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001265
Iteration 141/1000 | Loss: 0.00001265
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001265
Iteration 150/1000 | Loss: 0.00001265
Iteration 151/1000 | Loss: 0.00001265
Iteration 152/1000 | Loss: 0.00001265
Iteration 153/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.26509521578555e-05, 1.26509521578555e-05, 1.26509521578555e-05, 1.26509521578555e-05, 1.26509521578555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.26509521578555e-05

Optimization complete. Final v2v error: 2.825122594833374 mm

Highest mean error: 4.97964334487915 mm for frame 87

Lowest mean error: 2.2198214530944824 mm for frame 128

Saving results

Total time: 40.58877468109131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007401
Iteration 2/25 | Loss: 0.00268798
Iteration 3/25 | Loss: 0.00187314
Iteration 4/25 | Loss: 0.00180785
Iteration 5/25 | Loss: 0.00178418
Iteration 6/25 | Loss: 0.00127101
Iteration 7/25 | Loss: 0.00117089
Iteration 8/25 | Loss: 0.00119375
Iteration 9/25 | Loss: 0.00114991
Iteration 10/25 | Loss: 0.00115196
Iteration 11/25 | Loss: 0.00113805
Iteration 12/25 | Loss: 0.00113585
Iteration 13/25 | Loss: 0.00113250
Iteration 14/25 | Loss: 0.00113468
Iteration 15/25 | Loss: 0.00113213
Iteration 16/25 | Loss: 0.00113121
Iteration 17/25 | Loss: 0.00113505
Iteration 18/25 | Loss: 0.00113389
Iteration 19/25 | Loss: 0.00113231
Iteration 20/25 | Loss: 0.00112328
Iteration 21/25 | Loss: 0.00112259
Iteration 22/25 | Loss: 0.00112253
Iteration 23/25 | Loss: 0.00112253
Iteration 24/25 | Loss: 0.00112252
Iteration 25/25 | Loss: 0.00112252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38041735
Iteration 2/25 | Loss: 0.00070622
Iteration 3/25 | Loss: 0.00070621
Iteration 4/25 | Loss: 0.00070621
Iteration 5/25 | Loss: 0.00070621
Iteration 6/25 | Loss: 0.00070621
Iteration 7/25 | Loss: 0.00070621
Iteration 8/25 | Loss: 0.00070621
Iteration 9/25 | Loss: 0.00070621
Iteration 10/25 | Loss: 0.00070621
Iteration 11/25 | Loss: 0.00070621
Iteration 12/25 | Loss: 0.00070621
Iteration 13/25 | Loss: 0.00070621
Iteration 14/25 | Loss: 0.00070621
Iteration 15/25 | Loss: 0.00070621
Iteration 16/25 | Loss: 0.00070621
Iteration 17/25 | Loss: 0.00070621
Iteration 18/25 | Loss: 0.00070621
Iteration 19/25 | Loss: 0.00070621
Iteration 20/25 | Loss: 0.00070621
Iteration 21/25 | Loss: 0.00070621
Iteration 22/25 | Loss: 0.00070621
Iteration 23/25 | Loss: 0.00070621
Iteration 24/25 | Loss: 0.00070621
Iteration 25/25 | Loss: 0.00070621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070621
Iteration 2/1000 | Loss: 0.00108591
Iteration 3/1000 | Loss: 0.00004266
Iteration 4/1000 | Loss: 0.00003529
Iteration 5/1000 | Loss: 0.00003236
Iteration 6/1000 | Loss: 0.00003057
Iteration 7/1000 | Loss: 0.00002951
Iteration 8/1000 | Loss: 0.00002849
Iteration 9/1000 | Loss: 0.00002804
Iteration 10/1000 | Loss: 0.00002759
Iteration 11/1000 | Loss: 0.00002736
Iteration 12/1000 | Loss: 0.00002706
Iteration 13/1000 | Loss: 0.00093571
Iteration 14/1000 | Loss: 0.00005122
Iteration 15/1000 | Loss: 0.00003157
Iteration 16/1000 | Loss: 0.00002711
Iteration 17/1000 | Loss: 0.00002415
Iteration 18/1000 | Loss: 0.00002198
Iteration 19/1000 | Loss: 0.00002067
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001941
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001804
Iteration 33/1000 | Loss: 0.00001804
Iteration 34/1000 | Loss: 0.00001804
Iteration 35/1000 | Loss: 0.00001804
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00001804
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001801
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001795
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001795
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001794
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001793
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001793
Iteration 67/1000 | Loss: 0.00001793
Iteration 68/1000 | Loss: 0.00001793
Iteration 69/1000 | Loss: 0.00001793
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001792
Iteration 72/1000 | Loss: 0.00001792
Iteration 73/1000 | Loss: 0.00001792
Iteration 74/1000 | Loss: 0.00001791
Iteration 75/1000 | Loss: 0.00001791
Iteration 76/1000 | Loss: 0.00001791
Iteration 77/1000 | Loss: 0.00001791
Iteration 78/1000 | Loss: 0.00001791
Iteration 79/1000 | Loss: 0.00001791
Iteration 80/1000 | Loss: 0.00001791
Iteration 81/1000 | Loss: 0.00001791
Iteration 82/1000 | Loss: 0.00001791
Iteration 83/1000 | Loss: 0.00001791
Iteration 84/1000 | Loss: 0.00001791
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001791
Iteration 89/1000 | Loss: 0.00001791
Iteration 90/1000 | Loss: 0.00001791
Iteration 91/1000 | Loss: 0.00001790
Iteration 92/1000 | Loss: 0.00001790
Iteration 93/1000 | Loss: 0.00001790
Iteration 94/1000 | Loss: 0.00001790
Iteration 95/1000 | Loss: 0.00001790
Iteration 96/1000 | Loss: 0.00001790
Iteration 97/1000 | Loss: 0.00001790
Iteration 98/1000 | Loss: 0.00001790
Iteration 99/1000 | Loss: 0.00001790
Iteration 100/1000 | Loss: 0.00001790
Iteration 101/1000 | Loss: 0.00001790
Iteration 102/1000 | Loss: 0.00001790
Iteration 103/1000 | Loss: 0.00001790
Iteration 104/1000 | Loss: 0.00001790
Iteration 105/1000 | Loss: 0.00001790
Iteration 106/1000 | Loss: 0.00001790
Iteration 107/1000 | Loss: 0.00001790
Iteration 108/1000 | Loss: 0.00001789
Iteration 109/1000 | Loss: 0.00001789
Iteration 110/1000 | Loss: 0.00001789
Iteration 111/1000 | Loss: 0.00001789
Iteration 112/1000 | Loss: 0.00001789
Iteration 113/1000 | Loss: 0.00001789
Iteration 114/1000 | Loss: 0.00001789
Iteration 115/1000 | Loss: 0.00001789
Iteration 116/1000 | Loss: 0.00001789
Iteration 117/1000 | Loss: 0.00001789
Iteration 118/1000 | Loss: 0.00001789
Iteration 119/1000 | Loss: 0.00001789
Iteration 120/1000 | Loss: 0.00001789
Iteration 121/1000 | Loss: 0.00001789
Iteration 122/1000 | Loss: 0.00001789
Iteration 123/1000 | Loss: 0.00001789
Iteration 124/1000 | Loss: 0.00001789
Iteration 125/1000 | Loss: 0.00001789
Iteration 126/1000 | Loss: 0.00001789
Iteration 127/1000 | Loss: 0.00001789
Iteration 128/1000 | Loss: 0.00001788
Iteration 129/1000 | Loss: 0.00001788
Iteration 130/1000 | Loss: 0.00001788
Iteration 131/1000 | Loss: 0.00001788
Iteration 132/1000 | Loss: 0.00001788
Iteration 133/1000 | Loss: 0.00001788
Iteration 134/1000 | Loss: 0.00001788
Iteration 135/1000 | Loss: 0.00001788
Iteration 136/1000 | Loss: 0.00001788
Iteration 137/1000 | Loss: 0.00001788
Iteration 138/1000 | Loss: 0.00001788
Iteration 139/1000 | Loss: 0.00001788
Iteration 140/1000 | Loss: 0.00001788
Iteration 141/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.7884296539705247e-05, 1.7884296539705247e-05, 1.7884296539705247e-05, 1.7884296539705247e-05, 1.7884296539705247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7884296539705247e-05

Optimization complete. Final v2v error: 3.554637908935547 mm

Highest mean error: 3.837042808532715 mm for frame 140

Lowest mean error: 3.2147903442382812 mm for frame 10

Saving results

Total time: 88.28029274940491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380870
Iteration 2/25 | Loss: 0.00109291
Iteration 3/25 | Loss: 0.00099285
Iteration 4/25 | Loss: 0.00097299
Iteration 5/25 | Loss: 0.00096662
Iteration 6/25 | Loss: 0.00096600
Iteration 7/25 | Loss: 0.00096600
Iteration 8/25 | Loss: 0.00096600
Iteration 9/25 | Loss: 0.00096600
Iteration 10/25 | Loss: 0.00096600
Iteration 11/25 | Loss: 0.00096600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009660047944635153, 0.0009660047944635153, 0.0009660047944635153, 0.0009660047944635153, 0.0009660047944635153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009660047944635153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36366582
Iteration 2/25 | Loss: 0.00086610
Iteration 3/25 | Loss: 0.00086610
Iteration 4/25 | Loss: 0.00086610
Iteration 5/25 | Loss: 0.00086610
Iteration 6/25 | Loss: 0.00086610
Iteration 7/25 | Loss: 0.00086610
Iteration 8/25 | Loss: 0.00086610
Iteration 9/25 | Loss: 0.00086610
Iteration 10/25 | Loss: 0.00086610
Iteration 11/25 | Loss: 0.00086610
Iteration 12/25 | Loss: 0.00086610
Iteration 13/25 | Loss: 0.00086610
Iteration 14/25 | Loss: 0.00086610
Iteration 15/25 | Loss: 0.00086610
Iteration 16/25 | Loss: 0.00086610
Iteration 17/25 | Loss: 0.00086610
Iteration 18/25 | Loss: 0.00086610
Iteration 19/25 | Loss: 0.00086610
Iteration 20/25 | Loss: 0.00086610
Iteration 21/25 | Loss: 0.00086610
Iteration 22/25 | Loss: 0.00086610
Iteration 23/25 | Loss: 0.00086610
Iteration 24/25 | Loss: 0.00086610
Iteration 25/25 | Loss: 0.00086610

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086610
Iteration 2/1000 | Loss: 0.00003965
Iteration 3/1000 | Loss: 0.00002813
Iteration 4/1000 | Loss: 0.00002304
Iteration 5/1000 | Loss: 0.00002111
Iteration 6/1000 | Loss: 0.00002012
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001812
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001731
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001715
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001714
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001701
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001689
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001689
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001685
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Iteration 169/1000 | Loss: 0.00001683
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001682
Iteration 174/1000 | Loss: 0.00001682
Iteration 175/1000 | Loss: 0.00001682
Iteration 176/1000 | Loss: 0.00001682
Iteration 177/1000 | Loss: 0.00001682
Iteration 178/1000 | Loss: 0.00001682
Iteration 179/1000 | Loss: 0.00001682
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Iteration 183/1000 | Loss: 0.00001682
Iteration 184/1000 | Loss: 0.00001681
Iteration 185/1000 | Loss: 0.00001681
Iteration 186/1000 | Loss: 0.00001681
Iteration 187/1000 | Loss: 0.00001681
Iteration 188/1000 | Loss: 0.00001681
Iteration 189/1000 | Loss: 0.00001681
Iteration 190/1000 | Loss: 0.00001681
Iteration 191/1000 | Loss: 0.00001681
Iteration 192/1000 | Loss: 0.00001681
Iteration 193/1000 | Loss: 0.00001681
Iteration 194/1000 | Loss: 0.00001681
Iteration 195/1000 | Loss: 0.00001681
Iteration 196/1000 | Loss: 0.00001681
Iteration 197/1000 | Loss: 0.00001681
Iteration 198/1000 | Loss: 0.00001681
Iteration 199/1000 | Loss: 0.00001681
Iteration 200/1000 | Loss: 0.00001681
Iteration 201/1000 | Loss: 0.00001681
Iteration 202/1000 | Loss: 0.00001681
Iteration 203/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.681327194091864e-05, 1.681327194091864e-05, 1.681327194091864e-05, 1.681327194091864e-05, 1.681327194091864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.681327194091864e-05

Optimization complete. Final v2v error: 3.2780566215515137 mm

Highest mean error: 4.265017509460449 mm for frame 103

Lowest mean error: 2.4324569702148438 mm for frame 0

Saving results

Total time: 45.44199085235596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832753
Iteration 2/25 | Loss: 0.00141914
Iteration 3/25 | Loss: 0.00109055
Iteration 4/25 | Loss: 0.00105523
Iteration 5/25 | Loss: 0.00104861
Iteration 6/25 | Loss: 0.00104646
Iteration 7/25 | Loss: 0.00104939
Iteration 8/25 | Loss: 0.00104808
Iteration 9/25 | Loss: 0.00104482
Iteration 10/25 | Loss: 0.00104248
Iteration 11/25 | Loss: 0.00104467
Iteration 12/25 | Loss: 0.00104367
Iteration 13/25 | Loss: 0.00104147
Iteration 14/25 | Loss: 0.00104383
Iteration 15/25 | Loss: 0.00104227
Iteration 16/25 | Loss: 0.00103931
Iteration 17/25 | Loss: 0.00104145
Iteration 18/25 | Loss: 0.00103941
Iteration 19/25 | Loss: 0.00103741
Iteration 20/25 | Loss: 0.00103701
Iteration 21/25 | Loss: 0.00103684
Iteration 22/25 | Loss: 0.00103672
Iteration 23/25 | Loss: 0.00103660
Iteration 24/25 | Loss: 0.00104005
Iteration 25/25 | Loss: 0.00104096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31431055
Iteration 2/25 | Loss: 0.00067145
Iteration 3/25 | Loss: 0.00067144
Iteration 4/25 | Loss: 0.00067144
Iteration 5/25 | Loss: 0.00067144
Iteration 6/25 | Loss: 0.00067144
Iteration 7/25 | Loss: 0.00067144
Iteration 8/25 | Loss: 0.00067144
Iteration 9/25 | Loss: 0.00067144
Iteration 10/25 | Loss: 0.00067144
Iteration 11/25 | Loss: 0.00067144
Iteration 12/25 | Loss: 0.00067144
Iteration 13/25 | Loss: 0.00067144
Iteration 14/25 | Loss: 0.00067144
Iteration 15/25 | Loss: 0.00067144
Iteration 16/25 | Loss: 0.00067144
Iteration 17/25 | Loss: 0.00067144
Iteration 18/25 | Loss: 0.00067144
Iteration 19/25 | Loss: 0.00067144
Iteration 20/25 | Loss: 0.00067144
Iteration 21/25 | Loss: 0.00067144
Iteration 22/25 | Loss: 0.00067144
Iteration 23/25 | Loss: 0.00067144
Iteration 24/25 | Loss: 0.00067144
Iteration 25/25 | Loss: 0.00067144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067144
Iteration 2/1000 | Loss: 0.00006319
Iteration 3/1000 | Loss: 0.00044824
Iteration 4/1000 | Loss: 0.00005143
Iteration 5/1000 | Loss: 0.00003290
Iteration 6/1000 | Loss: 0.00002581
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00001795
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001345
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001311
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001285
Iteration 21/1000 | Loss: 0.00001285
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001283
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001273
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001270
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001269
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001262
Iteration 55/1000 | Loss: 0.00001262
Iteration 56/1000 | Loss: 0.00001262
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001260
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001254
Iteration 117/1000 | Loss: 0.00001254
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2536962458398193e-05, 1.2536962458398193e-05, 1.2536962458398193e-05, 1.2536962458398193e-05, 1.2536962458398193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2536962458398193e-05

Optimization complete. Final v2v error: 3.0095558166503906 mm

Highest mean error: 3.763779640197754 mm for frame 153

Lowest mean error: 2.6030497550964355 mm for frame 117

Saving results

Total time: 87.88230109214783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031998
Iteration 2/25 | Loss: 0.00244992
Iteration 3/25 | Loss: 0.00202231
Iteration 4/25 | Loss: 0.00190560
Iteration 5/25 | Loss: 0.00172314
Iteration 6/25 | Loss: 0.00135379
Iteration 7/25 | Loss: 0.00120452
Iteration 8/25 | Loss: 0.00115418
Iteration 9/25 | Loss: 0.00114666
Iteration 10/25 | Loss: 0.00116570
Iteration 11/25 | Loss: 0.00113056
Iteration 12/25 | Loss: 0.00112234
Iteration 13/25 | Loss: 0.00112071
Iteration 14/25 | Loss: 0.00112037
Iteration 15/25 | Loss: 0.00112032
Iteration 16/25 | Loss: 0.00112031
Iteration 17/25 | Loss: 0.00112031
Iteration 18/25 | Loss: 0.00112031
Iteration 19/25 | Loss: 0.00112031
Iteration 20/25 | Loss: 0.00112031
Iteration 21/25 | Loss: 0.00112031
Iteration 22/25 | Loss: 0.00112031
Iteration 23/25 | Loss: 0.00112031
Iteration 24/25 | Loss: 0.00112031
Iteration 25/25 | Loss: 0.00112030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42292583
Iteration 2/25 | Loss: 0.00052349
Iteration 3/25 | Loss: 0.00052349
Iteration 4/25 | Loss: 0.00052349
Iteration 5/25 | Loss: 0.00052349
Iteration 6/25 | Loss: 0.00052349
Iteration 7/25 | Loss: 0.00052349
Iteration 8/25 | Loss: 0.00052349
Iteration 9/25 | Loss: 0.00052349
Iteration 10/25 | Loss: 0.00052349
Iteration 11/25 | Loss: 0.00052349
Iteration 12/25 | Loss: 0.00052349
Iteration 13/25 | Loss: 0.00052349
Iteration 14/25 | Loss: 0.00052349
Iteration 15/25 | Loss: 0.00052349
Iteration 16/25 | Loss: 0.00052349
Iteration 17/25 | Loss: 0.00052349
Iteration 18/25 | Loss: 0.00052349
Iteration 19/25 | Loss: 0.00052349
Iteration 20/25 | Loss: 0.00052349
Iteration 21/25 | Loss: 0.00052349
Iteration 22/25 | Loss: 0.00052349
Iteration 23/25 | Loss: 0.00052349
Iteration 24/25 | Loss: 0.00052349
Iteration 25/25 | Loss: 0.00052349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052349
Iteration 2/1000 | Loss: 0.00003896
Iteration 3/1000 | Loss: 0.00002709
Iteration 4/1000 | Loss: 0.00002481
Iteration 5/1000 | Loss: 0.00002373
Iteration 6/1000 | Loss: 0.00002316
Iteration 7/1000 | Loss: 0.00002273
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00002217
Iteration 11/1000 | Loss: 0.00002215
Iteration 12/1000 | Loss: 0.00002213
Iteration 13/1000 | Loss: 0.00002208
Iteration 14/1000 | Loss: 0.00002206
Iteration 15/1000 | Loss: 0.00002205
Iteration 16/1000 | Loss: 0.00002204
Iteration 17/1000 | Loss: 0.00002202
Iteration 18/1000 | Loss: 0.00002202
Iteration 19/1000 | Loss: 0.00002200
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002189
Iteration 22/1000 | Loss: 0.00002189
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002186
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002185
Iteration 28/1000 | Loss: 0.00002184
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002182
Iteration 31/1000 | Loss: 0.00002182
Iteration 32/1000 | Loss: 0.00002182
Iteration 33/1000 | Loss: 0.00002181
Iteration 34/1000 | Loss: 0.00002181
Iteration 35/1000 | Loss: 0.00002181
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002178
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002178
Iteration 49/1000 | Loss: 0.00002177
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002177
Iteration 53/1000 | Loss: 0.00002177
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002176
Iteration 56/1000 | Loss: 0.00002175
Iteration 57/1000 | Loss: 0.00002175
Iteration 58/1000 | Loss: 0.00002175
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002174
Iteration 62/1000 | Loss: 0.00002174
Iteration 63/1000 | Loss: 0.00002174
Iteration 64/1000 | Loss: 0.00002174
Iteration 65/1000 | Loss: 0.00002173
Iteration 66/1000 | Loss: 0.00002173
Iteration 67/1000 | Loss: 0.00002173
Iteration 68/1000 | Loss: 0.00002172
Iteration 69/1000 | Loss: 0.00002172
Iteration 70/1000 | Loss: 0.00002172
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002171
Iteration 73/1000 | Loss: 0.00002171
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002171
Iteration 78/1000 | Loss: 0.00002170
Iteration 79/1000 | Loss: 0.00002170
Iteration 80/1000 | Loss: 0.00002170
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002170
Iteration 83/1000 | Loss: 0.00002169
Iteration 84/1000 | Loss: 0.00002169
Iteration 85/1000 | Loss: 0.00002169
Iteration 86/1000 | Loss: 0.00002169
Iteration 87/1000 | Loss: 0.00002169
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002168
Iteration 90/1000 | Loss: 0.00002168
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002168
Iteration 93/1000 | Loss: 0.00002168
Iteration 94/1000 | Loss: 0.00002167
Iteration 95/1000 | Loss: 0.00002167
Iteration 96/1000 | Loss: 0.00002167
Iteration 97/1000 | Loss: 0.00002167
Iteration 98/1000 | Loss: 0.00002167
Iteration 99/1000 | Loss: 0.00002167
Iteration 100/1000 | Loss: 0.00002167
Iteration 101/1000 | Loss: 0.00002167
Iteration 102/1000 | Loss: 0.00002167
Iteration 103/1000 | Loss: 0.00002167
Iteration 104/1000 | Loss: 0.00002167
Iteration 105/1000 | Loss: 0.00002167
Iteration 106/1000 | Loss: 0.00002167
Iteration 107/1000 | Loss: 0.00002167
Iteration 108/1000 | Loss: 0.00002167
Iteration 109/1000 | Loss: 0.00002167
Iteration 110/1000 | Loss: 0.00002167
Iteration 111/1000 | Loss: 0.00002167
Iteration 112/1000 | Loss: 0.00002167
Iteration 113/1000 | Loss: 0.00002167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.1669957277481444e-05, 2.1669957277481444e-05, 2.1669957277481444e-05, 2.1669957277481444e-05, 2.1669957277481444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1669957277481444e-05

Optimization complete. Final v2v error: 3.9486846923828125 mm

Highest mean error: 6.130992412567139 mm for frame 2

Lowest mean error: 3.463223457336426 mm for frame 7

Saving results

Total time: 50.836180686950684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433508
Iteration 2/25 | Loss: 0.00108149
Iteration 3/25 | Loss: 0.00100776
Iteration 4/25 | Loss: 0.00099607
Iteration 5/25 | Loss: 0.00099217
Iteration 6/25 | Loss: 0.00099158
Iteration 7/25 | Loss: 0.00099158
Iteration 8/25 | Loss: 0.00099158
Iteration 9/25 | Loss: 0.00099158
Iteration 10/25 | Loss: 0.00099158
Iteration 11/25 | Loss: 0.00099158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000991581822745502, 0.000991581822745502, 0.000991581822745502, 0.000991581822745502, 0.000991581822745502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991581822745502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37189305
Iteration 2/25 | Loss: 0.00066471
Iteration 3/25 | Loss: 0.00066470
Iteration 4/25 | Loss: 0.00066470
Iteration 5/25 | Loss: 0.00066470
Iteration 6/25 | Loss: 0.00066470
Iteration 7/25 | Loss: 0.00066470
Iteration 8/25 | Loss: 0.00066470
Iteration 9/25 | Loss: 0.00066470
Iteration 10/25 | Loss: 0.00066470
Iteration 11/25 | Loss: 0.00066470
Iteration 12/25 | Loss: 0.00066470
Iteration 13/25 | Loss: 0.00066470
Iteration 14/25 | Loss: 0.00066470
Iteration 15/25 | Loss: 0.00066470
Iteration 16/25 | Loss: 0.00066470
Iteration 17/25 | Loss: 0.00066470
Iteration 18/25 | Loss: 0.00066470
Iteration 19/25 | Loss: 0.00066470
Iteration 20/25 | Loss: 0.00066470
Iteration 21/25 | Loss: 0.00066470
Iteration 22/25 | Loss: 0.00066470
Iteration 23/25 | Loss: 0.00066470
Iteration 24/25 | Loss: 0.00066470
Iteration 25/25 | Loss: 0.00066470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066470
Iteration 2/1000 | Loss: 0.00002881
Iteration 3/1000 | Loss: 0.00002114
Iteration 4/1000 | Loss: 0.00001827
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001584
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001554
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001545
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001531
Iteration 15/1000 | Loss: 0.00001528
Iteration 16/1000 | Loss: 0.00001513
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001491
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001486
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001481
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001480
Iteration 43/1000 | Loss: 0.00001480
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001477
Iteration 51/1000 | Loss: 0.00001476
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001473
Iteration 57/1000 | Loss: 0.00001473
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001455
Iteration 101/1000 | Loss: 0.00001455
Iteration 102/1000 | Loss: 0.00001455
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001452
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001452
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00001451
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001450
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001450
Iteration 137/1000 | Loss: 0.00001450
Iteration 138/1000 | Loss: 0.00001449
Iteration 139/1000 | Loss: 0.00001449
Iteration 140/1000 | Loss: 0.00001449
Iteration 141/1000 | Loss: 0.00001449
Iteration 142/1000 | Loss: 0.00001449
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001449
Iteration 147/1000 | Loss: 0.00001449
Iteration 148/1000 | Loss: 0.00001449
Iteration 149/1000 | Loss: 0.00001449
Iteration 150/1000 | Loss: 0.00001449
Iteration 151/1000 | Loss: 0.00001449
Iteration 152/1000 | Loss: 0.00001449
Iteration 153/1000 | Loss: 0.00001449
Iteration 154/1000 | Loss: 0.00001449
Iteration 155/1000 | Loss: 0.00001449
Iteration 156/1000 | Loss: 0.00001449
Iteration 157/1000 | Loss: 0.00001449
Iteration 158/1000 | Loss: 0.00001449
Iteration 159/1000 | Loss: 0.00001449
Iteration 160/1000 | Loss: 0.00001449
Iteration 161/1000 | Loss: 0.00001449
Iteration 162/1000 | Loss: 0.00001449
Iteration 163/1000 | Loss: 0.00001449
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.4488441593130119e-05, 1.4488441593130119e-05, 1.4488441593130119e-05, 1.4488441593130119e-05, 1.4488441593130119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4488441593130119e-05

Optimization complete. Final v2v error: 3.2233052253723145 mm

Highest mean error: 3.4117019176483154 mm for frame 112

Lowest mean error: 3.0129027366638184 mm for frame 40

Saving results

Total time: 38.2302885055542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713611
Iteration 2/25 | Loss: 0.00127845
Iteration 3/25 | Loss: 0.00112687
Iteration 4/25 | Loss: 0.00111011
Iteration 5/25 | Loss: 0.00110702
Iteration 6/25 | Loss: 0.00110702
Iteration 7/25 | Loss: 0.00110702
Iteration 8/25 | Loss: 0.00110702
Iteration 9/25 | Loss: 0.00110702
Iteration 10/25 | Loss: 0.00110702
Iteration 11/25 | Loss: 0.00110702
Iteration 12/25 | Loss: 0.00110702
Iteration 13/25 | Loss: 0.00110702
Iteration 14/25 | Loss: 0.00110702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011070223990827799, 0.0011070223990827799, 0.0011070223990827799, 0.0011070223990827799, 0.0011070223990827799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011070223990827799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.02279520
Iteration 2/25 | Loss: 0.00086229
Iteration 3/25 | Loss: 0.00086226
Iteration 4/25 | Loss: 0.00086226
Iteration 5/25 | Loss: 0.00086226
Iteration 6/25 | Loss: 0.00086226
Iteration 7/25 | Loss: 0.00086226
Iteration 8/25 | Loss: 0.00086226
Iteration 9/25 | Loss: 0.00086226
Iteration 10/25 | Loss: 0.00086226
Iteration 11/25 | Loss: 0.00086226
Iteration 12/25 | Loss: 0.00086226
Iteration 13/25 | Loss: 0.00086226
Iteration 14/25 | Loss: 0.00086226
Iteration 15/25 | Loss: 0.00086226
Iteration 16/25 | Loss: 0.00086226
Iteration 17/25 | Loss: 0.00086226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008622570312581956, 0.0008622570312581956, 0.0008622570312581956, 0.0008622570312581956, 0.0008622570312581956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008622570312581956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086226
Iteration 2/1000 | Loss: 0.00003802
Iteration 3/1000 | Loss: 0.00002578
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002176
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00002018
Iteration 8/1000 | Loss: 0.00001990
Iteration 9/1000 | Loss: 0.00001966
Iteration 10/1000 | Loss: 0.00001959
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001946
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001913
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001909
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001906
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001905
Iteration 56/1000 | Loss: 0.00001905
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001904
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001902
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001900
Iteration 73/1000 | Loss: 0.00001900
Iteration 74/1000 | Loss: 0.00001900
Iteration 75/1000 | Loss: 0.00001900
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001899
Iteration 79/1000 | Loss: 0.00001899
Iteration 80/1000 | Loss: 0.00001899
Iteration 81/1000 | Loss: 0.00001898
Iteration 82/1000 | Loss: 0.00001898
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001889
Iteration 99/1000 | Loss: 0.00001889
Iteration 100/1000 | Loss: 0.00001888
Iteration 101/1000 | Loss: 0.00001888
Iteration 102/1000 | Loss: 0.00001887
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00001885
Iteration 105/1000 | Loss: 0.00001885
Iteration 106/1000 | Loss: 0.00001885
Iteration 107/1000 | Loss: 0.00001885
Iteration 108/1000 | Loss: 0.00001885
Iteration 109/1000 | Loss: 0.00001884
Iteration 110/1000 | Loss: 0.00001884
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001880
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001880
Iteration 122/1000 | Loss: 0.00001880
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.8804363207891583e-05, 1.8804363207891583e-05, 1.8804363207891583e-05, 1.8804363207891583e-05, 1.8804363207891583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8804363207891583e-05

Optimization complete. Final v2v error: 3.5892493724823 mm

Highest mean error: 3.977034330368042 mm for frame 10

Lowest mean error: 3.1062657833099365 mm for frame 239

Saving results

Total time: 39.04767036437988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496828
Iteration 2/25 | Loss: 0.00121054
Iteration 3/25 | Loss: 0.00103388
Iteration 4/25 | Loss: 0.00101476
Iteration 5/25 | Loss: 0.00100738
Iteration 6/25 | Loss: 0.00100532
Iteration 7/25 | Loss: 0.00100532
Iteration 8/25 | Loss: 0.00100532
Iteration 9/25 | Loss: 0.00100532
Iteration 10/25 | Loss: 0.00100532
Iteration 11/25 | Loss: 0.00100532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010053240694105625, 0.0010053240694105625, 0.0010053240694105625, 0.0010053240694105625, 0.0010053240694105625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010053240694105625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81381220
Iteration 2/25 | Loss: 0.00070182
Iteration 3/25 | Loss: 0.00070182
Iteration 4/25 | Loss: 0.00070182
Iteration 5/25 | Loss: 0.00070182
Iteration 6/25 | Loss: 0.00070182
Iteration 7/25 | Loss: 0.00070182
Iteration 8/25 | Loss: 0.00070182
Iteration 9/25 | Loss: 0.00070182
Iteration 10/25 | Loss: 0.00070182
Iteration 11/25 | Loss: 0.00070182
Iteration 12/25 | Loss: 0.00070182
Iteration 13/25 | Loss: 0.00070182
Iteration 14/25 | Loss: 0.00070182
Iteration 15/25 | Loss: 0.00070182
Iteration 16/25 | Loss: 0.00070182
Iteration 17/25 | Loss: 0.00070182
Iteration 18/25 | Loss: 0.00070182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000701818848028779, 0.000701818848028779, 0.000701818848028779, 0.000701818848028779, 0.000701818848028779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000701818848028779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070182
Iteration 2/1000 | Loss: 0.00003429
Iteration 3/1000 | Loss: 0.00002320
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001929
Iteration 8/1000 | Loss: 0.00001884
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001809
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001760
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001674
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001665
Iteration 24/1000 | Loss: 0.00001656
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001653
Iteration 28/1000 | Loss: 0.00001652
Iteration 29/1000 | Loss: 0.00001651
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001649
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001643
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001641
Iteration 57/1000 | Loss: 0.00001640
Iteration 58/1000 | Loss: 0.00001640
Iteration 59/1000 | Loss: 0.00001639
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001633
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001627
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001619
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001614
Iteration 143/1000 | Loss: 0.00001614
Iteration 144/1000 | Loss: 0.00001614
Iteration 145/1000 | Loss: 0.00001614
Iteration 146/1000 | Loss: 0.00001613
Iteration 147/1000 | Loss: 0.00001613
Iteration 148/1000 | Loss: 0.00001613
Iteration 149/1000 | Loss: 0.00001613
Iteration 150/1000 | Loss: 0.00001613
Iteration 151/1000 | Loss: 0.00001613
Iteration 152/1000 | Loss: 0.00001613
Iteration 153/1000 | Loss: 0.00001613
Iteration 154/1000 | Loss: 0.00001613
Iteration 155/1000 | Loss: 0.00001613
Iteration 156/1000 | Loss: 0.00001613
Iteration 157/1000 | Loss: 0.00001613
Iteration 158/1000 | Loss: 0.00001613
Iteration 159/1000 | Loss: 0.00001613
Iteration 160/1000 | Loss: 0.00001612
Iteration 161/1000 | Loss: 0.00001612
Iteration 162/1000 | Loss: 0.00001612
Iteration 163/1000 | Loss: 0.00001612
Iteration 164/1000 | Loss: 0.00001612
Iteration 165/1000 | Loss: 0.00001612
Iteration 166/1000 | Loss: 0.00001612
Iteration 167/1000 | Loss: 0.00001612
Iteration 168/1000 | Loss: 0.00001612
Iteration 169/1000 | Loss: 0.00001612
Iteration 170/1000 | Loss: 0.00001611
Iteration 171/1000 | Loss: 0.00001611
Iteration 172/1000 | Loss: 0.00001611
Iteration 173/1000 | Loss: 0.00001611
Iteration 174/1000 | Loss: 0.00001611
Iteration 175/1000 | Loss: 0.00001611
Iteration 176/1000 | Loss: 0.00001611
Iteration 177/1000 | Loss: 0.00001611
Iteration 178/1000 | Loss: 0.00001611
Iteration 179/1000 | Loss: 0.00001611
Iteration 180/1000 | Loss: 0.00001611
Iteration 181/1000 | Loss: 0.00001611
Iteration 182/1000 | Loss: 0.00001611
Iteration 183/1000 | Loss: 0.00001611
Iteration 184/1000 | Loss: 0.00001611
Iteration 185/1000 | Loss: 0.00001611
Iteration 186/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.6110996511997655e-05, 1.6110996511997655e-05, 1.6110996511997655e-05, 1.6110996511997655e-05, 1.6110996511997655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6110996511997655e-05

Optimization complete. Final v2v error: 3.371281147003174 mm

Highest mean error: 4.129557132720947 mm for frame 263

Lowest mean error: 3.2659754753112793 mm for frame 89

Saving results

Total time: 58.33035969734192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883895
Iteration 2/25 | Loss: 0.00185757
Iteration 3/25 | Loss: 0.00127231
Iteration 4/25 | Loss: 0.00124736
Iteration 5/25 | Loss: 0.00124074
Iteration 6/25 | Loss: 0.00123864
Iteration 7/25 | Loss: 0.00123864
Iteration 8/25 | Loss: 0.00123864
Iteration 9/25 | Loss: 0.00123864
Iteration 10/25 | Loss: 0.00123864
Iteration 11/25 | Loss: 0.00123864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012386416783556342, 0.0012386416783556342, 0.0012386416783556342, 0.0012386416783556342, 0.0012386416783556342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012386416783556342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91481268
Iteration 2/25 | Loss: 0.00072158
Iteration 3/25 | Loss: 0.00072158
Iteration 4/25 | Loss: 0.00072157
Iteration 5/25 | Loss: 0.00072157
Iteration 6/25 | Loss: 0.00072157
Iteration 7/25 | Loss: 0.00072157
Iteration 8/25 | Loss: 0.00072157
Iteration 9/25 | Loss: 0.00072157
Iteration 10/25 | Loss: 0.00072157
Iteration 11/25 | Loss: 0.00072157
Iteration 12/25 | Loss: 0.00072157
Iteration 13/25 | Loss: 0.00072157
Iteration 14/25 | Loss: 0.00072157
Iteration 15/25 | Loss: 0.00072157
Iteration 16/25 | Loss: 0.00072157
Iteration 17/25 | Loss: 0.00072157
Iteration 18/25 | Loss: 0.00072157
Iteration 19/25 | Loss: 0.00072157
Iteration 20/25 | Loss: 0.00072157
Iteration 21/25 | Loss: 0.00072157
Iteration 22/25 | Loss: 0.00072157
Iteration 23/25 | Loss: 0.00072157
Iteration 24/25 | Loss: 0.00072157
Iteration 25/25 | Loss: 0.00072157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072157
Iteration 2/1000 | Loss: 0.00009062
Iteration 3/1000 | Loss: 0.00006408
Iteration 4/1000 | Loss: 0.00005437
Iteration 5/1000 | Loss: 0.00005115
Iteration 6/1000 | Loss: 0.00004952
Iteration 7/1000 | Loss: 0.00004835
Iteration 8/1000 | Loss: 0.00004716
Iteration 9/1000 | Loss: 0.00004595
Iteration 10/1000 | Loss: 0.00004500
Iteration 11/1000 | Loss: 0.00004403
Iteration 12/1000 | Loss: 0.00004349
Iteration 13/1000 | Loss: 0.00004301
Iteration 14/1000 | Loss: 0.00004243
Iteration 15/1000 | Loss: 0.00004205
Iteration 16/1000 | Loss: 0.00004166
Iteration 17/1000 | Loss: 0.00004139
Iteration 18/1000 | Loss: 0.00004100
Iteration 19/1000 | Loss: 0.00004067
Iteration 20/1000 | Loss: 0.00004037
Iteration 21/1000 | Loss: 0.00004011
Iteration 22/1000 | Loss: 0.00003987
Iteration 23/1000 | Loss: 0.00003969
Iteration 24/1000 | Loss: 0.00003953
Iteration 25/1000 | Loss: 0.00003941
Iteration 26/1000 | Loss: 0.00003939
Iteration 27/1000 | Loss: 0.00003931
Iteration 28/1000 | Loss: 0.00003929
Iteration 29/1000 | Loss: 0.00003929
Iteration 30/1000 | Loss: 0.00003925
Iteration 31/1000 | Loss: 0.00003923
Iteration 32/1000 | Loss: 0.00003923
Iteration 33/1000 | Loss: 0.00003920
Iteration 34/1000 | Loss: 0.00003920
Iteration 35/1000 | Loss: 0.00003920
Iteration 36/1000 | Loss: 0.00003920
Iteration 37/1000 | Loss: 0.00003920
Iteration 38/1000 | Loss: 0.00003920
Iteration 39/1000 | Loss: 0.00003919
Iteration 40/1000 | Loss: 0.00003919
Iteration 41/1000 | Loss: 0.00003916
Iteration 42/1000 | Loss: 0.00003916
Iteration 43/1000 | Loss: 0.00003916
Iteration 44/1000 | Loss: 0.00003916
Iteration 45/1000 | Loss: 0.00003916
Iteration 46/1000 | Loss: 0.00003915
Iteration 47/1000 | Loss: 0.00003915
Iteration 48/1000 | Loss: 0.00003915
Iteration 49/1000 | Loss: 0.00003915
Iteration 50/1000 | Loss: 0.00003915
Iteration 51/1000 | Loss: 0.00003909
Iteration 52/1000 | Loss: 0.00003904
Iteration 53/1000 | Loss: 0.00003903
Iteration 54/1000 | Loss: 0.00003903
Iteration 55/1000 | Loss: 0.00003900
Iteration 56/1000 | Loss: 0.00003899
Iteration 57/1000 | Loss: 0.00003899
Iteration 58/1000 | Loss: 0.00003897
Iteration 59/1000 | Loss: 0.00003895
Iteration 60/1000 | Loss: 0.00003895
Iteration 61/1000 | Loss: 0.00003895
Iteration 62/1000 | Loss: 0.00003895
Iteration 63/1000 | Loss: 0.00003895
Iteration 64/1000 | Loss: 0.00003895
Iteration 65/1000 | Loss: 0.00003895
Iteration 66/1000 | Loss: 0.00003895
Iteration 67/1000 | Loss: 0.00003895
Iteration 68/1000 | Loss: 0.00003895
Iteration 69/1000 | Loss: 0.00003895
Iteration 70/1000 | Loss: 0.00003894
Iteration 71/1000 | Loss: 0.00003894
Iteration 72/1000 | Loss: 0.00003894
Iteration 73/1000 | Loss: 0.00003894
Iteration 74/1000 | Loss: 0.00003894
Iteration 75/1000 | Loss: 0.00003893
Iteration 76/1000 | Loss: 0.00003893
Iteration 77/1000 | Loss: 0.00003893
Iteration 78/1000 | Loss: 0.00003893
Iteration 79/1000 | Loss: 0.00003893
Iteration 80/1000 | Loss: 0.00003893
Iteration 81/1000 | Loss: 0.00003893
Iteration 82/1000 | Loss: 0.00003892
Iteration 83/1000 | Loss: 0.00003892
Iteration 84/1000 | Loss: 0.00003892
Iteration 85/1000 | Loss: 0.00003892
Iteration 86/1000 | Loss: 0.00003891
Iteration 87/1000 | Loss: 0.00003891
Iteration 88/1000 | Loss: 0.00003891
Iteration 89/1000 | Loss: 0.00003891
Iteration 90/1000 | Loss: 0.00003891
Iteration 91/1000 | Loss: 0.00003891
Iteration 92/1000 | Loss: 0.00003891
Iteration 93/1000 | Loss: 0.00003891
Iteration 94/1000 | Loss: 0.00003891
Iteration 95/1000 | Loss: 0.00003891
Iteration 96/1000 | Loss: 0.00003891
Iteration 97/1000 | Loss: 0.00003891
Iteration 98/1000 | Loss: 0.00003891
Iteration 99/1000 | Loss: 0.00003890
Iteration 100/1000 | Loss: 0.00003890
Iteration 101/1000 | Loss: 0.00003890
Iteration 102/1000 | Loss: 0.00003890
Iteration 103/1000 | Loss: 0.00003890
Iteration 104/1000 | Loss: 0.00003890
Iteration 105/1000 | Loss: 0.00003889
Iteration 106/1000 | Loss: 0.00003889
Iteration 107/1000 | Loss: 0.00003889
Iteration 108/1000 | Loss: 0.00003889
Iteration 109/1000 | Loss: 0.00003889
Iteration 110/1000 | Loss: 0.00003889
Iteration 111/1000 | Loss: 0.00003888
Iteration 112/1000 | Loss: 0.00003888
Iteration 113/1000 | Loss: 0.00003888
Iteration 114/1000 | Loss: 0.00003888
Iteration 115/1000 | Loss: 0.00003888
Iteration 116/1000 | Loss: 0.00003888
Iteration 117/1000 | Loss: 0.00003888
Iteration 118/1000 | Loss: 0.00003887
Iteration 119/1000 | Loss: 0.00003887
Iteration 120/1000 | Loss: 0.00003887
Iteration 121/1000 | Loss: 0.00003887
Iteration 122/1000 | Loss: 0.00003887
Iteration 123/1000 | Loss: 0.00003887
Iteration 124/1000 | Loss: 0.00003887
Iteration 125/1000 | Loss: 0.00003887
Iteration 126/1000 | Loss: 0.00003887
Iteration 127/1000 | Loss: 0.00003887
Iteration 128/1000 | Loss: 0.00003887
Iteration 129/1000 | Loss: 0.00003886
Iteration 130/1000 | Loss: 0.00003886
Iteration 131/1000 | Loss: 0.00003886
Iteration 132/1000 | Loss: 0.00003886
Iteration 133/1000 | Loss: 0.00003886
Iteration 134/1000 | Loss: 0.00003886
Iteration 135/1000 | Loss: 0.00003886
Iteration 136/1000 | Loss: 0.00003885
Iteration 137/1000 | Loss: 0.00003885
Iteration 138/1000 | Loss: 0.00003885
Iteration 139/1000 | Loss: 0.00003885
Iteration 140/1000 | Loss: 0.00003885
Iteration 141/1000 | Loss: 0.00003885
Iteration 142/1000 | Loss: 0.00003885
Iteration 143/1000 | Loss: 0.00003885
Iteration 144/1000 | Loss: 0.00003884
Iteration 145/1000 | Loss: 0.00003884
Iteration 146/1000 | Loss: 0.00003884
Iteration 147/1000 | Loss: 0.00003884
Iteration 148/1000 | Loss: 0.00003884
Iteration 149/1000 | Loss: 0.00003884
Iteration 150/1000 | Loss: 0.00003884
Iteration 151/1000 | Loss: 0.00003884
Iteration 152/1000 | Loss: 0.00003884
Iteration 153/1000 | Loss: 0.00003884
Iteration 154/1000 | Loss: 0.00003884
Iteration 155/1000 | Loss: 0.00003884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.884061152348295e-05, 3.884061152348295e-05, 3.884061152348295e-05, 3.884061152348295e-05, 3.884061152348295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.884061152348295e-05

Optimization complete. Final v2v error: 5.127974987030029 mm

Highest mean error: 5.69361686706543 mm for frame 16

Lowest mean error: 4.575675964355469 mm for frame 183

Saving results

Total time: 68.24480319023132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00180094
Iteration 2/25 | Loss: 0.00099939
Iteration 3/25 | Loss: 0.00093552
Iteration 4/25 | Loss: 0.00091992
Iteration 5/25 | Loss: 0.00091244
Iteration 6/25 | Loss: 0.00091001
Iteration 7/25 | Loss: 0.00090982
Iteration 8/25 | Loss: 0.00090982
Iteration 9/25 | Loss: 0.00090982
Iteration 10/25 | Loss: 0.00090982
Iteration 11/25 | Loss: 0.00090982
Iteration 12/25 | Loss: 0.00090982
Iteration 13/25 | Loss: 0.00090982
Iteration 14/25 | Loss: 0.00090982
Iteration 15/25 | Loss: 0.00090982
Iteration 16/25 | Loss: 0.00090982
Iteration 17/25 | Loss: 0.00090982
Iteration 18/25 | Loss: 0.00090982
Iteration 19/25 | Loss: 0.00090982
Iteration 20/25 | Loss: 0.00090982
Iteration 21/25 | Loss: 0.00090982
Iteration 22/25 | Loss: 0.00090982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009098245645873249, 0.0009098245645873249, 0.0009098245645873249, 0.0009098245645873249, 0.0009098245645873249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009098245645873249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37178504
Iteration 2/25 | Loss: 0.00107589
Iteration 3/25 | Loss: 0.00107589
Iteration 4/25 | Loss: 0.00107589
Iteration 5/25 | Loss: 0.00107589
Iteration 6/25 | Loss: 0.00107589
Iteration 7/25 | Loss: 0.00107589
Iteration 8/25 | Loss: 0.00107589
Iteration 9/25 | Loss: 0.00107589
Iteration 10/25 | Loss: 0.00107589
Iteration 11/25 | Loss: 0.00107589
Iteration 12/25 | Loss: 0.00107589
Iteration 13/25 | Loss: 0.00107589
Iteration 14/25 | Loss: 0.00107589
Iteration 15/25 | Loss: 0.00107589
Iteration 16/25 | Loss: 0.00107589
Iteration 17/25 | Loss: 0.00107589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010758878197520971, 0.0010758878197520971, 0.0010758878197520971, 0.0010758878197520971, 0.0010758878197520971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010758878197520971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107589
Iteration 2/1000 | Loss: 0.00002979
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001088
Iteration 6/1000 | Loss: 0.00001016
Iteration 7/1000 | Loss: 0.00000978
Iteration 8/1000 | Loss: 0.00000951
Iteration 9/1000 | Loss: 0.00000947
Iteration 10/1000 | Loss: 0.00000946
Iteration 11/1000 | Loss: 0.00000945
Iteration 12/1000 | Loss: 0.00000945
Iteration 13/1000 | Loss: 0.00000931
Iteration 14/1000 | Loss: 0.00000910
Iteration 15/1000 | Loss: 0.00000897
Iteration 16/1000 | Loss: 0.00000897
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000894
Iteration 22/1000 | Loss: 0.00000893
Iteration 23/1000 | Loss: 0.00000892
Iteration 24/1000 | Loss: 0.00000890
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000890
Iteration 27/1000 | Loss: 0.00000890
Iteration 28/1000 | Loss: 0.00000890
Iteration 29/1000 | Loss: 0.00000890
Iteration 30/1000 | Loss: 0.00000890
Iteration 31/1000 | Loss: 0.00000888
Iteration 32/1000 | Loss: 0.00000884
Iteration 33/1000 | Loss: 0.00000883
Iteration 34/1000 | Loss: 0.00000883
Iteration 35/1000 | Loss: 0.00000883
Iteration 36/1000 | Loss: 0.00000882
Iteration 37/1000 | Loss: 0.00000881
Iteration 38/1000 | Loss: 0.00000881
Iteration 39/1000 | Loss: 0.00000881
Iteration 40/1000 | Loss: 0.00000881
Iteration 41/1000 | Loss: 0.00000880
Iteration 42/1000 | Loss: 0.00000880
Iteration 43/1000 | Loss: 0.00000880
Iteration 44/1000 | Loss: 0.00000879
Iteration 45/1000 | Loss: 0.00000879
Iteration 46/1000 | Loss: 0.00000879
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000879
Iteration 52/1000 | Loss: 0.00000879
Iteration 53/1000 | Loss: 0.00000879
Iteration 54/1000 | Loss: 0.00000879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [8.793342203716747e-06, 8.793342203716747e-06, 8.793342203716747e-06, 8.793342203716747e-06, 8.793342203716747e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.793342203716747e-06

Optimization complete. Final v2v error: 2.569131851196289 mm

Highest mean error: 2.9435133934020996 mm for frame 38

Lowest mean error: 2.352978229522705 mm for frame 45

Saving results

Total time: 32.15924549102783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503225
Iteration 2/25 | Loss: 0.00112302
Iteration 3/25 | Loss: 0.00104296
Iteration 4/25 | Loss: 0.00103221
Iteration 5/25 | Loss: 0.00102867
Iteration 6/25 | Loss: 0.00102803
Iteration 7/25 | Loss: 0.00102803
Iteration 8/25 | Loss: 0.00102803
Iteration 9/25 | Loss: 0.00102803
Iteration 10/25 | Loss: 0.00102803
Iteration 11/25 | Loss: 0.00102803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010280337883159518, 0.0010280337883159518, 0.0010280337883159518, 0.0010280337883159518, 0.0010280337883159518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010280337883159518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38791811
Iteration 2/25 | Loss: 0.00074945
Iteration 3/25 | Loss: 0.00074942
Iteration 4/25 | Loss: 0.00074942
Iteration 5/25 | Loss: 0.00074942
Iteration 6/25 | Loss: 0.00074942
Iteration 7/25 | Loss: 0.00074942
Iteration 8/25 | Loss: 0.00074942
Iteration 9/25 | Loss: 0.00074942
Iteration 10/25 | Loss: 0.00074942
Iteration 11/25 | Loss: 0.00074942
Iteration 12/25 | Loss: 0.00074942
Iteration 13/25 | Loss: 0.00074942
Iteration 14/25 | Loss: 0.00074942
Iteration 15/25 | Loss: 0.00074942
Iteration 16/25 | Loss: 0.00074942
Iteration 17/25 | Loss: 0.00074942
Iteration 18/25 | Loss: 0.00074942
Iteration 19/25 | Loss: 0.00074942
Iteration 20/25 | Loss: 0.00074942
Iteration 21/25 | Loss: 0.00074942
Iteration 22/25 | Loss: 0.00074942
Iteration 23/25 | Loss: 0.00074942
Iteration 24/25 | Loss: 0.00074942
Iteration 25/25 | Loss: 0.00074942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074942
Iteration 2/1000 | Loss: 0.00003560
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00001557
Iteration 6/1000 | Loss: 0.00001498
Iteration 7/1000 | Loss: 0.00001456
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001368
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001340
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001337
Iteration 79/1000 | Loss: 0.00001337
Iteration 80/1000 | Loss: 0.00001337
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001334
Iteration 92/1000 | Loss: 0.00001334
Iteration 93/1000 | Loss: 0.00001334
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001333
Iteration 99/1000 | Loss: 0.00001333
Iteration 100/1000 | Loss: 0.00001333
Iteration 101/1000 | Loss: 0.00001333
Iteration 102/1000 | Loss: 0.00001333
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001331
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001329
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001326
Iteration 137/1000 | Loss: 0.00001326
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001326
Iteration 142/1000 | Loss: 0.00001326
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001324
Iteration 152/1000 | Loss: 0.00001324
Iteration 153/1000 | Loss: 0.00001324
Iteration 154/1000 | Loss: 0.00001324
Iteration 155/1000 | Loss: 0.00001324
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001323
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001322
Iteration 174/1000 | Loss: 0.00001322
Iteration 175/1000 | Loss: 0.00001322
Iteration 176/1000 | Loss: 0.00001321
Iteration 177/1000 | Loss: 0.00001321
Iteration 178/1000 | Loss: 0.00001321
Iteration 179/1000 | Loss: 0.00001321
Iteration 180/1000 | Loss: 0.00001321
Iteration 181/1000 | Loss: 0.00001320
Iteration 182/1000 | Loss: 0.00001320
Iteration 183/1000 | Loss: 0.00001320
Iteration 184/1000 | Loss: 0.00001320
Iteration 185/1000 | Loss: 0.00001320
Iteration 186/1000 | Loss: 0.00001319
Iteration 187/1000 | Loss: 0.00001319
Iteration 188/1000 | Loss: 0.00001319
Iteration 189/1000 | Loss: 0.00001319
Iteration 190/1000 | Loss: 0.00001319
Iteration 191/1000 | Loss: 0.00001319
Iteration 192/1000 | Loss: 0.00001318
Iteration 193/1000 | Loss: 0.00001318
Iteration 194/1000 | Loss: 0.00001318
Iteration 195/1000 | Loss: 0.00001318
Iteration 196/1000 | Loss: 0.00001318
Iteration 197/1000 | Loss: 0.00001318
Iteration 198/1000 | Loss: 0.00001318
Iteration 199/1000 | Loss: 0.00001318
Iteration 200/1000 | Loss: 0.00001318
Iteration 201/1000 | Loss: 0.00001318
Iteration 202/1000 | Loss: 0.00001318
Iteration 203/1000 | Loss: 0.00001318
Iteration 204/1000 | Loss: 0.00001318
Iteration 205/1000 | Loss: 0.00001318
Iteration 206/1000 | Loss: 0.00001318
Iteration 207/1000 | Loss: 0.00001318
Iteration 208/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.317785972787533e-05, 1.317785972787533e-05, 1.317785972787533e-05, 1.317785972787533e-05, 1.317785972787533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.317785972787533e-05

Optimization complete. Final v2v error: 3.0123074054718018 mm

Highest mean error: 3.7591605186462402 mm for frame 159

Lowest mean error: 2.5795724391937256 mm for frame 54

Saving results

Total time: 44.46819043159485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806142
Iteration 2/25 | Loss: 0.00106910
Iteration 3/25 | Loss: 0.00095701
Iteration 4/25 | Loss: 0.00094501
Iteration 5/25 | Loss: 0.00094250
Iteration 6/25 | Loss: 0.00094219
Iteration 7/25 | Loss: 0.00094219
Iteration 8/25 | Loss: 0.00094219
Iteration 9/25 | Loss: 0.00094220
Iteration 10/25 | Loss: 0.00094219
Iteration 11/25 | Loss: 0.00094219
Iteration 12/25 | Loss: 0.00094219
Iteration 13/25 | Loss: 0.00094219
Iteration 14/25 | Loss: 0.00094219
Iteration 15/25 | Loss: 0.00094219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009421949507668614, 0.0009421949507668614, 0.0009421949507668614, 0.0009421949507668614, 0.0009421949507668614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009421949507668614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38771343
Iteration 2/25 | Loss: 0.00066672
Iteration 3/25 | Loss: 0.00066672
Iteration 4/25 | Loss: 0.00066672
Iteration 5/25 | Loss: 0.00066672
Iteration 6/25 | Loss: 0.00066672
Iteration 7/25 | Loss: 0.00066672
Iteration 8/25 | Loss: 0.00066672
Iteration 9/25 | Loss: 0.00066672
Iteration 10/25 | Loss: 0.00066672
Iteration 11/25 | Loss: 0.00066672
Iteration 12/25 | Loss: 0.00066672
Iteration 13/25 | Loss: 0.00066672
Iteration 14/25 | Loss: 0.00066672
Iteration 15/25 | Loss: 0.00066672
Iteration 16/25 | Loss: 0.00066672
Iteration 17/25 | Loss: 0.00066672
Iteration 18/25 | Loss: 0.00066672
Iteration 19/25 | Loss: 0.00066672
Iteration 20/25 | Loss: 0.00066672
Iteration 21/25 | Loss: 0.00066672
Iteration 22/25 | Loss: 0.00066672
Iteration 23/25 | Loss: 0.00066672
Iteration 24/25 | Loss: 0.00066672
Iteration 25/25 | Loss: 0.00066672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006667186971753836, 0.0006667186971753836, 0.0006667186971753836, 0.0006667186971753836, 0.0006667186971753836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006667186971753836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066672
Iteration 2/1000 | Loss: 0.00001639
Iteration 3/1000 | Loss: 0.00001119
Iteration 4/1000 | Loss: 0.00000970
Iteration 5/1000 | Loss: 0.00000907
Iteration 6/1000 | Loss: 0.00000870
Iteration 7/1000 | Loss: 0.00000832
Iteration 8/1000 | Loss: 0.00000821
Iteration 9/1000 | Loss: 0.00000804
Iteration 10/1000 | Loss: 0.00000788
Iteration 11/1000 | Loss: 0.00000783
Iteration 12/1000 | Loss: 0.00000783
Iteration 13/1000 | Loss: 0.00000782
Iteration 14/1000 | Loss: 0.00000781
Iteration 15/1000 | Loss: 0.00000780
Iteration 16/1000 | Loss: 0.00000779
Iteration 17/1000 | Loss: 0.00000779
Iteration 18/1000 | Loss: 0.00000777
Iteration 19/1000 | Loss: 0.00000774
Iteration 20/1000 | Loss: 0.00000773
Iteration 21/1000 | Loss: 0.00000772
Iteration 22/1000 | Loss: 0.00000772
Iteration 23/1000 | Loss: 0.00000772
Iteration 24/1000 | Loss: 0.00000771
Iteration 25/1000 | Loss: 0.00000771
Iteration 26/1000 | Loss: 0.00000771
Iteration 27/1000 | Loss: 0.00000770
Iteration 28/1000 | Loss: 0.00000769
Iteration 29/1000 | Loss: 0.00000768
Iteration 30/1000 | Loss: 0.00000767
Iteration 31/1000 | Loss: 0.00000767
Iteration 32/1000 | Loss: 0.00000767
Iteration 33/1000 | Loss: 0.00000767
Iteration 34/1000 | Loss: 0.00000766
Iteration 35/1000 | Loss: 0.00000766
Iteration 36/1000 | Loss: 0.00000766
Iteration 37/1000 | Loss: 0.00000766
Iteration 38/1000 | Loss: 0.00000765
Iteration 39/1000 | Loss: 0.00000764
Iteration 40/1000 | Loss: 0.00000763
Iteration 41/1000 | Loss: 0.00000763
Iteration 42/1000 | Loss: 0.00000762
Iteration 43/1000 | Loss: 0.00000762
Iteration 44/1000 | Loss: 0.00000761
Iteration 45/1000 | Loss: 0.00000761
Iteration 46/1000 | Loss: 0.00000761
Iteration 47/1000 | Loss: 0.00000761
Iteration 48/1000 | Loss: 0.00000761
Iteration 49/1000 | Loss: 0.00000760
Iteration 50/1000 | Loss: 0.00000760
Iteration 51/1000 | Loss: 0.00000760
Iteration 52/1000 | Loss: 0.00000759
Iteration 53/1000 | Loss: 0.00000759
Iteration 54/1000 | Loss: 0.00000759
Iteration 55/1000 | Loss: 0.00000759
Iteration 56/1000 | Loss: 0.00000759
Iteration 57/1000 | Loss: 0.00000759
Iteration 58/1000 | Loss: 0.00000759
Iteration 59/1000 | Loss: 0.00000758
Iteration 60/1000 | Loss: 0.00000758
Iteration 61/1000 | Loss: 0.00000758
Iteration 62/1000 | Loss: 0.00000758
Iteration 63/1000 | Loss: 0.00000758
Iteration 64/1000 | Loss: 0.00000758
Iteration 65/1000 | Loss: 0.00000758
Iteration 66/1000 | Loss: 0.00000758
Iteration 67/1000 | Loss: 0.00000758
Iteration 68/1000 | Loss: 0.00000757
Iteration 69/1000 | Loss: 0.00000757
Iteration 70/1000 | Loss: 0.00000756
Iteration 71/1000 | Loss: 0.00000756
Iteration 72/1000 | Loss: 0.00000756
Iteration 73/1000 | Loss: 0.00000756
Iteration 74/1000 | Loss: 0.00000756
Iteration 75/1000 | Loss: 0.00000756
Iteration 76/1000 | Loss: 0.00000755
Iteration 77/1000 | Loss: 0.00000755
Iteration 78/1000 | Loss: 0.00000755
Iteration 79/1000 | Loss: 0.00000755
Iteration 80/1000 | Loss: 0.00000755
Iteration 81/1000 | Loss: 0.00000755
Iteration 82/1000 | Loss: 0.00000755
Iteration 83/1000 | Loss: 0.00000755
Iteration 84/1000 | Loss: 0.00000755
Iteration 85/1000 | Loss: 0.00000755
Iteration 86/1000 | Loss: 0.00000755
Iteration 87/1000 | Loss: 0.00000755
Iteration 88/1000 | Loss: 0.00000755
Iteration 89/1000 | Loss: 0.00000755
Iteration 90/1000 | Loss: 0.00000755
Iteration 91/1000 | Loss: 0.00000755
Iteration 92/1000 | Loss: 0.00000755
Iteration 93/1000 | Loss: 0.00000755
Iteration 94/1000 | Loss: 0.00000755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [7.551258022431284e-06, 7.551258022431284e-06, 7.551258022431284e-06, 7.551258022431284e-06, 7.551258022431284e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.551258022431284e-06

Optimization complete. Final v2v error: 2.3517377376556396 mm

Highest mean error: 2.576378583908081 mm for frame 31

Lowest mean error: 2.2532525062561035 mm for frame 174

Saving results

Total time: 29.683695554733276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014597
Iteration 2/25 | Loss: 0.01014597
Iteration 3/25 | Loss: 0.00297699
Iteration 4/25 | Loss: 0.00211767
Iteration 5/25 | Loss: 0.00184772
Iteration 6/25 | Loss: 0.00160440
Iteration 7/25 | Loss: 0.00147600
Iteration 8/25 | Loss: 0.00143643
Iteration 9/25 | Loss: 0.00134023
Iteration 10/25 | Loss: 0.00130039
Iteration 11/25 | Loss: 0.00127396
Iteration 12/25 | Loss: 0.00129040
Iteration 13/25 | Loss: 0.00126969
Iteration 14/25 | Loss: 0.00125491
Iteration 15/25 | Loss: 0.00126289
Iteration 16/25 | Loss: 0.00123725
Iteration 17/25 | Loss: 0.00122922
Iteration 18/25 | Loss: 0.00122254
Iteration 19/25 | Loss: 0.00122766
Iteration 20/25 | Loss: 0.00122863
Iteration 21/25 | Loss: 0.00122453
Iteration 22/25 | Loss: 0.00122193
Iteration 23/25 | Loss: 0.00121934
Iteration 24/25 | Loss: 0.00122004
Iteration 25/25 | Loss: 0.00121802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56326652
Iteration 2/25 | Loss: 0.00283243
Iteration 3/25 | Loss: 0.00384778
Iteration 4/25 | Loss: 0.00243359
Iteration 5/25 | Loss: 0.00203457
Iteration 6/25 | Loss: 0.00203457
Iteration 7/25 | Loss: 0.00203457
Iteration 8/25 | Loss: 0.00203457
Iteration 9/25 | Loss: 0.00203457
Iteration 10/25 | Loss: 0.00203457
Iteration 11/25 | Loss: 0.00203456
Iteration 12/25 | Loss: 0.00203456
Iteration 13/25 | Loss: 0.00203456
Iteration 14/25 | Loss: 0.00203456
Iteration 15/25 | Loss: 0.00203456
Iteration 16/25 | Loss: 0.00203456
Iteration 17/25 | Loss: 0.00203456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002034564036875963, 0.002034564036875963, 0.002034564036875963, 0.002034564036875963, 0.002034564036875963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002034564036875963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203456
Iteration 2/1000 | Loss: 0.00155290
Iteration 3/1000 | Loss: 0.00153056
Iteration 4/1000 | Loss: 0.00028069
Iteration 5/1000 | Loss: 0.00038486
Iteration 6/1000 | Loss: 0.00027826
Iteration 7/1000 | Loss: 0.00047900
Iteration 8/1000 | Loss: 0.00045343
Iteration 9/1000 | Loss: 0.00104725
Iteration 10/1000 | Loss: 0.00037159
Iteration 11/1000 | Loss: 0.00176906
Iteration 12/1000 | Loss: 0.00078527
Iteration 13/1000 | Loss: 0.00030950
Iteration 14/1000 | Loss: 0.00071375
Iteration 15/1000 | Loss: 0.00079606
Iteration 16/1000 | Loss: 0.00052406
Iteration 17/1000 | Loss: 0.00015399
Iteration 18/1000 | Loss: 0.00027061
Iteration 19/1000 | Loss: 0.00066852
Iteration 20/1000 | Loss: 0.00029911
Iteration 21/1000 | Loss: 0.00013038
Iteration 22/1000 | Loss: 0.00034849
Iteration 23/1000 | Loss: 0.00016899
Iteration 24/1000 | Loss: 0.00080759
Iteration 25/1000 | Loss: 0.00028903
Iteration 26/1000 | Loss: 0.00120079
Iteration 27/1000 | Loss: 0.00043268
Iteration 28/1000 | Loss: 0.00033138
Iteration 29/1000 | Loss: 0.00014535
Iteration 30/1000 | Loss: 0.00039692
Iteration 31/1000 | Loss: 0.00046107
Iteration 32/1000 | Loss: 0.00055818
Iteration 33/1000 | Loss: 0.00019278
Iteration 34/1000 | Loss: 0.00038440
Iteration 35/1000 | Loss: 0.00033146
Iteration 36/1000 | Loss: 0.00027298
Iteration 37/1000 | Loss: 0.00012259
Iteration 38/1000 | Loss: 0.00022342
Iteration 39/1000 | Loss: 0.00010121
Iteration 40/1000 | Loss: 0.00045043
Iteration 41/1000 | Loss: 0.00038611
Iteration 42/1000 | Loss: 0.00060548
Iteration 43/1000 | Loss: 0.00018644
Iteration 44/1000 | Loss: 0.00009902
Iteration 45/1000 | Loss: 0.00034882
Iteration 46/1000 | Loss: 0.00017051
Iteration 47/1000 | Loss: 0.00026104
Iteration 48/1000 | Loss: 0.00162683
Iteration 49/1000 | Loss: 0.00010516
Iteration 50/1000 | Loss: 0.00036680
Iteration 51/1000 | Loss: 0.00018203
Iteration 52/1000 | Loss: 0.00033399
Iteration 53/1000 | Loss: 0.00011217
Iteration 54/1000 | Loss: 0.00018669
Iteration 55/1000 | Loss: 0.00008193
Iteration 56/1000 | Loss: 0.00014702
Iteration 57/1000 | Loss: 0.00007614
Iteration 58/1000 | Loss: 0.00032543
Iteration 59/1000 | Loss: 0.00053458
Iteration 60/1000 | Loss: 0.00029703
Iteration 61/1000 | Loss: 0.00089858
Iteration 62/1000 | Loss: 0.00023953
Iteration 63/1000 | Loss: 0.00125273
Iteration 64/1000 | Loss: 0.00040057
Iteration 65/1000 | Loss: 0.00034909
Iteration 66/1000 | Loss: 0.00009611
Iteration 67/1000 | Loss: 0.00009033
Iteration 68/1000 | Loss: 0.00013434
Iteration 69/1000 | Loss: 0.00015585
Iteration 70/1000 | Loss: 0.00008332
Iteration 71/1000 | Loss: 0.00016653
Iteration 72/1000 | Loss: 0.00191802
Iteration 73/1000 | Loss: 0.00084132
Iteration 74/1000 | Loss: 0.00043528
Iteration 75/1000 | Loss: 0.00045578
Iteration 76/1000 | Loss: 0.00022956
Iteration 77/1000 | Loss: 0.00017887
Iteration 78/1000 | Loss: 0.00035063
Iteration 79/1000 | Loss: 0.00071045
Iteration 80/1000 | Loss: 0.00090107
Iteration 81/1000 | Loss: 0.00084973
Iteration 82/1000 | Loss: 0.00039081
Iteration 83/1000 | Loss: 0.00019947
Iteration 84/1000 | Loss: 0.00067581
Iteration 85/1000 | Loss: 0.00043886
Iteration 86/1000 | Loss: 0.00037225
Iteration 87/1000 | Loss: 0.00013896
Iteration 88/1000 | Loss: 0.00006926
Iteration 89/1000 | Loss: 0.00038812
Iteration 90/1000 | Loss: 0.00025288
Iteration 91/1000 | Loss: 0.00032926
Iteration 92/1000 | Loss: 0.00026192
Iteration 93/1000 | Loss: 0.00045508
Iteration 94/1000 | Loss: 0.00039256
Iteration 95/1000 | Loss: 0.00046167
Iteration 96/1000 | Loss: 0.00036914
Iteration 97/1000 | Loss: 0.00046182
Iteration 98/1000 | Loss: 0.00043347
Iteration 99/1000 | Loss: 0.00029854
Iteration 100/1000 | Loss: 0.00034924
Iteration 101/1000 | Loss: 0.00030918
Iteration 102/1000 | Loss: 0.00006250
Iteration 103/1000 | Loss: 0.00005648
Iteration 104/1000 | Loss: 0.00005744
Iteration 105/1000 | Loss: 0.00005464
Iteration 106/1000 | Loss: 0.00017656
Iteration 107/1000 | Loss: 0.00005999
Iteration 108/1000 | Loss: 0.00004962
Iteration 109/1000 | Loss: 0.00004849
Iteration 110/1000 | Loss: 0.00004590
Iteration 111/1000 | Loss: 0.00004719
Iteration 112/1000 | Loss: 0.00005171
Iteration 113/1000 | Loss: 0.00008683
Iteration 114/1000 | Loss: 0.00005500
Iteration 115/1000 | Loss: 0.00005656
Iteration 116/1000 | Loss: 0.00005035
Iteration 117/1000 | Loss: 0.00005694
Iteration 118/1000 | Loss: 0.00004515
Iteration 119/1000 | Loss: 0.00009695
Iteration 120/1000 | Loss: 0.00004019
Iteration 121/1000 | Loss: 0.00005068
Iteration 122/1000 | Loss: 0.00003840
Iteration 123/1000 | Loss: 0.00010503
Iteration 124/1000 | Loss: 0.00021095
Iteration 125/1000 | Loss: 0.00015956
Iteration 126/1000 | Loss: 0.00006348
Iteration 127/1000 | Loss: 0.00005734
Iteration 128/1000 | Loss: 0.00003570
Iteration 129/1000 | Loss: 0.00003462
Iteration 130/1000 | Loss: 0.00003526
Iteration 131/1000 | Loss: 0.00008083
Iteration 132/1000 | Loss: 0.00003235
Iteration 133/1000 | Loss: 0.00004893
Iteration 134/1000 | Loss: 0.00003906
Iteration 135/1000 | Loss: 0.00003305
Iteration 136/1000 | Loss: 0.00003109
Iteration 137/1000 | Loss: 0.00003088
Iteration 138/1000 | Loss: 0.00003062
Iteration 139/1000 | Loss: 0.00008472
Iteration 140/1000 | Loss: 0.00003129
Iteration 141/1000 | Loss: 0.00003013
Iteration 142/1000 | Loss: 0.00028419
Iteration 143/1000 | Loss: 0.00021792
Iteration 144/1000 | Loss: 0.00014524
Iteration 145/1000 | Loss: 0.00018309
Iteration 146/1000 | Loss: 0.00004098
Iteration 147/1000 | Loss: 0.00004915
Iteration 148/1000 | Loss: 0.00003364
Iteration 149/1000 | Loss: 0.00005496
Iteration 150/1000 | Loss: 0.00006467
Iteration 151/1000 | Loss: 0.00003052
Iteration 152/1000 | Loss: 0.00003757
Iteration 153/1000 | Loss: 0.00002900
Iteration 154/1000 | Loss: 0.00002999
Iteration 155/1000 | Loss: 0.00004185
Iteration 156/1000 | Loss: 0.00004069
Iteration 157/1000 | Loss: 0.00004026
Iteration 158/1000 | Loss: 0.00002805
Iteration 159/1000 | Loss: 0.00010044
Iteration 160/1000 | Loss: 0.00003007
Iteration 161/1000 | Loss: 0.00005268
Iteration 162/1000 | Loss: 0.00007325
Iteration 163/1000 | Loss: 0.00006146
Iteration 164/1000 | Loss: 0.00002611
Iteration 165/1000 | Loss: 0.00005614
Iteration 166/1000 | Loss: 0.00003512
Iteration 167/1000 | Loss: 0.00006219
Iteration 168/1000 | Loss: 0.00002519
Iteration 169/1000 | Loss: 0.00003836
Iteration 170/1000 | Loss: 0.00002502
Iteration 171/1000 | Loss: 0.00003841
Iteration 172/1000 | Loss: 0.00002480
Iteration 173/1000 | Loss: 0.00002480
Iteration 174/1000 | Loss: 0.00002480
Iteration 175/1000 | Loss: 0.00002480
Iteration 176/1000 | Loss: 0.00004838
Iteration 177/1000 | Loss: 0.00002475
Iteration 178/1000 | Loss: 0.00002473
Iteration 179/1000 | Loss: 0.00002473
Iteration 180/1000 | Loss: 0.00002472
Iteration 181/1000 | Loss: 0.00002472
Iteration 182/1000 | Loss: 0.00002471
Iteration 183/1000 | Loss: 0.00003457
Iteration 184/1000 | Loss: 0.00002468
Iteration 185/1000 | Loss: 0.00002468
Iteration 186/1000 | Loss: 0.00002468
Iteration 187/1000 | Loss: 0.00002467
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002466
Iteration 192/1000 | Loss: 0.00002465
Iteration 193/1000 | Loss: 0.00002465
Iteration 194/1000 | Loss: 0.00002465
Iteration 195/1000 | Loss: 0.00002465
Iteration 196/1000 | Loss: 0.00002465
Iteration 197/1000 | Loss: 0.00002464
Iteration 198/1000 | Loss: 0.00002464
Iteration 199/1000 | Loss: 0.00002464
Iteration 200/1000 | Loss: 0.00002464
Iteration 201/1000 | Loss: 0.00002464
Iteration 202/1000 | Loss: 0.00002464
Iteration 203/1000 | Loss: 0.00002464
Iteration 204/1000 | Loss: 0.00002464
Iteration 205/1000 | Loss: 0.00002463
Iteration 206/1000 | Loss: 0.00002463
Iteration 207/1000 | Loss: 0.00002463
Iteration 208/1000 | Loss: 0.00002463
Iteration 209/1000 | Loss: 0.00002463
Iteration 210/1000 | Loss: 0.00002463
Iteration 211/1000 | Loss: 0.00002463
Iteration 212/1000 | Loss: 0.00002462
Iteration 213/1000 | Loss: 0.00002462
Iteration 214/1000 | Loss: 0.00002461
Iteration 215/1000 | Loss: 0.00002461
Iteration 216/1000 | Loss: 0.00004046
Iteration 217/1000 | Loss: 0.00002462
Iteration 218/1000 | Loss: 0.00002462
Iteration 219/1000 | Loss: 0.00002461
Iteration 220/1000 | Loss: 0.00002461
Iteration 221/1000 | Loss: 0.00002461
Iteration 222/1000 | Loss: 0.00002461
Iteration 223/1000 | Loss: 0.00002461
Iteration 224/1000 | Loss: 0.00002461
Iteration 225/1000 | Loss: 0.00002461
Iteration 226/1000 | Loss: 0.00002461
Iteration 227/1000 | Loss: 0.00002461
Iteration 228/1000 | Loss: 0.00002461
Iteration 229/1000 | Loss: 0.00002460
Iteration 230/1000 | Loss: 0.00002460
Iteration 231/1000 | Loss: 0.00002971
Iteration 232/1000 | Loss: 0.00002499
Iteration 233/1000 | Loss: 0.00002456
Iteration 234/1000 | Loss: 0.00002456
Iteration 235/1000 | Loss: 0.00002455
Iteration 236/1000 | Loss: 0.00002455
Iteration 237/1000 | Loss: 0.00002455
Iteration 238/1000 | Loss: 0.00002455
Iteration 239/1000 | Loss: 0.00002455
Iteration 240/1000 | Loss: 0.00002455
Iteration 241/1000 | Loss: 0.00002455
Iteration 242/1000 | Loss: 0.00002455
Iteration 243/1000 | Loss: 0.00002455
Iteration 244/1000 | Loss: 0.00002455
Iteration 245/1000 | Loss: 0.00002455
Iteration 246/1000 | Loss: 0.00002455
Iteration 247/1000 | Loss: 0.00002455
Iteration 248/1000 | Loss: 0.00002455
Iteration 249/1000 | Loss: 0.00002455
Iteration 250/1000 | Loss: 0.00002455
Iteration 251/1000 | Loss: 0.00002455
Iteration 252/1000 | Loss: 0.00002455
Iteration 253/1000 | Loss: 0.00002455
Iteration 254/1000 | Loss: 0.00002455
Iteration 255/1000 | Loss: 0.00002455
Iteration 256/1000 | Loss: 0.00002455
Iteration 257/1000 | Loss: 0.00002455
Iteration 258/1000 | Loss: 0.00002455
Iteration 259/1000 | Loss: 0.00002455
Iteration 260/1000 | Loss: 0.00002455
Iteration 261/1000 | Loss: 0.00002455
Iteration 262/1000 | Loss: 0.00002455
Iteration 263/1000 | Loss: 0.00002455
Iteration 264/1000 | Loss: 0.00002455
Iteration 265/1000 | Loss: 0.00002455
Iteration 266/1000 | Loss: 0.00002455
Iteration 267/1000 | Loss: 0.00002455
Iteration 268/1000 | Loss: 0.00002455
Iteration 269/1000 | Loss: 0.00002455
Iteration 270/1000 | Loss: 0.00002455
Iteration 271/1000 | Loss: 0.00002455
Iteration 272/1000 | Loss: 0.00002455
Iteration 273/1000 | Loss: 0.00002455
Iteration 274/1000 | Loss: 0.00002455
Iteration 275/1000 | Loss: 0.00002455
Iteration 276/1000 | Loss: 0.00002455
Iteration 277/1000 | Loss: 0.00002455
Iteration 278/1000 | Loss: 0.00002455
Iteration 279/1000 | Loss: 0.00002455
Iteration 280/1000 | Loss: 0.00002455
Iteration 281/1000 | Loss: 0.00002455
Iteration 282/1000 | Loss: 0.00002455
Iteration 283/1000 | Loss: 0.00002455
Iteration 284/1000 | Loss: 0.00002455
Iteration 285/1000 | Loss: 0.00002455
Iteration 286/1000 | Loss: 0.00002455
Iteration 287/1000 | Loss: 0.00002455
Iteration 288/1000 | Loss: 0.00002455
Iteration 289/1000 | Loss: 0.00002455
Iteration 290/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [2.4551663955207914e-05, 2.4551663955207914e-05, 2.4551663955207914e-05, 2.4551663955207914e-05, 2.4551663955207914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4551663955207914e-05

Optimization complete. Final v2v error: 3.189638614654541 mm

Highest mean error: 11.198712348937988 mm for frame 119

Lowest mean error: 2.3680379390716553 mm for frame 22

Saving results

Total time: 341.25993752479553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058029
Iteration 2/25 | Loss: 0.00253977
Iteration 3/25 | Loss: 0.00152912
Iteration 4/25 | Loss: 0.00141161
Iteration 5/25 | Loss: 0.00134205
Iteration 6/25 | Loss: 0.00132936
Iteration 7/25 | Loss: 0.00129998
Iteration 8/25 | Loss: 0.00130611
Iteration 9/25 | Loss: 0.00127455
Iteration 10/25 | Loss: 0.00123970
Iteration 11/25 | Loss: 0.00123162
Iteration 12/25 | Loss: 0.00123192
Iteration 13/25 | Loss: 0.00122679
Iteration 14/25 | Loss: 0.00121676
Iteration 15/25 | Loss: 0.00121657
Iteration 16/25 | Loss: 0.00121315
Iteration 17/25 | Loss: 0.00121133
Iteration 18/25 | Loss: 0.00121559
Iteration 19/25 | Loss: 0.00121630
Iteration 20/25 | Loss: 0.00120729
Iteration 21/25 | Loss: 0.00120130
Iteration 22/25 | Loss: 0.00119831
Iteration 23/25 | Loss: 0.00119715
Iteration 24/25 | Loss: 0.00119650
Iteration 25/25 | Loss: 0.00119462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24466848
Iteration 2/25 | Loss: 0.00123997
Iteration 3/25 | Loss: 0.00123993
Iteration 4/25 | Loss: 0.00123993
Iteration 5/25 | Loss: 0.00123993
Iteration 6/25 | Loss: 0.00123993
Iteration 7/25 | Loss: 0.00123993
Iteration 8/25 | Loss: 0.00123993
Iteration 9/25 | Loss: 0.00123993
Iteration 10/25 | Loss: 0.00123993
Iteration 11/25 | Loss: 0.00123993
Iteration 12/25 | Loss: 0.00123993
Iteration 13/25 | Loss: 0.00123993
Iteration 14/25 | Loss: 0.00123993
Iteration 15/25 | Loss: 0.00123993
Iteration 16/25 | Loss: 0.00123993
Iteration 17/25 | Loss: 0.00123993
Iteration 18/25 | Loss: 0.00123993
Iteration 19/25 | Loss: 0.00123993
Iteration 20/25 | Loss: 0.00123993
Iteration 21/25 | Loss: 0.00123993
Iteration 22/25 | Loss: 0.00123993
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012399284169077873, 0.0012399284169077873, 0.0012399284169077873, 0.0012399284169077873, 0.0012399284169077873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012399284169077873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123993
Iteration 2/1000 | Loss: 0.00013280
Iteration 3/1000 | Loss: 0.00007648
Iteration 4/1000 | Loss: 0.00007268
Iteration 5/1000 | Loss: 0.00007091
Iteration 6/1000 | Loss: 0.00006821
Iteration 7/1000 | Loss: 0.00006429
Iteration 8/1000 | Loss: 0.00006246
Iteration 9/1000 | Loss: 0.00043113
Iteration 10/1000 | Loss: 0.00048628
Iteration 11/1000 | Loss: 0.00032900
Iteration 12/1000 | Loss: 0.00039281
Iteration 13/1000 | Loss: 0.00028616
Iteration 14/1000 | Loss: 0.00025797
Iteration 15/1000 | Loss: 0.00005636
Iteration 16/1000 | Loss: 0.00161239
Iteration 17/1000 | Loss: 0.00269226
Iteration 18/1000 | Loss: 0.00013700
Iteration 19/1000 | Loss: 0.00005947
Iteration 20/1000 | Loss: 0.00004599
Iteration 21/1000 | Loss: 0.00003900
Iteration 22/1000 | Loss: 0.00003324
Iteration 23/1000 | Loss: 0.00003028
Iteration 24/1000 | Loss: 0.00002829
Iteration 25/1000 | Loss: 0.00002719
Iteration 26/1000 | Loss: 0.00002610
Iteration 27/1000 | Loss: 0.00002519
Iteration 28/1000 | Loss: 0.00002428
Iteration 29/1000 | Loss: 0.00002359
Iteration 30/1000 | Loss: 0.00002318
Iteration 31/1000 | Loss: 0.00002302
Iteration 32/1000 | Loss: 0.00002298
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002271
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002262
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002257
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002247
Iteration 44/1000 | Loss: 0.00002245
Iteration 45/1000 | Loss: 0.00002245
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00002244
Iteration 51/1000 | Loss: 0.00002244
Iteration 52/1000 | Loss: 0.00002243
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00002243
Iteration 55/1000 | Loss: 0.00002242
Iteration 56/1000 | Loss: 0.00002242
Iteration 57/1000 | Loss: 0.00002242
Iteration 58/1000 | Loss: 0.00002241
Iteration 59/1000 | Loss: 0.00002240
Iteration 60/1000 | Loss: 0.00002240
Iteration 61/1000 | Loss: 0.00002240
Iteration 62/1000 | Loss: 0.00002240
Iteration 63/1000 | Loss: 0.00002240
Iteration 64/1000 | Loss: 0.00002240
Iteration 65/1000 | Loss: 0.00002240
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00002240
Iteration 68/1000 | Loss: 0.00002240
Iteration 69/1000 | Loss: 0.00002240
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002239
Iteration 72/1000 | Loss: 0.00002239
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002238
Iteration 75/1000 | Loss: 0.00002238
Iteration 76/1000 | Loss: 0.00002238
Iteration 77/1000 | Loss: 0.00002238
Iteration 78/1000 | Loss: 0.00002238
Iteration 79/1000 | Loss: 0.00002237
Iteration 80/1000 | Loss: 0.00002237
Iteration 81/1000 | Loss: 0.00002237
Iteration 82/1000 | Loss: 0.00002237
Iteration 83/1000 | Loss: 0.00002237
Iteration 84/1000 | Loss: 0.00002237
Iteration 85/1000 | Loss: 0.00002237
Iteration 86/1000 | Loss: 0.00002236
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002236
Iteration 90/1000 | Loss: 0.00002236
Iteration 91/1000 | Loss: 0.00002236
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002236
Iteration 94/1000 | Loss: 0.00002235
Iteration 95/1000 | Loss: 0.00002235
Iteration 96/1000 | Loss: 0.00002235
Iteration 97/1000 | Loss: 0.00002235
Iteration 98/1000 | Loss: 0.00002235
Iteration 99/1000 | Loss: 0.00002235
Iteration 100/1000 | Loss: 0.00002234
Iteration 101/1000 | Loss: 0.00002234
Iteration 102/1000 | Loss: 0.00002234
Iteration 103/1000 | Loss: 0.00002234
Iteration 104/1000 | Loss: 0.00002234
Iteration 105/1000 | Loss: 0.00002234
Iteration 106/1000 | Loss: 0.00002234
Iteration 107/1000 | Loss: 0.00002234
Iteration 108/1000 | Loss: 0.00002234
Iteration 109/1000 | Loss: 0.00002234
Iteration 110/1000 | Loss: 0.00002234
Iteration 111/1000 | Loss: 0.00002234
Iteration 112/1000 | Loss: 0.00002234
Iteration 113/1000 | Loss: 0.00002234
Iteration 114/1000 | Loss: 0.00002234
Iteration 115/1000 | Loss: 0.00002234
Iteration 116/1000 | Loss: 0.00002234
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.2336984329740517e-05, 2.2336984329740517e-05, 2.2336984329740517e-05, 2.2336984329740517e-05, 2.2336984329740517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2336984329740517e-05

Optimization complete. Final v2v error: 3.7862768173217773 mm

Highest mean error: 5.198249816894531 mm for frame 229

Lowest mean error: 3.299870014190674 mm for frame 3

Saving results

Total time: 114.45074963569641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819468
Iteration 2/25 | Loss: 0.00149786
Iteration 3/25 | Loss: 0.00113348
Iteration 4/25 | Loss: 0.00106891
Iteration 5/25 | Loss: 0.00106870
Iteration 6/25 | Loss: 0.00103631
Iteration 7/25 | Loss: 0.00103039
Iteration 8/25 | Loss: 0.00102959
Iteration 9/25 | Loss: 0.00102915
Iteration 10/25 | Loss: 0.00103268
Iteration 11/25 | Loss: 0.00104384
Iteration 12/25 | Loss: 0.00103485
Iteration 13/25 | Loss: 0.00103116
Iteration 14/25 | Loss: 0.00102970
Iteration 15/25 | Loss: 0.00102803
Iteration 16/25 | Loss: 0.00102486
Iteration 17/25 | Loss: 0.00102812
Iteration 18/25 | Loss: 0.00102769
Iteration 19/25 | Loss: 0.00102642
Iteration 20/25 | Loss: 0.00102536
Iteration 21/25 | Loss: 0.00102509
Iteration 22/25 | Loss: 0.00102882
Iteration 23/25 | Loss: 0.00102798
Iteration 24/25 | Loss: 0.00102469
Iteration 25/25 | Loss: 0.00102279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40438008
Iteration 2/25 | Loss: 0.00056957
Iteration 3/25 | Loss: 0.00056956
Iteration 4/25 | Loss: 0.00056956
Iteration 5/25 | Loss: 0.00056956
Iteration 6/25 | Loss: 0.00056956
Iteration 7/25 | Loss: 0.00056956
Iteration 8/25 | Loss: 0.00056955
Iteration 9/25 | Loss: 0.00056955
Iteration 10/25 | Loss: 0.00056955
Iteration 11/25 | Loss: 0.00056955
Iteration 12/25 | Loss: 0.00056955
Iteration 13/25 | Loss: 0.00056955
Iteration 14/25 | Loss: 0.00056955
Iteration 15/25 | Loss: 0.00056955
Iteration 16/25 | Loss: 0.00056955
Iteration 17/25 | Loss: 0.00056955
Iteration 18/25 | Loss: 0.00056955
Iteration 19/25 | Loss: 0.00056955
Iteration 20/25 | Loss: 0.00056955
Iteration 21/25 | Loss: 0.00056955
Iteration 22/25 | Loss: 0.00056955
Iteration 23/25 | Loss: 0.00056955
Iteration 24/25 | Loss: 0.00056955
Iteration 25/25 | Loss: 0.00056955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056955
Iteration 2/1000 | Loss: 0.00003294
Iteration 3/1000 | Loss: 0.00001997
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001497
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001324
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001264
Iteration 24/1000 | Loss: 0.00001263
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001262
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001261
Iteration 29/1000 | Loss: 0.00001260
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001249
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.2421796782291494e-05, 1.2421796782291494e-05, 1.2421796782291494e-05, 1.2421796782291494e-05, 1.2421796782291494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2421796782291494e-05

Optimization complete. Final v2v error: 2.963759660720825 mm

Highest mean error: 3.70556902885437 mm for frame 12

Lowest mean error: 2.417073965072632 mm for frame 239

Saving results

Total time: 78.34975838661194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616025
Iteration 2/25 | Loss: 0.00132188
Iteration 3/25 | Loss: 0.00112098
Iteration 4/25 | Loss: 0.00110836
Iteration 5/25 | Loss: 0.00110560
Iteration 6/25 | Loss: 0.00110560
Iteration 7/25 | Loss: 0.00110560
Iteration 8/25 | Loss: 0.00110560
Iteration 9/25 | Loss: 0.00110560
Iteration 10/25 | Loss: 0.00110560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011055998038500547, 0.0011055998038500547, 0.0011055998038500547, 0.0011055998038500547, 0.0011055998038500547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011055998038500547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.60931778
Iteration 2/25 | Loss: 0.00069934
Iteration 3/25 | Loss: 0.00069917
Iteration 4/25 | Loss: 0.00069917
Iteration 5/25 | Loss: 0.00069917
Iteration 6/25 | Loss: 0.00069917
Iteration 7/25 | Loss: 0.00069917
Iteration 8/25 | Loss: 0.00069917
Iteration 9/25 | Loss: 0.00069917
Iteration 10/25 | Loss: 0.00069917
Iteration 11/25 | Loss: 0.00069917
Iteration 12/25 | Loss: 0.00069917
Iteration 13/25 | Loss: 0.00069917
Iteration 14/25 | Loss: 0.00069917
Iteration 15/25 | Loss: 0.00069917
Iteration 16/25 | Loss: 0.00069917
Iteration 17/25 | Loss: 0.00069917
Iteration 18/25 | Loss: 0.00069917
Iteration 19/25 | Loss: 0.00069917
Iteration 20/25 | Loss: 0.00069917
Iteration 21/25 | Loss: 0.00069917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006991681875661016, 0.0006991681875661016, 0.0006991681875661016, 0.0006991681875661016, 0.0006991681875661016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006991681875661016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069917
Iteration 2/1000 | Loss: 0.00003638
Iteration 3/1000 | Loss: 0.00001974
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001587
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001499
Iteration 8/1000 | Loss: 0.00001482
Iteration 9/1000 | Loss: 0.00001467
Iteration 10/1000 | Loss: 0.00001459
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001439
Iteration 14/1000 | Loss: 0.00001438
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001434
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001434
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001433
Iteration 24/1000 | Loss: 0.00001433
Iteration 25/1000 | Loss: 0.00001432
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001430
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001426
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001426
Iteration 48/1000 | Loss: 0.00001426
Iteration 49/1000 | Loss: 0.00001426
Iteration 50/1000 | Loss: 0.00001426
Iteration 51/1000 | Loss: 0.00001426
Iteration 52/1000 | Loss: 0.00001426
Iteration 53/1000 | Loss: 0.00001426
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001425
Iteration 78/1000 | Loss: 0.00001425
Iteration 79/1000 | Loss: 0.00001425
Iteration 80/1000 | Loss: 0.00001425
Iteration 81/1000 | Loss: 0.00001425
Iteration 82/1000 | Loss: 0.00001425
Iteration 83/1000 | Loss: 0.00001425
Iteration 84/1000 | Loss: 0.00001425
Iteration 85/1000 | Loss: 0.00001425
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001425
Iteration 90/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.4254879715736024e-05, 1.4254879715736024e-05, 1.4254879715736024e-05, 1.4254879715736024e-05, 1.4254879715736024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4254879715736024e-05

Optimization complete. Final v2v error: 3.1122794151306152 mm

Highest mean error: 3.5378189086914062 mm for frame 169

Lowest mean error: 2.5808396339416504 mm for frame 209

Saving results

Total time: 31.486706972122192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440249
Iteration 2/25 | Loss: 0.00109140
Iteration 3/25 | Loss: 0.00100467
Iteration 4/25 | Loss: 0.00099300
Iteration 5/25 | Loss: 0.00098885
Iteration 6/25 | Loss: 0.00098825
Iteration 7/25 | Loss: 0.00098825
Iteration 8/25 | Loss: 0.00098825
Iteration 9/25 | Loss: 0.00098825
Iteration 10/25 | Loss: 0.00098825
Iteration 11/25 | Loss: 0.00098825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009882451267912984, 0.0009882451267912984, 0.0009882451267912984, 0.0009882451267912984, 0.0009882451267912984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009882451267912984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39988577
Iteration 2/25 | Loss: 0.00079304
Iteration 3/25 | Loss: 0.00079304
Iteration 4/25 | Loss: 0.00079304
Iteration 5/25 | Loss: 0.00079304
Iteration 6/25 | Loss: 0.00079304
Iteration 7/25 | Loss: 0.00079304
Iteration 8/25 | Loss: 0.00079304
Iteration 9/25 | Loss: 0.00079303
Iteration 10/25 | Loss: 0.00079303
Iteration 11/25 | Loss: 0.00079303
Iteration 12/25 | Loss: 0.00079303
Iteration 13/25 | Loss: 0.00079303
Iteration 14/25 | Loss: 0.00079303
Iteration 15/25 | Loss: 0.00079303
Iteration 16/25 | Loss: 0.00079303
Iteration 17/25 | Loss: 0.00079303
Iteration 18/25 | Loss: 0.00079303
Iteration 19/25 | Loss: 0.00079303
Iteration 20/25 | Loss: 0.00079303
Iteration 21/25 | Loss: 0.00079303
Iteration 22/25 | Loss: 0.00079303
Iteration 23/25 | Loss: 0.00079303
Iteration 24/25 | Loss: 0.00079303
Iteration 25/25 | Loss: 0.00079303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079303
Iteration 2/1000 | Loss: 0.00002151
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001506
Iteration 6/1000 | Loss: 0.00001465
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001422
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001380
Iteration 21/1000 | Loss: 0.00001380
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001377
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001374
Iteration 36/1000 | Loss: 0.00001374
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001368
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001366
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001365
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001360
Iteration 83/1000 | Loss: 0.00001360
Iteration 84/1000 | Loss: 0.00001360
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001359
Iteration 87/1000 | Loss: 0.00001359
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001351
Iteration 116/1000 | Loss: 0.00001351
Iteration 117/1000 | Loss: 0.00001351
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001349
Iteration 121/1000 | Loss: 0.00001349
Iteration 122/1000 | Loss: 0.00001349
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001348
Iteration 127/1000 | Loss: 0.00001348
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001347
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00001346
Iteration 133/1000 | Loss: 0.00001346
Iteration 134/1000 | Loss: 0.00001346
Iteration 135/1000 | Loss: 0.00001346
Iteration 136/1000 | Loss: 0.00001346
Iteration 137/1000 | Loss: 0.00001346
Iteration 138/1000 | Loss: 0.00001346
Iteration 139/1000 | Loss: 0.00001346
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001342
Iteration 149/1000 | Loss: 0.00001342
Iteration 150/1000 | Loss: 0.00001342
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001341
Iteration 157/1000 | Loss: 0.00001341
Iteration 158/1000 | Loss: 0.00001341
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001340
Iteration 175/1000 | Loss: 0.00001340
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001339
Iteration 178/1000 | Loss: 0.00001339
Iteration 179/1000 | Loss: 0.00001339
Iteration 180/1000 | Loss: 0.00001339
Iteration 181/1000 | Loss: 0.00001339
Iteration 182/1000 | Loss: 0.00001339
Iteration 183/1000 | Loss: 0.00001339
Iteration 184/1000 | Loss: 0.00001339
Iteration 185/1000 | Loss: 0.00001339
Iteration 186/1000 | Loss: 0.00001339
Iteration 187/1000 | Loss: 0.00001338
Iteration 188/1000 | Loss: 0.00001338
Iteration 189/1000 | Loss: 0.00001338
Iteration 190/1000 | Loss: 0.00001338
Iteration 191/1000 | Loss: 0.00001338
Iteration 192/1000 | Loss: 0.00001338
Iteration 193/1000 | Loss: 0.00001338
Iteration 194/1000 | Loss: 0.00001338
Iteration 195/1000 | Loss: 0.00001338
Iteration 196/1000 | Loss: 0.00001338
Iteration 197/1000 | Loss: 0.00001338
Iteration 198/1000 | Loss: 0.00001338
Iteration 199/1000 | Loss: 0.00001338
Iteration 200/1000 | Loss: 0.00001338
Iteration 201/1000 | Loss: 0.00001338
Iteration 202/1000 | Loss: 0.00001338
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001338
Iteration 205/1000 | Loss: 0.00001338
Iteration 206/1000 | Loss: 0.00001338
Iteration 207/1000 | Loss: 0.00001338
Iteration 208/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.338357378699584e-05, 1.338357378699584e-05, 1.338357378699584e-05, 1.338357378699584e-05, 1.338357378699584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.338357378699584e-05

Optimization complete. Final v2v error: 3.0401806831359863 mm

Highest mean error: 3.63509464263916 mm for frame 130

Lowest mean error: 2.7214128971099854 mm for frame 26

Saving results

Total time: 43.121246099472046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030565
Iteration 2/25 | Loss: 0.00246792
Iteration 3/25 | Loss: 0.00205172
Iteration 4/25 | Loss: 0.00198285
Iteration 5/25 | Loss: 0.00191365
Iteration 6/25 | Loss: 0.00165781
Iteration 7/25 | Loss: 0.00146193
Iteration 8/25 | Loss: 0.00135718
Iteration 9/25 | Loss: 0.00131601
Iteration 10/25 | Loss: 0.00131100
Iteration 11/25 | Loss: 0.00130531
Iteration 12/25 | Loss: 0.00130005
Iteration 13/25 | Loss: 0.00130182
Iteration 14/25 | Loss: 0.00129093
Iteration 15/25 | Loss: 0.00128831
Iteration 16/25 | Loss: 0.00128665
Iteration 17/25 | Loss: 0.00128536
Iteration 18/25 | Loss: 0.00129255
Iteration 19/25 | Loss: 0.00129179
Iteration 20/25 | Loss: 0.00128861
Iteration 21/25 | Loss: 0.00128954
Iteration 22/25 | Loss: 0.00128380
Iteration 23/25 | Loss: 0.00128206
Iteration 24/25 | Loss: 0.00128013
Iteration 25/25 | Loss: 0.00127968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36858654
Iteration 2/25 | Loss: 0.00267783
Iteration 3/25 | Loss: 0.00267783
Iteration 4/25 | Loss: 0.00267783
Iteration 5/25 | Loss: 0.00267783
Iteration 6/25 | Loss: 0.00176544
Iteration 7/25 | Loss: 0.00176543
Iteration 8/25 | Loss: 0.00176543
Iteration 9/25 | Loss: 0.00176543
Iteration 10/25 | Loss: 0.00176543
Iteration 11/25 | Loss: 0.00176543
Iteration 12/25 | Loss: 0.00176543
Iteration 13/25 | Loss: 0.00176543
Iteration 14/25 | Loss: 0.00176543
Iteration 15/25 | Loss: 0.00176543
Iteration 16/25 | Loss: 0.00176543
Iteration 17/25 | Loss: 0.00176543
Iteration 18/25 | Loss: 0.00176543
Iteration 19/25 | Loss: 0.00176543
Iteration 20/25 | Loss: 0.00176543
Iteration 21/25 | Loss: 0.00176543
Iteration 22/25 | Loss: 0.00176543
Iteration 23/25 | Loss: 0.00176543
Iteration 24/25 | Loss: 0.00176543
Iteration 25/25 | Loss: 0.00176543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176543
Iteration 2/1000 | Loss: 0.00099381
Iteration 3/1000 | Loss: 0.00095574
Iteration 4/1000 | Loss: 0.00064944
Iteration 5/1000 | Loss: 0.00028172
Iteration 6/1000 | Loss: 0.00115484
Iteration 7/1000 | Loss: 0.00049304
Iteration 8/1000 | Loss: 0.00034159
Iteration 9/1000 | Loss: 0.00086909
Iteration 10/1000 | Loss: 0.00015971
Iteration 11/1000 | Loss: 0.00026222
Iteration 12/1000 | Loss: 0.00019981
Iteration 13/1000 | Loss: 0.00016849
Iteration 14/1000 | Loss: 0.00027227
Iteration 15/1000 | Loss: 0.00034139
Iteration 16/1000 | Loss: 0.00050838
Iteration 17/1000 | Loss: 0.00032804
Iteration 18/1000 | Loss: 0.00034973
Iteration 19/1000 | Loss: 0.00018090
Iteration 20/1000 | Loss: 0.00015035
Iteration 21/1000 | Loss: 0.00046267
Iteration 22/1000 | Loss: 0.00042903
Iteration 23/1000 | Loss: 0.00020771
Iteration 24/1000 | Loss: 0.00016961
Iteration 25/1000 | Loss: 0.00013696
Iteration 26/1000 | Loss: 0.00019735
Iteration 27/1000 | Loss: 0.02351893
Iteration 28/1000 | Loss: 0.00307088
Iteration 29/1000 | Loss: 0.00078233
Iteration 30/1000 | Loss: 0.00063820
Iteration 31/1000 | Loss: 0.00032806
Iteration 32/1000 | Loss: 0.00078187
Iteration 33/1000 | Loss: 0.00037140
Iteration 34/1000 | Loss: 0.00014154
Iteration 35/1000 | Loss: 0.00042484
Iteration 36/1000 | Loss: 0.00029429
Iteration 37/1000 | Loss: 0.00030908
Iteration 38/1000 | Loss: 0.00022319
Iteration 39/1000 | Loss: 0.00010603
Iteration 40/1000 | Loss: 0.00028246
Iteration 41/1000 | Loss: 0.00030302
Iteration 42/1000 | Loss: 0.00073972
Iteration 43/1000 | Loss: 0.00032690
Iteration 44/1000 | Loss: 0.00031515
Iteration 45/1000 | Loss: 0.00042237
Iteration 46/1000 | Loss: 0.00022307
Iteration 47/1000 | Loss: 0.00012346
Iteration 48/1000 | Loss: 0.00108045
Iteration 49/1000 | Loss: 0.00070196
Iteration 50/1000 | Loss: 0.00021408
Iteration 51/1000 | Loss: 0.00093233
Iteration 52/1000 | Loss: 0.00036651
Iteration 53/1000 | Loss: 0.00023812
Iteration 54/1000 | Loss: 0.00007924
Iteration 55/1000 | Loss: 0.00040962
Iteration 56/1000 | Loss: 0.00033905
Iteration 57/1000 | Loss: 0.00008285
Iteration 58/1000 | Loss: 0.00021853
Iteration 59/1000 | Loss: 0.00005423
Iteration 60/1000 | Loss: 0.00023512
Iteration 61/1000 | Loss: 0.00028242
Iteration 62/1000 | Loss: 0.00017352
Iteration 63/1000 | Loss: 0.00109971
Iteration 64/1000 | Loss: 0.00040597
Iteration 65/1000 | Loss: 0.00014491
Iteration 66/1000 | Loss: 0.00025625
Iteration 67/1000 | Loss: 0.00010249
Iteration 68/1000 | Loss: 0.00003421
Iteration 69/1000 | Loss: 0.00002433
Iteration 70/1000 | Loss: 0.00009508
Iteration 71/1000 | Loss: 0.00005814
Iteration 72/1000 | Loss: 0.00003134
Iteration 73/1000 | Loss: 0.00002440
Iteration 74/1000 | Loss: 0.00016602
Iteration 75/1000 | Loss: 0.00007917
Iteration 76/1000 | Loss: 0.00003024
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00025677
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00002967
Iteration 82/1000 | Loss: 0.00003339
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001734
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00002940
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00002535
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001698
Iteration 117/1000 | Loss: 0.00003518
Iteration 118/1000 | Loss: 0.00001698
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00002446
Iteration 121/1000 | Loss: 0.00002445
Iteration 122/1000 | Loss: 0.00012261
Iteration 123/1000 | Loss: 0.00001742
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00002856
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00002796
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001682
Iteration 158/1000 | Loss: 0.00001682
Iteration 159/1000 | Loss: 0.00002041
Iteration 160/1000 | Loss: 0.00001853
Iteration 161/1000 | Loss: 0.00002160
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001684
Iteration 172/1000 | Loss: 0.00001684
Iteration 173/1000 | Loss: 0.00001684
Iteration 174/1000 | Loss: 0.00001683
Iteration 175/1000 | Loss: 0.00001686
Iteration 176/1000 | Loss: 0.00001695
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001684
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Iteration 183/1000 | Loss: 0.00001682
Iteration 184/1000 | Loss: 0.00001682
Iteration 185/1000 | Loss: 0.00001682
Iteration 186/1000 | Loss: 0.00001682
Iteration 187/1000 | Loss: 0.00001682
Iteration 188/1000 | Loss: 0.00001682
Iteration 189/1000 | Loss: 0.00001682
Iteration 190/1000 | Loss: 0.00001682
Iteration 191/1000 | Loss: 0.00001682
Iteration 192/1000 | Loss: 0.00001682
Iteration 193/1000 | Loss: 0.00001682
Iteration 194/1000 | Loss: 0.00001682
Iteration 195/1000 | Loss: 0.00001682
Iteration 196/1000 | Loss: 0.00001682
Iteration 197/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.6819039956317283e-05, 1.6819039956317283e-05, 1.6819039956317283e-05, 1.6819039956317283e-05, 1.6819039956317283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6819039956317283e-05

Optimization complete. Final v2v error: 2.5744717121124268 mm

Highest mean error: 20.890060424804688 mm for frame 100

Lowest mean error: 2.252115249633789 mm for frame 88

Saving results

Total time: 187.77564072608948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042400
Iteration 2/25 | Loss: 0.00165687
Iteration 3/25 | Loss: 0.00128795
Iteration 4/25 | Loss: 0.00125922
Iteration 5/25 | Loss: 0.00115969
Iteration 6/25 | Loss: 0.00111867
Iteration 7/25 | Loss: 0.00114076
Iteration 8/25 | Loss: 0.00112009
Iteration 9/25 | Loss: 0.00107760
Iteration 10/25 | Loss: 0.00105749
Iteration 11/25 | Loss: 0.00106900
Iteration 12/25 | Loss: 0.00102709
Iteration 13/25 | Loss: 0.00103931
Iteration 14/25 | Loss: 0.00102880
Iteration 15/25 | Loss: 0.00103300
Iteration 16/25 | Loss: 0.00099859
Iteration 17/25 | Loss: 0.00099791
Iteration 18/25 | Loss: 0.00103501
Iteration 19/25 | Loss: 0.00100288
Iteration 20/25 | Loss: 0.00103512
Iteration 21/25 | Loss: 0.00099285
Iteration 22/25 | Loss: 0.00098881
Iteration 23/25 | Loss: 0.00098766
Iteration 24/25 | Loss: 0.00098759
Iteration 25/25 | Loss: 0.00098758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55352795
Iteration 2/25 | Loss: 0.00085374
Iteration 3/25 | Loss: 0.00085374
Iteration 4/25 | Loss: 0.00085373
Iteration 5/25 | Loss: 0.00085373
Iteration 6/25 | Loss: 0.00085373
Iteration 7/25 | Loss: 0.00085373
Iteration 8/25 | Loss: 0.00085373
Iteration 9/25 | Loss: 0.00085373
Iteration 10/25 | Loss: 0.00085373
Iteration 11/25 | Loss: 0.00085373
Iteration 12/25 | Loss: 0.00085373
Iteration 13/25 | Loss: 0.00085373
Iteration 14/25 | Loss: 0.00085373
Iteration 15/25 | Loss: 0.00085373
Iteration 16/25 | Loss: 0.00085373
Iteration 17/25 | Loss: 0.00085373
Iteration 18/25 | Loss: 0.00085373
Iteration 19/25 | Loss: 0.00085373
Iteration 20/25 | Loss: 0.00085373
Iteration 21/25 | Loss: 0.00085373
Iteration 22/25 | Loss: 0.00085373
Iteration 23/25 | Loss: 0.00085373
Iteration 24/25 | Loss: 0.00085373
Iteration 25/25 | Loss: 0.00085373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085373
Iteration 2/1000 | Loss: 0.00004266
Iteration 3/1000 | Loss: 0.00003001
Iteration 4/1000 | Loss: 0.00002544
Iteration 5/1000 | Loss: 0.00002341
Iteration 6/1000 | Loss: 0.00307431
Iteration 7/1000 | Loss: 0.00578627
Iteration 8/1000 | Loss: 0.00197651
Iteration 9/1000 | Loss: 0.00321119
Iteration 10/1000 | Loss: 0.00288149
Iteration 11/1000 | Loss: 0.00096621
Iteration 12/1000 | Loss: 0.00133898
Iteration 13/1000 | Loss: 0.00184910
Iteration 14/1000 | Loss: 0.00266944
Iteration 15/1000 | Loss: 0.00304703
Iteration 16/1000 | Loss: 0.00261285
Iteration 17/1000 | Loss: 0.00146744
Iteration 18/1000 | Loss: 0.00220668
Iteration 19/1000 | Loss: 0.00147057
Iteration 20/1000 | Loss: 0.00016294
Iteration 21/1000 | Loss: 0.00007293
Iteration 22/1000 | Loss: 0.00109945
Iteration 23/1000 | Loss: 0.00165968
Iteration 24/1000 | Loss: 0.00173476
Iteration 25/1000 | Loss: 0.00089129
Iteration 26/1000 | Loss: 0.00117225
Iteration 27/1000 | Loss: 0.00047098
Iteration 28/1000 | Loss: 0.00108168
Iteration 29/1000 | Loss: 0.00115171
Iteration 30/1000 | Loss: 0.00075182
Iteration 31/1000 | Loss: 0.00113906
Iteration 32/1000 | Loss: 0.00129935
Iteration 33/1000 | Loss: 0.00153304
Iteration 34/1000 | Loss: 0.00112906
Iteration 35/1000 | Loss: 0.00015551
Iteration 36/1000 | Loss: 0.00105049
Iteration 37/1000 | Loss: 0.00076427
Iteration 38/1000 | Loss: 0.00003362
Iteration 39/1000 | Loss: 0.00002936
Iteration 40/1000 | Loss: 0.00002562
Iteration 41/1000 | Loss: 0.00015465
Iteration 42/1000 | Loss: 0.00009201
Iteration 43/1000 | Loss: 0.00003561
Iteration 44/1000 | Loss: 0.00002418
Iteration 45/1000 | Loss: 0.00004981
Iteration 46/1000 | Loss: 0.00002104
Iteration 47/1000 | Loss: 0.00002051
Iteration 48/1000 | Loss: 0.00134806
Iteration 49/1000 | Loss: 0.00065018
Iteration 50/1000 | Loss: 0.00005631
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00147176
Iteration 55/1000 | Loss: 0.00136091
Iteration 56/1000 | Loss: 0.00124664
Iteration 57/1000 | Loss: 0.00086139
Iteration 58/1000 | Loss: 0.00098544
Iteration 59/1000 | Loss: 0.00010943
Iteration 60/1000 | Loss: 0.00004327
Iteration 61/1000 | Loss: 0.00204506
Iteration 62/1000 | Loss: 0.00124655
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00056446
Iteration 65/1000 | Loss: 0.00140908
Iteration 66/1000 | Loss: 0.00080161
Iteration 67/1000 | Loss: 0.00127304
Iteration 68/1000 | Loss: 0.00164452
Iteration 69/1000 | Loss: 0.00110804
Iteration 70/1000 | Loss: 0.00238167
Iteration 71/1000 | Loss: 0.00140309
Iteration 72/1000 | Loss: 0.00164909
Iteration 73/1000 | Loss: 0.00131750
Iteration 74/1000 | Loss: 0.00044573
Iteration 75/1000 | Loss: 0.00160854
Iteration 76/1000 | Loss: 0.00010623
Iteration 77/1000 | Loss: 0.00007485
Iteration 78/1000 | Loss: 0.00002495
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001052
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00001125
Iteration 91/1000 | Loss: 0.00000980
Iteration 92/1000 | Loss: 0.00000979
Iteration 93/1000 | Loss: 0.00000979
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000977
Iteration 96/1000 | Loss: 0.00000970
Iteration 97/1000 | Loss: 0.00001809
Iteration 98/1000 | Loss: 0.00000972
Iteration 99/1000 | Loss: 0.00001050
Iteration 100/1000 | Loss: 0.00000943
Iteration 101/1000 | Loss: 0.00000940
Iteration 102/1000 | Loss: 0.00000940
Iteration 103/1000 | Loss: 0.00000940
Iteration 104/1000 | Loss: 0.00000940
Iteration 105/1000 | Loss: 0.00000940
Iteration 106/1000 | Loss: 0.00000940
Iteration 107/1000 | Loss: 0.00000935
Iteration 108/1000 | Loss: 0.00000935
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000932
Iteration 111/1000 | Loss: 0.00000931
Iteration 112/1000 | Loss: 0.00000931
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00000922
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00000986
Iteration 117/1000 | Loss: 0.00001119
Iteration 118/1000 | Loss: 0.00000917
Iteration 119/1000 | Loss: 0.00000917
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000916
Iteration 124/1000 | Loss: 0.00000916
Iteration 125/1000 | Loss: 0.00000916
Iteration 126/1000 | Loss: 0.00000916
Iteration 127/1000 | Loss: 0.00000915
Iteration 128/1000 | Loss: 0.00000914
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000914
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000914
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000912
Iteration 143/1000 | Loss: 0.00001160
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000911
Iteration 146/1000 | Loss: 0.00000911
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000910
Iteration 154/1000 | Loss: 0.00000910
Iteration 155/1000 | Loss: 0.00000910
Iteration 156/1000 | Loss: 0.00000910
Iteration 157/1000 | Loss: 0.00000910
Iteration 158/1000 | Loss: 0.00000910
Iteration 159/1000 | Loss: 0.00000910
Iteration 160/1000 | Loss: 0.00000909
Iteration 161/1000 | Loss: 0.00000909
Iteration 162/1000 | Loss: 0.00000909
Iteration 163/1000 | Loss: 0.00000909
Iteration 164/1000 | Loss: 0.00000909
Iteration 165/1000 | Loss: 0.00000909
Iteration 166/1000 | Loss: 0.00000909
Iteration 167/1000 | Loss: 0.00000909
Iteration 168/1000 | Loss: 0.00000909
Iteration 169/1000 | Loss: 0.00000909
Iteration 170/1000 | Loss: 0.00000909
Iteration 171/1000 | Loss: 0.00000909
Iteration 172/1000 | Loss: 0.00000909
Iteration 173/1000 | Loss: 0.00000909
Iteration 174/1000 | Loss: 0.00000909
Iteration 175/1000 | Loss: 0.00000909
Iteration 176/1000 | Loss: 0.00000909
Iteration 177/1000 | Loss: 0.00000909
Iteration 178/1000 | Loss: 0.00000908
Iteration 179/1000 | Loss: 0.00000908
Iteration 180/1000 | Loss: 0.00000908
Iteration 181/1000 | Loss: 0.00000908
Iteration 182/1000 | Loss: 0.00000908
Iteration 183/1000 | Loss: 0.00000908
Iteration 184/1000 | Loss: 0.00000908
Iteration 185/1000 | Loss: 0.00000908
Iteration 186/1000 | Loss: 0.00000908
Iteration 187/1000 | Loss: 0.00000908
Iteration 188/1000 | Loss: 0.00000908
Iteration 189/1000 | Loss: 0.00000908
Iteration 190/1000 | Loss: 0.00000908
Iteration 191/1000 | Loss: 0.00000908
Iteration 192/1000 | Loss: 0.00000908
Iteration 193/1000 | Loss: 0.00000908
Iteration 194/1000 | Loss: 0.00000908
Iteration 195/1000 | Loss: 0.00000908
Iteration 196/1000 | Loss: 0.00000908
Iteration 197/1000 | Loss: 0.00000908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [9.081831194635015e-06, 9.081831194635015e-06, 9.081831194635015e-06, 9.081831194635015e-06, 9.081831194635015e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.081831194635015e-06

Optimization complete. Final v2v error: 2.5475287437438965 mm

Highest mean error: 4.195899486541748 mm for frame 47

Lowest mean error: 2.2968273162841797 mm for frame 135

Saving results

Total time: 185.74921035766602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034214
Iteration 2/25 | Loss: 0.00265064
Iteration 3/25 | Loss: 0.00181272
Iteration 4/25 | Loss: 0.00163497
Iteration 5/25 | Loss: 0.00153705
Iteration 6/25 | Loss: 0.00137518
Iteration 7/25 | Loss: 0.00126681
Iteration 8/25 | Loss: 0.00122665
Iteration 9/25 | Loss: 0.00118324
Iteration 10/25 | Loss: 0.00116167
Iteration 11/25 | Loss: 0.00115023
Iteration 12/25 | Loss: 0.00114873
Iteration 13/25 | Loss: 0.00113768
Iteration 14/25 | Loss: 0.00113535
Iteration 15/25 | Loss: 0.00113313
Iteration 16/25 | Loss: 0.00112801
Iteration 17/25 | Loss: 0.00112290
Iteration 18/25 | Loss: 0.00112457
Iteration 19/25 | Loss: 0.00112217
Iteration 20/25 | Loss: 0.00112417
Iteration 21/25 | Loss: 0.00111976
Iteration 22/25 | Loss: 0.00112154
Iteration 23/25 | Loss: 0.00111791
Iteration 24/25 | Loss: 0.00111164
Iteration 25/25 | Loss: 0.00111584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40620267
Iteration 2/25 | Loss: 0.00356406
Iteration 3/25 | Loss: 0.00243893
Iteration 4/25 | Loss: 0.00243879
Iteration 5/25 | Loss: 0.00243879
Iteration 6/25 | Loss: 0.00243879
Iteration 7/25 | Loss: 0.00243879
Iteration 8/25 | Loss: 0.00243879
Iteration 9/25 | Loss: 0.00243879
Iteration 10/25 | Loss: 0.00243879
Iteration 11/25 | Loss: 0.00243879
Iteration 12/25 | Loss: 0.00243879
Iteration 13/25 | Loss: 0.00243879
Iteration 14/25 | Loss: 0.00243879
Iteration 15/25 | Loss: 0.00243879
Iteration 16/25 | Loss: 0.00243879
Iteration 17/25 | Loss: 0.00243879
Iteration 18/25 | Loss: 0.00243879
Iteration 19/25 | Loss: 0.00243879
Iteration 20/25 | Loss: 0.00243879
Iteration 21/25 | Loss: 0.00243879
Iteration 22/25 | Loss: 0.00243879
Iteration 23/25 | Loss: 0.00243879
Iteration 24/25 | Loss: 0.00243879
Iteration 25/25 | Loss: 0.00243879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243879
Iteration 2/1000 | Loss: 0.00175629
Iteration 3/1000 | Loss: 0.00020904
Iteration 4/1000 | Loss: 0.00025517
Iteration 5/1000 | Loss: 0.00064334
Iteration 6/1000 | Loss: 0.00030670
Iteration 7/1000 | Loss: 0.00027158
Iteration 8/1000 | Loss: 0.00025407
Iteration 9/1000 | Loss: 0.00068536
Iteration 10/1000 | Loss: 0.00018194
Iteration 11/1000 | Loss: 0.00010217
Iteration 12/1000 | Loss: 0.00017713
Iteration 13/1000 | Loss: 0.00015733
Iteration 14/1000 | Loss: 0.00009989
Iteration 15/1000 | Loss: 0.00023730
Iteration 16/1000 | Loss: 0.00010032
Iteration 17/1000 | Loss: 0.00008157
Iteration 18/1000 | Loss: 0.00024673
Iteration 19/1000 | Loss: 0.00008397
Iteration 20/1000 | Loss: 0.00008074
Iteration 21/1000 | Loss: 0.00050192
Iteration 22/1000 | Loss: 0.00029405
Iteration 23/1000 | Loss: 0.00039056
Iteration 24/1000 | Loss: 0.00009489
Iteration 25/1000 | Loss: 0.00017902
Iteration 26/1000 | Loss: 0.00007548
Iteration 27/1000 | Loss: 0.00011706
Iteration 28/1000 | Loss: 0.00008597
Iteration 29/1000 | Loss: 0.00021279
Iteration 30/1000 | Loss: 0.00050815
Iteration 31/1000 | Loss: 0.00023166
Iteration 32/1000 | Loss: 0.00042686
Iteration 33/1000 | Loss: 0.00042087
Iteration 34/1000 | Loss: 0.00042480
Iteration 35/1000 | Loss: 0.00009268
Iteration 36/1000 | Loss: 0.00008528
Iteration 37/1000 | Loss: 0.00056782
Iteration 38/1000 | Loss: 0.00061106
Iteration 39/1000 | Loss: 0.00050729
Iteration 40/1000 | Loss: 0.00008103
Iteration 41/1000 | Loss: 0.00007801
Iteration 42/1000 | Loss: 0.00006785
Iteration 43/1000 | Loss: 0.00006454
Iteration 44/1000 | Loss: 0.00006319
Iteration 45/1000 | Loss: 0.00010952
Iteration 46/1000 | Loss: 0.00007868
Iteration 47/1000 | Loss: 0.00007838
Iteration 48/1000 | Loss: 0.00018493
Iteration 49/1000 | Loss: 0.00007402
Iteration 50/1000 | Loss: 0.00006971
Iteration 51/1000 | Loss: 0.00006340
Iteration 52/1000 | Loss: 0.00007382
Iteration 53/1000 | Loss: 0.00016474
Iteration 54/1000 | Loss: 0.00012485
Iteration 55/1000 | Loss: 0.00008598
Iteration 56/1000 | Loss: 0.00017207
Iteration 57/1000 | Loss: 0.00010807
Iteration 58/1000 | Loss: 0.00009446
Iteration 59/1000 | Loss: 0.00009425
Iteration 60/1000 | Loss: 0.00018418
Iteration 61/1000 | Loss: 0.00011218
Iteration 62/1000 | Loss: 0.00013169
Iteration 63/1000 | Loss: 0.00013730
Iteration 64/1000 | Loss: 0.00006169
Iteration 65/1000 | Loss: 0.00006085
Iteration 66/1000 | Loss: 0.00007951
Iteration 67/1000 | Loss: 0.00006034
Iteration 68/1000 | Loss: 0.00005973
Iteration 69/1000 | Loss: 0.00015307
Iteration 70/1000 | Loss: 0.00008320
Iteration 71/1000 | Loss: 0.00005928
Iteration 72/1000 | Loss: 0.00017341
Iteration 73/1000 | Loss: 0.00012452
Iteration 74/1000 | Loss: 0.00006447
Iteration 75/1000 | Loss: 0.00006269
Iteration 76/1000 | Loss: 0.00006102
Iteration 77/1000 | Loss: 0.00005982
Iteration 78/1000 | Loss: 0.00019582
Iteration 79/1000 | Loss: 0.00005915
Iteration 80/1000 | Loss: 0.00010665
Iteration 81/1000 | Loss: 0.00005859
Iteration 82/1000 | Loss: 0.00005765
Iteration 83/1000 | Loss: 0.00005663
Iteration 84/1000 | Loss: 0.00005603
Iteration 85/1000 | Loss: 0.00005557
Iteration 86/1000 | Loss: 0.00005518
Iteration 87/1000 | Loss: 0.00005491
Iteration 88/1000 | Loss: 0.00005460
Iteration 89/1000 | Loss: 0.00023666
Iteration 90/1000 | Loss: 0.00034690
Iteration 91/1000 | Loss: 0.00031862
Iteration 92/1000 | Loss: 0.00026168
Iteration 93/1000 | Loss: 0.00013532
Iteration 94/1000 | Loss: 0.00006044
Iteration 95/1000 | Loss: 0.00005452
Iteration 96/1000 | Loss: 0.00004758
Iteration 97/1000 | Loss: 0.00014889
Iteration 98/1000 | Loss: 0.00005636
Iteration 99/1000 | Loss: 0.00004142
Iteration 100/1000 | Loss: 0.00004044
Iteration 101/1000 | Loss: 0.00003954
Iteration 102/1000 | Loss: 0.00003898
Iteration 103/1000 | Loss: 0.00003854
Iteration 104/1000 | Loss: 0.00003834
Iteration 105/1000 | Loss: 0.00003808
Iteration 106/1000 | Loss: 0.00003804
Iteration 107/1000 | Loss: 0.00003797
Iteration 108/1000 | Loss: 0.00003796
Iteration 109/1000 | Loss: 0.00003795
Iteration 110/1000 | Loss: 0.00003792
Iteration 111/1000 | Loss: 0.00003787
Iteration 112/1000 | Loss: 0.00003784
Iteration 113/1000 | Loss: 0.00003784
Iteration 114/1000 | Loss: 0.00003784
Iteration 115/1000 | Loss: 0.00003784
Iteration 116/1000 | Loss: 0.00003783
Iteration 117/1000 | Loss: 0.00003783
Iteration 118/1000 | Loss: 0.00003782
Iteration 119/1000 | Loss: 0.00003782
Iteration 120/1000 | Loss: 0.00003782
Iteration 121/1000 | Loss: 0.00003781
Iteration 122/1000 | Loss: 0.00003781
Iteration 123/1000 | Loss: 0.00003780
Iteration 124/1000 | Loss: 0.00003780
Iteration 125/1000 | Loss: 0.00003780
Iteration 126/1000 | Loss: 0.00003780
Iteration 127/1000 | Loss: 0.00003780
Iteration 128/1000 | Loss: 0.00003780
Iteration 129/1000 | Loss: 0.00003780
Iteration 130/1000 | Loss: 0.00003780
Iteration 131/1000 | Loss: 0.00003780
Iteration 132/1000 | Loss: 0.00003780
Iteration 133/1000 | Loss: 0.00003779
Iteration 134/1000 | Loss: 0.00003779
Iteration 135/1000 | Loss: 0.00003779
Iteration 136/1000 | Loss: 0.00003779
Iteration 137/1000 | Loss: 0.00003778
Iteration 138/1000 | Loss: 0.00003778
Iteration 139/1000 | Loss: 0.00003778
Iteration 140/1000 | Loss: 0.00003778
Iteration 141/1000 | Loss: 0.00003778
Iteration 142/1000 | Loss: 0.00003778
Iteration 143/1000 | Loss: 0.00003778
Iteration 144/1000 | Loss: 0.00003778
Iteration 145/1000 | Loss: 0.00003778
Iteration 146/1000 | Loss: 0.00003778
Iteration 147/1000 | Loss: 0.00003778
Iteration 148/1000 | Loss: 0.00003778
Iteration 149/1000 | Loss: 0.00003778
Iteration 150/1000 | Loss: 0.00003777
Iteration 151/1000 | Loss: 0.00003777
Iteration 152/1000 | Loss: 0.00003777
Iteration 153/1000 | Loss: 0.00003777
Iteration 154/1000 | Loss: 0.00003777
Iteration 155/1000 | Loss: 0.00003777
Iteration 156/1000 | Loss: 0.00003777
Iteration 157/1000 | Loss: 0.00003776
Iteration 158/1000 | Loss: 0.00003776
Iteration 159/1000 | Loss: 0.00003776
Iteration 160/1000 | Loss: 0.00003776
Iteration 161/1000 | Loss: 0.00003776
Iteration 162/1000 | Loss: 0.00003776
Iteration 163/1000 | Loss: 0.00003776
Iteration 164/1000 | Loss: 0.00003776
Iteration 165/1000 | Loss: 0.00003776
Iteration 166/1000 | Loss: 0.00003776
Iteration 167/1000 | Loss: 0.00003775
Iteration 168/1000 | Loss: 0.00003775
Iteration 169/1000 | Loss: 0.00003775
Iteration 170/1000 | Loss: 0.00003775
Iteration 171/1000 | Loss: 0.00003775
Iteration 172/1000 | Loss: 0.00003775
Iteration 173/1000 | Loss: 0.00003775
Iteration 174/1000 | Loss: 0.00003775
Iteration 175/1000 | Loss: 0.00003775
Iteration 176/1000 | Loss: 0.00003775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.7752157368231565e-05, 3.7752157368231565e-05, 3.7752157368231565e-05, 3.7752157368231565e-05, 3.7752157368231565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7752157368231565e-05

Optimization complete. Final v2v error: 3.341707706451416 mm

Highest mean error: 11.40433406829834 mm for frame 138

Lowest mean error: 2.471226215362549 mm for frame 62

Saving results

Total time: 232.2246696949005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046879
Iteration 2/25 | Loss: 0.01046879
Iteration 3/25 | Loss: 0.01046878
Iteration 4/25 | Loss: 0.01046878
Iteration 5/25 | Loss: 0.01046878
Iteration 6/25 | Loss: 0.01046878
Iteration 7/25 | Loss: 0.01046878
Iteration 8/25 | Loss: 0.01046878
Iteration 9/25 | Loss: 0.01046878
Iteration 10/25 | Loss: 0.01046877
Iteration 11/25 | Loss: 0.01046877
Iteration 12/25 | Loss: 0.01046877
Iteration 13/25 | Loss: 0.01046877
Iteration 14/25 | Loss: 0.01046877
Iteration 15/25 | Loss: 0.01046877
Iteration 16/25 | Loss: 0.01046877
Iteration 17/25 | Loss: 0.01046877
Iteration 18/25 | Loss: 0.01046877
Iteration 19/25 | Loss: 0.01046876
Iteration 20/25 | Loss: 0.01046876
Iteration 21/25 | Loss: 0.01046876
Iteration 22/25 | Loss: 0.01046876
Iteration 23/25 | Loss: 0.01046876
Iteration 24/25 | Loss: 0.01046876
Iteration 25/25 | Loss: 0.01046876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97188950
Iteration 2/25 | Loss: 0.18158406
Iteration 3/25 | Loss: 0.17352541
Iteration 4/25 | Loss: 0.17249425
Iteration 5/25 | Loss: 0.17249422
Iteration 6/25 | Loss: 0.17249422
Iteration 7/25 | Loss: 0.17249422
Iteration 8/25 | Loss: 0.17249422
Iteration 9/25 | Loss: 0.17249422
Iteration 10/25 | Loss: 0.17249422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.17249421775341034, 0.17249421775341034, 0.17249421775341034, 0.17249421775341034, 0.17249421775341034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17249421775341034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17249422
Iteration 2/1000 | Loss: 0.00543015
Iteration 3/1000 | Loss: 0.00738283
Iteration 4/1000 | Loss: 0.00258202
Iteration 5/1000 | Loss: 0.00536229
Iteration 6/1000 | Loss: 0.00042803
Iteration 7/1000 | Loss: 0.00091222
Iteration 8/1000 | Loss: 0.00709908
Iteration 9/1000 | Loss: 0.00031807
Iteration 10/1000 | Loss: 0.00085302
Iteration 11/1000 | Loss: 0.00015757
Iteration 12/1000 | Loss: 0.00017283
Iteration 13/1000 | Loss: 0.00008023
Iteration 14/1000 | Loss: 0.00029894
Iteration 15/1000 | Loss: 0.00009695
Iteration 16/1000 | Loss: 0.00014482
Iteration 17/1000 | Loss: 0.00009793
Iteration 18/1000 | Loss: 0.00005397
Iteration 19/1000 | Loss: 0.00009135
Iteration 20/1000 | Loss: 0.00004299
Iteration 21/1000 | Loss: 0.00021008
Iteration 22/1000 | Loss: 0.00013435
Iteration 23/1000 | Loss: 0.00003314
Iteration 24/1000 | Loss: 0.00019102
Iteration 25/1000 | Loss: 0.00038521
Iteration 26/1000 | Loss: 0.00013788
Iteration 27/1000 | Loss: 0.00004946
Iteration 28/1000 | Loss: 0.00002572
Iteration 29/1000 | Loss: 0.00017631
Iteration 30/1000 | Loss: 0.00002491
Iteration 31/1000 | Loss: 0.00006278
Iteration 32/1000 | Loss: 0.00015731
Iteration 33/1000 | Loss: 0.00004691
Iteration 34/1000 | Loss: 0.00002347
Iteration 35/1000 | Loss: 0.00021307
Iteration 36/1000 | Loss: 0.00012190
Iteration 37/1000 | Loss: 0.00004476
Iteration 38/1000 | Loss: 0.00008690
Iteration 39/1000 | Loss: 0.00013235
Iteration 40/1000 | Loss: 0.00002228
Iteration 41/1000 | Loss: 0.00002193
Iteration 42/1000 | Loss: 0.00004343
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00003893
Iteration 45/1000 | Loss: 0.00002095
Iteration 46/1000 | Loss: 0.00016806
Iteration 47/1000 | Loss: 0.00007853
Iteration 48/1000 | Loss: 0.00003002
Iteration 49/1000 | Loss: 0.00002183
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00009054
Iteration 52/1000 | Loss: 0.00008557
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00020516
Iteration 56/1000 | Loss: 0.00016511
Iteration 57/1000 | Loss: 0.00155383
Iteration 58/1000 | Loss: 0.00008204
Iteration 59/1000 | Loss: 0.00005399
Iteration 60/1000 | Loss: 0.00001997
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00003309
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00003726
Iteration 66/1000 | Loss: 0.00002154
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001889
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001879
Iteration 81/1000 | Loss: 0.00029044
Iteration 82/1000 | Loss: 0.00029044
Iteration 83/1000 | Loss: 0.00054825
Iteration 84/1000 | Loss: 0.00003275
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00009283
Iteration 88/1000 | Loss: 0.00002795
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00031954
Iteration 91/1000 | Loss: 0.00024032
Iteration 92/1000 | Loss: 0.00006049
Iteration 93/1000 | Loss: 0.00003594
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001847
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001845
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001841
Iteration 108/1000 | Loss: 0.00001841
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00002993
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001835
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.835368493630085e-05, 1.835368493630085e-05, 1.835368493630085e-05, 1.835368493630085e-05, 1.835368493630085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.835368493630085e-05

Optimization complete. Final v2v error: 3.5305986404418945 mm

Highest mean error: 5.143275260925293 mm for frame 221

Lowest mean error: 2.7040650844573975 mm for frame 38

Saving results

Total time: 144.68315815925598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029147
Iteration 2/25 | Loss: 0.00171520
Iteration 3/25 | Loss: 0.00131936
Iteration 4/25 | Loss: 0.00122119
Iteration 5/25 | Loss: 0.00122314
Iteration 6/25 | Loss: 0.00126939
Iteration 7/25 | Loss: 0.00121741
Iteration 8/25 | Loss: 0.00114558
Iteration 9/25 | Loss: 0.00116524
Iteration 10/25 | Loss: 0.00108872
Iteration 11/25 | Loss: 0.00106645
Iteration 12/25 | Loss: 0.00104663
Iteration 13/25 | Loss: 0.00104583
Iteration 14/25 | Loss: 0.00104786
Iteration 15/25 | Loss: 0.00104079
Iteration 16/25 | Loss: 0.00104854
Iteration 17/25 | Loss: 0.00104325
Iteration 18/25 | Loss: 0.00103297
Iteration 19/25 | Loss: 0.00102849
Iteration 20/25 | Loss: 0.00103320
Iteration 21/25 | Loss: 0.00103182
Iteration 22/25 | Loss: 0.00103358
Iteration 23/25 | Loss: 0.00103070
Iteration 24/25 | Loss: 0.00103657
Iteration 25/25 | Loss: 0.00103597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43600464
Iteration 2/25 | Loss: 0.00132139
Iteration 3/25 | Loss: 0.00132138
Iteration 4/25 | Loss: 0.00132138
Iteration 5/25 | Loss: 0.00132138
Iteration 6/25 | Loss: 0.00132138
Iteration 7/25 | Loss: 0.00132138
Iteration 8/25 | Loss: 0.00132138
Iteration 9/25 | Loss: 0.00132138
Iteration 10/25 | Loss: 0.00132138
Iteration 11/25 | Loss: 0.00132138
Iteration 12/25 | Loss: 0.00132138
Iteration 13/25 | Loss: 0.00132138
Iteration 14/25 | Loss: 0.00132138
Iteration 15/25 | Loss: 0.00132138
Iteration 16/25 | Loss: 0.00132138
Iteration 17/25 | Loss: 0.00132138
Iteration 18/25 | Loss: 0.00132138
Iteration 19/25 | Loss: 0.00132138
Iteration 20/25 | Loss: 0.00132138
Iteration 21/25 | Loss: 0.00132138
Iteration 22/25 | Loss: 0.00132138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013213811907917261, 0.0013213811907917261, 0.0013213811907917261, 0.0013213811907917261, 0.0013213811907917261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013213811907917261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132138
Iteration 2/1000 | Loss: 0.00017854
Iteration 3/1000 | Loss: 0.00037027
Iteration 4/1000 | Loss: 0.00023814
Iteration 5/1000 | Loss: 0.00030753
Iteration 6/1000 | Loss: 0.00045437
Iteration 7/1000 | Loss: 0.00041723
Iteration 8/1000 | Loss: 0.00074331
Iteration 9/1000 | Loss: 0.00046420
Iteration 10/1000 | Loss: 0.00018069
Iteration 11/1000 | Loss: 0.00007399
Iteration 12/1000 | Loss: 0.00034116
Iteration 13/1000 | Loss: 0.00036195
Iteration 14/1000 | Loss: 0.00029592
Iteration 15/1000 | Loss: 0.00047249
Iteration 16/1000 | Loss: 0.00024389
Iteration 17/1000 | Loss: 0.00031130
Iteration 18/1000 | Loss: 0.00024612
Iteration 19/1000 | Loss: 0.00005407
Iteration 20/1000 | Loss: 0.00004582
Iteration 21/1000 | Loss: 0.00062736
Iteration 22/1000 | Loss: 0.00078555
Iteration 23/1000 | Loss: 0.00049515
Iteration 24/1000 | Loss: 0.00119708
Iteration 25/1000 | Loss: 0.00061838
Iteration 26/1000 | Loss: 0.00003840
Iteration 27/1000 | Loss: 0.00025461
Iteration 28/1000 | Loss: 0.00041072
Iteration 29/1000 | Loss: 0.00013722
Iteration 30/1000 | Loss: 0.00040094
Iteration 31/1000 | Loss: 0.00015132
Iteration 32/1000 | Loss: 0.00014621
Iteration 33/1000 | Loss: 0.00031368
Iteration 34/1000 | Loss: 0.00033027
Iteration 35/1000 | Loss: 0.00017367
Iteration 36/1000 | Loss: 0.00026650
Iteration 37/1000 | Loss: 0.00015357
Iteration 38/1000 | Loss: 0.00035452
Iteration 39/1000 | Loss: 0.00022179
Iteration 40/1000 | Loss: 0.00020821
Iteration 41/1000 | Loss: 0.00010185
Iteration 42/1000 | Loss: 0.00017732
Iteration 43/1000 | Loss: 0.00008980
Iteration 44/1000 | Loss: 0.00032939
Iteration 45/1000 | Loss: 0.00010037
Iteration 46/1000 | Loss: 0.00028058
Iteration 47/1000 | Loss: 0.00043502
Iteration 48/1000 | Loss: 0.00027537
Iteration 49/1000 | Loss: 0.00026277
Iteration 50/1000 | Loss: 0.00023946
Iteration 51/1000 | Loss: 0.00044858
Iteration 52/1000 | Loss: 0.00034256
Iteration 53/1000 | Loss: 0.00013996
Iteration 54/1000 | Loss: 0.00003384
Iteration 55/1000 | Loss: 0.00002955
Iteration 56/1000 | Loss: 0.00002784
Iteration 57/1000 | Loss: 0.00002376
Iteration 58/1000 | Loss: 0.00025890
Iteration 59/1000 | Loss: 0.00019821
Iteration 60/1000 | Loss: 0.00002190
Iteration 61/1000 | Loss: 0.00002034
Iteration 62/1000 | Loss: 0.00022965
Iteration 63/1000 | Loss: 0.00002780
Iteration 64/1000 | Loss: 0.00002356
Iteration 65/1000 | Loss: 0.00002125
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00022885
Iteration 68/1000 | Loss: 0.00002788
Iteration 69/1000 | Loss: 0.00002273
Iteration 70/1000 | Loss: 0.00002103
Iteration 71/1000 | Loss: 0.00018947
Iteration 72/1000 | Loss: 0.00065448
Iteration 73/1000 | Loss: 0.00038713
Iteration 74/1000 | Loss: 0.00006025
Iteration 75/1000 | Loss: 0.00012021
Iteration 76/1000 | Loss: 0.00002534
Iteration 77/1000 | Loss: 0.00002275
Iteration 78/1000 | Loss: 0.00005779
Iteration 79/1000 | Loss: 0.00013529
Iteration 80/1000 | Loss: 0.00009509
Iteration 81/1000 | Loss: 0.00002555
Iteration 82/1000 | Loss: 0.00003198
Iteration 83/1000 | Loss: 0.00002363
Iteration 84/1000 | Loss: 0.00014722
Iteration 85/1000 | Loss: 0.00011568
Iteration 86/1000 | Loss: 0.00019393
Iteration 87/1000 | Loss: 0.00012289
Iteration 88/1000 | Loss: 0.00007374
Iteration 89/1000 | Loss: 0.00004604
Iteration 90/1000 | Loss: 0.00004846
Iteration 91/1000 | Loss: 0.00005107
Iteration 92/1000 | Loss: 0.00004452
Iteration 93/1000 | Loss: 0.00002293
Iteration 94/1000 | Loss: 0.00019016
Iteration 95/1000 | Loss: 0.00047252
Iteration 96/1000 | Loss: 0.00014298
Iteration 97/1000 | Loss: 0.00016359
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00017530
Iteration 100/1000 | Loss: 0.00014549
Iteration 101/1000 | Loss: 0.00002884
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001480
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001224
Iteration 108/1000 | Loss: 0.00001135
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001050
Iteration 111/1000 | Loss: 0.00001028
Iteration 112/1000 | Loss: 0.00001028
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001026
Iteration 115/1000 | Loss: 0.00001026
Iteration 116/1000 | Loss: 0.00001025
Iteration 117/1000 | Loss: 0.00001025
Iteration 118/1000 | Loss: 0.00001024
Iteration 119/1000 | Loss: 0.00001024
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001022
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001021
Iteration 124/1000 | Loss: 0.00001021
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001018
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001017
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001015
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001013
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001012
Iteration 155/1000 | Loss: 0.00001012
Iteration 156/1000 | Loss: 0.00001012
Iteration 157/1000 | Loss: 0.00001011
Iteration 158/1000 | Loss: 0.00001011
Iteration 159/1000 | Loss: 0.00001011
Iteration 160/1000 | Loss: 0.00001010
Iteration 161/1000 | Loss: 0.00001010
Iteration 162/1000 | Loss: 0.00001010
Iteration 163/1000 | Loss: 0.00001010
Iteration 164/1000 | Loss: 0.00001010
Iteration 165/1000 | Loss: 0.00001009
Iteration 166/1000 | Loss: 0.00001009
Iteration 167/1000 | Loss: 0.00001009
Iteration 168/1000 | Loss: 0.00001009
Iteration 169/1000 | Loss: 0.00001009
Iteration 170/1000 | Loss: 0.00001009
Iteration 171/1000 | Loss: 0.00001009
Iteration 172/1000 | Loss: 0.00001008
Iteration 173/1000 | Loss: 0.00001008
Iteration 174/1000 | Loss: 0.00001008
Iteration 175/1000 | Loss: 0.00001007
Iteration 176/1000 | Loss: 0.00001007
Iteration 177/1000 | Loss: 0.00001007
Iteration 178/1000 | Loss: 0.00001007
Iteration 179/1000 | Loss: 0.00001006
Iteration 180/1000 | Loss: 0.00001006
Iteration 181/1000 | Loss: 0.00001006
Iteration 182/1000 | Loss: 0.00001005
Iteration 183/1000 | Loss: 0.00001005
Iteration 184/1000 | Loss: 0.00001005
Iteration 185/1000 | Loss: 0.00001005
Iteration 186/1000 | Loss: 0.00001005
Iteration 187/1000 | Loss: 0.00001005
Iteration 188/1000 | Loss: 0.00001004
Iteration 189/1000 | Loss: 0.00001004
Iteration 190/1000 | Loss: 0.00001004
Iteration 191/1000 | Loss: 0.00001004
Iteration 192/1000 | Loss: 0.00001004
Iteration 193/1000 | Loss: 0.00001004
Iteration 194/1000 | Loss: 0.00001004
Iteration 195/1000 | Loss: 0.00001004
Iteration 196/1000 | Loss: 0.00001004
Iteration 197/1000 | Loss: 0.00001004
Iteration 198/1000 | Loss: 0.00001004
Iteration 199/1000 | Loss: 0.00001004
Iteration 200/1000 | Loss: 0.00001004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.0042144822364207e-05, 1.0042144822364207e-05, 1.0042144822364207e-05, 1.0042144822364207e-05, 1.0042144822364207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0042144822364207e-05

Optimization complete. Final v2v error: 2.6314010620117188 mm

Highest mean error: 4.482104301452637 mm for frame 84

Lowest mean error: 2.175616502761841 mm for frame 3

Saving results

Total time: 210.43903422355652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058736
Iteration 2/25 | Loss: 0.00146373
Iteration 3/25 | Loss: 0.00111354
Iteration 4/25 | Loss: 0.00106522
Iteration 5/25 | Loss: 0.00105191
Iteration 6/25 | Loss: 0.00105895
Iteration 7/25 | Loss: 0.00105698
Iteration 8/25 | Loss: 0.00105012
Iteration 9/25 | Loss: 0.00104015
Iteration 10/25 | Loss: 0.00103626
Iteration 11/25 | Loss: 0.00103502
Iteration 12/25 | Loss: 0.00103487
Iteration 13/25 | Loss: 0.00103487
Iteration 14/25 | Loss: 0.00103487
Iteration 15/25 | Loss: 0.00103487
Iteration 16/25 | Loss: 0.00103487
Iteration 17/25 | Loss: 0.00103487
Iteration 18/25 | Loss: 0.00103486
Iteration 19/25 | Loss: 0.00103486
Iteration 20/25 | Loss: 0.00103486
Iteration 21/25 | Loss: 0.00103486
Iteration 22/25 | Loss: 0.00103486
Iteration 23/25 | Loss: 0.00103486
Iteration 24/25 | Loss: 0.00103486
Iteration 25/25 | Loss: 0.00103485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43074989
Iteration 2/25 | Loss: 0.00063266
Iteration 3/25 | Loss: 0.00063266
Iteration 4/25 | Loss: 0.00063266
Iteration 5/25 | Loss: 0.00063265
Iteration 6/25 | Loss: 0.00063265
Iteration 7/25 | Loss: 0.00063265
Iteration 8/25 | Loss: 0.00063265
Iteration 9/25 | Loss: 0.00063265
Iteration 10/25 | Loss: 0.00063265
Iteration 11/25 | Loss: 0.00063265
Iteration 12/25 | Loss: 0.00063265
Iteration 13/25 | Loss: 0.00063265
Iteration 14/25 | Loss: 0.00063265
Iteration 15/25 | Loss: 0.00063265
Iteration 16/25 | Loss: 0.00063265
Iteration 17/25 | Loss: 0.00063265
Iteration 18/25 | Loss: 0.00063265
Iteration 19/25 | Loss: 0.00063265
Iteration 20/25 | Loss: 0.00063265
Iteration 21/25 | Loss: 0.00063265
Iteration 22/25 | Loss: 0.00063265
Iteration 23/25 | Loss: 0.00063265
Iteration 24/25 | Loss: 0.00063265
Iteration 25/25 | Loss: 0.00063265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063265
Iteration 2/1000 | Loss: 0.00002254
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001584
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001565
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001547
Iteration 20/1000 | Loss: 0.00001544
Iteration 21/1000 | Loss: 0.00001538
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001529
Iteration 28/1000 | Loss: 0.00001529
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001528
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001521
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001520
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001518
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001516
Iteration 59/1000 | Loss: 0.00001516
Iteration 60/1000 | Loss: 0.00001516
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001516
Iteration 63/1000 | Loss: 0.00001515
Iteration 64/1000 | Loss: 0.00001515
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001515
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001511
Iteration 101/1000 | Loss: 0.00001511
Iteration 102/1000 | Loss: 0.00001511
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001511
Iteration 106/1000 | Loss: 0.00001511
Iteration 107/1000 | Loss: 0.00001511
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00001511
Iteration 110/1000 | Loss: 0.00001511
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001510
Iteration 113/1000 | Loss: 0.00001510
Iteration 114/1000 | Loss: 0.00001510
Iteration 115/1000 | Loss: 0.00001510
Iteration 116/1000 | Loss: 0.00001510
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001510
Iteration 120/1000 | Loss: 0.00001510
Iteration 121/1000 | Loss: 0.00001510
Iteration 122/1000 | Loss: 0.00001510
Iteration 123/1000 | Loss: 0.00001510
Iteration 124/1000 | Loss: 0.00001510
Iteration 125/1000 | Loss: 0.00001510
Iteration 126/1000 | Loss: 0.00001510
Iteration 127/1000 | Loss: 0.00001510
Iteration 128/1000 | Loss: 0.00001510
Iteration 129/1000 | Loss: 0.00001510
Iteration 130/1000 | Loss: 0.00001510
Iteration 131/1000 | Loss: 0.00001510
Iteration 132/1000 | Loss: 0.00001510
Iteration 133/1000 | Loss: 0.00001510
Iteration 134/1000 | Loss: 0.00001510
Iteration 135/1000 | Loss: 0.00001510
Iteration 136/1000 | Loss: 0.00001510
Iteration 137/1000 | Loss: 0.00001510
Iteration 138/1000 | Loss: 0.00001510
Iteration 139/1000 | Loss: 0.00001510
Iteration 140/1000 | Loss: 0.00001510
Iteration 141/1000 | Loss: 0.00001510
Iteration 142/1000 | Loss: 0.00001510
Iteration 143/1000 | Loss: 0.00001510
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5101769349712413e-05, 1.5101769349712413e-05, 1.5101769349712413e-05, 1.5101769349712413e-05, 1.5101769349712413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5101769349712413e-05

Optimization complete. Final v2v error: 3.2521233558654785 mm

Highest mean error: 3.8332393169403076 mm for frame 15

Lowest mean error: 2.8731789588928223 mm for frame 151

Saving results

Total time: 51.204482316970825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796721
Iteration 2/25 | Loss: 0.00112525
Iteration 3/25 | Loss: 0.00097777
Iteration 4/25 | Loss: 0.00096400
Iteration 5/25 | Loss: 0.00096199
Iteration 6/25 | Loss: 0.00096169
Iteration 7/25 | Loss: 0.00096169
Iteration 8/25 | Loss: 0.00096169
Iteration 9/25 | Loss: 0.00096169
Iteration 10/25 | Loss: 0.00096169
Iteration 11/25 | Loss: 0.00096169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009616881143301725, 0.0009616881143301725, 0.0009616881143301725, 0.0009616881143301725, 0.0009616881143301725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009616881143301725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37187207
Iteration 2/25 | Loss: 0.00052328
Iteration 3/25 | Loss: 0.00052326
Iteration 4/25 | Loss: 0.00052326
Iteration 5/25 | Loss: 0.00052326
Iteration 6/25 | Loss: 0.00052326
Iteration 7/25 | Loss: 0.00052326
Iteration 8/25 | Loss: 0.00052326
Iteration 9/25 | Loss: 0.00052326
Iteration 10/25 | Loss: 0.00052326
Iteration 11/25 | Loss: 0.00052326
Iteration 12/25 | Loss: 0.00052326
Iteration 13/25 | Loss: 0.00052326
Iteration 14/25 | Loss: 0.00052326
Iteration 15/25 | Loss: 0.00052326
Iteration 16/25 | Loss: 0.00052326
Iteration 17/25 | Loss: 0.00052326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005232577677816153, 0.0005232577677816153, 0.0005232577677816153, 0.0005232577677816153, 0.0005232577677816153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005232577677816153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052326
Iteration 2/1000 | Loss: 0.00002198
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001349
Iteration 5/1000 | Loss: 0.00001240
Iteration 6/1000 | Loss: 0.00001171
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001097
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001017
Iteration 15/1000 | Loss: 0.00001001
Iteration 16/1000 | Loss: 0.00000993
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000990
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000989
Iteration 24/1000 | Loss: 0.00000989
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000987
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000986
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000986
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000983
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000980
Iteration 42/1000 | Loss: 0.00000980
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000974
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000973
Iteration 56/1000 | Loss: 0.00000973
Iteration 57/1000 | Loss: 0.00000972
Iteration 58/1000 | Loss: 0.00000972
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000972
Iteration 62/1000 | Loss: 0.00000972
Iteration 63/1000 | Loss: 0.00000971
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000969
Iteration 71/1000 | Loss: 0.00000969
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000967
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000966
Iteration 78/1000 | Loss: 0.00000966
Iteration 79/1000 | Loss: 0.00000966
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000965
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000964
Iteration 90/1000 | Loss: 0.00000964
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000964
Iteration 96/1000 | Loss: 0.00000964
Iteration 97/1000 | Loss: 0.00000964
Iteration 98/1000 | Loss: 0.00000964
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000964
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000964
Iteration 108/1000 | Loss: 0.00000964
Iteration 109/1000 | Loss: 0.00000964
Iteration 110/1000 | Loss: 0.00000964
Iteration 111/1000 | Loss: 0.00000964
Iteration 112/1000 | Loss: 0.00000964
Iteration 113/1000 | Loss: 0.00000964
Iteration 114/1000 | Loss: 0.00000964
Iteration 115/1000 | Loss: 0.00000964
Iteration 116/1000 | Loss: 0.00000964
Iteration 117/1000 | Loss: 0.00000964
Iteration 118/1000 | Loss: 0.00000964
Iteration 119/1000 | Loss: 0.00000964
Iteration 120/1000 | Loss: 0.00000964
Iteration 121/1000 | Loss: 0.00000964
Iteration 122/1000 | Loss: 0.00000964
Iteration 123/1000 | Loss: 0.00000964
Iteration 124/1000 | Loss: 0.00000964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [9.643697922001593e-06, 9.643697922001593e-06, 9.643697922001593e-06, 9.643697922001593e-06, 9.643697922001593e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.643697922001593e-06

Optimization complete. Final v2v error: 2.6318976879119873 mm

Highest mean error: 2.9948744773864746 mm for frame 63

Lowest mean error: 2.283210277557373 mm for frame 37

Saving results

Total time: 34.739596366882324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887778
Iteration 2/25 | Loss: 0.00128968
Iteration 3/25 | Loss: 0.00112521
Iteration 4/25 | Loss: 0.00110088
Iteration 5/25 | Loss: 0.00109301
Iteration 6/25 | Loss: 0.00109116
Iteration 7/25 | Loss: 0.00109116
Iteration 8/25 | Loss: 0.00109116
Iteration 9/25 | Loss: 0.00109116
Iteration 10/25 | Loss: 0.00109116
Iteration 11/25 | Loss: 0.00109116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010911630233749747, 0.0010911630233749747, 0.0010911630233749747, 0.0010911630233749747, 0.0010911630233749747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010911630233749747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31988835
Iteration 2/25 | Loss: 0.00073667
Iteration 3/25 | Loss: 0.00073661
Iteration 4/25 | Loss: 0.00073660
Iteration 5/25 | Loss: 0.00073660
Iteration 6/25 | Loss: 0.00073660
Iteration 7/25 | Loss: 0.00073660
Iteration 8/25 | Loss: 0.00073660
Iteration 9/25 | Loss: 0.00073660
Iteration 10/25 | Loss: 0.00073660
Iteration 11/25 | Loss: 0.00073660
Iteration 12/25 | Loss: 0.00073660
Iteration 13/25 | Loss: 0.00073660
Iteration 14/25 | Loss: 0.00073660
Iteration 15/25 | Loss: 0.00073660
Iteration 16/25 | Loss: 0.00073660
Iteration 17/25 | Loss: 0.00073660
Iteration 18/25 | Loss: 0.00073660
Iteration 19/25 | Loss: 0.00073660
Iteration 20/25 | Loss: 0.00073660
Iteration 21/25 | Loss: 0.00073660
Iteration 22/25 | Loss: 0.00073660
Iteration 23/25 | Loss: 0.00073660
Iteration 24/25 | Loss: 0.00073660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007366013014689088, 0.0007366013014689088, 0.0007366013014689088, 0.0007366013014689088, 0.0007366013014689088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007366013014689088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073660
Iteration 2/1000 | Loss: 0.00004640
Iteration 3/1000 | Loss: 0.00003025
Iteration 4/1000 | Loss: 0.00002672
Iteration 5/1000 | Loss: 0.00002553
Iteration 6/1000 | Loss: 0.00002450
Iteration 7/1000 | Loss: 0.00002393
Iteration 8/1000 | Loss: 0.00002363
Iteration 9/1000 | Loss: 0.00002360
Iteration 10/1000 | Loss: 0.00002339
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002292
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002286
Iteration 15/1000 | Loss: 0.00002262
Iteration 16/1000 | Loss: 0.00002251
Iteration 17/1000 | Loss: 0.00002250
Iteration 18/1000 | Loss: 0.00002243
Iteration 19/1000 | Loss: 0.00002236
Iteration 20/1000 | Loss: 0.00002233
Iteration 21/1000 | Loss: 0.00002232
Iteration 22/1000 | Loss: 0.00002232
Iteration 23/1000 | Loss: 0.00002232
Iteration 24/1000 | Loss: 0.00002231
Iteration 25/1000 | Loss: 0.00002231
Iteration 26/1000 | Loss: 0.00002229
Iteration 27/1000 | Loss: 0.00002228
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002227
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002226
Iteration 32/1000 | Loss: 0.00002225
Iteration 33/1000 | Loss: 0.00002224
Iteration 34/1000 | Loss: 0.00002224
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002221
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002219
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002218
Iteration 44/1000 | Loss: 0.00002218
Iteration 45/1000 | Loss: 0.00002217
Iteration 46/1000 | Loss: 0.00002215
Iteration 47/1000 | Loss: 0.00002215
Iteration 48/1000 | Loss: 0.00002215
Iteration 49/1000 | Loss: 0.00002215
Iteration 50/1000 | Loss: 0.00002215
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002214
Iteration 53/1000 | Loss: 0.00002213
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002213
Iteration 59/1000 | Loss: 0.00002213
Iteration 60/1000 | Loss: 0.00002212
Iteration 61/1000 | Loss: 0.00002212
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002211
Iteration 68/1000 | Loss: 0.00002211
Iteration 69/1000 | Loss: 0.00002211
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00002210
Iteration 72/1000 | Loss: 0.00002210
Iteration 73/1000 | Loss: 0.00002210
Iteration 74/1000 | Loss: 0.00002210
Iteration 75/1000 | Loss: 0.00002210
Iteration 76/1000 | Loss: 0.00002210
Iteration 77/1000 | Loss: 0.00002210
Iteration 78/1000 | Loss: 0.00002209
Iteration 79/1000 | Loss: 0.00002209
Iteration 80/1000 | Loss: 0.00002209
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002208
Iteration 83/1000 | Loss: 0.00002208
Iteration 84/1000 | Loss: 0.00002208
Iteration 85/1000 | Loss: 0.00002208
Iteration 86/1000 | Loss: 0.00002207
Iteration 87/1000 | Loss: 0.00002207
Iteration 88/1000 | Loss: 0.00002207
Iteration 89/1000 | Loss: 0.00002207
Iteration 90/1000 | Loss: 0.00002206
Iteration 91/1000 | Loss: 0.00002206
Iteration 92/1000 | Loss: 0.00002206
Iteration 93/1000 | Loss: 0.00002206
Iteration 94/1000 | Loss: 0.00002206
Iteration 95/1000 | Loss: 0.00002206
Iteration 96/1000 | Loss: 0.00002206
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002205
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002204
Iteration 103/1000 | Loss: 0.00002204
Iteration 104/1000 | Loss: 0.00002204
Iteration 105/1000 | Loss: 0.00002204
Iteration 106/1000 | Loss: 0.00002204
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002204
Iteration 110/1000 | Loss: 0.00002204
Iteration 111/1000 | Loss: 0.00002203
Iteration 112/1000 | Loss: 0.00002203
Iteration 113/1000 | Loss: 0.00002203
Iteration 114/1000 | Loss: 0.00002203
Iteration 115/1000 | Loss: 0.00002203
Iteration 116/1000 | Loss: 0.00002203
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002202
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002202
Iteration 122/1000 | Loss: 0.00002202
Iteration 123/1000 | Loss: 0.00002202
Iteration 124/1000 | Loss: 0.00002202
Iteration 125/1000 | Loss: 0.00002202
Iteration 126/1000 | Loss: 0.00002202
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002202
Iteration 130/1000 | Loss: 0.00002202
Iteration 131/1000 | Loss: 0.00002202
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002201
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002201
Iteration 143/1000 | Loss: 0.00002201
Iteration 144/1000 | Loss: 0.00002201
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.200637391069904e-05, 2.200637391069904e-05, 2.200637391069904e-05, 2.200637391069904e-05, 2.200637391069904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.200637391069904e-05

Optimization complete. Final v2v error: 3.858340263366699 mm

Highest mean error: 4.222940444946289 mm for frame 155

Lowest mean error: 3.3128368854522705 mm for frame 29

Saving results

Total time: 44.38525748252869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383644
Iteration 2/25 | Loss: 0.00106945
Iteration 3/25 | Loss: 0.00095426
Iteration 4/25 | Loss: 0.00094276
Iteration 5/25 | Loss: 0.00094100
Iteration 6/25 | Loss: 0.00094100
Iteration 7/25 | Loss: 0.00094100
Iteration 8/25 | Loss: 0.00094100
Iteration 9/25 | Loss: 0.00094100
Iteration 10/25 | Loss: 0.00094100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009409953490830958, 0.0009409953490830958, 0.0009409953490830958, 0.0009409953490830958, 0.0009409953490830958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009409953490830958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37152147
Iteration 2/25 | Loss: 0.00051597
Iteration 3/25 | Loss: 0.00051597
Iteration 4/25 | Loss: 0.00051597
Iteration 5/25 | Loss: 0.00051597
Iteration 6/25 | Loss: 0.00051597
Iteration 7/25 | Loss: 0.00051597
Iteration 8/25 | Loss: 0.00051597
Iteration 9/25 | Loss: 0.00051597
Iteration 10/25 | Loss: 0.00051596
Iteration 11/25 | Loss: 0.00051596
Iteration 12/25 | Loss: 0.00051596
Iteration 13/25 | Loss: 0.00051596
Iteration 14/25 | Loss: 0.00051596
Iteration 15/25 | Loss: 0.00051596
Iteration 16/25 | Loss: 0.00051596
Iteration 17/25 | Loss: 0.00051596
Iteration 18/25 | Loss: 0.00051596
Iteration 19/25 | Loss: 0.00051596
Iteration 20/25 | Loss: 0.00051596
Iteration 21/25 | Loss: 0.00051596
Iteration 22/25 | Loss: 0.00051596
Iteration 23/25 | Loss: 0.00051596
Iteration 24/25 | Loss: 0.00051596
Iteration 25/25 | Loss: 0.00051596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051596
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001350
Iteration 4/1000 | Loss: 0.00001215
Iteration 5/1000 | Loss: 0.00001142
Iteration 6/1000 | Loss: 0.00001076
Iteration 7/1000 | Loss: 0.00001029
Iteration 8/1000 | Loss: 0.00001000
Iteration 9/1000 | Loss: 0.00000973
Iteration 10/1000 | Loss: 0.00000957
Iteration 11/1000 | Loss: 0.00000953
Iteration 12/1000 | Loss: 0.00000942
Iteration 13/1000 | Loss: 0.00000941
Iteration 14/1000 | Loss: 0.00000924
Iteration 15/1000 | Loss: 0.00000924
Iteration 16/1000 | Loss: 0.00000923
Iteration 17/1000 | Loss: 0.00000918
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000918
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000917
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000917
Iteration 26/1000 | Loss: 0.00000917
Iteration 27/1000 | Loss: 0.00000917
Iteration 28/1000 | Loss: 0.00000915
Iteration 29/1000 | Loss: 0.00000915
Iteration 30/1000 | Loss: 0.00000915
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000912
Iteration 33/1000 | Loss: 0.00000912
Iteration 34/1000 | Loss: 0.00000911
Iteration 35/1000 | Loss: 0.00000911
Iteration 36/1000 | Loss: 0.00000910
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000904
Iteration 40/1000 | Loss: 0.00000903
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000902
Iteration 46/1000 | Loss: 0.00000902
Iteration 47/1000 | Loss: 0.00000902
Iteration 48/1000 | Loss: 0.00000902
Iteration 49/1000 | Loss: 0.00000902
Iteration 50/1000 | Loss: 0.00000901
Iteration 51/1000 | Loss: 0.00000901
Iteration 52/1000 | Loss: 0.00000901
Iteration 53/1000 | Loss: 0.00000901
Iteration 54/1000 | Loss: 0.00000901
Iteration 55/1000 | Loss: 0.00000901
Iteration 56/1000 | Loss: 0.00000901
Iteration 57/1000 | Loss: 0.00000901
Iteration 58/1000 | Loss: 0.00000901
Iteration 59/1000 | Loss: 0.00000901
Iteration 60/1000 | Loss: 0.00000900
Iteration 61/1000 | Loss: 0.00000899
Iteration 62/1000 | Loss: 0.00000899
Iteration 63/1000 | Loss: 0.00000899
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000897
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000895
Iteration 70/1000 | Loss: 0.00000894
Iteration 71/1000 | Loss: 0.00000894
Iteration 72/1000 | Loss: 0.00000893
Iteration 73/1000 | Loss: 0.00000893
Iteration 74/1000 | Loss: 0.00000892
Iteration 75/1000 | Loss: 0.00000892
Iteration 76/1000 | Loss: 0.00000892
Iteration 77/1000 | Loss: 0.00000891
Iteration 78/1000 | Loss: 0.00000891
Iteration 79/1000 | Loss: 0.00000890
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000888
Iteration 82/1000 | Loss: 0.00000888
Iteration 83/1000 | Loss: 0.00000888
Iteration 84/1000 | Loss: 0.00000888
Iteration 85/1000 | Loss: 0.00000887
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000884
Iteration 89/1000 | Loss: 0.00000883
Iteration 90/1000 | Loss: 0.00000883
Iteration 91/1000 | Loss: 0.00000883
Iteration 92/1000 | Loss: 0.00000883
Iteration 93/1000 | Loss: 0.00000883
Iteration 94/1000 | Loss: 0.00000882
Iteration 95/1000 | Loss: 0.00000882
Iteration 96/1000 | Loss: 0.00000882
Iteration 97/1000 | Loss: 0.00000881
Iteration 98/1000 | Loss: 0.00000881
Iteration 99/1000 | Loss: 0.00000881
Iteration 100/1000 | Loss: 0.00000881
Iteration 101/1000 | Loss: 0.00000881
Iteration 102/1000 | Loss: 0.00000880
Iteration 103/1000 | Loss: 0.00000880
Iteration 104/1000 | Loss: 0.00000879
Iteration 105/1000 | Loss: 0.00000879
Iteration 106/1000 | Loss: 0.00000879
Iteration 107/1000 | Loss: 0.00000879
Iteration 108/1000 | Loss: 0.00000879
Iteration 109/1000 | Loss: 0.00000879
Iteration 110/1000 | Loss: 0.00000879
Iteration 111/1000 | Loss: 0.00000878
Iteration 112/1000 | Loss: 0.00000878
Iteration 113/1000 | Loss: 0.00000878
Iteration 114/1000 | Loss: 0.00000878
Iteration 115/1000 | Loss: 0.00000877
Iteration 116/1000 | Loss: 0.00000877
Iteration 117/1000 | Loss: 0.00000876
Iteration 118/1000 | Loss: 0.00000876
Iteration 119/1000 | Loss: 0.00000876
Iteration 120/1000 | Loss: 0.00000876
Iteration 121/1000 | Loss: 0.00000876
Iteration 122/1000 | Loss: 0.00000876
Iteration 123/1000 | Loss: 0.00000876
Iteration 124/1000 | Loss: 0.00000876
Iteration 125/1000 | Loss: 0.00000875
Iteration 126/1000 | Loss: 0.00000875
Iteration 127/1000 | Loss: 0.00000875
Iteration 128/1000 | Loss: 0.00000875
Iteration 129/1000 | Loss: 0.00000874
Iteration 130/1000 | Loss: 0.00000874
Iteration 131/1000 | Loss: 0.00000874
Iteration 132/1000 | Loss: 0.00000874
Iteration 133/1000 | Loss: 0.00000874
Iteration 134/1000 | Loss: 0.00000874
Iteration 135/1000 | Loss: 0.00000874
Iteration 136/1000 | Loss: 0.00000874
Iteration 137/1000 | Loss: 0.00000874
Iteration 138/1000 | Loss: 0.00000874
Iteration 139/1000 | Loss: 0.00000874
Iteration 140/1000 | Loss: 0.00000874
Iteration 141/1000 | Loss: 0.00000874
Iteration 142/1000 | Loss: 0.00000874
Iteration 143/1000 | Loss: 0.00000873
Iteration 144/1000 | Loss: 0.00000873
Iteration 145/1000 | Loss: 0.00000873
Iteration 146/1000 | Loss: 0.00000873
Iteration 147/1000 | Loss: 0.00000873
Iteration 148/1000 | Loss: 0.00000873
Iteration 149/1000 | Loss: 0.00000873
Iteration 150/1000 | Loss: 0.00000872
Iteration 151/1000 | Loss: 0.00000872
Iteration 152/1000 | Loss: 0.00000872
Iteration 153/1000 | Loss: 0.00000872
Iteration 154/1000 | Loss: 0.00000872
Iteration 155/1000 | Loss: 0.00000872
Iteration 156/1000 | Loss: 0.00000872
Iteration 157/1000 | Loss: 0.00000871
Iteration 158/1000 | Loss: 0.00000871
Iteration 159/1000 | Loss: 0.00000871
Iteration 160/1000 | Loss: 0.00000871
Iteration 161/1000 | Loss: 0.00000871
Iteration 162/1000 | Loss: 0.00000871
Iteration 163/1000 | Loss: 0.00000871
Iteration 164/1000 | Loss: 0.00000871
Iteration 165/1000 | Loss: 0.00000871
Iteration 166/1000 | Loss: 0.00000871
Iteration 167/1000 | Loss: 0.00000871
Iteration 168/1000 | Loss: 0.00000871
Iteration 169/1000 | Loss: 0.00000871
Iteration 170/1000 | Loss: 0.00000871
Iteration 171/1000 | Loss: 0.00000871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [8.711253030924127e-06, 8.711253030924127e-06, 8.711253030924127e-06, 8.711253030924127e-06, 8.711253030924127e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.711253030924127e-06

Optimization complete. Final v2v error: 2.5250864028930664 mm

Highest mean error: 2.8800299167633057 mm for frame 82

Lowest mean error: 2.3156116008758545 mm for frame 165

Saving results

Total time: 38.72679042816162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781861
Iteration 2/25 | Loss: 0.00130879
Iteration 3/25 | Loss: 0.00106972
Iteration 4/25 | Loss: 0.00103665
Iteration 5/25 | Loss: 0.00103491
Iteration 6/25 | Loss: 0.00102531
Iteration 7/25 | Loss: 0.00101622
Iteration 8/25 | Loss: 0.00101134
Iteration 9/25 | Loss: 0.00101405
Iteration 10/25 | Loss: 0.00101246
Iteration 11/25 | Loss: 0.00101038
Iteration 12/25 | Loss: 0.00101364
Iteration 13/25 | Loss: 0.00101269
Iteration 14/25 | Loss: 0.00101108
Iteration 15/25 | Loss: 0.00101022
Iteration 16/25 | Loss: 0.00101200
Iteration 17/25 | Loss: 0.00101320
Iteration 18/25 | Loss: 0.00101189
Iteration 19/25 | Loss: 0.00101012
Iteration 20/25 | Loss: 0.00100995
Iteration 21/25 | Loss: 0.00100994
Iteration 22/25 | Loss: 0.00100994
Iteration 23/25 | Loss: 0.00100984
Iteration 24/25 | Loss: 0.00101335
Iteration 25/25 | Loss: 0.00100944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90784132
Iteration 2/25 | Loss: 0.00078162
Iteration 3/25 | Loss: 0.00078162
Iteration 4/25 | Loss: 0.00078161
Iteration 5/25 | Loss: 0.00078161
Iteration 6/25 | Loss: 0.00078161
Iteration 7/25 | Loss: 0.00078161
Iteration 8/25 | Loss: 0.00078161
Iteration 9/25 | Loss: 0.00078161
Iteration 10/25 | Loss: 0.00078161
Iteration 11/25 | Loss: 0.00078161
Iteration 12/25 | Loss: 0.00078161
Iteration 13/25 | Loss: 0.00078161
Iteration 14/25 | Loss: 0.00078161
Iteration 15/25 | Loss: 0.00078161
Iteration 16/25 | Loss: 0.00078161
Iteration 17/25 | Loss: 0.00078161
Iteration 18/25 | Loss: 0.00078161
Iteration 19/25 | Loss: 0.00078161
Iteration 20/25 | Loss: 0.00078161
Iteration 21/25 | Loss: 0.00078161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007816111901775002, 0.0007816111901775002, 0.0007816111901775002, 0.0007816111901775002, 0.0007816111901775002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007816111901775002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078161
Iteration 2/1000 | Loss: 0.00002148
Iteration 3/1000 | Loss: 0.00001646
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001307
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001198
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001197
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001197
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001184
Iteration 82/1000 | Loss: 0.00001184
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001184
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.1773126971093006e-05, 1.1773126971093006e-05, 1.1773126971093006e-05, 1.1773126971093006e-05, 1.1773126971093006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1773126971093006e-05

Optimization complete. Final v2v error: 2.880847215652466 mm

Highest mean error: 3.5551350116729736 mm for frame 38

Lowest mean error: 2.5003867149353027 mm for frame 188

Saving results

Total time: 81.90166449546814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792479
Iteration 2/25 | Loss: 0.00108996
Iteration 3/25 | Loss: 0.00097797
Iteration 4/25 | Loss: 0.00096490
Iteration 5/25 | Loss: 0.00096068
Iteration 6/25 | Loss: 0.00095960
Iteration 7/25 | Loss: 0.00095956
Iteration 8/25 | Loss: 0.00095956
Iteration 9/25 | Loss: 0.00095956
Iteration 10/25 | Loss: 0.00095956
Iteration 11/25 | Loss: 0.00095956
Iteration 12/25 | Loss: 0.00095956
Iteration 13/25 | Loss: 0.00095956
Iteration 14/25 | Loss: 0.00095956
Iteration 15/25 | Loss: 0.00095956
Iteration 16/25 | Loss: 0.00095956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009595635929144919, 0.0009595635929144919, 0.0009595635929144919, 0.0009595635929144919, 0.0009595635929144919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009595635929144919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46013761
Iteration 2/25 | Loss: 0.00068911
Iteration 3/25 | Loss: 0.00068911
Iteration 4/25 | Loss: 0.00068911
Iteration 5/25 | Loss: 0.00068911
Iteration 6/25 | Loss: 0.00068911
Iteration 7/25 | Loss: 0.00068911
Iteration 8/25 | Loss: 0.00068911
Iteration 9/25 | Loss: 0.00068911
Iteration 10/25 | Loss: 0.00068911
Iteration 11/25 | Loss: 0.00068911
Iteration 12/25 | Loss: 0.00068911
Iteration 13/25 | Loss: 0.00068911
Iteration 14/25 | Loss: 0.00068911
Iteration 15/25 | Loss: 0.00068911
Iteration 16/25 | Loss: 0.00068911
Iteration 17/25 | Loss: 0.00068911
Iteration 18/25 | Loss: 0.00068911
Iteration 19/25 | Loss: 0.00068911
Iteration 20/25 | Loss: 0.00068911
Iteration 21/25 | Loss: 0.00068911
Iteration 22/25 | Loss: 0.00068911
Iteration 23/25 | Loss: 0.00068911
Iteration 24/25 | Loss: 0.00068911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006891062948852777, 0.0006891062948852777, 0.0006891062948852777, 0.0006891062948852777, 0.0006891062948852777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006891062948852777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068911
Iteration 2/1000 | Loss: 0.00002626
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001214
Iteration 5/1000 | Loss: 0.00001082
Iteration 6/1000 | Loss: 0.00001023
Iteration 7/1000 | Loss: 0.00000989
Iteration 8/1000 | Loss: 0.00000975
Iteration 9/1000 | Loss: 0.00000947
Iteration 10/1000 | Loss: 0.00000942
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000937
Iteration 13/1000 | Loss: 0.00000935
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000934
Iteration 16/1000 | Loss: 0.00000932
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000908
Iteration 20/1000 | Loss: 0.00000908
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000905
Iteration 23/1000 | Loss: 0.00000905
Iteration 24/1000 | Loss: 0.00000904
Iteration 25/1000 | Loss: 0.00000904
Iteration 26/1000 | Loss: 0.00000901
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000900
Iteration 29/1000 | Loss: 0.00000900
Iteration 30/1000 | Loss: 0.00000899
Iteration 31/1000 | Loss: 0.00000899
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000897
Iteration 34/1000 | Loss: 0.00000897
Iteration 35/1000 | Loss: 0.00000897
Iteration 36/1000 | Loss: 0.00000897
Iteration 37/1000 | Loss: 0.00000897
Iteration 38/1000 | Loss: 0.00000896
Iteration 39/1000 | Loss: 0.00000896
Iteration 40/1000 | Loss: 0.00000896
Iteration 41/1000 | Loss: 0.00000896
Iteration 42/1000 | Loss: 0.00000896
Iteration 43/1000 | Loss: 0.00000896
Iteration 44/1000 | Loss: 0.00000896
Iteration 45/1000 | Loss: 0.00000895
Iteration 46/1000 | Loss: 0.00000894
Iteration 47/1000 | Loss: 0.00000894
Iteration 48/1000 | Loss: 0.00000894
Iteration 49/1000 | Loss: 0.00000894
Iteration 50/1000 | Loss: 0.00000894
Iteration 51/1000 | Loss: 0.00000894
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000893
Iteration 54/1000 | Loss: 0.00000893
Iteration 55/1000 | Loss: 0.00000893
Iteration 56/1000 | Loss: 0.00000892
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000891
Iteration 61/1000 | Loss: 0.00000891
Iteration 62/1000 | Loss: 0.00000891
Iteration 63/1000 | Loss: 0.00000891
Iteration 64/1000 | Loss: 0.00000891
Iteration 65/1000 | Loss: 0.00000891
Iteration 66/1000 | Loss: 0.00000891
Iteration 67/1000 | Loss: 0.00000891
Iteration 68/1000 | Loss: 0.00000891
Iteration 69/1000 | Loss: 0.00000890
Iteration 70/1000 | Loss: 0.00000890
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000889
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000887
Iteration 83/1000 | Loss: 0.00000887
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000885
Iteration 91/1000 | Loss: 0.00000884
Iteration 92/1000 | Loss: 0.00000884
Iteration 93/1000 | Loss: 0.00000884
Iteration 94/1000 | Loss: 0.00000884
Iteration 95/1000 | Loss: 0.00000884
Iteration 96/1000 | Loss: 0.00000884
Iteration 97/1000 | Loss: 0.00000884
Iteration 98/1000 | Loss: 0.00000884
Iteration 99/1000 | Loss: 0.00000884
Iteration 100/1000 | Loss: 0.00000883
Iteration 101/1000 | Loss: 0.00000883
Iteration 102/1000 | Loss: 0.00000883
Iteration 103/1000 | Loss: 0.00000883
Iteration 104/1000 | Loss: 0.00000882
Iteration 105/1000 | Loss: 0.00000882
Iteration 106/1000 | Loss: 0.00000882
Iteration 107/1000 | Loss: 0.00000882
Iteration 108/1000 | Loss: 0.00000882
Iteration 109/1000 | Loss: 0.00000881
Iteration 110/1000 | Loss: 0.00000881
Iteration 111/1000 | Loss: 0.00000881
Iteration 112/1000 | Loss: 0.00000881
Iteration 113/1000 | Loss: 0.00000881
Iteration 114/1000 | Loss: 0.00000881
Iteration 115/1000 | Loss: 0.00000880
Iteration 116/1000 | Loss: 0.00000880
Iteration 117/1000 | Loss: 0.00000880
Iteration 118/1000 | Loss: 0.00000880
Iteration 119/1000 | Loss: 0.00000880
Iteration 120/1000 | Loss: 0.00000880
Iteration 121/1000 | Loss: 0.00000880
Iteration 122/1000 | Loss: 0.00000880
Iteration 123/1000 | Loss: 0.00000879
Iteration 124/1000 | Loss: 0.00000879
Iteration 125/1000 | Loss: 0.00000879
Iteration 126/1000 | Loss: 0.00000879
Iteration 127/1000 | Loss: 0.00000879
Iteration 128/1000 | Loss: 0.00000879
Iteration 129/1000 | Loss: 0.00000879
Iteration 130/1000 | Loss: 0.00000879
Iteration 131/1000 | Loss: 0.00000879
Iteration 132/1000 | Loss: 0.00000879
Iteration 133/1000 | Loss: 0.00000879
Iteration 134/1000 | Loss: 0.00000879
Iteration 135/1000 | Loss: 0.00000879
Iteration 136/1000 | Loss: 0.00000879
Iteration 137/1000 | Loss: 0.00000879
Iteration 138/1000 | Loss: 0.00000878
Iteration 139/1000 | Loss: 0.00000878
Iteration 140/1000 | Loss: 0.00000878
Iteration 141/1000 | Loss: 0.00000878
Iteration 142/1000 | Loss: 0.00000878
Iteration 143/1000 | Loss: 0.00000878
Iteration 144/1000 | Loss: 0.00000878
Iteration 145/1000 | Loss: 0.00000878
Iteration 146/1000 | Loss: 0.00000878
Iteration 147/1000 | Loss: 0.00000878
Iteration 148/1000 | Loss: 0.00000878
Iteration 149/1000 | Loss: 0.00000878
Iteration 150/1000 | Loss: 0.00000877
Iteration 151/1000 | Loss: 0.00000877
Iteration 152/1000 | Loss: 0.00000877
Iteration 153/1000 | Loss: 0.00000877
Iteration 154/1000 | Loss: 0.00000876
Iteration 155/1000 | Loss: 0.00000876
Iteration 156/1000 | Loss: 0.00000876
Iteration 157/1000 | Loss: 0.00000876
Iteration 158/1000 | Loss: 0.00000876
Iteration 159/1000 | Loss: 0.00000876
Iteration 160/1000 | Loss: 0.00000876
Iteration 161/1000 | Loss: 0.00000876
Iteration 162/1000 | Loss: 0.00000876
Iteration 163/1000 | Loss: 0.00000875
Iteration 164/1000 | Loss: 0.00000875
Iteration 165/1000 | Loss: 0.00000875
Iteration 166/1000 | Loss: 0.00000875
Iteration 167/1000 | Loss: 0.00000875
Iteration 168/1000 | Loss: 0.00000875
Iteration 169/1000 | Loss: 0.00000875
Iteration 170/1000 | Loss: 0.00000875
Iteration 171/1000 | Loss: 0.00000874
Iteration 172/1000 | Loss: 0.00000874
Iteration 173/1000 | Loss: 0.00000874
Iteration 174/1000 | Loss: 0.00000874
Iteration 175/1000 | Loss: 0.00000874
Iteration 176/1000 | Loss: 0.00000874
Iteration 177/1000 | Loss: 0.00000874
Iteration 178/1000 | Loss: 0.00000874
Iteration 179/1000 | Loss: 0.00000874
Iteration 180/1000 | Loss: 0.00000873
Iteration 181/1000 | Loss: 0.00000873
Iteration 182/1000 | Loss: 0.00000873
Iteration 183/1000 | Loss: 0.00000873
Iteration 184/1000 | Loss: 0.00000873
Iteration 185/1000 | Loss: 0.00000873
Iteration 186/1000 | Loss: 0.00000873
Iteration 187/1000 | Loss: 0.00000873
Iteration 188/1000 | Loss: 0.00000873
Iteration 189/1000 | Loss: 0.00000872
Iteration 190/1000 | Loss: 0.00000872
Iteration 191/1000 | Loss: 0.00000872
Iteration 192/1000 | Loss: 0.00000872
Iteration 193/1000 | Loss: 0.00000872
Iteration 194/1000 | Loss: 0.00000872
Iteration 195/1000 | Loss: 0.00000872
Iteration 196/1000 | Loss: 0.00000872
Iteration 197/1000 | Loss: 0.00000872
Iteration 198/1000 | Loss: 0.00000872
Iteration 199/1000 | Loss: 0.00000871
Iteration 200/1000 | Loss: 0.00000871
Iteration 201/1000 | Loss: 0.00000871
Iteration 202/1000 | Loss: 0.00000871
Iteration 203/1000 | Loss: 0.00000871
Iteration 204/1000 | Loss: 0.00000871
Iteration 205/1000 | Loss: 0.00000871
Iteration 206/1000 | Loss: 0.00000871
Iteration 207/1000 | Loss: 0.00000871
Iteration 208/1000 | Loss: 0.00000871
Iteration 209/1000 | Loss: 0.00000871
Iteration 210/1000 | Loss: 0.00000870
Iteration 211/1000 | Loss: 0.00000870
Iteration 212/1000 | Loss: 0.00000870
Iteration 213/1000 | Loss: 0.00000870
Iteration 214/1000 | Loss: 0.00000870
Iteration 215/1000 | Loss: 0.00000870
Iteration 216/1000 | Loss: 0.00000870
Iteration 217/1000 | Loss: 0.00000870
Iteration 218/1000 | Loss: 0.00000870
Iteration 219/1000 | Loss: 0.00000870
Iteration 220/1000 | Loss: 0.00000870
Iteration 221/1000 | Loss: 0.00000870
Iteration 222/1000 | Loss: 0.00000870
Iteration 223/1000 | Loss: 0.00000870
Iteration 224/1000 | Loss: 0.00000870
Iteration 225/1000 | Loss: 0.00000870
Iteration 226/1000 | Loss: 0.00000870
Iteration 227/1000 | Loss: 0.00000869
Iteration 228/1000 | Loss: 0.00000869
Iteration 229/1000 | Loss: 0.00000869
Iteration 230/1000 | Loss: 0.00000869
Iteration 231/1000 | Loss: 0.00000869
Iteration 232/1000 | Loss: 0.00000869
Iteration 233/1000 | Loss: 0.00000869
Iteration 234/1000 | Loss: 0.00000868
Iteration 235/1000 | Loss: 0.00000868
Iteration 236/1000 | Loss: 0.00000868
Iteration 237/1000 | Loss: 0.00000868
Iteration 238/1000 | Loss: 0.00000868
Iteration 239/1000 | Loss: 0.00000868
Iteration 240/1000 | Loss: 0.00000868
Iteration 241/1000 | Loss: 0.00000868
Iteration 242/1000 | Loss: 0.00000868
Iteration 243/1000 | Loss: 0.00000868
Iteration 244/1000 | Loss: 0.00000868
Iteration 245/1000 | Loss: 0.00000868
Iteration 246/1000 | Loss: 0.00000868
Iteration 247/1000 | Loss: 0.00000868
Iteration 248/1000 | Loss: 0.00000867
Iteration 249/1000 | Loss: 0.00000867
Iteration 250/1000 | Loss: 0.00000867
Iteration 251/1000 | Loss: 0.00000867
Iteration 252/1000 | Loss: 0.00000867
Iteration 253/1000 | Loss: 0.00000867
Iteration 254/1000 | Loss: 0.00000867
Iteration 255/1000 | Loss: 0.00000867
Iteration 256/1000 | Loss: 0.00000867
Iteration 257/1000 | Loss: 0.00000867
Iteration 258/1000 | Loss: 0.00000867
Iteration 259/1000 | Loss: 0.00000867
Iteration 260/1000 | Loss: 0.00000867
Iteration 261/1000 | Loss: 0.00000867
Iteration 262/1000 | Loss: 0.00000867
Iteration 263/1000 | Loss: 0.00000867
Iteration 264/1000 | Loss: 0.00000867
Iteration 265/1000 | Loss: 0.00000867
Iteration 266/1000 | Loss: 0.00000867
Iteration 267/1000 | Loss: 0.00000867
Iteration 268/1000 | Loss: 0.00000867
Iteration 269/1000 | Loss: 0.00000867
Iteration 270/1000 | Loss: 0.00000867
Iteration 271/1000 | Loss: 0.00000867
Iteration 272/1000 | Loss: 0.00000867
Iteration 273/1000 | Loss: 0.00000867
Iteration 274/1000 | Loss: 0.00000867
Iteration 275/1000 | Loss: 0.00000867
Iteration 276/1000 | Loss: 0.00000867
Iteration 277/1000 | Loss: 0.00000867
Iteration 278/1000 | Loss: 0.00000867
Iteration 279/1000 | Loss: 0.00000867
Iteration 280/1000 | Loss: 0.00000867
Iteration 281/1000 | Loss: 0.00000867
Iteration 282/1000 | Loss: 0.00000867
Iteration 283/1000 | Loss: 0.00000867
Iteration 284/1000 | Loss: 0.00000867
Iteration 285/1000 | Loss: 0.00000867
Iteration 286/1000 | Loss: 0.00000867
Iteration 287/1000 | Loss: 0.00000867
Iteration 288/1000 | Loss: 0.00000867
Iteration 289/1000 | Loss: 0.00000867
Iteration 290/1000 | Loss: 0.00000867
Iteration 291/1000 | Loss: 0.00000867
Iteration 292/1000 | Loss: 0.00000867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [8.667718248034362e-06, 8.667718248034362e-06, 8.667718248034362e-06, 8.667718248034362e-06, 8.667718248034362e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.667718248034362e-06

Optimization complete. Final v2v error: 2.4854631423950195 mm

Highest mean error: 3.4623260498046875 mm for frame 60

Lowest mean error: 2.2215359210968018 mm for frame 24

Saving results

Total time: 42.19224286079407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070143
Iteration 2/25 | Loss: 0.00151390
Iteration 3/25 | Loss: 0.00122512
Iteration 4/25 | Loss: 0.00115188
Iteration 5/25 | Loss: 0.00113504
Iteration 6/25 | Loss: 0.00111806
Iteration 7/25 | Loss: 0.00110080
Iteration 8/25 | Loss: 0.00109684
Iteration 9/25 | Loss: 0.00109502
Iteration 10/25 | Loss: 0.00109423
Iteration 11/25 | Loss: 0.00109401
Iteration 12/25 | Loss: 0.00109394
Iteration 13/25 | Loss: 0.00109394
Iteration 14/25 | Loss: 0.00109393
Iteration 15/25 | Loss: 0.00109393
Iteration 16/25 | Loss: 0.00109393
Iteration 17/25 | Loss: 0.00109393
Iteration 18/25 | Loss: 0.00109393
Iteration 19/25 | Loss: 0.00109393
Iteration 20/25 | Loss: 0.00109393
Iteration 21/25 | Loss: 0.00109393
Iteration 22/25 | Loss: 0.00109393
Iteration 23/25 | Loss: 0.00109393
Iteration 24/25 | Loss: 0.00109393
Iteration 25/25 | Loss: 0.00109393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.07298279
Iteration 2/25 | Loss: 0.00096216
Iteration 3/25 | Loss: 0.00096195
Iteration 4/25 | Loss: 0.00096195
Iteration 5/25 | Loss: 0.00096195
Iteration 6/25 | Loss: 0.00096194
Iteration 7/25 | Loss: 0.00096194
Iteration 8/25 | Loss: 0.00096194
Iteration 9/25 | Loss: 0.00096194
Iteration 10/25 | Loss: 0.00096194
Iteration 11/25 | Loss: 0.00096194
Iteration 12/25 | Loss: 0.00096194
Iteration 13/25 | Loss: 0.00096194
Iteration 14/25 | Loss: 0.00096194
Iteration 15/25 | Loss: 0.00096194
Iteration 16/25 | Loss: 0.00096194
Iteration 17/25 | Loss: 0.00096194
Iteration 18/25 | Loss: 0.00096194
Iteration 19/25 | Loss: 0.00096194
Iteration 20/25 | Loss: 0.00096194
Iteration 21/25 | Loss: 0.00096194
Iteration 22/25 | Loss: 0.00096194
Iteration 23/25 | Loss: 0.00096194
Iteration 24/25 | Loss: 0.00096194
Iteration 25/25 | Loss: 0.00096194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096194
Iteration 2/1000 | Loss: 0.00010086
Iteration 3/1000 | Loss: 0.00006554
Iteration 4/1000 | Loss: 0.00009661
Iteration 5/1000 | Loss: 0.00004347
Iteration 6/1000 | Loss: 0.00004174
Iteration 7/1000 | Loss: 0.00009553
Iteration 8/1000 | Loss: 0.00003875
Iteration 9/1000 | Loss: 0.00003776
Iteration 10/1000 | Loss: 0.00003708
Iteration 11/1000 | Loss: 0.00012633
Iteration 12/1000 | Loss: 0.00004457
Iteration 13/1000 | Loss: 0.00003614
Iteration 14/1000 | Loss: 0.00003578
Iteration 15/1000 | Loss: 0.00003550
Iteration 16/1000 | Loss: 0.00006367
Iteration 17/1000 | Loss: 0.00003538
Iteration 18/1000 | Loss: 0.00003505
Iteration 19/1000 | Loss: 0.00003484
Iteration 20/1000 | Loss: 0.00003481
Iteration 21/1000 | Loss: 0.00003473
Iteration 22/1000 | Loss: 0.00009848
Iteration 23/1000 | Loss: 0.00004619
Iteration 24/1000 | Loss: 0.00003454
Iteration 25/1000 | Loss: 0.00003450
Iteration 26/1000 | Loss: 0.00003448
Iteration 27/1000 | Loss: 0.00003446
Iteration 28/1000 | Loss: 0.00003446
Iteration 29/1000 | Loss: 0.00003446
Iteration 30/1000 | Loss: 0.00003446
Iteration 31/1000 | Loss: 0.00003445
Iteration 32/1000 | Loss: 0.00003441
Iteration 33/1000 | Loss: 0.00003439
Iteration 34/1000 | Loss: 0.00003439
Iteration 35/1000 | Loss: 0.00003439
Iteration 36/1000 | Loss: 0.00003439
Iteration 37/1000 | Loss: 0.00003439
Iteration 38/1000 | Loss: 0.00003439
Iteration 39/1000 | Loss: 0.00003438
Iteration 40/1000 | Loss: 0.00003438
Iteration 41/1000 | Loss: 0.00003438
Iteration 42/1000 | Loss: 0.00003438
Iteration 43/1000 | Loss: 0.00003434
Iteration 44/1000 | Loss: 0.00003433
Iteration 45/1000 | Loss: 0.00003430
Iteration 46/1000 | Loss: 0.00003428
Iteration 47/1000 | Loss: 0.00003425
Iteration 48/1000 | Loss: 0.00003423
Iteration 49/1000 | Loss: 0.00003422
Iteration 50/1000 | Loss: 0.00003421
Iteration 51/1000 | Loss: 0.00003419
Iteration 52/1000 | Loss: 0.00003418
Iteration 53/1000 | Loss: 0.00003418
Iteration 54/1000 | Loss: 0.00003417
Iteration 55/1000 | Loss: 0.00003417
Iteration 56/1000 | Loss: 0.00003417
Iteration 57/1000 | Loss: 0.00003417
Iteration 58/1000 | Loss: 0.00003416
Iteration 59/1000 | Loss: 0.00003416
Iteration 60/1000 | Loss: 0.00003415
Iteration 61/1000 | Loss: 0.00003415
Iteration 62/1000 | Loss: 0.00003414
Iteration 63/1000 | Loss: 0.00003411
Iteration 64/1000 | Loss: 0.00003411
Iteration 65/1000 | Loss: 0.00003409
Iteration 66/1000 | Loss: 0.00003409
Iteration 67/1000 | Loss: 0.00003408
Iteration 68/1000 | Loss: 0.00003408
Iteration 69/1000 | Loss: 0.00003407
Iteration 70/1000 | Loss: 0.00003407
Iteration 71/1000 | Loss: 0.00003405
Iteration 72/1000 | Loss: 0.00003405
Iteration 73/1000 | Loss: 0.00003405
Iteration 74/1000 | Loss: 0.00003405
Iteration 75/1000 | Loss: 0.00003405
Iteration 76/1000 | Loss: 0.00003405
Iteration 77/1000 | Loss: 0.00003404
Iteration 78/1000 | Loss: 0.00003404
Iteration 79/1000 | Loss: 0.00003403
Iteration 80/1000 | Loss: 0.00003403
Iteration 81/1000 | Loss: 0.00003403
Iteration 82/1000 | Loss: 0.00003402
Iteration 83/1000 | Loss: 0.00003402
Iteration 84/1000 | Loss: 0.00003402
Iteration 85/1000 | Loss: 0.00003401
Iteration 86/1000 | Loss: 0.00003401
Iteration 87/1000 | Loss: 0.00003401
Iteration 88/1000 | Loss: 0.00003400
Iteration 89/1000 | Loss: 0.00003400
Iteration 90/1000 | Loss: 0.00003400
Iteration 91/1000 | Loss: 0.00003399
Iteration 92/1000 | Loss: 0.00003399
Iteration 93/1000 | Loss: 0.00003398
Iteration 94/1000 | Loss: 0.00003398
Iteration 95/1000 | Loss: 0.00003398
Iteration 96/1000 | Loss: 0.00003397
Iteration 97/1000 | Loss: 0.00003397
Iteration 98/1000 | Loss: 0.00003397
Iteration 99/1000 | Loss: 0.00003396
Iteration 100/1000 | Loss: 0.00003396
Iteration 101/1000 | Loss: 0.00003395
Iteration 102/1000 | Loss: 0.00003395
Iteration 103/1000 | Loss: 0.00003394
Iteration 104/1000 | Loss: 0.00003394
Iteration 105/1000 | Loss: 0.00003393
Iteration 106/1000 | Loss: 0.00003393
Iteration 107/1000 | Loss: 0.00003393
Iteration 108/1000 | Loss: 0.00003392
Iteration 109/1000 | Loss: 0.00003392
Iteration 110/1000 | Loss: 0.00003392
Iteration 111/1000 | Loss: 0.00003392
Iteration 112/1000 | Loss: 0.00003391
Iteration 113/1000 | Loss: 0.00003391
Iteration 114/1000 | Loss: 0.00003391
Iteration 115/1000 | Loss: 0.00003391
Iteration 116/1000 | Loss: 0.00003390
Iteration 117/1000 | Loss: 0.00003390
Iteration 118/1000 | Loss: 0.00003390
Iteration 119/1000 | Loss: 0.00003390
Iteration 120/1000 | Loss: 0.00003390
Iteration 121/1000 | Loss: 0.00003390
Iteration 122/1000 | Loss: 0.00003390
Iteration 123/1000 | Loss: 0.00003389
Iteration 124/1000 | Loss: 0.00003389
Iteration 125/1000 | Loss: 0.00003389
Iteration 126/1000 | Loss: 0.00003389
Iteration 127/1000 | Loss: 0.00003389
Iteration 128/1000 | Loss: 0.00003388
Iteration 129/1000 | Loss: 0.00003388
Iteration 130/1000 | Loss: 0.00003388
Iteration 131/1000 | Loss: 0.00003388
Iteration 132/1000 | Loss: 0.00003388
Iteration 133/1000 | Loss: 0.00003388
Iteration 134/1000 | Loss: 0.00003388
Iteration 135/1000 | Loss: 0.00003388
Iteration 136/1000 | Loss: 0.00003387
Iteration 137/1000 | Loss: 0.00003387
Iteration 138/1000 | Loss: 0.00003387
Iteration 139/1000 | Loss: 0.00003387
Iteration 140/1000 | Loss: 0.00003387
Iteration 141/1000 | Loss: 0.00003387
Iteration 142/1000 | Loss: 0.00003387
Iteration 143/1000 | Loss: 0.00003387
Iteration 144/1000 | Loss: 0.00003387
Iteration 145/1000 | Loss: 0.00003387
Iteration 146/1000 | Loss: 0.00003386
Iteration 147/1000 | Loss: 0.00003386
Iteration 148/1000 | Loss: 0.00003386
Iteration 149/1000 | Loss: 0.00003386
Iteration 150/1000 | Loss: 0.00003386
Iteration 151/1000 | Loss: 0.00003386
Iteration 152/1000 | Loss: 0.00003385
Iteration 153/1000 | Loss: 0.00003385
Iteration 154/1000 | Loss: 0.00003385
Iteration 155/1000 | Loss: 0.00003385
Iteration 156/1000 | Loss: 0.00003385
Iteration 157/1000 | Loss: 0.00003385
Iteration 158/1000 | Loss: 0.00003385
Iteration 159/1000 | Loss: 0.00003384
Iteration 160/1000 | Loss: 0.00003384
Iteration 161/1000 | Loss: 0.00003384
Iteration 162/1000 | Loss: 0.00003384
Iteration 163/1000 | Loss: 0.00003384
Iteration 164/1000 | Loss: 0.00003383
Iteration 165/1000 | Loss: 0.00003383
Iteration 166/1000 | Loss: 0.00003383
Iteration 167/1000 | Loss: 0.00003382
Iteration 168/1000 | Loss: 0.00003382
Iteration 169/1000 | Loss: 0.00003382
Iteration 170/1000 | Loss: 0.00003382
Iteration 171/1000 | Loss: 0.00003382
Iteration 172/1000 | Loss: 0.00003381
Iteration 173/1000 | Loss: 0.00003381
Iteration 174/1000 | Loss: 0.00003381
Iteration 175/1000 | Loss: 0.00003381
Iteration 176/1000 | Loss: 0.00003381
Iteration 177/1000 | Loss: 0.00003381
Iteration 178/1000 | Loss: 0.00003380
Iteration 179/1000 | Loss: 0.00003380
Iteration 180/1000 | Loss: 0.00003380
Iteration 181/1000 | Loss: 0.00003380
Iteration 182/1000 | Loss: 0.00003380
Iteration 183/1000 | Loss: 0.00003380
Iteration 184/1000 | Loss: 0.00003380
Iteration 185/1000 | Loss: 0.00003380
Iteration 186/1000 | Loss: 0.00003380
Iteration 187/1000 | Loss: 0.00003379
Iteration 188/1000 | Loss: 0.00003379
Iteration 189/1000 | Loss: 0.00003379
Iteration 190/1000 | Loss: 0.00003379
Iteration 191/1000 | Loss: 0.00003379
Iteration 192/1000 | Loss: 0.00003379
Iteration 193/1000 | Loss: 0.00003379
Iteration 194/1000 | Loss: 0.00003379
Iteration 195/1000 | Loss: 0.00003379
Iteration 196/1000 | Loss: 0.00003379
Iteration 197/1000 | Loss: 0.00003379
Iteration 198/1000 | Loss: 0.00003379
Iteration 199/1000 | Loss: 0.00003378
Iteration 200/1000 | Loss: 0.00003378
Iteration 201/1000 | Loss: 0.00003378
Iteration 202/1000 | Loss: 0.00003378
Iteration 203/1000 | Loss: 0.00003378
Iteration 204/1000 | Loss: 0.00003378
Iteration 205/1000 | Loss: 0.00003377
Iteration 206/1000 | Loss: 0.00003377
Iteration 207/1000 | Loss: 0.00003377
Iteration 208/1000 | Loss: 0.00003377
Iteration 209/1000 | Loss: 0.00003377
Iteration 210/1000 | Loss: 0.00003377
Iteration 211/1000 | Loss: 0.00003376
Iteration 212/1000 | Loss: 0.00003376
Iteration 213/1000 | Loss: 0.00003376
Iteration 214/1000 | Loss: 0.00003376
Iteration 215/1000 | Loss: 0.00003376
Iteration 216/1000 | Loss: 0.00003375
Iteration 217/1000 | Loss: 0.00003375
Iteration 218/1000 | Loss: 0.00003375
Iteration 219/1000 | Loss: 0.00003375
Iteration 220/1000 | Loss: 0.00003375
Iteration 221/1000 | Loss: 0.00003375
Iteration 222/1000 | Loss: 0.00003374
Iteration 223/1000 | Loss: 0.00003374
Iteration 224/1000 | Loss: 0.00003374
Iteration 225/1000 | Loss: 0.00003374
Iteration 226/1000 | Loss: 0.00003374
Iteration 227/1000 | Loss: 0.00003374
Iteration 228/1000 | Loss: 0.00003374
Iteration 229/1000 | Loss: 0.00003374
Iteration 230/1000 | Loss: 0.00003373
Iteration 231/1000 | Loss: 0.00003373
Iteration 232/1000 | Loss: 0.00003373
Iteration 233/1000 | Loss: 0.00003373
Iteration 234/1000 | Loss: 0.00003373
Iteration 235/1000 | Loss: 0.00003373
Iteration 236/1000 | Loss: 0.00003373
Iteration 237/1000 | Loss: 0.00003373
Iteration 238/1000 | Loss: 0.00003373
Iteration 239/1000 | Loss: 0.00003373
Iteration 240/1000 | Loss: 0.00003373
Iteration 241/1000 | Loss: 0.00003373
Iteration 242/1000 | Loss: 0.00003373
Iteration 243/1000 | Loss: 0.00003373
Iteration 244/1000 | Loss: 0.00003373
Iteration 245/1000 | Loss: 0.00003373
Iteration 246/1000 | Loss: 0.00003373
Iteration 247/1000 | Loss: 0.00003373
Iteration 248/1000 | Loss: 0.00003373
Iteration 249/1000 | Loss: 0.00003372
Iteration 250/1000 | Loss: 0.00003372
Iteration 251/1000 | Loss: 0.00003372
Iteration 252/1000 | Loss: 0.00003372
Iteration 253/1000 | Loss: 0.00003372
Iteration 254/1000 | Loss: 0.00003372
Iteration 255/1000 | Loss: 0.00003372
Iteration 256/1000 | Loss: 0.00003372
Iteration 257/1000 | Loss: 0.00003372
Iteration 258/1000 | Loss: 0.00003372
Iteration 259/1000 | Loss: 0.00003372
Iteration 260/1000 | Loss: 0.00003372
Iteration 261/1000 | Loss: 0.00003372
Iteration 262/1000 | Loss: 0.00003372
Iteration 263/1000 | Loss: 0.00003372
Iteration 264/1000 | Loss: 0.00003372
Iteration 265/1000 | Loss: 0.00003372
Iteration 266/1000 | Loss: 0.00003372
Iteration 267/1000 | Loss: 0.00003371
Iteration 268/1000 | Loss: 0.00003371
Iteration 269/1000 | Loss: 0.00003371
Iteration 270/1000 | Loss: 0.00003371
Iteration 271/1000 | Loss: 0.00003371
Iteration 272/1000 | Loss: 0.00003371
Iteration 273/1000 | Loss: 0.00003371
Iteration 274/1000 | Loss: 0.00003371
Iteration 275/1000 | Loss: 0.00003371
Iteration 276/1000 | Loss: 0.00003371
Iteration 277/1000 | Loss: 0.00003371
Iteration 278/1000 | Loss: 0.00003371
Iteration 279/1000 | Loss: 0.00003371
Iteration 280/1000 | Loss: 0.00003371
Iteration 281/1000 | Loss: 0.00003371
Iteration 282/1000 | Loss: 0.00003370
Iteration 283/1000 | Loss: 0.00003370
Iteration 284/1000 | Loss: 0.00003370
Iteration 285/1000 | Loss: 0.00003370
Iteration 286/1000 | Loss: 0.00003370
Iteration 287/1000 | Loss: 0.00003370
Iteration 288/1000 | Loss: 0.00003370
Iteration 289/1000 | Loss: 0.00003370
Iteration 290/1000 | Loss: 0.00003370
Iteration 291/1000 | Loss: 0.00003370
Iteration 292/1000 | Loss: 0.00003370
Iteration 293/1000 | Loss: 0.00003370
Iteration 294/1000 | Loss: 0.00003370
Iteration 295/1000 | Loss: 0.00003370
Iteration 296/1000 | Loss: 0.00003370
Iteration 297/1000 | Loss: 0.00003370
Iteration 298/1000 | Loss: 0.00003370
Iteration 299/1000 | Loss: 0.00003370
Iteration 300/1000 | Loss: 0.00003370
Iteration 301/1000 | Loss: 0.00003370
Iteration 302/1000 | Loss: 0.00003370
Iteration 303/1000 | Loss: 0.00003370
Iteration 304/1000 | Loss: 0.00003370
Iteration 305/1000 | Loss: 0.00003370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [3.3702504879329354e-05, 3.3702504879329354e-05, 3.3702504879329354e-05, 3.3702504879329354e-05, 3.3702504879329354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3702504879329354e-05

Optimization complete. Final v2v error: 4.587226867675781 mm

Highest mean error: 7.251587867736816 mm for frame 99

Lowest mean error: 3.039653778076172 mm for frame 140

Saving results

Total time: 74.24666666984558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476581
Iteration 2/25 | Loss: 0.00116222
Iteration 3/25 | Loss: 0.00105498
Iteration 4/25 | Loss: 0.00104568
Iteration 5/25 | Loss: 0.00104317
Iteration 6/25 | Loss: 0.00104317
Iteration 7/25 | Loss: 0.00104317
Iteration 8/25 | Loss: 0.00104317
Iteration 9/25 | Loss: 0.00104317
Iteration 10/25 | Loss: 0.00104317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010431688278913498, 0.0010431688278913498, 0.0010431688278913498, 0.0010431688278913498, 0.0010431688278913498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010431688278913498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38748920
Iteration 2/25 | Loss: 0.00075195
Iteration 3/25 | Loss: 0.00075195
Iteration 4/25 | Loss: 0.00075195
Iteration 5/25 | Loss: 0.00075195
Iteration 6/25 | Loss: 0.00075195
Iteration 7/25 | Loss: 0.00075195
Iteration 8/25 | Loss: 0.00075195
Iteration 9/25 | Loss: 0.00075195
Iteration 10/25 | Loss: 0.00075195
Iteration 11/25 | Loss: 0.00075195
Iteration 12/25 | Loss: 0.00075195
Iteration 13/25 | Loss: 0.00075195
Iteration 14/25 | Loss: 0.00075195
Iteration 15/25 | Loss: 0.00075195
Iteration 16/25 | Loss: 0.00075195
Iteration 17/25 | Loss: 0.00075195
Iteration 18/25 | Loss: 0.00075195
Iteration 19/25 | Loss: 0.00075195
Iteration 20/25 | Loss: 0.00075195
Iteration 21/25 | Loss: 0.00075195
Iteration 22/25 | Loss: 0.00075195
Iteration 23/25 | Loss: 0.00075195
Iteration 24/25 | Loss: 0.00075195
Iteration 25/25 | Loss: 0.00075195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075195
Iteration 2/1000 | Loss: 0.00002925
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001415
Iteration 9/1000 | Loss: 0.00001410
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001356
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001353
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001351
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001343
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001337
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001312
Iteration 165/1000 | Loss: 0.00001312
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001311
Iteration 171/1000 | Loss: 0.00001311
Iteration 172/1000 | Loss: 0.00001311
Iteration 173/1000 | Loss: 0.00001311
Iteration 174/1000 | Loss: 0.00001311
Iteration 175/1000 | Loss: 0.00001311
Iteration 176/1000 | Loss: 0.00001310
Iteration 177/1000 | Loss: 0.00001310
Iteration 178/1000 | Loss: 0.00001310
Iteration 179/1000 | Loss: 0.00001310
Iteration 180/1000 | Loss: 0.00001310
Iteration 181/1000 | Loss: 0.00001310
Iteration 182/1000 | Loss: 0.00001310
Iteration 183/1000 | Loss: 0.00001310
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001309
Iteration 186/1000 | Loss: 0.00001309
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001307
Iteration 191/1000 | Loss: 0.00001307
Iteration 192/1000 | Loss: 0.00001307
Iteration 193/1000 | Loss: 0.00001307
Iteration 194/1000 | Loss: 0.00001307
Iteration 195/1000 | Loss: 0.00001307
Iteration 196/1000 | Loss: 0.00001306
Iteration 197/1000 | Loss: 0.00001306
Iteration 198/1000 | Loss: 0.00001306
Iteration 199/1000 | Loss: 0.00001306
Iteration 200/1000 | Loss: 0.00001305
Iteration 201/1000 | Loss: 0.00001305
Iteration 202/1000 | Loss: 0.00001305
Iteration 203/1000 | Loss: 0.00001305
Iteration 204/1000 | Loss: 0.00001305
Iteration 205/1000 | Loss: 0.00001304
Iteration 206/1000 | Loss: 0.00001304
Iteration 207/1000 | Loss: 0.00001304
Iteration 208/1000 | Loss: 0.00001303
Iteration 209/1000 | Loss: 0.00001303
Iteration 210/1000 | Loss: 0.00001303
Iteration 211/1000 | Loss: 0.00001303
Iteration 212/1000 | Loss: 0.00001303
Iteration 213/1000 | Loss: 0.00001303
Iteration 214/1000 | Loss: 0.00001303
Iteration 215/1000 | Loss: 0.00001303
Iteration 216/1000 | Loss: 0.00001303
Iteration 217/1000 | Loss: 0.00001303
Iteration 218/1000 | Loss: 0.00001303
Iteration 219/1000 | Loss: 0.00001302
Iteration 220/1000 | Loss: 0.00001302
Iteration 221/1000 | Loss: 0.00001302
Iteration 222/1000 | Loss: 0.00001302
Iteration 223/1000 | Loss: 0.00001302
Iteration 224/1000 | Loss: 0.00001302
Iteration 225/1000 | Loss: 0.00001302
Iteration 226/1000 | Loss: 0.00001302
Iteration 227/1000 | Loss: 0.00001302
Iteration 228/1000 | Loss: 0.00001301
Iteration 229/1000 | Loss: 0.00001301
Iteration 230/1000 | Loss: 0.00001301
Iteration 231/1000 | Loss: 0.00001301
Iteration 232/1000 | Loss: 0.00001301
Iteration 233/1000 | Loss: 0.00001301
Iteration 234/1000 | Loss: 0.00001301
Iteration 235/1000 | Loss: 0.00001301
Iteration 236/1000 | Loss: 0.00001301
Iteration 237/1000 | Loss: 0.00001301
Iteration 238/1000 | Loss: 0.00001301
Iteration 239/1000 | Loss: 0.00001300
Iteration 240/1000 | Loss: 0.00001300
Iteration 241/1000 | Loss: 0.00001300
Iteration 242/1000 | Loss: 0.00001300
Iteration 243/1000 | Loss: 0.00001300
Iteration 244/1000 | Loss: 0.00001299
Iteration 245/1000 | Loss: 0.00001299
Iteration 246/1000 | Loss: 0.00001299
Iteration 247/1000 | Loss: 0.00001299
Iteration 248/1000 | Loss: 0.00001299
Iteration 249/1000 | Loss: 0.00001299
Iteration 250/1000 | Loss: 0.00001299
Iteration 251/1000 | Loss: 0.00001299
Iteration 252/1000 | Loss: 0.00001299
Iteration 253/1000 | Loss: 0.00001299
Iteration 254/1000 | Loss: 0.00001299
Iteration 255/1000 | Loss: 0.00001299
Iteration 256/1000 | Loss: 0.00001299
Iteration 257/1000 | Loss: 0.00001299
Iteration 258/1000 | Loss: 0.00001299
Iteration 259/1000 | Loss: 0.00001299
Iteration 260/1000 | Loss: 0.00001299
Iteration 261/1000 | Loss: 0.00001299
Iteration 262/1000 | Loss: 0.00001298
Iteration 263/1000 | Loss: 0.00001298
Iteration 264/1000 | Loss: 0.00001298
Iteration 265/1000 | Loss: 0.00001298
Iteration 266/1000 | Loss: 0.00001298
Iteration 267/1000 | Loss: 0.00001298
Iteration 268/1000 | Loss: 0.00001298
Iteration 269/1000 | Loss: 0.00001298
Iteration 270/1000 | Loss: 0.00001298
Iteration 271/1000 | Loss: 0.00001298
Iteration 272/1000 | Loss: 0.00001298
Iteration 273/1000 | Loss: 0.00001298
Iteration 274/1000 | Loss: 0.00001297
Iteration 275/1000 | Loss: 0.00001297
Iteration 276/1000 | Loss: 0.00001297
Iteration 277/1000 | Loss: 0.00001297
Iteration 278/1000 | Loss: 0.00001297
Iteration 279/1000 | Loss: 0.00001297
Iteration 280/1000 | Loss: 0.00001297
Iteration 281/1000 | Loss: 0.00001297
Iteration 282/1000 | Loss: 0.00001297
Iteration 283/1000 | Loss: 0.00001297
Iteration 284/1000 | Loss: 0.00001296
Iteration 285/1000 | Loss: 0.00001296
Iteration 286/1000 | Loss: 0.00001296
Iteration 287/1000 | Loss: 0.00001296
Iteration 288/1000 | Loss: 0.00001296
Iteration 289/1000 | Loss: 0.00001295
Iteration 290/1000 | Loss: 0.00001295
Iteration 291/1000 | Loss: 0.00001295
Iteration 292/1000 | Loss: 0.00001295
Iteration 293/1000 | Loss: 0.00001295
Iteration 294/1000 | Loss: 0.00001295
Iteration 295/1000 | Loss: 0.00001295
Iteration 296/1000 | Loss: 0.00001295
Iteration 297/1000 | Loss: 0.00001295
Iteration 298/1000 | Loss: 0.00001295
Iteration 299/1000 | Loss: 0.00001295
Iteration 300/1000 | Loss: 0.00001295
Iteration 301/1000 | Loss: 0.00001295
Iteration 302/1000 | Loss: 0.00001295
Iteration 303/1000 | Loss: 0.00001295
Iteration 304/1000 | Loss: 0.00001295
Iteration 305/1000 | Loss: 0.00001295
Iteration 306/1000 | Loss: 0.00001295
Iteration 307/1000 | Loss: 0.00001295
Iteration 308/1000 | Loss: 0.00001295
Iteration 309/1000 | Loss: 0.00001295
Iteration 310/1000 | Loss: 0.00001295
Iteration 311/1000 | Loss: 0.00001295
Iteration 312/1000 | Loss: 0.00001295
Iteration 313/1000 | Loss: 0.00001295
Iteration 314/1000 | Loss: 0.00001295
Iteration 315/1000 | Loss: 0.00001295
Iteration 316/1000 | Loss: 0.00001295
Iteration 317/1000 | Loss: 0.00001295
Iteration 318/1000 | Loss: 0.00001295
Iteration 319/1000 | Loss: 0.00001295
Iteration 320/1000 | Loss: 0.00001295
Iteration 321/1000 | Loss: 0.00001295
Iteration 322/1000 | Loss: 0.00001295
Iteration 323/1000 | Loss: 0.00001295
Iteration 324/1000 | Loss: 0.00001295
Iteration 325/1000 | Loss: 0.00001295
Iteration 326/1000 | Loss: 0.00001295
Iteration 327/1000 | Loss: 0.00001295
Iteration 328/1000 | Loss: 0.00001295
Iteration 329/1000 | Loss: 0.00001295
Iteration 330/1000 | Loss: 0.00001295
Iteration 331/1000 | Loss: 0.00001295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.2952663382748142e-05, 1.2952663382748142e-05, 1.2952663382748142e-05, 1.2952663382748142e-05, 1.2952663382748142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2952663382748142e-05

Optimization complete. Final v2v error: 2.990751266479492 mm

Highest mean error: 3.3725740909576416 mm for frame 61

Lowest mean error: 2.8405065536499023 mm for frame 31

Saving results

Total time: 47.764777421951294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020703
Iteration 2/25 | Loss: 0.00261040
Iteration 3/25 | Loss: 0.00173354
Iteration 4/25 | Loss: 0.00158085
Iteration 5/25 | Loss: 0.00133404
Iteration 6/25 | Loss: 0.00127804
Iteration 7/25 | Loss: 0.00125792
Iteration 8/25 | Loss: 0.00123477
Iteration 9/25 | Loss: 0.00120754
Iteration 10/25 | Loss: 0.00119656
Iteration 11/25 | Loss: 0.00120007
Iteration 12/25 | Loss: 0.00119738
Iteration 13/25 | Loss: 0.00119041
Iteration 14/25 | Loss: 0.00120049
Iteration 15/25 | Loss: 0.00119994
Iteration 16/25 | Loss: 0.00117697
Iteration 17/25 | Loss: 0.00117352
Iteration 18/25 | Loss: 0.00117168
Iteration 19/25 | Loss: 0.00117090
Iteration 20/25 | Loss: 0.00117803
Iteration 21/25 | Loss: 0.00116818
Iteration 22/25 | Loss: 0.00116788
Iteration 23/25 | Loss: 0.00117086
Iteration 24/25 | Loss: 0.00117038
Iteration 25/25 | Loss: 0.00116719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42133939
Iteration 2/25 | Loss: 0.00161983
Iteration 3/25 | Loss: 0.00156653
Iteration 4/25 | Loss: 0.00156652
Iteration 5/25 | Loss: 0.00156652
Iteration 6/25 | Loss: 0.00156652
Iteration 7/25 | Loss: 0.00156652
Iteration 8/25 | Loss: 0.00156652
Iteration 9/25 | Loss: 0.00156652
Iteration 10/25 | Loss: 0.00156652
Iteration 11/25 | Loss: 0.00156652
Iteration 12/25 | Loss: 0.00156652
Iteration 13/25 | Loss: 0.00156652
Iteration 14/25 | Loss: 0.00156652
Iteration 15/25 | Loss: 0.00156652
Iteration 16/25 | Loss: 0.00156652
Iteration 17/25 | Loss: 0.00156652
Iteration 18/25 | Loss: 0.00156652
Iteration 19/25 | Loss: 0.00156652
Iteration 20/25 | Loss: 0.00156652
Iteration 21/25 | Loss: 0.00156652
Iteration 22/25 | Loss: 0.00156652
Iteration 23/25 | Loss: 0.00156652
Iteration 24/25 | Loss: 0.00156652
Iteration 25/25 | Loss: 0.00156652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156652
Iteration 2/1000 | Loss: 0.00024153
Iteration 3/1000 | Loss: 0.00030143
Iteration 4/1000 | Loss: 0.00040541
Iteration 5/1000 | Loss: 0.00010337
Iteration 6/1000 | Loss: 0.00016710
Iteration 7/1000 | Loss: 0.00036720
Iteration 8/1000 | Loss: 0.00027543
Iteration 9/1000 | Loss: 0.00023846
Iteration 10/1000 | Loss: 0.00042957
Iteration 11/1000 | Loss: 0.00010418
Iteration 12/1000 | Loss: 0.00008564
Iteration 13/1000 | Loss: 0.00011680
Iteration 14/1000 | Loss: 0.00023007
Iteration 15/1000 | Loss: 0.00008798
Iteration 16/1000 | Loss: 0.00015930
Iteration 17/1000 | Loss: 0.00011617
Iteration 18/1000 | Loss: 0.00014177
Iteration 19/1000 | Loss: 0.00007488
Iteration 20/1000 | Loss: 0.00011658
Iteration 21/1000 | Loss: 0.00009865
Iteration 22/1000 | Loss: 0.00009199
Iteration 23/1000 | Loss: 0.00015487
Iteration 24/1000 | Loss: 0.00010150
Iteration 25/1000 | Loss: 0.00011447
Iteration 26/1000 | Loss: 0.00007797
Iteration 27/1000 | Loss: 0.00014734
Iteration 28/1000 | Loss: 0.00022589
Iteration 29/1000 | Loss: 0.00011493
Iteration 30/1000 | Loss: 0.00011615
Iteration 31/1000 | Loss: 0.00018447
Iteration 32/1000 | Loss: 0.00012041
Iteration 33/1000 | Loss: 0.00011992
Iteration 34/1000 | Loss: 0.00010620
Iteration 35/1000 | Loss: 0.00009018
Iteration 36/1000 | Loss: 0.00013483
Iteration 37/1000 | Loss: 0.00044548
Iteration 38/1000 | Loss: 0.00035033
Iteration 39/1000 | Loss: 0.00012592
Iteration 40/1000 | Loss: 0.00010877
Iteration 41/1000 | Loss: 0.00011086
Iteration 42/1000 | Loss: 0.00010794
Iteration 43/1000 | Loss: 0.00043031
Iteration 44/1000 | Loss: 0.00046291
Iteration 45/1000 | Loss: 0.00015203
Iteration 46/1000 | Loss: 0.00012725
Iteration 47/1000 | Loss: 0.00293031
Iteration 48/1000 | Loss: 0.00126524
Iteration 49/1000 | Loss: 0.00077497
Iteration 50/1000 | Loss: 0.00033916
Iteration 51/1000 | Loss: 0.00070007
Iteration 52/1000 | Loss: 0.00008864
Iteration 53/1000 | Loss: 0.00036811
Iteration 54/1000 | Loss: 0.00014225
Iteration 55/1000 | Loss: 0.00050696
Iteration 56/1000 | Loss: 0.00007209
Iteration 57/1000 | Loss: 0.00012663
Iteration 58/1000 | Loss: 0.00034650
Iteration 59/1000 | Loss: 0.00003631
Iteration 60/1000 | Loss: 0.00006948
Iteration 61/1000 | Loss: 0.00009403
Iteration 62/1000 | Loss: 0.00008127
Iteration 63/1000 | Loss: 0.00007214
Iteration 64/1000 | Loss: 0.00003860
Iteration 65/1000 | Loss: 0.00002166
Iteration 66/1000 | Loss: 0.00008285
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00007741
Iteration 69/1000 | Loss: 0.00007938
Iteration 70/1000 | Loss: 0.00009608
Iteration 71/1000 | Loss: 0.00003756
Iteration 72/1000 | Loss: 0.00013131
Iteration 73/1000 | Loss: 0.00002083
Iteration 74/1000 | Loss: 0.00004377
Iteration 75/1000 | Loss: 0.00002107
Iteration 76/1000 | Loss: 0.00003073
Iteration 77/1000 | Loss: 0.00009142
Iteration 78/1000 | Loss: 0.00005285
Iteration 79/1000 | Loss: 0.00007803
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00003607
Iteration 82/1000 | Loss: 0.00001485
Iteration 83/1000 | Loss: 0.00003688
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001449
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001424
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00004502
Iteration 93/1000 | Loss: 0.00005168
Iteration 94/1000 | Loss: 0.00015315
Iteration 95/1000 | Loss: 0.00001768
Iteration 96/1000 | Loss: 0.00003934
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00002524
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001407
Iteration 108/1000 | Loss: 0.00001407
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001406
Iteration 111/1000 | Loss: 0.00001406
Iteration 112/1000 | Loss: 0.00001406
Iteration 113/1000 | Loss: 0.00001406
Iteration 114/1000 | Loss: 0.00001406
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001406
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001404
Iteration 127/1000 | Loss: 0.00004000
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001399
Iteration 147/1000 | Loss: 0.00001399
Iteration 148/1000 | Loss: 0.00001399
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001398
Iteration 159/1000 | Loss: 0.00001398
Iteration 160/1000 | Loss: 0.00001398
Iteration 161/1000 | Loss: 0.00001398
Iteration 162/1000 | Loss: 0.00001397
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001397
Iteration 169/1000 | Loss: 0.00001397
Iteration 170/1000 | Loss: 0.00001397
Iteration 171/1000 | Loss: 0.00001397
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001396
Iteration 175/1000 | Loss: 0.00001396
Iteration 176/1000 | Loss: 0.00001396
Iteration 177/1000 | Loss: 0.00001396
Iteration 178/1000 | Loss: 0.00001396
Iteration 179/1000 | Loss: 0.00001396
Iteration 180/1000 | Loss: 0.00001396
Iteration 181/1000 | Loss: 0.00001396
Iteration 182/1000 | Loss: 0.00001396
Iteration 183/1000 | Loss: 0.00001396
Iteration 184/1000 | Loss: 0.00001396
Iteration 185/1000 | Loss: 0.00001396
Iteration 186/1000 | Loss: 0.00001396
Iteration 187/1000 | Loss: 0.00001396
Iteration 188/1000 | Loss: 0.00001395
Iteration 189/1000 | Loss: 0.00001395
Iteration 190/1000 | Loss: 0.00001395
Iteration 191/1000 | Loss: 0.00001395
Iteration 192/1000 | Loss: 0.00001395
Iteration 193/1000 | Loss: 0.00001395
Iteration 194/1000 | Loss: 0.00001395
Iteration 195/1000 | Loss: 0.00001395
Iteration 196/1000 | Loss: 0.00001395
Iteration 197/1000 | Loss: 0.00001395
Iteration 198/1000 | Loss: 0.00001395
Iteration 199/1000 | Loss: 0.00004979
Iteration 200/1000 | Loss: 0.00010347
Iteration 201/1000 | Loss: 0.00007896
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00005818
Iteration 204/1000 | Loss: 0.00002477
Iteration 205/1000 | Loss: 0.00001400
Iteration 206/1000 | Loss: 0.00001397
Iteration 207/1000 | Loss: 0.00001396
Iteration 208/1000 | Loss: 0.00001396
Iteration 209/1000 | Loss: 0.00001395
Iteration 210/1000 | Loss: 0.00001395
Iteration 211/1000 | Loss: 0.00001395
Iteration 212/1000 | Loss: 0.00001395
Iteration 213/1000 | Loss: 0.00002311
Iteration 214/1000 | Loss: 0.00001920
Iteration 215/1000 | Loss: 0.00001393
Iteration 216/1000 | Loss: 0.00001393
Iteration 217/1000 | Loss: 0.00001393
Iteration 218/1000 | Loss: 0.00001393
Iteration 219/1000 | Loss: 0.00001393
Iteration 220/1000 | Loss: 0.00001393
Iteration 221/1000 | Loss: 0.00001393
Iteration 222/1000 | Loss: 0.00001393
Iteration 223/1000 | Loss: 0.00001393
Iteration 224/1000 | Loss: 0.00001393
Iteration 225/1000 | Loss: 0.00001393
Iteration 226/1000 | Loss: 0.00001393
Iteration 227/1000 | Loss: 0.00002045
Iteration 228/1000 | Loss: 0.00001391
Iteration 229/1000 | Loss: 0.00001391
Iteration 230/1000 | Loss: 0.00001391
Iteration 231/1000 | Loss: 0.00001390
Iteration 232/1000 | Loss: 0.00001390
Iteration 233/1000 | Loss: 0.00001390
Iteration 234/1000 | Loss: 0.00001390
Iteration 235/1000 | Loss: 0.00001390
Iteration 236/1000 | Loss: 0.00001390
Iteration 237/1000 | Loss: 0.00001390
Iteration 238/1000 | Loss: 0.00001390
Iteration 239/1000 | Loss: 0.00001390
Iteration 240/1000 | Loss: 0.00001390
Iteration 241/1000 | Loss: 0.00001390
Iteration 242/1000 | Loss: 0.00001390
Iteration 243/1000 | Loss: 0.00001390
Iteration 244/1000 | Loss: 0.00001390
Iteration 245/1000 | Loss: 0.00001390
Iteration 246/1000 | Loss: 0.00001390
Iteration 247/1000 | Loss: 0.00001390
Iteration 248/1000 | Loss: 0.00001390
Iteration 249/1000 | Loss: 0.00001390
Iteration 250/1000 | Loss: 0.00001390
Iteration 251/1000 | Loss: 0.00001390
Iteration 252/1000 | Loss: 0.00001390
Iteration 253/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.3901941201766022e-05, 1.3901941201766022e-05, 1.3901941201766022e-05, 1.3901941201766022e-05, 1.3901941201766022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3901941201766022e-05

Optimization complete. Final v2v error: 3.1059460639953613 mm

Highest mean error: 3.4383227825164795 mm for frame 81

Lowest mean error: 2.7553842067718506 mm for frame 104

Saving results

Total time: 195.37070631980896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420460
Iteration 2/25 | Loss: 0.00105871
Iteration 3/25 | Loss: 0.00097550
Iteration 4/25 | Loss: 0.00096232
Iteration 5/25 | Loss: 0.00095960
Iteration 6/25 | Loss: 0.00095902
Iteration 7/25 | Loss: 0.00095902
Iteration 8/25 | Loss: 0.00095902
Iteration 9/25 | Loss: 0.00095902
Iteration 10/25 | Loss: 0.00095902
Iteration 11/25 | Loss: 0.00095902
Iteration 12/25 | Loss: 0.00095902
Iteration 13/25 | Loss: 0.00095902
Iteration 14/25 | Loss: 0.00095902
Iteration 15/25 | Loss: 0.00095902
Iteration 16/25 | Loss: 0.00095902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009590177796781063, 0.0009590177796781063, 0.0009590177796781063, 0.0009590177796781063, 0.0009590177796781063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009590177796781063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11908877
Iteration 2/25 | Loss: 0.00044274
Iteration 3/25 | Loss: 0.00044274
Iteration 4/25 | Loss: 0.00044274
Iteration 5/25 | Loss: 0.00044274
Iteration 6/25 | Loss: 0.00044274
Iteration 7/25 | Loss: 0.00044274
Iteration 8/25 | Loss: 0.00044274
Iteration 9/25 | Loss: 0.00044274
Iteration 10/25 | Loss: 0.00044274
Iteration 11/25 | Loss: 0.00044274
Iteration 12/25 | Loss: 0.00044274
Iteration 13/25 | Loss: 0.00044274
Iteration 14/25 | Loss: 0.00044274
Iteration 15/25 | Loss: 0.00044274
Iteration 16/25 | Loss: 0.00044274
Iteration 17/25 | Loss: 0.00044274
Iteration 18/25 | Loss: 0.00044274
Iteration 19/25 | Loss: 0.00044274
Iteration 20/25 | Loss: 0.00044274
Iteration 21/25 | Loss: 0.00044274
Iteration 22/25 | Loss: 0.00044274
Iteration 23/25 | Loss: 0.00044274
Iteration 24/25 | Loss: 0.00044274
Iteration 25/25 | Loss: 0.00044274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044274
Iteration 2/1000 | Loss: 0.00003457
Iteration 3/1000 | Loss: 0.00002004
Iteration 4/1000 | Loss: 0.00001571
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00001142
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001142
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001141
Iteration 37/1000 | Loss: 0.00001141
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001138
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001138
Iteration 48/1000 | Loss: 0.00001137
Iteration 49/1000 | Loss: 0.00001137
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001136
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001136
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001136
Iteration 64/1000 | Loss: 0.00001136
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00001135
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001135
Iteration 75/1000 | Loss: 0.00001135
Iteration 76/1000 | Loss: 0.00001135
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001135
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001134
Iteration 85/1000 | Loss: 0.00001134
Iteration 86/1000 | Loss: 0.00001134
Iteration 87/1000 | Loss: 0.00001134
Iteration 88/1000 | Loss: 0.00001134
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001134
Iteration 91/1000 | Loss: 0.00001134
Iteration 92/1000 | Loss: 0.00001134
Iteration 93/1000 | Loss: 0.00001134
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001132
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001131
Iteration 109/1000 | Loss: 0.00001131
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001131
Iteration 113/1000 | Loss: 0.00001131
Iteration 114/1000 | Loss: 0.00001131
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001130
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.130329656007234e-05, 1.130329656007234e-05, 1.130329656007234e-05, 1.130329656007234e-05, 1.130329656007234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.130329656007234e-05

Optimization complete. Final v2v error: 2.884399175643921 mm

Highest mean error: 2.940991163253784 mm for frame 95

Lowest mean error: 2.8446662425994873 mm for frame 20

Saving results

Total time: 33.06213355064392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025363
Iteration 2/25 | Loss: 0.00140851
Iteration 3/25 | Loss: 0.00116272
Iteration 4/25 | Loss: 0.00113083
Iteration 5/25 | Loss: 0.00112253
Iteration 6/25 | Loss: 0.00112052
Iteration 7/25 | Loss: 0.00112044
Iteration 8/25 | Loss: 0.00112044
Iteration 9/25 | Loss: 0.00112044
Iteration 10/25 | Loss: 0.00112044
Iteration 11/25 | Loss: 0.00112044
Iteration 12/25 | Loss: 0.00112044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011204397305846214, 0.0011204397305846214, 0.0011204397305846214, 0.0011204397305846214, 0.0011204397305846214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011204397305846214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71649569
Iteration 2/25 | Loss: 0.00090823
Iteration 3/25 | Loss: 0.00090819
Iteration 4/25 | Loss: 0.00090819
Iteration 5/25 | Loss: 0.00090819
Iteration 6/25 | Loss: 0.00090819
Iteration 7/25 | Loss: 0.00090819
Iteration 8/25 | Loss: 0.00090819
Iteration 9/25 | Loss: 0.00090819
Iteration 10/25 | Loss: 0.00090819
Iteration 11/25 | Loss: 0.00090818
Iteration 12/25 | Loss: 0.00090818
Iteration 13/25 | Loss: 0.00090818
Iteration 14/25 | Loss: 0.00090818
Iteration 15/25 | Loss: 0.00090818
Iteration 16/25 | Loss: 0.00090818
Iteration 17/25 | Loss: 0.00090818
Iteration 18/25 | Loss: 0.00090818
Iteration 19/25 | Loss: 0.00090818
Iteration 20/25 | Loss: 0.00090818
Iteration 21/25 | Loss: 0.00090818
Iteration 22/25 | Loss: 0.00090818
Iteration 23/25 | Loss: 0.00090818
Iteration 24/25 | Loss: 0.00090818
Iteration 25/25 | Loss: 0.00090818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090818
Iteration 2/1000 | Loss: 0.00006112
Iteration 3/1000 | Loss: 0.00004350
Iteration 4/1000 | Loss: 0.00003551
Iteration 5/1000 | Loss: 0.00003328
Iteration 6/1000 | Loss: 0.00003196
Iteration 7/1000 | Loss: 0.00003086
Iteration 8/1000 | Loss: 0.00003002
Iteration 9/1000 | Loss: 0.00002936
Iteration 10/1000 | Loss: 0.00002883
Iteration 11/1000 | Loss: 0.00002854
Iteration 12/1000 | Loss: 0.00002816
Iteration 13/1000 | Loss: 0.00002794
Iteration 14/1000 | Loss: 0.00002767
Iteration 15/1000 | Loss: 0.00002747
Iteration 16/1000 | Loss: 0.00002739
Iteration 17/1000 | Loss: 0.00002733
Iteration 18/1000 | Loss: 0.00002733
Iteration 19/1000 | Loss: 0.00002732
Iteration 20/1000 | Loss: 0.00002732
Iteration 21/1000 | Loss: 0.00002731
Iteration 22/1000 | Loss: 0.00002724
Iteration 23/1000 | Loss: 0.00002724
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00002722
Iteration 26/1000 | Loss: 0.00002722
Iteration 27/1000 | Loss: 0.00002721
Iteration 28/1000 | Loss: 0.00002721
Iteration 29/1000 | Loss: 0.00002720
Iteration 30/1000 | Loss: 0.00002719
Iteration 31/1000 | Loss: 0.00002718
Iteration 32/1000 | Loss: 0.00002717
Iteration 33/1000 | Loss: 0.00002717
Iteration 34/1000 | Loss: 0.00002717
Iteration 35/1000 | Loss: 0.00002717
Iteration 36/1000 | Loss: 0.00002717
Iteration 37/1000 | Loss: 0.00002717
Iteration 38/1000 | Loss: 0.00002716
Iteration 39/1000 | Loss: 0.00002715
Iteration 40/1000 | Loss: 0.00002715
Iteration 41/1000 | Loss: 0.00002714
Iteration 42/1000 | Loss: 0.00002714
Iteration 43/1000 | Loss: 0.00002713
Iteration 44/1000 | Loss: 0.00002713
Iteration 45/1000 | Loss: 0.00002713
Iteration 46/1000 | Loss: 0.00002713
Iteration 47/1000 | Loss: 0.00002712
Iteration 48/1000 | Loss: 0.00002712
Iteration 49/1000 | Loss: 0.00002712
Iteration 50/1000 | Loss: 0.00002712
Iteration 51/1000 | Loss: 0.00002711
Iteration 52/1000 | Loss: 0.00002711
Iteration 53/1000 | Loss: 0.00002711
Iteration 54/1000 | Loss: 0.00002711
Iteration 55/1000 | Loss: 0.00002710
Iteration 56/1000 | Loss: 0.00002710
Iteration 57/1000 | Loss: 0.00002710
Iteration 58/1000 | Loss: 0.00002710
Iteration 59/1000 | Loss: 0.00002710
Iteration 60/1000 | Loss: 0.00002710
Iteration 61/1000 | Loss: 0.00002710
Iteration 62/1000 | Loss: 0.00002709
Iteration 63/1000 | Loss: 0.00002709
Iteration 64/1000 | Loss: 0.00002709
Iteration 65/1000 | Loss: 0.00002709
Iteration 66/1000 | Loss: 0.00002709
Iteration 67/1000 | Loss: 0.00002709
Iteration 68/1000 | Loss: 0.00002709
Iteration 69/1000 | Loss: 0.00002709
Iteration 70/1000 | Loss: 0.00002709
Iteration 71/1000 | Loss: 0.00002709
Iteration 72/1000 | Loss: 0.00002709
Iteration 73/1000 | Loss: 0.00002709
Iteration 74/1000 | Loss: 0.00002709
Iteration 75/1000 | Loss: 0.00002709
Iteration 76/1000 | Loss: 0.00002709
Iteration 77/1000 | Loss: 0.00002709
Iteration 78/1000 | Loss: 0.00002709
Iteration 79/1000 | Loss: 0.00002709
Iteration 80/1000 | Loss: 0.00002709
Iteration 81/1000 | Loss: 0.00002709
Iteration 82/1000 | Loss: 0.00002709
Iteration 83/1000 | Loss: 0.00002709
Iteration 84/1000 | Loss: 0.00002709
Iteration 85/1000 | Loss: 0.00002709
Iteration 86/1000 | Loss: 0.00002709
Iteration 87/1000 | Loss: 0.00002709
Iteration 88/1000 | Loss: 0.00002709
Iteration 89/1000 | Loss: 0.00002709
Iteration 90/1000 | Loss: 0.00002709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.7088273782283068e-05, 2.7088273782283068e-05, 2.7088273782283068e-05, 2.7088273782283068e-05, 2.7088273782283068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7088273782283068e-05

Optimization complete. Final v2v error: 4.327132225036621 mm

Highest mean error: 4.913877964019775 mm for frame 71

Lowest mean error: 3.9660608768463135 mm for frame 55

Saving results

Total time: 39.34882473945618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035424
Iteration 2/25 | Loss: 0.00235311
Iteration 3/25 | Loss: 0.00153009
Iteration 4/25 | Loss: 0.00140908
Iteration 5/25 | Loss: 0.00143777
Iteration 6/25 | Loss: 0.00136149
Iteration 7/25 | Loss: 0.00117944
Iteration 8/25 | Loss: 0.00121697
Iteration 9/25 | Loss: 0.00108234
Iteration 10/25 | Loss: 0.00105311
Iteration 11/25 | Loss: 0.00103865
Iteration 12/25 | Loss: 0.00102888
Iteration 13/25 | Loss: 0.00101272
Iteration 14/25 | Loss: 0.00101585
Iteration 15/25 | Loss: 0.00100478
Iteration 16/25 | Loss: 0.00100276
Iteration 17/25 | Loss: 0.00100075
Iteration 18/25 | Loss: 0.00099753
Iteration 19/25 | Loss: 0.00099661
Iteration 20/25 | Loss: 0.00099708
Iteration 21/25 | Loss: 0.00099453
Iteration 22/25 | Loss: 0.00099504
Iteration 23/25 | Loss: 0.00099770
Iteration 24/25 | Loss: 0.00099702
Iteration 25/25 | Loss: 0.00099597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47123373
Iteration 2/25 | Loss: 0.00090754
Iteration 3/25 | Loss: 0.00088068
Iteration 4/25 | Loss: 0.00088068
Iteration 5/25 | Loss: 0.00088067
Iteration 6/25 | Loss: 0.00088067
Iteration 7/25 | Loss: 0.00088067
Iteration 8/25 | Loss: 0.00088067
Iteration 9/25 | Loss: 0.00088067
Iteration 10/25 | Loss: 0.00088067
Iteration 11/25 | Loss: 0.00088067
Iteration 12/25 | Loss: 0.00088067
Iteration 13/25 | Loss: 0.00088067
Iteration 14/25 | Loss: 0.00088067
Iteration 15/25 | Loss: 0.00088067
Iteration 16/25 | Loss: 0.00088067
Iteration 17/25 | Loss: 0.00088067
Iteration 18/25 | Loss: 0.00088067
Iteration 19/25 | Loss: 0.00088067
Iteration 20/25 | Loss: 0.00088067
Iteration 21/25 | Loss: 0.00088067
Iteration 22/25 | Loss: 0.00088067
Iteration 23/25 | Loss: 0.00088067
Iteration 24/25 | Loss: 0.00088067
Iteration 25/25 | Loss: 0.00088067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088067
Iteration 2/1000 | Loss: 0.00005858
Iteration 3/1000 | Loss: 0.00005775
Iteration 4/1000 | Loss: 0.00003303
Iteration 5/1000 | Loss: 0.00005171
Iteration 6/1000 | Loss: 0.00005286
Iteration 7/1000 | Loss: 0.00004907
Iteration 8/1000 | Loss: 0.00005595
Iteration 9/1000 | Loss: 0.00003437
Iteration 10/1000 | Loss: 0.00002545
Iteration 11/1000 | Loss: 0.00002237
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00005776
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00003909
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00003813
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00002363
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00004416
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001564
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001317
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001317
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001317
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001297
Iteration 46/1000 | Loss: 0.00001297
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001297
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00002654
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001284
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001290
Iteration 93/1000 | Loss: 0.00001289
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.2785331819031853e-05, 1.2785331819031853e-05, 1.2785331819031853e-05, 1.2785331819031853e-05, 1.2785331819031853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2785331819031853e-05

Optimization complete. Final v2v error: 2.984811305999756 mm

Highest mean error: 4.561854362487793 mm for frame 67

Lowest mean error: 2.4219541549682617 mm for frame 107

Saving results

Total time: 105.87796354293823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892389
Iteration 2/25 | Loss: 0.00136755
Iteration 3/25 | Loss: 0.00110908
Iteration 4/25 | Loss: 0.00109326
Iteration 5/25 | Loss: 0.00108539
Iteration 6/25 | Loss: 0.00109044
Iteration 7/25 | Loss: 0.00108612
Iteration 8/25 | Loss: 0.00108456
Iteration 9/25 | Loss: 0.00107899
Iteration 10/25 | Loss: 0.00107860
Iteration 11/25 | Loss: 0.00107843
Iteration 12/25 | Loss: 0.00108266
Iteration 13/25 | Loss: 0.00108184
Iteration 14/25 | Loss: 0.00108256
Iteration 15/25 | Loss: 0.00108078
Iteration 16/25 | Loss: 0.00108201
Iteration 17/25 | Loss: 0.00108301
Iteration 18/25 | Loss: 0.00108129
Iteration 19/25 | Loss: 0.00107850
Iteration 20/25 | Loss: 0.00107840
Iteration 21/25 | Loss: 0.00107838
Iteration 22/25 | Loss: 0.00107838
Iteration 23/25 | Loss: 0.00107838
Iteration 24/25 | Loss: 0.00107838
Iteration 25/25 | Loss: 0.00107837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.37543488
Iteration 2/25 | Loss: 0.00060034
Iteration 3/25 | Loss: 0.00060023
Iteration 4/25 | Loss: 0.00060023
Iteration 5/25 | Loss: 0.00060023
Iteration 6/25 | Loss: 0.00060023
Iteration 7/25 | Loss: 0.00060023
Iteration 8/25 | Loss: 0.00060023
Iteration 9/25 | Loss: 0.00060023
Iteration 10/25 | Loss: 0.00060023
Iteration 11/25 | Loss: 0.00060023
Iteration 12/25 | Loss: 0.00060023
Iteration 13/25 | Loss: 0.00060023
Iteration 14/25 | Loss: 0.00060023
Iteration 15/25 | Loss: 0.00060023
Iteration 16/25 | Loss: 0.00060023
Iteration 17/25 | Loss: 0.00060023
Iteration 18/25 | Loss: 0.00060023
Iteration 19/25 | Loss: 0.00060023
Iteration 20/25 | Loss: 0.00060023
Iteration 21/25 | Loss: 0.00060023
Iteration 22/25 | Loss: 0.00060023
Iteration 23/25 | Loss: 0.00060023
Iteration 24/25 | Loss: 0.00060023
Iteration 25/25 | Loss: 0.00060023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060023
Iteration 2/1000 | Loss: 0.00019940
Iteration 3/1000 | Loss: 0.00003562
Iteration 4/1000 | Loss: 0.00020604
Iteration 5/1000 | Loss: 0.00004352
Iteration 6/1000 | Loss: 0.00003361
Iteration 7/1000 | Loss: 0.00002926
Iteration 8/1000 | Loss: 0.00002680
Iteration 9/1000 | Loss: 0.00002477
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002250
Iteration 12/1000 | Loss: 0.00002175
Iteration 13/1000 | Loss: 0.00002122
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00002064
Iteration 16/1000 | Loss: 0.00002033
Iteration 17/1000 | Loss: 0.00002013
Iteration 18/1000 | Loss: 0.00002008
Iteration 19/1000 | Loss: 0.00002008
Iteration 20/1000 | Loss: 0.00002005
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00002004
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001995
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001994
Iteration 35/1000 | Loss: 0.00001994
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001994
Iteration 39/1000 | Loss: 0.00001994
Iteration 40/1000 | Loss: 0.00001994
Iteration 41/1000 | Loss: 0.00001993
Iteration 42/1000 | Loss: 0.00001993
Iteration 43/1000 | Loss: 0.00001993
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001992
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001990
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001984
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001982
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001979
Iteration 77/1000 | Loss: 0.00001979
Iteration 78/1000 | Loss: 0.00001979
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001978
Iteration 81/1000 | Loss: 0.00001978
Iteration 82/1000 | Loss: 0.00001978
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001978
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001977
Iteration 93/1000 | Loss: 0.00001977
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00001977
Iteration 96/1000 | Loss: 0.00001977
Iteration 97/1000 | Loss: 0.00001977
Iteration 98/1000 | Loss: 0.00001977
Iteration 99/1000 | Loss: 0.00001977
Iteration 100/1000 | Loss: 0.00001977
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001976
Iteration 104/1000 | Loss: 0.00001976
Iteration 105/1000 | Loss: 0.00001976
Iteration 106/1000 | Loss: 0.00001976
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001975
Iteration 120/1000 | Loss: 0.00001975
Iteration 121/1000 | Loss: 0.00001975
Iteration 122/1000 | Loss: 0.00001975
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001975
Iteration 125/1000 | Loss: 0.00001975
Iteration 126/1000 | Loss: 0.00001975
Iteration 127/1000 | Loss: 0.00001975
Iteration 128/1000 | Loss: 0.00001975
Iteration 129/1000 | Loss: 0.00001975
Iteration 130/1000 | Loss: 0.00001975
Iteration 131/1000 | Loss: 0.00001975
Iteration 132/1000 | Loss: 0.00001975
Iteration 133/1000 | Loss: 0.00001975
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001975
Iteration 136/1000 | Loss: 0.00001975
Iteration 137/1000 | Loss: 0.00001975
Iteration 138/1000 | Loss: 0.00001975
Iteration 139/1000 | Loss: 0.00001975
Iteration 140/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.9754304958041757e-05, 1.9754304958041757e-05, 1.9754304958041757e-05, 1.9754304958041757e-05, 1.9754304958041757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9754304958041757e-05

Optimization complete. Final v2v error: 3.699054718017578 mm

Highest mean error: 4.784794807434082 mm for frame 30

Lowest mean error: 3.0550403594970703 mm for frame 206

Saving results

Total time: 77.55238962173462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987369
Iteration 2/25 | Loss: 0.00210679
Iteration 3/25 | Loss: 0.00159436
Iteration 4/25 | Loss: 0.00136078
Iteration 5/25 | Loss: 0.00144367
Iteration 6/25 | Loss: 0.00124633
Iteration 7/25 | Loss: 0.00124069
Iteration 8/25 | Loss: 0.00113405
Iteration 9/25 | Loss: 0.00116582
Iteration 10/25 | Loss: 0.00113655
Iteration 11/25 | Loss: 0.00115227
Iteration 12/25 | Loss: 0.00112686
Iteration 13/25 | Loss: 0.00114314
Iteration 14/25 | Loss: 0.00108199
Iteration 15/25 | Loss: 0.00106981
Iteration 16/25 | Loss: 0.00106479
Iteration 17/25 | Loss: 0.00106403
Iteration 18/25 | Loss: 0.00106387
Iteration 19/25 | Loss: 0.00106377
Iteration 20/25 | Loss: 0.00106376
Iteration 21/25 | Loss: 0.00106376
Iteration 22/25 | Loss: 0.00106376
Iteration 23/25 | Loss: 0.00106376
Iteration 24/25 | Loss: 0.00106376
Iteration 25/25 | Loss: 0.00106376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80964279
Iteration 2/25 | Loss: 0.00127347
Iteration 3/25 | Loss: 0.00127347
Iteration 4/25 | Loss: 0.00127347
Iteration 5/25 | Loss: 0.00127346
Iteration 6/25 | Loss: 0.00127346
Iteration 7/25 | Loss: 0.00127346
Iteration 8/25 | Loss: 0.00127346
Iteration 9/25 | Loss: 0.00127346
Iteration 10/25 | Loss: 0.00127346
Iteration 11/25 | Loss: 0.00127346
Iteration 12/25 | Loss: 0.00127346
Iteration 13/25 | Loss: 0.00127346
Iteration 14/25 | Loss: 0.00127346
Iteration 15/25 | Loss: 0.00127346
Iteration 16/25 | Loss: 0.00127346
Iteration 17/25 | Loss: 0.00127346
Iteration 18/25 | Loss: 0.00127346
Iteration 19/25 | Loss: 0.00127346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001273461850360036, 0.001273461850360036, 0.001273461850360036, 0.001273461850360036, 0.001273461850360036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273461850360036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127346
Iteration 2/1000 | Loss: 0.00012535
Iteration 3/1000 | Loss: 0.00019055
Iteration 4/1000 | Loss: 0.00005949
Iteration 5/1000 | Loss: 0.00005094
Iteration 6/1000 | Loss: 0.00004053
Iteration 7/1000 | Loss: 0.00011767
Iteration 8/1000 | Loss: 0.00023730
Iteration 9/1000 | Loss: 0.00003444
Iteration 10/1000 | Loss: 0.00003052
Iteration 11/1000 | Loss: 0.00044004
Iteration 12/1000 | Loss: 0.00004397
Iteration 13/1000 | Loss: 0.00002699
Iteration 14/1000 | Loss: 0.00002220
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00001327
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001323
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001319
Iteration 44/1000 | Loss: 0.00001319
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001318
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001314
Iteration 52/1000 | Loss: 0.00001314
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001312
Iteration 66/1000 | Loss: 0.00001312
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001311
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001311
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001310
Iteration 89/1000 | Loss: 0.00001310
Iteration 90/1000 | Loss: 0.00001310
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001309
Iteration 93/1000 | Loss: 0.00001309
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001304
Iteration 117/1000 | Loss: 0.00001304
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001303
Iteration 122/1000 | Loss: 0.00001303
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001302
Iteration 134/1000 | Loss: 0.00001302
Iteration 135/1000 | Loss: 0.00001302
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001302
Iteration 141/1000 | Loss: 0.00001302
Iteration 142/1000 | Loss: 0.00001302
Iteration 143/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3020318874623626e-05, 1.3020318874623626e-05, 1.3020318874623626e-05, 1.3020318874623626e-05, 1.3020318874623626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3020318874623626e-05

Optimization complete. Final v2v error: 3.021407127380371 mm

Highest mean error: 4.303924560546875 mm for frame 63

Lowest mean error: 2.677546501159668 mm for frame 107

Saving results

Total time: 73.26241183280945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480843
Iteration 2/25 | Loss: 0.00118440
Iteration 3/25 | Loss: 0.00105430
Iteration 4/25 | Loss: 0.00103454
Iteration 5/25 | Loss: 0.00102953
Iteration 6/25 | Loss: 0.00102856
Iteration 7/25 | Loss: 0.00102829
Iteration 8/25 | Loss: 0.00102829
Iteration 9/25 | Loss: 0.00102829
Iteration 10/25 | Loss: 0.00102829
Iteration 11/25 | Loss: 0.00102829
Iteration 12/25 | Loss: 0.00102829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010282931616529822, 0.0010282931616529822, 0.0010282931616529822, 0.0010282931616529822, 0.0010282931616529822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010282931616529822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39882636
Iteration 2/25 | Loss: 0.00068860
Iteration 3/25 | Loss: 0.00068860
Iteration 4/25 | Loss: 0.00068860
Iteration 5/25 | Loss: 0.00068860
Iteration 6/25 | Loss: 0.00068860
Iteration 7/25 | Loss: 0.00068860
Iteration 8/25 | Loss: 0.00068860
Iteration 9/25 | Loss: 0.00068860
Iteration 10/25 | Loss: 0.00068860
Iteration 11/25 | Loss: 0.00068860
Iteration 12/25 | Loss: 0.00068860
Iteration 13/25 | Loss: 0.00068860
Iteration 14/25 | Loss: 0.00068860
Iteration 15/25 | Loss: 0.00068860
Iteration 16/25 | Loss: 0.00068860
Iteration 17/25 | Loss: 0.00068860
Iteration 18/25 | Loss: 0.00068860
Iteration 19/25 | Loss: 0.00068860
Iteration 20/25 | Loss: 0.00068860
Iteration 21/25 | Loss: 0.00068860
Iteration 22/25 | Loss: 0.00068860
Iteration 23/25 | Loss: 0.00068860
Iteration 24/25 | Loss: 0.00068860
Iteration 25/25 | Loss: 0.00068860

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068860
Iteration 2/1000 | Loss: 0.00003688
Iteration 3/1000 | Loss: 0.00002597
Iteration 4/1000 | Loss: 0.00002297
Iteration 5/1000 | Loss: 0.00002204
Iteration 6/1000 | Loss: 0.00002141
Iteration 7/1000 | Loss: 0.00002092
Iteration 8/1000 | Loss: 0.00002038
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00001991
Iteration 11/1000 | Loss: 0.00001968
Iteration 12/1000 | Loss: 0.00001952
Iteration 13/1000 | Loss: 0.00001940
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001923
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001921
Iteration 20/1000 | Loss: 0.00001919
Iteration 21/1000 | Loss: 0.00001917
Iteration 22/1000 | Loss: 0.00001917
Iteration 23/1000 | Loss: 0.00001916
Iteration 24/1000 | Loss: 0.00001916
Iteration 25/1000 | Loss: 0.00001915
Iteration 26/1000 | Loss: 0.00001913
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001901
Iteration 40/1000 | Loss: 0.00001900
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001900
Iteration 43/1000 | Loss: 0.00001900
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001898
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001893
Iteration 57/1000 | Loss: 0.00001893
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001892
Iteration 60/1000 | Loss: 0.00001892
Iteration 61/1000 | Loss: 0.00001892
Iteration 62/1000 | Loss: 0.00001892
Iteration 63/1000 | Loss: 0.00001891
Iteration 64/1000 | Loss: 0.00001891
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001890
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001889
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001888
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001884
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001882
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001882
Iteration 123/1000 | Loss: 0.00001882
Iteration 124/1000 | Loss: 0.00001882
Iteration 125/1000 | Loss: 0.00001882
Iteration 126/1000 | Loss: 0.00001882
Iteration 127/1000 | Loss: 0.00001882
Iteration 128/1000 | Loss: 0.00001881
Iteration 129/1000 | Loss: 0.00001881
Iteration 130/1000 | Loss: 0.00001881
Iteration 131/1000 | Loss: 0.00001881
Iteration 132/1000 | Loss: 0.00001881
Iteration 133/1000 | Loss: 0.00001881
Iteration 134/1000 | Loss: 0.00001881
Iteration 135/1000 | Loss: 0.00001881
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001881
Iteration 144/1000 | Loss: 0.00001881
Iteration 145/1000 | Loss: 0.00001881
Iteration 146/1000 | Loss: 0.00001881
Iteration 147/1000 | Loss: 0.00001881
Iteration 148/1000 | Loss: 0.00001880
Iteration 149/1000 | Loss: 0.00001880
Iteration 150/1000 | Loss: 0.00001880
Iteration 151/1000 | Loss: 0.00001880
Iteration 152/1000 | Loss: 0.00001880
Iteration 153/1000 | Loss: 0.00001880
Iteration 154/1000 | Loss: 0.00001880
Iteration 155/1000 | Loss: 0.00001880
Iteration 156/1000 | Loss: 0.00001880
Iteration 157/1000 | Loss: 0.00001880
Iteration 158/1000 | Loss: 0.00001880
Iteration 159/1000 | Loss: 0.00001880
Iteration 160/1000 | Loss: 0.00001879
Iteration 161/1000 | Loss: 0.00001879
Iteration 162/1000 | Loss: 0.00001879
Iteration 163/1000 | Loss: 0.00001879
Iteration 164/1000 | Loss: 0.00001879
Iteration 165/1000 | Loss: 0.00001879
Iteration 166/1000 | Loss: 0.00001879
Iteration 167/1000 | Loss: 0.00001879
Iteration 168/1000 | Loss: 0.00001879
Iteration 169/1000 | Loss: 0.00001879
Iteration 170/1000 | Loss: 0.00001879
Iteration 171/1000 | Loss: 0.00001879
Iteration 172/1000 | Loss: 0.00001879
Iteration 173/1000 | Loss: 0.00001879
Iteration 174/1000 | Loss: 0.00001879
Iteration 175/1000 | Loss: 0.00001879
Iteration 176/1000 | Loss: 0.00001879
Iteration 177/1000 | Loss: 0.00001879
Iteration 178/1000 | Loss: 0.00001879
Iteration 179/1000 | Loss: 0.00001879
Iteration 180/1000 | Loss: 0.00001879
Iteration 181/1000 | Loss: 0.00001879
Iteration 182/1000 | Loss: 0.00001879
Iteration 183/1000 | Loss: 0.00001879
Iteration 184/1000 | Loss: 0.00001879
Iteration 185/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.8787532098940574e-05, 1.8787532098940574e-05, 1.8787532098940574e-05, 1.8787532098940574e-05, 1.8787532098940574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8787532098940574e-05

Optimization complete. Final v2v error: 3.5645248889923096 mm

Highest mean error: 5.256283283233643 mm for frame 39

Lowest mean error: 3.191638231277466 mm for frame 20

Saving results

Total time: 43.6300311088562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470313
Iteration 2/25 | Loss: 0.00103102
Iteration 3/25 | Loss: 0.00096756
Iteration 4/25 | Loss: 0.00095878
Iteration 5/25 | Loss: 0.00095538
Iteration 6/25 | Loss: 0.00095459
Iteration 7/25 | Loss: 0.00095459
Iteration 8/25 | Loss: 0.00095459
Iteration 9/25 | Loss: 0.00095459
Iteration 10/25 | Loss: 0.00095459
Iteration 11/25 | Loss: 0.00095459
Iteration 12/25 | Loss: 0.00095459
Iteration 13/25 | Loss: 0.00095459
Iteration 14/25 | Loss: 0.00095459
Iteration 15/25 | Loss: 0.00095459
Iteration 16/25 | Loss: 0.00095459
Iteration 17/25 | Loss: 0.00095459
Iteration 18/25 | Loss: 0.00095459
Iteration 19/25 | Loss: 0.00095459
Iteration 20/25 | Loss: 0.00095459
Iteration 21/25 | Loss: 0.00095459
Iteration 22/25 | Loss: 0.00095459
Iteration 23/25 | Loss: 0.00095459
Iteration 24/25 | Loss: 0.00095459
Iteration 25/25 | Loss: 0.00095459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70574903
Iteration 2/25 | Loss: 0.00068852
Iteration 3/25 | Loss: 0.00068851
Iteration 4/25 | Loss: 0.00068851
Iteration 5/25 | Loss: 0.00068851
Iteration 6/25 | Loss: 0.00068851
Iteration 7/25 | Loss: 0.00068851
Iteration 8/25 | Loss: 0.00068851
Iteration 9/25 | Loss: 0.00068851
Iteration 10/25 | Loss: 0.00068851
Iteration 11/25 | Loss: 0.00068851
Iteration 12/25 | Loss: 0.00068851
Iteration 13/25 | Loss: 0.00068851
Iteration 14/25 | Loss: 0.00068851
Iteration 15/25 | Loss: 0.00068851
Iteration 16/25 | Loss: 0.00068851
Iteration 17/25 | Loss: 0.00068851
Iteration 18/25 | Loss: 0.00068851
Iteration 19/25 | Loss: 0.00068851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006885088514536619, 0.0006885088514536619, 0.0006885088514536619, 0.0006885088514536619, 0.0006885088514536619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006885088514536619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068851
Iteration 2/1000 | Loss: 0.00002432
Iteration 3/1000 | Loss: 0.00001482
Iteration 4/1000 | Loss: 0.00001162
Iteration 5/1000 | Loss: 0.00001091
Iteration 6/1000 | Loss: 0.00001040
Iteration 7/1000 | Loss: 0.00001007
Iteration 8/1000 | Loss: 0.00000976
Iteration 9/1000 | Loss: 0.00000960
Iteration 10/1000 | Loss: 0.00000950
Iteration 11/1000 | Loss: 0.00000928
Iteration 12/1000 | Loss: 0.00000917
Iteration 13/1000 | Loss: 0.00000913
Iteration 14/1000 | Loss: 0.00000911
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000907
Iteration 17/1000 | Loss: 0.00000905
Iteration 18/1000 | Loss: 0.00000902
Iteration 19/1000 | Loss: 0.00000902
Iteration 20/1000 | Loss: 0.00000901
Iteration 21/1000 | Loss: 0.00000901
Iteration 22/1000 | Loss: 0.00000900
Iteration 23/1000 | Loss: 0.00000900
Iteration 24/1000 | Loss: 0.00000900
Iteration 25/1000 | Loss: 0.00000899
Iteration 26/1000 | Loss: 0.00000896
Iteration 27/1000 | Loss: 0.00000895
Iteration 28/1000 | Loss: 0.00000895
Iteration 29/1000 | Loss: 0.00000894
Iteration 30/1000 | Loss: 0.00000894
Iteration 31/1000 | Loss: 0.00000894
Iteration 32/1000 | Loss: 0.00000893
Iteration 33/1000 | Loss: 0.00000893
Iteration 34/1000 | Loss: 0.00000892
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000892
Iteration 37/1000 | Loss: 0.00000891
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000890
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000889
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000889
Iteration 44/1000 | Loss: 0.00000889
Iteration 45/1000 | Loss: 0.00000889
Iteration 46/1000 | Loss: 0.00000889
Iteration 47/1000 | Loss: 0.00000889
Iteration 48/1000 | Loss: 0.00000889
Iteration 49/1000 | Loss: 0.00000888
Iteration 50/1000 | Loss: 0.00000887
Iteration 51/1000 | Loss: 0.00000887
Iteration 52/1000 | Loss: 0.00000886
Iteration 53/1000 | Loss: 0.00000886
Iteration 54/1000 | Loss: 0.00000885
Iteration 55/1000 | Loss: 0.00000885
Iteration 56/1000 | Loss: 0.00000885
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000884
Iteration 59/1000 | Loss: 0.00000884
Iteration 60/1000 | Loss: 0.00000883
Iteration 61/1000 | Loss: 0.00000882
Iteration 62/1000 | Loss: 0.00000882
Iteration 63/1000 | Loss: 0.00000882
Iteration 64/1000 | Loss: 0.00000882
Iteration 65/1000 | Loss: 0.00000881
Iteration 66/1000 | Loss: 0.00000881
Iteration 67/1000 | Loss: 0.00000881
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000879
Iteration 70/1000 | Loss: 0.00000879
Iteration 71/1000 | Loss: 0.00000879
Iteration 72/1000 | Loss: 0.00000878
Iteration 73/1000 | Loss: 0.00000878
Iteration 74/1000 | Loss: 0.00000878
Iteration 75/1000 | Loss: 0.00000878
Iteration 76/1000 | Loss: 0.00000878
Iteration 77/1000 | Loss: 0.00000878
Iteration 78/1000 | Loss: 0.00000878
Iteration 79/1000 | Loss: 0.00000878
Iteration 80/1000 | Loss: 0.00000878
Iteration 81/1000 | Loss: 0.00000878
Iteration 82/1000 | Loss: 0.00000878
Iteration 83/1000 | Loss: 0.00000878
Iteration 84/1000 | Loss: 0.00000878
Iteration 85/1000 | Loss: 0.00000878
Iteration 86/1000 | Loss: 0.00000878
Iteration 87/1000 | Loss: 0.00000878
Iteration 88/1000 | Loss: 0.00000878
Iteration 89/1000 | Loss: 0.00000878
Iteration 90/1000 | Loss: 0.00000878
Iteration 91/1000 | Loss: 0.00000878
Iteration 92/1000 | Loss: 0.00000878
Iteration 93/1000 | Loss: 0.00000878
Iteration 94/1000 | Loss: 0.00000878
Iteration 95/1000 | Loss: 0.00000878
Iteration 96/1000 | Loss: 0.00000878
Iteration 97/1000 | Loss: 0.00000878
Iteration 98/1000 | Loss: 0.00000878
Iteration 99/1000 | Loss: 0.00000878
Iteration 100/1000 | Loss: 0.00000878
Iteration 101/1000 | Loss: 0.00000878
Iteration 102/1000 | Loss: 0.00000878
Iteration 103/1000 | Loss: 0.00000878
Iteration 104/1000 | Loss: 0.00000878
Iteration 105/1000 | Loss: 0.00000878
Iteration 106/1000 | Loss: 0.00000878
Iteration 107/1000 | Loss: 0.00000878
Iteration 108/1000 | Loss: 0.00000878
Iteration 109/1000 | Loss: 0.00000878
Iteration 110/1000 | Loss: 0.00000878
Iteration 111/1000 | Loss: 0.00000878
Iteration 112/1000 | Loss: 0.00000878
Iteration 113/1000 | Loss: 0.00000878
Iteration 114/1000 | Loss: 0.00000878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [8.779807103564963e-06, 8.779807103564963e-06, 8.779807103564963e-06, 8.779807103564963e-06, 8.779807103564963e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.779807103564963e-06

Optimization complete. Final v2v error: 2.5627546310424805 mm

Highest mean error: 2.903177261352539 mm for frame 76

Lowest mean error: 2.309654951095581 mm for frame 8

Saving results

Total time: 32.17622685432434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013423
Iteration 2/25 | Loss: 0.00169111
Iteration 3/25 | Loss: 0.00138806
Iteration 4/25 | Loss: 0.00126134
Iteration 5/25 | Loss: 0.00121599
Iteration 6/25 | Loss: 0.00122100
Iteration 7/25 | Loss: 0.00115047
Iteration 8/25 | Loss: 0.00110618
Iteration 9/25 | Loss: 0.00106641
Iteration 10/25 | Loss: 0.00106265
Iteration 11/25 | Loss: 0.00106339
Iteration 12/25 | Loss: 0.00105133
Iteration 13/25 | Loss: 0.00104571
Iteration 14/25 | Loss: 0.00103557
Iteration 15/25 | Loss: 0.00102769
Iteration 16/25 | Loss: 0.00102062
Iteration 17/25 | Loss: 0.00102159
Iteration 18/25 | Loss: 0.00101583
Iteration 19/25 | Loss: 0.00101575
Iteration 20/25 | Loss: 0.00101358
Iteration 21/25 | Loss: 0.00101497
Iteration 22/25 | Loss: 0.00101667
Iteration 23/25 | Loss: 0.00101360
Iteration 24/25 | Loss: 0.00101281
Iteration 25/25 | Loss: 0.00101622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39570284
Iteration 2/25 | Loss: 0.00092213
Iteration 3/25 | Loss: 0.00088994
Iteration 4/25 | Loss: 0.00088994
Iteration 5/25 | Loss: 0.00088994
Iteration 6/25 | Loss: 0.00088994
Iteration 7/25 | Loss: 0.00088994
Iteration 8/25 | Loss: 0.00088994
Iteration 9/25 | Loss: 0.00088994
Iteration 10/25 | Loss: 0.00088994
Iteration 11/25 | Loss: 0.00088994
Iteration 12/25 | Loss: 0.00088994
Iteration 13/25 | Loss: 0.00088994
Iteration 14/25 | Loss: 0.00088994
Iteration 15/25 | Loss: 0.00088994
Iteration 16/25 | Loss: 0.00088994
Iteration 17/25 | Loss: 0.00088994
Iteration 18/25 | Loss: 0.00088994
Iteration 19/25 | Loss: 0.00088994
Iteration 20/25 | Loss: 0.00088994
Iteration 21/25 | Loss: 0.00088994
Iteration 22/25 | Loss: 0.00088994
Iteration 23/25 | Loss: 0.00088994
Iteration 24/25 | Loss: 0.00088994
Iteration 25/25 | Loss: 0.00088994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088994
Iteration 2/1000 | Loss: 0.00015439
Iteration 3/1000 | Loss: 0.00014576
Iteration 4/1000 | Loss: 0.00021641
Iteration 5/1000 | Loss: 0.00012746
Iteration 6/1000 | Loss: 0.00006573
Iteration 7/1000 | Loss: 0.00011034
Iteration 8/1000 | Loss: 0.00016070
Iteration 9/1000 | Loss: 0.00028994
Iteration 10/1000 | Loss: 0.00018181
Iteration 11/1000 | Loss: 0.00004103
Iteration 12/1000 | Loss: 0.00004883
Iteration 13/1000 | Loss: 0.00030433
Iteration 14/1000 | Loss: 0.00063734
Iteration 15/1000 | Loss: 0.00009546
Iteration 16/1000 | Loss: 0.00016300
Iteration 17/1000 | Loss: 0.00013671
Iteration 18/1000 | Loss: 0.00029674
Iteration 19/1000 | Loss: 0.00015292
Iteration 20/1000 | Loss: 0.00022772
Iteration 21/1000 | Loss: 0.00023000
Iteration 22/1000 | Loss: 0.00022371
Iteration 23/1000 | Loss: 0.00023203
Iteration 24/1000 | Loss: 0.00019838
Iteration 25/1000 | Loss: 0.00025402
Iteration 26/1000 | Loss: 0.00026989
Iteration 27/1000 | Loss: 0.00005257
Iteration 28/1000 | Loss: 0.00004261
Iteration 29/1000 | Loss: 0.00004156
Iteration 30/1000 | Loss: 0.00018694
Iteration 31/1000 | Loss: 0.00004024
Iteration 32/1000 | Loss: 0.00004361
Iteration 33/1000 | Loss: 0.00003389
Iteration 34/1000 | Loss: 0.00036472
Iteration 35/1000 | Loss: 0.00036663
Iteration 36/1000 | Loss: 0.00035847
Iteration 37/1000 | Loss: 0.00004002
Iteration 38/1000 | Loss: 0.00002544
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00019717
Iteration 46/1000 | Loss: 0.00002026
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001392
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00008650
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001271
Iteration 60/1000 | Loss: 0.00001243
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001148
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00011478
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00007259
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001119
Iteration 93/1000 | Loss: 0.00001119
Iteration 94/1000 | Loss: 0.00001119
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001116
Iteration 116/1000 | Loss: 0.00001116
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001116
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001116
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001116
Iteration 127/1000 | Loss: 0.00001116
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001116
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001116
Iteration 140/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.1157295375596732e-05, 1.1157295375596732e-05, 1.1157295375596732e-05, 1.1157295375596732e-05, 1.1157295375596732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1157295375596732e-05

Optimization complete. Final v2v error: 2.7044754028320312 mm

Highest mean error: 4.792891979217529 mm for frame 44

Lowest mean error: 2.319528818130493 mm for frame 11

Saving results

Total time: 144.0099322795868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476080
Iteration 2/25 | Loss: 0.00108406
Iteration 3/25 | Loss: 0.00097181
Iteration 4/25 | Loss: 0.00095768
Iteration 5/25 | Loss: 0.00095471
Iteration 6/25 | Loss: 0.00095402
Iteration 7/25 | Loss: 0.00095402
Iteration 8/25 | Loss: 0.00095402
Iteration 9/25 | Loss: 0.00095402
Iteration 10/25 | Loss: 0.00095402
Iteration 11/25 | Loss: 0.00095402
Iteration 12/25 | Loss: 0.00095402
Iteration 13/25 | Loss: 0.00095402
Iteration 14/25 | Loss: 0.00095402
Iteration 15/25 | Loss: 0.00095402
Iteration 16/25 | Loss: 0.00095402
Iteration 17/25 | Loss: 0.00095402
Iteration 18/25 | Loss: 0.00095402
Iteration 19/25 | Loss: 0.00095402
Iteration 20/25 | Loss: 0.00095402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009540185565128922, 0.0009540185565128922, 0.0009540185565128922, 0.0009540185565128922, 0.0009540185565128922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009540185565128922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63344097
Iteration 2/25 | Loss: 0.00066437
Iteration 3/25 | Loss: 0.00066436
Iteration 4/25 | Loss: 0.00066436
Iteration 5/25 | Loss: 0.00066436
Iteration 6/25 | Loss: 0.00066436
Iteration 7/25 | Loss: 0.00066436
Iteration 8/25 | Loss: 0.00066436
Iteration 9/25 | Loss: 0.00066436
Iteration 10/25 | Loss: 0.00066436
Iteration 11/25 | Loss: 0.00066436
Iteration 12/25 | Loss: 0.00066436
Iteration 13/25 | Loss: 0.00066436
Iteration 14/25 | Loss: 0.00066436
Iteration 15/25 | Loss: 0.00066436
Iteration 16/25 | Loss: 0.00066436
Iteration 17/25 | Loss: 0.00066436
Iteration 18/25 | Loss: 0.00066436
Iteration 19/25 | Loss: 0.00066436
Iteration 20/25 | Loss: 0.00066436
Iteration 21/25 | Loss: 0.00066436
Iteration 22/25 | Loss: 0.00066436
Iteration 23/25 | Loss: 0.00066436
Iteration 24/25 | Loss: 0.00066436
Iteration 25/25 | Loss: 0.00066436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006643577944487333, 0.0006643577944487333, 0.0006643577944487333, 0.0006643577944487333, 0.0006643577944487333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006643577944487333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066436
Iteration 2/1000 | Loss: 0.00002402
Iteration 3/1000 | Loss: 0.00001607
Iteration 4/1000 | Loss: 0.00001263
Iteration 5/1000 | Loss: 0.00001176
Iteration 6/1000 | Loss: 0.00001109
Iteration 7/1000 | Loss: 0.00001068
Iteration 8/1000 | Loss: 0.00001017
Iteration 9/1000 | Loss: 0.00000995
Iteration 10/1000 | Loss: 0.00000990
Iteration 11/1000 | Loss: 0.00000983
Iteration 12/1000 | Loss: 0.00000965
Iteration 13/1000 | Loss: 0.00000961
Iteration 14/1000 | Loss: 0.00000961
Iteration 15/1000 | Loss: 0.00000957
Iteration 16/1000 | Loss: 0.00000956
Iteration 17/1000 | Loss: 0.00000956
Iteration 18/1000 | Loss: 0.00000956
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000956
Iteration 23/1000 | Loss: 0.00000955
Iteration 24/1000 | Loss: 0.00000955
Iteration 25/1000 | Loss: 0.00000955
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000947
Iteration 30/1000 | Loss: 0.00000942
Iteration 31/1000 | Loss: 0.00000941
Iteration 32/1000 | Loss: 0.00000940
Iteration 33/1000 | Loss: 0.00000939
Iteration 34/1000 | Loss: 0.00000938
Iteration 35/1000 | Loss: 0.00000938
Iteration 36/1000 | Loss: 0.00000937
Iteration 37/1000 | Loss: 0.00000936
Iteration 38/1000 | Loss: 0.00000936
Iteration 39/1000 | Loss: 0.00000935
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000933
Iteration 43/1000 | Loss: 0.00000933
Iteration 44/1000 | Loss: 0.00000932
Iteration 45/1000 | Loss: 0.00000932
Iteration 46/1000 | Loss: 0.00000932
Iteration 47/1000 | Loss: 0.00000932
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000931
Iteration 50/1000 | Loss: 0.00000931
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000930
Iteration 53/1000 | Loss: 0.00000930
Iteration 54/1000 | Loss: 0.00000929
Iteration 55/1000 | Loss: 0.00000929
Iteration 56/1000 | Loss: 0.00000929
Iteration 57/1000 | Loss: 0.00000928
Iteration 58/1000 | Loss: 0.00000928
Iteration 59/1000 | Loss: 0.00000928
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000927
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000927
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000927
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000925
Iteration 74/1000 | Loss: 0.00000925
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000922
Iteration 90/1000 | Loss: 0.00000922
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000920
Iteration 100/1000 | Loss: 0.00000920
Iteration 101/1000 | Loss: 0.00000920
Iteration 102/1000 | Loss: 0.00000920
Iteration 103/1000 | Loss: 0.00000920
Iteration 104/1000 | Loss: 0.00000920
Iteration 105/1000 | Loss: 0.00000920
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000919
Iteration 110/1000 | Loss: 0.00000919
Iteration 111/1000 | Loss: 0.00000919
Iteration 112/1000 | Loss: 0.00000919
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000918
Iteration 116/1000 | Loss: 0.00000918
Iteration 117/1000 | Loss: 0.00000918
Iteration 118/1000 | Loss: 0.00000918
Iteration 119/1000 | Loss: 0.00000918
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000917
Iteration 122/1000 | Loss: 0.00000917
Iteration 123/1000 | Loss: 0.00000917
Iteration 124/1000 | Loss: 0.00000917
Iteration 125/1000 | Loss: 0.00000917
Iteration 126/1000 | Loss: 0.00000917
Iteration 127/1000 | Loss: 0.00000917
Iteration 128/1000 | Loss: 0.00000916
Iteration 129/1000 | Loss: 0.00000916
Iteration 130/1000 | Loss: 0.00000916
Iteration 131/1000 | Loss: 0.00000916
Iteration 132/1000 | Loss: 0.00000916
Iteration 133/1000 | Loss: 0.00000916
Iteration 134/1000 | Loss: 0.00000915
Iteration 135/1000 | Loss: 0.00000915
Iteration 136/1000 | Loss: 0.00000915
Iteration 137/1000 | Loss: 0.00000915
Iteration 138/1000 | Loss: 0.00000915
Iteration 139/1000 | Loss: 0.00000915
Iteration 140/1000 | Loss: 0.00000914
Iteration 141/1000 | Loss: 0.00000914
Iteration 142/1000 | Loss: 0.00000914
Iteration 143/1000 | Loss: 0.00000914
Iteration 144/1000 | Loss: 0.00000914
Iteration 145/1000 | Loss: 0.00000914
Iteration 146/1000 | Loss: 0.00000914
Iteration 147/1000 | Loss: 0.00000914
Iteration 148/1000 | Loss: 0.00000914
Iteration 149/1000 | Loss: 0.00000914
Iteration 150/1000 | Loss: 0.00000914
Iteration 151/1000 | Loss: 0.00000913
Iteration 152/1000 | Loss: 0.00000913
Iteration 153/1000 | Loss: 0.00000913
Iteration 154/1000 | Loss: 0.00000913
Iteration 155/1000 | Loss: 0.00000912
Iteration 156/1000 | Loss: 0.00000912
Iteration 157/1000 | Loss: 0.00000912
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000910
Iteration 164/1000 | Loss: 0.00000910
Iteration 165/1000 | Loss: 0.00000909
Iteration 166/1000 | Loss: 0.00000909
Iteration 167/1000 | Loss: 0.00000909
Iteration 168/1000 | Loss: 0.00000909
Iteration 169/1000 | Loss: 0.00000909
Iteration 170/1000 | Loss: 0.00000909
Iteration 171/1000 | Loss: 0.00000909
Iteration 172/1000 | Loss: 0.00000909
Iteration 173/1000 | Loss: 0.00000909
Iteration 174/1000 | Loss: 0.00000909
Iteration 175/1000 | Loss: 0.00000909
Iteration 176/1000 | Loss: 0.00000908
Iteration 177/1000 | Loss: 0.00000908
Iteration 178/1000 | Loss: 0.00000908
Iteration 179/1000 | Loss: 0.00000907
Iteration 180/1000 | Loss: 0.00000907
Iteration 181/1000 | Loss: 0.00000907
Iteration 182/1000 | Loss: 0.00000907
Iteration 183/1000 | Loss: 0.00000907
Iteration 184/1000 | Loss: 0.00000907
Iteration 185/1000 | Loss: 0.00000907
Iteration 186/1000 | Loss: 0.00000907
Iteration 187/1000 | Loss: 0.00000907
Iteration 188/1000 | Loss: 0.00000907
Iteration 189/1000 | Loss: 0.00000907
Iteration 190/1000 | Loss: 0.00000906
Iteration 191/1000 | Loss: 0.00000906
Iteration 192/1000 | Loss: 0.00000906
Iteration 193/1000 | Loss: 0.00000906
Iteration 194/1000 | Loss: 0.00000906
Iteration 195/1000 | Loss: 0.00000906
Iteration 196/1000 | Loss: 0.00000906
Iteration 197/1000 | Loss: 0.00000906
Iteration 198/1000 | Loss: 0.00000906
Iteration 199/1000 | Loss: 0.00000905
Iteration 200/1000 | Loss: 0.00000905
Iteration 201/1000 | Loss: 0.00000905
Iteration 202/1000 | Loss: 0.00000905
Iteration 203/1000 | Loss: 0.00000905
Iteration 204/1000 | Loss: 0.00000905
Iteration 205/1000 | Loss: 0.00000905
Iteration 206/1000 | Loss: 0.00000905
Iteration 207/1000 | Loss: 0.00000904
Iteration 208/1000 | Loss: 0.00000904
Iteration 209/1000 | Loss: 0.00000904
Iteration 210/1000 | Loss: 0.00000904
Iteration 211/1000 | Loss: 0.00000904
Iteration 212/1000 | Loss: 0.00000904
Iteration 213/1000 | Loss: 0.00000904
Iteration 214/1000 | Loss: 0.00000904
Iteration 215/1000 | Loss: 0.00000904
Iteration 216/1000 | Loss: 0.00000904
Iteration 217/1000 | Loss: 0.00000903
Iteration 218/1000 | Loss: 0.00000903
Iteration 219/1000 | Loss: 0.00000903
Iteration 220/1000 | Loss: 0.00000903
Iteration 221/1000 | Loss: 0.00000903
Iteration 222/1000 | Loss: 0.00000903
Iteration 223/1000 | Loss: 0.00000903
Iteration 224/1000 | Loss: 0.00000903
Iteration 225/1000 | Loss: 0.00000903
Iteration 226/1000 | Loss: 0.00000903
Iteration 227/1000 | Loss: 0.00000903
Iteration 228/1000 | Loss: 0.00000902
Iteration 229/1000 | Loss: 0.00000902
Iteration 230/1000 | Loss: 0.00000902
Iteration 231/1000 | Loss: 0.00000902
Iteration 232/1000 | Loss: 0.00000902
Iteration 233/1000 | Loss: 0.00000902
Iteration 234/1000 | Loss: 0.00000902
Iteration 235/1000 | Loss: 0.00000902
Iteration 236/1000 | Loss: 0.00000902
Iteration 237/1000 | Loss: 0.00000902
Iteration 238/1000 | Loss: 0.00000902
Iteration 239/1000 | Loss: 0.00000901
Iteration 240/1000 | Loss: 0.00000901
Iteration 241/1000 | Loss: 0.00000901
Iteration 242/1000 | Loss: 0.00000901
Iteration 243/1000 | Loss: 0.00000901
Iteration 244/1000 | Loss: 0.00000901
Iteration 245/1000 | Loss: 0.00000901
Iteration 246/1000 | Loss: 0.00000901
Iteration 247/1000 | Loss: 0.00000901
Iteration 248/1000 | Loss: 0.00000901
Iteration 249/1000 | Loss: 0.00000901
Iteration 250/1000 | Loss: 0.00000901
Iteration 251/1000 | Loss: 0.00000901
Iteration 252/1000 | Loss: 0.00000901
Iteration 253/1000 | Loss: 0.00000901
Iteration 254/1000 | Loss: 0.00000901
Iteration 255/1000 | Loss: 0.00000901
Iteration 256/1000 | Loss: 0.00000901
Iteration 257/1000 | Loss: 0.00000901
Iteration 258/1000 | Loss: 0.00000901
Iteration 259/1000 | Loss: 0.00000901
Iteration 260/1000 | Loss: 0.00000901
Iteration 261/1000 | Loss: 0.00000901
Iteration 262/1000 | Loss: 0.00000901
Iteration 263/1000 | Loss: 0.00000901
Iteration 264/1000 | Loss: 0.00000900
Iteration 265/1000 | Loss: 0.00000900
Iteration 266/1000 | Loss: 0.00000900
Iteration 267/1000 | Loss: 0.00000900
Iteration 268/1000 | Loss: 0.00000900
Iteration 269/1000 | Loss: 0.00000900
Iteration 270/1000 | Loss: 0.00000900
Iteration 271/1000 | Loss: 0.00000900
Iteration 272/1000 | Loss: 0.00000900
Iteration 273/1000 | Loss: 0.00000900
Iteration 274/1000 | Loss: 0.00000899
Iteration 275/1000 | Loss: 0.00000899
Iteration 276/1000 | Loss: 0.00000899
Iteration 277/1000 | Loss: 0.00000899
Iteration 278/1000 | Loss: 0.00000899
Iteration 279/1000 | Loss: 0.00000899
Iteration 280/1000 | Loss: 0.00000899
Iteration 281/1000 | Loss: 0.00000899
Iteration 282/1000 | Loss: 0.00000899
Iteration 283/1000 | Loss: 0.00000899
Iteration 284/1000 | Loss: 0.00000899
Iteration 285/1000 | Loss: 0.00000899
Iteration 286/1000 | Loss: 0.00000898
Iteration 287/1000 | Loss: 0.00000898
Iteration 288/1000 | Loss: 0.00000898
Iteration 289/1000 | Loss: 0.00000898
Iteration 290/1000 | Loss: 0.00000898
Iteration 291/1000 | Loss: 0.00000898
Iteration 292/1000 | Loss: 0.00000898
Iteration 293/1000 | Loss: 0.00000898
Iteration 294/1000 | Loss: 0.00000898
Iteration 295/1000 | Loss: 0.00000898
Iteration 296/1000 | Loss: 0.00000898
Iteration 297/1000 | Loss: 0.00000898
Iteration 298/1000 | Loss: 0.00000898
Iteration 299/1000 | Loss: 0.00000898
Iteration 300/1000 | Loss: 0.00000898
Iteration 301/1000 | Loss: 0.00000898
Iteration 302/1000 | Loss: 0.00000898
Iteration 303/1000 | Loss: 0.00000898
Iteration 304/1000 | Loss: 0.00000898
Iteration 305/1000 | Loss: 0.00000897
Iteration 306/1000 | Loss: 0.00000897
Iteration 307/1000 | Loss: 0.00000897
Iteration 308/1000 | Loss: 0.00000897
Iteration 309/1000 | Loss: 0.00000897
Iteration 310/1000 | Loss: 0.00000897
Iteration 311/1000 | Loss: 0.00000897
Iteration 312/1000 | Loss: 0.00000897
Iteration 313/1000 | Loss: 0.00000897
Iteration 314/1000 | Loss: 0.00000897
Iteration 315/1000 | Loss: 0.00000897
Iteration 316/1000 | Loss: 0.00000897
Iteration 317/1000 | Loss: 0.00000897
Iteration 318/1000 | Loss: 0.00000897
Iteration 319/1000 | Loss: 0.00000897
Iteration 320/1000 | Loss: 0.00000897
Iteration 321/1000 | Loss: 0.00000896
Iteration 322/1000 | Loss: 0.00000896
Iteration 323/1000 | Loss: 0.00000896
Iteration 324/1000 | Loss: 0.00000896
Iteration 325/1000 | Loss: 0.00000896
Iteration 326/1000 | Loss: 0.00000896
Iteration 327/1000 | Loss: 0.00000896
Iteration 328/1000 | Loss: 0.00000896
Iteration 329/1000 | Loss: 0.00000896
Iteration 330/1000 | Loss: 0.00000896
Iteration 331/1000 | Loss: 0.00000896
Iteration 332/1000 | Loss: 0.00000896
Iteration 333/1000 | Loss: 0.00000896
Iteration 334/1000 | Loss: 0.00000896
Iteration 335/1000 | Loss: 0.00000896
Iteration 336/1000 | Loss: 0.00000896
Iteration 337/1000 | Loss: 0.00000896
Iteration 338/1000 | Loss: 0.00000896
Iteration 339/1000 | Loss: 0.00000896
Iteration 340/1000 | Loss: 0.00000896
Iteration 341/1000 | Loss: 0.00000896
Iteration 342/1000 | Loss: 0.00000896
Iteration 343/1000 | Loss: 0.00000896
Iteration 344/1000 | Loss: 0.00000896
Iteration 345/1000 | Loss: 0.00000896
Iteration 346/1000 | Loss: 0.00000896
Iteration 347/1000 | Loss: 0.00000896
Iteration 348/1000 | Loss: 0.00000896
Iteration 349/1000 | Loss: 0.00000896
Iteration 350/1000 | Loss: 0.00000896
Iteration 351/1000 | Loss: 0.00000896
Iteration 352/1000 | Loss: 0.00000896
Iteration 353/1000 | Loss: 0.00000896
Iteration 354/1000 | Loss: 0.00000896
Iteration 355/1000 | Loss: 0.00000896
Iteration 356/1000 | Loss: 0.00000896
Iteration 357/1000 | Loss: 0.00000896
Iteration 358/1000 | Loss: 0.00000896
Iteration 359/1000 | Loss: 0.00000896
Iteration 360/1000 | Loss: 0.00000896
Iteration 361/1000 | Loss: 0.00000896
Iteration 362/1000 | Loss: 0.00000896
Iteration 363/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 363. Stopping optimization.
Last 5 losses: [8.95818266144488e-06, 8.95818266144488e-06, 8.95818266144488e-06, 8.95818266144488e-06, 8.95818266144488e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.95818266144488e-06

Optimization complete. Final v2v error: 2.5723023414611816 mm

Highest mean error: 3.0825111865997314 mm for frame 62

Lowest mean error: 2.2810754776000977 mm for frame 132

Saving results

Total time: 47.4503116607666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918667
Iteration 2/25 | Loss: 0.00118337
Iteration 3/25 | Loss: 0.00104509
Iteration 4/25 | Loss: 0.00102085
Iteration 5/25 | Loss: 0.00101251
Iteration 6/25 | Loss: 0.00101010
Iteration 7/25 | Loss: 0.00100990
Iteration 8/25 | Loss: 0.00100990
Iteration 9/25 | Loss: 0.00100990
Iteration 10/25 | Loss: 0.00100990
Iteration 11/25 | Loss: 0.00100990
Iteration 12/25 | Loss: 0.00100990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010098981438204646, 0.0010098981438204646, 0.0010098981438204646, 0.0010098981438204646, 0.0010098981438204646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010098981438204646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38074768
Iteration 2/25 | Loss: 0.00071487
Iteration 3/25 | Loss: 0.00071486
Iteration 4/25 | Loss: 0.00071485
Iteration 5/25 | Loss: 0.00071485
Iteration 6/25 | Loss: 0.00071485
Iteration 7/25 | Loss: 0.00071485
Iteration 8/25 | Loss: 0.00071485
Iteration 9/25 | Loss: 0.00071485
Iteration 10/25 | Loss: 0.00071485
Iteration 11/25 | Loss: 0.00071485
Iteration 12/25 | Loss: 0.00071485
Iteration 13/25 | Loss: 0.00071485
Iteration 14/25 | Loss: 0.00071485
Iteration 15/25 | Loss: 0.00071485
Iteration 16/25 | Loss: 0.00071485
Iteration 17/25 | Loss: 0.00071485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007148530567064881, 0.0007148530567064881, 0.0007148530567064881, 0.0007148530567064881, 0.0007148530567064881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007148530567064881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071485
Iteration 2/1000 | Loss: 0.00003334
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001866
Iteration 6/1000 | Loss: 0.00001806
Iteration 7/1000 | Loss: 0.00001759
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001655
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001649
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001645
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001641
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001626
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001617
Iteration 28/1000 | Loss: 0.00001617
Iteration 29/1000 | Loss: 0.00001617
Iteration 30/1000 | Loss: 0.00001617
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001610
Iteration 48/1000 | Loss: 0.00001609
Iteration 49/1000 | Loss: 0.00001609
Iteration 50/1000 | Loss: 0.00001609
Iteration 51/1000 | Loss: 0.00001609
Iteration 52/1000 | Loss: 0.00001609
Iteration 53/1000 | Loss: 0.00001609
Iteration 54/1000 | Loss: 0.00001608
Iteration 55/1000 | Loss: 0.00001608
Iteration 56/1000 | Loss: 0.00001608
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001607
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001606
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001606
Iteration 70/1000 | Loss: 0.00001605
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001604
Iteration 75/1000 | Loss: 0.00001604
Iteration 76/1000 | Loss: 0.00001604
Iteration 77/1000 | Loss: 0.00001604
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001601
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001601
Iteration 103/1000 | Loss: 0.00001601
Iteration 104/1000 | Loss: 0.00001601
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001601
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001599
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001598
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001597
Iteration 144/1000 | Loss: 0.00001597
Iteration 145/1000 | Loss: 0.00001597
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001596
Iteration 148/1000 | Loss: 0.00001596
Iteration 149/1000 | Loss: 0.00001596
Iteration 150/1000 | Loss: 0.00001596
Iteration 151/1000 | Loss: 0.00001596
Iteration 152/1000 | Loss: 0.00001596
Iteration 153/1000 | Loss: 0.00001596
Iteration 154/1000 | Loss: 0.00001596
Iteration 155/1000 | Loss: 0.00001596
Iteration 156/1000 | Loss: 0.00001595
Iteration 157/1000 | Loss: 0.00001595
Iteration 158/1000 | Loss: 0.00001595
Iteration 159/1000 | Loss: 0.00001595
Iteration 160/1000 | Loss: 0.00001594
Iteration 161/1000 | Loss: 0.00001594
Iteration 162/1000 | Loss: 0.00001594
Iteration 163/1000 | Loss: 0.00001594
Iteration 164/1000 | Loss: 0.00001594
Iteration 165/1000 | Loss: 0.00001594
Iteration 166/1000 | Loss: 0.00001594
Iteration 167/1000 | Loss: 0.00001594
Iteration 168/1000 | Loss: 0.00001594
Iteration 169/1000 | Loss: 0.00001594
Iteration 170/1000 | Loss: 0.00001594
Iteration 171/1000 | Loss: 0.00001594
Iteration 172/1000 | Loss: 0.00001594
Iteration 173/1000 | Loss: 0.00001594
Iteration 174/1000 | Loss: 0.00001594
Iteration 175/1000 | Loss: 0.00001594
Iteration 176/1000 | Loss: 0.00001594
Iteration 177/1000 | Loss: 0.00001594
Iteration 178/1000 | Loss: 0.00001594
Iteration 179/1000 | Loss: 0.00001594
Iteration 180/1000 | Loss: 0.00001594
Iteration 181/1000 | Loss: 0.00001594
Iteration 182/1000 | Loss: 0.00001594
Iteration 183/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.594346213096287e-05, 1.594346213096287e-05, 1.594346213096287e-05, 1.594346213096287e-05, 1.594346213096287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.594346213096287e-05

Optimization complete. Final v2v error: 3.294813632965088 mm

Highest mean error: 5.321316719055176 mm for frame 70

Lowest mean error: 2.774533271789551 mm for frame 95

Saving results

Total time: 38.66595125198364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047093
Iteration 2/25 | Loss: 0.00339454
Iteration 3/25 | Loss: 0.00175860
Iteration 4/25 | Loss: 0.00148817
Iteration 5/25 | Loss: 0.00139979
Iteration 6/25 | Loss: 0.00145484
Iteration 7/25 | Loss: 0.00131968
Iteration 8/25 | Loss: 0.00119873
Iteration 9/25 | Loss: 0.00114696
Iteration 10/25 | Loss: 0.00112814
Iteration 11/25 | Loss: 0.00110630
Iteration 12/25 | Loss: 0.00109885
Iteration 13/25 | Loss: 0.00109424
Iteration 14/25 | Loss: 0.00109084
Iteration 15/25 | Loss: 0.00108919
Iteration 16/25 | Loss: 0.00108819
Iteration 17/25 | Loss: 0.00108549
Iteration 18/25 | Loss: 0.00108113
Iteration 19/25 | Loss: 0.00108551
Iteration 20/25 | Loss: 0.00108730
Iteration 21/25 | Loss: 0.00108064
Iteration 22/25 | Loss: 0.00108245
Iteration 23/25 | Loss: 0.00108111
Iteration 24/25 | Loss: 0.00107775
Iteration 25/25 | Loss: 0.00107699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73466575
Iteration 2/25 | Loss: 0.00112062
Iteration 3/25 | Loss: 0.00110829
Iteration 4/25 | Loss: 0.00110829
Iteration 5/25 | Loss: 0.00110829
Iteration 6/25 | Loss: 0.00110829
Iteration 7/25 | Loss: 0.00110829
Iteration 8/25 | Loss: 0.00110829
Iteration 9/25 | Loss: 0.00110829
Iteration 10/25 | Loss: 0.00110829
Iteration 11/25 | Loss: 0.00110829
Iteration 12/25 | Loss: 0.00110829
Iteration 13/25 | Loss: 0.00110829
Iteration 14/25 | Loss: 0.00110829
Iteration 15/25 | Loss: 0.00110829
Iteration 16/25 | Loss: 0.00110829
Iteration 17/25 | Loss: 0.00110829
Iteration 18/25 | Loss: 0.00110829
Iteration 19/25 | Loss: 0.00110829
Iteration 20/25 | Loss: 0.00110829
Iteration 21/25 | Loss: 0.00110829
Iteration 22/25 | Loss: 0.00110829
Iteration 23/25 | Loss: 0.00110829
Iteration 24/25 | Loss: 0.00110829
Iteration 25/25 | Loss: 0.00110829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110829
Iteration 2/1000 | Loss: 0.00012717
Iteration 3/1000 | Loss: 0.00008779
Iteration 4/1000 | Loss: 0.00045143
Iteration 5/1000 | Loss: 0.00045879
Iteration 6/1000 | Loss: 0.00038349
Iteration 7/1000 | Loss: 0.00062111
Iteration 8/1000 | Loss: 0.00032500
Iteration 9/1000 | Loss: 0.00017565
Iteration 10/1000 | Loss: 0.00032154
Iteration 11/1000 | Loss: 0.00023387
Iteration 12/1000 | Loss: 0.00016250
Iteration 13/1000 | Loss: 0.00013638
Iteration 14/1000 | Loss: 0.00009663
Iteration 15/1000 | Loss: 0.00008358
Iteration 16/1000 | Loss: 0.00010266
Iteration 17/1000 | Loss: 0.00005929
Iteration 18/1000 | Loss: 0.00006574
Iteration 19/1000 | Loss: 0.00006288
Iteration 20/1000 | Loss: 0.00005041
Iteration 21/1000 | Loss: 0.00013425
Iteration 22/1000 | Loss: 0.00005259
Iteration 23/1000 | Loss: 0.00006735
Iteration 24/1000 | Loss: 0.00197526
Iteration 25/1000 | Loss: 0.00220227
Iteration 26/1000 | Loss: 0.00018092
Iteration 27/1000 | Loss: 0.00024153
Iteration 28/1000 | Loss: 0.00009828
Iteration 29/1000 | Loss: 0.00015790
Iteration 30/1000 | Loss: 0.00007186
Iteration 31/1000 | Loss: 0.00005019
Iteration 32/1000 | Loss: 0.00003184
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00004515
Iteration 35/1000 | Loss: 0.00001742
Iteration 36/1000 | Loss: 0.00002219
Iteration 37/1000 | Loss: 0.00002749
Iteration 38/1000 | Loss: 0.00002875
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001690
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00003137
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001178
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001161
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001160
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001156
Iteration 84/1000 | Loss: 0.00001156
Iteration 85/1000 | Loss: 0.00001156
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001155
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001154
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001152
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001150
Iteration 112/1000 | Loss: 0.00001150
Iteration 113/1000 | Loss: 0.00001150
Iteration 114/1000 | Loss: 0.00001150
Iteration 115/1000 | Loss: 0.00001150
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001149
Iteration 122/1000 | Loss: 0.00001149
Iteration 123/1000 | Loss: 0.00001149
Iteration 124/1000 | Loss: 0.00001149
Iteration 125/1000 | Loss: 0.00001149
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001149
Iteration 129/1000 | Loss: 0.00001149
Iteration 130/1000 | Loss: 0.00001149
Iteration 131/1000 | Loss: 0.00001149
Iteration 132/1000 | Loss: 0.00001149
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001148
Iteration 141/1000 | Loss: 0.00001148
Iteration 142/1000 | Loss: 0.00001148
Iteration 143/1000 | Loss: 0.00001148
Iteration 144/1000 | Loss: 0.00001148
Iteration 145/1000 | Loss: 0.00001148
Iteration 146/1000 | Loss: 0.00001148
Iteration 147/1000 | Loss: 0.00001148
Iteration 148/1000 | Loss: 0.00001148
Iteration 149/1000 | Loss: 0.00001148
Iteration 150/1000 | Loss: 0.00001148
Iteration 151/1000 | Loss: 0.00001147
Iteration 152/1000 | Loss: 0.00001147
Iteration 153/1000 | Loss: 0.00001147
Iteration 154/1000 | Loss: 0.00001147
Iteration 155/1000 | Loss: 0.00001147
Iteration 156/1000 | Loss: 0.00001147
Iteration 157/1000 | Loss: 0.00001147
Iteration 158/1000 | Loss: 0.00001147
Iteration 159/1000 | Loss: 0.00001147
Iteration 160/1000 | Loss: 0.00001147
Iteration 161/1000 | Loss: 0.00001147
Iteration 162/1000 | Loss: 0.00001146
Iteration 163/1000 | Loss: 0.00001146
Iteration 164/1000 | Loss: 0.00001146
Iteration 165/1000 | Loss: 0.00001146
Iteration 166/1000 | Loss: 0.00001146
Iteration 167/1000 | Loss: 0.00001146
Iteration 168/1000 | Loss: 0.00001146
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Iteration 173/1000 | Loss: 0.00001145
Iteration 174/1000 | Loss: 0.00001145
Iteration 175/1000 | Loss: 0.00001145
Iteration 176/1000 | Loss: 0.00001145
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.1454468221927527e-05, 1.1454468221927527e-05, 1.1454468221927527e-05, 1.1454468221927527e-05, 1.1454468221927527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1454468221927527e-05

Optimization complete. Final v2v error: 2.8384406566619873 mm

Highest mean error: 4.194960594177246 mm for frame 78

Lowest mean error: 2.4901483058929443 mm for frame 23

Saving results

Total time: 122.20722484588623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806782
Iteration 2/25 | Loss: 0.00186029
Iteration 3/25 | Loss: 0.00132773
Iteration 4/25 | Loss: 0.00128957
Iteration 5/25 | Loss: 0.00132899
Iteration 6/25 | Loss: 0.00119570
Iteration 7/25 | Loss: 0.00115371
Iteration 8/25 | Loss: 0.00121661
Iteration 9/25 | Loss: 0.00111391
Iteration 10/25 | Loss: 0.00110284
Iteration 11/25 | Loss: 0.00109933
Iteration 12/25 | Loss: 0.00109853
Iteration 13/25 | Loss: 0.00109845
Iteration 14/25 | Loss: 0.00109845
Iteration 15/25 | Loss: 0.00109845
Iteration 16/25 | Loss: 0.00109844
Iteration 17/25 | Loss: 0.00109844
Iteration 18/25 | Loss: 0.00109844
Iteration 19/25 | Loss: 0.00109844
Iteration 20/25 | Loss: 0.00109844
Iteration 21/25 | Loss: 0.00109844
Iteration 22/25 | Loss: 0.00109844
Iteration 23/25 | Loss: 0.00109844
Iteration 24/25 | Loss: 0.00109844
Iteration 25/25 | Loss: 0.00109844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53046131
Iteration 2/25 | Loss: 0.00074265
Iteration 3/25 | Loss: 0.00074265
Iteration 4/25 | Loss: 0.00074265
Iteration 5/25 | Loss: 0.00074265
Iteration 6/25 | Loss: 0.00074265
Iteration 7/25 | Loss: 0.00074265
Iteration 8/25 | Loss: 0.00074265
Iteration 9/25 | Loss: 0.00074265
Iteration 10/25 | Loss: 0.00074265
Iteration 11/25 | Loss: 0.00074265
Iteration 12/25 | Loss: 0.00074265
Iteration 13/25 | Loss: 0.00074264
Iteration 14/25 | Loss: 0.00074264
Iteration 15/25 | Loss: 0.00074264
Iteration 16/25 | Loss: 0.00074264
Iteration 17/25 | Loss: 0.00074264
Iteration 18/25 | Loss: 0.00074264
Iteration 19/25 | Loss: 0.00074264
Iteration 20/25 | Loss: 0.00074264
Iteration 21/25 | Loss: 0.00074264
Iteration 22/25 | Loss: 0.00074264
Iteration 23/25 | Loss: 0.00074264
Iteration 24/25 | Loss: 0.00074264
Iteration 25/25 | Loss: 0.00074264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074264
Iteration 2/1000 | Loss: 0.00005119
Iteration 3/1000 | Loss: 0.00003201
Iteration 4/1000 | Loss: 0.00002639
Iteration 5/1000 | Loss: 0.00002382
Iteration 6/1000 | Loss: 0.00002242
Iteration 7/1000 | Loss: 0.00002150
Iteration 8/1000 | Loss: 0.00002087
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002013
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00002001
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001993
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001967
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001964
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001958
Iteration 24/1000 | Loss: 0.00001956
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001954
Iteration 28/1000 | Loss: 0.00001953
Iteration 29/1000 | Loss: 0.00001952
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001948
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001943
Iteration 39/1000 | Loss: 0.00001943
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001943
Iteration 42/1000 | Loss: 0.00001943
Iteration 43/1000 | Loss: 0.00001943
Iteration 44/1000 | Loss: 0.00001943
Iteration 45/1000 | Loss: 0.00001943
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001940
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001939
Iteration 66/1000 | Loss: 0.00001939
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001934
Iteration 97/1000 | Loss: 0.00001934
Iteration 98/1000 | Loss: 0.00001933
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001933
Iteration 102/1000 | Loss: 0.00001933
Iteration 103/1000 | Loss: 0.00001933
Iteration 104/1000 | Loss: 0.00001933
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001933
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001932
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001931
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001930
Iteration 124/1000 | Loss: 0.00001930
Iteration 125/1000 | Loss: 0.00001930
Iteration 126/1000 | Loss: 0.00001930
Iteration 127/1000 | Loss: 0.00001930
Iteration 128/1000 | Loss: 0.00001930
Iteration 129/1000 | Loss: 0.00001930
Iteration 130/1000 | Loss: 0.00001930
Iteration 131/1000 | Loss: 0.00001930
Iteration 132/1000 | Loss: 0.00001930
Iteration 133/1000 | Loss: 0.00001929
Iteration 134/1000 | Loss: 0.00001929
Iteration 135/1000 | Loss: 0.00001929
Iteration 136/1000 | Loss: 0.00001929
Iteration 137/1000 | Loss: 0.00001929
Iteration 138/1000 | Loss: 0.00001929
Iteration 139/1000 | Loss: 0.00001929
Iteration 140/1000 | Loss: 0.00001929
Iteration 141/1000 | Loss: 0.00001929
Iteration 142/1000 | Loss: 0.00001929
Iteration 143/1000 | Loss: 0.00001929
Iteration 144/1000 | Loss: 0.00001929
Iteration 145/1000 | Loss: 0.00001929
Iteration 146/1000 | Loss: 0.00001929
Iteration 147/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.929321479110513e-05, 1.929321479110513e-05, 1.929321479110513e-05, 1.929321479110513e-05, 1.929321479110513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.929321479110513e-05

Optimization complete. Final v2v error: 3.5617189407348633 mm

Highest mean error: 4.341898441314697 mm for frame 41

Lowest mean error: 2.648253917694092 mm for frame 2

Saving results

Total time: 50.08783936500549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829270
Iteration 2/25 | Loss: 0.00109189
Iteration 3/25 | Loss: 0.00096157
Iteration 4/25 | Loss: 0.00094830
Iteration 5/25 | Loss: 0.00094564
Iteration 6/25 | Loss: 0.00094499
Iteration 7/25 | Loss: 0.00094499
Iteration 8/25 | Loss: 0.00094499
Iteration 9/25 | Loss: 0.00094499
Iteration 10/25 | Loss: 0.00094499
Iteration 11/25 | Loss: 0.00094499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009449929930269718, 0.0009449929930269718, 0.0009449929930269718, 0.0009449929930269718, 0.0009449929930269718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009449929930269718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38214409
Iteration 2/25 | Loss: 0.00057622
Iteration 3/25 | Loss: 0.00057620
Iteration 4/25 | Loss: 0.00057620
Iteration 5/25 | Loss: 0.00057619
Iteration 6/25 | Loss: 0.00057619
Iteration 7/25 | Loss: 0.00057619
Iteration 8/25 | Loss: 0.00057619
Iteration 9/25 | Loss: 0.00057619
Iteration 10/25 | Loss: 0.00057619
Iteration 11/25 | Loss: 0.00057619
Iteration 12/25 | Loss: 0.00057619
Iteration 13/25 | Loss: 0.00057619
Iteration 14/25 | Loss: 0.00057619
Iteration 15/25 | Loss: 0.00057619
Iteration 16/25 | Loss: 0.00057619
Iteration 17/25 | Loss: 0.00057619
Iteration 18/25 | Loss: 0.00057619
Iteration 19/25 | Loss: 0.00057619
Iteration 20/25 | Loss: 0.00057619
Iteration 21/25 | Loss: 0.00057619
Iteration 22/25 | Loss: 0.00057619
Iteration 23/25 | Loss: 0.00057619
Iteration 24/25 | Loss: 0.00057619
Iteration 25/25 | Loss: 0.00057619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057619
Iteration 2/1000 | Loss: 0.00001734
Iteration 3/1000 | Loss: 0.00001035
Iteration 4/1000 | Loss: 0.00000874
Iteration 5/1000 | Loss: 0.00000815
Iteration 6/1000 | Loss: 0.00000762
Iteration 7/1000 | Loss: 0.00000727
Iteration 8/1000 | Loss: 0.00000718
Iteration 9/1000 | Loss: 0.00000714
Iteration 10/1000 | Loss: 0.00000713
Iteration 11/1000 | Loss: 0.00000713
Iteration 12/1000 | Loss: 0.00000712
Iteration 13/1000 | Loss: 0.00000712
Iteration 14/1000 | Loss: 0.00000711
Iteration 15/1000 | Loss: 0.00000711
Iteration 16/1000 | Loss: 0.00000710
Iteration 17/1000 | Loss: 0.00000708
Iteration 18/1000 | Loss: 0.00000708
Iteration 19/1000 | Loss: 0.00000703
Iteration 20/1000 | Loss: 0.00000697
Iteration 21/1000 | Loss: 0.00000691
Iteration 22/1000 | Loss: 0.00000691
Iteration 23/1000 | Loss: 0.00000689
Iteration 24/1000 | Loss: 0.00000687
Iteration 25/1000 | Loss: 0.00000686
Iteration 26/1000 | Loss: 0.00000686
Iteration 27/1000 | Loss: 0.00000686
Iteration 28/1000 | Loss: 0.00000685
Iteration 29/1000 | Loss: 0.00000684
Iteration 30/1000 | Loss: 0.00000684
Iteration 31/1000 | Loss: 0.00000683
Iteration 32/1000 | Loss: 0.00000683
Iteration 33/1000 | Loss: 0.00000683
Iteration 34/1000 | Loss: 0.00000681
Iteration 35/1000 | Loss: 0.00000681
Iteration 36/1000 | Loss: 0.00000681
Iteration 37/1000 | Loss: 0.00000681
Iteration 38/1000 | Loss: 0.00000681
Iteration 39/1000 | Loss: 0.00000681
Iteration 40/1000 | Loss: 0.00000681
Iteration 41/1000 | Loss: 0.00000680
Iteration 42/1000 | Loss: 0.00000680
Iteration 43/1000 | Loss: 0.00000680
Iteration 44/1000 | Loss: 0.00000680
Iteration 45/1000 | Loss: 0.00000679
Iteration 46/1000 | Loss: 0.00000679
Iteration 47/1000 | Loss: 0.00000679
Iteration 48/1000 | Loss: 0.00000678
Iteration 49/1000 | Loss: 0.00000678
Iteration 50/1000 | Loss: 0.00000677
Iteration 51/1000 | Loss: 0.00000677
Iteration 52/1000 | Loss: 0.00000677
Iteration 53/1000 | Loss: 0.00000677
Iteration 54/1000 | Loss: 0.00000676
Iteration 55/1000 | Loss: 0.00000676
Iteration 56/1000 | Loss: 0.00000676
Iteration 57/1000 | Loss: 0.00000676
Iteration 58/1000 | Loss: 0.00000676
Iteration 59/1000 | Loss: 0.00000675
Iteration 60/1000 | Loss: 0.00000675
Iteration 61/1000 | Loss: 0.00000674
Iteration 62/1000 | Loss: 0.00000674
Iteration 63/1000 | Loss: 0.00000673
Iteration 64/1000 | Loss: 0.00000673
Iteration 65/1000 | Loss: 0.00000673
Iteration 66/1000 | Loss: 0.00000672
Iteration 67/1000 | Loss: 0.00000672
Iteration 68/1000 | Loss: 0.00000671
Iteration 69/1000 | Loss: 0.00000671
Iteration 70/1000 | Loss: 0.00000671
Iteration 71/1000 | Loss: 0.00000671
Iteration 72/1000 | Loss: 0.00000671
Iteration 73/1000 | Loss: 0.00000670
Iteration 74/1000 | Loss: 0.00000670
Iteration 75/1000 | Loss: 0.00000669
Iteration 76/1000 | Loss: 0.00000669
Iteration 77/1000 | Loss: 0.00000669
Iteration 78/1000 | Loss: 0.00000667
Iteration 79/1000 | Loss: 0.00000667
Iteration 80/1000 | Loss: 0.00000667
Iteration 81/1000 | Loss: 0.00000667
Iteration 82/1000 | Loss: 0.00000667
Iteration 83/1000 | Loss: 0.00000667
Iteration 84/1000 | Loss: 0.00000667
Iteration 85/1000 | Loss: 0.00000667
Iteration 86/1000 | Loss: 0.00000667
Iteration 87/1000 | Loss: 0.00000666
Iteration 88/1000 | Loss: 0.00000666
Iteration 89/1000 | Loss: 0.00000666
Iteration 90/1000 | Loss: 0.00000665
Iteration 91/1000 | Loss: 0.00000665
Iteration 92/1000 | Loss: 0.00000665
Iteration 93/1000 | Loss: 0.00000665
Iteration 94/1000 | Loss: 0.00000665
Iteration 95/1000 | Loss: 0.00000665
Iteration 96/1000 | Loss: 0.00000665
Iteration 97/1000 | Loss: 0.00000665
Iteration 98/1000 | Loss: 0.00000664
Iteration 99/1000 | Loss: 0.00000664
Iteration 100/1000 | Loss: 0.00000664
Iteration 101/1000 | Loss: 0.00000664
Iteration 102/1000 | Loss: 0.00000664
Iteration 103/1000 | Loss: 0.00000663
Iteration 104/1000 | Loss: 0.00000663
Iteration 105/1000 | Loss: 0.00000663
Iteration 106/1000 | Loss: 0.00000663
Iteration 107/1000 | Loss: 0.00000663
Iteration 108/1000 | Loss: 0.00000663
Iteration 109/1000 | Loss: 0.00000663
Iteration 110/1000 | Loss: 0.00000663
Iteration 111/1000 | Loss: 0.00000663
Iteration 112/1000 | Loss: 0.00000663
Iteration 113/1000 | Loss: 0.00000663
Iteration 114/1000 | Loss: 0.00000663
Iteration 115/1000 | Loss: 0.00000663
Iteration 116/1000 | Loss: 0.00000663
Iteration 117/1000 | Loss: 0.00000663
Iteration 118/1000 | Loss: 0.00000663
Iteration 119/1000 | Loss: 0.00000662
Iteration 120/1000 | Loss: 0.00000662
Iteration 121/1000 | Loss: 0.00000662
Iteration 122/1000 | Loss: 0.00000662
Iteration 123/1000 | Loss: 0.00000662
Iteration 124/1000 | Loss: 0.00000662
Iteration 125/1000 | Loss: 0.00000662
Iteration 126/1000 | Loss: 0.00000662
Iteration 127/1000 | Loss: 0.00000662
Iteration 128/1000 | Loss: 0.00000662
Iteration 129/1000 | Loss: 0.00000662
Iteration 130/1000 | Loss: 0.00000662
Iteration 131/1000 | Loss: 0.00000662
Iteration 132/1000 | Loss: 0.00000662
Iteration 133/1000 | Loss: 0.00000662
Iteration 134/1000 | Loss: 0.00000662
Iteration 135/1000 | Loss: 0.00000662
Iteration 136/1000 | Loss: 0.00000662
Iteration 137/1000 | Loss: 0.00000662
Iteration 138/1000 | Loss: 0.00000662
Iteration 139/1000 | Loss: 0.00000662
Iteration 140/1000 | Loss: 0.00000662
Iteration 141/1000 | Loss: 0.00000662
Iteration 142/1000 | Loss: 0.00000662
Iteration 143/1000 | Loss: 0.00000662
Iteration 144/1000 | Loss: 0.00000662
Iteration 145/1000 | Loss: 0.00000662
Iteration 146/1000 | Loss: 0.00000662
Iteration 147/1000 | Loss: 0.00000662
Iteration 148/1000 | Loss: 0.00000662
Iteration 149/1000 | Loss: 0.00000662
Iteration 150/1000 | Loss: 0.00000662
Iteration 151/1000 | Loss: 0.00000662
Iteration 152/1000 | Loss: 0.00000662
Iteration 153/1000 | Loss: 0.00000662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [6.615122401854023e-06, 6.615122401854023e-06, 6.615122401854023e-06, 6.615122401854023e-06, 6.615122401854023e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.615122401854023e-06

Optimization complete. Final v2v error: 2.2220046520233154 mm

Highest mean error: 2.6103246212005615 mm for frame 0

Lowest mean error: 2.09110951423645 mm for frame 97

Saving results

Total time: 31.70537257194519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977935
Iteration 2/25 | Loss: 0.00319276
Iteration 3/25 | Loss: 0.00244642
Iteration 4/25 | Loss: 0.00209095
Iteration 5/25 | Loss: 0.00192884
Iteration 6/25 | Loss: 0.00179904
Iteration 7/25 | Loss: 0.00179515
Iteration 8/25 | Loss: 0.00179182
Iteration 9/25 | Loss: 0.00179345
Iteration 10/25 | Loss: 0.00178139
Iteration 11/25 | Loss: 0.00178451
Iteration 12/25 | Loss: 0.00177727
Iteration 13/25 | Loss: 0.00177574
Iteration 14/25 | Loss: 0.00177947
Iteration 15/25 | Loss: 0.00177197
Iteration 16/25 | Loss: 0.00176524
Iteration 17/25 | Loss: 0.00176261
Iteration 18/25 | Loss: 0.00176750
Iteration 19/25 | Loss: 0.00176055
Iteration 20/25 | Loss: 0.00175969
Iteration 21/25 | Loss: 0.00175963
Iteration 22/25 | Loss: 0.00175963
Iteration 23/25 | Loss: 0.00175963
Iteration 24/25 | Loss: 0.00175962
Iteration 25/25 | Loss: 0.00175962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31969130
Iteration 2/25 | Loss: 0.00730918
Iteration 3/25 | Loss: 0.00438594
Iteration 4/25 | Loss: 0.00433386
Iteration 5/25 | Loss: 0.00433386
Iteration 6/25 | Loss: 0.00433386
Iteration 7/25 | Loss: 0.00433386
Iteration 8/25 | Loss: 0.00433386
Iteration 9/25 | Loss: 0.00433386
Iteration 10/25 | Loss: 0.00433386
Iteration 11/25 | Loss: 0.00433386
Iteration 12/25 | Loss: 0.00433386
Iteration 13/25 | Loss: 0.00433386
Iteration 14/25 | Loss: 0.00433386
Iteration 15/25 | Loss: 0.00433386
Iteration 16/25 | Loss: 0.00433386
Iteration 17/25 | Loss: 0.00433386
Iteration 18/25 | Loss: 0.00433386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004333855584263802, 0.004333855584263802, 0.004333855584263802, 0.004333855584263802, 0.004333855584263802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004333855584263802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00433386
Iteration 2/1000 | Loss: 0.00584986
Iteration 3/1000 | Loss: 0.00077032
Iteration 4/1000 | Loss: 0.00098649
Iteration 5/1000 | Loss: 0.00686228
Iteration 6/1000 | Loss: 0.00081864
Iteration 7/1000 | Loss: 0.00284649
Iteration 8/1000 | Loss: 0.00042262
Iteration 9/1000 | Loss: 0.00038347
Iteration 10/1000 | Loss: 0.00035848
Iteration 11/1000 | Loss: 0.00034157
Iteration 12/1000 | Loss: 0.00092209
Iteration 13/1000 | Loss: 0.01651756
Iteration 14/1000 | Loss: 0.00606311
Iteration 15/1000 | Loss: 0.00056348
Iteration 16/1000 | Loss: 0.00175132
Iteration 17/1000 | Loss: 0.00147987
Iteration 18/1000 | Loss: 0.00024168
Iteration 19/1000 | Loss: 0.00145435
Iteration 20/1000 | Loss: 0.00044945
Iteration 21/1000 | Loss: 0.00010142
Iteration 22/1000 | Loss: 0.00034370
Iteration 23/1000 | Loss: 0.00013092
Iteration 24/1000 | Loss: 0.00040226
Iteration 25/1000 | Loss: 0.00005415
Iteration 26/1000 | Loss: 0.00081266
Iteration 27/1000 | Loss: 0.00273085
Iteration 28/1000 | Loss: 0.00004118
Iteration 29/1000 | Loss: 0.00004451
Iteration 30/1000 | Loss: 0.00039351
Iteration 31/1000 | Loss: 0.00908707
Iteration 32/1000 | Loss: 0.00224984
Iteration 33/1000 | Loss: 0.00054893
Iteration 34/1000 | Loss: 0.00044557
Iteration 35/1000 | Loss: 0.00348756
Iteration 36/1000 | Loss: 0.00163117
Iteration 37/1000 | Loss: 0.00059852
Iteration 38/1000 | Loss: 0.00064643
Iteration 39/1000 | Loss: 0.00042182
Iteration 40/1000 | Loss: 0.00021151
Iteration 41/1000 | Loss: 0.00004672
Iteration 42/1000 | Loss: 0.00045727
Iteration 43/1000 | Loss: 0.00007054
Iteration 44/1000 | Loss: 0.00002534
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00038425
Iteration 47/1000 | Loss: 0.00003390
Iteration 48/1000 | Loss: 0.00015104
Iteration 49/1000 | Loss: 0.00021595
Iteration 50/1000 | Loss: 0.00028210
Iteration 51/1000 | Loss: 0.00004085
Iteration 52/1000 | Loss: 0.00009973
Iteration 53/1000 | Loss: 0.00017507
Iteration 54/1000 | Loss: 0.00120948
Iteration 55/1000 | Loss: 0.00002900
Iteration 56/1000 | Loss: 0.00008883
Iteration 57/1000 | Loss: 0.00001378
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00027733
Iteration 61/1000 | Loss: 0.00007758
Iteration 62/1000 | Loss: 0.00004536
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00007547
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00009997
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00003171
Iteration 82/1000 | Loss: 0.00001804
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00004156
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001181
Iteration 149/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1813324817921966e-05, 1.1813324817921966e-05, 1.1813324817921966e-05, 1.1813324817921966e-05, 1.1813324817921966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1813324817921966e-05

Optimization complete. Final v2v error: 2.9682469367980957 mm

Highest mean error: 3.4678430557250977 mm for frame 71

Lowest mean error: 2.767305850982666 mm for frame 5

Saving results

Total time: 149.546462059021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075233
Iteration 2/25 | Loss: 0.00184788
Iteration 3/25 | Loss: 0.00122384
Iteration 4/25 | Loss: 0.00110182
Iteration 5/25 | Loss: 0.00105852
Iteration 6/25 | Loss: 0.00108862
Iteration 7/25 | Loss: 0.00106372
Iteration 8/25 | Loss: 0.00105893
Iteration 9/25 | Loss: 0.00103927
Iteration 10/25 | Loss: 0.00102320
Iteration 11/25 | Loss: 0.00102452
Iteration 12/25 | Loss: 0.00102130
Iteration 13/25 | Loss: 0.00101154
Iteration 14/25 | Loss: 0.00101527
Iteration 15/25 | Loss: 0.00101402
Iteration 16/25 | Loss: 0.00100753
Iteration 17/25 | Loss: 0.00100564
Iteration 18/25 | Loss: 0.00100635
Iteration 19/25 | Loss: 0.00101067
Iteration 20/25 | Loss: 0.00100595
Iteration 21/25 | Loss: 0.00100639
Iteration 22/25 | Loss: 0.00100515
Iteration 23/25 | Loss: 0.00101234
Iteration 24/25 | Loss: 0.00101264
Iteration 25/25 | Loss: 0.00101544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.53814411
Iteration 2/25 | Loss: 0.00098432
Iteration 3/25 | Loss: 0.00098432
Iteration 4/25 | Loss: 0.00098432
Iteration 5/25 | Loss: 0.00098431
Iteration 6/25 | Loss: 0.00098431
Iteration 7/25 | Loss: 0.00093093
Iteration 8/25 | Loss: 0.00093093
Iteration 9/25 | Loss: 0.00093093
Iteration 10/25 | Loss: 0.00093093
Iteration 11/25 | Loss: 0.00093093
Iteration 12/25 | Loss: 0.00093093
Iteration 13/25 | Loss: 0.00093093
Iteration 14/25 | Loss: 0.00093093
Iteration 15/25 | Loss: 0.00093093
Iteration 16/25 | Loss: 0.00093093
Iteration 17/25 | Loss: 0.00093093
Iteration 18/25 | Loss: 0.00093093
Iteration 19/25 | Loss: 0.00093093
Iteration 20/25 | Loss: 0.00093093
Iteration 21/25 | Loss: 0.00093093
Iteration 22/25 | Loss: 0.00093093
Iteration 23/25 | Loss: 0.00093093
Iteration 24/25 | Loss: 0.00093093
Iteration 25/25 | Loss: 0.00093093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009309317101724446, 0.0009309317101724446, 0.0009309317101724446, 0.0009309317101724446, 0.0009309317101724446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009309317101724446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093093
Iteration 2/1000 | Loss: 0.00034562
Iteration 3/1000 | Loss: 0.00025758
Iteration 4/1000 | Loss: 0.00014180
Iteration 5/1000 | Loss: 0.00026143
Iteration 6/1000 | Loss: 0.00023665
Iteration 7/1000 | Loss: 0.00029901
Iteration 8/1000 | Loss: 0.00028340
Iteration 9/1000 | Loss: 0.00033504
Iteration 10/1000 | Loss: 0.00017237
Iteration 11/1000 | Loss: 0.00018268
Iteration 12/1000 | Loss: 0.00022290
Iteration 13/1000 | Loss: 0.00016614
Iteration 14/1000 | Loss: 0.00013483
Iteration 15/1000 | Loss: 0.00021945
Iteration 16/1000 | Loss: 0.00021881
Iteration 17/1000 | Loss: 0.00013607
Iteration 18/1000 | Loss: 0.00015752
Iteration 19/1000 | Loss: 0.00022085
Iteration 20/1000 | Loss: 0.00005028
Iteration 21/1000 | Loss: 0.00019286
Iteration 22/1000 | Loss: 0.00015427
Iteration 23/1000 | Loss: 0.00019079
Iteration 24/1000 | Loss: 0.00004990
Iteration 25/1000 | Loss: 0.00003021
Iteration 26/1000 | Loss: 0.00019175
Iteration 27/1000 | Loss: 0.00014619
Iteration 28/1000 | Loss: 0.00009304
Iteration 29/1000 | Loss: 0.00010343
Iteration 30/1000 | Loss: 0.00008009
Iteration 31/1000 | Loss: 0.00012413
Iteration 32/1000 | Loss: 0.00012102
Iteration 33/1000 | Loss: 0.00031673
Iteration 34/1000 | Loss: 0.00011934
Iteration 35/1000 | Loss: 0.00018106
Iteration 36/1000 | Loss: 0.00015528
Iteration 37/1000 | Loss: 0.00033586
Iteration 38/1000 | Loss: 0.00024359
Iteration 39/1000 | Loss: 0.00018861
Iteration 40/1000 | Loss: 0.00016830
Iteration 41/1000 | Loss: 0.00021472
Iteration 42/1000 | Loss: 0.00030748
Iteration 43/1000 | Loss: 0.00068974
Iteration 44/1000 | Loss: 0.00047362
Iteration 45/1000 | Loss: 0.00020848
Iteration 46/1000 | Loss: 0.00014602
Iteration 47/1000 | Loss: 0.00018582
Iteration 48/1000 | Loss: 0.00027053
Iteration 49/1000 | Loss: 0.00015360
Iteration 50/1000 | Loss: 0.00018172
Iteration 51/1000 | Loss: 0.00014557
Iteration 52/1000 | Loss: 0.00017262
Iteration 53/1000 | Loss: 0.00013988
Iteration 54/1000 | Loss: 0.00015548
Iteration 55/1000 | Loss: 0.00014500
Iteration 56/1000 | Loss: 0.00013273
Iteration 57/1000 | Loss: 0.00016738
Iteration 58/1000 | Loss: 0.00017560
Iteration 59/1000 | Loss: 0.00015053
Iteration 60/1000 | Loss: 0.00002425
Iteration 61/1000 | Loss: 0.00010875
Iteration 62/1000 | Loss: 0.00012381
Iteration 63/1000 | Loss: 0.00012309
Iteration 64/1000 | Loss: 0.00008236
Iteration 65/1000 | Loss: 0.00006039
Iteration 66/1000 | Loss: 0.00005114
Iteration 67/1000 | Loss: 0.00030206
Iteration 68/1000 | Loss: 0.00023203
Iteration 69/1000 | Loss: 0.00016948
Iteration 70/1000 | Loss: 0.00016543
Iteration 71/1000 | Loss: 0.00009675
Iteration 72/1000 | Loss: 0.00009005
Iteration 73/1000 | Loss: 0.00010266
Iteration 74/1000 | Loss: 0.00014650
Iteration 75/1000 | Loss: 0.00011824
Iteration 76/1000 | Loss: 0.00036464
Iteration 77/1000 | Loss: 0.00023508
Iteration 78/1000 | Loss: 0.00024419
Iteration 79/1000 | Loss: 0.00018949
Iteration 80/1000 | Loss: 0.00057630
Iteration 81/1000 | Loss: 0.00012638
Iteration 82/1000 | Loss: 0.00018065
Iteration 83/1000 | Loss: 0.00016274
Iteration 84/1000 | Loss: 0.00013914
Iteration 85/1000 | Loss: 0.00026548
Iteration 86/1000 | Loss: 0.00014029
Iteration 87/1000 | Loss: 0.00004200
Iteration 88/1000 | Loss: 0.00015544
Iteration 89/1000 | Loss: 0.00024734
Iteration 90/1000 | Loss: 0.00014505
Iteration 91/1000 | Loss: 0.00021687
Iteration 92/1000 | Loss: 0.00007795
Iteration 93/1000 | Loss: 0.00014190
Iteration 94/1000 | Loss: 0.00019153
Iteration 95/1000 | Loss: 0.00009603
Iteration 96/1000 | Loss: 0.00006872
Iteration 97/1000 | Loss: 0.00016302
Iteration 98/1000 | Loss: 0.00012092
Iteration 99/1000 | Loss: 0.00011107
Iteration 100/1000 | Loss: 0.00010708
Iteration 101/1000 | Loss: 0.00006296
Iteration 102/1000 | Loss: 0.00010594
Iteration 103/1000 | Loss: 0.00018917
Iteration 104/1000 | Loss: 0.00007866
Iteration 105/1000 | Loss: 0.00027205
Iteration 106/1000 | Loss: 0.00003493
Iteration 107/1000 | Loss: 0.00016186
Iteration 108/1000 | Loss: 0.00016262
Iteration 109/1000 | Loss: 0.00026564
Iteration 110/1000 | Loss: 0.00015979
Iteration 111/1000 | Loss: 0.00002351
Iteration 112/1000 | Loss: 0.00001964
Iteration 113/1000 | Loss: 0.00003698
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002446
Iteration 117/1000 | Loss: 0.00002169
Iteration 118/1000 | Loss: 0.00002661
Iteration 119/1000 | Loss: 0.00003222
Iteration 120/1000 | Loss: 0.00002880
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001377
Iteration 128/1000 | Loss: 0.00001361
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001333
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001318
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001316
Iteration 153/1000 | Loss: 0.00001316
Iteration 154/1000 | Loss: 0.00001316
Iteration 155/1000 | Loss: 0.00001315
Iteration 156/1000 | Loss: 0.00001315
Iteration 157/1000 | Loss: 0.00001315
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001312
Iteration 165/1000 | Loss: 0.00001312
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001312
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001311
Iteration 171/1000 | Loss: 0.00001311
Iteration 172/1000 | Loss: 0.00001311
Iteration 173/1000 | Loss: 0.00001311
Iteration 174/1000 | Loss: 0.00001311
Iteration 175/1000 | Loss: 0.00001311
Iteration 176/1000 | Loss: 0.00001311
Iteration 177/1000 | Loss: 0.00001310
Iteration 178/1000 | Loss: 0.00001310
Iteration 179/1000 | Loss: 0.00001310
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001309
Iteration 183/1000 | Loss: 0.00001309
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001309
Iteration 186/1000 | Loss: 0.00001309
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001308
Iteration 191/1000 | Loss: 0.00001308
Iteration 192/1000 | Loss: 0.00001307
Iteration 193/1000 | Loss: 0.00001307
Iteration 194/1000 | Loss: 0.00001307
Iteration 195/1000 | Loss: 0.00001307
Iteration 196/1000 | Loss: 0.00001307
Iteration 197/1000 | Loss: 0.00001307
Iteration 198/1000 | Loss: 0.00001307
Iteration 199/1000 | Loss: 0.00001307
Iteration 200/1000 | Loss: 0.00001307
Iteration 201/1000 | Loss: 0.00001307
Iteration 202/1000 | Loss: 0.00001306
Iteration 203/1000 | Loss: 0.00001306
Iteration 204/1000 | Loss: 0.00001306
Iteration 205/1000 | Loss: 0.00001306
Iteration 206/1000 | Loss: 0.00001306
Iteration 207/1000 | Loss: 0.00001306
Iteration 208/1000 | Loss: 0.00001306
Iteration 209/1000 | Loss: 0.00001306
Iteration 210/1000 | Loss: 0.00001306
Iteration 211/1000 | Loss: 0.00001306
Iteration 212/1000 | Loss: 0.00001306
Iteration 213/1000 | Loss: 0.00001306
Iteration 214/1000 | Loss: 0.00001306
Iteration 215/1000 | Loss: 0.00001306
Iteration 216/1000 | Loss: 0.00001306
Iteration 217/1000 | Loss: 0.00001306
Iteration 218/1000 | Loss: 0.00001306
Iteration 219/1000 | Loss: 0.00001305
Iteration 220/1000 | Loss: 0.00001305
Iteration 221/1000 | Loss: 0.00001305
Iteration 222/1000 | Loss: 0.00001305
Iteration 223/1000 | Loss: 0.00001305
Iteration 224/1000 | Loss: 0.00001305
Iteration 225/1000 | Loss: 0.00001305
Iteration 226/1000 | Loss: 0.00001305
Iteration 227/1000 | Loss: 0.00001305
Iteration 228/1000 | Loss: 0.00001305
Iteration 229/1000 | Loss: 0.00001305
Iteration 230/1000 | Loss: 0.00001305
Iteration 231/1000 | Loss: 0.00001305
Iteration 232/1000 | Loss: 0.00001305
Iteration 233/1000 | Loss: 0.00001305
Iteration 234/1000 | Loss: 0.00001305
Iteration 235/1000 | Loss: 0.00001305
Iteration 236/1000 | Loss: 0.00001305
Iteration 237/1000 | Loss: 0.00001305
Iteration 238/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.3051590940449387e-05, 1.3051590940449387e-05, 1.3051590940449387e-05, 1.3051590940449387e-05, 1.3051590940449387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3051590940449387e-05

Optimization complete. Final v2v error: 3.0311458110809326 mm

Highest mean error: 5.99149227142334 mm for frame 139

Lowest mean error: 2.4871959686279297 mm for frame 235

Saving results

Total time: 274.4122860431671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420887
Iteration 2/25 | Loss: 0.00127390
Iteration 3/25 | Loss: 0.00107619
Iteration 4/25 | Loss: 0.00105217
Iteration 5/25 | Loss: 0.00104687
Iteration 6/25 | Loss: 0.00104521
Iteration 7/25 | Loss: 0.00104501
Iteration 8/25 | Loss: 0.00104501
Iteration 9/25 | Loss: 0.00104501
Iteration 10/25 | Loss: 0.00104501
Iteration 11/25 | Loss: 0.00104501
Iteration 12/25 | Loss: 0.00104501
Iteration 13/25 | Loss: 0.00104501
Iteration 14/25 | Loss: 0.00104501
Iteration 15/25 | Loss: 0.00104501
Iteration 16/25 | Loss: 0.00104501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010450115660205483, 0.0010450115660205483, 0.0010450115660205483, 0.0010450115660205483, 0.0010450115660205483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010450115660205483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37326956
Iteration 2/25 | Loss: 0.00065128
Iteration 3/25 | Loss: 0.00065125
Iteration 4/25 | Loss: 0.00065125
Iteration 5/25 | Loss: 0.00065125
Iteration 6/25 | Loss: 0.00065125
Iteration 7/25 | Loss: 0.00065125
Iteration 8/25 | Loss: 0.00065125
Iteration 9/25 | Loss: 0.00065125
Iteration 10/25 | Loss: 0.00065125
Iteration 11/25 | Loss: 0.00065125
Iteration 12/25 | Loss: 0.00065125
Iteration 13/25 | Loss: 0.00065125
Iteration 14/25 | Loss: 0.00065125
Iteration 15/25 | Loss: 0.00065125
Iteration 16/25 | Loss: 0.00065125
Iteration 17/25 | Loss: 0.00065125
Iteration 18/25 | Loss: 0.00065125
Iteration 19/25 | Loss: 0.00065125
Iteration 20/25 | Loss: 0.00065125
Iteration 21/25 | Loss: 0.00065125
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000651245703920722, 0.000651245703920722, 0.000651245703920722, 0.000651245703920722, 0.000651245703920722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000651245703920722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065125
Iteration 2/1000 | Loss: 0.00003228
Iteration 3/1000 | Loss: 0.00002232
Iteration 4/1000 | Loss: 0.00002059
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001804
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001791
Iteration 12/1000 | Loss: 0.00001791
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001764
Iteration 16/1000 | Loss: 0.00001758
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001755
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001754
Iteration 21/1000 | Loss: 0.00001754
Iteration 22/1000 | Loss: 0.00001754
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001753
Iteration 26/1000 | Loss: 0.00001752
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001750
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001742
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001740
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001739
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001738
Iteration 49/1000 | Loss: 0.00001738
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001737
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001737
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001732
Iteration 94/1000 | Loss: 0.00001732
Iteration 95/1000 | Loss: 0.00001732
Iteration 96/1000 | Loss: 0.00001732
Iteration 97/1000 | Loss: 0.00001732
Iteration 98/1000 | Loss: 0.00001732
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.7317888705292717e-05, 1.7317888705292717e-05, 1.7317888705292717e-05, 1.7317888705292717e-05, 1.7317888705292717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7317888705292717e-05

Optimization complete. Final v2v error: 3.507969617843628 mm

Highest mean error: 4.381115913391113 mm for frame 143

Lowest mean error: 3.1682472229003906 mm for frame 76

Saving results

Total time: 34.7498574256897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784585
Iteration 2/25 | Loss: 0.00139713
Iteration 3/25 | Loss: 0.00111036
Iteration 4/25 | Loss: 0.00108287
Iteration 5/25 | Loss: 0.00107801
Iteration 6/25 | Loss: 0.00107721
Iteration 7/25 | Loss: 0.00107721
Iteration 8/25 | Loss: 0.00107721
Iteration 9/25 | Loss: 0.00107721
Iteration 10/25 | Loss: 0.00107721
Iteration 11/25 | Loss: 0.00107721
Iteration 12/25 | Loss: 0.00107721
Iteration 13/25 | Loss: 0.00107721
Iteration 14/25 | Loss: 0.00107721
Iteration 15/25 | Loss: 0.00107721
Iteration 16/25 | Loss: 0.00107721
Iteration 17/25 | Loss: 0.00107721
Iteration 18/25 | Loss: 0.00107721
Iteration 19/25 | Loss: 0.00107721
Iteration 20/25 | Loss: 0.00107721
Iteration 21/25 | Loss: 0.00107721
Iteration 22/25 | Loss: 0.00107721
Iteration 23/25 | Loss: 0.00107721
Iteration 24/25 | Loss: 0.00107721
Iteration 25/25 | Loss: 0.00107721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36170781
Iteration 2/25 | Loss: 0.00057959
Iteration 3/25 | Loss: 0.00057958
Iteration 4/25 | Loss: 0.00057958
Iteration 5/25 | Loss: 0.00057958
Iteration 6/25 | Loss: 0.00057958
Iteration 7/25 | Loss: 0.00057958
Iteration 8/25 | Loss: 0.00057958
Iteration 9/25 | Loss: 0.00057958
Iteration 10/25 | Loss: 0.00057958
Iteration 11/25 | Loss: 0.00057958
Iteration 12/25 | Loss: 0.00057958
Iteration 13/25 | Loss: 0.00057958
Iteration 14/25 | Loss: 0.00057958
Iteration 15/25 | Loss: 0.00057958
Iteration 16/25 | Loss: 0.00057958
Iteration 17/25 | Loss: 0.00057958
Iteration 18/25 | Loss: 0.00057958
Iteration 19/25 | Loss: 0.00057958
Iteration 20/25 | Loss: 0.00057958
Iteration 21/25 | Loss: 0.00057958
Iteration 22/25 | Loss: 0.00057958
Iteration 23/25 | Loss: 0.00057958
Iteration 24/25 | Loss: 0.00057958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000579575018491596, 0.000579575018491596, 0.000579575018491596, 0.000579575018491596, 0.000579575018491596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000579575018491596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057958
Iteration 2/1000 | Loss: 0.00002988
Iteration 3/1000 | Loss: 0.00002146
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001870
Iteration 6/1000 | Loss: 0.00001798
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001739
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001710
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001687
Iteration 15/1000 | Loss: 0.00001684
Iteration 16/1000 | Loss: 0.00001684
Iteration 17/1000 | Loss: 0.00001678
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001671
Iteration 22/1000 | Loss: 0.00001671
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001665
Iteration 25/1000 | Loss: 0.00001663
Iteration 26/1000 | Loss: 0.00001662
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001661
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001659
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001658
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001654
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.6540359865757637e-05, 1.6540359865757637e-05, 1.6540359865757637e-05, 1.6540359865757637e-05, 1.6540359865757637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6540359865757637e-05

Optimization complete. Final v2v error: 3.419950246810913 mm

Highest mean error: 3.565023422241211 mm for frame 175

Lowest mean error: 3.2303547859191895 mm for frame 1

Saving results

Total time: 34.87511324882507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00685753
Iteration 2/25 | Loss: 0.00123161
Iteration 3/25 | Loss: 0.00111917
Iteration 4/25 | Loss: 0.00108998
Iteration 5/25 | Loss: 0.00107879
Iteration 6/25 | Loss: 0.00107686
Iteration 7/25 | Loss: 0.00107686
Iteration 8/25 | Loss: 0.00107686
Iteration 9/25 | Loss: 0.00107686
Iteration 10/25 | Loss: 0.00107686
Iteration 11/25 | Loss: 0.00107686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010768587235361338, 0.0010768587235361338, 0.0010768587235361338, 0.0010768587235361338, 0.0010768587235361338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010768587235361338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63941514
Iteration 2/25 | Loss: 0.00083123
Iteration 3/25 | Loss: 0.00083122
Iteration 4/25 | Loss: 0.00083122
Iteration 5/25 | Loss: 0.00083122
Iteration 6/25 | Loss: 0.00083122
Iteration 7/25 | Loss: 0.00083122
Iteration 8/25 | Loss: 0.00083122
Iteration 9/25 | Loss: 0.00083122
Iteration 10/25 | Loss: 0.00083122
Iteration 11/25 | Loss: 0.00083122
Iteration 12/25 | Loss: 0.00083122
Iteration 13/25 | Loss: 0.00083122
Iteration 14/25 | Loss: 0.00083122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008312187856063247, 0.0008312187856063247, 0.0008312187856063247, 0.0008312187856063247, 0.0008312187856063247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008312187856063247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083122
Iteration 2/1000 | Loss: 0.00006865
Iteration 3/1000 | Loss: 0.00004100
Iteration 4/1000 | Loss: 0.00003150
Iteration 5/1000 | Loss: 0.00002869
Iteration 6/1000 | Loss: 0.00002738
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002537
Iteration 9/1000 | Loss: 0.00002484
Iteration 10/1000 | Loss: 0.00002446
Iteration 11/1000 | Loss: 0.00002421
Iteration 12/1000 | Loss: 0.00002399
Iteration 13/1000 | Loss: 0.00002391
Iteration 14/1000 | Loss: 0.00002386
Iteration 15/1000 | Loss: 0.00002377
Iteration 16/1000 | Loss: 0.00002371
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002360
Iteration 19/1000 | Loss: 0.00002359
Iteration 20/1000 | Loss: 0.00002358
Iteration 21/1000 | Loss: 0.00002358
Iteration 22/1000 | Loss: 0.00002356
Iteration 23/1000 | Loss: 0.00002355
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002354
Iteration 26/1000 | Loss: 0.00002354
Iteration 27/1000 | Loss: 0.00002353
Iteration 28/1000 | Loss: 0.00002352
Iteration 29/1000 | Loss: 0.00002352
Iteration 30/1000 | Loss: 0.00002351
Iteration 31/1000 | Loss: 0.00002350
Iteration 32/1000 | Loss: 0.00002350
Iteration 33/1000 | Loss: 0.00002350
Iteration 34/1000 | Loss: 0.00002349
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002348
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002347
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002345
Iteration 49/1000 | Loss: 0.00002345
Iteration 50/1000 | Loss: 0.00002344
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002344
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002343
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002342
Iteration 58/1000 | Loss: 0.00002342
Iteration 59/1000 | Loss: 0.00002342
Iteration 60/1000 | Loss: 0.00002341
Iteration 61/1000 | Loss: 0.00002341
Iteration 62/1000 | Loss: 0.00002341
Iteration 63/1000 | Loss: 0.00002341
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002340
Iteration 66/1000 | Loss: 0.00002340
Iteration 67/1000 | Loss: 0.00002340
Iteration 68/1000 | Loss: 0.00002340
Iteration 69/1000 | Loss: 0.00002340
Iteration 70/1000 | Loss: 0.00002340
Iteration 71/1000 | Loss: 0.00002339
Iteration 72/1000 | Loss: 0.00002339
Iteration 73/1000 | Loss: 0.00002339
Iteration 74/1000 | Loss: 0.00002339
Iteration 75/1000 | Loss: 0.00002339
Iteration 76/1000 | Loss: 0.00002338
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002337
Iteration 79/1000 | Loss: 0.00002337
Iteration 80/1000 | Loss: 0.00002337
Iteration 81/1000 | Loss: 0.00002336
Iteration 82/1000 | Loss: 0.00002336
Iteration 83/1000 | Loss: 0.00002336
Iteration 84/1000 | Loss: 0.00002336
Iteration 85/1000 | Loss: 0.00002336
Iteration 86/1000 | Loss: 0.00002336
Iteration 87/1000 | Loss: 0.00002335
Iteration 88/1000 | Loss: 0.00002335
Iteration 89/1000 | Loss: 0.00002335
Iteration 90/1000 | Loss: 0.00002335
Iteration 91/1000 | Loss: 0.00002335
Iteration 92/1000 | Loss: 0.00002334
Iteration 93/1000 | Loss: 0.00002334
Iteration 94/1000 | Loss: 0.00002334
Iteration 95/1000 | Loss: 0.00002333
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00002333
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002332
Iteration 100/1000 | Loss: 0.00002332
Iteration 101/1000 | Loss: 0.00002332
Iteration 102/1000 | Loss: 0.00002331
Iteration 103/1000 | Loss: 0.00002331
Iteration 104/1000 | Loss: 0.00002331
Iteration 105/1000 | Loss: 0.00002330
Iteration 106/1000 | Loss: 0.00002330
Iteration 107/1000 | Loss: 0.00002330
Iteration 108/1000 | Loss: 0.00002329
Iteration 109/1000 | Loss: 0.00002329
Iteration 110/1000 | Loss: 0.00002329
Iteration 111/1000 | Loss: 0.00002329
Iteration 112/1000 | Loss: 0.00002329
Iteration 113/1000 | Loss: 0.00002329
Iteration 114/1000 | Loss: 0.00002329
Iteration 115/1000 | Loss: 0.00002329
Iteration 116/1000 | Loss: 0.00002329
Iteration 117/1000 | Loss: 0.00002329
Iteration 118/1000 | Loss: 0.00002328
Iteration 119/1000 | Loss: 0.00002328
Iteration 120/1000 | Loss: 0.00002328
Iteration 121/1000 | Loss: 0.00002328
Iteration 122/1000 | Loss: 0.00002328
Iteration 123/1000 | Loss: 0.00002328
Iteration 124/1000 | Loss: 0.00002328
Iteration 125/1000 | Loss: 0.00002327
Iteration 126/1000 | Loss: 0.00002327
Iteration 127/1000 | Loss: 0.00002327
Iteration 128/1000 | Loss: 0.00002327
Iteration 129/1000 | Loss: 0.00002327
Iteration 130/1000 | Loss: 0.00002327
Iteration 131/1000 | Loss: 0.00002327
Iteration 132/1000 | Loss: 0.00002327
Iteration 133/1000 | Loss: 0.00002327
Iteration 134/1000 | Loss: 0.00002327
Iteration 135/1000 | Loss: 0.00002326
Iteration 136/1000 | Loss: 0.00002326
Iteration 137/1000 | Loss: 0.00002326
Iteration 138/1000 | Loss: 0.00002326
Iteration 139/1000 | Loss: 0.00002326
Iteration 140/1000 | Loss: 0.00002326
Iteration 141/1000 | Loss: 0.00002326
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.32640522881411e-05, 2.32640522881411e-05, 2.32640522881411e-05, 2.32640522881411e-05, 2.32640522881411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.32640522881411e-05

Optimization complete. Final v2v error: 3.893885374069214 mm

Highest mean error: 4.92830228805542 mm for frame 164

Lowest mean error: 3.152677536010742 mm for frame 66

Saving results

Total time: 44.16936802864075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774785
Iteration 2/25 | Loss: 0.00213212
Iteration 3/25 | Loss: 0.00149612
Iteration 4/25 | Loss: 0.00136195
Iteration 5/25 | Loss: 0.00133776
Iteration 6/25 | Loss: 0.00122860
Iteration 7/25 | Loss: 0.00117056
Iteration 8/25 | Loss: 0.00116909
Iteration 9/25 | Loss: 0.00114789
Iteration 10/25 | Loss: 0.00112266
Iteration 11/25 | Loss: 0.00111621
Iteration 12/25 | Loss: 0.00111384
Iteration 13/25 | Loss: 0.00110499
Iteration 14/25 | Loss: 0.00110375
Iteration 15/25 | Loss: 0.00110363
Iteration 16/25 | Loss: 0.00110363
Iteration 17/25 | Loss: 0.00110363
Iteration 18/25 | Loss: 0.00110363
Iteration 19/25 | Loss: 0.00110360
Iteration 20/25 | Loss: 0.00110360
Iteration 21/25 | Loss: 0.00110360
Iteration 22/25 | Loss: 0.00110360
Iteration 23/25 | Loss: 0.00110360
Iteration 24/25 | Loss: 0.00110360
Iteration 25/25 | Loss: 0.00110360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.15738201
Iteration 2/25 | Loss: 0.00064861
Iteration 3/25 | Loss: 0.00064847
Iteration 4/25 | Loss: 0.00064847
Iteration 5/25 | Loss: 0.00064847
Iteration 6/25 | Loss: 0.00064847
Iteration 7/25 | Loss: 0.00064847
Iteration 8/25 | Loss: 0.00064847
Iteration 9/25 | Loss: 0.00064847
Iteration 10/25 | Loss: 0.00064847
Iteration 11/25 | Loss: 0.00064847
Iteration 12/25 | Loss: 0.00064847
Iteration 13/25 | Loss: 0.00064847
Iteration 14/25 | Loss: 0.00064847
Iteration 15/25 | Loss: 0.00064847
Iteration 16/25 | Loss: 0.00064847
Iteration 17/25 | Loss: 0.00064847
Iteration 18/25 | Loss: 0.00064847
Iteration 19/25 | Loss: 0.00064847
Iteration 20/25 | Loss: 0.00064847
Iteration 21/25 | Loss: 0.00064847
Iteration 22/25 | Loss: 0.00064847
Iteration 23/25 | Loss: 0.00064847
Iteration 24/25 | Loss: 0.00064847
Iteration 25/25 | Loss: 0.00064847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064847
Iteration 2/1000 | Loss: 0.00004202
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002289
Iteration 5/1000 | Loss: 0.00016833
Iteration 6/1000 | Loss: 0.00002106
Iteration 7/1000 | Loss: 0.00002045
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001967
Iteration 10/1000 | Loss: 0.00010470
Iteration 11/1000 | Loss: 0.00002201
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001776
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001737
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001699
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001698
Iteration 92/1000 | Loss: 0.00001698
Iteration 93/1000 | Loss: 0.00001698
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001697
Iteration 97/1000 | Loss: 0.00001697
Iteration 98/1000 | Loss: 0.00001697
Iteration 99/1000 | Loss: 0.00001697
Iteration 100/1000 | Loss: 0.00001697
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001696
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001694
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001694
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001693
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001693
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001692
Iteration 141/1000 | Loss: 0.00001692
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001692
Iteration 144/1000 | Loss: 0.00001692
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001690
Iteration 152/1000 | Loss: 0.00001690
Iteration 153/1000 | Loss: 0.00001690
Iteration 154/1000 | Loss: 0.00001690
Iteration 155/1000 | Loss: 0.00001690
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001689
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001688
Iteration 171/1000 | Loss: 0.00001688
Iteration 172/1000 | Loss: 0.00001688
Iteration 173/1000 | Loss: 0.00001688
Iteration 174/1000 | Loss: 0.00001688
Iteration 175/1000 | Loss: 0.00001688
Iteration 176/1000 | Loss: 0.00001688
Iteration 177/1000 | Loss: 0.00001688
Iteration 178/1000 | Loss: 0.00001688
Iteration 179/1000 | Loss: 0.00001688
Iteration 180/1000 | Loss: 0.00001688
Iteration 181/1000 | Loss: 0.00001688
Iteration 182/1000 | Loss: 0.00001688
Iteration 183/1000 | Loss: 0.00001688
Iteration 184/1000 | Loss: 0.00001688
Iteration 185/1000 | Loss: 0.00001688
Iteration 186/1000 | Loss: 0.00001688
Iteration 187/1000 | Loss: 0.00001688
Iteration 188/1000 | Loss: 0.00001688
Iteration 189/1000 | Loss: 0.00001688
Iteration 190/1000 | Loss: 0.00001688
Iteration 191/1000 | Loss: 0.00001688
Iteration 192/1000 | Loss: 0.00001688
Iteration 193/1000 | Loss: 0.00001688
Iteration 194/1000 | Loss: 0.00001688
Iteration 195/1000 | Loss: 0.00001688
Iteration 196/1000 | Loss: 0.00001688
Iteration 197/1000 | Loss: 0.00001688
Iteration 198/1000 | Loss: 0.00001688
Iteration 199/1000 | Loss: 0.00001688
Iteration 200/1000 | Loss: 0.00001688
Iteration 201/1000 | Loss: 0.00001688
Iteration 202/1000 | Loss: 0.00001688
Iteration 203/1000 | Loss: 0.00001688
Iteration 204/1000 | Loss: 0.00001688
Iteration 205/1000 | Loss: 0.00001688
Iteration 206/1000 | Loss: 0.00001688
Iteration 207/1000 | Loss: 0.00001688
Iteration 208/1000 | Loss: 0.00001688
Iteration 209/1000 | Loss: 0.00001688
Iteration 210/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.688062729954254e-05, 1.688062729954254e-05, 1.688062729954254e-05, 1.688062729954254e-05, 1.688062729954254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.688062729954254e-05

Optimization complete. Final v2v error: 3.392991542816162 mm

Highest mean error: 3.9668428897857666 mm for frame 7

Lowest mean error: 3.0576679706573486 mm for frame 0

Saving results

Total time: 67.2305371761322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855728
Iteration 2/25 | Loss: 0.00132421
Iteration 3/25 | Loss: 0.00107532
Iteration 4/25 | Loss: 0.00104457
Iteration 5/25 | Loss: 0.00104065
Iteration 6/25 | Loss: 0.00104049
Iteration 7/25 | Loss: 0.00104049
Iteration 8/25 | Loss: 0.00104049
Iteration 9/25 | Loss: 0.00104049
Iteration 10/25 | Loss: 0.00104049
Iteration 11/25 | Loss: 0.00104049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010404853383079171, 0.0010404853383079171, 0.0010404853383079171, 0.0010404853383079171, 0.0010404853383079171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010404853383079171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93666220
Iteration 2/25 | Loss: 0.00033885
Iteration 3/25 | Loss: 0.00033884
Iteration 4/25 | Loss: 0.00033884
Iteration 5/25 | Loss: 0.00033884
Iteration 6/25 | Loss: 0.00033884
Iteration 7/25 | Loss: 0.00033884
Iteration 8/25 | Loss: 0.00033884
Iteration 9/25 | Loss: 0.00033884
Iteration 10/25 | Loss: 0.00033884
Iteration 11/25 | Loss: 0.00033884
Iteration 12/25 | Loss: 0.00033884
Iteration 13/25 | Loss: 0.00033884
Iteration 14/25 | Loss: 0.00033884
Iteration 15/25 | Loss: 0.00033884
Iteration 16/25 | Loss: 0.00033884
Iteration 17/25 | Loss: 0.00033884
Iteration 18/25 | Loss: 0.00033884
Iteration 19/25 | Loss: 0.00033884
Iteration 20/25 | Loss: 0.00033884
Iteration 21/25 | Loss: 0.00033884
Iteration 22/25 | Loss: 0.00033884
Iteration 23/25 | Loss: 0.00033884
Iteration 24/25 | Loss: 0.00033884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00033883797004818916, 0.00033883797004818916, 0.00033883797004818916, 0.00033883797004818916, 0.00033883797004818916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033883797004818916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033884
Iteration 2/1000 | Loss: 0.00002955
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00002100
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001932
Iteration 7/1000 | Loss: 0.00001892
Iteration 8/1000 | Loss: 0.00001848
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001781
Iteration 16/1000 | Loss: 0.00001781
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001780
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001769
Iteration 22/1000 | Loss: 0.00001768
Iteration 23/1000 | Loss: 0.00001767
Iteration 24/1000 | Loss: 0.00001767
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001764
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001759
Iteration 39/1000 | Loss: 0.00001759
Iteration 40/1000 | Loss: 0.00001759
Iteration 41/1000 | Loss: 0.00001759
Iteration 42/1000 | Loss: 0.00001759
Iteration 43/1000 | Loss: 0.00001759
Iteration 44/1000 | Loss: 0.00001758
Iteration 45/1000 | Loss: 0.00001758
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001753
Iteration 48/1000 | Loss: 0.00001753
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001752
Iteration 52/1000 | Loss: 0.00001752
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001751
Iteration 55/1000 | Loss: 0.00001751
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001751
Iteration 58/1000 | Loss: 0.00001751
Iteration 59/1000 | Loss: 0.00001751
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001751
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.751313902786933e-05, 1.751313902786933e-05, 1.751313902786933e-05, 1.751313902786933e-05, 1.751313902786933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.751313902786933e-05

Optimization complete. Final v2v error: 3.5232300758361816 mm

Highest mean error: 3.658050537109375 mm for frame 125

Lowest mean error: 3.427701234817505 mm for frame 10

Saving results

Total time: 33.911226749420166
