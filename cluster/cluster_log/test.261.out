Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=261, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14616-14671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414348
Iteration 2/25 | Loss: 0.00090532
Iteration 3/25 | Loss: 0.00078502
Iteration 4/25 | Loss: 0.00075815
Iteration 5/25 | Loss: 0.00074991
Iteration 6/25 | Loss: 0.00074811
Iteration 7/25 | Loss: 0.00074756
Iteration 8/25 | Loss: 0.00074756
Iteration 9/25 | Loss: 0.00074756
Iteration 10/25 | Loss: 0.00074756
Iteration 11/25 | Loss: 0.00074756
Iteration 12/25 | Loss: 0.00074756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007475554011762142, 0.0007475554011762142, 0.0007475554011762142, 0.0007475554011762142, 0.0007475554011762142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007475554011762142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.60200405
Iteration 2/25 | Loss: 0.00125256
Iteration 3/25 | Loss: 0.00125256
Iteration 4/25 | Loss: 0.00125256
Iteration 5/25 | Loss: 0.00125256
Iteration 6/25 | Loss: 0.00125256
Iteration 7/25 | Loss: 0.00125256
Iteration 8/25 | Loss: 0.00125256
Iteration 9/25 | Loss: 0.00125256
Iteration 10/25 | Loss: 0.00125255
Iteration 11/25 | Loss: 0.00125255
Iteration 12/25 | Loss: 0.00125255
Iteration 13/25 | Loss: 0.00125255
Iteration 14/25 | Loss: 0.00125255
Iteration 15/25 | Loss: 0.00125255
Iteration 16/25 | Loss: 0.00125255
Iteration 17/25 | Loss: 0.00125255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001252554589882493, 0.001252554589882493, 0.001252554589882493, 0.001252554589882493, 0.001252554589882493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001252554589882493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125255
Iteration 2/1000 | Loss: 0.00003051
Iteration 3/1000 | Loss: 0.00001897
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001499
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001429
Iteration 19/1000 | Loss: 0.00001429
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001424
Iteration 25/1000 | Loss: 0.00001424
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001416
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001411
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001402
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001401
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001396
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001395
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001389
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001388
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001382
Iteration 105/1000 | Loss: 0.00001382
Iteration 106/1000 | Loss: 0.00001382
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001382
Iteration 111/1000 | Loss: 0.00001382
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001381
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001381
Iteration 150/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.3813455552735832e-05, 1.3813455552735832e-05, 1.3813455552735832e-05, 1.3813455552735832e-05, 1.3813455552735832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3813455552735832e-05

Optimization complete. Final v2v error: 3.168182611465454 mm

Highest mean error: 3.689473867416382 mm for frame 51

Lowest mean error: 2.9767537117004395 mm for frame 123

Saving results

Total time: 36.38149356842041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459221
Iteration 2/25 | Loss: 0.00097599
Iteration 3/25 | Loss: 0.00084525
Iteration 4/25 | Loss: 0.00081929
Iteration 5/25 | Loss: 0.00081319
Iteration 6/25 | Loss: 0.00081145
Iteration 7/25 | Loss: 0.00081118
Iteration 8/25 | Loss: 0.00081118
Iteration 9/25 | Loss: 0.00081118
Iteration 10/25 | Loss: 0.00081118
Iteration 11/25 | Loss: 0.00081118
Iteration 12/25 | Loss: 0.00081118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008111802162602544, 0.0008111802162602544, 0.0008111802162602544, 0.0008111802162602544, 0.0008111802162602544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008111802162602544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58901632
Iteration 2/25 | Loss: 0.00123991
Iteration 3/25 | Loss: 0.00123991
Iteration 4/25 | Loss: 0.00123991
Iteration 5/25 | Loss: 0.00123991
Iteration 6/25 | Loss: 0.00123991
Iteration 7/25 | Loss: 0.00123991
Iteration 8/25 | Loss: 0.00123991
Iteration 9/25 | Loss: 0.00123991
Iteration 10/25 | Loss: 0.00123991
Iteration 11/25 | Loss: 0.00123991
Iteration 12/25 | Loss: 0.00123991
Iteration 13/25 | Loss: 0.00123991
Iteration 14/25 | Loss: 0.00123991
Iteration 15/25 | Loss: 0.00123991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012399052502587438, 0.0012399052502587438, 0.0012399052502587438, 0.0012399052502587438, 0.0012399052502587438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012399052502587438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123991
Iteration 2/1000 | Loss: 0.00003259
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001781
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001577
Iteration 9/1000 | Loss: 0.00001567
Iteration 10/1000 | Loss: 0.00001562
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001536
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001526
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001525
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001522
Iteration 28/1000 | Loss: 0.00001522
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001518
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001516
Iteration 43/1000 | Loss: 0.00001516
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001513
Iteration 51/1000 | Loss: 0.00001513
Iteration 52/1000 | Loss: 0.00001512
Iteration 53/1000 | Loss: 0.00001512
Iteration 54/1000 | Loss: 0.00001512
Iteration 55/1000 | Loss: 0.00001511
Iteration 56/1000 | Loss: 0.00001511
Iteration 57/1000 | Loss: 0.00001511
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001509
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001509
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001508
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001506
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001506
Iteration 83/1000 | Loss: 0.00001506
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001504
Iteration 101/1000 | Loss: 0.00001504
Iteration 102/1000 | Loss: 0.00001504
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001502
Iteration 110/1000 | Loss: 0.00001502
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001502
Iteration 117/1000 | Loss: 0.00001502
Iteration 118/1000 | Loss: 0.00001501
Iteration 119/1000 | Loss: 0.00001501
Iteration 120/1000 | Loss: 0.00001501
Iteration 121/1000 | Loss: 0.00001501
Iteration 122/1000 | Loss: 0.00001501
Iteration 123/1000 | Loss: 0.00001501
Iteration 124/1000 | Loss: 0.00001501
Iteration 125/1000 | Loss: 0.00001501
Iteration 126/1000 | Loss: 0.00001501
Iteration 127/1000 | Loss: 0.00001501
Iteration 128/1000 | Loss: 0.00001500
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001500
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001500
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001500
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001500
Iteration 140/1000 | Loss: 0.00001500
Iteration 141/1000 | Loss: 0.00001500
Iteration 142/1000 | Loss: 0.00001500
Iteration 143/1000 | Loss: 0.00001500
Iteration 144/1000 | Loss: 0.00001500
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.4995417586760595e-05, 1.4995417586760595e-05, 1.4995417586760595e-05, 1.4995417586760595e-05, 1.4995417586760595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4995417586760595e-05

Optimization complete. Final v2v error: 3.1833224296569824 mm

Highest mean error: 3.553023338317871 mm for frame 40

Lowest mean error: 2.8185791969299316 mm for frame 5

Saving results

Total time: 34.69196009635925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063639
Iteration 2/25 | Loss: 0.00251317
Iteration 3/25 | Loss: 0.00143315
Iteration 4/25 | Loss: 0.00127831
Iteration 5/25 | Loss: 0.00119409
Iteration 6/25 | Loss: 0.00115955
Iteration 7/25 | Loss: 0.00111186
Iteration 8/25 | Loss: 0.00112547
Iteration 9/25 | Loss: 0.00118603
Iteration 10/25 | Loss: 0.00117065
Iteration 11/25 | Loss: 0.00099025
Iteration 12/25 | Loss: 0.00093128
Iteration 13/25 | Loss: 0.00091054
Iteration 14/25 | Loss: 0.00091365
Iteration 15/25 | Loss: 0.00091865
Iteration 16/25 | Loss: 0.00089223
Iteration 17/25 | Loss: 0.00085976
Iteration 18/25 | Loss: 0.00084581
Iteration 19/25 | Loss: 0.00082114
Iteration 20/25 | Loss: 0.00080780
Iteration 21/25 | Loss: 0.00080224
Iteration 22/25 | Loss: 0.00079223
Iteration 23/25 | Loss: 0.00079208
Iteration 24/25 | Loss: 0.00078934
Iteration 25/25 | Loss: 0.00078727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81318963
Iteration 2/25 | Loss: 0.00141952
Iteration 3/25 | Loss: 0.00141952
Iteration 4/25 | Loss: 0.00141952
Iteration 5/25 | Loss: 0.00141952
Iteration 6/25 | Loss: 0.00141952
Iteration 7/25 | Loss: 0.00141952
Iteration 8/25 | Loss: 0.00141952
Iteration 9/25 | Loss: 0.00141952
Iteration 10/25 | Loss: 0.00141952
Iteration 11/25 | Loss: 0.00141952
Iteration 12/25 | Loss: 0.00141952
Iteration 13/25 | Loss: 0.00141952
Iteration 14/25 | Loss: 0.00141952
Iteration 15/25 | Loss: 0.00141952
Iteration 16/25 | Loss: 0.00141952
Iteration 17/25 | Loss: 0.00141952
Iteration 18/25 | Loss: 0.00141952
Iteration 19/25 | Loss: 0.00141952
Iteration 20/25 | Loss: 0.00141952
Iteration 21/25 | Loss: 0.00141952
Iteration 22/25 | Loss: 0.00141952
Iteration 23/25 | Loss: 0.00141952
Iteration 24/25 | Loss: 0.00141952
Iteration 25/25 | Loss: 0.00141952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00141951534897089, 0.00141951534897089, 0.00141951534897089, 0.00141951534897089, 0.00141951534897089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00141951534897089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141952
Iteration 2/1000 | Loss: 0.00003510
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00002523
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001355
Iteration 10/1000 | Loss: 0.00001323
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001242
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001236
Iteration 26/1000 | Loss: 0.00001236
Iteration 27/1000 | Loss: 0.00001235
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001233
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001218
Iteration 70/1000 | Loss: 0.00001218
Iteration 71/1000 | Loss: 0.00001218
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001217
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001217
Iteration 78/1000 | Loss: 0.00001217
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.2174096809758339e-05, 1.2174096809758339e-05, 1.2174096809758339e-05, 1.2174096809758339e-05, 1.2174096809758339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2174096809758339e-05

Optimization complete. Final v2v error: 2.952021360397339 mm

Highest mean error: 3.736156463623047 mm for frame 85

Lowest mean error: 2.5167500972747803 mm for frame 39

Saving results

Total time: 70.29369640350342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073196
Iteration 2/25 | Loss: 0.00136435
Iteration 3/25 | Loss: 0.00088437
Iteration 4/25 | Loss: 0.00079921
Iteration 5/25 | Loss: 0.00077726
Iteration 6/25 | Loss: 0.00077319
Iteration 7/25 | Loss: 0.00076978
Iteration 8/25 | Loss: 0.00076832
Iteration 9/25 | Loss: 0.00076714
Iteration 10/25 | Loss: 0.00076626
Iteration 11/25 | Loss: 0.00076570
Iteration 12/25 | Loss: 0.00076526
Iteration 13/25 | Loss: 0.00076518
Iteration 14/25 | Loss: 0.00076518
Iteration 15/25 | Loss: 0.00076518
Iteration 16/25 | Loss: 0.00076518
Iteration 17/25 | Loss: 0.00076517
Iteration 18/25 | Loss: 0.00076517
Iteration 19/25 | Loss: 0.00076517
Iteration 20/25 | Loss: 0.00076517
Iteration 21/25 | Loss: 0.00076517
Iteration 22/25 | Loss: 0.00076517
Iteration 23/25 | Loss: 0.00076517
Iteration 24/25 | Loss: 0.00076517
Iteration 25/25 | Loss: 0.00076517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58551121
Iteration 2/25 | Loss: 0.00123361
Iteration 3/25 | Loss: 0.00123361
Iteration 4/25 | Loss: 0.00123361
Iteration 5/25 | Loss: 0.00123361
Iteration 6/25 | Loss: 0.00123361
Iteration 7/25 | Loss: 0.00123361
Iteration 8/25 | Loss: 0.00123361
Iteration 9/25 | Loss: 0.00123361
Iteration 10/25 | Loss: 0.00123361
Iteration 11/25 | Loss: 0.00123361
Iteration 12/25 | Loss: 0.00123361
Iteration 13/25 | Loss: 0.00123361
Iteration 14/25 | Loss: 0.00123361
Iteration 15/25 | Loss: 0.00123361
Iteration 16/25 | Loss: 0.00123361
Iteration 17/25 | Loss: 0.00123361
Iteration 18/25 | Loss: 0.00123361
Iteration 19/25 | Loss: 0.00123361
Iteration 20/25 | Loss: 0.00123361
Iteration 21/25 | Loss: 0.00123361
Iteration 22/25 | Loss: 0.00123361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012336070649325848, 0.0012336070649325848, 0.0012336070649325848, 0.0012336070649325848, 0.0012336070649325848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012336070649325848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123361
Iteration 2/1000 | Loss: 0.00002896
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00003191
Iteration 6/1000 | Loss: 0.00001739
Iteration 7/1000 | Loss: 0.00002422
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00004281
Iteration 19/1000 | Loss: 0.00001767
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001572
Iteration 26/1000 | Loss: 0.00001572
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001568
Iteration 48/1000 | Loss: 0.00001568
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001568
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.5567529771942645e-05, 1.5567529771942645e-05, 1.5567529771942645e-05, 1.5567529771942645e-05, 1.5567529771942645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5567529771942645e-05

Optimization complete. Final v2v error: 3.3749992847442627 mm

Highest mean error: 3.5181047916412354 mm for frame 76

Lowest mean error: 2.9865918159484863 mm for frame 0

Saving results

Total time: 47.99657487869263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545329
Iteration 2/25 | Loss: 0.00097897
Iteration 3/25 | Loss: 0.00085602
Iteration 4/25 | Loss: 0.00083496
Iteration 5/25 | Loss: 0.00082916
Iteration 6/25 | Loss: 0.00082729
Iteration 7/25 | Loss: 0.00082697
Iteration 8/25 | Loss: 0.00082697
Iteration 9/25 | Loss: 0.00082697
Iteration 10/25 | Loss: 0.00082697
Iteration 11/25 | Loss: 0.00082697
Iteration 12/25 | Loss: 0.00082697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008269735844805837, 0.0008269735844805837, 0.0008269735844805837, 0.0008269735844805837, 0.0008269735844805837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008269735844805837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57181334
Iteration 2/25 | Loss: 0.00118340
Iteration 3/25 | Loss: 0.00118336
Iteration 4/25 | Loss: 0.00118335
Iteration 5/25 | Loss: 0.00118335
Iteration 6/25 | Loss: 0.00118335
Iteration 7/25 | Loss: 0.00118335
Iteration 8/25 | Loss: 0.00118335
Iteration 9/25 | Loss: 0.00118335
Iteration 10/25 | Loss: 0.00118335
Iteration 11/25 | Loss: 0.00118335
Iteration 12/25 | Loss: 0.00118335
Iteration 13/25 | Loss: 0.00118335
Iteration 14/25 | Loss: 0.00118335
Iteration 15/25 | Loss: 0.00118335
Iteration 16/25 | Loss: 0.00118335
Iteration 17/25 | Loss: 0.00118335
Iteration 18/25 | Loss: 0.00118335
Iteration 19/25 | Loss: 0.00118335
Iteration 20/25 | Loss: 0.00118335
Iteration 21/25 | Loss: 0.00118335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011833531316369772, 0.0011833531316369772, 0.0011833531316369772, 0.0011833531316369772, 0.0011833531316369772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011833531316369772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118335
Iteration 2/1000 | Loss: 0.00004404
Iteration 3/1000 | Loss: 0.00002743
Iteration 4/1000 | Loss: 0.00002434
Iteration 5/1000 | Loss: 0.00002260
Iteration 6/1000 | Loss: 0.00002179
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001981
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001941
Iteration 14/1000 | Loss: 0.00001941
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001925
Iteration 19/1000 | Loss: 0.00001925
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001919
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001915
Iteration 24/1000 | Loss: 0.00001914
Iteration 25/1000 | Loss: 0.00001914
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001908
Iteration 32/1000 | Loss: 0.00001907
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001904
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001901
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001896
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.8958209693664685e-05, 1.8958209693664685e-05, 1.8958209693664685e-05, 1.8958209693664685e-05, 1.8958209693664685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8958209693664685e-05

Optimization complete. Final v2v error: 3.6779515743255615 mm

Highest mean error: 4.117783546447754 mm for frame 103

Lowest mean error: 3.112820625305176 mm for frame 38

Saving results

Total time: 40.19159197807312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469909
Iteration 2/25 | Loss: 0.00089673
Iteration 3/25 | Loss: 0.00077221
Iteration 4/25 | Loss: 0.00075999
Iteration 5/25 | Loss: 0.00075719
Iteration 6/25 | Loss: 0.00075608
Iteration 7/25 | Loss: 0.00075593
Iteration 8/25 | Loss: 0.00075593
Iteration 9/25 | Loss: 0.00075593
Iteration 10/25 | Loss: 0.00075593
Iteration 11/25 | Loss: 0.00075593
Iteration 12/25 | Loss: 0.00075593
Iteration 13/25 | Loss: 0.00075593
Iteration 14/25 | Loss: 0.00075593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007559342193417251, 0.0007559342193417251, 0.0007559342193417251, 0.0007559342193417251, 0.0007559342193417251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007559342193417251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58245683
Iteration 2/25 | Loss: 0.00107544
Iteration 3/25 | Loss: 0.00107542
Iteration 4/25 | Loss: 0.00107542
Iteration 5/25 | Loss: 0.00107542
Iteration 6/25 | Loss: 0.00107542
Iteration 7/25 | Loss: 0.00107542
Iteration 8/25 | Loss: 0.00107542
Iteration 9/25 | Loss: 0.00107542
Iteration 10/25 | Loss: 0.00107542
Iteration 11/25 | Loss: 0.00107542
Iteration 12/25 | Loss: 0.00107542
Iteration 13/25 | Loss: 0.00107542
Iteration 14/25 | Loss: 0.00107542
Iteration 15/25 | Loss: 0.00107542
Iteration 16/25 | Loss: 0.00107542
Iteration 17/25 | Loss: 0.00107542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001075421110726893, 0.001075421110726893, 0.001075421110726893, 0.001075421110726893, 0.001075421110726893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001075421110726893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107542
Iteration 2/1000 | Loss: 0.00002608
Iteration 3/1000 | Loss: 0.00001689
Iteration 4/1000 | Loss: 0.00001452
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001310
Iteration 7/1000 | Loss: 0.00001274
Iteration 8/1000 | Loss: 0.00001258
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00001239
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001233
Iteration 20/1000 | Loss: 0.00001233
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001222
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001221
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001218
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001211
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001209
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001207
Iteration 131/1000 | Loss: 0.00001207
Iteration 132/1000 | Loss: 0.00001207
Iteration 133/1000 | Loss: 0.00001207
Iteration 134/1000 | Loss: 0.00001207
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.2068553587596398e-05, 1.2068553587596398e-05, 1.2068553587596398e-05, 1.2068553587596398e-05, 1.2068553587596398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2068553587596398e-05

Optimization complete. Final v2v error: 2.815101385116577 mm

Highest mean error: 3.5700604915618896 mm for frame 70

Lowest mean error: 2.5286834239959717 mm for frame 114

Saving results

Total time: 32.20736575126648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828345
Iteration 2/25 | Loss: 0.00087581
Iteration 3/25 | Loss: 0.00076688
Iteration 4/25 | Loss: 0.00075440
Iteration 5/25 | Loss: 0.00075087
Iteration 6/25 | Loss: 0.00074969
Iteration 7/25 | Loss: 0.00074946
Iteration 8/25 | Loss: 0.00074946
Iteration 9/25 | Loss: 0.00074946
Iteration 10/25 | Loss: 0.00074946
Iteration 11/25 | Loss: 0.00074946
Iteration 12/25 | Loss: 0.00074946
Iteration 13/25 | Loss: 0.00074946
Iteration 14/25 | Loss: 0.00074946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007494609453715384, 0.0007494609453715384, 0.0007494609453715384, 0.0007494609453715384, 0.0007494609453715384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007494609453715384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60189092
Iteration 2/25 | Loss: 0.00132990
Iteration 3/25 | Loss: 0.00132990
Iteration 4/25 | Loss: 0.00132990
Iteration 5/25 | Loss: 0.00132990
Iteration 6/25 | Loss: 0.00132990
Iteration 7/25 | Loss: 0.00132990
Iteration 8/25 | Loss: 0.00132990
Iteration 9/25 | Loss: 0.00132990
Iteration 10/25 | Loss: 0.00132990
Iteration 11/25 | Loss: 0.00132990
Iteration 12/25 | Loss: 0.00132990
Iteration 13/25 | Loss: 0.00132990
Iteration 14/25 | Loss: 0.00132990
Iteration 15/25 | Loss: 0.00132990
Iteration 16/25 | Loss: 0.00132990
Iteration 17/25 | Loss: 0.00132990
Iteration 18/25 | Loss: 0.00132990
Iteration 19/25 | Loss: 0.00132990
Iteration 20/25 | Loss: 0.00132990
Iteration 21/25 | Loss: 0.00132990
Iteration 22/25 | Loss: 0.00132990
Iteration 23/25 | Loss: 0.00132990
Iteration 24/25 | Loss: 0.00132990
Iteration 25/25 | Loss: 0.00132990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132990
Iteration 2/1000 | Loss: 0.00002339
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001234
Iteration 5/1000 | Loss: 0.00001157
Iteration 6/1000 | Loss: 0.00001119
Iteration 7/1000 | Loss: 0.00001087
Iteration 8/1000 | Loss: 0.00001072
Iteration 9/1000 | Loss: 0.00001072
Iteration 10/1000 | Loss: 0.00001068
Iteration 11/1000 | Loss: 0.00001067
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001061
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001060
Iteration 22/1000 | Loss: 0.00001060
Iteration 23/1000 | Loss: 0.00001060
Iteration 24/1000 | Loss: 0.00001059
Iteration 25/1000 | Loss: 0.00001059
Iteration 26/1000 | Loss: 0.00001058
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001056
Iteration 29/1000 | Loss: 0.00001055
Iteration 30/1000 | Loss: 0.00001055
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001055
Iteration 35/1000 | Loss: 0.00001054
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001053
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001051
Iteration 43/1000 | Loss: 0.00001051
Iteration 44/1000 | Loss: 0.00001048
Iteration 45/1000 | Loss: 0.00001048
Iteration 46/1000 | Loss: 0.00001047
Iteration 47/1000 | Loss: 0.00001047
Iteration 48/1000 | Loss: 0.00001047
Iteration 49/1000 | Loss: 0.00001047
Iteration 50/1000 | Loss: 0.00001046
Iteration 51/1000 | Loss: 0.00001046
Iteration 52/1000 | Loss: 0.00001045
Iteration 53/1000 | Loss: 0.00001045
Iteration 54/1000 | Loss: 0.00001045
Iteration 55/1000 | Loss: 0.00001044
Iteration 56/1000 | Loss: 0.00001044
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001044
Iteration 60/1000 | Loss: 0.00001043
Iteration 61/1000 | Loss: 0.00001042
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001037
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001035
Iteration 80/1000 | Loss: 0.00001035
Iteration 81/1000 | Loss: 0.00001035
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001034
Iteration 85/1000 | Loss: 0.00001034
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001034
Iteration 88/1000 | Loss: 0.00001034
Iteration 89/1000 | Loss: 0.00001034
Iteration 90/1000 | Loss: 0.00001034
Iteration 91/1000 | Loss: 0.00001034
Iteration 92/1000 | Loss: 0.00001033
Iteration 93/1000 | Loss: 0.00001033
Iteration 94/1000 | Loss: 0.00001033
Iteration 95/1000 | Loss: 0.00001033
Iteration 96/1000 | Loss: 0.00001033
Iteration 97/1000 | Loss: 0.00001033
Iteration 98/1000 | Loss: 0.00001033
Iteration 99/1000 | Loss: 0.00001032
Iteration 100/1000 | Loss: 0.00001032
Iteration 101/1000 | Loss: 0.00001032
Iteration 102/1000 | Loss: 0.00001032
Iteration 103/1000 | Loss: 0.00001032
Iteration 104/1000 | Loss: 0.00001032
Iteration 105/1000 | Loss: 0.00001032
Iteration 106/1000 | Loss: 0.00001032
Iteration 107/1000 | Loss: 0.00001032
Iteration 108/1000 | Loss: 0.00001032
Iteration 109/1000 | Loss: 0.00001032
Iteration 110/1000 | Loss: 0.00001032
Iteration 111/1000 | Loss: 0.00001032
Iteration 112/1000 | Loss: 0.00001032
Iteration 113/1000 | Loss: 0.00001032
Iteration 114/1000 | Loss: 0.00001032
Iteration 115/1000 | Loss: 0.00001032
Iteration 116/1000 | Loss: 0.00001031
Iteration 117/1000 | Loss: 0.00001031
Iteration 118/1000 | Loss: 0.00001031
Iteration 119/1000 | Loss: 0.00001031
Iteration 120/1000 | Loss: 0.00001031
Iteration 121/1000 | Loss: 0.00001031
Iteration 122/1000 | Loss: 0.00001031
Iteration 123/1000 | Loss: 0.00001031
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001031
Iteration 126/1000 | Loss: 0.00001031
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001030
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001030
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001030
Iteration 141/1000 | Loss: 0.00001030
Iteration 142/1000 | Loss: 0.00001030
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001030
Iteration 148/1000 | Loss: 0.00001030
Iteration 149/1000 | Loss: 0.00001030
Iteration 150/1000 | Loss: 0.00001030
Iteration 151/1000 | Loss: 0.00001030
Iteration 152/1000 | Loss: 0.00001030
Iteration 153/1000 | Loss: 0.00001030
Iteration 154/1000 | Loss: 0.00001030
Iteration 155/1000 | Loss: 0.00001030
Iteration 156/1000 | Loss: 0.00001030
Iteration 157/1000 | Loss: 0.00001030
Iteration 158/1000 | Loss: 0.00001030
Iteration 159/1000 | Loss: 0.00001030
Iteration 160/1000 | Loss: 0.00001030
Iteration 161/1000 | Loss: 0.00001030
Iteration 162/1000 | Loss: 0.00001030
Iteration 163/1000 | Loss: 0.00001030
Iteration 164/1000 | Loss: 0.00001030
Iteration 165/1000 | Loss: 0.00001030
Iteration 166/1000 | Loss: 0.00001030
Iteration 167/1000 | Loss: 0.00001030
Iteration 168/1000 | Loss: 0.00001030
Iteration 169/1000 | Loss: 0.00001030
Iteration 170/1000 | Loss: 0.00001030
Iteration 171/1000 | Loss: 0.00001030
Iteration 172/1000 | Loss: 0.00001030
Iteration 173/1000 | Loss: 0.00001030
Iteration 174/1000 | Loss: 0.00001030
Iteration 175/1000 | Loss: 0.00001030
Iteration 176/1000 | Loss: 0.00001030
Iteration 177/1000 | Loss: 0.00001030
Iteration 178/1000 | Loss: 0.00001030
Iteration 179/1000 | Loss: 0.00001030
Iteration 180/1000 | Loss: 0.00001030
Iteration 181/1000 | Loss: 0.00001030
Iteration 182/1000 | Loss: 0.00001030
Iteration 183/1000 | Loss: 0.00001030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.0304610441380646e-05, 1.0304610441380646e-05, 1.0304610441380646e-05, 1.0304610441380646e-05, 1.0304610441380646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0304610441380646e-05

Optimization complete. Final v2v error: 2.719467878341675 mm

Highest mean error: 2.839740037918091 mm for frame 37

Lowest mean error: 2.607819080352783 mm for frame 6

Saving results

Total time: 32.72813844680786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141928
Iteration 2/25 | Loss: 0.00231711
Iteration 3/25 | Loss: 0.00154463
Iteration 4/25 | Loss: 0.00135914
Iteration 5/25 | Loss: 0.00155448
Iteration 6/25 | Loss: 0.00176589
Iteration 7/25 | Loss: 0.00153895
Iteration 8/25 | Loss: 0.00147652
Iteration 9/25 | Loss: 0.00150248
Iteration 10/25 | Loss: 0.00141585
Iteration 11/25 | Loss: 0.00139009
Iteration 12/25 | Loss: 0.00133756
Iteration 13/25 | Loss: 0.00128910
Iteration 14/25 | Loss: 0.00127568
Iteration 15/25 | Loss: 0.00123112
Iteration 16/25 | Loss: 0.00126334
Iteration 17/25 | Loss: 0.00122403
Iteration 18/25 | Loss: 0.00120339
Iteration 19/25 | Loss: 0.00122006
Iteration 20/25 | Loss: 0.00119728
Iteration 21/25 | Loss: 0.00117630
Iteration 22/25 | Loss: 0.00114444
Iteration 23/25 | Loss: 0.00113542
Iteration 24/25 | Loss: 0.00113499
Iteration 25/25 | Loss: 0.00111684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39118981
Iteration 2/25 | Loss: 0.00487261
Iteration 3/25 | Loss: 0.00401686
Iteration 4/25 | Loss: 0.00401686
Iteration 5/25 | Loss: 0.00401686
Iteration 6/25 | Loss: 0.00401685
Iteration 7/25 | Loss: 0.00401685
Iteration 8/25 | Loss: 0.00401685
Iteration 9/25 | Loss: 0.00401685
Iteration 10/25 | Loss: 0.00401685
Iteration 11/25 | Loss: 0.00401685
Iteration 12/25 | Loss: 0.00401685
Iteration 13/25 | Loss: 0.00401685
Iteration 14/25 | Loss: 0.00401685
Iteration 15/25 | Loss: 0.00401685
Iteration 16/25 | Loss: 0.00401685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004016853403300047, 0.004016853403300047, 0.004016853403300047, 0.004016853403300047, 0.004016853403300047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004016853403300047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00401685
Iteration 2/1000 | Loss: 0.00272417
Iteration 3/1000 | Loss: 0.00131158
Iteration 4/1000 | Loss: 0.00180514
Iteration 5/1000 | Loss: 0.00175222
Iteration 6/1000 | Loss: 0.00193055
Iteration 7/1000 | Loss: 0.00256328
Iteration 8/1000 | Loss: 0.00121222
Iteration 9/1000 | Loss: 0.00107829
Iteration 10/1000 | Loss: 0.00140379
Iteration 11/1000 | Loss: 0.00102669
Iteration 12/1000 | Loss: 0.00097486
Iteration 13/1000 | Loss: 0.00153127
Iteration 14/1000 | Loss: 0.00174742
Iteration 15/1000 | Loss: 0.00276930
Iteration 16/1000 | Loss: 0.00150860
Iteration 17/1000 | Loss: 0.00149301
Iteration 18/1000 | Loss: 0.00083828
Iteration 19/1000 | Loss: 0.00053931
Iteration 20/1000 | Loss: 0.00067706
Iteration 21/1000 | Loss: 0.00101853
Iteration 22/1000 | Loss: 0.00068242
Iteration 23/1000 | Loss: 0.00067611
Iteration 24/1000 | Loss: 0.00076368
Iteration 25/1000 | Loss: 0.00062776
Iteration 26/1000 | Loss: 0.00055043
Iteration 27/1000 | Loss: 0.00063061
Iteration 28/1000 | Loss: 0.00058901
Iteration 29/1000 | Loss: 0.00028995
Iteration 30/1000 | Loss: 0.00074135
Iteration 31/1000 | Loss: 0.00075339
Iteration 32/1000 | Loss: 0.00064051
Iteration 33/1000 | Loss: 0.00071102
Iteration 34/1000 | Loss: 0.00151941
Iteration 35/1000 | Loss: 0.00090062
Iteration 36/1000 | Loss: 0.00050971
Iteration 37/1000 | Loss: 0.00077567
Iteration 38/1000 | Loss: 0.00030610
Iteration 39/1000 | Loss: 0.00036473
Iteration 40/1000 | Loss: 0.00040184
Iteration 41/1000 | Loss: 0.00038027
Iteration 42/1000 | Loss: 0.00125772
Iteration 43/1000 | Loss: 0.00061949
Iteration 44/1000 | Loss: 0.00131043
Iteration 45/1000 | Loss: 0.00122799
Iteration 46/1000 | Loss: 0.00073812
Iteration 47/1000 | Loss: 0.00058712
Iteration 48/1000 | Loss: 0.00069810
Iteration 49/1000 | Loss: 0.00040035
Iteration 50/1000 | Loss: 0.00029705
Iteration 51/1000 | Loss: 0.00038204
Iteration 52/1000 | Loss: 0.00036830
Iteration 53/1000 | Loss: 0.00062535
Iteration 54/1000 | Loss: 0.00061307
Iteration 55/1000 | Loss: 0.00067785
Iteration 56/1000 | Loss: 0.00015634
Iteration 57/1000 | Loss: 0.00127096
Iteration 58/1000 | Loss: 0.00112391
Iteration 59/1000 | Loss: 0.00046496
Iteration 60/1000 | Loss: 0.00096421
Iteration 61/1000 | Loss: 0.00188627
Iteration 62/1000 | Loss: 0.00282575
Iteration 63/1000 | Loss: 0.00181277
Iteration 64/1000 | Loss: 0.00050492
Iteration 65/1000 | Loss: 0.00073664
Iteration 66/1000 | Loss: 0.00088873
Iteration 67/1000 | Loss: 0.00044598
Iteration 68/1000 | Loss: 0.00028288
Iteration 69/1000 | Loss: 0.00021570
Iteration 70/1000 | Loss: 0.00041663
Iteration 71/1000 | Loss: 0.00096703
Iteration 72/1000 | Loss: 0.00064281
Iteration 73/1000 | Loss: 0.00067762
Iteration 74/1000 | Loss: 0.00059353
Iteration 75/1000 | Loss: 0.00039604
Iteration 76/1000 | Loss: 0.00013313
Iteration 77/1000 | Loss: 0.00012243
Iteration 78/1000 | Loss: 0.00008199
Iteration 79/1000 | Loss: 0.00041954
Iteration 80/1000 | Loss: 0.00073403
Iteration 81/1000 | Loss: 0.00010438
Iteration 82/1000 | Loss: 0.00041371
Iteration 83/1000 | Loss: 0.00036245
Iteration 84/1000 | Loss: 0.00040152
Iteration 85/1000 | Loss: 0.00028537
Iteration 86/1000 | Loss: 0.00039074
Iteration 87/1000 | Loss: 0.00022858
Iteration 88/1000 | Loss: 0.00018420
Iteration 89/1000 | Loss: 0.00021178
Iteration 90/1000 | Loss: 0.00158296
Iteration 91/1000 | Loss: 0.00026147
Iteration 92/1000 | Loss: 0.00007424
Iteration 93/1000 | Loss: 0.00033756
Iteration 94/1000 | Loss: 0.00068738
Iteration 95/1000 | Loss: 0.00068811
Iteration 96/1000 | Loss: 0.00056183
Iteration 97/1000 | Loss: 0.00026521
Iteration 98/1000 | Loss: 0.00010465
Iteration 99/1000 | Loss: 0.00009665
Iteration 100/1000 | Loss: 0.00008235
Iteration 101/1000 | Loss: 0.00082382
Iteration 102/1000 | Loss: 0.00009648
Iteration 103/1000 | Loss: 0.00007206
Iteration 104/1000 | Loss: 0.00008196
Iteration 105/1000 | Loss: 0.00005797
Iteration 106/1000 | Loss: 0.00005318
Iteration 107/1000 | Loss: 0.00041355
Iteration 108/1000 | Loss: 0.00033399
Iteration 109/1000 | Loss: 0.00035155
Iteration 110/1000 | Loss: 0.00110606
Iteration 111/1000 | Loss: 0.00065110
Iteration 112/1000 | Loss: 0.00048223
Iteration 113/1000 | Loss: 0.00095483
Iteration 114/1000 | Loss: 0.00057160
Iteration 115/1000 | Loss: 0.00054704
Iteration 116/1000 | Loss: 0.00039346
Iteration 117/1000 | Loss: 0.00053766
Iteration 118/1000 | Loss: 0.00057420
Iteration 119/1000 | Loss: 0.00081940
Iteration 120/1000 | Loss: 0.00059337
Iteration 121/1000 | Loss: 0.00066014
Iteration 122/1000 | Loss: 0.00030436
Iteration 123/1000 | Loss: 0.00050005
Iteration 124/1000 | Loss: 0.00055283
Iteration 125/1000 | Loss: 0.00044023
Iteration 126/1000 | Loss: 0.00048798
Iteration 127/1000 | Loss: 0.00070700
Iteration 128/1000 | Loss: 0.00005603
Iteration 129/1000 | Loss: 0.00005188
Iteration 130/1000 | Loss: 0.00004673
Iteration 131/1000 | Loss: 0.00039030
Iteration 132/1000 | Loss: 0.00041850
Iteration 133/1000 | Loss: 0.00038852
Iteration 134/1000 | Loss: 0.00023871
Iteration 135/1000 | Loss: 0.00030180
Iteration 136/1000 | Loss: 0.00023947
Iteration 137/1000 | Loss: 0.00004395
Iteration 138/1000 | Loss: 0.00004007
Iteration 139/1000 | Loss: 0.00037801
Iteration 140/1000 | Loss: 0.00004746
Iteration 141/1000 | Loss: 0.00053770
Iteration 142/1000 | Loss: 0.00010125
Iteration 143/1000 | Loss: 0.00004777
Iteration 144/1000 | Loss: 0.00003918
Iteration 145/1000 | Loss: 0.00003706
Iteration 146/1000 | Loss: 0.00003499
Iteration 147/1000 | Loss: 0.00003273
Iteration 148/1000 | Loss: 0.00003187
Iteration 149/1000 | Loss: 0.00003097
Iteration 150/1000 | Loss: 0.00003004
Iteration 151/1000 | Loss: 0.00005102
Iteration 152/1000 | Loss: 0.00003596
Iteration 153/1000 | Loss: 0.00003590
Iteration 154/1000 | Loss: 0.00002982
Iteration 155/1000 | Loss: 0.00002841
Iteration 156/1000 | Loss: 0.00002741
Iteration 157/1000 | Loss: 0.00002691
Iteration 158/1000 | Loss: 0.00002660
Iteration 159/1000 | Loss: 0.00002627
Iteration 160/1000 | Loss: 0.00002592
Iteration 161/1000 | Loss: 0.00002590
Iteration 162/1000 | Loss: 0.00002578
Iteration 163/1000 | Loss: 0.00002562
Iteration 164/1000 | Loss: 0.00002561
Iteration 165/1000 | Loss: 0.00002548
Iteration 166/1000 | Loss: 0.00002547
Iteration 167/1000 | Loss: 0.00002544
Iteration 168/1000 | Loss: 0.00002542
Iteration 169/1000 | Loss: 0.00002542
Iteration 170/1000 | Loss: 0.00002541
Iteration 171/1000 | Loss: 0.00002540
Iteration 172/1000 | Loss: 0.00002536
Iteration 173/1000 | Loss: 0.00004716
Iteration 174/1000 | Loss: 0.00003609
Iteration 175/1000 | Loss: 0.00002916
Iteration 176/1000 | Loss: 0.00002775
Iteration 177/1000 | Loss: 0.00002577
Iteration 178/1000 | Loss: 0.00002530
Iteration 179/1000 | Loss: 0.00004789
Iteration 180/1000 | Loss: 0.00036840
Iteration 181/1000 | Loss: 0.00003980
Iteration 182/1000 | Loss: 0.00023190
Iteration 183/1000 | Loss: 0.00029230
Iteration 184/1000 | Loss: 0.00003003
Iteration 185/1000 | Loss: 0.00002751
Iteration 186/1000 | Loss: 0.00002600
Iteration 187/1000 | Loss: 0.00002523
Iteration 188/1000 | Loss: 0.00002464
Iteration 189/1000 | Loss: 0.00002431
Iteration 190/1000 | Loss: 0.00002399
Iteration 191/1000 | Loss: 0.00002395
Iteration 192/1000 | Loss: 0.00002386
Iteration 193/1000 | Loss: 0.00002379
Iteration 194/1000 | Loss: 0.00002378
Iteration 195/1000 | Loss: 0.00002373
Iteration 196/1000 | Loss: 0.00002371
Iteration 197/1000 | Loss: 0.00002369
Iteration 198/1000 | Loss: 0.00002368
Iteration 199/1000 | Loss: 0.00002368
Iteration 200/1000 | Loss: 0.00002367
Iteration 201/1000 | Loss: 0.00002366
Iteration 202/1000 | Loss: 0.00002365
Iteration 203/1000 | Loss: 0.00002365
Iteration 204/1000 | Loss: 0.00002364
Iteration 205/1000 | Loss: 0.00031089
Iteration 206/1000 | Loss: 0.00014806
Iteration 207/1000 | Loss: 0.00002369
Iteration 208/1000 | Loss: 0.00002364
Iteration 209/1000 | Loss: 0.00002362
Iteration 210/1000 | Loss: 0.00002362
Iteration 211/1000 | Loss: 0.00002361
Iteration 212/1000 | Loss: 0.00002360
Iteration 213/1000 | Loss: 0.00031431
Iteration 214/1000 | Loss: 0.00015861
Iteration 215/1000 | Loss: 0.00033514
Iteration 216/1000 | Loss: 0.00023336
Iteration 217/1000 | Loss: 0.00056880
Iteration 218/1000 | Loss: 0.00006525
Iteration 219/1000 | Loss: 0.00031590
Iteration 220/1000 | Loss: 0.00002629
Iteration 221/1000 | Loss: 0.00010016
Iteration 222/1000 | Loss: 0.00003510
Iteration 223/1000 | Loss: 0.00002646
Iteration 224/1000 | Loss: 0.00002396
Iteration 225/1000 | Loss: 0.00002334
Iteration 226/1000 | Loss: 0.00002297
Iteration 227/1000 | Loss: 0.00002276
Iteration 228/1000 | Loss: 0.00002272
Iteration 229/1000 | Loss: 0.00002268
Iteration 230/1000 | Loss: 0.00002267
Iteration 231/1000 | Loss: 0.00002264
Iteration 232/1000 | Loss: 0.00002264
Iteration 233/1000 | Loss: 0.00002263
Iteration 234/1000 | Loss: 0.00002263
Iteration 235/1000 | Loss: 0.00002263
Iteration 236/1000 | Loss: 0.00002262
Iteration 237/1000 | Loss: 0.00002262
Iteration 238/1000 | Loss: 0.00002262
Iteration 239/1000 | Loss: 0.00002258
Iteration 240/1000 | Loss: 0.00002254
Iteration 241/1000 | Loss: 0.00002254
Iteration 242/1000 | Loss: 0.00002254
Iteration 243/1000 | Loss: 0.00002254
Iteration 244/1000 | Loss: 0.00002253
Iteration 245/1000 | Loss: 0.00002253
Iteration 246/1000 | Loss: 0.00002253
Iteration 247/1000 | Loss: 0.00002253
Iteration 248/1000 | Loss: 0.00002253
Iteration 249/1000 | Loss: 0.00002253
Iteration 250/1000 | Loss: 0.00002253
Iteration 251/1000 | Loss: 0.00002253
Iteration 252/1000 | Loss: 0.00002253
Iteration 253/1000 | Loss: 0.00002253
Iteration 254/1000 | Loss: 0.00002253
Iteration 255/1000 | Loss: 0.00002252
Iteration 256/1000 | Loss: 0.00002252
Iteration 257/1000 | Loss: 0.00002252
Iteration 258/1000 | Loss: 0.00002251
Iteration 259/1000 | Loss: 0.00002251
Iteration 260/1000 | Loss: 0.00002251
Iteration 261/1000 | Loss: 0.00002250
Iteration 262/1000 | Loss: 0.00002250
Iteration 263/1000 | Loss: 0.00002250
Iteration 264/1000 | Loss: 0.00002249
Iteration 265/1000 | Loss: 0.00002249
Iteration 266/1000 | Loss: 0.00002249
Iteration 267/1000 | Loss: 0.00002249
Iteration 268/1000 | Loss: 0.00002249
Iteration 269/1000 | Loss: 0.00002249
Iteration 270/1000 | Loss: 0.00002248
Iteration 271/1000 | Loss: 0.00002248
Iteration 272/1000 | Loss: 0.00002248
Iteration 273/1000 | Loss: 0.00002248
Iteration 274/1000 | Loss: 0.00002248
Iteration 275/1000 | Loss: 0.00002248
Iteration 276/1000 | Loss: 0.00002248
Iteration 277/1000 | Loss: 0.00002248
Iteration 278/1000 | Loss: 0.00002248
Iteration 279/1000 | Loss: 0.00002248
Iteration 280/1000 | Loss: 0.00002247
Iteration 281/1000 | Loss: 0.00002247
Iteration 282/1000 | Loss: 0.00002247
Iteration 283/1000 | Loss: 0.00002247
Iteration 284/1000 | Loss: 0.00002247
Iteration 285/1000 | Loss: 0.00002247
Iteration 286/1000 | Loss: 0.00002246
Iteration 287/1000 | Loss: 0.00002246
Iteration 288/1000 | Loss: 0.00002246
Iteration 289/1000 | Loss: 0.00002246
Iteration 290/1000 | Loss: 0.00002246
Iteration 291/1000 | Loss: 0.00002246
Iteration 292/1000 | Loss: 0.00002246
Iteration 293/1000 | Loss: 0.00002246
Iteration 294/1000 | Loss: 0.00002246
Iteration 295/1000 | Loss: 0.00002246
Iteration 296/1000 | Loss: 0.00002246
Iteration 297/1000 | Loss: 0.00002246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [2.246185795229394e-05, 2.246185795229394e-05, 2.246185795229394e-05, 2.246185795229394e-05, 2.246185795229394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.246185795229394e-05

Optimization complete. Final v2v error: 3.956777572631836 mm

Highest mean error: 6.819642066955566 mm for frame 17

Lowest mean error: 3.712526798248291 mm for frame 9

Saving results

Total time: 325.58664202690125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105445
Iteration 2/25 | Loss: 0.00245098
Iteration 3/25 | Loss: 0.00161266
Iteration 4/25 | Loss: 0.00119269
Iteration 5/25 | Loss: 0.00106523
Iteration 6/25 | Loss: 0.00104055
Iteration 7/25 | Loss: 0.00101840
Iteration 8/25 | Loss: 0.00102031
Iteration 9/25 | Loss: 0.00100645
Iteration 10/25 | Loss: 0.00100276
Iteration 11/25 | Loss: 0.00100421
Iteration 12/25 | Loss: 0.00100224
Iteration 13/25 | Loss: 0.00100426
Iteration 14/25 | Loss: 0.00099931
Iteration 15/25 | Loss: 0.00100223
Iteration 16/25 | Loss: 0.00100323
Iteration 17/25 | Loss: 0.00100951
Iteration 18/25 | Loss: 0.00100811
Iteration 19/25 | Loss: 0.00100078
Iteration 20/25 | Loss: 0.00099709
Iteration 21/25 | Loss: 0.00099611
Iteration 22/25 | Loss: 0.00099570
Iteration 23/25 | Loss: 0.00099519
Iteration 24/25 | Loss: 0.00099486
Iteration 25/25 | Loss: 0.00099737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63099921
Iteration 2/25 | Loss: 0.00240932
Iteration 3/25 | Loss: 0.00240932
Iteration 4/25 | Loss: 0.00240932
Iteration 5/25 | Loss: 0.00240931
Iteration 6/25 | Loss: 0.00240931
Iteration 7/25 | Loss: 0.00240931
Iteration 8/25 | Loss: 0.00240931
Iteration 9/25 | Loss: 0.00240931
Iteration 10/25 | Loss: 0.00240931
Iteration 11/25 | Loss: 0.00240931
Iteration 12/25 | Loss: 0.00240931
Iteration 13/25 | Loss: 0.00240931
Iteration 14/25 | Loss: 0.00240931
Iteration 15/25 | Loss: 0.00240931
Iteration 16/25 | Loss: 0.00240931
Iteration 17/25 | Loss: 0.00240931
Iteration 18/25 | Loss: 0.00240931
Iteration 19/25 | Loss: 0.00240931
Iteration 20/25 | Loss: 0.00240931
Iteration 21/25 | Loss: 0.00240931
Iteration 22/25 | Loss: 0.00240931
Iteration 23/25 | Loss: 0.00240931
Iteration 24/25 | Loss: 0.00240931
Iteration 25/25 | Loss: 0.00240931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240931
Iteration 2/1000 | Loss: 0.00012006
Iteration 3/1000 | Loss: 0.00008160
Iteration 4/1000 | Loss: 0.00082613
Iteration 5/1000 | Loss: 0.00106153
Iteration 6/1000 | Loss: 0.00013120
Iteration 7/1000 | Loss: 0.00004817
Iteration 8/1000 | Loss: 0.00004461
Iteration 9/1000 | Loss: 0.00004271
Iteration 10/1000 | Loss: 0.00057265
Iteration 11/1000 | Loss: 0.00039439
Iteration 12/1000 | Loss: 0.00069508
Iteration 13/1000 | Loss: 0.00094010
Iteration 14/1000 | Loss: 0.00081748
Iteration 15/1000 | Loss: 0.00055554
Iteration 16/1000 | Loss: 0.00046158
Iteration 17/1000 | Loss: 0.00047433
Iteration 18/1000 | Loss: 0.00051392
Iteration 19/1000 | Loss: 0.00005880
Iteration 20/1000 | Loss: 0.00003889
Iteration 21/1000 | Loss: 0.00003772
Iteration 22/1000 | Loss: 0.00003703
Iteration 23/1000 | Loss: 0.00004366
Iteration 24/1000 | Loss: 0.00005540
Iteration 25/1000 | Loss: 0.00003498
Iteration 26/1000 | Loss: 0.00004854
Iteration 27/1000 | Loss: 0.00003370
Iteration 28/1000 | Loss: 0.00003301
Iteration 29/1000 | Loss: 0.00003245
Iteration 30/1000 | Loss: 0.00003223
Iteration 31/1000 | Loss: 0.00003211
Iteration 32/1000 | Loss: 0.00003210
Iteration 33/1000 | Loss: 0.00003192
Iteration 34/1000 | Loss: 0.00003188
Iteration 35/1000 | Loss: 0.00003187
Iteration 36/1000 | Loss: 0.00003186
Iteration 37/1000 | Loss: 0.00023498
Iteration 38/1000 | Loss: 0.00032361
Iteration 39/1000 | Loss: 0.00003709
Iteration 40/1000 | Loss: 0.00003220
Iteration 41/1000 | Loss: 0.00003172
Iteration 42/1000 | Loss: 0.00003171
Iteration 43/1000 | Loss: 0.00003170
Iteration 44/1000 | Loss: 0.00003170
Iteration 45/1000 | Loss: 0.00003169
Iteration 46/1000 | Loss: 0.00003169
Iteration 47/1000 | Loss: 0.00003168
Iteration 48/1000 | Loss: 0.00003168
Iteration 49/1000 | Loss: 0.00003164
Iteration 50/1000 | Loss: 0.00003164
Iteration 51/1000 | Loss: 0.00003164
Iteration 52/1000 | Loss: 0.00003164
Iteration 53/1000 | Loss: 0.00003163
Iteration 54/1000 | Loss: 0.00003163
Iteration 55/1000 | Loss: 0.00003162
Iteration 56/1000 | Loss: 0.00003162
Iteration 57/1000 | Loss: 0.00003162
Iteration 58/1000 | Loss: 0.00003161
Iteration 59/1000 | Loss: 0.00003161
Iteration 60/1000 | Loss: 0.00003161
Iteration 61/1000 | Loss: 0.00003161
Iteration 62/1000 | Loss: 0.00003161
Iteration 63/1000 | Loss: 0.00003161
Iteration 64/1000 | Loss: 0.00003161
Iteration 65/1000 | Loss: 0.00003160
Iteration 66/1000 | Loss: 0.00003158
Iteration 67/1000 | Loss: 0.00003158
Iteration 68/1000 | Loss: 0.00003157
Iteration 69/1000 | Loss: 0.00003157
Iteration 70/1000 | Loss: 0.00003156
Iteration 71/1000 | Loss: 0.00003156
Iteration 72/1000 | Loss: 0.00003155
Iteration 73/1000 | Loss: 0.00003155
Iteration 74/1000 | Loss: 0.00003154
Iteration 75/1000 | Loss: 0.00003154
Iteration 76/1000 | Loss: 0.00003153
Iteration 77/1000 | Loss: 0.00003153
Iteration 78/1000 | Loss: 0.00003152
Iteration 79/1000 | Loss: 0.00003152
Iteration 80/1000 | Loss: 0.00003152
Iteration 81/1000 | Loss: 0.00003151
Iteration 82/1000 | Loss: 0.00003151
Iteration 83/1000 | Loss: 0.00003151
Iteration 84/1000 | Loss: 0.00003151
Iteration 85/1000 | Loss: 0.00003150
Iteration 86/1000 | Loss: 0.00003150
Iteration 87/1000 | Loss: 0.00003150
Iteration 88/1000 | Loss: 0.00003150
Iteration 89/1000 | Loss: 0.00003150
Iteration 90/1000 | Loss: 0.00003150
Iteration 91/1000 | Loss: 0.00003150
Iteration 92/1000 | Loss: 0.00003149
Iteration 93/1000 | Loss: 0.00003149
Iteration 94/1000 | Loss: 0.00003149
Iteration 95/1000 | Loss: 0.00003149
Iteration 96/1000 | Loss: 0.00003149
Iteration 97/1000 | Loss: 0.00023789
Iteration 98/1000 | Loss: 0.00003514
Iteration 99/1000 | Loss: 0.00003291
Iteration 100/1000 | Loss: 0.00003187
Iteration 101/1000 | Loss: 0.00003158
Iteration 102/1000 | Loss: 0.00003153
Iteration 103/1000 | Loss: 0.00003153
Iteration 104/1000 | Loss: 0.00003153
Iteration 105/1000 | Loss: 0.00003153
Iteration 106/1000 | Loss: 0.00003153
Iteration 107/1000 | Loss: 0.00003153
Iteration 108/1000 | Loss: 0.00003153
Iteration 109/1000 | Loss: 0.00003153
Iteration 110/1000 | Loss: 0.00003153
Iteration 111/1000 | Loss: 0.00003152
Iteration 112/1000 | Loss: 0.00003152
Iteration 113/1000 | Loss: 0.00003149
Iteration 114/1000 | Loss: 0.00003149
Iteration 115/1000 | Loss: 0.00003149
Iteration 116/1000 | Loss: 0.00003148
Iteration 117/1000 | Loss: 0.00003148
Iteration 118/1000 | Loss: 0.00003147
Iteration 119/1000 | Loss: 0.00003146
Iteration 120/1000 | Loss: 0.00003146
Iteration 121/1000 | Loss: 0.00003146
Iteration 122/1000 | Loss: 0.00003146
Iteration 123/1000 | Loss: 0.00003146
Iteration 124/1000 | Loss: 0.00003146
Iteration 125/1000 | Loss: 0.00003146
Iteration 126/1000 | Loss: 0.00003146
Iteration 127/1000 | Loss: 0.00003146
Iteration 128/1000 | Loss: 0.00003146
Iteration 129/1000 | Loss: 0.00003145
Iteration 130/1000 | Loss: 0.00003145
Iteration 131/1000 | Loss: 0.00003145
Iteration 132/1000 | Loss: 0.00003145
Iteration 133/1000 | Loss: 0.00003144
Iteration 134/1000 | Loss: 0.00003144
Iteration 135/1000 | Loss: 0.00003144
Iteration 136/1000 | Loss: 0.00003144
Iteration 137/1000 | Loss: 0.00003144
Iteration 138/1000 | Loss: 0.00003144
Iteration 139/1000 | Loss: 0.00003144
Iteration 140/1000 | Loss: 0.00003144
Iteration 141/1000 | Loss: 0.00003144
Iteration 142/1000 | Loss: 0.00003143
Iteration 143/1000 | Loss: 0.00003143
Iteration 144/1000 | Loss: 0.00003143
Iteration 145/1000 | Loss: 0.00003143
Iteration 146/1000 | Loss: 0.00003143
Iteration 147/1000 | Loss: 0.00003143
Iteration 148/1000 | Loss: 0.00003142
Iteration 149/1000 | Loss: 0.00003142
Iteration 150/1000 | Loss: 0.00003142
Iteration 151/1000 | Loss: 0.00003142
Iteration 152/1000 | Loss: 0.00003142
Iteration 153/1000 | Loss: 0.00003142
Iteration 154/1000 | Loss: 0.00003142
Iteration 155/1000 | Loss: 0.00003142
Iteration 156/1000 | Loss: 0.00003142
Iteration 157/1000 | Loss: 0.00003142
Iteration 158/1000 | Loss: 0.00003142
Iteration 159/1000 | Loss: 0.00003142
Iteration 160/1000 | Loss: 0.00003142
Iteration 161/1000 | Loss: 0.00003142
Iteration 162/1000 | Loss: 0.00003142
Iteration 163/1000 | Loss: 0.00003142
Iteration 164/1000 | Loss: 0.00003142
Iteration 165/1000 | Loss: 0.00003142
Iteration 166/1000 | Loss: 0.00003142
Iteration 167/1000 | Loss: 0.00003142
Iteration 168/1000 | Loss: 0.00003142
Iteration 169/1000 | Loss: 0.00003142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.141598790534772e-05, 3.141598790534772e-05, 3.141598790534772e-05, 3.141598790534772e-05, 3.141598790534772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.141598790534772e-05

Optimization complete. Final v2v error: 4.438222885131836 mm

Highest mean error: 16.329729080200195 mm for frame 138

Lowest mean error: 3.675191640853882 mm for frame 2

Saving results

Total time: 119.0153603553772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402612
Iteration 2/25 | Loss: 0.00097496
Iteration 3/25 | Loss: 0.00078883
Iteration 4/25 | Loss: 0.00076660
Iteration 5/25 | Loss: 0.00075875
Iteration 6/25 | Loss: 0.00075621
Iteration 7/25 | Loss: 0.00075540
Iteration 8/25 | Loss: 0.00075529
Iteration 9/25 | Loss: 0.00075529
Iteration 10/25 | Loss: 0.00075529
Iteration 11/25 | Loss: 0.00075529
Iteration 12/25 | Loss: 0.00075529
Iteration 13/25 | Loss: 0.00075529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007552933529950678, 0.0007552933529950678, 0.0007552933529950678, 0.0007552933529950678, 0.0007552933529950678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007552933529950678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45756376
Iteration 2/25 | Loss: 0.00118958
Iteration 3/25 | Loss: 0.00118956
Iteration 4/25 | Loss: 0.00118956
Iteration 5/25 | Loss: 0.00118956
Iteration 6/25 | Loss: 0.00118956
Iteration 7/25 | Loss: 0.00118956
Iteration 8/25 | Loss: 0.00118956
Iteration 9/25 | Loss: 0.00118955
Iteration 10/25 | Loss: 0.00118955
Iteration 11/25 | Loss: 0.00118955
Iteration 12/25 | Loss: 0.00118955
Iteration 13/25 | Loss: 0.00118955
Iteration 14/25 | Loss: 0.00118955
Iteration 15/25 | Loss: 0.00118955
Iteration 16/25 | Loss: 0.00118955
Iteration 17/25 | Loss: 0.00118955
Iteration 18/25 | Loss: 0.00118955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011895543430000544, 0.0011895543430000544, 0.0011895543430000544, 0.0011895543430000544, 0.0011895543430000544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011895543430000544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118955
Iteration 2/1000 | Loss: 0.00003694
Iteration 3/1000 | Loss: 0.00002542
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001521
Iteration 10/1000 | Loss: 0.00001502
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001474
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001472
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001462
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001458
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001452
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001446
Iteration 61/1000 | Loss: 0.00001446
Iteration 62/1000 | Loss: 0.00001445
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001441
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001433
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001433
Iteration 128/1000 | Loss: 0.00001433
Iteration 129/1000 | Loss: 0.00001433
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001433
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.43284814839717e-05, 1.43284814839717e-05, 1.43284814839717e-05, 1.43284814839717e-05, 1.43284814839717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.43284814839717e-05

Optimization complete. Final v2v error: 3.035688877105713 mm

Highest mean error: 5.19302225112915 mm for frame 88

Lowest mean error: 2.4108426570892334 mm for frame 127

Saving results

Total time: 40.322508811950684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471652
Iteration 2/25 | Loss: 0.00117071
Iteration 3/25 | Loss: 0.00091013
Iteration 4/25 | Loss: 0.00087215
Iteration 5/25 | Loss: 0.00086539
Iteration 6/25 | Loss: 0.00086339
Iteration 7/25 | Loss: 0.00086273
Iteration 8/25 | Loss: 0.00086273
Iteration 9/25 | Loss: 0.00086273
Iteration 10/25 | Loss: 0.00086273
Iteration 11/25 | Loss: 0.00086273
Iteration 12/25 | Loss: 0.00086273
Iteration 13/25 | Loss: 0.00086273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008627258939668536, 0.0008627258939668536, 0.0008627258939668536, 0.0008627258939668536, 0.0008627258939668536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008627258939668536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33811939
Iteration 2/25 | Loss: 0.00105405
Iteration 3/25 | Loss: 0.00105403
Iteration 4/25 | Loss: 0.00105403
Iteration 5/25 | Loss: 0.00105402
Iteration 6/25 | Loss: 0.00105402
Iteration 7/25 | Loss: 0.00105402
Iteration 8/25 | Loss: 0.00105402
Iteration 9/25 | Loss: 0.00105402
Iteration 10/25 | Loss: 0.00105402
Iteration 11/25 | Loss: 0.00105402
Iteration 12/25 | Loss: 0.00105402
Iteration 13/25 | Loss: 0.00105402
Iteration 14/25 | Loss: 0.00105402
Iteration 15/25 | Loss: 0.00105402
Iteration 16/25 | Loss: 0.00105402
Iteration 17/25 | Loss: 0.00105402
Iteration 18/25 | Loss: 0.00105402
Iteration 19/25 | Loss: 0.00105402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001054022228345275, 0.001054022228345275, 0.001054022228345275, 0.001054022228345275, 0.001054022228345275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001054022228345275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105402
Iteration 2/1000 | Loss: 0.00007386
Iteration 3/1000 | Loss: 0.00004862
Iteration 4/1000 | Loss: 0.00003918
Iteration 5/1000 | Loss: 0.00003700
Iteration 6/1000 | Loss: 0.00003565
Iteration 7/1000 | Loss: 0.00003438
Iteration 8/1000 | Loss: 0.00003383
Iteration 9/1000 | Loss: 0.00003327
Iteration 10/1000 | Loss: 0.00003280
Iteration 11/1000 | Loss: 0.00003246
Iteration 12/1000 | Loss: 0.00003217
Iteration 13/1000 | Loss: 0.00003213
Iteration 14/1000 | Loss: 0.00003194
Iteration 15/1000 | Loss: 0.00003179
Iteration 16/1000 | Loss: 0.00003170
Iteration 17/1000 | Loss: 0.00003154
Iteration 18/1000 | Loss: 0.00003148
Iteration 19/1000 | Loss: 0.00003133
Iteration 20/1000 | Loss: 0.00003131
Iteration 21/1000 | Loss: 0.00003121
Iteration 22/1000 | Loss: 0.00003121
Iteration 23/1000 | Loss: 0.00003118
Iteration 24/1000 | Loss: 0.00003116
Iteration 25/1000 | Loss: 0.00003116
Iteration 26/1000 | Loss: 0.00003115
Iteration 27/1000 | Loss: 0.00003114
Iteration 28/1000 | Loss: 0.00003114
Iteration 29/1000 | Loss: 0.00003113
Iteration 30/1000 | Loss: 0.00003110
Iteration 31/1000 | Loss: 0.00003109
Iteration 32/1000 | Loss: 0.00003108
Iteration 33/1000 | Loss: 0.00003108
Iteration 34/1000 | Loss: 0.00003107
Iteration 35/1000 | Loss: 0.00003107
Iteration 36/1000 | Loss: 0.00003107
Iteration 37/1000 | Loss: 0.00003104
Iteration 38/1000 | Loss: 0.00003101
Iteration 39/1000 | Loss: 0.00003097
Iteration 40/1000 | Loss: 0.00003096
Iteration 41/1000 | Loss: 0.00003093
Iteration 42/1000 | Loss: 0.00003093
Iteration 43/1000 | Loss: 0.00003092
Iteration 44/1000 | Loss: 0.00003091
Iteration 45/1000 | Loss: 0.00003089
Iteration 46/1000 | Loss: 0.00003089
Iteration 47/1000 | Loss: 0.00003088
Iteration 48/1000 | Loss: 0.00003088
Iteration 49/1000 | Loss: 0.00003087
Iteration 50/1000 | Loss: 0.00003087
Iteration 51/1000 | Loss: 0.00003087
Iteration 52/1000 | Loss: 0.00003086
Iteration 53/1000 | Loss: 0.00003086
Iteration 54/1000 | Loss: 0.00003085
Iteration 55/1000 | Loss: 0.00003084
Iteration 56/1000 | Loss: 0.00003083
Iteration 57/1000 | Loss: 0.00003082
Iteration 58/1000 | Loss: 0.00003082
Iteration 59/1000 | Loss: 0.00003082
Iteration 60/1000 | Loss: 0.00003081
Iteration 61/1000 | Loss: 0.00003081
Iteration 62/1000 | Loss: 0.00003081
Iteration 63/1000 | Loss: 0.00003081
Iteration 64/1000 | Loss: 0.00003080
Iteration 65/1000 | Loss: 0.00003080
Iteration 66/1000 | Loss: 0.00003080
Iteration 67/1000 | Loss: 0.00003079
Iteration 68/1000 | Loss: 0.00003079
Iteration 69/1000 | Loss: 0.00003078
Iteration 70/1000 | Loss: 0.00003078
Iteration 71/1000 | Loss: 0.00003077
Iteration 72/1000 | Loss: 0.00003076
Iteration 73/1000 | Loss: 0.00003076
Iteration 74/1000 | Loss: 0.00003076
Iteration 75/1000 | Loss: 0.00003076
Iteration 76/1000 | Loss: 0.00003076
Iteration 77/1000 | Loss: 0.00003076
Iteration 78/1000 | Loss: 0.00003076
Iteration 79/1000 | Loss: 0.00003076
Iteration 80/1000 | Loss: 0.00003076
Iteration 81/1000 | Loss: 0.00003076
Iteration 82/1000 | Loss: 0.00003075
Iteration 83/1000 | Loss: 0.00003075
Iteration 84/1000 | Loss: 0.00003075
Iteration 85/1000 | Loss: 0.00003075
Iteration 86/1000 | Loss: 0.00003075
Iteration 87/1000 | Loss: 0.00003075
Iteration 88/1000 | Loss: 0.00003075
Iteration 89/1000 | Loss: 0.00003075
Iteration 90/1000 | Loss: 0.00003074
Iteration 91/1000 | Loss: 0.00003074
Iteration 92/1000 | Loss: 0.00003073
Iteration 93/1000 | Loss: 0.00003073
Iteration 94/1000 | Loss: 0.00003073
Iteration 95/1000 | Loss: 0.00003072
Iteration 96/1000 | Loss: 0.00003072
Iteration 97/1000 | Loss: 0.00003072
Iteration 98/1000 | Loss: 0.00003072
Iteration 99/1000 | Loss: 0.00003072
Iteration 100/1000 | Loss: 0.00003072
Iteration 101/1000 | Loss: 0.00003072
Iteration 102/1000 | Loss: 0.00003072
Iteration 103/1000 | Loss: 0.00003072
Iteration 104/1000 | Loss: 0.00003072
Iteration 105/1000 | Loss: 0.00003071
Iteration 106/1000 | Loss: 0.00003071
Iteration 107/1000 | Loss: 0.00003071
Iteration 108/1000 | Loss: 0.00003071
Iteration 109/1000 | Loss: 0.00003070
Iteration 110/1000 | Loss: 0.00003070
Iteration 111/1000 | Loss: 0.00003070
Iteration 112/1000 | Loss: 0.00003069
Iteration 113/1000 | Loss: 0.00003069
Iteration 114/1000 | Loss: 0.00003069
Iteration 115/1000 | Loss: 0.00003069
Iteration 116/1000 | Loss: 0.00003068
Iteration 117/1000 | Loss: 0.00003068
Iteration 118/1000 | Loss: 0.00003068
Iteration 119/1000 | Loss: 0.00003068
Iteration 120/1000 | Loss: 0.00003068
Iteration 121/1000 | Loss: 0.00003067
Iteration 122/1000 | Loss: 0.00003067
Iteration 123/1000 | Loss: 0.00003067
Iteration 124/1000 | Loss: 0.00003067
Iteration 125/1000 | Loss: 0.00003067
Iteration 126/1000 | Loss: 0.00003067
Iteration 127/1000 | Loss: 0.00003067
Iteration 128/1000 | Loss: 0.00003066
Iteration 129/1000 | Loss: 0.00003066
Iteration 130/1000 | Loss: 0.00003066
Iteration 131/1000 | Loss: 0.00003065
Iteration 132/1000 | Loss: 0.00003065
Iteration 133/1000 | Loss: 0.00003065
Iteration 134/1000 | Loss: 0.00003065
Iteration 135/1000 | Loss: 0.00003065
Iteration 136/1000 | Loss: 0.00003064
Iteration 137/1000 | Loss: 0.00003064
Iteration 138/1000 | Loss: 0.00003064
Iteration 139/1000 | Loss: 0.00003064
Iteration 140/1000 | Loss: 0.00003064
Iteration 141/1000 | Loss: 0.00003064
Iteration 142/1000 | Loss: 0.00003063
Iteration 143/1000 | Loss: 0.00003063
Iteration 144/1000 | Loss: 0.00003063
Iteration 145/1000 | Loss: 0.00003063
Iteration 146/1000 | Loss: 0.00003063
Iteration 147/1000 | Loss: 0.00003063
Iteration 148/1000 | Loss: 0.00003063
Iteration 149/1000 | Loss: 0.00003063
Iteration 150/1000 | Loss: 0.00003062
Iteration 151/1000 | Loss: 0.00003062
Iteration 152/1000 | Loss: 0.00003062
Iteration 153/1000 | Loss: 0.00003062
Iteration 154/1000 | Loss: 0.00003062
Iteration 155/1000 | Loss: 0.00003061
Iteration 156/1000 | Loss: 0.00003061
Iteration 157/1000 | Loss: 0.00003061
Iteration 158/1000 | Loss: 0.00003061
Iteration 159/1000 | Loss: 0.00003061
Iteration 160/1000 | Loss: 0.00003061
Iteration 161/1000 | Loss: 0.00003061
Iteration 162/1000 | Loss: 0.00003061
Iteration 163/1000 | Loss: 0.00003061
Iteration 164/1000 | Loss: 0.00003060
Iteration 165/1000 | Loss: 0.00003060
Iteration 166/1000 | Loss: 0.00003060
Iteration 167/1000 | Loss: 0.00003060
Iteration 168/1000 | Loss: 0.00003060
Iteration 169/1000 | Loss: 0.00003060
Iteration 170/1000 | Loss: 0.00003060
Iteration 171/1000 | Loss: 0.00003060
Iteration 172/1000 | Loss: 0.00003060
Iteration 173/1000 | Loss: 0.00003060
Iteration 174/1000 | Loss: 0.00003060
Iteration 175/1000 | Loss: 0.00003060
Iteration 176/1000 | Loss: 0.00003060
Iteration 177/1000 | Loss: 0.00003060
Iteration 178/1000 | Loss: 0.00003059
Iteration 179/1000 | Loss: 0.00003059
Iteration 180/1000 | Loss: 0.00003059
Iteration 181/1000 | Loss: 0.00003059
Iteration 182/1000 | Loss: 0.00003059
Iteration 183/1000 | Loss: 0.00003059
Iteration 184/1000 | Loss: 0.00003059
Iteration 185/1000 | Loss: 0.00003059
Iteration 186/1000 | Loss: 0.00003059
Iteration 187/1000 | Loss: 0.00003059
Iteration 188/1000 | Loss: 0.00003059
Iteration 189/1000 | Loss: 0.00003058
Iteration 190/1000 | Loss: 0.00003058
Iteration 191/1000 | Loss: 0.00003058
Iteration 192/1000 | Loss: 0.00003058
Iteration 193/1000 | Loss: 0.00003058
Iteration 194/1000 | Loss: 0.00003058
Iteration 195/1000 | Loss: 0.00003058
Iteration 196/1000 | Loss: 0.00003058
Iteration 197/1000 | Loss: 0.00003058
Iteration 198/1000 | Loss: 0.00003058
Iteration 199/1000 | Loss: 0.00003058
Iteration 200/1000 | Loss: 0.00003058
Iteration 201/1000 | Loss: 0.00003057
Iteration 202/1000 | Loss: 0.00003057
Iteration 203/1000 | Loss: 0.00003057
Iteration 204/1000 | Loss: 0.00003057
Iteration 205/1000 | Loss: 0.00003057
Iteration 206/1000 | Loss: 0.00003057
Iteration 207/1000 | Loss: 0.00003057
Iteration 208/1000 | Loss: 0.00003057
Iteration 209/1000 | Loss: 0.00003057
Iteration 210/1000 | Loss: 0.00003056
Iteration 211/1000 | Loss: 0.00003056
Iteration 212/1000 | Loss: 0.00003056
Iteration 213/1000 | Loss: 0.00003056
Iteration 214/1000 | Loss: 0.00003056
Iteration 215/1000 | Loss: 0.00003056
Iteration 216/1000 | Loss: 0.00003056
Iteration 217/1000 | Loss: 0.00003056
Iteration 218/1000 | Loss: 0.00003055
Iteration 219/1000 | Loss: 0.00003055
Iteration 220/1000 | Loss: 0.00003055
Iteration 221/1000 | Loss: 0.00003055
Iteration 222/1000 | Loss: 0.00003055
Iteration 223/1000 | Loss: 0.00003055
Iteration 224/1000 | Loss: 0.00003055
Iteration 225/1000 | Loss: 0.00003055
Iteration 226/1000 | Loss: 0.00003055
Iteration 227/1000 | Loss: 0.00003055
Iteration 228/1000 | Loss: 0.00003054
Iteration 229/1000 | Loss: 0.00003054
Iteration 230/1000 | Loss: 0.00003054
Iteration 231/1000 | Loss: 0.00003054
Iteration 232/1000 | Loss: 0.00003054
Iteration 233/1000 | Loss: 0.00003054
Iteration 234/1000 | Loss: 0.00003054
Iteration 235/1000 | Loss: 0.00003054
Iteration 236/1000 | Loss: 0.00003054
Iteration 237/1000 | Loss: 0.00003053
Iteration 238/1000 | Loss: 0.00003053
Iteration 239/1000 | Loss: 0.00003053
Iteration 240/1000 | Loss: 0.00003053
Iteration 241/1000 | Loss: 0.00003053
Iteration 242/1000 | Loss: 0.00003053
Iteration 243/1000 | Loss: 0.00003053
Iteration 244/1000 | Loss: 0.00003053
Iteration 245/1000 | Loss: 0.00003053
Iteration 246/1000 | Loss: 0.00003053
Iteration 247/1000 | Loss: 0.00003053
Iteration 248/1000 | Loss: 0.00003053
Iteration 249/1000 | Loss: 0.00003053
Iteration 250/1000 | Loss: 0.00003053
Iteration 251/1000 | Loss: 0.00003053
Iteration 252/1000 | Loss: 0.00003053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.05304529319983e-05, 3.05304529319983e-05, 3.05304529319983e-05, 3.05304529319983e-05, 3.05304529319983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.05304529319983e-05

Optimization complete. Final v2v error: 4.449879169464111 mm

Highest mean error: 6.042940139770508 mm for frame 74

Lowest mean error: 3.4121196269989014 mm for frame 35

Saving results

Total time: 55.35626435279846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069288
Iteration 2/25 | Loss: 0.00280849
Iteration 3/25 | Loss: 0.00213618
Iteration 4/25 | Loss: 0.00185575
Iteration 5/25 | Loss: 0.00195548
Iteration 6/25 | Loss: 0.00198858
Iteration 7/25 | Loss: 0.00152325
Iteration 8/25 | Loss: 0.00115978
Iteration 9/25 | Loss: 0.00099420
Iteration 10/25 | Loss: 0.00094480
Iteration 11/25 | Loss: 0.00092339
Iteration 12/25 | Loss: 0.00091249
Iteration 13/25 | Loss: 0.00090434
Iteration 14/25 | Loss: 0.00090096
Iteration 15/25 | Loss: 0.00090701
Iteration 16/25 | Loss: 0.00090198
Iteration 17/25 | Loss: 0.00090124
Iteration 18/25 | Loss: 0.00089222
Iteration 19/25 | Loss: 0.00089016
Iteration 20/25 | Loss: 0.00088876
Iteration 21/25 | Loss: 0.00088845
Iteration 22/25 | Loss: 0.00088829
Iteration 23/25 | Loss: 0.00088827
Iteration 24/25 | Loss: 0.00088827
Iteration 25/25 | Loss: 0.00088827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47879803
Iteration 2/25 | Loss: 0.00139468
Iteration 3/25 | Loss: 0.00129683
Iteration 4/25 | Loss: 0.00129682
Iteration 5/25 | Loss: 0.00129682
Iteration 6/25 | Loss: 0.00129682
Iteration 7/25 | Loss: 0.00129682
Iteration 8/25 | Loss: 0.00129682
Iteration 9/25 | Loss: 0.00129682
Iteration 10/25 | Loss: 0.00129682
Iteration 11/25 | Loss: 0.00129682
Iteration 12/25 | Loss: 0.00129682
Iteration 13/25 | Loss: 0.00129682
Iteration 14/25 | Loss: 0.00129682
Iteration 15/25 | Loss: 0.00129682
Iteration 16/25 | Loss: 0.00129682
Iteration 17/25 | Loss: 0.00129682
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012968223309144378, 0.0012968223309144378, 0.0012968223309144378, 0.0012968223309144378, 0.0012968223309144378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012968223309144378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129682
Iteration 2/1000 | Loss: 0.00018215
Iteration 3/1000 | Loss: 0.00058897
Iteration 4/1000 | Loss: 0.00008778
Iteration 5/1000 | Loss: 0.00004617
Iteration 6/1000 | Loss: 0.00003810
Iteration 7/1000 | Loss: 0.00003394
Iteration 8/1000 | Loss: 0.00003073
Iteration 9/1000 | Loss: 0.00002867
Iteration 10/1000 | Loss: 0.00023960
Iteration 11/1000 | Loss: 0.00002791
Iteration 12/1000 | Loss: 0.00002701
Iteration 13/1000 | Loss: 0.00028245
Iteration 14/1000 | Loss: 0.00008420
Iteration 15/1000 | Loss: 0.00006538
Iteration 16/1000 | Loss: 0.00002561
Iteration 17/1000 | Loss: 0.00016653
Iteration 18/1000 | Loss: 0.00002377
Iteration 19/1000 | Loss: 0.00002310
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002234
Iteration 22/1000 | Loss: 0.00002223
Iteration 23/1000 | Loss: 0.00012047
Iteration 24/1000 | Loss: 0.00013036
Iteration 25/1000 | Loss: 0.00002337
Iteration 26/1000 | Loss: 0.00002225
Iteration 27/1000 | Loss: 0.00002206
Iteration 28/1000 | Loss: 0.00006869
Iteration 29/1000 | Loss: 0.00002211
Iteration 30/1000 | Loss: 0.00002193
Iteration 31/1000 | Loss: 0.00002193
Iteration 32/1000 | Loss: 0.00002193
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002193
Iteration 35/1000 | Loss: 0.00002193
Iteration 36/1000 | Loss: 0.00002193
Iteration 37/1000 | Loss: 0.00002193
Iteration 38/1000 | Loss: 0.00002193
Iteration 39/1000 | Loss: 0.00002193
Iteration 40/1000 | Loss: 0.00002193
Iteration 41/1000 | Loss: 0.00002192
Iteration 42/1000 | Loss: 0.00002192
Iteration 43/1000 | Loss: 0.00002192
Iteration 44/1000 | Loss: 0.00002192
Iteration 45/1000 | Loss: 0.00002191
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002190
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002190
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002189
Iteration 54/1000 | Loss: 0.00002189
Iteration 55/1000 | Loss: 0.00002188
Iteration 56/1000 | Loss: 0.00002188
Iteration 57/1000 | Loss: 0.00002188
Iteration 58/1000 | Loss: 0.00002188
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002187
Iteration 61/1000 | Loss: 0.00002187
Iteration 62/1000 | Loss: 0.00002187
Iteration 63/1000 | Loss: 0.00002187
Iteration 64/1000 | Loss: 0.00002187
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002187
Iteration 68/1000 | Loss: 0.00002187
Iteration 69/1000 | Loss: 0.00002187
Iteration 70/1000 | Loss: 0.00002187
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002186
Iteration 77/1000 | Loss: 0.00002186
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00006732
Iteration 80/1000 | Loss: 0.00006675
Iteration 81/1000 | Loss: 0.00002195
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002186
Iteration 84/1000 | Loss: 0.00002186
Iteration 85/1000 | Loss: 0.00002186
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002186
Iteration 89/1000 | Loss: 0.00002186
Iteration 90/1000 | Loss: 0.00002186
Iteration 91/1000 | Loss: 0.00002186
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002185
Iteration 96/1000 | Loss: 0.00002185
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002184
Iteration 101/1000 | Loss: 0.00002184
Iteration 102/1000 | Loss: 0.00002184
Iteration 103/1000 | Loss: 0.00002184
Iteration 104/1000 | Loss: 0.00002184
Iteration 105/1000 | Loss: 0.00002184
Iteration 106/1000 | Loss: 0.00002184
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00004564
Iteration 111/1000 | Loss: 0.00002189
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002185
Iteration 115/1000 | Loss: 0.00002185
Iteration 116/1000 | Loss: 0.00002185
Iteration 117/1000 | Loss: 0.00002185
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002184
Iteration 121/1000 | Loss: 0.00002183
Iteration 122/1000 | Loss: 0.00002183
Iteration 123/1000 | Loss: 0.00002183
Iteration 124/1000 | Loss: 0.00002182
Iteration 125/1000 | Loss: 0.00002182
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002181
Iteration 128/1000 | Loss: 0.00002181
Iteration 129/1000 | Loss: 0.00002181
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002180
Iteration 133/1000 | Loss: 0.00002180
Iteration 134/1000 | Loss: 0.00002180
Iteration 135/1000 | Loss: 0.00002179
Iteration 136/1000 | Loss: 0.00002179
Iteration 137/1000 | Loss: 0.00002179
Iteration 138/1000 | Loss: 0.00002179
Iteration 139/1000 | Loss: 0.00002179
Iteration 140/1000 | Loss: 0.00002178
Iteration 141/1000 | Loss: 0.00002178
Iteration 142/1000 | Loss: 0.00002178
Iteration 143/1000 | Loss: 0.00002178
Iteration 144/1000 | Loss: 0.00002178
Iteration 145/1000 | Loss: 0.00002178
Iteration 146/1000 | Loss: 0.00002178
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002177
Iteration 149/1000 | Loss: 0.00002177
Iteration 150/1000 | Loss: 0.00002177
Iteration 151/1000 | Loss: 0.00002177
Iteration 152/1000 | Loss: 0.00002177
Iteration 153/1000 | Loss: 0.00002177
Iteration 154/1000 | Loss: 0.00002177
Iteration 155/1000 | Loss: 0.00002177
Iteration 156/1000 | Loss: 0.00002177
Iteration 157/1000 | Loss: 0.00002177
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002176
Iteration 160/1000 | Loss: 0.00002176
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002176
Iteration 167/1000 | Loss: 0.00002176
Iteration 168/1000 | Loss: 0.00002176
Iteration 169/1000 | Loss: 0.00002176
Iteration 170/1000 | Loss: 0.00002176
Iteration 171/1000 | Loss: 0.00002176
Iteration 172/1000 | Loss: 0.00002176
Iteration 173/1000 | Loss: 0.00002176
Iteration 174/1000 | Loss: 0.00002176
Iteration 175/1000 | Loss: 0.00002176
Iteration 176/1000 | Loss: 0.00002176
Iteration 177/1000 | Loss: 0.00002176
Iteration 178/1000 | Loss: 0.00002176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.1762556571047753e-05, 2.1762556571047753e-05, 2.1762556571047753e-05, 2.1762556571047753e-05, 2.1762556571047753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1762556571047753e-05

Optimization complete. Final v2v error: 3.9859511852264404 mm

Highest mean error: 4.271743297576904 mm for frame 59

Lowest mean error: 3.8121132850646973 mm for frame 129

Saving results

Total time: 92.07937479019165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454648
Iteration 2/25 | Loss: 0.00104653
Iteration 3/25 | Loss: 0.00088699
Iteration 4/25 | Loss: 0.00085726
Iteration 5/25 | Loss: 0.00085307
Iteration 6/25 | Loss: 0.00085220
Iteration 7/25 | Loss: 0.00085204
Iteration 8/25 | Loss: 0.00085204
Iteration 9/25 | Loss: 0.00085204
Iteration 10/25 | Loss: 0.00085204
Iteration 11/25 | Loss: 0.00085204
Iteration 12/25 | Loss: 0.00085204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008520403644070029, 0.0008520403644070029, 0.0008520403644070029, 0.0008520403644070029, 0.0008520403644070029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008520403644070029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55501306
Iteration 2/25 | Loss: 0.00128623
Iteration 3/25 | Loss: 0.00128623
Iteration 4/25 | Loss: 0.00128623
Iteration 5/25 | Loss: 0.00128623
Iteration 6/25 | Loss: 0.00128623
Iteration 7/25 | Loss: 0.00128623
Iteration 8/25 | Loss: 0.00128623
Iteration 9/25 | Loss: 0.00128623
Iteration 10/25 | Loss: 0.00128623
Iteration 11/25 | Loss: 0.00128623
Iteration 12/25 | Loss: 0.00128623
Iteration 13/25 | Loss: 0.00128623
Iteration 14/25 | Loss: 0.00128623
Iteration 15/25 | Loss: 0.00128623
Iteration 16/25 | Loss: 0.00128623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001286230399273336, 0.001286230399273336, 0.001286230399273336, 0.001286230399273336, 0.001286230399273336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001286230399273336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128623
Iteration 2/1000 | Loss: 0.00006123
Iteration 3/1000 | Loss: 0.00003678
Iteration 4/1000 | Loss: 0.00003306
Iteration 5/1000 | Loss: 0.00003157
Iteration 6/1000 | Loss: 0.00003053
Iteration 7/1000 | Loss: 0.00002983
Iteration 8/1000 | Loss: 0.00002925
Iteration 9/1000 | Loss: 0.00002889
Iteration 10/1000 | Loss: 0.00002856
Iteration 11/1000 | Loss: 0.00002839
Iteration 12/1000 | Loss: 0.00002818
Iteration 13/1000 | Loss: 0.00002811
Iteration 14/1000 | Loss: 0.00002811
Iteration 15/1000 | Loss: 0.00002808
Iteration 16/1000 | Loss: 0.00002807
Iteration 17/1000 | Loss: 0.00002807
Iteration 18/1000 | Loss: 0.00002807
Iteration 19/1000 | Loss: 0.00002806
Iteration 20/1000 | Loss: 0.00002806
Iteration 21/1000 | Loss: 0.00002791
Iteration 22/1000 | Loss: 0.00002787
Iteration 23/1000 | Loss: 0.00002781
Iteration 24/1000 | Loss: 0.00002780
Iteration 25/1000 | Loss: 0.00002780
Iteration 26/1000 | Loss: 0.00002779
Iteration 27/1000 | Loss: 0.00002779
Iteration 28/1000 | Loss: 0.00002779
Iteration 29/1000 | Loss: 0.00002778
Iteration 30/1000 | Loss: 0.00002778
Iteration 31/1000 | Loss: 0.00002778
Iteration 32/1000 | Loss: 0.00002778
Iteration 33/1000 | Loss: 0.00002778
Iteration 34/1000 | Loss: 0.00002778
Iteration 35/1000 | Loss: 0.00002778
Iteration 36/1000 | Loss: 0.00002778
Iteration 37/1000 | Loss: 0.00002778
Iteration 38/1000 | Loss: 0.00002778
Iteration 39/1000 | Loss: 0.00002778
Iteration 40/1000 | Loss: 0.00002777
Iteration 41/1000 | Loss: 0.00002777
Iteration 42/1000 | Loss: 0.00002777
Iteration 43/1000 | Loss: 0.00002777
Iteration 44/1000 | Loss: 0.00002776
Iteration 45/1000 | Loss: 0.00002776
Iteration 46/1000 | Loss: 0.00002776
Iteration 47/1000 | Loss: 0.00002776
Iteration 48/1000 | Loss: 0.00002775
Iteration 49/1000 | Loss: 0.00002775
Iteration 50/1000 | Loss: 0.00002774
Iteration 51/1000 | Loss: 0.00002774
Iteration 52/1000 | Loss: 0.00002774
Iteration 53/1000 | Loss: 0.00002773
Iteration 54/1000 | Loss: 0.00002773
Iteration 55/1000 | Loss: 0.00002773
Iteration 56/1000 | Loss: 0.00002773
Iteration 57/1000 | Loss: 0.00002773
Iteration 58/1000 | Loss: 0.00002772
Iteration 59/1000 | Loss: 0.00002772
Iteration 60/1000 | Loss: 0.00002772
Iteration 61/1000 | Loss: 0.00002772
Iteration 62/1000 | Loss: 0.00002771
Iteration 63/1000 | Loss: 0.00002771
Iteration 64/1000 | Loss: 0.00002771
Iteration 65/1000 | Loss: 0.00002771
Iteration 66/1000 | Loss: 0.00002770
Iteration 67/1000 | Loss: 0.00002770
Iteration 68/1000 | Loss: 0.00002770
Iteration 69/1000 | Loss: 0.00002769
Iteration 70/1000 | Loss: 0.00002769
Iteration 71/1000 | Loss: 0.00002769
Iteration 72/1000 | Loss: 0.00002768
Iteration 73/1000 | Loss: 0.00002768
Iteration 74/1000 | Loss: 0.00002768
Iteration 75/1000 | Loss: 0.00002768
Iteration 76/1000 | Loss: 0.00002767
Iteration 77/1000 | Loss: 0.00002767
Iteration 78/1000 | Loss: 0.00002767
Iteration 79/1000 | Loss: 0.00002766
Iteration 80/1000 | Loss: 0.00002766
Iteration 81/1000 | Loss: 0.00002766
Iteration 82/1000 | Loss: 0.00002766
Iteration 83/1000 | Loss: 0.00002766
Iteration 84/1000 | Loss: 0.00002766
Iteration 85/1000 | Loss: 0.00002766
Iteration 86/1000 | Loss: 0.00002766
Iteration 87/1000 | Loss: 0.00002766
Iteration 88/1000 | Loss: 0.00002766
Iteration 89/1000 | Loss: 0.00002766
Iteration 90/1000 | Loss: 0.00002766
Iteration 91/1000 | Loss: 0.00002765
Iteration 92/1000 | Loss: 0.00002765
Iteration 93/1000 | Loss: 0.00002765
Iteration 94/1000 | Loss: 0.00002765
Iteration 95/1000 | Loss: 0.00002764
Iteration 96/1000 | Loss: 0.00002764
Iteration 97/1000 | Loss: 0.00002764
Iteration 98/1000 | Loss: 0.00002764
Iteration 99/1000 | Loss: 0.00002764
Iteration 100/1000 | Loss: 0.00002764
Iteration 101/1000 | Loss: 0.00002764
Iteration 102/1000 | Loss: 0.00002764
Iteration 103/1000 | Loss: 0.00002764
Iteration 104/1000 | Loss: 0.00002764
Iteration 105/1000 | Loss: 0.00002764
Iteration 106/1000 | Loss: 0.00002764
Iteration 107/1000 | Loss: 0.00002763
Iteration 108/1000 | Loss: 0.00002763
Iteration 109/1000 | Loss: 0.00002763
Iteration 110/1000 | Loss: 0.00002763
Iteration 111/1000 | Loss: 0.00002763
Iteration 112/1000 | Loss: 0.00002763
Iteration 113/1000 | Loss: 0.00002763
Iteration 114/1000 | Loss: 0.00002763
Iteration 115/1000 | Loss: 0.00002763
Iteration 116/1000 | Loss: 0.00002762
Iteration 117/1000 | Loss: 0.00002762
Iteration 118/1000 | Loss: 0.00002762
Iteration 119/1000 | Loss: 0.00002762
Iteration 120/1000 | Loss: 0.00002762
Iteration 121/1000 | Loss: 0.00002762
Iteration 122/1000 | Loss: 0.00002762
Iteration 123/1000 | Loss: 0.00002762
Iteration 124/1000 | Loss: 0.00002761
Iteration 125/1000 | Loss: 0.00002761
Iteration 126/1000 | Loss: 0.00002761
Iteration 127/1000 | Loss: 0.00002761
Iteration 128/1000 | Loss: 0.00002761
Iteration 129/1000 | Loss: 0.00002761
Iteration 130/1000 | Loss: 0.00002761
Iteration 131/1000 | Loss: 0.00002760
Iteration 132/1000 | Loss: 0.00002760
Iteration 133/1000 | Loss: 0.00002760
Iteration 134/1000 | Loss: 0.00002760
Iteration 135/1000 | Loss: 0.00002760
Iteration 136/1000 | Loss: 0.00002760
Iteration 137/1000 | Loss: 0.00002760
Iteration 138/1000 | Loss: 0.00002760
Iteration 139/1000 | Loss: 0.00002760
Iteration 140/1000 | Loss: 0.00002760
Iteration 141/1000 | Loss: 0.00002760
Iteration 142/1000 | Loss: 0.00002760
Iteration 143/1000 | Loss: 0.00002760
Iteration 144/1000 | Loss: 0.00002760
Iteration 145/1000 | Loss: 0.00002759
Iteration 146/1000 | Loss: 0.00002759
Iteration 147/1000 | Loss: 0.00002759
Iteration 148/1000 | Loss: 0.00002759
Iteration 149/1000 | Loss: 0.00002759
Iteration 150/1000 | Loss: 0.00002759
Iteration 151/1000 | Loss: 0.00002759
Iteration 152/1000 | Loss: 0.00002759
Iteration 153/1000 | Loss: 0.00002759
Iteration 154/1000 | Loss: 0.00002759
Iteration 155/1000 | Loss: 0.00002759
Iteration 156/1000 | Loss: 0.00002759
Iteration 157/1000 | Loss: 0.00002759
Iteration 158/1000 | Loss: 0.00002759
Iteration 159/1000 | Loss: 0.00002758
Iteration 160/1000 | Loss: 0.00002758
Iteration 161/1000 | Loss: 0.00002758
Iteration 162/1000 | Loss: 0.00002758
Iteration 163/1000 | Loss: 0.00002758
Iteration 164/1000 | Loss: 0.00002758
Iteration 165/1000 | Loss: 0.00002758
Iteration 166/1000 | Loss: 0.00002758
Iteration 167/1000 | Loss: 0.00002758
Iteration 168/1000 | Loss: 0.00002758
Iteration 169/1000 | Loss: 0.00002758
Iteration 170/1000 | Loss: 0.00002758
Iteration 171/1000 | Loss: 0.00002758
Iteration 172/1000 | Loss: 0.00002758
Iteration 173/1000 | Loss: 0.00002758
Iteration 174/1000 | Loss: 0.00002758
Iteration 175/1000 | Loss: 0.00002758
Iteration 176/1000 | Loss: 0.00002758
Iteration 177/1000 | Loss: 0.00002758
Iteration 178/1000 | Loss: 0.00002758
Iteration 179/1000 | Loss: 0.00002757
Iteration 180/1000 | Loss: 0.00002757
Iteration 181/1000 | Loss: 0.00002757
Iteration 182/1000 | Loss: 0.00002757
Iteration 183/1000 | Loss: 0.00002757
Iteration 184/1000 | Loss: 0.00002757
Iteration 185/1000 | Loss: 0.00002757
Iteration 186/1000 | Loss: 0.00002757
Iteration 187/1000 | Loss: 0.00002757
Iteration 188/1000 | Loss: 0.00002757
Iteration 189/1000 | Loss: 0.00002757
Iteration 190/1000 | Loss: 0.00002757
Iteration 191/1000 | Loss: 0.00002757
Iteration 192/1000 | Loss: 0.00002757
Iteration 193/1000 | Loss: 0.00002757
Iteration 194/1000 | Loss: 0.00002757
Iteration 195/1000 | Loss: 0.00002757
Iteration 196/1000 | Loss: 0.00002757
Iteration 197/1000 | Loss: 0.00002757
Iteration 198/1000 | Loss: 0.00002757
Iteration 199/1000 | Loss: 0.00002757
Iteration 200/1000 | Loss: 0.00002757
Iteration 201/1000 | Loss: 0.00002757
Iteration 202/1000 | Loss: 0.00002757
Iteration 203/1000 | Loss: 0.00002757
Iteration 204/1000 | Loss: 0.00002757
Iteration 205/1000 | Loss: 0.00002757
Iteration 206/1000 | Loss: 0.00002757
Iteration 207/1000 | Loss: 0.00002757
Iteration 208/1000 | Loss: 0.00002757
Iteration 209/1000 | Loss: 0.00002757
Iteration 210/1000 | Loss: 0.00002757
Iteration 211/1000 | Loss: 0.00002757
Iteration 212/1000 | Loss: 0.00002757
Iteration 213/1000 | Loss: 0.00002757
Iteration 214/1000 | Loss: 0.00002757
Iteration 215/1000 | Loss: 0.00002757
Iteration 216/1000 | Loss: 0.00002757
Iteration 217/1000 | Loss: 0.00002757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.756517096713651e-05, 2.756517096713651e-05, 2.756517096713651e-05, 2.756517096713651e-05, 2.756517096713651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.756517096713651e-05

Optimization complete. Final v2v error: 4.329107284545898 mm

Highest mean error: 4.946897983551025 mm for frame 31

Lowest mean error: 4.161247730255127 mm for frame 17

Saving results

Total time: 42.46611213684082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860392
Iteration 2/25 | Loss: 0.00115089
Iteration 3/25 | Loss: 0.00093041
Iteration 4/25 | Loss: 0.00087743
Iteration 5/25 | Loss: 0.00085030
Iteration 6/25 | Loss: 0.00084465
Iteration 7/25 | Loss: 0.00084242
Iteration 8/25 | Loss: 0.00084168
Iteration 9/25 | Loss: 0.00084168
Iteration 10/25 | Loss: 0.00084168
Iteration 11/25 | Loss: 0.00084168
Iteration 12/25 | Loss: 0.00084168
Iteration 13/25 | Loss: 0.00084168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008416823111474514, 0.0008416823111474514, 0.0008416823111474514, 0.0008416823111474514, 0.0008416823111474514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008416823111474514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76254773
Iteration 2/25 | Loss: 0.00196583
Iteration 3/25 | Loss: 0.00196583
Iteration 4/25 | Loss: 0.00196583
Iteration 5/25 | Loss: 0.00196583
Iteration 6/25 | Loss: 0.00196583
Iteration 7/25 | Loss: 0.00196583
Iteration 8/25 | Loss: 0.00196583
Iteration 9/25 | Loss: 0.00196583
Iteration 10/25 | Loss: 0.00196583
Iteration 11/25 | Loss: 0.00196583
Iteration 12/25 | Loss: 0.00196583
Iteration 13/25 | Loss: 0.00196583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001965827774256468, 0.001965827774256468, 0.001965827774256468, 0.001965827774256468, 0.001965827774256468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001965827774256468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196583
Iteration 2/1000 | Loss: 0.00005445
Iteration 3/1000 | Loss: 0.00004301
Iteration 4/1000 | Loss: 0.00003415
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00002960
Iteration 7/1000 | Loss: 0.00002832
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002640
Iteration 10/1000 | Loss: 0.00002569
Iteration 11/1000 | Loss: 0.00002530
Iteration 12/1000 | Loss: 0.00002500
Iteration 13/1000 | Loss: 0.00002473
Iteration 14/1000 | Loss: 0.00002448
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00002431
Iteration 17/1000 | Loss: 0.00002424
Iteration 18/1000 | Loss: 0.00002424
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00002416
Iteration 21/1000 | Loss: 0.00002415
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002413
Iteration 24/1000 | Loss: 0.00002413
Iteration 25/1000 | Loss: 0.00002413
Iteration 26/1000 | Loss: 0.00002413
Iteration 27/1000 | Loss: 0.00002413
Iteration 28/1000 | Loss: 0.00002413
Iteration 29/1000 | Loss: 0.00002412
Iteration 30/1000 | Loss: 0.00002412
Iteration 31/1000 | Loss: 0.00002412
Iteration 32/1000 | Loss: 0.00002412
Iteration 33/1000 | Loss: 0.00002412
Iteration 34/1000 | Loss: 0.00002412
Iteration 35/1000 | Loss: 0.00002412
Iteration 36/1000 | Loss: 0.00002412
Iteration 37/1000 | Loss: 0.00002412
Iteration 38/1000 | Loss: 0.00002412
Iteration 39/1000 | Loss: 0.00002410
Iteration 40/1000 | Loss: 0.00002409
Iteration 41/1000 | Loss: 0.00002409
Iteration 42/1000 | Loss: 0.00002408
Iteration 43/1000 | Loss: 0.00002408
Iteration 44/1000 | Loss: 0.00002407
Iteration 45/1000 | Loss: 0.00002407
Iteration 46/1000 | Loss: 0.00002407
Iteration 47/1000 | Loss: 0.00002406
Iteration 48/1000 | Loss: 0.00002406
Iteration 49/1000 | Loss: 0.00002406
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00002405
Iteration 52/1000 | Loss: 0.00002404
Iteration 53/1000 | Loss: 0.00002404
Iteration 54/1000 | Loss: 0.00002404
Iteration 55/1000 | Loss: 0.00002403
Iteration 56/1000 | Loss: 0.00002403
Iteration 57/1000 | Loss: 0.00002403
Iteration 58/1000 | Loss: 0.00002403
Iteration 59/1000 | Loss: 0.00002402
Iteration 60/1000 | Loss: 0.00002402
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002400
Iteration 63/1000 | Loss: 0.00002400
Iteration 64/1000 | Loss: 0.00002400
Iteration 65/1000 | Loss: 0.00002399
Iteration 66/1000 | Loss: 0.00002399
Iteration 67/1000 | Loss: 0.00002399
Iteration 68/1000 | Loss: 0.00002398
Iteration 69/1000 | Loss: 0.00002398
Iteration 70/1000 | Loss: 0.00002398
Iteration 71/1000 | Loss: 0.00002397
Iteration 72/1000 | Loss: 0.00002397
Iteration 73/1000 | Loss: 0.00002397
Iteration 74/1000 | Loss: 0.00002397
Iteration 75/1000 | Loss: 0.00002396
Iteration 76/1000 | Loss: 0.00002396
Iteration 77/1000 | Loss: 0.00002396
Iteration 78/1000 | Loss: 0.00002396
Iteration 79/1000 | Loss: 0.00002396
Iteration 80/1000 | Loss: 0.00002396
Iteration 81/1000 | Loss: 0.00002396
Iteration 82/1000 | Loss: 0.00002396
Iteration 83/1000 | Loss: 0.00002396
Iteration 84/1000 | Loss: 0.00002396
Iteration 85/1000 | Loss: 0.00002395
Iteration 86/1000 | Loss: 0.00002395
Iteration 87/1000 | Loss: 0.00002395
Iteration 88/1000 | Loss: 0.00002395
Iteration 89/1000 | Loss: 0.00002394
Iteration 90/1000 | Loss: 0.00002394
Iteration 91/1000 | Loss: 0.00002394
Iteration 92/1000 | Loss: 0.00002394
Iteration 93/1000 | Loss: 0.00002393
Iteration 94/1000 | Loss: 0.00002393
Iteration 95/1000 | Loss: 0.00002393
Iteration 96/1000 | Loss: 0.00002393
Iteration 97/1000 | Loss: 0.00002392
Iteration 98/1000 | Loss: 0.00002392
Iteration 99/1000 | Loss: 0.00002392
Iteration 100/1000 | Loss: 0.00002392
Iteration 101/1000 | Loss: 0.00002392
Iteration 102/1000 | Loss: 0.00002392
Iteration 103/1000 | Loss: 0.00002391
Iteration 104/1000 | Loss: 0.00002391
Iteration 105/1000 | Loss: 0.00002391
Iteration 106/1000 | Loss: 0.00002391
Iteration 107/1000 | Loss: 0.00002391
Iteration 108/1000 | Loss: 0.00002391
Iteration 109/1000 | Loss: 0.00002391
Iteration 110/1000 | Loss: 0.00002391
Iteration 111/1000 | Loss: 0.00002391
Iteration 112/1000 | Loss: 0.00002391
Iteration 113/1000 | Loss: 0.00002391
Iteration 114/1000 | Loss: 0.00002390
Iteration 115/1000 | Loss: 0.00002390
Iteration 116/1000 | Loss: 0.00002390
Iteration 117/1000 | Loss: 0.00002390
Iteration 118/1000 | Loss: 0.00002390
Iteration 119/1000 | Loss: 0.00002390
Iteration 120/1000 | Loss: 0.00002390
Iteration 121/1000 | Loss: 0.00002390
Iteration 122/1000 | Loss: 0.00002390
Iteration 123/1000 | Loss: 0.00002390
Iteration 124/1000 | Loss: 0.00002390
Iteration 125/1000 | Loss: 0.00002390
Iteration 126/1000 | Loss: 0.00002390
Iteration 127/1000 | Loss: 0.00002390
Iteration 128/1000 | Loss: 0.00002390
Iteration 129/1000 | Loss: 0.00002390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.3903130568214692e-05, 2.3903130568214692e-05, 2.3903130568214692e-05, 2.3903130568214692e-05, 2.3903130568214692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3903130568214692e-05

Optimization complete. Final v2v error: 3.984523057937622 mm

Highest mean error: 6.049233913421631 mm for frame 124

Lowest mean error: 2.8254010677337646 mm for frame 1

Saving results

Total time: 42.339751958847046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077662
Iteration 2/25 | Loss: 0.00097091
Iteration 3/25 | Loss: 0.00081549
Iteration 4/25 | Loss: 0.00079559
Iteration 5/25 | Loss: 0.00078901
Iteration 6/25 | Loss: 0.00078753
Iteration 7/25 | Loss: 0.00078722
Iteration 8/25 | Loss: 0.00078722
Iteration 9/25 | Loss: 0.00078722
Iteration 10/25 | Loss: 0.00078722
Iteration 11/25 | Loss: 0.00078722
Iteration 12/25 | Loss: 0.00078722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000787219381891191, 0.000787219381891191, 0.000787219381891191, 0.000787219381891191, 0.000787219381891191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000787219381891191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.38046074
Iteration 2/25 | Loss: 0.00127316
Iteration 3/25 | Loss: 0.00127316
Iteration 4/25 | Loss: 0.00127316
Iteration 5/25 | Loss: 0.00127316
Iteration 6/25 | Loss: 0.00127316
Iteration 7/25 | Loss: 0.00127316
Iteration 8/25 | Loss: 0.00127316
Iteration 9/25 | Loss: 0.00127316
Iteration 10/25 | Loss: 0.00127316
Iteration 11/25 | Loss: 0.00127316
Iteration 12/25 | Loss: 0.00127316
Iteration 13/25 | Loss: 0.00127316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001273161033168435, 0.001273161033168435, 0.001273161033168435, 0.001273161033168435, 0.001273161033168435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273161033168435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127316
Iteration 2/1000 | Loss: 0.00002705
Iteration 3/1000 | Loss: 0.00001816
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001385
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001385
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001359
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001351
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001343
Iteration 75/1000 | Loss: 0.00001343
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001342
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001341
Iteration 83/1000 | Loss: 0.00001341
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001336
Iteration 108/1000 | Loss: 0.00001336
Iteration 109/1000 | Loss: 0.00001336
Iteration 110/1000 | Loss: 0.00001336
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001336
Iteration 116/1000 | Loss: 0.00001336
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.3360319826460909e-05, 1.3360319826460909e-05, 1.3360319826460909e-05, 1.3360319826460909e-05, 1.3360319826460909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3360319826460909e-05

Optimization complete. Final v2v error: 3.1126468181610107 mm

Highest mean error: 3.3181955814361572 mm for frame 99

Lowest mean error: 2.9506688117980957 mm for frame 36

Saving results

Total time: 32.70629262924194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504279
Iteration 2/25 | Loss: 0.00106505
Iteration 3/25 | Loss: 0.00083990
Iteration 4/25 | Loss: 0.00080737
Iteration 5/25 | Loss: 0.00079919
Iteration 6/25 | Loss: 0.00079780
Iteration 7/25 | Loss: 0.00079780
Iteration 8/25 | Loss: 0.00079780
Iteration 9/25 | Loss: 0.00079780
Iteration 10/25 | Loss: 0.00079780
Iteration 11/25 | Loss: 0.00079780
Iteration 12/25 | Loss: 0.00079780
Iteration 13/25 | Loss: 0.00079780
Iteration 14/25 | Loss: 0.00079780
Iteration 15/25 | Loss: 0.00079780
Iteration 16/25 | Loss: 0.00079780
Iteration 17/25 | Loss: 0.00079780
Iteration 18/25 | Loss: 0.00079780
Iteration 19/25 | Loss: 0.00079780
Iteration 20/25 | Loss: 0.00079780
Iteration 21/25 | Loss: 0.00079780
Iteration 22/25 | Loss: 0.00079780
Iteration 23/25 | Loss: 0.00079780
Iteration 24/25 | Loss: 0.00079780
Iteration 25/25 | Loss: 0.00079780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86268079
Iteration 2/25 | Loss: 0.00105247
Iteration 3/25 | Loss: 0.00105247
Iteration 4/25 | Loss: 0.00105247
Iteration 5/25 | Loss: 0.00105247
Iteration 6/25 | Loss: 0.00105247
Iteration 7/25 | Loss: 0.00105247
Iteration 8/25 | Loss: 0.00105247
Iteration 9/25 | Loss: 0.00105247
Iteration 10/25 | Loss: 0.00105247
Iteration 11/25 | Loss: 0.00105247
Iteration 12/25 | Loss: 0.00105247
Iteration 13/25 | Loss: 0.00105247
Iteration 14/25 | Loss: 0.00105247
Iteration 15/25 | Loss: 0.00105247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010524655226618052, 0.0010524655226618052, 0.0010524655226618052, 0.0010524655226618052, 0.0010524655226618052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010524655226618052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105247
Iteration 2/1000 | Loss: 0.00003698
Iteration 3/1000 | Loss: 0.00002850
Iteration 4/1000 | Loss: 0.00002727
Iteration 5/1000 | Loss: 0.00002598
Iteration 6/1000 | Loss: 0.00002522
Iteration 7/1000 | Loss: 0.00002440
Iteration 8/1000 | Loss: 0.00002384
Iteration 9/1000 | Loss: 0.00002350
Iteration 10/1000 | Loss: 0.00002327
Iteration 11/1000 | Loss: 0.00002304
Iteration 12/1000 | Loss: 0.00002302
Iteration 13/1000 | Loss: 0.00002301
Iteration 14/1000 | Loss: 0.00002285
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002269
Iteration 17/1000 | Loss: 0.00002268
Iteration 18/1000 | Loss: 0.00002266
Iteration 19/1000 | Loss: 0.00002265
Iteration 20/1000 | Loss: 0.00002264
Iteration 21/1000 | Loss: 0.00002264
Iteration 22/1000 | Loss: 0.00002263
Iteration 23/1000 | Loss: 0.00002261
Iteration 24/1000 | Loss: 0.00002260
Iteration 25/1000 | Loss: 0.00002260
Iteration 26/1000 | Loss: 0.00002258
Iteration 27/1000 | Loss: 0.00002257
Iteration 28/1000 | Loss: 0.00002256
Iteration 29/1000 | Loss: 0.00002255
Iteration 30/1000 | Loss: 0.00002255
Iteration 31/1000 | Loss: 0.00002254
Iteration 32/1000 | Loss: 0.00002254
Iteration 33/1000 | Loss: 0.00002253
Iteration 34/1000 | Loss: 0.00002252
Iteration 35/1000 | Loss: 0.00002252
Iteration 36/1000 | Loss: 0.00002252
Iteration 37/1000 | Loss: 0.00002251
Iteration 38/1000 | Loss: 0.00002251
Iteration 39/1000 | Loss: 0.00002250
Iteration 40/1000 | Loss: 0.00002250
Iteration 41/1000 | Loss: 0.00002249
Iteration 42/1000 | Loss: 0.00002249
Iteration 43/1000 | Loss: 0.00002249
Iteration 44/1000 | Loss: 0.00002248
Iteration 45/1000 | Loss: 0.00002247
Iteration 46/1000 | Loss: 0.00002247
Iteration 47/1000 | Loss: 0.00002246
Iteration 48/1000 | Loss: 0.00002246
Iteration 49/1000 | Loss: 0.00002246
Iteration 50/1000 | Loss: 0.00002244
Iteration 51/1000 | Loss: 0.00002244
Iteration 52/1000 | Loss: 0.00002244
Iteration 53/1000 | Loss: 0.00002244
Iteration 54/1000 | Loss: 0.00002244
Iteration 55/1000 | Loss: 0.00002244
Iteration 56/1000 | Loss: 0.00002243
Iteration 57/1000 | Loss: 0.00002242
Iteration 58/1000 | Loss: 0.00002241
Iteration 59/1000 | Loss: 0.00002240
Iteration 60/1000 | Loss: 0.00002239
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002238
Iteration 64/1000 | Loss: 0.00002238
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002237
Iteration 67/1000 | Loss: 0.00002237
Iteration 68/1000 | Loss: 0.00002237
Iteration 69/1000 | Loss: 0.00002237
Iteration 70/1000 | Loss: 0.00002237
Iteration 71/1000 | Loss: 0.00002237
Iteration 72/1000 | Loss: 0.00002237
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002235
Iteration 77/1000 | Loss: 0.00002234
Iteration 78/1000 | Loss: 0.00002234
Iteration 79/1000 | Loss: 0.00002233
Iteration 80/1000 | Loss: 0.00002233
Iteration 81/1000 | Loss: 0.00002233
Iteration 82/1000 | Loss: 0.00002233
Iteration 83/1000 | Loss: 0.00002233
Iteration 84/1000 | Loss: 0.00002233
Iteration 85/1000 | Loss: 0.00002233
Iteration 86/1000 | Loss: 0.00002232
Iteration 87/1000 | Loss: 0.00002232
Iteration 88/1000 | Loss: 0.00002232
Iteration 89/1000 | Loss: 0.00002232
Iteration 90/1000 | Loss: 0.00002232
Iteration 91/1000 | Loss: 0.00002232
Iteration 92/1000 | Loss: 0.00002231
Iteration 93/1000 | Loss: 0.00002230
Iteration 94/1000 | Loss: 0.00002230
Iteration 95/1000 | Loss: 0.00002230
Iteration 96/1000 | Loss: 0.00002230
Iteration 97/1000 | Loss: 0.00002230
Iteration 98/1000 | Loss: 0.00002230
Iteration 99/1000 | Loss: 0.00002229
Iteration 100/1000 | Loss: 0.00002229
Iteration 101/1000 | Loss: 0.00002229
Iteration 102/1000 | Loss: 0.00002229
Iteration 103/1000 | Loss: 0.00002228
Iteration 104/1000 | Loss: 0.00002228
Iteration 105/1000 | Loss: 0.00002228
Iteration 106/1000 | Loss: 0.00002228
Iteration 107/1000 | Loss: 0.00002228
Iteration 108/1000 | Loss: 0.00002228
Iteration 109/1000 | Loss: 0.00002228
Iteration 110/1000 | Loss: 0.00002228
Iteration 111/1000 | Loss: 0.00002228
Iteration 112/1000 | Loss: 0.00002228
Iteration 113/1000 | Loss: 0.00002227
Iteration 114/1000 | Loss: 0.00002227
Iteration 115/1000 | Loss: 0.00002227
Iteration 116/1000 | Loss: 0.00002227
Iteration 117/1000 | Loss: 0.00002227
Iteration 118/1000 | Loss: 0.00002226
Iteration 119/1000 | Loss: 0.00002226
Iteration 120/1000 | Loss: 0.00002226
Iteration 121/1000 | Loss: 0.00002225
Iteration 122/1000 | Loss: 0.00002225
Iteration 123/1000 | Loss: 0.00002225
Iteration 124/1000 | Loss: 0.00002224
Iteration 125/1000 | Loss: 0.00002224
Iteration 126/1000 | Loss: 0.00002224
Iteration 127/1000 | Loss: 0.00002224
Iteration 128/1000 | Loss: 0.00002224
Iteration 129/1000 | Loss: 0.00002223
Iteration 130/1000 | Loss: 0.00002223
Iteration 131/1000 | Loss: 0.00002223
Iteration 132/1000 | Loss: 0.00002223
Iteration 133/1000 | Loss: 0.00002223
Iteration 134/1000 | Loss: 0.00002223
Iteration 135/1000 | Loss: 0.00002223
Iteration 136/1000 | Loss: 0.00002223
Iteration 137/1000 | Loss: 0.00002223
Iteration 138/1000 | Loss: 0.00002223
Iteration 139/1000 | Loss: 0.00002223
Iteration 140/1000 | Loss: 0.00002222
Iteration 141/1000 | Loss: 0.00002222
Iteration 142/1000 | Loss: 0.00002222
Iteration 143/1000 | Loss: 0.00002222
Iteration 144/1000 | Loss: 0.00002221
Iteration 145/1000 | Loss: 0.00002221
Iteration 146/1000 | Loss: 0.00002221
Iteration 147/1000 | Loss: 0.00002221
Iteration 148/1000 | Loss: 0.00002221
Iteration 149/1000 | Loss: 0.00002221
Iteration 150/1000 | Loss: 0.00002221
Iteration 151/1000 | Loss: 0.00002221
Iteration 152/1000 | Loss: 0.00002221
Iteration 153/1000 | Loss: 0.00002221
Iteration 154/1000 | Loss: 0.00002221
Iteration 155/1000 | Loss: 0.00002220
Iteration 156/1000 | Loss: 0.00002220
Iteration 157/1000 | Loss: 0.00002220
Iteration 158/1000 | Loss: 0.00002220
Iteration 159/1000 | Loss: 0.00002220
Iteration 160/1000 | Loss: 0.00002219
Iteration 161/1000 | Loss: 0.00002219
Iteration 162/1000 | Loss: 0.00002219
Iteration 163/1000 | Loss: 0.00002219
Iteration 164/1000 | Loss: 0.00002219
Iteration 165/1000 | Loss: 0.00002219
Iteration 166/1000 | Loss: 0.00002218
Iteration 167/1000 | Loss: 0.00002218
Iteration 168/1000 | Loss: 0.00002218
Iteration 169/1000 | Loss: 0.00002218
Iteration 170/1000 | Loss: 0.00002218
Iteration 171/1000 | Loss: 0.00002218
Iteration 172/1000 | Loss: 0.00002218
Iteration 173/1000 | Loss: 0.00002217
Iteration 174/1000 | Loss: 0.00002217
Iteration 175/1000 | Loss: 0.00002217
Iteration 176/1000 | Loss: 0.00002217
Iteration 177/1000 | Loss: 0.00002217
Iteration 178/1000 | Loss: 0.00002217
Iteration 179/1000 | Loss: 0.00002216
Iteration 180/1000 | Loss: 0.00002216
Iteration 181/1000 | Loss: 0.00002216
Iteration 182/1000 | Loss: 0.00002216
Iteration 183/1000 | Loss: 0.00002216
Iteration 184/1000 | Loss: 0.00002216
Iteration 185/1000 | Loss: 0.00002216
Iteration 186/1000 | Loss: 0.00002216
Iteration 187/1000 | Loss: 0.00002216
Iteration 188/1000 | Loss: 0.00002215
Iteration 189/1000 | Loss: 0.00002215
Iteration 190/1000 | Loss: 0.00002215
Iteration 191/1000 | Loss: 0.00002215
Iteration 192/1000 | Loss: 0.00002215
Iteration 193/1000 | Loss: 0.00002215
Iteration 194/1000 | Loss: 0.00002215
Iteration 195/1000 | Loss: 0.00002215
Iteration 196/1000 | Loss: 0.00002215
Iteration 197/1000 | Loss: 0.00002215
Iteration 198/1000 | Loss: 0.00002215
Iteration 199/1000 | Loss: 0.00002215
Iteration 200/1000 | Loss: 0.00002215
Iteration 201/1000 | Loss: 0.00002215
Iteration 202/1000 | Loss: 0.00002214
Iteration 203/1000 | Loss: 0.00002214
Iteration 204/1000 | Loss: 0.00002214
Iteration 205/1000 | Loss: 0.00002214
Iteration 206/1000 | Loss: 0.00002214
Iteration 207/1000 | Loss: 0.00002214
Iteration 208/1000 | Loss: 0.00002214
Iteration 209/1000 | Loss: 0.00002214
Iteration 210/1000 | Loss: 0.00002214
Iteration 211/1000 | Loss: 0.00002214
Iteration 212/1000 | Loss: 0.00002214
Iteration 213/1000 | Loss: 0.00002214
Iteration 214/1000 | Loss: 0.00002214
Iteration 215/1000 | Loss: 0.00002214
Iteration 216/1000 | Loss: 0.00002214
Iteration 217/1000 | Loss: 0.00002214
Iteration 218/1000 | Loss: 0.00002214
Iteration 219/1000 | Loss: 0.00002214
Iteration 220/1000 | Loss: 0.00002214
Iteration 221/1000 | Loss: 0.00002214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.213503648818005e-05, 2.213503648818005e-05, 2.213503648818005e-05, 2.213503648818005e-05, 2.213503648818005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.213503648818005e-05

Optimization complete. Final v2v error: 3.980219841003418 mm

Highest mean error: 4.730015754699707 mm for frame 246

Lowest mean error: 3.782898426055908 mm for frame 25

Saving results

Total time: 52.58348512649536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793995
Iteration 2/25 | Loss: 0.00141610
Iteration 3/25 | Loss: 0.00089852
Iteration 4/25 | Loss: 0.00083372
Iteration 5/25 | Loss: 0.00082013
Iteration 6/25 | Loss: 0.00081753
Iteration 7/25 | Loss: 0.00081748
Iteration 8/25 | Loss: 0.00081748
Iteration 9/25 | Loss: 0.00081748
Iteration 10/25 | Loss: 0.00081748
Iteration 11/25 | Loss: 0.00081748
Iteration 12/25 | Loss: 0.00081748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000817475316580385, 0.000817475316580385, 0.000817475316580385, 0.000817475316580385, 0.000817475316580385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000817475316580385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59429479
Iteration 2/25 | Loss: 0.00139894
Iteration 3/25 | Loss: 0.00139892
Iteration 4/25 | Loss: 0.00139892
Iteration 5/25 | Loss: 0.00139892
Iteration 6/25 | Loss: 0.00139892
Iteration 7/25 | Loss: 0.00139892
Iteration 8/25 | Loss: 0.00139892
Iteration 9/25 | Loss: 0.00139892
Iteration 10/25 | Loss: 0.00139892
Iteration 11/25 | Loss: 0.00139892
Iteration 12/25 | Loss: 0.00139892
Iteration 13/25 | Loss: 0.00139892
Iteration 14/25 | Loss: 0.00139892
Iteration 15/25 | Loss: 0.00139892
Iteration 16/25 | Loss: 0.00139892
Iteration 17/25 | Loss: 0.00139892
Iteration 18/25 | Loss: 0.00139892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013989198487251997, 0.0013989198487251997, 0.0013989198487251997, 0.0013989198487251997, 0.0013989198487251997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013989198487251997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139892
Iteration 2/1000 | Loss: 0.00002424
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00001562
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001479
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001423
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001419
Iteration 18/1000 | Loss: 0.00001418
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001414
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001402
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001400
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001396
Iteration 76/1000 | Loss: 0.00001396
Iteration 77/1000 | Loss: 0.00001396
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.3947887964604888e-05, 1.3947887964604888e-05, 1.3947887964604888e-05, 1.3947887964604888e-05, 1.3947887964604888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3947887964604888e-05

Optimization complete. Final v2v error: 3.207460880279541 mm

Highest mean error: 3.473771572113037 mm for frame 0

Lowest mean error: 3.0869696140289307 mm for frame 180

Saving results

Total time: 35.51675367355347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846061
Iteration 2/25 | Loss: 0.00111344
Iteration 3/25 | Loss: 0.00090779
Iteration 4/25 | Loss: 0.00085055
Iteration 5/25 | Loss: 0.00083760
Iteration 6/25 | Loss: 0.00083434
Iteration 7/25 | Loss: 0.00083284
Iteration 8/25 | Loss: 0.00083265
Iteration 9/25 | Loss: 0.00083265
Iteration 10/25 | Loss: 0.00083265
Iteration 11/25 | Loss: 0.00083265
Iteration 12/25 | Loss: 0.00083265
Iteration 13/25 | Loss: 0.00083265
Iteration 14/25 | Loss: 0.00083265
Iteration 15/25 | Loss: 0.00083265
Iteration 16/25 | Loss: 0.00083265
Iteration 17/25 | Loss: 0.00083265
Iteration 18/25 | Loss: 0.00083265
Iteration 19/25 | Loss: 0.00083265
Iteration 20/25 | Loss: 0.00083265
Iteration 21/25 | Loss: 0.00083265
Iteration 22/25 | Loss: 0.00083265
Iteration 23/25 | Loss: 0.00083265
Iteration 24/25 | Loss: 0.00083265
Iteration 25/25 | Loss: 0.00083265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55523121
Iteration 2/25 | Loss: 0.00155647
Iteration 3/25 | Loss: 0.00155647
Iteration 4/25 | Loss: 0.00155647
Iteration 5/25 | Loss: 0.00155647
Iteration 6/25 | Loss: 0.00155647
Iteration 7/25 | Loss: 0.00155646
Iteration 8/25 | Loss: 0.00155646
Iteration 9/25 | Loss: 0.00155646
Iteration 10/25 | Loss: 0.00155646
Iteration 11/25 | Loss: 0.00155646
Iteration 12/25 | Loss: 0.00155646
Iteration 13/25 | Loss: 0.00155646
Iteration 14/25 | Loss: 0.00155646
Iteration 15/25 | Loss: 0.00155646
Iteration 16/25 | Loss: 0.00155646
Iteration 17/25 | Loss: 0.00155646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001556464354507625, 0.001556464354507625, 0.001556464354507625, 0.001556464354507625, 0.001556464354507625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001556464354507625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155646
Iteration 2/1000 | Loss: 0.00004404
Iteration 3/1000 | Loss: 0.00003272
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002618
Iteration 6/1000 | Loss: 0.00002525
Iteration 7/1000 | Loss: 0.00002462
Iteration 8/1000 | Loss: 0.00002397
Iteration 9/1000 | Loss: 0.00002355
Iteration 10/1000 | Loss: 0.00002322
Iteration 11/1000 | Loss: 0.00002294
Iteration 12/1000 | Loss: 0.00002277
Iteration 13/1000 | Loss: 0.00002277
Iteration 14/1000 | Loss: 0.00002274
Iteration 15/1000 | Loss: 0.00002260
Iteration 16/1000 | Loss: 0.00002258
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002251
Iteration 19/1000 | Loss: 0.00002242
Iteration 20/1000 | Loss: 0.00002240
Iteration 21/1000 | Loss: 0.00002239
Iteration 22/1000 | Loss: 0.00002239
Iteration 23/1000 | Loss: 0.00002238
Iteration 24/1000 | Loss: 0.00002237
Iteration 25/1000 | Loss: 0.00002237
Iteration 26/1000 | Loss: 0.00002236
Iteration 27/1000 | Loss: 0.00002236
Iteration 28/1000 | Loss: 0.00002236
Iteration 29/1000 | Loss: 0.00002236
Iteration 30/1000 | Loss: 0.00002235
Iteration 31/1000 | Loss: 0.00002235
Iteration 32/1000 | Loss: 0.00002234
Iteration 33/1000 | Loss: 0.00002234
Iteration 34/1000 | Loss: 0.00002234
Iteration 35/1000 | Loss: 0.00002234
Iteration 36/1000 | Loss: 0.00002234
Iteration 37/1000 | Loss: 0.00002234
Iteration 38/1000 | Loss: 0.00002234
Iteration 39/1000 | Loss: 0.00002234
Iteration 40/1000 | Loss: 0.00002234
Iteration 41/1000 | Loss: 0.00002233
Iteration 42/1000 | Loss: 0.00002232
Iteration 43/1000 | Loss: 0.00002232
Iteration 44/1000 | Loss: 0.00002231
Iteration 45/1000 | Loss: 0.00002231
Iteration 46/1000 | Loss: 0.00002231
Iteration 47/1000 | Loss: 0.00002231
Iteration 48/1000 | Loss: 0.00002231
Iteration 49/1000 | Loss: 0.00002231
Iteration 50/1000 | Loss: 0.00002231
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002228
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002227
Iteration 66/1000 | Loss: 0.00002227
Iteration 67/1000 | Loss: 0.00002227
Iteration 68/1000 | Loss: 0.00002227
Iteration 69/1000 | Loss: 0.00002227
Iteration 70/1000 | Loss: 0.00002227
Iteration 71/1000 | Loss: 0.00002226
Iteration 72/1000 | Loss: 0.00002226
Iteration 73/1000 | Loss: 0.00002226
Iteration 74/1000 | Loss: 0.00002226
Iteration 75/1000 | Loss: 0.00002225
Iteration 76/1000 | Loss: 0.00002225
Iteration 77/1000 | Loss: 0.00002225
Iteration 78/1000 | Loss: 0.00002225
Iteration 79/1000 | Loss: 0.00002225
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00002225
Iteration 82/1000 | Loss: 0.00002225
Iteration 83/1000 | Loss: 0.00002225
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002224
Iteration 86/1000 | Loss: 0.00002224
Iteration 87/1000 | Loss: 0.00002224
Iteration 88/1000 | Loss: 0.00002223
Iteration 89/1000 | Loss: 0.00002223
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002223
Iteration 92/1000 | Loss: 0.00002223
Iteration 93/1000 | Loss: 0.00002223
Iteration 94/1000 | Loss: 0.00002222
Iteration 95/1000 | Loss: 0.00002222
Iteration 96/1000 | Loss: 0.00002222
Iteration 97/1000 | Loss: 0.00002222
Iteration 98/1000 | Loss: 0.00002222
Iteration 99/1000 | Loss: 0.00002222
Iteration 100/1000 | Loss: 0.00002222
Iteration 101/1000 | Loss: 0.00002222
Iteration 102/1000 | Loss: 0.00002222
Iteration 103/1000 | Loss: 0.00002222
Iteration 104/1000 | Loss: 0.00002221
Iteration 105/1000 | Loss: 0.00002221
Iteration 106/1000 | Loss: 0.00002221
Iteration 107/1000 | Loss: 0.00002221
Iteration 108/1000 | Loss: 0.00002221
Iteration 109/1000 | Loss: 0.00002221
Iteration 110/1000 | Loss: 0.00002221
Iteration 111/1000 | Loss: 0.00002221
Iteration 112/1000 | Loss: 0.00002221
Iteration 113/1000 | Loss: 0.00002220
Iteration 114/1000 | Loss: 0.00002220
Iteration 115/1000 | Loss: 0.00002220
Iteration 116/1000 | Loss: 0.00002220
Iteration 117/1000 | Loss: 0.00002220
Iteration 118/1000 | Loss: 0.00002220
Iteration 119/1000 | Loss: 0.00002220
Iteration 120/1000 | Loss: 0.00002220
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002220
Iteration 125/1000 | Loss: 0.00002220
Iteration 126/1000 | Loss: 0.00002220
Iteration 127/1000 | Loss: 0.00002220
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002220
Iteration 131/1000 | Loss: 0.00002220
Iteration 132/1000 | Loss: 0.00002220
Iteration 133/1000 | Loss: 0.00002220
Iteration 134/1000 | Loss: 0.00002220
Iteration 135/1000 | Loss: 0.00002220
Iteration 136/1000 | Loss: 0.00002220
Iteration 137/1000 | Loss: 0.00002220
Iteration 138/1000 | Loss: 0.00002220
Iteration 139/1000 | Loss: 0.00002220
Iteration 140/1000 | Loss: 0.00002220
Iteration 141/1000 | Loss: 0.00002220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2202360923984088e-05, 2.2202360923984088e-05, 2.2202360923984088e-05, 2.2202360923984088e-05, 2.2202360923984088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2202360923984088e-05

Optimization complete. Final v2v error: 3.92134952545166 mm

Highest mean error: 4.731960773468018 mm for frame 66

Lowest mean error: 3.7220458984375 mm for frame 79

Saving results

Total time: 39.29065728187561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509145
Iteration 2/25 | Loss: 0.00133100
Iteration 3/25 | Loss: 0.00092887
Iteration 4/25 | Loss: 0.00082575
Iteration 5/25 | Loss: 0.00080896
Iteration 6/25 | Loss: 0.00080689
Iteration 7/25 | Loss: 0.00080630
Iteration 8/25 | Loss: 0.00080623
Iteration 9/25 | Loss: 0.00080623
Iteration 10/25 | Loss: 0.00080623
Iteration 11/25 | Loss: 0.00080623
Iteration 12/25 | Loss: 0.00080623
Iteration 13/25 | Loss: 0.00080623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008062265114858747, 0.0008062265114858747, 0.0008062265114858747, 0.0008062265114858747, 0.0008062265114858747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008062265114858747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64187264
Iteration 2/25 | Loss: 0.00162394
Iteration 3/25 | Loss: 0.00162394
Iteration 4/25 | Loss: 0.00162394
Iteration 5/25 | Loss: 0.00162394
Iteration 6/25 | Loss: 0.00162394
Iteration 7/25 | Loss: 0.00162394
Iteration 8/25 | Loss: 0.00162394
Iteration 9/25 | Loss: 0.00162394
Iteration 10/25 | Loss: 0.00162394
Iteration 11/25 | Loss: 0.00162394
Iteration 12/25 | Loss: 0.00162394
Iteration 13/25 | Loss: 0.00162394
Iteration 14/25 | Loss: 0.00162394
Iteration 15/25 | Loss: 0.00162394
Iteration 16/25 | Loss: 0.00162394
Iteration 17/25 | Loss: 0.00162394
Iteration 18/25 | Loss: 0.00162394
Iteration 19/25 | Loss: 0.00162394
Iteration 20/25 | Loss: 0.00162394
Iteration 21/25 | Loss: 0.00162394
Iteration 22/25 | Loss: 0.00162394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016239357646554708, 0.0016239357646554708, 0.0016239357646554708, 0.0016239357646554708, 0.0016239357646554708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016239357646554708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162394
Iteration 2/1000 | Loss: 0.00003035
Iteration 3/1000 | Loss: 0.00002281
Iteration 4/1000 | Loss: 0.00001790
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001556
Iteration 8/1000 | Loss: 0.00001524
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001519
Iteration 11/1000 | Loss: 0.00001518
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001507
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00001503
Iteration 22/1000 | Loss: 0.00001502
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001498
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001497
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001496
Iteration 38/1000 | Loss: 0.00001496
Iteration 39/1000 | Loss: 0.00001496
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001495
Iteration 42/1000 | Loss: 0.00001495
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001490
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001478
Iteration 81/1000 | Loss: 0.00001478
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001473
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001473
Iteration 109/1000 | Loss: 0.00001473
Iteration 110/1000 | Loss: 0.00001473
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001470
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001467
Iteration 154/1000 | Loss: 0.00001467
Iteration 155/1000 | Loss: 0.00001467
Iteration 156/1000 | Loss: 0.00001467
Iteration 157/1000 | Loss: 0.00001467
Iteration 158/1000 | Loss: 0.00001467
Iteration 159/1000 | Loss: 0.00001466
Iteration 160/1000 | Loss: 0.00001466
Iteration 161/1000 | Loss: 0.00001466
Iteration 162/1000 | Loss: 0.00001466
Iteration 163/1000 | Loss: 0.00001466
Iteration 164/1000 | Loss: 0.00001466
Iteration 165/1000 | Loss: 0.00001466
Iteration 166/1000 | Loss: 0.00001466
Iteration 167/1000 | Loss: 0.00001466
Iteration 168/1000 | Loss: 0.00001466
Iteration 169/1000 | Loss: 0.00001466
Iteration 170/1000 | Loss: 0.00001466
Iteration 171/1000 | Loss: 0.00001466
Iteration 172/1000 | Loss: 0.00001466
Iteration 173/1000 | Loss: 0.00001466
Iteration 174/1000 | Loss: 0.00001466
Iteration 175/1000 | Loss: 0.00001466
Iteration 176/1000 | Loss: 0.00001466
Iteration 177/1000 | Loss: 0.00001466
Iteration 178/1000 | Loss: 0.00001466
Iteration 179/1000 | Loss: 0.00001466
Iteration 180/1000 | Loss: 0.00001466
Iteration 181/1000 | Loss: 0.00001466
Iteration 182/1000 | Loss: 0.00001466
Iteration 183/1000 | Loss: 0.00001466
Iteration 184/1000 | Loss: 0.00001466
Iteration 185/1000 | Loss: 0.00001466
Iteration 186/1000 | Loss: 0.00001466
Iteration 187/1000 | Loss: 0.00001466
Iteration 188/1000 | Loss: 0.00001466
Iteration 189/1000 | Loss: 0.00001466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.4660516171716154e-05, 1.4660516171716154e-05, 1.4660516171716154e-05, 1.4660516171716154e-05, 1.4660516171716154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4660516171716154e-05

Optimization complete. Final v2v error: 3.24639892578125 mm

Highest mean error: 3.383934497833252 mm for frame 0

Lowest mean error: 3.1389060020446777 mm for frame 7

Saving results

Total time: 36.92337608337402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492730
Iteration 2/25 | Loss: 0.00092605
Iteration 3/25 | Loss: 0.00078426
Iteration 4/25 | Loss: 0.00076099
Iteration 5/25 | Loss: 0.00075481
Iteration 6/25 | Loss: 0.00075324
Iteration 7/25 | Loss: 0.00075298
Iteration 8/25 | Loss: 0.00075298
Iteration 9/25 | Loss: 0.00075298
Iteration 10/25 | Loss: 0.00075298
Iteration 11/25 | Loss: 0.00075298
Iteration 12/25 | Loss: 0.00075298
Iteration 13/25 | Loss: 0.00075298
Iteration 14/25 | Loss: 0.00075298
Iteration 15/25 | Loss: 0.00075298
Iteration 16/25 | Loss: 0.00075298
Iteration 17/25 | Loss: 0.00075298
Iteration 18/25 | Loss: 0.00075298
Iteration 19/25 | Loss: 0.00075298
Iteration 20/25 | Loss: 0.00075298
Iteration 21/25 | Loss: 0.00075298
Iteration 22/25 | Loss: 0.00075298
Iteration 23/25 | Loss: 0.00075298
Iteration 24/25 | Loss: 0.00075298
Iteration 25/25 | Loss: 0.00075298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007529837312176824, 0.0007529837312176824, 0.0007529837312176824, 0.0007529837312176824, 0.0007529837312176824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007529837312176824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47100008
Iteration 2/25 | Loss: 0.00111359
Iteration 3/25 | Loss: 0.00111355
Iteration 4/25 | Loss: 0.00111355
Iteration 5/25 | Loss: 0.00111355
Iteration 6/25 | Loss: 0.00111355
Iteration 7/25 | Loss: 0.00111355
Iteration 8/25 | Loss: 0.00111355
Iteration 9/25 | Loss: 0.00111355
Iteration 10/25 | Loss: 0.00111355
Iteration 11/25 | Loss: 0.00111355
Iteration 12/25 | Loss: 0.00111355
Iteration 13/25 | Loss: 0.00111355
Iteration 14/25 | Loss: 0.00111355
Iteration 15/25 | Loss: 0.00111355
Iteration 16/25 | Loss: 0.00111355
Iteration 17/25 | Loss: 0.00111355
Iteration 18/25 | Loss: 0.00111355
Iteration 19/25 | Loss: 0.00111355
Iteration 20/25 | Loss: 0.00111355
Iteration 21/25 | Loss: 0.00111355
Iteration 22/25 | Loss: 0.00111355
Iteration 23/25 | Loss: 0.00111355
Iteration 24/25 | Loss: 0.00111355
Iteration 25/25 | Loss: 0.00111355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111355
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001611
Iteration 5/1000 | Loss: 0.00001508
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001406
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001357
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001347
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001346
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001331
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001326
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001323
Iteration 44/1000 | Loss: 0.00001323
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001315
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001304
Iteration 117/1000 | Loss: 0.00001304
Iteration 118/1000 | Loss: 0.00001304
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001302
Iteration 134/1000 | Loss: 0.00001302
Iteration 135/1000 | Loss: 0.00001302
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001301
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001301
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001301
Iteration 144/1000 | Loss: 0.00001301
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001300
Iteration 164/1000 | Loss: 0.00001300
Iteration 165/1000 | Loss: 0.00001300
Iteration 166/1000 | Loss: 0.00001300
Iteration 167/1000 | Loss: 0.00001300
Iteration 168/1000 | Loss: 0.00001300
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001300
Iteration 179/1000 | Loss: 0.00001300
Iteration 180/1000 | Loss: 0.00001300
Iteration 181/1000 | Loss: 0.00001300
Iteration 182/1000 | Loss: 0.00001300
Iteration 183/1000 | Loss: 0.00001300
Iteration 184/1000 | Loss: 0.00001300
Iteration 185/1000 | Loss: 0.00001300
Iteration 186/1000 | Loss: 0.00001300
Iteration 187/1000 | Loss: 0.00001300
Iteration 188/1000 | Loss: 0.00001300
Iteration 189/1000 | Loss: 0.00001300
Iteration 190/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.300195526710013e-05, 1.300195526710013e-05, 1.300195526710013e-05, 1.300195526710013e-05, 1.300195526710013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.300195526710013e-05

Optimization complete. Final v2v error: 3.04970383644104 mm

Highest mean error: 3.4040987491607666 mm for frame 30

Lowest mean error: 2.688952684402466 mm for frame 48

Saving results

Total time: 37.17270088195801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00692296
Iteration 2/25 | Loss: 0.00167533
Iteration 3/25 | Loss: 0.00107323
Iteration 4/25 | Loss: 0.00099489
Iteration 5/25 | Loss: 0.00097293
Iteration 6/25 | Loss: 0.00094876
Iteration 7/25 | Loss: 0.00093865
Iteration 8/25 | Loss: 0.00093416
Iteration 9/25 | Loss: 0.00093156
Iteration 10/25 | Loss: 0.00093082
Iteration 11/25 | Loss: 0.00092984
Iteration 12/25 | Loss: 0.00092773
Iteration 13/25 | Loss: 0.00092668
Iteration 14/25 | Loss: 0.00092634
Iteration 15/25 | Loss: 0.00092610
Iteration 16/25 | Loss: 0.00092596
Iteration 17/25 | Loss: 0.00092595
Iteration 18/25 | Loss: 0.00092594
Iteration 19/25 | Loss: 0.00092594
Iteration 20/25 | Loss: 0.00092594
Iteration 21/25 | Loss: 0.00092594
Iteration 22/25 | Loss: 0.00092594
Iteration 23/25 | Loss: 0.00092594
Iteration 24/25 | Loss: 0.00092594
Iteration 25/25 | Loss: 0.00092594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63845217
Iteration 2/25 | Loss: 0.00124259
Iteration 3/25 | Loss: 0.00124259
Iteration 4/25 | Loss: 0.00124259
Iteration 5/25 | Loss: 0.00124259
Iteration 6/25 | Loss: 0.00124258
Iteration 7/25 | Loss: 0.00124258
Iteration 8/25 | Loss: 0.00124258
Iteration 9/25 | Loss: 0.00124258
Iteration 10/25 | Loss: 0.00124258
Iteration 11/25 | Loss: 0.00124258
Iteration 12/25 | Loss: 0.00124258
Iteration 13/25 | Loss: 0.00124258
Iteration 14/25 | Loss: 0.00124258
Iteration 15/25 | Loss: 0.00124258
Iteration 16/25 | Loss: 0.00124258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012425839668139815, 0.0012425839668139815, 0.0012425839668139815, 0.0012425839668139815, 0.0012425839668139815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012425839668139815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124258
Iteration 2/1000 | Loss: 0.00005019
Iteration 3/1000 | Loss: 0.00003463
Iteration 4/1000 | Loss: 0.00003150
Iteration 5/1000 | Loss: 0.00003006
Iteration 6/1000 | Loss: 0.00002902
Iteration 7/1000 | Loss: 0.00002834
Iteration 8/1000 | Loss: 0.00002777
Iteration 9/1000 | Loss: 0.00002735
Iteration 10/1000 | Loss: 0.00002702
Iteration 11/1000 | Loss: 0.00002679
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002666
Iteration 14/1000 | Loss: 0.00002650
Iteration 15/1000 | Loss: 0.00002642
Iteration 16/1000 | Loss: 0.00002641
Iteration 17/1000 | Loss: 0.00002637
Iteration 18/1000 | Loss: 0.00002637
Iteration 19/1000 | Loss: 0.00002637
Iteration 20/1000 | Loss: 0.00002636
Iteration 21/1000 | Loss: 0.00002636
Iteration 22/1000 | Loss: 0.00002633
Iteration 23/1000 | Loss: 0.00002633
Iteration 24/1000 | Loss: 0.00002633
Iteration 25/1000 | Loss: 0.00002633
Iteration 26/1000 | Loss: 0.00002633
Iteration 27/1000 | Loss: 0.00002633
Iteration 28/1000 | Loss: 0.00002633
Iteration 29/1000 | Loss: 0.00002633
Iteration 30/1000 | Loss: 0.00002632
Iteration 31/1000 | Loss: 0.00002632
Iteration 32/1000 | Loss: 0.00002631
Iteration 33/1000 | Loss: 0.00002631
Iteration 34/1000 | Loss: 0.00002631
Iteration 35/1000 | Loss: 0.00002630
Iteration 36/1000 | Loss: 0.00002630
Iteration 37/1000 | Loss: 0.00002630
Iteration 38/1000 | Loss: 0.00002630
Iteration 39/1000 | Loss: 0.00002629
Iteration 40/1000 | Loss: 0.00002629
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002629
Iteration 43/1000 | Loss: 0.00002629
Iteration 44/1000 | Loss: 0.00002629
Iteration 45/1000 | Loss: 0.00002629
Iteration 46/1000 | Loss: 0.00002629
Iteration 47/1000 | Loss: 0.00002629
Iteration 48/1000 | Loss: 0.00002629
Iteration 49/1000 | Loss: 0.00002629
Iteration 50/1000 | Loss: 0.00002629
Iteration 51/1000 | Loss: 0.00002628
Iteration 52/1000 | Loss: 0.00002628
Iteration 53/1000 | Loss: 0.00002628
Iteration 54/1000 | Loss: 0.00002628
Iteration 55/1000 | Loss: 0.00002628
Iteration 56/1000 | Loss: 0.00002627
Iteration 57/1000 | Loss: 0.00002627
Iteration 58/1000 | Loss: 0.00002627
Iteration 59/1000 | Loss: 0.00002627
Iteration 60/1000 | Loss: 0.00002627
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002626
Iteration 65/1000 | Loss: 0.00002626
Iteration 66/1000 | Loss: 0.00002626
Iteration 67/1000 | Loss: 0.00002626
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002625
Iteration 70/1000 | Loss: 0.00002625
Iteration 71/1000 | Loss: 0.00002624
Iteration 72/1000 | Loss: 0.00002624
Iteration 73/1000 | Loss: 0.00002624
Iteration 74/1000 | Loss: 0.00002624
Iteration 75/1000 | Loss: 0.00002623
Iteration 76/1000 | Loss: 0.00002623
Iteration 77/1000 | Loss: 0.00002623
Iteration 78/1000 | Loss: 0.00002622
Iteration 79/1000 | Loss: 0.00002622
Iteration 80/1000 | Loss: 0.00002622
Iteration 81/1000 | Loss: 0.00002622
Iteration 82/1000 | Loss: 0.00002621
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002621
Iteration 86/1000 | Loss: 0.00002621
Iteration 87/1000 | Loss: 0.00002621
Iteration 88/1000 | Loss: 0.00002620
Iteration 89/1000 | Loss: 0.00002620
Iteration 90/1000 | Loss: 0.00002620
Iteration 91/1000 | Loss: 0.00002620
Iteration 92/1000 | Loss: 0.00002620
Iteration 93/1000 | Loss: 0.00002619
Iteration 94/1000 | Loss: 0.00002619
Iteration 95/1000 | Loss: 0.00002619
Iteration 96/1000 | Loss: 0.00002619
Iteration 97/1000 | Loss: 0.00002618
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002618
Iteration 100/1000 | Loss: 0.00002618
Iteration 101/1000 | Loss: 0.00002618
Iteration 102/1000 | Loss: 0.00002617
Iteration 103/1000 | Loss: 0.00002617
Iteration 104/1000 | Loss: 0.00002617
Iteration 105/1000 | Loss: 0.00002617
Iteration 106/1000 | Loss: 0.00002616
Iteration 107/1000 | Loss: 0.00002616
Iteration 108/1000 | Loss: 0.00002616
Iteration 109/1000 | Loss: 0.00002615
Iteration 110/1000 | Loss: 0.00002615
Iteration 111/1000 | Loss: 0.00002615
Iteration 112/1000 | Loss: 0.00002615
Iteration 113/1000 | Loss: 0.00002615
Iteration 114/1000 | Loss: 0.00002614
Iteration 115/1000 | Loss: 0.00002613
Iteration 116/1000 | Loss: 0.00002613
Iteration 117/1000 | Loss: 0.00002613
Iteration 118/1000 | Loss: 0.00002613
Iteration 119/1000 | Loss: 0.00002613
Iteration 120/1000 | Loss: 0.00002613
Iteration 121/1000 | Loss: 0.00002613
Iteration 122/1000 | Loss: 0.00002613
Iteration 123/1000 | Loss: 0.00002613
Iteration 124/1000 | Loss: 0.00002613
Iteration 125/1000 | Loss: 0.00002613
Iteration 126/1000 | Loss: 0.00002613
Iteration 127/1000 | Loss: 0.00002613
Iteration 128/1000 | Loss: 0.00002613
Iteration 129/1000 | Loss: 0.00002613
Iteration 130/1000 | Loss: 0.00002613
Iteration 131/1000 | Loss: 0.00002613
Iteration 132/1000 | Loss: 0.00002613
Iteration 133/1000 | Loss: 0.00002613
Iteration 134/1000 | Loss: 0.00002613
Iteration 135/1000 | Loss: 0.00002613
Iteration 136/1000 | Loss: 0.00002613
Iteration 137/1000 | Loss: 0.00002613
Iteration 138/1000 | Loss: 0.00002613
Iteration 139/1000 | Loss: 0.00002613
Iteration 140/1000 | Loss: 0.00002613
Iteration 141/1000 | Loss: 0.00002613
Iteration 142/1000 | Loss: 0.00002613
Iteration 143/1000 | Loss: 0.00002613
Iteration 144/1000 | Loss: 0.00002613
Iteration 145/1000 | Loss: 0.00002613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.6132427592528984e-05, 2.6132427592528984e-05, 2.6132427592528984e-05, 2.6132427592528984e-05, 2.6132427592528984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6132427592528984e-05

Optimization complete. Final v2v error: 4.2526021003723145 mm

Highest mean error: 5.450808525085449 mm for frame 44

Lowest mean error: 3.3466455936431885 mm for frame 136

Saving results

Total time: 63.80656838417053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816208
Iteration 2/25 | Loss: 0.00155955
Iteration 3/25 | Loss: 0.00110963
Iteration 4/25 | Loss: 0.00100023
Iteration 5/25 | Loss: 0.00098595
Iteration 6/25 | Loss: 0.00096770
Iteration 7/25 | Loss: 0.00096313
Iteration 8/25 | Loss: 0.00096102
Iteration 9/25 | Loss: 0.00097016
Iteration 10/25 | Loss: 0.00095571
Iteration 11/25 | Loss: 0.00096149
Iteration 12/25 | Loss: 0.00095415
Iteration 13/25 | Loss: 0.00097455
Iteration 14/25 | Loss: 0.00096209
Iteration 15/25 | Loss: 0.00095602
Iteration 16/25 | Loss: 0.00095541
Iteration 17/25 | Loss: 0.00095447
Iteration 18/25 | Loss: 0.00095318
Iteration 19/25 | Loss: 0.00095403
Iteration 20/25 | Loss: 0.00094895
Iteration 21/25 | Loss: 0.00094733
Iteration 22/25 | Loss: 0.00094821
Iteration 23/25 | Loss: 0.00095427
Iteration 24/25 | Loss: 0.00095750
Iteration 25/25 | Loss: 0.00094619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.05483723
Iteration 2/25 | Loss: 0.00301603
Iteration 3/25 | Loss: 0.00301392
Iteration 4/25 | Loss: 0.00301392
Iteration 5/25 | Loss: 0.00301392
Iteration 6/25 | Loss: 0.00301392
Iteration 7/25 | Loss: 0.00301392
Iteration 8/25 | Loss: 0.00301392
Iteration 9/25 | Loss: 0.00301392
Iteration 10/25 | Loss: 0.00301392
Iteration 11/25 | Loss: 0.00301392
Iteration 12/25 | Loss: 0.00301392
Iteration 13/25 | Loss: 0.00301392
Iteration 14/25 | Loss: 0.00301392
Iteration 15/25 | Loss: 0.00301392
Iteration 16/25 | Loss: 0.00301392
Iteration 17/25 | Loss: 0.00301392
Iteration 18/25 | Loss: 0.00301392
Iteration 19/25 | Loss: 0.00301392
Iteration 20/25 | Loss: 0.00301392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003013920271769166, 0.003013920271769166, 0.003013920271769166, 0.003013920271769166, 0.003013920271769166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003013920271769166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301392
Iteration 2/1000 | Loss: 0.00066452
Iteration 3/1000 | Loss: 0.00769756
Iteration 4/1000 | Loss: 0.00081840
Iteration 5/1000 | Loss: 0.00661205
Iteration 6/1000 | Loss: 0.01410973
Iteration 7/1000 | Loss: 0.00776045
Iteration 8/1000 | Loss: 0.00214362
Iteration 9/1000 | Loss: 0.00153428
Iteration 10/1000 | Loss: 0.00116713
Iteration 11/1000 | Loss: 0.00130602
Iteration 12/1000 | Loss: 0.00155218
Iteration 13/1000 | Loss: 0.00355528
Iteration 14/1000 | Loss: 0.00053937
Iteration 15/1000 | Loss: 0.00218416
Iteration 16/1000 | Loss: 0.00203325
Iteration 17/1000 | Loss: 0.00204639
Iteration 18/1000 | Loss: 0.00588526
Iteration 19/1000 | Loss: 0.00107725
Iteration 20/1000 | Loss: 0.00040765
Iteration 21/1000 | Loss: 0.00072641
Iteration 22/1000 | Loss: 0.00060805
Iteration 23/1000 | Loss: 0.00052210
Iteration 24/1000 | Loss: 0.00045649
Iteration 25/1000 | Loss: 0.00026393
Iteration 26/1000 | Loss: 0.00116477
Iteration 27/1000 | Loss: 0.00115187
Iteration 28/1000 | Loss: 0.00102082
Iteration 29/1000 | Loss: 0.00166654
Iteration 30/1000 | Loss: 0.00074396
Iteration 31/1000 | Loss: 0.00047649
Iteration 32/1000 | Loss: 0.00022627
Iteration 33/1000 | Loss: 0.00007804
Iteration 34/1000 | Loss: 0.00006770
Iteration 35/1000 | Loss: 0.00025868
Iteration 36/1000 | Loss: 0.00005762
Iteration 37/1000 | Loss: 0.00004702
Iteration 38/1000 | Loss: 0.00004153
Iteration 39/1000 | Loss: 0.00003705
Iteration 40/1000 | Loss: 0.00003462
Iteration 41/1000 | Loss: 0.00003223
Iteration 42/1000 | Loss: 0.00003072
Iteration 43/1000 | Loss: 0.00002934
Iteration 44/1000 | Loss: 0.00002841
Iteration 45/1000 | Loss: 0.00002792
Iteration 46/1000 | Loss: 0.00002730
Iteration 47/1000 | Loss: 0.00002682
Iteration 48/1000 | Loss: 0.00002641
Iteration 49/1000 | Loss: 0.00002607
Iteration 50/1000 | Loss: 0.00002577
Iteration 51/1000 | Loss: 0.00002554
Iteration 52/1000 | Loss: 0.00002524
Iteration 53/1000 | Loss: 0.00002504
Iteration 54/1000 | Loss: 0.00002489
Iteration 55/1000 | Loss: 0.00002489
Iteration 56/1000 | Loss: 0.00002488
Iteration 57/1000 | Loss: 0.00002488
Iteration 58/1000 | Loss: 0.00002487
Iteration 59/1000 | Loss: 0.00002487
Iteration 60/1000 | Loss: 0.00002487
Iteration 61/1000 | Loss: 0.00002486
Iteration 62/1000 | Loss: 0.00002485
Iteration 63/1000 | Loss: 0.00002485
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002485
Iteration 67/1000 | Loss: 0.00002484
Iteration 68/1000 | Loss: 0.00002484
Iteration 69/1000 | Loss: 0.00002484
Iteration 70/1000 | Loss: 0.00002483
Iteration 71/1000 | Loss: 0.00002483
Iteration 72/1000 | Loss: 0.00002483
Iteration 73/1000 | Loss: 0.00002482
Iteration 74/1000 | Loss: 0.00002482
Iteration 75/1000 | Loss: 0.00002482
Iteration 76/1000 | Loss: 0.00002482
Iteration 77/1000 | Loss: 0.00002482
Iteration 78/1000 | Loss: 0.00002482
Iteration 79/1000 | Loss: 0.00002482
Iteration 80/1000 | Loss: 0.00002481
Iteration 81/1000 | Loss: 0.00002481
Iteration 82/1000 | Loss: 0.00002481
Iteration 83/1000 | Loss: 0.00002480
Iteration 84/1000 | Loss: 0.00002480
Iteration 85/1000 | Loss: 0.00002478
Iteration 86/1000 | Loss: 0.00002478
Iteration 87/1000 | Loss: 0.00002478
Iteration 88/1000 | Loss: 0.00002477
Iteration 89/1000 | Loss: 0.00002477
Iteration 90/1000 | Loss: 0.00002476
Iteration 91/1000 | Loss: 0.00002475
Iteration 92/1000 | Loss: 0.00002475
Iteration 93/1000 | Loss: 0.00002474
Iteration 94/1000 | Loss: 0.00002474
Iteration 95/1000 | Loss: 0.00002474
Iteration 96/1000 | Loss: 0.00002473
Iteration 97/1000 | Loss: 0.00002473
Iteration 98/1000 | Loss: 0.00002473
Iteration 99/1000 | Loss: 0.00002473
Iteration 100/1000 | Loss: 0.00002472
Iteration 101/1000 | Loss: 0.00002472
Iteration 102/1000 | Loss: 0.00002472
Iteration 103/1000 | Loss: 0.00002472
Iteration 104/1000 | Loss: 0.00002471
Iteration 105/1000 | Loss: 0.00002471
Iteration 106/1000 | Loss: 0.00002471
Iteration 107/1000 | Loss: 0.00002471
Iteration 108/1000 | Loss: 0.00002471
Iteration 109/1000 | Loss: 0.00002470
Iteration 110/1000 | Loss: 0.00002470
Iteration 111/1000 | Loss: 0.00002470
Iteration 112/1000 | Loss: 0.00002470
Iteration 113/1000 | Loss: 0.00002470
Iteration 114/1000 | Loss: 0.00002470
Iteration 115/1000 | Loss: 0.00002470
Iteration 116/1000 | Loss: 0.00002470
Iteration 117/1000 | Loss: 0.00002470
Iteration 118/1000 | Loss: 0.00002470
Iteration 119/1000 | Loss: 0.00002470
Iteration 120/1000 | Loss: 0.00002470
Iteration 121/1000 | Loss: 0.00002470
Iteration 122/1000 | Loss: 0.00002470
Iteration 123/1000 | Loss: 0.00002469
Iteration 124/1000 | Loss: 0.00002469
Iteration 125/1000 | Loss: 0.00002469
Iteration 126/1000 | Loss: 0.00002469
Iteration 127/1000 | Loss: 0.00002469
Iteration 128/1000 | Loss: 0.00002469
Iteration 129/1000 | Loss: 0.00002469
Iteration 130/1000 | Loss: 0.00002469
Iteration 131/1000 | Loss: 0.00002468
Iteration 132/1000 | Loss: 0.00002468
Iteration 133/1000 | Loss: 0.00002468
Iteration 134/1000 | Loss: 0.00002468
Iteration 135/1000 | Loss: 0.00002468
Iteration 136/1000 | Loss: 0.00002468
Iteration 137/1000 | Loss: 0.00002467
Iteration 138/1000 | Loss: 0.00002467
Iteration 139/1000 | Loss: 0.00002467
Iteration 140/1000 | Loss: 0.00002467
Iteration 141/1000 | Loss: 0.00002467
Iteration 142/1000 | Loss: 0.00002467
Iteration 143/1000 | Loss: 0.00002467
Iteration 144/1000 | Loss: 0.00002466
Iteration 145/1000 | Loss: 0.00002466
Iteration 146/1000 | Loss: 0.00002466
Iteration 147/1000 | Loss: 0.00002466
Iteration 148/1000 | Loss: 0.00002466
Iteration 149/1000 | Loss: 0.00002466
Iteration 150/1000 | Loss: 0.00002466
Iteration 151/1000 | Loss: 0.00002466
Iteration 152/1000 | Loss: 0.00002466
Iteration 153/1000 | Loss: 0.00002465
Iteration 154/1000 | Loss: 0.00002465
Iteration 155/1000 | Loss: 0.00002465
Iteration 156/1000 | Loss: 0.00002465
Iteration 157/1000 | Loss: 0.00002465
Iteration 158/1000 | Loss: 0.00002465
Iteration 159/1000 | Loss: 0.00002465
Iteration 160/1000 | Loss: 0.00002465
Iteration 161/1000 | Loss: 0.00002465
Iteration 162/1000 | Loss: 0.00002465
Iteration 163/1000 | Loss: 0.00002465
Iteration 164/1000 | Loss: 0.00002465
Iteration 165/1000 | Loss: 0.00002465
Iteration 166/1000 | Loss: 0.00002465
Iteration 167/1000 | Loss: 0.00002465
Iteration 168/1000 | Loss: 0.00002465
Iteration 169/1000 | Loss: 0.00002464
Iteration 170/1000 | Loss: 0.00002464
Iteration 171/1000 | Loss: 0.00002464
Iteration 172/1000 | Loss: 0.00002464
Iteration 173/1000 | Loss: 0.00002464
Iteration 174/1000 | Loss: 0.00002464
Iteration 175/1000 | Loss: 0.00002464
Iteration 176/1000 | Loss: 0.00002464
Iteration 177/1000 | Loss: 0.00002464
Iteration 178/1000 | Loss: 0.00002464
Iteration 179/1000 | Loss: 0.00002464
Iteration 180/1000 | Loss: 0.00002464
Iteration 181/1000 | Loss: 0.00002464
Iteration 182/1000 | Loss: 0.00002464
Iteration 183/1000 | Loss: 0.00002464
Iteration 184/1000 | Loss: 0.00002464
Iteration 185/1000 | Loss: 0.00002464
Iteration 186/1000 | Loss: 0.00002464
Iteration 187/1000 | Loss: 0.00002464
Iteration 188/1000 | Loss: 0.00002464
Iteration 189/1000 | Loss: 0.00002464
Iteration 190/1000 | Loss: 0.00002464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.46374293055851e-05, 2.46374293055851e-05, 2.46374293055851e-05, 2.46374293055851e-05, 2.46374293055851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.46374293055851e-05

Optimization complete. Final v2v error: 3.9556398391723633 mm

Highest mean error: 6.449835777282715 mm for frame 34

Lowest mean error: 2.8296008110046387 mm for frame 71

Saving results

Total time: 132.43095636367798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404593
Iteration 2/25 | Loss: 0.00089581
Iteration 3/25 | Loss: 0.00075971
Iteration 4/25 | Loss: 0.00074063
Iteration 5/25 | Loss: 0.00073605
Iteration 6/25 | Loss: 0.00073452
Iteration 7/25 | Loss: 0.00073412
Iteration 8/25 | Loss: 0.00073412
Iteration 9/25 | Loss: 0.00073412
Iteration 10/25 | Loss: 0.00073412
Iteration 11/25 | Loss: 0.00073412
Iteration 12/25 | Loss: 0.00073412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007341185701079667, 0.0007341185701079667, 0.0007341185701079667, 0.0007341185701079667, 0.0007341185701079667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007341185701079667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59458852
Iteration 2/25 | Loss: 0.00113187
Iteration 3/25 | Loss: 0.00113186
Iteration 4/25 | Loss: 0.00113186
Iteration 5/25 | Loss: 0.00113186
Iteration 6/25 | Loss: 0.00113186
Iteration 7/25 | Loss: 0.00113186
Iteration 8/25 | Loss: 0.00113186
Iteration 9/25 | Loss: 0.00113186
Iteration 10/25 | Loss: 0.00113186
Iteration 11/25 | Loss: 0.00113186
Iteration 12/25 | Loss: 0.00113186
Iteration 13/25 | Loss: 0.00113186
Iteration 14/25 | Loss: 0.00113186
Iteration 15/25 | Loss: 0.00113186
Iteration 16/25 | Loss: 0.00113186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001131859258748591, 0.001131859258748591, 0.001131859258748591, 0.001131859258748591, 0.001131859258748591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001131859258748591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113186
Iteration 2/1000 | Loss: 0.00002157
Iteration 3/1000 | Loss: 0.00001340
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001136
Iteration 6/1000 | Loss: 0.00001098
Iteration 7/1000 | Loss: 0.00001067
Iteration 8/1000 | Loss: 0.00001052
Iteration 9/1000 | Loss: 0.00001044
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001041
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001040
Iteration 16/1000 | Loss: 0.00001038
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001037
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001036
Iteration 21/1000 | Loss: 0.00001034
Iteration 22/1000 | Loss: 0.00001034
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001034
Iteration 25/1000 | Loss: 0.00001032
Iteration 26/1000 | Loss: 0.00001031
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001031
Iteration 29/1000 | Loss: 0.00001031
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001030
Iteration 32/1000 | Loss: 0.00001030
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001027
Iteration 38/1000 | Loss: 0.00001027
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001027
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001026
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001026
Iteration 45/1000 | Loss: 0.00001026
Iteration 46/1000 | Loss: 0.00001026
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001024
Iteration 49/1000 | Loss: 0.00001023
Iteration 50/1000 | Loss: 0.00001022
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001016
Iteration 63/1000 | Loss: 0.00001016
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001013
Iteration 72/1000 | Loss: 0.00001013
Iteration 73/1000 | Loss: 0.00001011
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001010
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001009
Iteration 79/1000 | Loss: 0.00001009
Iteration 80/1000 | Loss: 0.00001009
Iteration 81/1000 | Loss: 0.00001009
Iteration 82/1000 | Loss: 0.00001009
Iteration 83/1000 | Loss: 0.00001009
Iteration 84/1000 | Loss: 0.00001008
Iteration 85/1000 | Loss: 0.00001008
Iteration 86/1000 | Loss: 0.00001008
Iteration 87/1000 | Loss: 0.00001008
Iteration 88/1000 | Loss: 0.00001008
Iteration 89/1000 | Loss: 0.00001008
Iteration 90/1000 | Loss: 0.00001008
Iteration 91/1000 | Loss: 0.00001008
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001007
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001007
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001007
Iteration 98/1000 | Loss: 0.00001007
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001007
Iteration 102/1000 | Loss: 0.00001007
Iteration 103/1000 | Loss: 0.00001007
Iteration 104/1000 | Loss: 0.00001006
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001006
Iteration 107/1000 | Loss: 0.00001006
Iteration 108/1000 | Loss: 0.00001006
Iteration 109/1000 | Loss: 0.00001005
Iteration 110/1000 | Loss: 0.00001005
Iteration 111/1000 | Loss: 0.00001005
Iteration 112/1000 | Loss: 0.00001005
Iteration 113/1000 | Loss: 0.00001004
Iteration 114/1000 | Loss: 0.00001004
Iteration 115/1000 | Loss: 0.00001004
Iteration 116/1000 | Loss: 0.00001004
Iteration 117/1000 | Loss: 0.00001004
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001003
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001000
Iteration 129/1000 | Loss: 0.00001000
Iteration 130/1000 | Loss: 0.00000999
Iteration 131/1000 | Loss: 0.00000999
Iteration 132/1000 | Loss: 0.00000999
Iteration 133/1000 | Loss: 0.00000999
Iteration 134/1000 | Loss: 0.00000999
Iteration 135/1000 | Loss: 0.00000999
Iteration 136/1000 | Loss: 0.00000999
Iteration 137/1000 | Loss: 0.00000999
Iteration 138/1000 | Loss: 0.00000999
Iteration 139/1000 | Loss: 0.00000999
Iteration 140/1000 | Loss: 0.00000999
Iteration 141/1000 | Loss: 0.00000999
Iteration 142/1000 | Loss: 0.00000999
Iteration 143/1000 | Loss: 0.00000999
Iteration 144/1000 | Loss: 0.00000998
Iteration 145/1000 | Loss: 0.00000998
Iteration 146/1000 | Loss: 0.00000998
Iteration 147/1000 | Loss: 0.00000998
Iteration 148/1000 | Loss: 0.00000998
Iteration 149/1000 | Loss: 0.00000998
Iteration 150/1000 | Loss: 0.00000998
Iteration 151/1000 | Loss: 0.00000998
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000997
Iteration 155/1000 | Loss: 0.00000997
Iteration 156/1000 | Loss: 0.00000997
Iteration 157/1000 | Loss: 0.00000997
Iteration 158/1000 | Loss: 0.00000997
Iteration 159/1000 | Loss: 0.00000997
Iteration 160/1000 | Loss: 0.00000997
Iteration 161/1000 | Loss: 0.00000997
Iteration 162/1000 | Loss: 0.00000997
Iteration 163/1000 | Loss: 0.00000997
Iteration 164/1000 | Loss: 0.00000997
Iteration 165/1000 | Loss: 0.00000997
Iteration 166/1000 | Loss: 0.00000997
Iteration 167/1000 | Loss: 0.00000996
Iteration 168/1000 | Loss: 0.00000996
Iteration 169/1000 | Loss: 0.00000996
Iteration 170/1000 | Loss: 0.00000996
Iteration 171/1000 | Loss: 0.00000996
Iteration 172/1000 | Loss: 0.00000996
Iteration 173/1000 | Loss: 0.00000996
Iteration 174/1000 | Loss: 0.00000996
Iteration 175/1000 | Loss: 0.00000996
Iteration 176/1000 | Loss: 0.00000996
Iteration 177/1000 | Loss: 0.00000995
Iteration 178/1000 | Loss: 0.00000995
Iteration 179/1000 | Loss: 0.00000995
Iteration 180/1000 | Loss: 0.00000995
Iteration 181/1000 | Loss: 0.00000995
Iteration 182/1000 | Loss: 0.00000995
Iteration 183/1000 | Loss: 0.00000995
Iteration 184/1000 | Loss: 0.00000995
Iteration 185/1000 | Loss: 0.00000995
Iteration 186/1000 | Loss: 0.00000995
Iteration 187/1000 | Loss: 0.00000995
Iteration 188/1000 | Loss: 0.00000995
Iteration 189/1000 | Loss: 0.00000995
Iteration 190/1000 | Loss: 0.00000995
Iteration 191/1000 | Loss: 0.00000995
Iteration 192/1000 | Loss: 0.00000995
Iteration 193/1000 | Loss: 0.00000995
Iteration 194/1000 | Loss: 0.00000995
Iteration 195/1000 | Loss: 0.00000995
Iteration 196/1000 | Loss: 0.00000995
Iteration 197/1000 | Loss: 0.00000995
Iteration 198/1000 | Loss: 0.00000995
Iteration 199/1000 | Loss: 0.00000995
Iteration 200/1000 | Loss: 0.00000995
Iteration 201/1000 | Loss: 0.00000995
Iteration 202/1000 | Loss: 0.00000995
Iteration 203/1000 | Loss: 0.00000995
Iteration 204/1000 | Loss: 0.00000995
Iteration 205/1000 | Loss: 0.00000995
Iteration 206/1000 | Loss: 0.00000995
Iteration 207/1000 | Loss: 0.00000995
Iteration 208/1000 | Loss: 0.00000995
Iteration 209/1000 | Loss: 0.00000995
Iteration 210/1000 | Loss: 0.00000995
Iteration 211/1000 | Loss: 0.00000995
Iteration 212/1000 | Loss: 0.00000995
Iteration 213/1000 | Loss: 0.00000995
Iteration 214/1000 | Loss: 0.00000995
Iteration 215/1000 | Loss: 0.00000995
Iteration 216/1000 | Loss: 0.00000995
Iteration 217/1000 | Loss: 0.00000995
Iteration 218/1000 | Loss: 0.00000995
Iteration 219/1000 | Loss: 0.00000995
Iteration 220/1000 | Loss: 0.00000995
Iteration 221/1000 | Loss: 0.00000995
Iteration 222/1000 | Loss: 0.00000995
Iteration 223/1000 | Loss: 0.00000995
Iteration 224/1000 | Loss: 0.00000995
Iteration 225/1000 | Loss: 0.00000995
Iteration 226/1000 | Loss: 0.00000995
Iteration 227/1000 | Loss: 0.00000995
Iteration 228/1000 | Loss: 0.00000995
Iteration 229/1000 | Loss: 0.00000995
Iteration 230/1000 | Loss: 0.00000995
Iteration 231/1000 | Loss: 0.00000995
Iteration 232/1000 | Loss: 0.00000995
Iteration 233/1000 | Loss: 0.00000995
Iteration 234/1000 | Loss: 0.00000995
Iteration 235/1000 | Loss: 0.00000995
Iteration 236/1000 | Loss: 0.00000995
Iteration 237/1000 | Loss: 0.00000995
Iteration 238/1000 | Loss: 0.00000995
Iteration 239/1000 | Loss: 0.00000995
Iteration 240/1000 | Loss: 0.00000995
Iteration 241/1000 | Loss: 0.00000995
Iteration 242/1000 | Loss: 0.00000995
Iteration 243/1000 | Loss: 0.00000995
Iteration 244/1000 | Loss: 0.00000995
Iteration 245/1000 | Loss: 0.00000995
Iteration 246/1000 | Loss: 0.00000995
Iteration 247/1000 | Loss: 0.00000995
Iteration 248/1000 | Loss: 0.00000995
Iteration 249/1000 | Loss: 0.00000995
Iteration 250/1000 | Loss: 0.00000995
Iteration 251/1000 | Loss: 0.00000995
Iteration 252/1000 | Loss: 0.00000995
Iteration 253/1000 | Loss: 0.00000995
Iteration 254/1000 | Loss: 0.00000995
Iteration 255/1000 | Loss: 0.00000995
Iteration 256/1000 | Loss: 0.00000995
Iteration 257/1000 | Loss: 0.00000995
Iteration 258/1000 | Loss: 0.00000995
Iteration 259/1000 | Loss: 0.00000995
Iteration 260/1000 | Loss: 0.00000995
Iteration 261/1000 | Loss: 0.00000995
Iteration 262/1000 | Loss: 0.00000995
Iteration 263/1000 | Loss: 0.00000995
Iteration 264/1000 | Loss: 0.00000995
Iteration 265/1000 | Loss: 0.00000995
Iteration 266/1000 | Loss: 0.00000995
Iteration 267/1000 | Loss: 0.00000995
Iteration 268/1000 | Loss: 0.00000995
Iteration 269/1000 | Loss: 0.00000995
Iteration 270/1000 | Loss: 0.00000995
Iteration 271/1000 | Loss: 0.00000995
Iteration 272/1000 | Loss: 0.00000995
Iteration 273/1000 | Loss: 0.00000995
Iteration 274/1000 | Loss: 0.00000995
Iteration 275/1000 | Loss: 0.00000995
Iteration 276/1000 | Loss: 0.00000995
Iteration 277/1000 | Loss: 0.00000995
Iteration 278/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [9.947239050234202e-06, 9.947239050234202e-06, 9.947239050234202e-06, 9.947239050234202e-06, 9.947239050234202e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.947239050234202e-06

Optimization complete. Final v2v error: 2.685560703277588 mm

Highest mean error: 3.4672248363494873 mm for frame 60

Lowest mean error: 2.5115315914154053 mm for frame 84

Saving results

Total time: 38.010035276412964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014246
Iteration 2/25 | Loss: 0.00246080
Iteration 3/25 | Loss: 0.00160615
Iteration 4/25 | Loss: 0.00143687
Iteration 5/25 | Loss: 0.00150745
Iteration 6/25 | Loss: 0.00143474
Iteration 7/25 | Loss: 0.00132083
Iteration 8/25 | Loss: 0.00120071
Iteration 9/25 | Loss: 0.00118650
Iteration 10/25 | Loss: 0.00107978
Iteration 11/25 | Loss: 0.00095988
Iteration 12/25 | Loss: 0.00091308
Iteration 13/25 | Loss: 0.00089974
Iteration 14/25 | Loss: 0.00088847
Iteration 15/25 | Loss: 0.00088501
Iteration 16/25 | Loss: 0.00087447
Iteration 17/25 | Loss: 0.00087551
Iteration 18/25 | Loss: 0.00087334
Iteration 19/25 | Loss: 0.00087106
Iteration 20/25 | Loss: 0.00087001
Iteration 21/25 | Loss: 0.00087246
Iteration 22/25 | Loss: 0.00087954
Iteration 23/25 | Loss: 0.00087737
Iteration 24/25 | Loss: 0.00087018
Iteration 25/25 | Loss: 0.00086835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62613654
Iteration 2/25 | Loss: 0.00202703
Iteration 3/25 | Loss: 0.00202703
Iteration 4/25 | Loss: 0.00202703
Iteration 5/25 | Loss: 0.00202703
Iteration 6/25 | Loss: 0.00202703
Iteration 7/25 | Loss: 0.00202703
Iteration 8/25 | Loss: 0.00202703
Iteration 9/25 | Loss: 0.00202703
Iteration 10/25 | Loss: 0.00202703
Iteration 11/25 | Loss: 0.00202703
Iteration 12/25 | Loss: 0.00202703
Iteration 13/25 | Loss: 0.00202703
Iteration 14/25 | Loss: 0.00202703
Iteration 15/25 | Loss: 0.00202703
Iteration 16/25 | Loss: 0.00202703
Iteration 17/25 | Loss: 0.00202703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002027027076110244, 0.002027027076110244, 0.002027027076110244, 0.002027027076110244, 0.002027027076110244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002027027076110244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202703
Iteration 2/1000 | Loss: 0.00037343
Iteration 3/1000 | Loss: 0.00032929
Iteration 4/1000 | Loss: 0.00029544
Iteration 5/1000 | Loss: 0.00016073
Iteration 6/1000 | Loss: 0.00033541
Iteration 7/1000 | Loss: 0.00045626
Iteration 8/1000 | Loss: 0.00077961
Iteration 9/1000 | Loss: 0.00023764
Iteration 10/1000 | Loss: 0.00023528
Iteration 11/1000 | Loss: 0.00035070
Iteration 12/1000 | Loss: 0.00014851
Iteration 13/1000 | Loss: 0.00061480
Iteration 14/1000 | Loss: 0.00136532
Iteration 15/1000 | Loss: 0.00103738
Iteration 16/1000 | Loss: 0.00098013
Iteration 17/1000 | Loss: 0.00080488
Iteration 18/1000 | Loss: 0.00046288
Iteration 19/1000 | Loss: 0.00028094
Iteration 20/1000 | Loss: 0.00005509
Iteration 21/1000 | Loss: 0.00006965
Iteration 22/1000 | Loss: 0.00060126
Iteration 23/1000 | Loss: 0.00059805
Iteration 24/1000 | Loss: 0.00027677
Iteration 25/1000 | Loss: 0.00021893
Iteration 26/1000 | Loss: 0.00067042
Iteration 27/1000 | Loss: 0.00027085
Iteration 28/1000 | Loss: 0.00106045
Iteration 29/1000 | Loss: 0.00021850
Iteration 30/1000 | Loss: 0.00024809
Iteration 31/1000 | Loss: 0.00020489
Iteration 32/1000 | Loss: 0.00019458
Iteration 33/1000 | Loss: 0.00003298
Iteration 34/1000 | Loss: 0.00003083
Iteration 35/1000 | Loss: 0.00002793
Iteration 36/1000 | Loss: 0.00002598
Iteration 37/1000 | Loss: 0.00003964
Iteration 38/1000 | Loss: 0.00008156
Iteration 39/1000 | Loss: 0.00004817
Iteration 40/1000 | Loss: 0.00002781
Iteration 41/1000 | Loss: 0.00004842
Iteration 42/1000 | Loss: 0.00017535
Iteration 43/1000 | Loss: 0.00003988
Iteration 44/1000 | Loss: 0.00005596
Iteration 45/1000 | Loss: 0.00004068
Iteration 46/1000 | Loss: 0.00008483
Iteration 47/1000 | Loss: 0.00006608
Iteration 48/1000 | Loss: 0.00005013
Iteration 49/1000 | Loss: 0.00004070
Iteration 50/1000 | Loss: 0.00002895
Iteration 51/1000 | Loss: 0.00018575
Iteration 52/1000 | Loss: 0.00004427
Iteration 53/1000 | Loss: 0.00002479
Iteration 54/1000 | Loss: 0.00004477
Iteration 55/1000 | Loss: 0.00003393
Iteration 56/1000 | Loss: 0.00003496
Iteration 57/1000 | Loss: 0.00003278
Iteration 58/1000 | Loss: 0.00002806
Iteration 59/1000 | Loss: 0.00003912
Iteration 60/1000 | Loss: 0.00007613
Iteration 61/1000 | Loss: 0.00003156
Iteration 62/1000 | Loss: 0.00002856
Iteration 63/1000 | Loss: 0.00004009
Iteration 64/1000 | Loss: 0.00004507
Iteration 65/1000 | Loss: 0.00003270
Iteration 66/1000 | Loss: 0.00004343
Iteration 67/1000 | Loss: 0.00003403
Iteration 68/1000 | Loss: 0.00004475
Iteration 69/1000 | Loss: 0.00005134
Iteration 70/1000 | Loss: 0.00004537
Iteration 71/1000 | Loss: 0.00003538
Iteration 72/1000 | Loss: 0.00003083
Iteration 73/1000 | Loss: 0.00004192
Iteration 74/1000 | Loss: 0.00004151
Iteration 75/1000 | Loss: 0.00002502
Iteration 76/1000 | Loss: 0.00002905
Iteration 77/1000 | Loss: 0.00003911
Iteration 78/1000 | Loss: 0.00003971
Iteration 79/1000 | Loss: 0.00004307
Iteration 80/1000 | Loss: 0.00003939
Iteration 81/1000 | Loss: 0.00004390
Iteration 82/1000 | Loss: 0.00003932
Iteration 83/1000 | Loss: 0.00004000
Iteration 84/1000 | Loss: 0.00004264
Iteration 85/1000 | Loss: 0.00004393
Iteration 86/1000 | Loss: 0.00003956
Iteration 87/1000 | Loss: 0.00004384
Iteration 88/1000 | Loss: 0.00003969
Iteration 89/1000 | Loss: 0.00004392
Iteration 90/1000 | Loss: 0.00003907
Iteration 91/1000 | Loss: 0.00005201
Iteration 92/1000 | Loss: 0.00003169
Iteration 93/1000 | Loss: 0.00004276
Iteration 94/1000 | Loss: 0.00003941
Iteration 95/1000 | Loss: 0.00004434
Iteration 96/1000 | Loss: 0.00003906
Iteration 97/1000 | Loss: 0.00004570
Iteration 98/1000 | Loss: 0.00003937
Iteration 99/1000 | Loss: 0.00003018
Iteration 100/1000 | Loss: 0.00002620
Iteration 101/1000 | Loss: 0.00002479
Iteration 102/1000 | Loss: 0.00008296
Iteration 103/1000 | Loss: 0.00002214
Iteration 104/1000 | Loss: 0.00003900
Iteration 105/1000 | Loss: 0.00002448
Iteration 106/1000 | Loss: 0.00005229
Iteration 107/1000 | Loss: 0.00002315
Iteration 108/1000 | Loss: 0.00004027
Iteration 109/1000 | Loss: 0.00002197
Iteration 110/1000 | Loss: 0.00002756
Iteration 111/1000 | Loss: 0.00002094
Iteration 112/1000 | Loss: 0.00002063
Iteration 113/1000 | Loss: 0.00002038
Iteration 114/1000 | Loss: 0.00002028
Iteration 115/1000 | Loss: 0.00002009
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002000
Iteration 118/1000 | Loss: 0.00001994
Iteration 119/1000 | Loss: 0.00001974
Iteration 120/1000 | Loss: 0.00001957
Iteration 121/1000 | Loss: 0.00073974
Iteration 122/1000 | Loss: 0.00002063
Iteration 123/1000 | Loss: 0.00001840
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001652
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001651
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001650
Iteration 134/1000 | Loss: 0.00001650
Iteration 135/1000 | Loss: 0.00001650
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001648
Iteration 138/1000 | Loss: 0.00001648
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001647
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001645
Iteration 144/1000 | Loss: 0.00001643
Iteration 145/1000 | Loss: 0.00001643
Iteration 146/1000 | Loss: 0.00001643
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001641
Iteration 149/1000 | Loss: 0.00001641
Iteration 150/1000 | Loss: 0.00001641
Iteration 151/1000 | Loss: 0.00001640
Iteration 152/1000 | Loss: 0.00001640
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001639
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Iteration 170/1000 | Loss: 0.00001637
Iteration 171/1000 | Loss: 0.00001636
Iteration 172/1000 | Loss: 0.00001636
Iteration 173/1000 | Loss: 0.00001635
Iteration 174/1000 | Loss: 0.00001635
Iteration 175/1000 | Loss: 0.00001635
Iteration 176/1000 | Loss: 0.00001634
Iteration 177/1000 | Loss: 0.00001634
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001634
Iteration 180/1000 | Loss: 0.00001634
Iteration 181/1000 | Loss: 0.00001633
Iteration 182/1000 | Loss: 0.00001633
Iteration 183/1000 | Loss: 0.00001633
Iteration 184/1000 | Loss: 0.00001633
Iteration 185/1000 | Loss: 0.00001633
Iteration 186/1000 | Loss: 0.00001633
Iteration 187/1000 | Loss: 0.00001633
Iteration 188/1000 | Loss: 0.00001633
Iteration 189/1000 | Loss: 0.00001632
Iteration 190/1000 | Loss: 0.00001632
Iteration 191/1000 | Loss: 0.00001632
Iteration 192/1000 | Loss: 0.00001632
Iteration 193/1000 | Loss: 0.00001632
Iteration 194/1000 | Loss: 0.00001632
Iteration 195/1000 | Loss: 0.00001632
Iteration 196/1000 | Loss: 0.00001632
Iteration 197/1000 | Loss: 0.00001632
Iteration 198/1000 | Loss: 0.00001631
Iteration 199/1000 | Loss: 0.00001631
Iteration 200/1000 | Loss: 0.00001631
Iteration 201/1000 | Loss: 0.00001631
Iteration 202/1000 | Loss: 0.00001631
Iteration 203/1000 | Loss: 0.00001631
Iteration 204/1000 | Loss: 0.00001631
Iteration 205/1000 | Loss: 0.00001631
Iteration 206/1000 | Loss: 0.00001631
Iteration 207/1000 | Loss: 0.00001631
Iteration 208/1000 | Loss: 0.00001631
Iteration 209/1000 | Loss: 0.00001631
Iteration 210/1000 | Loss: 0.00001630
Iteration 211/1000 | Loss: 0.00001630
Iteration 212/1000 | Loss: 0.00001630
Iteration 213/1000 | Loss: 0.00001630
Iteration 214/1000 | Loss: 0.00001630
Iteration 215/1000 | Loss: 0.00001630
Iteration 216/1000 | Loss: 0.00001630
Iteration 217/1000 | Loss: 0.00001630
Iteration 218/1000 | Loss: 0.00001630
Iteration 219/1000 | Loss: 0.00001630
Iteration 220/1000 | Loss: 0.00001630
Iteration 221/1000 | Loss: 0.00001630
Iteration 222/1000 | Loss: 0.00001630
Iteration 223/1000 | Loss: 0.00001630
Iteration 224/1000 | Loss: 0.00001630
Iteration 225/1000 | Loss: 0.00001630
Iteration 226/1000 | Loss: 0.00001630
Iteration 227/1000 | Loss: 0.00001630
Iteration 228/1000 | Loss: 0.00001629
Iteration 229/1000 | Loss: 0.00001629
Iteration 230/1000 | Loss: 0.00001629
Iteration 231/1000 | Loss: 0.00001629
Iteration 232/1000 | Loss: 0.00001629
Iteration 233/1000 | Loss: 0.00001629
Iteration 234/1000 | Loss: 0.00001629
Iteration 235/1000 | Loss: 0.00001629
Iteration 236/1000 | Loss: 0.00001629
Iteration 237/1000 | Loss: 0.00001629
Iteration 238/1000 | Loss: 0.00001629
Iteration 239/1000 | Loss: 0.00001629
Iteration 240/1000 | Loss: 0.00001629
Iteration 241/1000 | Loss: 0.00001629
Iteration 242/1000 | Loss: 0.00001629
Iteration 243/1000 | Loss: 0.00001629
Iteration 244/1000 | Loss: 0.00001629
Iteration 245/1000 | Loss: 0.00001628
Iteration 246/1000 | Loss: 0.00001628
Iteration 247/1000 | Loss: 0.00001628
Iteration 248/1000 | Loss: 0.00001628
Iteration 249/1000 | Loss: 0.00001628
Iteration 250/1000 | Loss: 0.00001628
Iteration 251/1000 | Loss: 0.00001628
Iteration 252/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.6284611774608493e-05, 1.6284611774608493e-05, 1.6284611774608493e-05, 1.6284611774608493e-05, 1.6284611774608493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6284611774608493e-05

Optimization complete. Final v2v error: 3.38260555267334 mm

Highest mean error: 4.707765579223633 mm for frame 102

Lowest mean error: 3.102108955383301 mm for frame 154

Saving results

Total time: 230.9838252067566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339327
Iteration 2/25 | Loss: 0.00110730
Iteration 3/25 | Loss: 0.00089974
Iteration 4/25 | Loss: 0.00084471
Iteration 5/25 | Loss: 0.00083008
Iteration 6/25 | Loss: 0.00082782
Iteration 7/25 | Loss: 0.00082699
Iteration 8/25 | Loss: 0.00082699
Iteration 9/25 | Loss: 0.00082699
Iteration 10/25 | Loss: 0.00082699
Iteration 11/25 | Loss: 0.00082699
Iteration 12/25 | Loss: 0.00082699
Iteration 13/25 | Loss: 0.00082699
Iteration 14/25 | Loss: 0.00082699
Iteration 15/25 | Loss: 0.00082699
Iteration 16/25 | Loss: 0.00082699
Iteration 17/25 | Loss: 0.00082699
Iteration 18/25 | Loss: 0.00082699
Iteration 19/25 | Loss: 0.00082699
Iteration 20/25 | Loss: 0.00082699
Iteration 21/25 | Loss: 0.00082699
Iteration 22/25 | Loss: 0.00082699
Iteration 23/25 | Loss: 0.00082699
Iteration 24/25 | Loss: 0.00082699
Iteration 25/25 | Loss: 0.00082699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52890265
Iteration 2/25 | Loss: 0.00153921
Iteration 3/25 | Loss: 0.00153920
Iteration 4/25 | Loss: 0.00153920
Iteration 5/25 | Loss: 0.00153920
Iteration 6/25 | Loss: 0.00153920
Iteration 7/25 | Loss: 0.00153920
Iteration 8/25 | Loss: 0.00153920
Iteration 9/25 | Loss: 0.00153920
Iteration 10/25 | Loss: 0.00153920
Iteration 11/25 | Loss: 0.00153920
Iteration 12/25 | Loss: 0.00153920
Iteration 13/25 | Loss: 0.00153920
Iteration 14/25 | Loss: 0.00153920
Iteration 15/25 | Loss: 0.00153920
Iteration 16/25 | Loss: 0.00153920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015392014756798744, 0.0015392014756798744, 0.0015392014756798744, 0.0015392014756798744, 0.0015392014756798744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015392014756798744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153920
Iteration 2/1000 | Loss: 0.00004534
Iteration 3/1000 | Loss: 0.00003053
Iteration 4/1000 | Loss: 0.00002659
Iteration 5/1000 | Loss: 0.00002525
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002286
Iteration 9/1000 | Loss: 0.00002249
Iteration 10/1000 | Loss: 0.00002219
Iteration 11/1000 | Loss: 0.00002195
Iteration 12/1000 | Loss: 0.00002179
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002163
Iteration 17/1000 | Loss: 0.00002163
Iteration 18/1000 | Loss: 0.00002157
Iteration 19/1000 | Loss: 0.00002155
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002152
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002151
Iteration 27/1000 | Loss: 0.00002151
Iteration 28/1000 | Loss: 0.00002151
Iteration 29/1000 | Loss: 0.00002150
Iteration 30/1000 | Loss: 0.00002150
Iteration 31/1000 | Loss: 0.00002150
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002149
Iteration 34/1000 | Loss: 0.00002148
Iteration 35/1000 | Loss: 0.00002148
Iteration 36/1000 | Loss: 0.00002148
Iteration 37/1000 | Loss: 0.00002148
Iteration 38/1000 | Loss: 0.00002147
Iteration 39/1000 | Loss: 0.00002147
Iteration 40/1000 | Loss: 0.00002147
Iteration 41/1000 | Loss: 0.00002146
Iteration 42/1000 | Loss: 0.00002146
Iteration 43/1000 | Loss: 0.00002146
Iteration 44/1000 | Loss: 0.00002146
Iteration 45/1000 | Loss: 0.00002145
Iteration 46/1000 | Loss: 0.00002145
Iteration 47/1000 | Loss: 0.00002145
Iteration 48/1000 | Loss: 0.00002145
Iteration 49/1000 | Loss: 0.00002145
Iteration 50/1000 | Loss: 0.00002145
Iteration 51/1000 | Loss: 0.00002145
Iteration 52/1000 | Loss: 0.00002145
Iteration 53/1000 | Loss: 0.00002144
Iteration 54/1000 | Loss: 0.00002144
Iteration 55/1000 | Loss: 0.00002144
Iteration 56/1000 | Loss: 0.00002143
Iteration 57/1000 | Loss: 0.00002143
Iteration 58/1000 | Loss: 0.00002143
Iteration 59/1000 | Loss: 0.00002142
Iteration 60/1000 | Loss: 0.00002142
Iteration 61/1000 | Loss: 0.00002142
Iteration 62/1000 | Loss: 0.00002142
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002141
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002140
Iteration 72/1000 | Loss: 0.00002140
Iteration 73/1000 | Loss: 0.00002140
Iteration 74/1000 | Loss: 0.00002140
Iteration 75/1000 | Loss: 0.00002140
Iteration 76/1000 | Loss: 0.00002140
Iteration 77/1000 | Loss: 0.00002140
Iteration 78/1000 | Loss: 0.00002140
Iteration 79/1000 | Loss: 0.00002140
Iteration 80/1000 | Loss: 0.00002139
Iteration 81/1000 | Loss: 0.00002139
Iteration 82/1000 | Loss: 0.00002139
Iteration 83/1000 | Loss: 0.00002139
Iteration 84/1000 | Loss: 0.00002138
Iteration 85/1000 | Loss: 0.00002138
Iteration 86/1000 | Loss: 0.00002138
Iteration 87/1000 | Loss: 0.00002138
Iteration 88/1000 | Loss: 0.00002137
Iteration 89/1000 | Loss: 0.00002137
Iteration 90/1000 | Loss: 0.00002137
Iteration 91/1000 | Loss: 0.00002137
Iteration 92/1000 | Loss: 0.00002137
Iteration 93/1000 | Loss: 0.00002136
Iteration 94/1000 | Loss: 0.00002136
Iteration 95/1000 | Loss: 0.00002136
Iteration 96/1000 | Loss: 0.00002136
Iteration 97/1000 | Loss: 0.00002136
Iteration 98/1000 | Loss: 0.00002135
Iteration 99/1000 | Loss: 0.00002135
Iteration 100/1000 | Loss: 0.00002135
Iteration 101/1000 | Loss: 0.00002135
Iteration 102/1000 | Loss: 0.00002135
Iteration 103/1000 | Loss: 0.00002135
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002134
Iteration 112/1000 | Loss: 0.00002134
Iteration 113/1000 | Loss: 0.00002134
Iteration 114/1000 | Loss: 0.00002134
Iteration 115/1000 | Loss: 0.00002134
Iteration 116/1000 | Loss: 0.00002134
Iteration 117/1000 | Loss: 0.00002134
Iteration 118/1000 | Loss: 0.00002134
Iteration 119/1000 | Loss: 0.00002134
Iteration 120/1000 | Loss: 0.00002134
Iteration 121/1000 | Loss: 0.00002134
Iteration 122/1000 | Loss: 0.00002134
Iteration 123/1000 | Loss: 0.00002134
Iteration 124/1000 | Loss: 0.00002134
Iteration 125/1000 | Loss: 0.00002134
Iteration 126/1000 | Loss: 0.00002134
Iteration 127/1000 | Loss: 0.00002134
Iteration 128/1000 | Loss: 0.00002134
Iteration 129/1000 | Loss: 0.00002134
Iteration 130/1000 | Loss: 0.00002134
Iteration 131/1000 | Loss: 0.00002134
Iteration 132/1000 | Loss: 0.00002134
Iteration 133/1000 | Loss: 0.00002134
Iteration 134/1000 | Loss: 0.00002134
Iteration 135/1000 | Loss: 0.00002134
Iteration 136/1000 | Loss: 0.00002134
Iteration 137/1000 | Loss: 0.00002134
Iteration 138/1000 | Loss: 0.00002134
Iteration 139/1000 | Loss: 0.00002134
Iteration 140/1000 | Loss: 0.00002134
Iteration 141/1000 | Loss: 0.00002134
Iteration 142/1000 | Loss: 0.00002134
Iteration 143/1000 | Loss: 0.00002134
Iteration 144/1000 | Loss: 0.00002134
Iteration 145/1000 | Loss: 0.00002134
Iteration 146/1000 | Loss: 0.00002134
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002134
Iteration 149/1000 | Loss: 0.00002134
Iteration 150/1000 | Loss: 0.00002134
Iteration 151/1000 | Loss: 0.00002134
Iteration 152/1000 | Loss: 0.00002134
Iteration 153/1000 | Loss: 0.00002134
Iteration 154/1000 | Loss: 0.00002134
Iteration 155/1000 | Loss: 0.00002134
Iteration 156/1000 | Loss: 0.00002134
Iteration 157/1000 | Loss: 0.00002134
Iteration 158/1000 | Loss: 0.00002134
Iteration 159/1000 | Loss: 0.00002134
Iteration 160/1000 | Loss: 0.00002134
Iteration 161/1000 | Loss: 0.00002134
Iteration 162/1000 | Loss: 0.00002134
Iteration 163/1000 | Loss: 0.00002134
Iteration 164/1000 | Loss: 0.00002134
Iteration 165/1000 | Loss: 0.00002134
Iteration 166/1000 | Loss: 0.00002134
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002134
Iteration 169/1000 | Loss: 0.00002134
Iteration 170/1000 | Loss: 0.00002134
Iteration 171/1000 | Loss: 0.00002134
Iteration 172/1000 | Loss: 0.00002134
Iteration 173/1000 | Loss: 0.00002134
Iteration 174/1000 | Loss: 0.00002134
Iteration 175/1000 | Loss: 0.00002134
Iteration 176/1000 | Loss: 0.00002134
Iteration 177/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.1341371393646114e-05, 2.1341371393646114e-05, 2.1341371393646114e-05, 2.1341371393646114e-05, 2.1341371393646114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1341371393646114e-05

Optimization complete. Final v2v error: 3.8037712574005127 mm

Highest mean error: 4.56789493560791 mm for frame 210

Lowest mean error: 2.7664849758148193 mm for frame 111

Saving results

Total time: 43.95097041130066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889768
Iteration 2/25 | Loss: 0.00134809
Iteration 3/25 | Loss: 0.00096457
Iteration 4/25 | Loss: 0.00089938
Iteration 5/25 | Loss: 0.00088741
Iteration 6/25 | Loss: 0.00088553
Iteration 7/25 | Loss: 0.00088520
Iteration 8/25 | Loss: 0.00088520
Iteration 9/25 | Loss: 0.00088520
Iteration 10/25 | Loss: 0.00088520
Iteration 11/25 | Loss: 0.00088520
Iteration 12/25 | Loss: 0.00088520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008851977763697505, 0.0008851977763697505, 0.0008851977763697505, 0.0008851977763697505, 0.0008851977763697505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008851977763697505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08457518
Iteration 2/25 | Loss: 0.00106638
Iteration 3/25 | Loss: 0.00106636
Iteration 4/25 | Loss: 0.00106636
Iteration 5/25 | Loss: 0.00106636
Iteration 6/25 | Loss: 0.00106636
Iteration 7/25 | Loss: 0.00106636
Iteration 8/25 | Loss: 0.00106636
Iteration 9/25 | Loss: 0.00106636
Iteration 10/25 | Loss: 0.00106636
Iteration 11/25 | Loss: 0.00106636
Iteration 12/25 | Loss: 0.00106636
Iteration 13/25 | Loss: 0.00106636
Iteration 14/25 | Loss: 0.00106636
Iteration 15/25 | Loss: 0.00106636
Iteration 16/25 | Loss: 0.00106636
Iteration 17/25 | Loss: 0.00106636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010663621360436082, 0.0010663621360436082, 0.0010663621360436082, 0.0010663621360436082, 0.0010663621360436082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010663621360436082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106636
Iteration 2/1000 | Loss: 0.00004360
Iteration 3/1000 | Loss: 0.00003164
Iteration 4/1000 | Loss: 0.00002763
Iteration 5/1000 | Loss: 0.00002618
Iteration 6/1000 | Loss: 0.00002500
Iteration 7/1000 | Loss: 0.00002441
Iteration 8/1000 | Loss: 0.00002368
Iteration 9/1000 | Loss: 0.00002349
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002329
Iteration 12/1000 | Loss: 0.00002320
Iteration 13/1000 | Loss: 0.00002311
Iteration 14/1000 | Loss: 0.00002310
Iteration 15/1000 | Loss: 0.00002309
Iteration 16/1000 | Loss: 0.00002307
Iteration 17/1000 | Loss: 0.00002306
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002293
Iteration 20/1000 | Loss: 0.00002293
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002292
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002292
Iteration 28/1000 | Loss: 0.00002292
Iteration 29/1000 | Loss: 0.00002292
Iteration 30/1000 | Loss: 0.00002292
Iteration 31/1000 | Loss: 0.00002292
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002292
Iteration 36/1000 | Loss: 0.00002292
Iteration 37/1000 | Loss: 0.00002292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [2.2918211470823735e-05, 2.2918211470823735e-05, 2.2918211470823735e-05, 2.2918211470823735e-05, 2.2918211470823735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2918211470823735e-05

Optimization complete. Final v2v error: 4.044980049133301 mm

Highest mean error: 4.7026519775390625 mm for frame 0

Lowest mean error: 3.9103963375091553 mm for frame 95

Saving results

Total time: 27.533689975738525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110698
Iteration 2/25 | Loss: 0.00306835
Iteration 3/25 | Loss: 0.00182019
Iteration 4/25 | Loss: 0.00158477
Iteration 5/25 | Loss: 0.00150660
Iteration 6/25 | Loss: 0.00147426
Iteration 7/25 | Loss: 0.00143669
Iteration 8/25 | Loss: 0.00141019
Iteration 9/25 | Loss: 0.00134797
Iteration 10/25 | Loss: 0.00133103
Iteration 11/25 | Loss: 0.00132389
Iteration 12/25 | Loss: 0.00130745
Iteration 13/25 | Loss: 0.00130968
Iteration 14/25 | Loss: 0.00129156
Iteration 15/25 | Loss: 0.00128409
Iteration 16/25 | Loss: 0.00127955
Iteration 17/25 | Loss: 0.00127865
Iteration 18/25 | Loss: 0.00128163
Iteration 19/25 | Loss: 0.00127456
Iteration 20/25 | Loss: 0.00127249
Iteration 21/25 | Loss: 0.00127123
Iteration 22/25 | Loss: 0.00127087
Iteration 23/25 | Loss: 0.00127055
Iteration 24/25 | Loss: 0.00127037
Iteration 25/25 | Loss: 0.00127028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.35004711
Iteration 2/25 | Loss: 0.00622231
Iteration 3/25 | Loss: 0.00517708
Iteration 4/25 | Loss: 0.00517702
Iteration 5/25 | Loss: 0.00517702
Iteration 6/25 | Loss: 0.00517702
Iteration 7/25 | Loss: 0.00517702
Iteration 8/25 | Loss: 0.00517701
Iteration 9/25 | Loss: 0.00517701
Iteration 10/25 | Loss: 0.00517701
Iteration 11/25 | Loss: 0.00517701
Iteration 12/25 | Loss: 0.00517701
Iteration 13/25 | Loss: 0.00517701
Iteration 14/25 | Loss: 0.00517701
Iteration 15/25 | Loss: 0.00517701
Iteration 16/25 | Loss: 0.00517701
Iteration 17/25 | Loss: 0.00517701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.005177014507353306, 0.005177014507353306, 0.005177014507353306, 0.005177014507353306, 0.005177014507353306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005177014507353306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00517701
Iteration 2/1000 | Loss: 0.00218611
Iteration 3/1000 | Loss: 0.01019456
Iteration 4/1000 | Loss: 0.00713413
Iteration 5/1000 | Loss: 0.00752184
Iteration 6/1000 | Loss: 0.00114932
Iteration 7/1000 | Loss: 0.00326568
Iteration 8/1000 | Loss: 0.00238144
Iteration 9/1000 | Loss: 0.00382249
Iteration 10/1000 | Loss: 0.00202522
Iteration 11/1000 | Loss: 0.00271721
Iteration 12/1000 | Loss: 0.00427310
Iteration 13/1000 | Loss: 0.00540791
Iteration 14/1000 | Loss: 0.00397260
Iteration 15/1000 | Loss: 0.00717570
Iteration 16/1000 | Loss: 0.01099346
Iteration 17/1000 | Loss: 0.01070882
Iteration 18/1000 | Loss: 0.01007154
Iteration 19/1000 | Loss: 0.01502778
Iteration 20/1000 | Loss: 0.00725856
Iteration 21/1000 | Loss: 0.00785707
Iteration 22/1000 | Loss: 0.00588199
Iteration 23/1000 | Loss: 0.00513247
Iteration 24/1000 | Loss: 0.00442947
Iteration 25/1000 | Loss: 0.00477972
Iteration 26/1000 | Loss: 0.00263453
Iteration 27/1000 | Loss: 0.00230123
Iteration 28/1000 | Loss: 0.00168165
Iteration 29/1000 | Loss: 0.00147138
Iteration 30/1000 | Loss: 0.00354469
Iteration 31/1000 | Loss: 0.00114758
Iteration 32/1000 | Loss: 0.00066761
Iteration 33/1000 | Loss: 0.00179965
Iteration 34/1000 | Loss: 0.00288771
Iteration 35/1000 | Loss: 0.00107738
Iteration 36/1000 | Loss: 0.00257865
Iteration 37/1000 | Loss: 0.00056182
Iteration 38/1000 | Loss: 0.00374247
Iteration 39/1000 | Loss: 0.00178029
Iteration 40/1000 | Loss: 0.00054059
Iteration 41/1000 | Loss: 0.00018449
Iteration 42/1000 | Loss: 0.00056702
Iteration 43/1000 | Loss: 0.00051744
Iteration 44/1000 | Loss: 0.00554106
Iteration 45/1000 | Loss: 0.00220790
Iteration 46/1000 | Loss: 0.00128109
Iteration 47/1000 | Loss: 0.00246018
Iteration 48/1000 | Loss: 0.00415068
Iteration 49/1000 | Loss: 0.00293118
Iteration 50/1000 | Loss: 0.00284842
Iteration 51/1000 | Loss: 0.00248429
Iteration 52/1000 | Loss: 0.00232767
Iteration 53/1000 | Loss: 0.00358400
Iteration 54/1000 | Loss: 0.00360008
Iteration 55/1000 | Loss: 0.00407642
Iteration 56/1000 | Loss: 0.00343683
Iteration 57/1000 | Loss: 0.00207281
Iteration 58/1000 | Loss: 0.00211925
Iteration 59/1000 | Loss: 0.00129566
Iteration 60/1000 | Loss: 0.00156359
Iteration 61/1000 | Loss: 0.00125632
Iteration 62/1000 | Loss: 0.00101253
Iteration 63/1000 | Loss: 0.00098601
Iteration 64/1000 | Loss: 0.00058631
Iteration 65/1000 | Loss: 0.00179542
Iteration 66/1000 | Loss: 0.00258047
Iteration 67/1000 | Loss: 0.00174433
Iteration 68/1000 | Loss: 0.00193574
Iteration 69/1000 | Loss: 0.00128877
Iteration 70/1000 | Loss: 0.00079845
Iteration 71/1000 | Loss: 0.00013614
Iteration 72/1000 | Loss: 0.00066493
Iteration 73/1000 | Loss: 0.00024467
Iteration 74/1000 | Loss: 0.00069240
Iteration 75/1000 | Loss: 0.00190822
Iteration 76/1000 | Loss: 0.00099598
Iteration 77/1000 | Loss: 0.00062294
Iteration 78/1000 | Loss: 0.00054673
Iteration 79/1000 | Loss: 0.00084991
Iteration 80/1000 | Loss: 0.00093082
Iteration 81/1000 | Loss: 0.00113783
Iteration 82/1000 | Loss: 0.00084026
Iteration 83/1000 | Loss: 0.00045302
Iteration 84/1000 | Loss: 0.00070957
Iteration 85/1000 | Loss: 0.00043476
Iteration 86/1000 | Loss: 0.00040806
Iteration 87/1000 | Loss: 0.00072931
Iteration 88/1000 | Loss: 0.00096699
Iteration 89/1000 | Loss: 0.00094578
Iteration 90/1000 | Loss: 0.00098325
Iteration 91/1000 | Loss: 0.00064454
Iteration 92/1000 | Loss: 0.00074592
Iteration 93/1000 | Loss: 0.00049188
Iteration 94/1000 | Loss: 0.00054215
Iteration 95/1000 | Loss: 0.00095084
Iteration 96/1000 | Loss: 0.00065897
Iteration 97/1000 | Loss: 0.00068268
Iteration 98/1000 | Loss: 0.00020404
Iteration 99/1000 | Loss: 0.00038438
Iteration 100/1000 | Loss: 0.00041642
Iteration 101/1000 | Loss: 0.00025428
Iteration 102/1000 | Loss: 0.00009347
Iteration 103/1000 | Loss: 0.00007909
Iteration 104/1000 | Loss: 0.00061463
Iteration 105/1000 | Loss: 0.00036207
Iteration 106/1000 | Loss: 0.00055628
Iteration 107/1000 | Loss: 0.00009814
Iteration 108/1000 | Loss: 0.00076983
Iteration 109/1000 | Loss: 0.00031985
Iteration 110/1000 | Loss: 0.00033658
Iteration 111/1000 | Loss: 0.00027148
Iteration 112/1000 | Loss: 0.00030772
Iteration 113/1000 | Loss: 0.00210570
Iteration 114/1000 | Loss: 0.00041379
Iteration 115/1000 | Loss: 0.00022017
Iteration 116/1000 | Loss: 0.00033248
Iteration 117/1000 | Loss: 0.00050100
Iteration 118/1000 | Loss: 0.00027242
Iteration 119/1000 | Loss: 0.00077091
Iteration 120/1000 | Loss: 0.00026338
Iteration 121/1000 | Loss: 0.00047683
Iteration 122/1000 | Loss: 0.00070240
Iteration 123/1000 | Loss: 0.00025196
Iteration 124/1000 | Loss: 0.00047707
Iteration 125/1000 | Loss: 0.00012626
Iteration 126/1000 | Loss: 0.00013627
Iteration 127/1000 | Loss: 0.00017122
Iteration 128/1000 | Loss: 0.00006008
Iteration 129/1000 | Loss: 0.00005598
Iteration 130/1000 | Loss: 0.00004645
Iteration 131/1000 | Loss: 0.00004416
Iteration 132/1000 | Loss: 0.00004192
Iteration 133/1000 | Loss: 0.00190067
Iteration 134/1000 | Loss: 0.00226135
Iteration 135/1000 | Loss: 0.00027489
Iteration 136/1000 | Loss: 0.00003860
Iteration 137/1000 | Loss: 0.00003494
Iteration 138/1000 | Loss: 0.00003214
Iteration 139/1000 | Loss: 0.00003080
Iteration 140/1000 | Loss: 0.00002982
Iteration 141/1000 | Loss: 0.00002922
Iteration 142/1000 | Loss: 0.00002859
Iteration 143/1000 | Loss: 0.00002797
Iteration 144/1000 | Loss: 0.00002755
Iteration 145/1000 | Loss: 0.00002704
Iteration 146/1000 | Loss: 0.00002683
Iteration 147/1000 | Loss: 0.00002662
Iteration 148/1000 | Loss: 0.00002646
Iteration 149/1000 | Loss: 0.00002645
Iteration 150/1000 | Loss: 0.00002634
Iteration 151/1000 | Loss: 0.00002632
Iteration 152/1000 | Loss: 0.00002625
Iteration 153/1000 | Loss: 0.00002619
Iteration 154/1000 | Loss: 0.00002619
Iteration 155/1000 | Loss: 0.00002616
Iteration 156/1000 | Loss: 0.00002615
Iteration 157/1000 | Loss: 0.00002614
Iteration 158/1000 | Loss: 0.00002614
Iteration 159/1000 | Loss: 0.00002613
Iteration 160/1000 | Loss: 0.00002613
Iteration 161/1000 | Loss: 0.00002613
Iteration 162/1000 | Loss: 0.00002612
Iteration 163/1000 | Loss: 0.00002612
Iteration 164/1000 | Loss: 0.00002612
Iteration 165/1000 | Loss: 0.00002612
Iteration 166/1000 | Loss: 0.00002611
Iteration 167/1000 | Loss: 0.00002611
Iteration 168/1000 | Loss: 0.00002611
Iteration 169/1000 | Loss: 0.00002611
Iteration 170/1000 | Loss: 0.00002611
Iteration 171/1000 | Loss: 0.00002611
Iteration 172/1000 | Loss: 0.00002610
Iteration 173/1000 | Loss: 0.00002610
Iteration 174/1000 | Loss: 0.00002610
Iteration 175/1000 | Loss: 0.00002609
Iteration 176/1000 | Loss: 0.00002609
Iteration 177/1000 | Loss: 0.00002609
Iteration 178/1000 | Loss: 0.00002608
Iteration 179/1000 | Loss: 0.00002608
Iteration 180/1000 | Loss: 0.00002608
Iteration 181/1000 | Loss: 0.00002607
Iteration 182/1000 | Loss: 0.00002607
Iteration 183/1000 | Loss: 0.00002607
Iteration 184/1000 | Loss: 0.00002605
Iteration 185/1000 | Loss: 0.00002605
Iteration 186/1000 | Loss: 0.00002604
Iteration 187/1000 | Loss: 0.00002604
Iteration 188/1000 | Loss: 0.00002604
Iteration 189/1000 | Loss: 0.00002604
Iteration 190/1000 | Loss: 0.00002604
Iteration 191/1000 | Loss: 0.00002604
Iteration 192/1000 | Loss: 0.00002604
Iteration 193/1000 | Loss: 0.00002604
Iteration 194/1000 | Loss: 0.00002604
Iteration 195/1000 | Loss: 0.00002603
Iteration 196/1000 | Loss: 0.00002602
Iteration 197/1000 | Loss: 0.00002602
Iteration 198/1000 | Loss: 0.00002602
Iteration 199/1000 | Loss: 0.00002601
Iteration 200/1000 | Loss: 0.00002601
Iteration 201/1000 | Loss: 0.00002601
Iteration 202/1000 | Loss: 0.00002601
Iteration 203/1000 | Loss: 0.00002601
Iteration 204/1000 | Loss: 0.00002601
Iteration 205/1000 | Loss: 0.00002601
Iteration 206/1000 | Loss: 0.00002601
Iteration 207/1000 | Loss: 0.00002601
Iteration 208/1000 | Loss: 0.00002601
Iteration 209/1000 | Loss: 0.00002601
Iteration 210/1000 | Loss: 0.00002601
Iteration 211/1000 | Loss: 0.00002601
Iteration 212/1000 | Loss: 0.00002601
Iteration 213/1000 | Loss: 0.00002601
Iteration 214/1000 | Loss: 0.00002600
Iteration 215/1000 | Loss: 0.00002600
Iteration 216/1000 | Loss: 0.00002600
Iteration 217/1000 | Loss: 0.00002599
Iteration 218/1000 | Loss: 0.00002599
Iteration 219/1000 | Loss: 0.00002599
Iteration 220/1000 | Loss: 0.00002599
Iteration 221/1000 | Loss: 0.00002598
Iteration 222/1000 | Loss: 0.00002598
Iteration 223/1000 | Loss: 0.00002598
Iteration 224/1000 | Loss: 0.00002598
Iteration 225/1000 | Loss: 0.00002598
Iteration 226/1000 | Loss: 0.00002597
Iteration 227/1000 | Loss: 0.00002597
Iteration 228/1000 | Loss: 0.00002597
Iteration 229/1000 | Loss: 0.00002597
Iteration 230/1000 | Loss: 0.00002597
Iteration 231/1000 | Loss: 0.00002597
Iteration 232/1000 | Loss: 0.00002596
Iteration 233/1000 | Loss: 0.00002596
Iteration 234/1000 | Loss: 0.00002596
Iteration 235/1000 | Loss: 0.00002596
Iteration 236/1000 | Loss: 0.00002596
Iteration 237/1000 | Loss: 0.00002595
Iteration 238/1000 | Loss: 0.00002595
Iteration 239/1000 | Loss: 0.00002595
Iteration 240/1000 | Loss: 0.00002595
Iteration 241/1000 | Loss: 0.00002595
Iteration 242/1000 | Loss: 0.00002595
Iteration 243/1000 | Loss: 0.00002595
Iteration 244/1000 | Loss: 0.00002594
Iteration 245/1000 | Loss: 0.00002594
Iteration 246/1000 | Loss: 0.00002594
Iteration 247/1000 | Loss: 0.00002594
Iteration 248/1000 | Loss: 0.00002594
Iteration 249/1000 | Loss: 0.00002594
Iteration 250/1000 | Loss: 0.00002594
Iteration 251/1000 | Loss: 0.00002594
Iteration 252/1000 | Loss: 0.00002594
Iteration 253/1000 | Loss: 0.00002594
Iteration 254/1000 | Loss: 0.00002594
Iteration 255/1000 | Loss: 0.00002594
Iteration 256/1000 | Loss: 0.00002594
Iteration 257/1000 | Loss: 0.00002594
Iteration 258/1000 | Loss: 0.00002594
Iteration 259/1000 | Loss: 0.00002593
Iteration 260/1000 | Loss: 0.00002593
Iteration 261/1000 | Loss: 0.00002593
Iteration 262/1000 | Loss: 0.00002593
Iteration 263/1000 | Loss: 0.00002593
Iteration 264/1000 | Loss: 0.00002593
Iteration 265/1000 | Loss: 0.00002593
Iteration 266/1000 | Loss: 0.00002593
Iteration 267/1000 | Loss: 0.00002593
Iteration 268/1000 | Loss: 0.00002593
Iteration 269/1000 | Loss: 0.00002593
Iteration 270/1000 | Loss: 0.00002593
Iteration 271/1000 | Loss: 0.00002593
Iteration 272/1000 | Loss: 0.00002593
Iteration 273/1000 | Loss: 0.00002593
Iteration 274/1000 | Loss: 0.00002592
Iteration 275/1000 | Loss: 0.00002592
Iteration 276/1000 | Loss: 0.00002592
Iteration 277/1000 | Loss: 0.00002592
Iteration 278/1000 | Loss: 0.00002592
Iteration 279/1000 | Loss: 0.00002592
Iteration 280/1000 | Loss: 0.00002592
Iteration 281/1000 | Loss: 0.00002592
Iteration 282/1000 | Loss: 0.00002592
Iteration 283/1000 | Loss: 0.00002592
Iteration 284/1000 | Loss: 0.00002592
Iteration 285/1000 | Loss: 0.00002592
Iteration 286/1000 | Loss: 0.00002592
Iteration 287/1000 | Loss: 0.00002592
Iteration 288/1000 | Loss: 0.00002592
Iteration 289/1000 | Loss: 0.00002592
Iteration 290/1000 | Loss: 0.00002592
Iteration 291/1000 | Loss: 0.00002592
Iteration 292/1000 | Loss: 0.00002592
Iteration 293/1000 | Loss: 0.00002592
Iteration 294/1000 | Loss: 0.00002592
Iteration 295/1000 | Loss: 0.00002592
Iteration 296/1000 | Loss: 0.00002592
Iteration 297/1000 | Loss: 0.00002592
Iteration 298/1000 | Loss: 0.00002592
Iteration 299/1000 | Loss: 0.00002592
Iteration 300/1000 | Loss: 0.00002592
Iteration 301/1000 | Loss: 0.00002592
Iteration 302/1000 | Loss: 0.00002592
Iteration 303/1000 | Loss: 0.00002592
Iteration 304/1000 | Loss: 0.00002592
Iteration 305/1000 | Loss: 0.00002592
Iteration 306/1000 | Loss: 0.00002592
Iteration 307/1000 | Loss: 0.00002592
Iteration 308/1000 | Loss: 0.00002592
Iteration 309/1000 | Loss: 0.00002592
Iteration 310/1000 | Loss: 0.00002592
Iteration 311/1000 | Loss: 0.00002592
Iteration 312/1000 | Loss: 0.00002592
Iteration 313/1000 | Loss: 0.00002592
Iteration 314/1000 | Loss: 0.00002592
Iteration 315/1000 | Loss: 0.00002592
Iteration 316/1000 | Loss: 0.00002592
Iteration 317/1000 | Loss: 0.00002592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.5922896384145133e-05, 2.5922896384145133e-05, 2.5922896384145133e-05, 2.5922896384145133e-05, 2.5922896384145133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5922896384145133e-05

Optimization complete. Final v2v error: 4.107937812805176 mm

Highest mean error: 6.116192817687988 mm for frame 32

Lowest mean error: 3.2598774433135986 mm for frame 133

Saving results

Total time: 265.57946252822876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914128
Iteration 2/25 | Loss: 0.00153226
Iteration 3/25 | Loss: 0.00111915
Iteration 4/25 | Loss: 0.00103537
Iteration 5/25 | Loss: 0.00101991
Iteration 6/25 | Loss: 0.00102545
Iteration 7/25 | Loss: 0.00101116
Iteration 8/25 | Loss: 0.00102924
Iteration 9/25 | Loss: 0.00103179
Iteration 10/25 | Loss: 0.00098460
Iteration 11/25 | Loss: 0.00096206
Iteration 12/25 | Loss: 0.00095538
Iteration 13/25 | Loss: 0.00095461
Iteration 14/25 | Loss: 0.00095443
Iteration 15/25 | Loss: 0.00095439
Iteration 16/25 | Loss: 0.00095438
Iteration 17/25 | Loss: 0.00095438
Iteration 18/25 | Loss: 0.00095437
Iteration 19/25 | Loss: 0.00095437
Iteration 20/25 | Loss: 0.00095437
Iteration 21/25 | Loss: 0.00095437
Iteration 22/25 | Loss: 0.00095437
Iteration 23/25 | Loss: 0.00095437
Iteration 24/25 | Loss: 0.00095437
Iteration 25/25 | Loss: 0.00095437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09012842
Iteration 2/25 | Loss: 0.00108903
Iteration 3/25 | Loss: 0.00108902
Iteration 4/25 | Loss: 0.00108902
Iteration 5/25 | Loss: 0.00108902
Iteration 6/25 | Loss: 0.00108902
Iteration 7/25 | Loss: 0.00108902
Iteration 8/25 | Loss: 0.00108902
Iteration 9/25 | Loss: 0.00108902
Iteration 10/25 | Loss: 0.00108902
Iteration 11/25 | Loss: 0.00108902
Iteration 12/25 | Loss: 0.00108902
Iteration 13/25 | Loss: 0.00108902
Iteration 14/25 | Loss: 0.00108902
Iteration 15/25 | Loss: 0.00108902
Iteration 16/25 | Loss: 0.00108902
Iteration 17/25 | Loss: 0.00108902
Iteration 18/25 | Loss: 0.00108902
Iteration 19/25 | Loss: 0.00108902
Iteration 20/25 | Loss: 0.00108902
Iteration 21/25 | Loss: 0.00108902
Iteration 22/25 | Loss: 0.00108902
Iteration 23/25 | Loss: 0.00108902
Iteration 24/25 | Loss: 0.00108902
Iteration 25/25 | Loss: 0.00108902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108902
Iteration 2/1000 | Loss: 0.00005323
Iteration 3/1000 | Loss: 0.00004164
Iteration 4/1000 | Loss: 0.00003811
Iteration 5/1000 | Loss: 0.00003622
Iteration 6/1000 | Loss: 0.00003486
Iteration 7/1000 | Loss: 0.00003396
Iteration 8/1000 | Loss: 0.00003341
Iteration 9/1000 | Loss: 0.00003315
Iteration 10/1000 | Loss: 0.00003289
Iteration 11/1000 | Loss: 0.00003289
Iteration 12/1000 | Loss: 0.00003276
Iteration 13/1000 | Loss: 0.00003273
Iteration 14/1000 | Loss: 0.00003266
Iteration 15/1000 | Loss: 0.00003260
Iteration 16/1000 | Loss: 0.00003259
Iteration 17/1000 | Loss: 0.00003251
Iteration 18/1000 | Loss: 0.00003250
Iteration 19/1000 | Loss: 0.00003250
Iteration 20/1000 | Loss: 0.00003250
Iteration 21/1000 | Loss: 0.00003249
Iteration 22/1000 | Loss: 0.00003249
Iteration 23/1000 | Loss: 0.00003249
Iteration 24/1000 | Loss: 0.00003249
Iteration 25/1000 | Loss: 0.00003249
Iteration 26/1000 | Loss: 0.00003248
Iteration 27/1000 | Loss: 0.00003248
Iteration 28/1000 | Loss: 0.00003248
Iteration 29/1000 | Loss: 0.00003248
Iteration 30/1000 | Loss: 0.00003248
Iteration 31/1000 | Loss: 0.00003248
Iteration 32/1000 | Loss: 0.00003248
Iteration 33/1000 | Loss: 0.00003247
Iteration 34/1000 | Loss: 0.00003247
Iteration 35/1000 | Loss: 0.00003247
Iteration 36/1000 | Loss: 0.00003247
Iteration 37/1000 | Loss: 0.00003247
Iteration 38/1000 | Loss: 0.00003246
Iteration 39/1000 | Loss: 0.00003246
Iteration 40/1000 | Loss: 0.00003246
Iteration 41/1000 | Loss: 0.00003246
Iteration 42/1000 | Loss: 0.00003246
Iteration 43/1000 | Loss: 0.00003246
Iteration 44/1000 | Loss: 0.00003246
Iteration 45/1000 | Loss: 0.00003245
Iteration 46/1000 | Loss: 0.00003245
Iteration 47/1000 | Loss: 0.00003245
Iteration 48/1000 | Loss: 0.00003245
Iteration 49/1000 | Loss: 0.00003245
Iteration 50/1000 | Loss: 0.00003245
Iteration 51/1000 | Loss: 0.00003244
Iteration 52/1000 | Loss: 0.00003244
Iteration 53/1000 | Loss: 0.00003243
Iteration 54/1000 | Loss: 0.00003243
Iteration 55/1000 | Loss: 0.00003243
Iteration 56/1000 | Loss: 0.00003242
Iteration 57/1000 | Loss: 0.00003242
Iteration 58/1000 | Loss: 0.00003242
Iteration 59/1000 | Loss: 0.00003242
Iteration 60/1000 | Loss: 0.00003242
Iteration 61/1000 | Loss: 0.00003242
Iteration 62/1000 | Loss: 0.00003242
Iteration 63/1000 | Loss: 0.00003242
Iteration 64/1000 | Loss: 0.00003242
Iteration 65/1000 | Loss: 0.00003242
Iteration 66/1000 | Loss: 0.00003242
Iteration 67/1000 | Loss: 0.00003242
Iteration 68/1000 | Loss: 0.00003242
Iteration 69/1000 | Loss: 0.00003241
Iteration 70/1000 | Loss: 0.00003241
Iteration 71/1000 | Loss: 0.00003241
Iteration 72/1000 | Loss: 0.00003241
Iteration 73/1000 | Loss: 0.00003241
Iteration 74/1000 | Loss: 0.00003241
Iteration 75/1000 | Loss: 0.00003241
Iteration 76/1000 | Loss: 0.00003241
Iteration 77/1000 | Loss: 0.00003241
Iteration 78/1000 | Loss: 0.00003241
Iteration 79/1000 | Loss: 0.00003241
Iteration 80/1000 | Loss: 0.00003241
Iteration 81/1000 | Loss: 0.00003241
Iteration 82/1000 | Loss: 0.00003240
Iteration 83/1000 | Loss: 0.00003240
Iteration 84/1000 | Loss: 0.00003240
Iteration 85/1000 | Loss: 0.00003240
Iteration 86/1000 | Loss: 0.00003240
Iteration 87/1000 | Loss: 0.00003240
Iteration 88/1000 | Loss: 0.00003240
Iteration 89/1000 | Loss: 0.00003240
Iteration 90/1000 | Loss: 0.00003240
Iteration 91/1000 | Loss: 0.00003240
Iteration 92/1000 | Loss: 0.00003240
Iteration 93/1000 | Loss: 0.00003240
Iteration 94/1000 | Loss: 0.00003240
Iteration 95/1000 | Loss: 0.00003240
Iteration 96/1000 | Loss: 0.00003240
Iteration 97/1000 | Loss: 0.00003239
Iteration 98/1000 | Loss: 0.00003239
Iteration 99/1000 | Loss: 0.00003239
Iteration 100/1000 | Loss: 0.00003239
Iteration 101/1000 | Loss: 0.00003239
Iteration 102/1000 | Loss: 0.00003238
Iteration 103/1000 | Loss: 0.00003238
Iteration 104/1000 | Loss: 0.00003238
Iteration 105/1000 | Loss: 0.00003238
Iteration 106/1000 | Loss: 0.00003238
Iteration 107/1000 | Loss: 0.00003238
Iteration 108/1000 | Loss: 0.00003237
Iteration 109/1000 | Loss: 0.00003237
Iteration 110/1000 | Loss: 0.00003237
Iteration 111/1000 | Loss: 0.00003236
Iteration 112/1000 | Loss: 0.00003236
Iteration 113/1000 | Loss: 0.00003236
Iteration 114/1000 | Loss: 0.00003236
Iteration 115/1000 | Loss: 0.00003236
Iteration 116/1000 | Loss: 0.00003236
Iteration 117/1000 | Loss: 0.00003236
Iteration 118/1000 | Loss: 0.00003236
Iteration 119/1000 | Loss: 0.00003236
Iteration 120/1000 | Loss: 0.00003236
Iteration 121/1000 | Loss: 0.00003236
Iteration 122/1000 | Loss: 0.00003236
Iteration 123/1000 | Loss: 0.00003236
Iteration 124/1000 | Loss: 0.00003235
Iteration 125/1000 | Loss: 0.00003235
Iteration 126/1000 | Loss: 0.00003235
Iteration 127/1000 | Loss: 0.00003235
Iteration 128/1000 | Loss: 0.00003235
Iteration 129/1000 | Loss: 0.00003235
Iteration 130/1000 | Loss: 0.00003235
Iteration 131/1000 | Loss: 0.00003235
Iteration 132/1000 | Loss: 0.00003235
Iteration 133/1000 | Loss: 0.00003235
Iteration 134/1000 | Loss: 0.00003235
Iteration 135/1000 | Loss: 0.00003235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [3.234781252103858e-05, 3.234781252103858e-05, 3.234781252103858e-05, 3.234781252103858e-05, 3.234781252103858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.234781252103858e-05

Optimization complete. Final v2v error: 4.745206356048584 mm

Highest mean error: 4.9806132316589355 mm for frame 150

Lowest mean error: 4.603025436401367 mm for frame 39

Saving results

Total time: 51.47696828842163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077454
Iteration 2/25 | Loss: 0.00258484
Iteration 3/25 | Loss: 0.00157663
Iteration 4/25 | Loss: 0.00143924
Iteration 5/25 | Loss: 0.00148127
Iteration 6/25 | Loss: 0.00136620
Iteration 7/25 | Loss: 0.00136088
Iteration 8/25 | Loss: 0.00122863
Iteration 9/25 | Loss: 0.00118265
Iteration 10/25 | Loss: 0.00105598
Iteration 11/25 | Loss: 0.00104930
Iteration 12/25 | Loss: 0.00090505
Iteration 13/25 | Loss: 0.00089843
Iteration 14/25 | Loss: 0.00087831
Iteration 15/25 | Loss: 0.00086473
Iteration 16/25 | Loss: 0.00087064
Iteration 17/25 | Loss: 0.00086042
Iteration 18/25 | Loss: 0.00085984
Iteration 19/25 | Loss: 0.00090577
Iteration 20/25 | Loss: 0.00089818
Iteration 21/25 | Loss: 0.00090029
Iteration 22/25 | Loss: 0.00088473
Iteration 23/25 | Loss: 0.00088431
Iteration 24/25 | Loss: 0.00090080
Iteration 25/25 | Loss: 0.00088920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67823732
Iteration 2/25 | Loss: 0.00168035
Iteration 3/25 | Loss: 0.00167048
Iteration 4/25 | Loss: 0.00167048
Iteration 5/25 | Loss: 0.00167048
Iteration 6/25 | Loss: 0.00167048
Iteration 7/25 | Loss: 0.00167048
Iteration 8/25 | Loss: 0.00167048
Iteration 9/25 | Loss: 0.00167048
Iteration 10/25 | Loss: 0.00167048
Iteration 11/25 | Loss: 0.00167048
Iteration 12/25 | Loss: 0.00167048
Iteration 13/25 | Loss: 0.00167048
Iteration 14/25 | Loss: 0.00167048
Iteration 15/25 | Loss: 0.00167048
Iteration 16/25 | Loss: 0.00167048
Iteration 17/25 | Loss: 0.00167048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016704760491847992, 0.0016704760491847992, 0.0016704760491847992, 0.0016704760491847992, 0.0016704760491847992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016704760491847992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167048
Iteration 2/1000 | Loss: 0.00018090
Iteration 3/1000 | Loss: 0.00017455
Iteration 4/1000 | Loss: 0.00010941
Iteration 5/1000 | Loss: 0.00093192
Iteration 6/1000 | Loss: 0.00067531
Iteration 7/1000 | Loss: 0.00018492
Iteration 8/1000 | Loss: 0.00008635
Iteration 9/1000 | Loss: 0.00080319
Iteration 10/1000 | Loss: 0.00080220
Iteration 11/1000 | Loss: 0.00019855
Iteration 12/1000 | Loss: 0.00024030
Iteration 13/1000 | Loss: 0.00028557
Iteration 14/1000 | Loss: 0.00026666
Iteration 15/1000 | Loss: 0.00013467
Iteration 16/1000 | Loss: 0.00023716
Iteration 17/1000 | Loss: 0.00051658
Iteration 18/1000 | Loss: 0.00021955
Iteration 19/1000 | Loss: 0.00038751
Iteration 20/1000 | Loss: 0.00015912
Iteration 21/1000 | Loss: 0.00022845
Iteration 22/1000 | Loss: 0.00025259
Iteration 23/1000 | Loss: 0.00021681
Iteration 24/1000 | Loss: 0.00019983
Iteration 25/1000 | Loss: 0.00019178
Iteration 26/1000 | Loss: 0.00024118
Iteration 27/1000 | Loss: 0.00029228
Iteration 28/1000 | Loss: 0.00026844
Iteration 29/1000 | Loss: 0.00025643
Iteration 30/1000 | Loss: 0.00083717
Iteration 31/1000 | Loss: 0.00022324
Iteration 32/1000 | Loss: 0.00018895
Iteration 33/1000 | Loss: 0.00032290
Iteration 34/1000 | Loss: 0.00020511
Iteration 35/1000 | Loss: 0.00017187
Iteration 36/1000 | Loss: 0.00046263
Iteration 37/1000 | Loss: 0.00033475
Iteration 38/1000 | Loss: 0.00038876
Iteration 39/1000 | Loss: 0.00039556
Iteration 40/1000 | Loss: 0.00024764
Iteration 41/1000 | Loss: 0.00016368
Iteration 42/1000 | Loss: 0.00019596
Iteration 43/1000 | Loss: 0.00024747
Iteration 44/1000 | Loss: 0.00019110
Iteration 45/1000 | Loss: 0.00020655
Iteration 46/1000 | Loss: 0.00017234
Iteration 47/1000 | Loss: 0.00023232
Iteration 48/1000 | Loss: 0.00025210
Iteration 49/1000 | Loss: 0.00029960
Iteration 50/1000 | Loss: 0.00025938
Iteration 51/1000 | Loss: 0.00028006
Iteration 52/1000 | Loss: 0.00044965
Iteration 53/1000 | Loss: 0.00030385
Iteration 54/1000 | Loss: 0.00023324
Iteration 55/1000 | Loss: 0.00011488
Iteration 56/1000 | Loss: 0.00061581
Iteration 57/1000 | Loss: 0.00020577
Iteration 58/1000 | Loss: 0.00031992
Iteration 59/1000 | Loss: 0.00028258
Iteration 60/1000 | Loss: 0.00053760
Iteration 61/1000 | Loss: 0.00045725
Iteration 62/1000 | Loss: 0.00028229
Iteration 63/1000 | Loss: 0.00025753
Iteration 64/1000 | Loss: 0.00034175
Iteration 65/1000 | Loss: 0.00030683
Iteration 66/1000 | Loss: 0.00030468
Iteration 67/1000 | Loss: 0.00030464
Iteration 68/1000 | Loss: 0.00032009
Iteration 69/1000 | Loss: 0.00029419
Iteration 70/1000 | Loss: 0.00031039
Iteration 71/1000 | Loss: 0.00032717
Iteration 72/1000 | Loss: 0.00007742
Iteration 73/1000 | Loss: 0.00046884
Iteration 74/1000 | Loss: 0.00051389
Iteration 75/1000 | Loss: 0.00023409
Iteration 76/1000 | Loss: 0.00024402
Iteration 77/1000 | Loss: 0.00018154
Iteration 78/1000 | Loss: 0.00016021
Iteration 79/1000 | Loss: 0.00014066
Iteration 80/1000 | Loss: 0.00025783
Iteration 81/1000 | Loss: 0.00014088
Iteration 82/1000 | Loss: 0.00227161
Iteration 83/1000 | Loss: 0.00325973
Iteration 84/1000 | Loss: 0.00124067
Iteration 85/1000 | Loss: 0.00006317
Iteration 86/1000 | Loss: 0.00005571
Iteration 87/1000 | Loss: 0.00006138
Iteration 88/1000 | Loss: 0.00008886
Iteration 89/1000 | Loss: 0.00009498
Iteration 90/1000 | Loss: 0.00009606
Iteration 91/1000 | Loss: 0.00013405
Iteration 92/1000 | Loss: 0.00009047
Iteration 93/1000 | Loss: 0.00230194
Iteration 94/1000 | Loss: 0.00173637
Iteration 95/1000 | Loss: 0.00139059
Iteration 96/1000 | Loss: 0.00110818
Iteration 97/1000 | Loss: 0.00248176
Iteration 98/1000 | Loss: 0.00322232
Iteration 99/1000 | Loss: 0.00275722
Iteration 100/1000 | Loss: 0.00055728
Iteration 101/1000 | Loss: 0.00059615
Iteration 102/1000 | Loss: 0.00059221
Iteration 103/1000 | Loss: 0.00039553
Iteration 104/1000 | Loss: 0.00032554
Iteration 105/1000 | Loss: 0.00025626
Iteration 106/1000 | Loss: 0.00024996
Iteration 107/1000 | Loss: 0.00026203
Iteration 108/1000 | Loss: 0.00025206
Iteration 109/1000 | Loss: 0.00024561
Iteration 110/1000 | Loss: 0.00040054
Iteration 111/1000 | Loss: 0.00041132
Iteration 112/1000 | Loss: 0.00011167
Iteration 113/1000 | Loss: 0.00045982
Iteration 114/1000 | Loss: 0.00299982
Iteration 115/1000 | Loss: 0.00288693
Iteration 116/1000 | Loss: 0.00269014
Iteration 117/1000 | Loss: 0.00024981
Iteration 118/1000 | Loss: 0.00295681
Iteration 119/1000 | Loss: 0.00131718
Iteration 120/1000 | Loss: 0.00254131
Iteration 121/1000 | Loss: 0.00064166
Iteration 122/1000 | Loss: 0.00043849
Iteration 123/1000 | Loss: 0.00233489
Iteration 124/1000 | Loss: 0.00046760
Iteration 125/1000 | Loss: 0.00025861
Iteration 126/1000 | Loss: 0.00011155
Iteration 127/1000 | Loss: 0.00010598
Iteration 128/1000 | Loss: 0.00010403
Iteration 129/1000 | Loss: 0.00012293
Iteration 130/1000 | Loss: 0.00010068
Iteration 131/1000 | Loss: 0.00010653
Iteration 132/1000 | Loss: 0.00009949
Iteration 133/1000 | Loss: 0.00011282
Iteration 134/1000 | Loss: 0.00140786
Iteration 135/1000 | Loss: 0.00151283
Iteration 136/1000 | Loss: 0.00133739
Iteration 137/1000 | Loss: 0.00224097
Iteration 138/1000 | Loss: 0.00184436
Iteration 139/1000 | Loss: 0.00140061
Iteration 140/1000 | Loss: 0.00086915
Iteration 141/1000 | Loss: 0.00266683
Iteration 142/1000 | Loss: 0.00334442
Iteration 143/1000 | Loss: 0.00053344
Iteration 144/1000 | Loss: 0.00221337
Iteration 145/1000 | Loss: 0.00012700
Iteration 146/1000 | Loss: 0.00263527
Iteration 147/1000 | Loss: 0.00174235
Iteration 148/1000 | Loss: 0.00145952
Iteration 149/1000 | Loss: 0.00131022
Iteration 150/1000 | Loss: 0.00118515
Iteration 151/1000 | Loss: 0.00165423
Iteration 152/1000 | Loss: 0.00248129
Iteration 153/1000 | Loss: 0.00237286
Iteration 154/1000 | Loss: 0.00193585
Iteration 155/1000 | Loss: 0.00130410
Iteration 156/1000 | Loss: 0.00008902
Iteration 157/1000 | Loss: 0.00005394
Iteration 158/1000 | Loss: 0.00291487
Iteration 159/1000 | Loss: 0.00461121
Iteration 160/1000 | Loss: 0.00040460
Iteration 161/1000 | Loss: 0.00212924
Iteration 162/1000 | Loss: 0.00021182
Iteration 163/1000 | Loss: 0.00005095
Iteration 164/1000 | Loss: 0.00165471
Iteration 165/1000 | Loss: 0.00108717
Iteration 166/1000 | Loss: 0.00211909
Iteration 167/1000 | Loss: 0.00172160
Iteration 168/1000 | Loss: 0.00038391
Iteration 169/1000 | Loss: 0.00190672
Iteration 170/1000 | Loss: 0.00262056
Iteration 171/1000 | Loss: 0.00126054
Iteration 172/1000 | Loss: 0.00169665
Iteration 173/1000 | Loss: 0.00235273
Iteration 174/1000 | Loss: 0.00234265
Iteration 175/1000 | Loss: 0.00109557
Iteration 176/1000 | Loss: 0.00218740
Iteration 177/1000 | Loss: 0.00179681
Iteration 178/1000 | Loss: 0.00193693
Iteration 179/1000 | Loss: 0.00013291
Iteration 180/1000 | Loss: 0.00148069
Iteration 181/1000 | Loss: 0.00047795
Iteration 182/1000 | Loss: 0.00014909
Iteration 183/1000 | Loss: 0.00300717
Iteration 184/1000 | Loss: 0.00251375
Iteration 185/1000 | Loss: 0.00097243
Iteration 186/1000 | Loss: 0.00008181
Iteration 187/1000 | Loss: 0.00005208
Iteration 188/1000 | Loss: 0.00009173
Iteration 189/1000 | Loss: 0.00006051
Iteration 190/1000 | Loss: 0.00005251
Iteration 191/1000 | Loss: 0.00011728
Iteration 192/1000 | Loss: 0.00010925
Iteration 193/1000 | Loss: 0.00003771
Iteration 194/1000 | Loss: 0.00004177
Iteration 195/1000 | Loss: 0.00005475
Iteration 196/1000 | Loss: 0.00006397
Iteration 197/1000 | Loss: 0.00005205
Iteration 198/1000 | Loss: 0.00004543
Iteration 199/1000 | Loss: 0.00005539
Iteration 200/1000 | Loss: 0.00004451
Iteration 201/1000 | Loss: 0.00006073
Iteration 202/1000 | Loss: 0.00004235
Iteration 203/1000 | Loss: 0.00004084
Iteration 204/1000 | Loss: 0.00004525
Iteration 205/1000 | Loss: 0.00005115
Iteration 206/1000 | Loss: 0.00008693
Iteration 207/1000 | Loss: 0.00005093
Iteration 208/1000 | Loss: 0.00005262
Iteration 209/1000 | Loss: 0.00004071
Iteration 210/1000 | Loss: 0.00004399
Iteration 211/1000 | Loss: 0.00004435
Iteration 212/1000 | Loss: 0.00004198
Iteration 213/1000 | Loss: 0.00004686
Iteration 214/1000 | Loss: 0.00005595
Iteration 215/1000 | Loss: 0.00004198
Iteration 216/1000 | Loss: 0.00004567
Iteration 217/1000 | Loss: 0.00004542
Iteration 218/1000 | Loss: 0.00004155
Iteration 219/1000 | Loss: 0.00004220
Iteration 220/1000 | Loss: 0.00006580
Iteration 221/1000 | Loss: 0.00004634
Iteration 222/1000 | Loss: 0.00004114
Iteration 223/1000 | Loss: 0.00004546
Iteration 224/1000 | Loss: 0.00004238
Iteration 225/1000 | Loss: 0.00004960
Iteration 226/1000 | Loss: 0.00004671
Iteration 227/1000 | Loss: 0.00003844
Iteration 228/1000 | Loss: 0.00004880
Iteration 229/1000 | Loss: 0.00004826
Iteration 230/1000 | Loss: 0.00004640
Iteration 231/1000 | Loss: 0.00004629
Iteration 232/1000 | Loss: 0.00004563
Iteration 233/1000 | Loss: 0.00004563
Iteration 234/1000 | Loss: 0.00004684
Iteration 235/1000 | Loss: 0.00004347
Iteration 236/1000 | Loss: 0.00004404
Iteration 237/1000 | Loss: 0.00007712
Iteration 238/1000 | Loss: 0.00004436
Iteration 239/1000 | Loss: 0.00004393
Iteration 240/1000 | Loss: 0.00002809
Iteration 241/1000 | Loss: 0.00004919
Iteration 242/1000 | Loss: 0.00004567
Iteration 243/1000 | Loss: 0.00007803
Iteration 244/1000 | Loss: 0.00004782
Iteration 245/1000 | Loss: 0.00004463
Iteration 246/1000 | Loss: 0.00004517
Iteration 247/1000 | Loss: 0.00004741
Iteration 248/1000 | Loss: 0.00004254
Iteration 249/1000 | Loss: 0.00004618
Iteration 250/1000 | Loss: 0.00004732
Iteration 251/1000 | Loss: 0.00004504
Iteration 252/1000 | Loss: 0.00004744
Iteration 253/1000 | Loss: 0.00005452
Iteration 254/1000 | Loss: 0.00006174
Iteration 255/1000 | Loss: 0.00004992
Iteration 256/1000 | Loss: 0.00004776
Iteration 257/1000 | Loss: 0.00004909
Iteration 258/1000 | Loss: 0.00003929
Iteration 259/1000 | Loss: 0.00005220
Iteration 260/1000 | Loss: 0.00004675
Iteration 261/1000 | Loss: 0.00004585
Iteration 262/1000 | Loss: 0.00004367
Iteration 263/1000 | Loss: 0.00006189
Iteration 264/1000 | Loss: 0.00004839
Iteration 265/1000 | Loss: 0.00004497
Iteration 266/1000 | Loss: 0.00003922
Iteration 267/1000 | Loss: 0.00005701
Iteration 268/1000 | Loss: 0.00004128
Iteration 269/1000 | Loss: 0.00004943
Iteration 270/1000 | Loss: 0.00004542
Iteration 271/1000 | Loss: 0.00004732
Iteration 272/1000 | Loss: 0.00004839
Iteration 273/1000 | Loss: 0.00005148
Iteration 274/1000 | Loss: 0.00004671
Iteration 275/1000 | Loss: 0.00006120
Iteration 276/1000 | Loss: 0.00004554
Iteration 277/1000 | Loss: 0.00005778
Iteration 278/1000 | Loss: 0.00004505
Iteration 279/1000 | Loss: 0.00007246
Iteration 280/1000 | Loss: 0.00003898
Iteration 281/1000 | Loss: 0.00004573
Iteration 282/1000 | Loss: 0.00004559
Iteration 283/1000 | Loss: 0.00004416
Iteration 284/1000 | Loss: 0.00005141
Iteration 285/1000 | Loss: 0.00004710
Iteration 286/1000 | Loss: 0.00004963
Iteration 287/1000 | Loss: 0.00004520
Iteration 288/1000 | Loss: 0.00004456
Iteration 289/1000 | Loss: 0.00004427
Iteration 290/1000 | Loss: 0.00004449
Iteration 291/1000 | Loss: 0.00004962
Iteration 292/1000 | Loss: 0.00004447
Iteration 293/1000 | Loss: 0.00005788
Iteration 294/1000 | Loss: 0.00004453
Iteration 295/1000 | Loss: 0.00003333
Iteration 296/1000 | Loss: 0.00008670
Iteration 297/1000 | Loss: 0.00004430
Iteration 298/1000 | Loss: 0.00004620
Iteration 299/1000 | Loss: 0.00004963
Iteration 300/1000 | Loss: 0.00004608
Iteration 301/1000 | Loss: 0.00004993
Iteration 302/1000 | Loss: 0.00004537
Iteration 303/1000 | Loss: 0.00005460
Iteration 304/1000 | Loss: 0.00004938
Iteration 305/1000 | Loss: 0.00004674
Iteration 306/1000 | Loss: 0.00004681
Iteration 307/1000 | Loss: 0.00004898
Iteration 308/1000 | Loss: 0.00004453
Iteration 309/1000 | Loss: 0.00005452
Iteration 310/1000 | Loss: 0.00003864
Iteration 311/1000 | Loss: 0.00004184
Iteration 312/1000 | Loss: 0.00006777
Iteration 313/1000 | Loss: 0.00004550
Iteration 314/1000 | Loss: 0.00004423
Iteration 315/1000 | Loss: 0.00004879
Iteration 316/1000 | Loss: 0.00004310
Iteration 317/1000 | Loss: 0.00004999
Iteration 318/1000 | Loss: 0.00004555
Iteration 319/1000 | Loss: 0.00004768
Iteration 320/1000 | Loss: 0.00004740
Iteration 321/1000 | Loss: 0.00004739
Iteration 322/1000 | Loss: 0.00004493
Iteration 323/1000 | Loss: 0.00004942
Iteration 324/1000 | Loss: 0.00007926
Iteration 325/1000 | Loss: 0.00004383
Iteration 326/1000 | Loss: 0.00004499
Iteration 327/1000 | Loss: 0.00005043
Iteration 328/1000 | Loss: 0.00004222
Iteration 329/1000 | Loss: 0.00004390
Iteration 330/1000 | Loss: 0.00004851
Iteration 331/1000 | Loss: 0.00004356
Iteration 332/1000 | Loss: 0.00005119
Iteration 333/1000 | Loss: 0.00004337
Iteration 334/1000 | Loss: 0.00004329
Iteration 335/1000 | Loss: 0.00004739
Iteration 336/1000 | Loss: 0.00004914
Iteration 337/1000 | Loss: 0.00004451
Iteration 338/1000 | Loss: 0.00004578
Iteration 339/1000 | Loss: 0.00004254
Iteration 340/1000 | Loss: 0.00004593
Iteration 341/1000 | Loss: 0.00004590
Iteration 342/1000 | Loss: 0.00004093
Iteration 343/1000 | Loss: 0.00005569
Iteration 344/1000 | Loss: 0.00003716
Iteration 345/1000 | Loss: 0.00004523
Iteration 346/1000 | Loss: 0.00004278
Iteration 347/1000 | Loss: 0.00005549
Iteration 348/1000 | Loss: 0.00004301
Iteration 349/1000 | Loss: 0.00002978
Iteration 350/1000 | Loss: 0.00004335
Iteration 351/1000 | Loss: 0.00004395
Iteration 352/1000 | Loss: 0.00004315
Iteration 353/1000 | Loss: 0.00004360
Iteration 354/1000 | Loss: 0.00004097
Iteration 355/1000 | Loss: 0.00004554
Iteration 356/1000 | Loss: 0.00004776
Iteration 357/1000 | Loss: 0.00004805
Iteration 358/1000 | Loss: 0.00005144
Iteration 359/1000 | Loss: 0.00004326
Iteration 360/1000 | Loss: 0.00004780
Iteration 361/1000 | Loss: 0.00004359
Iteration 362/1000 | Loss: 0.00004385
Iteration 363/1000 | Loss: 0.00005917
Iteration 364/1000 | Loss: 0.00005703
Iteration 365/1000 | Loss: 0.00004457
Iteration 366/1000 | Loss: 0.00004628
Iteration 367/1000 | Loss: 0.00004990
Iteration 368/1000 | Loss: 0.00004519
Iteration 369/1000 | Loss: 0.00003845
Iteration 370/1000 | Loss: 0.00004923
Iteration 371/1000 | Loss: 0.00004277
Iteration 372/1000 | Loss: 0.00005214
Iteration 373/1000 | Loss: 0.00003558
Iteration 374/1000 | Loss: 0.00004632
Iteration 375/1000 | Loss: 0.00003743
Iteration 376/1000 | Loss: 0.00006881
Iteration 377/1000 | Loss: 0.00004608
Iteration 378/1000 | Loss: 0.00006004
Iteration 379/1000 | Loss: 0.00007691
Iteration 380/1000 | Loss: 0.00005700
Iteration 381/1000 | Loss: 0.00003449
Iteration 382/1000 | Loss: 0.00005251
Iteration 383/1000 | Loss: 0.00004436
Iteration 384/1000 | Loss: 0.00004964
Iteration 385/1000 | Loss: 0.00004050
Iteration 386/1000 | Loss: 0.00004617
Iteration 387/1000 | Loss: 0.00004201
Iteration 388/1000 | Loss: 0.00004660
Iteration 389/1000 | Loss: 0.00005442
Iteration 390/1000 | Loss: 0.00004515
Iteration 391/1000 | Loss: 0.00009765
Iteration 392/1000 | Loss: 0.00004548
Iteration 393/1000 | Loss: 0.00004789
Iteration 394/1000 | Loss: 0.00005550
Iteration 395/1000 | Loss: 0.00004652
Iteration 396/1000 | Loss: 0.00004536
Iteration 397/1000 | Loss: 0.00003474
Iteration 398/1000 | Loss: 0.00004885
Iteration 399/1000 | Loss: 0.00005917
Iteration 400/1000 | Loss: 0.00005843
Iteration 401/1000 | Loss: 0.00004373
Iteration 402/1000 | Loss: 0.00004973
Iteration 403/1000 | Loss: 0.00004790
Iteration 404/1000 | Loss: 0.00004588
Iteration 405/1000 | Loss: 0.00004173
Iteration 406/1000 | Loss: 0.00004538
Iteration 407/1000 | Loss: 0.00005835
Iteration 408/1000 | Loss: 0.00004583
Iteration 409/1000 | Loss: 0.00005782
Iteration 410/1000 | Loss: 0.00004495
Iteration 411/1000 | Loss: 0.00005155
Iteration 412/1000 | Loss: 0.00005683
Iteration 413/1000 | Loss: 0.00005888
Iteration 414/1000 | Loss: 0.00004613
Iteration 415/1000 | Loss: 0.00004502
Iteration 416/1000 | Loss: 0.00007479
Iteration 417/1000 | Loss: 0.00003410
Iteration 418/1000 | Loss: 0.00003607
Iteration 419/1000 | Loss: 0.00004636
Iteration 420/1000 | Loss: 0.00008411
Iteration 421/1000 | Loss: 0.00004382
Iteration 422/1000 | Loss: 0.00006647
Iteration 423/1000 | Loss: 0.00004763
Iteration 424/1000 | Loss: 0.00008661
Iteration 425/1000 | Loss: 0.00004875
Iteration 426/1000 | Loss: 0.00004968
Iteration 427/1000 | Loss: 0.00004193
Iteration 428/1000 | Loss: 0.00004740
Iteration 429/1000 | Loss: 0.00004183
Iteration 430/1000 | Loss: 0.00006470
Iteration 431/1000 | Loss: 0.00004310
Iteration 432/1000 | Loss: 0.00004644
Iteration 433/1000 | Loss: 0.00004419
Iteration 434/1000 | Loss: 0.00004766
Iteration 435/1000 | Loss: 0.00004433
Iteration 436/1000 | Loss: 0.00005710
Iteration 437/1000 | Loss: 0.00005335
Iteration 438/1000 | Loss: 0.00010344
Iteration 439/1000 | Loss: 0.00004114
Iteration 440/1000 | Loss: 0.00004648
Iteration 441/1000 | Loss: 0.00005749
Iteration 442/1000 | Loss: 0.00004488
Iteration 443/1000 | Loss: 0.00005055
Iteration 444/1000 | Loss: 0.00004632
Iteration 445/1000 | Loss: 0.00005002
Iteration 446/1000 | Loss: 0.00004450
Iteration 447/1000 | Loss: 0.00007025
Iteration 448/1000 | Loss: 0.00005318
Iteration 449/1000 | Loss: 0.00004518
Iteration 450/1000 | Loss: 0.00007831
Iteration 451/1000 | Loss: 0.00004322
Iteration 452/1000 | Loss: 0.00004573
Iteration 453/1000 | Loss: 0.00004427
Iteration 454/1000 | Loss: 0.00004713
Iteration 455/1000 | Loss: 0.00005200
Iteration 456/1000 | Loss: 0.00004463
Iteration 457/1000 | Loss: 0.00004271
Iteration 458/1000 | Loss: 0.00005177
Iteration 459/1000 | Loss: 0.00005220
Iteration 460/1000 | Loss: 0.00004756
Iteration 461/1000 | Loss: 0.00004721
Iteration 462/1000 | Loss: 0.00010192
Iteration 463/1000 | Loss: 0.00004897
Iteration 464/1000 | Loss: 0.00005879
Iteration 465/1000 | Loss: 0.00005849
Iteration 466/1000 | Loss: 0.00007072
Iteration 467/1000 | Loss: 0.00005037
Iteration 468/1000 | Loss: 0.00004518
Iteration 469/1000 | Loss: 0.00004889
Iteration 470/1000 | Loss: 0.00004711
Iteration 471/1000 | Loss: 0.00004299
Iteration 472/1000 | Loss: 0.00005169
Iteration 473/1000 | Loss: 0.00004928
Iteration 474/1000 | Loss: 0.00004757
Iteration 475/1000 | Loss: 0.00004577
Iteration 476/1000 | Loss: 0.00004738
Iteration 477/1000 | Loss: 0.00004496
Iteration 478/1000 | Loss: 0.00006547
Iteration 479/1000 | Loss: 0.00004880
Iteration 480/1000 | Loss: 0.00004893
Iteration 481/1000 | Loss: 0.00004413
Iteration 482/1000 | Loss: 0.00004457
Iteration 483/1000 | Loss: 0.00004286
Iteration 484/1000 | Loss: 0.00006734
Iteration 485/1000 | Loss: 0.00004374
Iteration 486/1000 | Loss: 0.00005496
Iteration 487/1000 | Loss: 0.00004357
Iteration 488/1000 | Loss: 0.00004879
Iteration 489/1000 | Loss: 0.00004522
Iteration 490/1000 | Loss: 0.00004980
Iteration 491/1000 | Loss: 0.00004921
Iteration 492/1000 | Loss: 0.00004737
Iteration 493/1000 | Loss: 0.00006194
Iteration 494/1000 | Loss: 0.00006441
Iteration 495/1000 | Loss: 0.00005119
Iteration 496/1000 | Loss: 0.00007319
Iteration 497/1000 | Loss: 0.00004808
Iteration 498/1000 | Loss: 0.00004419
Iteration 499/1000 | Loss: 0.00004848
Iteration 500/1000 | Loss: 0.00004731
Iteration 501/1000 | Loss: 0.00004248
Iteration 502/1000 | Loss: 0.00005057
Iteration 503/1000 | Loss: 0.00004276
Iteration 504/1000 | Loss: 0.00005129
Iteration 505/1000 | Loss: 0.00007672
Iteration 506/1000 | Loss: 0.00004837
Iteration 507/1000 | Loss: 0.00004062
Iteration 508/1000 | Loss: 0.00004965
Iteration 509/1000 | Loss: 0.00004579
Iteration 510/1000 | Loss: 0.00005195
Iteration 511/1000 | Loss: 0.00004823
Iteration 512/1000 | Loss: 0.00005430
Iteration 513/1000 | Loss: 0.00006700
Iteration 514/1000 | Loss: 0.00004501
Iteration 515/1000 | Loss: 0.00005402
Iteration 516/1000 | Loss: 0.00004594
Iteration 517/1000 | Loss: 0.00006431
Iteration 518/1000 | Loss: 0.00004469
Iteration 519/1000 | Loss: 0.00004274
Iteration 520/1000 | Loss: 0.00004129
Iteration 521/1000 | Loss: 0.00005379
Iteration 522/1000 | Loss: 0.00006440
Iteration 523/1000 | Loss: 0.00006966
Iteration 524/1000 | Loss: 0.00004961
Iteration 525/1000 | Loss: 0.00005000
Iteration 526/1000 | Loss: 0.00004716
Iteration 527/1000 | Loss: 0.00004415
Iteration 528/1000 | Loss: 0.00004806
Iteration 529/1000 | Loss: 0.00004459
Iteration 530/1000 | Loss: 0.00004526
Iteration 531/1000 | Loss: 0.00006061
Iteration 532/1000 | Loss: 0.00005736
Iteration 533/1000 | Loss: 0.00002540
Iteration 534/1000 | Loss: 0.00002331
Iteration 535/1000 | Loss: 0.00003926
Iteration 536/1000 | Loss: 0.00002341
Iteration 537/1000 | Loss: 0.00002459
Iteration 538/1000 | Loss: 0.00002598
Iteration 539/1000 | Loss: 0.00002273
Iteration 540/1000 | Loss: 0.00002037
Iteration 541/1000 | Loss: 0.00005979
Iteration 542/1000 | Loss: 0.00002226
Iteration 543/1000 | Loss: 0.00001981
Iteration 544/1000 | Loss: 0.00001981
Iteration 545/1000 | Loss: 0.00005513
Iteration 546/1000 | Loss: 0.00001946
Iteration 547/1000 | Loss: 0.00001937
Iteration 548/1000 | Loss: 0.00001934
Iteration 549/1000 | Loss: 0.00001929
Iteration 550/1000 | Loss: 0.00001928
Iteration 551/1000 | Loss: 0.00004902
Iteration 552/1000 | Loss: 0.00001923
Iteration 553/1000 | Loss: 0.00002460
Iteration 554/1000 | Loss: 0.00002436
Iteration 555/1000 | Loss: 0.00002662
Iteration 556/1000 | Loss: 0.00001919
Iteration 557/1000 | Loss: 0.00001917
Iteration 558/1000 | Loss: 0.00001917
Iteration 559/1000 | Loss: 0.00001916
Iteration 560/1000 | Loss: 0.00001916
Iteration 561/1000 | Loss: 0.00001916
Iteration 562/1000 | Loss: 0.00001916
Iteration 563/1000 | Loss: 0.00001916
Iteration 564/1000 | Loss: 0.00001916
Iteration 565/1000 | Loss: 0.00001916
Iteration 566/1000 | Loss: 0.00001916
Iteration 567/1000 | Loss: 0.00001916
Iteration 568/1000 | Loss: 0.00001916
Iteration 569/1000 | Loss: 0.00004642
Iteration 570/1000 | Loss: 0.00002150
Iteration 571/1000 | Loss: 0.00001917
Iteration 572/1000 | Loss: 0.00001917
Iteration 573/1000 | Loss: 0.00001917
Iteration 574/1000 | Loss: 0.00001917
Iteration 575/1000 | Loss: 0.00001917
Iteration 576/1000 | Loss: 0.00001917
Iteration 577/1000 | Loss: 0.00001917
Iteration 578/1000 | Loss: 0.00001917
Iteration 579/1000 | Loss: 0.00004329
Iteration 580/1000 | Loss: 0.00001935
Iteration 581/1000 | Loss: 0.00002545
Iteration 582/1000 | Loss: 0.00001914
Iteration 583/1000 | Loss: 0.00001913
Iteration 584/1000 | Loss: 0.00001913
Iteration 585/1000 | Loss: 0.00001913
Iteration 586/1000 | Loss: 0.00002325
Iteration 587/1000 | Loss: 0.00001913
Iteration 588/1000 | Loss: 0.00001913
Iteration 589/1000 | Loss: 0.00001913
Iteration 590/1000 | Loss: 0.00001912
Iteration 591/1000 | Loss: 0.00001912
Iteration 592/1000 | Loss: 0.00001912
Iteration 593/1000 | Loss: 0.00001912
Iteration 594/1000 | Loss: 0.00001912
Iteration 595/1000 | Loss: 0.00001912
Iteration 596/1000 | Loss: 0.00001912
Iteration 597/1000 | Loss: 0.00001912
Iteration 598/1000 | Loss: 0.00001912
Iteration 599/1000 | Loss: 0.00001912
Iteration 600/1000 | Loss: 0.00001912
Iteration 601/1000 | Loss: 0.00001912
Iteration 602/1000 | Loss: 0.00001911
Iteration 603/1000 | Loss: 0.00001911
Iteration 604/1000 | Loss: 0.00001911
Iteration 605/1000 | Loss: 0.00001911
Iteration 606/1000 | Loss: 0.00001911
Iteration 607/1000 | Loss: 0.00001911
Iteration 608/1000 | Loss: 0.00001911
Iteration 609/1000 | Loss: 0.00001911
Iteration 610/1000 | Loss: 0.00001911
Iteration 611/1000 | Loss: 0.00001911
Iteration 612/1000 | Loss: 0.00001911
Iteration 613/1000 | Loss: 0.00001911
Iteration 614/1000 | Loss: 0.00001911
Iteration 615/1000 | Loss: 0.00001911
Iteration 616/1000 | Loss: 0.00001911
Iteration 617/1000 | Loss: 0.00001911
Iteration 618/1000 | Loss: 0.00001911
Iteration 619/1000 | Loss: 0.00001911
Iteration 620/1000 | Loss: 0.00001911
Iteration 621/1000 | Loss: 0.00001911
Iteration 622/1000 | Loss: 0.00001911
Iteration 623/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 623. Stopping optimization.
Last 5 losses: [1.910541504912544e-05, 1.910541504912544e-05, 1.910541504912544e-05, 1.910541504912544e-05, 1.910541504912544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.910541504912544e-05

Optimization complete. Final v2v error: 3.6494948863983154 mm

Highest mean error: 5.022406101226807 mm for frame 66

Lowest mean error: 3.013075351715088 mm for frame 105

Saving results

Total time: 820.7647416591644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00207873
Iteration 2/25 | Loss: 0.00100182
Iteration 3/25 | Loss: 0.00086041
Iteration 4/25 | Loss: 0.00080519
Iteration 5/25 | Loss: 0.00079647
Iteration 6/25 | Loss: 0.00079364
Iteration 7/25 | Loss: 0.00079261
Iteration 8/25 | Loss: 0.00079257
Iteration 9/25 | Loss: 0.00079257
Iteration 10/25 | Loss: 0.00079257
Iteration 11/25 | Loss: 0.00079257
Iteration 12/25 | Loss: 0.00079257
Iteration 13/25 | Loss: 0.00079257
Iteration 14/25 | Loss: 0.00079257
Iteration 15/25 | Loss: 0.00079257
Iteration 16/25 | Loss: 0.00079257
Iteration 17/25 | Loss: 0.00079257
Iteration 18/25 | Loss: 0.00079257
Iteration 19/25 | Loss: 0.00079257
Iteration 20/25 | Loss: 0.00079257
Iteration 21/25 | Loss: 0.00079257
Iteration 22/25 | Loss: 0.00079257
Iteration 23/25 | Loss: 0.00079257
Iteration 24/25 | Loss: 0.00079257
Iteration 25/25 | Loss: 0.00079257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66637266
Iteration 2/25 | Loss: 0.00170249
Iteration 3/25 | Loss: 0.00170249
Iteration 4/25 | Loss: 0.00170249
Iteration 5/25 | Loss: 0.00170249
Iteration 6/25 | Loss: 0.00170249
Iteration 7/25 | Loss: 0.00170249
Iteration 8/25 | Loss: 0.00170249
Iteration 9/25 | Loss: 0.00170249
Iteration 10/25 | Loss: 0.00170249
Iteration 11/25 | Loss: 0.00170249
Iteration 12/25 | Loss: 0.00170249
Iteration 13/25 | Loss: 0.00170249
Iteration 14/25 | Loss: 0.00170249
Iteration 15/25 | Loss: 0.00170249
Iteration 16/25 | Loss: 0.00170249
Iteration 17/25 | Loss: 0.00170249
Iteration 18/25 | Loss: 0.00170249
Iteration 19/25 | Loss: 0.00170249
Iteration 20/25 | Loss: 0.00170249
Iteration 21/25 | Loss: 0.00170249
Iteration 22/25 | Loss: 0.00170249
Iteration 23/25 | Loss: 0.00170249
Iteration 24/25 | Loss: 0.00170249
Iteration 25/25 | Loss: 0.00170249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017024888657033443, 0.0017024888657033443, 0.0017024888657033443, 0.0017024888657033443, 0.0017024888657033443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017024888657033443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170249
Iteration 2/1000 | Loss: 0.00005615
Iteration 3/1000 | Loss: 0.00003055
Iteration 4/1000 | Loss: 0.00002462
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002142
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002029
Iteration 9/1000 | Loss: 0.00001987
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001908
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001888
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001884
Iteration 26/1000 | Loss: 0.00001883
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001880
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001880
Iteration 31/1000 | Loss: 0.00001879
Iteration 32/1000 | Loss: 0.00001879
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001878
Iteration 37/1000 | Loss: 0.00001878
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001878
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001874
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001872
Iteration 67/1000 | Loss: 0.00001872
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001872
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001871
Iteration 74/1000 | Loss: 0.00001871
Iteration 75/1000 | Loss: 0.00001871
Iteration 76/1000 | Loss: 0.00001871
Iteration 77/1000 | Loss: 0.00001870
Iteration 78/1000 | Loss: 0.00001870
Iteration 79/1000 | Loss: 0.00001870
Iteration 80/1000 | Loss: 0.00001870
Iteration 81/1000 | Loss: 0.00001870
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001869
Iteration 84/1000 | Loss: 0.00001869
Iteration 85/1000 | Loss: 0.00001869
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001866
Iteration 95/1000 | Loss: 0.00001866
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001866
Iteration 98/1000 | Loss: 0.00001866
Iteration 99/1000 | Loss: 0.00001866
Iteration 100/1000 | Loss: 0.00001865
Iteration 101/1000 | Loss: 0.00001865
Iteration 102/1000 | Loss: 0.00001865
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001865
Iteration 105/1000 | Loss: 0.00001865
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001864
Iteration 110/1000 | Loss: 0.00001864
Iteration 111/1000 | Loss: 0.00001864
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001864
Iteration 114/1000 | Loss: 0.00001864
Iteration 115/1000 | Loss: 0.00001864
Iteration 116/1000 | Loss: 0.00001864
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001864
Iteration 122/1000 | Loss: 0.00001864
Iteration 123/1000 | Loss: 0.00001864
Iteration 124/1000 | Loss: 0.00001864
Iteration 125/1000 | Loss: 0.00001864
Iteration 126/1000 | Loss: 0.00001864
Iteration 127/1000 | Loss: 0.00001864
Iteration 128/1000 | Loss: 0.00001864
Iteration 129/1000 | Loss: 0.00001864
Iteration 130/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.864315345301293e-05, 1.864315345301293e-05, 1.864315345301293e-05, 1.864315345301293e-05, 1.864315345301293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.864315345301293e-05

Optimization complete. Final v2v error: 3.616941213607788 mm

Highest mean error: 4.048766136169434 mm for frame 61

Lowest mean error: 3.261580228805542 mm for frame 34

Saving results

Total time: 39.104403018951416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868977
Iteration 2/25 | Loss: 0.00114319
Iteration 3/25 | Loss: 0.00089990
Iteration 4/25 | Loss: 0.00085454
Iteration 5/25 | Loss: 0.00083955
Iteration 6/25 | Loss: 0.00083648
Iteration 7/25 | Loss: 0.00083601
Iteration 8/25 | Loss: 0.00083601
Iteration 9/25 | Loss: 0.00083601
Iteration 10/25 | Loss: 0.00083601
Iteration 11/25 | Loss: 0.00083601
Iteration 12/25 | Loss: 0.00083601
Iteration 13/25 | Loss: 0.00083601
Iteration 14/25 | Loss: 0.00083601
Iteration 15/25 | Loss: 0.00083601
Iteration 16/25 | Loss: 0.00083601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008360084611922503, 0.0008360084611922503, 0.0008360084611922503, 0.0008360084611922503, 0.0008360084611922503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008360084611922503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56735623
Iteration 2/25 | Loss: 0.00130072
Iteration 3/25 | Loss: 0.00130066
Iteration 4/25 | Loss: 0.00130066
Iteration 5/25 | Loss: 0.00130066
Iteration 6/25 | Loss: 0.00130066
Iteration 7/25 | Loss: 0.00130066
Iteration 8/25 | Loss: 0.00130066
Iteration 9/25 | Loss: 0.00130066
Iteration 10/25 | Loss: 0.00130066
Iteration 11/25 | Loss: 0.00130066
Iteration 12/25 | Loss: 0.00130066
Iteration 13/25 | Loss: 0.00130066
Iteration 14/25 | Loss: 0.00130066
Iteration 15/25 | Loss: 0.00130066
Iteration 16/25 | Loss: 0.00130066
Iteration 17/25 | Loss: 0.00130066
Iteration 18/25 | Loss: 0.00130066
Iteration 19/25 | Loss: 0.00130066
Iteration 20/25 | Loss: 0.00130066
Iteration 21/25 | Loss: 0.00130066
Iteration 22/25 | Loss: 0.00130066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013006608933210373, 0.0013006608933210373, 0.0013006608933210373, 0.0013006608933210373, 0.0013006608933210373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013006608933210373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130066
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002589
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001908
Iteration 8/1000 | Loss: 0.00001861
Iteration 9/1000 | Loss: 0.00001828
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001755
Iteration 14/1000 | Loss: 0.00001742
Iteration 15/1000 | Loss: 0.00001741
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001739
Iteration 19/1000 | Loss: 0.00001739
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001735
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001730
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001728
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.7168767953990027e-05, 1.7168767953990027e-05, 1.7168767953990027e-05, 1.7168767953990027e-05, 1.7168767953990027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7168767953990027e-05

Optimization complete. Final v2v error: 3.415466547012329 mm

Highest mean error: 4.075860977172852 mm for frame 220

Lowest mean error: 2.99444580078125 mm for frame 237

Saving results

Total time: 40.93476748466492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068150
Iteration 2/25 | Loss: 0.00232423
Iteration 3/25 | Loss: 0.00161622
Iteration 4/25 | Loss: 0.00136781
Iteration 5/25 | Loss: 0.00123761
Iteration 6/25 | Loss: 0.00127251
Iteration 7/25 | Loss: 0.00116966
Iteration 8/25 | Loss: 0.00109185
Iteration 9/25 | Loss: 0.00101477
Iteration 10/25 | Loss: 0.00100803
Iteration 11/25 | Loss: 0.00101009
Iteration 12/25 | Loss: 0.00098947
Iteration 13/25 | Loss: 0.00095231
Iteration 14/25 | Loss: 0.00091600
Iteration 15/25 | Loss: 0.00089720
Iteration 16/25 | Loss: 0.00088952
Iteration 17/25 | Loss: 0.00088879
Iteration 18/25 | Loss: 0.00088776
Iteration 19/25 | Loss: 0.00088934
Iteration 20/25 | Loss: 0.00089028
Iteration 21/25 | Loss: 0.00088736
Iteration 22/25 | Loss: 0.00088468
Iteration 23/25 | Loss: 0.00088531
Iteration 24/25 | Loss: 0.00088292
Iteration 25/25 | Loss: 0.00088088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62106550
Iteration 2/25 | Loss: 0.00155471
Iteration 3/25 | Loss: 0.00155470
Iteration 4/25 | Loss: 0.00155470
Iteration 5/25 | Loss: 0.00155470
Iteration 6/25 | Loss: 0.00155470
Iteration 7/25 | Loss: 0.00155470
Iteration 8/25 | Loss: 0.00155470
Iteration 9/25 | Loss: 0.00155470
Iteration 10/25 | Loss: 0.00155470
Iteration 11/25 | Loss: 0.00155470
Iteration 12/25 | Loss: 0.00155470
Iteration 13/25 | Loss: 0.00155470
Iteration 14/25 | Loss: 0.00155470
Iteration 15/25 | Loss: 0.00155470
Iteration 16/25 | Loss: 0.00155470
Iteration 17/25 | Loss: 0.00155470
Iteration 18/25 | Loss: 0.00155470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015546997310593724, 0.0015546997310593724, 0.0015546997310593724, 0.0015546997310593724, 0.0015546997310593724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015546997310593724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155470
Iteration 2/1000 | Loss: 0.00067993
Iteration 3/1000 | Loss: 0.00025829
Iteration 4/1000 | Loss: 0.00008133
Iteration 5/1000 | Loss: 0.00013567
Iteration 6/1000 | Loss: 0.00019435
Iteration 7/1000 | Loss: 0.00016025
Iteration 8/1000 | Loss: 0.00015766
Iteration 9/1000 | Loss: 0.00070845
Iteration 10/1000 | Loss: 0.00014538
Iteration 11/1000 | Loss: 0.00038428
Iteration 12/1000 | Loss: 0.00024777
Iteration 13/1000 | Loss: 0.00022837
Iteration 14/1000 | Loss: 0.00026366
Iteration 15/1000 | Loss: 0.00006931
Iteration 16/1000 | Loss: 0.00014190
Iteration 17/1000 | Loss: 0.00011018
Iteration 18/1000 | Loss: 0.00004837
Iteration 19/1000 | Loss: 0.00013544
Iteration 20/1000 | Loss: 0.00025585
Iteration 21/1000 | Loss: 0.00031596
Iteration 22/1000 | Loss: 0.00047101
Iteration 23/1000 | Loss: 0.00043408
Iteration 24/1000 | Loss: 0.00022795
Iteration 25/1000 | Loss: 0.00004541
Iteration 26/1000 | Loss: 0.00003744
Iteration 27/1000 | Loss: 0.00003123
Iteration 28/1000 | Loss: 0.00002760
Iteration 29/1000 | Loss: 0.00002957
Iteration 30/1000 | Loss: 0.00002486
Iteration 31/1000 | Loss: 0.00002309
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002065
Iteration 34/1000 | Loss: 0.00016044
Iteration 35/1000 | Loss: 0.00004117
Iteration 36/1000 | Loss: 0.00002594
Iteration 37/1000 | Loss: 0.00002297
Iteration 38/1000 | Loss: 0.00002154
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00001989
Iteration 42/1000 | Loss: 0.00018078
Iteration 43/1000 | Loss: 0.00023406
Iteration 44/1000 | Loss: 0.00005428
Iteration 45/1000 | Loss: 0.00003909
Iteration 46/1000 | Loss: 0.00002119
Iteration 47/1000 | Loss: 0.00001996
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001809
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001769
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001764
Iteration 58/1000 | Loss: 0.00001764
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001749
Iteration 86/1000 | Loss: 0.00001749
Iteration 87/1000 | Loss: 0.00001749
Iteration 88/1000 | Loss: 0.00001749
Iteration 89/1000 | Loss: 0.00001749
Iteration 90/1000 | Loss: 0.00001749
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001748
Iteration 93/1000 | Loss: 0.00001748
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001747
Iteration 96/1000 | Loss: 0.00001747
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001746
Iteration 99/1000 | Loss: 0.00001746
Iteration 100/1000 | Loss: 0.00001746
Iteration 101/1000 | Loss: 0.00001746
Iteration 102/1000 | Loss: 0.00001746
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001746
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001745
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001745
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001745
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Iteration 134/1000 | Loss: 0.00001744
Iteration 135/1000 | Loss: 0.00001744
Iteration 136/1000 | Loss: 0.00001744
Iteration 137/1000 | Loss: 0.00001743
Iteration 138/1000 | Loss: 0.00001743
Iteration 139/1000 | Loss: 0.00001743
Iteration 140/1000 | Loss: 0.00001743
Iteration 141/1000 | Loss: 0.00001743
Iteration 142/1000 | Loss: 0.00001743
Iteration 143/1000 | Loss: 0.00001742
Iteration 144/1000 | Loss: 0.00001742
Iteration 145/1000 | Loss: 0.00001742
Iteration 146/1000 | Loss: 0.00001742
Iteration 147/1000 | Loss: 0.00001742
Iteration 148/1000 | Loss: 0.00001742
Iteration 149/1000 | Loss: 0.00001742
Iteration 150/1000 | Loss: 0.00001742
Iteration 151/1000 | Loss: 0.00001742
Iteration 152/1000 | Loss: 0.00001742
Iteration 153/1000 | Loss: 0.00001742
Iteration 154/1000 | Loss: 0.00001742
Iteration 155/1000 | Loss: 0.00001742
Iteration 156/1000 | Loss: 0.00001742
Iteration 157/1000 | Loss: 0.00001742
Iteration 158/1000 | Loss: 0.00001741
Iteration 159/1000 | Loss: 0.00001741
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001741
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001741
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Iteration 172/1000 | Loss: 0.00001741
Iteration 173/1000 | Loss: 0.00001741
Iteration 174/1000 | Loss: 0.00001741
Iteration 175/1000 | Loss: 0.00001741
Iteration 176/1000 | Loss: 0.00001741
Iteration 177/1000 | Loss: 0.00001741
Iteration 178/1000 | Loss: 0.00001741
Iteration 179/1000 | Loss: 0.00001741
Iteration 180/1000 | Loss: 0.00001740
Iteration 181/1000 | Loss: 0.00001740
Iteration 182/1000 | Loss: 0.00001740
Iteration 183/1000 | Loss: 0.00001740
Iteration 184/1000 | Loss: 0.00001740
Iteration 185/1000 | Loss: 0.00001740
Iteration 186/1000 | Loss: 0.00001740
Iteration 187/1000 | Loss: 0.00001740
Iteration 188/1000 | Loss: 0.00001740
Iteration 189/1000 | Loss: 0.00001740
Iteration 190/1000 | Loss: 0.00001740
Iteration 191/1000 | Loss: 0.00001740
Iteration 192/1000 | Loss: 0.00001740
Iteration 193/1000 | Loss: 0.00001740
Iteration 194/1000 | Loss: 0.00001740
Iteration 195/1000 | Loss: 0.00001740
Iteration 196/1000 | Loss: 0.00001740
Iteration 197/1000 | Loss: 0.00001740
Iteration 198/1000 | Loss: 0.00001740
Iteration 199/1000 | Loss: 0.00001740
Iteration 200/1000 | Loss: 0.00001739
Iteration 201/1000 | Loss: 0.00001739
Iteration 202/1000 | Loss: 0.00001739
Iteration 203/1000 | Loss: 0.00001739
Iteration 204/1000 | Loss: 0.00001739
Iteration 205/1000 | Loss: 0.00001739
Iteration 206/1000 | Loss: 0.00001739
Iteration 207/1000 | Loss: 0.00001739
Iteration 208/1000 | Loss: 0.00001739
Iteration 209/1000 | Loss: 0.00001739
Iteration 210/1000 | Loss: 0.00001739
Iteration 211/1000 | Loss: 0.00001739
Iteration 212/1000 | Loss: 0.00001739
Iteration 213/1000 | Loss: 0.00001739
Iteration 214/1000 | Loss: 0.00001739
Iteration 215/1000 | Loss: 0.00001739
Iteration 216/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.7392470908816904e-05, 1.7392470908816904e-05, 1.7392470908816904e-05, 1.7392470908816904e-05, 1.7392470908816904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7392470908816904e-05

Optimization complete. Final v2v error: 3.426785469055176 mm

Highest mean error: 6.119457244873047 mm for frame 6

Lowest mean error: 3.0078248977661133 mm for frame 28

Saving results

Total time: 148.30444812774658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925819
Iteration 2/25 | Loss: 0.00145225
Iteration 3/25 | Loss: 0.00108937
Iteration 4/25 | Loss: 0.00099541
Iteration 5/25 | Loss: 0.00096881
Iteration 6/25 | Loss: 0.00097062
Iteration 7/25 | Loss: 0.00095798
Iteration 8/25 | Loss: 0.00094377
Iteration 9/25 | Loss: 0.00093305
Iteration 10/25 | Loss: 0.00092969
Iteration 11/25 | Loss: 0.00092192
Iteration 12/25 | Loss: 0.00091043
Iteration 13/25 | Loss: 0.00090349
Iteration 14/25 | Loss: 0.00090432
Iteration 15/25 | Loss: 0.00090245
Iteration 16/25 | Loss: 0.00089919
Iteration 17/25 | Loss: 0.00089844
Iteration 18/25 | Loss: 0.00089346
Iteration 19/25 | Loss: 0.00089203
Iteration 20/25 | Loss: 0.00089186
Iteration 21/25 | Loss: 0.00089185
Iteration 22/25 | Loss: 0.00089185
Iteration 23/25 | Loss: 0.00089185
Iteration 24/25 | Loss: 0.00089185
Iteration 25/25 | Loss: 0.00089185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.39504051
Iteration 2/25 | Loss: 0.00163596
Iteration 3/25 | Loss: 0.00163594
Iteration 4/25 | Loss: 0.00163593
Iteration 5/25 | Loss: 0.00163593
Iteration 6/25 | Loss: 0.00163593
Iteration 7/25 | Loss: 0.00163593
Iteration 8/25 | Loss: 0.00163593
Iteration 9/25 | Loss: 0.00163593
Iteration 10/25 | Loss: 0.00163593
Iteration 11/25 | Loss: 0.00163593
Iteration 12/25 | Loss: 0.00163593
Iteration 13/25 | Loss: 0.00163593
Iteration 14/25 | Loss: 0.00163593
Iteration 15/25 | Loss: 0.00163593
Iteration 16/25 | Loss: 0.00163593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001635933294892311, 0.001635933294892311, 0.001635933294892311, 0.001635933294892311, 0.001635933294892311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001635933294892311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163593
Iteration 2/1000 | Loss: 0.00054101
Iteration 3/1000 | Loss: 0.00062521
Iteration 4/1000 | Loss: 0.00019251
Iteration 5/1000 | Loss: 0.00007748
Iteration 6/1000 | Loss: 0.00005428
Iteration 7/1000 | Loss: 0.00004360
Iteration 8/1000 | Loss: 0.00004023
Iteration 9/1000 | Loss: 0.00021252
Iteration 10/1000 | Loss: 0.00019427
Iteration 11/1000 | Loss: 0.00007190
Iteration 12/1000 | Loss: 0.00005646
Iteration 13/1000 | Loss: 0.00005065
Iteration 14/1000 | Loss: 0.00004718
Iteration 15/1000 | Loss: 0.00004532
Iteration 16/1000 | Loss: 0.00004361
Iteration 17/1000 | Loss: 0.00013363
Iteration 18/1000 | Loss: 0.00017002
Iteration 19/1000 | Loss: 0.00009304
Iteration 20/1000 | Loss: 0.00015514
Iteration 21/1000 | Loss: 0.00005398
Iteration 22/1000 | Loss: 0.00004499
Iteration 23/1000 | Loss: 0.00016757
Iteration 24/1000 | Loss: 0.00005088
Iteration 25/1000 | Loss: 0.00004357
Iteration 26/1000 | Loss: 0.00014965
Iteration 27/1000 | Loss: 0.00005036
Iteration 28/1000 | Loss: 0.00004450
Iteration 29/1000 | Loss: 0.00017093
Iteration 30/1000 | Loss: 0.00012810
Iteration 31/1000 | Loss: 0.00004619
Iteration 32/1000 | Loss: 0.00003884
Iteration 33/1000 | Loss: 0.00003664
Iteration 34/1000 | Loss: 0.00005315
Iteration 35/1000 | Loss: 0.00004593
Iteration 36/1000 | Loss: 0.00004912
Iteration 37/1000 | Loss: 0.00003772
Iteration 38/1000 | Loss: 0.00017431
Iteration 39/1000 | Loss: 0.00010317
Iteration 40/1000 | Loss: 0.00004649
Iteration 41/1000 | Loss: 0.00003886
Iteration 42/1000 | Loss: 0.00003666
Iteration 43/1000 | Loss: 0.00003585
Iteration 44/1000 | Loss: 0.00003493
Iteration 45/1000 | Loss: 0.00003399
Iteration 46/1000 | Loss: 0.00003329
Iteration 47/1000 | Loss: 0.00003274
Iteration 48/1000 | Loss: 0.00003241
Iteration 49/1000 | Loss: 0.00030519
Iteration 50/1000 | Loss: 0.00004780
Iteration 51/1000 | Loss: 0.00003801
Iteration 52/1000 | Loss: 0.00003528
Iteration 53/1000 | Loss: 0.00003449
Iteration 54/1000 | Loss: 0.00003392
Iteration 55/1000 | Loss: 0.00003331
Iteration 56/1000 | Loss: 0.00003248
Iteration 57/1000 | Loss: 0.00003187
Iteration 58/1000 | Loss: 0.00003114
Iteration 59/1000 | Loss: 0.00003074
Iteration 60/1000 | Loss: 0.00003044
Iteration 61/1000 | Loss: 0.00003021
Iteration 62/1000 | Loss: 0.00002994
Iteration 63/1000 | Loss: 0.00002972
Iteration 64/1000 | Loss: 0.00002945
Iteration 65/1000 | Loss: 0.00002925
Iteration 66/1000 | Loss: 0.00002913
Iteration 67/1000 | Loss: 0.00002909
Iteration 68/1000 | Loss: 0.00002904
Iteration 69/1000 | Loss: 0.00002897
Iteration 70/1000 | Loss: 0.00002894
Iteration 71/1000 | Loss: 0.00002893
Iteration 72/1000 | Loss: 0.00002891
Iteration 73/1000 | Loss: 0.00002891
Iteration 74/1000 | Loss: 0.00002890
Iteration 75/1000 | Loss: 0.00002890
Iteration 76/1000 | Loss: 0.00002886
Iteration 77/1000 | Loss: 0.00005425
Iteration 78/1000 | Loss: 0.00003486
Iteration 79/1000 | Loss: 0.00004778
Iteration 80/1000 | Loss: 0.00002886
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00005313
Iteration 83/1000 | Loss: 0.00005313
Iteration 84/1000 | Loss: 0.00004769
Iteration 85/1000 | Loss: 0.00002900
Iteration 86/1000 | Loss: 0.00002880
Iteration 87/1000 | Loss: 0.00002880
Iteration 88/1000 | Loss: 0.00002879
Iteration 89/1000 | Loss: 0.00005314
Iteration 90/1000 | Loss: 0.00003510
Iteration 91/1000 | Loss: 0.00003186
Iteration 92/1000 | Loss: 0.00008375
Iteration 93/1000 | Loss: 0.00005217
Iteration 94/1000 | Loss: 0.00004945
Iteration 95/1000 | Loss: 0.00004350
Iteration 96/1000 | Loss: 0.00026605
Iteration 97/1000 | Loss: 0.00004255
Iteration 98/1000 | Loss: 0.00003550
Iteration 99/1000 | Loss: 0.00003328
Iteration 100/1000 | Loss: 0.00003187
Iteration 101/1000 | Loss: 0.00003127
Iteration 102/1000 | Loss: 0.00003071
Iteration 103/1000 | Loss: 0.00003016
Iteration 104/1000 | Loss: 0.00002988
Iteration 105/1000 | Loss: 0.00002968
Iteration 106/1000 | Loss: 0.00002952
Iteration 107/1000 | Loss: 0.00002937
Iteration 108/1000 | Loss: 0.00002935
Iteration 109/1000 | Loss: 0.00002929
Iteration 110/1000 | Loss: 0.00002925
Iteration 111/1000 | Loss: 0.00005205
Iteration 112/1000 | Loss: 0.00004557
Iteration 113/1000 | Loss: 0.00003027
Iteration 114/1000 | Loss: 0.00002969
Iteration 115/1000 | Loss: 0.00005142
Iteration 116/1000 | Loss: 0.00004410
Iteration 117/1000 | Loss: 0.00005176
Iteration 118/1000 | Loss: 0.00004174
Iteration 119/1000 | Loss: 0.00005000
Iteration 120/1000 | Loss: 0.00003738
Iteration 121/1000 | Loss: 0.00002935
Iteration 122/1000 | Loss: 0.00004938
Iteration 123/1000 | Loss: 0.00025566
Iteration 124/1000 | Loss: 0.00019902
Iteration 125/1000 | Loss: 0.00003067
Iteration 126/1000 | Loss: 0.00002974
Iteration 127/1000 | Loss: 0.00003002
Iteration 128/1000 | Loss: 0.00003456
Iteration 129/1000 | Loss: 0.00002908
Iteration 130/1000 | Loss: 0.00002880
Iteration 131/1000 | Loss: 0.00002848
Iteration 132/1000 | Loss: 0.00002818
Iteration 133/1000 | Loss: 0.00025361
Iteration 134/1000 | Loss: 0.00006256
Iteration 135/1000 | Loss: 0.00018564
Iteration 136/1000 | Loss: 0.00005310
Iteration 137/1000 | Loss: 0.00003391
Iteration 138/1000 | Loss: 0.00003102
Iteration 139/1000 | Loss: 0.00003034
Iteration 140/1000 | Loss: 0.00002990
Iteration 141/1000 | Loss: 0.00002958
Iteration 142/1000 | Loss: 0.00002925
Iteration 143/1000 | Loss: 0.00011598
Iteration 144/1000 | Loss: 0.00008069
Iteration 145/1000 | Loss: 0.00002933
Iteration 146/1000 | Loss: 0.00011831
Iteration 147/1000 | Loss: 0.00009016
Iteration 148/1000 | Loss: 0.00011627
Iteration 149/1000 | Loss: 0.00009931
Iteration 150/1000 | Loss: 0.00011664
Iteration 151/1000 | Loss: 0.00005403
Iteration 152/1000 | Loss: 0.00004378
Iteration 153/1000 | Loss: 0.00003388
Iteration 154/1000 | Loss: 0.00003210
Iteration 155/1000 | Loss: 0.00003173
Iteration 156/1000 | Loss: 0.00003155
Iteration 157/1000 | Loss: 0.00014279
Iteration 158/1000 | Loss: 0.00010753
Iteration 159/1000 | Loss: 0.00003186
Iteration 160/1000 | Loss: 0.00014374
Iteration 161/1000 | Loss: 0.00003479
Iteration 162/1000 | Loss: 0.00003300
Iteration 163/1000 | Loss: 0.00003231
Iteration 164/1000 | Loss: 0.00010712
Iteration 165/1000 | Loss: 0.00003985
Iteration 166/1000 | Loss: 0.00003422
Iteration 167/1000 | Loss: 0.00003137
Iteration 168/1000 | Loss: 0.00002983
Iteration 169/1000 | Loss: 0.00002948
Iteration 170/1000 | Loss: 0.00002918
Iteration 171/1000 | Loss: 0.00002890
Iteration 172/1000 | Loss: 0.00002887
Iteration 173/1000 | Loss: 0.00002883
Iteration 174/1000 | Loss: 0.00002882
Iteration 175/1000 | Loss: 0.00002882
Iteration 176/1000 | Loss: 0.00002881
Iteration 177/1000 | Loss: 0.00002881
Iteration 178/1000 | Loss: 0.00002881
Iteration 179/1000 | Loss: 0.00002881
Iteration 180/1000 | Loss: 0.00002881
Iteration 181/1000 | Loss: 0.00002881
Iteration 182/1000 | Loss: 0.00002881
Iteration 183/1000 | Loss: 0.00002880
Iteration 184/1000 | Loss: 0.00002880
Iteration 185/1000 | Loss: 0.00002880
Iteration 186/1000 | Loss: 0.00002879
Iteration 187/1000 | Loss: 0.00002878
Iteration 188/1000 | Loss: 0.00002878
Iteration 189/1000 | Loss: 0.00002878
Iteration 190/1000 | Loss: 0.00002877
Iteration 191/1000 | Loss: 0.00002877
Iteration 192/1000 | Loss: 0.00002876
Iteration 193/1000 | Loss: 0.00002876
Iteration 194/1000 | Loss: 0.00002876
Iteration 195/1000 | Loss: 0.00002873
Iteration 196/1000 | Loss: 0.00002873
Iteration 197/1000 | Loss: 0.00002873
Iteration 198/1000 | Loss: 0.00002873
Iteration 199/1000 | Loss: 0.00002872
Iteration 200/1000 | Loss: 0.00002869
Iteration 201/1000 | Loss: 0.00002869
Iteration 202/1000 | Loss: 0.00002868
Iteration 203/1000 | Loss: 0.00002867
Iteration 204/1000 | Loss: 0.00002867
Iteration 205/1000 | Loss: 0.00002866
Iteration 206/1000 | Loss: 0.00002866
Iteration 207/1000 | Loss: 0.00002854
Iteration 208/1000 | Loss: 0.00002853
Iteration 209/1000 | Loss: 0.00002853
Iteration 210/1000 | Loss: 0.00002851
Iteration 211/1000 | Loss: 0.00002850
Iteration 212/1000 | Loss: 0.00002848
Iteration 213/1000 | Loss: 0.00002848
Iteration 214/1000 | Loss: 0.00002844
Iteration 215/1000 | Loss: 0.00005518
Iteration 216/1000 | Loss: 0.00003945
Iteration 217/1000 | Loss: 0.00022209
Iteration 218/1000 | Loss: 0.00015639
Iteration 219/1000 | Loss: 0.00007757
Iteration 220/1000 | Loss: 0.00005199
Iteration 221/1000 | Loss: 0.00004229
Iteration 222/1000 | Loss: 0.00022593
Iteration 223/1000 | Loss: 0.00003361
Iteration 224/1000 | Loss: 0.00003113
Iteration 225/1000 | Loss: 0.00002958
Iteration 226/1000 | Loss: 0.00002896
Iteration 227/1000 | Loss: 0.00002799
Iteration 228/1000 | Loss: 0.00002752
Iteration 229/1000 | Loss: 0.00002737
Iteration 230/1000 | Loss: 0.00002732
Iteration 231/1000 | Loss: 0.00002729
Iteration 232/1000 | Loss: 0.00002729
Iteration 233/1000 | Loss: 0.00002728
Iteration 234/1000 | Loss: 0.00002728
Iteration 235/1000 | Loss: 0.00002727
Iteration 236/1000 | Loss: 0.00002726
Iteration 237/1000 | Loss: 0.00002721
Iteration 238/1000 | Loss: 0.00002718
Iteration 239/1000 | Loss: 0.00002717
Iteration 240/1000 | Loss: 0.00002717
Iteration 241/1000 | Loss: 0.00002716
Iteration 242/1000 | Loss: 0.00002716
Iteration 243/1000 | Loss: 0.00002716
Iteration 244/1000 | Loss: 0.00002716
Iteration 245/1000 | Loss: 0.00002716
Iteration 246/1000 | Loss: 0.00002716
Iteration 247/1000 | Loss: 0.00002716
Iteration 248/1000 | Loss: 0.00002716
Iteration 249/1000 | Loss: 0.00002716
Iteration 250/1000 | Loss: 0.00002716
Iteration 251/1000 | Loss: 0.00002716
Iteration 252/1000 | Loss: 0.00002716
Iteration 253/1000 | Loss: 0.00002716
Iteration 254/1000 | Loss: 0.00002716
Iteration 255/1000 | Loss: 0.00002716
Iteration 256/1000 | Loss: 0.00002716
Iteration 257/1000 | Loss: 0.00002716
Iteration 258/1000 | Loss: 0.00002716
Iteration 259/1000 | Loss: 0.00002716
Iteration 260/1000 | Loss: 0.00002716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.7159439923707396e-05, 2.7159439923707396e-05, 2.7159439923707396e-05, 2.7159439923707396e-05, 2.7159439923707396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7159439923707396e-05

Optimization complete. Final v2v error: 4.275763511657715 mm

Highest mean error: 7.942534923553467 mm for frame 97

Lowest mean error: 3.5111496448516846 mm for frame 133

Saving results

Total time: 300.35981702804565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087282
Iteration 2/25 | Loss: 0.01087282
Iteration 3/25 | Loss: 0.01087282
Iteration 4/25 | Loss: 0.01087282
Iteration 5/25 | Loss: 0.01087282
Iteration 6/25 | Loss: 0.01087282
Iteration 7/25 | Loss: 0.01087282
Iteration 8/25 | Loss: 0.01087282
Iteration 9/25 | Loss: 0.01087282
Iteration 10/25 | Loss: 0.01087282
Iteration 11/25 | Loss: 0.01087281
Iteration 12/25 | Loss: 0.01087281
Iteration 13/25 | Loss: 0.01087281
Iteration 14/25 | Loss: 0.01087281
Iteration 15/25 | Loss: 0.01087281
Iteration 16/25 | Loss: 0.01087281
Iteration 17/25 | Loss: 0.01087281
Iteration 18/25 | Loss: 0.01087281
Iteration 19/25 | Loss: 0.01087281
Iteration 20/25 | Loss: 0.01087281
Iteration 21/25 | Loss: 0.01087281
Iteration 22/25 | Loss: 0.01087281
Iteration 23/25 | Loss: 0.01087281
Iteration 24/25 | Loss: 0.01087281
Iteration 25/25 | Loss: 0.01087280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14044619
Iteration 2/25 | Loss: 0.05445934
Iteration 3/25 | Loss: 0.05445749
Iteration 4/25 | Loss: 0.05445748
Iteration 5/25 | Loss: 0.05445747
Iteration 6/25 | Loss: 0.05445747
Iteration 7/25 | Loss: 0.05445747
Iteration 8/25 | Loss: 0.05445747
Iteration 9/25 | Loss: 0.05445747
Iteration 10/25 | Loss: 0.05445746
Iteration 11/25 | Loss: 0.05445746
Iteration 12/25 | Loss: 0.05445746
Iteration 13/25 | Loss: 0.05445746
Iteration 14/25 | Loss: 0.05445746
Iteration 15/25 | Loss: 0.05445746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.05445745959877968, 0.05445745959877968, 0.05445745959877968, 0.05445745959877968, 0.05445745959877968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05445745959877968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05445746
Iteration 2/1000 | Loss: 0.00681429
Iteration 3/1000 | Loss: 0.00193271
Iteration 4/1000 | Loss: 0.00076695
Iteration 5/1000 | Loss: 0.00104534
Iteration 6/1000 | Loss: 0.00866268
Iteration 7/1000 | Loss: 0.00364703
Iteration 8/1000 | Loss: 0.00100039
Iteration 9/1000 | Loss: 0.00077114
Iteration 10/1000 | Loss: 0.00034020
Iteration 11/1000 | Loss: 0.00102724
Iteration 12/1000 | Loss: 0.00024021
Iteration 13/1000 | Loss: 0.00082682
Iteration 14/1000 | Loss: 0.00009960
Iteration 15/1000 | Loss: 0.00035642
Iteration 16/1000 | Loss: 0.00039363
Iteration 17/1000 | Loss: 0.00106278
Iteration 18/1000 | Loss: 0.00030015
Iteration 19/1000 | Loss: 0.00032183
Iteration 20/1000 | Loss: 0.00005775
Iteration 21/1000 | Loss: 0.00046742
Iteration 22/1000 | Loss: 0.00299968
Iteration 23/1000 | Loss: 0.00022913
Iteration 24/1000 | Loss: 0.00015118
Iteration 25/1000 | Loss: 0.00013454
Iteration 26/1000 | Loss: 0.00084811
Iteration 27/1000 | Loss: 0.00011680
Iteration 28/1000 | Loss: 0.00004549
Iteration 29/1000 | Loss: 0.00007608
Iteration 30/1000 | Loss: 0.00003424
Iteration 31/1000 | Loss: 0.00063642
Iteration 32/1000 | Loss: 0.00004432
Iteration 33/1000 | Loss: 0.00015254
Iteration 34/1000 | Loss: 0.00010808
Iteration 35/1000 | Loss: 0.00012206
Iteration 36/1000 | Loss: 0.00006671
Iteration 37/1000 | Loss: 0.00010468
Iteration 38/1000 | Loss: 0.00003653
Iteration 39/1000 | Loss: 0.00037969
Iteration 40/1000 | Loss: 0.00011314
Iteration 41/1000 | Loss: 0.00008321
Iteration 42/1000 | Loss: 0.00040081
Iteration 43/1000 | Loss: 0.00002550
Iteration 44/1000 | Loss: 0.00050641
Iteration 45/1000 | Loss: 0.00098257
Iteration 46/1000 | Loss: 0.00040000
Iteration 47/1000 | Loss: 0.00236808
Iteration 48/1000 | Loss: 0.00016474
Iteration 49/1000 | Loss: 0.00029040
Iteration 50/1000 | Loss: 0.00022688
Iteration 51/1000 | Loss: 0.00002997
Iteration 52/1000 | Loss: 0.00005625
Iteration 53/1000 | Loss: 0.00010216
Iteration 54/1000 | Loss: 0.00004952
Iteration 55/1000 | Loss: 0.00004278
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00014757
Iteration 58/1000 | Loss: 0.00002407
Iteration 59/1000 | Loss: 0.00003979
Iteration 60/1000 | Loss: 0.00003546
Iteration 61/1000 | Loss: 0.00002164
Iteration 62/1000 | Loss: 0.00002162
Iteration 63/1000 | Loss: 0.00002937
Iteration 64/1000 | Loss: 0.00002122
Iteration 65/1000 | Loss: 0.00005757
Iteration 66/1000 | Loss: 0.00090411
Iteration 67/1000 | Loss: 0.00404712
Iteration 68/1000 | Loss: 0.00013819
Iteration 69/1000 | Loss: 0.00021658
Iteration 70/1000 | Loss: 0.00011067
Iteration 71/1000 | Loss: 0.00004842
Iteration 72/1000 | Loss: 0.00006214
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00003814
Iteration 75/1000 | Loss: 0.00004044
Iteration 76/1000 | Loss: 0.00002079
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00004508
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00017099
Iteration 81/1000 | Loss: 0.00005986
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00002168
Iteration 84/1000 | Loss: 0.00005102
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001897
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00006744
Iteration 93/1000 | Loss: 0.00001900
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001888
Iteration 100/1000 | Loss: 0.00001888
Iteration 101/1000 | Loss: 0.00001888
Iteration 102/1000 | Loss: 0.00001888
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001887
Iteration 106/1000 | Loss: 0.00001887
Iteration 107/1000 | Loss: 0.00001887
Iteration 108/1000 | Loss: 0.00001887
Iteration 109/1000 | Loss: 0.00001887
Iteration 110/1000 | Loss: 0.00005497
Iteration 111/1000 | Loss: 0.00009666
Iteration 112/1000 | Loss: 0.00002988
Iteration 113/1000 | Loss: 0.00003051
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001901
Iteration 116/1000 | Loss: 0.00001884
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001881
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001879
Iteration 129/1000 | Loss: 0.00001879
Iteration 130/1000 | Loss: 0.00001879
Iteration 131/1000 | Loss: 0.00001879
Iteration 132/1000 | Loss: 0.00001879
Iteration 133/1000 | Loss: 0.00001878
Iteration 134/1000 | Loss: 0.00001878
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Iteration 137/1000 | Loss: 0.00001878
Iteration 138/1000 | Loss: 0.00001877
Iteration 139/1000 | Loss: 0.00001877
Iteration 140/1000 | Loss: 0.00001877
Iteration 141/1000 | Loss: 0.00001877
Iteration 142/1000 | Loss: 0.00001970
Iteration 143/1000 | Loss: 0.00001877
Iteration 144/1000 | Loss: 0.00001876
Iteration 145/1000 | Loss: 0.00001876
Iteration 146/1000 | Loss: 0.00001876
Iteration 147/1000 | Loss: 0.00001876
Iteration 148/1000 | Loss: 0.00001876
Iteration 149/1000 | Loss: 0.00001875
Iteration 150/1000 | Loss: 0.00001875
Iteration 151/1000 | Loss: 0.00001875
Iteration 152/1000 | Loss: 0.00001874
Iteration 153/1000 | Loss: 0.00003755
Iteration 154/1000 | Loss: 0.00048276
Iteration 155/1000 | Loss: 0.00005458
Iteration 156/1000 | Loss: 0.00003584
Iteration 157/1000 | Loss: 0.00005084
Iteration 158/1000 | Loss: 0.00005515
Iteration 159/1000 | Loss: 0.00002060
Iteration 160/1000 | Loss: 0.00001908
Iteration 161/1000 | Loss: 0.00001908
Iteration 162/1000 | Loss: 0.00002594
Iteration 163/1000 | Loss: 0.00001916
Iteration 164/1000 | Loss: 0.00001982
Iteration 165/1000 | Loss: 0.00001867
Iteration 166/1000 | Loss: 0.00001867
Iteration 167/1000 | Loss: 0.00001867
Iteration 168/1000 | Loss: 0.00001867
Iteration 169/1000 | Loss: 0.00001867
Iteration 170/1000 | Loss: 0.00001867
Iteration 171/1000 | Loss: 0.00001867
Iteration 172/1000 | Loss: 0.00001867
Iteration 173/1000 | Loss: 0.00001867
Iteration 174/1000 | Loss: 0.00001867
Iteration 175/1000 | Loss: 0.00001866
Iteration 176/1000 | Loss: 0.00001866
Iteration 177/1000 | Loss: 0.00001866
Iteration 178/1000 | Loss: 0.00001866
Iteration 179/1000 | Loss: 0.00001866
Iteration 180/1000 | Loss: 0.00001866
Iteration 181/1000 | Loss: 0.00001866
Iteration 182/1000 | Loss: 0.00001866
Iteration 183/1000 | Loss: 0.00001866
Iteration 184/1000 | Loss: 0.00001866
Iteration 185/1000 | Loss: 0.00001866
Iteration 186/1000 | Loss: 0.00001866
Iteration 187/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.8664672097656876e-05, 1.8664672097656876e-05, 1.8664672097656876e-05, 1.8664672097656876e-05, 1.8664672097656876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8664672097656876e-05

Optimization complete. Final v2v error: 3.3904831409454346 mm

Highest mean error: 14.23275089263916 mm for frame 80

Lowest mean error: 2.7706730365753174 mm for frame 9

Saving results

Total time: 169.76315569877625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997567
Iteration 2/25 | Loss: 0.00192846
Iteration 3/25 | Loss: 0.00108106
Iteration 4/25 | Loss: 0.00103982
Iteration 5/25 | Loss: 0.00102789
Iteration 6/25 | Loss: 0.00102548
Iteration 7/25 | Loss: 0.00102510
Iteration 8/25 | Loss: 0.00102510
Iteration 9/25 | Loss: 0.00102510
Iteration 10/25 | Loss: 0.00102510
Iteration 11/25 | Loss: 0.00102510
Iteration 12/25 | Loss: 0.00102510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010250984923914075, 0.0010250984923914075, 0.0010250984923914075, 0.0010250984923914075, 0.0010250984923914075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010250984923914075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25883567
Iteration 2/25 | Loss: 0.00048389
Iteration 3/25 | Loss: 0.00048389
Iteration 4/25 | Loss: 0.00048389
Iteration 5/25 | Loss: 0.00048389
Iteration 6/25 | Loss: 0.00048389
Iteration 7/25 | Loss: 0.00048389
Iteration 8/25 | Loss: 0.00048389
Iteration 9/25 | Loss: 0.00048389
Iteration 10/25 | Loss: 0.00048389
Iteration 11/25 | Loss: 0.00048389
Iteration 12/25 | Loss: 0.00048389
Iteration 13/25 | Loss: 0.00048389
Iteration 14/25 | Loss: 0.00048389
Iteration 15/25 | Loss: 0.00048389
Iteration 16/25 | Loss: 0.00048389
Iteration 17/25 | Loss: 0.00048389
Iteration 18/25 | Loss: 0.00048389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00048388680443167686, 0.00048388680443167686, 0.00048388680443167686, 0.00048388680443167686, 0.00048388680443167686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048388680443167686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048389
Iteration 2/1000 | Loss: 0.00006587
Iteration 3/1000 | Loss: 0.00004509
Iteration 4/1000 | Loss: 0.00003713
Iteration 5/1000 | Loss: 0.00003521
Iteration 6/1000 | Loss: 0.00003406
Iteration 7/1000 | Loss: 0.00003350
Iteration 8/1000 | Loss: 0.00003271
Iteration 9/1000 | Loss: 0.00003197
Iteration 10/1000 | Loss: 0.00003150
Iteration 11/1000 | Loss: 0.00003111
Iteration 12/1000 | Loss: 0.00003075
Iteration 13/1000 | Loss: 0.00003048
Iteration 14/1000 | Loss: 0.00003025
Iteration 15/1000 | Loss: 0.00003010
Iteration 16/1000 | Loss: 0.00002996
Iteration 17/1000 | Loss: 0.00002992
Iteration 18/1000 | Loss: 0.00002984
Iteration 19/1000 | Loss: 0.00002980
Iteration 20/1000 | Loss: 0.00002967
Iteration 21/1000 | Loss: 0.00002955
Iteration 22/1000 | Loss: 0.00002955
Iteration 23/1000 | Loss: 0.00002951
Iteration 24/1000 | Loss: 0.00002944
Iteration 25/1000 | Loss: 0.00002944
Iteration 26/1000 | Loss: 0.00002944
Iteration 27/1000 | Loss: 0.00002944
Iteration 28/1000 | Loss: 0.00002944
Iteration 29/1000 | Loss: 0.00002943
Iteration 30/1000 | Loss: 0.00002943
Iteration 31/1000 | Loss: 0.00002943
Iteration 32/1000 | Loss: 0.00002943
Iteration 33/1000 | Loss: 0.00002942
Iteration 34/1000 | Loss: 0.00002937
Iteration 35/1000 | Loss: 0.00002933
Iteration 36/1000 | Loss: 0.00002933
Iteration 37/1000 | Loss: 0.00002933
Iteration 38/1000 | Loss: 0.00002932
Iteration 39/1000 | Loss: 0.00002932
Iteration 40/1000 | Loss: 0.00002932
Iteration 41/1000 | Loss: 0.00002932
Iteration 42/1000 | Loss: 0.00002932
Iteration 43/1000 | Loss: 0.00002932
Iteration 44/1000 | Loss: 0.00002931
Iteration 45/1000 | Loss: 0.00002931
Iteration 46/1000 | Loss: 0.00002930
Iteration 47/1000 | Loss: 0.00002930
Iteration 48/1000 | Loss: 0.00002930
Iteration 49/1000 | Loss: 0.00002930
Iteration 50/1000 | Loss: 0.00002929
Iteration 51/1000 | Loss: 0.00002929
Iteration 52/1000 | Loss: 0.00002929
Iteration 53/1000 | Loss: 0.00002929
Iteration 54/1000 | Loss: 0.00002928
Iteration 55/1000 | Loss: 0.00002928
Iteration 56/1000 | Loss: 0.00002928
Iteration 57/1000 | Loss: 0.00002928
Iteration 58/1000 | Loss: 0.00002928
Iteration 59/1000 | Loss: 0.00002926
Iteration 60/1000 | Loss: 0.00002926
Iteration 61/1000 | Loss: 0.00002926
Iteration 62/1000 | Loss: 0.00002925
Iteration 63/1000 | Loss: 0.00002924
Iteration 64/1000 | Loss: 0.00002923
Iteration 65/1000 | Loss: 0.00002923
Iteration 66/1000 | Loss: 0.00002923
Iteration 67/1000 | Loss: 0.00002921
Iteration 68/1000 | Loss: 0.00002921
Iteration 69/1000 | Loss: 0.00002921
Iteration 70/1000 | Loss: 0.00002921
Iteration 71/1000 | Loss: 0.00002921
Iteration 72/1000 | Loss: 0.00002920
Iteration 73/1000 | Loss: 0.00002920
Iteration 74/1000 | Loss: 0.00002920
Iteration 75/1000 | Loss: 0.00002920
Iteration 76/1000 | Loss: 0.00002920
Iteration 77/1000 | Loss: 0.00002920
Iteration 78/1000 | Loss: 0.00002919
Iteration 79/1000 | Loss: 0.00002918
Iteration 80/1000 | Loss: 0.00002916
Iteration 81/1000 | Loss: 0.00002915
Iteration 82/1000 | Loss: 0.00002915
Iteration 83/1000 | Loss: 0.00002914
Iteration 84/1000 | Loss: 0.00002914
Iteration 85/1000 | Loss: 0.00002914
Iteration 86/1000 | Loss: 0.00002914
Iteration 87/1000 | Loss: 0.00002914
Iteration 88/1000 | Loss: 0.00002914
Iteration 89/1000 | Loss: 0.00002914
Iteration 90/1000 | Loss: 0.00002913
Iteration 91/1000 | Loss: 0.00002913
Iteration 92/1000 | Loss: 0.00002913
Iteration 93/1000 | Loss: 0.00002913
Iteration 94/1000 | Loss: 0.00002913
Iteration 95/1000 | Loss: 0.00002913
Iteration 96/1000 | Loss: 0.00002913
Iteration 97/1000 | Loss: 0.00002913
Iteration 98/1000 | Loss: 0.00002913
Iteration 99/1000 | Loss: 0.00002912
Iteration 100/1000 | Loss: 0.00002912
Iteration 101/1000 | Loss: 0.00002912
Iteration 102/1000 | Loss: 0.00002912
Iteration 103/1000 | Loss: 0.00002912
Iteration 104/1000 | Loss: 0.00002912
Iteration 105/1000 | Loss: 0.00002911
Iteration 106/1000 | Loss: 0.00002911
Iteration 107/1000 | Loss: 0.00002911
Iteration 108/1000 | Loss: 0.00002911
Iteration 109/1000 | Loss: 0.00002910
Iteration 110/1000 | Loss: 0.00002910
Iteration 111/1000 | Loss: 0.00002910
Iteration 112/1000 | Loss: 0.00002910
Iteration 113/1000 | Loss: 0.00002910
Iteration 114/1000 | Loss: 0.00002910
Iteration 115/1000 | Loss: 0.00002910
Iteration 116/1000 | Loss: 0.00002909
Iteration 117/1000 | Loss: 0.00002909
Iteration 118/1000 | Loss: 0.00002909
Iteration 119/1000 | Loss: 0.00002909
Iteration 120/1000 | Loss: 0.00002909
Iteration 121/1000 | Loss: 0.00002909
Iteration 122/1000 | Loss: 0.00002909
Iteration 123/1000 | Loss: 0.00002908
Iteration 124/1000 | Loss: 0.00002908
Iteration 125/1000 | Loss: 0.00002908
Iteration 126/1000 | Loss: 0.00002908
Iteration 127/1000 | Loss: 0.00002908
Iteration 128/1000 | Loss: 0.00002908
Iteration 129/1000 | Loss: 0.00002908
Iteration 130/1000 | Loss: 0.00002908
Iteration 131/1000 | Loss: 0.00002908
Iteration 132/1000 | Loss: 0.00002907
Iteration 133/1000 | Loss: 0.00002907
Iteration 134/1000 | Loss: 0.00002907
Iteration 135/1000 | Loss: 0.00002907
Iteration 136/1000 | Loss: 0.00002907
Iteration 137/1000 | Loss: 0.00002907
Iteration 138/1000 | Loss: 0.00002907
Iteration 139/1000 | Loss: 0.00002907
Iteration 140/1000 | Loss: 0.00002907
Iteration 141/1000 | Loss: 0.00002907
Iteration 142/1000 | Loss: 0.00002907
Iteration 143/1000 | Loss: 0.00002907
Iteration 144/1000 | Loss: 0.00002907
Iteration 145/1000 | Loss: 0.00002907
Iteration 146/1000 | Loss: 0.00002907
Iteration 147/1000 | Loss: 0.00002907
Iteration 148/1000 | Loss: 0.00002907
Iteration 149/1000 | Loss: 0.00002907
Iteration 150/1000 | Loss: 0.00002906
Iteration 151/1000 | Loss: 0.00002906
Iteration 152/1000 | Loss: 0.00002906
Iteration 153/1000 | Loss: 0.00002906
Iteration 154/1000 | Loss: 0.00002906
Iteration 155/1000 | Loss: 0.00002906
Iteration 156/1000 | Loss: 0.00002905
Iteration 157/1000 | Loss: 0.00002905
Iteration 158/1000 | Loss: 0.00002905
Iteration 159/1000 | Loss: 0.00002905
Iteration 160/1000 | Loss: 0.00002905
Iteration 161/1000 | Loss: 0.00002905
Iteration 162/1000 | Loss: 0.00002905
Iteration 163/1000 | Loss: 0.00002904
Iteration 164/1000 | Loss: 0.00002903
Iteration 165/1000 | Loss: 0.00002903
Iteration 166/1000 | Loss: 0.00002903
Iteration 167/1000 | Loss: 0.00002903
Iteration 168/1000 | Loss: 0.00002903
Iteration 169/1000 | Loss: 0.00002903
Iteration 170/1000 | Loss: 0.00002903
Iteration 171/1000 | Loss: 0.00002903
Iteration 172/1000 | Loss: 0.00002903
Iteration 173/1000 | Loss: 0.00002903
Iteration 174/1000 | Loss: 0.00002903
Iteration 175/1000 | Loss: 0.00002903
Iteration 176/1000 | Loss: 0.00002903
Iteration 177/1000 | Loss: 0.00002902
Iteration 178/1000 | Loss: 0.00002902
Iteration 179/1000 | Loss: 0.00002902
Iteration 180/1000 | Loss: 0.00002902
Iteration 181/1000 | Loss: 0.00002902
Iteration 182/1000 | Loss: 0.00002902
Iteration 183/1000 | Loss: 0.00002901
Iteration 184/1000 | Loss: 0.00002901
Iteration 185/1000 | Loss: 0.00002901
Iteration 186/1000 | Loss: 0.00002901
Iteration 187/1000 | Loss: 0.00002901
Iteration 188/1000 | Loss: 0.00002901
Iteration 189/1000 | Loss: 0.00002901
Iteration 190/1000 | Loss: 0.00002901
Iteration 191/1000 | Loss: 0.00002901
Iteration 192/1000 | Loss: 0.00002901
Iteration 193/1000 | Loss: 0.00002901
Iteration 194/1000 | Loss: 0.00002901
Iteration 195/1000 | Loss: 0.00002900
Iteration 196/1000 | Loss: 0.00002900
Iteration 197/1000 | Loss: 0.00002900
Iteration 198/1000 | Loss: 0.00002900
Iteration 199/1000 | Loss: 0.00002900
Iteration 200/1000 | Loss: 0.00002900
Iteration 201/1000 | Loss: 0.00002900
Iteration 202/1000 | Loss: 0.00002900
Iteration 203/1000 | Loss: 0.00002900
Iteration 204/1000 | Loss: 0.00002900
Iteration 205/1000 | Loss: 0.00002899
Iteration 206/1000 | Loss: 0.00002899
Iteration 207/1000 | Loss: 0.00002899
Iteration 208/1000 | Loss: 0.00002899
Iteration 209/1000 | Loss: 0.00002899
Iteration 210/1000 | Loss: 0.00002898
Iteration 211/1000 | Loss: 0.00002898
Iteration 212/1000 | Loss: 0.00002898
Iteration 213/1000 | Loss: 0.00002898
Iteration 214/1000 | Loss: 0.00002898
Iteration 215/1000 | Loss: 0.00002898
Iteration 216/1000 | Loss: 0.00002898
Iteration 217/1000 | Loss: 0.00002898
Iteration 218/1000 | Loss: 0.00002898
Iteration 219/1000 | Loss: 0.00002897
Iteration 220/1000 | Loss: 0.00002897
Iteration 221/1000 | Loss: 0.00002897
Iteration 222/1000 | Loss: 0.00002897
Iteration 223/1000 | Loss: 0.00002897
Iteration 224/1000 | Loss: 0.00002897
Iteration 225/1000 | Loss: 0.00002897
Iteration 226/1000 | Loss: 0.00002897
Iteration 227/1000 | Loss: 0.00002897
Iteration 228/1000 | Loss: 0.00002897
Iteration 229/1000 | Loss: 0.00002897
Iteration 230/1000 | Loss: 0.00002897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.8972586733289063e-05, 2.8972586733289063e-05, 2.8972586733289063e-05, 2.8972586733289063e-05, 2.8972586733289063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8972586733289063e-05

Optimization complete. Final v2v error: 4.326235771179199 mm

Highest mean error: 5.070509910583496 mm for frame 132

Lowest mean error: 3.877486228942871 mm for frame 14

Saving results

Total time: 54.76592826843262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825196
Iteration 2/25 | Loss: 0.00153582
Iteration 3/25 | Loss: 0.00096794
Iteration 4/25 | Loss: 0.00090492
Iteration 5/25 | Loss: 0.00088467
Iteration 6/25 | Loss: 0.00087511
Iteration 7/25 | Loss: 0.00087435
Iteration 8/25 | Loss: 0.00087147
Iteration 9/25 | Loss: 0.00086193
Iteration 10/25 | Loss: 0.00085854
Iteration 11/25 | Loss: 0.00085804
Iteration 12/25 | Loss: 0.00085801
Iteration 13/25 | Loss: 0.00085801
Iteration 14/25 | Loss: 0.00085801
Iteration 15/25 | Loss: 0.00085801
Iteration 16/25 | Loss: 0.00085800
Iteration 17/25 | Loss: 0.00085800
Iteration 18/25 | Loss: 0.00085800
Iteration 19/25 | Loss: 0.00085800
Iteration 20/25 | Loss: 0.00085800
Iteration 21/25 | Loss: 0.00085800
Iteration 22/25 | Loss: 0.00085800
Iteration 23/25 | Loss: 0.00085800
Iteration 24/25 | Loss: 0.00085800
Iteration 25/25 | Loss: 0.00085800

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93114209
Iteration 2/25 | Loss: 0.00135544
Iteration 3/25 | Loss: 0.00135540
Iteration 4/25 | Loss: 0.00135540
Iteration 5/25 | Loss: 0.00135540
Iteration 6/25 | Loss: 0.00135540
Iteration 7/25 | Loss: 0.00135540
Iteration 8/25 | Loss: 0.00135540
Iteration 9/25 | Loss: 0.00135540
Iteration 10/25 | Loss: 0.00135540
Iteration 11/25 | Loss: 0.00135540
Iteration 12/25 | Loss: 0.00135540
Iteration 13/25 | Loss: 0.00135540
Iteration 14/25 | Loss: 0.00135540
Iteration 15/25 | Loss: 0.00135540
Iteration 16/25 | Loss: 0.00135540
Iteration 17/25 | Loss: 0.00135540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013554001925513148, 0.0013554001925513148, 0.0013554001925513148, 0.0013554001925513148, 0.0013554001925513148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013554001925513148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135540
Iteration 2/1000 | Loss: 0.00002742
Iteration 3/1000 | Loss: 0.00002187
Iteration 4/1000 | Loss: 0.00002065
Iteration 5/1000 | Loss: 0.00001970
Iteration 6/1000 | Loss: 0.00001919
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001849
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001818
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001816
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001815
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001814
Iteration 23/1000 | Loss: 0.00001814
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001814
Iteration 26/1000 | Loss: 0.00001814
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001813
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001812
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001811
Iteration 43/1000 | Loss: 0.00001811
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001810
Iteration 48/1000 | Loss: 0.00001810
Iteration 49/1000 | Loss: 0.00001810
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001809
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001809
Iteration 63/1000 | Loss: 0.00001809
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001808
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001807
Iteration 76/1000 | Loss: 0.00001807
Iteration 77/1000 | Loss: 0.00001807
Iteration 78/1000 | Loss: 0.00001807
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001806
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.8057233319268562e-05, 1.8057233319268562e-05, 1.8057233319268562e-05, 1.8057233319268562e-05, 1.8057233319268562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8057233319268562e-05

Optimization complete. Final v2v error: 3.606998920440674 mm

Highest mean error: 4.067223072052002 mm for frame 7

Lowest mean error: 3.2099878787994385 mm for frame 49

Saving results

Total time: 41.71450877189636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400749
Iteration 2/25 | Loss: 0.00099588
Iteration 3/25 | Loss: 0.00080501
Iteration 4/25 | Loss: 0.00076711
Iteration 5/25 | Loss: 0.00075761
Iteration 6/25 | Loss: 0.00075459
Iteration 7/25 | Loss: 0.00075355
Iteration 8/25 | Loss: 0.00075355
Iteration 9/25 | Loss: 0.00075355
Iteration 10/25 | Loss: 0.00075355
Iteration 11/25 | Loss: 0.00075355
Iteration 12/25 | Loss: 0.00075355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007535461918450892, 0.0007535461918450892, 0.0007535461918450892, 0.0007535461918450892, 0.0007535461918450892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007535461918450892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58268261
Iteration 2/25 | Loss: 0.00124983
Iteration 3/25 | Loss: 0.00124983
Iteration 4/25 | Loss: 0.00124983
Iteration 5/25 | Loss: 0.00124983
Iteration 6/25 | Loss: 0.00124983
Iteration 7/25 | Loss: 0.00124983
Iteration 8/25 | Loss: 0.00124983
Iteration 9/25 | Loss: 0.00124983
Iteration 10/25 | Loss: 0.00124983
Iteration 11/25 | Loss: 0.00124983
Iteration 12/25 | Loss: 0.00124983
Iteration 13/25 | Loss: 0.00124983
Iteration 14/25 | Loss: 0.00124983
Iteration 15/25 | Loss: 0.00124983
Iteration 16/25 | Loss: 0.00124983
Iteration 17/25 | Loss: 0.00124983
Iteration 18/25 | Loss: 0.00124983
Iteration 19/25 | Loss: 0.00124983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012498265132308006, 0.0012498265132308006, 0.0012498265132308006, 0.0012498265132308006, 0.0012498265132308006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012498265132308006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124983
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001847
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001452
Iteration 11/1000 | Loss: 0.00001439
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001390
Iteration 22/1000 | Loss: 0.00001390
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001389
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001389
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001387
Iteration 37/1000 | Loss: 0.00001387
Iteration 38/1000 | Loss: 0.00001387
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001387
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001384
Iteration 48/1000 | Loss: 0.00001384
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001383
Iteration 53/1000 | Loss: 0.00001383
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001382
Iteration 56/1000 | Loss: 0.00001382
Iteration 57/1000 | Loss: 0.00001382
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001381
Iteration 60/1000 | Loss: 0.00001381
Iteration 61/1000 | Loss: 0.00001381
Iteration 62/1000 | Loss: 0.00001381
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001378
Iteration 70/1000 | Loss: 0.00001378
Iteration 71/1000 | Loss: 0.00001378
Iteration 72/1000 | Loss: 0.00001377
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001376
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001375
Iteration 83/1000 | Loss: 0.00001375
Iteration 84/1000 | Loss: 0.00001375
Iteration 85/1000 | Loss: 0.00001375
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001374
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001371
Iteration 126/1000 | Loss: 0.00001371
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001371
Iteration 140/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3707403013540898e-05, 1.3707403013540898e-05, 1.3707403013540898e-05, 1.3707403013540898e-05, 1.3707403013540898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3707403013540898e-05

Optimization complete. Final v2v error: 3.1259372234344482 mm

Highest mean error: 3.458742618560791 mm for frame 66

Lowest mean error: 2.8524293899536133 mm for frame 1

Saving results

Total time: 37.075188636779785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929283
Iteration 2/25 | Loss: 0.00100795
Iteration 3/25 | Loss: 0.00087068
Iteration 4/25 | Loss: 0.00082706
Iteration 5/25 | Loss: 0.00081967
Iteration 6/25 | Loss: 0.00081753
Iteration 7/25 | Loss: 0.00081724
Iteration 8/25 | Loss: 0.00081724
Iteration 9/25 | Loss: 0.00081724
Iteration 10/25 | Loss: 0.00081724
Iteration 11/25 | Loss: 0.00081724
Iteration 12/25 | Loss: 0.00081724
Iteration 13/25 | Loss: 0.00081724
Iteration 14/25 | Loss: 0.00081724
Iteration 15/25 | Loss: 0.00081724
Iteration 16/25 | Loss: 0.00081724
Iteration 17/25 | Loss: 0.00081724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008172353263944387, 0.0008172353263944387, 0.0008172353263944387, 0.0008172353263944387, 0.0008172353263944387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008172353263944387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53213501
Iteration 2/25 | Loss: 0.00149772
Iteration 3/25 | Loss: 0.00149772
Iteration 4/25 | Loss: 0.00149771
Iteration 5/25 | Loss: 0.00149771
Iteration 6/25 | Loss: 0.00149771
Iteration 7/25 | Loss: 0.00149771
Iteration 8/25 | Loss: 0.00149771
Iteration 9/25 | Loss: 0.00149771
Iteration 10/25 | Loss: 0.00149771
Iteration 11/25 | Loss: 0.00149771
Iteration 12/25 | Loss: 0.00149771
Iteration 13/25 | Loss: 0.00149771
Iteration 14/25 | Loss: 0.00149771
Iteration 15/25 | Loss: 0.00149771
Iteration 16/25 | Loss: 0.00149771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001497713034041226, 0.001497713034041226, 0.001497713034041226, 0.001497713034041226, 0.001497713034041226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001497713034041226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149771
Iteration 2/1000 | Loss: 0.00003774
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00002259
Iteration 5/1000 | Loss: 0.00002147
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00002001
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001918
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00001886
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001885
Iteration 17/1000 | Loss: 0.00001883
Iteration 18/1000 | Loss: 0.00001883
Iteration 19/1000 | Loss: 0.00001882
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001870
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001863
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001862
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001858
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001857
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001857
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001855
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001852
Iteration 95/1000 | Loss: 0.00001852
Iteration 96/1000 | Loss: 0.00001852
Iteration 97/1000 | Loss: 0.00001852
Iteration 98/1000 | Loss: 0.00001852
Iteration 99/1000 | Loss: 0.00001852
Iteration 100/1000 | Loss: 0.00001852
Iteration 101/1000 | Loss: 0.00001852
Iteration 102/1000 | Loss: 0.00001852
Iteration 103/1000 | Loss: 0.00001852
Iteration 104/1000 | Loss: 0.00001852
Iteration 105/1000 | Loss: 0.00001852
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00001852
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001852
Iteration 129/1000 | Loss: 0.00001852
Iteration 130/1000 | Loss: 0.00001852
Iteration 131/1000 | Loss: 0.00001852
Iteration 132/1000 | Loss: 0.00001852
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.8520311641623266e-05, 1.8520311641623266e-05, 1.8520311641623266e-05, 1.8520311641623266e-05, 1.8520311641623266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8520311641623266e-05

Optimization complete. Final v2v error: 3.6087448596954346 mm

Highest mean error: 3.958791494369507 mm for frame 167

Lowest mean error: 3.1036157608032227 mm for frame 216

Saving results

Total time: 37.99583983421326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048592
Iteration 2/25 | Loss: 0.00181544
Iteration 3/25 | Loss: 0.00129399
Iteration 4/25 | Loss: 0.00137076
Iteration 5/25 | Loss: 0.00121361
Iteration 6/25 | Loss: 0.00109204
Iteration 7/25 | Loss: 0.00106905
Iteration 8/25 | Loss: 0.00100267
Iteration 9/25 | Loss: 0.00100497
Iteration 10/25 | Loss: 0.00104166
Iteration 11/25 | Loss: 0.00096766
Iteration 12/25 | Loss: 0.00095953
Iteration 13/25 | Loss: 0.00095771
Iteration 14/25 | Loss: 0.00095053
Iteration 15/25 | Loss: 0.00095008
Iteration 16/25 | Loss: 0.00094995
Iteration 17/25 | Loss: 0.00094977
Iteration 18/25 | Loss: 0.00096981
Iteration 19/25 | Loss: 0.00094669
Iteration 20/25 | Loss: 0.00093800
Iteration 21/25 | Loss: 0.00093600
Iteration 22/25 | Loss: 0.00093544
Iteration 23/25 | Loss: 0.00093531
Iteration 24/25 | Loss: 0.00093528
Iteration 25/25 | Loss: 0.00093527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06626511
Iteration 2/25 | Loss: 0.00229389
Iteration 3/25 | Loss: 0.00229385
Iteration 4/25 | Loss: 0.00229385
Iteration 5/25 | Loss: 0.00229385
Iteration 6/25 | Loss: 0.00229385
Iteration 7/25 | Loss: 0.00229385
Iteration 8/25 | Loss: 0.00229385
Iteration 9/25 | Loss: 0.00229385
Iteration 10/25 | Loss: 0.00229385
Iteration 11/25 | Loss: 0.00229385
Iteration 12/25 | Loss: 0.00229385
Iteration 13/25 | Loss: 0.00229385
Iteration 14/25 | Loss: 0.00229385
Iteration 15/25 | Loss: 0.00229385
Iteration 16/25 | Loss: 0.00229385
Iteration 17/25 | Loss: 0.00229385
Iteration 18/25 | Loss: 0.00229385
Iteration 19/25 | Loss: 0.00229385
Iteration 20/25 | Loss: 0.00229385
Iteration 21/25 | Loss: 0.00229385
Iteration 22/25 | Loss: 0.00229385
Iteration 23/25 | Loss: 0.00229385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0022938500624150038, 0.0022938500624150038, 0.0022938500624150038, 0.0022938500624150038, 0.0022938500624150038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022938500624150038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229385
Iteration 2/1000 | Loss: 0.00582585
Iteration 3/1000 | Loss: 0.00041421
Iteration 4/1000 | Loss: 0.00015712
Iteration 5/1000 | Loss: 0.00009968
Iteration 6/1000 | Loss: 0.00046829
Iteration 7/1000 | Loss: 0.00007521
Iteration 8/1000 | Loss: 0.00045230
Iteration 9/1000 | Loss: 0.00006806
Iteration 10/1000 | Loss: 0.00004399
Iteration 11/1000 | Loss: 0.00003522
Iteration 12/1000 | Loss: 0.00003016
Iteration 13/1000 | Loss: 0.00062657
Iteration 14/1000 | Loss: 0.00049030
Iteration 15/1000 | Loss: 0.00080241
Iteration 16/1000 | Loss: 0.00075839
Iteration 17/1000 | Loss: 0.00003193
Iteration 18/1000 | Loss: 0.00002589
Iteration 19/1000 | Loss: 0.00002431
Iteration 20/1000 | Loss: 0.00002348
Iteration 21/1000 | Loss: 0.00002236
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00026634
Iteration 24/1000 | Loss: 0.00002419
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00028812
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00009841
Iteration 29/1000 | Loss: 0.00002219
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001709
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001664
Iteration 41/1000 | Loss: 0.00001660
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001655
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001653
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001652
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001644
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00001642
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001641
Iteration 71/1000 | Loss: 0.00001641
Iteration 72/1000 | Loss: 0.00001641
Iteration 73/1000 | Loss: 0.00001641
Iteration 74/1000 | Loss: 0.00001641
Iteration 75/1000 | Loss: 0.00001641
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001640
Iteration 79/1000 | Loss: 0.00001640
Iteration 80/1000 | Loss: 0.00001640
Iteration 81/1000 | Loss: 0.00001640
Iteration 82/1000 | Loss: 0.00001640
Iteration 83/1000 | Loss: 0.00001639
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001639
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001638
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001637
Iteration 95/1000 | Loss: 0.00001637
Iteration 96/1000 | Loss: 0.00001637
Iteration 97/1000 | Loss: 0.00001637
Iteration 98/1000 | Loss: 0.00001637
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001636
Iteration 114/1000 | Loss: 0.00001636
Iteration 115/1000 | Loss: 0.00001636
Iteration 116/1000 | Loss: 0.00001636
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001636
Iteration 122/1000 | Loss: 0.00001636
Iteration 123/1000 | Loss: 0.00001636
Iteration 124/1000 | Loss: 0.00001636
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001636
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001636
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Iteration 135/1000 | Loss: 0.00001636
Iteration 136/1000 | Loss: 0.00001636
Iteration 137/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6355683328583837e-05, 1.6355683328583837e-05, 1.6355683328583837e-05, 1.6355683328583837e-05, 1.6355683328583837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6355683328583837e-05

Optimization complete. Final v2v error: 3.316688299179077 mm

Highest mean error: 4.29810905456543 mm for frame 60

Lowest mean error: 2.6772701740264893 mm for frame 31

Saving results

Total time: 100.94531607627869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976043
Iteration 2/25 | Loss: 0.00145900
Iteration 3/25 | Loss: 0.00116592
Iteration 4/25 | Loss: 0.00109911
Iteration 5/25 | Loss: 0.00107391
Iteration 6/25 | Loss: 0.00106152
Iteration 7/25 | Loss: 0.00105328
Iteration 8/25 | Loss: 0.00104660
Iteration 9/25 | Loss: 0.00104381
Iteration 10/25 | Loss: 0.00103846
Iteration 11/25 | Loss: 0.00103221
Iteration 12/25 | Loss: 0.00103088
Iteration 13/25 | Loss: 0.00102960
Iteration 14/25 | Loss: 0.00103100
Iteration 15/25 | Loss: 0.00102581
Iteration 16/25 | Loss: 0.00102487
Iteration 17/25 | Loss: 0.00102465
Iteration 18/25 | Loss: 0.00102444
Iteration 19/25 | Loss: 0.00102941
Iteration 20/25 | Loss: 0.00102202
Iteration 21/25 | Loss: 0.00102030
Iteration 22/25 | Loss: 0.00102008
Iteration 23/25 | Loss: 0.00101999
Iteration 24/25 | Loss: 0.00101999
Iteration 25/25 | Loss: 0.00101999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55384099
Iteration 2/25 | Loss: 0.00190576
Iteration 3/25 | Loss: 0.00190576
Iteration 4/25 | Loss: 0.00190576
Iteration 5/25 | Loss: 0.00190576
Iteration 6/25 | Loss: 0.00190576
Iteration 7/25 | Loss: 0.00190576
Iteration 8/25 | Loss: 0.00190576
Iteration 9/25 | Loss: 0.00190576
Iteration 10/25 | Loss: 0.00190576
Iteration 11/25 | Loss: 0.00190576
Iteration 12/25 | Loss: 0.00190576
Iteration 13/25 | Loss: 0.00190576
Iteration 14/25 | Loss: 0.00190576
Iteration 15/25 | Loss: 0.00190576
Iteration 16/25 | Loss: 0.00190576
Iteration 17/25 | Loss: 0.00190576
Iteration 18/25 | Loss: 0.00190576
Iteration 19/25 | Loss: 0.00190576
Iteration 20/25 | Loss: 0.00190576
Iteration 21/25 | Loss: 0.00190576
Iteration 22/25 | Loss: 0.00190576
Iteration 23/25 | Loss: 0.00190576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019057565368711948, 0.0019057565368711948, 0.0019057565368711948, 0.0019057565368711948, 0.0019057565368711948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019057565368711948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190576
Iteration 2/1000 | Loss: 0.00018792
Iteration 3/1000 | Loss: 0.00021376
Iteration 4/1000 | Loss: 0.00086957
Iteration 5/1000 | Loss: 0.00011605
Iteration 6/1000 | Loss: 0.00010194
Iteration 7/1000 | Loss: 0.00009163
Iteration 8/1000 | Loss: 0.00008294
Iteration 9/1000 | Loss: 0.00054114
Iteration 10/1000 | Loss: 0.00009731
Iteration 11/1000 | Loss: 0.00060500
Iteration 12/1000 | Loss: 0.00009750
Iteration 13/1000 | Loss: 0.00007746
Iteration 14/1000 | Loss: 0.00007417
Iteration 15/1000 | Loss: 0.00049292
Iteration 16/1000 | Loss: 0.00102846
Iteration 17/1000 | Loss: 0.00046187
Iteration 18/1000 | Loss: 0.00043336
Iteration 19/1000 | Loss: 0.00221403
Iteration 20/1000 | Loss: 0.00394671
Iteration 21/1000 | Loss: 0.00016950
Iteration 22/1000 | Loss: 0.00039634
Iteration 23/1000 | Loss: 0.00020506
Iteration 24/1000 | Loss: 0.00027547
Iteration 25/1000 | Loss: 0.00012398
Iteration 26/1000 | Loss: 0.00006488
Iteration 27/1000 | Loss: 0.00006162
Iteration 28/1000 | Loss: 0.00005071
Iteration 29/1000 | Loss: 0.00008082
Iteration 30/1000 | Loss: 0.00006759
Iteration 31/1000 | Loss: 0.00006556
Iteration 32/1000 | Loss: 0.00004272
Iteration 33/1000 | Loss: 0.00003843
Iteration 34/1000 | Loss: 0.00003601
Iteration 35/1000 | Loss: 0.00003422
Iteration 36/1000 | Loss: 0.00003298
Iteration 37/1000 | Loss: 0.00043973
Iteration 38/1000 | Loss: 0.00003530
Iteration 39/1000 | Loss: 0.00003240
Iteration 40/1000 | Loss: 0.00003114
Iteration 41/1000 | Loss: 0.00003015
Iteration 42/1000 | Loss: 0.00002958
Iteration 43/1000 | Loss: 0.00002911
Iteration 44/1000 | Loss: 0.00002884
Iteration 45/1000 | Loss: 0.00002866
Iteration 46/1000 | Loss: 0.00002860
Iteration 47/1000 | Loss: 0.00002856
Iteration 48/1000 | Loss: 0.00002855
Iteration 49/1000 | Loss: 0.00002855
Iteration 50/1000 | Loss: 0.00002855
Iteration 51/1000 | Loss: 0.00002854
Iteration 52/1000 | Loss: 0.00002851
Iteration 53/1000 | Loss: 0.00002848
Iteration 54/1000 | Loss: 0.00002847
Iteration 55/1000 | Loss: 0.00002847
Iteration 56/1000 | Loss: 0.00002846
Iteration 57/1000 | Loss: 0.00002845
Iteration 58/1000 | Loss: 0.00002845
Iteration 59/1000 | Loss: 0.00002844
Iteration 60/1000 | Loss: 0.00002844
Iteration 61/1000 | Loss: 0.00002843
Iteration 62/1000 | Loss: 0.00002843
Iteration 63/1000 | Loss: 0.00002842
Iteration 64/1000 | Loss: 0.00002842
Iteration 65/1000 | Loss: 0.00002841
Iteration 66/1000 | Loss: 0.00002840
Iteration 67/1000 | Loss: 0.00002840
Iteration 68/1000 | Loss: 0.00002839
Iteration 69/1000 | Loss: 0.00002839
Iteration 70/1000 | Loss: 0.00002839
Iteration 71/1000 | Loss: 0.00002839
Iteration 72/1000 | Loss: 0.00002839
Iteration 73/1000 | Loss: 0.00002838
Iteration 74/1000 | Loss: 0.00002838
Iteration 75/1000 | Loss: 0.00002838
Iteration 76/1000 | Loss: 0.00002838
Iteration 77/1000 | Loss: 0.00002838
Iteration 78/1000 | Loss: 0.00002838
Iteration 79/1000 | Loss: 0.00002838
Iteration 80/1000 | Loss: 0.00002838
Iteration 81/1000 | Loss: 0.00002838
Iteration 82/1000 | Loss: 0.00002837
Iteration 83/1000 | Loss: 0.00002837
Iteration 84/1000 | Loss: 0.00002837
Iteration 85/1000 | Loss: 0.00002836
Iteration 86/1000 | Loss: 0.00002836
Iteration 87/1000 | Loss: 0.00002836
Iteration 88/1000 | Loss: 0.00002835
Iteration 89/1000 | Loss: 0.00002835
Iteration 90/1000 | Loss: 0.00002835
Iteration 91/1000 | Loss: 0.00002835
Iteration 92/1000 | Loss: 0.00002835
Iteration 93/1000 | Loss: 0.00002835
Iteration 94/1000 | Loss: 0.00002834
Iteration 95/1000 | Loss: 0.00002834
Iteration 96/1000 | Loss: 0.00002834
Iteration 97/1000 | Loss: 0.00002834
Iteration 98/1000 | Loss: 0.00002833
Iteration 99/1000 | Loss: 0.00002833
Iteration 100/1000 | Loss: 0.00002833
Iteration 101/1000 | Loss: 0.00002832
Iteration 102/1000 | Loss: 0.00002832
Iteration 103/1000 | Loss: 0.00002832
Iteration 104/1000 | Loss: 0.00002832
Iteration 105/1000 | Loss: 0.00002832
Iteration 106/1000 | Loss: 0.00002831
Iteration 107/1000 | Loss: 0.00002831
Iteration 108/1000 | Loss: 0.00002830
Iteration 109/1000 | Loss: 0.00002830
Iteration 110/1000 | Loss: 0.00002830
Iteration 111/1000 | Loss: 0.00002829
Iteration 112/1000 | Loss: 0.00002829
Iteration 113/1000 | Loss: 0.00002829
Iteration 114/1000 | Loss: 0.00002828
Iteration 115/1000 | Loss: 0.00002828
Iteration 116/1000 | Loss: 0.00002828
Iteration 117/1000 | Loss: 0.00002827
Iteration 118/1000 | Loss: 0.00002827
Iteration 119/1000 | Loss: 0.00002827
Iteration 120/1000 | Loss: 0.00002826
Iteration 121/1000 | Loss: 0.00002826
Iteration 122/1000 | Loss: 0.00002826
Iteration 123/1000 | Loss: 0.00002826
Iteration 124/1000 | Loss: 0.00002825
Iteration 125/1000 | Loss: 0.00002824
Iteration 126/1000 | Loss: 0.00002824
Iteration 127/1000 | Loss: 0.00002824
Iteration 128/1000 | Loss: 0.00002824
Iteration 129/1000 | Loss: 0.00002823
Iteration 130/1000 | Loss: 0.00002823
Iteration 131/1000 | Loss: 0.00002823
Iteration 132/1000 | Loss: 0.00002823
Iteration 133/1000 | Loss: 0.00002823
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002822
Iteration 136/1000 | Loss: 0.00002822
Iteration 137/1000 | Loss: 0.00002822
Iteration 138/1000 | Loss: 0.00002822
Iteration 139/1000 | Loss: 0.00002821
Iteration 140/1000 | Loss: 0.00002821
Iteration 141/1000 | Loss: 0.00002821
Iteration 142/1000 | Loss: 0.00002821
Iteration 143/1000 | Loss: 0.00002821
Iteration 144/1000 | Loss: 0.00002821
Iteration 145/1000 | Loss: 0.00002821
Iteration 146/1000 | Loss: 0.00002821
Iteration 147/1000 | Loss: 0.00002820
Iteration 148/1000 | Loss: 0.00002820
Iteration 149/1000 | Loss: 0.00002820
Iteration 150/1000 | Loss: 0.00002820
Iteration 151/1000 | Loss: 0.00002820
Iteration 152/1000 | Loss: 0.00002820
Iteration 153/1000 | Loss: 0.00002820
Iteration 154/1000 | Loss: 0.00002820
Iteration 155/1000 | Loss: 0.00002820
Iteration 156/1000 | Loss: 0.00002820
Iteration 157/1000 | Loss: 0.00002820
Iteration 158/1000 | Loss: 0.00002820
Iteration 159/1000 | Loss: 0.00002820
Iteration 160/1000 | Loss: 0.00002820
Iteration 161/1000 | Loss: 0.00002820
Iteration 162/1000 | Loss: 0.00002820
Iteration 163/1000 | Loss: 0.00002820
Iteration 164/1000 | Loss: 0.00002820
Iteration 165/1000 | Loss: 0.00002820
Iteration 166/1000 | Loss: 0.00002820
Iteration 167/1000 | Loss: 0.00002820
Iteration 168/1000 | Loss: 0.00002820
Iteration 169/1000 | Loss: 0.00002820
Iteration 170/1000 | Loss: 0.00002820
Iteration 171/1000 | Loss: 0.00002820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.819924702635035e-05, 2.819924702635035e-05, 2.819924702635035e-05, 2.819924702635035e-05, 2.819924702635035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.819924702635035e-05

Optimization complete. Final v2v error: 4.363375663757324 mm

Highest mean error: 5.603165626525879 mm for frame 116

Lowest mean error: 3.43476939201355 mm for frame 216

Saving results

Total time: 131.041565656662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088662
Iteration 2/25 | Loss: 0.00117930
Iteration 3/25 | Loss: 0.00087617
Iteration 4/25 | Loss: 0.00081336
Iteration 5/25 | Loss: 0.00079923
Iteration 6/25 | Loss: 0.00079569
Iteration 7/25 | Loss: 0.00079470
Iteration 8/25 | Loss: 0.00079470
Iteration 9/25 | Loss: 0.00079470
Iteration 10/25 | Loss: 0.00079470
Iteration 11/25 | Loss: 0.00079470
Iteration 12/25 | Loss: 0.00079470
Iteration 13/25 | Loss: 0.00079470
Iteration 14/25 | Loss: 0.00079470
Iteration 15/25 | Loss: 0.00079470
Iteration 16/25 | Loss: 0.00079470
Iteration 17/25 | Loss: 0.00079470
Iteration 18/25 | Loss: 0.00079470
Iteration 19/25 | Loss: 0.00079470
Iteration 20/25 | Loss: 0.00079470
Iteration 21/25 | Loss: 0.00079470
Iteration 22/25 | Loss: 0.00079470
Iteration 23/25 | Loss: 0.00079470
Iteration 24/25 | Loss: 0.00079470
Iteration 25/25 | Loss: 0.00079470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66177726
Iteration 2/25 | Loss: 0.00121120
Iteration 3/25 | Loss: 0.00121120
Iteration 4/25 | Loss: 0.00121120
Iteration 5/25 | Loss: 0.00121120
Iteration 6/25 | Loss: 0.00121120
Iteration 7/25 | Loss: 0.00121120
Iteration 8/25 | Loss: 0.00121120
Iteration 9/25 | Loss: 0.00121120
Iteration 10/25 | Loss: 0.00121120
Iteration 11/25 | Loss: 0.00121120
Iteration 12/25 | Loss: 0.00121120
Iteration 13/25 | Loss: 0.00121120
Iteration 14/25 | Loss: 0.00121120
Iteration 15/25 | Loss: 0.00121120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012111965334042907, 0.0012111965334042907, 0.0012111965334042907, 0.0012111965334042907, 0.0012111965334042907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012111965334042907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121120
Iteration 2/1000 | Loss: 0.00033899
Iteration 3/1000 | Loss: 0.00004253
Iteration 4/1000 | Loss: 0.00002822
Iteration 5/1000 | Loss: 0.00002243
Iteration 6/1000 | Loss: 0.00001992
Iteration 7/1000 | Loss: 0.00001867
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00017537
Iteration 14/1000 | Loss: 0.00001888
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001618
Iteration 18/1000 | Loss: 0.00047986
Iteration 19/1000 | Loss: 0.00017039
Iteration 20/1000 | Loss: 0.00011108
Iteration 21/1000 | Loss: 0.00026648
Iteration 22/1000 | Loss: 0.00003019
Iteration 23/1000 | Loss: 0.00002640
Iteration 24/1000 | Loss: 0.00002142
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001698
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001389
Iteration 35/1000 | Loss: 0.00001389
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001386
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001385
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001383
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001382
Iteration 57/1000 | Loss: 0.00001382
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001381
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001379
Iteration 62/1000 | Loss: 0.00001379
Iteration 63/1000 | Loss: 0.00001379
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001372
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001370
Iteration 96/1000 | Loss: 0.00001370
Iteration 97/1000 | Loss: 0.00001370
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00017393
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001472
Iteration 140/1000 | Loss: 0.00001472
Iteration 141/1000 | Loss: 0.00001456
Iteration 142/1000 | Loss: 0.00001451
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001437
Iteration 146/1000 | Loss: 0.00001437
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001429
Iteration 149/1000 | Loss: 0.00001426
Iteration 150/1000 | Loss: 0.00001421
Iteration 151/1000 | Loss: 0.00001417
Iteration 152/1000 | Loss: 0.00001409
Iteration 153/1000 | Loss: 0.00001403
Iteration 154/1000 | Loss: 0.00001402
Iteration 155/1000 | Loss: 0.00001402
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001397
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001396
Iteration 161/1000 | Loss: 0.00001395
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001391
Iteration 166/1000 | Loss: 0.00001391
Iteration 167/1000 | Loss: 0.00001386
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001386
Iteration 170/1000 | Loss: 0.00001386
Iteration 171/1000 | Loss: 0.00001385
Iteration 172/1000 | Loss: 0.00001385
Iteration 173/1000 | Loss: 0.00001385
Iteration 174/1000 | Loss: 0.00001385
Iteration 175/1000 | Loss: 0.00001385
Iteration 176/1000 | Loss: 0.00001384
Iteration 177/1000 | Loss: 0.00001384
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001384
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001382
Iteration 183/1000 | Loss: 0.00001382
Iteration 184/1000 | Loss: 0.00001381
Iteration 185/1000 | Loss: 0.00001381
Iteration 186/1000 | Loss: 0.00001381
Iteration 187/1000 | Loss: 0.00001381
Iteration 188/1000 | Loss: 0.00001380
Iteration 189/1000 | Loss: 0.00001380
Iteration 190/1000 | Loss: 0.00001379
Iteration 191/1000 | Loss: 0.00001379
Iteration 192/1000 | Loss: 0.00001378
Iteration 193/1000 | Loss: 0.00001378
Iteration 194/1000 | Loss: 0.00001378
Iteration 195/1000 | Loss: 0.00001378
Iteration 196/1000 | Loss: 0.00001378
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001377
Iteration 202/1000 | Loss: 0.00001377
Iteration 203/1000 | Loss: 0.00001377
Iteration 204/1000 | Loss: 0.00001377
Iteration 205/1000 | Loss: 0.00001377
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001376
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Iteration 214/1000 | Loss: 0.00001375
Iteration 215/1000 | Loss: 0.00001374
Iteration 216/1000 | Loss: 0.00001374
Iteration 217/1000 | Loss: 0.00001374
Iteration 218/1000 | Loss: 0.00001374
Iteration 219/1000 | Loss: 0.00001373
Iteration 220/1000 | Loss: 0.00001373
Iteration 221/1000 | Loss: 0.00001373
Iteration 222/1000 | Loss: 0.00001373
Iteration 223/1000 | Loss: 0.00001372
Iteration 224/1000 | Loss: 0.00001372
Iteration 225/1000 | Loss: 0.00001372
Iteration 226/1000 | Loss: 0.00001372
Iteration 227/1000 | Loss: 0.00001372
Iteration 228/1000 | Loss: 0.00001372
Iteration 229/1000 | Loss: 0.00001372
Iteration 230/1000 | Loss: 0.00001372
Iteration 231/1000 | Loss: 0.00001372
Iteration 232/1000 | Loss: 0.00001371
Iteration 233/1000 | Loss: 0.00001371
Iteration 234/1000 | Loss: 0.00001371
Iteration 235/1000 | Loss: 0.00001371
Iteration 236/1000 | Loss: 0.00001371
Iteration 237/1000 | Loss: 0.00001371
Iteration 238/1000 | Loss: 0.00001371
Iteration 239/1000 | Loss: 0.00001370
Iteration 240/1000 | Loss: 0.00001370
Iteration 241/1000 | Loss: 0.00001370
Iteration 242/1000 | Loss: 0.00001370
Iteration 243/1000 | Loss: 0.00001370
Iteration 244/1000 | Loss: 0.00001370
Iteration 245/1000 | Loss: 0.00001370
Iteration 246/1000 | Loss: 0.00001369
Iteration 247/1000 | Loss: 0.00001369
Iteration 248/1000 | Loss: 0.00001369
Iteration 249/1000 | Loss: 0.00001369
Iteration 250/1000 | Loss: 0.00001369
Iteration 251/1000 | Loss: 0.00001369
Iteration 252/1000 | Loss: 0.00001369
Iteration 253/1000 | Loss: 0.00001368
Iteration 254/1000 | Loss: 0.00001368
Iteration 255/1000 | Loss: 0.00001368
Iteration 256/1000 | Loss: 0.00001368
Iteration 257/1000 | Loss: 0.00001368
Iteration 258/1000 | Loss: 0.00001368
Iteration 259/1000 | Loss: 0.00001368
Iteration 260/1000 | Loss: 0.00001367
Iteration 261/1000 | Loss: 0.00001367
Iteration 262/1000 | Loss: 0.00001367
Iteration 263/1000 | Loss: 0.00001367
Iteration 264/1000 | Loss: 0.00001367
Iteration 265/1000 | Loss: 0.00001367
Iteration 266/1000 | Loss: 0.00001367
Iteration 267/1000 | Loss: 0.00001367
Iteration 268/1000 | Loss: 0.00001367
Iteration 269/1000 | Loss: 0.00001367
Iteration 270/1000 | Loss: 0.00001367
Iteration 271/1000 | Loss: 0.00001367
Iteration 272/1000 | Loss: 0.00001367
Iteration 273/1000 | Loss: 0.00001367
Iteration 274/1000 | Loss: 0.00001367
Iteration 275/1000 | Loss: 0.00001367
Iteration 276/1000 | Loss: 0.00001367
Iteration 277/1000 | Loss: 0.00001367
Iteration 278/1000 | Loss: 0.00001367
Iteration 279/1000 | Loss: 0.00001367
Iteration 280/1000 | Loss: 0.00001367
Iteration 281/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.366594915452879e-05, 1.366594915452879e-05, 1.366594915452879e-05, 1.366594915452879e-05, 1.366594915452879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.366594915452879e-05

Optimization complete. Final v2v error: 3.083589553833008 mm

Highest mean error: 6.496691703796387 mm for frame 0

Lowest mean error: 2.8642284870147705 mm for frame 80

Saving results

Total time: 100.00443243980408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559143
Iteration 2/25 | Loss: 0.00111489
Iteration 3/25 | Loss: 0.00092829
Iteration 4/25 | Loss: 0.00089676
Iteration 5/25 | Loss: 0.00088558
Iteration 6/25 | Loss: 0.00088362
Iteration 7/25 | Loss: 0.00088345
Iteration 8/25 | Loss: 0.00088345
Iteration 9/25 | Loss: 0.00088345
Iteration 10/25 | Loss: 0.00088345
Iteration 11/25 | Loss: 0.00088345
Iteration 12/25 | Loss: 0.00088345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008834486361593008, 0.0008834486361593008, 0.0008834486361593008, 0.0008834486361593008, 0.0008834486361593008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008834486361593008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06690311
Iteration 2/25 | Loss: 0.00123420
Iteration 3/25 | Loss: 0.00123418
Iteration 4/25 | Loss: 0.00123418
Iteration 5/25 | Loss: 0.00123418
Iteration 6/25 | Loss: 0.00123418
Iteration 7/25 | Loss: 0.00123418
Iteration 8/25 | Loss: 0.00123418
Iteration 9/25 | Loss: 0.00123418
Iteration 10/25 | Loss: 0.00123418
Iteration 11/25 | Loss: 0.00123418
Iteration 12/25 | Loss: 0.00123418
Iteration 13/25 | Loss: 0.00123418
Iteration 14/25 | Loss: 0.00123418
Iteration 15/25 | Loss: 0.00123418
Iteration 16/25 | Loss: 0.00123418
Iteration 17/25 | Loss: 0.00123418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012341765686869621, 0.0012341765686869621, 0.0012341765686869621, 0.0012341765686869621, 0.0012341765686869621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012341765686869621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123418
Iteration 2/1000 | Loss: 0.00003780
Iteration 3/1000 | Loss: 0.00002509
Iteration 4/1000 | Loss: 0.00002289
Iteration 5/1000 | Loss: 0.00002189
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00002012
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00001996
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001984
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001977
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001973
Iteration 22/1000 | Loss: 0.00001973
Iteration 23/1000 | Loss: 0.00001973
Iteration 24/1000 | Loss: 0.00001972
Iteration 25/1000 | Loss: 0.00001972
Iteration 26/1000 | Loss: 0.00001972
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001971
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001968
Iteration 39/1000 | Loss: 0.00001965
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001964
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001963
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00001962
Iteration 51/1000 | Loss: 0.00001962
Iteration 52/1000 | Loss: 0.00001962
Iteration 53/1000 | Loss: 0.00001961
Iteration 54/1000 | Loss: 0.00001961
Iteration 55/1000 | Loss: 0.00001961
Iteration 56/1000 | Loss: 0.00001961
Iteration 57/1000 | Loss: 0.00001961
Iteration 58/1000 | Loss: 0.00001961
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.9573022655094974e-05, 1.9573022655094974e-05, 1.9573022655094974e-05, 1.9573022655094974e-05, 1.9573022655094974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9573022655094974e-05

Optimization complete. Final v2v error: 3.7032723426818848 mm

Highest mean error: 4.044719219207764 mm for frame 164

Lowest mean error: 3.364366292953491 mm for frame 219

Saving results

Total time: 35.99543642997742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492094
Iteration 2/25 | Loss: 0.00092140
Iteration 3/25 | Loss: 0.00079016
Iteration 4/25 | Loss: 0.00077495
Iteration 5/25 | Loss: 0.00077087
Iteration 6/25 | Loss: 0.00076978
Iteration 7/25 | Loss: 0.00076973
Iteration 8/25 | Loss: 0.00076973
Iteration 9/25 | Loss: 0.00076973
Iteration 10/25 | Loss: 0.00076973
Iteration 11/25 | Loss: 0.00076973
Iteration 12/25 | Loss: 0.00076973
Iteration 13/25 | Loss: 0.00076973
Iteration 14/25 | Loss: 0.00076973
Iteration 15/25 | Loss: 0.00076973
Iteration 16/25 | Loss: 0.00076973
Iteration 17/25 | Loss: 0.00076973
Iteration 18/25 | Loss: 0.00076973
Iteration 19/25 | Loss: 0.00076973
Iteration 20/25 | Loss: 0.00076973
Iteration 21/25 | Loss: 0.00076973
Iteration 22/25 | Loss: 0.00076973
Iteration 23/25 | Loss: 0.00076973
Iteration 24/25 | Loss: 0.00076973
Iteration 25/25 | Loss: 0.00076973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.89651918
Iteration 2/25 | Loss: 0.00116157
Iteration 3/25 | Loss: 0.00116157
Iteration 4/25 | Loss: 0.00116157
Iteration 5/25 | Loss: 0.00116157
Iteration 6/25 | Loss: 0.00116156
Iteration 7/25 | Loss: 0.00116156
Iteration 8/25 | Loss: 0.00116156
Iteration 9/25 | Loss: 0.00116156
Iteration 10/25 | Loss: 0.00116156
Iteration 11/25 | Loss: 0.00116156
Iteration 12/25 | Loss: 0.00116156
Iteration 13/25 | Loss: 0.00116156
Iteration 14/25 | Loss: 0.00116156
Iteration 15/25 | Loss: 0.00116156
Iteration 16/25 | Loss: 0.00116156
Iteration 17/25 | Loss: 0.00116156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011615636758506298, 0.0011615636758506298, 0.0011615636758506298, 0.0011615636758506298, 0.0011615636758506298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011615636758506298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116156
Iteration 2/1000 | Loss: 0.00002688
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001451
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001377
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001325
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001313
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001312
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001312
Iteration 25/1000 | Loss: 0.00001312
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001308
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001306
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001301
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001297
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001295
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001290
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001289
Iteration 125/1000 | Loss: 0.00001289
Iteration 126/1000 | Loss: 0.00001289
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001288
Iteration 135/1000 | Loss: 0.00001288
Iteration 136/1000 | Loss: 0.00001288
Iteration 137/1000 | Loss: 0.00001288
Iteration 138/1000 | Loss: 0.00001288
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.2874138519691769e-05, 1.2874138519691769e-05, 1.2874138519691769e-05, 1.2874138519691769e-05, 1.2874138519691769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2874138519691769e-05

Optimization complete. Final v2v error: 3.0695149898529053 mm

Highest mean error: 3.356381893157959 mm for frame 100

Lowest mean error: 2.805171251296997 mm for frame 161

Saving results

Total time: 37.71210074424744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848058
Iteration 2/25 | Loss: 0.00102350
Iteration 3/25 | Loss: 0.00083342
Iteration 4/25 | Loss: 0.00079770
Iteration 5/25 | Loss: 0.00078713
Iteration 6/25 | Loss: 0.00078509
Iteration 7/25 | Loss: 0.00078452
Iteration 8/25 | Loss: 0.00078452
Iteration 9/25 | Loss: 0.00078452
Iteration 10/25 | Loss: 0.00078452
Iteration 11/25 | Loss: 0.00078452
Iteration 12/25 | Loss: 0.00078452
Iteration 13/25 | Loss: 0.00078452
Iteration 14/25 | Loss: 0.00078452
Iteration 15/25 | Loss: 0.00078452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007845234358683228, 0.0007845234358683228, 0.0007845234358683228, 0.0007845234358683228, 0.0007845234358683228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007845234358683228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60414004
Iteration 2/25 | Loss: 0.00120729
Iteration 3/25 | Loss: 0.00120728
Iteration 4/25 | Loss: 0.00120728
Iteration 5/25 | Loss: 0.00120728
Iteration 6/25 | Loss: 0.00120728
Iteration 7/25 | Loss: 0.00120728
Iteration 8/25 | Loss: 0.00120728
Iteration 9/25 | Loss: 0.00120728
Iteration 10/25 | Loss: 0.00120728
Iteration 11/25 | Loss: 0.00120728
Iteration 12/25 | Loss: 0.00120728
Iteration 13/25 | Loss: 0.00120728
Iteration 14/25 | Loss: 0.00120728
Iteration 15/25 | Loss: 0.00120728
Iteration 16/25 | Loss: 0.00120728
Iteration 17/25 | Loss: 0.00120728
Iteration 18/25 | Loss: 0.00120728
Iteration 19/25 | Loss: 0.00120728
Iteration 20/25 | Loss: 0.00120728
Iteration 21/25 | Loss: 0.00120728
Iteration 22/25 | Loss: 0.00120728
Iteration 23/25 | Loss: 0.00120728
Iteration 24/25 | Loss: 0.00120728
Iteration 25/25 | Loss: 0.00120728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120728
Iteration 2/1000 | Loss: 0.00002932
Iteration 3/1000 | Loss: 0.00002069
Iteration 4/1000 | Loss: 0.00001858
Iteration 5/1000 | Loss: 0.00001757
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001621
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001585
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001556
Iteration 18/1000 | Loss: 0.00001553
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001552
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001550
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001544
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001543
Iteration 46/1000 | Loss: 0.00001543
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001542
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001536
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001533
Iteration 82/1000 | Loss: 0.00001533
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001532
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001532
Iteration 100/1000 | Loss: 0.00001532
Iteration 101/1000 | Loss: 0.00001532
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001532
Iteration 107/1000 | Loss: 0.00001532
Iteration 108/1000 | Loss: 0.00001532
Iteration 109/1000 | Loss: 0.00001532
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001532
Iteration 116/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5315576092689298e-05, 1.5315576092689298e-05, 1.5315576092689298e-05, 1.5315576092689298e-05, 1.5315576092689298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5315576092689298e-05

Optimization complete. Final v2v error: 3.283116340637207 mm

Highest mean error: 4.218077182769775 mm for frame 85

Lowest mean error: 2.921712875366211 mm for frame 114

Saving results

Total time: 34.79831600189209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834899
Iteration 2/25 | Loss: 0.00096535
Iteration 3/25 | Loss: 0.00083257
Iteration 4/25 | Loss: 0.00080074
Iteration 5/25 | Loss: 0.00079710
Iteration 6/25 | Loss: 0.00079603
Iteration 7/25 | Loss: 0.00079588
Iteration 8/25 | Loss: 0.00079588
Iteration 9/25 | Loss: 0.00079588
Iteration 10/25 | Loss: 0.00079588
Iteration 11/25 | Loss: 0.00079588
Iteration 12/25 | Loss: 0.00079588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000795881322119385, 0.000795881322119385, 0.000795881322119385, 0.000795881322119385, 0.000795881322119385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000795881322119385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65432239
Iteration 2/25 | Loss: 0.00135158
Iteration 3/25 | Loss: 0.00135158
Iteration 4/25 | Loss: 0.00135158
Iteration 5/25 | Loss: 0.00135158
Iteration 6/25 | Loss: 0.00135158
Iteration 7/25 | Loss: 0.00135158
Iteration 8/25 | Loss: 0.00135158
Iteration 9/25 | Loss: 0.00135158
Iteration 10/25 | Loss: 0.00135158
Iteration 11/25 | Loss: 0.00135158
Iteration 12/25 | Loss: 0.00135158
Iteration 13/25 | Loss: 0.00135158
Iteration 14/25 | Loss: 0.00135158
Iteration 15/25 | Loss: 0.00135158
Iteration 16/25 | Loss: 0.00135158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013515765313059092, 0.0013515765313059092, 0.0013515765313059092, 0.0013515765313059092, 0.0013515765313059092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013515765313059092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135158
Iteration 2/1000 | Loss: 0.00003414
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001781
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001688
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001636
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001636
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001633
Iteration 50/1000 | Loss: 0.00001633
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001632
Iteration 54/1000 | Loss: 0.00001632
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001629
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001628
Iteration 67/1000 | Loss: 0.00001628
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001618
Iteration 102/1000 | Loss: 0.00001618
Iteration 103/1000 | Loss: 0.00001618
Iteration 104/1000 | Loss: 0.00001618
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001617
Iteration 107/1000 | Loss: 0.00001617
Iteration 108/1000 | Loss: 0.00001617
Iteration 109/1000 | Loss: 0.00001617
Iteration 110/1000 | Loss: 0.00001617
Iteration 111/1000 | Loss: 0.00001617
Iteration 112/1000 | Loss: 0.00001617
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001617
Iteration 115/1000 | Loss: 0.00001617
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001614
Iteration 137/1000 | Loss: 0.00001614
Iteration 138/1000 | Loss: 0.00001614
Iteration 139/1000 | Loss: 0.00001614
Iteration 140/1000 | Loss: 0.00001614
Iteration 141/1000 | Loss: 0.00001614
Iteration 142/1000 | Loss: 0.00001614
Iteration 143/1000 | Loss: 0.00001614
Iteration 144/1000 | Loss: 0.00001614
Iteration 145/1000 | Loss: 0.00001614
Iteration 146/1000 | Loss: 0.00001614
Iteration 147/1000 | Loss: 0.00001614
Iteration 148/1000 | Loss: 0.00001614
Iteration 149/1000 | Loss: 0.00001614
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001614
Iteration 154/1000 | Loss: 0.00001614
Iteration 155/1000 | Loss: 0.00001614
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001614
Iteration 164/1000 | Loss: 0.00001614
Iteration 165/1000 | Loss: 0.00001614
Iteration 166/1000 | Loss: 0.00001614
Iteration 167/1000 | Loss: 0.00001614
Iteration 168/1000 | Loss: 0.00001614
Iteration 169/1000 | Loss: 0.00001614
Iteration 170/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.6139518265845254e-05, 1.6139518265845254e-05, 1.6139518265845254e-05, 1.6139518265845254e-05, 1.6139518265845254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6139518265845254e-05

Optimization complete. Final v2v error: 3.4325194358825684 mm

Highest mean error: 4.017734050750732 mm for frame 99

Lowest mean error: 3.2233753204345703 mm for frame 40

Saving results

Total time: 36.16146993637085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615800
Iteration 2/25 | Loss: 0.00129434
Iteration 3/25 | Loss: 0.00098037
Iteration 4/25 | Loss: 0.00092021
Iteration 5/25 | Loss: 0.00091465
Iteration 6/25 | Loss: 0.00091435
Iteration 7/25 | Loss: 0.00091435
Iteration 8/25 | Loss: 0.00091435
Iteration 9/25 | Loss: 0.00091435
Iteration 10/25 | Loss: 0.00091435
Iteration 11/25 | Loss: 0.00091435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009143527713604271, 0.0009143527713604271, 0.0009143527713604271, 0.0009143527713604271, 0.0009143527713604271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009143527713604271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19499648
Iteration 2/25 | Loss: 0.00096811
Iteration 3/25 | Loss: 0.00096811
Iteration 4/25 | Loss: 0.00096811
Iteration 5/25 | Loss: 0.00096811
Iteration 6/25 | Loss: 0.00096811
Iteration 7/25 | Loss: 0.00096811
Iteration 8/25 | Loss: 0.00096811
Iteration 9/25 | Loss: 0.00096811
Iteration 10/25 | Loss: 0.00096811
Iteration 11/25 | Loss: 0.00096811
Iteration 12/25 | Loss: 0.00096811
Iteration 13/25 | Loss: 0.00096811
Iteration 14/25 | Loss: 0.00096811
Iteration 15/25 | Loss: 0.00096811
Iteration 16/25 | Loss: 0.00096811
Iteration 17/25 | Loss: 0.00096811
Iteration 18/25 | Loss: 0.00096811
Iteration 19/25 | Loss: 0.00096811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009681089431978762, 0.0009681089431978762, 0.0009681089431978762, 0.0009681089431978762, 0.0009681089431978762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009681089431978762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096811
Iteration 2/1000 | Loss: 0.00003492
Iteration 3/1000 | Loss: 0.00002828
Iteration 4/1000 | Loss: 0.00002670
Iteration 5/1000 | Loss: 0.00002560
Iteration 6/1000 | Loss: 0.00002513
Iteration 7/1000 | Loss: 0.00002447
Iteration 8/1000 | Loss: 0.00002391
Iteration 9/1000 | Loss: 0.00002366
Iteration 10/1000 | Loss: 0.00002343
Iteration 11/1000 | Loss: 0.00002320
Iteration 12/1000 | Loss: 0.00002296
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002287
Iteration 15/1000 | Loss: 0.00002274
Iteration 16/1000 | Loss: 0.00002273
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002273
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002273
Iteration 22/1000 | Loss: 0.00002273
Iteration 23/1000 | Loss: 0.00002272
Iteration 24/1000 | Loss: 0.00002272
Iteration 25/1000 | Loss: 0.00002272
Iteration 26/1000 | Loss: 0.00002272
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002272
Iteration 29/1000 | Loss: 0.00002272
Iteration 30/1000 | Loss: 0.00002269
Iteration 31/1000 | Loss: 0.00002268
Iteration 32/1000 | Loss: 0.00002266
Iteration 33/1000 | Loss: 0.00002265
Iteration 34/1000 | Loss: 0.00002265
Iteration 35/1000 | Loss: 0.00002264
Iteration 36/1000 | Loss: 0.00002264
Iteration 37/1000 | Loss: 0.00002264
Iteration 38/1000 | Loss: 0.00002264
Iteration 39/1000 | Loss: 0.00002264
Iteration 40/1000 | Loss: 0.00002264
Iteration 41/1000 | Loss: 0.00002264
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002263
Iteration 45/1000 | Loss: 0.00002263
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002263
Iteration 50/1000 | Loss: 0.00002263
Iteration 51/1000 | Loss: 0.00002263
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002262
Iteration 54/1000 | Loss: 0.00002262
Iteration 55/1000 | Loss: 0.00002262
Iteration 56/1000 | Loss: 0.00002262
Iteration 57/1000 | Loss: 0.00002262
Iteration 58/1000 | Loss: 0.00002262
Iteration 59/1000 | Loss: 0.00002262
Iteration 60/1000 | Loss: 0.00002262
Iteration 61/1000 | Loss: 0.00002262
Iteration 62/1000 | Loss: 0.00002262
Iteration 63/1000 | Loss: 0.00002262
Iteration 64/1000 | Loss: 0.00002262
Iteration 65/1000 | Loss: 0.00002262
Iteration 66/1000 | Loss: 0.00002262
Iteration 67/1000 | Loss: 0.00002262
Iteration 68/1000 | Loss: 0.00002262
Iteration 69/1000 | Loss: 0.00002262
Iteration 70/1000 | Loss: 0.00002261
Iteration 71/1000 | Loss: 0.00002261
Iteration 72/1000 | Loss: 0.00002261
Iteration 73/1000 | Loss: 0.00002261
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002260
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002259
Iteration 78/1000 | Loss: 0.00002259
Iteration 79/1000 | Loss: 0.00002259
Iteration 80/1000 | Loss: 0.00002259
Iteration 81/1000 | Loss: 0.00002259
Iteration 82/1000 | Loss: 0.00002259
Iteration 83/1000 | Loss: 0.00002259
Iteration 84/1000 | Loss: 0.00002258
Iteration 85/1000 | Loss: 0.00002258
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002257
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002257
Iteration 94/1000 | Loss: 0.00002257
Iteration 95/1000 | Loss: 0.00002257
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002256
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002256
Iteration 103/1000 | Loss: 0.00002256
Iteration 104/1000 | Loss: 0.00002256
Iteration 105/1000 | Loss: 0.00002256
Iteration 106/1000 | Loss: 0.00002256
Iteration 107/1000 | Loss: 0.00002256
Iteration 108/1000 | Loss: 0.00002256
Iteration 109/1000 | Loss: 0.00002256
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002256
Iteration 114/1000 | Loss: 0.00002256
Iteration 115/1000 | Loss: 0.00002256
Iteration 116/1000 | Loss: 0.00002256
Iteration 117/1000 | Loss: 0.00002256
Iteration 118/1000 | Loss: 0.00002256
Iteration 119/1000 | Loss: 0.00002256
Iteration 120/1000 | Loss: 0.00002256
Iteration 121/1000 | Loss: 0.00002256
Iteration 122/1000 | Loss: 0.00002256
Iteration 123/1000 | Loss: 0.00002256
Iteration 124/1000 | Loss: 0.00002256
Iteration 125/1000 | Loss: 0.00002256
Iteration 126/1000 | Loss: 0.00002256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.255565232189838e-05, 2.255565232189838e-05, 2.255565232189838e-05, 2.255565232189838e-05, 2.255565232189838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.255565232189838e-05

Optimization complete. Final v2v error: 3.9845519065856934 mm

Highest mean error: 4.0707197189331055 mm for frame 39

Lowest mean error: 3.904697895050049 mm for frame 97

Saving results

Total time: 36.772189140319824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499835
Iteration 2/25 | Loss: 0.00106138
Iteration 3/25 | Loss: 0.00089721
Iteration 4/25 | Loss: 0.00085434
Iteration 5/25 | Loss: 0.00084460
Iteration 6/25 | Loss: 0.00084158
Iteration 7/25 | Loss: 0.00084058
Iteration 8/25 | Loss: 0.00084052
Iteration 9/25 | Loss: 0.00084052
Iteration 10/25 | Loss: 0.00084052
Iteration 11/25 | Loss: 0.00084052
Iteration 12/25 | Loss: 0.00084052
Iteration 13/25 | Loss: 0.00084052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008405231637880206, 0.0008405231637880206, 0.0008405231637880206, 0.0008405231637880206, 0.0008405231637880206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008405231637880206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81617010
Iteration 2/25 | Loss: 0.00167432
Iteration 3/25 | Loss: 0.00167432
Iteration 4/25 | Loss: 0.00167432
Iteration 5/25 | Loss: 0.00167432
Iteration 6/25 | Loss: 0.00167432
Iteration 7/25 | Loss: 0.00167432
Iteration 8/25 | Loss: 0.00167432
Iteration 9/25 | Loss: 0.00167432
Iteration 10/25 | Loss: 0.00167431
Iteration 11/25 | Loss: 0.00167431
Iteration 12/25 | Loss: 0.00167431
Iteration 13/25 | Loss: 0.00167431
Iteration 14/25 | Loss: 0.00167431
Iteration 15/25 | Loss: 0.00167431
Iteration 16/25 | Loss: 0.00167431
Iteration 17/25 | Loss: 0.00167431
Iteration 18/25 | Loss: 0.00167431
Iteration 19/25 | Loss: 0.00167431
Iteration 20/25 | Loss: 0.00167431
Iteration 21/25 | Loss: 0.00167431
Iteration 22/25 | Loss: 0.00167431
Iteration 23/25 | Loss: 0.00167431
Iteration 24/25 | Loss: 0.00167431
Iteration 25/25 | Loss: 0.00167431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167431
Iteration 2/1000 | Loss: 0.00004932
Iteration 3/1000 | Loss: 0.00003534
Iteration 4/1000 | Loss: 0.00003012
Iteration 5/1000 | Loss: 0.00002804
Iteration 6/1000 | Loss: 0.00002662
Iteration 7/1000 | Loss: 0.00002559
Iteration 8/1000 | Loss: 0.00002496
Iteration 9/1000 | Loss: 0.00002442
Iteration 10/1000 | Loss: 0.00002406
Iteration 11/1000 | Loss: 0.00002379
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002331
Iteration 14/1000 | Loss: 0.00002315
Iteration 15/1000 | Loss: 0.00002305
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002293
Iteration 18/1000 | Loss: 0.00002283
Iteration 19/1000 | Loss: 0.00002280
Iteration 20/1000 | Loss: 0.00002279
Iteration 21/1000 | Loss: 0.00002278
Iteration 22/1000 | Loss: 0.00002277
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00002276
Iteration 25/1000 | Loss: 0.00002275
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002271
Iteration 29/1000 | Loss: 0.00002271
Iteration 30/1000 | Loss: 0.00002270
Iteration 31/1000 | Loss: 0.00002267
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002257
Iteration 34/1000 | Loss: 0.00002257
Iteration 35/1000 | Loss: 0.00002254
Iteration 36/1000 | Loss: 0.00002254
Iteration 37/1000 | Loss: 0.00002254
Iteration 38/1000 | Loss: 0.00002253
Iteration 39/1000 | Loss: 0.00002253
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002251
Iteration 43/1000 | Loss: 0.00002250
Iteration 44/1000 | Loss: 0.00002250
Iteration 45/1000 | Loss: 0.00002250
Iteration 46/1000 | Loss: 0.00002250
Iteration 47/1000 | Loss: 0.00002249
Iteration 48/1000 | Loss: 0.00002249
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002248
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002247
Iteration 53/1000 | Loss: 0.00002247
Iteration 54/1000 | Loss: 0.00002247
Iteration 55/1000 | Loss: 0.00002247
Iteration 56/1000 | Loss: 0.00002247
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002246
Iteration 61/1000 | Loss: 0.00002246
Iteration 62/1000 | Loss: 0.00002246
Iteration 63/1000 | Loss: 0.00002246
Iteration 64/1000 | Loss: 0.00002246
Iteration 65/1000 | Loss: 0.00002246
Iteration 66/1000 | Loss: 0.00002246
Iteration 67/1000 | Loss: 0.00002245
Iteration 68/1000 | Loss: 0.00002245
Iteration 69/1000 | Loss: 0.00002245
Iteration 70/1000 | Loss: 0.00002245
Iteration 71/1000 | Loss: 0.00002245
Iteration 72/1000 | Loss: 0.00002245
Iteration 73/1000 | Loss: 0.00002245
Iteration 74/1000 | Loss: 0.00002245
Iteration 75/1000 | Loss: 0.00002245
Iteration 76/1000 | Loss: 0.00002245
Iteration 77/1000 | Loss: 0.00002245
Iteration 78/1000 | Loss: 0.00002244
Iteration 79/1000 | Loss: 0.00002244
Iteration 80/1000 | Loss: 0.00002244
Iteration 81/1000 | Loss: 0.00002244
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002243
Iteration 85/1000 | Loss: 0.00002243
Iteration 86/1000 | Loss: 0.00002243
Iteration 87/1000 | Loss: 0.00002243
Iteration 88/1000 | Loss: 0.00002243
Iteration 89/1000 | Loss: 0.00002242
Iteration 90/1000 | Loss: 0.00002242
Iteration 91/1000 | Loss: 0.00002242
Iteration 92/1000 | Loss: 0.00002242
Iteration 93/1000 | Loss: 0.00002242
Iteration 94/1000 | Loss: 0.00002242
Iteration 95/1000 | Loss: 0.00002242
Iteration 96/1000 | Loss: 0.00002241
Iteration 97/1000 | Loss: 0.00002241
Iteration 98/1000 | Loss: 0.00002241
Iteration 99/1000 | Loss: 0.00002241
Iteration 100/1000 | Loss: 0.00002241
Iteration 101/1000 | Loss: 0.00002241
Iteration 102/1000 | Loss: 0.00002240
Iteration 103/1000 | Loss: 0.00002240
Iteration 104/1000 | Loss: 0.00002240
Iteration 105/1000 | Loss: 0.00002240
Iteration 106/1000 | Loss: 0.00002240
Iteration 107/1000 | Loss: 0.00002240
Iteration 108/1000 | Loss: 0.00002240
Iteration 109/1000 | Loss: 0.00002240
Iteration 110/1000 | Loss: 0.00002240
Iteration 111/1000 | Loss: 0.00002240
Iteration 112/1000 | Loss: 0.00002239
Iteration 113/1000 | Loss: 0.00002239
Iteration 114/1000 | Loss: 0.00002239
Iteration 115/1000 | Loss: 0.00002239
Iteration 116/1000 | Loss: 0.00002239
Iteration 117/1000 | Loss: 0.00002239
Iteration 118/1000 | Loss: 0.00002239
Iteration 119/1000 | Loss: 0.00002239
Iteration 120/1000 | Loss: 0.00002239
Iteration 121/1000 | Loss: 0.00002239
Iteration 122/1000 | Loss: 0.00002239
Iteration 123/1000 | Loss: 0.00002239
Iteration 124/1000 | Loss: 0.00002239
Iteration 125/1000 | Loss: 0.00002239
Iteration 126/1000 | Loss: 0.00002239
Iteration 127/1000 | Loss: 0.00002238
Iteration 128/1000 | Loss: 0.00002238
Iteration 129/1000 | Loss: 0.00002238
Iteration 130/1000 | Loss: 0.00002238
Iteration 131/1000 | Loss: 0.00002238
Iteration 132/1000 | Loss: 0.00002238
Iteration 133/1000 | Loss: 0.00002238
Iteration 134/1000 | Loss: 0.00002238
Iteration 135/1000 | Loss: 0.00002238
Iteration 136/1000 | Loss: 0.00002238
Iteration 137/1000 | Loss: 0.00002238
Iteration 138/1000 | Loss: 0.00002238
Iteration 139/1000 | Loss: 0.00002238
Iteration 140/1000 | Loss: 0.00002238
Iteration 141/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2379936126526445e-05, 2.2379936126526445e-05, 2.2379936126526445e-05, 2.2379936126526445e-05, 2.2379936126526445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2379936126526445e-05

Optimization complete. Final v2v error: 3.9172520637512207 mm

Highest mean error: 5.1162638664245605 mm for frame 54

Lowest mean error: 3.0434091091156006 mm for frame 239

Saving results

Total time: 51.165595054626465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071141
Iteration 2/25 | Loss: 0.00137541
Iteration 3/25 | Loss: 0.00101206
Iteration 4/25 | Loss: 0.00093970
Iteration 5/25 | Loss: 0.00092957
Iteration 6/25 | Loss: 0.00092711
Iteration 7/25 | Loss: 0.00092660
Iteration 8/25 | Loss: 0.00092660
Iteration 9/25 | Loss: 0.00092660
Iteration 10/25 | Loss: 0.00092660
Iteration 11/25 | Loss: 0.00092660
Iteration 12/25 | Loss: 0.00092660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000926603504922241, 0.000926603504922241, 0.000926603504922241, 0.000926603504922241, 0.000926603504922241]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000926603504922241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80968374
Iteration 2/25 | Loss: 0.00096212
Iteration 3/25 | Loss: 0.00096207
Iteration 4/25 | Loss: 0.00096207
Iteration 5/25 | Loss: 0.00096207
Iteration 6/25 | Loss: 0.00096207
Iteration 7/25 | Loss: 0.00096207
Iteration 8/25 | Loss: 0.00096207
Iteration 9/25 | Loss: 0.00096207
Iteration 10/25 | Loss: 0.00096207
Iteration 11/25 | Loss: 0.00096207
Iteration 12/25 | Loss: 0.00096207
Iteration 13/25 | Loss: 0.00096207
Iteration 14/25 | Loss: 0.00096207
Iteration 15/25 | Loss: 0.00096207
Iteration 16/25 | Loss: 0.00096207
Iteration 17/25 | Loss: 0.00096207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009620663477107882, 0.0009620663477107882, 0.0009620663477107882, 0.0009620663477107882, 0.0009620663477107882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009620663477107882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096207
Iteration 2/1000 | Loss: 0.00005715
Iteration 3/1000 | Loss: 0.00003991
Iteration 4/1000 | Loss: 0.00003448
Iteration 5/1000 | Loss: 0.00003325
Iteration 6/1000 | Loss: 0.00003200
Iteration 7/1000 | Loss: 0.00003125
Iteration 8/1000 | Loss: 0.00003054
Iteration 9/1000 | Loss: 0.00003008
Iteration 10/1000 | Loss: 0.00002957
Iteration 11/1000 | Loss: 0.00002936
Iteration 12/1000 | Loss: 0.00002913
Iteration 13/1000 | Loss: 0.00002896
Iteration 14/1000 | Loss: 0.00002874
Iteration 15/1000 | Loss: 0.00002859
Iteration 16/1000 | Loss: 0.00002854
Iteration 17/1000 | Loss: 0.00002846
Iteration 18/1000 | Loss: 0.00002846
Iteration 19/1000 | Loss: 0.00002846
Iteration 20/1000 | Loss: 0.00002846
Iteration 21/1000 | Loss: 0.00002846
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002845
Iteration 24/1000 | Loss: 0.00002845
Iteration 25/1000 | Loss: 0.00002845
Iteration 26/1000 | Loss: 0.00002845
Iteration 27/1000 | Loss: 0.00002845
Iteration 28/1000 | Loss: 0.00002845
Iteration 29/1000 | Loss: 0.00002845
Iteration 30/1000 | Loss: 0.00002845
Iteration 31/1000 | Loss: 0.00002845
Iteration 32/1000 | Loss: 0.00002844
Iteration 33/1000 | Loss: 0.00002844
Iteration 34/1000 | Loss: 0.00002844
Iteration 35/1000 | Loss: 0.00002844
Iteration 36/1000 | Loss: 0.00002844
Iteration 37/1000 | Loss: 0.00002844
Iteration 38/1000 | Loss: 0.00002844
Iteration 39/1000 | Loss: 0.00002843
Iteration 40/1000 | Loss: 0.00002843
Iteration 41/1000 | Loss: 0.00002843
Iteration 42/1000 | Loss: 0.00002843
Iteration 43/1000 | Loss: 0.00002843
Iteration 44/1000 | Loss: 0.00002843
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00002843
Iteration 47/1000 | Loss: 0.00002842
Iteration 48/1000 | Loss: 0.00002840
Iteration 49/1000 | Loss: 0.00002840
Iteration 50/1000 | Loss: 0.00002839
Iteration 51/1000 | Loss: 0.00002839
Iteration 52/1000 | Loss: 0.00002839
Iteration 53/1000 | Loss: 0.00002837
Iteration 54/1000 | Loss: 0.00002836
Iteration 55/1000 | Loss: 0.00002836
Iteration 56/1000 | Loss: 0.00002835
Iteration 57/1000 | Loss: 0.00002835
Iteration 58/1000 | Loss: 0.00002835
Iteration 59/1000 | Loss: 0.00002835
Iteration 60/1000 | Loss: 0.00002835
Iteration 61/1000 | Loss: 0.00002835
Iteration 62/1000 | Loss: 0.00002835
Iteration 63/1000 | Loss: 0.00002835
Iteration 64/1000 | Loss: 0.00002835
Iteration 65/1000 | Loss: 0.00002835
Iteration 66/1000 | Loss: 0.00002835
Iteration 67/1000 | Loss: 0.00002834
Iteration 68/1000 | Loss: 0.00002834
Iteration 69/1000 | Loss: 0.00002834
Iteration 70/1000 | Loss: 0.00002833
Iteration 71/1000 | Loss: 0.00002833
Iteration 72/1000 | Loss: 0.00002833
Iteration 73/1000 | Loss: 0.00002832
Iteration 74/1000 | Loss: 0.00002832
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002831
Iteration 77/1000 | Loss: 0.00002830
Iteration 78/1000 | Loss: 0.00002830
Iteration 79/1000 | Loss: 0.00002829
Iteration 80/1000 | Loss: 0.00002828
Iteration 81/1000 | Loss: 0.00002827
Iteration 82/1000 | Loss: 0.00002827
Iteration 83/1000 | Loss: 0.00002827
Iteration 84/1000 | Loss: 0.00002826
Iteration 85/1000 | Loss: 0.00002826
Iteration 86/1000 | Loss: 0.00002826
Iteration 87/1000 | Loss: 0.00002826
Iteration 88/1000 | Loss: 0.00002826
Iteration 89/1000 | Loss: 0.00002826
Iteration 90/1000 | Loss: 0.00002826
Iteration 91/1000 | Loss: 0.00002826
Iteration 92/1000 | Loss: 0.00002826
Iteration 93/1000 | Loss: 0.00002826
Iteration 94/1000 | Loss: 0.00002826
Iteration 95/1000 | Loss: 0.00002825
Iteration 96/1000 | Loss: 0.00002825
Iteration 97/1000 | Loss: 0.00002825
Iteration 98/1000 | Loss: 0.00002825
Iteration 99/1000 | Loss: 0.00002824
Iteration 100/1000 | Loss: 0.00002824
Iteration 101/1000 | Loss: 0.00002824
Iteration 102/1000 | Loss: 0.00002824
Iteration 103/1000 | Loss: 0.00002824
Iteration 104/1000 | Loss: 0.00002824
Iteration 105/1000 | Loss: 0.00002823
Iteration 106/1000 | Loss: 0.00002823
Iteration 107/1000 | Loss: 0.00002823
Iteration 108/1000 | Loss: 0.00002823
Iteration 109/1000 | Loss: 0.00002823
Iteration 110/1000 | Loss: 0.00002823
Iteration 111/1000 | Loss: 0.00002822
Iteration 112/1000 | Loss: 0.00002822
Iteration 113/1000 | Loss: 0.00002822
Iteration 114/1000 | Loss: 0.00002822
Iteration 115/1000 | Loss: 0.00002822
Iteration 116/1000 | Loss: 0.00002822
Iteration 117/1000 | Loss: 0.00002822
Iteration 118/1000 | Loss: 0.00002822
Iteration 119/1000 | Loss: 0.00002822
Iteration 120/1000 | Loss: 0.00002822
Iteration 121/1000 | Loss: 0.00002821
Iteration 122/1000 | Loss: 0.00002821
Iteration 123/1000 | Loss: 0.00002821
Iteration 124/1000 | Loss: 0.00002821
Iteration 125/1000 | Loss: 0.00002821
Iteration 126/1000 | Loss: 0.00002821
Iteration 127/1000 | Loss: 0.00002821
Iteration 128/1000 | Loss: 0.00002821
Iteration 129/1000 | Loss: 0.00002821
Iteration 130/1000 | Loss: 0.00002821
Iteration 131/1000 | Loss: 0.00002820
Iteration 132/1000 | Loss: 0.00002820
Iteration 133/1000 | Loss: 0.00002820
Iteration 134/1000 | Loss: 0.00002820
Iteration 135/1000 | Loss: 0.00002820
Iteration 136/1000 | Loss: 0.00002819
Iteration 137/1000 | Loss: 0.00002819
Iteration 138/1000 | Loss: 0.00002819
Iteration 139/1000 | Loss: 0.00002819
Iteration 140/1000 | Loss: 0.00002819
Iteration 141/1000 | Loss: 0.00002819
Iteration 142/1000 | Loss: 0.00002818
Iteration 143/1000 | Loss: 0.00002818
Iteration 144/1000 | Loss: 0.00002818
Iteration 145/1000 | Loss: 0.00002817
Iteration 146/1000 | Loss: 0.00002817
Iteration 147/1000 | Loss: 0.00002817
Iteration 148/1000 | Loss: 0.00002817
Iteration 149/1000 | Loss: 0.00002817
Iteration 150/1000 | Loss: 0.00002817
Iteration 151/1000 | Loss: 0.00002817
Iteration 152/1000 | Loss: 0.00002817
Iteration 153/1000 | Loss: 0.00002816
Iteration 154/1000 | Loss: 0.00002816
Iteration 155/1000 | Loss: 0.00002816
Iteration 156/1000 | Loss: 0.00002816
Iteration 157/1000 | Loss: 0.00002815
Iteration 158/1000 | Loss: 0.00002815
Iteration 159/1000 | Loss: 0.00002815
Iteration 160/1000 | Loss: 0.00002815
Iteration 161/1000 | Loss: 0.00002815
Iteration 162/1000 | Loss: 0.00002815
Iteration 163/1000 | Loss: 0.00002815
Iteration 164/1000 | Loss: 0.00002815
Iteration 165/1000 | Loss: 0.00002815
Iteration 166/1000 | Loss: 0.00002815
Iteration 167/1000 | Loss: 0.00002815
Iteration 168/1000 | Loss: 0.00002815
Iteration 169/1000 | Loss: 0.00002815
Iteration 170/1000 | Loss: 0.00002815
Iteration 171/1000 | Loss: 0.00002815
Iteration 172/1000 | Loss: 0.00002815
Iteration 173/1000 | Loss: 0.00002815
Iteration 174/1000 | Loss: 0.00002815
Iteration 175/1000 | Loss: 0.00002815
Iteration 176/1000 | Loss: 0.00002815
Iteration 177/1000 | Loss: 0.00002815
Iteration 178/1000 | Loss: 0.00002815
Iteration 179/1000 | Loss: 0.00002815
Iteration 180/1000 | Loss: 0.00002815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.8146336262580007e-05, 2.8146336262580007e-05, 2.8146336262580007e-05, 2.8146336262580007e-05, 2.8146336262580007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8146336262580007e-05

Optimization complete. Final v2v error: 4.356302261352539 mm

Highest mean error: 5.516849994659424 mm for frame 1

Lowest mean error: 3.9963879585266113 mm for frame 148

Saving results

Total time: 43.67019581794739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097822
Iteration 2/25 | Loss: 0.01097822
Iteration 3/25 | Loss: 0.01097822
Iteration 4/25 | Loss: 0.00254335
Iteration 5/25 | Loss: 0.00158133
Iteration 6/25 | Loss: 0.00123743
Iteration 7/25 | Loss: 0.00114637
Iteration 8/25 | Loss: 0.00115407
Iteration 9/25 | Loss: 0.00110995
Iteration 10/25 | Loss: 0.00100306
Iteration 11/25 | Loss: 0.00090287
Iteration 12/25 | Loss: 0.00084588
Iteration 13/25 | Loss: 0.00082255
Iteration 14/25 | Loss: 0.00080617
Iteration 15/25 | Loss: 0.00079000
Iteration 16/25 | Loss: 0.00079153
Iteration 17/25 | Loss: 0.00078793
Iteration 18/25 | Loss: 0.00078828
Iteration 19/25 | Loss: 0.00079026
Iteration 20/25 | Loss: 0.00079752
Iteration 21/25 | Loss: 0.00079676
Iteration 22/25 | Loss: 0.00078391
Iteration 23/25 | Loss: 0.00078245
Iteration 24/25 | Loss: 0.00078182
Iteration 25/25 | Loss: 0.00078110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.94350576
Iteration 2/25 | Loss: 0.00149575
Iteration 3/25 | Loss: 0.00152575
Iteration 4/25 | Loss: 0.00152575
Iteration 5/25 | Loss: 0.00152574
Iteration 6/25 | Loss: 0.00152574
Iteration 7/25 | Loss: 0.00152574
Iteration 8/25 | Loss: 0.00152574
Iteration 9/25 | Loss: 0.00152574
Iteration 10/25 | Loss: 0.00152574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015257447957992554, 0.0015257447957992554, 0.0015257447957992554, 0.0015257447957992554, 0.0015257447957992554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015257447957992554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152574
Iteration 2/1000 | Loss: 0.00009653
Iteration 3/1000 | Loss: 0.00009299
Iteration 4/1000 | Loss: 0.00009057
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00006839
Iteration 7/1000 | Loss: 0.00003403
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002317
Iteration 10/1000 | Loss: 0.00080654
Iteration 11/1000 | Loss: 0.00014754
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00006854
Iteration 14/1000 | Loss: 0.00003246
Iteration 15/1000 | Loss: 0.00007632
Iteration 16/1000 | Loss: 0.00021107
Iteration 17/1000 | Loss: 0.00016070
Iteration 18/1000 | Loss: 0.00002952
Iteration 19/1000 | Loss: 0.00003379
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00003409
Iteration 22/1000 | Loss: 0.00001832
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00002084
Iteration 25/1000 | Loss: 0.00005571
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00003408
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001786
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001784
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001768
Iteration 37/1000 | Loss: 0.00001768
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001767
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001767
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001766
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001766
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001765
Iteration 49/1000 | Loss: 0.00001765
Iteration 50/1000 | Loss: 0.00001765
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001758
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001757
Iteration 74/1000 | Loss: 0.00001757
Iteration 75/1000 | Loss: 0.00003440
Iteration 76/1000 | Loss: 0.00011610
Iteration 77/1000 | Loss: 0.00058908
Iteration 78/1000 | Loss: 0.00120241
Iteration 79/1000 | Loss: 0.00010548
Iteration 80/1000 | Loss: 0.00003837
Iteration 81/1000 | Loss: 0.00002483
Iteration 82/1000 | Loss: 0.00009692
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001760
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001759
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001753
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001752
Iteration 107/1000 | Loss: 0.00001752
Iteration 108/1000 | Loss: 0.00001752
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001752
Iteration 112/1000 | Loss: 0.00001752
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001752
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001751
Iteration 122/1000 | Loss: 0.00001751
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001750
Iteration 127/1000 | Loss: 0.00001750
Iteration 128/1000 | Loss: 0.00001750
Iteration 129/1000 | Loss: 0.00001750
Iteration 130/1000 | Loss: 0.00001750
Iteration 131/1000 | Loss: 0.00001750
Iteration 132/1000 | Loss: 0.00001750
Iteration 133/1000 | Loss: 0.00001750
Iteration 134/1000 | Loss: 0.00001750
Iteration 135/1000 | Loss: 0.00001750
Iteration 136/1000 | Loss: 0.00001750
Iteration 137/1000 | Loss: 0.00001750
Iteration 138/1000 | Loss: 0.00001750
Iteration 139/1000 | Loss: 0.00001750
Iteration 140/1000 | Loss: 0.00001750
Iteration 141/1000 | Loss: 0.00001750
Iteration 142/1000 | Loss: 0.00001750
Iteration 143/1000 | Loss: 0.00001750
Iteration 144/1000 | Loss: 0.00001750
Iteration 145/1000 | Loss: 0.00001750
Iteration 146/1000 | Loss: 0.00001750
Iteration 147/1000 | Loss: 0.00001750
Iteration 148/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.7501282854937017e-05, 1.7501282854937017e-05, 1.7501282854937017e-05, 1.7501282854937017e-05, 1.7501282854937017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7501282854937017e-05

Optimization complete. Final v2v error: 3.504788875579834 mm

Highest mean error: 4.406502723693848 mm for frame 130

Lowest mean error: 3.150376796722412 mm for frame 105

Saving results

Total time: 107.9819130897522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420059
Iteration 2/25 | Loss: 0.00096307
Iteration 3/25 | Loss: 0.00086366
Iteration 4/25 | Loss: 0.00083766
Iteration 5/25 | Loss: 0.00083254
Iteration 6/25 | Loss: 0.00083131
Iteration 7/25 | Loss: 0.00083103
Iteration 8/25 | Loss: 0.00083103
Iteration 9/25 | Loss: 0.00083103
Iteration 10/25 | Loss: 0.00083103
Iteration 11/25 | Loss: 0.00083103
Iteration 12/25 | Loss: 0.00083103
Iteration 13/25 | Loss: 0.00083103
Iteration 14/25 | Loss: 0.00083103
Iteration 15/25 | Loss: 0.00083103
Iteration 16/25 | Loss: 0.00083103
Iteration 17/25 | Loss: 0.00083103
Iteration 18/25 | Loss: 0.00083103
Iteration 19/25 | Loss: 0.00083103
Iteration 20/25 | Loss: 0.00083103
Iteration 21/25 | Loss: 0.00083103
Iteration 22/25 | Loss: 0.00083103
Iteration 23/25 | Loss: 0.00083103
Iteration 24/25 | Loss: 0.00083103
Iteration 25/25 | Loss: 0.00083103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59522402
Iteration 2/25 | Loss: 0.00129554
Iteration 3/25 | Loss: 0.00129554
Iteration 4/25 | Loss: 0.00129554
Iteration 5/25 | Loss: 0.00129554
Iteration 6/25 | Loss: 0.00129554
Iteration 7/25 | Loss: 0.00129554
Iteration 8/25 | Loss: 0.00129554
Iteration 9/25 | Loss: 0.00129554
Iteration 10/25 | Loss: 0.00129554
Iteration 11/25 | Loss: 0.00129554
Iteration 12/25 | Loss: 0.00129554
Iteration 13/25 | Loss: 0.00129554
Iteration 14/25 | Loss: 0.00129554
Iteration 15/25 | Loss: 0.00129554
Iteration 16/25 | Loss: 0.00129554
Iteration 17/25 | Loss: 0.00129554
Iteration 18/25 | Loss: 0.00129554
Iteration 19/25 | Loss: 0.00129554
Iteration 20/25 | Loss: 0.00129554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012955374550074339, 0.0012955374550074339, 0.0012955374550074339, 0.0012955374550074339, 0.0012955374550074339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012955374550074339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129554
Iteration 2/1000 | Loss: 0.00005570
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00002998
Iteration 5/1000 | Loss: 0.00002854
Iteration 6/1000 | Loss: 0.00002764
Iteration 7/1000 | Loss: 0.00002694
Iteration 8/1000 | Loss: 0.00002637
Iteration 9/1000 | Loss: 0.00002602
Iteration 10/1000 | Loss: 0.00002576
Iteration 11/1000 | Loss: 0.00002559
Iteration 12/1000 | Loss: 0.00002543
Iteration 13/1000 | Loss: 0.00002532
Iteration 14/1000 | Loss: 0.00002529
Iteration 15/1000 | Loss: 0.00002527
Iteration 16/1000 | Loss: 0.00002527
Iteration 17/1000 | Loss: 0.00002525
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002521
Iteration 20/1000 | Loss: 0.00002521
Iteration 21/1000 | Loss: 0.00002517
Iteration 22/1000 | Loss: 0.00002515
Iteration 23/1000 | Loss: 0.00002509
Iteration 24/1000 | Loss: 0.00002507
Iteration 25/1000 | Loss: 0.00002502
Iteration 26/1000 | Loss: 0.00002500
Iteration 27/1000 | Loss: 0.00002499
Iteration 28/1000 | Loss: 0.00002498
Iteration 29/1000 | Loss: 0.00002496
Iteration 30/1000 | Loss: 0.00002496
Iteration 31/1000 | Loss: 0.00002496
Iteration 32/1000 | Loss: 0.00002496
Iteration 33/1000 | Loss: 0.00002496
Iteration 34/1000 | Loss: 0.00002496
Iteration 35/1000 | Loss: 0.00002496
Iteration 36/1000 | Loss: 0.00002496
Iteration 37/1000 | Loss: 0.00002496
Iteration 38/1000 | Loss: 0.00002496
Iteration 39/1000 | Loss: 0.00002496
Iteration 40/1000 | Loss: 0.00002495
Iteration 41/1000 | Loss: 0.00002495
Iteration 42/1000 | Loss: 0.00002494
Iteration 43/1000 | Loss: 0.00002494
Iteration 44/1000 | Loss: 0.00002494
Iteration 45/1000 | Loss: 0.00002494
Iteration 46/1000 | Loss: 0.00002493
Iteration 47/1000 | Loss: 0.00002493
Iteration 48/1000 | Loss: 0.00002493
Iteration 49/1000 | Loss: 0.00002493
Iteration 50/1000 | Loss: 0.00002493
Iteration 51/1000 | Loss: 0.00002492
Iteration 52/1000 | Loss: 0.00002492
Iteration 53/1000 | Loss: 0.00002492
Iteration 54/1000 | Loss: 0.00002491
Iteration 55/1000 | Loss: 0.00002491
Iteration 56/1000 | Loss: 0.00002491
Iteration 57/1000 | Loss: 0.00002490
Iteration 58/1000 | Loss: 0.00002490
Iteration 59/1000 | Loss: 0.00002490
Iteration 60/1000 | Loss: 0.00002490
Iteration 61/1000 | Loss: 0.00002490
Iteration 62/1000 | Loss: 0.00002490
Iteration 63/1000 | Loss: 0.00002490
Iteration 64/1000 | Loss: 0.00002489
Iteration 65/1000 | Loss: 0.00002489
Iteration 66/1000 | Loss: 0.00002489
Iteration 67/1000 | Loss: 0.00002489
Iteration 68/1000 | Loss: 0.00002489
Iteration 69/1000 | Loss: 0.00002489
Iteration 70/1000 | Loss: 0.00002488
Iteration 71/1000 | Loss: 0.00002488
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002488
Iteration 74/1000 | Loss: 0.00002487
Iteration 75/1000 | Loss: 0.00002487
Iteration 76/1000 | Loss: 0.00002487
Iteration 77/1000 | Loss: 0.00002487
Iteration 78/1000 | Loss: 0.00002487
Iteration 79/1000 | Loss: 0.00002487
Iteration 80/1000 | Loss: 0.00002487
Iteration 81/1000 | Loss: 0.00002487
Iteration 82/1000 | Loss: 0.00002486
Iteration 83/1000 | Loss: 0.00002486
Iteration 84/1000 | Loss: 0.00002486
Iteration 85/1000 | Loss: 0.00002486
Iteration 86/1000 | Loss: 0.00002486
Iteration 87/1000 | Loss: 0.00002486
Iteration 88/1000 | Loss: 0.00002485
Iteration 89/1000 | Loss: 0.00002485
Iteration 90/1000 | Loss: 0.00002485
Iteration 91/1000 | Loss: 0.00002485
Iteration 92/1000 | Loss: 0.00002485
Iteration 93/1000 | Loss: 0.00002485
Iteration 94/1000 | Loss: 0.00002485
Iteration 95/1000 | Loss: 0.00002485
Iteration 96/1000 | Loss: 0.00002485
Iteration 97/1000 | Loss: 0.00002485
Iteration 98/1000 | Loss: 0.00002485
Iteration 99/1000 | Loss: 0.00002485
Iteration 100/1000 | Loss: 0.00002484
Iteration 101/1000 | Loss: 0.00002484
Iteration 102/1000 | Loss: 0.00002484
Iteration 103/1000 | Loss: 0.00002484
Iteration 104/1000 | Loss: 0.00002484
Iteration 105/1000 | Loss: 0.00002484
Iteration 106/1000 | Loss: 0.00002484
Iteration 107/1000 | Loss: 0.00002483
Iteration 108/1000 | Loss: 0.00002483
Iteration 109/1000 | Loss: 0.00002483
Iteration 110/1000 | Loss: 0.00002483
Iteration 111/1000 | Loss: 0.00002483
Iteration 112/1000 | Loss: 0.00002483
Iteration 113/1000 | Loss: 0.00002483
Iteration 114/1000 | Loss: 0.00002483
Iteration 115/1000 | Loss: 0.00002483
Iteration 116/1000 | Loss: 0.00002483
Iteration 117/1000 | Loss: 0.00002483
Iteration 118/1000 | Loss: 0.00002483
Iteration 119/1000 | Loss: 0.00002483
Iteration 120/1000 | Loss: 0.00002483
Iteration 121/1000 | Loss: 0.00002483
Iteration 122/1000 | Loss: 0.00002483
Iteration 123/1000 | Loss: 0.00002483
Iteration 124/1000 | Loss: 0.00002483
Iteration 125/1000 | Loss: 0.00002483
Iteration 126/1000 | Loss: 0.00002483
Iteration 127/1000 | Loss: 0.00002483
Iteration 128/1000 | Loss: 0.00002483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.4828295863699168e-05, 2.4828295863699168e-05, 2.4828295863699168e-05, 2.4828295863699168e-05, 2.4828295863699168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4828295863699168e-05

Optimization complete. Final v2v error: 4.139948844909668 mm

Highest mean error: 4.408021926879883 mm for frame 26

Lowest mean error: 3.6720826625823975 mm for frame 46

Saving results

Total time: 37.47014236450195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029471
Iteration 2/25 | Loss: 0.00389693
Iteration 3/25 | Loss: 0.00235872
Iteration 4/25 | Loss: 0.00211759
Iteration 5/25 | Loss: 0.00173575
Iteration 6/25 | Loss: 0.00156277
Iteration 7/25 | Loss: 0.00146178
Iteration 8/25 | Loss: 0.00141918
Iteration 9/25 | Loss: 0.00139428
Iteration 10/25 | Loss: 0.00137137
Iteration 11/25 | Loss: 0.00135167
Iteration 12/25 | Loss: 0.00133411
Iteration 13/25 | Loss: 0.00129743
Iteration 14/25 | Loss: 0.00125775
Iteration 15/25 | Loss: 0.00124947
Iteration 16/25 | Loss: 0.00121572
Iteration 17/25 | Loss: 0.00121186
Iteration 18/25 | Loss: 0.00120987
Iteration 19/25 | Loss: 0.00120587
Iteration 20/25 | Loss: 0.00119664
Iteration 21/25 | Loss: 0.00118583
Iteration 22/25 | Loss: 0.00119013
Iteration 23/25 | Loss: 0.00121286
Iteration 24/25 | Loss: 0.00115004
Iteration 25/25 | Loss: 0.00111443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64218581
Iteration 2/25 | Loss: 0.00613303
Iteration 3/25 | Loss: 0.00462466
Iteration 4/25 | Loss: 0.00462465
Iteration 5/25 | Loss: 0.00462465
Iteration 6/25 | Loss: 0.00462465
Iteration 7/25 | Loss: 0.00462465
Iteration 8/25 | Loss: 0.00462465
Iteration 9/25 | Loss: 0.00462465
Iteration 10/25 | Loss: 0.00462465
Iteration 11/25 | Loss: 0.00462465
Iteration 12/25 | Loss: 0.00462465
Iteration 13/25 | Loss: 0.00462465
Iteration 14/25 | Loss: 0.00462465
Iteration 15/25 | Loss: 0.00462465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004624651279300451, 0.004624651279300451, 0.004624651279300451, 0.004624651279300451, 0.004624651279300451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004624651279300451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00462465
Iteration 2/1000 | Loss: 0.01020102
Iteration 3/1000 | Loss: 0.00297236
Iteration 4/1000 | Loss: 0.00107803
Iteration 5/1000 | Loss: 0.00187809
Iteration 6/1000 | Loss: 0.00239608
Iteration 7/1000 | Loss: 0.00124614
Iteration 8/1000 | Loss: 0.00149533
Iteration 9/1000 | Loss: 0.00185957
Iteration 10/1000 | Loss: 0.00064106
Iteration 11/1000 | Loss: 0.00107874
Iteration 12/1000 | Loss: 0.00140211
Iteration 13/1000 | Loss: 0.00082761
Iteration 14/1000 | Loss: 0.00132853
Iteration 15/1000 | Loss: 0.00076575
Iteration 16/1000 | Loss: 0.00068780
Iteration 17/1000 | Loss: 0.00080696
Iteration 18/1000 | Loss: 0.00042274
Iteration 19/1000 | Loss: 0.00086972
Iteration 20/1000 | Loss: 0.00088604
Iteration 21/1000 | Loss: 0.00075886
Iteration 22/1000 | Loss: 0.00145713
Iteration 23/1000 | Loss: 0.00105622
Iteration 24/1000 | Loss: 0.00092226
Iteration 25/1000 | Loss: 0.00121869
Iteration 26/1000 | Loss: 0.00056955
Iteration 27/1000 | Loss: 0.00049882
Iteration 28/1000 | Loss: 0.00046745
Iteration 29/1000 | Loss: 0.00166640
Iteration 30/1000 | Loss: 0.00073162
Iteration 31/1000 | Loss: 0.00049111
Iteration 32/1000 | Loss: 0.00088066
Iteration 33/1000 | Loss: 0.00122028
Iteration 34/1000 | Loss: 0.00101524
Iteration 35/1000 | Loss: 0.00101240
Iteration 36/1000 | Loss: 0.00098020
Iteration 37/1000 | Loss: 0.00043083
Iteration 38/1000 | Loss: 0.00042641
Iteration 39/1000 | Loss: 0.00061905
Iteration 40/1000 | Loss: 0.00082819
Iteration 41/1000 | Loss: 0.00053346
Iteration 42/1000 | Loss: 0.00068656
Iteration 43/1000 | Loss: 0.00026247
Iteration 44/1000 | Loss: 0.00025056
Iteration 45/1000 | Loss: 0.00115014
Iteration 46/1000 | Loss: 0.00027903
Iteration 47/1000 | Loss: 0.00049180
Iteration 48/1000 | Loss: 0.00136793
Iteration 49/1000 | Loss: 0.00073938
Iteration 50/1000 | Loss: 0.00098791
Iteration 51/1000 | Loss: 0.00051434
Iteration 52/1000 | Loss: 0.00100007
Iteration 53/1000 | Loss: 0.00065882
Iteration 54/1000 | Loss: 0.00034916
Iteration 55/1000 | Loss: 0.00052071
Iteration 56/1000 | Loss: 0.00065669
Iteration 57/1000 | Loss: 0.00075163
Iteration 58/1000 | Loss: 0.00067843
Iteration 59/1000 | Loss: 0.00090543
Iteration 60/1000 | Loss: 0.00070500
Iteration 61/1000 | Loss: 0.00035787
Iteration 62/1000 | Loss: 0.00045827
Iteration 63/1000 | Loss: 0.00046513
Iteration 64/1000 | Loss: 0.00035914
Iteration 65/1000 | Loss: 0.00039974
Iteration 66/1000 | Loss: 0.00040326
Iteration 67/1000 | Loss: 0.00038049
Iteration 68/1000 | Loss: 0.00037477
Iteration 69/1000 | Loss: 0.00060836
Iteration 70/1000 | Loss: 0.00067450
Iteration 71/1000 | Loss: 0.00058044
Iteration 72/1000 | Loss: 0.00045985
Iteration 73/1000 | Loss: 0.00041766
Iteration 74/1000 | Loss: 0.00070139
Iteration 75/1000 | Loss: 0.00087351
Iteration 76/1000 | Loss: 0.00036928
Iteration 77/1000 | Loss: 0.00020535
Iteration 78/1000 | Loss: 0.00013993
Iteration 79/1000 | Loss: 0.00008944
Iteration 80/1000 | Loss: 0.00029194
Iteration 81/1000 | Loss: 0.00042511
Iteration 82/1000 | Loss: 0.00028745
Iteration 83/1000 | Loss: 0.00056385
Iteration 84/1000 | Loss: 0.00033740
Iteration 85/1000 | Loss: 0.00065452
Iteration 86/1000 | Loss: 0.00080559
Iteration 87/1000 | Loss: 0.00057658
Iteration 88/1000 | Loss: 0.00025852
Iteration 89/1000 | Loss: 0.00024930
Iteration 90/1000 | Loss: 0.00026580
Iteration 91/1000 | Loss: 0.00052843
Iteration 92/1000 | Loss: 0.00078992
Iteration 93/1000 | Loss: 0.00065382
Iteration 94/1000 | Loss: 0.00048712
Iteration 95/1000 | Loss: 0.00049247
Iteration 96/1000 | Loss: 0.00075346
Iteration 97/1000 | Loss: 0.00080121
Iteration 98/1000 | Loss: 0.00080360
Iteration 99/1000 | Loss: 0.00062072
Iteration 100/1000 | Loss: 0.00089252
Iteration 101/1000 | Loss: 0.00010598
Iteration 102/1000 | Loss: 0.00023876
Iteration 103/1000 | Loss: 0.00024297
Iteration 104/1000 | Loss: 0.00085936
Iteration 105/1000 | Loss: 0.00176294
Iteration 106/1000 | Loss: 0.00143614
Iteration 107/1000 | Loss: 0.00052665
Iteration 108/1000 | Loss: 0.00125818
Iteration 109/1000 | Loss: 0.00075427
Iteration 110/1000 | Loss: 0.00036910
Iteration 111/1000 | Loss: 0.00017068
Iteration 112/1000 | Loss: 0.00040905
Iteration 113/1000 | Loss: 0.00014117
Iteration 114/1000 | Loss: 0.00017958
Iteration 115/1000 | Loss: 0.00015943
Iteration 116/1000 | Loss: 0.00039739
Iteration 117/1000 | Loss: 0.00026318
Iteration 118/1000 | Loss: 0.00009019
Iteration 119/1000 | Loss: 0.00021991
Iteration 120/1000 | Loss: 0.00022597
Iteration 121/1000 | Loss: 0.00033603
Iteration 122/1000 | Loss: 0.00015324
Iteration 123/1000 | Loss: 0.00037906
Iteration 124/1000 | Loss: 0.00074549
Iteration 125/1000 | Loss: 0.00029897
Iteration 126/1000 | Loss: 0.00065912
Iteration 127/1000 | Loss: 0.00041631
Iteration 128/1000 | Loss: 0.00080051
Iteration 129/1000 | Loss: 0.00036740
Iteration 130/1000 | Loss: 0.00016178
Iteration 131/1000 | Loss: 0.00018322
Iteration 132/1000 | Loss: 0.00034344
Iteration 133/1000 | Loss: 0.00018802
Iteration 134/1000 | Loss: 0.00023231
Iteration 135/1000 | Loss: 0.00019746
Iteration 136/1000 | Loss: 0.00058868
Iteration 137/1000 | Loss: 0.00030636
Iteration 138/1000 | Loss: 0.00020163
Iteration 139/1000 | Loss: 0.00019991
Iteration 140/1000 | Loss: 0.00019682
Iteration 141/1000 | Loss: 0.00059076
Iteration 142/1000 | Loss: 0.00029272
Iteration 143/1000 | Loss: 0.00019699
Iteration 144/1000 | Loss: 0.00018347
Iteration 145/1000 | Loss: 0.00012666
Iteration 146/1000 | Loss: 0.00009688
Iteration 147/1000 | Loss: 0.00007770
Iteration 148/1000 | Loss: 0.00010052
Iteration 149/1000 | Loss: 0.00034690
Iteration 150/1000 | Loss: 0.00018710
Iteration 151/1000 | Loss: 0.00074377
Iteration 152/1000 | Loss: 0.00005826
Iteration 153/1000 | Loss: 0.00042369
Iteration 154/1000 | Loss: 0.00006648
Iteration 155/1000 | Loss: 0.00006279
Iteration 156/1000 | Loss: 0.00007247
Iteration 157/1000 | Loss: 0.00017463
Iteration 158/1000 | Loss: 0.00007113
Iteration 159/1000 | Loss: 0.00030910
Iteration 160/1000 | Loss: 0.00019370
Iteration 161/1000 | Loss: 0.00008085
Iteration 162/1000 | Loss: 0.00007032
Iteration 163/1000 | Loss: 0.00016526
Iteration 164/1000 | Loss: 0.00014054
Iteration 165/1000 | Loss: 0.00016470
Iteration 166/1000 | Loss: 0.00008053
Iteration 167/1000 | Loss: 0.00028749
Iteration 168/1000 | Loss: 0.00014272
Iteration 169/1000 | Loss: 0.00020911
Iteration 170/1000 | Loss: 0.00012419
Iteration 171/1000 | Loss: 0.00062274
Iteration 172/1000 | Loss: 0.00021371
Iteration 173/1000 | Loss: 0.00024006
Iteration 174/1000 | Loss: 0.00032595
Iteration 175/1000 | Loss: 0.00022301
Iteration 176/1000 | Loss: 0.00008528
Iteration 177/1000 | Loss: 0.00015386
Iteration 178/1000 | Loss: 0.00017703
Iteration 179/1000 | Loss: 0.00007382
Iteration 180/1000 | Loss: 0.00063946
Iteration 181/1000 | Loss: 0.00039729
Iteration 182/1000 | Loss: 0.00028547
Iteration 183/1000 | Loss: 0.00020447
Iteration 184/1000 | Loss: 0.00043714
Iteration 185/1000 | Loss: 0.00031728
Iteration 186/1000 | Loss: 0.00022245
Iteration 187/1000 | Loss: 0.00036898
Iteration 188/1000 | Loss: 0.00020916
Iteration 189/1000 | Loss: 0.00021623
Iteration 190/1000 | Loss: 0.00026529
Iteration 191/1000 | Loss: 0.00032297
Iteration 192/1000 | Loss: 0.00023285
Iteration 193/1000 | Loss: 0.00017019
Iteration 194/1000 | Loss: 0.00011109
Iteration 195/1000 | Loss: 0.00011837
Iteration 196/1000 | Loss: 0.00009445
Iteration 197/1000 | Loss: 0.00011764
Iteration 198/1000 | Loss: 0.00018210
Iteration 199/1000 | Loss: 0.00016895
Iteration 200/1000 | Loss: 0.00018312
Iteration 201/1000 | Loss: 0.00016195
Iteration 202/1000 | Loss: 0.00017256
Iteration 203/1000 | Loss: 0.00043771
Iteration 204/1000 | Loss: 0.00017781
Iteration 205/1000 | Loss: 0.00017472
Iteration 206/1000 | Loss: 0.00020998
Iteration 207/1000 | Loss: 0.00016951
Iteration 208/1000 | Loss: 0.00021194
Iteration 209/1000 | Loss: 0.00016389
Iteration 210/1000 | Loss: 0.00017241
Iteration 211/1000 | Loss: 0.00016162
Iteration 212/1000 | Loss: 0.00017001
Iteration 213/1000 | Loss: 0.00016651
Iteration 214/1000 | Loss: 0.00017186
Iteration 215/1000 | Loss: 0.00016971
Iteration 216/1000 | Loss: 0.00070829
Iteration 217/1000 | Loss: 0.00016358
Iteration 218/1000 | Loss: 0.00046660
Iteration 219/1000 | Loss: 0.00016402
Iteration 220/1000 | Loss: 0.00036185
Iteration 221/1000 | Loss: 0.00018171
Iteration 222/1000 | Loss: 0.00024339
Iteration 223/1000 | Loss: 0.00020714
Iteration 224/1000 | Loss: 0.00017752
Iteration 225/1000 | Loss: 0.00040222
Iteration 226/1000 | Loss: 0.00007454
Iteration 227/1000 | Loss: 0.00028475
Iteration 228/1000 | Loss: 0.00006094
Iteration 229/1000 | Loss: 0.00027034
Iteration 230/1000 | Loss: 0.00030844
Iteration 231/1000 | Loss: 0.00016835
Iteration 232/1000 | Loss: 0.00005478
Iteration 233/1000 | Loss: 0.00014318
Iteration 234/1000 | Loss: 0.00012530
Iteration 235/1000 | Loss: 0.00011137
Iteration 236/1000 | Loss: 0.00009646
Iteration 237/1000 | Loss: 0.00005145
Iteration 238/1000 | Loss: 0.00009647
Iteration 239/1000 | Loss: 0.00005566
Iteration 240/1000 | Loss: 0.00009364
Iteration 241/1000 | Loss: 0.00029487
Iteration 242/1000 | Loss: 0.00006939
Iteration 243/1000 | Loss: 0.00010184
Iteration 244/1000 | Loss: 0.00007671
Iteration 245/1000 | Loss: 0.00009420
Iteration 246/1000 | Loss: 0.00008743
Iteration 247/1000 | Loss: 0.00007701
Iteration 248/1000 | Loss: 0.00012444
Iteration 249/1000 | Loss: 0.00007918
Iteration 250/1000 | Loss: 0.00006877
Iteration 251/1000 | Loss: 0.00006426
Iteration 252/1000 | Loss: 0.00007045
Iteration 253/1000 | Loss: 0.00017856
Iteration 254/1000 | Loss: 0.00007152
Iteration 255/1000 | Loss: 0.00016115
Iteration 256/1000 | Loss: 0.00006946
Iteration 257/1000 | Loss: 0.00004985
Iteration 258/1000 | Loss: 0.00006134
Iteration 259/1000 | Loss: 0.00006792
Iteration 260/1000 | Loss: 0.00006435
Iteration 261/1000 | Loss: 0.00009429
Iteration 262/1000 | Loss: 0.00006040
Iteration 263/1000 | Loss: 0.00035540
Iteration 264/1000 | Loss: 0.00006831
Iteration 265/1000 | Loss: 0.00006444
Iteration 266/1000 | Loss: 0.00006663
Iteration 267/1000 | Loss: 0.00047884
Iteration 268/1000 | Loss: 0.00023004
Iteration 269/1000 | Loss: 0.00016136
Iteration 270/1000 | Loss: 0.00018484
Iteration 271/1000 | Loss: 0.00008409
Iteration 272/1000 | Loss: 0.00008261
Iteration 273/1000 | Loss: 0.00010026
Iteration 274/1000 | Loss: 0.00010391
Iteration 275/1000 | Loss: 0.00006499
Iteration 276/1000 | Loss: 0.00006620
Iteration 277/1000 | Loss: 0.00006052
Iteration 278/1000 | Loss: 0.00005871
Iteration 279/1000 | Loss: 0.00006273
Iteration 280/1000 | Loss: 0.00006344
Iteration 281/1000 | Loss: 0.00015706
Iteration 282/1000 | Loss: 0.00011934
Iteration 283/1000 | Loss: 0.00006430
Iteration 284/1000 | Loss: 0.00006418
Iteration 285/1000 | Loss: 0.00006150
Iteration 286/1000 | Loss: 0.00006017
Iteration 287/1000 | Loss: 0.00021068
Iteration 288/1000 | Loss: 0.00011939
Iteration 289/1000 | Loss: 0.00018490
Iteration 290/1000 | Loss: 0.00010427
Iteration 291/1000 | Loss: 0.00016694
Iteration 292/1000 | Loss: 0.00011860
Iteration 293/1000 | Loss: 0.00013459
Iteration 294/1000 | Loss: 0.00007188
Iteration 295/1000 | Loss: 0.00006526
Iteration 296/1000 | Loss: 0.00006403
Iteration 297/1000 | Loss: 0.00005660
Iteration 298/1000 | Loss: 0.00006510
Iteration 299/1000 | Loss: 0.00008982
Iteration 300/1000 | Loss: 0.00008550
Iteration 301/1000 | Loss: 0.00005849
Iteration 302/1000 | Loss: 0.00006404
Iteration 303/1000 | Loss: 0.00006500
Iteration 304/1000 | Loss: 0.00009176
Iteration 305/1000 | Loss: 0.00006892
Iteration 306/1000 | Loss: 0.00006448
Iteration 307/1000 | Loss: 0.00006666
Iteration 308/1000 | Loss: 0.00007616
Iteration 309/1000 | Loss: 0.00007031
Iteration 310/1000 | Loss: 0.00006503
Iteration 311/1000 | Loss: 0.00006597
Iteration 312/1000 | Loss: 0.00006433
Iteration 313/1000 | Loss: 0.00006559
Iteration 314/1000 | Loss: 0.00006710
Iteration 315/1000 | Loss: 0.00006528
Iteration 316/1000 | Loss: 0.00020801
Iteration 317/1000 | Loss: 0.00006339
Iteration 318/1000 | Loss: 0.00006289
Iteration 319/1000 | Loss: 0.00006320
Iteration 320/1000 | Loss: 0.00005984
Iteration 321/1000 | Loss: 0.00022226
Iteration 322/1000 | Loss: 0.00006323
Iteration 323/1000 | Loss: 0.00006443
Iteration 324/1000 | Loss: 0.00006472
Iteration 325/1000 | Loss: 0.00006298
Iteration 326/1000 | Loss: 0.00006591
Iteration 327/1000 | Loss: 0.00010075
Iteration 328/1000 | Loss: 0.00006912
Iteration 329/1000 | Loss: 0.00013486
Iteration 330/1000 | Loss: 0.00007438
Iteration 331/1000 | Loss: 0.00006602
Iteration 332/1000 | Loss: 0.00006019
Iteration 333/1000 | Loss: 0.00006400
Iteration 334/1000 | Loss: 0.00006400
Iteration 335/1000 | Loss: 0.00006218
Iteration 336/1000 | Loss: 0.00006399
Iteration 337/1000 | Loss: 0.00011995
Iteration 338/1000 | Loss: 0.00006992
Iteration 339/1000 | Loss: 0.00008112
Iteration 340/1000 | Loss: 0.00006377
Iteration 341/1000 | Loss: 0.00006344
Iteration 342/1000 | Loss: 0.00006539
Iteration 343/1000 | Loss: 0.00006200
Iteration 344/1000 | Loss: 0.00006790
Iteration 345/1000 | Loss: 0.00006183
Iteration 346/1000 | Loss: 0.00007978
Iteration 347/1000 | Loss: 0.00006184
Iteration 348/1000 | Loss: 0.00007439
Iteration 349/1000 | Loss: 0.00006296
Iteration 350/1000 | Loss: 0.00006497
Iteration 351/1000 | Loss: 0.00006250
Iteration 352/1000 | Loss: 0.00006420
Iteration 353/1000 | Loss: 0.00006175
Iteration 354/1000 | Loss: 0.00006400
Iteration 355/1000 | Loss: 0.00006182
Iteration 356/1000 | Loss: 0.00006527
Iteration 357/1000 | Loss: 0.00015035
Iteration 358/1000 | Loss: 0.00007589
Iteration 359/1000 | Loss: 0.00006830
Iteration 360/1000 | Loss: 0.00007142
Iteration 361/1000 | Loss: 0.00005757
Iteration 362/1000 | Loss: 0.00005556
Iteration 363/1000 | Loss: 0.00006083
Iteration 364/1000 | Loss: 0.00006367
Iteration 365/1000 | Loss: 0.00006403
Iteration 366/1000 | Loss: 0.00005881
Iteration 367/1000 | Loss: 0.00006156
Iteration 368/1000 | Loss: 0.00006304
Iteration 369/1000 | Loss: 0.00008872
Iteration 370/1000 | Loss: 0.00006405
Iteration 371/1000 | Loss: 0.00006478
Iteration 372/1000 | Loss: 0.00008560
Iteration 373/1000 | Loss: 0.00006464
Iteration 374/1000 | Loss: 0.00006310
Iteration 375/1000 | Loss: 0.00006420
Iteration 376/1000 | Loss: 0.00006284
Iteration 377/1000 | Loss: 0.00006454
Iteration 378/1000 | Loss: 0.00009571
Iteration 379/1000 | Loss: 0.00007074
Iteration 380/1000 | Loss: 0.00007102
Iteration 381/1000 | Loss: 0.00006474
Iteration 382/1000 | Loss: 0.00006453
Iteration 383/1000 | Loss: 0.00006480
Iteration 384/1000 | Loss: 0.00006258
Iteration 385/1000 | Loss: 0.00006283
Iteration 386/1000 | Loss: 0.00007413
Iteration 387/1000 | Loss: 0.00019938
Iteration 388/1000 | Loss: 0.00006912
Iteration 389/1000 | Loss: 0.00006544
Iteration 390/1000 | Loss: 0.00008353
Iteration 391/1000 | Loss: 0.00007171
Iteration 392/1000 | Loss: 0.00006386
Iteration 393/1000 | Loss: 0.00006430
Iteration 394/1000 | Loss: 0.00006331
Iteration 395/1000 | Loss: 0.00007183
Iteration 396/1000 | Loss: 0.00006376
Iteration 397/1000 | Loss: 0.00006469
Iteration 398/1000 | Loss: 0.00006681
Iteration 399/1000 | Loss: 0.00018485
Iteration 400/1000 | Loss: 0.00008333
Iteration 401/1000 | Loss: 0.00006465
Iteration 402/1000 | Loss: 0.00013990
Iteration 403/1000 | Loss: 0.00008516
Iteration 404/1000 | Loss: 0.00006505
Iteration 405/1000 | Loss: 0.00006492
Iteration 406/1000 | Loss: 0.00008692
Iteration 407/1000 | Loss: 0.00007627
Iteration 408/1000 | Loss: 0.00006687
Iteration 409/1000 | Loss: 0.00006651
Iteration 410/1000 | Loss: 0.00006937
Iteration 411/1000 | Loss: 0.00006480
Iteration 412/1000 | Loss: 0.00012190
Iteration 413/1000 | Loss: 0.00006746
Iteration 414/1000 | Loss: 0.00011994
Iteration 415/1000 | Loss: 0.00016103
Iteration 416/1000 | Loss: 0.00010615
Iteration 417/1000 | Loss: 0.00008351
Iteration 418/1000 | Loss: 0.00016042
Iteration 419/1000 | Loss: 0.00007359
Iteration 420/1000 | Loss: 0.00006782
Iteration 421/1000 | Loss: 0.00007697
Iteration 422/1000 | Loss: 0.00006379
Iteration 423/1000 | Loss: 0.00006628
Iteration 424/1000 | Loss: 0.00006853
Iteration 425/1000 | Loss: 0.00006641
Iteration 426/1000 | Loss: 0.00022174
Iteration 427/1000 | Loss: 0.00006976
Iteration 428/1000 | Loss: 0.00006668
Iteration 429/1000 | Loss: 0.00008431
Iteration 430/1000 | Loss: 0.00009816
Iteration 431/1000 | Loss: 0.00006428
Iteration 432/1000 | Loss: 0.00008446
Iteration 433/1000 | Loss: 0.00006394
Iteration 434/1000 | Loss: 0.00006595
Iteration 435/1000 | Loss: 0.00008115
Iteration 436/1000 | Loss: 0.00006378
Iteration 437/1000 | Loss: 0.00006395
Iteration 438/1000 | Loss: 0.00006447
Iteration 439/1000 | Loss: 0.00006346
Iteration 440/1000 | Loss: 0.00008625
Iteration 441/1000 | Loss: 0.00014172
Iteration 442/1000 | Loss: 0.00014770
Iteration 443/1000 | Loss: 0.00006285
Iteration 444/1000 | Loss: 0.00006495
Iteration 445/1000 | Loss: 0.00006293
Iteration 446/1000 | Loss: 0.00006761
Iteration 447/1000 | Loss: 0.00008607
Iteration 448/1000 | Loss: 0.00007177
Iteration 449/1000 | Loss: 0.00006281
Iteration 450/1000 | Loss: 0.00008546
Iteration 451/1000 | Loss: 0.00007717
Iteration 452/1000 | Loss: 0.00006832
Iteration 453/1000 | Loss: 0.00006388
Iteration 454/1000 | Loss: 0.00006448
Iteration 455/1000 | Loss: 0.00006356
Iteration 456/1000 | Loss: 0.00006518
Iteration 457/1000 | Loss: 0.00006649
Iteration 458/1000 | Loss: 0.00006409
Iteration 459/1000 | Loss: 0.00006397
Iteration 460/1000 | Loss: 0.00006366
Iteration 461/1000 | Loss: 0.00006297
Iteration 462/1000 | Loss: 0.00006331
Iteration 463/1000 | Loss: 0.00006585
Iteration 464/1000 | Loss: 0.00007245
Iteration 465/1000 | Loss: 0.00006460
Iteration 466/1000 | Loss: 0.00006398
Iteration 467/1000 | Loss: 0.00006258
Iteration 468/1000 | Loss: 0.00006346
Iteration 469/1000 | Loss: 0.00006503
Iteration 470/1000 | Loss: 0.00006335
Iteration 471/1000 | Loss: 0.00006470
Iteration 472/1000 | Loss: 0.00006713
Iteration 473/1000 | Loss: 0.00006920
Iteration 474/1000 | Loss: 0.00012875
Iteration 475/1000 | Loss: 0.00009339
Iteration 476/1000 | Loss: 0.00006515
Iteration 477/1000 | Loss: 0.00009736
Iteration 478/1000 | Loss: 0.00007017
Iteration 479/1000 | Loss: 0.00005638
Iteration 480/1000 | Loss: 0.00006640
Iteration 481/1000 | Loss: 0.00006497
Iteration 482/1000 | Loss: 0.00006365
Iteration 483/1000 | Loss: 0.00009294
Iteration 484/1000 | Loss: 0.00007508
Iteration 485/1000 | Loss: 0.00005434
Iteration 486/1000 | Loss: 0.00005131
Iteration 487/1000 | Loss: 0.00007274
Iteration 488/1000 | Loss: 0.00006262
Iteration 489/1000 | Loss: 0.00006551
Iteration 490/1000 | Loss: 0.00012364
Iteration 491/1000 | Loss: 0.00008103
Iteration 492/1000 | Loss: 0.00006842
Iteration 493/1000 | Loss: 0.00007204
Iteration 494/1000 | Loss: 0.00005233
Iteration 495/1000 | Loss: 0.00007116
Iteration 496/1000 | Loss: 0.00009172
Iteration 497/1000 | Loss: 0.00005552
Iteration 498/1000 | Loss: 0.00006626
Iteration 499/1000 | Loss: 0.00006813
Iteration 500/1000 | Loss: 0.00005992
Iteration 501/1000 | Loss: 0.00005792
Iteration 502/1000 | Loss: 0.00006271
Iteration 503/1000 | Loss: 0.00006963
Iteration 504/1000 | Loss: 0.00005382
Iteration 505/1000 | Loss: 0.00006039
Iteration 506/1000 | Loss: 0.00006271
Iteration 507/1000 | Loss: 0.00006812
Iteration 508/1000 | Loss: 0.00006388
Iteration 509/1000 | Loss: 0.00006363
Iteration 510/1000 | Loss: 0.00006205
Iteration 511/1000 | Loss: 0.00005767
Iteration 512/1000 | Loss: 0.00005108
Iteration 513/1000 | Loss: 0.00006526
Iteration 514/1000 | Loss: 0.00006335
Iteration 515/1000 | Loss: 0.00006086
Iteration 516/1000 | Loss: 0.00009559
Iteration 517/1000 | Loss: 0.00007334
Iteration 518/1000 | Loss: 0.00008505
Iteration 519/1000 | Loss: 0.00007330
Iteration 520/1000 | Loss: 0.00006244
Iteration 521/1000 | Loss: 0.00006154
Iteration 522/1000 | Loss: 0.00006128
Iteration 523/1000 | Loss: 0.00007118
Iteration 524/1000 | Loss: 0.00006931
Iteration 525/1000 | Loss: 0.00006203
Iteration 526/1000 | Loss: 0.00006806
Iteration 527/1000 | Loss: 0.00005209
Iteration 528/1000 | Loss: 0.00004922
Iteration 529/1000 | Loss: 0.00004694
Iteration 530/1000 | Loss: 0.00006389
Iteration 531/1000 | Loss: 0.00009501
Iteration 532/1000 | Loss: 0.00004740
Iteration 533/1000 | Loss: 0.00008057
Iteration 534/1000 | Loss: 0.00006293
Iteration 535/1000 | Loss: 0.00006302
Iteration 536/1000 | Loss: 0.00005806
Iteration 537/1000 | Loss: 0.00008016
Iteration 538/1000 | Loss: 0.00005174
Iteration 539/1000 | Loss: 0.00004452
Iteration 540/1000 | Loss: 0.00004392
Iteration 541/1000 | Loss: 0.00004373
Iteration 542/1000 | Loss: 0.00004372
Iteration 543/1000 | Loss: 0.00004367
Iteration 544/1000 | Loss: 0.00004365
Iteration 545/1000 | Loss: 0.00004364
Iteration 546/1000 | Loss: 0.00005865
Iteration 547/1000 | Loss: 0.00004367
Iteration 548/1000 | Loss: 0.00004361
Iteration 549/1000 | Loss: 0.00004361
Iteration 550/1000 | Loss: 0.00004360
Iteration 551/1000 | Loss: 0.00004638
Iteration 552/1000 | Loss: 0.00004360
Iteration 553/1000 | Loss: 0.00004358
Iteration 554/1000 | Loss: 0.00004358
Iteration 555/1000 | Loss: 0.00024602
Iteration 556/1000 | Loss: 0.00020836
Iteration 557/1000 | Loss: 0.00023762
Iteration 558/1000 | Loss: 0.00012769
Iteration 559/1000 | Loss: 0.00009433
Iteration 560/1000 | Loss: 0.00014939
Iteration 561/1000 | Loss: 0.00004444
Iteration 562/1000 | Loss: 0.00004383
Iteration 563/1000 | Loss: 0.00004373
Iteration 564/1000 | Loss: 0.00004355
Iteration 565/1000 | Loss: 0.00004354
Iteration 566/1000 | Loss: 0.00004354
Iteration 567/1000 | Loss: 0.00004353
Iteration 568/1000 | Loss: 0.00004353
Iteration 569/1000 | Loss: 0.00004352
Iteration 570/1000 | Loss: 0.00025103
Iteration 571/1000 | Loss: 0.00013348
Iteration 572/1000 | Loss: 0.00006139
Iteration 573/1000 | Loss: 0.00006951
Iteration 574/1000 | Loss: 0.00050958
Iteration 575/1000 | Loss: 0.00020567
Iteration 576/1000 | Loss: 0.00007218
Iteration 577/1000 | Loss: 0.00006097
Iteration 578/1000 | Loss: 0.00007529
Iteration 579/1000 | Loss: 0.00005730
Iteration 580/1000 | Loss: 0.00004426
Iteration 581/1000 | Loss: 0.00005148
Iteration 582/1000 | Loss: 0.00005239
Iteration 583/1000 | Loss: 0.00004373
Iteration 584/1000 | Loss: 0.00007296
Iteration 585/1000 | Loss: 0.00004305
Iteration 586/1000 | Loss: 0.00005720
Iteration 587/1000 | Loss: 0.00004269
Iteration 588/1000 | Loss: 0.00004252
Iteration 589/1000 | Loss: 0.00004234
Iteration 590/1000 | Loss: 0.00016507
Iteration 591/1000 | Loss: 0.00007661
Iteration 592/1000 | Loss: 0.00004227
Iteration 593/1000 | Loss: 0.00006520
Iteration 594/1000 | Loss: 0.00004230
Iteration 595/1000 | Loss: 0.00004212
Iteration 596/1000 | Loss: 0.00004204
Iteration 597/1000 | Loss: 0.00004204
Iteration 598/1000 | Loss: 0.00004203
Iteration 599/1000 | Loss: 0.00004203
Iteration 600/1000 | Loss: 0.00004202
Iteration 601/1000 | Loss: 0.00004202
Iteration 602/1000 | Loss: 0.00004202
Iteration 603/1000 | Loss: 0.00004201
Iteration 604/1000 | Loss: 0.00004201
Iteration 605/1000 | Loss: 0.00004201
Iteration 606/1000 | Loss: 0.00004201
Iteration 607/1000 | Loss: 0.00004201
Iteration 608/1000 | Loss: 0.00004200
Iteration 609/1000 | Loss: 0.00004200
Iteration 610/1000 | Loss: 0.00004200
Iteration 611/1000 | Loss: 0.00004199
Iteration 612/1000 | Loss: 0.00004199
Iteration 613/1000 | Loss: 0.00004199
Iteration 614/1000 | Loss: 0.00004199
Iteration 615/1000 | Loss: 0.00004198
Iteration 616/1000 | Loss: 0.00004198
Iteration 617/1000 | Loss: 0.00004198
Iteration 618/1000 | Loss: 0.00004197
Iteration 619/1000 | Loss: 0.00004197
Iteration 620/1000 | Loss: 0.00004197
Iteration 621/1000 | Loss: 0.00004197
Iteration 622/1000 | Loss: 0.00004197
Iteration 623/1000 | Loss: 0.00004197
Iteration 624/1000 | Loss: 0.00004197
Iteration 625/1000 | Loss: 0.00004196
Iteration 626/1000 | Loss: 0.00004196
Iteration 627/1000 | Loss: 0.00004196
Iteration 628/1000 | Loss: 0.00004196
Iteration 629/1000 | Loss: 0.00004196
Iteration 630/1000 | Loss: 0.00004196
Iteration 631/1000 | Loss: 0.00004196
Iteration 632/1000 | Loss: 0.00004195
Iteration 633/1000 | Loss: 0.00008403
Iteration 634/1000 | Loss: 0.00004252
Iteration 635/1000 | Loss: 0.00004204
Iteration 636/1000 | Loss: 0.00005108
Iteration 637/1000 | Loss: 0.00004198
Iteration 638/1000 | Loss: 0.00004197
Iteration 639/1000 | Loss: 0.00004197
Iteration 640/1000 | Loss: 0.00004197
Iteration 641/1000 | Loss: 0.00004197
Iteration 642/1000 | Loss: 0.00004196
Iteration 643/1000 | Loss: 0.00004196
Iteration 644/1000 | Loss: 0.00004195
Iteration 645/1000 | Loss: 0.00004195
Iteration 646/1000 | Loss: 0.00004195
Iteration 647/1000 | Loss: 0.00004194
Iteration 648/1000 | Loss: 0.00004194
Iteration 649/1000 | Loss: 0.00004194
Iteration 650/1000 | Loss: 0.00004193
Iteration 651/1000 | Loss: 0.00004193
Iteration 652/1000 | Loss: 0.00004193
Iteration 653/1000 | Loss: 0.00004193
Iteration 654/1000 | Loss: 0.00004192
Iteration 655/1000 | Loss: 0.00004192
Iteration 656/1000 | Loss: 0.00004192
Iteration 657/1000 | Loss: 0.00004192
Iteration 658/1000 | Loss: 0.00004192
Iteration 659/1000 | Loss: 0.00004192
Iteration 660/1000 | Loss: 0.00004192
Iteration 661/1000 | Loss: 0.00004192
Iteration 662/1000 | Loss: 0.00004192
Iteration 663/1000 | Loss: 0.00004191
Iteration 664/1000 | Loss: 0.00004191
Iteration 665/1000 | Loss: 0.00004191
Iteration 666/1000 | Loss: 0.00004191
Iteration 667/1000 | Loss: 0.00004191
Iteration 668/1000 | Loss: 0.00004191
Iteration 669/1000 | Loss: 0.00004190
Iteration 670/1000 | Loss: 0.00004190
Iteration 671/1000 | Loss: 0.00004190
Iteration 672/1000 | Loss: 0.00004190
Iteration 673/1000 | Loss: 0.00004190
Iteration 674/1000 | Loss: 0.00004189
Iteration 675/1000 | Loss: 0.00004189
Iteration 676/1000 | Loss: 0.00004189
Iteration 677/1000 | Loss: 0.00004189
Iteration 678/1000 | Loss: 0.00004188
Iteration 679/1000 | Loss: 0.00004188
Iteration 680/1000 | Loss: 0.00004188
Iteration 681/1000 | Loss: 0.00004188
Iteration 682/1000 | Loss: 0.00004188
Iteration 683/1000 | Loss: 0.00004188
Iteration 684/1000 | Loss: 0.00004188
Iteration 685/1000 | Loss: 0.00004187
Iteration 686/1000 | Loss: 0.00004187
Iteration 687/1000 | Loss: 0.00004187
Iteration 688/1000 | Loss: 0.00004187
Iteration 689/1000 | Loss: 0.00004186
Iteration 690/1000 | Loss: 0.00004186
Iteration 691/1000 | Loss: 0.00004186
Iteration 692/1000 | Loss: 0.00004186
Iteration 693/1000 | Loss: 0.00004186
Iteration 694/1000 | Loss: 0.00004186
Iteration 695/1000 | Loss: 0.00004186
Iteration 696/1000 | Loss: 0.00004185
Iteration 697/1000 | Loss: 0.00004185
Iteration 698/1000 | Loss: 0.00004185
Iteration 699/1000 | Loss: 0.00004185
Iteration 700/1000 | Loss: 0.00004185
Iteration 701/1000 | Loss: 0.00004185
Iteration 702/1000 | Loss: 0.00004184
Iteration 703/1000 | Loss: 0.00004184
Iteration 704/1000 | Loss: 0.00004184
Iteration 705/1000 | Loss: 0.00004183
Iteration 706/1000 | Loss: 0.00004183
Iteration 707/1000 | Loss: 0.00004183
Iteration 708/1000 | Loss: 0.00004183
Iteration 709/1000 | Loss: 0.00004183
Iteration 710/1000 | Loss: 0.00004183
Iteration 711/1000 | Loss: 0.00004183
Iteration 712/1000 | Loss: 0.00004183
Iteration 713/1000 | Loss: 0.00004183
Iteration 714/1000 | Loss: 0.00004182
Iteration 715/1000 | Loss: 0.00004182
Iteration 716/1000 | Loss: 0.00004182
Iteration 717/1000 | Loss: 0.00004182
Iteration 718/1000 | Loss: 0.00004182
Iteration 719/1000 | Loss: 0.00004182
Iteration 720/1000 | Loss: 0.00004182
Iteration 721/1000 | Loss: 0.00004182
Iteration 722/1000 | Loss: 0.00004181
Iteration 723/1000 | Loss: 0.00004181
Iteration 724/1000 | Loss: 0.00004181
Iteration 725/1000 | Loss: 0.00004181
Iteration 726/1000 | Loss: 0.00004181
Iteration 727/1000 | Loss: 0.00004181
Iteration 728/1000 | Loss: 0.00004181
Iteration 729/1000 | Loss: 0.00004181
Iteration 730/1000 | Loss: 0.00004181
Iteration 731/1000 | Loss: 0.00004181
Iteration 732/1000 | Loss: 0.00004180
Iteration 733/1000 | Loss: 0.00004180
Iteration 734/1000 | Loss: 0.00004180
Iteration 735/1000 | Loss: 0.00004180
Iteration 736/1000 | Loss: 0.00004180
Iteration 737/1000 | Loss: 0.00004180
Iteration 738/1000 | Loss: 0.00004180
Iteration 739/1000 | Loss: 0.00004180
Iteration 740/1000 | Loss: 0.00004180
Iteration 741/1000 | Loss: 0.00004179
Iteration 742/1000 | Loss: 0.00004179
Iteration 743/1000 | Loss: 0.00004179
Iteration 744/1000 | Loss: 0.00004179
Iteration 745/1000 | Loss: 0.00004179
Iteration 746/1000 | Loss: 0.00004179
Iteration 747/1000 | Loss: 0.00004179
Iteration 748/1000 | Loss: 0.00004179
Iteration 749/1000 | Loss: 0.00004179
Iteration 750/1000 | Loss: 0.00004179
Iteration 751/1000 | Loss: 0.00004179
Iteration 752/1000 | Loss: 0.00004179
Iteration 753/1000 | Loss: 0.00004179
Iteration 754/1000 | Loss: 0.00004179
Iteration 755/1000 | Loss: 0.00004179
Iteration 756/1000 | Loss: 0.00004179
Iteration 757/1000 | Loss: 0.00004179
Iteration 758/1000 | Loss: 0.00004179
Iteration 759/1000 | Loss: 0.00004179
Iteration 760/1000 | Loss: 0.00004178
Iteration 761/1000 | Loss: 0.00004178
Iteration 762/1000 | Loss: 0.00004178
Iteration 763/1000 | Loss: 0.00004178
Iteration 764/1000 | Loss: 0.00004178
Iteration 765/1000 | Loss: 0.00004178
Iteration 766/1000 | Loss: 0.00004178
Iteration 767/1000 | Loss: 0.00004178
Iteration 768/1000 | Loss: 0.00004178
Iteration 769/1000 | Loss: 0.00004178
Iteration 770/1000 | Loss: 0.00004178
Iteration 771/1000 | Loss: 0.00004178
Iteration 772/1000 | Loss: 0.00004178
Iteration 773/1000 | Loss: 0.00004178
Iteration 774/1000 | Loss: 0.00004178
Iteration 775/1000 | Loss: 0.00004178
Iteration 776/1000 | Loss: 0.00004178
Iteration 777/1000 | Loss: 0.00004178
Iteration 778/1000 | Loss: 0.00004178
Iteration 779/1000 | Loss: 0.00004178
Iteration 780/1000 | Loss: 0.00004178
Iteration 781/1000 | Loss: 0.00004178
Iteration 782/1000 | Loss: 0.00004178
Iteration 783/1000 | Loss: 0.00004178
Iteration 784/1000 | Loss: 0.00004178
Iteration 785/1000 | Loss: 0.00004178
Iteration 786/1000 | Loss: 0.00004178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 786. Stopping optimization.
Last 5 losses: [4.1781157051445916e-05, 4.1781157051445916e-05, 4.1781157051445916e-05, 4.1781157051445916e-05, 4.1781157051445916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1781157051445916e-05

Optimization complete. Final v2v error: 4.507203102111816 mm

Highest mean error: 13.290009498596191 mm for frame 80

Lowest mean error: 3.6183347702026367 mm for frame 216

Saving results

Total time: 973.4716958999634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432646
Iteration 2/25 | Loss: 0.00115378
Iteration 3/25 | Loss: 0.00083604
Iteration 4/25 | Loss: 0.00080362
Iteration 5/25 | Loss: 0.00079774
Iteration 6/25 | Loss: 0.00079547
Iteration 7/25 | Loss: 0.00079465
Iteration 8/25 | Loss: 0.00079458
Iteration 9/25 | Loss: 0.00079458
Iteration 10/25 | Loss: 0.00079458
Iteration 11/25 | Loss: 0.00079458
Iteration 12/25 | Loss: 0.00079458
Iteration 13/25 | Loss: 0.00079458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007945799734443426, 0.0007945799734443426, 0.0007945799734443426, 0.0007945799734443426, 0.0007945799734443426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007945799734443426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.04179668
Iteration 2/25 | Loss: 0.00130471
Iteration 3/25 | Loss: 0.00130471
Iteration 4/25 | Loss: 0.00130471
Iteration 5/25 | Loss: 0.00130471
Iteration 6/25 | Loss: 0.00130471
Iteration 7/25 | Loss: 0.00130471
Iteration 8/25 | Loss: 0.00130471
Iteration 9/25 | Loss: 0.00130471
Iteration 10/25 | Loss: 0.00130471
Iteration 11/25 | Loss: 0.00130471
Iteration 12/25 | Loss: 0.00130471
Iteration 13/25 | Loss: 0.00130471
Iteration 14/25 | Loss: 0.00130471
Iteration 15/25 | Loss: 0.00130471
Iteration 16/25 | Loss: 0.00130471
Iteration 17/25 | Loss: 0.00130471
Iteration 18/25 | Loss: 0.00130471
Iteration 19/25 | Loss: 0.00130471
Iteration 20/25 | Loss: 0.00130471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013047104002907872, 0.0013047104002907872, 0.0013047104002907872, 0.0013047104002907872, 0.0013047104002907872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013047104002907872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130471
Iteration 2/1000 | Loss: 0.00002720
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001355
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001321
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001310
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001309
Iteration 40/1000 | Loss: 0.00001309
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001306
Iteration 47/1000 | Loss: 0.00001306
Iteration 48/1000 | Loss: 0.00001306
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001301
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001301
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001300
Iteration 85/1000 | Loss: 0.00001300
Iteration 86/1000 | Loss: 0.00001300
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001295
Iteration 120/1000 | Loss: 0.00001295
Iteration 121/1000 | Loss: 0.00001295
Iteration 122/1000 | Loss: 0.00001295
Iteration 123/1000 | Loss: 0.00001295
Iteration 124/1000 | Loss: 0.00001295
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001293
Iteration 146/1000 | Loss: 0.00001293
Iteration 147/1000 | Loss: 0.00001293
Iteration 148/1000 | Loss: 0.00001293
Iteration 149/1000 | Loss: 0.00001293
Iteration 150/1000 | Loss: 0.00001293
Iteration 151/1000 | Loss: 0.00001293
Iteration 152/1000 | Loss: 0.00001293
Iteration 153/1000 | Loss: 0.00001293
Iteration 154/1000 | Loss: 0.00001293
Iteration 155/1000 | Loss: 0.00001293
Iteration 156/1000 | Loss: 0.00001293
Iteration 157/1000 | Loss: 0.00001293
Iteration 158/1000 | Loss: 0.00001293
Iteration 159/1000 | Loss: 0.00001293
Iteration 160/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.2927086572744884e-05, 1.2927086572744884e-05, 1.2927086572744884e-05, 1.2927086572744884e-05, 1.2927086572744884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2927086572744884e-05

Optimization complete. Final v2v error: 3.065767765045166 mm

Highest mean error: 3.5430452823638916 mm for frame 85

Lowest mean error: 2.8605754375457764 mm for frame 5

Saving results

Total time: 36.33098316192627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500408
Iteration 2/25 | Loss: 0.00091115
Iteration 3/25 | Loss: 0.00077579
Iteration 4/25 | Loss: 0.00074038
Iteration 5/25 | Loss: 0.00073438
Iteration 6/25 | Loss: 0.00073313
Iteration 7/25 | Loss: 0.00073309
Iteration 8/25 | Loss: 0.00073309
Iteration 9/25 | Loss: 0.00073309
Iteration 10/25 | Loss: 0.00073309
Iteration 11/25 | Loss: 0.00073309
Iteration 12/25 | Loss: 0.00073309
Iteration 13/25 | Loss: 0.00073309
Iteration 14/25 | Loss: 0.00073309
Iteration 15/25 | Loss: 0.00073309
Iteration 16/25 | Loss: 0.00073309
Iteration 17/25 | Loss: 0.00073309
Iteration 18/25 | Loss: 0.00073309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000733092543669045, 0.000733092543669045, 0.000733092543669045, 0.000733092543669045, 0.000733092543669045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000733092543669045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59209752
Iteration 2/25 | Loss: 0.00106849
Iteration 3/25 | Loss: 0.00106844
Iteration 4/25 | Loss: 0.00106844
Iteration 5/25 | Loss: 0.00106844
Iteration 6/25 | Loss: 0.00106844
Iteration 7/25 | Loss: 0.00106844
Iteration 8/25 | Loss: 0.00106844
Iteration 9/25 | Loss: 0.00106844
Iteration 10/25 | Loss: 0.00106844
Iteration 11/25 | Loss: 0.00106844
Iteration 12/25 | Loss: 0.00106844
Iteration 13/25 | Loss: 0.00106844
Iteration 14/25 | Loss: 0.00106844
Iteration 15/25 | Loss: 0.00106844
Iteration 16/25 | Loss: 0.00106844
Iteration 17/25 | Loss: 0.00106844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010684388689696789, 0.0010684388689696789, 0.0010684388689696789, 0.0010684388689696789, 0.0010684388689696789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010684388689696789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106844
Iteration 2/1000 | Loss: 0.00002356
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001371
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001265
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001247
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001237
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001233
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001212
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001212
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.2117938240407966e-05, 1.2117938240407966e-05, 1.2117938240407966e-05, 1.2117938240407966e-05, 1.2117938240407966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2117938240407966e-05

Optimization complete. Final v2v error: 2.959235906600952 mm

Highest mean error: 3.390773057937622 mm for frame 27

Lowest mean error: 2.637129306793213 mm for frame 57

Saving results

Total time: 29.824384689331055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031552
Iteration 2/25 | Loss: 0.00645574
Iteration 3/25 | Loss: 0.00278505
Iteration 4/25 | Loss: 0.00213037
Iteration 5/25 | Loss: 0.00192207
Iteration 6/25 | Loss: 0.00167081
Iteration 7/25 | Loss: 0.00155317
Iteration 8/25 | Loss: 0.00140899
Iteration 9/25 | Loss: 0.00132013
Iteration 10/25 | Loss: 0.00129375
Iteration 11/25 | Loss: 0.00126935
Iteration 12/25 | Loss: 0.00123887
Iteration 13/25 | Loss: 0.00119885
Iteration 14/25 | Loss: 0.00118131
Iteration 15/25 | Loss: 0.00117000
Iteration 16/25 | Loss: 0.00116063
Iteration 17/25 | Loss: 0.00116471
Iteration 18/25 | Loss: 0.00117169
Iteration 19/25 | Loss: 0.00116261
Iteration 20/25 | Loss: 0.00116450
Iteration 21/25 | Loss: 0.00115306
Iteration 22/25 | Loss: 0.00115397
Iteration 23/25 | Loss: 0.00114586
Iteration 24/25 | Loss: 0.00113452
Iteration 25/25 | Loss: 0.00112968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63258362
Iteration 2/25 | Loss: 0.00456285
Iteration 3/25 | Loss: 0.00415503
Iteration 4/25 | Loss: 0.00415502
Iteration 5/25 | Loss: 0.00415502
Iteration 6/25 | Loss: 0.00415502
Iteration 7/25 | Loss: 0.00415502
Iteration 8/25 | Loss: 0.00415502
Iteration 9/25 | Loss: 0.00415502
Iteration 10/25 | Loss: 0.00415502
Iteration 11/25 | Loss: 0.00415502
Iteration 12/25 | Loss: 0.00415502
Iteration 13/25 | Loss: 0.00415502
Iteration 14/25 | Loss: 0.00415502
Iteration 15/25 | Loss: 0.00415502
Iteration 16/25 | Loss: 0.00415502
Iteration 17/25 | Loss: 0.00415502
Iteration 18/25 | Loss: 0.00415502
Iteration 19/25 | Loss: 0.00415502
Iteration 20/25 | Loss: 0.00415502
Iteration 21/25 | Loss: 0.00415502
Iteration 22/25 | Loss: 0.00415502
Iteration 23/25 | Loss: 0.00415502
Iteration 24/25 | Loss: 0.00415502
Iteration 25/25 | Loss: 0.00415502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004155022092163563, 0.004155022092163563, 0.004155022092163563, 0.004155022092163563, 0.004155022092163563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004155022092163563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00415502
Iteration 2/1000 | Loss: 0.00197490
Iteration 3/1000 | Loss: 0.00343194
Iteration 4/1000 | Loss: 0.00334124
Iteration 5/1000 | Loss: 0.00092439
Iteration 6/1000 | Loss: 0.00052121
Iteration 7/1000 | Loss: 0.00193968
Iteration 8/1000 | Loss: 0.00049163
Iteration 9/1000 | Loss: 0.00051380
Iteration 10/1000 | Loss: 0.00081709
Iteration 11/1000 | Loss: 0.00039546
Iteration 12/1000 | Loss: 0.00025394
Iteration 13/1000 | Loss: 0.00035617
Iteration 14/1000 | Loss: 0.00032837
Iteration 15/1000 | Loss: 0.00233622
Iteration 16/1000 | Loss: 0.00048951
Iteration 17/1000 | Loss: 0.00190037
Iteration 18/1000 | Loss: 0.00065120
Iteration 19/1000 | Loss: 0.00105319
Iteration 20/1000 | Loss: 0.00222660
Iteration 21/1000 | Loss: 0.00262123
Iteration 22/1000 | Loss: 0.00647129
Iteration 23/1000 | Loss: 0.00279440
Iteration 24/1000 | Loss: 0.00071928
Iteration 25/1000 | Loss: 0.00064214
Iteration 26/1000 | Loss: 0.00063672
Iteration 27/1000 | Loss: 0.00033686
Iteration 28/1000 | Loss: 0.00194564
Iteration 29/1000 | Loss: 0.00029726
Iteration 30/1000 | Loss: 0.00095255
Iteration 31/1000 | Loss: 0.00027546
Iteration 32/1000 | Loss: 0.00012654
Iteration 33/1000 | Loss: 0.00049220
Iteration 34/1000 | Loss: 0.00075647
Iteration 35/1000 | Loss: 0.00108276
Iteration 36/1000 | Loss: 0.00105840
Iteration 37/1000 | Loss: 0.00024960
Iteration 38/1000 | Loss: 0.00061665
Iteration 39/1000 | Loss: 0.00004636
Iteration 40/1000 | Loss: 0.00006118
Iteration 41/1000 | Loss: 0.00018973
Iteration 42/1000 | Loss: 0.00061836
Iteration 43/1000 | Loss: 0.00006416
Iteration 44/1000 | Loss: 0.00040208
Iteration 45/1000 | Loss: 0.00039714
Iteration 46/1000 | Loss: 0.00053366
Iteration 47/1000 | Loss: 0.00014851
Iteration 48/1000 | Loss: 0.00005055
Iteration 49/1000 | Loss: 0.00018561
Iteration 50/1000 | Loss: 0.00006616
Iteration 51/1000 | Loss: 0.00009334
Iteration 52/1000 | Loss: 0.00010298
Iteration 53/1000 | Loss: 0.00056074
Iteration 54/1000 | Loss: 0.00012996
Iteration 55/1000 | Loss: 0.00007596
Iteration 56/1000 | Loss: 0.00020292
Iteration 57/1000 | Loss: 0.00007254
Iteration 58/1000 | Loss: 0.00011435
Iteration 59/1000 | Loss: 0.00006194
Iteration 60/1000 | Loss: 0.00006718
Iteration 61/1000 | Loss: 0.00005115
Iteration 62/1000 | Loss: 0.00002836
Iteration 63/1000 | Loss: 0.00004444
Iteration 64/1000 | Loss: 0.00012340
Iteration 65/1000 | Loss: 0.00002813
Iteration 66/1000 | Loss: 0.00007322
Iteration 67/1000 | Loss: 0.00002491
Iteration 68/1000 | Loss: 0.00012438
Iteration 69/1000 | Loss: 0.00003392
Iteration 70/1000 | Loss: 0.00002623
Iteration 71/1000 | Loss: 0.00011849
Iteration 72/1000 | Loss: 0.00002276
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00004423
Iteration 75/1000 | Loss: 0.00004296
Iteration 76/1000 | Loss: 0.00005496
Iteration 77/1000 | Loss: 0.00002609
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00009836
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001968
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00002038
Iteration 86/1000 | Loss: 0.00019322
Iteration 87/1000 | Loss: 0.00002428
Iteration 88/1000 | Loss: 0.00002258
Iteration 89/1000 | Loss: 0.00001769
Iteration 90/1000 | Loss: 0.00001764
Iteration 91/1000 | Loss: 0.00001764
Iteration 92/1000 | Loss: 0.00001764
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001763
Iteration 101/1000 | Loss: 0.00001763
Iteration 102/1000 | Loss: 0.00001763
Iteration 103/1000 | Loss: 0.00001763
Iteration 104/1000 | Loss: 0.00001894
Iteration 105/1000 | Loss: 0.00001759
Iteration 106/1000 | Loss: 0.00001759
Iteration 107/1000 | Loss: 0.00001759
Iteration 108/1000 | Loss: 0.00001759
Iteration 109/1000 | Loss: 0.00001759
Iteration 110/1000 | Loss: 0.00001759
Iteration 111/1000 | Loss: 0.00001759
Iteration 112/1000 | Loss: 0.00001758
Iteration 113/1000 | Loss: 0.00001758
Iteration 114/1000 | Loss: 0.00001758
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001758
Iteration 119/1000 | Loss: 0.00001758
Iteration 120/1000 | Loss: 0.00001758
Iteration 121/1000 | Loss: 0.00001758
Iteration 122/1000 | Loss: 0.00001758
Iteration 123/1000 | Loss: 0.00001758
Iteration 124/1000 | Loss: 0.00001758
Iteration 125/1000 | Loss: 0.00001758
Iteration 126/1000 | Loss: 0.00001758
Iteration 127/1000 | Loss: 0.00001758
Iteration 128/1000 | Loss: 0.00001758
Iteration 129/1000 | Loss: 0.00001758
Iteration 130/1000 | Loss: 0.00001757
Iteration 131/1000 | Loss: 0.00001757
Iteration 132/1000 | Loss: 0.00001757
Iteration 133/1000 | Loss: 0.00001757
Iteration 134/1000 | Loss: 0.00001757
Iteration 135/1000 | Loss: 0.00001757
Iteration 136/1000 | Loss: 0.00001757
Iteration 137/1000 | Loss: 0.00001757
Iteration 138/1000 | Loss: 0.00001757
Iteration 139/1000 | Loss: 0.00001756
Iteration 140/1000 | Loss: 0.00001756
Iteration 141/1000 | Loss: 0.00001756
Iteration 142/1000 | Loss: 0.00001756
Iteration 143/1000 | Loss: 0.00001756
Iteration 144/1000 | Loss: 0.00001756
Iteration 145/1000 | Loss: 0.00001756
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001755
Iteration 148/1000 | Loss: 0.00001755
Iteration 149/1000 | Loss: 0.00001755
Iteration 150/1000 | Loss: 0.00001755
Iteration 151/1000 | Loss: 0.00001755
Iteration 152/1000 | Loss: 0.00001755
Iteration 153/1000 | Loss: 0.00001754
Iteration 154/1000 | Loss: 0.00001754
Iteration 155/1000 | Loss: 0.00001754
Iteration 156/1000 | Loss: 0.00004094
Iteration 157/1000 | Loss: 0.00001754
Iteration 158/1000 | Loss: 0.00001754
Iteration 159/1000 | Loss: 0.00001754
Iteration 160/1000 | Loss: 0.00004093
Iteration 161/1000 | Loss: 0.00004093
Iteration 162/1000 | Loss: 0.00029963
Iteration 163/1000 | Loss: 0.00004192
Iteration 164/1000 | Loss: 0.00024436
Iteration 165/1000 | Loss: 0.00020768
Iteration 166/1000 | Loss: 0.00004983
Iteration 167/1000 | Loss: 0.00003326
Iteration 168/1000 | Loss: 0.00004793
Iteration 169/1000 | Loss: 0.00003630
Iteration 170/1000 | Loss: 0.00019363
Iteration 171/1000 | Loss: 0.00007051
Iteration 172/1000 | Loss: 0.00001763
Iteration 173/1000 | Loss: 0.00002784
Iteration 174/1000 | Loss: 0.00002784
Iteration 175/1000 | Loss: 0.00002467
Iteration 176/1000 | Loss: 0.00001845
Iteration 177/1000 | Loss: 0.00001948
Iteration 178/1000 | Loss: 0.00002852
Iteration 179/1000 | Loss: 0.00001819
Iteration 180/1000 | Loss: 0.00001751
Iteration 181/1000 | Loss: 0.00001750
Iteration 182/1000 | Loss: 0.00001749
Iteration 183/1000 | Loss: 0.00001749
Iteration 184/1000 | Loss: 0.00001749
Iteration 185/1000 | Loss: 0.00001749
Iteration 186/1000 | Loss: 0.00001749
Iteration 187/1000 | Loss: 0.00001749
Iteration 188/1000 | Loss: 0.00001829
Iteration 189/1000 | Loss: 0.00001748
Iteration 190/1000 | Loss: 0.00001748
Iteration 191/1000 | Loss: 0.00001748
Iteration 192/1000 | Loss: 0.00001748
Iteration 193/1000 | Loss: 0.00001748
Iteration 194/1000 | Loss: 0.00001748
Iteration 195/1000 | Loss: 0.00001748
Iteration 196/1000 | Loss: 0.00001748
Iteration 197/1000 | Loss: 0.00001748
Iteration 198/1000 | Loss: 0.00001928
Iteration 199/1000 | Loss: 0.00001806
Iteration 200/1000 | Loss: 0.00001806
Iteration 201/1000 | Loss: 0.00001806
Iteration 202/1000 | Loss: 0.00003421
Iteration 203/1000 | Loss: 0.00001782
Iteration 204/1000 | Loss: 0.00001752
Iteration 205/1000 | Loss: 0.00001750
Iteration 206/1000 | Loss: 0.00001749
Iteration 207/1000 | Loss: 0.00001749
Iteration 208/1000 | Loss: 0.00001809
Iteration 209/1000 | Loss: 0.00001749
Iteration 210/1000 | Loss: 0.00001748
Iteration 211/1000 | Loss: 0.00001748
Iteration 212/1000 | Loss: 0.00001748
Iteration 213/1000 | Loss: 0.00001748
Iteration 214/1000 | Loss: 0.00001748
Iteration 215/1000 | Loss: 0.00001748
Iteration 216/1000 | Loss: 0.00001748
Iteration 217/1000 | Loss: 0.00001748
Iteration 218/1000 | Loss: 0.00001748
Iteration 219/1000 | Loss: 0.00001748
Iteration 220/1000 | Loss: 0.00001748
Iteration 221/1000 | Loss: 0.00001748
Iteration 222/1000 | Loss: 0.00001748
Iteration 223/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.7476982975495048e-05, 1.7476982975495048e-05, 1.7476982975495048e-05, 1.7476982975495048e-05, 1.7476982975495048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7476982975495048e-05

Optimization complete. Final v2v error: 3.226809024810791 mm

Highest mean error: 13.73656940460205 mm for frame 171

Lowest mean error: 2.7191569805145264 mm for frame 29

Saving results

Total time: 226.75564622879028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049135
Iteration 2/25 | Loss: 0.01049135
Iteration 3/25 | Loss: 0.00262315
Iteration 4/25 | Loss: 0.00181517
Iteration 5/25 | Loss: 0.00177421
Iteration 6/25 | Loss: 0.00153736
Iteration 7/25 | Loss: 0.00119988
Iteration 8/25 | Loss: 0.00106500
Iteration 9/25 | Loss: 0.00101798
Iteration 10/25 | Loss: 0.00102309
Iteration 11/25 | Loss: 0.00100806
Iteration 12/25 | Loss: 0.00099863
Iteration 13/25 | Loss: 0.00098837
Iteration 14/25 | Loss: 0.00098410
Iteration 15/25 | Loss: 0.00098251
Iteration 16/25 | Loss: 0.00098109
Iteration 17/25 | Loss: 0.00098347
Iteration 18/25 | Loss: 0.00097821
Iteration 19/25 | Loss: 0.00097709
Iteration 20/25 | Loss: 0.00097651
Iteration 21/25 | Loss: 0.00097634
Iteration 22/25 | Loss: 0.00097634
Iteration 23/25 | Loss: 0.00097634
Iteration 24/25 | Loss: 0.00097634
Iteration 25/25 | Loss: 0.00097634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51745176
Iteration 2/25 | Loss: 0.00325349
Iteration 3/25 | Loss: 0.00325349
Iteration 4/25 | Loss: 0.00325349
Iteration 5/25 | Loss: 0.00325349
Iteration 6/25 | Loss: 0.00325349
Iteration 7/25 | Loss: 0.00325348
Iteration 8/25 | Loss: 0.00325348
Iteration 9/25 | Loss: 0.00325348
Iteration 10/25 | Loss: 0.00325348
Iteration 11/25 | Loss: 0.00325348
Iteration 12/25 | Loss: 0.00325348
Iteration 13/25 | Loss: 0.00325348
Iteration 14/25 | Loss: 0.00325348
Iteration 15/25 | Loss: 0.00325348
Iteration 16/25 | Loss: 0.00325348
Iteration 17/25 | Loss: 0.00325348
Iteration 18/25 | Loss: 0.00325348
Iteration 19/25 | Loss: 0.00325348
Iteration 20/25 | Loss: 0.00325348
Iteration 21/25 | Loss: 0.00325348
Iteration 22/25 | Loss: 0.00325348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.003253484144806862, 0.003253484144806862, 0.003253484144806862, 0.003253484144806862, 0.003253484144806862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003253484144806862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325348
Iteration 2/1000 | Loss: 0.00292298
Iteration 3/1000 | Loss: 0.00182857
Iteration 4/1000 | Loss: 0.00019502
Iteration 5/1000 | Loss: 0.00014330
Iteration 6/1000 | Loss: 0.00009962
Iteration 7/1000 | Loss: 0.00006540
Iteration 8/1000 | Loss: 0.00004601
Iteration 9/1000 | Loss: 0.00003647
Iteration 10/1000 | Loss: 0.00003017
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002176
Iteration 13/1000 | Loss: 0.00001966
Iteration 14/1000 | Loss: 0.00001824
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001649
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001596
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001568
Iteration 31/1000 | Loss: 0.00001568
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001566
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001559
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001550
Iteration 95/1000 | Loss: 0.00001550
Iteration 96/1000 | Loss: 0.00001550
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001549
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001549
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001548
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00001548
Iteration 114/1000 | Loss: 0.00001548
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.5461351722478867e-05, 1.5461351722478867e-05, 1.5461351722478867e-05, 1.5461351722478867e-05, 1.5461351722478867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5461351722478867e-05

Optimization complete. Final v2v error: 3.3254356384277344 mm

Highest mean error: 3.5108752250671387 mm for frame 0

Lowest mean error: 3.1706480979919434 mm for frame 82

Saving results

Total time: 82.87274599075317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382143
Iteration 2/25 | Loss: 0.00097514
Iteration 3/25 | Loss: 0.00079799
Iteration 4/25 | Loss: 0.00078058
Iteration 5/25 | Loss: 0.00077354
Iteration 6/25 | Loss: 0.00077125
Iteration 7/25 | Loss: 0.00077066
Iteration 8/25 | Loss: 0.00077066
Iteration 9/25 | Loss: 0.00077066
Iteration 10/25 | Loss: 0.00077066
Iteration 11/25 | Loss: 0.00077066
Iteration 12/25 | Loss: 0.00077066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007706647738814354, 0.0007706647738814354, 0.0007706647738814354, 0.0007706647738814354, 0.0007706647738814354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007706647738814354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74383676
Iteration 2/25 | Loss: 0.00143794
Iteration 3/25 | Loss: 0.00143794
Iteration 4/25 | Loss: 0.00143794
Iteration 5/25 | Loss: 0.00143794
Iteration 6/25 | Loss: 0.00143794
Iteration 7/25 | Loss: 0.00143794
Iteration 8/25 | Loss: 0.00143794
Iteration 9/25 | Loss: 0.00143794
Iteration 10/25 | Loss: 0.00143794
Iteration 11/25 | Loss: 0.00143794
Iteration 12/25 | Loss: 0.00143794
Iteration 13/25 | Loss: 0.00143794
Iteration 14/25 | Loss: 0.00143794
Iteration 15/25 | Loss: 0.00143794
Iteration 16/25 | Loss: 0.00143794
Iteration 17/25 | Loss: 0.00143794
Iteration 18/25 | Loss: 0.00143794
Iteration 19/25 | Loss: 0.00143794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001437939004972577, 0.001437939004972577, 0.001437939004972577, 0.001437939004972577, 0.001437939004972577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001437939004972577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143794
Iteration 2/1000 | Loss: 0.00002785
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001363
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001224
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001181
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001172
Iteration 21/1000 | Loss: 0.00001172
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001167
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001161
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001152
Iteration 82/1000 | Loss: 0.00001152
Iteration 83/1000 | Loss: 0.00001152
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.1517472557898145e-05, 1.1517472557898145e-05, 1.1517472557898145e-05, 1.1517472557898145e-05, 1.1517472557898145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1517472557898145e-05

Optimization complete. Final v2v error: 2.9045798778533936 mm

Highest mean error: 3.263392448425293 mm for frame 131

Lowest mean error: 2.704728841781616 mm for frame 21

Saving results

Total time: 36.718108892440796
