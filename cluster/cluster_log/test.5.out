Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=5, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 280-335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448285
Iteration 2/25 | Loss: 0.00147668
Iteration 3/25 | Loss: 0.00129974
Iteration 4/25 | Loss: 0.00128410
Iteration 5/25 | Loss: 0.00128177
Iteration 6/25 | Loss: 0.00128166
Iteration 7/25 | Loss: 0.00128166
Iteration 8/25 | Loss: 0.00128166
Iteration 9/25 | Loss: 0.00128166
Iteration 10/25 | Loss: 0.00128166
Iteration 11/25 | Loss: 0.00128166
Iteration 12/25 | Loss: 0.00128166
Iteration 13/25 | Loss: 0.00128166
Iteration 14/25 | Loss: 0.00128166
Iteration 15/25 | Loss: 0.00128166
Iteration 16/25 | Loss: 0.00128166
Iteration 17/25 | Loss: 0.00128166
Iteration 18/25 | Loss: 0.00128166
Iteration 19/25 | Loss: 0.00128166
Iteration 20/25 | Loss: 0.00128166
Iteration 21/25 | Loss: 0.00128166
Iteration 22/25 | Loss: 0.00128166
Iteration 23/25 | Loss: 0.00128166
Iteration 24/25 | Loss: 0.00128166
Iteration 25/25 | Loss: 0.00128166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25861847
Iteration 2/25 | Loss: 0.00121210
Iteration 3/25 | Loss: 0.00121209
Iteration 4/25 | Loss: 0.00121209
Iteration 5/25 | Loss: 0.00121209
Iteration 6/25 | Loss: 0.00121209
Iteration 7/25 | Loss: 0.00121209
Iteration 8/25 | Loss: 0.00121209
Iteration 9/25 | Loss: 0.00121209
Iteration 10/25 | Loss: 0.00121209
Iteration 11/25 | Loss: 0.00121209
Iteration 12/25 | Loss: 0.00121209
Iteration 13/25 | Loss: 0.00121209
Iteration 14/25 | Loss: 0.00121209
Iteration 15/25 | Loss: 0.00121209
Iteration 16/25 | Loss: 0.00121209
Iteration 17/25 | Loss: 0.00121209
Iteration 18/25 | Loss: 0.00121209
Iteration 19/25 | Loss: 0.00121209
Iteration 20/25 | Loss: 0.00121209
Iteration 21/25 | Loss: 0.00121209
Iteration 22/25 | Loss: 0.00121209
Iteration 23/25 | Loss: 0.00121209
Iteration 24/25 | Loss: 0.00121209
Iteration 25/25 | Loss: 0.00121209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121209
Iteration 2/1000 | Loss: 0.00002601
Iteration 3/1000 | Loss: 0.00001709
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001301
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001230
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001199
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001178
Iteration 18/1000 | Loss: 0.00001177
Iteration 19/1000 | Loss: 0.00001171
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001167
Iteration 27/1000 | Loss: 0.00001167
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001166
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001161
Iteration 33/1000 | Loss: 0.00001161
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001136
Iteration 39/1000 | Loss: 0.00001136
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001136
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001129
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001126
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001125
Iteration 63/1000 | Loss: 0.00001125
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001124
Iteration 66/1000 | Loss: 0.00001124
Iteration 67/1000 | Loss: 0.00001124
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001123
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001123
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001121
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001116
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001116
Iteration 127/1000 | Loss: 0.00001116
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001115
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001115
Iteration 144/1000 | Loss: 0.00001115
Iteration 145/1000 | Loss: 0.00001115
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001115
Iteration 148/1000 | Loss: 0.00001115
Iteration 149/1000 | Loss: 0.00001114
Iteration 150/1000 | Loss: 0.00001114
Iteration 151/1000 | Loss: 0.00001114
Iteration 152/1000 | Loss: 0.00001114
Iteration 153/1000 | Loss: 0.00001114
Iteration 154/1000 | Loss: 0.00001114
Iteration 155/1000 | Loss: 0.00001114
Iteration 156/1000 | Loss: 0.00001113
Iteration 157/1000 | Loss: 0.00001113
Iteration 158/1000 | Loss: 0.00001113
Iteration 159/1000 | Loss: 0.00001113
Iteration 160/1000 | Loss: 0.00001112
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001112
Iteration 163/1000 | Loss: 0.00001112
Iteration 164/1000 | Loss: 0.00001112
Iteration 165/1000 | Loss: 0.00001112
Iteration 166/1000 | Loss: 0.00001111
Iteration 167/1000 | Loss: 0.00001111
Iteration 168/1000 | Loss: 0.00001111
Iteration 169/1000 | Loss: 0.00001111
Iteration 170/1000 | Loss: 0.00001110
Iteration 171/1000 | Loss: 0.00001110
Iteration 172/1000 | Loss: 0.00001110
Iteration 173/1000 | Loss: 0.00001110
Iteration 174/1000 | Loss: 0.00001109
Iteration 175/1000 | Loss: 0.00001109
Iteration 176/1000 | Loss: 0.00001109
Iteration 177/1000 | Loss: 0.00001109
Iteration 178/1000 | Loss: 0.00001109
Iteration 179/1000 | Loss: 0.00001109
Iteration 180/1000 | Loss: 0.00001109
Iteration 181/1000 | Loss: 0.00001108
Iteration 182/1000 | Loss: 0.00001108
Iteration 183/1000 | Loss: 0.00001108
Iteration 184/1000 | Loss: 0.00001108
Iteration 185/1000 | Loss: 0.00001108
Iteration 186/1000 | Loss: 0.00001108
Iteration 187/1000 | Loss: 0.00001108
Iteration 188/1000 | Loss: 0.00001108
Iteration 189/1000 | Loss: 0.00001108
Iteration 190/1000 | Loss: 0.00001108
Iteration 191/1000 | Loss: 0.00001108
Iteration 192/1000 | Loss: 0.00001108
Iteration 193/1000 | Loss: 0.00001107
Iteration 194/1000 | Loss: 0.00001107
Iteration 195/1000 | Loss: 0.00001107
Iteration 196/1000 | Loss: 0.00001107
Iteration 197/1000 | Loss: 0.00001107
Iteration 198/1000 | Loss: 0.00001107
Iteration 199/1000 | Loss: 0.00001107
Iteration 200/1000 | Loss: 0.00001107
Iteration 201/1000 | Loss: 0.00001107
Iteration 202/1000 | Loss: 0.00001107
Iteration 203/1000 | Loss: 0.00001107
Iteration 204/1000 | Loss: 0.00001107
Iteration 205/1000 | Loss: 0.00001107
Iteration 206/1000 | Loss: 0.00001107
Iteration 207/1000 | Loss: 0.00001107
Iteration 208/1000 | Loss: 0.00001107
Iteration 209/1000 | Loss: 0.00001107
Iteration 210/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1066346814914141e-05, 1.1066346814914141e-05, 1.1066346814914141e-05, 1.1066346814914141e-05, 1.1066346814914141e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1066346814914141e-05

Optimization complete. Final v2v error: 2.8460235595703125 mm

Highest mean error: 3.1161441802978516 mm for frame 94

Lowest mean error: 2.709204912185669 mm for frame 155

Saving results

Total time: 40.612290143966675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497611
Iteration 2/25 | Loss: 0.00133028
Iteration 3/25 | Loss: 0.00124199
Iteration 4/25 | Loss: 0.00123125
Iteration 5/25 | Loss: 0.00122967
Iteration 6/25 | Loss: 0.00122967
Iteration 7/25 | Loss: 0.00122967
Iteration 8/25 | Loss: 0.00122967
Iteration 9/25 | Loss: 0.00122967
Iteration 10/25 | Loss: 0.00122967
Iteration 11/25 | Loss: 0.00122967
Iteration 12/25 | Loss: 0.00122967
Iteration 13/25 | Loss: 0.00122967
Iteration 14/25 | Loss: 0.00122967
Iteration 15/25 | Loss: 0.00122967
Iteration 16/25 | Loss: 0.00122967
Iteration 17/25 | Loss: 0.00122967
Iteration 18/25 | Loss: 0.00122967
Iteration 19/25 | Loss: 0.00122967
Iteration 20/25 | Loss: 0.00122967
Iteration 21/25 | Loss: 0.00122967
Iteration 22/25 | Loss: 0.00122967
Iteration 23/25 | Loss: 0.00122967
Iteration 24/25 | Loss: 0.00122967
Iteration 25/25 | Loss: 0.00122967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29219472
Iteration 2/25 | Loss: 0.00109902
Iteration 3/25 | Loss: 0.00109901
Iteration 4/25 | Loss: 0.00109901
Iteration 5/25 | Loss: 0.00109901
Iteration 6/25 | Loss: 0.00109901
Iteration 7/25 | Loss: 0.00109901
Iteration 8/25 | Loss: 0.00109901
Iteration 9/25 | Loss: 0.00109901
Iteration 10/25 | Loss: 0.00109901
Iteration 11/25 | Loss: 0.00109901
Iteration 12/25 | Loss: 0.00109901
Iteration 13/25 | Loss: 0.00109901
Iteration 14/25 | Loss: 0.00109901
Iteration 15/25 | Loss: 0.00109901
Iteration 16/25 | Loss: 0.00109901
Iteration 17/25 | Loss: 0.00109901
Iteration 18/25 | Loss: 0.00109901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010990051086992025, 0.0010990051086992025, 0.0010990051086992025, 0.0010990051086992025, 0.0010990051086992025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010990051086992025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109901
Iteration 2/1000 | Loss: 0.00002071
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001500
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001338
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001192
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001184
Iteration 38/1000 | Loss: 0.00001184
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001181
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001171
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001157
Iteration 145/1000 | Loss: 0.00001157
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001156
Iteration 154/1000 | Loss: 0.00001156
Iteration 155/1000 | Loss: 0.00001156
Iteration 156/1000 | Loss: 0.00001156
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Iteration 163/1000 | Loss: 0.00001155
Iteration 164/1000 | Loss: 0.00001155
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001154
Iteration 168/1000 | Loss: 0.00001154
Iteration 169/1000 | Loss: 0.00001154
Iteration 170/1000 | Loss: 0.00001154
Iteration 171/1000 | Loss: 0.00001154
Iteration 172/1000 | Loss: 0.00001154
Iteration 173/1000 | Loss: 0.00001154
Iteration 174/1000 | Loss: 0.00001154
Iteration 175/1000 | Loss: 0.00001154
Iteration 176/1000 | Loss: 0.00001154
Iteration 177/1000 | Loss: 0.00001154
Iteration 178/1000 | Loss: 0.00001154
Iteration 179/1000 | Loss: 0.00001154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1543286746018566e-05, 1.1543286746018566e-05, 1.1543286746018566e-05, 1.1543286746018566e-05, 1.1543286746018566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1543286746018566e-05

Optimization complete. Final v2v error: 2.872854232788086 mm

Highest mean error: 3.0857439041137695 mm for frame 105

Lowest mean error: 2.591059684753418 mm for frame 223

Saving results

Total time: 47.11981153488159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531416
Iteration 2/25 | Loss: 0.00124879
Iteration 3/25 | Loss: 0.00119875
Iteration 4/25 | Loss: 0.00119046
Iteration 5/25 | Loss: 0.00118778
Iteration 6/25 | Loss: 0.00118727
Iteration 7/25 | Loss: 0.00118727
Iteration 8/25 | Loss: 0.00118727
Iteration 9/25 | Loss: 0.00118727
Iteration 10/25 | Loss: 0.00118727
Iteration 11/25 | Loss: 0.00118727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011872731847688556, 0.0011872731847688556, 0.0011872731847688556, 0.0011872731847688556, 0.0011872731847688556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011872731847688556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06517410
Iteration 2/25 | Loss: 0.00133527
Iteration 3/25 | Loss: 0.00133526
Iteration 4/25 | Loss: 0.00133526
Iteration 5/25 | Loss: 0.00133526
Iteration 6/25 | Loss: 0.00133526
Iteration 7/25 | Loss: 0.00133526
Iteration 8/25 | Loss: 0.00133526
Iteration 9/25 | Loss: 0.00133526
Iteration 10/25 | Loss: 0.00133526
Iteration 11/25 | Loss: 0.00133526
Iteration 12/25 | Loss: 0.00133526
Iteration 13/25 | Loss: 0.00133526
Iteration 14/25 | Loss: 0.00133526
Iteration 15/25 | Loss: 0.00133526
Iteration 16/25 | Loss: 0.00133526
Iteration 17/25 | Loss: 0.00133526
Iteration 18/25 | Loss: 0.00133526
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013352561509236693, 0.0013352561509236693, 0.0013352561509236693, 0.0013352561509236693, 0.0013352561509236693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013352561509236693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133526
Iteration 2/1000 | Loss: 0.00002102
Iteration 3/1000 | Loss: 0.00001567
Iteration 4/1000 | Loss: 0.00001446
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001152
Iteration 22/1000 | Loss: 0.00001150
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001142
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001132
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001120
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001116
Iteration 62/1000 | Loss: 0.00001116
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001114
Iteration 66/1000 | Loss: 0.00001114
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001111
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001109
Iteration 75/1000 | Loss: 0.00001109
Iteration 76/1000 | Loss: 0.00001109
Iteration 77/1000 | Loss: 0.00001109
Iteration 78/1000 | Loss: 0.00001109
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001108
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001107
Iteration 87/1000 | Loss: 0.00001107
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001107
Iteration 93/1000 | Loss: 0.00001106
Iteration 94/1000 | Loss: 0.00001106
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001106
Iteration 98/1000 | Loss: 0.00001105
Iteration 99/1000 | Loss: 0.00001105
Iteration 100/1000 | Loss: 0.00001105
Iteration 101/1000 | Loss: 0.00001105
Iteration 102/1000 | Loss: 0.00001105
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001104
Iteration 106/1000 | Loss: 0.00001104
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001104
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.1044799975934438e-05, 1.1044799975934438e-05, 1.1044799975934438e-05, 1.1044799975934438e-05, 1.1044799975934438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1044799975934438e-05

Optimization complete. Final v2v error: 2.8619422912597656 mm

Highest mean error: 3.420376777648926 mm for frame 59

Lowest mean error: 2.56489896774292 mm for frame 34

Saving results

Total time: 35.03952598571777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791577
Iteration 2/25 | Loss: 0.00127303
Iteration 3/25 | Loss: 0.00117712
Iteration 4/25 | Loss: 0.00116744
Iteration 5/25 | Loss: 0.00116546
Iteration 6/25 | Loss: 0.00116546
Iteration 7/25 | Loss: 0.00116546
Iteration 8/25 | Loss: 0.00116546
Iteration 9/25 | Loss: 0.00116546
Iteration 10/25 | Loss: 0.00116546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011654599802568555, 0.0011654599802568555, 0.0011654599802568555, 0.0011654599802568555, 0.0011654599802568555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011654599802568555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29568386
Iteration 2/25 | Loss: 0.00135684
Iteration 3/25 | Loss: 0.00135684
Iteration 4/25 | Loss: 0.00135684
Iteration 5/25 | Loss: 0.00135684
Iteration 6/25 | Loss: 0.00135684
Iteration 7/25 | Loss: 0.00135684
Iteration 8/25 | Loss: 0.00135684
Iteration 9/25 | Loss: 0.00135684
Iteration 10/25 | Loss: 0.00135684
Iteration 11/25 | Loss: 0.00135684
Iteration 12/25 | Loss: 0.00135684
Iteration 13/25 | Loss: 0.00135684
Iteration 14/25 | Loss: 0.00135684
Iteration 15/25 | Loss: 0.00135684
Iteration 16/25 | Loss: 0.00135684
Iteration 17/25 | Loss: 0.00135684
Iteration 18/25 | Loss: 0.00135684
Iteration 19/25 | Loss: 0.00135684
Iteration 20/25 | Loss: 0.00135684
Iteration 21/25 | Loss: 0.00135684
Iteration 22/25 | Loss: 0.00135684
Iteration 23/25 | Loss: 0.00135684
Iteration 24/25 | Loss: 0.00135684
Iteration 25/25 | Loss: 0.00135684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135684
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001457
Iteration 4/1000 | Loss: 0.00001290
Iteration 5/1000 | Loss: 0.00001206
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001094
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001049
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001001
Iteration 12/1000 | Loss: 0.00000998
Iteration 13/1000 | Loss: 0.00000994
Iteration 14/1000 | Loss: 0.00000994
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000989
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000986
Iteration 19/1000 | Loss: 0.00000983
Iteration 20/1000 | Loss: 0.00000977
Iteration 21/1000 | Loss: 0.00000977
Iteration 22/1000 | Loss: 0.00000977
Iteration 23/1000 | Loss: 0.00000976
Iteration 24/1000 | Loss: 0.00000970
Iteration 25/1000 | Loss: 0.00000970
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000969
Iteration 28/1000 | Loss: 0.00000965
Iteration 29/1000 | Loss: 0.00000965
Iteration 30/1000 | Loss: 0.00000964
Iteration 31/1000 | Loss: 0.00000960
Iteration 32/1000 | Loss: 0.00000959
Iteration 33/1000 | Loss: 0.00000959
Iteration 34/1000 | Loss: 0.00000958
Iteration 35/1000 | Loss: 0.00000957
Iteration 36/1000 | Loss: 0.00000954
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000952
Iteration 39/1000 | Loss: 0.00000952
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000947
Iteration 45/1000 | Loss: 0.00000946
Iteration 46/1000 | Loss: 0.00000946
Iteration 47/1000 | Loss: 0.00000946
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000945
Iteration 50/1000 | Loss: 0.00000945
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00000939
Iteration 53/1000 | Loss: 0.00000938
Iteration 54/1000 | Loss: 0.00000938
Iteration 55/1000 | Loss: 0.00000935
Iteration 56/1000 | Loss: 0.00000934
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000931
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000930
Iteration 63/1000 | Loss: 0.00000930
Iteration 64/1000 | Loss: 0.00000930
Iteration 65/1000 | Loss: 0.00000930
Iteration 66/1000 | Loss: 0.00000930
Iteration 67/1000 | Loss: 0.00000930
Iteration 68/1000 | Loss: 0.00000930
Iteration 69/1000 | Loss: 0.00000930
Iteration 70/1000 | Loss: 0.00000929
Iteration 71/1000 | Loss: 0.00000929
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000925
Iteration 74/1000 | Loss: 0.00000925
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000925
Iteration 79/1000 | Loss: 0.00000925
Iteration 80/1000 | Loss: 0.00000925
Iteration 81/1000 | Loss: 0.00000924
Iteration 82/1000 | Loss: 0.00000924
Iteration 83/1000 | Loss: 0.00000923
Iteration 84/1000 | Loss: 0.00000923
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000922
Iteration 90/1000 | Loss: 0.00000922
Iteration 91/1000 | Loss: 0.00000922
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000919
Iteration 99/1000 | Loss: 0.00000919
Iteration 100/1000 | Loss: 0.00000918
Iteration 101/1000 | Loss: 0.00000918
Iteration 102/1000 | Loss: 0.00000918
Iteration 103/1000 | Loss: 0.00000917
Iteration 104/1000 | Loss: 0.00000917
Iteration 105/1000 | Loss: 0.00000917
Iteration 106/1000 | Loss: 0.00000917
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000916
Iteration 109/1000 | Loss: 0.00000915
Iteration 110/1000 | Loss: 0.00000915
Iteration 111/1000 | Loss: 0.00000915
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000915
Iteration 114/1000 | Loss: 0.00000915
Iteration 115/1000 | Loss: 0.00000914
Iteration 116/1000 | Loss: 0.00000914
Iteration 117/1000 | Loss: 0.00000914
Iteration 118/1000 | Loss: 0.00000914
Iteration 119/1000 | Loss: 0.00000914
Iteration 120/1000 | Loss: 0.00000914
Iteration 121/1000 | Loss: 0.00000914
Iteration 122/1000 | Loss: 0.00000914
Iteration 123/1000 | Loss: 0.00000913
Iteration 124/1000 | Loss: 0.00000913
Iteration 125/1000 | Loss: 0.00000913
Iteration 126/1000 | Loss: 0.00000913
Iteration 127/1000 | Loss: 0.00000913
Iteration 128/1000 | Loss: 0.00000912
Iteration 129/1000 | Loss: 0.00000912
Iteration 130/1000 | Loss: 0.00000912
Iteration 131/1000 | Loss: 0.00000912
Iteration 132/1000 | Loss: 0.00000911
Iteration 133/1000 | Loss: 0.00000911
Iteration 134/1000 | Loss: 0.00000911
Iteration 135/1000 | Loss: 0.00000911
Iteration 136/1000 | Loss: 0.00000910
Iteration 137/1000 | Loss: 0.00000910
Iteration 138/1000 | Loss: 0.00000910
Iteration 139/1000 | Loss: 0.00000910
Iteration 140/1000 | Loss: 0.00000910
Iteration 141/1000 | Loss: 0.00000910
Iteration 142/1000 | Loss: 0.00000910
Iteration 143/1000 | Loss: 0.00000910
Iteration 144/1000 | Loss: 0.00000909
Iteration 145/1000 | Loss: 0.00000909
Iteration 146/1000 | Loss: 0.00000909
Iteration 147/1000 | Loss: 0.00000909
Iteration 148/1000 | Loss: 0.00000908
Iteration 149/1000 | Loss: 0.00000908
Iteration 150/1000 | Loss: 0.00000908
Iteration 151/1000 | Loss: 0.00000908
Iteration 152/1000 | Loss: 0.00000908
Iteration 153/1000 | Loss: 0.00000908
Iteration 154/1000 | Loss: 0.00000908
Iteration 155/1000 | Loss: 0.00000908
Iteration 156/1000 | Loss: 0.00000908
Iteration 157/1000 | Loss: 0.00000908
Iteration 158/1000 | Loss: 0.00000908
Iteration 159/1000 | Loss: 0.00000908
Iteration 160/1000 | Loss: 0.00000908
Iteration 161/1000 | Loss: 0.00000908
Iteration 162/1000 | Loss: 0.00000907
Iteration 163/1000 | Loss: 0.00000907
Iteration 164/1000 | Loss: 0.00000907
Iteration 165/1000 | Loss: 0.00000907
Iteration 166/1000 | Loss: 0.00000907
Iteration 167/1000 | Loss: 0.00000907
Iteration 168/1000 | Loss: 0.00000906
Iteration 169/1000 | Loss: 0.00000906
Iteration 170/1000 | Loss: 0.00000906
Iteration 171/1000 | Loss: 0.00000906
Iteration 172/1000 | Loss: 0.00000906
Iteration 173/1000 | Loss: 0.00000906
Iteration 174/1000 | Loss: 0.00000906
Iteration 175/1000 | Loss: 0.00000906
Iteration 176/1000 | Loss: 0.00000906
Iteration 177/1000 | Loss: 0.00000906
Iteration 178/1000 | Loss: 0.00000905
Iteration 179/1000 | Loss: 0.00000905
Iteration 180/1000 | Loss: 0.00000905
Iteration 181/1000 | Loss: 0.00000905
Iteration 182/1000 | Loss: 0.00000905
Iteration 183/1000 | Loss: 0.00000905
Iteration 184/1000 | Loss: 0.00000905
Iteration 185/1000 | Loss: 0.00000905
Iteration 186/1000 | Loss: 0.00000905
Iteration 187/1000 | Loss: 0.00000905
Iteration 188/1000 | Loss: 0.00000905
Iteration 189/1000 | Loss: 0.00000905
Iteration 190/1000 | Loss: 0.00000904
Iteration 191/1000 | Loss: 0.00000904
Iteration 192/1000 | Loss: 0.00000904
Iteration 193/1000 | Loss: 0.00000904
Iteration 194/1000 | Loss: 0.00000904
Iteration 195/1000 | Loss: 0.00000904
Iteration 196/1000 | Loss: 0.00000904
Iteration 197/1000 | Loss: 0.00000903
Iteration 198/1000 | Loss: 0.00000903
Iteration 199/1000 | Loss: 0.00000903
Iteration 200/1000 | Loss: 0.00000903
Iteration 201/1000 | Loss: 0.00000903
Iteration 202/1000 | Loss: 0.00000903
Iteration 203/1000 | Loss: 0.00000903
Iteration 204/1000 | Loss: 0.00000903
Iteration 205/1000 | Loss: 0.00000903
Iteration 206/1000 | Loss: 0.00000903
Iteration 207/1000 | Loss: 0.00000903
Iteration 208/1000 | Loss: 0.00000903
Iteration 209/1000 | Loss: 0.00000903
Iteration 210/1000 | Loss: 0.00000903
Iteration 211/1000 | Loss: 0.00000903
Iteration 212/1000 | Loss: 0.00000903
Iteration 213/1000 | Loss: 0.00000903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [9.027635314851068e-06, 9.027635314851068e-06, 9.027635314851068e-06, 9.027635314851068e-06, 9.027635314851068e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.027635314851068e-06

Optimization complete. Final v2v error: 2.568319320678711 mm

Highest mean error: 2.758168935775757 mm for frame 65

Lowest mean error: 2.407132387161255 mm for frame 30

Saving results

Total time: 41.568422079086304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526575
Iteration 2/25 | Loss: 0.00126290
Iteration 3/25 | Loss: 0.00119628
Iteration 4/25 | Loss: 0.00118687
Iteration 5/25 | Loss: 0.00118401
Iteration 6/25 | Loss: 0.00118349
Iteration 7/25 | Loss: 0.00118349
Iteration 8/25 | Loss: 0.00118349
Iteration 9/25 | Loss: 0.00118349
Iteration 10/25 | Loss: 0.00118349
Iteration 11/25 | Loss: 0.00118349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001183491782285273, 0.001183491782285273, 0.001183491782285273, 0.001183491782285273, 0.001183491782285273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001183491782285273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65641880
Iteration 2/25 | Loss: 0.00129604
Iteration 3/25 | Loss: 0.00129603
Iteration 4/25 | Loss: 0.00129603
Iteration 5/25 | Loss: 0.00129603
Iteration 6/25 | Loss: 0.00129603
Iteration 7/25 | Loss: 0.00129603
Iteration 8/25 | Loss: 0.00129602
Iteration 9/25 | Loss: 0.00129602
Iteration 10/25 | Loss: 0.00129602
Iteration 11/25 | Loss: 0.00129602
Iteration 12/25 | Loss: 0.00129602
Iteration 13/25 | Loss: 0.00129602
Iteration 14/25 | Loss: 0.00129602
Iteration 15/25 | Loss: 0.00129602
Iteration 16/25 | Loss: 0.00129602
Iteration 17/25 | Loss: 0.00129602
Iteration 18/25 | Loss: 0.00129602
Iteration 19/25 | Loss: 0.00129602
Iteration 20/25 | Loss: 0.00129602
Iteration 21/25 | Loss: 0.00129602
Iteration 22/25 | Loss: 0.00129602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012960238382220268, 0.0012960238382220268, 0.0012960238382220268, 0.0012960238382220268, 0.0012960238382220268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012960238382220268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129602
Iteration 2/1000 | Loss: 0.00002428
Iteration 3/1000 | Loss: 0.00001796
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001403
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001258
Iteration 9/1000 | Loss: 0.00001236
Iteration 10/1000 | Loss: 0.00001201
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001145
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001135
Iteration 21/1000 | Loss: 0.00001133
Iteration 22/1000 | Loss: 0.00001133
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001132
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001117
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001097
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001096
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001093
Iteration 91/1000 | Loss: 0.00001093
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001091
Iteration 98/1000 | Loss: 0.00001091
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001090
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001089
Iteration 108/1000 | Loss: 0.00001089
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001086
Iteration 127/1000 | Loss: 0.00001086
Iteration 128/1000 | Loss: 0.00001086
Iteration 129/1000 | Loss: 0.00001086
Iteration 130/1000 | Loss: 0.00001086
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001085
Iteration 140/1000 | Loss: 0.00001085
Iteration 141/1000 | Loss: 0.00001084
Iteration 142/1000 | Loss: 0.00001084
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001084
Iteration 148/1000 | Loss: 0.00001084
Iteration 149/1000 | Loss: 0.00001084
Iteration 150/1000 | Loss: 0.00001083
Iteration 151/1000 | Loss: 0.00001083
Iteration 152/1000 | Loss: 0.00001083
Iteration 153/1000 | Loss: 0.00001083
Iteration 154/1000 | Loss: 0.00001083
Iteration 155/1000 | Loss: 0.00001083
Iteration 156/1000 | Loss: 0.00001083
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001082
Iteration 159/1000 | Loss: 0.00001082
Iteration 160/1000 | Loss: 0.00001082
Iteration 161/1000 | Loss: 0.00001082
Iteration 162/1000 | Loss: 0.00001082
Iteration 163/1000 | Loss: 0.00001082
Iteration 164/1000 | Loss: 0.00001082
Iteration 165/1000 | Loss: 0.00001082
Iteration 166/1000 | Loss: 0.00001082
Iteration 167/1000 | Loss: 0.00001082
Iteration 168/1000 | Loss: 0.00001082
Iteration 169/1000 | Loss: 0.00001082
Iteration 170/1000 | Loss: 0.00001082
Iteration 171/1000 | Loss: 0.00001082
Iteration 172/1000 | Loss: 0.00001082
Iteration 173/1000 | Loss: 0.00001082
Iteration 174/1000 | Loss: 0.00001082
Iteration 175/1000 | Loss: 0.00001082
Iteration 176/1000 | Loss: 0.00001081
Iteration 177/1000 | Loss: 0.00001081
Iteration 178/1000 | Loss: 0.00001081
Iteration 179/1000 | Loss: 0.00001081
Iteration 180/1000 | Loss: 0.00001081
Iteration 181/1000 | Loss: 0.00001081
Iteration 182/1000 | Loss: 0.00001081
Iteration 183/1000 | Loss: 0.00001081
Iteration 184/1000 | Loss: 0.00001081
Iteration 185/1000 | Loss: 0.00001081
Iteration 186/1000 | Loss: 0.00001081
Iteration 187/1000 | Loss: 0.00001081
Iteration 188/1000 | Loss: 0.00001081
Iteration 189/1000 | Loss: 0.00001081
Iteration 190/1000 | Loss: 0.00001081
Iteration 191/1000 | Loss: 0.00001081
Iteration 192/1000 | Loss: 0.00001081
Iteration 193/1000 | Loss: 0.00001081
Iteration 194/1000 | Loss: 0.00001081
Iteration 195/1000 | Loss: 0.00001081
Iteration 196/1000 | Loss: 0.00001081
Iteration 197/1000 | Loss: 0.00001081
Iteration 198/1000 | Loss: 0.00001081
Iteration 199/1000 | Loss: 0.00001081
Iteration 200/1000 | Loss: 0.00001081
Iteration 201/1000 | Loss: 0.00001081
Iteration 202/1000 | Loss: 0.00001081
Iteration 203/1000 | Loss: 0.00001081
Iteration 204/1000 | Loss: 0.00001081
Iteration 205/1000 | Loss: 0.00001081
Iteration 206/1000 | Loss: 0.00001081
Iteration 207/1000 | Loss: 0.00001081
Iteration 208/1000 | Loss: 0.00001081
Iteration 209/1000 | Loss: 0.00001081
Iteration 210/1000 | Loss: 0.00001081
Iteration 211/1000 | Loss: 0.00001081
Iteration 212/1000 | Loss: 0.00001081
Iteration 213/1000 | Loss: 0.00001081
Iteration 214/1000 | Loss: 0.00001081
Iteration 215/1000 | Loss: 0.00001081
Iteration 216/1000 | Loss: 0.00001081
Iteration 217/1000 | Loss: 0.00001081
Iteration 218/1000 | Loss: 0.00001081
Iteration 219/1000 | Loss: 0.00001081
Iteration 220/1000 | Loss: 0.00001081
Iteration 221/1000 | Loss: 0.00001081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.0812061191245448e-05, 1.0812061191245448e-05, 1.0812061191245448e-05, 1.0812061191245448e-05, 1.0812061191245448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0812061191245448e-05

Optimization complete. Final v2v error: 2.820152997970581 mm

Highest mean error: 3.573535203933716 mm for frame 70

Lowest mean error: 2.5187184810638428 mm for frame 30

Saving results

Total time: 42.19154477119446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392444
Iteration 2/25 | Loss: 0.00123643
Iteration 3/25 | Loss: 0.00116641
Iteration 4/25 | Loss: 0.00116074
Iteration 5/25 | Loss: 0.00115945
Iteration 6/25 | Loss: 0.00115945
Iteration 7/25 | Loss: 0.00115945
Iteration 8/25 | Loss: 0.00115945
Iteration 9/25 | Loss: 0.00115945
Iteration 10/25 | Loss: 0.00115945
Iteration 11/25 | Loss: 0.00115945
Iteration 12/25 | Loss: 0.00115945
Iteration 13/25 | Loss: 0.00115945
Iteration 14/25 | Loss: 0.00115945
Iteration 15/25 | Loss: 0.00115945
Iteration 16/25 | Loss: 0.00115945
Iteration 17/25 | Loss: 0.00115945
Iteration 18/25 | Loss: 0.00115945
Iteration 19/25 | Loss: 0.00115945
Iteration 20/25 | Loss: 0.00115945
Iteration 21/25 | Loss: 0.00115945
Iteration 22/25 | Loss: 0.00115945
Iteration 23/25 | Loss: 0.00115945
Iteration 24/25 | Loss: 0.00115945
Iteration 25/25 | Loss: 0.00115945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28966844
Iteration 2/25 | Loss: 0.00154317
Iteration 3/25 | Loss: 0.00154316
Iteration 4/25 | Loss: 0.00154316
Iteration 5/25 | Loss: 0.00154316
Iteration 6/25 | Loss: 0.00154316
Iteration 7/25 | Loss: 0.00154316
Iteration 8/25 | Loss: 0.00154316
Iteration 9/25 | Loss: 0.00154316
Iteration 10/25 | Loss: 0.00154316
Iteration 11/25 | Loss: 0.00154316
Iteration 12/25 | Loss: 0.00154316
Iteration 13/25 | Loss: 0.00154316
Iteration 14/25 | Loss: 0.00154316
Iteration 15/25 | Loss: 0.00154316
Iteration 16/25 | Loss: 0.00154316
Iteration 17/25 | Loss: 0.00154316
Iteration 18/25 | Loss: 0.00154316
Iteration 19/25 | Loss: 0.00154316
Iteration 20/25 | Loss: 0.00154316
Iteration 21/25 | Loss: 0.00154316
Iteration 22/25 | Loss: 0.00154316
Iteration 23/25 | Loss: 0.00154316
Iteration 24/25 | Loss: 0.00154316
Iteration 25/25 | Loss: 0.00154316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00154315575491637, 0.00154315575491637, 0.00154315575491637, 0.00154315575491637, 0.00154315575491637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00154315575491637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154316
Iteration 2/1000 | Loss: 0.00002160
Iteration 3/1000 | Loss: 0.00001255
Iteration 4/1000 | Loss: 0.00001081
Iteration 5/1000 | Loss: 0.00001003
Iteration 6/1000 | Loss: 0.00000954
Iteration 7/1000 | Loss: 0.00000913
Iteration 8/1000 | Loss: 0.00000892
Iteration 9/1000 | Loss: 0.00000888
Iteration 10/1000 | Loss: 0.00000877
Iteration 11/1000 | Loss: 0.00000858
Iteration 12/1000 | Loss: 0.00000856
Iteration 13/1000 | Loss: 0.00000852
Iteration 14/1000 | Loss: 0.00000845
Iteration 15/1000 | Loss: 0.00000836
Iteration 16/1000 | Loss: 0.00000836
Iteration 17/1000 | Loss: 0.00000835
Iteration 18/1000 | Loss: 0.00000832
Iteration 19/1000 | Loss: 0.00000825
Iteration 20/1000 | Loss: 0.00000823
Iteration 21/1000 | Loss: 0.00000820
Iteration 22/1000 | Loss: 0.00000819
Iteration 23/1000 | Loss: 0.00000818
Iteration 24/1000 | Loss: 0.00000815
Iteration 25/1000 | Loss: 0.00000815
Iteration 26/1000 | Loss: 0.00000814
Iteration 27/1000 | Loss: 0.00000814
Iteration 28/1000 | Loss: 0.00000814
Iteration 29/1000 | Loss: 0.00000814
Iteration 30/1000 | Loss: 0.00000813
Iteration 31/1000 | Loss: 0.00000813
Iteration 32/1000 | Loss: 0.00000812
Iteration 33/1000 | Loss: 0.00000812
Iteration 34/1000 | Loss: 0.00000811
Iteration 35/1000 | Loss: 0.00000810
Iteration 36/1000 | Loss: 0.00000809
Iteration 37/1000 | Loss: 0.00000808
Iteration 38/1000 | Loss: 0.00000808
Iteration 39/1000 | Loss: 0.00000808
Iteration 40/1000 | Loss: 0.00000807
Iteration 41/1000 | Loss: 0.00000807
Iteration 42/1000 | Loss: 0.00000806
Iteration 43/1000 | Loss: 0.00000804
Iteration 44/1000 | Loss: 0.00000804
Iteration 45/1000 | Loss: 0.00000803
Iteration 46/1000 | Loss: 0.00000803
Iteration 47/1000 | Loss: 0.00000803
Iteration 48/1000 | Loss: 0.00000803
Iteration 49/1000 | Loss: 0.00000803
Iteration 50/1000 | Loss: 0.00000803
Iteration 51/1000 | Loss: 0.00000803
Iteration 52/1000 | Loss: 0.00000803
Iteration 53/1000 | Loss: 0.00000801
Iteration 54/1000 | Loss: 0.00000801
Iteration 55/1000 | Loss: 0.00000801
Iteration 56/1000 | Loss: 0.00000801
Iteration 57/1000 | Loss: 0.00000799
Iteration 58/1000 | Loss: 0.00000799
Iteration 59/1000 | Loss: 0.00000799
Iteration 60/1000 | Loss: 0.00000799
Iteration 61/1000 | Loss: 0.00000798
Iteration 62/1000 | Loss: 0.00000798
Iteration 63/1000 | Loss: 0.00000797
Iteration 64/1000 | Loss: 0.00000797
Iteration 65/1000 | Loss: 0.00000796
Iteration 66/1000 | Loss: 0.00000796
Iteration 67/1000 | Loss: 0.00000796
Iteration 68/1000 | Loss: 0.00000795
Iteration 69/1000 | Loss: 0.00000795
Iteration 70/1000 | Loss: 0.00000795
Iteration 71/1000 | Loss: 0.00000795
Iteration 72/1000 | Loss: 0.00000794
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000793
Iteration 75/1000 | Loss: 0.00000792
Iteration 76/1000 | Loss: 0.00000791
Iteration 77/1000 | Loss: 0.00000791
Iteration 78/1000 | Loss: 0.00000790
Iteration 79/1000 | Loss: 0.00000790
Iteration 80/1000 | Loss: 0.00000790
Iteration 81/1000 | Loss: 0.00000790
Iteration 82/1000 | Loss: 0.00000789
Iteration 83/1000 | Loss: 0.00000788
Iteration 84/1000 | Loss: 0.00000788
Iteration 85/1000 | Loss: 0.00000787
Iteration 86/1000 | Loss: 0.00000787
Iteration 87/1000 | Loss: 0.00000787
Iteration 88/1000 | Loss: 0.00000787
Iteration 89/1000 | Loss: 0.00000787
Iteration 90/1000 | Loss: 0.00000787
Iteration 91/1000 | Loss: 0.00000786
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000784
Iteration 95/1000 | Loss: 0.00000784
Iteration 96/1000 | Loss: 0.00000784
Iteration 97/1000 | Loss: 0.00000784
Iteration 98/1000 | Loss: 0.00000783
Iteration 99/1000 | Loss: 0.00000783
Iteration 100/1000 | Loss: 0.00000783
Iteration 101/1000 | Loss: 0.00000783
Iteration 102/1000 | Loss: 0.00000783
Iteration 103/1000 | Loss: 0.00000783
Iteration 104/1000 | Loss: 0.00000783
Iteration 105/1000 | Loss: 0.00000782
Iteration 106/1000 | Loss: 0.00000782
Iteration 107/1000 | Loss: 0.00000782
Iteration 108/1000 | Loss: 0.00000782
Iteration 109/1000 | Loss: 0.00000781
Iteration 110/1000 | Loss: 0.00000781
Iteration 111/1000 | Loss: 0.00000781
Iteration 112/1000 | Loss: 0.00000781
Iteration 113/1000 | Loss: 0.00000781
Iteration 114/1000 | Loss: 0.00000781
Iteration 115/1000 | Loss: 0.00000781
Iteration 116/1000 | Loss: 0.00000781
Iteration 117/1000 | Loss: 0.00000780
Iteration 118/1000 | Loss: 0.00000780
Iteration 119/1000 | Loss: 0.00000780
Iteration 120/1000 | Loss: 0.00000780
Iteration 121/1000 | Loss: 0.00000780
Iteration 122/1000 | Loss: 0.00000780
Iteration 123/1000 | Loss: 0.00000780
Iteration 124/1000 | Loss: 0.00000780
Iteration 125/1000 | Loss: 0.00000780
Iteration 126/1000 | Loss: 0.00000780
Iteration 127/1000 | Loss: 0.00000780
Iteration 128/1000 | Loss: 0.00000780
Iteration 129/1000 | Loss: 0.00000780
Iteration 130/1000 | Loss: 0.00000780
Iteration 131/1000 | Loss: 0.00000780
Iteration 132/1000 | Loss: 0.00000779
Iteration 133/1000 | Loss: 0.00000779
Iteration 134/1000 | Loss: 0.00000779
Iteration 135/1000 | Loss: 0.00000779
Iteration 136/1000 | Loss: 0.00000779
Iteration 137/1000 | Loss: 0.00000779
Iteration 138/1000 | Loss: 0.00000779
Iteration 139/1000 | Loss: 0.00000779
Iteration 140/1000 | Loss: 0.00000779
Iteration 141/1000 | Loss: 0.00000779
Iteration 142/1000 | Loss: 0.00000778
Iteration 143/1000 | Loss: 0.00000778
Iteration 144/1000 | Loss: 0.00000778
Iteration 145/1000 | Loss: 0.00000778
Iteration 146/1000 | Loss: 0.00000778
Iteration 147/1000 | Loss: 0.00000778
Iteration 148/1000 | Loss: 0.00000778
Iteration 149/1000 | Loss: 0.00000778
Iteration 150/1000 | Loss: 0.00000778
Iteration 151/1000 | Loss: 0.00000778
Iteration 152/1000 | Loss: 0.00000778
Iteration 153/1000 | Loss: 0.00000778
Iteration 154/1000 | Loss: 0.00000778
Iteration 155/1000 | Loss: 0.00000778
Iteration 156/1000 | Loss: 0.00000778
Iteration 157/1000 | Loss: 0.00000777
Iteration 158/1000 | Loss: 0.00000777
Iteration 159/1000 | Loss: 0.00000777
Iteration 160/1000 | Loss: 0.00000777
Iteration 161/1000 | Loss: 0.00000776
Iteration 162/1000 | Loss: 0.00000776
Iteration 163/1000 | Loss: 0.00000776
Iteration 164/1000 | Loss: 0.00000776
Iteration 165/1000 | Loss: 0.00000776
Iteration 166/1000 | Loss: 0.00000776
Iteration 167/1000 | Loss: 0.00000776
Iteration 168/1000 | Loss: 0.00000775
Iteration 169/1000 | Loss: 0.00000775
Iteration 170/1000 | Loss: 0.00000775
Iteration 171/1000 | Loss: 0.00000775
Iteration 172/1000 | Loss: 0.00000775
Iteration 173/1000 | Loss: 0.00000775
Iteration 174/1000 | Loss: 0.00000775
Iteration 175/1000 | Loss: 0.00000775
Iteration 176/1000 | Loss: 0.00000775
Iteration 177/1000 | Loss: 0.00000775
Iteration 178/1000 | Loss: 0.00000775
Iteration 179/1000 | Loss: 0.00000775
Iteration 180/1000 | Loss: 0.00000775
Iteration 181/1000 | Loss: 0.00000775
Iteration 182/1000 | Loss: 0.00000775
Iteration 183/1000 | Loss: 0.00000775
Iteration 184/1000 | Loss: 0.00000774
Iteration 185/1000 | Loss: 0.00000774
Iteration 186/1000 | Loss: 0.00000774
Iteration 187/1000 | Loss: 0.00000774
Iteration 188/1000 | Loss: 0.00000774
Iteration 189/1000 | Loss: 0.00000773
Iteration 190/1000 | Loss: 0.00000773
Iteration 191/1000 | Loss: 0.00000773
Iteration 192/1000 | Loss: 0.00000773
Iteration 193/1000 | Loss: 0.00000773
Iteration 194/1000 | Loss: 0.00000773
Iteration 195/1000 | Loss: 0.00000772
Iteration 196/1000 | Loss: 0.00000772
Iteration 197/1000 | Loss: 0.00000772
Iteration 198/1000 | Loss: 0.00000772
Iteration 199/1000 | Loss: 0.00000772
Iteration 200/1000 | Loss: 0.00000772
Iteration 201/1000 | Loss: 0.00000771
Iteration 202/1000 | Loss: 0.00000771
Iteration 203/1000 | Loss: 0.00000771
Iteration 204/1000 | Loss: 0.00000770
Iteration 205/1000 | Loss: 0.00000770
Iteration 206/1000 | Loss: 0.00000770
Iteration 207/1000 | Loss: 0.00000770
Iteration 208/1000 | Loss: 0.00000770
Iteration 209/1000 | Loss: 0.00000770
Iteration 210/1000 | Loss: 0.00000770
Iteration 211/1000 | Loss: 0.00000770
Iteration 212/1000 | Loss: 0.00000770
Iteration 213/1000 | Loss: 0.00000770
Iteration 214/1000 | Loss: 0.00000770
Iteration 215/1000 | Loss: 0.00000769
Iteration 216/1000 | Loss: 0.00000769
Iteration 217/1000 | Loss: 0.00000769
Iteration 218/1000 | Loss: 0.00000769
Iteration 219/1000 | Loss: 0.00000769
Iteration 220/1000 | Loss: 0.00000769
Iteration 221/1000 | Loss: 0.00000768
Iteration 222/1000 | Loss: 0.00000768
Iteration 223/1000 | Loss: 0.00000768
Iteration 224/1000 | Loss: 0.00000768
Iteration 225/1000 | Loss: 0.00000768
Iteration 226/1000 | Loss: 0.00000768
Iteration 227/1000 | Loss: 0.00000768
Iteration 228/1000 | Loss: 0.00000768
Iteration 229/1000 | Loss: 0.00000767
Iteration 230/1000 | Loss: 0.00000767
Iteration 231/1000 | Loss: 0.00000767
Iteration 232/1000 | Loss: 0.00000767
Iteration 233/1000 | Loss: 0.00000767
Iteration 234/1000 | Loss: 0.00000767
Iteration 235/1000 | Loss: 0.00000767
Iteration 236/1000 | Loss: 0.00000767
Iteration 237/1000 | Loss: 0.00000767
Iteration 238/1000 | Loss: 0.00000767
Iteration 239/1000 | Loss: 0.00000767
Iteration 240/1000 | Loss: 0.00000767
Iteration 241/1000 | Loss: 0.00000767
Iteration 242/1000 | Loss: 0.00000767
Iteration 243/1000 | Loss: 0.00000767
Iteration 244/1000 | Loss: 0.00000767
Iteration 245/1000 | Loss: 0.00000766
Iteration 246/1000 | Loss: 0.00000766
Iteration 247/1000 | Loss: 0.00000766
Iteration 248/1000 | Loss: 0.00000766
Iteration 249/1000 | Loss: 0.00000766
Iteration 250/1000 | Loss: 0.00000766
Iteration 251/1000 | Loss: 0.00000766
Iteration 252/1000 | Loss: 0.00000766
Iteration 253/1000 | Loss: 0.00000766
Iteration 254/1000 | Loss: 0.00000766
Iteration 255/1000 | Loss: 0.00000766
Iteration 256/1000 | Loss: 0.00000766
Iteration 257/1000 | Loss: 0.00000766
Iteration 258/1000 | Loss: 0.00000766
Iteration 259/1000 | Loss: 0.00000766
Iteration 260/1000 | Loss: 0.00000766
Iteration 261/1000 | Loss: 0.00000765
Iteration 262/1000 | Loss: 0.00000765
Iteration 263/1000 | Loss: 0.00000765
Iteration 264/1000 | Loss: 0.00000765
Iteration 265/1000 | Loss: 0.00000765
Iteration 266/1000 | Loss: 0.00000765
Iteration 267/1000 | Loss: 0.00000765
Iteration 268/1000 | Loss: 0.00000765
Iteration 269/1000 | Loss: 0.00000765
Iteration 270/1000 | Loss: 0.00000765
Iteration 271/1000 | Loss: 0.00000765
Iteration 272/1000 | Loss: 0.00000765
Iteration 273/1000 | Loss: 0.00000765
Iteration 274/1000 | Loss: 0.00000765
Iteration 275/1000 | Loss: 0.00000765
Iteration 276/1000 | Loss: 0.00000765
Iteration 277/1000 | Loss: 0.00000765
Iteration 278/1000 | Loss: 0.00000765
Iteration 279/1000 | Loss: 0.00000765
Iteration 280/1000 | Loss: 0.00000765
Iteration 281/1000 | Loss: 0.00000765
Iteration 282/1000 | Loss: 0.00000764
Iteration 283/1000 | Loss: 0.00000764
Iteration 284/1000 | Loss: 0.00000764
Iteration 285/1000 | Loss: 0.00000764
Iteration 286/1000 | Loss: 0.00000764
Iteration 287/1000 | Loss: 0.00000764
Iteration 288/1000 | Loss: 0.00000764
Iteration 289/1000 | Loss: 0.00000764
Iteration 290/1000 | Loss: 0.00000764
Iteration 291/1000 | Loss: 0.00000764
Iteration 292/1000 | Loss: 0.00000764
Iteration 293/1000 | Loss: 0.00000764
Iteration 294/1000 | Loss: 0.00000763
Iteration 295/1000 | Loss: 0.00000763
Iteration 296/1000 | Loss: 0.00000763
Iteration 297/1000 | Loss: 0.00000763
Iteration 298/1000 | Loss: 0.00000763
Iteration 299/1000 | Loss: 0.00000763
Iteration 300/1000 | Loss: 0.00000763
Iteration 301/1000 | Loss: 0.00000763
Iteration 302/1000 | Loss: 0.00000763
Iteration 303/1000 | Loss: 0.00000763
Iteration 304/1000 | Loss: 0.00000763
Iteration 305/1000 | Loss: 0.00000763
Iteration 306/1000 | Loss: 0.00000763
Iteration 307/1000 | Loss: 0.00000763
Iteration 308/1000 | Loss: 0.00000763
Iteration 309/1000 | Loss: 0.00000763
Iteration 310/1000 | Loss: 0.00000763
Iteration 311/1000 | Loss: 0.00000763
Iteration 312/1000 | Loss: 0.00000762
Iteration 313/1000 | Loss: 0.00000762
Iteration 314/1000 | Loss: 0.00000762
Iteration 315/1000 | Loss: 0.00000762
Iteration 316/1000 | Loss: 0.00000762
Iteration 317/1000 | Loss: 0.00000762
Iteration 318/1000 | Loss: 0.00000762
Iteration 319/1000 | Loss: 0.00000762
Iteration 320/1000 | Loss: 0.00000762
Iteration 321/1000 | Loss: 0.00000762
Iteration 322/1000 | Loss: 0.00000762
Iteration 323/1000 | Loss: 0.00000762
Iteration 324/1000 | Loss: 0.00000762
Iteration 325/1000 | Loss: 0.00000762
Iteration 326/1000 | Loss: 0.00000762
Iteration 327/1000 | Loss: 0.00000762
Iteration 328/1000 | Loss: 0.00000762
Iteration 329/1000 | Loss: 0.00000762
Iteration 330/1000 | Loss: 0.00000762
Iteration 331/1000 | Loss: 0.00000762
Iteration 332/1000 | Loss: 0.00000762
Iteration 333/1000 | Loss: 0.00000762
Iteration 334/1000 | Loss: 0.00000762
Iteration 335/1000 | Loss: 0.00000762
Iteration 336/1000 | Loss: 0.00000762
Iteration 337/1000 | Loss: 0.00000762
Iteration 338/1000 | Loss: 0.00000762
Iteration 339/1000 | Loss: 0.00000762
Iteration 340/1000 | Loss: 0.00000762
Iteration 341/1000 | Loss: 0.00000762
Iteration 342/1000 | Loss: 0.00000762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [7.624798854521941e-06, 7.624798854521941e-06, 7.624798854521941e-06, 7.624798854521941e-06, 7.624798854521941e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.624798854521941e-06

Optimization complete. Final v2v error: 2.38081693649292 mm

Highest mean error: 2.49928879737854 mm for frame 82

Lowest mean error: 2.321934461593628 mm for frame 133

Saving results

Total time: 47.378759145736694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550224
Iteration 2/25 | Loss: 0.00173762
Iteration 3/25 | Loss: 0.00135964
Iteration 4/25 | Loss: 0.00133706
Iteration 5/25 | Loss: 0.00133415
Iteration 6/25 | Loss: 0.00133363
Iteration 7/25 | Loss: 0.00133363
Iteration 8/25 | Loss: 0.00133363
Iteration 9/25 | Loss: 0.00133363
Iteration 10/25 | Loss: 0.00133363
Iteration 11/25 | Loss: 0.00133363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013336341362446547, 0.0013336341362446547, 0.0013336341362446547, 0.0013336341362446547, 0.0013336341362446547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013336341362446547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97959143
Iteration 2/25 | Loss: 0.00123512
Iteration 3/25 | Loss: 0.00123510
Iteration 4/25 | Loss: 0.00123510
Iteration 5/25 | Loss: 0.00123510
Iteration 6/25 | Loss: 0.00123510
Iteration 7/25 | Loss: 0.00123510
Iteration 8/25 | Loss: 0.00123510
Iteration 9/25 | Loss: 0.00123510
Iteration 10/25 | Loss: 0.00123510
Iteration 11/25 | Loss: 0.00123510
Iteration 12/25 | Loss: 0.00123510
Iteration 13/25 | Loss: 0.00123510
Iteration 14/25 | Loss: 0.00123510
Iteration 15/25 | Loss: 0.00123510
Iteration 16/25 | Loss: 0.00123510
Iteration 17/25 | Loss: 0.00123510
Iteration 18/25 | Loss: 0.00123510
Iteration 19/25 | Loss: 0.00123510
Iteration 20/25 | Loss: 0.00123510
Iteration 21/25 | Loss: 0.00123510
Iteration 22/25 | Loss: 0.00123510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012351003242656589, 0.0012351003242656589, 0.0012351003242656589, 0.0012351003242656589, 0.0012351003242656589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012351003242656589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123510
Iteration 2/1000 | Loss: 0.00004482
Iteration 3/1000 | Loss: 0.00002867
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002411
Iteration 6/1000 | Loss: 0.00002355
Iteration 7/1000 | Loss: 0.00002305
Iteration 8/1000 | Loss: 0.00002248
Iteration 9/1000 | Loss: 0.00002213
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002153
Iteration 12/1000 | Loss: 0.00002129
Iteration 13/1000 | Loss: 0.00002106
Iteration 14/1000 | Loss: 0.00002085
Iteration 15/1000 | Loss: 0.00002067
Iteration 16/1000 | Loss: 0.00002060
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002035
Iteration 22/1000 | Loss: 0.00002034
Iteration 23/1000 | Loss: 0.00002033
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002032
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002031
Iteration 28/1000 | Loss: 0.00002030
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002022
Iteration 32/1000 | Loss: 0.00002022
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002020
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002018
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002017
Iteration 43/1000 | Loss: 0.00002017
Iteration 44/1000 | Loss: 0.00002017
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00002016
Iteration 47/1000 | Loss: 0.00002016
Iteration 48/1000 | Loss: 0.00002016
Iteration 49/1000 | Loss: 0.00002015
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00002014
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00002014
Iteration 54/1000 | Loss: 0.00002014
Iteration 55/1000 | Loss: 0.00002014
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002013
Iteration 58/1000 | Loss: 0.00002013
Iteration 59/1000 | Loss: 0.00002013
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002013
Iteration 62/1000 | Loss: 0.00002013
Iteration 63/1000 | Loss: 0.00002013
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002011
Iteration 74/1000 | Loss: 0.00002011
Iteration 75/1000 | Loss: 0.00002010
Iteration 76/1000 | Loss: 0.00002010
Iteration 77/1000 | Loss: 0.00002010
Iteration 78/1000 | Loss: 0.00002010
Iteration 79/1000 | Loss: 0.00002010
Iteration 80/1000 | Loss: 0.00002009
Iteration 81/1000 | Loss: 0.00002009
Iteration 82/1000 | Loss: 0.00002009
Iteration 83/1000 | Loss: 0.00002009
Iteration 84/1000 | Loss: 0.00002009
Iteration 85/1000 | Loss: 0.00002009
Iteration 86/1000 | Loss: 0.00002009
Iteration 87/1000 | Loss: 0.00002009
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00002009
Iteration 90/1000 | Loss: 0.00002008
Iteration 91/1000 | Loss: 0.00002008
Iteration 92/1000 | Loss: 0.00002008
Iteration 93/1000 | Loss: 0.00002008
Iteration 94/1000 | Loss: 0.00002008
Iteration 95/1000 | Loss: 0.00002008
Iteration 96/1000 | Loss: 0.00002007
Iteration 97/1000 | Loss: 0.00002007
Iteration 98/1000 | Loss: 0.00002007
Iteration 99/1000 | Loss: 0.00002007
Iteration 100/1000 | Loss: 0.00002007
Iteration 101/1000 | Loss: 0.00002007
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00002007
Iteration 104/1000 | Loss: 0.00002006
Iteration 105/1000 | Loss: 0.00002006
Iteration 106/1000 | Loss: 0.00002006
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002006
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002005
Iteration 118/1000 | Loss: 0.00002005
Iteration 119/1000 | Loss: 0.00002005
Iteration 120/1000 | Loss: 0.00002005
Iteration 121/1000 | Loss: 0.00002005
Iteration 122/1000 | Loss: 0.00002005
Iteration 123/1000 | Loss: 0.00002005
Iteration 124/1000 | Loss: 0.00002004
Iteration 125/1000 | Loss: 0.00002004
Iteration 126/1000 | Loss: 0.00002004
Iteration 127/1000 | Loss: 0.00002004
Iteration 128/1000 | Loss: 0.00002004
Iteration 129/1000 | Loss: 0.00002004
Iteration 130/1000 | Loss: 0.00002004
Iteration 131/1000 | Loss: 0.00002004
Iteration 132/1000 | Loss: 0.00002004
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002003
Iteration 136/1000 | Loss: 0.00002003
Iteration 137/1000 | Loss: 0.00002003
Iteration 138/1000 | Loss: 0.00002003
Iteration 139/1000 | Loss: 0.00002003
Iteration 140/1000 | Loss: 0.00002003
Iteration 141/1000 | Loss: 0.00002003
Iteration 142/1000 | Loss: 0.00002003
Iteration 143/1000 | Loss: 0.00002002
Iteration 144/1000 | Loss: 0.00002002
Iteration 145/1000 | Loss: 0.00002002
Iteration 146/1000 | Loss: 0.00002002
Iteration 147/1000 | Loss: 0.00002002
Iteration 148/1000 | Loss: 0.00002002
Iteration 149/1000 | Loss: 0.00002002
Iteration 150/1000 | Loss: 0.00002001
Iteration 151/1000 | Loss: 0.00002001
Iteration 152/1000 | Loss: 0.00002001
Iteration 153/1000 | Loss: 0.00002001
Iteration 154/1000 | Loss: 0.00002001
Iteration 155/1000 | Loss: 0.00002001
Iteration 156/1000 | Loss: 0.00002001
Iteration 157/1000 | Loss: 0.00002001
Iteration 158/1000 | Loss: 0.00002001
Iteration 159/1000 | Loss: 0.00002001
Iteration 160/1000 | Loss: 0.00002000
Iteration 161/1000 | Loss: 0.00002000
Iteration 162/1000 | Loss: 0.00002000
Iteration 163/1000 | Loss: 0.00002000
Iteration 164/1000 | Loss: 0.00002000
Iteration 165/1000 | Loss: 0.00002000
Iteration 166/1000 | Loss: 0.00002000
Iteration 167/1000 | Loss: 0.00002000
Iteration 168/1000 | Loss: 0.00002000
Iteration 169/1000 | Loss: 0.00002000
Iteration 170/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.0002391465823166e-05, 2.0002391465823166e-05, 2.0002391465823166e-05, 2.0002391465823166e-05, 2.0002391465823166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0002391465823166e-05

Optimization complete. Final v2v error: 3.5663135051727295 mm

Highest mean error: 4.589746952056885 mm for frame 59

Lowest mean error: 2.720581531524658 mm for frame 136

Saving results

Total time: 45.195173263549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877089
Iteration 2/25 | Loss: 0.00164966
Iteration 3/25 | Loss: 0.00137874
Iteration 4/25 | Loss: 0.00133318
Iteration 5/25 | Loss: 0.00133510
Iteration 6/25 | Loss: 0.00133527
Iteration 7/25 | Loss: 0.00131410
Iteration 8/25 | Loss: 0.00129420
Iteration 9/25 | Loss: 0.00127994
Iteration 10/25 | Loss: 0.00127203
Iteration 11/25 | Loss: 0.00127769
Iteration 12/25 | Loss: 0.00128575
Iteration 13/25 | Loss: 0.00128253
Iteration 14/25 | Loss: 0.00127364
Iteration 15/25 | Loss: 0.00126749
Iteration 16/25 | Loss: 0.00126355
Iteration 17/25 | Loss: 0.00126229
Iteration 18/25 | Loss: 0.00126455
Iteration 19/25 | Loss: 0.00126466
Iteration 20/25 | Loss: 0.00126498
Iteration 21/25 | Loss: 0.00126474
Iteration 22/25 | Loss: 0.00126414
Iteration 23/25 | Loss: 0.00126348
Iteration 24/25 | Loss: 0.00126435
Iteration 25/25 | Loss: 0.00126420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22879457
Iteration 2/25 | Loss: 0.00158977
Iteration 3/25 | Loss: 0.00158977
Iteration 4/25 | Loss: 0.00158977
Iteration 5/25 | Loss: 0.00158977
Iteration 6/25 | Loss: 0.00158977
Iteration 7/25 | Loss: 0.00158977
Iteration 8/25 | Loss: 0.00158977
Iteration 9/25 | Loss: 0.00158977
Iteration 10/25 | Loss: 0.00158977
Iteration 11/25 | Loss: 0.00158977
Iteration 12/25 | Loss: 0.00158977
Iteration 13/25 | Loss: 0.00158977
Iteration 14/25 | Loss: 0.00158977
Iteration 15/25 | Loss: 0.00158977
Iteration 16/25 | Loss: 0.00158977
Iteration 17/25 | Loss: 0.00158977
Iteration 18/25 | Loss: 0.00158977
Iteration 19/25 | Loss: 0.00158977
Iteration 20/25 | Loss: 0.00158977
Iteration 21/25 | Loss: 0.00158977
Iteration 22/25 | Loss: 0.00158977
Iteration 23/25 | Loss: 0.00158977
Iteration 24/25 | Loss: 0.00158977
Iteration 25/25 | Loss: 0.00158977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158977
Iteration 2/1000 | Loss: 0.00014512
Iteration 3/1000 | Loss: 0.00028230
Iteration 4/1000 | Loss: 0.00004440
Iteration 5/1000 | Loss: 0.00003116
Iteration 6/1000 | Loss: 0.00002737
Iteration 7/1000 | Loss: 0.00003039
Iteration 8/1000 | Loss: 0.00002738
Iteration 9/1000 | Loss: 0.00002867
Iteration 10/1000 | Loss: 0.00002894
Iteration 11/1000 | Loss: 0.00002734
Iteration 12/1000 | Loss: 0.00002540
Iteration 13/1000 | Loss: 0.00003145
Iteration 14/1000 | Loss: 0.00002961
Iteration 15/1000 | Loss: 0.00002669
Iteration 16/1000 | Loss: 0.00002353
Iteration 17/1000 | Loss: 0.00002716
Iteration 18/1000 | Loss: 0.00002360
Iteration 19/1000 | Loss: 0.00002334
Iteration 20/1000 | Loss: 0.00002259
Iteration 21/1000 | Loss: 0.00003708
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002876
Iteration 24/1000 | Loss: 0.00002637
Iteration 25/1000 | Loss: 0.00003105
Iteration 26/1000 | Loss: 0.00002840
Iteration 27/1000 | Loss: 0.00003641
Iteration 28/1000 | Loss: 0.00002742
Iteration 29/1000 | Loss: 0.00002204
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002061
Iteration 32/1000 | Loss: 0.00002038
Iteration 33/1000 | Loss: 0.00003449
Iteration 34/1000 | Loss: 0.00002459
Iteration 35/1000 | Loss: 0.00003413
Iteration 36/1000 | Loss: 0.00002455
Iteration 37/1000 | Loss: 0.00003321
Iteration 38/1000 | Loss: 0.00002537
Iteration 39/1000 | Loss: 0.00003400
Iteration 40/1000 | Loss: 0.00002538
Iteration 41/1000 | Loss: 0.00003355
Iteration 42/1000 | Loss: 0.00003355
Iteration 43/1000 | Loss: 0.00002595
Iteration 44/1000 | Loss: 0.00003407
Iteration 45/1000 | Loss: 0.00002315
Iteration 46/1000 | Loss: 0.00002196
Iteration 47/1000 | Loss: 0.00002153
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002113
Iteration 50/1000 | Loss: 0.00002104
Iteration 51/1000 | Loss: 0.00002100
Iteration 52/1000 | Loss: 0.00002100
Iteration 53/1000 | Loss: 0.00002096
Iteration 54/1000 | Loss: 0.00005095
Iteration 55/1000 | Loss: 0.00004249
Iteration 56/1000 | Loss: 0.00002597
Iteration 57/1000 | Loss: 0.00002164
Iteration 58/1000 | Loss: 0.00004728
Iteration 59/1000 | Loss: 0.00002700
Iteration 60/1000 | Loss: 0.00003787
Iteration 61/1000 | Loss: 0.00002705
Iteration 62/1000 | Loss: 0.00002458
Iteration 63/1000 | Loss: 0.00002086
Iteration 64/1000 | Loss: 0.00004810
Iteration 65/1000 | Loss: 0.00003545
Iteration 66/1000 | Loss: 0.00002157
Iteration 67/1000 | Loss: 0.00004736
Iteration 68/1000 | Loss: 0.00003287
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00004700
Iteration 71/1000 | Loss: 0.00003046
Iteration 72/1000 | Loss: 0.00004321
Iteration 73/1000 | Loss: 0.00002912
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002161
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002023
Iteration 78/1000 | Loss: 0.00002000
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001961
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001928
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001928
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001927
Iteration 99/1000 | Loss: 0.00001927
Iteration 100/1000 | Loss: 0.00001927
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001924
Iteration 114/1000 | Loss: 0.00001924
Iteration 115/1000 | Loss: 0.00001924
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001922
Iteration 131/1000 | Loss: 0.00001922
Iteration 132/1000 | Loss: 0.00001922
Iteration 133/1000 | Loss: 0.00001922
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Iteration 137/1000 | Loss: 0.00001922
Iteration 138/1000 | Loss: 0.00001922
Iteration 139/1000 | Loss: 0.00001922
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00001922
Iteration 142/1000 | Loss: 0.00001922
Iteration 143/1000 | Loss: 0.00001921
Iteration 144/1000 | Loss: 0.00001921
Iteration 145/1000 | Loss: 0.00001921
Iteration 146/1000 | Loss: 0.00001921
Iteration 147/1000 | Loss: 0.00001921
Iteration 148/1000 | Loss: 0.00001921
Iteration 149/1000 | Loss: 0.00001921
Iteration 150/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.9213965060771443e-05, 1.9213965060771443e-05, 1.9213965060771443e-05, 1.9213965060771443e-05, 1.9213965060771443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9213965060771443e-05

Optimization complete. Final v2v error: 3.66253924369812 mm

Highest mean error: 6.302947521209717 mm for frame 98

Lowest mean error: 3.112142324447632 mm for frame 134

Saving results

Total time: 163.7673351764679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409018
Iteration 2/25 | Loss: 0.00128524
Iteration 3/25 | Loss: 0.00121176
Iteration 4/25 | Loss: 0.00119323
Iteration 5/25 | Loss: 0.00118694
Iteration 6/25 | Loss: 0.00118625
Iteration 7/25 | Loss: 0.00118622
Iteration 8/25 | Loss: 0.00118622
Iteration 9/25 | Loss: 0.00118622
Iteration 10/25 | Loss: 0.00118622
Iteration 11/25 | Loss: 0.00118622
Iteration 12/25 | Loss: 0.00118622
Iteration 13/25 | Loss: 0.00118622
Iteration 14/25 | Loss: 0.00118622
Iteration 15/25 | Loss: 0.00118622
Iteration 16/25 | Loss: 0.00118622
Iteration 17/25 | Loss: 0.00118622
Iteration 18/25 | Loss: 0.00118622
Iteration 19/25 | Loss: 0.00118622
Iteration 20/25 | Loss: 0.00118622
Iteration 21/25 | Loss: 0.00118622
Iteration 22/25 | Loss: 0.00118622
Iteration 23/25 | Loss: 0.00118622
Iteration 24/25 | Loss: 0.00118622
Iteration 25/25 | Loss: 0.00118622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29296505
Iteration 2/25 | Loss: 0.00124821
Iteration 3/25 | Loss: 0.00124821
Iteration 4/25 | Loss: 0.00124821
Iteration 5/25 | Loss: 0.00124821
Iteration 6/25 | Loss: 0.00124821
Iteration 7/25 | Loss: 0.00124821
Iteration 8/25 | Loss: 0.00124821
Iteration 9/25 | Loss: 0.00124821
Iteration 10/25 | Loss: 0.00124821
Iteration 11/25 | Loss: 0.00124821
Iteration 12/25 | Loss: 0.00124821
Iteration 13/25 | Loss: 0.00124821
Iteration 14/25 | Loss: 0.00124821
Iteration 15/25 | Loss: 0.00124821
Iteration 16/25 | Loss: 0.00124821
Iteration 17/25 | Loss: 0.00124821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001248207758180797, 0.001248207758180797, 0.001248207758180797, 0.001248207758180797, 0.001248207758180797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248207758180797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124821
Iteration 2/1000 | Loss: 0.00002012
Iteration 3/1000 | Loss: 0.00001584
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001189
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001187
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001180
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.1795651516877115e-05, 1.1795651516877115e-05, 1.1795651516877115e-05, 1.1795651516877115e-05, 1.1795651516877115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1795651516877115e-05

Optimization complete. Final v2v error: 3.0030465126037598 mm

Highest mean error: 3.030959129333496 mm for frame 156

Lowest mean error: 2.9362916946411133 mm for frame 121

Saving results

Total time: 30.522169828414917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027067
Iteration 2/25 | Loss: 0.00227198
Iteration 3/25 | Loss: 0.00162382
Iteration 4/25 | Loss: 0.00147053
Iteration 5/25 | Loss: 0.00142818
Iteration 6/25 | Loss: 0.00132017
Iteration 7/25 | Loss: 0.00126289
Iteration 8/25 | Loss: 0.00123423
Iteration 9/25 | Loss: 0.00119864
Iteration 10/25 | Loss: 0.00118620
Iteration 11/25 | Loss: 0.00118366
Iteration 12/25 | Loss: 0.00117899
Iteration 13/25 | Loss: 0.00117770
Iteration 14/25 | Loss: 0.00117733
Iteration 15/25 | Loss: 0.00117716
Iteration 16/25 | Loss: 0.00117706
Iteration 17/25 | Loss: 0.00117696
Iteration 18/25 | Loss: 0.00117681
Iteration 19/25 | Loss: 0.00117654
Iteration 20/25 | Loss: 0.00117552
Iteration 21/25 | Loss: 0.00117388
Iteration 22/25 | Loss: 0.00117334
Iteration 23/25 | Loss: 0.00117321
Iteration 24/25 | Loss: 0.00117312
Iteration 25/25 | Loss: 0.00117308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80100799
Iteration 2/25 | Loss: 0.00138851
Iteration 3/25 | Loss: 0.00130018
Iteration 4/25 | Loss: 0.00130018
Iteration 5/25 | Loss: 0.00130018
Iteration 6/25 | Loss: 0.00130018
Iteration 7/25 | Loss: 0.00130017
Iteration 8/25 | Loss: 0.00130017
Iteration 9/25 | Loss: 0.00130017
Iteration 10/25 | Loss: 0.00130017
Iteration 11/25 | Loss: 0.00130017
Iteration 12/25 | Loss: 0.00130017
Iteration 13/25 | Loss: 0.00130017
Iteration 14/25 | Loss: 0.00130017
Iteration 15/25 | Loss: 0.00130017
Iteration 16/25 | Loss: 0.00130017
Iteration 17/25 | Loss: 0.00130017
Iteration 18/25 | Loss: 0.00130017
Iteration 19/25 | Loss: 0.00130017
Iteration 20/25 | Loss: 0.00130017
Iteration 21/25 | Loss: 0.00130017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013001724146306515, 0.0013001724146306515, 0.0013001724146306515, 0.0013001724146306515, 0.0013001724146306515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013001724146306515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130017
Iteration 2/1000 | Loss: 0.00010423
Iteration 3/1000 | Loss: 0.00014846
Iteration 4/1000 | Loss: 0.00003029
Iteration 5/1000 | Loss: 0.00011992
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00003691
Iteration 8/1000 | Loss: 0.00001399
Iteration 9/1000 | Loss: 0.00009184
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00007877
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00010788
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00003241
Iteration 23/1000 | Loss: 0.00005671
Iteration 24/1000 | Loss: 0.00009613
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00003317
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001206
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00002490
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001184
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001171
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001171
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001170
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001169
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001165
Iteration 113/1000 | Loss: 0.00001165
Iteration 114/1000 | Loss: 0.00001165
Iteration 115/1000 | Loss: 0.00001165
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001164
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001163
Iteration 137/1000 | Loss: 0.00001163
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001163
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001163
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001163
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001163
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.1628879292402416e-05, 1.1628879292402416e-05, 1.1628879292402416e-05, 1.1628879292402416e-05, 1.1628879292402416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1628879292402416e-05

Optimization complete. Final v2v error: 2.9332759380340576 mm

Highest mean error: 3.2247517108917236 mm for frame 115

Lowest mean error: 2.6111390590667725 mm for frame 63

Saving results

Total time: 98.85111832618713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026145
Iteration 2/25 | Loss: 0.00277413
Iteration 3/25 | Loss: 0.00195339
Iteration 4/25 | Loss: 0.00164579
Iteration 5/25 | Loss: 0.00152681
Iteration 6/25 | Loss: 0.00145427
Iteration 7/25 | Loss: 0.00144623
Iteration 8/25 | Loss: 0.00141848
Iteration 9/25 | Loss: 0.00139045
Iteration 10/25 | Loss: 0.00137377
Iteration 11/25 | Loss: 0.00137469
Iteration 12/25 | Loss: 0.00136134
Iteration 13/25 | Loss: 0.00134798
Iteration 14/25 | Loss: 0.00135155
Iteration 15/25 | Loss: 0.00134640
Iteration 16/25 | Loss: 0.00133690
Iteration 17/25 | Loss: 0.00132952
Iteration 18/25 | Loss: 0.00132485
Iteration 19/25 | Loss: 0.00132545
Iteration 20/25 | Loss: 0.00132217
Iteration 21/25 | Loss: 0.00132125
Iteration 22/25 | Loss: 0.00132076
Iteration 23/25 | Loss: 0.00132077
Iteration 24/25 | Loss: 0.00132131
Iteration 25/25 | Loss: 0.00132075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22407103
Iteration 2/25 | Loss: 0.00430930
Iteration 3/25 | Loss: 0.00269512
Iteration 4/25 | Loss: 0.00269293
Iteration 5/25 | Loss: 0.00269293
Iteration 6/25 | Loss: 0.00269293
Iteration 7/25 | Loss: 0.00269293
Iteration 8/25 | Loss: 0.00269293
Iteration 9/25 | Loss: 0.00269293
Iteration 10/25 | Loss: 0.00269293
Iteration 11/25 | Loss: 0.00269293
Iteration 12/25 | Loss: 0.00269293
Iteration 13/25 | Loss: 0.00269293
Iteration 14/25 | Loss: 0.00269293
Iteration 15/25 | Loss: 0.00269293
Iteration 16/25 | Loss: 0.00269293
Iteration 17/25 | Loss: 0.00269293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026929262094199657, 0.0026929262094199657, 0.0026929262094199657, 0.0026929262094199657, 0.0026929262094199657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026929262094199657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269293
Iteration 2/1000 | Loss: 0.00025136
Iteration 3/1000 | Loss: 0.00039047
Iteration 4/1000 | Loss: 0.00044351
Iteration 5/1000 | Loss: 0.00052620
Iteration 6/1000 | Loss: 0.00041156
Iteration 7/1000 | Loss: 0.00114796
Iteration 8/1000 | Loss: 0.00032797
Iteration 9/1000 | Loss: 0.00014243
Iteration 10/1000 | Loss: 0.00013723
Iteration 11/1000 | Loss: 0.00012684
Iteration 12/1000 | Loss: 0.00008770
Iteration 13/1000 | Loss: 0.00009201
Iteration 14/1000 | Loss: 0.00077498
Iteration 15/1000 | Loss: 0.00082453
Iteration 16/1000 | Loss: 0.00008940
Iteration 17/1000 | Loss: 0.00008980
Iteration 18/1000 | Loss: 0.00019123
Iteration 19/1000 | Loss: 0.00218685
Iteration 20/1000 | Loss: 0.00149827
Iteration 21/1000 | Loss: 0.00183843
Iteration 22/1000 | Loss: 0.00511591
Iteration 23/1000 | Loss: 0.00352550
Iteration 24/1000 | Loss: 0.00312636
Iteration 25/1000 | Loss: 0.00339715
Iteration 26/1000 | Loss: 0.00210255
Iteration 27/1000 | Loss: 0.00075210
Iteration 28/1000 | Loss: 0.00062461
Iteration 29/1000 | Loss: 0.00016807
Iteration 30/1000 | Loss: 0.00009833
Iteration 31/1000 | Loss: 0.00059078
Iteration 32/1000 | Loss: 0.00233707
Iteration 33/1000 | Loss: 0.00022863
Iteration 34/1000 | Loss: 0.00084023
Iteration 35/1000 | Loss: 0.00095743
Iteration 36/1000 | Loss: 0.00010953
Iteration 37/1000 | Loss: 0.00042980
Iteration 38/1000 | Loss: 0.00021785
Iteration 39/1000 | Loss: 0.00021814
Iteration 40/1000 | Loss: 0.00010331
Iteration 41/1000 | Loss: 0.00082519
Iteration 42/1000 | Loss: 0.00037829
Iteration 43/1000 | Loss: 0.00043333
Iteration 44/1000 | Loss: 0.00025870
Iteration 45/1000 | Loss: 0.00030172
Iteration 46/1000 | Loss: 0.00142176
Iteration 47/1000 | Loss: 0.00185158
Iteration 48/1000 | Loss: 0.00320615
Iteration 49/1000 | Loss: 0.00117036
Iteration 50/1000 | Loss: 0.00079809
Iteration 51/1000 | Loss: 0.00010814
Iteration 52/1000 | Loss: 0.00053981
Iteration 53/1000 | Loss: 0.00035982
Iteration 54/1000 | Loss: 0.00038361
Iteration 55/1000 | Loss: 0.00006479
Iteration 56/1000 | Loss: 0.00005381
Iteration 57/1000 | Loss: 0.00006089
Iteration 58/1000 | Loss: 0.00010965
Iteration 59/1000 | Loss: 0.00004156
Iteration 60/1000 | Loss: 0.00050549
Iteration 61/1000 | Loss: 0.00026036
Iteration 62/1000 | Loss: 0.00035431
Iteration 63/1000 | Loss: 0.00005440
Iteration 64/1000 | Loss: 0.00002292
Iteration 65/1000 | Loss: 0.00002103
Iteration 66/1000 | Loss: 0.00007379
Iteration 67/1000 | Loss: 0.00002412
Iteration 68/1000 | Loss: 0.00002100
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00007847
Iteration 71/1000 | Loss: 0.00001865
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001641
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001594
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00007356
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002529
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001577
Iteration 99/1000 | Loss: 0.00001577
Iteration 100/1000 | Loss: 0.00001576
Iteration 101/1000 | Loss: 0.00001576
Iteration 102/1000 | Loss: 0.00001576
Iteration 103/1000 | Loss: 0.00001575
Iteration 104/1000 | Loss: 0.00001575
Iteration 105/1000 | Loss: 0.00001575
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001573
Iteration 110/1000 | Loss: 0.00001573
Iteration 111/1000 | Loss: 0.00001573
Iteration 112/1000 | Loss: 0.00001572
Iteration 113/1000 | Loss: 0.00001572
Iteration 114/1000 | Loss: 0.00001572
Iteration 115/1000 | Loss: 0.00001572
Iteration 116/1000 | Loss: 0.00001572
Iteration 117/1000 | Loss: 0.00001572
Iteration 118/1000 | Loss: 0.00001571
Iteration 119/1000 | Loss: 0.00001571
Iteration 120/1000 | Loss: 0.00001570
Iteration 121/1000 | Loss: 0.00001570
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001566
Iteration 131/1000 | Loss: 0.00001566
Iteration 132/1000 | Loss: 0.00001565
Iteration 133/1000 | Loss: 0.00001565
Iteration 134/1000 | Loss: 0.00001565
Iteration 135/1000 | Loss: 0.00001564
Iteration 136/1000 | Loss: 0.00001564
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001562
Iteration 145/1000 | Loss: 0.00001562
Iteration 146/1000 | Loss: 0.00001562
Iteration 147/1000 | Loss: 0.00001562
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001562
Iteration 151/1000 | Loss: 0.00001561
Iteration 152/1000 | Loss: 0.00001561
Iteration 153/1000 | Loss: 0.00001561
Iteration 154/1000 | Loss: 0.00001560
Iteration 155/1000 | Loss: 0.00001560
Iteration 156/1000 | Loss: 0.00001560
Iteration 157/1000 | Loss: 0.00001559
Iteration 158/1000 | Loss: 0.00001559
Iteration 159/1000 | Loss: 0.00001559
Iteration 160/1000 | Loss: 0.00001559
Iteration 161/1000 | Loss: 0.00001558
Iteration 162/1000 | Loss: 0.00001558
Iteration 163/1000 | Loss: 0.00001558
Iteration 164/1000 | Loss: 0.00001558
Iteration 165/1000 | Loss: 0.00001558
Iteration 166/1000 | Loss: 0.00001558
Iteration 167/1000 | Loss: 0.00001557
Iteration 168/1000 | Loss: 0.00001556
Iteration 169/1000 | Loss: 0.00001556
Iteration 170/1000 | Loss: 0.00001556
Iteration 171/1000 | Loss: 0.00001555
Iteration 172/1000 | Loss: 0.00001555
Iteration 173/1000 | Loss: 0.00001555
Iteration 174/1000 | Loss: 0.00001555
Iteration 175/1000 | Loss: 0.00001555
Iteration 176/1000 | Loss: 0.00001555
Iteration 177/1000 | Loss: 0.00001555
Iteration 178/1000 | Loss: 0.00001555
Iteration 179/1000 | Loss: 0.00001555
Iteration 180/1000 | Loss: 0.00001555
Iteration 181/1000 | Loss: 0.00001554
Iteration 182/1000 | Loss: 0.00001554
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001554
Iteration 185/1000 | Loss: 0.00001554
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001554
Iteration 188/1000 | Loss: 0.00001554
Iteration 189/1000 | Loss: 0.00001554
Iteration 190/1000 | Loss: 0.00001554
Iteration 191/1000 | Loss: 0.00001554
Iteration 192/1000 | Loss: 0.00001554
Iteration 193/1000 | Loss: 0.00001554
Iteration 194/1000 | Loss: 0.00001554
Iteration 195/1000 | Loss: 0.00001554
Iteration 196/1000 | Loss: 0.00001554
Iteration 197/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.554304799356032e-05, 1.554304799356032e-05, 1.554304799356032e-05, 1.554304799356032e-05, 1.554304799356032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.554304799356032e-05

Optimization complete. Final v2v error: 3.3554677963256836 mm

Highest mean error: 4.6414570808410645 mm for frame 197

Lowest mean error: 2.7654123306274414 mm for frame 220

Saving results

Total time: 189.6230878829956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810706
Iteration 2/25 | Loss: 0.00128639
Iteration 3/25 | Loss: 0.00118415
Iteration 4/25 | Loss: 0.00117273
Iteration 5/25 | Loss: 0.00117058
Iteration 6/25 | Loss: 0.00117040
Iteration 7/25 | Loss: 0.00117040
Iteration 8/25 | Loss: 0.00117040
Iteration 9/25 | Loss: 0.00117040
Iteration 10/25 | Loss: 0.00117040
Iteration 11/25 | Loss: 0.00117040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001170403091236949, 0.001170403091236949, 0.001170403091236949, 0.001170403091236949, 0.001170403091236949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001170403091236949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28859258
Iteration 2/25 | Loss: 0.00120563
Iteration 3/25 | Loss: 0.00120561
Iteration 4/25 | Loss: 0.00120561
Iteration 5/25 | Loss: 0.00120561
Iteration 6/25 | Loss: 0.00120561
Iteration 7/25 | Loss: 0.00120561
Iteration 8/25 | Loss: 0.00120561
Iteration 9/25 | Loss: 0.00120561
Iteration 10/25 | Loss: 0.00120561
Iteration 11/25 | Loss: 0.00120561
Iteration 12/25 | Loss: 0.00120561
Iteration 13/25 | Loss: 0.00120561
Iteration 14/25 | Loss: 0.00120561
Iteration 15/25 | Loss: 0.00120561
Iteration 16/25 | Loss: 0.00120561
Iteration 17/25 | Loss: 0.00120561
Iteration 18/25 | Loss: 0.00120561
Iteration 19/25 | Loss: 0.00120561
Iteration 20/25 | Loss: 0.00120561
Iteration 21/25 | Loss: 0.00120561
Iteration 22/25 | Loss: 0.00120561
Iteration 23/25 | Loss: 0.00120561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012056081322953105, 0.0012056081322953105, 0.0012056081322953105, 0.0012056081322953105, 0.0012056081322953105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012056081322953105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120561
Iteration 2/1000 | Loss: 0.00001921
Iteration 3/1000 | Loss: 0.00001320
Iteration 4/1000 | Loss: 0.00001180
Iteration 5/1000 | Loss: 0.00001068
Iteration 6/1000 | Loss: 0.00001015
Iteration 7/1000 | Loss: 0.00000962
Iteration 8/1000 | Loss: 0.00000934
Iteration 9/1000 | Loss: 0.00000926
Iteration 10/1000 | Loss: 0.00000924
Iteration 11/1000 | Loss: 0.00000902
Iteration 12/1000 | Loss: 0.00000886
Iteration 13/1000 | Loss: 0.00000880
Iteration 14/1000 | Loss: 0.00000880
Iteration 15/1000 | Loss: 0.00000879
Iteration 16/1000 | Loss: 0.00000876
Iteration 17/1000 | Loss: 0.00000875
Iteration 18/1000 | Loss: 0.00000874
Iteration 19/1000 | Loss: 0.00000871
Iteration 20/1000 | Loss: 0.00000869
Iteration 21/1000 | Loss: 0.00000864
Iteration 22/1000 | Loss: 0.00000862
Iteration 23/1000 | Loss: 0.00000861
Iteration 24/1000 | Loss: 0.00000860
Iteration 25/1000 | Loss: 0.00000859
Iteration 26/1000 | Loss: 0.00000858
Iteration 27/1000 | Loss: 0.00000857
Iteration 28/1000 | Loss: 0.00000857
Iteration 29/1000 | Loss: 0.00000856
Iteration 30/1000 | Loss: 0.00000854
Iteration 31/1000 | Loss: 0.00000854
Iteration 32/1000 | Loss: 0.00000853
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000852
Iteration 35/1000 | Loss: 0.00000852
Iteration 36/1000 | Loss: 0.00000851
Iteration 37/1000 | Loss: 0.00000850
Iteration 38/1000 | Loss: 0.00000848
Iteration 39/1000 | Loss: 0.00000848
Iteration 40/1000 | Loss: 0.00000848
Iteration 41/1000 | Loss: 0.00000848
Iteration 42/1000 | Loss: 0.00000848
Iteration 43/1000 | Loss: 0.00000848
Iteration 44/1000 | Loss: 0.00000847
Iteration 45/1000 | Loss: 0.00000847
Iteration 46/1000 | Loss: 0.00000847
Iteration 47/1000 | Loss: 0.00000846
Iteration 48/1000 | Loss: 0.00000846
Iteration 49/1000 | Loss: 0.00000846
Iteration 50/1000 | Loss: 0.00000845
Iteration 51/1000 | Loss: 0.00000845
Iteration 52/1000 | Loss: 0.00000845
Iteration 53/1000 | Loss: 0.00000845
Iteration 54/1000 | Loss: 0.00000845
Iteration 55/1000 | Loss: 0.00000845
Iteration 56/1000 | Loss: 0.00000845
Iteration 57/1000 | Loss: 0.00000844
Iteration 58/1000 | Loss: 0.00000844
Iteration 59/1000 | Loss: 0.00000844
Iteration 60/1000 | Loss: 0.00000844
Iteration 61/1000 | Loss: 0.00000844
Iteration 62/1000 | Loss: 0.00000844
Iteration 63/1000 | Loss: 0.00000844
Iteration 64/1000 | Loss: 0.00000844
Iteration 65/1000 | Loss: 0.00000844
Iteration 66/1000 | Loss: 0.00000843
Iteration 67/1000 | Loss: 0.00000843
Iteration 68/1000 | Loss: 0.00000843
Iteration 69/1000 | Loss: 0.00000842
Iteration 70/1000 | Loss: 0.00000842
Iteration 71/1000 | Loss: 0.00000842
Iteration 72/1000 | Loss: 0.00000842
Iteration 73/1000 | Loss: 0.00000841
Iteration 74/1000 | Loss: 0.00000841
Iteration 75/1000 | Loss: 0.00000841
Iteration 76/1000 | Loss: 0.00000840
Iteration 77/1000 | Loss: 0.00000840
Iteration 78/1000 | Loss: 0.00000839
Iteration 79/1000 | Loss: 0.00000839
Iteration 80/1000 | Loss: 0.00000839
Iteration 81/1000 | Loss: 0.00000839
Iteration 82/1000 | Loss: 0.00000838
Iteration 83/1000 | Loss: 0.00000838
Iteration 84/1000 | Loss: 0.00000838
Iteration 85/1000 | Loss: 0.00000837
Iteration 86/1000 | Loss: 0.00000837
Iteration 87/1000 | Loss: 0.00000837
Iteration 88/1000 | Loss: 0.00000837
Iteration 89/1000 | Loss: 0.00000837
Iteration 90/1000 | Loss: 0.00000837
Iteration 91/1000 | Loss: 0.00000837
Iteration 92/1000 | Loss: 0.00000836
Iteration 93/1000 | Loss: 0.00000836
Iteration 94/1000 | Loss: 0.00000835
Iteration 95/1000 | Loss: 0.00000835
Iteration 96/1000 | Loss: 0.00000835
Iteration 97/1000 | Loss: 0.00000835
Iteration 98/1000 | Loss: 0.00000834
Iteration 99/1000 | Loss: 0.00000833
Iteration 100/1000 | Loss: 0.00000833
Iteration 101/1000 | Loss: 0.00000833
Iteration 102/1000 | Loss: 0.00000832
Iteration 103/1000 | Loss: 0.00000832
Iteration 104/1000 | Loss: 0.00000832
Iteration 105/1000 | Loss: 0.00000832
Iteration 106/1000 | Loss: 0.00000832
Iteration 107/1000 | Loss: 0.00000831
Iteration 108/1000 | Loss: 0.00000831
Iteration 109/1000 | Loss: 0.00000831
Iteration 110/1000 | Loss: 0.00000831
Iteration 111/1000 | Loss: 0.00000830
Iteration 112/1000 | Loss: 0.00000830
Iteration 113/1000 | Loss: 0.00000830
Iteration 114/1000 | Loss: 0.00000829
Iteration 115/1000 | Loss: 0.00000829
Iteration 116/1000 | Loss: 0.00000828
Iteration 117/1000 | Loss: 0.00000828
Iteration 118/1000 | Loss: 0.00000828
Iteration 119/1000 | Loss: 0.00000828
Iteration 120/1000 | Loss: 0.00000828
Iteration 121/1000 | Loss: 0.00000828
Iteration 122/1000 | Loss: 0.00000828
Iteration 123/1000 | Loss: 0.00000827
Iteration 124/1000 | Loss: 0.00000827
Iteration 125/1000 | Loss: 0.00000827
Iteration 126/1000 | Loss: 0.00000826
Iteration 127/1000 | Loss: 0.00000825
Iteration 128/1000 | Loss: 0.00000825
Iteration 129/1000 | Loss: 0.00000825
Iteration 130/1000 | Loss: 0.00000825
Iteration 131/1000 | Loss: 0.00000824
Iteration 132/1000 | Loss: 0.00000824
Iteration 133/1000 | Loss: 0.00000824
Iteration 134/1000 | Loss: 0.00000824
Iteration 135/1000 | Loss: 0.00000824
Iteration 136/1000 | Loss: 0.00000824
Iteration 137/1000 | Loss: 0.00000824
Iteration 138/1000 | Loss: 0.00000824
Iteration 139/1000 | Loss: 0.00000824
Iteration 140/1000 | Loss: 0.00000824
Iteration 141/1000 | Loss: 0.00000824
Iteration 142/1000 | Loss: 0.00000824
Iteration 143/1000 | Loss: 0.00000823
Iteration 144/1000 | Loss: 0.00000823
Iteration 145/1000 | Loss: 0.00000823
Iteration 146/1000 | Loss: 0.00000823
Iteration 147/1000 | Loss: 0.00000822
Iteration 148/1000 | Loss: 0.00000822
Iteration 149/1000 | Loss: 0.00000822
Iteration 150/1000 | Loss: 0.00000822
Iteration 151/1000 | Loss: 0.00000822
Iteration 152/1000 | Loss: 0.00000821
Iteration 153/1000 | Loss: 0.00000821
Iteration 154/1000 | Loss: 0.00000821
Iteration 155/1000 | Loss: 0.00000821
Iteration 156/1000 | Loss: 0.00000821
Iteration 157/1000 | Loss: 0.00000820
Iteration 158/1000 | Loss: 0.00000820
Iteration 159/1000 | Loss: 0.00000820
Iteration 160/1000 | Loss: 0.00000820
Iteration 161/1000 | Loss: 0.00000820
Iteration 162/1000 | Loss: 0.00000820
Iteration 163/1000 | Loss: 0.00000820
Iteration 164/1000 | Loss: 0.00000820
Iteration 165/1000 | Loss: 0.00000820
Iteration 166/1000 | Loss: 0.00000820
Iteration 167/1000 | Loss: 0.00000819
Iteration 168/1000 | Loss: 0.00000819
Iteration 169/1000 | Loss: 0.00000819
Iteration 170/1000 | Loss: 0.00000819
Iteration 171/1000 | Loss: 0.00000819
Iteration 172/1000 | Loss: 0.00000819
Iteration 173/1000 | Loss: 0.00000819
Iteration 174/1000 | Loss: 0.00000819
Iteration 175/1000 | Loss: 0.00000819
Iteration 176/1000 | Loss: 0.00000819
Iteration 177/1000 | Loss: 0.00000819
Iteration 178/1000 | Loss: 0.00000819
Iteration 179/1000 | Loss: 0.00000819
Iteration 180/1000 | Loss: 0.00000819
Iteration 181/1000 | Loss: 0.00000819
Iteration 182/1000 | Loss: 0.00000819
Iteration 183/1000 | Loss: 0.00000818
Iteration 184/1000 | Loss: 0.00000818
Iteration 185/1000 | Loss: 0.00000818
Iteration 186/1000 | Loss: 0.00000818
Iteration 187/1000 | Loss: 0.00000818
Iteration 188/1000 | Loss: 0.00000818
Iteration 189/1000 | Loss: 0.00000818
Iteration 190/1000 | Loss: 0.00000818
Iteration 191/1000 | Loss: 0.00000818
Iteration 192/1000 | Loss: 0.00000818
Iteration 193/1000 | Loss: 0.00000818
Iteration 194/1000 | Loss: 0.00000818
Iteration 195/1000 | Loss: 0.00000818
Iteration 196/1000 | Loss: 0.00000818
Iteration 197/1000 | Loss: 0.00000818
Iteration 198/1000 | Loss: 0.00000818
Iteration 199/1000 | Loss: 0.00000818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [8.17666750663193e-06, 8.17666750663193e-06, 8.17666750663193e-06, 8.17666750663193e-06, 8.17666750663193e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.17666750663193e-06

Optimization complete. Final v2v error: 2.477997064590454 mm

Highest mean error: 2.656947135925293 mm for frame 4

Lowest mean error: 2.3637278079986572 mm for frame 48

Saving results

Total time: 36.84731698036194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827029
Iteration 2/25 | Loss: 0.00126717
Iteration 3/25 | Loss: 0.00119427
Iteration 4/25 | Loss: 0.00118230
Iteration 5/25 | Loss: 0.00117812
Iteration 6/25 | Loss: 0.00117796
Iteration 7/25 | Loss: 0.00117796
Iteration 8/25 | Loss: 0.00117796
Iteration 9/25 | Loss: 0.00117796
Iteration 10/25 | Loss: 0.00117796
Iteration 11/25 | Loss: 0.00117796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001177959144115448, 0.001177959144115448, 0.001177959144115448, 0.001177959144115448, 0.001177959144115448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001177959144115448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.91504097
Iteration 2/25 | Loss: 0.00129538
Iteration 3/25 | Loss: 0.00129536
Iteration 4/25 | Loss: 0.00129536
Iteration 5/25 | Loss: 0.00129536
Iteration 6/25 | Loss: 0.00129536
Iteration 7/25 | Loss: 0.00129536
Iteration 8/25 | Loss: 0.00129536
Iteration 9/25 | Loss: 0.00129536
Iteration 10/25 | Loss: 0.00129536
Iteration 11/25 | Loss: 0.00129536
Iteration 12/25 | Loss: 0.00129536
Iteration 13/25 | Loss: 0.00129536
Iteration 14/25 | Loss: 0.00129536
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012953558471053839, 0.0012953558471053839, 0.0012953558471053839, 0.0012953558471053839, 0.0012953558471053839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012953558471053839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129536
Iteration 2/1000 | Loss: 0.00001885
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001303
Iteration 7/1000 | Loss: 0.00001269
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001210
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001115
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001114
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001108
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001102
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001097
Iteration 75/1000 | Loss: 0.00001097
Iteration 76/1000 | Loss: 0.00001097
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001096
Iteration 79/1000 | Loss: 0.00001096
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001096
Iteration 83/1000 | Loss: 0.00001096
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001093
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.0891092642850708e-05, 1.0891092642850708e-05, 1.0891092642850708e-05, 1.0891092642850708e-05, 1.0891092642850708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0891092642850708e-05

Optimization complete. Final v2v error: 2.8510279655456543 mm

Highest mean error: 3.2805075645446777 mm for frame 24

Lowest mean error: 2.658045768737793 mm for frame 119

Saving results

Total time: 38.83725309371948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018623
Iteration 2/25 | Loss: 0.00261084
Iteration 3/25 | Loss: 0.00194980
Iteration 4/25 | Loss: 0.00191448
Iteration 5/25 | Loss: 0.00171142
Iteration 6/25 | Loss: 0.00162955
Iteration 7/25 | Loss: 0.00161722
Iteration 8/25 | Loss: 0.00155073
Iteration 9/25 | Loss: 0.00152708
Iteration 10/25 | Loss: 0.00149416
Iteration 11/25 | Loss: 0.00150008
Iteration 12/25 | Loss: 0.00150995
Iteration 13/25 | Loss: 0.00149705
Iteration 14/25 | Loss: 0.00147942
Iteration 15/25 | Loss: 0.00148354
Iteration 16/25 | Loss: 0.00147443
Iteration 17/25 | Loss: 0.00147246
Iteration 18/25 | Loss: 0.00146343
Iteration 19/25 | Loss: 0.00146164
Iteration 20/25 | Loss: 0.00146128
Iteration 21/25 | Loss: 0.00146062
Iteration 22/25 | Loss: 0.00146017
Iteration 23/25 | Loss: 0.00145984
Iteration 24/25 | Loss: 0.00145968
Iteration 25/25 | Loss: 0.00145947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76749754
Iteration 2/25 | Loss: 0.00280012
Iteration 3/25 | Loss: 0.00280010
Iteration 4/25 | Loss: 0.00280010
Iteration 5/25 | Loss: 0.00280010
Iteration 6/25 | Loss: 0.00280010
Iteration 7/25 | Loss: 0.00280010
Iteration 8/25 | Loss: 0.00280010
Iteration 9/25 | Loss: 0.00280010
Iteration 10/25 | Loss: 0.00280010
Iteration 11/25 | Loss: 0.00280010
Iteration 12/25 | Loss: 0.00280010
Iteration 13/25 | Loss: 0.00280010
Iteration 14/25 | Loss: 0.00280010
Iteration 15/25 | Loss: 0.00280010
Iteration 16/25 | Loss: 0.00280010
Iteration 17/25 | Loss: 0.00280010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002800097456201911, 0.002800097456201911, 0.002800097456201911, 0.002800097456201911, 0.002800097456201911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002800097456201911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280010
Iteration 2/1000 | Loss: 0.00031268
Iteration 3/1000 | Loss: 0.00021862
Iteration 4/1000 | Loss: 0.00078556
Iteration 5/1000 | Loss: 0.00042385
Iteration 6/1000 | Loss: 0.00051875
Iteration 7/1000 | Loss: 0.00020488
Iteration 8/1000 | Loss: 0.00016701
Iteration 9/1000 | Loss: 0.00014846
Iteration 10/1000 | Loss: 0.00013606
Iteration 11/1000 | Loss: 0.00052547
Iteration 12/1000 | Loss: 0.00044918
Iteration 13/1000 | Loss: 0.00041761
Iteration 14/1000 | Loss: 0.00020470
Iteration 15/1000 | Loss: 0.00024467
Iteration 16/1000 | Loss: 0.00046489
Iteration 17/1000 | Loss: 0.00033162
Iteration 18/1000 | Loss: 0.00045804
Iteration 19/1000 | Loss: 0.00014704
Iteration 20/1000 | Loss: 0.00022287
Iteration 21/1000 | Loss: 0.00034956
Iteration 22/1000 | Loss: 0.00049137
Iteration 23/1000 | Loss: 0.00036185
Iteration 24/1000 | Loss: 0.00032245
Iteration 25/1000 | Loss: 0.00041928
Iteration 26/1000 | Loss: 0.00047789
Iteration 27/1000 | Loss: 0.00048084
Iteration 28/1000 | Loss: 0.00024203
Iteration 29/1000 | Loss: 0.00011671
Iteration 30/1000 | Loss: 0.00056539
Iteration 31/1000 | Loss: 0.00034408
Iteration 32/1000 | Loss: 0.00015949
Iteration 33/1000 | Loss: 0.00030269
Iteration 34/1000 | Loss: 0.00026717
Iteration 35/1000 | Loss: 0.00060669
Iteration 36/1000 | Loss: 0.00048578
Iteration 37/1000 | Loss: 0.00026367
Iteration 38/1000 | Loss: 0.00076409
Iteration 39/1000 | Loss: 0.00014392
Iteration 40/1000 | Loss: 0.00024331
Iteration 41/1000 | Loss: 0.00020110
Iteration 42/1000 | Loss: 0.00028655
Iteration 43/1000 | Loss: 0.00010977
Iteration 44/1000 | Loss: 0.00010141
Iteration 45/1000 | Loss: 0.00013619
Iteration 46/1000 | Loss: 0.00009694
Iteration 47/1000 | Loss: 0.00009566
Iteration 48/1000 | Loss: 0.00018512
Iteration 49/1000 | Loss: 0.00019829
Iteration 50/1000 | Loss: 0.00009608
Iteration 51/1000 | Loss: 0.00017357
Iteration 52/1000 | Loss: 0.00009512
Iteration 53/1000 | Loss: 0.00016803
Iteration 54/1000 | Loss: 0.00015139
Iteration 55/1000 | Loss: 0.00015770
Iteration 56/1000 | Loss: 0.00014527
Iteration 57/1000 | Loss: 0.00009197
Iteration 58/1000 | Loss: 0.00009070
Iteration 59/1000 | Loss: 0.00008976
Iteration 60/1000 | Loss: 0.00008919
Iteration 61/1000 | Loss: 0.00008868
Iteration 62/1000 | Loss: 0.00035431
Iteration 63/1000 | Loss: 0.00021360
Iteration 64/1000 | Loss: 0.00025370
Iteration 65/1000 | Loss: 0.00008999
Iteration 66/1000 | Loss: 0.00008848
Iteration 67/1000 | Loss: 0.00018272
Iteration 68/1000 | Loss: 0.00027969
Iteration 69/1000 | Loss: 0.00121558
Iteration 70/1000 | Loss: 0.00181618
Iteration 71/1000 | Loss: 0.00094094
Iteration 72/1000 | Loss: 0.00085133
Iteration 73/1000 | Loss: 0.00015736
Iteration 74/1000 | Loss: 0.00011949
Iteration 75/1000 | Loss: 0.00024771
Iteration 76/1000 | Loss: 0.00008762
Iteration 77/1000 | Loss: 0.00007933
Iteration 78/1000 | Loss: 0.00007400
Iteration 79/1000 | Loss: 0.00006995
Iteration 80/1000 | Loss: 0.00006704
Iteration 81/1000 | Loss: 0.00006536
Iteration 82/1000 | Loss: 0.00006400
Iteration 83/1000 | Loss: 0.00006303
Iteration 84/1000 | Loss: 0.00006219
Iteration 85/1000 | Loss: 0.00006162
Iteration 86/1000 | Loss: 0.00006119
Iteration 87/1000 | Loss: 0.00006093
Iteration 88/1000 | Loss: 0.00006070
Iteration 89/1000 | Loss: 0.00006064
Iteration 90/1000 | Loss: 0.00006047
Iteration 91/1000 | Loss: 0.00006043
Iteration 92/1000 | Loss: 0.00006030
Iteration 93/1000 | Loss: 0.00006028
Iteration 94/1000 | Loss: 0.00006020
Iteration 95/1000 | Loss: 0.00006020
Iteration 96/1000 | Loss: 0.00006019
Iteration 97/1000 | Loss: 0.00006019
Iteration 98/1000 | Loss: 0.00006018
Iteration 99/1000 | Loss: 0.00006018
Iteration 100/1000 | Loss: 0.00006018
Iteration 101/1000 | Loss: 0.00006017
Iteration 102/1000 | Loss: 0.00006017
Iteration 103/1000 | Loss: 0.00006017
Iteration 104/1000 | Loss: 0.00006017
Iteration 105/1000 | Loss: 0.00006016
Iteration 106/1000 | Loss: 0.00006016
Iteration 107/1000 | Loss: 0.00006016
Iteration 108/1000 | Loss: 0.00006016
Iteration 109/1000 | Loss: 0.00006015
Iteration 110/1000 | Loss: 0.00006015
Iteration 111/1000 | Loss: 0.00006015
Iteration 112/1000 | Loss: 0.00006015
Iteration 113/1000 | Loss: 0.00006014
Iteration 114/1000 | Loss: 0.00006014
Iteration 115/1000 | Loss: 0.00006014
Iteration 116/1000 | Loss: 0.00006014
Iteration 117/1000 | Loss: 0.00006014
Iteration 118/1000 | Loss: 0.00006013
Iteration 119/1000 | Loss: 0.00006013
Iteration 120/1000 | Loss: 0.00006013
Iteration 121/1000 | Loss: 0.00006012
Iteration 122/1000 | Loss: 0.00006012
Iteration 123/1000 | Loss: 0.00006012
Iteration 124/1000 | Loss: 0.00006011
Iteration 125/1000 | Loss: 0.00006011
Iteration 126/1000 | Loss: 0.00006011
Iteration 127/1000 | Loss: 0.00006010
Iteration 128/1000 | Loss: 0.00006010
Iteration 129/1000 | Loss: 0.00006010
Iteration 130/1000 | Loss: 0.00006009
Iteration 131/1000 | Loss: 0.00006009
Iteration 132/1000 | Loss: 0.00006009
Iteration 133/1000 | Loss: 0.00006009
Iteration 134/1000 | Loss: 0.00006009
Iteration 135/1000 | Loss: 0.00006008
Iteration 136/1000 | Loss: 0.00006008
Iteration 137/1000 | Loss: 0.00006008
Iteration 138/1000 | Loss: 0.00006008
Iteration 139/1000 | Loss: 0.00006008
Iteration 140/1000 | Loss: 0.00006008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [6.008472337271087e-05, 6.008472337271087e-05, 6.008472337271087e-05, 6.008472337271087e-05, 6.008472337271087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.008472337271087e-05

Optimization complete. Final v2v error: 4.430857181549072 mm

Highest mean error: 11.687149047851562 mm for frame 51

Lowest mean error: 2.9291718006134033 mm for frame 5

Saving results

Total time: 169.33437371253967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774284
Iteration 2/25 | Loss: 0.00146772
Iteration 3/25 | Loss: 0.00127979
Iteration 4/25 | Loss: 0.00126676
Iteration 5/25 | Loss: 0.00126426
Iteration 6/25 | Loss: 0.00126426
Iteration 7/25 | Loss: 0.00126426
Iteration 8/25 | Loss: 0.00126426
Iteration 9/25 | Loss: 0.00126426
Iteration 10/25 | Loss: 0.00126426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012642593355849385, 0.0012642593355849385, 0.0012642593355849385, 0.0012642593355849385, 0.0012642593355849385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012642593355849385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27160311
Iteration 2/25 | Loss: 0.00118617
Iteration 3/25 | Loss: 0.00118616
Iteration 4/25 | Loss: 0.00118616
Iteration 5/25 | Loss: 0.00118616
Iteration 6/25 | Loss: 0.00118616
Iteration 7/25 | Loss: 0.00118616
Iteration 8/25 | Loss: 0.00118616
Iteration 9/25 | Loss: 0.00118616
Iteration 10/25 | Loss: 0.00118616
Iteration 11/25 | Loss: 0.00118616
Iteration 12/25 | Loss: 0.00118616
Iteration 13/25 | Loss: 0.00118616
Iteration 14/25 | Loss: 0.00118616
Iteration 15/25 | Loss: 0.00118616
Iteration 16/25 | Loss: 0.00118616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011861597886309028, 0.0011861597886309028, 0.0011861597886309028, 0.0011861597886309028, 0.0011861597886309028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011861597886309028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118616
Iteration 2/1000 | Loss: 0.00003214
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002132
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001927
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00001812
Iteration 12/1000 | Loss: 0.00001811
Iteration 13/1000 | Loss: 0.00001793
Iteration 14/1000 | Loss: 0.00001793
Iteration 15/1000 | Loss: 0.00001788
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001767
Iteration 19/1000 | Loss: 0.00001761
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001760
Iteration 22/1000 | Loss: 0.00001755
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001753
Iteration 26/1000 | Loss: 0.00001753
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001752
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001749
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001749
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001748
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001747
Iteration 54/1000 | Loss: 0.00001747
Iteration 55/1000 | Loss: 0.00001747
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001746
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001746
Iteration 62/1000 | Loss: 0.00001746
Iteration 63/1000 | Loss: 0.00001746
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001745
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001743
Iteration 76/1000 | Loss: 0.00001743
Iteration 77/1000 | Loss: 0.00001743
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001739
Iteration 87/1000 | Loss: 0.00001739
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001738
Iteration 90/1000 | Loss: 0.00001738
Iteration 91/1000 | Loss: 0.00001738
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001737
Iteration 94/1000 | Loss: 0.00001737
Iteration 95/1000 | Loss: 0.00001737
Iteration 96/1000 | Loss: 0.00001737
Iteration 97/1000 | Loss: 0.00001737
Iteration 98/1000 | Loss: 0.00001737
Iteration 99/1000 | Loss: 0.00001737
Iteration 100/1000 | Loss: 0.00001737
Iteration 101/1000 | Loss: 0.00001737
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001737
Iteration 106/1000 | Loss: 0.00001737
Iteration 107/1000 | Loss: 0.00001737
Iteration 108/1000 | Loss: 0.00001737
Iteration 109/1000 | Loss: 0.00001737
Iteration 110/1000 | Loss: 0.00001737
Iteration 111/1000 | Loss: 0.00001737
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001734
Iteration 117/1000 | Loss: 0.00001734
Iteration 118/1000 | Loss: 0.00001734
Iteration 119/1000 | Loss: 0.00001733
Iteration 120/1000 | Loss: 0.00001733
Iteration 121/1000 | Loss: 0.00001733
Iteration 122/1000 | Loss: 0.00001733
Iteration 123/1000 | Loss: 0.00001733
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001732
Iteration 127/1000 | Loss: 0.00001732
Iteration 128/1000 | Loss: 0.00001732
Iteration 129/1000 | Loss: 0.00001731
Iteration 130/1000 | Loss: 0.00001731
Iteration 131/1000 | Loss: 0.00001731
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001731
Iteration 136/1000 | Loss: 0.00001731
Iteration 137/1000 | Loss: 0.00001730
Iteration 138/1000 | Loss: 0.00001730
Iteration 139/1000 | Loss: 0.00001730
Iteration 140/1000 | Loss: 0.00001729
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001728
Iteration 144/1000 | Loss: 0.00001728
Iteration 145/1000 | Loss: 0.00001727
Iteration 146/1000 | Loss: 0.00001727
Iteration 147/1000 | Loss: 0.00001727
Iteration 148/1000 | Loss: 0.00001727
Iteration 149/1000 | Loss: 0.00001727
Iteration 150/1000 | Loss: 0.00001727
Iteration 151/1000 | Loss: 0.00001727
Iteration 152/1000 | Loss: 0.00001727
Iteration 153/1000 | Loss: 0.00001727
Iteration 154/1000 | Loss: 0.00001727
Iteration 155/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7269963791477494e-05, 1.7269963791477494e-05, 1.7269963791477494e-05, 1.7269963791477494e-05, 1.7269963791477494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7269963791477494e-05

Optimization complete. Final v2v error: 3.488274335861206 mm

Highest mean error: 3.709679365158081 mm for frame 213

Lowest mean error: 3.1778061389923096 mm for frame 1

Saving results

Total time: 42.21796536445618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567474
Iteration 2/25 | Loss: 0.00155396
Iteration 3/25 | Loss: 0.00133852
Iteration 4/25 | Loss: 0.00131070
Iteration 5/25 | Loss: 0.00130401
Iteration 6/25 | Loss: 0.00130339
Iteration 7/25 | Loss: 0.00130339
Iteration 8/25 | Loss: 0.00130339
Iteration 9/25 | Loss: 0.00130339
Iteration 10/25 | Loss: 0.00130339
Iteration 11/25 | Loss: 0.00130339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013033892028033733, 0.0013033892028033733, 0.0013033892028033733, 0.0013033892028033733, 0.0013033892028033733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013033892028033733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82814956
Iteration 2/25 | Loss: 0.00110579
Iteration 3/25 | Loss: 0.00110575
Iteration 4/25 | Loss: 0.00110575
Iteration 5/25 | Loss: 0.00110575
Iteration 6/25 | Loss: 0.00110575
Iteration 7/25 | Loss: 0.00110575
Iteration 8/25 | Loss: 0.00110574
Iteration 9/25 | Loss: 0.00110574
Iteration 10/25 | Loss: 0.00110574
Iteration 11/25 | Loss: 0.00110574
Iteration 12/25 | Loss: 0.00110574
Iteration 13/25 | Loss: 0.00110574
Iteration 14/25 | Loss: 0.00110574
Iteration 15/25 | Loss: 0.00110574
Iteration 16/25 | Loss: 0.00110574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011057446245104074, 0.0011057446245104074, 0.0011057446245104074, 0.0011057446245104074, 0.0011057446245104074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011057446245104074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110574
Iteration 2/1000 | Loss: 0.00005032
Iteration 3/1000 | Loss: 0.00003151
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002583
Iteration 6/1000 | Loss: 0.00002471
Iteration 7/1000 | Loss: 0.00002410
Iteration 8/1000 | Loss: 0.00002368
Iteration 9/1000 | Loss: 0.00002336
Iteration 10/1000 | Loss: 0.00002307
Iteration 11/1000 | Loss: 0.00002285
Iteration 12/1000 | Loss: 0.00002283
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002266
Iteration 15/1000 | Loss: 0.00002264
Iteration 16/1000 | Loss: 0.00002263
Iteration 17/1000 | Loss: 0.00002258
Iteration 18/1000 | Loss: 0.00002258
Iteration 19/1000 | Loss: 0.00002257
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002256
Iteration 23/1000 | Loss: 0.00002256
Iteration 24/1000 | Loss: 0.00002256
Iteration 25/1000 | Loss: 0.00002256
Iteration 26/1000 | Loss: 0.00002256
Iteration 27/1000 | Loss: 0.00002256
Iteration 28/1000 | Loss: 0.00002256
Iteration 29/1000 | Loss: 0.00002256
Iteration 30/1000 | Loss: 0.00002256
Iteration 31/1000 | Loss: 0.00002256
Iteration 32/1000 | Loss: 0.00002255
Iteration 33/1000 | Loss: 0.00002251
Iteration 34/1000 | Loss: 0.00002251
Iteration 35/1000 | Loss: 0.00002251
Iteration 36/1000 | Loss: 0.00002251
Iteration 37/1000 | Loss: 0.00002251
Iteration 38/1000 | Loss: 0.00002249
Iteration 39/1000 | Loss: 0.00002248
Iteration 40/1000 | Loss: 0.00002248
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002247
Iteration 43/1000 | Loss: 0.00002247
Iteration 44/1000 | Loss: 0.00002246
Iteration 45/1000 | Loss: 0.00002246
Iteration 46/1000 | Loss: 0.00002246
Iteration 47/1000 | Loss: 0.00002246
Iteration 48/1000 | Loss: 0.00002246
Iteration 49/1000 | Loss: 0.00002246
Iteration 50/1000 | Loss: 0.00002246
Iteration 51/1000 | Loss: 0.00002245
Iteration 52/1000 | Loss: 0.00002245
Iteration 53/1000 | Loss: 0.00002245
Iteration 54/1000 | Loss: 0.00002245
Iteration 55/1000 | Loss: 0.00002245
Iteration 56/1000 | Loss: 0.00002244
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00002244
Iteration 59/1000 | Loss: 0.00002244
Iteration 60/1000 | Loss: 0.00002244
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002243
Iteration 63/1000 | Loss: 0.00002243
Iteration 64/1000 | Loss: 0.00002243
Iteration 65/1000 | Loss: 0.00002243
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002242
Iteration 70/1000 | Loss: 0.00002242
Iteration 71/1000 | Loss: 0.00002242
Iteration 72/1000 | Loss: 0.00002242
Iteration 73/1000 | Loss: 0.00002242
Iteration 74/1000 | Loss: 0.00002242
Iteration 75/1000 | Loss: 0.00002241
Iteration 76/1000 | Loss: 0.00002241
Iteration 77/1000 | Loss: 0.00002241
Iteration 78/1000 | Loss: 0.00002241
Iteration 79/1000 | Loss: 0.00002241
Iteration 80/1000 | Loss: 0.00002241
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00002241
Iteration 84/1000 | Loss: 0.00002241
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002241
Iteration 87/1000 | Loss: 0.00002241
Iteration 88/1000 | Loss: 0.00002241
Iteration 89/1000 | Loss: 0.00002241
Iteration 90/1000 | Loss: 0.00002241
Iteration 91/1000 | Loss: 0.00002241
Iteration 92/1000 | Loss: 0.00002241
Iteration 93/1000 | Loss: 0.00002241
Iteration 94/1000 | Loss: 0.00002241
Iteration 95/1000 | Loss: 0.00002241
Iteration 96/1000 | Loss: 0.00002241
Iteration 97/1000 | Loss: 0.00002241
Iteration 98/1000 | Loss: 0.00002241
Iteration 99/1000 | Loss: 0.00002241
Iteration 100/1000 | Loss: 0.00002241
Iteration 101/1000 | Loss: 0.00002241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.2409398297895677e-05, 2.2409398297895677e-05, 2.2409398297895677e-05, 2.2409398297895677e-05, 2.2409398297895677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2409398297895677e-05

Optimization complete. Final v2v error: 3.9896247386932373 mm

Highest mean error: 4.359123706817627 mm for frame 114

Lowest mean error: 3.742309331893921 mm for frame 7

Saving results

Total time: 31.863271713256836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061316
Iteration 2/25 | Loss: 0.00267860
Iteration 3/25 | Loss: 0.00225689
Iteration 4/25 | Loss: 0.00240531
Iteration 5/25 | Loss: 0.00164222
Iteration 6/25 | Loss: 0.00154601
Iteration 7/25 | Loss: 0.00153472
Iteration 8/25 | Loss: 0.00152483
Iteration 9/25 | Loss: 0.00151895
Iteration 10/25 | Loss: 0.00151160
Iteration 11/25 | Loss: 0.00150680
Iteration 12/25 | Loss: 0.00151056
Iteration 13/25 | Loss: 0.00150714
Iteration 14/25 | Loss: 0.00150987
Iteration 15/25 | Loss: 0.00150948
Iteration 16/25 | Loss: 0.00150044
Iteration 17/25 | Loss: 0.00150009
Iteration 18/25 | Loss: 0.00149718
Iteration 19/25 | Loss: 0.00149614
Iteration 20/25 | Loss: 0.00149598
Iteration 21/25 | Loss: 0.00149598
Iteration 22/25 | Loss: 0.00149598
Iteration 23/25 | Loss: 0.00149596
Iteration 24/25 | Loss: 0.00149596
Iteration 25/25 | Loss: 0.00149596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57235742
Iteration 2/25 | Loss: 0.00176842
Iteration 3/25 | Loss: 0.00174513
Iteration 4/25 | Loss: 0.00174513
Iteration 5/25 | Loss: 0.00174513
Iteration 6/25 | Loss: 0.00174513
Iteration 7/25 | Loss: 0.00174513
Iteration 8/25 | Loss: 0.00174513
Iteration 9/25 | Loss: 0.00174513
Iteration 10/25 | Loss: 0.00174513
Iteration 11/25 | Loss: 0.00174513
Iteration 12/25 | Loss: 0.00174513
Iteration 13/25 | Loss: 0.00174513
Iteration 14/25 | Loss: 0.00174513
Iteration 15/25 | Loss: 0.00174513
Iteration 16/25 | Loss: 0.00174513
Iteration 17/25 | Loss: 0.00174513
Iteration 18/25 | Loss: 0.00174513
Iteration 19/25 | Loss: 0.00174513
Iteration 20/25 | Loss: 0.00174513
Iteration 21/25 | Loss: 0.00174513
Iteration 22/25 | Loss: 0.00174513
Iteration 23/25 | Loss: 0.00174513
Iteration 24/25 | Loss: 0.00174513
Iteration 25/25 | Loss: 0.00174513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174513
Iteration 2/1000 | Loss: 0.00011409
Iteration 3/1000 | Loss: 0.00007090
Iteration 4/1000 | Loss: 0.00006515
Iteration 5/1000 | Loss: 0.00006778
Iteration 6/1000 | Loss: 0.00005986
Iteration 7/1000 | Loss: 0.00005401
Iteration 8/1000 | Loss: 0.00005312
Iteration 9/1000 | Loss: 0.00005214
Iteration 10/1000 | Loss: 0.00005147
Iteration 11/1000 | Loss: 0.00005397
Iteration 12/1000 | Loss: 0.00005302
Iteration 13/1000 | Loss: 0.00007715
Iteration 14/1000 | Loss: 0.00005938
Iteration 15/1000 | Loss: 0.00005124
Iteration 16/1000 | Loss: 0.00005845
Iteration 17/1000 | Loss: 0.00015534
Iteration 18/1000 | Loss: 0.00116525
Iteration 19/1000 | Loss: 0.00025257
Iteration 20/1000 | Loss: 0.00005103
Iteration 21/1000 | Loss: 0.00039041
Iteration 22/1000 | Loss: 0.00022542
Iteration 23/1000 | Loss: 0.00025907
Iteration 24/1000 | Loss: 0.00005815
Iteration 25/1000 | Loss: 0.00015955
Iteration 26/1000 | Loss: 0.00006344
Iteration 27/1000 | Loss: 0.00005321
Iteration 28/1000 | Loss: 0.00005122
Iteration 29/1000 | Loss: 0.00015139
Iteration 30/1000 | Loss: 0.00009386
Iteration 31/1000 | Loss: 0.00005856
Iteration 32/1000 | Loss: 0.00025063
Iteration 33/1000 | Loss: 0.00009349
Iteration 34/1000 | Loss: 0.00005986
Iteration 35/1000 | Loss: 0.00023091
Iteration 36/1000 | Loss: 0.00007997
Iteration 37/1000 | Loss: 0.00005656
Iteration 38/1000 | Loss: 0.00018187
Iteration 39/1000 | Loss: 0.00006758
Iteration 40/1000 | Loss: 0.00012330
Iteration 41/1000 | Loss: 0.00007529
Iteration 42/1000 | Loss: 0.00005860
Iteration 43/1000 | Loss: 0.00013732
Iteration 44/1000 | Loss: 0.00005959
Iteration 45/1000 | Loss: 0.00005115
Iteration 46/1000 | Loss: 0.00020170
Iteration 47/1000 | Loss: 0.00006111
Iteration 48/1000 | Loss: 0.00020758
Iteration 49/1000 | Loss: 0.00007311
Iteration 50/1000 | Loss: 0.00019159
Iteration 51/1000 | Loss: 0.00007261
Iteration 52/1000 | Loss: 0.00017250
Iteration 53/1000 | Loss: 0.00007214
Iteration 54/1000 | Loss: 0.00010628
Iteration 55/1000 | Loss: 0.00005395
Iteration 56/1000 | Loss: 0.00005795
Iteration 57/1000 | Loss: 0.00012093
Iteration 58/1000 | Loss: 0.00008536
Iteration 59/1000 | Loss: 0.00011272
Iteration 60/1000 | Loss: 0.00006211
Iteration 61/1000 | Loss: 0.00016968
Iteration 62/1000 | Loss: 0.00006857
Iteration 63/1000 | Loss: 0.00008226
Iteration 64/1000 | Loss: 0.00006064
Iteration 65/1000 | Loss: 0.00011633
Iteration 66/1000 | Loss: 0.00006460
Iteration 67/1000 | Loss: 0.00008099
Iteration 68/1000 | Loss: 0.00007238
Iteration 69/1000 | Loss: 0.00006263
Iteration 70/1000 | Loss: 0.00008028
Iteration 71/1000 | Loss: 0.00005975
Iteration 72/1000 | Loss: 0.00005040
Iteration 73/1000 | Loss: 0.00004962
Iteration 74/1000 | Loss: 0.00004934
Iteration 75/1000 | Loss: 0.00005956
Iteration 76/1000 | Loss: 0.00004921
Iteration 77/1000 | Loss: 0.00006095
Iteration 78/1000 | Loss: 0.00004902
Iteration 79/1000 | Loss: 0.00004901
Iteration 80/1000 | Loss: 0.00012678
Iteration 81/1000 | Loss: 0.00005416
Iteration 82/1000 | Loss: 0.00013571
Iteration 83/1000 | Loss: 0.00008086
Iteration 84/1000 | Loss: 0.00005374
Iteration 85/1000 | Loss: 0.00005190
Iteration 86/1000 | Loss: 0.00004910
Iteration 87/1000 | Loss: 0.00004878
Iteration 88/1000 | Loss: 0.00006496
Iteration 89/1000 | Loss: 0.00004874
Iteration 90/1000 | Loss: 0.00004865
Iteration 91/1000 | Loss: 0.00004861
Iteration 92/1000 | Loss: 0.00004861
Iteration 93/1000 | Loss: 0.00004861
Iteration 94/1000 | Loss: 0.00004860
Iteration 95/1000 | Loss: 0.00004860
Iteration 96/1000 | Loss: 0.00004860
Iteration 97/1000 | Loss: 0.00004860
Iteration 98/1000 | Loss: 0.00004860
Iteration 99/1000 | Loss: 0.00004860
Iteration 100/1000 | Loss: 0.00015984
Iteration 101/1000 | Loss: 0.00006262
Iteration 102/1000 | Loss: 0.00004921
Iteration 103/1000 | Loss: 0.00005893
Iteration 104/1000 | Loss: 0.00004902
Iteration 105/1000 | Loss: 0.00004862
Iteration 106/1000 | Loss: 0.00004862
Iteration 107/1000 | Loss: 0.00004861
Iteration 108/1000 | Loss: 0.00005050
Iteration 109/1000 | Loss: 0.00015879
Iteration 110/1000 | Loss: 0.00016329
Iteration 111/1000 | Loss: 0.00005573
Iteration 112/1000 | Loss: 0.00006977
Iteration 113/1000 | Loss: 0.00005498
Iteration 114/1000 | Loss: 0.00005192
Iteration 115/1000 | Loss: 0.00004939
Iteration 116/1000 | Loss: 0.00010446
Iteration 117/1000 | Loss: 0.00005463
Iteration 118/1000 | Loss: 0.00008914
Iteration 119/1000 | Loss: 0.00005379
Iteration 120/1000 | Loss: 0.00011001
Iteration 121/1000 | Loss: 0.00006900
Iteration 122/1000 | Loss: 0.00005246
Iteration 123/1000 | Loss: 0.00005269
Iteration 124/1000 | Loss: 0.00004891
Iteration 125/1000 | Loss: 0.00004887
Iteration 126/1000 | Loss: 0.00012833
Iteration 127/1000 | Loss: 0.00005923
Iteration 128/1000 | Loss: 0.00004978
Iteration 129/1000 | Loss: 0.00004910
Iteration 130/1000 | Loss: 0.00004880
Iteration 131/1000 | Loss: 0.00016872
Iteration 132/1000 | Loss: 0.00006716
Iteration 133/1000 | Loss: 0.00005481
Iteration 134/1000 | Loss: 0.00004862
Iteration 135/1000 | Loss: 0.00004858
Iteration 136/1000 | Loss: 0.00004858
Iteration 137/1000 | Loss: 0.00004858
Iteration 138/1000 | Loss: 0.00004858
Iteration 139/1000 | Loss: 0.00004858
Iteration 140/1000 | Loss: 0.00004858
Iteration 141/1000 | Loss: 0.00004858
Iteration 142/1000 | Loss: 0.00004858
Iteration 143/1000 | Loss: 0.00004858
Iteration 144/1000 | Loss: 0.00004857
Iteration 145/1000 | Loss: 0.00004857
Iteration 146/1000 | Loss: 0.00004857
Iteration 147/1000 | Loss: 0.00004856
Iteration 148/1000 | Loss: 0.00004856
Iteration 149/1000 | Loss: 0.00004856
Iteration 150/1000 | Loss: 0.00004856
Iteration 151/1000 | Loss: 0.00004856
Iteration 152/1000 | Loss: 0.00004856
Iteration 153/1000 | Loss: 0.00004855
Iteration 154/1000 | Loss: 0.00015566
Iteration 155/1000 | Loss: 0.00006014
Iteration 156/1000 | Loss: 0.00005585
Iteration 157/1000 | Loss: 0.00009151
Iteration 158/1000 | Loss: 0.00005006
Iteration 159/1000 | Loss: 0.00004885
Iteration 160/1000 | Loss: 0.00004874
Iteration 161/1000 | Loss: 0.00015720
Iteration 162/1000 | Loss: 0.00005136
Iteration 163/1000 | Loss: 0.00025646
Iteration 164/1000 | Loss: 0.00025027
Iteration 165/1000 | Loss: 0.00021650
Iteration 166/1000 | Loss: 0.00005007
Iteration 167/1000 | Loss: 0.00004890
Iteration 168/1000 | Loss: 0.00004866
Iteration 169/1000 | Loss: 0.00004866
Iteration 170/1000 | Loss: 0.00004866
Iteration 171/1000 | Loss: 0.00004856
Iteration 172/1000 | Loss: 0.00004852
Iteration 173/1000 | Loss: 0.00004852
Iteration 174/1000 | Loss: 0.00004851
Iteration 175/1000 | Loss: 0.00004850
Iteration 176/1000 | Loss: 0.00004862
Iteration 177/1000 | Loss: 0.00006445
Iteration 178/1000 | Loss: 0.00004869
Iteration 179/1000 | Loss: 0.00004852
Iteration 180/1000 | Loss: 0.00004858
Iteration 181/1000 | Loss: 0.00004854
Iteration 182/1000 | Loss: 0.00004853
Iteration 183/1000 | Loss: 0.00004853
Iteration 184/1000 | Loss: 0.00004853
Iteration 185/1000 | Loss: 0.00004853
Iteration 186/1000 | Loss: 0.00004853
Iteration 187/1000 | Loss: 0.00004853
Iteration 188/1000 | Loss: 0.00004853
Iteration 189/1000 | Loss: 0.00004853
Iteration 190/1000 | Loss: 0.00004853
Iteration 191/1000 | Loss: 0.00004852
Iteration 192/1000 | Loss: 0.00004852
Iteration 193/1000 | Loss: 0.00004852
Iteration 194/1000 | Loss: 0.00004852
Iteration 195/1000 | Loss: 0.00004852
Iteration 196/1000 | Loss: 0.00004852
Iteration 197/1000 | Loss: 0.00004852
Iteration 198/1000 | Loss: 0.00004852
Iteration 199/1000 | Loss: 0.00004851
Iteration 200/1000 | Loss: 0.00004851
Iteration 201/1000 | Loss: 0.00004851
Iteration 202/1000 | Loss: 0.00004850
Iteration 203/1000 | Loss: 0.00004850
Iteration 204/1000 | Loss: 0.00004850
Iteration 205/1000 | Loss: 0.00004850
Iteration 206/1000 | Loss: 0.00004850
Iteration 207/1000 | Loss: 0.00004857
Iteration 208/1000 | Loss: 0.00016264
Iteration 209/1000 | Loss: 0.00011907
Iteration 210/1000 | Loss: 0.00005112
Iteration 211/1000 | Loss: 0.00019446
Iteration 212/1000 | Loss: 0.00006717
Iteration 213/1000 | Loss: 0.00005291
Iteration 214/1000 | Loss: 0.00014539
Iteration 215/1000 | Loss: 0.00007773
Iteration 216/1000 | Loss: 0.00005379
Iteration 217/1000 | Loss: 0.00004993
Iteration 218/1000 | Loss: 0.00004924
Iteration 219/1000 | Loss: 0.00004879
Iteration 220/1000 | Loss: 0.00004867
Iteration 221/1000 | Loss: 0.00004864
Iteration 222/1000 | Loss: 0.00004864
Iteration 223/1000 | Loss: 0.00004864
Iteration 224/1000 | Loss: 0.00004864
Iteration 225/1000 | Loss: 0.00004863
Iteration 226/1000 | Loss: 0.00004863
Iteration 227/1000 | Loss: 0.00006938
Iteration 228/1000 | Loss: 0.00006938
Iteration 229/1000 | Loss: 0.00006937
Iteration 230/1000 | Loss: 0.00030468
Iteration 231/1000 | Loss: 0.00008597
Iteration 232/1000 | Loss: 0.00006264
Iteration 233/1000 | Loss: 0.00011511
Iteration 234/1000 | Loss: 0.00006692
Iteration 235/1000 | Loss: 0.00009756
Iteration 236/1000 | Loss: 0.00005100
Iteration 237/1000 | Loss: 0.00005030
Iteration 238/1000 | Loss: 0.00007645
Iteration 239/1000 | Loss: 0.00007375
Iteration 240/1000 | Loss: 0.00006104
Iteration 241/1000 | Loss: 0.00007692
Iteration 242/1000 | Loss: 0.00005814
Iteration 243/1000 | Loss: 0.00006974
Iteration 244/1000 | Loss: 0.00006812
Iteration 245/1000 | Loss: 0.00007209
Iteration 246/1000 | Loss: 0.00004927
Iteration 247/1000 | Loss: 0.00004896
Iteration 248/1000 | Loss: 0.00004871
Iteration 249/1000 | Loss: 0.00004870
Iteration 250/1000 | Loss: 0.00004869
Iteration 251/1000 | Loss: 0.00004869
Iteration 252/1000 | Loss: 0.00004867
Iteration 253/1000 | Loss: 0.00004865
Iteration 254/1000 | Loss: 0.00004863
Iteration 255/1000 | Loss: 0.00004862
Iteration 256/1000 | Loss: 0.00004862
Iteration 257/1000 | Loss: 0.00004862
Iteration 258/1000 | Loss: 0.00004881
Iteration 259/1000 | Loss: 0.00004879
Iteration 260/1000 | Loss: 0.00004857
Iteration 261/1000 | Loss: 0.00004854
Iteration 262/1000 | Loss: 0.00004854
Iteration 263/1000 | Loss: 0.00004853
Iteration 264/1000 | Loss: 0.00004853
Iteration 265/1000 | Loss: 0.00004853
Iteration 266/1000 | Loss: 0.00004853
Iteration 267/1000 | Loss: 0.00004852
Iteration 268/1000 | Loss: 0.00004852
Iteration 269/1000 | Loss: 0.00004852
Iteration 270/1000 | Loss: 0.00004852
Iteration 271/1000 | Loss: 0.00004852
Iteration 272/1000 | Loss: 0.00004867
Iteration 273/1000 | Loss: 0.00004855
Iteration 274/1000 | Loss: 0.00004862
Iteration 275/1000 | Loss: 0.00004862
Iteration 276/1000 | Loss: 0.00004855
Iteration 277/1000 | Loss: 0.00004859
Iteration 278/1000 | Loss: 0.00004852
Iteration 279/1000 | Loss: 0.00004861
Iteration 280/1000 | Loss: 0.00004854
Iteration 281/1000 | Loss: 0.00004856
Iteration 282/1000 | Loss: 0.00004856
Iteration 283/1000 | Loss: 0.00004858
Iteration 284/1000 | Loss: 0.00004858
Iteration 285/1000 | Loss: 0.00004857
Iteration 286/1000 | Loss: 0.00004853
Iteration 287/1000 | Loss: 0.00004853
Iteration 288/1000 | Loss: 0.00004853
Iteration 289/1000 | Loss: 0.00004852
Iteration 290/1000 | Loss: 0.00004852
Iteration 291/1000 | Loss: 0.00004852
Iteration 292/1000 | Loss: 0.00004852
Iteration 293/1000 | Loss: 0.00004852
Iteration 294/1000 | Loss: 0.00004852
Iteration 295/1000 | Loss: 0.00004852
Iteration 296/1000 | Loss: 0.00004852
Iteration 297/1000 | Loss: 0.00004851
Iteration 298/1000 | Loss: 0.00004851
Iteration 299/1000 | Loss: 0.00004851
Iteration 300/1000 | Loss: 0.00004851
Iteration 301/1000 | Loss: 0.00004851
Iteration 302/1000 | Loss: 0.00004851
Iteration 303/1000 | Loss: 0.00004851
Iteration 304/1000 | Loss: 0.00004851
Iteration 305/1000 | Loss: 0.00004851
Iteration 306/1000 | Loss: 0.00004851
Iteration 307/1000 | Loss: 0.00004851
Iteration 308/1000 | Loss: 0.00004851
Iteration 309/1000 | Loss: 0.00004851
Iteration 310/1000 | Loss: 0.00004851
Iteration 311/1000 | Loss: 0.00004851
Iteration 312/1000 | Loss: 0.00004851
Iteration 313/1000 | Loss: 0.00004851
Iteration 314/1000 | Loss: 0.00004851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [4.850915138376877e-05, 4.850915138376877e-05, 4.850915138376877e-05, 4.850915138376877e-05, 4.850915138376877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.850915138376877e-05

Optimization complete. Final v2v error: 4.314637184143066 mm

Highest mean error: 19.49118423461914 mm for frame 17

Lowest mean error: 3.1391332149505615 mm for frame 149

Saving results

Total time: 276.25226855278015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763035
Iteration 2/25 | Loss: 0.00153145
Iteration 3/25 | Loss: 0.00134871
Iteration 4/25 | Loss: 0.00131787
Iteration 5/25 | Loss: 0.00130436
Iteration 6/25 | Loss: 0.00130687
Iteration 7/25 | Loss: 0.00129895
Iteration 8/25 | Loss: 0.00129816
Iteration 9/25 | Loss: 0.00129799
Iteration 10/25 | Loss: 0.00129795
Iteration 11/25 | Loss: 0.00129795
Iteration 12/25 | Loss: 0.00129795
Iteration 13/25 | Loss: 0.00129795
Iteration 14/25 | Loss: 0.00129795
Iteration 15/25 | Loss: 0.00129795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012979480670765042, 0.0012979480670765042, 0.0012979480670765042, 0.0012979480670765042, 0.0012979480670765042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012979480670765042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.05696750
Iteration 2/25 | Loss: 0.00154977
Iteration 3/25 | Loss: 0.00154977
Iteration 4/25 | Loss: 0.00154977
Iteration 5/25 | Loss: 0.00154977
Iteration 6/25 | Loss: 0.00154977
Iteration 7/25 | Loss: 0.00154977
Iteration 8/25 | Loss: 0.00154977
Iteration 9/25 | Loss: 0.00154977
Iteration 10/25 | Loss: 0.00154977
Iteration 11/25 | Loss: 0.00154977
Iteration 12/25 | Loss: 0.00154977
Iteration 13/25 | Loss: 0.00154977
Iteration 14/25 | Loss: 0.00154977
Iteration 15/25 | Loss: 0.00154977
Iteration 16/25 | Loss: 0.00154977
Iteration 17/25 | Loss: 0.00154977
Iteration 18/25 | Loss: 0.00154977
Iteration 19/25 | Loss: 0.00154977
Iteration 20/25 | Loss: 0.00154977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001549766049720347, 0.001549766049720347, 0.001549766049720347, 0.001549766049720347, 0.001549766049720347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001549766049720347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154977
Iteration 2/1000 | Loss: 0.00003115
Iteration 3/1000 | Loss: 0.00002421
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002191
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002058
Iteration 9/1000 | Loss: 0.00002025
Iteration 10/1000 | Loss: 0.00002014
Iteration 11/1000 | Loss: 0.00002010
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00001998
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001985
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001968
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001960
Iteration 24/1000 | Loss: 0.00001960
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001957
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001957
Iteration 33/1000 | Loss: 0.00001957
Iteration 34/1000 | Loss: 0.00001957
Iteration 35/1000 | Loss: 0.00001956
Iteration 36/1000 | Loss: 0.00001956
Iteration 37/1000 | Loss: 0.00001955
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001953
Iteration 42/1000 | Loss: 0.00001953
Iteration 43/1000 | Loss: 0.00001953
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001952
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001951
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001951
Iteration 55/1000 | Loss: 0.00001951
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001951
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001949
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001948
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001948
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001947
Iteration 99/1000 | Loss: 0.00001947
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001946
Iteration 103/1000 | Loss: 0.00001946
Iteration 104/1000 | Loss: 0.00001946
Iteration 105/1000 | Loss: 0.00001946
Iteration 106/1000 | Loss: 0.00001946
Iteration 107/1000 | Loss: 0.00001946
Iteration 108/1000 | Loss: 0.00001946
Iteration 109/1000 | Loss: 0.00001946
Iteration 110/1000 | Loss: 0.00001946
Iteration 111/1000 | Loss: 0.00001946
Iteration 112/1000 | Loss: 0.00001946
Iteration 113/1000 | Loss: 0.00001946
Iteration 114/1000 | Loss: 0.00001945
Iteration 115/1000 | Loss: 0.00001945
Iteration 116/1000 | Loss: 0.00001945
Iteration 117/1000 | Loss: 0.00001945
Iteration 118/1000 | Loss: 0.00001945
Iteration 119/1000 | Loss: 0.00001945
Iteration 120/1000 | Loss: 0.00001944
Iteration 121/1000 | Loss: 0.00001944
Iteration 122/1000 | Loss: 0.00001944
Iteration 123/1000 | Loss: 0.00001944
Iteration 124/1000 | Loss: 0.00001943
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001943
Iteration 127/1000 | Loss: 0.00001943
Iteration 128/1000 | Loss: 0.00001943
Iteration 129/1000 | Loss: 0.00001943
Iteration 130/1000 | Loss: 0.00001943
Iteration 131/1000 | Loss: 0.00001943
Iteration 132/1000 | Loss: 0.00001943
Iteration 133/1000 | Loss: 0.00001943
Iteration 134/1000 | Loss: 0.00001942
Iteration 135/1000 | Loss: 0.00001942
Iteration 136/1000 | Loss: 0.00001942
Iteration 137/1000 | Loss: 0.00001942
Iteration 138/1000 | Loss: 0.00001942
Iteration 139/1000 | Loss: 0.00001942
Iteration 140/1000 | Loss: 0.00001942
Iteration 141/1000 | Loss: 0.00001942
Iteration 142/1000 | Loss: 0.00001942
Iteration 143/1000 | Loss: 0.00001942
Iteration 144/1000 | Loss: 0.00001942
Iteration 145/1000 | Loss: 0.00001942
Iteration 146/1000 | Loss: 0.00001942
Iteration 147/1000 | Loss: 0.00001942
Iteration 148/1000 | Loss: 0.00001942
Iteration 149/1000 | Loss: 0.00001942
Iteration 150/1000 | Loss: 0.00001941
Iteration 151/1000 | Loss: 0.00001941
Iteration 152/1000 | Loss: 0.00001941
Iteration 153/1000 | Loss: 0.00001941
Iteration 154/1000 | Loss: 0.00001941
Iteration 155/1000 | Loss: 0.00001941
Iteration 156/1000 | Loss: 0.00001941
Iteration 157/1000 | Loss: 0.00001941
Iteration 158/1000 | Loss: 0.00001941
Iteration 159/1000 | Loss: 0.00001941
Iteration 160/1000 | Loss: 0.00001941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.940942274814006e-05, 1.940942274814006e-05, 1.940942274814006e-05, 1.940942274814006e-05, 1.940942274814006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.940942274814006e-05

Optimization complete. Final v2v error: 3.727983236312866 mm

Highest mean error: 4.109450340270996 mm for frame 186

Lowest mean error: 3.5427467823028564 mm for frame 12

Saving results

Total time: 44.39449214935303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976112
Iteration 2/25 | Loss: 0.00187895
Iteration 3/25 | Loss: 0.00142388
Iteration 4/25 | Loss: 0.00137185
Iteration 5/25 | Loss: 0.00130538
Iteration 6/25 | Loss: 0.00129525
Iteration 7/25 | Loss: 0.00125269
Iteration 8/25 | Loss: 0.00124053
Iteration 9/25 | Loss: 0.00123811
Iteration 10/25 | Loss: 0.00124081
Iteration 11/25 | Loss: 0.00124086
Iteration 12/25 | Loss: 0.00123959
Iteration 13/25 | Loss: 0.00124284
Iteration 14/25 | Loss: 0.00123756
Iteration 15/25 | Loss: 0.00123328
Iteration 16/25 | Loss: 0.00123230
Iteration 17/25 | Loss: 0.00123569
Iteration 18/25 | Loss: 0.00123480
Iteration 19/25 | Loss: 0.00123183
Iteration 20/25 | Loss: 0.00123265
Iteration 21/25 | Loss: 0.00123180
Iteration 22/25 | Loss: 0.00123102
Iteration 23/25 | Loss: 0.00123145
Iteration 24/25 | Loss: 0.00123549
Iteration 25/25 | Loss: 0.00123114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29653275
Iteration 2/25 | Loss: 0.00168225
Iteration 3/25 | Loss: 0.00168225
Iteration 4/25 | Loss: 0.00168225
Iteration 5/25 | Loss: 0.00168225
Iteration 6/25 | Loss: 0.00168225
Iteration 7/25 | Loss: 0.00168225
Iteration 8/25 | Loss: 0.00168225
Iteration 9/25 | Loss: 0.00168225
Iteration 10/25 | Loss: 0.00168225
Iteration 11/25 | Loss: 0.00168225
Iteration 12/25 | Loss: 0.00168225
Iteration 13/25 | Loss: 0.00168225
Iteration 14/25 | Loss: 0.00168225
Iteration 15/25 | Loss: 0.00168225
Iteration 16/25 | Loss: 0.00168225
Iteration 17/25 | Loss: 0.00168225
Iteration 18/25 | Loss: 0.00168225
Iteration 19/25 | Loss: 0.00168225
Iteration 20/25 | Loss: 0.00168225
Iteration 21/25 | Loss: 0.00168225
Iteration 22/25 | Loss: 0.00168225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00168224994558841, 0.00168224994558841, 0.00168224994558841, 0.00168224994558841, 0.00168224994558841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00168224994558841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168225
Iteration 2/1000 | Loss: 0.00004612
Iteration 3/1000 | Loss: 0.00004374
Iteration 4/1000 | Loss: 0.00002741
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00003144
Iteration 7/1000 | Loss: 0.00003130
Iteration 8/1000 | Loss: 0.00003096
Iteration 9/1000 | Loss: 0.00002956
Iteration 10/1000 | Loss: 0.00002928
Iteration 11/1000 | Loss: 0.00002696
Iteration 12/1000 | Loss: 0.00003224
Iteration 13/1000 | Loss: 0.00002813
Iteration 14/1000 | Loss: 0.00002780
Iteration 15/1000 | Loss: 0.00002815
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001468
Iteration 20/1000 | Loss: 0.00002170
Iteration 21/1000 | Loss: 0.00002080
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001309
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001305
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001296
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001295
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001294
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00002533
Iteration 80/1000 | Loss: 0.00002533
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001285
Iteration 103/1000 | Loss: 0.00001285
Iteration 104/1000 | Loss: 0.00001285
Iteration 105/1000 | Loss: 0.00001285
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001284
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001281
Iteration 136/1000 | Loss: 0.00001281
Iteration 137/1000 | Loss: 0.00001280
Iteration 138/1000 | Loss: 0.00001280
Iteration 139/1000 | Loss: 0.00001280
Iteration 140/1000 | Loss: 0.00001280
Iteration 141/1000 | Loss: 0.00001280
Iteration 142/1000 | Loss: 0.00001280
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001280
Iteration 149/1000 | Loss: 0.00001280
Iteration 150/1000 | Loss: 0.00001280
Iteration 151/1000 | Loss: 0.00001280
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001279
Iteration 154/1000 | Loss: 0.00001279
Iteration 155/1000 | Loss: 0.00001279
Iteration 156/1000 | Loss: 0.00001279
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.2794634130841587e-05, 1.2794634130841587e-05, 1.2794634130841587e-05, 1.2794634130841587e-05, 1.2794634130841587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2794634130841587e-05

Optimization complete. Final v2v error: 2.978382110595703 mm

Highest mean error: 5.929844856262207 mm for frame 0

Lowest mean error: 2.4488906860351562 mm for frame 236

Saving results

Total time: 103.8407666683197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814213
Iteration 2/25 | Loss: 0.00139372
Iteration 3/25 | Loss: 0.00122515
Iteration 4/25 | Loss: 0.00120643
Iteration 5/25 | Loss: 0.00120396
Iteration 6/25 | Loss: 0.00120396
Iteration 7/25 | Loss: 0.00120396
Iteration 8/25 | Loss: 0.00120396
Iteration 9/25 | Loss: 0.00120396
Iteration 10/25 | Loss: 0.00120396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001203957013785839, 0.001203957013785839, 0.001203957013785839, 0.001203957013785839, 0.001203957013785839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203957013785839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27267730
Iteration 2/25 | Loss: 0.00101550
Iteration 3/25 | Loss: 0.00101547
Iteration 4/25 | Loss: 0.00101547
Iteration 5/25 | Loss: 0.00101547
Iteration 6/25 | Loss: 0.00101547
Iteration 7/25 | Loss: 0.00101547
Iteration 8/25 | Loss: 0.00101547
Iteration 9/25 | Loss: 0.00101547
Iteration 10/25 | Loss: 0.00101547
Iteration 11/25 | Loss: 0.00101547
Iteration 12/25 | Loss: 0.00101547
Iteration 13/25 | Loss: 0.00101547
Iteration 14/25 | Loss: 0.00101546
Iteration 15/25 | Loss: 0.00101546
Iteration 16/25 | Loss: 0.00101546
Iteration 17/25 | Loss: 0.00101546
Iteration 18/25 | Loss: 0.00101546
Iteration 19/25 | Loss: 0.00101546
Iteration 20/25 | Loss: 0.00101546
Iteration 21/25 | Loss: 0.00101546
Iteration 22/25 | Loss: 0.00101546
Iteration 23/25 | Loss: 0.00101546
Iteration 24/25 | Loss: 0.00101546
Iteration 25/25 | Loss: 0.00101546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101546
Iteration 2/1000 | Loss: 0.00002576
Iteration 3/1000 | Loss: 0.00001915
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001634
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001419
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001299
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001260
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001248
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001228
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001218
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001217
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001213
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001209
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001208
Iteration 120/1000 | Loss: 0.00001208
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.207613695441978e-05, 1.207613695441978e-05, 1.207613695441978e-05, 1.207613695441978e-05, 1.207613695441978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.207613695441978e-05

Optimization complete. Final v2v error: 2.9683053493499756 mm

Highest mean error: 3.8164103031158447 mm for frame 36

Lowest mean error: 2.698728561401367 mm for frame 8

Saving results

Total time: 41.00937557220459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961938
Iteration 2/25 | Loss: 0.00300383
Iteration 3/25 | Loss: 0.00250780
Iteration 4/25 | Loss: 0.00217438
Iteration 5/25 | Loss: 0.00214078
Iteration 6/25 | Loss: 0.00204425
Iteration 7/25 | Loss: 0.00203453
Iteration 8/25 | Loss: 0.00199388
Iteration 9/25 | Loss: 0.00196020
Iteration 10/25 | Loss: 0.00194938
Iteration 11/25 | Loss: 0.00194000
Iteration 12/25 | Loss: 0.00193770
Iteration 13/25 | Loss: 0.00193726
Iteration 14/25 | Loss: 0.00194189
Iteration 15/25 | Loss: 0.00193484
Iteration 16/25 | Loss: 0.00193427
Iteration 17/25 | Loss: 0.00193411
Iteration 18/25 | Loss: 0.00193406
Iteration 19/25 | Loss: 0.00193406
Iteration 20/25 | Loss: 0.00193406
Iteration 21/25 | Loss: 0.00193406
Iteration 22/25 | Loss: 0.00193406
Iteration 23/25 | Loss: 0.00193406
Iteration 24/25 | Loss: 0.00193405
Iteration 25/25 | Loss: 0.00193405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23225915
Iteration 2/25 | Loss: 0.00801257
Iteration 3/25 | Loss: 0.00501793
Iteration 4/25 | Loss: 0.00491376
Iteration 5/25 | Loss: 0.00491376
Iteration 6/25 | Loss: 0.00491376
Iteration 7/25 | Loss: 0.00491376
Iteration 8/25 | Loss: 0.00491376
Iteration 9/25 | Loss: 0.00491376
Iteration 10/25 | Loss: 0.00491376
Iteration 11/25 | Loss: 0.00491376
Iteration 12/25 | Loss: 0.00491376
Iteration 13/25 | Loss: 0.00491376
Iteration 14/25 | Loss: 0.00491376
Iteration 15/25 | Loss: 0.00491376
Iteration 16/25 | Loss: 0.00491376
Iteration 17/25 | Loss: 0.00491376
Iteration 18/25 | Loss: 0.00491376
Iteration 19/25 | Loss: 0.00491376
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004913761746138334, 0.004913761746138334, 0.004913761746138334, 0.004913761746138334, 0.004913761746138334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004913761746138334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00491376
Iteration 2/1000 | Loss: 0.00408790
Iteration 3/1000 | Loss: 0.00072245
Iteration 4/1000 | Loss: 0.00131983
Iteration 5/1000 | Loss: 0.00645682
Iteration 6/1000 | Loss: 0.00064857
Iteration 7/1000 | Loss: 0.00119838
Iteration 8/1000 | Loss: 0.00038384
Iteration 9/1000 | Loss: 0.00035712
Iteration 10/1000 | Loss: 0.00033756
Iteration 11/1000 | Loss: 0.00044638
Iteration 12/1000 | Loss: 0.00171338
Iteration 13/1000 | Loss: 0.01294018
Iteration 14/1000 | Loss: 0.01949593
Iteration 15/1000 | Loss: 0.00305249
Iteration 16/1000 | Loss: 0.00227375
Iteration 17/1000 | Loss: 0.00064853
Iteration 18/1000 | Loss: 0.00122725
Iteration 19/1000 | Loss: 0.00095987
Iteration 20/1000 | Loss: 0.00027609
Iteration 21/1000 | Loss: 0.00040484
Iteration 22/1000 | Loss: 0.00071299
Iteration 23/1000 | Loss: 0.00046676
Iteration 24/1000 | Loss: 0.00117854
Iteration 25/1000 | Loss: 0.00033559
Iteration 26/1000 | Loss: 0.00035391
Iteration 27/1000 | Loss: 0.00021527
Iteration 28/1000 | Loss: 0.00081298
Iteration 29/1000 | Loss: 0.00036895
Iteration 30/1000 | Loss: 0.00081679
Iteration 31/1000 | Loss: 0.00005527
Iteration 32/1000 | Loss: 0.00011774
Iteration 33/1000 | Loss: 0.00018468
Iteration 34/1000 | Loss: 0.00026253
Iteration 35/1000 | Loss: 0.00026894
Iteration 36/1000 | Loss: 0.00041876
Iteration 37/1000 | Loss: 0.00056098
Iteration 38/1000 | Loss: 0.00002315
Iteration 39/1000 | Loss: 0.00062940
Iteration 40/1000 | Loss: 0.00034289
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00077104
Iteration 43/1000 | Loss: 0.00030586
Iteration 44/1000 | Loss: 0.00034116
Iteration 45/1000 | Loss: 0.00009490
Iteration 46/1000 | Loss: 0.00027964
Iteration 47/1000 | Loss: 0.00054442
Iteration 48/1000 | Loss: 0.00007168
Iteration 49/1000 | Loss: 0.00003318
Iteration 50/1000 | Loss: 0.00003359
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00003871
Iteration 53/1000 | Loss: 0.00006968
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00063867
Iteration 56/1000 | Loss: 0.00009488
Iteration 57/1000 | Loss: 0.00002065
Iteration 58/1000 | Loss: 0.00031039
Iteration 59/1000 | Loss: 0.00004941
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00019235
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00009196
Iteration 64/1000 | Loss: 0.00004962
Iteration 65/1000 | Loss: 0.00002550
Iteration 66/1000 | Loss: 0.00001391
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001345
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001343
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00011528
Iteration 82/1000 | Loss: 0.00030517
Iteration 83/1000 | Loss: 0.00002409
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00010311
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Iteration 127/1000 | Loss: 0.00001325
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001323
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001323
Iteration 142/1000 | Loss: 0.00001323
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001322
Iteration 146/1000 | Loss: 0.00001322
Iteration 147/1000 | Loss: 0.00001322
Iteration 148/1000 | Loss: 0.00001322
Iteration 149/1000 | Loss: 0.00001322
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001321
Iteration 154/1000 | Loss: 0.00001321
Iteration 155/1000 | Loss: 0.00001321
Iteration 156/1000 | Loss: 0.00001321
Iteration 157/1000 | Loss: 0.00001321
Iteration 158/1000 | Loss: 0.00001320
Iteration 159/1000 | Loss: 0.00001320
Iteration 160/1000 | Loss: 0.00001320
Iteration 161/1000 | Loss: 0.00001320
Iteration 162/1000 | Loss: 0.00001320
Iteration 163/1000 | Loss: 0.00001320
Iteration 164/1000 | Loss: 0.00001320
Iteration 165/1000 | Loss: 0.00001320
Iteration 166/1000 | Loss: 0.00001320
Iteration 167/1000 | Loss: 0.00001319
Iteration 168/1000 | Loss: 0.00001319
Iteration 169/1000 | Loss: 0.00001319
Iteration 170/1000 | Loss: 0.00001319
Iteration 171/1000 | Loss: 0.00001319
Iteration 172/1000 | Loss: 0.00001318
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001318
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001318
Iteration 177/1000 | Loss: 0.00001318
Iteration 178/1000 | Loss: 0.00001318
Iteration 179/1000 | Loss: 0.00001318
Iteration 180/1000 | Loss: 0.00001318
Iteration 181/1000 | Loss: 0.00001318
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.317165242653573e-05, 1.317165242653573e-05, 1.317165242653573e-05, 1.317165242653573e-05, 1.317165242653573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.317165242653573e-05

Optimization complete. Final v2v error: 3.1445178985595703 mm

Highest mean error: 3.3600363731384277 mm for frame 2

Lowest mean error: 2.953803539276123 mm for frame 6

Saving results

Total time: 142.45799684524536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521420
Iteration 2/25 | Loss: 0.00145558
Iteration 3/25 | Loss: 0.00126227
Iteration 4/25 | Loss: 0.00124320
Iteration 5/25 | Loss: 0.00123962
Iteration 6/25 | Loss: 0.00123886
Iteration 7/25 | Loss: 0.00123886
Iteration 8/25 | Loss: 0.00123886
Iteration 9/25 | Loss: 0.00123886
Iteration 10/25 | Loss: 0.00123886
Iteration 11/25 | Loss: 0.00123886
Iteration 12/25 | Loss: 0.00123886
Iteration 13/25 | Loss: 0.00123886
Iteration 14/25 | Loss: 0.00123886
Iteration 15/25 | Loss: 0.00123886
Iteration 16/25 | Loss: 0.00123886
Iteration 17/25 | Loss: 0.00123886
Iteration 18/25 | Loss: 0.00123886
Iteration 19/25 | Loss: 0.00123886
Iteration 20/25 | Loss: 0.00123886
Iteration 21/25 | Loss: 0.00123886
Iteration 22/25 | Loss: 0.00123886
Iteration 23/25 | Loss: 0.00123886
Iteration 24/25 | Loss: 0.00123886
Iteration 25/25 | Loss: 0.00123886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28532326
Iteration 2/25 | Loss: 0.00108132
Iteration 3/25 | Loss: 0.00108130
Iteration 4/25 | Loss: 0.00108130
Iteration 5/25 | Loss: 0.00108130
Iteration 6/25 | Loss: 0.00108130
Iteration 7/25 | Loss: 0.00108130
Iteration 8/25 | Loss: 0.00108130
Iteration 9/25 | Loss: 0.00108130
Iteration 10/25 | Loss: 0.00108130
Iteration 11/25 | Loss: 0.00108130
Iteration 12/25 | Loss: 0.00108130
Iteration 13/25 | Loss: 0.00108130
Iteration 14/25 | Loss: 0.00108130
Iteration 15/25 | Loss: 0.00108130
Iteration 16/25 | Loss: 0.00108130
Iteration 17/25 | Loss: 0.00108130
Iteration 18/25 | Loss: 0.00108130
Iteration 19/25 | Loss: 0.00108130
Iteration 20/25 | Loss: 0.00108130
Iteration 21/25 | Loss: 0.00108130
Iteration 22/25 | Loss: 0.00108130
Iteration 23/25 | Loss: 0.00108130
Iteration 24/25 | Loss: 0.00108130
Iteration 25/25 | Loss: 0.00108130

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108130
Iteration 2/1000 | Loss: 0.00002846
Iteration 3/1000 | Loss: 0.00002034
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001724
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001589
Iteration 9/1000 | Loss: 0.00001587
Iteration 10/1000 | Loss: 0.00001561
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001523
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001505
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001485
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001480
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001473
Iteration 32/1000 | Loss: 0.00001473
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001455
Iteration 63/1000 | Loss: 0.00001455
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001453
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001453
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001452
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001448
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001447
Iteration 103/1000 | Loss: 0.00001447
Iteration 104/1000 | Loss: 0.00001447
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001445
Iteration 111/1000 | Loss: 0.00001445
Iteration 112/1000 | Loss: 0.00001445
Iteration 113/1000 | Loss: 0.00001445
Iteration 114/1000 | Loss: 0.00001445
Iteration 115/1000 | Loss: 0.00001445
Iteration 116/1000 | Loss: 0.00001445
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001442
Iteration 152/1000 | Loss: 0.00001442
Iteration 153/1000 | Loss: 0.00001442
Iteration 154/1000 | Loss: 0.00001442
Iteration 155/1000 | Loss: 0.00001442
Iteration 156/1000 | Loss: 0.00001442
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001441
Iteration 160/1000 | Loss: 0.00001441
Iteration 161/1000 | Loss: 0.00001441
Iteration 162/1000 | Loss: 0.00001441
Iteration 163/1000 | Loss: 0.00001441
Iteration 164/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4414929864869919e-05, 1.4414929864869919e-05, 1.4414929864869919e-05, 1.4414929864869919e-05, 1.4414929864869919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4414929864869919e-05

Optimization complete. Final v2v error: 3.1770219802856445 mm

Highest mean error: 3.615910053253174 mm for frame 53

Lowest mean error: 2.761806011199951 mm for frame 107

Saving results

Total time: 38.824520111083984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014283
Iteration 2/25 | Loss: 0.00328146
Iteration 3/25 | Loss: 0.00212875
Iteration 4/25 | Loss: 0.00164879
Iteration 5/25 | Loss: 0.00151436
Iteration 6/25 | Loss: 0.00150425
Iteration 7/25 | Loss: 0.00151904
Iteration 8/25 | Loss: 0.00136005
Iteration 9/25 | Loss: 0.00133616
Iteration 10/25 | Loss: 0.00127192
Iteration 11/25 | Loss: 0.00125675
Iteration 12/25 | Loss: 0.00124823
Iteration 13/25 | Loss: 0.00124594
Iteration 14/25 | Loss: 0.00124701
Iteration 15/25 | Loss: 0.00124587
Iteration 16/25 | Loss: 0.00124372
Iteration 17/25 | Loss: 0.00124327
Iteration 18/25 | Loss: 0.00124303
Iteration 19/25 | Loss: 0.00124268
Iteration 20/25 | Loss: 0.00124112
Iteration 21/25 | Loss: 0.00124043
Iteration 22/25 | Loss: 0.00124148
Iteration 23/25 | Loss: 0.00124075
Iteration 24/25 | Loss: 0.00123935
Iteration 25/25 | Loss: 0.00123888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92121947
Iteration 2/25 | Loss: 0.00165867
Iteration 3/25 | Loss: 0.00165867
Iteration 4/25 | Loss: 0.00165867
Iteration 5/25 | Loss: 0.00165867
Iteration 6/25 | Loss: 0.00165867
Iteration 7/25 | Loss: 0.00165867
Iteration 8/25 | Loss: 0.00165867
Iteration 9/25 | Loss: 0.00165867
Iteration 10/25 | Loss: 0.00165867
Iteration 11/25 | Loss: 0.00165867
Iteration 12/25 | Loss: 0.00165867
Iteration 13/25 | Loss: 0.00165867
Iteration 14/25 | Loss: 0.00165867
Iteration 15/25 | Loss: 0.00165867
Iteration 16/25 | Loss: 0.00165867
Iteration 17/25 | Loss: 0.00165867
Iteration 18/25 | Loss: 0.00165867
Iteration 19/25 | Loss: 0.00165867
Iteration 20/25 | Loss: 0.00165867
Iteration 21/25 | Loss: 0.00165867
Iteration 22/25 | Loss: 0.00165867
Iteration 23/25 | Loss: 0.00165867
Iteration 24/25 | Loss: 0.00165867
Iteration 25/25 | Loss: 0.00165867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165867
Iteration 2/1000 | Loss: 0.00005294
Iteration 3/1000 | Loss: 0.00003778
Iteration 4/1000 | Loss: 0.00003097
Iteration 5/1000 | Loss: 0.00002909
Iteration 6/1000 | Loss: 0.00024473
Iteration 7/1000 | Loss: 0.00017946
Iteration 8/1000 | Loss: 0.00013184
Iteration 9/1000 | Loss: 0.00002680
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002475
Iteration 12/1000 | Loss: 0.00002396
Iteration 13/1000 | Loss: 0.00002341
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00072582
Iteration 16/1000 | Loss: 0.00014411
Iteration 17/1000 | Loss: 0.00003109
Iteration 18/1000 | Loss: 0.00002521
Iteration 19/1000 | Loss: 0.00002260
Iteration 20/1000 | Loss: 0.00002013
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001622
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001512
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001508
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001506
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001503
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001502
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001500
Iteration 100/1000 | Loss: 0.00001499
Iteration 101/1000 | Loss: 0.00001499
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001499
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001498
Iteration 108/1000 | Loss: 0.00001498
Iteration 109/1000 | Loss: 0.00001498
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Iteration 131/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.496568711445434e-05, 1.496568711445434e-05, 1.496568711445434e-05, 1.496568711445434e-05, 1.496568711445434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.496568711445434e-05

Optimization complete. Final v2v error: 3.3207223415374756 mm

Highest mean error: 4.041721343994141 mm for frame 177

Lowest mean error: 2.9881744384765625 mm for frame 131

Saving results

Total time: 108.82976412773132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724217
Iteration 2/25 | Loss: 0.00151985
Iteration 3/25 | Loss: 0.00142348
Iteration 4/25 | Loss: 0.00122740
Iteration 5/25 | Loss: 0.00122295
Iteration 6/25 | Loss: 0.00121866
Iteration 7/25 | Loss: 0.00121440
Iteration 8/25 | Loss: 0.00121339
Iteration 9/25 | Loss: 0.00121240
Iteration 10/25 | Loss: 0.00121072
Iteration 11/25 | Loss: 0.00120966
Iteration 12/25 | Loss: 0.00120936
Iteration 13/25 | Loss: 0.00120928
Iteration 14/25 | Loss: 0.00120928
Iteration 15/25 | Loss: 0.00120927
Iteration 16/25 | Loss: 0.00120927
Iteration 17/25 | Loss: 0.00120927
Iteration 18/25 | Loss: 0.00120927
Iteration 19/25 | Loss: 0.00120926
Iteration 20/25 | Loss: 0.00120926
Iteration 21/25 | Loss: 0.00120926
Iteration 22/25 | Loss: 0.00120926
Iteration 23/25 | Loss: 0.00120926
Iteration 24/25 | Loss: 0.00120926
Iteration 25/25 | Loss: 0.00120926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77080274
Iteration 2/25 | Loss: 0.00138216
Iteration 3/25 | Loss: 0.00138215
Iteration 4/25 | Loss: 0.00138215
Iteration 5/25 | Loss: 0.00138215
Iteration 6/25 | Loss: 0.00138215
Iteration 7/25 | Loss: 0.00138215
Iteration 8/25 | Loss: 0.00138215
Iteration 9/25 | Loss: 0.00138215
Iteration 10/25 | Loss: 0.00138215
Iteration 11/25 | Loss: 0.00138215
Iteration 12/25 | Loss: 0.00138215
Iteration 13/25 | Loss: 0.00138215
Iteration 14/25 | Loss: 0.00138215
Iteration 15/25 | Loss: 0.00138215
Iteration 16/25 | Loss: 0.00138215
Iteration 17/25 | Loss: 0.00138215
Iteration 18/25 | Loss: 0.00138215
Iteration 19/25 | Loss: 0.00138215
Iteration 20/25 | Loss: 0.00138215
Iteration 21/25 | Loss: 0.00138215
Iteration 22/25 | Loss: 0.00138215
Iteration 23/25 | Loss: 0.00138215
Iteration 24/25 | Loss: 0.00138215
Iteration 25/25 | Loss: 0.00138215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138215
Iteration 2/1000 | Loss: 0.00002780
Iteration 3/1000 | Loss: 0.00016589
Iteration 4/1000 | Loss: 0.00002823
Iteration 5/1000 | Loss: 0.00002176
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00019953
Iteration 9/1000 | Loss: 0.00001871
Iteration 10/1000 | Loss: 0.00001809
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001745
Iteration 13/1000 | Loss: 0.00001722
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001661
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001657
Iteration 26/1000 | Loss: 0.00001654
Iteration 27/1000 | Loss: 0.00014185
Iteration 28/1000 | Loss: 0.00019823
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00012035
Iteration 31/1000 | Loss: 0.00031636
Iteration 32/1000 | Loss: 0.00027956
Iteration 33/1000 | Loss: 0.00001913
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001632
Iteration 44/1000 | Loss: 0.00001632
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001631
Iteration 47/1000 | Loss: 0.00001631
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001628
Iteration 67/1000 | Loss: 0.00001628
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00001627
Iteration 74/1000 | Loss: 0.00001627
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001627
Iteration 78/1000 | Loss: 0.00001627
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001626
Iteration 82/1000 | Loss: 0.00001626
Iteration 83/1000 | Loss: 0.00001626
Iteration 84/1000 | Loss: 0.00001626
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001625
Iteration 89/1000 | Loss: 0.00001625
Iteration 90/1000 | Loss: 0.00001625
Iteration 91/1000 | Loss: 0.00001625
Iteration 92/1000 | Loss: 0.00001625
Iteration 93/1000 | Loss: 0.00001625
Iteration 94/1000 | Loss: 0.00001625
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00001625
Iteration 98/1000 | Loss: 0.00001625
Iteration 99/1000 | Loss: 0.00001625
Iteration 100/1000 | Loss: 0.00001625
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00001625
Iteration 103/1000 | Loss: 0.00001625
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001625
Iteration 106/1000 | Loss: 0.00001625
Iteration 107/1000 | Loss: 0.00001625
Iteration 108/1000 | Loss: 0.00001625
Iteration 109/1000 | Loss: 0.00001625
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001625
Iteration 112/1000 | Loss: 0.00001625
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.625305594643578e-05, 1.625305594643578e-05, 1.625305594643578e-05, 1.625305594643578e-05, 1.625305594643578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.625305594643578e-05

Optimization complete. Final v2v error: 3.450833559036255 mm

Highest mean error: 3.9819400310516357 mm for frame 106

Lowest mean error: 3.141852378845215 mm for frame 128

Saving results

Total time: 59.656005859375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904374
Iteration 2/25 | Loss: 0.00135238
Iteration 3/25 | Loss: 0.00125436
Iteration 4/25 | Loss: 0.00123516
Iteration 5/25 | Loss: 0.00122882
Iteration 6/25 | Loss: 0.00122728
Iteration 7/25 | Loss: 0.00122728
Iteration 8/25 | Loss: 0.00122728
Iteration 9/25 | Loss: 0.00122728
Iteration 10/25 | Loss: 0.00122728
Iteration 11/25 | Loss: 0.00122728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012272819876670837, 0.0012272819876670837, 0.0012272819876670837, 0.0012272819876670837, 0.0012272819876670837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012272819876670837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29866540
Iteration 2/25 | Loss: 0.00138484
Iteration 3/25 | Loss: 0.00138482
Iteration 4/25 | Loss: 0.00138482
Iteration 5/25 | Loss: 0.00138482
Iteration 6/25 | Loss: 0.00138482
Iteration 7/25 | Loss: 0.00138482
Iteration 8/25 | Loss: 0.00138482
Iteration 9/25 | Loss: 0.00138482
Iteration 10/25 | Loss: 0.00138482
Iteration 11/25 | Loss: 0.00138482
Iteration 12/25 | Loss: 0.00138482
Iteration 13/25 | Loss: 0.00138482
Iteration 14/25 | Loss: 0.00138482
Iteration 15/25 | Loss: 0.00138482
Iteration 16/25 | Loss: 0.00138482
Iteration 17/25 | Loss: 0.00138482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013848177623003721, 0.0013848177623003721, 0.0013848177623003721, 0.0013848177623003721, 0.0013848177623003721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013848177623003721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138482
Iteration 2/1000 | Loss: 0.00004562
Iteration 3/1000 | Loss: 0.00003158
Iteration 4/1000 | Loss: 0.00002554
Iteration 5/1000 | Loss: 0.00002363
Iteration 6/1000 | Loss: 0.00002244
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002113
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002033
Iteration 11/1000 | Loss: 0.00002008
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001975
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001967
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001960
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001945
Iteration 24/1000 | Loss: 0.00001944
Iteration 25/1000 | Loss: 0.00001944
Iteration 26/1000 | Loss: 0.00001942
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001926
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001917
Iteration 66/1000 | Loss: 0.00001917
Iteration 67/1000 | Loss: 0.00001917
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001916
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001915
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001910
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001907
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001906
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001905
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001905
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001903
Iteration 131/1000 | Loss: 0.00001903
Iteration 132/1000 | Loss: 0.00001903
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001902
Iteration 138/1000 | Loss: 0.00001902
Iteration 139/1000 | Loss: 0.00001902
Iteration 140/1000 | Loss: 0.00001902
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001902
Iteration 143/1000 | Loss: 0.00001901
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001901
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Iteration 153/1000 | Loss: 0.00001900
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001899
Iteration 158/1000 | Loss: 0.00001899
Iteration 159/1000 | Loss: 0.00001899
Iteration 160/1000 | Loss: 0.00001899
Iteration 161/1000 | Loss: 0.00001898
Iteration 162/1000 | Loss: 0.00001898
Iteration 163/1000 | Loss: 0.00001898
Iteration 164/1000 | Loss: 0.00001898
Iteration 165/1000 | Loss: 0.00001897
Iteration 166/1000 | Loss: 0.00001897
Iteration 167/1000 | Loss: 0.00001897
Iteration 168/1000 | Loss: 0.00001897
Iteration 169/1000 | Loss: 0.00001897
Iteration 170/1000 | Loss: 0.00001897
Iteration 171/1000 | Loss: 0.00001897
Iteration 172/1000 | Loss: 0.00001897
Iteration 173/1000 | Loss: 0.00001897
Iteration 174/1000 | Loss: 0.00001897
Iteration 175/1000 | Loss: 0.00001897
Iteration 176/1000 | Loss: 0.00001896
Iteration 177/1000 | Loss: 0.00001896
Iteration 178/1000 | Loss: 0.00001896
Iteration 179/1000 | Loss: 0.00001896
Iteration 180/1000 | Loss: 0.00001896
Iteration 181/1000 | Loss: 0.00001896
Iteration 182/1000 | Loss: 0.00001896
Iteration 183/1000 | Loss: 0.00001895
Iteration 184/1000 | Loss: 0.00001895
Iteration 185/1000 | Loss: 0.00001895
Iteration 186/1000 | Loss: 0.00001895
Iteration 187/1000 | Loss: 0.00001895
Iteration 188/1000 | Loss: 0.00001895
Iteration 189/1000 | Loss: 0.00001895
Iteration 190/1000 | Loss: 0.00001894
Iteration 191/1000 | Loss: 0.00001894
Iteration 192/1000 | Loss: 0.00001894
Iteration 193/1000 | Loss: 0.00001894
Iteration 194/1000 | Loss: 0.00001894
Iteration 195/1000 | Loss: 0.00001894
Iteration 196/1000 | Loss: 0.00001894
Iteration 197/1000 | Loss: 0.00001893
Iteration 198/1000 | Loss: 0.00001893
Iteration 199/1000 | Loss: 0.00001893
Iteration 200/1000 | Loss: 0.00001893
Iteration 201/1000 | Loss: 0.00001893
Iteration 202/1000 | Loss: 0.00001893
Iteration 203/1000 | Loss: 0.00001893
Iteration 204/1000 | Loss: 0.00001893
Iteration 205/1000 | Loss: 0.00001893
Iteration 206/1000 | Loss: 0.00001893
Iteration 207/1000 | Loss: 0.00001893
Iteration 208/1000 | Loss: 0.00001893
Iteration 209/1000 | Loss: 0.00001893
Iteration 210/1000 | Loss: 0.00001893
Iteration 211/1000 | Loss: 0.00001893
Iteration 212/1000 | Loss: 0.00001893
Iteration 213/1000 | Loss: 0.00001893
Iteration 214/1000 | Loss: 0.00001893
Iteration 215/1000 | Loss: 0.00001893
Iteration 216/1000 | Loss: 0.00001893
Iteration 217/1000 | Loss: 0.00001893
Iteration 218/1000 | Loss: 0.00001893
Iteration 219/1000 | Loss: 0.00001893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.8928729332401417e-05, 1.8928729332401417e-05, 1.8928729332401417e-05, 1.8928729332401417e-05, 1.8928729332401417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8928729332401417e-05

Optimization complete. Final v2v error: 3.629135847091675 mm

Highest mean error: 5.702735424041748 mm for frame 70

Lowest mean error: 3.15614652633667 mm for frame 3

Saving results

Total time: 43.63181757926941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811746
Iteration 2/25 | Loss: 0.00126987
Iteration 3/25 | Loss: 0.00117968
Iteration 4/25 | Loss: 0.00117030
Iteration 5/25 | Loss: 0.00116850
Iteration 6/25 | Loss: 0.00116837
Iteration 7/25 | Loss: 0.00116837
Iteration 8/25 | Loss: 0.00116837
Iteration 9/25 | Loss: 0.00116837
Iteration 10/25 | Loss: 0.00116837
Iteration 11/25 | Loss: 0.00116837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011683667544275522, 0.0011683667544275522, 0.0011683667544275522, 0.0011683667544275522, 0.0011683667544275522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011683667544275522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29153371
Iteration 2/25 | Loss: 0.00119895
Iteration 3/25 | Loss: 0.00119893
Iteration 4/25 | Loss: 0.00119893
Iteration 5/25 | Loss: 0.00119893
Iteration 6/25 | Loss: 0.00119893
Iteration 7/25 | Loss: 0.00119893
Iteration 8/25 | Loss: 0.00119893
Iteration 9/25 | Loss: 0.00119893
Iteration 10/25 | Loss: 0.00119893
Iteration 11/25 | Loss: 0.00119893
Iteration 12/25 | Loss: 0.00119893
Iteration 13/25 | Loss: 0.00119893
Iteration 14/25 | Loss: 0.00119893
Iteration 15/25 | Loss: 0.00119893
Iteration 16/25 | Loss: 0.00119893
Iteration 17/25 | Loss: 0.00119893
Iteration 18/25 | Loss: 0.00119893
Iteration 19/25 | Loss: 0.00119893
Iteration 20/25 | Loss: 0.00119893
Iteration 21/25 | Loss: 0.00119893
Iteration 22/25 | Loss: 0.00119893
Iteration 23/25 | Loss: 0.00119893
Iteration 24/25 | Loss: 0.00119893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011989254271611571, 0.0011989254271611571, 0.0011989254271611571, 0.0011989254271611571, 0.0011989254271611571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011989254271611571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119893
Iteration 2/1000 | Loss: 0.00001849
Iteration 3/1000 | Loss: 0.00001261
Iteration 4/1000 | Loss: 0.00001113
Iteration 5/1000 | Loss: 0.00001033
Iteration 6/1000 | Loss: 0.00000977
Iteration 7/1000 | Loss: 0.00000931
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000886
Iteration 10/1000 | Loss: 0.00000869
Iteration 11/1000 | Loss: 0.00000857
Iteration 12/1000 | Loss: 0.00000854
Iteration 13/1000 | Loss: 0.00000852
Iteration 14/1000 | Loss: 0.00000850
Iteration 15/1000 | Loss: 0.00000849
Iteration 16/1000 | Loss: 0.00000849
Iteration 17/1000 | Loss: 0.00000848
Iteration 18/1000 | Loss: 0.00000847
Iteration 19/1000 | Loss: 0.00000847
Iteration 20/1000 | Loss: 0.00000846
Iteration 21/1000 | Loss: 0.00000842
Iteration 22/1000 | Loss: 0.00000841
Iteration 23/1000 | Loss: 0.00000840
Iteration 24/1000 | Loss: 0.00000838
Iteration 25/1000 | Loss: 0.00000835
Iteration 26/1000 | Loss: 0.00000835
Iteration 27/1000 | Loss: 0.00000834
Iteration 28/1000 | Loss: 0.00000833
Iteration 29/1000 | Loss: 0.00000832
Iteration 30/1000 | Loss: 0.00000828
Iteration 31/1000 | Loss: 0.00000828
Iteration 32/1000 | Loss: 0.00000827
Iteration 33/1000 | Loss: 0.00000827
Iteration 34/1000 | Loss: 0.00000826
Iteration 35/1000 | Loss: 0.00000825
Iteration 36/1000 | Loss: 0.00000824
Iteration 37/1000 | Loss: 0.00000824
Iteration 38/1000 | Loss: 0.00000824
Iteration 39/1000 | Loss: 0.00000824
Iteration 40/1000 | Loss: 0.00000824
Iteration 41/1000 | Loss: 0.00000823
Iteration 42/1000 | Loss: 0.00000823
Iteration 43/1000 | Loss: 0.00000823
Iteration 44/1000 | Loss: 0.00000823
Iteration 45/1000 | Loss: 0.00000822
Iteration 46/1000 | Loss: 0.00000822
Iteration 47/1000 | Loss: 0.00000821
Iteration 48/1000 | Loss: 0.00000821
Iteration 49/1000 | Loss: 0.00000821
Iteration 50/1000 | Loss: 0.00000821
Iteration 51/1000 | Loss: 0.00000820
Iteration 52/1000 | Loss: 0.00000820
Iteration 53/1000 | Loss: 0.00000820
Iteration 54/1000 | Loss: 0.00000819
Iteration 55/1000 | Loss: 0.00000819
Iteration 56/1000 | Loss: 0.00000818
Iteration 57/1000 | Loss: 0.00000818
Iteration 58/1000 | Loss: 0.00000817
Iteration 59/1000 | Loss: 0.00000817
Iteration 60/1000 | Loss: 0.00000816
Iteration 61/1000 | Loss: 0.00000815
Iteration 62/1000 | Loss: 0.00000815
Iteration 63/1000 | Loss: 0.00000814
Iteration 64/1000 | Loss: 0.00000814
Iteration 65/1000 | Loss: 0.00000813
Iteration 66/1000 | Loss: 0.00000813
Iteration 67/1000 | Loss: 0.00000813
Iteration 68/1000 | Loss: 0.00000813
Iteration 69/1000 | Loss: 0.00000813
Iteration 70/1000 | Loss: 0.00000813
Iteration 71/1000 | Loss: 0.00000813
Iteration 72/1000 | Loss: 0.00000812
Iteration 73/1000 | Loss: 0.00000812
Iteration 74/1000 | Loss: 0.00000812
Iteration 75/1000 | Loss: 0.00000812
Iteration 76/1000 | Loss: 0.00000812
Iteration 77/1000 | Loss: 0.00000811
Iteration 78/1000 | Loss: 0.00000811
Iteration 79/1000 | Loss: 0.00000811
Iteration 80/1000 | Loss: 0.00000810
Iteration 81/1000 | Loss: 0.00000810
Iteration 82/1000 | Loss: 0.00000810
Iteration 83/1000 | Loss: 0.00000810
Iteration 84/1000 | Loss: 0.00000810
Iteration 85/1000 | Loss: 0.00000809
Iteration 86/1000 | Loss: 0.00000809
Iteration 87/1000 | Loss: 0.00000809
Iteration 88/1000 | Loss: 0.00000809
Iteration 89/1000 | Loss: 0.00000808
Iteration 90/1000 | Loss: 0.00000808
Iteration 91/1000 | Loss: 0.00000807
Iteration 92/1000 | Loss: 0.00000806
Iteration 93/1000 | Loss: 0.00000806
Iteration 94/1000 | Loss: 0.00000806
Iteration 95/1000 | Loss: 0.00000806
Iteration 96/1000 | Loss: 0.00000805
Iteration 97/1000 | Loss: 0.00000805
Iteration 98/1000 | Loss: 0.00000805
Iteration 99/1000 | Loss: 0.00000804
Iteration 100/1000 | Loss: 0.00000804
Iteration 101/1000 | Loss: 0.00000804
Iteration 102/1000 | Loss: 0.00000804
Iteration 103/1000 | Loss: 0.00000803
Iteration 104/1000 | Loss: 0.00000803
Iteration 105/1000 | Loss: 0.00000802
Iteration 106/1000 | Loss: 0.00000802
Iteration 107/1000 | Loss: 0.00000801
Iteration 108/1000 | Loss: 0.00000801
Iteration 109/1000 | Loss: 0.00000801
Iteration 110/1000 | Loss: 0.00000800
Iteration 111/1000 | Loss: 0.00000800
Iteration 112/1000 | Loss: 0.00000800
Iteration 113/1000 | Loss: 0.00000799
Iteration 114/1000 | Loss: 0.00000799
Iteration 115/1000 | Loss: 0.00000799
Iteration 116/1000 | Loss: 0.00000799
Iteration 117/1000 | Loss: 0.00000799
Iteration 118/1000 | Loss: 0.00000798
Iteration 119/1000 | Loss: 0.00000798
Iteration 120/1000 | Loss: 0.00000798
Iteration 121/1000 | Loss: 0.00000798
Iteration 122/1000 | Loss: 0.00000798
Iteration 123/1000 | Loss: 0.00000798
Iteration 124/1000 | Loss: 0.00000798
Iteration 125/1000 | Loss: 0.00000798
Iteration 126/1000 | Loss: 0.00000798
Iteration 127/1000 | Loss: 0.00000798
Iteration 128/1000 | Loss: 0.00000798
Iteration 129/1000 | Loss: 0.00000798
Iteration 130/1000 | Loss: 0.00000798
Iteration 131/1000 | Loss: 0.00000798
Iteration 132/1000 | Loss: 0.00000798
Iteration 133/1000 | Loss: 0.00000797
Iteration 134/1000 | Loss: 0.00000797
Iteration 135/1000 | Loss: 0.00000797
Iteration 136/1000 | Loss: 0.00000797
Iteration 137/1000 | Loss: 0.00000797
Iteration 138/1000 | Loss: 0.00000797
Iteration 139/1000 | Loss: 0.00000797
Iteration 140/1000 | Loss: 0.00000797
Iteration 141/1000 | Loss: 0.00000797
Iteration 142/1000 | Loss: 0.00000797
Iteration 143/1000 | Loss: 0.00000797
Iteration 144/1000 | Loss: 0.00000797
Iteration 145/1000 | Loss: 0.00000797
Iteration 146/1000 | Loss: 0.00000797
Iteration 147/1000 | Loss: 0.00000796
Iteration 148/1000 | Loss: 0.00000796
Iteration 149/1000 | Loss: 0.00000796
Iteration 150/1000 | Loss: 0.00000796
Iteration 151/1000 | Loss: 0.00000796
Iteration 152/1000 | Loss: 0.00000796
Iteration 153/1000 | Loss: 0.00000796
Iteration 154/1000 | Loss: 0.00000796
Iteration 155/1000 | Loss: 0.00000795
Iteration 156/1000 | Loss: 0.00000795
Iteration 157/1000 | Loss: 0.00000795
Iteration 158/1000 | Loss: 0.00000795
Iteration 159/1000 | Loss: 0.00000795
Iteration 160/1000 | Loss: 0.00000795
Iteration 161/1000 | Loss: 0.00000795
Iteration 162/1000 | Loss: 0.00000794
Iteration 163/1000 | Loss: 0.00000794
Iteration 164/1000 | Loss: 0.00000794
Iteration 165/1000 | Loss: 0.00000794
Iteration 166/1000 | Loss: 0.00000794
Iteration 167/1000 | Loss: 0.00000794
Iteration 168/1000 | Loss: 0.00000794
Iteration 169/1000 | Loss: 0.00000794
Iteration 170/1000 | Loss: 0.00000794
Iteration 171/1000 | Loss: 0.00000794
Iteration 172/1000 | Loss: 0.00000794
Iteration 173/1000 | Loss: 0.00000794
Iteration 174/1000 | Loss: 0.00000794
Iteration 175/1000 | Loss: 0.00000794
Iteration 176/1000 | Loss: 0.00000793
Iteration 177/1000 | Loss: 0.00000793
Iteration 178/1000 | Loss: 0.00000793
Iteration 179/1000 | Loss: 0.00000793
Iteration 180/1000 | Loss: 0.00000793
Iteration 181/1000 | Loss: 0.00000793
Iteration 182/1000 | Loss: 0.00000793
Iteration 183/1000 | Loss: 0.00000793
Iteration 184/1000 | Loss: 0.00000793
Iteration 185/1000 | Loss: 0.00000793
Iteration 186/1000 | Loss: 0.00000793
Iteration 187/1000 | Loss: 0.00000793
Iteration 188/1000 | Loss: 0.00000793
Iteration 189/1000 | Loss: 0.00000793
Iteration 190/1000 | Loss: 0.00000792
Iteration 191/1000 | Loss: 0.00000792
Iteration 192/1000 | Loss: 0.00000792
Iteration 193/1000 | Loss: 0.00000792
Iteration 194/1000 | Loss: 0.00000792
Iteration 195/1000 | Loss: 0.00000792
Iteration 196/1000 | Loss: 0.00000792
Iteration 197/1000 | Loss: 0.00000792
Iteration 198/1000 | Loss: 0.00000792
Iteration 199/1000 | Loss: 0.00000792
Iteration 200/1000 | Loss: 0.00000792
Iteration 201/1000 | Loss: 0.00000792
Iteration 202/1000 | Loss: 0.00000792
Iteration 203/1000 | Loss: 0.00000792
Iteration 204/1000 | Loss: 0.00000792
Iteration 205/1000 | Loss: 0.00000792
Iteration 206/1000 | Loss: 0.00000792
Iteration 207/1000 | Loss: 0.00000792
Iteration 208/1000 | Loss: 0.00000792
Iteration 209/1000 | Loss: 0.00000792
Iteration 210/1000 | Loss: 0.00000792
Iteration 211/1000 | Loss: 0.00000792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [7.921133146737702e-06, 7.921133146737702e-06, 7.921133146737702e-06, 7.921133146737702e-06, 7.921133146737702e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.921133146737702e-06

Optimization complete. Final v2v error: 2.44512939453125 mm

Highest mean error: 2.5878751277923584 mm for frame 26

Lowest mean error: 2.3449034690856934 mm for frame 48

Saving results

Total time: 38.668556928634644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995375
Iteration 2/25 | Loss: 0.00190953
Iteration 3/25 | Loss: 0.00183479
Iteration 4/25 | Loss: 0.00145368
Iteration 5/25 | Loss: 0.00141048
Iteration 6/25 | Loss: 0.00135726
Iteration 7/25 | Loss: 0.00136141
Iteration 8/25 | Loss: 0.00135026
Iteration 9/25 | Loss: 0.00132102
Iteration 10/25 | Loss: 0.00131541
Iteration 11/25 | Loss: 0.00134296
Iteration 12/25 | Loss: 0.00127936
Iteration 13/25 | Loss: 0.00127240
Iteration 14/25 | Loss: 0.00126890
Iteration 15/25 | Loss: 0.00126878
Iteration 16/25 | Loss: 0.00126897
Iteration 17/25 | Loss: 0.00126838
Iteration 18/25 | Loss: 0.00126807
Iteration 19/25 | Loss: 0.00126807
Iteration 20/25 | Loss: 0.00126447
Iteration 21/25 | Loss: 0.00126442
Iteration 22/25 | Loss: 0.00126566
Iteration 23/25 | Loss: 0.00126449
Iteration 24/25 | Loss: 0.00126565
Iteration 25/25 | Loss: 0.00126445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31119049
Iteration 2/25 | Loss: 0.00214146
Iteration 3/25 | Loss: 0.00175854
Iteration 4/25 | Loss: 0.00175854
Iteration 5/25 | Loss: 0.00175854
Iteration 6/25 | Loss: 0.00175854
Iteration 7/25 | Loss: 0.00175854
Iteration 8/25 | Loss: 0.00175854
Iteration 9/25 | Loss: 0.00175854
Iteration 10/25 | Loss: 0.00175854
Iteration 11/25 | Loss: 0.00175854
Iteration 12/25 | Loss: 0.00175854
Iteration 13/25 | Loss: 0.00175854
Iteration 14/25 | Loss: 0.00175854
Iteration 15/25 | Loss: 0.00175854
Iteration 16/25 | Loss: 0.00175854
Iteration 17/25 | Loss: 0.00175854
Iteration 18/25 | Loss: 0.00175854
Iteration 19/25 | Loss: 0.00175853
Iteration 20/25 | Loss: 0.00175853
Iteration 21/25 | Loss: 0.00175853
Iteration 22/25 | Loss: 0.00175853
Iteration 23/25 | Loss: 0.00175853
Iteration 24/25 | Loss: 0.00175853
Iteration 25/25 | Loss: 0.00175853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175853
Iteration 2/1000 | Loss: 0.00427470
Iteration 3/1000 | Loss: 0.00079099
Iteration 4/1000 | Loss: 0.00186595
Iteration 5/1000 | Loss: 0.00092593
Iteration 6/1000 | Loss: 0.00171925
Iteration 7/1000 | Loss: 0.00069963
Iteration 8/1000 | Loss: 0.00011974
Iteration 9/1000 | Loss: 0.00004499
Iteration 10/1000 | Loss: 0.00018182
Iteration 11/1000 | Loss: 0.00011700
Iteration 12/1000 | Loss: 0.00011280
Iteration 13/1000 | Loss: 0.00014371
Iteration 14/1000 | Loss: 0.00016200
Iteration 15/1000 | Loss: 0.00014786
Iteration 16/1000 | Loss: 0.00005670
Iteration 17/1000 | Loss: 0.00008001
Iteration 18/1000 | Loss: 0.00006940
Iteration 19/1000 | Loss: 0.00014654
Iteration 20/1000 | Loss: 0.00010992
Iteration 21/1000 | Loss: 0.00041150
Iteration 22/1000 | Loss: 0.00013871
Iteration 23/1000 | Loss: 0.00013972
Iteration 24/1000 | Loss: 0.00012062
Iteration 25/1000 | Loss: 0.00028678
Iteration 26/1000 | Loss: 0.00012848
Iteration 27/1000 | Loss: 0.00018946
Iteration 28/1000 | Loss: 0.00012700
Iteration 29/1000 | Loss: 0.00014795
Iteration 30/1000 | Loss: 0.00020646
Iteration 31/1000 | Loss: 0.00018720
Iteration 32/1000 | Loss: 0.00020660
Iteration 33/1000 | Loss: 0.00003671
Iteration 34/1000 | Loss: 0.00002810
Iteration 35/1000 | Loss: 0.00013746
Iteration 36/1000 | Loss: 0.00034047
Iteration 37/1000 | Loss: 0.00015413
Iteration 38/1000 | Loss: 0.00024997
Iteration 39/1000 | Loss: 0.00004945
Iteration 40/1000 | Loss: 0.00021751
Iteration 41/1000 | Loss: 0.00017168
Iteration 42/1000 | Loss: 0.00058559
Iteration 43/1000 | Loss: 0.00019048
Iteration 44/1000 | Loss: 0.00011412
Iteration 45/1000 | Loss: 0.00017860
Iteration 46/1000 | Loss: 0.00020279
Iteration 47/1000 | Loss: 0.00014424
Iteration 48/1000 | Loss: 0.00009829
Iteration 49/1000 | Loss: 0.00012718
Iteration 50/1000 | Loss: 0.00015260
Iteration 51/1000 | Loss: 0.00019001
Iteration 52/1000 | Loss: 0.00016719
Iteration 53/1000 | Loss: 0.00015866
Iteration 54/1000 | Loss: 0.00015765
Iteration 55/1000 | Loss: 0.00022387
Iteration 56/1000 | Loss: 0.00005314
Iteration 57/1000 | Loss: 0.00007110
Iteration 58/1000 | Loss: 0.00020287
Iteration 59/1000 | Loss: 0.00002791
Iteration 60/1000 | Loss: 0.00003603
Iteration 61/1000 | Loss: 0.00004137
Iteration 62/1000 | Loss: 0.00003668
Iteration 63/1000 | Loss: 0.00003460
Iteration 64/1000 | Loss: 0.00003405
Iteration 65/1000 | Loss: 0.00003802
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002468
Iteration 68/1000 | Loss: 0.00002219
Iteration 69/1000 | Loss: 0.00001881
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00003092
Iteration 73/1000 | Loss: 0.00003616
Iteration 74/1000 | Loss: 0.00004733
Iteration 75/1000 | Loss: 0.00004261
Iteration 76/1000 | Loss: 0.00003880
Iteration 77/1000 | Loss: 0.00003477
Iteration 78/1000 | Loss: 0.00003042
Iteration 79/1000 | Loss: 0.00003292
Iteration 80/1000 | Loss: 0.00002717
Iteration 81/1000 | Loss: 0.00003111
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00003346
Iteration 84/1000 | Loss: 0.00003451
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001569
Iteration 87/1000 | Loss: 0.00001453
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001290
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001253
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001250
Iteration 113/1000 | Loss: 0.00001250
Iteration 114/1000 | Loss: 0.00001249
Iteration 115/1000 | Loss: 0.00001249
Iteration 116/1000 | Loss: 0.00001249
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001249
Iteration 121/1000 | Loss: 0.00001249
Iteration 122/1000 | Loss: 0.00001249
Iteration 123/1000 | Loss: 0.00001249
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001247
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001247
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001247
Iteration 139/1000 | Loss: 0.00001247
Iteration 140/1000 | Loss: 0.00001247
Iteration 141/1000 | Loss: 0.00001247
Iteration 142/1000 | Loss: 0.00001246
Iteration 143/1000 | Loss: 0.00001246
Iteration 144/1000 | Loss: 0.00001246
Iteration 145/1000 | Loss: 0.00001246
Iteration 146/1000 | Loss: 0.00001246
Iteration 147/1000 | Loss: 0.00001246
Iteration 148/1000 | Loss: 0.00001246
Iteration 149/1000 | Loss: 0.00001246
Iteration 150/1000 | Loss: 0.00001246
Iteration 151/1000 | Loss: 0.00001245
Iteration 152/1000 | Loss: 0.00001245
Iteration 153/1000 | Loss: 0.00001245
Iteration 154/1000 | Loss: 0.00001245
Iteration 155/1000 | Loss: 0.00001245
Iteration 156/1000 | Loss: 0.00001244
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001244
Iteration 159/1000 | Loss: 0.00001244
Iteration 160/1000 | Loss: 0.00001244
Iteration 161/1000 | Loss: 0.00001244
Iteration 162/1000 | Loss: 0.00001244
Iteration 163/1000 | Loss: 0.00001244
Iteration 164/1000 | Loss: 0.00001243
Iteration 165/1000 | Loss: 0.00001243
Iteration 166/1000 | Loss: 0.00001243
Iteration 167/1000 | Loss: 0.00001243
Iteration 168/1000 | Loss: 0.00001242
Iteration 169/1000 | Loss: 0.00001242
Iteration 170/1000 | Loss: 0.00001242
Iteration 171/1000 | Loss: 0.00001242
Iteration 172/1000 | Loss: 0.00001242
Iteration 173/1000 | Loss: 0.00001242
Iteration 174/1000 | Loss: 0.00001241
Iteration 175/1000 | Loss: 0.00001241
Iteration 176/1000 | Loss: 0.00001241
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001240
Iteration 182/1000 | Loss: 0.00001240
Iteration 183/1000 | Loss: 0.00001240
Iteration 184/1000 | Loss: 0.00001240
Iteration 185/1000 | Loss: 0.00001240
Iteration 186/1000 | Loss: 0.00001240
Iteration 187/1000 | Loss: 0.00001240
Iteration 188/1000 | Loss: 0.00001240
Iteration 189/1000 | Loss: 0.00001240
Iteration 190/1000 | Loss: 0.00001240
Iteration 191/1000 | Loss: 0.00001240
Iteration 192/1000 | Loss: 0.00001240
Iteration 193/1000 | Loss: 0.00001239
Iteration 194/1000 | Loss: 0.00001239
Iteration 195/1000 | Loss: 0.00001239
Iteration 196/1000 | Loss: 0.00001239
Iteration 197/1000 | Loss: 0.00001239
Iteration 198/1000 | Loss: 0.00001239
Iteration 199/1000 | Loss: 0.00001239
Iteration 200/1000 | Loss: 0.00001239
Iteration 201/1000 | Loss: 0.00001239
Iteration 202/1000 | Loss: 0.00001239
Iteration 203/1000 | Loss: 0.00001239
Iteration 204/1000 | Loss: 0.00001239
Iteration 205/1000 | Loss: 0.00001239
Iteration 206/1000 | Loss: 0.00001239
Iteration 207/1000 | Loss: 0.00001239
Iteration 208/1000 | Loss: 0.00001238
Iteration 209/1000 | Loss: 0.00001238
Iteration 210/1000 | Loss: 0.00001238
Iteration 211/1000 | Loss: 0.00001238
Iteration 212/1000 | Loss: 0.00001238
Iteration 213/1000 | Loss: 0.00001238
Iteration 214/1000 | Loss: 0.00001238
Iteration 215/1000 | Loss: 0.00001237
Iteration 216/1000 | Loss: 0.00001237
Iteration 217/1000 | Loss: 0.00001237
Iteration 218/1000 | Loss: 0.00001237
Iteration 219/1000 | Loss: 0.00001236
Iteration 220/1000 | Loss: 0.00001236
Iteration 221/1000 | Loss: 0.00001236
Iteration 222/1000 | Loss: 0.00001236
Iteration 223/1000 | Loss: 0.00001236
Iteration 224/1000 | Loss: 0.00001236
Iteration 225/1000 | Loss: 0.00001236
Iteration 226/1000 | Loss: 0.00001236
Iteration 227/1000 | Loss: 0.00001236
Iteration 228/1000 | Loss: 0.00001236
Iteration 229/1000 | Loss: 0.00001236
Iteration 230/1000 | Loss: 0.00001236
Iteration 231/1000 | Loss: 0.00001236
Iteration 232/1000 | Loss: 0.00001235
Iteration 233/1000 | Loss: 0.00001235
Iteration 234/1000 | Loss: 0.00001235
Iteration 235/1000 | Loss: 0.00001235
Iteration 236/1000 | Loss: 0.00001235
Iteration 237/1000 | Loss: 0.00001235
Iteration 238/1000 | Loss: 0.00001235
Iteration 239/1000 | Loss: 0.00001235
Iteration 240/1000 | Loss: 0.00001235
Iteration 241/1000 | Loss: 0.00001235
Iteration 242/1000 | Loss: 0.00001235
Iteration 243/1000 | Loss: 0.00001235
Iteration 244/1000 | Loss: 0.00001235
Iteration 245/1000 | Loss: 0.00001235
Iteration 246/1000 | Loss: 0.00001235
Iteration 247/1000 | Loss: 0.00001235
Iteration 248/1000 | Loss: 0.00001235
Iteration 249/1000 | Loss: 0.00001235
Iteration 250/1000 | Loss: 0.00001235
Iteration 251/1000 | Loss: 0.00001235
Iteration 252/1000 | Loss: 0.00001235
Iteration 253/1000 | Loss: 0.00001235
Iteration 254/1000 | Loss: 0.00001235
Iteration 255/1000 | Loss: 0.00001235
Iteration 256/1000 | Loss: 0.00001235
Iteration 257/1000 | Loss: 0.00001235
Iteration 258/1000 | Loss: 0.00001235
Iteration 259/1000 | Loss: 0.00001235
Iteration 260/1000 | Loss: 0.00001235
Iteration 261/1000 | Loss: 0.00001235
Iteration 262/1000 | Loss: 0.00001235
Iteration 263/1000 | Loss: 0.00001235
Iteration 264/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.2347201845841482e-05, 1.2347201845841482e-05, 1.2347201845841482e-05, 1.2347201845841482e-05, 1.2347201845841482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2347201845841482e-05

Optimization complete. Final v2v error: 2.991892099380493 mm

Highest mean error: 3.968803644180298 mm for frame 58

Lowest mean error: 2.676060438156128 mm for frame 27

Saving results

Total time: 187.24869680404663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445676
Iteration 2/25 | Loss: 0.00141159
Iteration 3/25 | Loss: 0.00122279
Iteration 4/25 | Loss: 0.00120471
Iteration 5/25 | Loss: 0.00120203
Iteration 6/25 | Loss: 0.00120148
Iteration 7/25 | Loss: 0.00120148
Iteration 8/25 | Loss: 0.00120148
Iteration 9/25 | Loss: 0.00120148
Iteration 10/25 | Loss: 0.00120148
Iteration 11/25 | Loss: 0.00120148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012014778330922127, 0.0012014778330922127, 0.0012014778330922127, 0.0012014778330922127, 0.0012014778330922127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012014778330922127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39667308
Iteration 2/25 | Loss: 0.00121984
Iteration 3/25 | Loss: 0.00121984
Iteration 4/25 | Loss: 0.00121984
Iteration 5/25 | Loss: 0.00121984
Iteration 6/25 | Loss: 0.00121984
Iteration 7/25 | Loss: 0.00121984
Iteration 8/25 | Loss: 0.00121984
Iteration 9/25 | Loss: 0.00121984
Iteration 10/25 | Loss: 0.00121983
Iteration 11/25 | Loss: 0.00121983
Iteration 12/25 | Loss: 0.00121983
Iteration 13/25 | Loss: 0.00121983
Iteration 14/25 | Loss: 0.00121983
Iteration 15/25 | Loss: 0.00121983
Iteration 16/25 | Loss: 0.00121983
Iteration 17/25 | Loss: 0.00121983
Iteration 18/25 | Loss: 0.00121983
Iteration 19/25 | Loss: 0.00121983
Iteration 20/25 | Loss: 0.00121983
Iteration 21/25 | Loss: 0.00121983
Iteration 22/25 | Loss: 0.00121983
Iteration 23/25 | Loss: 0.00121983
Iteration 24/25 | Loss: 0.00121983
Iteration 25/25 | Loss: 0.00121983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121983
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001732
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001329
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001233
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001225
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001218
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001217
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001217
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001213
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001208
Iteration 120/1000 | Loss: 0.00001208
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.2075021004420705e-05, 1.2075021004420705e-05, 1.2075021004420705e-05, 1.2075021004420705e-05, 1.2075021004420705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2075021004420705e-05

Optimization complete. Final v2v error: 2.949212074279785 mm

Highest mean error: 4.115646839141846 mm for frame 105

Lowest mean error: 2.5590121746063232 mm for frame 160

Saving results

Total time: 39.569401264190674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397400
Iteration 2/25 | Loss: 0.00131943
Iteration 3/25 | Loss: 0.00119866
Iteration 4/25 | Loss: 0.00118926
Iteration 5/25 | Loss: 0.00118721
Iteration 6/25 | Loss: 0.00118721
Iteration 7/25 | Loss: 0.00118721
Iteration 8/25 | Loss: 0.00118721
Iteration 9/25 | Loss: 0.00118721
Iteration 10/25 | Loss: 0.00118721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011872098548337817, 0.0011872098548337817, 0.0011872098548337817, 0.0011872098548337817, 0.0011872098548337817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011872098548337817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49533534
Iteration 2/25 | Loss: 0.00121839
Iteration 3/25 | Loss: 0.00121837
Iteration 4/25 | Loss: 0.00121837
Iteration 5/25 | Loss: 0.00121837
Iteration 6/25 | Loss: 0.00121837
Iteration 7/25 | Loss: 0.00121837
Iteration 8/25 | Loss: 0.00121837
Iteration 9/25 | Loss: 0.00121837
Iteration 10/25 | Loss: 0.00121837
Iteration 11/25 | Loss: 0.00121837
Iteration 12/25 | Loss: 0.00121837
Iteration 13/25 | Loss: 0.00121837
Iteration 14/25 | Loss: 0.00121837
Iteration 15/25 | Loss: 0.00121837
Iteration 16/25 | Loss: 0.00121837
Iteration 17/25 | Loss: 0.00121837
Iteration 18/25 | Loss: 0.00121837
Iteration 19/25 | Loss: 0.00121837
Iteration 20/25 | Loss: 0.00121837
Iteration 21/25 | Loss: 0.00121837
Iteration 22/25 | Loss: 0.00121837
Iteration 23/25 | Loss: 0.00121837
Iteration 24/25 | Loss: 0.00121837
Iteration 25/25 | Loss: 0.00121837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121837
Iteration 2/1000 | Loss: 0.00002625
Iteration 3/1000 | Loss: 0.00001514
Iteration 4/1000 | Loss: 0.00001321
Iteration 5/1000 | Loss: 0.00001213
Iteration 6/1000 | Loss: 0.00001162
Iteration 7/1000 | Loss: 0.00001134
Iteration 8/1000 | Loss: 0.00001100
Iteration 9/1000 | Loss: 0.00001064
Iteration 10/1000 | Loss: 0.00001035
Iteration 11/1000 | Loss: 0.00001025
Iteration 12/1000 | Loss: 0.00001009
Iteration 13/1000 | Loss: 0.00001006
Iteration 14/1000 | Loss: 0.00001004
Iteration 15/1000 | Loss: 0.00000998
Iteration 16/1000 | Loss: 0.00000998
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000998
Iteration 19/1000 | Loss: 0.00000998
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000997
Iteration 22/1000 | Loss: 0.00000997
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000991
Iteration 26/1000 | Loss: 0.00000991
Iteration 27/1000 | Loss: 0.00000989
Iteration 28/1000 | Loss: 0.00000987
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000984
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000981
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000978
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000976
Iteration 46/1000 | Loss: 0.00000976
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000976
Iteration 55/1000 | Loss: 0.00000976
Iteration 56/1000 | Loss: 0.00000976
Iteration 57/1000 | Loss: 0.00000976
Iteration 58/1000 | Loss: 0.00000976
Iteration 59/1000 | Loss: 0.00000976
Iteration 60/1000 | Loss: 0.00000976
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000976
Iteration 64/1000 | Loss: 0.00000976
Iteration 65/1000 | Loss: 0.00000976
Iteration 66/1000 | Loss: 0.00000976
Iteration 67/1000 | Loss: 0.00000976
Iteration 68/1000 | Loss: 0.00000976
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000976
Iteration 74/1000 | Loss: 0.00000976
Iteration 75/1000 | Loss: 0.00000976
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [9.755322025739588e-06, 9.755322025739588e-06, 9.755322025739588e-06, 9.755322025739588e-06, 9.755322025739588e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.755322025739588e-06

Optimization complete. Final v2v error: 2.70477032661438 mm

Highest mean error: 2.914900541305542 mm for frame 170

Lowest mean error: 2.5673911571502686 mm for frame 261

Saving results

Total time: 34.01345705986023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055520
Iteration 2/25 | Loss: 0.00156638
Iteration 3/25 | Loss: 0.00134525
Iteration 4/25 | Loss: 0.00124408
Iteration 5/25 | Loss: 0.00122222
Iteration 6/25 | Loss: 0.00121530
Iteration 7/25 | Loss: 0.00121372
Iteration 8/25 | Loss: 0.00121352
Iteration 9/25 | Loss: 0.00121349
Iteration 10/25 | Loss: 0.00121349
Iteration 11/25 | Loss: 0.00121348
Iteration 12/25 | Loss: 0.00121348
Iteration 13/25 | Loss: 0.00121348
Iteration 14/25 | Loss: 0.00121348
Iteration 15/25 | Loss: 0.00121348
Iteration 16/25 | Loss: 0.00121346
Iteration 17/25 | Loss: 0.00121345
Iteration 18/25 | Loss: 0.00121345
Iteration 19/25 | Loss: 0.00121345
Iteration 20/25 | Loss: 0.00121345
Iteration 21/25 | Loss: 0.00121345
Iteration 22/25 | Loss: 0.00121345
Iteration 23/25 | Loss: 0.00121345
Iteration 24/25 | Loss: 0.00121345
Iteration 25/25 | Loss: 0.00121344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.37597847
Iteration 2/25 | Loss: 0.00137956
Iteration 3/25 | Loss: 0.00137956
Iteration 4/25 | Loss: 0.00137956
Iteration 5/25 | Loss: 0.00137956
Iteration 6/25 | Loss: 0.00137956
Iteration 7/25 | Loss: 0.00137956
Iteration 8/25 | Loss: 0.00137956
Iteration 9/25 | Loss: 0.00137956
Iteration 10/25 | Loss: 0.00137956
Iteration 11/25 | Loss: 0.00137956
Iteration 12/25 | Loss: 0.00137956
Iteration 13/25 | Loss: 0.00137956
Iteration 14/25 | Loss: 0.00137956
Iteration 15/25 | Loss: 0.00137956
Iteration 16/25 | Loss: 0.00137956
Iteration 17/25 | Loss: 0.00137956
Iteration 18/25 | Loss: 0.00137956
Iteration 19/25 | Loss: 0.00137956
Iteration 20/25 | Loss: 0.00137956
Iteration 21/25 | Loss: 0.00137956
Iteration 22/25 | Loss: 0.00137956
Iteration 23/25 | Loss: 0.00137956
Iteration 24/25 | Loss: 0.00137956
Iteration 25/25 | Loss: 0.00137956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137956
Iteration 2/1000 | Loss: 0.00001882
Iteration 3/1000 | Loss: 0.00001386
Iteration 4/1000 | Loss: 0.00006657
Iteration 5/1000 | Loss: 0.00020853
Iteration 6/1000 | Loss: 0.00001966
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00012126
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00004232
Iteration 12/1000 | Loss: 0.00022620
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00003868
Iteration 16/1000 | Loss: 0.00004224
Iteration 17/1000 | Loss: 0.00009371
Iteration 18/1000 | Loss: 0.00001221
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001074
Iteration 21/1000 | Loss: 0.00008516
Iteration 22/1000 | Loss: 0.00004294
Iteration 23/1000 | Loss: 0.00001228
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001059
Iteration 29/1000 | Loss: 0.00001059
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001059
Iteration 33/1000 | Loss: 0.00001058
Iteration 34/1000 | Loss: 0.00001058
Iteration 35/1000 | Loss: 0.00001058
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001057
Iteration 44/1000 | Loss: 0.00001057
Iteration 45/1000 | Loss: 0.00001056
Iteration 46/1000 | Loss: 0.00001055
Iteration 47/1000 | Loss: 0.00001055
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00007125
Iteration 52/1000 | Loss: 0.00002785
Iteration 53/1000 | Loss: 0.00004418
Iteration 54/1000 | Loss: 0.00002668
Iteration 55/1000 | Loss: 0.00016273
Iteration 56/1000 | Loss: 0.00002409
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001046
Iteration 60/1000 | Loss: 0.00001046
Iteration 61/1000 | Loss: 0.00001044
Iteration 62/1000 | Loss: 0.00001044
Iteration 63/1000 | Loss: 0.00001044
Iteration 64/1000 | Loss: 0.00001044
Iteration 65/1000 | Loss: 0.00001044
Iteration 66/1000 | Loss: 0.00001044
Iteration 67/1000 | Loss: 0.00001043
Iteration 68/1000 | Loss: 0.00001043
Iteration 69/1000 | Loss: 0.00001043
Iteration 70/1000 | Loss: 0.00001043
Iteration 71/1000 | Loss: 0.00001043
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001042
Iteration 74/1000 | Loss: 0.00001042
Iteration 75/1000 | Loss: 0.00001041
Iteration 76/1000 | Loss: 0.00001041
Iteration 77/1000 | Loss: 0.00001041
Iteration 78/1000 | Loss: 0.00001041
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001040
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001078
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001038
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001037
Iteration 96/1000 | Loss: 0.00001037
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001037
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Iteration 103/1000 | Loss: 0.00001037
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001037
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001037
Iteration 109/1000 | Loss: 0.00001037
Iteration 110/1000 | Loss: 0.00001037
Iteration 111/1000 | Loss: 0.00001037
Iteration 112/1000 | Loss: 0.00001037
Iteration 113/1000 | Loss: 0.00001037
Iteration 114/1000 | Loss: 0.00001037
Iteration 115/1000 | Loss: 0.00001037
Iteration 116/1000 | Loss: 0.00001037
Iteration 117/1000 | Loss: 0.00001037
Iteration 118/1000 | Loss: 0.00001037
Iteration 119/1000 | Loss: 0.00001037
Iteration 120/1000 | Loss: 0.00001037
Iteration 121/1000 | Loss: 0.00001037
Iteration 122/1000 | Loss: 0.00001037
Iteration 123/1000 | Loss: 0.00001037
Iteration 124/1000 | Loss: 0.00001037
Iteration 125/1000 | Loss: 0.00001037
Iteration 126/1000 | Loss: 0.00001037
Iteration 127/1000 | Loss: 0.00001037
Iteration 128/1000 | Loss: 0.00001037
Iteration 129/1000 | Loss: 0.00001037
Iteration 130/1000 | Loss: 0.00001037
Iteration 131/1000 | Loss: 0.00001037
Iteration 132/1000 | Loss: 0.00001037
Iteration 133/1000 | Loss: 0.00001037
Iteration 134/1000 | Loss: 0.00001037
Iteration 135/1000 | Loss: 0.00001037
Iteration 136/1000 | Loss: 0.00001037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.0371545613452327e-05, 1.0371545613452327e-05, 1.0371545613452327e-05, 1.0371545613452327e-05, 1.0371545613452327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0371545613452327e-05

Optimization complete. Final v2v error: 2.742645025253296 mm

Highest mean error: 3.2112038135528564 mm for frame 119

Lowest mean error: 2.5658175945281982 mm for frame 159

Saving results

Total time: 71.27650547027588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825776
Iteration 2/25 | Loss: 0.00125513
Iteration 3/25 | Loss: 0.00119277
Iteration 4/25 | Loss: 0.00118257
Iteration 5/25 | Loss: 0.00118056
Iteration 6/25 | Loss: 0.00118056
Iteration 7/25 | Loss: 0.00118056
Iteration 8/25 | Loss: 0.00118056
Iteration 9/25 | Loss: 0.00118056
Iteration 10/25 | Loss: 0.00118056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001180559629574418, 0.001180559629574418, 0.001180559629574418, 0.001180559629574418, 0.001180559629574418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001180559629574418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30959499
Iteration 2/25 | Loss: 0.00135173
Iteration 3/25 | Loss: 0.00135173
Iteration 4/25 | Loss: 0.00135172
Iteration 5/25 | Loss: 0.00135172
Iteration 6/25 | Loss: 0.00135172
Iteration 7/25 | Loss: 0.00135172
Iteration 8/25 | Loss: 0.00135172
Iteration 9/25 | Loss: 0.00135172
Iteration 10/25 | Loss: 0.00135172
Iteration 11/25 | Loss: 0.00135172
Iteration 12/25 | Loss: 0.00135172
Iteration 13/25 | Loss: 0.00135172
Iteration 14/25 | Loss: 0.00135172
Iteration 15/25 | Loss: 0.00135172
Iteration 16/25 | Loss: 0.00135172
Iteration 17/25 | Loss: 0.00135172
Iteration 18/25 | Loss: 0.00135172
Iteration 19/25 | Loss: 0.00135172
Iteration 20/25 | Loss: 0.00135172
Iteration 21/25 | Loss: 0.00135172
Iteration 22/25 | Loss: 0.00135172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013517220504581928, 0.0013517220504581928, 0.0013517220504581928, 0.0013517220504581928, 0.0013517220504581928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013517220504581928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135172
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001635
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001202
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001158
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001155
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001146
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001142
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001130
Iteration 50/1000 | Loss: 0.00001129
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001128
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001127
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001125
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001124
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001123
Iteration 72/1000 | Loss: 0.00001123
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001121
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001117
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001116
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001115
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001106
Iteration 141/1000 | Loss: 0.00001106
Iteration 142/1000 | Loss: 0.00001106
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001104
Iteration 162/1000 | Loss: 0.00001104
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001104
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001103
Iteration 168/1000 | Loss: 0.00001103
Iteration 169/1000 | Loss: 0.00001103
Iteration 170/1000 | Loss: 0.00001103
Iteration 171/1000 | Loss: 0.00001103
Iteration 172/1000 | Loss: 0.00001103
Iteration 173/1000 | Loss: 0.00001103
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001102
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001102
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001101
Iteration 200/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.1005358828697354e-05, 1.1005358828697354e-05, 1.1005358828697354e-05, 1.1005358828697354e-05, 1.1005358828697354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1005358828697354e-05

Optimization complete. Final v2v error: 2.8365297317504883 mm

Highest mean error: 3.2111458778381348 mm for frame 97

Lowest mean error: 2.6966476440429688 mm for frame 144

Saving results

Total time: 42.599876165390015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054718
Iteration 2/25 | Loss: 0.00495331
Iteration 3/25 | Loss: 0.00346346
Iteration 4/25 | Loss: 0.00321752
Iteration 5/25 | Loss: 0.00285366
Iteration 6/25 | Loss: 0.00240439
Iteration 7/25 | Loss: 0.00212643
Iteration 8/25 | Loss: 0.00215121
Iteration 9/25 | Loss: 0.00221889
Iteration 10/25 | Loss: 0.00193983
Iteration 11/25 | Loss: 0.00193459
Iteration 12/25 | Loss: 0.00192683
Iteration 13/25 | Loss: 0.00188787
Iteration 14/25 | Loss: 0.00184188
Iteration 15/25 | Loss: 0.00179486
Iteration 16/25 | Loss: 0.00178813
Iteration 17/25 | Loss: 0.00176378
Iteration 18/25 | Loss: 0.00174595
Iteration 19/25 | Loss: 0.00174534
Iteration 20/25 | Loss: 0.00173661
Iteration 21/25 | Loss: 0.00172670
Iteration 22/25 | Loss: 0.00171509
Iteration 23/25 | Loss: 0.00171494
Iteration 24/25 | Loss: 0.00171355
Iteration 25/25 | Loss: 0.00170942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.35843059
Iteration 2/25 | Loss: 0.00291859
Iteration 3/25 | Loss: 0.00193517
Iteration 4/25 | Loss: 0.00188964
Iteration 5/25 | Loss: 0.00188964
Iteration 6/25 | Loss: 0.00188964
Iteration 7/25 | Loss: 0.00188964
Iteration 8/25 | Loss: 0.00188964
Iteration 9/25 | Loss: 0.00188964
Iteration 10/25 | Loss: 0.00188964
Iteration 11/25 | Loss: 0.00188964
Iteration 12/25 | Loss: 0.00188964
Iteration 13/25 | Loss: 0.00188964
Iteration 14/25 | Loss: 0.00188964
Iteration 15/25 | Loss: 0.00188964
Iteration 16/25 | Loss: 0.00188964
Iteration 17/25 | Loss: 0.00188964
Iteration 18/25 | Loss: 0.00188964
Iteration 19/25 | Loss: 0.00188964
Iteration 20/25 | Loss: 0.00188964
Iteration 21/25 | Loss: 0.00188964
Iteration 22/25 | Loss: 0.00188964
Iteration 23/25 | Loss: 0.00188964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018896377878263593, 0.0018896377878263593, 0.0018896377878263593, 0.0018896377878263593, 0.0018896377878263593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018896377878263593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188964
Iteration 2/1000 | Loss: 0.00022200
Iteration 3/1000 | Loss: 0.00086286
Iteration 4/1000 | Loss: 0.00016312
Iteration 5/1000 | Loss: 0.00037067
Iteration 6/1000 | Loss: 0.00051094
Iteration 7/1000 | Loss: 0.00018775
Iteration 8/1000 | Loss: 0.00016065
Iteration 9/1000 | Loss: 0.00022025
Iteration 10/1000 | Loss: 0.00019696
Iteration 11/1000 | Loss: 0.00069673
Iteration 12/1000 | Loss: 0.00013234
Iteration 13/1000 | Loss: 0.00020928
Iteration 14/1000 | Loss: 0.00026811
Iteration 15/1000 | Loss: 0.00020137
Iteration 16/1000 | Loss: 0.00026338
Iteration 17/1000 | Loss: 0.00019977
Iteration 18/1000 | Loss: 0.00016797
Iteration 19/1000 | Loss: 0.00020989
Iteration 20/1000 | Loss: 0.00074903
Iteration 21/1000 | Loss: 0.00024474
Iteration 22/1000 | Loss: 0.00041969
Iteration 23/1000 | Loss: 0.00016754
Iteration 24/1000 | Loss: 0.00057235
Iteration 25/1000 | Loss: 0.00013712
Iteration 26/1000 | Loss: 0.00014037
Iteration 27/1000 | Loss: 0.00014479
Iteration 28/1000 | Loss: 0.00014760
Iteration 29/1000 | Loss: 0.00014091
Iteration 30/1000 | Loss: 0.00017846
Iteration 31/1000 | Loss: 0.00012377
Iteration 32/1000 | Loss: 0.00013587
Iteration 33/1000 | Loss: 0.00019512
Iteration 34/1000 | Loss: 0.00019740
Iteration 35/1000 | Loss: 0.00023821
Iteration 36/1000 | Loss: 0.00021192
Iteration 37/1000 | Loss: 0.00012480
Iteration 38/1000 | Loss: 0.00018685
Iteration 39/1000 | Loss: 0.00016221
Iteration 40/1000 | Loss: 0.00013534
Iteration 41/1000 | Loss: 0.00013137
Iteration 42/1000 | Loss: 0.00021332
Iteration 43/1000 | Loss: 0.00018408
Iteration 44/1000 | Loss: 0.00020974
Iteration 45/1000 | Loss: 0.00021435
Iteration 46/1000 | Loss: 0.00017167
Iteration 47/1000 | Loss: 0.00019333
Iteration 48/1000 | Loss: 0.00013615
Iteration 49/1000 | Loss: 0.00022762
Iteration 50/1000 | Loss: 0.00021489
Iteration 51/1000 | Loss: 0.00019369
Iteration 52/1000 | Loss: 0.00026407
Iteration 53/1000 | Loss: 0.00015406
Iteration 54/1000 | Loss: 0.00017629
Iteration 55/1000 | Loss: 0.00015752
Iteration 56/1000 | Loss: 0.00041423
Iteration 57/1000 | Loss: 0.00013444
Iteration 58/1000 | Loss: 0.00014287
Iteration 59/1000 | Loss: 0.00014257
Iteration 60/1000 | Loss: 0.00013631
Iteration 61/1000 | Loss: 0.00012988
Iteration 62/1000 | Loss: 0.00020182
Iteration 63/1000 | Loss: 0.00011682
Iteration 64/1000 | Loss: 0.00013559
Iteration 65/1000 | Loss: 0.00013172
Iteration 66/1000 | Loss: 0.00014124
Iteration 67/1000 | Loss: 0.00012723
Iteration 68/1000 | Loss: 0.00027313
Iteration 69/1000 | Loss: 0.00023651
Iteration 70/1000 | Loss: 0.00025784
Iteration 71/1000 | Loss: 0.00020214
Iteration 72/1000 | Loss: 0.00033980
Iteration 73/1000 | Loss: 0.00013075
Iteration 74/1000 | Loss: 0.00013742
Iteration 75/1000 | Loss: 0.00011657
Iteration 76/1000 | Loss: 0.00013215
Iteration 77/1000 | Loss: 0.00012810
Iteration 78/1000 | Loss: 0.00012710
Iteration 79/1000 | Loss: 0.00013747
Iteration 80/1000 | Loss: 0.00012005
Iteration 81/1000 | Loss: 0.00012598
Iteration 82/1000 | Loss: 0.00013176
Iteration 83/1000 | Loss: 0.00013630
Iteration 84/1000 | Loss: 0.00013354
Iteration 85/1000 | Loss: 0.00013221
Iteration 86/1000 | Loss: 0.00012560
Iteration 87/1000 | Loss: 0.00013676
Iteration 88/1000 | Loss: 0.00012961
Iteration 89/1000 | Loss: 0.00014589
Iteration 90/1000 | Loss: 0.00013910
Iteration 91/1000 | Loss: 0.00014795
Iteration 92/1000 | Loss: 0.00013237
Iteration 93/1000 | Loss: 0.00014304
Iteration 94/1000 | Loss: 0.00013177
Iteration 95/1000 | Loss: 0.00013328
Iteration 96/1000 | Loss: 0.00013988
Iteration 97/1000 | Loss: 0.00014364
Iteration 98/1000 | Loss: 0.00013317
Iteration 99/1000 | Loss: 0.00013850
Iteration 100/1000 | Loss: 0.00013306
Iteration 101/1000 | Loss: 0.00012982
Iteration 102/1000 | Loss: 0.00013415
Iteration 103/1000 | Loss: 0.00013532
Iteration 104/1000 | Loss: 0.00013007
Iteration 105/1000 | Loss: 0.00012884
Iteration 106/1000 | Loss: 0.00013026
Iteration 107/1000 | Loss: 0.00023832
Iteration 108/1000 | Loss: 0.00021970
Iteration 109/1000 | Loss: 0.00022210
Iteration 110/1000 | Loss: 0.00028069
Iteration 111/1000 | Loss: 0.00024434
Iteration 112/1000 | Loss: 0.00025049
Iteration 113/1000 | Loss: 0.00026239
Iteration 114/1000 | Loss: 0.00026162
Iteration 115/1000 | Loss: 0.00013905
Iteration 116/1000 | Loss: 0.00012931
Iteration 117/1000 | Loss: 0.00022819
Iteration 118/1000 | Loss: 0.00019799
Iteration 119/1000 | Loss: 0.00020117
Iteration 120/1000 | Loss: 0.00013582
Iteration 121/1000 | Loss: 0.00012007
Iteration 122/1000 | Loss: 0.00011374
Iteration 123/1000 | Loss: 0.00020004
Iteration 124/1000 | Loss: 0.00020514
Iteration 125/1000 | Loss: 0.00024307
Iteration 126/1000 | Loss: 0.00023494
Iteration 127/1000 | Loss: 0.00019656
Iteration 128/1000 | Loss: 0.00023367
Iteration 129/1000 | Loss: 0.00022865
Iteration 130/1000 | Loss: 0.00024435
Iteration 131/1000 | Loss: 0.00020653
Iteration 132/1000 | Loss: 0.00022090
Iteration 133/1000 | Loss: 0.00018215
Iteration 134/1000 | Loss: 0.00021273
Iteration 135/1000 | Loss: 0.00012659
Iteration 136/1000 | Loss: 0.00012921
Iteration 137/1000 | Loss: 0.00016927
Iteration 138/1000 | Loss: 0.00016581
Iteration 139/1000 | Loss: 0.00023629
Iteration 140/1000 | Loss: 0.00019338
Iteration 141/1000 | Loss: 0.00024599
Iteration 142/1000 | Loss: 0.00026947
Iteration 143/1000 | Loss: 0.00022075
Iteration 144/1000 | Loss: 0.00012979
Iteration 145/1000 | Loss: 0.00016242
Iteration 146/1000 | Loss: 0.00014026
Iteration 147/1000 | Loss: 0.00013235
Iteration 148/1000 | Loss: 0.00015480
Iteration 149/1000 | Loss: 0.00021092
Iteration 150/1000 | Loss: 0.00020763
Iteration 151/1000 | Loss: 0.00023060
Iteration 152/1000 | Loss: 0.00022574
Iteration 153/1000 | Loss: 0.00021018
Iteration 154/1000 | Loss: 0.00017317
Iteration 155/1000 | Loss: 0.00020945
Iteration 156/1000 | Loss: 0.00015066
Iteration 157/1000 | Loss: 0.00015097
Iteration 158/1000 | Loss: 0.00014680
Iteration 159/1000 | Loss: 0.00016214
Iteration 160/1000 | Loss: 0.00021640
Iteration 161/1000 | Loss: 0.00021859
Iteration 162/1000 | Loss: 0.00015267
Iteration 163/1000 | Loss: 0.00012845
Iteration 164/1000 | Loss: 0.00019538
Iteration 165/1000 | Loss: 0.00017249
Iteration 166/1000 | Loss: 0.00018136
Iteration 167/1000 | Loss: 0.00016426
Iteration 168/1000 | Loss: 0.00019828
Iteration 169/1000 | Loss: 0.00024722
Iteration 170/1000 | Loss: 0.00019339
Iteration 171/1000 | Loss: 0.00029889
Iteration 172/1000 | Loss: 0.00019212
Iteration 173/1000 | Loss: 0.00022955
Iteration 174/1000 | Loss: 0.00019856
Iteration 175/1000 | Loss: 0.00021693
Iteration 176/1000 | Loss: 0.00015216
Iteration 177/1000 | Loss: 0.00014151
Iteration 178/1000 | Loss: 0.00013114
Iteration 179/1000 | Loss: 0.00012939
Iteration 180/1000 | Loss: 0.00013091
Iteration 181/1000 | Loss: 0.00012468
Iteration 182/1000 | Loss: 0.00018877
Iteration 183/1000 | Loss: 0.00022240
Iteration 184/1000 | Loss: 0.00011492
Iteration 185/1000 | Loss: 0.00022067
Iteration 186/1000 | Loss: 0.00017342
Iteration 187/1000 | Loss: 0.00022084
Iteration 188/1000 | Loss: 0.00020871
Iteration 189/1000 | Loss: 0.00014275
Iteration 190/1000 | Loss: 0.00014237
Iteration 191/1000 | Loss: 0.00013171
Iteration 192/1000 | Loss: 0.00014062
Iteration 193/1000 | Loss: 0.00021574
Iteration 194/1000 | Loss: 0.00019746
Iteration 195/1000 | Loss: 0.00018515
Iteration 196/1000 | Loss: 0.00019655
Iteration 197/1000 | Loss: 0.00019956
Iteration 198/1000 | Loss: 0.00021224
Iteration 199/1000 | Loss: 0.00022317
Iteration 200/1000 | Loss: 0.00013404
Iteration 201/1000 | Loss: 0.00017895
Iteration 202/1000 | Loss: 0.00021631
Iteration 203/1000 | Loss: 0.00019148
Iteration 204/1000 | Loss: 0.00020769
Iteration 205/1000 | Loss: 0.00019318
Iteration 206/1000 | Loss: 0.00022293
Iteration 207/1000 | Loss: 0.00021037
Iteration 208/1000 | Loss: 0.00014979
Iteration 209/1000 | Loss: 0.00013037
Iteration 210/1000 | Loss: 0.00011808
Iteration 211/1000 | Loss: 0.00012444
Iteration 212/1000 | Loss: 0.00013512
Iteration 213/1000 | Loss: 0.00013022
Iteration 214/1000 | Loss: 0.00012209
Iteration 215/1000 | Loss: 0.00012225
Iteration 216/1000 | Loss: 0.00013722
Iteration 217/1000 | Loss: 0.00014180
Iteration 218/1000 | Loss: 0.00013072
Iteration 219/1000 | Loss: 0.00013676
Iteration 220/1000 | Loss: 0.00012950
Iteration 221/1000 | Loss: 0.00013390
Iteration 222/1000 | Loss: 0.00018442
Iteration 223/1000 | Loss: 0.00021849
Iteration 224/1000 | Loss: 0.00014789
Iteration 225/1000 | Loss: 0.00024499
Iteration 226/1000 | Loss: 0.00013873
Iteration 227/1000 | Loss: 0.00011534
Iteration 228/1000 | Loss: 0.00013696
Iteration 229/1000 | Loss: 0.00012719
Iteration 230/1000 | Loss: 0.00013496
Iteration 231/1000 | Loss: 0.00012668
Iteration 232/1000 | Loss: 0.00013135
Iteration 233/1000 | Loss: 0.00012019
Iteration 234/1000 | Loss: 0.00012563
Iteration 235/1000 | Loss: 0.00010949
Iteration 236/1000 | Loss: 0.00008778
Iteration 237/1000 | Loss: 0.00012110
Iteration 238/1000 | Loss: 0.00012997
Iteration 239/1000 | Loss: 0.00013115
Iteration 240/1000 | Loss: 0.00030244
Iteration 241/1000 | Loss: 0.00011569
Iteration 242/1000 | Loss: 0.00013220
Iteration 243/1000 | Loss: 0.00013262
Iteration 244/1000 | Loss: 0.00011853
Iteration 245/1000 | Loss: 0.00011699
Iteration 246/1000 | Loss: 0.00013498
Iteration 247/1000 | Loss: 0.00013083
Iteration 248/1000 | Loss: 0.00012713
Iteration 249/1000 | Loss: 0.00013140
Iteration 250/1000 | Loss: 0.00013178
Iteration 251/1000 | Loss: 0.00012899
Iteration 252/1000 | Loss: 0.00013222
Iteration 253/1000 | Loss: 0.00013529
Iteration 254/1000 | Loss: 0.00013059
Iteration 255/1000 | Loss: 0.00013045
Iteration 256/1000 | Loss: 0.00013287
Iteration 257/1000 | Loss: 0.00013272
Iteration 258/1000 | Loss: 0.00013119
Iteration 259/1000 | Loss: 0.00009837
Iteration 260/1000 | Loss: 0.00009491
Iteration 261/1000 | Loss: 0.00010224
Iteration 262/1000 | Loss: 0.00008616
Iteration 263/1000 | Loss: 0.00010759
Iteration 264/1000 | Loss: 0.00008341
Iteration 265/1000 | Loss: 0.00008332
Iteration 266/1000 | Loss: 0.00007964
Iteration 267/1000 | Loss: 0.00008963
Iteration 268/1000 | Loss: 0.00008518
Iteration 269/1000 | Loss: 0.00008011
Iteration 270/1000 | Loss: 0.00008539
Iteration 271/1000 | Loss: 0.00009025
Iteration 272/1000 | Loss: 0.00008715
Iteration 273/1000 | Loss: 0.00008792
Iteration 274/1000 | Loss: 0.00008772
Iteration 275/1000 | Loss: 0.00008794
Iteration 276/1000 | Loss: 0.00008605
Iteration 277/1000 | Loss: 0.00008642
Iteration 278/1000 | Loss: 0.00008565
Iteration 279/1000 | Loss: 0.00008575
Iteration 280/1000 | Loss: 0.00008464
Iteration 281/1000 | Loss: 0.00008648
Iteration 282/1000 | Loss: 0.00008423
Iteration 283/1000 | Loss: 0.00008559
Iteration 284/1000 | Loss: 0.00008383
Iteration 285/1000 | Loss: 0.00008516
Iteration 286/1000 | Loss: 0.00008341
Iteration 287/1000 | Loss: 0.00008146
Iteration 288/1000 | Loss: 0.00008351
Iteration 289/1000 | Loss: 0.00008278
Iteration 290/1000 | Loss: 0.00007906
Iteration 291/1000 | Loss: 0.00008247
Iteration 292/1000 | Loss: 0.00007971
Iteration 293/1000 | Loss: 0.00008203
Iteration 294/1000 | Loss: 0.00007649
Iteration 295/1000 | Loss: 0.00008534
Iteration 296/1000 | Loss: 0.00008756
Iteration 297/1000 | Loss: 0.00008543
Iteration 298/1000 | Loss: 0.00008562
Iteration 299/1000 | Loss: 0.00008728
Iteration 300/1000 | Loss: 0.00008561
Iteration 301/1000 | Loss: 0.00007872
Iteration 302/1000 | Loss: 0.00008398
Iteration 303/1000 | Loss: 0.00007988
Iteration 304/1000 | Loss: 0.00007907
Iteration 305/1000 | Loss: 0.00008476
Iteration 306/1000 | Loss: 0.00008097
Iteration 307/1000 | Loss: 0.00009414
Iteration 308/1000 | Loss: 0.00008515
Iteration 309/1000 | Loss: 0.00008370
Iteration 310/1000 | Loss: 0.00009058
Iteration 311/1000 | Loss: 0.00008545
Iteration 312/1000 | Loss: 0.00009179
Iteration 313/1000 | Loss: 0.00008828
Iteration 314/1000 | Loss: 0.00009171
Iteration 315/1000 | Loss: 0.00008470
Iteration 316/1000 | Loss: 0.00009110
Iteration 317/1000 | Loss: 0.00007816
Iteration 318/1000 | Loss: 0.00007541
Iteration 319/1000 | Loss: 0.00007462
Iteration 320/1000 | Loss: 0.00007418
Iteration 321/1000 | Loss: 0.00007388
Iteration 322/1000 | Loss: 0.00007367
Iteration 323/1000 | Loss: 0.00007348
Iteration 324/1000 | Loss: 0.00007344
Iteration 325/1000 | Loss: 0.00007326
Iteration 326/1000 | Loss: 0.00007308
Iteration 327/1000 | Loss: 0.00007291
Iteration 328/1000 | Loss: 0.00007277
Iteration 329/1000 | Loss: 0.00007269
Iteration 330/1000 | Loss: 0.00007269
Iteration 331/1000 | Loss: 0.00007263
Iteration 332/1000 | Loss: 0.00007261
Iteration 333/1000 | Loss: 0.00007260
Iteration 334/1000 | Loss: 0.00007260
Iteration 335/1000 | Loss: 0.00007260
Iteration 336/1000 | Loss: 0.00007260
Iteration 337/1000 | Loss: 0.00007260
Iteration 338/1000 | Loss: 0.00007260
Iteration 339/1000 | Loss: 0.00007260
Iteration 340/1000 | Loss: 0.00007260
Iteration 341/1000 | Loss: 0.00007260
Iteration 342/1000 | Loss: 0.00007260
Iteration 343/1000 | Loss: 0.00007260
Iteration 344/1000 | Loss: 0.00007259
Iteration 345/1000 | Loss: 0.00007258
Iteration 346/1000 | Loss: 0.00007258
Iteration 347/1000 | Loss: 0.00007258
Iteration 348/1000 | Loss: 0.00007258
Iteration 349/1000 | Loss: 0.00007258
Iteration 350/1000 | Loss: 0.00007258
Iteration 351/1000 | Loss: 0.00007258
Iteration 352/1000 | Loss: 0.00007258
Iteration 353/1000 | Loss: 0.00007257
Iteration 354/1000 | Loss: 0.00007257
Iteration 355/1000 | Loss: 0.00007256
Iteration 356/1000 | Loss: 0.00007256
Iteration 357/1000 | Loss: 0.00007255
Iteration 358/1000 | Loss: 0.00007255
Iteration 359/1000 | Loss: 0.00007255
Iteration 360/1000 | Loss: 0.00007255
Iteration 361/1000 | Loss: 0.00007254
Iteration 362/1000 | Loss: 0.00007253
Iteration 363/1000 | Loss: 0.00007253
Iteration 364/1000 | Loss: 0.00007252
Iteration 365/1000 | Loss: 0.00007252
Iteration 366/1000 | Loss: 0.00007251
Iteration 367/1000 | Loss: 0.00007250
Iteration 368/1000 | Loss: 0.00007248
Iteration 369/1000 | Loss: 0.00007248
Iteration 370/1000 | Loss: 0.00007247
Iteration 371/1000 | Loss: 0.00007247
Iteration 372/1000 | Loss: 0.00007247
Iteration 373/1000 | Loss: 0.00007247
Iteration 374/1000 | Loss: 0.00007247
Iteration 375/1000 | Loss: 0.00007247
Iteration 376/1000 | Loss: 0.00007247
Iteration 377/1000 | Loss: 0.00007247
Iteration 378/1000 | Loss: 0.00007247
Iteration 379/1000 | Loss: 0.00007247
Iteration 380/1000 | Loss: 0.00007247
Iteration 381/1000 | Loss: 0.00007246
Iteration 382/1000 | Loss: 0.00007246
Iteration 383/1000 | Loss: 0.00007246
Iteration 384/1000 | Loss: 0.00007245
Iteration 385/1000 | Loss: 0.00007245
Iteration 386/1000 | Loss: 0.00007245
Iteration 387/1000 | Loss: 0.00007244
Iteration 388/1000 | Loss: 0.00007238
Iteration 389/1000 | Loss: 0.00007238
Iteration 390/1000 | Loss: 0.00007235
Iteration 391/1000 | Loss: 0.00007234
Iteration 392/1000 | Loss: 0.00007234
Iteration 393/1000 | Loss: 0.00007234
Iteration 394/1000 | Loss: 0.00007234
Iteration 395/1000 | Loss: 0.00007234
Iteration 396/1000 | Loss: 0.00007234
Iteration 397/1000 | Loss: 0.00007234
Iteration 398/1000 | Loss: 0.00007234
Iteration 399/1000 | Loss: 0.00007234
Iteration 400/1000 | Loss: 0.00007234
Iteration 401/1000 | Loss: 0.00007234
Iteration 402/1000 | Loss: 0.00007234
Iteration 403/1000 | Loss: 0.00007234
Iteration 404/1000 | Loss: 0.00007234
Iteration 405/1000 | Loss: 0.00007233
Iteration 406/1000 | Loss: 0.00007233
Iteration 407/1000 | Loss: 0.00007233
Iteration 408/1000 | Loss: 0.00007233
Iteration 409/1000 | Loss: 0.00007233
Iteration 410/1000 | Loss: 0.00007233
Iteration 411/1000 | Loss: 0.00007232
Iteration 412/1000 | Loss: 0.00007232
Iteration 413/1000 | Loss: 0.00007232
Iteration 414/1000 | Loss: 0.00007232
Iteration 415/1000 | Loss: 0.00007232
Iteration 416/1000 | Loss: 0.00007232
Iteration 417/1000 | Loss: 0.00007232
Iteration 418/1000 | Loss: 0.00007232
Iteration 419/1000 | Loss: 0.00007232
Iteration 420/1000 | Loss: 0.00007232
Iteration 421/1000 | Loss: 0.00007232
Iteration 422/1000 | Loss: 0.00007232
Iteration 423/1000 | Loss: 0.00007232
Iteration 424/1000 | Loss: 0.00007232
Iteration 425/1000 | Loss: 0.00007232
Iteration 426/1000 | Loss: 0.00007232
Iteration 427/1000 | Loss: 0.00007232
Iteration 428/1000 | Loss: 0.00007232
Iteration 429/1000 | Loss: 0.00007232
Iteration 430/1000 | Loss: 0.00007231
Iteration 431/1000 | Loss: 0.00007231
Iteration 432/1000 | Loss: 0.00007231
Iteration 433/1000 | Loss: 0.00007231
Iteration 434/1000 | Loss: 0.00007231
Iteration 435/1000 | Loss: 0.00007231
Iteration 436/1000 | Loss: 0.00007231
Iteration 437/1000 | Loss: 0.00007231
Iteration 438/1000 | Loss: 0.00007231
Iteration 439/1000 | Loss: 0.00007231
Iteration 440/1000 | Loss: 0.00007231
Iteration 441/1000 | Loss: 0.00007231
Iteration 442/1000 | Loss: 0.00007231
Iteration 443/1000 | Loss: 0.00007231
Iteration 444/1000 | Loss: 0.00007231
Iteration 445/1000 | Loss: 0.00007231
Iteration 446/1000 | Loss: 0.00007231
Iteration 447/1000 | Loss: 0.00007231
Iteration 448/1000 | Loss: 0.00007231
Iteration 449/1000 | Loss: 0.00007231
Iteration 450/1000 | Loss: 0.00007231
Iteration 451/1000 | Loss: 0.00007231
Iteration 452/1000 | Loss: 0.00007231
Iteration 453/1000 | Loss: 0.00007231
Iteration 454/1000 | Loss: 0.00007231
Iteration 455/1000 | Loss: 0.00007231
Iteration 456/1000 | Loss: 0.00007231
Iteration 457/1000 | Loss: 0.00007231
Iteration 458/1000 | Loss: 0.00007231
Iteration 459/1000 | Loss: 0.00007231
Iteration 460/1000 | Loss: 0.00007231
Iteration 461/1000 | Loss: 0.00007231
Iteration 462/1000 | Loss: 0.00007231
Iteration 463/1000 | Loss: 0.00007231
Iteration 464/1000 | Loss: 0.00007231
Iteration 465/1000 | Loss: 0.00007231
Iteration 466/1000 | Loss: 0.00007231
Iteration 467/1000 | Loss: 0.00007231
Iteration 468/1000 | Loss: 0.00007231
Iteration 469/1000 | Loss: 0.00007231
Iteration 470/1000 | Loss: 0.00007231
Iteration 471/1000 | Loss: 0.00007231
Iteration 472/1000 | Loss: 0.00007231
Iteration 473/1000 | Loss: 0.00007231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 473. Stopping optimization.
Last 5 losses: [7.23118573660031e-05, 7.23118573660031e-05, 7.23118573660031e-05, 7.23118573660031e-05, 7.23118573660031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.23118573660031e-05

Optimization complete. Final v2v error: 6.144603252410889 mm

Highest mean error: 11.866698265075684 mm for frame 25

Lowest mean error: 4.335385322570801 mm for frame 10

Saving results

Total time: 584.3806958198547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652688
Iteration 2/25 | Loss: 0.00144970
Iteration 3/25 | Loss: 0.00130543
Iteration 4/25 | Loss: 0.00129528
Iteration 5/25 | Loss: 0.00129315
Iteration 6/25 | Loss: 0.00129275
Iteration 7/25 | Loss: 0.00129275
Iteration 8/25 | Loss: 0.00129275
Iteration 9/25 | Loss: 0.00129275
Iteration 10/25 | Loss: 0.00129275
Iteration 11/25 | Loss: 0.00129275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012927515199407935, 0.0012927515199407935, 0.0012927515199407935, 0.0012927515199407935, 0.0012927515199407935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012927515199407935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.90264511
Iteration 2/25 | Loss: 0.00112651
Iteration 3/25 | Loss: 0.00112637
Iteration 4/25 | Loss: 0.00112637
Iteration 5/25 | Loss: 0.00112637
Iteration 6/25 | Loss: 0.00112637
Iteration 7/25 | Loss: 0.00112637
Iteration 8/25 | Loss: 0.00112637
Iteration 9/25 | Loss: 0.00112637
Iteration 10/25 | Loss: 0.00112637
Iteration 11/25 | Loss: 0.00112637
Iteration 12/25 | Loss: 0.00112637
Iteration 13/25 | Loss: 0.00112637
Iteration 14/25 | Loss: 0.00112637
Iteration 15/25 | Loss: 0.00112637
Iteration 16/25 | Loss: 0.00112637
Iteration 17/25 | Loss: 0.00112637
Iteration 18/25 | Loss: 0.00112637
Iteration 19/25 | Loss: 0.00112637
Iteration 20/25 | Loss: 0.00112637
Iteration 21/25 | Loss: 0.00112637
Iteration 22/25 | Loss: 0.00112637
Iteration 23/25 | Loss: 0.00112637
Iteration 24/25 | Loss: 0.00112637
Iteration 25/25 | Loss: 0.00112637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112637
Iteration 2/1000 | Loss: 0.00004506
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002163
Iteration 6/1000 | Loss: 0.00002090
Iteration 7/1000 | Loss: 0.00002034
Iteration 8/1000 | Loss: 0.00001982
Iteration 9/1000 | Loss: 0.00001943
Iteration 10/1000 | Loss: 0.00001911
Iteration 11/1000 | Loss: 0.00001884
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001807
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001794
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001788
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001782
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001780
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001772
Iteration 69/1000 | Loss: 0.00001772
Iteration 70/1000 | Loss: 0.00001772
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001771
Iteration 77/1000 | Loss: 0.00001771
Iteration 78/1000 | Loss: 0.00001771
Iteration 79/1000 | Loss: 0.00001771
Iteration 80/1000 | Loss: 0.00001770
Iteration 81/1000 | Loss: 0.00001770
Iteration 82/1000 | Loss: 0.00001770
Iteration 83/1000 | Loss: 0.00001770
Iteration 84/1000 | Loss: 0.00001770
Iteration 85/1000 | Loss: 0.00001770
Iteration 86/1000 | Loss: 0.00001770
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001769
Iteration 89/1000 | Loss: 0.00001769
Iteration 90/1000 | Loss: 0.00001769
Iteration 91/1000 | Loss: 0.00001768
Iteration 92/1000 | Loss: 0.00001768
Iteration 93/1000 | Loss: 0.00001768
Iteration 94/1000 | Loss: 0.00001768
Iteration 95/1000 | Loss: 0.00001768
Iteration 96/1000 | Loss: 0.00001768
Iteration 97/1000 | Loss: 0.00001768
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.7672660760581493e-05, 1.7672660760581493e-05, 1.7672660760581493e-05, 1.7672660760581493e-05, 1.7672660760581493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7672660760581493e-05

Optimization complete. Final v2v error: 3.503617763519287 mm

Highest mean error: 4.236001491546631 mm for frame 124

Lowest mean error: 2.7475361824035645 mm for frame 20

Saving results

Total time: 37.921396255493164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980570
Iteration 2/25 | Loss: 0.00213072
Iteration 3/25 | Loss: 0.00165106
Iteration 4/25 | Loss: 0.00157462
Iteration 5/25 | Loss: 0.00155199
Iteration 6/25 | Loss: 0.00139511
Iteration 7/25 | Loss: 0.00134306
Iteration 8/25 | Loss: 0.00132291
Iteration 9/25 | Loss: 0.00130076
Iteration 10/25 | Loss: 0.00128990
Iteration 11/25 | Loss: 0.00133310
Iteration 12/25 | Loss: 0.00128640
Iteration 13/25 | Loss: 0.00126212
Iteration 14/25 | Loss: 0.00125748
Iteration 15/25 | Loss: 0.00124599
Iteration 16/25 | Loss: 0.00123810
Iteration 17/25 | Loss: 0.00123568
Iteration 18/25 | Loss: 0.00123511
Iteration 19/25 | Loss: 0.00123468
Iteration 20/25 | Loss: 0.00123665
Iteration 21/25 | Loss: 0.00123512
Iteration 22/25 | Loss: 0.00123275
Iteration 23/25 | Loss: 0.00123094
Iteration 24/25 | Loss: 0.00123025
Iteration 25/25 | Loss: 0.00123002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30037844
Iteration 2/25 | Loss: 0.00143483
Iteration 3/25 | Loss: 0.00143483
Iteration 4/25 | Loss: 0.00143483
Iteration 5/25 | Loss: 0.00143483
Iteration 6/25 | Loss: 0.00143483
Iteration 7/25 | Loss: 0.00143483
Iteration 8/25 | Loss: 0.00143483
Iteration 9/25 | Loss: 0.00143483
Iteration 10/25 | Loss: 0.00143483
Iteration 11/25 | Loss: 0.00143482
Iteration 12/25 | Loss: 0.00143482
Iteration 13/25 | Loss: 0.00143482
Iteration 14/25 | Loss: 0.00143482
Iteration 15/25 | Loss: 0.00143482
Iteration 16/25 | Loss: 0.00143482
Iteration 17/25 | Loss: 0.00143482
Iteration 18/25 | Loss: 0.00143482
Iteration 19/25 | Loss: 0.00143482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014348244294524193, 0.0014348244294524193, 0.0014348244294524193, 0.0014348244294524193, 0.0014348244294524193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014348244294524193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143482
Iteration 2/1000 | Loss: 0.00006912
Iteration 3/1000 | Loss: 0.00004464
Iteration 4/1000 | Loss: 0.00003410
Iteration 5/1000 | Loss: 0.00002870
Iteration 6/1000 | Loss: 0.00023681
Iteration 7/1000 | Loss: 0.00014698
Iteration 8/1000 | Loss: 0.00021641
Iteration 9/1000 | Loss: 0.00011207
Iteration 10/1000 | Loss: 0.00003365
Iteration 11/1000 | Loss: 0.00002680
Iteration 12/1000 | Loss: 0.00002448
Iteration 13/1000 | Loss: 0.00002347
Iteration 14/1000 | Loss: 0.00002289
Iteration 15/1000 | Loss: 0.00002186
Iteration 16/1000 | Loss: 0.00002139
Iteration 17/1000 | Loss: 0.00015296
Iteration 18/1000 | Loss: 0.00025821
Iteration 19/1000 | Loss: 0.00003086
Iteration 20/1000 | Loss: 0.00002252
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00004316
Iteration 23/1000 | Loss: 0.00001630
Iteration 24/1000 | Loss: 0.00003566
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00005324
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00004122
Iteration 32/1000 | Loss: 0.00001338
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001327
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00003350
Iteration 39/1000 | Loss: 0.00001356
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001313
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001312
Iteration 53/1000 | Loss: 0.00001312
Iteration 54/1000 | Loss: 0.00001311
Iteration 55/1000 | Loss: 0.00001311
Iteration 56/1000 | Loss: 0.00001311
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001311
Iteration 60/1000 | Loss: 0.00001311
Iteration 61/1000 | Loss: 0.00001311
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001310
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001310
Iteration 68/1000 | Loss: 0.00001310
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001310
Iteration 80/1000 | Loss: 0.00001310
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001309
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001309
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001303
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001302
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001301
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001300
Iteration 124/1000 | Loss: 0.00001300
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001300
Iteration 128/1000 | Loss: 0.00001300
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001300
Iteration 142/1000 | Loss: 0.00001300
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001299
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001299
Iteration 171/1000 | Loss: 0.00001299
Iteration 172/1000 | Loss: 0.00001299
Iteration 173/1000 | Loss: 0.00001299
Iteration 174/1000 | Loss: 0.00001299
Iteration 175/1000 | Loss: 0.00001299
Iteration 176/1000 | Loss: 0.00001299
Iteration 177/1000 | Loss: 0.00001299
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001299
Iteration 180/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.2993658856430557e-05, 1.2993658856430557e-05, 1.2993658856430557e-05, 1.2993658856430557e-05, 1.2993658856430557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2993658856430557e-05

Optimization complete. Final v2v error: 3.1150712966918945 mm

Highest mean error: 4.065288066864014 mm for frame 81

Lowest mean error: 2.6939284801483154 mm for frame 125

Saving results

Total time: 99.10485315322876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990386
Iteration 2/25 | Loss: 0.00990386
Iteration 3/25 | Loss: 0.00249956
Iteration 4/25 | Loss: 0.00201892
Iteration 5/25 | Loss: 0.00194872
Iteration 6/25 | Loss: 0.00190502
Iteration 7/25 | Loss: 0.00182488
Iteration 8/25 | Loss: 0.00182502
Iteration 9/25 | Loss: 0.00176321
Iteration 10/25 | Loss: 0.00174022
Iteration 11/25 | Loss: 0.00171850
Iteration 12/25 | Loss: 0.00171166
Iteration 13/25 | Loss: 0.00169275
Iteration 14/25 | Loss: 0.00168108
Iteration 15/25 | Loss: 0.00167986
Iteration 16/25 | Loss: 0.00167037
Iteration 17/25 | Loss: 0.00166031
Iteration 18/25 | Loss: 0.00166060
Iteration 19/25 | Loss: 0.00165610
Iteration 20/25 | Loss: 0.00165734
Iteration 21/25 | Loss: 0.00165538
Iteration 22/25 | Loss: 0.00165107
Iteration 23/25 | Loss: 0.00165073
Iteration 24/25 | Loss: 0.00165072
Iteration 25/25 | Loss: 0.00165071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24771214
Iteration 2/25 | Loss: 0.00494723
Iteration 3/25 | Loss: 0.00444381
Iteration 4/25 | Loss: 0.00444381
Iteration 5/25 | Loss: 0.00444381
Iteration 6/25 | Loss: 0.00444381
Iteration 7/25 | Loss: 0.00444381
Iteration 8/25 | Loss: 0.00444381
Iteration 9/25 | Loss: 0.00444381
Iteration 10/25 | Loss: 0.00444381
Iteration 11/25 | Loss: 0.00444381
Iteration 12/25 | Loss: 0.00444381
Iteration 13/25 | Loss: 0.00444381
Iteration 14/25 | Loss: 0.00444381
Iteration 15/25 | Loss: 0.00444381
Iteration 16/25 | Loss: 0.00444381
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004443810321390629, 0.004443810321390629, 0.004443810321390629, 0.004443810321390629, 0.004443810321390629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004443810321390629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00444381
Iteration 2/1000 | Loss: 0.00087896
Iteration 3/1000 | Loss: 0.00041698
Iteration 4/1000 | Loss: 0.00042104
Iteration 5/1000 | Loss: 0.00031151
Iteration 6/1000 | Loss: 0.00045017
Iteration 7/1000 | Loss: 0.00028713
Iteration 8/1000 | Loss: 0.00042046
Iteration 9/1000 | Loss: 0.00036140
Iteration 10/1000 | Loss: 0.00075808
Iteration 11/1000 | Loss: 0.00026479
Iteration 12/1000 | Loss: 0.00024329
Iteration 13/1000 | Loss: 0.00023531
Iteration 14/1000 | Loss: 0.00022823
Iteration 15/1000 | Loss: 0.00022260
Iteration 16/1000 | Loss: 0.00037565
Iteration 17/1000 | Loss: 0.00034686
Iteration 18/1000 | Loss: 0.00198303
Iteration 19/1000 | Loss: 0.00424000
Iteration 20/1000 | Loss: 0.00548966
Iteration 21/1000 | Loss: 0.00214199
Iteration 22/1000 | Loss: 0.00379550
Iteration 23/1000 | Loss: 0.00070303
Iteration 24/1000 | Loss: 0.00081164
Iteration 25/1000 | Loss: 0.00045457
Iteration 26/1000 | Loss: 0.00049655
Iteration 27/1000 | Loss: 0.00029216
Iteration 28/1000 | Loss: 0.00017656
Iteration 29/1000 | Loss: 0.00026705
Iteration 30/1000 | Loss: 0.00013575
Iteration 31/1000 | Loss: 0.00011946
Iteration 32/1000 | Loss: 0.00032689
Iteration 33/1000 | Loss: 0.00031382
Iteration 34/1000 | Loss: 0.00012739
Iteration 35/1000 | Loss: 0.00010034
Iteration 36/1000 | Loss: 0.00033075
Iteration 37/1000 | Loss: 0.00015563
Iteration 38/1000 | Loss: 0.00018599
Iteration 39/1000 | Loss: 0.00028405
Iteration 40/1000 | Loss: 0.00027215
Iteration 41/1000 | Loss: 0.00032918
Iteration 42/1000 | Loss: 0.00013946
Iteration 43/1000 | Loss: 0.00010044
Iteration 44/1000 | Loss: 0.00009773
Iteration 45/1000 | Loss: 0.00008250
Iteration 46/1000 | Loss: 0.00007515
Iteration 47/1000 | Loss: 0.00019656
Iteration 48/1000 | Loss: 0.00016244
Iteration 49/1000 | Loss: 0.00007229
Iteration 50/1000 | Loss: 0.00007031
Iteration 51/1000 | Loss: 0.00006903
Iteration 52/1000 | Loss: 0.00006759
Iteration 53/1000 | Loss: 0.00007929
Iteration 54/1000 | Loss: 0.00007104
Iteration 55/1000 | Loss: 0.00006756
Iteration 56/1000 | Loss: 0.00006656
Iteration 57/1000 | Loss: 0.00006555
Iteration 58/1000 | Loss: 0.00006477
Iteration 59/1000 | Loss: 0.00006434
Iteration 60/1000 | Loss: 0.00006403
Iteration 61/1000 | Loss: 0.00006368
Iteration 62/1000 | Loss: 0.00006333
Iteration 63/1000 | Loss: 0.00006307
Iteration 64/1000 | Loss: 0.00006303
Iteration 65/1000 | Loss: 0.00007469
Iteration 66/1000 | Loss: 0.00013785
Iteration 67/1000 | Loss: 0.00011886
Iteration 68/1000 | Loss: 0.00007414
Iteration 69/1000 | Loss: 0.00006750
Iteration 70/1000 | Loss: 0.00007505
Iteration 71/1000 | Loss: 0.00014716
Iteration 72/1000 | Loss: 0.00008787
Iteration 73/1000 | Loss: 0.00035269
Iteration 74/1000 | Loss: 0.00009451
Iteration 75/1000 | Loss: 0.00014677
Iteration 76/1000 | Loss: 0.00008950
Iteration 77/1000 | Loss: 0.00009200
Iteration 78/1000 | Loss: 0.00008684
Iteration 79/1000 | Loss: 0.00008519
Iteration 80/1000 | Loss: 0.00008046
Iteration 81/1000 | Loss: 0.00008200
Iteration 82/1000 | Loss: 0.00007766
Iteration 83/1000 | Loss: 0.00007970
Iteration 84/1000 | Loss: 0.00007726
Iteration 85/1000 | Loss: 0.00007280
Iteration 86/1000 | Loss: 0.00007291
Iteration 87/1000 | Loss: 0.00008707
Iteration 88/1000 | Loss: 0.00006737
Iteration 89/1000 | Loss: 0.00006271
Iteration 90/1000 | Loss: 0.00006231
Iteration 91/1000 | Loss: 0.00006194
Iteration 92/1000 | Loss: 0.00006172
Iteration 93/1000 | Loss: 0.00006171
Iteration 94/1000 | Loss: 0.00006150
Iteration 95/1000 | Loss: 0.00006136
Iteration 96/1000 | Loss: 0.00006114
Iteration 97/1000 | Loss: 0.00006111
Iteration 98/1000 | Loss: 0.00006110
Iteration 99/1000 | Loss: 0.00006108
Iteration 100/1000 | Loss: 0.00006108
Iteration 101/1000 | Loss: 0.00006107
Iteration 102/1000 | Loss: 0.00006107
Iteration 103/1000 | Loss: 0.00006100
Iteration 104/1000 | Loss: 0.00006100
Iteration 105/1000 | Loss: 0.00006100
Iteration 106/1000 | Loss: 0.00006100
Iteration 107/1000 | Loss: 0.00006100
Iteration 108/1000 | Loss: 0.00006100
Iteration 109/1000 | Loss: 0.00006100
Iteration 110/1000 | Loss: 0.00006100
Iteration 111/1000 | Loss: 0.00006099
Iteration 112/1000 | Loss: 0.00006099
Iteration 113/1000 | Loss: 0.00006099
Iteration 114/1000 | Loss: 0.00006098
Iteration 115/1000 | Loss: 0.00006098
Iteration 116/1000 | Loss: 0.00006098
Iteration 117/1000 | Loss: 0.00006097
Iteration 118/1000 | Loss: 0.00006097
Iteration 119/1000 | Loss: 0.00006097
Iteration 120/1000 | Loss: 0.00006096
Iteration 121/1000 | Loss: 0.00006096
Iteration 122/1000 | Loss: 0.00006096
Iteration 123/1000 | Loss: 0.00006096
Iteration 124/1000 | Loss: 0.00006096
Iteration 125/1000 | Loss: 0.00006095
Iteration 126/1000 | Loss: 0.00006095
Iteration 127/1000 | Loss: 0.00006094
Iteration 128/1000 | Loss: 0.00006094
Iteration 129/1000 | Loss: 0.00006093
Iteration 130/1000 | Loss: 0.00006093
Iteration 131/1000 | Loss: 0.00006093
Iteration 132/1000 | Loss: 0.00006092
Iteration 133/1000 | Loss: 0.00006092
Iteration 134/1000 | Loss: 0.00006092
Iteration 135/1000 | Loss: 0.00006092
Iteration 136/1000 | Loss: 0.00006092
Iteration 137/1000 | Loss: 0.00006091
Iteration 138/1000 | Loss: 0.00006091
Iteration 139/1000 | Loss: 0.00006091
Iteration 140/1000 | Loss: 0.00006090
Iteration 141/1000 | Loss: 0.00006090
Iteration 142/1000 | Loss: 0.00006090
Iteration 143/1000 | Loss: 0.00012806
Iteration 144/1000 | Loss: 0.00008553
Iteration 145/1000 | Loss: 0.00006095
Iteration 146/1000 | Loss: 0.00006088
Iteration 147/1000 | Loss: 0.00006087
Iteration 148/1000 | Loss: 0.00006087
Iteration 149/1000 | Loss: 0.00006087
Iteration 150/1000 | Loss: 0.00006087
Iteration 151/1000 | Loss: 0.00012372
Iteration 152/1000 | Loss: 0.00008618
Iteration 153/1000 | Loss: 0.00006094
Iteration 154/1000 | Loss: 0.00006089
Iteration 155/1000 | Loss: 0.00006087
Iteration 156/1000 | Loss: 0.00006087
Iteration 157/1000 | Loss: 0.00006087
Iteration 158/1000 | Loss: 0.00006086
Iteration 159/1000 | Loss: 0.00006086
Iteration 160/1000 | Loss: 0.00006086
Iteration 161/1000 | Loss: 0.00006086
Iteration 162/1000 | Loss: 0.00006086
Iteration 163/1000 | Loss: 0.00006086
Iteration 164/1000 | Loss: 0.00006085
Iteration 165/1000 | Loss: 0.00006085
Iteration 166/1000 | Loss: 0.00006085
Iteration 167/1000 | Loss: 0.00006085
Iteration 168/1000 | Loss: 0.00006085
Iteration 169/1000 | Loss: 0.00006085
Iteration 170/1000 | Loss: 0.00006085
Iteration 171/1000 | Loss: 0.00006085
Iteration 172/1000 | Loss: 0.00006085
Iteration 173/1000 | Loss: 0.00006085
Iteration 174/1000 | Loss: 0.00006085
Iteration 175/1000 | Loss: 0.00006085
Iteration 176/1000 | Loss: 0.00006085
Iteration 177/1000 | Loss: 0.00006085
Iteration 178/1000 | Loss: 0.00006085
Iteration 179/1000 | Loss: 0.00006085
Iteration 180/1000 | Loss: 0.00006085
Iteration 181/1000 | Loss: 0.00006085
Iteration 182/1000 | Loss: 0.00006085
Iteration 183/1000 | Loss: 0.00006085
Iteration 184/1000 | Loss: 0.00006085
Iteration 185/1000 | Loss: 0.00006085
Iteration 186/1000 | Loss: 0.00006085
Iteration 187/1000 | Loss: 0.00006085
Iteration 188/1000 | Loss: 0.00006085
Iteration 189/1000 | Loss: 0.00006085
Iteration 190/1000 | Loss: 0.00006085
Iteration 191/1000 | Loss: 0.00006085
Iteration 192/1000 | Loss: 0.00006085
Iteration 193/1000 | Loss: 0.00006085
Iteration 194/1000 | Loss: 0.00006085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [6.084521010052413e-05, 6.084521010052413e-05, 6.084521010052413e-05, 6.084521010052413e-05, 6.084521010052413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.084521010052413e-05

Optimization complete. Final v2v error: 4.0135817527771 mm

Highest mean error: 11.746490478515625 mm for frame 162

Lowest mean error: 2.8975160121917725 mm for frame 183

Saving results

Total time: 207.45726227760315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752766
Iteration 2/25 | Loss: 0.00174382
Iteration 3/25 | Loss: 0.00138981
Iteration 4/25 | Loss: 0.00134639
Iteration 5/25 | Loss: 0.00133934
Iteration 6/25 | Loss: 0.00133742
Iteration 7/25 | Loss: 0.00133668
Iteration 8/25 | Loss: 0.00133649
Iteration 9/25 | Loss: 0.00133649
Iteration 10/25 | Loss: 0.00133649
Iteration 11/25 | Loss: 0.00133649
Iteration 12/25 | Loss: 0.00133649
Iteration 13/25 | Loss: 0.00133649
Iteration 14/25 | Loss: 0.00133649
Iteration 15/25 | Loss: 0.00133649
Iteration 16/25 | Loss: 0.00133649
Iteration 17/25 | Loss: 0.00133649
Iteration 18/25 | Loss: 0.00133649
Iteration 19/25 | Loss: 0.00133649
Iteration 20/25 | Loss: 0.00133649
Iteration 21/25 | Loss: 0.00133649
Iteration 22/25 | Loss: 0.00133649
Iteration 23/25 | Loss: 0.00133649
Iteration 24/25 | Loss: 0.00133649
Iteration 25/25 | Loss: 0.00133649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53323764
Iteration 2/25 | Loss: 0.00132141
Iteration 3/25 | Loss: 0.00132140
Iteration 4/25 | Loss: 0.00132140
Iteration 5/25 | Loss: 0.00132140
Iteration 6/25 | Loss: 0.00132140
Iteration 7/25 | Loss: 0.00132140
Iteration 8/25 | Loss: 0.00132140
Iteration 9/25 | Loss: 0.00132140
Iteration 10/25 | Loss: 0.00132140
Iteration 11/25 | Loss: 0.00132139
Iteration 12/25 | Loss: 0.00132139
Iteration 13/25 | Loss: 0.00132139
Iteration 14/25 | Loss: 0.00132139
Iteration 15/25 | Loss: 0.00132139
Iteration 16/25 | Loss: 0.00132139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013213948113843799, 0.0013213948113843799, 0.0013213948113843799, 0.0013213948113843799, 0.0013213948113843799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013213948113843799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132139
Iteration 2/1000 | Loss: 0.00005725
Iteration 3/1000 | Loss: 0.00003671
Iteration 4/1000 | Loss: 0.00002846
Iteration 5/1000 | Loss: 0.00002578
Iteration 6/1000 | Loss: 0.00002431
Iteration 7/1000 | Loss: 0.00002347
Iteration 8/1000 | Loss: 0.00002294
Iteration 9/1000 | Loss: 0.00002249
Iteration 10/1000 | Loss: 0.00002221
Iteration 11/1000 | Loss: 0.00002199
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002177
Iteration 14/1000 | Loss: 0.00002170
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002154
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002151
Iteration 19/1000 | Loss: 0.00002151
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002151
Iteration 22/1000 | Loss: 0.00002151
Iteration 23/1000 | Loss: 0.00002151
Iteration 24/1000 | Loss: 0.00002151
Iteration 25/1000 | Loss: 0.00002151
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002150
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002150
Iteration 30/1000 | Loss: 0.00002149
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002149
Iteration 34/1000 | Loss: 0.00002148
Iteration 35/1000 | Loss: 0.00002148
Iteration 36/1000 | Loss: 0.00002147
Iteration 37/1000 | Loss: 0.00002147
Iteration 38/1000 | Loss: 0.00002145
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002145
Iteration 41/1000 | Loss: 0.00002145
Iteration 42/1000 | Loss: 0.00002144
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002144
Iteration 45/1000 | Loss: 0.00002143
Iteration 46/1000 | Loss: 0.00002142
Iteration 47/1000 | Loss: 0.00002142
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00002141
Iteration 50/1000 | Loss: 0.00002141
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002139
Iteration 53/1000 | Loss: 0.00002139
Iteration 54/1000 | Loss: 0.00002139
Iteration 55/1000 | Loss: 0.00002139
Iteration 56/1000 | Loss: 0.00002139
Iteration 57/1000 | Loss: 0.00002139
Iteration 58/1000 | Loss: 0.00002139
Iteration 59/1000 | Loss: 0.00002138
Iteration 60/1000 | Loss: 0.00002137
Iteration 61/1000 | Loss: 0.00002136
Iteration 62/1000 | Loss: 0.00002136
Iteration 63/1000 | Loss: 0.00002136
Iteration 64/1000 | Loss: 0.00002136
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002135
Iteration 67/1000 | Loss: 0.00002135
Iteration 68/1000 | Loss: 0.00002135
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002134
Iteration 71/1000 | Loss: 0.00002134
Iteration 72/1000 | Loss: 0.00002134
Iteration 73/1000 | Loss: 0.00002133
Iteration 74/1000 | Loss: 0.00002133
Iteration 75/1000 | Loss: 0.00002133
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00002132
Iteration 79/1000 | Loss: 0.00002132
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002130
Iteration 88/1000 | Loss: 0.00002130
Iteration 89/1000 | Loss: 0.00002130
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002129
Iteration 92/1000 | Loss: 0.00002129
Iteration 93/1000 | Loss: 0.00002129
Iteration 94/1000 | Loss: 0.00002129
Iteration 95/1000 | Loss: 0.00002129
Iteration 96/1000 | Loss: 0.00002128
Iteration 97/1000 | Loss: 0.00002128
Iteration 98/1000 | Loss: 0.00002128
Iteration 99/1000 | Loss: 0.00002128
Iteration 100/1000 | Loss: 0.00002128
Iteration 101/1000 | Loss: 0.00002128
Iteration 102/1000 | Loss: 0.00002127
Iteration 103/1000 | Loss: 0.00002127
Iteration 104/1000 | Loss: 0.00002127
Iteration 105/1000 | Loss: 0.00002127
Iteration 106/1000 | Loss: 0.00002127
Iteration 107/1000 | Loss: 0.00002127
Iteration 108/1000 | Loss: 0.00002127
Iteration 109/1000 | Loss: 0.00002126
Iteration 110/1000 | Loss: 0.00002126
Iteration 111/1000 | Loss: 0.00002126
Iteration 112/1000 | Loss: 0.00002126
Iteration 113/1000 | Loss: 0.00002126
Iteration 114/1000 | Loss: 0.00002125
Iteration 115/1000 | Loss: 0.00002125
Iteration 116/1000 | Loss: 0.00002125
Iteration 117/1000 | Loss: 0.00002125
Iteration 118/1000 | Loss: 0.00002125
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002123
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002122
Iteration 135/1000 | Loss: 0.00002122
Iteration 136/1000 | Loss: 0.00002122
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002122
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002121
Iteration 142/1000 | Loss: 0.00002121
Iteration 143/1000 | Loss: 0.00002121
Iteration 144/1000 | Loss: 0.00002121
Iteration 145/1000 | Loss: 0.00002121
Iteration 146/1000 | Loss: 0.00002121
Iteration 147/1000 | Loss: 0.00002121
Iteration 148/1000 | Loss: 0.00002121
Iteration 149/1000 | Loss: 0.00002121
Iteration 150/1000 | Loss: 0.00002121
Iteration 151/1000 | Loss: 0.00002121
Iteration 152/1000 | Loss: 0.00002121
Iteration 153/1000 | Loss: 0.00002120
Iteration 154/1000 | Loss: 0.00002120
Iteration 155/1000 | Loss: 0.00002120
Iteration 156/1000 | Loss: 0.00002120
Iteration 157/1000 | Loss: 0.00002120
Iteration 158/1000 | Loss: 0.00002120
Iteration 159/1000 | Loss: 0.00002120
Iteration 160/1000 | Loss: 0.00002120
Iteration 161/1000 | Loss: 0.00002120
Iteration 162/1000 | Loss: 0.00002119
Iteration 163/1000 | Loss: 0.00002119
Iteration 164/1000 | Loss: 0.00002119
Iteration 165/1000 | Loss: 0.00002119
Iteration 166/1000 | Loss: 0.00002119
Iteration 167/1000 | Loss: 0.00002119
Iteration 168/1000 | Loss: 0.00002119
Iteration 169/1000 | Loss: 0.00002119
Iteration 170/1000 | Loss: 0.00002118
Iteration 171/1000 | Loss: 0.00002118
Iteration 172/1000 | Loss: 0.00002118
Iteration 173/1000 | Loss: 0.00002118
Iteration 174/1000 | Loss: 0.00002118
Iteration 175/1000 | Loss: 0.00002118
Iteration 176/1000 | Loss: 0.00002118
Iteration 177/1000 | Loss: 0.00002118
Iteration 178/1000 | Loss: 0.00002117
Iteration 179/1000 | Loss: 0.00002117
Iteration 180/1000 | Loss: 0.00002117
Iteration 181/1000 | Loss: 0.00002117
Iteration 182/1000 | Loss: 0.00002117
Iteration 183/1000 | Loss: 0.00002117
Iteration 184/1000 | Loss: 0.00002117
Iteration 185/1000 | Loss: 0.00002117
Iteration 186/1000 | Loss: 0.00002117
Iteration 187/1000 | Loss: 0.00002117
Iteration 188/1000 | Loss: 0.00002117
Iteration 189/1000 | Loss: 0.00002117
Iteration 190/1000 | Loss: 0.00002117
Iteration 191/1000 | Loss: 0.00002117
Iteration 192/1000 | Loss: 0.00002116
Iteration 193/1000 | Loss: 0.00002116
Iteration 194/1000 | Loss: 0.00002116
Iteration 195/1000 | Loss: 0.00002116
Iteration 196/1000 | Loss: 0.00002116
Iteration 197/1000 | Loss: 0.00002116
Iteration 198/1000 | Loss: 0.00002116
Iteration 199/1000 | Loss: 0.00002116
Iteration 200/1000 | Loss: 0.00002116
Iteration 201/1000 | Loss: 0.00002116
Iteration 202/1000 | Loss: 0.00002116
Iteration 203/1000 | Loss: 0.00002116
Iteration 204/1000 | Loss: 0.00002116
Iteration 205/1000 | Loss: 0.00002116
Iteration 206/1000 | Loss: 0.00002116
Iteration 207/1000 | Loss: 0.00002116
Iteration 208/1000 | Loss: 0.00002116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.1160440155654214e-05, 2.1160440155654214e-05, 2.1160440155654214e-05, 2.1160440155654214e-05, 2.1160440155654214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1160440155654214e-05

Optimization complete. Final v2v error: 3.845855474472046 mm

Highest mean error: 5.527400016784668 mm for frame 34

Lowest mean error: 3.161635160446167 mm for frame 0

Saving results

Total time: 43.807119369506836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395613
Iteration 2/25 | Loss: 0.00122186
Iteration 3/25 | Loss: 0.00116664
Iteration 4/25 | Loss: 0.00115776
Iteration 5/25 | Loss: 0.00115491
Iteration 6/25 | Loss: 0.00115431
Iteration 7/25 | Loss: 0.00115431
Iteration 8/25 | Loss: 0.00115431
Iteration 9/25 | Loss: 0.00115431
Iteration 10/25 | Loss: 0.00115431
Iteration 11/25 | Loss: 0.00115431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001154311466962099, 0.001154311466962099, 0.001154311466962099, 0.001154311466962099, 0.001154311466962099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001154311466962099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46100354
Iteration 2/25 | Loss: 0.00129828
Iteration 3/25 | Loss: 0.00129827
Iteration 4/25 | Loss: 0.00129827
Iteration 5/25 | Loss: 0.00129827
Iteration 6/25 | Loss: 0.00129827
Iteration 7/25 | Loss: 0.00129827
Iteration 8/25 | Loss: 0.00129827
Iteration 9/25 | Loss: 0.00129827
Iteration 10/25 | Loss: 0.00129827
Iteration 11/25 | Loss: 0.00129827
Iteration 12/25 | Loss: 0.00129827
Iteration 13/25 | Loss: 0.00129827
Iteration 14/25 | Loss: 0.00129827
Iteration 15/25 | Loss: 0.00129827
Iteration 16/25 | Loss: 0.00129827
Iteration 17/25 | Loss: 0.00129827
Iteration 18/25 | Loss: 0.00129827
Iteration 19/25 | Loss: 0.00129827
Iteration 20/25 | Loss: 0.00129827
Iteration 21/25 | Loss: 0.00129827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012982652988284826, 0.0012982652988284826, 0.0012982652988284826, 0.0012982652988284826, 0.0012982652988284826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012982652988284826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129827
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001551
Iteration 4/1000 | Loss: 0.00001307
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001168
Iteration 7/1000 | Loss: 0.00001123
Iteration 8/1000 | Loss: 0.00001098
Iteration 9/1000 | Loss: 0.00001068
Iteration 10/1000 | Loss: 0.00001043
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001031
Iteration 13/1000 | Loss: 0.00001029
Iteration 14/1000 | Loss: 0.00001028
Iteration 15/1000 | Loss: 0.00001027
Iteration 16/1000 | Loss: 0.00001014
Iteration 17/1000 | Loss: 0.00001009
Iteration 18/1000 | Loss: 0.00001005
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00000995
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000988
Iteration 26/1000 | Loss: 0.00000987
Iteration 27/1000 | Loss: 0.00000986
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000985
Iteration 36/1000 | Loss: 0.00000985
Iteration 37/1000 | Loss: 0.00000985
Iteration 38/1000 | Loss: 0.00000984
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000977
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000974
Iteration 45/1000 | Loss: 0.00000974
Iteration 46/1000 | Loss: 0.00000973
Iteration 47/1000 | Loss: 0.00000971
Iteration 48/1000 | Loss: 0.00000971
Iteration 49/1000 | Loss: 0.00000971
Iteration 50/1000 | Loss: 0.00000970
Iteration 51/1000 | Loss: 0.00000970
Iteration 52/1000 | Loss: 0.00000969
Iteration 53/1000 | Loss: 0.00000969
Iteration 54/1000 | Loss: 0.00000968
Iteration 55/1000 | Loss: 0.00000968
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000967
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000965
Iteration 63/1000 | Loss: 0.00000965
Iteration 64/1000 | Loss: 0.00000964
Iteration 65/1000 | Loss: 0.00000964
Iteration 66/1000 | Loss: 0.00000963
Iteration 67/1000 | Loss: 0.00000963
Iteration 68/1000 | Loss: 0.00000963
Iteration 69/1000 | Loss: 0.00000963
Iteration 70/1000 | Loss: 0.00000963
Iteration 71/1000 | Loss: 0.00000963
Iteration 72/1000 | Loss: 0.00000963
Iteration 73/1000 | Loss: 0.00000963
Iteration 74/1000 | Loss: 0.00000963
Iteration 75/1000 | Loss: 0.00000963
Iteration 76/1000 | Loss: 0.00000962
Iteration 77/1000 | Loss: 0.00000962
Iteration 78/1000 | Loss: 0.00000962
Iteration 79/1000 | Loss: 0.00000962
Iteration 80/1000 | Loss: 0.00000961
Iteration 81/1000 | Loss: 0.00000961
Iteration 82/1000 | Loss: 0.00000961
Iteration 83/1000 | Loss: 0.00000961
Iteration 84/1000 | Loss: 0.00000960
Iteration 85/1000 | Loss: 0.00000960
Iteration 86/1000 | Loss: 0.00000960
Iteration 87/1000 | Loss: 0.00000960
Iteration 88/1000 | Loss: 0.00000960
Iteration 89/1000 | Loss: 0.00000960
Iteration 90/1000 | Loss: 0.00000960
Iteration 91/1000 | Loss: 0.00000959
Iteration 92/1000 | Loss: 0.00000959
Iteration 93/1000 | Loss: 0.00000959
Iteration 94/1000 | Loss: 0.00000959
Iteration 95/1000 | Loss: 0.00000958
Iteration 96/1000 | Loss: 0.00000958
Iteration 97/1000 | Loss: 0.00000958
Iteration 98/1000 | Loss: 0.00000958
Iteration 99/1000 | Loss: 0.00000957
Iteration 100/1000 | Loss: 0.00000957
Iteration 101/1000 | Loss: 0.00000957
Iteration 102/1000 | Loss: 0.00000957
Iteration 103/1000 | Loss: 0.00000957
Iteration 104/1000 | Loss: 0.00000957
Iteration 105/1000 | Loss: 0.00000957
Iteration 106/1000 | Loss: 0.00000956
Iteration 107/1000 | Loss: 0.00000956
Iteration 108/1000 | Loss: 0.00000956
Iteration 109/1000 | Loss: 0.00000956
Iteration 110/1000 | Loss: 0.00000956
Iteration 111/1000 | Loss: 0.00000956
Iteration 112/1000 | Loss: 0.00000956
Iteration 113/1000 | Loss: 0.00000955
Iteration 114/1000 | Loss: 0.00000955
Iteration 115/1000 | Loss: 0.00000955
Iteration 116/1000 | Loss: 0.00000954
Iteration 117/1000 | Loss: 0.00000954
Iteration 118/1000 | Loss: 0.00000954
Iteration 119/1000 | Loss: 0.00000954
Iteration 120/1000 | Loss: 0.00000954
Iteration 121/1000 | Loss: 0.00000954
Iteration 122/1000 | Loss: 0.00000954
Iteration 123/1000 | Loss: 0.00000954
Iteration 124/1000 | Loss: 0.00000954
Iteration 125/1000 | Loss: 0.00000954
Iteration 126/1000 | Loss: 0.00000954
Iteration 127/1000 | Loss: 0.00000954
Iteration 128/1000 | Loss: 0.00000953
Iteration 129/1000 | Loss: 0.00000953
Iteration 130/1000 | Loss: 0.00000953
Iteration 131/1000 | Loss: 0.00000953
Iteration 132/1000 | Loss: 0.00000953
Iteration 133/1000 | Loss: 0.00000953
Iteration 134/1000 | Loss: 0.00000953
Iteration 135/1000 | Loss: 0.00000953
Iteration 136/1000 | Loss: 0.00000953
Iteration 137/1000 | Loss: 0.00000953
Iteration 138/1000 | Loss: 0.00000953
Iteration 139/1000 | Loss: 0.00000953
Iteration 140/1000 | Loss: 0.00000953
Iteration 141/1000 | Loss: 0.00000953
Iteration 142/1000 | Loss: 0.00000953
Iteration 143/1000 | Loss: 0.00000953
Iteration 144/1000 | Loss: 0.00000953
Iteration 145/1000 | Loss: 0.00000953
Iteration 146/1000 | Loss: 0.00000953
Iteration 147/1000 | Loss: 0.00000953
Iteration 148/1000 | Loss: 0.00000953
Iteration 149/1000 | Loss: 0.00000953
Iteration 150/1000 | Loss: 0.00000953
Iteration 151/1000 | Loss: 0.00000953
Iteration 152/1000 | Loss: 0.00000953
Iteration 153/1000 | Loss: 0.00000953
Iteration 154/1000 | Loss: 0.00000953
Iteration 155/1000 | Loss: 0.00000953
Iteration 156/1000 | Loss: 0.00000953
Iteration 157/1000 | Loss: 0.00000953
Iteration 158/1000 | Loss: 0.00000953
Iteration 159/1000 | Loss: 0.00000953
Iteration 160/1000 | Loss: 0.00000953
Iteration 161/1000 | Loss: 0.00000953
Iteration 162/1000 | Loss: 0.00000953
Iteration 163/1000 | Loss: 0.00000953
Iteration 164/1000 | Loss: 0.00000953
Iteration 165/1000 | Loss: 0.00000953
Iteration 166/1000 | Loss: 0.00000953
Iteration 167/1000 | Loss: 0.00000953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [9.530240276944824e-06, 9.530240276944824e-06, 9.530240276944824e-06, 9.530240276944824e-06, 9.530240276944824e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.530240276944824e-06

Optimization complete. Final v2v error: 2.685131311416626 mm

Highest mean error: 2.98079252243042 mm for frame 80

Lowest mean error: 2.5582311153411865 mm for frame 140

Saving results

Total time: 38.278640031814575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887669
Iteration 2/25 | Loss: 0.00221450
Iteration 3/25 | Loss: 0.00191703
Iteration 4/25 | Loss: 0.00194696
Iteration 5/25 | Loss: 0.00173232
Iteration 6/25 | Loss: 0.00151089
Iteration 7/25 | Loss: 0.00138408
Iteration 8/25 | Loss: 0.00133968
Iteration 9/25 | Loss: 0.00131041
Iteration 10/25 | Loss: 0.00128590
Iteration 11/25 | Loss: 0.00128089
Iteration 12/25 | Loss: 0.00128454
Iteration 13/25 | Loss: 0.00127108
Iteration 14/25 | Loss: 0.00126068
Iteration 15/25 | Loss: 0.00125476
Iteration 16/25 | Loss: 0.00125256
Iteration 17/25 | Loss: 0.00125205
Iteration 18/25 | Loss: 0.00125189
Iteration 19/25 | Loss: 0.00125187
Iteration 20/25 | Loss: 0.00125187
Iteration 21/25 | Loss: 0.00125186
Iteration 22/25 | Loss: 0.00125186
Iteration 23/25 | Loss: 0.00125186
Iteration 24/25 | Loss: 0.00125185
Iteration 25/25 | Loss: 0.00125185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26231694
Iteration 2/25 | Loss: 0.00140615
Iteration 3/25 | Loss: 0.00140615
Iteration 4/25 | Loss: 0.00140614
Iteration 5/25 | Loss: 0.00140614
Iteration 6/25 | Loss: 0.00140614
Iteration 7/25 | Loss: 0.00140614
Iteration 8/25 | Loss: 0.00140614
Iteration 9/25 | Loss: 0.00140614
Iteration 10/25 | Loss: 0.00140614
Iteration 11/25 | Loss: 0.00140614
Iteration 12/25 | Loss: 0.00140614
Iteration 13/25 | Loss: 0.00140614
Iteration 14/25 | Loss: 0.00140614
Iteration 15/25 | Loss: 0.00140614
Iteration 16/25 | Loss: 0.00140614
Iteration 17/25 | Loss: 0.00140614
Iteration 18/25 | Loss: 0.00140614
Iteration 19/25 | Loss: 0.00140614
Iteration 20/25 | Loss: 0.00140614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014061415567994118, 0.0014061415567994118, 0.0014061415567994118, 0.0014061415567994118, 0.0014061415567994118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014061415567994118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140614
Iteration 2/1000 | Loss: 0.00033990
Iteration 3/1000 | Loss: 0.00029585
Iteration 4/1000 | Loss: 0.00024961
Iteration 5/1000 | Loss: 0.00021062
Iteration 6/1000 | Loss: 0.00012950
Iteration 7/1000 | Loss: 0.00003735
Iteration 8/1000 | Loss: 0.00003297
Iteration 9/1000 | Loss: 0.00003030
Iteration 10/1000 | Loss: 0.00024800
Iteration 11/1000 | Loss: 0.00010469
Iteration 12/1000 | Loss: 0.00039829
Iteration 13/1000 | Loss: 0.00011768
Iteration 14/1000 | Loss: 0.00018535
Iteration 15/1000 | Loss: 0.00007825
Iteration 16/1000 | Loss: 0.00004513
Iteration 17/1000 | Loss: 0.00002979
Iteration 18/1000 | Loss: 0.00005107
Iteration 19/1000 | Loss: 0.00002444
Iteration 20/1000 | Loss: 0.00002195
Iteration 21/1000 | Loss: 0.00002070
Iteration 22/1000 | Loss: 0.00001960
Iteration 23/1000 | Loss: 0.00001923
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001846
Iteration 27/1000 | Loss: 0.00001820
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001775
Iteration 33/1000 | Loss: 0.00001774
Iteration 34/1000 | Loss: 0.00001774
Iteration 35/1000 | Loss: 0.00001773
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001752
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001744
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001743
Iteration 51/1000 | Loss: 0.00001740
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001730
Iteration 64/1000 | Loss: 0.00001730
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001727
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001723
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001722
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001715
Iteration 102/1000 | Loss: 0.00001715
Iteration 103/1000 | Loss: 0.00001715
Iteration 104/1000 | Loss: 0.00001715
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001713
Iteration 113/1000 | Loss: 0.00001713
Iteration 114/1000 | Loss: 0.00001713
Iteration 115/1000 | Loss: 0.00001713
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001713
Iteration 118/1000 | Loss: 0.00001712
Iteration 119/1000 | Loss: 0.00001712
Iteration 120/1000 | Loss: 0.00001712
Iteration 121/1000 | Loss: 0.00001712
Iteration 122/1000 | Loss: 0.00001712
Iteration 123/1000 | Loss: 0.00001712
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001711
Iteration 127/1000 | Loss: 0.00001711
Iteration 128/1000 | Loss: 0.00001711
Iteration 129/1000 | Loss: 0.00001710
Iteration 130/1000 | Loss: 0.00001710
Iteration 131/1000 | Loss: 0.00001710
Iteration 132/1000 | Loss: 0.00001710
Iteration 133/1000 | Loss: 0.00001709
Iteration 134/1000 | Loss: 0.00001709
Iteration 135/1000 | Loss: 0.00001709
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001707
Iteration 146/1000 | Loss: 0.00001707
Iteration 147/1000 | Loss: 0.00001707
Iteration 148/1000 | Loss: 0.00001707
Iteration 149/1000 | Loss: 0.00001707
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001706
Iteration 155/1000 | Loss: 0.00001706
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001705
Iteration 166/1000 | Loss: 0.00001705
Iteration 167/1000 | Loss: 0.00001705
Iteration 168/1000 | Loss: 0.00001705
Iteration 169/1000 | Loss: 0.00001705
Iteration 170/1000 | Loss: 0.00001705
Iteration 171/1000 | Loss: 0.00001705
Iteration 172/1000 | Loss: 0.00001704
Iteration 173/1000 | Loss: 0.00001704
Iteration 174/1000 | Loss: 0.00001704
Iteration 175/1000 | Loss: 0.00001704
Iteration 176/1000 | Loss: 0.00001704
Iteration 177/1000 | Loss: 0.00001704
Iteration 178/1000 | Loss: 0.00001704
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001704
Iteration 182/1000 | Loss: 0.00001704
Iteration 183/1000 | Loss: 0.00001704
Iteration 184/1000 | Loss: 0.00001704
Iteration 185/1000 | Loss: 0.00001704
Iteration 186/1000 | Loss: 0.00001704
Iteration 187/1000 | Loss: 0.00001704
Iteration 188/1000 | Loss: 0.00001704
Iteration 189/1000 | Loss: 0.00001704
Iteration 190/1000 | Loss: 0.00001704
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.7041833416442387e-05, 1.7041833416442387e-05, 1.7041833416442387e-05, 1.7041833416442387e-05, 1.7041833416442387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7041833416442387e-05

Optimization complete. Final v2v error: 3.530824661254883 mm

Highest mean error: 5.537834167480469 mm for frame 33

Lowest mean error: 3.22306227684021 mm for frame 41

Saving results

Total time: 84.54780173301697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790711
Iteration 2/25 | Loss: 0.00135392
Iteration 3/25 | Loss: 0.00123295
Iteration 4/25 | Loss: 0.00121882
Iteration 5/25 | Loss: 0.00121365
Iteration 6/25 | Loss: 0.00121209
Iteration 7/25 | Loss: 0.00121209
Iteration 8/25 | Loss: 0.00121209
Iteration 9/25 | Loss: 0.00121209
Iteration 10/25 | Loss: 0.00121209
Iteration 11/25 | Loss: 0.00121209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012120897881686687, 0.0012120897881686687, 0.0012120897881686687, 0.0012120897881686687, 0.0012120897881686687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012120897881686687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44661939
Iteration 2/25 | Loss: 0.00170424
Iteration 3/25 | Loss: 0.00170416
Iteration 4/25 | Loss: 0.00170416
Iteration 5/25 | Loss: 0.00170416
Iteration 6/25 | Loss: 0.00170416
Iteration 7/25 | Loss: 0.00170416
Iteration 8/25 | Loss: 0.00170416
Iteration 9/25 | Loss: 0.00170416
Iteration 10/25 | Loss: 0.00170416
Iteration 11/25 | Loss: 0.00170416
Iteration 12/25 | Loss: 0.00170416
Iteration 13/25 | Loss: 0.00170416
Iteration 14/25 | Loss: 0.00170416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001704155234619975, 0.001704155234619975, 0.001704155234619975, 0.001704155234619975, 0.001704155234619975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001704155234619975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170416
Iteration 2/1000 | Loss: 0.00004000
Iteration 3/1000 | Loss: 0.00002719
Iteration 4/1000 | Loss: 0.00002311
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002036
Iteration 7/1000 | Loss: 0.00001935
Iteration 8/1000 | Loss: 0.00001889
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001838
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001778
Iteration 18/1000 | Loss: 0.00001773
Iteration 19/1000 | Loss: 0.00001771
Iteration 20/1000 | Loss: 0.00001770
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001768
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001765
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001763
Iteration 34/1000 | Loss: 0.00001763
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001761
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001757
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001754
Iteration 54/1000 | Loss: 0.00001754
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001752
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001746
Iteration 99/1000 | Loss: 0.00001746
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001745
Iteration 103/1000 | Loss: 0.00001745
Iteration 104/1000 | Loss: 0.00001745
Iteration 105/1000 | Loss: 0.00001745
Iteration 106/1000 | Loss: 0.00001745
Iteration 107/1000 | Loss: 0.00001745
Iteration 108/1000 | Loss: 0.00001745
Iteration 109/1000 | Loss: 0.00001745
Iteration 110/1000 | Loss: 0.00001745
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001744
Iteration 117/1000 | Loss: 0.00001744
Iteration 118/1000 | Loss: 0.00001744
Iteration 119/1000 | Loss: 0.00001744
Iteration 120/1000 | Loss: 0.00001744
Iteration 121/1000 | Loss: 0.00001744
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.7442616808693856e-05, 1.7442616808693856e-05, 1.7442616808693856e-05, 1.7442616808693856e-05, 1.7442616808693856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7442616808693856e-05

Optimization complete. Final v2v error: 3.6212339401245117 mm

Highest mean error: 3.8907127380371094 mm for frame 124

Lowest mean error: 3.236185073852539 mm for frame 183

Saving results

Total time: 37.750508546829224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774462
Iteration 2/25 | Loss: 0.00152086
Iteration 3/25 | Loss: 0.00129774
Iteration 4/25 | Loss: 0.00127455
Iteration 5/25 | Loss: 0.00127415
Iteration 6/25 | Loss: 0.00127415
Iteration 7/25 | Loss: 0.00127415
Iteration 8/25 | Loss: 0.00127415
Iteration 9/25 | Loss: 0.00127415
Iteration 10/25 | Loss: 0.00127415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012741454411298037, 0.0012741454411298037, 0.0012741454411298037, 0.0012741454411298037, 0.0012741454411298037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012741454411298037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26769578
Iteration 2/25 | Loss: 0.00111877
Iteration 3/25 | Loss: 0.00111875
Iteration 4/25 | Loss: 0.00111875
Iteration 5/25 | Loss: 0.00111875
Iteration 6/25 | Loss: 0.00111875
Iteration 7/25 | Loss: 0.00111875
Iteration 8/25 | Loss: 0.00111875
Iteration 9/25 | Loss: 0.00111875
Iteration 10/25 | Loss: 0.00111875
Iteration 11/25 | Loss: 0.00111875
Iteration 12/25 | Loss: 0.00111875
Iteration 13/25 | Loss: 0.00111875
Iteration 14/25 | Loss: 0.00111875
Iteration 15/25 | Loss: 0.00111875
Iteration 16/25 | Loss: 0.00111875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011187518248334527, 0.0011187518248334527, 0.0011187518248334527, 0.0011187518248334527, 0.0011187518248334527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011187518248334527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111875
Iteration 2/1000 | Loss: 0.00002900
Iteration 3/1000 | Loss: 0.00002297
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001987
Iteration 8/1000 | Loss: 0.00001926
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001813
Iteration 18/1000 | Loss: 0.00001813
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001808
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001808
Iteration 26/1000 | Loss: 0.00001808
Iteration 27/1000 | Loss: 0.00001808
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001807
Iteration 32/1000 | Loss: 0.00001807
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001807
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001806
Iteration 39/1000 | Loss: 0.00001806
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001805
Iteration 42/1000 | Loss: 0.00001805
Iteration 43/1000 | Loss: 0.00001805
Iteration 44/1000 | Loss: 0.00001805
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001803
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001803
Iteration 50/1000 | Loss: 0.00001802
Iteration 51/1000 | Loss: 0.00001802
Iteration 52/1000 | Loss: 0.00001800
Iteration 53/1000 | Loss: 0.00001800
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001800
Iteration 57/1000 | Loss: 0.00001800
Iteration 58/1000 | Loss: 0.00001799
Iteration 59/1000 | Loss: 0.00001796
Iteration 60/1000 | Loss: 0.00001796
Iteration 61/1000 | Loss: 0.00001796
Iteration 62/1000 | Loss: 0.00001796
Iteration 63/1000 | Loss: 0.00001795
Iteration 64/1000 | Loss: 0.00001795
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001793
Iteration 67/1000 | Loss: 0.00001793
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001787
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001785
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001782
Iteration 98/1000 | Loss: 0.00001782
Iteration 99/1000 | Loss: 0.00001782
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001782
Iteration 103/1000 | Loss: 0.00001782
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001782
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.781836908776313e-05, 1.781836908776313e-05, 1.781836908776313e-05, 1.781836908776313e-05, 1.781836908776313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.781836908776313e-05

Optimization complete. Final v2v error: 3.543940305709839 mm

Highest mean error: 3.7176003456115723 mm for frame 175

Lowest mean error: 3.3139991760253906 mm for frame 0

Saving results

Total time: 37.48788785934448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000077
Iteration 2/25 | Loss: 0.01000077
Iteration 3/25 | Loss: 0.01000077
Iteration 4/25 | Loss: 0.01000077
Iteration 5/25 | Loss: 0.01000077
Iteration 6/25 | Loss: 0.01000077
Iteration 7/25 | Loss: 0.01000077
Iteration 8/25 | Loss: 0.01000077
Iteration 9/25 | Loss: 0.01000077
Iteration 10/25 | Loss: 0.01000077
Iteration 11/25 | Loss: 0.01000076
Iteration 12/25 | Loss: 0.01000076
Iteration 13/25 | Loss: 0.01000076
Iteration 14/25 | Loss: 0.01000076
Iteration 15/25 | Loss: 0.01000076
Iteration 16/25 | Loss: 0.01000076
Iteration 17/25 | Loss: 0.01000076
Iteration 18/25 | Loss: 0.01000076
Iteration 19/25 | Loss: 0.01000076
Iteration 20/25 | Loss: 0.01000076
Iteration 21/25 | Loss: 0.01000076
Iteration 22/25 | Loss: 0.01000076
Iteration 23/25 | Loss: 0.01000076
Iteration 24/25 | Loss: 0.01000076
Iteration 25/25 | Loss: 0.01000075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52037489
Iteration 2/25 | Loss: 0.11865056
Iteration 3/25 | Loss: 0.11839910
Iteration 4/25 | Loss: 0.11839910
Iteration 5/25 | Loss: 0.11839910
Iteration 6/25 | Loss: 0.11839908
Iteration 7/25 | Loss: 0.11839908
Iteration 8/25 | Loss: 0.11839907
Iteration 9/25 | Loss: 0.11839907
Iteration 10/25 | Loss: 0.11839906
Iteration 11/25 | Loss: 0.11839906
Iteration 12/25 | Loss: 0.11839906
Iteration 13/25 | Loss: 0.11839906
Iteration 14/25 | Loss: 0.11839906
Iteration 15/25 | Loss: 0.11839906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.11839906126260757, 0.11839906126260757, 0.11839906126260757, 0.11839906126260757, 0.11839906126260757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11839906126260757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11839906
Iteration 2/1000 | Loss: 0.00296089
Iteration 3/1000 | Loss: 0.00430140
Iteration 4/1000 | Loss: 0.00124885
Iteration 5/1000 | Loss: 0.00070115
Iteration 6/1000 | Loss: 0.00022984
Iteration 7/1000 | Loss: 0.00013159
Iteration 8/1000 | Loss: 0.00006625
Iteration 9/1000 | Loss: 0.00005289
Iteration 10/1000 | Loss: 0.00031426
Iteration 11/1000 | Loss: 0.00043630
Iteration 12/1000 | Loss: 0.00004302
Iteration 13/1000 | Loss: 0.00034658
Iteration 14/1000 | Loss: 0.00021229
Iteration 15/1000 | Loss: 0.00007686
Iteration 16/1000 | Loss: 0.00098614
Iteration 17/1000 | Loss: 0.00004871
Iteration 18/1000 | Loss: 0.00006374
Iteration 19/1000 | Loss: 0.00002815
Iteration 20/1000 | Loss: 0.00002441
Iteration 21/1000 | Loss: 0.00008073
Iteration 22/1000 | Loss: 0.00004350
Iteration 23/1000 | Loss: 0.00019741
Iteration 24/1000 | Loss: 0.00002098
Iteration 25/1000 | Loss: 0.00029292
Iteration 26/1000 | Loss: 0.00016340
Iteration 27/1000 | Loss: 0.00008448
Iteration 28/1000 | Loss: 0.00008406
Iteration 29/1000 | Loss: 0.00004305
Iteration 30/1000 | Loss: 0.00002979
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00003562
Iteration 33/1000 | Loss: 0.00004032
Iteration 34/1000 | Loss: 0.00007446
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00011324
Iteration 37/1000 | Loss: 0.00008733
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00005127
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00015218
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00003996
Iteration 45/1000 | Loss: 0.00037938
Iteration 46/1000 | Loss: 0.00089444
Iteration 47/1000 | Loss: 0.00004924
Iteration 48/1000 | Loss: 0.00013077
Iteration 49/1000 | Loss: 0.00004858
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00004178
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001925
Iteration 70/1000 | Loss: 0.00002333
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00002876
Iteration 74/1000 | Loss: 0.00010766
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00002258
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001617
Iteration 87/1000 | Loss: 0.00002629
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00009628
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00003645
Iteration 94/1000 | Loss: 0.00003543
Iteration 95/1000 | Loss: 0.00058352
Iteration 96/1000 | Loss: 0.00009304
Iteration 97/1000 | Loss: 0.00010481
Iteration 98/1000 | Loss: 0.00003730
Iteration 99/1000 | Loss: 0.00002075
Iteration 100/1000 | Loss: 0.00001515
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001420
Iteration 104/1000 | Loss: 0.00001420
Iteration 105/1000 | Loss: 0.00001420
Iteration 106/1000 | Loss: 0.00001420
Iteration 107/1000 | Loss: 0.00001420
Iteration 108/1000 | Loss: 0.00001420
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00002593
Iteration 114/1000 | Loss: 0.00002593
Iteration 115/1000 | Loss: 0.00003605
Iteration 116/1000 | Loss: 0.00002325
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001416
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001416
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001416
Iteration 129/1000 | Loss: 0.00001416
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.4155615644995123e-05, 1.4155615644995123e-05, 1.4155615644995123e-05, 1.4155615644995123e-05, 1.4155615644995123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4155615644995123e-05

Optimization complete. Final v2v error: 3.195626974105835 mm

Highest mean error: 3.4682793617248535 mm for frame 87

Lowest mean error: 3.1097469329833984 mm for frame 197

Saving results

Total time: 130.6385624408722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744278
Iteration 2/25 | Loss: 0.00178317
Iteration 3/25 | Loss: 0.00131658
Iteration 4/25 | Loss: 0.00122843
Iteration 5/25 | Loss: 0.00120746
Iteration 6/25 | Loss: 0.00120881
Iteration 7/25 | Loss: 0.00120330
Iteration 8/25 | Loss: 0.00119639
Iteration 9/25 | Loss: 0.00119625
Iteration 10/25 | Loss: 0.00117904
Iteration 11/25 | Loss: 0.00117245
Iteration 12/25 | Loss: 0.00117020
Iteration 13/25 | Loss: 0.00116951
Iteration 14/25 | Loss: 0.00116889
Iteration 15/25 | Loss: 0.00116941
Iteration 16/25 | Loss: 0.00116899
Iteration 17/25 | Loss: 0.00116789
Iteration 18/25 | Loss: 0.00116854
Iteration 19/25 | Loss: 0.00116750
Iteration 20/25 | Loss: 0.00116749
Iteration 21/25 | Loss: 0.00116718
Iteration 22/25 | Loss: 0.00116700
Iteration 23/25 | Loss: 0.00116700
Iteration 24/25 | Loss: 0.00116699
Iteration 25/25 | Loss: 0.00116699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03056216
Iteration 2/25 | Loss: 0.00139460
Iteration 3/25 | Loss: 0.00139460
Iteration 4/25 | Loss: 0.00139460
Iteration 5/25 | Loss: 0.00139460
Iteration 6/25 | Loss: 0.00139460
Iteration 7/25 | Loss: 0.00139041
Iteration 8/25 | Loss: 0.00139041
Iteration 9/25 | Loss: 0.00139041
Iteration 10/25 | Loss: 0.00139041
Iteration 11/25 | Loss: 0.00139041
Iteration 12/25 | Loss: 0.00139041
Iteration 13/25 | Loss: 0.00139041
Iteration 14/25 | Loss: 0.00139041
Iteration 15/25 | Loss: 0.00139041
Iteration 16/25 | Loss: 0.00139041
Iteration 17/25 | Loss: 0.00139041
Iteration 18/25 | Loss: 0.00139041
Iteration 19/25 | Loss: 0.00139041
Iteration 20/25 | Loss: 0.00139041
Iteration 21/25 | Loss: 0.00139041
Iteration 22/25 | Loss: 0.00139041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013904066290706396, 0.0013904066290706396, 0.0013904066290706396, 0.0013904066290706396, 0.0013904066290706396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013904066290706396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139041
Iteration 2/1000 | Loss: 0.00002617
Iteration 3/1000 | Loss: 0.00002324
Iteration 4/1000 | Loss: 0.00005817
Iteration 5/1000 | Loss: 0.00004801
Iteration 6/1000 | Loss: 0.00009281
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00004501
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001184
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00004839
Iteration 16/1000 | Loss: 0.00001150
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00006444
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001093
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001092
Iteration 28/1000 | Loss: 0.00001092
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001090
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001089
Iteration 34/1000 | Loss: 0.00001089
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001088
Iteration 37/1000 | Loss: 0.00003432
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001079
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001079
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001075
Iteration 56/1000 | Loss: 0.00001074
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001066
Iteration 61/1000 | Loss: 0.00001066
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00003376
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001055
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001053
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001052
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001073
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001050
Iteration 91/1000 | Loss: 0.00001050
Iteration 92/1000 | Loss: 0.00001050
Iteration 93/1000 | Loss: 0.00001050
Iteration 94/1000 | Loss: 0.00001050
Iteration 95/1000 | Loss: 0.00001050
Iteration 96/1000 | Loss: 0.00001050
Iteration 97/1000 | Loss: 0.00001050
Iteration 98/1000 | Loss: 0.00001050
Iteration 99/1000 | Loss: 0.00001050
Iteration 100/1000 | Loss: 0.00001050
Iteration 101/1000 | Loss: 0.00001050
Iteration 102/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.0498221854504663e-05, 1.0498221854504663e-05, 1.0498221854504663e-05, 1.0498221854504663e-05, 1.0498221854504663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0498221854504663e-05

Optimization complete. Final v2v error: 2.8146276473999023 mm

Highest mean error: 3.1375253200531006 mm for frame 51

Lowest mean error: 2.54706072807312 mm for frame 3

Saving results

Total time: 89.93707704544067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779252
Iteration 2/25 | Loss: 0.00150936
Iteration 3/25 | Loss: 0.00123771
Iteration 4/25 | Loss: 0.00121646
Iteration 5/25 | Loss: 0.00121417
Iteration 6/25 | Loss: 0.00121417
Iteration 7/25 | Loss: 0.00121417
Iteration 8/25 | Loss: 0.00121417
Iteration 9/25 | Loss: 0.00121417
Iteration 10/25 | Loss: 0.00121417
Iteration 11/25 | Loss: 0.00121417
Iteration 12/25 | Loss: 0.00121417
Iteration 13/25 | Loss: 0.00121417
Iteration 14/25 | Loss: 0.00121417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012141659390181303, 0.0012141659390181303, 0.0012141659390181303, 0.0012141659390181303, 0.0012141659390181303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012141659390181303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28589690
Iteration 2/25 | Loss: 0.00120148
Iteration 3/25 | Loss: 0.00120148
Iteration 4/25 | Loss: 0.00120148
Iteration 5/25 | Loss: 0.00120148
Iteration 6/25 | Loss: 0.00120148
Iteration 7/25 | Loss: 0.00120148
Iteration 8/25 | Loss: 0.00120147
Iteration 9/25 | Loss: 0.00120147
Iteration 10/25 | Loss: 0.00120147
Iteration 11/25 | Loss: 0.00120147
Iteration 12/25 | Loss: 0.00120147
Iteration 13/25 | Loss: 0.00120147
Iteration 14/25 | Loss: 0.00120147
Iteration 15/25 | Loss: 0.00120147
Iteration 16/25 | Loss: 0.00120147
Iteration 17/25 | Loss: 0.00120147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012014741078019142, 0.0012014741078019142, 0.0012014741078019142, 0.0012014741078019142, 0.0012014741078019142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012014741078019142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120147
Iteration 2/1000 | Loss: 0.00003102
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00001776
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001279
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001233
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001209
Iteration 74/1000 | Loss: 0.00001209
Iteration 75/1000 | Loss: 0.00001209
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001208
Iteration 78/1000 | Loss: 0.00001208
Iteration 79/1000 | Loss: 0.00001208
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001206
Iteration 87/1000 | Loss: 0.00001206
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001204
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001201
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001196
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001196
Iteration 141/1000 | Loss: 0.00001196
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001195
Iteration 148/1000 | Loss: 0.00001195
Iteration 149/1000 | Loss: 0.00001195
Iteration 150/1000 | Loss: 0.00001195
Iteration 151/1000 | Loss: 0.00001194
Iteration 152/1000 | Loss: 0.00001194
Iteration 153/1000 | Loss: 0.00001194
Iteration 154/1000 | Loss: 0.00001194
Iteration 155/1000 | Loss: 0.00001194
Iteration 156/1000 | Loss: 0.00001194
Iteration 157/1000 | Loss: 0.00001194
Iteration 158/1000 | Loss: 0.00001194
Iteration 159/1000 | Loss: 0.00001194
Iteration 160/1000 | Loss: 0.00001194
Iteration 161/1000 | Loss: 0.00001194
Iteration 162/1000 | Loss: 0.00001194
Iteration 163/1000 | Loss: 0.00001194
Iteration 164/1000 | Loss: 0.00001194
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001193
Iteration 169/1000 | Loss: 0.00001193
Iteration 170/1000 | Loss: 0.00001193
Iteration 171/1000 | Loss: 0.00001193
Iteration 172/1000 | Loss: 0.00001193
Iteration 173/1000 | Loss: 0.00001193
Iteration 174/1000 | Loss: 0.00001193
Iteration 175/1000 | Loss: 0.00001193
Iteration 176/1000 | Loss: 0.00001193
Iteration 177/1000 | Loss: 0.00001193
Iteration 178/1000 | Loss: 0.00001193
Iteration 179/1000 | Loss: 0.00001193
Iteration 180/1000 | Loss: 0.00001193
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001193
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001193
Iteration 189/1000 | Loss: 0.00001193
Iteration 190/1000 | Loss: 0.00001193
Iteration 191/1000 | Loss: 0.00001193
Iteration 192/1000 | Loss: 0.00001193
Iteration 193/1000 | Loss: 0.00001193
Iteration 194/1000 | Loss: 0.00001193
Iteration 195/1000 | Loss: 0.00001193
Iteration 196/1000 | Loss: 0.00001193
Iteration 197/1000 | Loss: 0.00001193
Iteration 198/1000 | Loss: 0.00001193
Iteration 199/1000 | Loss: 0.00001193
Iteration 200/1000 | Loss: 0.00001193
Iteration 201/1000 | Loss: 0.00001193
Iteration 202/1000 | Loss: 0.00001193
Iteration 203/1000 | Loss: 0.00001193
Iteration 204/1000 | Loss: 0.00001193
Iteration 205/1000 | Loss: 0.00001193
Iteration 206/1000 | Loss: 0.00001193
Iteration 207/1000 | Loss: 0.00001193
Iteration 208/1000 | Loss: 0.00001193
Iteration 209/1000 | Loss: 0.00001193
Iteration 210/1000 | Loss: 0.00001193
Iteration 211/1000 | Loss: 0.00001193
Iteration 212/1000 | Loss: 0.00001193
Iteration 213/1000 | Loss: 0.00001193
Iteration 214/1000 | Loss: 0.00001193
Iteration 215/1000 | Loss: 0.00001193
Iteration 216/1000 | Loss: 0.00001193
Iteration 217/1000 | Loss: 0.00001193
Iteration 218/1000 | Loss: 0.00001193
Iteration 219/1000 | Loss: 0.00001193
Iteration 220/1000 | Loss: 0.00001193
Iteration 221/1000 | Loss: 0.00001193
Iteration 222/1000 | Loss: 0.00001193
Iteration 223/1000 | Loss: 0.00001193
Iteration 224/1000 | Loss: 0.00001193
Iteration 225/1000 | Loss: 0.00001193
Iteration 226/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1928196727239992e-05, 1.1928196727239992e-05, 1.1928196727239992e-05, 1.1928196727239992e-05, 1.1928196727239992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1928196727239992e-05

Optimization complete. Final v2v error: 2.9142653942108154 mm

Highest mean error: 3.2521915435791016 mm for frame 123

Lowest mean error: 2.6379787921905518 mm for frame 78

Saving results

Total time: 41.85453486442566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473112
Iteration 2/25 | Loss: 0.00129390
Iteration 3/25 | Loss: 0.00121139
Iteration 4/25 | Loss: 0.00120152
Iteration 5/25 | Loss: 0.00119989
Iteration 6/25 | Loss: 0.00119989
Iteration 7/25 | Loss: 0.00119989
Iteration 8/25 | Loss: 0.00119989
Iteration 9/25 | Loss: 0.00119989
Iteration 10/25 | Loss: 0.00119989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011998893460258842, 0.0011998893460258842, 0.0011998893460258842, 0.0011998893460258842, 0.0011998893460258842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011998893460258842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.96035004
Iteration 2/25 | Loss: 0.00119097
Iteration 3/25 | Loss: 0.00119097
Iteration 4/25 | Loss: 0.00119097
Iteration 5/25 | Loss: 0.00119097
Iteration 6/25 | Loss: 0.00119097
Iteration 7/25 | Loss: 0.00119097
Iteration 8/25 | Loss: 0.00119097
Iteration 9/25 | Loss: 0.00119097
Iteration 10/25 | Loss: 0.00119097
Iteration 11/25 | Loss: 0.00119097
Iteration 12/25 | Loss: 0.00119097
Iteration 13/25 | Loss: 0.00119096
Iteration 14/25 | Loss: 0.00119096
Iteration 15/25 | Loss: 0.00119096
Iteration 16/25 | Loss: 0.00119096
Iteration 17/25 | Loss: 0.00119096
Iteration 18/25 | Loss: 0.00119096
Iteration 19/25 | Loss: 0.00119096
Iteration 20/25 | Loss: 0.00119096
Iteration 21/25 | Loss: 0.00119096
Iteration 22/25 | Loss: 0.00119096
Iteration 23/25 | Loss: 0.00119096
Iteration 24/25 | Loss: 0.00119096
Iteration 25/25 | Loss: 0.00119096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119096
Iteration 2/1000 | Loss: 0.00002249
Iteration 3/1000 | Loss: 0.00001728
Iteration 4/1000 | Loss: 0.00001568
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001388
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001314
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001196
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001184
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001177
Iteration 46/1000 | Loss: 0.00001177
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001171
Iteration 67/1000 | Loss: 0.00001171
Iteration 68/1000 | Loss: 0.00001171
Iteration 69/1000 | Loss: 0.00001171
Iteration 70/1000 | Loss: 0.00001171
Iteration 71/1000 | Loss: 0.00001170
Iteration 72/1000 | Loss: 0.00001170
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001169
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001168
Iteration 82/1000 | Loss: 0.00001168
Iteration 83/1000 | Loss: 0.00001168
Iteration 84/1000 | Loss: 0.00001168
Iteration 85/1000 | Loss: 0.00001168
Iteration 86/1000 | Loss: 0.00001168
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001167
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001167
Iteration 110/1000 | Loss: 0.00001167
Iteration 111/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.167204391094856e-05, 1.167204391094856e-05, 1.167204391094856e-05, 1.167204391094856e-05, 1.167204391094856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.167204391094856e-05

Optimization complete. Final v2v error: 2.9432308673858643 mm

Highest mean error: 3.204645872116089 mm for frame 228

Lowest mean error: 2.776381015777588 mm for frame 169

Saving results

Total time: 40.310699462890625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020382
Iteration 2/25 | Loss: 0.00249542
Iteration 3/25 | Loss: 0.00194403
Iteration 4/25 | Loss: 0.00159756
Iteration 5/25 | Loss: 0.00154878
Iteration 6/25 | Loss: 0.00156637
Iteration 7/25 | Loss: 0.00143132
Iteration 8/25 | Loss: 0.00138743
Iteration 9/25 | Loss: 0.00136289
Iteration 10/25 | Loss: 0.00135337
Iteration 11/25 | Loss: 0.00134372
Iteration 12/25 | Loss: 0.00133554
Iteration 13/25 | Loss: 0.00134453
Iteration 14/25 | Loss: 0.00133224
Iteration 15/25 | Loss: 0.00133508
Iteration 16/25 | Loss: 0.00134412
Iteration 17/25 | Loss: 0.00134363
Iteration 18/25 | Loss: 0.00133320
Iteration 19/25 | Loss: 0.00132860
Iteration 20/25 | Loss: 0.00132845
Iteration 21/25 | Loss: 0.00132845
Iteration 22/25 | Loss: 0.00132844
Iteration 23/25 | Loss: 0.00132844
Iteration 24/25 | Loss: 0.00132844
Iteration 25/25 | Loss: 0.00132844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30948257
Iteration 2/25 | Loss: 0.00358450
Iteration 3/25 | Loss: 0.00220428
Iteration 4/25 | Loss: 0.00220428
Iteration 5/25 | Loss: 0.00220428
Iteration 6/25 | Loss: 0.00220428
Iteration 7/25 | Loss: 0.00220428
Iteration 8/25 | Loss: 0.00220428
Iteration 9/25 | Loss: 0.00220428
Iteration 10/25 | Loss: 0.00220428
Iteration 11/25 | Loss: 0.00220428
Iteration 12/25 | Loss: 0.00220428
Iteration 13/25 | Loss: 0.00220428
Iteration 14/25 | Loss: 0.00220428
Iteration 15/25 | Loss: 0.00220428
Iteration 16/25 | Loss: 0.00220428
Iteration 17/25 | Loss: 0.00220428
Iteration 18/25 | Loss: 0.00220428
Iteration 19/25 | Loss: 0.00220428
Iteration 20/25 | Loss: 0.00220428
Iteration 21/25 | Loss: 0.00220428
Iteration 22/25 | Loss: 0.00220428
Iteration 23/25 | Loss: 0.00220428
Iteration 24/25 | Loss: 0.00220428
Iteration 25/25 | Loss: 0.00220428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220428
Iteration 2/1000 | Loss: 0.00084947
Iteration 3/1000 | Loss: 0.00094887
Iteration 4/1000 | Loss: 0.00027891
Iteration 5/1000 | Loss: 0.00046679
Iteration 6/1000 | Loss: 0.00051412
Iteration 7/1000 | Loss: 0.00028005
Iteration 8/1000 | Loss: 0.00027343
Iteration 9/1000 | Loss: 0.00030865
Iteration 10/1000 | Loss: 0.00100329
Iteration 11/1000 | Loss: 0.00177871
Iteration 12/1000 | Loss: 0.00098516
Iteration 13/1000 | Loss: 0.00047734
Iteration 14/1000 | Loss: 0.00009818
Iteration 15/1000 | Loss: 0.00023050
Iteration 16/1000 | Loss: 0.00124091
Iteration 17/1000 | Loss: 0.00111820
Iteration 18/1000 | Loss: 0.00056798
Iteration 19/1000 | Loss: 0.00007839
Iteration 20/1000 | Loss: 0.00020980
Iteration 21/1000 | Loss: 0.00006667
Iteration 22/1000 | Loss: 0.00006284
Iteration 23/1000 | Loss: 0.00023800
Iteration 24/1000 | Loss: 0.00070346
Iteration 25/1000 | Loss: 0.00008750
Iteration 26/1000 | Loss: 0.00034312
Iteration 27/1000 | Loss: 0.00095145
Iteration 28/1000 | Loss: 0.00010396
Iteration 29/1000 | Loss: 0.00006603
Iteration 30/1000 | Loss: 0.00005749
Iteration 31/1000 | Loss: 0.00014906
Iteration 32/1000 | Loss: 0.00012944
Iteration 33/1000 | Loss: 0.00006816
Iteration 34/1000 | Loss: 0.00005119
Iteration 35/1000 | Loss: 0.00023359
Iteration 36/1000 | Loss: 0.00004882
Iteration 37/1000 | Loss: 0.00004788
Iteration 38/1000 | Loss: 0.00004689
Iteration 39/1000 | Loss: 0.00004614
Iteration 40/1000 | Loss: 0.00062985
Iteration 41/1000 | Loss: 0.00048938
Iteration 42/1000 | Loss: 0.00024684
Iteration 43/1000 | Loss: 0.00033699
Iteration 44/1000 | Loss: 0.00017135
Iteration 45/1000 | Loss: 0.00004511
Iteration 46/1000 | Loss: 0.00004366
Iteration 47/1000 | Loss: 0.00060030
Iteration 48/1000 | Loss: 0.00338190
Iteration 49/1000 | Loss: 0.00153418
Iteration 50/1000 | Loss: 0.00040252
Iteration 51/1000 | Loss: 0.00111506
Iteration 52/1000 | Loss: 0.00005199
Iteration 53/1000 | Loss: 0.00004382
Iteration 54/1000 | Loss: 0.00003079
Iteration 55/1000 | Loss: 0.00002740
Iteration 56/1000 | Loss: 0.00031771
Iteration 57/1000 | Loss: 0.00002327
Iteration 58/1000 | Loss: 0.00002165
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00019271
Iteration 61/1000 | Loss: 0.00002893
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00109443
Iteration 66/1000 | Loss: 0.00002404
Iteration 67/1000 | Loss: 0.00005008
Iteration 68/1000 | Loss: 0.00001824
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001492
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001486
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001468
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001460
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001455
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001451
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001451
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001450
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001449
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001447
Iteration 139/1000 | Loss: 0.00001447
Iteration 140/1000 | Loss: 0.00001447
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001447
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001447
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001445
Iteration 155/1000 | Loss: 0.00001445
Iteration 156/1000 | Loss: 0.00001445
Iteration 157/1000 | Loss: 0.00001445
Iteration 158/1000 | Loss: 0.00001445
Iteration 159/1000 | Loss: 0.00001445
Iteration 160/1000 | Loss: 0.00001445
Iteration 161/1000 | Loss: 0.00001445
Iteration 162/1000 | Loss: 0.00001445
Iteration 163/1000 | Loss: 0.00001445
Iteration 164/1000 | Loss: 0.00001445
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001445
Iteration 167/1000 | Loss: 0.00001445
Iteration 168/1000 | Loss: 0.00001445
Iteration 169/1000 | Loss: 0.00001445
Iteration 170/1000 | Loss: 0.00001445
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001445
Iteration 175/1000 | Loss: 0.00001445
Iteration 176/1000 | Loss: 0.00001445
Iteration 177/1000 | Loss: 0.00001445
Iteration 178/1000 | Loss: 0.00001445
Iteration 179/1000 | Loss: 0.00001445
Iteration 180/1000 | Loss: 0.00001445
Iteration 181/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.4449969057750423e-05, 1.4449969057750423e-05, 1.4449969057750423e-05, 1.4449969057750423e-05, 1.4449969057750423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4449969057750423e-05

Optimization complete. Final v2v error: 3.2255325317382812 mm

Highest mean error: 4.48788595199585 mm for frame 67

Lowest mean error: 2.8512442111968994 mm for frame 0

Saving results

Total time: 143.71129059791565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531672
Iteration 2/25 | Loss: 0.00125501
Iteration 3/25 | Loss: 0.00118539
Iteration 4/25 | Loss: 0.00117434
Iteration 5/25 | Loss: 0.00117033
Iteration 6/25 | Loss: 0.00116996
Iteration 7/25 | Loss: 0.00116996
Iteration 8/25 | Loss: 0.00116996
Iteration 9/25 | Loss: 0.00116996
Iteration 10/25 | Loss: 0.00116996
Iteration 11/25 | Loss: 0.00116996
Iteration 12/25 | Loss: 0.00116996
Iteration 13/25 | Loss: 0.00116996
Iteration 14/25 | Loss: 0.00116996
Iteration 15/25 | Loss: 0.00116996
Iteration 16/25 | Loss: 0.00116996
Iteration 17/25 | Loss: 0.00116996
Iteration 18/25 | Loss: 0.00116996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011699602473527193, 0.0011699602473527193, 0.0011699602473527193, 0.0011699602473527193, 0.0011699602473527193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011699602473527193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56560874
Iteration 2/25 | Loss: 0.00135190
Iteration 3/25 | Loss: 0.00135190
Iteration 4/25 | Loss: 0.00135190
Iteration 5/25 | Loss: 0.00135190
Iteration 6/25 | Loss: 0.00135190
Iteration 7/25 | Loss: 0.00135190
Iteration 8/25 | Loss: 0.00135190
Iteration 9/25 | Loss: 0.00135189
Iteration 10/25 | Loss: 0.00135189
Iteration 11/25 | Loss: 0.00135189
Iteration 12/25 | Loss: 0.00135189
Iteration 13/25 | Loss: 0.00135189
Iteration 14/25 | Loss: 0.00135189
Iteration 15/25 | Loss: 0.00135189
Iteration 16/25 | Loss: 0.00135189
Iteration 17/25 | Loss: 0.00135189
Iteration 18/25 | Loss: 0.00135189
Iteration 19/25 | Loss: 0.00135189
Iteration 20/25 | Loss: 0.00135189
Iteration 21/25 | Loss: 0.00135189
Iteration 22/25 | Loss: 0.00135189
Iteration 23/25 | Loss: 0.00135189
Iteration 24/25 | Loss: 0.00135189
Iteration 25/25 | Loss: 0.00135189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001351894810795784, 0.001351894810795784, 0.001351894810795784, 0.001351894810795784, 0.001351894810795784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001351894810795784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135189
Iteration 2/1000 | Loss: 0.00002225
Iteration 3/1000 | Loss: 0.00001662
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001304
Iteration 7/1000 | Loss: 0.00001255
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001102
Iteration 13/1000 | Loss: 0.00001091
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001059
Iteration 21/1000 | Loss: 0.00001045
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001044
Iteration 24/1000 | Loss: 0.00001039
Iteration 25/1000 | Loss: 0.00001039
Iteration 26/1000 | Loss: 0.00001039
Iteration 27/1000 | Loss: 0.00001039
Iteration 28/1000 | Loss: 0.00001039
Iteration 29/1000 | Loss: 0.00001038
Iteration 30/1000 | Loss: 0.00001038
Iteration 31/1000 | Loss: 0.00001037
Iteration 32/1000 | Loss: 0.00001036
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001034
Iteration 35/1000 | Loss: 0.00001034
Iteration 36/1000 | Loss: 0.00001034
Iteration 37/1000 | Loss: 0.00001034
Iteration 38/1000 | Loss: 0.00001033
Iteration 39/1000 | Loss: 0.00001033
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001033
Iteration 42/1000 | Loss: 0.00001031
Iteration 43/1000 | Loss: 0.00001031
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001030
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001028
Iteration 50/1000 | Loss: 0.00001028
Iteration 51/1000 | Loss: 0.00001028
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001027
Iteration 54/1000 | Loss: 0.00001027
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001025
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001023
Iteration 64/1000 | Loss: 0.00001022
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001022
Iteration 67/1000 | Loss: 0.00001022
Iteration 68/1000 | Loss: 0.00001022
Iteration 69/1000 | Loss: 0.00001022
Iteration 70/1000 | Loss: 0.00001022
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001021
Iteration 73/1000 | Loss: 0.00001021
Iteration 74/1000 | Loss: 0.00001020
Iteration 75/1000 | Loss: 0.00001019
Iteration 76/1000 | Loss: 0.00001019
Iteration 77/1000 | Loss: 0.00001019
Iteration 78/1000 | Loss: 0.00001018
Iteration 79/1000 | Loss: 0.00001018
Iteration 80/1000 | Loss: 0.00001018
Iteration 81/1000 | Loss: 0.00001018
Iteration 82/1000 | Loss: 0.00001017
Iteration 83/1000 | Loss: 0.00001017
Iteration 84/1000 | Loss: 0.00001016
Iteration 85/1000 | Loss: 0.00001016
Iteration 86/1000 | Loss: 0.00001016
Iteration 87/1000 | Loss: 0.00001015
Iteration 88/1000 | Loss: 0.00001015
Iteration 89/1000 | Loss: 0.00001015
Iteration 90/1000 | Loss: 0.00001015
Iteration 91/1000 | Loss: 0.00001014
Iteration 92/1000 | Loss: 0.00001014
Iteration 93/1000 | Loss: 0.00001014
Iteration 94/1000 | Loss: 0.00001014
Iteration 95/1000 | Loss: 0.00001014
Iteration 96/1000 | Loss: 0.00001013
Iteration 97/1000 | Loss: 0.00001013
Iteration 98/1000 | Loss: 0.00001013
Iteration 99/1000 | Loss: 0.00001013
Iteration 100/1000 | Loss: 0.00001013
Iteration 101/1000 | Loss: 0.00001013
Iteration 102/1000 | Loss: 0.00001013
Iteration 103/1000 | Loss: 0.00001013
Iteration 104/1000 | Loss: 0.00001013
Iteration 105/1000 | Loss: 0.00001012
Iteration 106/1000 | Loss: 0.00001012
Iteration 107/1000 | Loss: 0.00001012
Iteration 108/1000 | Loss: 0.00001012
Iteration 109/1000 | Loss: 0.00001012
Iteration 110/1000 | Loss: 0.00001011
Iteration 111/1000 | Loss: 0.00001011
Iteration 112/1000 | Loss: 0.00001011
Iteration 113/1000 | Loss: 0.00001011
Iteration 114/1000 | Loss: 0.00001011
Iteration 115/1000 | Loss: 0.00001010
Iteration 116/1000 | Loss: 0.00001010
Iteration 117/1000 | Loss: 0.00001010
Iteration 118/1000 | Loss: 0.00001010
Iteration 119/1000 | Loss: 0.00001009
Iteration 120/1000 | Loss: 0.00001008
Iteration 121/1000 | Loss: 0.00001008
Iteration 122/1000 | Loss: 0.00001008
Iteration 123/1000 | Loss: 0.00001008
Iteration 124/1000 | Loss: 0.00001007
Iteration 125/1000 | Loss: 0.00001007
Iteration 126/1000 | Loss: 0.00001007
Iteration 127/1000 | Loss: 0.00001007
Iteration 128/1000 | Loss: 0.00001007
Iteration 129/1000 | Loss: 0.00001007
Iteration 130/1000 | Loss: 0.00001007
Iteration 131/1000 | Loss: 0.00001007
Iteration 132/1000 | Loss: 0.00001007
Iteration 133/1000 | Loss: 0.00001006
Iteration 134/1000 | Loss: 0.00001006
Iteration 135/1000 | Loss: 0.00001006
Iteration 136/1000 | Loss: 0.00001006
Iteration 137/1000 | Loss: 0.00001006
Iteration 138/1000 | Loss: 0.00001006
Iteration 139/1000 | Loss: 0.00001006
Iteration 140/1000 | Loss: 0.00001006
Iteration 141/1000 | Loss: 0.00001006
Iteration 142/1000 | Loss: 0.00001006
Iteration 143/1000 | Loss: 0.00001006
Iteration 144/1000 | Loss: 0.00001006
Iteration 145/1000 | Loss: 0.00001005
Iteration 146/1000 | Loss: 0.00001005
Iteration 147/1000 | Loss: 0.00001005
Iteration 148/1000 | Loss: 0.00001005
Iteration 149/1000 | Loss: 0.00001005
Iteration 150/1000 | Loss: 0.00001005
Iteration 151/1000 | Loss: 0.00001005
Iteration 152/1000 | Loss: 0.00001005
Iteration 153/1000 | Loss: 0.00001005
Iteration 154/1000 | Loss: 0.00001005
Iteration 155/1000 | Loss: 0.00001005
Iteration 156/1000 | Loss: 0.00001005
Iteration 157/1000 | Loss: 0.00001005
Iteration 158/1000 | Loss: 0.00001005
Iteration 159/1000 | Loss: 0.00001005
Iteration 160/1000 | Loss: 0.00001005
Iteration 161/1000 | Loss: 0.00001005
Iteration 162/1000 | Loss: 0.00001005
Iteration 163/1000 | Loss: 0.00001005
Iteration 164/1000 | Loss: 0.00001005
Iteration 165/1000 | Loss: 0.00001005
Iteration 166/1000 | Loss: 0.00001005
Iteration 167/1000 | Loss: 0.00001005
Iteration 168/1000 | Loss: 0.00001005
Iteration 169/1000 | Loss: 0.00001005
Iteration 170/1000 | Loss: 0.00001005
Iteration 171/1000 | Loss: 0.00001005
Iteration 172/1000 | Loss: 0.00001004
Iteration 173/1000 | Loss: 0.00001004
Iteration 174/1000 | Loss: 0.00001004
Iteration 175/1000 | Loss: 0.00001004
Iteration 176/1000 | Loss: 0.00001004
Iteration 177/1000 | Loss: 0.00001004
Iteration 178/1000 | Loss: 0.00001004
Iteration 179/1000 | Loss: 0.00001004
Iteration 180/1000 | Loss: 0.00001004
Iteration 181/1000 | Loss: 0.00001004
Iteration 182/1000 | Loss: 0.00001004
Iteration 183/1000 | Loss: 0.00001004
Iteration 184/1000 | Loss: 0.00001004
Iteration 185/1000 | Loss: 0.00001004
Iteration 186/1000 | Loss: 0.00001004
Iteration 187/1000 | Loss: 0.00001004
Iteration 188/1000 | Loss: 0.00001004
Iteration 189/1000 | Loss: 0.00001004
Iteration 190/1000 | Loss: 0.00001004
Iteration 191/1000 | Loss: 0.00001004
Iteration 192/1000 | Loss: 0.00001004
Iteration 193/1000 | Loss: 0.00001004
Iteration 194/1000 | Loss: 0.00001004
Iteration 195/1000 | Loss: 0.00001004
Iteration 196/1000 | Loss: 0.00001004
Iteration 197/1000 | Loss: 0.00001004
Iteration 198/1000 | Loss: 0.00001004
Iteration 199/1000 | Loss: 0.00001004
Iteration 200/1000 | Loss: 0.00001004
Iteration 201/1000 | Loss: 0.00001004
Iteration 202/1000 | Loss: 0.00001004
Iteration 203/1000 | Loss: 0.00001004
Iteration 204/1000 | Loss: 0.00001004
Iteration 205/1000 | Loss: 0.00001004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.003686247713631e-05, 1.003686247713631e-05, 1.003686247713631e-05, 1.003686247713631e-05, 1.003686247713631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.003686247713631e-05

Optimization complete. Final v2v error: 2.7525858879089355 mm

Highest mean error: 3.126943826675415 mm for frame 150

Lowest mean error: 2.541788339614868 mm for frame 33

Saving results

Total time: 43.88800883293152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348751
Iteration 2/25 | Loss: 0.00129860
Iteration 3/25 | Loss: 0.00119838
Iteration 4/25 | Loss: 0.00117972
Iteration 5/25 | Loss: 0.00117281
Iteration 6/25 | Loss: 0.00117201
Iteration 7/25 | Loss: 0.00117201
Iteration 8/25 | Loss: 0.00117201
Iteration 9/25 | Loss: 0.00117201
Iteration 10/25 | Loss: 0.00117201
Iteration 11/25 | Loss: 0.00117201
Iteration 12/25 | Loss: 0.00117201
Iteration 13/25 | Loss: 0.00117201
Iteration 14/25 | Loss: 0.00117201
Iteration 15/25 | Loss: 0.00117201
Iteration 16/25 | Loss: 0.00117201
Iteration 17/25 | Loss: 0.00117201
Iteration 18/25 | Loss: 0.00117201
Iteration 19/25 | Loss: 0.00117201
Iteration 20/25 | Loss: 0.00117201
Iteration 21/25 | Loss: 0.00117201
Iteration 22/25 | Loss: 0.00117201
Iteration 23/25 | Loss: 0.00117201
Iteration 24/25 | Loss: 0.00117201
Iteration 25/25 | Loss: 0.00117201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23410499
Iteration 2/25 | Loss: 0.00187212
Iteration 3/25 | Loss: 0.00187212
Iteration 4/25 | Loss: 0.00187212
Iteration 5/25 | Loss: 0.00187212
Iteration 6/25 | Loss: 0.00187212
Iteration 7/25 | Loss: 0.00187212
Iteration 8/25 | Loss: 0.00187212
Iteration 9/25 | Loss: 0.00187212
Iteration 10/25 | Loss: 0.00187212
Iteration 11/25 | Loss: 0.00187212
Iteration 12/25 | Loss: 0.00187212
Iteration 13/25 | Loss: 0.00187212
Iteration 14/25 | Loss: 0.00187212
Iteration 15/25 | Loss: 0.00187212
Iteration 16/25 | Loss: 0.00187212
Iteration 17/25 | Loss: 0.00187212
Iteration 18/25 | Loss: 0.00187212
Iteration 19/25 | Loss: 0.00187212
Iteration 20/25 | Loss: 0.00187212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018721158849075437, 0.0018721158849075437, 0.0018721158849075437, 0.0018721158849075437, 0.0018721158849075437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018721158849075437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187212
Iteration 2/1000 | Loss: 0.00004423
Iteration 3/1000 | Loss: 0.00002992
Iteration 4/1000 | Loss: 0.00002355
Iteration 5/1000 | Loss: 0.00002163
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001892
Iteration 8/1000 | Loss: 0.00001829
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001757
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001694
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001666
Iteration 21/1000 | Loss: 0.00001665
Iteration 22/1000 | Loss: 0.00001665
Iteration 23/1000 | Loss: 0.00001665
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001663
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001657
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001640
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001631
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001631
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001619
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001614
Iteration 117/1000 | Loss: 0.00001614
Iteration 118/1000 | Loss: 0.00001614
Iteration 119/1000 | Loss: 0.00001614
Iteration 120/1000 | Loss: 0.00001613
Iteration 121/1000 | Loss: 0.00001613
Iteration 122/1000 | Loss: 0.00001613
Iteration 123/1000 | Loss: 0.00001613
Iteration 124/1000 | Loss: 0.00001613
Iteration 125/1000 | Loss: 0.00001613
Iteration 126/1000 | Loss: 0.00001612
Iteration 127/1000 | Loss: 0.00001612
Iteration 128/1000 | Loss: 0.00001612
Iteration 129/1000 | Loss: 0.00001612
Iteration 130/1000 | Loss: 0.00001612
Iteration 131/1000 | Loss: 0.00001612
Iteration 132/1000 | Loss: 0.00001612
Iteration 133/1000 | Loss: 0.00001612
Iteration 134/1000 | Loss: 0.00001612
Iteration 135/1000 | Loss: 0.00001611
Iteration 136/1000 | Loss: 0.00001611
Iteration 137/1000 | Loss: 0.00001611
Iteration 138/1000 | Loss: 0.00001611
Iteration 139/1000 | Loss: 0.00001611
Iteration 140/1000 | Loss: 0.00001611
Iteration 141/1000 | Loss: 0.00001611
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001611
Iteration 146/1000 | Loss: 0.00001611
Iteration 147/1000 | Loss: 0.00001611
Iteration 148/1000 | Loss: 0.00001610
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001610
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001609
Iteration 156/1000 | Loss: 0.00001609
Iteration 157/1000 | Loss: 0.00001609
Iteration 158/1000 | Loss: 0.00001609
Iteration 159/1000 | Loss: 0.00001609
Iteration 160/1000 | Loss: 0.00001609
Iteration 161/1000 | Loss: 0.00001609
Iteration 162/1000 | Loss: 0.00001609
Iteration 163/1000 | Loss: 0.00001609
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001609
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Iteration 170/1000 | Loss: 0.00001608
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001607
Iteration 179/1000 | Loss: 0.00001607
Iteration 180/1000 | Loss: 0.00001607
Iteration 181/1000 | Loss: 0.00001607
Iteration 182/1000 | Loss: 0.00001606
Iteration 183/1000 | Loss: 0.00001606
Iteration 184/1000 | Loss: 0.00001606
Iteration 185/1000 | Loss: 0.00001606
Iteration 186/1000 | Loss: 0.00001606
Iteration 187/1000 | Loss: 0.00001605
Iteration 188/1000 | Loss: 0.00001605
Iteration 189/1000 | Loss: 0.00001605
Iteration 190/1000 | Loss: 0.00001605
Iteration 191/1000 | Loss: 0.00001604
Iteration 192/1000 | Loss: 0.00001604
Iteration 193/1000 | Loss: 0.00001604
Iteration 194/1000 | Loss: 0.00001603
Iteration 195/1000 | Loss: 0.00001603
Iteration 196/1000 | Loss: 0.00001603
Iteration 197/1000 | Loss: 0.00001603
Iteration 198/1000 | Loss: 0.00001602
Iteration 199/1000 | Loss: 0.00001602
Iteration 200/1000 | Loss: 0.00001602
Iteration 201/1000 | Loss: 0.00001602
Iteration 202/1000 | Loss: 0.00001602
Iteration 203/1000 | Loss: 0.00001602
Iteration 204/1000 | Loss: 0.00001602
Iteration 205/1000 | Loss: 0.00001601
Iteration 206/1000 | Loss: 0.00001601
Iteration 207/1000 | Loss: 0.00001601
Iteration 208/1000 | Loss: 0.00001601
Iteration 209/1000 | Loss: 0.00001601
Iteration 210/1000 | Loss: 0.00001601
Iteration 211/1000 | Loss: 0.00001601
Iteration 212/1000 | Loss: 0.00001600
Iteration 213/1000 | Loss: 0.00001600
Iteration 214/1000 | Loss: 0.00001600
Iteration 215/1000 | Loss: 0.00001600
Iteration 216/1000 | Loss: 0.00001600
Iteration 217/1000 | Loss: 0.00001599
Iteration 218/1000 | Loss: 0.00001599
Iteration 219/1000 | Loss: 0.00001599
Iteration 220/1000 | Loss: 0.00001599
Iteration 221/1000 | Loss: 0.00001599
Iteration 222/1000 | Loss: 0.00001599
Iteration 223/1000 | Loss: 0.00001599
Iteration 224/1000 | Loss: 0.00001598
Iteration 225/1000 | Loss: 0.00001598
Iteration 226/1000 | Loss: 0.00001598
Iteration 227/1000 | Loss: 0.00001598
Iteration 228/1000 | Loss: 0.00001598
Iteration 229/1000 | Loss: 0.00001598
Iteration 230/1000 | Loss: 0.00001598
Iteration 231/1000 | Loss: 0.00001598
Iteration 232/1000 | Loss: 0.00001598
Iteration 233/1000 | Loss: 0.00001598
Iteration 234/1000 | Loss: 0.00001598
Iteration 235/1000 | Loss: 0.00001598
Iteration 236/1000 | Loss: 0.00001598
Iteration 237/1000 | Loss: 0.00001598
Iteration 238/1000 | Loss: 0.00001598
Iteration 239/1000 | Loss: 0.00001598
Iteration 240/1000 | Loss: 0.00001598
Iteration 241/1000 | Loss: 0.00001598
Iteration 242/1000 | Loss: 0.00001598
Iteration 243/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.598114067746792e-05, 1.598114067746792e-05, 1.598114067746792e-05, 1.598114067746792e-05, 1.598114067746792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.598114067746792e-05

Optimization complete. Final v2v error: 3.3501181602478027 mm

Highest mean error: 4.177579402923584 mm for frame 160

Lowest mean error: 2.738438367843628 mm for frame 196

Saving results

Total time: 55.19412851333618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531582
Iteration 2/25 | Loss: 0.00141682
Iteration 3/25 | Loss: 0.00126457
Iteration 4/25 | Loss: 0.00125174
Iteration 5/25 | Loss: 0.00125093
Iteration 6/25 | Loss: 0.00125093
Iteration 7/25 | Loss: 0.00125093
Iteration 8/25 | Loss: 0.00125093
Iteration 9/25 | Loss: 0.00125093
Iteration 10/25 | Loss: 0.00125093
Iteration 11/25 | Loss: 0.00125093
Iteration 12/25 | Loss: 0.00125093
Iteration 13/25 | Loss: 0.00125093
Iteration 14/25 | Loss: 0.00125093
Iteration 15/25 | Loss: 0.00125093
Iteration 16/25 | Loss: 0.00125093
Iteration 17/25 | Loss: 0.00125093
Iteration 18/25 | Loss: 0.00125093
Iteration 19/25 | Loss: 0.00125093
Iteration 20/25 | Loss: 0.00125093
Iteration 21/25 | Loss: 0.00125093
Iteration 22/25 | Loss: 0.00125093
Iteration 23/25 | Loss: 0.00125093
Iteration 24/25 | Loss: 0.00125093
Iteration 25/25 | Loss: 0.00125093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81358808
Iteration 2/25 | Loss: 0.00114789
Iteration 3/25 | Loss: 0.00114789
Iteration 4/25 | Loss: 0.00114789
Iteration 5/25 | Loss: 0.00114789
Iteration 6/25 | Loss: 0.00114788
Iteration 7/25 | Loss: 0.00114788
Iteration 8/25 | Loss: 0.00114788
Iteration 9/25 | Loss: 0.00114788
Iteration 10/25 | Loss: 0.00114788
Iteration 11/25 | Loss: 0.00114788
Iteration 12/25 | Loss: 0.00114788
Iteration 13/25 | Loss: 0.00114788
Iteration 14/25 | Loss: 0.00114788
Iteration 15/25 | Loss: 0.00114788
Iteration 16/25 | Loss: 0.00114788
Iteration 17/25 | Loss: 0.00114788
Iteration 18/25 | Loss: 0.00114788
Iteration 19/25 | Loss: 0.00114788
Iteration 20/25 | Loss: 0.00114788
Iteration 21/25 | Loss: 0.00114788
Iteration 22/25 | Loss: 0.00114788
Iteration 23/25 | Loss: 0.00114788
Iteration 24/25 | Loss: 0.00114788
Iteration 25/25 | Loss: 0.00114788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114788
Iteration 2/1000 | Loss: 0.00002926
Iteration 3/1000 | Loss: 0.00002173
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001826
Iteration 6/1000 | Loss: 0.00001774
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001675
Iteration 10/1000 | Loss: 0.00001640
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001544
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001480
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001454
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001445
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001444
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001442
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001441
Iteration 85/1000 | Loss: 0.00001441
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001441
Iteration 89/1000 | Loss: 0.00001440
Iteration 90/1000 | Loss: 0.00001440
Iteration 91/1000 | Loss: 0.00001440
Iteration 92/1000 | Loss: 0.00001440
Iteration 93/1000 | Loss: 0.00001440
Iteration 94/1000 | Loss: 0.00001440
Iteration 95/1000 | Loss: 0.00001440
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001438
Iteration 111/1000 | Loss: 0.00001438
Iteration 112/1000 | Loss: 0.00001438
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001437
Iteration 117/1000 | Loss: 0.00001436
Iteration 118/1000 | Loss: 0.00001436
Iteration 119/1000 | Loss: 0.00001436
Iteration 120/1000 | Loss: 0.00001436
Iteration 121/1000 | Loss: 0.00001436
Iteration 122/1000 | Loss: 0.00001436
Iteration 123/1000 | Loss: 0.00001436
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001435
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001435
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001432
Iteration 138/1000 | Loss: 0.00001432
Iteration 139/1000 | Loss: 0.00001432
Iteration 140/1000 | Loss: 0.00001432
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001431
Iteration 143/1000 | Loss: 0.00001431
Iteration 144/1000 | Loss: 0.00001431
Iteration 145/1000 | Loss: 0.00001430
Iteration 146/1000 | Loss: 0.00001430
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001429
Iteration 159/1000 | Loss: 0.00001429
Iteration 160/1000 | Loss: 0.00001429
Iteration 161/1000 | Loss: 0.00001429
Iteration 162/1000 | Loss: 0.00001428
Iteration 163/1000 | Loss: 0.00001428
Iteration 164/1000 | Loss: 0.00001428
Iteration 165/1000 | Loss: 0.00001428
Iteration 166/1000 | Loss: 0.00001428
Iteration 167/1000 | Loss: 0.00001428
Iteration 168/1000 | Loss: 0.00001428
Iteration 169/1000 | Loss: 0.00001428
Iteration 170/1000 | Loss: 0.00001428
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001427
Iteration 173/1000 | Loss: 0.00001427
Iteration 174/1000 | Loss: 0.00001427
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001427
Iteration 177/1000 | Loss: 0.00001427
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Iteration 182/1000 | Loss: 0.00001426
Iteration 183/1000 | Loss: 0.00001426
Iteration 184/1000 | Loss: 0.00001426
Iteration 185/1000 | Loss: 0.00001426
Iteration 186/1000 | Loss: 0.00001426
Iteration 187/1000 | Loss: 0.00001426
Iteration 188/1000 | Loss: 0.00001426
Iteration 189/1000 | Loss: 0.00001426
Iteration 190/1000 | Loss: 0.00001426
Iteration 191/1000 | Loss: 0.00001426
Iteration 192/1000 | Loss: 0.00001425
Iteration 193/1000 | Loss: 0.00001425
Iteration 194/1000 | Loss: 0.00001425
Iteration 195/1000 | Loss: 0.00001425
Iteration 196/1000 | Loss: 0.00001425
Iteration 197/1000 | Loss: 0.00001425
Iteration 198/1000 | Loss: 0.00001425
Iteration 199/1000 | Loss: 0.00001425
Iteration 200/1000 | Loss: 0.00001425
Iteration 201/1000 | Loss: 0.00001425
Iteration 202/1000 | Loss: 0.00001425
Iteration 203/1000 | Loss: 0.00001425
Iteration 204/1000 | Loss: 0.00001425
Iteration 205/1000 | Loss: 0.00001425
Iteration 206/1000 | Loss: 0.00001425
Iteration 207/1000 | Loss: 0.00001425
Iteration 208/1000 | Loss: 0.00001425
Iteration 209/1000 | Loss: 0.00001425
Iteration 210/1000 | Loss: 0.00001425
Iteration 211/1000 | Loss: 0.00001425
Iteration 212/1000 | Loss: 0.00001425
Iteration 213/1000 | Loss: 0.00001425
Iteration 214/1000 | Loss: 0.00001425
Iteration 215/1000 | Loss: 0.00001425
Iteration 216/1000 | Loss: 0.00001425
Iteration 217/1000 | Loss: 0.00001425
Iteration 218/1000 | Loss: 0.00001425
Iteration 219/1000 | Loss: 0.00001425
Iteration 220/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.4249880223360378e-05, 1.4249880223360378e-05, 1.4249880223360378e-05, 1.4249880223360378e-05, 1.4249880223360378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4249880223360378e-05

Optimization complete. Final v2v error: 3.1615352630615234 mm

Highest mean error: 3.4509267807006836 mm for frame 57

Lowest mean error: 2.7702503204345703 mm for frame 217

Saving results

Total time: 51.72248888015747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666380
Iteration 2/25 | Loss: 0.00125710
Iteration 3/25 | Loss: 0.00119261
Iteration 4/25 | Loss: 0.00118275
Iteration 5/25 | Loss: 0.00118003
Iteration 6/25 | Loss: 0.00117938
Iteration 7/25 | Loss: 0.00117938
Iteration 8/25 | Loss: 0.00117938
Iteration 9/25 | Loss: 0.00117938
Iteration 10/25 | Loss: 0.00117938
Iteration 11/25 | Loss: 0.00117938
Iteration 12/25 | Loss: 0.00117938
Iteration 13/25 | Loss: 0.00117938
Iteration 14/25 | Loss: 0.00117938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011793809244409204, 0.0011793809244409204, 0.0011793809244409204, 0.0011793809244409204, 0.0011793809244409204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011793809244409204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16242433
Iteration 2/25 | Loss: 0.00130406
Iteration 3/25 | Loss: 0.00130406
Iteration 4/25 | Loss: 0.00130406
Iteration 5/25 | Loss: 0.00130406
Iteration 6/25 | Loss: 0.00130406
Iteration 7/25 | Loss: 0.00130406
Iteration 8/25 | Loss: 0.00130406
Iteration 9/25 | Loss: 0.00130406
Iteration 10/25 | Loss: 0.00130406
Iteration 11/25 | Loss: 0.00130406
Iteration 12/25 | Loss: 0.00130406
Iteration 13/25 | Loss: 0.00130406
Iteration 14/25 | Loss: 0.00130406
Iteration 15/25 | Loss: 0.00130405
Iteration 16/25 | Loss: 0.00130405
Iteration 17/25 | Loss: 0.00130405
Iteration 18/25 | Loss: 0.00130405
Iteration 19/25 | Loss: 0.00130405
Iteration 20/25 | Loss: 0.00130405
Iteration 21/25 | Loss: 0.00130405
Iteration 22/25 | Loss: 0.00130405
Iteration 23/25 | Loss: 0.00130405
Iteration 24/25 | Loss: 0.00130405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013040548656135798, 0.0013040548656135798, 0.0013040548656135798, 0.0013040548656135798, 0.0013040548656135798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013040548656135798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130405
Iteration 2/1000 | Loss: 0.00003058
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001521
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001256
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001162
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001119
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001118
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001114
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001109
Iteration 44/1000 | Loss: 0.00001108
Iteration 45/1000 | Loss: 0.00001108
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001104
Iteration 56/1000 | Loss: 0.00001104
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001102
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001095
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001095
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001091
Iteration 121/1000 | Loss: 0.00001091
Iteration 122/1000 | Loss: 0.00001091
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001091
Iteration 126/1000 | Loss: 0.00001091
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Iteration 130/1000 | Loss: 0.00001090
Iteration 131/1000 | Loss: 0.00001090
Iteration 132/1000 | Loss: 0.00001090
Iteration 133/1000 | Loss: 0.00001090
Iteration 134/1000 | Loss: 0.00001090
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001090
Iteration 138/1000 | Loss: 0.00001090
Iteration 139/1000 | Loss: 0.00001090
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001089
Iteration 144/1000 | Loss: 0.00001089
Iteration 145/1000 | Loss: 0.00001089
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001089
Iteration 150/1000 | Loss: 0.00001089
Iteration 151/1000 | Loss: 0.00001089
Iteration 152/1000 | Loss: 0.00001089
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001088
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001088
Iteration 159/1000 | Loss: 0.00001088
Iteration 160/1000 | Loss: 0.00001088
Iteration 161/1000 | Loss: 0.00001088
Iteration 162/1000 | Loss: 0.00001088
Iteration 163/1000 | Loss: 0.00001088
Iteration 164/1000 | Loss: 0.00001088
Iteration 165/1000 | Loss: 0.00001088
Iteration 166/1000 | Loss: 0.00001088
Iteration 167/1000 | Loss: 0.00001088
Iteration 168/1000 | Loss: 0.00001088
Iteration 169/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.0876697160711046e-05, 1.0876697160711046e-05, 1.0876697160711046e-05, 1.0876697160711046e-05, 1.0876697160711046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0876697160711046e-05

Optimization complete. Final v2v error: 2.865062952041626 mm

Highest mean error: 3.006312847137451 mm for frame 69

Lowest mean error: 2.747276782989502 mm for frame 133

Saving results

Total time: 39.11129283905029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406175
Iteration 2/25 | Loss: 0.00127803
Iteration 3/25 | Loss: 0.00117755
Iteration 4/25 | Loss: 0.00116640
Iteration 5/25 | Loss: 0.00116407
Iteration 6/25 | Loss: 0.00116363
Iteration 7/25 | Loss: 0.00116363
Iteration 8/25 | Loss: 0.00116363
Iteration 9/25 | Loss: 0.00116363
Iteration 10/25 | Loss: 0.00116363
Iteration 11/25 | Loss: 0.00116363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011636337731033564, 0.0011636337731033564, 0.0011636337731033564, 0.0011636337731033564, 0.0011636337731033564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011636337731033564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30303240
Iteration 2/25 | Loss: 0.00149354
Iteration 3/25 | Loss: 0.00149353
Iteration 4/25 | Loss: 0.00149353
Iteration 5/25 | Loss: 0.00149353
Iteration 6/25 | Loss: 0.00149353
Iteration 7/25 | Loss: 0.00149353
Iteration 8/25 | Loss: 0.00149353
Iteration 9/25 | Loss: 0.00149353
Iteration 10/25 | Loss: 0.00149353
Iteration 11/25 | Loss: 0.00149353
Iteration 12/25 | Loss: 0.00149353
Iteration 13/25 | Loss: 0.00149353
Iteration 14/25 | Loss: 0.00149353
Iteration 15/25 | Loss: 0.00149353
Iteration 16/25 | Loss: 0.00149353
Iteration 17/25 | Loss: 0.00149353
Iteration 18/25 | Loss: 0.00149353
Iteration 19/25 | Loss: 0.00149353
Iteration 20/25 | Loss: 0.00149353
Iteration 21/25 | Loss: 0.00149353
Iteration 22/25 | Loss: 0.00149353
Iteration 23/25 | Loss: 0.00149353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001493530930019915, 0.001493530930019915, 0.001493530930019915, 0.001493530930019915, 0.001493530930019915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001493530930019915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149353
Iteration 2/1000 | Loss: 0.00001836
Iteration 3/1000 | Loss: 0.00001299
Iteration 4/1000 | Loss: 0.00001154
Iteration 5/1000 | Loss: 0.00001077
Iteration 6/1000 | Loss: 0.00001031
Iteration 7/1000 | Loss: 0.00000991
Iteration 8/1000 | Loss: 0.00000989
Iteration 9/1000 | Loss: 0.00000968
Iteration 10/1000 | Loss: 0.00000944
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000933
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000925
Iteration 16/1000 | Loss: 0.00000923
Iteration 17/1000 | Loss: 0.00000920
Iteration 18/1000 | Loss: 0.00000915
Iteration 19/1000 | Loss: 0.00000913
Iteration 20/1000 | Loss: 0.00000911
Iteration 21/1000 | Loss: 0.00000910
Iteration 22/1000 | Loss: 0.00000908
Iteration 23/1000 | Loss: 0.00000907
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000900
Iteration 26/1000 | Loss: 0.00000899
Iteration 27/1000 | Loss: 0.00000898
Iteration 28/1000 | Loss: 0.00000898
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000897
Iteration 31/1000 | Loss: 0.00000897
Iteration 32/1000 | Loss: 0.00000897
Iteration 33/1000 | Loss: 0.00000896
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000889
Iteration 37/1000 | Loss: 0.00000889
Iteration 38/1000 | Loss: 0.00000888
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000886
Iteration 41/1000 | Loss: 0.00000885
Iteration 42/1000 | Loss: 0.00000885
Iteration 43/1000 | Loss: 0.00000884
Iteration 44/1000 | Loss: 0.00000883
Iteration 45/1000 | Loss: 0.00000879
Iteration 46/1000 | Loss: 0.00000879
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000879
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000876
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000875
Iteration 64/1000 | Loss: 0.00000875
Iteration 65/1000 | Loss: 0.00000875
Iteration 66/1000 | Loss: 0.00000874
Iteration 67/1000 | Loss: 0.00000874
Iteration 68/1000 | Loss: 0.00000874
Iteration 69/1000 | Loss: 0.00000873
Iteration 70/1000 | Loss: 0.00000873
Iteration 71/1000 | Loss: 0.00000873
Iteration 72/1000 | Loss: 0.00000873
Iteration 73/1000 | Loss: 0.00000873
Iteration 74/1000 | Loss: 0.00000873
Iteration 75/1000 | Loss: 0.00000872
Iteration 76/1000 | Loss: 0.00000872
Iteration 77/1000 | Loss: 0.00000872
Iteration 78/1000 | Loss: 0.00000872
Iteration 79/1000 | Loss: 0.00000872
Iteration 80/1000 | Loss: 0.00000872
Iteration 81/1000 | Loss: 0.00000872
Iteration 82/1000 | Loss: 0.00000872
Iteration 83/1000 | Loss: 0.00000872
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000870
Iteration 90/1000 | Loss: 0.00000870
Iteration 91/1000 | Loss: 0.00000869
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000868
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000866
Iteration 102/1000 | Loss: 0.00000866
Iteration 103/1000 | Loss: 0.00000866
Iteration 104/1000 | Loss: 0.00000866
Iteration 105/1000 | Loss: 0.00000866
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000865
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000864
Iteration 113/1000 | Loss: 0.00000864
Iteration 114/1000 | Loss: 0.00000864
Iteration 115/1000 | Loss: 0.00000864
Iteration 116/1000 | Loss: 0.00000864
Iteration 117/1000 | Loss: 0.00000863
Iteration 118/1000 | Loss: 0.00000863
Iteration 119/1000 | Loss: 0.00000863
Iteration 120/1000 | Loss: 0.00000863
Iteration 121/1000 | Loss: 0.00000863
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000862
Iteration 124/1000 | Loss: 0.00000862
Iteration 125/1000 | Loss: 0.00000862
Iteration 126/1000 | Loss: 0.00000862
Iteration 127/1000 | Loss: 0.00000862
Iteration 128/1000 | Loss: 0.00000862
Iteration 129/1000 | Loss: 0.00000862
Iteration 130/1000 | Loss: 0.00000862
Iteration 131/1000 | Loss: 0.00000861
Iteration 132/1000 | Loss: 0.00000861
Iteration 133/1000 | Loss: 0.00000861
Iteration 134/1000 | Loss: 0.00000861
Iteration 135/1000 | Loss: 0.00000861
Iteration 136/1000 | Loss: 0.00000860
Iteration 137/1000 | Loss: 0.00000860
Iteration 138/1000 | Loss: 0.00000860
Iteration 139/1000 | Loss: 0.00000860
Iteration 140/1000 | Loss: 0.00000859
Iteration 141/1000 | Loss: 0.00000859
Iteration 142/1000 | Loss: 0.00000859
Iteration 143/1000 | Loss: 0.00000859
Iteration 144/1000 | Loss: 0.00000859
Iteration 145/1000 | Loss: 0.00000859
Iteration 146/1000 | Loss: 0.00000859
Iteration 147/1000 | Loss: 0.00000859
Iteration 148/1000 | Loss: 0.00000858
Iteration 149/1000 | Loss: 0.00000858
Iteration 150/1000 | Loss: 0.00000858
Iteration 151/1000 | Loss: 0.00000858
Iteration 152/1000 | Loss: 0.00000858
Iteration 153/1000 | Loss: 0.00000858
Iteration 154/1000 | Loss: 0.00000858
Iteration 155/1000 | Loss: 0.00000858
Iteration 156/1000 | Loss: 0.00000858
Iteration 157/1000 | Loss: 0.00000858
Iteration 158/1000 | Loss: 0.00000858
Iteration 159/1000 | Loss: 0.00000857
Iteration 160/1000 | Loss: 0.00000857
Iteration 161/1000 | Loss: 0.00000857
Iteration 162/1000 | Loss: 0.00000857
Iteration 163/1000 | Loss: 0.00000857
Iteration 164/1000 | Loss: 0.00000857
Iteration 165/1000 | Loss: 0.00000857
Iteration 166/1000 | Loss: 0.00000857
Iteration 167/1000 | Loss: 0.00000857
Iteration 168/1000 | Loss: 0.00000857
Iteration 169/1000 | Loss: 0.00000856
Iteration 170/1000 | Loss: 0.00000856
Iteration 171/1000 | Loss: 0.00000856
Iteration 172/1000 | Loss: 0.00000856
Iteration 173/1000 | Loss: 0.00000856
Iteration 174/1000 | Loss: 0.00000856
Iteration 175/1000 | Loss: 0.00000856
Iteration 176/1000 | Loss: 0.00000856
Iteration 177/1000 | Loss: 0.00000856
Iteration 178/1000 | Loss: 0.00000856
Iteration 179/1000 | Loss: 0.00000856
Iteration 180/1000 | Loss: 0.00000856
Iteration 181/1000 | Loss: 0.00000856
Iteration 182/1000 | Loss: 0.00000856
Iteration 183/1000 | Loss: 0.00000856
Iteration 184/1000 | Loss: 0.00000856
Iteration 185/1000 | Loss: 0.00000856
Iteration 186/1000 | Loss: 0.00000856
Iteration 187/1000 | Loss: 0.00000855
Iteration 188/1000 | Loss: 0.00000855
Iteration 189/1000 | Loss: 0.00000855
Iteration 190/1000 | Loss: 0.00000855
Iteration 191/1000 | Loss: 0.00000855
Iteration 192/1000 | Loss: 0.00000855
Iteration 193/1000 | Loss: 0.00000855
Iteration 194/1000 | Loss: 0.00000854
Iteration 195/1000 | Loss: 0.00000854
Iteration 196/1000 | Loss: 0.00000854
Iteration 197/1000 | Loss: 0.00000854
Iteration 198/1000 | Loss: 0.00000854
Iteration 199/1000 | Loss: 0.00000854
Iteration 200/1000 | Loss: 0.00000854
Iteration 201/1000 | Loss: 0.00000853
Iteration 202/1000 | Loss: 0.00000853
Iteration 203/1000 | Loss: 0.00000853
Iteration 204/1000 | Loss: 0.00000853
Iteration 205/1000 | Loss: 0.00000852
Iteration 206/1000 | Loss: 0.00000852
Iteration 207/1000 | Loss: 0.00000852
Iteration 208/1000 | Loss: 0.00000852
Iteration 209/1000 | Loss: 0.00000852
Iteration 210/1000 | Loss: 0.00000852
Iteration 211/1000 | Loss: 0.00000852
Iteration 212/1000 | Loss: 0.00000852
Iteration 213/1000 | Loss: 0.00000852
Iteration 214/1000 | Loss: 0.00000852
Iteration 215/1000 | Loss: 0.00000852
Iteration 216/1000 | Loss: 0.00000851
Iteration 217/1000 | Loss: 0.00000851
Iteration 218/1000 | Loss: 0.00000851
Iteration 219/1000 | Loss: 0.00000851
Iteration 220/1000 | Loss: 0.00000851
Iteration 221/1000 | Loss: 0.00000851
Iteration 222/1000 | Loss: 0.00000851
Iteration 223/1000 | Loss: 0.00000851
Iteration 224/1000 | Loss: 0.00000851
Iteration 225/1000 | Loss: 0.00000851
Iteration 226/1000 | Loss: 0.00000851
Iteration 227/1000 | Loss: 0.00000851
Iteration 228/1000 | Loss: 0.00000851
Iteration 229/1000 | Loss: 0.00000851
Iteration 230/1000 | Loss: 0.00000851
Iteration 231/1000 | Loss: 0.00000850
Iteration 232/1000 | Loss: 0.00000850
Iteration 233/1000 | Loss: 0.00000850
Iteration 234/1000 | Loss: 0.00000850
Iteration 235/1000 | Loss: 0.00000850
Iteration 236/1000 | Loss: 0.00000850
Iteration 237/1000 | Loss: 0.00000850
Iteration 238/1000 | Loss: 0.00000850
Iteration 239/1000 | Loss: 0.00000850
Iteration 240/1000 | Loss: 0.00000850
Iteration 241/1000 | Loss: 0.00000850
Iteration 242/1000 | Loss: 0.00000850
Iteration 243/1000 | Loss: 0.00000850
Iteration 244/1000 | Loss: 0.00000849
Iteration 245/1000 | Loss: 0.00000849
Iteration 246/1000 | Loss: 0.00000849
Iteration 247/1000 | Loss: 0.00000849
Iteration 248/1000 | Loss: 0.00000849
Iteration 249/1000 | Loss: 0.00000849
Iteration 250/1000 | Loss: 0.00000849
Iteration 251/1000 | Loss: 0.00000849
Iteration 252/1000 | Loss: 0.00000849
Iteration 253/1000 | Loss: 0.00000849
Iteration 254/1000 | Loss: 0.00000849
Iteration 255/1000 | Loss: 0.00000849
Iteration 256/1000 | Loss: 0.00000849
Iteration 257/1000 | Loss: 0.00000848
Iteration 258/1000 | Loss: 0.00000848
Iteration 259/1000 | Loss: 0.00000848
Iteration 260/1000 | Loss: 0.00000848
Iteration 261/1000 | Loss: 0.00000848
Iteration 262/1000 | Loss: 0.00000848
Iteration 263/1000 | Loss: 0.00000848
Iteration 264/1000 | Loss: 0.00000848
Iteration 265/1000 | Loss: 0.00000848
Iteration 266/1000 | Loss: 0.00000848
Iteration 267/1000 | Loss: 0.00000848
Iteration 268/1000 | Loss: 0.00000848
Iteration 269/1000 | Loss: 0.00000847
Iteration 270/1000 | Loss: 0.00000847
Iteration 271/1000 | Loss: 0.00000847
Iteration 272/1000 | Loss: 0.00000847
Iteration 273/1000 | Loss: 0.00000847
Iteration 274/1000 | Loss: 0.00000847
Iteration 275/1000 | Loss: 0.00000847
Iteration 276/1000 | Loss: 0.00000847
Iteration 277/1000 | Loss: 0.00000847
Iteration 278/1000 | Loss: 0.00000847
Iteration 279/1000 | Loss: 0.00000847
Iteration 280/1000 | Loss: 0.00000847
Iteration 281/1000 | Loss: 0.00000846
Iteration 282/1000 | Loss: 0.00000846
Iteration 283/1000 | Loss: 0.00000846
Iteration 284/1000 | Loss: 0.00000846
Iteration 285/1000 | Loss: 0.00000846
Iteration 286/1000 | Loss: 0.00000846
Iteration 287/1000 | Loss: 0.00000846
Iteration 288/1000 | Loss: 0.00000846
Iteration 289/1000 | Loss: 0.00000846
Iteration 290/1000 | Loss: 0.00000846
Iteration 291/1000 | Loss: 0.00000846
Iteration 292/1000 | Loss: 0.00000846
Iteration 293/1000 | Loss: 0.00000846
Iteration 294/1000 | Loss: 0.00000846
Iteration 295/1000 | Loss: 0.00000846
Iteration 296/1000 | Loss: 0.00000845
Iteration 297/1000 | Loss: 0.00000845
Iteration 298/1000 | Loss: 0.00000845
Iteration 299/1000 | Loss: 0.00000845
Iteration 300/1000 | Loss: 0.00000845
Iteration 301/1000 | Loss: 0.00000845
Iteration 302/1000 | Loss: 0.00000845
Iteration 303/1000 | Loss: 0.00000845
Iteration 304/1000 | Loss: 0.00000845
Iteration 305/1000 | Loss: 0.00000845
Iteration 306/1000 | Loss: 0.00000845
Iteration 307/1000 | Loss: 0.00000845
Iteration 308/1000 | Loss: 0.00000845
Iteration 309/1000 | Loss: 0.00000845
Iteration 310/1000 | Loss: 0.00000845
Iteration 311/1000 | Loss: 0.00000845
Iteration 312/1000 | Loss: 0.00000845
Iteration 313/1000 | Loss: 0.00000844
Iteration 314/1000 | Loss: 0.00000844
Iteration 315/1000 | Loss: 0.00000844
Iteration 316/1000 | Loss: 0.00000844
Iteration 317/1000 | Loss: 0.00000844
Iteration 318/1000 | Loss: 0.00000844
Iteration 319/1000 | Loss: 0.00000844
Iteration 320/1000 | Loss: 0.00000844
Iteration 321/1000 | Loss: 0.00000844
Iteration 322/1000 | Loss: 0.00000844
Iteration 323/1000 | Loss: 0.00000844
Iteration 324/1000 | Loss: 0.00000844
Iteration 325/1000 | Loss: 0.00000844
Iteration 326/1000 | Loss: 0.00000844
Iteration 327/1000 | Loss: 0.00000844
Iteration 328/1000 | Loss: 0.00000843
Iteration 329/1000 | Loss: 0.00000843
Iteration 330/1000 | Loss: 0.00000843
Iteration 331/1000 | Loss: 0.00000843
Iteration 332/1000 | Loss: 0.00000843
Iteration 333/1000 | Loss: 0.00000843
Iteration 334/1000 | Loss: 0.00000843
Iteration 335/1000 | Loss: 0.00000843
Iteration 336/1000 | Loss: 0.00000843
Iteration 337/1000 | Loss: 0.00000843
Iteration 338/1000 | Loss: 0.00000842
Iteration 339/1000 | Loss: 0.00000842
Iteration 340/1000 | Loss: 0.00000842
Iteration 341/1000 | Loss: 0.00000842
Iteration 342/1000 | Loss: 0.00000842
Iteration 343/1000 | Loss: 0.00000842
Iteration 344/1000 | Loss: 0.00000842
Iteration 345/1000 | Loss: 0.00000842
Iteration 346/1000 | Loss: 0.00000842
Iteration 347/1000 | Loss: 0.00000842
Iteration 348/1000 | Loss: 0.00000842
Iteration 349/1000 | Loss: 0.00000842
Iteration 350/1000 | Loss: 0.00000842
Iteration 351/1000 | Loss: 0.00000842
Iteration 352/1000 | Loss: 0.00000842
Iteration 353/1000 | Loss: 0.00000842
Iteration 354/1000 | Loss: 0.00000842
Iteration 355/1000 | Loss: 0.00000842
Iteration 356/1000 | Loss: 0.00000842
Iteration 357/1000 | Loss: 0.00000842
Iteration 358/1000 | Loss: 0.00000842
Iteration 359/1000 | Loss: 0.00000842
Iteration 360/1000 | Loss: 0.00000842
Iteration 361/1000 | Loss: 0.00000841
Iteration 362/1000 | Loss: 0.00000841
Iteration 363/1000 | Loss: 0.00000841
Iteration 364/1000 | Loss: 0.00000841
Iteration 365/1000 | Loss: 0.00000841
Iteration 366/1000 | Loss: 0.00000841
Iteration 367/1000 | Loss: 0.00000841
Iteration 368/1000 | Loss: 0.00000841
Iteration 369/1000 | Loss: 0.00000841
Iteration 370/1000 | Loss: 0.00000841
Iteration 371/1000 | Loss: 0.00000841
Iteration 372/1000 | Loss: 0.00000841
Iteration 373/1000 | Loss: 0.00000841
Iteration 374/1000 | Loss: 0.00000841
Iteration 375/1000 | Loss: 0.00000841
Iteration 376/1000 | Loss: 0.00000841
Iteration 377/1000 | Loss: 0.00000841
Iteration 378/1000 | Loss: 0.00000841
Iteration 379/1000 | Loss: 0.00000841
Iteration 380/1000 | Loss: 0.00000841
Iteration 381/1000 | Loss: 0.00000841
Iteration 382/1000 | Loss: 0.00000840
Iteration 383/1000 | Loss: 0.00000840
Iteration 384/1000 | Loss: 0.00000840
Iteration 385/1000 | Loss: 0.00000840
Iteration 386/1000 | Loss: 0.00000840
Iteration 387/1000 | Loss: 0.00000840
Iteration 388/1000 | Loss: 0.00000840
Iteration 389/1000 | Loss: 0.00000840
Iteration 390/1000 | Loss: 0.00000840
Iteration 391/1000 | Loss: 0.00000840
Iteration 392/1000 | Loss: 0.00000840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 392. Stopping optimization.
Last 5 losses: [8.403320862271357e-06, 8.403320862271357e-06, 8.403320862271357e-06, 8.403320862271357e-06, 8.403320862271357e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.403320862271357e-06

Optimization complete. Final v2v error: 2.4764068126678467 mm

Highest mean error: 3.3647401332855225 mm for frame 54

Lowest mean error: 2.3150458335876465 mm for frame 155

Saving results

Total time: 48.67725706100464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836980
Iteration 2/25 | Loss: 0.00125249
Iteration 3/25 | Loss: 0.00119232
Iteration 4/25 | Loss: 0.00118104
Iteration 5/25 | Loss: 0.00117940
Iteration 6/25 | Loss: 0.00117940
Iteration 7/25 | Loss: 0.00117940
Iteration 8/25 | Loss: 0.00117940
Iteration 9/25 | Loss: 0.00117940
Iteration 10/25 | Loss: 0.00117940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011793989688158035, 0.0011793989688158035, 0.0011793989688158035, 0.0011793989688158035, 0.0011793989688158035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011793989688158035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31101811
Iteration 2/25 | Loss: 0.00129684
Iteration 3/25 | Loss: 0.00129684
Iteration 4/25 | Loss: 0.00129683
Iteration 5/25 | Loss: 0.00129683
Iteration 6/25 | Loss: 0.00129683
Iteration 7/25 | Loss: 0.00129683
Iteration 8/25 | Loss: 0.00129683
Iteration 9/25 | Loss: 0.00129683
Iteration 10/25 | Loss: 0.00129683
Iteration 11/25 | Loss: 0.00129683
Iteration 12/25 | Loss: 0.00129683
Iteration 13/25 | Loss: 0.00129683
Iteration 14/25 | Loss: 0.00129683
Iteration 15/25 | Loss: 0.00129683
Iteration 16/25 | Loss: 0.00129683
Iteration 17/25 | Loss: 0.00129683
Iteration 18/25 | Loss: 0.00129683
Iteration 19/25 | Loss: 0.00129683
Iteration 20/25 | Loss: 0.00129683
Iteration 21/25 | Loss: 0.00129683
Iteration 22/25 | Loss: 0.00129683
Iteration 23/25 | Loss: 0.00129683
Iteration 24/25 | Loss: 0.00129683
Iteration 25/25 | Loss: 0.00129683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129683
Iteration 2/1000 | Loss: 0.00002093
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001509
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001177
Iteration 19/1000 | Loss: 0.00001169
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001160
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001130
Iteration 59/1000 | Loss: 0.00001130
Iteration 60/1000 | Loss: 0.00001129
Iteration 61/1000 | Loss: 0.00001129
Iteration 62/1000 | Loss: 0.00001129
Iteration 63/1000 | Loss: 0.00001129
Iteration 64/1000 | Loss: 0.00001128
Iteration 65/1000 | Loss: 0.00001128
Iteration 66/1000 | Loss: 0.00001128
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001127
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001126
Iteration 71/1000 | Loss: 0.00001126
Iteration 72/1000 | Loss: 0.00001126
Iteration 73/1000 | Loss: 0.00001126
Iteration 74/1000 | Loss: 0.00001126
Iteration 75/1000 | Loss: 0.00001125
Iteration 76/1000 | Loss: 0.00001125
Iteration 77/1000 | Loss: 0.00001125
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001123
Iteration 85/1000 | Loss: 0.00001122
Iteration 86/1000 | Loss: 0.00001122
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001116
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001114
Iteration 100/1000 | Loss: 0.00001114
Iteration 101/1000 | Loss: 0.00001114
Iteration 102/1000 | Loss: 0.00001114
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001112
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001110
Iteration 131/1000 | Loss: 0.00001110
Iteration 132/1000 | Loss: 0.00001110
Iteration 133/1000 | Loss: 0.00001110
Iteration 134/1000 | Loss: 0.00001110
Iteration 135/1000 | Loss: 0.00001110
Iteration 136/1000 | Loss: 0.00001110
Iteration 137/1000 | Loss: 0.00001110
Iteration 138/1000 | Loss: 0.00001110
Iteration 139/1000 | Loss: 0.00001110
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.110267658077646e-05, 1.110267658077646e-05, 1.110267658077646e-05, 1.110267658077646e-05, 1.110267658077646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.110267658077646e-05

Optimization complete. Final v2v error: 2.8921091556549072 mm

Highest mean error: 3.225182056427002 mm for frame 118

Lowest mean error: 2.7597262859344482 mm for frame 197

Saving results

Total time: 43.27953290939331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809237
Iteration 2/25 | Loss: 0.00181325
Iteration 3/25 | Loss: 0.00141615
Iteration 4/25 | Loss: 0.00136761
Iteration 5/25 | Loss: 0.00137259
Iteration 6/25 | Loss: 0.00136216
Iteration 7/25 | Loss: 0.00134781
Iteration 8/25 | Loss: 0.00131861
Iteration 9/25 | Loss: 0.00131347
Iteration 10/25 | Loss: 0.00130582
Iteration 11/25 | Loss: 0.00130394
Iteration 12/25 | Loss: 0.00130372
Iteration 13/25 | Loss: 0.00130361
Iteration 14/25 | Loss: 0.00130360
Iteration 15/25 | Loss: 0.00130360
Iteration 16/25 | Loss: 0.00130360
Iteration 17/25 | Loss: 0.00130360
Iteration 18/25 | Loss: 0.00130360
Iteration 19/25 | Loss: 0.00130360
Iteration 20/25 | Loss: 0.00130360
Iteration 21/25 | Loss: 0.00130359
Iteration 22/25 | Loss: 0.00130359
Iteration 23/25 | Loss: 0.00130359
Iteration 24/25 | Loss: 0.00130359
Iteration 25/25 | Loss: 0.00130359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.55211854
Iteration 2/25 | Loss: 0.00100368
Iteration 3/25 | Loss: 0.00100365
Iteration 4/25 | Loss: 0.00100365
Iteration 5/25 | Loss: 0.00100365
Iteration 6/25 | Loss: 0.00100365
Iteration 7/25 | Loss: 0.00100365
Iteration 8/25 | Loss: 0.00100365
Iteration 9/25 | Loss: 0.00100365
Iteration 10/25 | Loss: 0.00100365
Iteration 11/25 | Loss: 0.00100365
Iteration 12/25 | Loss: 0.00100365
Iteration 13/25 | Loss: 0.00100365
Iteration 14/25 | Loss: 0.00100365
Iteration 15/25 | Loss: 0.00100365
Iteration 16/25 | Loss: 0.00100365
Iteration 17/25 | Loss: 0.00100365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010036492021754384, 0.0010036492021754384, 0.0010036492021754384, 0.0010036492021754384, 0.0010036492021754384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010036492021754384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100365
Iteration 2/1000 | Loss: 0.00002739
Iteration 3/1000 | Loss: 0.00002232
Iteration 4/1000 | Loss: 0.00002097
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00001954
Iteration 7/1000 | Loss: 0.00001905
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00015624
Iteration 14/1000 | Loss: 0.00002161
Iteration 15/1000 | Loss: 0.00001999
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001731
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001731
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00001730
Iteration 38/1000 | Loss: 0.00001730
Iteration 39/1000 | Loss: 0.00001730
Iteration 40/1000 | Loss: 0.00001729
Iteration 41/1000 | Loss: 0.00001729
Iteration 42/1000 | Loss: 0.00001729
Iteration 43/1000 | Loss: 0.00001728
Iteration 44/1000 | Loss: 0.00001728
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001728
Iteration 48/1000 | Loss: 0.00001726
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001725
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001724
Iteration 61/1000 | Loss: 0.00001724
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001716
Iteration 89/1000 | Loss: 0.00001716
Iteration 90/1000 | Loss: 0.00001716
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001715
Iteration 97/1000 | Loss: 0.00001715
Iteration 98/1000 | Loss: 0.00001715
Iteration 99/1000 | Loss: 0.00001715
Iteration 100/1000 | Loss: 0.00001715
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001714
Iteration 104/1000 | Loss: 0.00001714
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001713
Iteration 113/1000 | Loss: 0.00001713
Iteration 114/1000 | Loss: 0.00001713
Iteration 115/1000 | Loss: 0.00001713
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001713
Iteration 118/1000 | Loss: 0.00001713
Iteration 119/1000 | Loss: 0.00001713
Iteration 120/1000 | Loss: 0.00001713
Iteration 121/1000 | Loss: 0.00001713
Iteration 122/1000 | Loss: 0.00001713
Iteration 123/1000 | Loss: 0.00001713
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001713
Iteration 130/1000 | Loss: 0.00001713
Iteration 131/1000 | Loss: 0.00001713
Iteration 132/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.7131344066001475e-05, 1.7131344066001475e-05, 1.7131344066001475e-05, 1.7131344066001475e-05, 1.7131344066001475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7131344066001475e-05

Optimization complete. Final v2v error: 3.4775984287261963 mm

Highest mean error: 4.1733551025390625 mm for frame 210

Lowest mean error: 3.1726574897766113 mm for frame 117

Saving results

Total time: 62.4396710395813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785296
Iteration 2/25 | Loss: 0.00141804
Iteration 3/25 | Loss: 0.00124083
Iteration 4/25 | Loss: 0.00121965
Iteration 5/25 | Loss: 0.00121228
Iteration 6/25 | Loss: 0.00121110
Iteration 7/25 | Loss: 0.00121110
Iteration 8/25 | Loss: 0.00121110
Iteration 9/25 | Loss: 0.00121110
Iteration 10/25 | Loss: 0.00121110
Iteration 11/25 | Loss: 0.00121110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001211103517562151, 0.001211103517562151, 0.001211103517562151, 0.001211103517562151, 0.001211103517562151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001211103517562151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65511703
Iteration 2/25 | Loss: 0.00122182
Iteration 3/25 | Loss: 0.00122182
Iteration 4/25 | Loss: 0.00122182
Iteration 5/25 | Loss: 0.00122182
Iteration 6/25 | Loss: 0.00122182
Iteration 7/25 | Loss: 0.00122182
Iteration 8/25 | Loss: 0.00122182
Iteration 9/25 | Loss: 0.00122182
Iteration 10/25 | Loss: 0.00122182
Iteration 11/25 | Loss: 0.00122182
Iteration 12/25 | Loss: 0.00122182
Iteration 13/25 | Loss: 0.00122182
Iteration 14/25 | Loss: 0.00122182
Iteration 15/25 | Loss: 0.00122182
Iteration 16/25 | Loss: 0.00122182
Iteration 17/25 | Loss: 0.00122182
Iteration 18/25 | Loss: 0.00122182
Iteration 19/25 | Loss: 0.00122182
Iteration 20/25 | Loss: 0.00122182
Iteration 21/25 | Loss: 0.00122182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012218158226460218, 0.0012218158226460218, 0.0012218158226460218, 0.0012218158226460218, 0.0012218158226460218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012218158226460218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122182
Iteration 2/1000 | Loss: 0.00003719
Iteration 3/1000 | Loss: 0.00002456
Iteration 4/1000 | Loss: 0.00002148
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001844
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001702
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001643
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001642
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00001641
Iteration 28/1000 | Loss: 0.00001641
Iteration 29/1000 | Loss: 0.00001641
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001641
Iteration 32/1000 | Loss: 0.00001641
Iteration 33/1000 | Loss: 0.00001640
Iteration 34/1000 | Loss: 0.00001640
Iteration 35/1000 | Loss: 0.00001640
Iteration 36/1000 | Loss: 0.00001640
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001639
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001637
Iteration 45/1000 | Loss: 0.00001637
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001632
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001631
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001630
Iteration 71/1000 | Loss: 0.00001630
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001630
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001630
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001628
Iteration 85/1000 | Loss: 0.00001628
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001628
Iteration 88/1000 | Loss: 0.00001628
Iteration 89/1000 | Loss: 0.00001628
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001625
Iteration 112/1000 | Loss: 0.00001625
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001625
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001620
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.6178117220988497e-05, 1.6178117220988497e-05, 1.6178117220988497e-05, 1.6178117220988497e-05, 1.6178117220988497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6178117220988497e-05

Optimization complete. Final v2v error: 3.44618558883667 mm

Highest mean error: 3.8151869773864746 mm for frame 42

Lowest mean error: 3.1286938190460205 mm for frame 165

Saving results

Total time: 45.77621102333069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521435
Iteration 2/25 | Loss: 0.00145429
Iteration 3/25 | Loss: 0.00126155
Iteration 4/25 | Loss: 0.00124307
Iteration 5/25 | Loss: 0.00123955
Iteration 6/25 | Loss: 0.00123881
Iteration 7/25 | Loss: 0.00123881
Iteration 8/25 | Loss: 0.00123881
Iteration 9/25 | Loss: 0.00123881
Iteration 10/25 | Loss: 0.00123881
Iteration 11/25 | Loss: 0.00123881
Iteration 12/25 | Loss: 0.00123881
Iteration 13/25 | Loss: 0.00123881
Iteration 14/25 | Loss: 0.00123881
Iteration 15/25 | Loss: 0.00123881
Iteration 16/25 | Loss: 0.00123881
Iteration 17/25 | Loss: 0.00123881
Iteration 18/25 | Loss: 0.00123881
Iteration 19/25 | Loss: 0.00123881
Iteration 20/25 | Loss: 0.00123881
Iteration 21/25 | Loss: 0.00123881
Iteration 22/25 | Loss: 0.00123881
Iteration 23/25 | Loss: 0.00123881
Iteration 24/25 | Loss: 0.00123881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001238807337358594, 0.001238807337358594, 0.001238807337358594, 0.001238807337358594, 0.001238807337358594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238807337358594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28525209
Iteration 2/25 | Loss: 0.00108026
Iteration 3/25 | Loss: 0.00108024
Iteration 4/25 | Loss: 0.00108024
Iteration 5/25 | Loss: 0.00108024
Iteration 6/25 | Loss: 0.00108024
Iteration 7/25 | Loss: 0.00108024
Iteration 8/25 | Loss: 0.00108024
Iteration 9/25 | Loss: 0.00108024
Iteration 10/25 | Loss: 0.00108024
Iteration 11/25 | Loss: 0.00108024
Iteration 12/25 | Loss: 0.00108024
Iteration 13/25 | Loss: 0.00108024
Iteration 14/25 | Loss: 0.00108024
Iteration 15/25 | Loss: 0.00108024
Iteration 16/25 | Loss: 0.00108024
Iteration 17/25 | Loss: 0.00108024
Iteration 18/25 | Loss: 0.00108024
Iteration 19/25 | Loss: 0.00108024
Iteration 20/25 | Loss: 0.00108024
Iteration 21/25 | Loss: 0.00108024
Iteration 22/25 | Loss: 0.00108024
Iteration 23/25 | Loss: 0.00108024
Iteration 24/25 | Loss: 0.00108024
Iteration 25/25 | Loss: 0.00108024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108024
Iteration 2/1000 | Loss: 0.00002854
Iteration 3/1000 | Loss: 0.00002037
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001637
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001516
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001505
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001495
Iteration 20/1000 | Loss: 0.00001489
Iteration 21/1000 | Loss: 0.00001488
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001484
Iteration 26/1000 | Loss: 0.00001483
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001476
Iteration 31/1000 | Loss: 0.00001476
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001472
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001471
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001457
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001452
Iteration 89/1000 | Loss: 0.00001452
Iteration 90/1000 | Loss: 0.00001452
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001449
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001449
Iteration 108/1000 | Loss: 0.00001449
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001448
Iteration 113/1000 | Loss: 0.00001448
Iteration 114/1000 | Loss: 0.00001448
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001448
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001447
Iteration 123/1000 | Loss: 0.00001447
Iteration 124/1000 | Loss: 0.00001447
Iteration 125/1000 | Loss: 0.00001447
Iteration 126/1000 | Loss: 0.00001447
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001446
Iteration 138/1000 | Loss: 0.00001446
Iteration 139/1000 | Loss: 0.00001446
Iteration 140/1000 | Loss: 0.00001446
Iteration 141/1000 | Loss: 0.00001446
Iteration 142/1000 | Loss: 0.00001446
Iteration 143/1000 | Loss: 0.00001446
Iteration 144/1000 | Loss: 0.00001445
Iteration 145/1000 | Loss: 0.00001445
Iteration 146/1000 | Loss: 0.00001445
Iteration 147/1000 | Loss: 0.00001445
Iteration 148/1000 | Loss: 0.00001445
Iteration 149/1000 | Loss: 0.00001445
Iteration 150/1000 | Loss: 0.00001445
Iteration 151/1000 | Loss: 0.00001445
Iteration 152/1000 | Loss: 0.00001445
Iteration 153/1000 | Loss: 0.00001445
Iteration 154/1000 | Loss: 0.00001445
Iteration 155/1000 | Loss: 0.00001445
Iteration 156/1000 | Loss: 0.00001445
Iteration 157/1000 | Loss: 0.00001445
Iteration 158/1000 | Loss: 0.00001444
Iteration 159/1000 | Loss: 0.00001444
Iteration 160/1000 | Loss: 0.00001444
Iteration 161/1000 | Loss: 0.00001444
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001444
Iteration 164/1000 | Loss: 0.00001444
Iteration 165/1000 | Loss: 0.00001444
Iteration 166/1000 | Loss: 0.00001444
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001443
Iteration 169/1000 | Loss: 0.00001443
Iteration 170/1000 | Loss: 0.00001443
Iteration 171/1000 | Loss: 0.00001443
Iteration 172/1000 | Loss: 0.00001443
Iteration 173/1000 | Loss: 0.00001443
Iteration 174/1000 | Loss: 0.00001443
Iteration 175/1000 | Loss: 0.00001442
Iteration 176/1000 | Loss: 0.00001442
Iteration 177/1000 | Loss: 0.00001442
Iteration 178/1000 | Loss: 0.00001442
Iteration 179/1000 | Loss: 0.00001442
Iteration 180/1000 | Loss: 0.00001442
Iteration 181/1000 | Loss: 0.00001442
Iteration 182/1000 | Loss: 0.00001442
Iteration 183/1000 | Loss: 0.00001442
Iteration 184/1000 | Loss: 0.00001442
Iteration 185/1000 | Loss: 0.00001442
Iteration 186/1000 | Loss: 0.00001442
Iteration 187/1000 | Loss: 0.00001442
Iteration 188/1000 | Loss: 0.00001442
Iteration 189/1000 | Loss: 0.00001442
Iteration 190/1000 | Loss: 0.00001441
Iteration 191/1000 | Loss: 0.00001441
Iteration 192/1000 | Loss: 0.00001441
Iteration 193/1000 | Loss: 0.00001441
Iteration 194/1000 | Loss: 0.00001441
Iteration 195/1000 | Loss: 0.00001441
Iteration 196/1000 | Loss: 0.00001441
Iteration 197/1000 | Loss: 0.00001441
Iteration 198/1000 | Loss: 0.00001440
Iteration 199/1000 | Loss: 0.00001440
Iteration 200/1000 | Loss: 0.00001440
Iteration 201/1000 | Loss: 0.00001440
Iteration 202/1000 | Loss: 0.00001440
Iteration 203/1000 | Loss: 0.00001440
Iteration 204/1000 | Loss: 0.00001440
Iteration 205/1000 | Loss: 0.00001440
Iteration 206/1000 | Loss: 0.00001440
Iteration 207/1000 | Loss: 0.00001440
Iteration 208/1000 | Loss: 0.00001440
Iteration 209/1000 | Loss: 0.00001440
Iteration 210/1000 | Loss: 0.00001440
Iteration 211/1000 | Loss: 0.00001440
Iteration 212/1000 | Loss: 0.00001440
Iteration 213/1000 | Loss: 0.00001440
Iteration 214/1000 | Loss: 0.00001440
Iteration 215/1000 | Loss: 0.00001440
Iteration 216/1000 | Loss: 0.00001440
Iteration 217/1000 | Loss: 0.00001440
Iteration 218/1000 | Loss: 0.00001440
Iteration 219/1000 | Loss: 0.00001440
Iteration 220/1000 | Loss: 0.00001440
Iteration 221/1000 | Loss: 0.00001440
Iteration 222/1000 | Loss: 0.00001440
Iteration 223/1000 | Loss: 0.00001440
Iteration 224/1000 | Loss: 0.00001440
Iteration 225/1000 | Loss: 0.00001440
Iteration 226/1000 | Loss: 0.00001440
Iteration 227/1000 | Loss: 0.00001440
Iteration 228/1000 | Loss: 0.00001440
Iteration 229/1000 | Loss: 0.00001440
Iteration 230/1000 | Loss: 0.00001440
Iteration 231/1000 | Loss: 0.00001440
Iteration 232/1000 | Loss: 0.00001440
Iteration 233/1000 | Loss: 0.00001440
Iteration 234/1000 | Loss: 0.00001440
Iteration 235/1000 | Loss: 0.00001440
Iteration 236/1000 | Loss: 0.00001440
Iteration 237/1000 | Loss: 0.00001440
Iteration 238/1000 | Loss: 0.00001440
Iteration 239/1000 | Loss: 0.00001440
Iteration 240/1000 | Loss: 0.00001440
Iteration 241/1000 | Loss: 0.00001440
Iteration 242/1000 | Loss: 0.00001440
Iteration 243/1000 | Loss: 0.00001440
Iteration 244/1000 | Loss: 0.00001440
Iteration 245/1000 | Loss: 0.00001440
Iteration 246/1000 | Loss: 0.00001440
Iteration 247/1000 | Loss: 0.00001440
Iteration 248/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.439962306903908e-05, 1.439962306903908e-05, 1.439962306903908e-05, 1.439962306903908e-05, 1.439962306903908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.439962306903908e-05

Optimization complete. Final v2v error: 3.1738831996917725 mm

Highest mean error: 3.627821683883667 mm for frame 48

Lowest mean error: 2.757207155227661 mm for frame 103

Saving results

Total time: 42.717310667037964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765384
Iteration 2/25 | Loss: 0.00158196
Iteration 3/25 | Loss: 0.00126884
Iteration 4/25 | Loss: 0.00123164
Iteration 5/25 | Loss: 0.00122522
Iteration 6/25 | Loss: 0.00122388
Iteration 7/25 | Loss: 0.00122388
Iteration 8/25 | Loss: 0.00122388
Iteration 9/25 | Loss: 0.00122388
Iteration 10/25 | Loss: 0.00122388
Iteration 11/25 | Loss: 0.00122388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012238842900842428, 0.0012238842900842428, 0.0012238842900842428, 0.0012238842900842428, 0.0012238842900842428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012238842900842428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33156037
Iteration 2/25 | Loss: 0.00147253
Iteration 3/25 | Loss: 0.00147253
Iteration 4/25 | Loss: 0.00147253
Iteration 5/25 | Loss: 0.00147253
Iteration 6/25 | Loss: 0.00147253
Iteration 7/25 | Loss: 0.00147253
Iteration 8/25 | Loss: 0.00147253
Iteration 9/25 | Loss: 0.00147253
Iteration 10/25 | Loss: 0.00147253
Iteration 11/25 | Loss: 0.00147253
Iteration 12/25 | Loss: 0.00147253
Iteration 13/25 | Loss: 0.00147253
Iteration 14/25 | Loss: 0.00147253
Iteration 15/25 | Loss: 0.00147253
Iteration 16/25 | Loss: 0.00147253
Iteration 17/25 | Loss: 0.00147253
Iteration 18/25 | Loss: 0.00147253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014725252985954285, 0.0014725252985954285, 0.0014725252985954285, 0.0014725252985954285, 0.0014725252985954285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014725252985954285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147253
Iteration 2/1000 | Loss: 0.00004579
Iteration 3/1000 | Loss: 0.00002280
Iteration 4/1000 | Loss: 0.00001802
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001566
Iteration 7/1000 | Loss: 0.00001501
Iteration 8/1000 | Loss: 0.00001454
Iteration 9/1000 | Loss: 0.00001426
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001377
Iteration 13/1000 | Loss: 0.00001373
Iteration 14/1000 | Loss: 0.00001367
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001345
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001330
Iteration 85/1000 | Loss: 0.00001330
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001328
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001323
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Iteration 152/1000 | Loss: 0.00001322
Iteration 153/1000 | Loss: 0.00001322
Iteration 154/1000 | Loss: 0.00001322
Iteration 155/1000 | Loss: 0.00001322
Iteration 156/1000 | Loss: 0.00001322
Iteration 157/1000 | Loss: 0.00001322
Iteration 158/1000 | Loss: 0.00001322
Iteration 159/1000 | Loss: 0.00001321
Iteration 160/1000 | Loss: 0.00001321
Iteration 161/1000 | Loss: 0.00001321
Iteration 162/1000 | Loss: 0.00001321
Iteration 163/1000 | Loss: 0.00001320
Iteration 164/1000 | Loss: 0.00001320
Iteration 165/1000 | Loss: 0.00001320
Iteration 166/1000 | Loss: 0.00001320
Iteration 167/1000 | Loss: 0.00001320
Iteration 168/1000 | Loss: 0.00001320
Iteration 169/1000 | Loss: 0.00001320
Iteration 170/1000 | Loss: 0.00001319
Iteration 171/1000 | Loss: 0.00001319
Iteration 172/1000 | Loss: 0.00001319
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001318
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001318
Iteration 177/1000 | Loss: 0.00001318
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001317
Iteration 193/1000 | Loss: 0.00001317
Iteration 194/1000 | Loss: 0.00001317
Iteration 195/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3174367268220522e-05, 1.3174367268220522e-05, 1.3174367268220522e-05, 1.3174367268220522e-05, 1.3174367268220522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3174367268220522e-05

Optimization complete. Final v2v error: 3.1083672046661377 mm

Highest mean error: 3.4737040996551514 mm for frame 55

Lowest mean error: 2.644049644470215 mm for frame 13

Saving results

Total time: 38.74380874633789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821421
Iteration 2/25 | Loss: 0.00196832
Iteration 3/25 | Loss: 0.00141610
Iteration 4/25 | Loss: 0.00129126
Iteration 5/25 | Loss: 0.00128111
Iteration 6/25 | Loss: 0.00133095
Iteration 7/25 | Loss: 0.00131495
Iteration 8/25 | Loss: 0.00126109
Iteration 9/25 | Loss: 0.00124973
Iteration 10/25 | Loss: 0.00123902
Iteration 11/25 | Loss: 0.00123294
Iteration 12/25 | Loss: 0.00123791
Iteration 13/25 | Loss: 0.00123117
Iteration 14/25 | Loss: 0.00123346
Iteration 15/25 | Loss: 0.00123778
Iteration 16/25 | Loss: 0.00123730
Iteration 17/25 | Loss: 0.00123350
Iteration 18/25 | Loss: 0.00123086
Iteration 19/25 | Loss: 0.00122842
Iteration 20/25 | Loss: 0.00122762
Iteration 21/25 | Loss: 0.00122789
Iteration 22/25 | Loss: 0.00122749
Iteration 23/25 | Loss: 0.00123211
Iteration 24/25 | Loss: 0.00122939
Iteration 25/25 | Loss: 0.00122898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17538738
Iteration 2/25 | Loss: 0.00140937
Iteration 3/25 | Loss: 0.00140935
Iteration 4/25 | Loss: 0.00140935
Iteration 5/25 | Loss: 0.00140935
Iteration 6/25 | Loss: 0.00140935
Iteration 7/25 | Loss: 0.00140935
Iteration 8/25 | Loss: 0.00140935
Iteration 9/25 | Loss: 0.00140935
Iteration 10/25 | Loss: 0.00140935
Iteration 11/25 | Loss: 0.00140935
Iteration 12/25 | Loss: 0.00140935
Iteration 13/25 | Loss: 0.00140935
Iteration 14/25 | Loss: 0.00140935
Iteration 15/25 | Loss: 0.00140935
Iteration 16/25 | Loss: 0.00140935
Iteration 17/25 | Loss: 0.00140935
Iteration 18/25 | Loss: 0.00140935
Iteration 19/25 | Loss: 0.00140935
Iteration 20/25 | Loss: 0.00140935
Iteration 21/25 | Loss: 0.00140935
Iteration 22/25 | Loss: 0.00140935
Iteration 23/25 | Loss: 0.00140935
Iteration 24/25 | Loss: 0.00140935
Iteration 25/25 | Loss: 0.00140935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140935
Iteration 2/1000 | Loss: 0.00040609
Iteration 3/1000 | Loss: 0.00010216
Iteration 4/1000 | Loss: 0.00002787
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00028295
Iteration 8/1000 | Loss: 0.00044855
Iteration 9/1000 | Loss: 0.00014140
Iteration 10/1000 | Loss: 0.00030052
Iteration 11/1000 | Loss: 0.00015348
Iteration 12/1000 | Loss: 0.00025417
Iteration 13/1000 | Loss: 0.00046751
Iteration 14/1000 | Loss: 0.00011084
Iteration 15/1000 | Loss: 0.00003999
Iteration 16/1000 | Loss: 0.00002335
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00016727
Iteration 19/1000 | Loss: 0.00002307
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00002253
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001480
Iteration 28/1000 | Loss: 0.00018516
Iteration 29/1000 | Loss: 0.00023978
Iteration 30/1000 | Loss: 0.00038102
Iteration 31/1000 | Loss: 0.00013394
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00014383
Iteration 34/1000 | Loss: 0.00004625
Iteration 35/1000 | Loss: 0.00001869
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001366
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001258
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001211
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001211
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001209
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001208
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001206
Iteration 110/1000 | Loss: 0.00001206
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001198
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001196
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Iteration 166/1000 | Loss: 0.00001194
Iteration 167/1000 | Loss: 0.00001194
Iteration 168/1000 | Loss: 0.00001194
Iteration 169/1000 | Loss: 0.00001194
Iteration 170/1000 | Loss: 0.00001194
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001194
Iteration 175/1000 | Loss: 0.00001194
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001193
Iteration 179/1000 | Loss: 0.00001193
Iteration 180/1000 | Loss: 0.00001193
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001193
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001192
Iteration 189/1000 | Loss: 0.00001192
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001192
Iteration 195/1000 | Loss: 0.00001192
Iteration 196/1000 | Loss: 0.00001192
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Iteration 204/1000 | Loss: 0.00001191
Iteration 205/1000 | Loss: 0.00001191
Iteration 206/1000 | Loss: 0.00001191
Iteration 207/1000 | Loss: 0.00001191
Iteration 208/1000 | Loss: 0.00001191
Iteration 209/1000 | Loss: 0.00001190
Iteration 210/1000 | Loss: 0.00001190
Iteration 211/1000 | Loss: 0.00001190
Iteration 212/1000 | Loss: 0.00001190
Iteration 213/1000 | Loss: 0.00001190
Iteration 214/1000 | Loss: 0.00001189
Iteration 215/1000 | Loss: 0.00001189
Iteration 216/1000 | Loss: 0.00001189
Iteration 217/1000 | Loss: 0.00001189
Iteration 218/1000 | Loss: 0.00001189
Iteration 219/1000 | Loss: 0.00001189
Iteration 220/1000 | Loss: 0.00001189
Iteration 221/1000 | Loss: 0.00001189
Iteration 222/1000 | Loss: 0.00001189
Iteration 223/1000 | Loss: 0.00001189
Iteration 224/1000 | Loss: 0.00001189
Iteration 225/1000 | Loss: 0.00001189
Iteration 226/1000 | Loss: 0.00001188
Iteration 227/1000 | Loss: 0.00001188
Iteration 228/1000 | Loss: 0.00001188
Iteration 229/1000 | Loss: 0.00001188
Iteration 230/1000 | Loss: 0.00001188
Iteration 231/1000 | Loss: 0.00001188
Iteration 232/1000 | Loss: 0.00001188
Iteration 233/1000 | Loss: 0.00001187
Iteration 234/1000 | Loss: 0.00001187
Iteration 235/1000 | Loss: 0.00001187
Iteration 236/1000 | Loss: 0.00001187
Iteration 237/1000 | Loss: 0.00001187
Iteration 238/1000 | Loss: 0.00001187
Iteration 239/1000 | Loss: 0.00001187
Iteration 240/1000 | Loss: 0.00001187
Iteration 241/1000 | Loss: 0.00001187
Iteration 242/1000 | Loss: 0.00001187
Iteration 243/1000 | Loss: 0.00001187
Iteration 244/1000 | Loss: 0.00001187
Iteration 245/1000 | Loss: 0.00001187
Iteration 246/1000 | Loss: 0.00001187
Iteration 247/1000 | Loss: 0.00001187
Iteration 248/1000 | Loss: 0.00001186
Iteration 249/1000 | Loss: 0.00001186
Iteration 250/1000 | Loss: 0.00001186
Iteration 251/1000 | Loss: 0.00001186
Iteration 252/1000 | Loss: 0.00001186
Iteration 253/1000 | Loss: 0.00001186
Iteration 254/1000 | Loss: 0.00001186
Iteration 255/1000 | Loss: 0.00001186
Iteration 256/1000 | Loss: 0.00001186
Iteration 257/1000 | Loss: 0.00001186
Iteration 258/1000 | Loss: 0.00001185
Iteration 259/1000 | Loss: 0.00001185
Iteration 260/1000 | Loss: 0.00001185
Iteration 261/1000 | Loss: 0.00001185
Iteration 262/1000 | Loss: 0.00001185
Iteration 263/1000 | Loss: 0.00001185
Iteration 264/1000 | Loss: 0.00001185
Iteration 265/1000 | Loss: 0.00001185
Iteration 266/1000 | Loss: 0.00001185
Iteration 267/1000 | Loss: 0.00001184
Iteration 268/1000 | Loss: 0.00001184
Iteration 269/1000 | Loss: 0.00001184
Iteration 270/1000 | Loss: 0.00001184
Iteration 271/1000 | Loss: 0.00001184
Iteration 272/1000 | Loss: 0.00001184
Iteration 273/1000 | Loss: 0.00001183
Iteration 274/1000 | Loss: 0.00001183
Iteration 275/1000 | Loss: 0.00001183
Iteration 276/1000 | Loss: 0.00001183
Iteration 277/1000 | Loss: 0.00001183
Iteration 278/1000 | Loss: 0.00001183
Iteration 279/1000 | Loss: 0.00001183
Iteration 280/1000 | Loss: 0.00001183
Iteration 281/1000 | Loss: 0.00001183
Iteration 282/1000 | Loss: 0.00001183
Iteration 283/1000 | Loss: 0.00001183
Iteration 284/1000 | Loss: 0.00001183
Iteration 285/1000 | Loss: 0.00001183
Iteration 286/1000 | Loss: 0.00001182
Iteration 287/1000 | Loss: 0.00001182
Iteration 288/1000 | Loss: 0.00001182
Iteration 289/1000 | Loss: 0.00001182
Iteration 290/1000 | Loss: 0.00001182
Iteration 291/1000 | Loss: 0.00001182
Iteration 292/1000 | Loss: 0.00001181
Iteration 293/1000 | Loss: 0.00001181
Iteration 294/1000 | Loss: 0.00001181
Iteration 295/1000 | Loss: 0.00001181
Iteration 296/1000 | Loss: 0.00001181
Iteration 297/1000 | Loss: 0.00001181
Iteration 298/1000 | Loss: 0.00001181
Iteration 299/1000 | Loss: 0.00001181
Iteration 300/1000 | Loss: 0.00001181
Iteration 301/1000 | Loss: 0.00001181
Iteration 302/1000 | Loss: 0.00001181
Iteration 303/1000 | Loss: 0.00001181
Iteration 304/1000 | Loss: 0.00001181
Iteration 305/1000 | Loss: 0.00001180
Iteration 306/1000 | Loss: 0.00001180
Iteration 307/1000 | Loss: 0.00001180
Iteration 308/1000 | Loss: 0.00001180
Iteration 309/1000 | Loss: 0.00001180
Iteration 310/1000 | Loss: 0.00001180
Iteration 311/1000 | Loss: 0.00001180
Iteration 312/1000 | Loss: 0.00001180
Iteration 313/1000 | Loss: 0.00001180
Iteration 314/1000 | Loss: 0.00001180
Iteration 315/1000 | Loss: 0.00001180
Iteration 316/1000 | Loss: 0.00001180
Iteration 317/1000 | Loss: 0.00001180
Iteration 318/1000 | Loss: 0.00001180
Iteration 319/1000 | Loss: 0.00001180
Iteration 320/1000 | Loss: 0.00001180
Iteration 321/1000 | Loss: 0.00001180
Iteration 322/1000 | Loss: 0.00001180
Iteration 323/1000 | Loss: 0.00001180
Iteration 324/1000 | Loss: 0.00001180
Iteration 325/1000 | Loss: 0.00001180
Iteration 326/1000 | Loss: 0.00001180
Iteration 327/1000 | Loss: 0.00001180
Iteration 328/1000 | Loss: 0.00001180
Iteration 329/1000 | Loss: 0.00001180
Iteration 330/1000 | Loss: 0.00001180
Iteration 331/1000 | Loss: 0.00001180
Iteration 332/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [1.1799908861576114e-05, 1.1799908861576114e-05, 1.1799908861576114e-05, 1.1799908861576114e-05, 1.1799908861576114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1799908861576114e-05

Optimization complete. Final v2v error: 2.9304163455963135 mm

Highest mean error: 4.050705909729004 mm for frame 79

Lowest mean error: 2.502426862716675 mm for frame 226

Saving results

Total time: 140.09784603118896
